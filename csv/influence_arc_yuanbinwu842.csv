2020.acl-main.299,D16-1257,0,0.134774,"018) investigates such a model which predicts probability on each candidate span. It achieves quite promising parsing results, while the simple local probability factorization still leaves room for improvements. From the perspective of linearization, there are many ways to transform a structured tree into a shallow sequence. As a recent example, Shen et al. (2018) linearizes a tree with a sequence of numbers, each of which indicates words’ syntactic distance in the tree (i.e., height of the lowest common ancestor of two adjacent words). Similar ideas are also applied in Vinyals et al. (2015), Choe and Charniak (2016) and transition-based systems (Cross and Huang, 2016; Liu and Zhang, 2017a). With tree linearizations, the training time can be further accelerated to O(n), but the parsers often sacrifice a clear connection with original spans in trees, which makes both features and supervision signals from spans hard to use. In this work, we propose a novel linearization of constituent trees tied on their span representations. Given a sentence W and its parsing tree T , for each split point after wi in the sentence, we assign it a parsing target di , where (di , i) is the longest span ending with i in T . We"
2020.acl-main.299,D16-1001,0,0.536505,"point, and then predicts a tree span from them. Compared with global models, our model is fast and parallelizable. Different from previous local models, our linearization method is tied on the spans directly and considers more local features when performing span prediction, which is more interpretable and effective. Experiments on PTB (95.8 F1) and CTB (92.1 F1) show that our model significantly outperforms existing local models and efficiently achieves competitive results with global models. 1 Introduction Constituent parsers map natural language sentences to hierarchically organized spans (Cross and Huang, 2016). According to the complexity of decoders, two types of parsers have been studied, globally normalized models which normalize probability of a constituent tree on the whole candidate tree space (e.g. chart parser (Stern et al., 2017a)) and locally normalized models which normalize tree probability on smaller subtrees or spans. It is believed that global models have better parsing performance (Gaddy et al., 2018). But with the fast development of neural-network-based feature representations (Hochreiter and Schmidhuber, 1997; Vaswani et al., 2017), local models are able to get competitive parsin"
2020.acl-main.299,N19-1423,0,0.0307641,"Here the length l represents lengths between [l, l + 4]. for CTB. For character encoding, we randomly initialize the character embeddings with dimension 64. We use Adam optimizer with initial learning rate 1.0 and epsilon 10−9 . For LSTM encoder, we use a hidden size of 1024, with 0.33 dropout in all the feed-forward and recurrent connections. For Transformer encoder, we use the same hyperparameters as Kitaev and Klein (2018a). For split point representation, we apply two 1024-dimensional hidden size feed-forward networks. All the dropout we use in the decoder layer is 0.33. We also use BERT (Devlin et al., 2019) (uncased, 24 layers, 16 attention heads per layer and 1024-dimensional hidden vectors) and use the output of the last layer as the pre-trained word embeddings. 5 Training Details We use PyTorch as our neural network toolkit and run the code on a NVIDIA GeForce GTX Titan Xp GPU and Intel Xeon E52603 v4 CPU. All models are trained for up to 150 epochs with batch size 150 (Zhou and Zhao, 2019). 4.2 Main Results Table 2 shows the final results on PTB test set. Our models (92.6 F1 with LSTM, 93.7 F1 with Trans5 The source code for our model is publicly available: https://github.com/AntNLP/ span-li"
2020.acl-main.299,N16-1024,0,0.132807,"Missing"
2020.acl-main.299,P18-2075,0,0.0283238,"Missing"
2020.acl-main.299,P17-2025,0,0.0272976,"Missing"
2020.acl-main.299,N18-1091,0,0.0565128,"ieved that global models have better parsing performance (Gaddy et al., 2018). But with the fast development of neural-network-based feature representations (Hochreiter and Schmidhuber, 1997; Vaswani et al., 2017), local models are able to get competitive parsing accuracy while enjoying fast training and testing speed, and thus become an active research topic in constituent parsing. Locally normalized parsers usually rely on tree decompositions or linearizations. From the perspective of decomposition, the probability of trees can be factorized, for example, on individual spans. Teng and Zhang (2018) investigates such a model which predicts probability on each candidate span. It achieves quite promising parsing results, while the simple local probability factorization still leaves room for improvements. From the perspective of linearization, there are many ways to transform a structured tree into a shallow sequence. As a recent example, Shen et al. (2018) linearizes a tree with a sequence of numbers, each of which indicates words’ syntactic distance in the tree (i.e., height of the lowest common ancestor of two adjacent words). Similar ideas are also applied in Vinyals et al. (2015), Choe"
2020.acl-main.299,D18-1162,0,0.0553168,"ieved that global models have better parsing performance (Gaddy et al., 2018). But with the fast development of neural-network-based feature representations (Hochreiter and Schmidhuber, 1997; Vaswani et al., 2017), local models are able to get competitive parsing accuracy while enjoying fast training and testing speed, and thus become an active research topic in constituent parsing. Locally normalized parsers usually rely on tree decompositions or linearizations. From the perspective of decomposition, the probability of trees can be factorized, for example, on individual spans. Teng and Zhang (2018) investigates such a model which predicts probability on each candidate span. It achieves quite promising parsing results, while the simple local probability factorization still leaves room for improvements. From the perspective of linearization, there are many ways to transform a structured tree into a shallow sequence. As a recent example, Shen et al. (2018) linearizes a tree with a sequence of numbers, each of which indicates words’ syntactic distance in the tree (i.e., height of the lowest common ancestor of two adjacent words). Similar ideas are also applied in Vinyals et al. (2015), Choe"
2020.acl-main.299,P82-1020,0,0.79849,"Missing"
2020.acl-main.299,P18-2076,0,0.038615,"Missing"
2020.acl-main.299,P19-1237,1,0.850836,"arallel. Sequence labeling models regard tree prediction as sequence prediction problem (G´omez-Rodr´ıguez and Vilares, 2018; Shen et al., 2018). These models have high efficiency, but their linearizations have no direct relation to the spans, so the performance is much worse than span-based models. We propose a novel linearization method closely related to the spans and decode the tree in O(n log n) complexity. Compared with Teng and Zhang (2018), we do normalization on more spans, thus achieve a better performance. In future work, we will apply graph neural network (Velickovic et al., 2018; Ji et al., 2019; Sun et al., 2019) to enhance the span representation. Due to the excellent properties of our linearization, we can jointly learn constituent parsing and dependency parsing in one graph-based model. In addition, there is also a right linearization defined on the set of right child spans. We can study how to combine the two linear representations to further improve the performance of the model. 6 Conclusion In this work, we propose a novel linearization of constituent trees tied on the spans tightly. In addition, we build a new normalization method, which can add constraints on all the spans w"
2020.acl-main.299,P18-1249,0,0.259343,"ed by a character-level LSTM and a randomly initialized part-of-speech tag embedding pi . We concatenate these three embeddings to generate a representation of word wi , xi = [ei ; ci ; pi ]. To get the representation of the split points, the word representation matrix X = [x1 , x2 , . . . , xn ] is fed into a bidirectional LSTM or Transformer (Vaswani et al., 2017) firstly. Then we calculate the representation of the split point between wi and wi+1 using the outputs from the encoders, 3269 → ← hi = [ h i ; h i+1 ]. (1) → Note that for Transformer encoder, h i is calculated in the same way as Kitaev and Klein (2018a). 3.2 Decoder Since a split point can play two different roles when it is the left or right boundary of a span, we use two different vectors to represent the two roles inspired by Dozat and Manning (2017). Concretely, we use two multi-layer perceptrons to generate two different representations, li = MLPl (hi ), ri = MLPr (hi ). (2) Then we can define the score of span (i, j) using a biaffine attention function (Dozat and Manning, 2017; Li et al., 2019), &gt; αij = li&gt; Wrj + b&gt; 1 li + b2 rj , where W, b1 and b2 are all model parameters. αij measures the possibility of (i, j) being a left child s"
2020.acl-main.299,N15-1142,0,0.0671447,"Missing"
2020.acl-main.299,Q17-1029,0,0.0938574,"an. It achieves quite promising parsing results, while the simple local probability factorization still leaves room for improvements. From the perspective of linearization, there are many ways to transform a structured tree into a shallow sequence. As a recent example, Shen et al. (2018) linearizes a tree with a sequence of numbers, each of which indicates words’ syntactic distance in the tree (i.e., height of the lowest common ancestor of two adjacent words). Similar ideas are also applied in Vinyals et al. (2015), Choe and Charniak (2016) and transition-based systems (Cross and Huang, 2016; Liu and Zhang, 2017a). With tree linearizations, the training time can be further accelerated to O(n), but the parsers often sacrifice a clear connection with original spans in trees, which makes both features and supervision signals from spans hard to use. In this work, we propose a novel linearization of constituent trees tied on their span representations. Given a sentence W and its parsing tree T , for each split point after wi in the sentence, we assign it a parsing target di , where (di , i) is the longest span ending with i in T . We can show that, for a binary parsing tree, the set {(di , i)} includes al"
2020.acl-main.299,Q17-1004,0,0.417416,"an. It achieves quite promising parsing results, while the simple local probability factorization still leaves room for improvements. From the perspective of linearization, there are many ways to transform a structured tree into a shallow sequence. As a recent example, Shen et al. (2018) linearizes a tree with a sequence of numbers, each of which indicates words’ syntactic distance in the tree (i.e., height of the lowest common ancestor of two adjacent words). Similar ideas are also applied in Vinyals et al. (2015), Choe and Charniak (2016) and transition-based systems (Cross and Huang, 2016; Liu and Zhang, 2017a). With tree linearizations, the training time can be further accelerated to O(n), but the parsers often sacrifice a clear connection with original spans in trees, which makes both features and supervision signals from spans hard to use. In this work, we propose a novel linearization of constituent trees tied on their span representations. Given a sentence W and its parsing tree T , for each split point after wi in the sentence, we assign it a parsing target di , where (di , i) is the longest span ending with i in T . We can show that, for a binary parsing tree, the set {(di , i)} includes al"
2020.acl-main.299,J93-2004,0,0.0698688,"eviewer for pointing out the connection. The following discussions are based on his/her detailed reviews. same span (i, j), their normalization is constrained within (i, j), while ours is over all i0 &lt; j. The main advantage of our parser is simpler span representations (not depend on parent spans): it makes the parser easy to batch for sentences with different lengths and tree structures since each di can be calculated offline before training. 4 Experiments 4.1 Data and Settings Datasets and Preprocessing All models are trained on two standard benchmark treebanks, English Penn Treebank (PTB) (Marcus et al., 1993) and Chinese Penn Treebank (CTB) 5.1. The POS tags are predicted using Stanford Tagger (Toutanova et al., 2003). To clean the treebanks, we strip the leaf nodes with POS tag -NONE- from the two treebanks and delete the root nodes with constituent type ROOT. For evaluating the results, we use the standard evaluation tool 4 . For words in the testing corpus but not in the training corpus, we replace them with a unique label &lt;UNK&gt;. We also replace the words in the training corpus with the unknown label &lt;UNK&gt; z with probability punk (w) = z+c(w) , where c(w) is the number of time word w appears in"
2020.acl-main.299,D14-1162,0,0.111998,"Missing"
2020.acl-main.299,P18-1108,0,0.471876,"earch topic in constituent parsing. Locally normalized parsers usually rely on tree decompositions or linearizations. From the perspective of decomposition, the probability of trees can be factorized, for example, on individual spans. Teng and Zhang (2018) investigates such a model which predicts probability on each candidate span. It achieves quite promising parsing results, while the simple local probability factorization still leaves room for improvements. From the perspective of linearization, there are many ways to transform a structured tree into a shallow sequence. As a recent example, Shen et al. (2018) linearizes a tree with a sequence of numbers, each of which indicates words’ syntactic distance in the tree (i.e., height of the lowest common ancestor of two adjacent words). Similar ideas are also applied in Vinyals et al. (2015), Choe and Charniak (2016) and transition-based systems (Cross and Huang, 2016; Liu and Zhang, 2017a). With tree linearizations, the training time can be further accelerated to O(n), but the parsers often sacrifice a clear connection with original spans in trees, which makes both features and supervision signals from spans hard to use. In this work, we propose a nov"
2020.acl-main.299,P17-1076,0,0.0899763,"atures when performing span prediction, which is more interpretable and effective. Experiments on PTB (95.8 F1) and CTB (92.1 F1) show that our model significantly outperforms existing local models and efficiently achieves competitive results with global models. 1 Introduction Constituent parsers map natural language sentences to hierarchically organized spans (Cross and Huang, 2016). According to the complexity of decoders, two types of parsers have been studied, globally normalized models which normalize probability of a constituent tree on the whole candidate tree space (e.g. chart parser (Stern et al., 2017a)) and locally normalized models which normalize tree probability on smaller subtrees or spans. It is believed that global models have better parsing performance (Gaddy et al., 2018). But with the fast development of neural-network-based feature representations (Hochreiter and Schmidhuber, 1997; Vaswani et al., 2017), local models are able to get competitive parsing accuracy while enjoying fast training and testing speed, and thus become an active research topic in constituent parsing. Locally normalized parsers usually rely on tree decompositions or linearizations. From the perspective of de"
2020.acl-main.299,D17-1178,0,0.0539423,"atures when performing span prediction, which is more interpretable and effective. Experiments on PTB (95.8 F1) and CTB (92.1 F1) show that our model significantly outperforms existing local models and efficiently achieves competitive results with global models. 1 Introduction Constituent parsers map natural language sentences to hierarchically organized spans (Cross and Huang, 2016). According to the complexity of decoders, two types of parsers have been studied, globally normalized models which normalize probability of a constituent tree on the whole candidate tree space (e.g. chart parser (Stern et al., 2017a)) and locally normalized models which normalize tree probability on smaller subtrees or spans. It is believed that global models have better parsing performance (Gaddy et al., 2018). But with the fast development of neural-network-based feature representations (Hochreiter and Schmidhuber, 1997; Vaswani et al., 2017), local models are able to get competitive parsing accuracy while enjoying fast training and testing speed, and thus become an active research topic in constituent parsing. Locally normalized parsers usually rely on tree decompositions or linearizations. From the perspective of de"
2020.acl-main.299,P19-1131,1,0.84054,"labeling models regard tree prediction as sequence prediction problem (G´omez-Rodr´ıguez and Vilares, 2018; Shen et al., 2018). These models have high efficiency, but their linearizations have no direct relation to the spans, so the performance is much worse than span-based models. We propose a novel linearization method closely related to the spans and decode the tree in O(n log n) complexity. Compared with Teng and Zhang (2018), we do normalization on more spans, thus achieve a better performance. In future work, we will apply graph neural network (Velickovic et al., 2018; Ji et al., 2019; Sun et al., 2019) to enhance the span representation. Due to the excellent properties of our linearization, we can jointly learn constituent parsing and dependency parsing in one graph-based model. In addition, there is also a right linearization defined on the set of right child spans. We can study how to combine the two linear representations to further improve the performance of the model. 6 Conclusion In this work, we propose a novel linearization of constituent trees tied on the spans tightly. In addition, we build a new normalization method, which can add constraints on all the spans with the same right"
2020.acl-main.299,C18-1011,0,0.0671695,"pans. It is believed that global models have better parsing performance (Gaddy et al., 2018). But with the fast development of neural-network-based feature representations (Hochreiter and Schmidhuber, 1997; Vaswani et al., 2017), local models are able to get competitive parsing accuracy while enjoying fast training and testing speed, and thus become an active research topic in constituent parsing. Locally normalized parsers usually rely on tree decompositions or linearizations. From the perspective of decomposition, the probability of trees can be factorized, for example, on individual spans. Teng and Zhang (2018) investigates such a model which predicts probability on each candidate span. It achieves quite promising parsing results, while the simple local probability factorization still leaves room for improvements. From the perspective of linearization, there are many ways to transform a structured tree into a shallow sequence. As a recent example, Shen et al. (2018) linearizes a tree with a sequence of numbers, each of which indicates words’ syntactic distance in the tree (i.e., height of the lowest common ancestor of two adjacent words). Similar ideas are also applied in Vinyals et al. (2015), Choe"
2020.acl-main.299,P19-1230,0,0.169526,"rameters as Kitaev and Klein (2018a). For split point representation, we apply two 1024-dimensional hidden size feed-forward networks. All the dropout we use in the decoder layer is 0.33. We also use BERT (Devlin et al., 2019) (uncased, 24 layers, 16 attention heads per layer and 1024-dimensional hidden vectors) and use the output of the last layer as the pre-trained word embeddings. 5 Training Details We use PyTorch as our neural network toolkit and run the code on a NVIDIA GeForce GTX Titan Xp GPU and Intel Xeon E52603 v4 CPU. All models are trained for up to 150 epochs with batch size 150 (Zhou and Zhao, 2019). 4.2 Main Results Table 2 shows the final results on PTB test set. Our models (92.6 F1 with LSTM, 93.7 F1 with Trans5 The source code for our model is publicly available: https://github.com/AntNLP/ span-linearization-parser former) significantly outperform the single locally normalized models. Compared with globally normalized models, our models also outperform those parsers with LSTM encoder and achieve a competitive result with Transformer encoder parsers. With the help of BERT (Devlin et al., 2018), our models with two encoders both achieve the same performance (95.8 F1) as the best parser"
2020.acl-main.299,N03-1033,0,0.294773,"me span (i, j), their normalization is constrained within (i, j), while ours is over all i0 &lt; j. The main advantage of our parser is simpler span representations (not depend on parent spans): it makes the parser easy to batch for sentences with different lengths and tree structures since each di can be calculated offline before training. 4 Experiments 4.1 Data and Settings Datasets and Preprocessing All models are trained on two standard benchmark treebanks, English Penn Treebank (PTB) (Marcus et al., 1993) and Chinese Penn Treebank (CTB) 5.1. The POS tags are predicted using Stanford Tagger (Toutanova et al., 2003). To clean the treebanks, we strip the leaf nodes with POS tag -NONE- from the two treebanks and delete the root nodes with constituent type ROOT. For evaluating the results, we use the standard evaluation tool 4 . For words in the testing corpus but not in the training corpus, we replace them with a unique label &lt;UNK&gt;. We also replace the words in the training corpus with the unknown label &lt;UNK&gt; z with probability punk (w) = z+c(w) , where c(w) is the number of time word w appears in the training corpus and we set z = 0.8375 as Cross and Huang (2016). Hyperparameters We use 100D GloVe (Pennin"
2020.acl-main.299,Q17-1019,0,0.018773,"ilares (2018). However, the performance is very poor, and this is largely due to the loss of structural information in the label prediction. Therefore, how to balance efficiency and label prediction accuracy might be a research Inference Algorithms Related Work Globally normalized parsers often have high performance on constituent parsing due to their search on the global state space (Stern et al., 2017a; Kitaev and Klein, 2018a; Zhou and Zhao, 2019). However, they suffer from high time complexity and are difficult to parallelize. Thus many efforts have been made to optimize their efficiency (Vieira and Eisner, 2017). Recently, the rapid development of encoders (Hochreiter and Schmidhuber, 1997; Vaswani et al., 2017) and pre-trained language models (Devlin et al., 2018) have enabled local models to achieve similar performance as global models. Teng and Zhang (2018) propose two local models, one does normalization on each candidate span and one on each grammar rule. Their models even outperform the global model in Stern et al. (2017a) thanks to the better representation of spans. However, they still need an O(n3 ) complexity inference algorithm to reconstruct the final parsing tree. 3274 6 https://cython.o"
2020.acl-main.299,N19-1341,0,0.060902,"mpler settings. Our own implementation achieves the same result as they report (92.4 F1). For convenience, we call their model per-span-normalization (PSN for short) model in the following. Influence of Span Length First, we analyse the influence of different lengths of spans and the results are shown in Figure 3. We find that for sentences of lengths between [11, 45], our model significantly outperforms PSN model. For short 3272 Model LR LP Global Model Stern et al. (2017a) 90.6 93.0 Gaddy et al. (2018) 91.8 92.4 Kitaev and Klein (2018a)♠ 93.2 93.9 Zhou and Zhao (2019)♠ 93.6 93.9 Local Model Vilares et al. (2019) Liu et al. (2018) Ma et al. (2017) Shen et al. (2018) 91.7 92.0 Liu and Zhang (2017a) Hong and Huang (2018) 91.5 92.5 Teng and Zhang (2018) 92.2 92.5 Dyer et al. (2016)♥ ♥ Stern et al. (2017b) 92.6 92.6 Our Model 92.3 92.9 Our Model♠ 93.3 94.1 Pre-training/Ensemble/Re-ranking Liu et al. (2018) Choe and Charniak (2016) Liu and Zhang (2017a) Fried et al. (2017) Kitaev and Klein (2018a)♠ 94.9 95.4 Kitaev and Klein (2018b)♠ 95.5 95.7 Zhou and Zhao (2019)♠ 95.7 96.0 Our Model (+BERT) 95.6 96.0 Our Model (+BERT)♠ 95.5 96.1 F1 Model LR LP Global Model Kitaev and Klein (2018a)♠ 86.8 88.1 Zhou and Zha"
2020.acl-main.299,P15-1113,0,0.112726,"018) have enabled local models to achieve similar performance as global models. Teng and Zhang (2018) propose two local models, one does normalization on each candidate span and one on each grammar rule. Their models even outperform the global model in Stern et al. (2017a) thanks to the better representation of spans. However, they still need an O(n3 ) complexity inference algorithm to reconstruct the final parsing tree. 3274 6 https://cython.org/ Meanwhile, many work do research on faster sequential models. Transition-based models predict a sequence of actions and achieve an O(n) complexity (Watanabe and Sumita, 2015; Cross and Huang, 2016; Liu and Zhang, 2017a). However, they suffer from the issue of error propagation and cannot be parallel. Sequence labeling models regard tree prediction as sequence prediction problem (G´omez-Rodr´ıguez and Vilares, 2018; Shen et al., 2018). These models have high efficiency, but their linearizations have no direct relation to the spans, so the performance is much worse than span-based models. We propose a novel linearization method closely related to the spans and decode the tree in O(n log n) complexity. Compared with Teng and Zhang (2018), we do normalization on more"
2020.acl-main.299,N03-1031,0,\N,Missing
2020.coling-main.49,P07-1056,0,0.657022,"set (Ni et al., 2019) and Yelp 2020 challenge dataset5 . Amazon dataset contains 233 million reviews within 29 domains (Ni et al., 2019). The total number of yelp reviews is about 8 million. We preprocess the text via NLTK6 and transfer all the letters into lower. We filter the text that contains less than 50 tokens or more than 512 tokens and sample the rating data in dealing with class-imbalance problem. The statistics information of top-20 emoticons is shown in Table 1. Sentiment Analysis To verify the effectiveness of S ENTI X, We evaluate on the widely used crossdomain sentiment dataset (Blitzer et al., 2007), containing four domains: Books (B), DVD (D), Electronic (E), and Kitchen & Housewares (K). Following the setting of previous works (Ziser and Reichart, 2018; Qu et al., 2019), we test on 12 cross-domain tasks. The model is trained on the source domain and tested on the target domains. As before, we split 200 samples from 2000 samples in source domain as a development set to find the best hyperparameters. Besides, we examine the model on four popular sentient classification datasets: SST-2-Root (Socher et al., 2013), SST-5-Root (Socher et al., 2013), IMDB (Maas et al., 2011) and Yelp (Zhang e"
2020.coling-main.49,N19-1423,0,0.527806,"language expressions for sentimental text usually vary across different domains. For instance, “fast” has a positive sentiment towards “service” in the restaurant domain (Figure 1), while in the laptop domain, “fast” expresses a negative sentiment for “power consumption”. Furthermore, models trained on the source domain tend to overfit, since they learn domain-specific knowledge excessively. Therefore, many studies (Du et al., 2020; Ziser and Reichart, 2018; Li et al., 2018) propose to address this issue by extracting domain-invariant features. Recently, pre-trained language models like BERT (Devlin et al., 2019) have achieved the state-of-theart performance on multiple sentiment analysis tasks (Hoang et al., 2019; Munikar et al., 2019; Raffel et al., 2019). However, when they are directly applied to cross-domain sentiment analysis (Du et al., 2020), two problems arise: 1) Existing pre-trained models focus on learning the semantic content via self-supervision strategies, while ignoring sentiment-specific knowledge at the pre-training phrase; 2) ˚ Yuanbin Wu and Liang He are the corresponding authors of this paper. This work was conducted when Jie Zhou was interning at Alibaba DAMO Academy. This work i"
2020.coling-main.49,P14-2009,0,0.0274439,"al., 2019), we test on 12 cross-domain tasks. The model is trained on the source domain and tested on the target domains. As before, we split 200 samples from 2000 samples in source domain as a development set to find the best hyperparameters. Besides, we examine the model on four popular sentient classification datasets: SST-2-Root (Socher et al., 2013), SST-5-Root (Socher et al., 2013), IMDB (Maas et al., 2011) and Yelp (Zhang et al., 2015), and three aspect-based sentiment classification datasets: Restaurant14 (Pontiki et al., 2016) and Laptop14 (Pontiki et al., 2016), and Twitter dataset (Dong et al., 2014). 4.2 Baselines We compare our model with the following strong baselines for cross-domain sentiment analysis, including DANN (Ganin et al., 2016), PBLM (Ziser and Reichart, 2018), HATN (Li et al., 2018), ACAN (Qu et al., 2019), IATN (Zhang et al., 2019a) and BERT-DAAT (Du et al., 2020). BERT-DAAT is regarded as the state-of-the-art model, which uses BERT for cross-domain sentiment analysis with adversarial training. We adopt the results of these baselines reported in (Du et al., 2020). For in-domain sentiment analysis, we compare our model with SentiLR-B (Ke et al., 2019), which is one of the"
2020.coling-main.49,2020.acl-main.370,0,0.346247,"entiment analysis has become a promising direction, which transfers (invariant) sentiment knowledge from the source domain to the target domain1 . The major challenge here is that language expressions for sentimental text usually vary across different domains. For instance, “fast” has a positive sentiment towards “service” in the restaurant domain (Figure 1), while in the laptop domain, “fast” expresses a negative sentiment for “power consumption”. Furthermore, models trained on the source domain tend to overfit, since they learn domain-specific knowledge excessively. Therefore, many studies (Du et al., 2020; Ziser and Reichart, 2018; Li et al., 2018) propose to address this issue by extracting domain-invariant features. Recently, pre-trained language models like BERT (Devlin et al., 2019) have achieved the state-of-theart performance on multiple sentiment analysis tasks (Hoang et al., 2019; Munikar et al., 2019; Raffel et al., 2019). However, when they are directly applied to cross-domain sentiment analysis (Du et al., 2020), two problems arise: 1) Existing pre-trained models focus on learning the semantic content via self-supervision strategies, while ignoring sentiment-specific knowledge at th"
2020.coling-main.49,W19-6120,0,0.0132226,"a positive sentiment towards “service” in the restaurant domain (Figure 1), while in the laptop domain, “fast” expresses a negative sentiment for “power consumption”. Furthermore, models trained on the source domain tend to overfit, since they learn domain-specific knowledge excessively. Therefore, many studies (Du et al., 2020; Ziser and Reichart, 2018; Li et al., 2018) propose to address this issue by extracting domain-invariant features. Recently, pre-trained language models like BERT (Devlin et al., 2019) have achieved the state-of-theart performance on multiple sentiment analysis tasks (Hoang et al., 2019; Munikar et al., 2019; Raffel et al., 2019). However, when they are directly applied to cross-domain sentiment analysis (Du et al., 2020), two problems arise: 1) Existing pre-trained models focus on learning the semantic content via self-supervision strategies, while ignoring sentiment-specific knowledge at the pre-training phrase; 2) ˚ Yuanbin Wu and Liang He are the corresponding authors of this paper. This work was conducted when Jie Zhou was interning at Alibaba DAMO Academy. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http:// cre"
2020.coling-main.49,P11-1015,0,0.0839079,"iment dataset (Blitzer et al., 2007), containing four domains: Books (B), DVD (D), Electronic (E), and Kitchen & Housewares (K). Following the setting of previous works (Ziser and Reichart, 2018; Qu et al., 2019), we test on 12 cross-domain tasks. The model is trained on the source domain and tested on the target domains. As before, we split 200 samples from 2000 samples in source domain as a development set to find the best hyperparameters. Besides, we examine the model on four popular sentient classification datasets: SST-2-Root (Socher et al., 2013), SST-5-Root (Socher et al., 2013), IMDB (Maas et al., 2011) and Yelp (Zhang et al., 2015), and three aspect-based sentiment classification datasets: Restaurant14 (Pontiki et al., 2016) and Laptop14 (Pontiki et al., 2016), and Twitter dataset (Dong et al., 2014). 4.2 Baselines We compare our model with the following strong baselines for cross-domain sentiment analysis, including DANN (Ganin et al., 2016), PBLM (Ziser and Reichart, 2018), HATN (Li et al., 2018), ACAN (Qu et al., 2019), IATN (Zhang et al., 2019a) and BERT-DAAT (Du et al., 2020). BERT-DAAT is regarded as the state-of-the-art model, which uses BERT for cross-domain sentiment analysis with"
2020.coling-main.49,D19-1018,0,0.0185346,"): :D (8 <3 ;-) Count 160,970 113,440 86,546 78,492 74,247 # 11 12 13 14 15 Emoticon :/ =) :-( 8: 8) Count 67,615 66,156 64,391 53,073 46,124 # 16 17 18 19 20 Emoticon (: :P ;D :o) =( Count 40,062 31,573 15,718 12,952 11,917 Table 1: Statistics information of top-20 emoticons. 3.3 Joint Training Finally, we jointly optimize the token-level objective LT and the sentence-level objective LS . The overall loss is L “ LT ` LS , where LT “ Lw ` Ls ` Le and LS “ Lr . 4 Experimental Setup 4.1 Datasets Pre-training The pre-training phase is conducted on two large-scale datasets: Amazon review dataset (Ni et al., 2019) and Yelp 2020 challenge dataset5 . Amazon dataset contains 233 million reviews within 29 domains (Ni et al., 2019). The total number of yelp reviews is about 8 million. We preprocess the text via NLTK6 and transfer all the letters into lower. We filter the text that contains less than 50 tokens or more than 512 tokens and sample the rating data in dealing with class-imbalance problem. The statistics information of top-20 emoticons is shown in Table 1. Sentiment Analysis To verify the effectiveness of S ENTI X, We evaluate on the widely used crossdomain sentiment dataset (Blitzer et al., 2007)"
2020.coling-main.49,P18-1233,0,0.0203972,"ce (Accuracy) Epoch 2 Epoch 3 Epoch 4 73.85 84.05 85.45 50.00 52.45 50.80 92.30 92.05 91.10 91.45 92.25 92.75 Epoch 5 86.55 51.10 90.85 93.05 Table 5: The results of complexity and convergence. We list the costed time of each epoch and space complexity of trainable parameters in B Ñ E task with the same batchsize. 6 Related Work Cross-domain Sentiment Analysis Due to the heavy cost of obtaining large quantities of labeled data for each domain, many approaches have been proposed for cross-domain sentiment analysis (Blitzer et al., 2007; Yu and Jiang, 2016; Li et al., 2013; Zhang et al., 2019a; Peng et al., 2018). Most of the previous works focus on capturing the pivots that are useful for both source domain and target domain (Ziser and Reichart, 2018; Li et al., 2018). Domain adaptation adversarial training (Ganin et al., 2016) is widelyused to learn the domain-common sentiment knowledge (Li et al., 2017; Qu et al., 2019). Recently, Du et al. (2020) integrated BERT into cross-domain sentiment analysis tasks to learn the domain-shared feature representation. However, most of the existing work focuses on learning the domain-shared representation in training or fine-tuning, how to learn domain-invariant"
2020.coling-main.49,N18-1202,0,0.0591782,"domain (Ziser and Reichart, 2018; Li et al., 2018). Domain adaptation adversarial training (Ganin et al., 2016) is widelyused to learn the domain-common sentiment knowledge (Li et al., 2017; Qu et al., 2019). Recently, Du et al. (2020) integrated BERT into cross-domain sentiment analysis tasks to learn the domain-shared feature representation. However, most of the existing work focuses on learning the domain-shared representation in training or fine-tuning, how to learn domain-invariant sentiment knowledge from the pre-training phase has not been explored. Pre-trained Model Existing studies (Peters et al., 2018; Devlin et al., 2019) have proved that pretraining on large-scale unlabelled corpus obtains state-of-the-art performances in the field of natural language processing (Qiu et al., 2020). On the one hand, many studies applied pre-trained models to downstream tasks via fine-tuning (Devlin et al., 2019; Dodge et al., 2020; Sun et al., 2019b; Xu et al., 2019). Devlin et al. (2019) fine-tuned the BERT model on many downstream tasks, such as name entity recognition and sentiment analysis. Sun et al. (2019a) converted aspect-based sentiment analysis task into a sentence pair classification task to be"
2020.coling-main.49,D19-1005,0,0.0326198,"Missing"
2020.coling-main.49,N19-1258,0,0.115047,"million. We preprocess the text via NLTK6 and transfer all the letters into lower. We filter the text that contains less than 50 tokens or more than 512 tokens and sample the rating data in dealing with class-imbalance problem. The statistics information of top-20 emoticons is shown in Table 1. Sentiment Analysis To verify the effectiveness of S ENTI X, We evaluate on the widely used crossdomain sentiment dataset (Blitzer et al., 2007), containing four domains: Books (B), DVD (D), Electronic (E), and Kitchen & Housewares (K). Following the setting of previous works (Ziser and Reichart, 2018; Qu et al., 2019), we test on 12 cross-domain tasks. The model is trained on the source domain and tested on the target domains. As before, we split 200 samples from 2000 samples in source domain as a development set to find the best hyperparameters. Besides, we examine the model on four popular sentient classification datasets: SST-2-Root (Socher et al., 2013), SST-5-Root (Socher et al., 2013), IMDB (Maas et al., 2011) and Yelp (Zhang et al., 2015), and three aspect-based sentiment classification datasets: Restaurant14 (Pontiki et al., 2016) and Laptop14 (Pontiki et al., 2016), and Twitter dataset (Dong et al"
2020.coling-main.49,D13-1170,0,0.00654472,"s of S ENTI X, We evaluate on the widely used crossdomain sentiment dataset (Blitzer et al., 2007), containing four domains: Books (B), DVD (D), Electronic (E), and Kitchen & Housewares (K). Following the setting of previous works (Ziser and Reichart, 2018; Qu et al., 2019), we test on 12 cross-domain tasks. The model is trained on the source domain and tested on the target domains. As before, we split 200 samples from 2000 samples in source domain as a development set to find the best hyperparameters. Besides, we examine the model on four popular sentient classification datasets: SST-2-Root (Socher et al., 2013), SST-5-Root (Socher et al., 2013), IMDB (Maas et al., 2011) and Yelp (Zhang et al., 2015), and three aspect-based sentiment classification datasets: Restaurant14 (Pontiki et al., 2016) and Laptop14 (Pontiki et al., 2016), and Twitter dataset (Dong et al., 2014). 4.2 Baselines We compare our model with the following strong baselines for cross-domain sentiment analysis, including DANN (Ganin et al., 2016), PBLM (Ziser and Reichart, 2018), HATN (Li et al., 2018), ACAN (Qu et al., 2019), IATN (Zhang et al., 2019a) and BERT-DAAT (Du et al., 2020). BERT-DAAT is regarded as the state-of-the-art mode"
2020.coling-main.49,N19-1035,0,0.02339,"tion. However, most of the existing work focuses on learning the domain-shared representation in training or fine-tuning, how to learn domain-invariant sentiment knowledge from the pre-training phase has not been explored. Pre-trained Model Existing studies (Peters et al., 2018; Devlin et al., 2019) have proved that pretraining on large-scale unlabelled corpus obtains state-of-the-art performances in the field of natural language processing (Qiu et al., 2020). On the one hand, many studies applied pre-trained models to downstream tasks via fine-tuning (Devlin et al., 2019; Dodge et al., 2020; Sun et al., 2019b; Xu et al., 2019). Devlin et al. (2019) fine-tuned the BERT model on many downstream tasks, such as name entity recognition and sentiment analysis. Sun et al. (2019a) converted aspect-based sentiment analysis task into a sentence pair classification task to better utilize the powerful representation of BERT. On the other hand, some work proposed to add external knowledge into pre-training BERT to enhance the representations (Zhang et al., 2020). LIBERT (Lauscher et al., 2019) integrated linguistic knowledge through an additional linguistic constraint task. ERINE (Zhang et al., 2019b) and Kno"
2020.coling-main.49,2020.acl-main.374,0,0.0405769,"k into a sentence pair classification task to better utilize the powerful representation of BERT. On the other hand, some work proposed to add external knowledge into pre-training BERT to enhance the representations (Zhang et al., 2020). LIBERT (Lauscher et al., 2019) integrated linguistic knowledge through an additional linguistic constraint task. ERINE (Zhang et al., 2019b) and KnowBERT (Peters 576 et al., 2019) integrated entity representation into BERT. Alternatively, Levine et at. (2019) introduced a SenseBERT to improve lexical understanding by predicting tokens’ supersenses in WordNet. Tian et al. (2020) and Ke et al. (2019) integrated external knowledge to learn sentiment information. They focused on improving the performance with fine-tuning on downstream sentiment analysis tasks by training on a relatively small or one domain dataset. Different from the existing studies, we design several pre-training objectives via rich domain-invariant sentiment knowledge in large-scale multi-domain unlabeled data for cross-domain sentiment analysis. 7 Conclusions In this paper, we pre-train our S ENTI X model to induce a general low dimensional representation based on domain-invariant sentiment knowledg"
2020.coling-main.49,N19-1242,0,0.0236828,"of the existing work focuses on learning the domain-shared representation in training or fine-tuning, how to learn domain-invariant sentiment knowledge from the pre-training phase has not been explored. Pre-trained Model Existing studies (Peters et al., 2018; Devlin et al., 2019) have proved that pretraining on large-scale unlabelled corpus obtains state-of-the-art performances in the field of natural language processing (Qiu et al., 2020). On the one hand, many studies applied pre-trained models to downstream tasks via fine-tuning (Devlin et al., 2019; Dodge et al., 2020; Sun et al., 2019b; Xu et al., 2019). Devlin et al. (2019) fine-tuned the BERT model on many downstream tasks, such as name entity recognition and sentiment analysis. Sun et al. (2019a) converted aspect-based sentiment analysis task into a sentence pair classification task to better utilize the powerful representation of BERT. On the other hand, some work proposed to add external knowledge into pre-training BERT to enhance the representations (Zhang et al., 2020). LIBERT (Lauscher et al., 2019) integrated linguistic knowledge through an additional linguistic constraint task. ERINE (Zhang et al., 2019b) and KnowBERT (Peters 576 e"
2020.coling-main.49,D16-1023,0,0.0202871,"133M 2K 133M 2K Epoch 1 52.20 50.65 92.60 90.55 Convergence (Accuracy) Epoch 2 Epoch 3 Epoch 4 73.85 84.05 85.45 50.00 52.45 50.80 92.30 92.05 91.10 91.45 92.25 92.75 Epoch 5 86.55 51.10 90.85 93.05 Table 5: The results of complexity and convergence. We list the costed time of each epoch and space complexity of trainable parameters in B Ñ E task with the same batchsize. 6 Related Work Cross-domain Sentiment Analysis Due to the heavy cost of obtaining large quantities of labeled data for each domain, many approaches have been proposed for cross-domain sentiment analysis (Blitzer et al., 2007; Yu and Jiang, 2016; Li et al., 2013; Zhang et al., 2019a; Peng et al., 2018). Most of the previous works focus on capturing the pivots that are useful for both source domain and target domain (Ziser and Reichart, 2018; Li et al., 2018). Domain adaptation adversarial training (Ganin et al., 2016) is widelyused to learn the domain-common sentiment knowledge (Li et al., 2017; Qu et al., 2019). Recently, Du et al. (2020) integrated BERT into cross-domain sentiment analysis tasks to learn the domain-shared feature representation. However, most of the existing work focuses on learning the domain-shared representation"
2020.coling-main.49,P19-1139,0,0.330715,"we examine the model on four popular sentient classification datasets: SST-2-Root (Socher et al., 2013), SST-5-Root (Socher et al., 2013), IMDB (Maas et al., 2011) and Yelp (Zhang et al., 2015), and three aspect-based sentiment classification datasets: Restaurant14 (Pontiki et al., 2016) and Laptop14 (Pontiki et al., 2016), and Twitter dataset (Dong et al., 2014). 4.2 Baselines We compare our model with the following strong baselines for cross-domain sentiment analysis, including DANN (Ganin et al., 2016), PBLM (Ziser and Reichart, 2018), HATN (Li et al., 2018), ACAN (Qu et al., 2019), IATN (Zhang et al., 2019a) and BERT-DAAT (Du et al., 2020). BERT-DAAT is regarded as the state-of-the-art model, which uses BERT for cross-domain sentiment analysis with adversarial training. We adopt the results of these baselines reported in (Du et al., 2020). For in-domain sentiment analysis, we compare our model with SentiLR-B (Ke et al., 2019), which is one of the state-of-the-art models based on BERT. BERT is extensively compared in our experiments. To exclude the impact of the pre-training dataset, we also compare S ENTI X with BERT˚ , which pre-trains on the same dataset with standard MLM task. Moreover, to v"
2020.coling-main.49,N18-1112,0,0.264706,"has become a promising direction, which transfers (invariant) sentiment knowledge from the source domain to the target domain1 . The major challenge here is that language expressions for sentimental text usually vary across different domains. For instance, “fast” has a positive sentiment towards “service” in the restaurant domain (Figure 1), while in the laptop domain, “fast” expresses a negative sentiment for “power consumption”. Furthermore, models trained on the source domain tend to overfit, since they learn domain-specific knowledge excessively. Therefore, many studies (Du et al., 2020; Ziser and Reichart, 2018; Li et al., 2018) propose to address this issue by extracting domain-invariant features. Recently, pre-trained language models like BERT (Devlin et al., 2019) have achieved the state-of-theart performance on multiple sentiment analysis tasks (Hoang et al., 2019; Munikar et al., 2019; Raffel et al., 2019). However, when they are directly applied to cross-domain sentiment analysis (Du et al., 2020), two problems arise: 1) Existing pre-trained models focus on learning the semantic content via self-supervision strategies, while ignoring sentiment-specific knowledge at the pre-training phrase; 2)"
2020.emnlp-main.132,D17-1004,0,0.0282645,"encoder to extract five feature vectors regarding the five spans. Let hL , hs1 , hM , hs2 , hR be the corresponding representations computed by span encoder. To allow the model to focus on more informative spans, we then represent the span pair p as a weighted sum of its contextualized span representations with a position-aware attention mechanism as X hp = aj hj , j∈{L,s1 ,M,s2 ,R} where the attention score aj is computed as aj = Softmax(ej ), ej = vT tanh(Wh hj + Wcls hcls + Ws1 psj 1 + Ws2 psj 2 ), where W∗ and v are parameters and hcls is the output of the first token ([CLS]). Following (Zhang et al., 2017), psj 1 and psj 2 are the relative position embedding with repsect to s1 and s2 . 3.2 Pre-training Objectives 4 We extract entities and relations for each sentence, so we omit the next sentence prediction in BERT, which is a sentence level objective. Learning powerful representations of span and span pair is crucial for the entity relation extraction task, 1694 To cls h1 h2 h3 h4 k L en ev cls el h1 h2 h3 h5 h4 CNN h5 Span Level cls L S R Transformer cls x1 x2 x3 x4 x5 Sp an Pa ir L Query ev Attention el Shared Parameters cls CNN CNN CNN CNN CNN L S1 M S2 R Figure 2: Overview of our pre-traini"
2020.emnlp-main.132,P19-1139,0,0.129179,"have a negative impact on other datasets as prior works (Sun and Wu, 2019). Besides, the distantly supervised dataset’s annotated labels are usually inconsistent with that of the target dataset. As expected, in the preliminary experiment, we observe that the performance of the model directly pre-trained with annotated data provided by distantly supervised dataset (such as NYT) is not improved or even gets worse when it is fine-tuned on other entity relation dataset (such as ACE05). In addition, there are several existing works for incorporating entity information into pre-training objectives (Zhang et al., 2019; Sun et al., 2019b). However, these methods rely on entity annotations, which brings additional cost. In this work, we focus on the unsupervised pretraining objectives. We present a novel pre-training 1692 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 1692–1705, c November 16–20, 2020. 2020 Association for Computational Linguistics network architecture customized for entity relation extraction. In addition to the default sentence encoder in existing pre-trained models (e.g., the Transformer encoder of BERT), we also pre-train a span encoder and"
2020.emnlp-main.132,P17-1113,0,0.0685263,"input vectors or sentence encoder. The typical works include tree LSTM-based model over dependency tree (Miwa and Bansal, 2016) and attention-based model without dependency tree (Katiyar and Cardie, 2017). However, this kind of method does not perform joint decoding, and it can not fully exploit the interaction between output entities and relations. To mitigate the above question, many joint decoding algorithms (Fu et al., 2019; Ren et al., 2017; Li et al., 2019) are applied into this task, such as ILP-based joint decoding algorithms (Yang and Cardie, 2013), joint sequence labelling tag set (Zheng et al., 2017), structured perceptron (Li and Ji, 2014), joint MRT (Sun et al., 2018), joint relational triplets extracting (Chen et al., 2019; Zeng et al., 2018), and transition system (Wang et al., 2018). Besides, (Sun et al., 2019a) perform the joint type inference with GCN on an entity-relation bipartite graph. Especially, our joint model for entity relation extraction is derived from (Sun et al., 2019a) without GCN. In this work, we mainly investigate whether pre-training can help entity relation extraction task. For simplicity, our joint model does not perform joint decoding (only sharing parameters)."
2021.acl-long.19,N19-1423,0,0.0134957,"16) Katiyar and Cardie (2017) Li et al. (2019) Wang and Lu (2020) Zhong and Chen (2020) Zhong and Chen (2020) Relation R F1 Table 3: Overall evaluation.  means that the model leverages cross-sentence context information. relation type is correct, as well as the boundaries and types of two argument entities are correct. Implementation Details We tune all hyperparameters based on the averaged entity F1 and relation F1 on ACE05 development set, then keep the same settings on ACE04 and SciERC. For fair comparison with previous works, we use three pre-trained language models: bert-base-uncased (Devlin et al., 2019), albert-xxlarge-v1 (Lan et al., 2019) and scibert-scivocab-uncased (Beltagy et al., 2019) as the sentence encoder and fine-tune them in training stage.12 For the MLP layer, we set the hidden size as d = 150 and use GELU as the activation function. We use AdamW optimizer (Loshchilov and Hutter, 2017) with β1 = 0.9 and β2 = 0.9, and observe a phenomenon similar to (Dozat and Manning, 2016) in that setting β2 from 0.9 to 0.999 causes a significant drop on final performance. The batch size is 32, and the learning rate is 5e-5 with weight decay 1e-5. We apply a linear warm-up learning rate schedul"
2021.acl-long.19,doddington-etal-2004-automatic,0,0.124527,"spot that relation label in the same way). In other words, if the adjacent rows/columns are different, there must be an entity boundary (i.e., one belonging to the entity and the other not belonging to the entity). Therefore, if our biaffine model is reasonably trained, given a model predicted table, we could use this property to find split positions of entity boundary. As expected, experiments (Figure 4) verify our assumption. We adapt this idea to the 3-dimensional probability tensor P. 224 4 Experiments Datasets We conduct experiments on three entity relation extraction benchmarks: ACE04 (Doddington et al., 2004),9 ACE05 (Walker et al., 2006),10 and SciERC (Luan et al., 2018).11 Table 2 shows the dataset statistics. Besides, we provide detailed dataset specifications in the Appendix B. Evaluation Following suggestions in (Taill´e et al., 2020), we evaluate Precision (P), Recall (R), and F1 scores with micro-averaging and adopt the Strict Evaluation criterion. Specifically, a predicted entity is correct if its type and boundaries are correct, and a predicted relation is correct if its 8 i and j denote start and end indices of the span. https://catalog.ldc.upenn.edu/LDC2005T09 10 https://catalog.ldc.upe"
2021.acl-long.19,P81-1022,0,0.584483,"Missing"
2021.acl-long.19,C16-1239,0,0.0333364,"Missing"
2021.acl-long.19,P17-1085,0,0.0146782,"can be roughly divided into two categories according to the adopted label space. Separate Label Spaces This category study this task as two separate sub-tasks: entity recognition and relation classification, which are defined in two separate label spaces. One early paradigm is the pipeline method (Zelenko et al., 2003; Miwa et al., 2009) that uses two independent models for two sub-tasks respectively. Then joint method handles this task with an end-to-end model to explore more interaction between entities and relations. The most basic joint paradigm, parameter sharing (Miwa and Bansal, 2016; Katiyar and Cardie, 2017), adopts two independent decoders based on a shared encoder. Recent span-based models (Luan et al., 2019b; Wadden et al., 2019) also use this paradigm. To enhance the connection of two decoders, many joint decoding algorithms are proposed, such as ILP-based joint decoder (Yang and Cardie, 2013), joint MRT (Sun et al., 2018), GCN-based joint inference (Sun et al., 2019). Actually, table filling method (Miwa and Sasaki, 2014; Gupta et al., 2016; Zhang et al., 2017; Wang et al., 2020) is a special case of parameter sharing in table structure. These joint models all focus on various joint algorith"
2021.acl-long.19,P14-1038,0,0.0845167,"Missing"
2021.acl-long.19,P19-1129,0,0.0134684,"NI RE hard decoding Table 5: Comparison of accuracy and efficiency on ACE05 and SciERC test sets with different context window sizes. † denotes the approximation version with a faster speed and a worse performance. Table 4: Results (F1 score) with different settings on ACE05 and SciERC test sets. Note that we use BERTBASE on ACE05. achieves better performance. Besides, our model can achieve better relation performance even with worse entity results on ACE04. Actually, our base model (BERTBASE ) has achieved competitive relation performance, which even exceeds prior models based on BERTLARGE (Li et al., 2019) and ALBERTXXLARGE (Wang and Lu, 2020). These results confirm the proposed unified label space is effective for exploring the interaction between entities and relations. Note that all subsequent experiment results on ACE04 and ACE05 are based on BERTBASE for efficiency. 4.2 ACE05 Rel Speed (F1) (sent/s) Parameters Ablation Study In this section, we analyze the effects of components in U NI RE with different settings (Table 4). Particularly, we implement a naive decoding algorithm for comparison, namely “hard decoding”, which takes the “intermediate table” as input. The “intermediate table” is"
2021.acl-long.19,D18-1360,0,0.018109,"acent rows/columns are different, there must be an entity boundary (i.e., one belonging to the entity and the other not belonging to the entity). Therefore, if our biaffine model is reasonably trained, given a model predicted table, we could use this property to find split positions of entity boundary. As expected, experiments (Figure 4) verify our assumption. We adapt this idea to the 3-dimensional probability tensor P. 224 4 Experiments Datasets We conduct experiments on three entity relation extraction benchmarks: ACE04 (Doddington et al., 2004),9 ACE05 (Walker et al., 2006),10 and SciERC (Luan et al., 2018).11 Table 2 shows the dataset statistics. Besides, we provide detailed dataset specifications in the Appendix B. Evaluation Following suggestions in (Taill´e et al., 2020), we evaluate Precision (P), Recall (R), and F1 scores with micro-averaging and adopt the Strict Evaluation criterion. Specifically, a predicted entity is correct if its type and boundaries are correct, and a predicted relation is correct if its 8 i and j denote start and end indices of the span. https://catalog.ldc.upenn.edu/LDC2005T09 10 https://catalog.ldc.upenn.edu/LDC2006T06 11 http://nlp.cs.washington.edu/sciIE/ 9 Datas"
2021.acl-long.19,N19-1308,0,0.27982,"4 and +1.7 for relation, on ACE04 and ACE05 respectively. For the best pipeline model (Zhong and Chen, 2020) (current SOTA), our model achieves superior performance on ACE04 and SciERC and comparable performance on ACE05. Comparing with ACE04/ACE05, SciERC is much smaller, so entity performance on SciERC drops sharply. Since (Zhong and Chen, 2020) is a pipeline method, its relation performance is severely influenced by the poor entity performance. Nevertheless, our model is less influenced in this case and 12 The first two are for ACE04 and ACE05, and the last one is for SciERC. 225 13 Since (Luan et al., 2019a; Wadden et al., 2019) neglect the argument entity type in relation evaluation and underperform our baseline (Zhang et al., 2020), we do not compare their results here. Settings ACE05 Ent Rel SciERC Ent Rel Model Default 88.8 64.3 68.4 36.9 w/o symmetry loss w/o implication loss w/o logit dropout w/o cross-sentence context 88.9 89.0 88.8 87.9 64.0 63.3 61.8 62.7 67.3 68.0 66.9 65.3 35.5 37.1 34.7 32.1 hard decoding 74.0 34.6 46.1 17.8 SciERC Rel Speed (F1) (sent/s) 100 100 64.6 - 14.7 237.6 36.7 - 19.9 194.7 110M 110M 100 200 63.6 64.3 340.6 194.2 34.0 36.9 314.8 200.1 110M 200 34.6 139.1 17."
2021.acl-long.19,P16-1105,0,0.140446,"asting research topic in NLP. Typically, it aims to recognize specific entities and relations for profiling the semantic of sentences. An example is shown in Figure 1, where a person entity “David Perkins” and a geography entity “California” have a physical location relation PHYS. Methods for detecting entities and relations can be categorized into pipeline models or joint models. In the pipeline setting, entity models and relation models are independent with disentangled feature spaces and output label spaces. In the joint setting, on the other hand, some parameter sharing of feature spaces (Miwa and Bansal, 2016; Katiyar and and David in Introduction ORG-AFF Figure 1: Example of a table for joint entity relation extraction. Each cell corresponds to a word pair. Entities are squares on diagonal, relations are rectangles off diagonal. Note that PER-SOC is a undirected (symmetrical) relation type, while PHYS and ORG-AFF are directed (asymmetrical) relation types. The table exactly expresses overlapped relations, e.g., the person entity “David Perkins” participates in two relations, (“David Perkins”, “wife”, PER-SOC) and (“David Perkins”, “California”, PHYS). For every cell, a same biaffine model predict"
2021.acl-long.19,D09-1013,0,0.0285477,"ility for each cell). “Decoded Table” presents the final results after decoding. Figure 6: Distribution of five relation extraction errors on ACE05 and SciERC test data. 5 it Related Work Entity relation extraction has been extensively studied over the decades. Existing methods can be roughly divided into two categories according to the adopted label space. Separate Label Spaces This category study this task as two separate sub-tasks: entity recognition and relation classification, which are defined in two separate label spaces. One early paradigm is the pipeline method (Zelenko et al., 2003; Miwa et al., 2009) that uses two independent models for two sub-tasks respectively. Then joint method handles this task with an end-to-end model to explore more interaction between entities and relations. The most basic joint paradigm, parameter sharing (Miwa and Bansal, 2016; Katiyar and Cardie, 2017), adopts two independent decoders based on a shared encoder. Recent span-based models (Luan et al., 2019b; Wadden et al., 2019) also use this paradigm. To enhance the connection of two decoders, many joint decoding algorithms are proposed, such as ILP-based joint decoder (Yang and Cardie, 2013), joint MRT (Sun et"
2021.acl-long.19,D14-1200,0,0.0952005,"input space is a two-dimensional table with each entry corresponding to a word pair in sentences (Figure 1). The joint model assign labels to each cell from a unified label space (union of entity type set and relation type set). Graphically, entities are squares on the diagonal, and relations are rectangles off the diagonal. This formulation retains full model expressiveness regarding existing entity-relation extraction scenarios (e.g., overlapped relations, directed relations, undirected relations). It is also different from the current table filling settings for entity relation extraction (Miwa and Sasaki, 2014; Gupta et al., 2016; Zhang et al., 2017; Wang and Lu, 2020), which still have separate label space for entities and relations, and treat on/off-diagonal entries differently. Based on the tabular formulation, our joint entity relation extractor performs two actions, filling and decoding. First, filling the table is to predict each word pair’s label, which is similar to arc prediction task in dependency parsing. We adopt the biaffine attention mechanism (Dozat and Manning, 2016) to learn interactions between word pairs. We also impose two structural constraints on the table through structural r"
2021.acl-long.19,P19-1131,1,0.956818,"l, relations are rectangles off diagonal. Note that PER-SOC is a undirected (symmetrical) relation type, while PHYS and ORG-AFF are directed (asymmetrical) relation types. The table exactly expresses overlapped relations, e.g., the person entity “David Perkins” participates in two relations, (“David Perkins”, “wife”, PER-SOC) and (“David Perkins”, “California”, PHYS). For every cell, a same biaffine model predicts its label. The joint decoder is set to find the best squares and rectangles. Equal contribution. Corresponding Author. Cardie, 2017) or decoding interactions (Yang and Cardie, 2013; Sun et al., 2019) are imposed to explore the common structure of the two tasks. It was believed that joint models could be better since they can alleviate error propagations among sub-models, have more compact parameter sets, and uniformly encode prior knowledge (e.g., constraints) on both tasks. However, Zhong and Chen (2020) recently show 220 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 220–231 August 1–6, 2021. ©2021 Association for Computational Linguistics that with the help of mode"
2021.acl-long.19,D18-1249,1,0.848912,", 2009) that uses two independent models for two sub-tasks respectively. Then joint method handles this task with an end-to-end model to explore more interaction between entities and relations. The most basic joint paradigm, parameter sharing (Miwa and Bansal, 2016; Katiyar and Cardie, 2017), adopts two independent decoders based on a shared encoder. Recent span-based models (Luan et al., 2019b; Wadden et al., 2019) also use this paradigm. To enhance the connection of two decoders, many joint decoding algorithms are proposed, such as ILP-based joint decoder (Yang and Cardie, 2013), joint MRT (Sun et al., 2018), GCN-based joint inference (Sun et al., 2019). Actually, table filling method (Miwa and Sasaki, 2014; Gupta et al., 2016; Zhang et al., 2017; Wang et al., 2020) is a special case of parameter sharing in table structure. These joint models all focus on various joint algorithms but ignore the fact that they are essentially based on separate label spaces. Unified Label Space This family of methods aims to unify two sub-tasks and tackle this task in a unified label space. Entity relation extraction has been converted into a tagging problem (Zheng et al., 2017), a transition-based parsing problem"
2021.acl-long.19,2020.emnlp-main.301,0,0.0348268,"Missing"
2021.acl-long.19,D17-1182,0,0.0334741,"Missing"
2021.acl-long.19,D19-1585,0,0.0713147,"ion, on ACE04 and ACE05 respectively. For the best pipeline model (Zhong and Chen, 2020) (current SOTA), our model achieves superior performance on ACE04 and SciERC and comparable performance on ACE05. Comparing with ACE04/ACE05, SciERC is much smaller, so entity performance on SciERC drops sharply. Since (Zhong and Chen, 2020) is a pipeline method, its relation performance is severely influenced by the poor entity performance. Nevertheless, our model is less influenced in this case and 12 The first two are for ACE04 and ACE05, and the last one is for SciERC. 225 13 Since (Luan et al., 2019a; Wadden et al., 2019) neglect the argument entity type in relation evaluation and underperform our baseline (Zhang et al., 2020), we do not compare their results here. Settings ACE05 Ent Rel SciERC Ent Rel Model Default 88.8 64.3 68.4 36.9 w/o symmetry loss w/o implication loss w/o logit dropout w/o cross-sentence context 88.9 89.0 88.8 87.9 64.0 63.3 61.8 62.7 67.3 68.0 66.9 65.3 35.5 37.1 34.7 32.1 hard decoding 74.0 34.6 46.1 17.8 SciERC Rel Speed (F1) (sent/s) 100 100 64.6 - 14.7 237.6 36.7 - 19.9 194.7 110M 110M 100 200 63.6 64.3 340.6 194.2 34.0 36.9 314.8 200.1 110M 200 34.6 139.1 17.8 113.0 W Z&C(2020) Z&C"
2021.acl-long.19,2020.emnlp-main.133,0,0.354126,"onding to a word pair in sentences (Figure 1). The joint model assign labels to each cell from a unified label space (union of entity type set and relation type set). Graphically, entities are squares on the diagonal, and relations are rectangles off the diagonal. This formulation retains full model expressiveness regarding existing entity-relation extraction scenarios (e.g., overlapped relations, directed relations, undirected relations). It is also different from the current table filling settings for entity relation extraction (Miwa and Sasaki, 2014; Gupta et al., 2016; Zhang et al., 2017; Wang and Lu, 2020), which still have separate label space for entities and relations, and treat on/off-diagonal entries differently. Based on the tabular formulation, our joint entity relation extractor performs two actions, filling and decoding. First, filling the table is to predict each word pair’s label, which is similar to arc prediction task in dependency parsing. We adopt the biaffine attention mechanism (Dozat and Manning, 2016) to learn interactions between word pairs. We also impose two structural constraints on the table through structural regularizations. Next, given the table filling with label log"
2021.acl-long.19,2020.emnlp-main.132,1,0.759126,"n between entities and relations. The most basic joint paradigm, parameter sharing (Miwa and Bansal, 2016; Katiyar and Cardie, 2017), adopts two independent decoders based on a shared encoder. Recent span-based models (Luan et al., 2019b; Wadden et al., 2019) also use this paradigm. To enhance the connection of two decoders, many joint decoding algorithms are proposed, such as ILP-based joint decoder (Yang and Cardie, 2013), joint MRT (Sun et al., 2018), GCN-based joint inference (Sun et al., 2019). Actually, table filling method (Miwa and Sasaki, 2014; Gupta et al., 2016; Zhang et al., 2017; Wang et al., 2020) is a special case of parameter sharing in table structure. These joint models all focus on various joint algorithms but ignore the fact that they are essentially based on separate label spaces. Unified Label Space This family of methods aims to unify two sub-tasks and tackle this task in a unified label space. Entity relation extraction has been converted into a tagging problem (Zheng et al., 2017), a transition-based parsing problem (Wang et al., 2018), and a generation problem with Seq2Seq framework (Zeng et al., 2018; Nayak and Ng, 2020). We follow this trend and propose a new unified labe"
2021.acl-long.19,P13-1161,0,0.167846,"are squares on diagonal, relations are rectangles off diagonal. Note that PER-SOC is a undirected (symmetrical) relation type, while PHYS and ORG-AFF are directed (asymmetrical) relation types. The table exactly expresses overlapped relations, e.g., the person entity “David Perkins” participates in two relations, (“David Perkins”, “wife”, PER-SOC) and (“David Perkins”, “California”, PHYS). For every cell, a same biaffine model predicts its label. The joint decoder is set to find the best squares and rectangles. Equal contribution. Corresponding Author. Cardie, 2017) or decoding interactions (Yang and Cardie, 2013; Sun et al., 2019) are imposed to explore the common structure of the two tasks. It was believed that joint models could be better since they can alleviate error propagations among sub-models, have more compact parameter sets, and uniformly encode prior knowledge (e.g., constraints) on both tasks. However, Zhong and Chen (2020) recently show 220 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 220–231 August 1–6, 2021. ©2021 Association for Computational Linguistics that wi"
2021.acl-long.19,P18-1047,0,0.0120585,"g method (Miwa and Sasaki, 2014; Gupta et al., 2016; Zhang et al., 2017; Wang et al., 2020) is a special case of parameter sharing in table structure. These joint models all focus on various joint algorithms but ignore the fact that they are essentially based on separate label spaces. Unified Label Space This family of methods aims to unify two sub-tasks and tackle this task in a unified label space. Entity relation extraction has been converted into a tagging problem (Zheng et al., 2017), a transition-based parsing problem (Wang et al., 2018), and a generation problem with Seq2Seq framework (Zeng et al., 2018; Nayak and Ng, 2020). We follow this trend and propose a new unified label space. We introduce a 2D table to tackle the overlapping relation problem in (Zheng et al., 2017). Also, our model is more versatile as not relying on complex expertise like (Wang et al., 2018), which requires external expert knowledge to design a complex transition system. 6 Conclusion In this work, we extract entities and relations in a unified label space to better mine the interaction between both sub-tasks. We propose a novel table that presents entities and relations as squares and rectangles. Then this task can"
2021.acl-long.19,P17-1113,0,0.110478,"ty models and relation models share encoders, usually their label spaces are still separate (even in models with joint decoders). Therefore, parallel to (Zhong and Chen, 2020), we would ask whether joint encoders (decoders) deserve joint label spaces? The challenge of developing a unified entityrelation label space is that the two sub-tasks are usually formulated into different learning problems (e.g., entity detection as sequence labeling, relation classification as multi-class classification), and their labels are placed on different things (e.g., words v.s. words pairs). One prior attempt (Zheng et al., 2017) is to handle both sub-tasks with one sequence labeling model. A compound label set was devised to encode both entities and relations. However, the model’s expressiveness is sacrificed: it can detect neither overlapping relations (i.e., entities participating in multiple relation) nor isolated entities (i.e., entities not appearing in any relation). Our key idea of defining a new unified label space is that, if we think Zheng et al. (2017)’s solution is to perform relation classification during entity labeling, we could also consider the reverse direction by seeing entity detection as a specia"
2021.eacl-main.251,P19-1129,0,0.0317583,"Missing"
2021.eacl-main.251,N19-1308,0,0.194291,"epresentations for Joint Entity Relation Extraction Yijun Wang1, 2 , Changzhi Sun4 , Yuanbin Wu3 , Hao Zhou4 , Lei Li4 , and Junchi Yan1, 2 1 Department of Computer Science and Engineering, Shanghai Jiao Tong University MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University 3 School of Computer Science and Technology, East China Normal University 4 ByteDance, AI Lab yijunwang.cs@gmail.com ybwu@cs.ecnu.edu.cn yanjunchi@sjtu.edu.cn {sunchangzhi, zhouhao.nlp, lileilab}@bytedance.com 2 Abstract MET Current state-of-the-art systems for joint entity relation extraction (Luan et al., 2019; Wadden et al., 2019) usually adopt the multi-task learning framework. However, annotations for these additional tasks such as coreference resolution and event extraction are always equally hard (or even harder) to obtain. In this work, we propose a pre-training method E N PA R to improve the joint extraction performance. E N PA R requires only the additional entity annotations that are much easier to collect. Unlike most existing works that only consider incorporating entity information into the sentence encoder, we further utilize the entity pair information. Specifically, we devise four no"
2021.eacl-main.251,P16-1105,0,0.0798562,"(entity) types and 7 relation types. We use the same data split and preprocessing of SciERC dataset (350 training, 50 validating and 100 testing) as (Luan et al., 2019). NYT The NYT dataset8 is a large-scale corpus which automatically annotates a collection of New York Times news articles. NYT contains 3 types of entities and 12 types of relations. The training set is automatically annotated by distant supervision. While the validation and testing data are manually labeled by (Jia et al., 2019). We choose the latest version of NYT released by (Jia et al., 2019). Evaluation. As previous works (Miwa and Bansal, 2016; Sun et al., 2019a), we evaluate the 6 https://github.com/tticoin/LSTM-ER http://nlp.cs.washington.edu/sciIE/ 8 https://github.com/PaddlePaddle/models/tree/develop/ PaddleNLP/Research/ACL2019-ARNOR/ 2881 7 Model Sun, 2019a Li, 2019 Luan, 2019?, ◦ Wadden, 2019 , ◦ E N PA R  Entity Relation Relation (exactly) 84.2 84.8 88.4 88.6 86.9 – – 63.2 63.4 66.1 59.1 60.2 – – 63.5 Table 1: Results on the ACE05 test data.  means that the model uses BERT. ? means that the model uses ELMo as token embeddings. ◦ stands for training the model with multi-task learning. E N PA R is the proposed model fine-t"
2021.eacl-main.251,P19-1131,1,0.71528,"roduced in the fine-tuning stage, which may futher impair the joint extraction performance. To address the first limitation, recent several works try to incorporate entity-related information 1 https://spacy.io/ 2877 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 2877–2887 April 19 - 23, 2021. ©2021 Association for Computational Linguistics into pre-training objectives. Zhang et al. (2019) fuses heterogeneous information from both texts and knowledge graphs and proposes a denoising entity auto-encoder objective based on BERT. Sun et al. (2019c) presents two knowledge masking strategies in the pre-training stage (entity-level masking and phrase-level masking). Both of them utilize extra entity annotations (i.e., entities in knowledge graphs and automatic entity annotations, respectively). In this paper, we follow this line of works and build a large-scale entity annotated corpus using the spaCy NER tool. For the second limitation, we propose E N PA R, a pre-training method customized for entity relation extraction. E N PA R consists of an underlying sentence encoder, an entity encoder, and an entity pair encoder. Compared with BERT"
2021.eacl-main.251,D18-1249,1,0.853386,"1080 Ti GPU. 3 Experiments We conduct experiments on three benchmark entity relation extraction datasets: ACE05, SciERC, and NYT. For space limitation, we will mainly discuss the results on ACE05 and report basic results on the remaining two datasets. ACE05 The ACE05 dataset 6 that is a standard corpus for entity relation extraction task annotates entity and relation labels for a collection of documents. ACE05 contains 7 entity types and 6 relation types. We use the same data split and preprocessing of ACE05 dataset (351 training, 80 validating and 80 testing) as (Miwa and Bansal, 2016) and (Sun et al., 2018). SciERC The SciERC dataset 7 annotates entity, coreference and relation labels for 500 scientific abstracts from 12 AI conference/workshop proceedings. We only use the annotations of entities and relations. SciERC contains 6 scientific term (entity) types and 7 relation types. We use the same data split and preprocessing of SciERC dataset (350 training, 50 validating and 100 testing) as (Luan et al., 2019). NYT The NYT dataset8 is a large-scale corpus which automatically annotates a collection of New York Times news articles. NYT contains 3 types of entities and 12 types of relations. The tra"
2021.eacl-main.251,D19-1585,0,0.273858,"Joint Entity Relation Extraction Yijun Wang1, 2 , Changzhi Sun4 , Yuanbin Wu3 , Hao Zhou4 , Lei Li4 , and Junchi Yan1, 2 1 Department of Computer Science and Engineering, Shanghai Jiao Tong University MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University 3 School of Computer Science and Technology, East China Normal University 4 ByteDance, AI Lab yijunwang.cs@gmail.com ybwu@cs.ecnu.edu.cn yanjunchi@sjtu.edu.cn {sunchangzhi, zhouhao.nlp, lileilab}@bytedance.com 2 Abstract MET Current state-of-the-art systems for joint entity relation extraction (Luan et al., 2019; Wadden et al., 2019) usually adopt the multi-task learning framework. However, annotations for these additional tasks such as coreference resolution and event extraction are always equally hard (or even harder) to obtain. In this work, we propose a pre-training method E N PA R to improve the joint extraction performance. E N PA R requires only the additional entity annotations that are much easier to collect. Unlike most existing works that only consider incorporating entity information into the sentence encoder, we further utilize the entity pair information. Specifically, we devise four novel objectives, i.e.,"
2021.eacl-main.251,P13-1161,0,0.0234792,"tperforms BERT on entity performance and relation performance. This again verifies the effectiveness of our proposed pre-training method. 4 Related Work Joint entity relation extraction is an important task that has been extensively studied. One simple method to achieve joint learning is through parameters sharing, which usually share some input embeddings or sentence encoders (Miwa and Bansal, 2016; Katiyar and Cardie, 2017). To further explore the interactions between the outputs of the entity model and the relation model, many joint decoding algorithms were introduced into this joint task (Yang and Cardie, 2013; Li and Ji, 2014; Katiyar and Cardie, 2016; Zheng et al., 2017; Ren et al., 2017; Wang et al., 2018; Sun et al., 2018; Fu et al., 2019). Besides, (Li et al., 2019) tackle this task under the framework of multi-turn QA. And (Sun et al., 2019a) conduct joint type inference via GCN on a bipartite graph composed of entities and relations. Recently, transfer learning (Sun and Wu, 2019), multi-task learning (Sanh et al., 2019; Wadden et al., 2019; Luan et al., 2019) were also applied in this task. In this work, we investigate the pre-trained model for entity relation extraction. For simplicity, we"
2021.eacl-main.251,P19-1139,0,0.226172,"ntences, but not entities and entity pairs. To obtain the representations for entities and entity pairs, additional parameters that are not pre-trained are introduced in the fine-tuning stage, which may futher impair the joint extraction performance. To address the first limitation, recent several works try to incorporate entity-related information 1 https://spacy.io/ 2877 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 2877–2887 April 19 - 23, 2021. ©2021 Association for Computational Linguistics into pre-training objectives. Zhang et al. (2019) fuses heterogeneous information from both texts and knowledge graphs and proposes a denoising entity auto-encoder objective based on BERT. Sun et al. (2019c) presents two knowledge masking strategies in the pre-training stage (entity-level masking and phrase-level masking). Both of them utilize extra entity annotations (i.e., entities in knowledge graphs and automatic entity annotations, respectively). In this paper, we follow this line of works and build a large-scale entity annotated corpus using the spaCy NER tool. For the second limitation, we propose E N PA R, a pre-training method custo"
2021.eacl-main.251,P17-1113,0,0.0199405,"s again verifies the effectiveness of our proposed pre-training method. 4 Related Work Joint entity relation extraction is an important task that has been extensively studied. One simple method to achieve joint learning is through parameters sharing, which usually share some input embeddings or sentence encoders (Miwa and Bansal, 2016; Katiyar and Cardie, 2017). To further explore the interactions between the outputs of the entity model and the relation model, many joint decoding algorithms were introduced into this joint task (Yang and Cardie, 2013; Li and Ji, 2014; Katiyar and Cardie, 2016; Zheng et al., 2017; Ren et al., 2017; Wang et al., 2018; Sun et al., 2018; Fu et al., 2019). Besides, (Li et al., 2019) tackle this task under the framework of multi-turn QA. And (Sun et al., 2019a) conduct joint type inference via GCN on a bipartite graph composed of entities and relations. Recently, transfer learning (Sun and Wu, 2019), multi-task learning (Sanh et al., 2019; Wadden et al., 2019; Luan et al., 2019) were also applied in this task. In this work, we investigate the pre-trained model for entity relation extraction. For simplicity, we restrict the joint model of parameters sharing, which can be ea"
2021.eacl-main.49,P16-1068,0,0.0155197,"hus provide new powerful tools for knowledge mining and extraction (Davison et al., 2019; Petroni et al., 2019). Since the commonsense opinions are closely related to human commonsense and background knowledge, we adopt pre-trained language models to mine the commonsense sentiment from texts automatically. Gradient-based methods (Goodfellow et al., 2015) have been widely applied into computer version and NLP (Zeiler and Fergus, 2014; Liang et al., 2018). The gradient-based approach is also used to understand the decisions of the text classiﬁcation models from the token level (Li et al., 2016; Alikaniotis et al., 2016). In addition, Rei et al. (2018) 615 adopted gradient-based approach to detect the important tokens in the sentence via the sentence-level label. In this paper, we design a continuous perturbation algorithm to discover the target-aware opinion words using the gradient. 8 Conclusion In this paper, we propose a framework for automatic target-aware sentiment mining from texts without manual annotations or linguistic rules. We evaluate the proposed framework on two largescale online review domains: restaurant and electronic with both manual checking and automatic downstream tasks. We also achieve"
2021.eacl-main.49,D09-1062,0,0.0415647,"liest+ sucks∗ fast+ Table 5: We list top-10 opinion words of several targets for two domains: electronic and restaurant. The marker + and ∗ represent positive and negative sentiment respectively. and 0.96) for L and Lc over restaurant (electronic). All these indicate that commonsense lexicon Lc is more diverse than general lexicon Lg over different targets. In addition, the commonly used general opinion words and commonsense sentiment words are different for different targets. 7 Related Work Domain adaptation has been studied for a long time in the ﬁeld of sentiment analysis (Wu et al., 2017; Choi and Cardie, 2009; Cambria et al., 2018; Zhou et al., 2020c). We mainly summarize the related work about lexicon domain adaptation that aims to build a domain-speciﬁc sentiment lexicon (Ofek et al., 2016; Vo and Zhang, 2016; Hamilton et al., 2016). In (Hamilton et al., 2016), authors inferred the orientation of words from general opinion words by building a graph for each domain. Xing et al. (2019) judged the word polarity via a document-level sentiment classiﬁer. However, it is time-consuming for they have to retrain the model for each word after changing the polarity randomly. Moreover, these existing method"
2021.eacl-main.49,J90-1003,0,0.502719,"Missing"
2021.eacl-main.49,D19-1109,0,0.0270086,"aurus are also made used. Since it is not easy to apply on different domains, we develop a framework to automatically mine aspect-aware commonsense sentiment from texts without extensive annotations and elaborated linguistic rules. Pre-trained models (e.g., ELMo (Peters et al., 2018), GPT (Radford et al., 2019), BERT (Devlin et al., 2019)) have achieved great success in NLP recently. By exploring a large number of open domain texts, pre-trained models are able to encode rich semantic information hidden in human languages and thus provide new powerful tools for knowledge mining and extraction (Davison et al., 2019; Petroni et al., 2019). Since the commonsense opinions are closely related to human commonsense and background knowledge, we adopt pre-trained language models to mine the commonsense sentiment from texts automatically. Gradient-based methods (Goodfellow et al., 2015) have been widely applied into computer version and NLP (Zeiler and Fergus, 2014; Liang et al., 2018). The gradient-based approach is also used to understand the decisions of the text classiﬁcation models from the token level (Li et al., 2016; Alikaniotis et al., 2016). In addition, Rei et al. (2018) 615 adopted gradient-based app"
2021.eacl-main.49,N19-1423,0,0.0907804,"t one sentence (target) in it is positive. By seeing a large amount of positive documents, a classiﬁer may be able to generalize patterns of their positive sentences, thus may help ﬁnding sentence-level (target-level) opinions. Here we simply build a document-level sentiment classiﬁer, and apply it on sentences to get pseudo target-level sentiment labels (for simplicity, we assume one sentence contains one target). Advanced distant supervision models could also be applied, but we ﬁnd this simple method preforms quite well in our experiments. To build the sentiment classiﬁer, we ﬁne-tune BERT (Devlin et al., 2019) on D to encode domain speciﬁc semantics and augment it with a sentiment prediction task to encode sentiment information. For a document d, we feed its word sequence into BERT and obtain a vector representation d = BERT(d), then we apply a softmax operator on d to get the probability of its sentiment P (y|d), P (y|d) = softmax(Wc d + bc ), |D| 1  log P (yi |di ). |D| (2) i=1 For each sentence s containing t, we apply above classiﬁer to predict pseudo sentiment label yp of s. In the following sections, we will rely on the set St = {(s, yp )|t ∈ S} to extract target-aware opinion words of t. 4"
2021.eacl-main.49,N19-1259,0,0.0396286,"Missing"
2021.eacl-main.49,D16-1057,0,0.0864231,"negative sentiment when commenting a computer hardware and a positive sentiment when commenting a pizza, even itself alone is identiﬁed without any general orientation. In these situations, it is the composition of a word, contexts, and commonsense carries an opinion. Automatically detecting such context dependent sentiments would strengthen both our understanding of implicit opinions in languages and improve existing sentiment analyses models, which is the main topic of this work. To handle shifts of word sentiment, prior works studied how to adapt existing sentiment lexicons to new domains (Hamilton et al., 2016; Xing et al., 2019). By modeling differences and similarities of text topics, they can detect new sentiments of words as the domain changes. The basic assumption of those domain-level sentiment lexicons is that a word keeps a consistent sentiment within a domain. This assumption, however, might be strong for ﬁne-granularity analyses of text sentiments: words (especially, neural words such as “long”, “fast”) could exhibit different orientations even in the same domain (Figure 1). To collect more detailed information of a sentiment, another branch of works (aspect-based sentiment analysis (Pont"
2021.eacl-main.49,N16-1082,0,0.0304062,"n languages and thus provide new powerful tools for knowledge mining and extraction (Davison et al., 2019; Petroni et al., 2019). Since the commonsense opinions are closely related to human commonsense and background knowledge, we adopt pre-trained language models to mine the commonsense sentiment from texts automatically. Gradient-based methods (Goodfellow et al., 2015) have been widely applied into computer version and NLP (Zeiler and Fergus, 2014; Liang et al., 2018). The gradient-based approach is also used to understand the decisions of the text classiﬁcation models from the token level (Li et al., 2016; Alikaniotis et al., 2016). In addition, Rei et al. (2018) 615 adopted gradient-based approach to detect the important tokens in the sentence via the sentence-level label. In this paper, we design a continuous perturbation algorithm to discover the target-aware opinion words using the gradient. 8 Conclusion In this paper, we propose a framework for automatic target-aware sentiment mining from texts without manual annotations or linguistic rules. We evaluate the proposed framework on two largescale online review domains: restaurant and electronic with both manual checking and automatic downstr"
2021.eacl-main.49,D15-1168,0,0.0449795,"Missing"
2021.eacl-main.49,P09-1113,0,0.0398288,"tain multiple targets (7 in average). Therefore, directly using 1 We use the 5-level label set Y = {1, 2, 3, 4, 5} (the larger a number, the more positive it represents). 2 Here we mainly focus on online reviews, but the methods could be applied to other sentiment-bearing texts. 609 document-level sentiment labels could be inappropriate for target-level analyses. On the other hand, it is quite expensive to annotate target-level sentiments, and existing datasets are far from enough for a robust commonsense opinion extractor. To deal with this problem, we borrow the idea of distant supervision (Mintz et al., 2009): if a document is labelled as positive, at least one sentence (target) in it is positive. By seeing a large amount of positive documents, a classiﬁer may be able to generalize patterns of their positive sentences, thus may help ﬁnding sentence-level (target-level) opinions. Here we simply build a document-level sentiment classiﬁer, and apply it on sentences to get pseudo target-level sentiment labels (for simplicity, we assume one sentence contains one target). Advanced distant supervision models could also be applied, but we ﬁnd this simple method preforms quite well in our experiments. To b"
2021.eacl-main.49,N18-1202,0,0.0213324,"ons and handcrafted external resources. To take the target into account, Wu et al. (2019) proposed to construct a target-speciﬁc sentiment lexicon. However, both NLP preprocessing pipelines (e.g., parsing, POS tagging) and linguistic rules are integrated into their algorithm. Available resources like general sentiment lexicon and thesaurus are also made used. Since it is not easy to apply on different domains, we develop a framework to automatically mine aspect-aware commonsense sentiment from texts without extensive annotations and elaborated linguistic rules. Pre-trained models (e.g., ELMo (Peters et al., 2018), GPT (Radford et al., 2019), BERT (Devlin et al., 2019)) have achieved great success in NLP recently. By exploring a large number of open domain texts, pre-trained models are able to encode rich semantic information hidden in human languages and thus provide new powerful tools for knowledge mining and extraction (Davison et al., 2019; Petroni et al., 2019). Since the commonsense opinions are closely related to human commonsense and background knowledge, we adopt pre-trained language models to mine the commonsense sentiment from texts automatically. Gradient-based methods (Goodfellow et al., 2"
2021.eacl-main.49,D19-1250,0,0.0334635,"Missing"
2021.eacl-main.49,S14-2004,0,0.0453459,"2016; Xing et al., 2019). By modeling differences and similarities of text topics, they can detect new sentiments of words as the domain changes. The basic assumption of those domain-level sentiment lexicons is that a word keeps a consistent sentiment within a domain. This assumption, however, might be strong for ﬁne-granularity analyses of text sentiments: words (especially, neural words such as “long”, “fast”) could exhibit different orientations even in the same domain (Figure 1). To collect more detailed information of a sentiment, another branch of works (aspect-based sentiment analysis (Pontiki et al., 2014; Zhou et al., 2020a,b), opinion relation extraction (Sun et al., 2017)) attempt ﬁnd answers of “who express what opinion on which target” for opinion bearing texts. Existing solutions heavily rely on manual annotations and linguistic rules, which are either hard to scale-up or hard to be complete. In this work, we study the task of extracting target-aware sentiment lexicons. An entry of such lexicon is a pair of a sentiment word and a target word, and their collocation expresses a sentiment. It improves existing domain-dependent lexicons by being more concrete and accurate on describing opini"
2021.eacl-main.49,W14-5905,0,0.0327492,"con from (Hu and Liu, 2004) to ﬁlter the general sentiment words and obtain the commonsense lexicon. This general lexicon contains around 6800 positive and negative opinion words or sentiment words for the English language. We adopt BERTbase as the basis for all experiments. Adam (Kingma and Ba, 2015) is adopted as the optimizer with learning rate 5e-5 for ﬁne-tuning and sentiment classiﬁcation. 3 http://jmcauley.ucsd.edu/data/amazon/ https://www.yelp.com/dataset/challenge 5 Here we use the targets from existing datasets, but the targets could be extracted automatically through existing work (Poria et al., 2014) or be inputted by users. 4 6.2 Human Evaluation To evaluate the quality of the target-aware sentiment lexicon, we test its performance through human evaluation. For quantitative evaluation, we sample 50 targets with top-20 opinion words in each domain to investigate the performance of L and Lc . Finally, we obtain 3122 and 2877 (t, o) pairs after ﬁltering repetitive pairs for electronic and restaurant, respectively. We ask ten annotators to label them to make sure each pair is marked with three times. Then, we obtain the label through voting. We calculate the Krippendorff’s alpha coefﬁcient ("
2021.eacl-main.49,N18-1027,0,0.031509,"Missing"
2021.eacl-main.49,E17-1097,1,0.850441,"topics, they can detect new sentiments of words as the domain changes. The basic assumption of those domain-level sentiment lexicons is that a word keeps a consistent sentiment within a domain. This assumption, however, might be strong for ﬁne-granularity analyses of text sentiments: words (especially, neural words such as “long”, “fast”) could exhibit different orientations even in the same domain (Figure 1). To collect more detailed information of a sentiment, another branch of works (aspect-based sentiment analysis (Pontiki et al., 2014; Zhou et al., 2020a,b), opinion relation extraction (Sun et al., 2017)) attempt ﬁnd answers of “who express what opinion on which target” for opinion bearing texts. Existing solutions heavily rely on manual annotations and linguistic rules, which are either hard to scale-up or hard to be complete. In this work, we study the task of extracting target-aware sentiment lexicons. An entry of such lexicon is a pair of a sentiment word and a target word, and their collocation expresses a sentiment. It improves existing domain-dependent lexicons by being more concrete and accurate on describing opinions. Departing from approaches adopted in existing aspect-based analyse"
2021.eacl-main.49,P16-2036,0,0.0297844,"L and Lc over restaurant (electronic). All these indicate that commonsense lexicon Lc is more diverse than general lexicon Lg over different targets. In addition, the commonly used general opinion words and commonsense sentiment words are different for different targets. 7 Related Work Domain adaptation has been studied for a long time in the ﬁeld of sentiment analysis (Wu et al., 2017; Choi and Cardie, 2009; Cambria et al., 2018; Zhou et al., 2020c). We mainly summarize the related work about lexicon domain adaptation that aims to build a domain-speciﬁc sentiment lexicon (Ofek et al., 2016; Vo and Zhang, 2016; Hamilton et al., 2016). In (Hamilton et al., 2016), authors inferred the orientation of words from general opinion words by building a graph for each domain. Xing et al. (2019) judged the word polarity via a document-level sentiment classiﬁer. However, it is time-consuming for they have to retrain the model for each word after changing the polarity randomly. Moreover, these existing methods mainly focus on the domain-level, while the sentiment polarities of some words depend on their opinion targets (Liu and Zhang, 2012). It is essential to predict the sentiment in target-level by integratin"
2021.eacl-main.49,P17-1156,0,0.0353875,"Missing"
2021.eacl-main.49,D12-1015,0,0.0175096,"erred the orientation of words from general opinion words by building a graph for each domain. Xing et al. (2019) judged the word polarity via a document-level sentiment classiﬁer. However, it is time-consuming for they have to retrain the model for each word after changing the polarity randomly. Moreover, these existing methods mainly focus on the domain-level, while the sentiment polarities of some words depend on their opinion targets (Liu and Zhang, 2012). It is essential to predict the sentiment in target-level by integrating both target and opinion words. The most related work to us is (Zhao et al., 2012). Zhao et al. (2012) focused on inferring the polarity of a binary tuple of a polarity word and a target via search engine, while target-aware opinion words extraction is not fully explored. To take the target into account, Wu et al. (2019) proposed to construct a target-speciﬁc sentiment lexicon. However, both NLP preprocessing pipelines (e.g., parsing, POS tagging) and linguistic rules are integrated into their algorithm. Different from them, we ﬁrst extract the target-aware commonsense opinion words via pretrained models, which learned rich commonsense knowledge hidden in human languages. T"
2021.eacl-main.49,2020.coling-main.49,1,0.857223,"9). By modeling differences and similarities of text topics, they can detect new sentiments of words as the domain changes. The basic assumption of those domain-level sentiment lexicons is that a word keeps a consistent sentiment within a domain. This assumption, however, might be strong for ﬁne-granularity analyses of text sentiments: words (especially, neural words such as “long”, “fast”) could exhibit different orientations even in the same domain (Figure 1). To collect more detailed information of a sentiment, another branch of works (aspect-based sentiment analysis (Pontiki et al., 2014; Zhou et al., 2020a,b), opinion relation extraction (Sun et al., 2017)) attempt ﬁnd answers of “who express what opinion on which target” for opinion bearing texts. Existing solutions heavily rely on manual annotations and linguistic rules, which are either hard to scale-up or hard to be complete. In this work, we study the task of extracting target-aware sentiment lexicons. An entry of such lexicon is a pair of a sentiment word and a target word, and their collocation expresses a sentiment. It improves existing domain-dependent lexicons by being more concrete and accurate on describing opinions. Departing from"
2021.emnlp-main.226,P19-1140,0,0.318335,"ational Linguistics linear operations and massive parameters makes GNN hard to be interpreted thoroughly. As a result, it is hard to judge whether the new designs are universal or just over-fitting on a specific dataset. A recent summary (Zhang et al., 2020) notes that several &quot;advanced&quot; EA methods are even beaten by the conventional methods on several public datasets. (2) Low Efficiency: To further increase the performance, newly proposed EA methods try to stack novel techniques, e.g., Graph Attention Networks (Wu et al., 2019a), Graph Matching Networks (Xu et al., 2019), and Joint Learning (Cao et al., 2019a). Consequently, the overall architectures become more and more unnecessarily complex, resulting in their time-space complexities also dramatically increase. Zhao et al. (2020) present that the running time of complex methods (e.g., RDGCN (Wu et al., 2019a)) is 10× more than that of vanilla GCN (Wang et al., 2018). In this paper, we notice that existing GNN-based EA methods inherit considerable complexity from their neural network lineage. Naturally, we consider eliminating the redundant designs from existing EA methods to enhance interpretability and efficiency without losing accuracy. Lever"
2021.emnlp-main.226,2021.eacl-main.53,0,0.0294768,"ts on DBP15K and SRPRS. Baselines are separated in accord with the three groups described in Section 5.2. Most results are from the original papers. Some recent papers are failed to run on missing datasets or do not release the source code yet. We will fill in these blanks after contacting their authors. 5.2 Baselines We compare our method against the following three groups of advanced EA methods: (1) Structure: These methods only use the structure information (i.e., triples): GCN-Align (Wang et al., 2018), MuGNN (Cao et al., 2019a), BootEA (Sun et al., 2018), MRAEA (Mao et al., 2020), JEANS (Chen et al., 2021). (2) Word-level: These methods average the pre-trained entity name vectors to construct the initial features: GM-Align (Xu et al., 2019), RDGCN (Wu et al., 2019a), HGCN (Wu et al., 2019b), DAT (Zeng et al., 2020b), DGMC (Fey et al., 2020). (3) Char-level: These EA methods further adopt the char-level textual features: AttrGNN (Liu et al., 2020), CEA (Zeng et al., 2020a), EPEA (Wang et al., 2020). For our proposed method, SEU(word) and SEU(char) represent the model only using the word and char features as the inputs, respectively. SEU(w+c) represents concatenating the word and char features to"
2021.emnlp-main.226,2020.emnlp-main.515,0,0.114609,"to map entity names into a unified semantic space and then average the pretrained entity name vectors to construct the initial features. To make fair comparisons, we adopt the same entity name translations and word vectors provided by Xu et al. (2019). Char-Level. Because of the contradiction between the extensive existence of proper nouns (e.g., person and city name) and the limited size of word vocabulary, the word-level EA methods suffer from a serious out of vocabulary (OOV) issue. Therefore, many EA methods explore the char-level features, using char-CNN (Wang et al., 2020) or name-BERT (Liu et al., 2020) to extract the char/sub-word features of entities. In order to keep the simplicity and consistency of our proposed method, we adopt the character bigrams of translated entity names as the char-level input textual features instead of complex neural networks. In addition to these text-based methods, we notice that some structure-based EA methods (Wang et al., 2018; Guo et al., 2019) do not require any textual information at all, where the entity features are randomly initialized. Section 5.6 will discuss the connection between text-based and structure-based methods and challenge the necessity o"
2021.emnlp-main.226,D18-1032,0,0.379971,"n by the conventional methods on several public datasets. (2) Low Efficiency: To further increase the performance, newly proposed EA methods try to stack novel techniques, e.g., Graph Attention Networks (Wu et al., 2019a), Graph Matching Networks (Xu et al., 2019), and Joint Learning (Cao et al., 2019a). Consequently, the overall architectures become more and more unnecessarily complex, resulting in their time-space complexities also dramatically increase. Zhao et al. (2020) present that the running time of complex methods (e.g., RDGCN (Wu et al., 2019a)) is 10× more than that of vanilla GCN (Wang et al., 2018). In this paper, we notice that existing GNN-based EA methods inherit considerable complexity from their neural network lineage. Naturally, we consider eliminating the redundant designs from existing EA methods to enhance interpretability and efficiency without losing accuracy. Leveraging the core premise of GNN-based EA methods, we restate the assumption that both structures and textual features of source and target KGs are isomorphic. With this assumption, we are able to successfully transform the cross-lingual EA problem into an assignment problem, which is a fundamental and well-studied co"
2021.emnlp-main.226,2020.emnlp-main.130,0,0.0207116,"er-fitting on a specific dataset. A recent summary (Zhang et al., 2020) notes that several &quot;advanced&quot; EA methods are even beaten by the conventional methods on several public datasets. (2) Low Efficiency: To further increase the performance, newly proposed EA methods try to stack novel techniques, e.g., Graph Attention Networks (Wu et al., 2019a), Graph Matching Networks (Xu et al., 2019), and Joint Learning (Cao et al., 2019a). Consequently, the overall architectures become more and more unnecessarily complex, resulting in their time-space complexities also dramatically increase. Zhao et al. (2020) present that the running time of complex methods (e.g., RDGCN (Wu et al., 2019a)) is 10× more than that of vanilla GCN (Wang et al., 2018). In this paper, we notice that existing GNN-based EA methods inherit considerable complexity from their neural network lineage. Naturally, we consider eliminating the redundant designs from existing EA methods to enhance interpretability and efficiency without losing accuracy. Leveraging the core premise of GNN-based EA methods, we restate the assumption that both structures and textual features of source and target KGs are isomorphic. With this assumption"
2021.emnlp-main.226,D19-1023,0,0.265298,"1 Introduction (Kipf and Welling, 2017) and subsequent Graph Neural Network (GNN) variants have achieved The knowledge graph (KG) represents a collection state-of-the-art results in various graph application. of interlinked descriptions of real-world objects and events, or abstract concepts (e.g., documents), Intuitively, GNN is better in capturing structural inwhich has facilitated many downstream applica- formation of KGs to compensate for the shortcomtions, such as recommendation systems (Cao et al., ing of conventional methods. Specifically, several GNN-based EA methods (Xu et al., 2019; Wu et al., 2019b; Wang et al., 2019) and question-answering (Zhao et al., 2020; Qiu et al., 2020). Over re- 2019a; Wang et al., 2020) indeed demonstrate decent performance improvements on public datasets. cent years, a large number of KGs are constructed All these GNN-based EA methods are built upon from different domains and languages by different a core premise, i.e., entities and their counterparts organizations. These cross-lingual KGs usually have similar neighborhood structures. However, hold unique information individually but also share better performance is not the only outcome of ussome overlapping"
2021.emnlp-main.226,P19-1304,0,0.313814,"nal Network (GCN) 1 Introduction (Kipf and Welling, 2017) and subsequent Graph Neural Network (GNN) variants have achieved The knowledge graph (KG) represents a collection state-of-the-art results in various graph application. of interlinked descriptions of real-world objects and events, or abstract concepts (e.g., documents), Intuitively, GNN is better in capturing structural inwhich has facilitated many downstream applica- formation of KGs to compensate for the shortcomtions, such as recommendation systems (Cao et al., ing of conventional methods. Specifically, several GNN-based EA methods (Xu et al., 2019; Wu et al., 2019b; Wang et al., 2019) and question-answering (Zhao et al., 2020; Qiu et al., 2020). Over re- 2019a; Wang et al., 2020) indeed demonstrate decent performance improvements on public datasets. cent years, a large number of KGs are constructed All these GNN-based EA methods are built upon from different domains and languages by different a core premise, i.e., entities and their counterparts organizations. These cross-lingual KGs usually have similar neighborhood structures. However, hold unique information individually but also share better performance is not the only outcome of u"
2021.emnlp-main.226,D19-1451,0,0.0191904,"isomorphic graph. Based on the vanilla GCN, many EA methods design task-specific modules for improving the performance of EA. Cao et al. (2019a) propose a multichannel GCN to learn multi-aspect information from KGs. Wu et al. (2019a) use a relation-aware dual-graph network to incorporate relation information with structural information. Moreover, due to the lack of labeled data, some methods (Sun et al., 2018; Mao et al., 2020) apply iterative strategies to generate semi-supervised data. In order to provide a multi-aspect view from both structure and semantic, some methods (Wu et al., 2019b; Yang et al., 2019) use word vectors of translated entity names as the input features of GNNs. 3.2 Assignment Problem The assignment problem is a fundamental and wellstudied combinatorial optimization problem. An intuitive instance is to assign N jobs for N workers. Assuming that each worker can do each job at a term, though with varying degrees of efficiency, let xij be the profit if the i-th worker is assigned to the j-th job. Then the problem is to find the best assignment plan (which job should be assigned to which person in one-to-one basis) so that the total profit of performing all jobs is maximum. Formal"
2021.emnlp-main.226,2020.coling-industry.17,0,0.0428977,"er-fitting on a specific dataset. A recent summary (Zhang et al., 2020) notes that several &quot;advanced&quot; EA methods are even beaten by the conventional methods on several public datasets. (2) Low Efficiency: To further increase the performance, newly proposed EA methods try to stack novel techniques, e.g., Graph Attention Networks (Wu et al., 2019a), Graph Matching Networks (Xu et al., 2019), and Joint Learning (Cao et al., 2019a). Consequently, the overall architectures become more and more unnecessarily complex, resulting in their time-space complexities also dramatically increase. Zhao et al. (2020) present that the running time of complex methods (e.g., RDGCN (Wu et al., 2019a)) is 10× more than that of vanilla GCN (Wang et al., 2018). In this paper, we notice that existing GNN-based EA methods inherit considerable complexity from their neural network lineage. Naturally, we consider eliminating the redundant designs from existing EA methods to enhance interpretability and efficiency without losing accuracy. Leveraging the core premise of GNN-based EA methods, we restate the assumption that both structures and textual features of source and target KGs are isomorphic. With this assumption"
2021.emnlp-main.338,N19-1423,0,0.185995,"gnment put and output spaces over languages. For example, tools (Tiedemann et al., 2014; Zhang et al., 2019) the universal dependency project (McDonald et al., (thanks to the universal word forms). 2013) constructs a universal output space for cross• we don’t really need to perform the reordering lingual dependency parsers, and cross-lingual word action. Instead, the correct order can be implicrepresentation learning algorithms helps aligning itly encoded by multi-task learning: word order word forms of different languages (Conneau et al., information accesses the model as a supervision 2017; Devlin et al., 2019). signal. Beyond word form, word order is another imThe separation of reordering module and structured portant factor in cross-lingual structured prediction prediction module provides a new way to both ex∗ This work was conducted when Tao Ji was interning at plore and transfer order information. Alibaba DAMO Academy. 1 We suggest a distillation framework (Hinton https://github.com/AntNLP/zero-shot-structuredprediction. et al., 2015) for learning the reordering module. 4109 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4109–4120 c November 7–11, 2"
2021.emnlp-main.338,D11-1006,0,0.116698,"Missing"
2021.emnlp-main.338,P18-2077,0,0.0542468,"Missing"
2021.emnlp-main.338,P19-1311,0,0.0114392,"rom a task-generic, unsupervised, and large-scale pre-trained reordering teacher. Experiments show that, our model can significantly improve cross-lingual performances on three tasks without obviously hurting source language performance. Future work contains two parts: extending to multi-source transfer and extending to more structured prediction tasks such as NER which requires span-level reordering. There has been a lot of recent research on the cross-lingual transfer of structured prediction tasks, including dependency parsing (Wang and Eisner, 2018a; Ahmad et al., 2019b), and POS tagging (He et al., 2019; Kim et al., 2017). Early work built delexicalized models to direct transfer, but at the expense of performance (McDonald et al., 2011; Rosa and Žabokrtský, 2015). With the development of cross-lingual word embedding techniques (Conneau et al., 2017; Devlin et al., 2019), the recent work has utilized it to retain lexical information (Ahmad et al., 2019b; Wang et al., 2019). Acknowledgments Data augmentation can enrich the word order that appears on the source language, thereby in- The authors wish to thank the reviewers for their helpful comments and suggestions, thank Peng Li creasing the in"
2021.emnlp-main.338,D17-1302,0,0.0291786,"c, unsupervised, and large-scale pre-trained reordering teacher. Experiments show that, our model can significantly improve cross-lingual performances on three tasks without obviously hurting source language performance. Future work contains two parts: extending to multi-source transfer and extending to more structured prediction tasks such as NER which requires span-level reordering. There has been a lot of recent research on the cross-lingual transfer of structured prediction tasks, including dependency parsing (Wang and Eisner, 2018a; Ahmad et al., 2019b), and POS tagging (He et al., 2019; Kim et al., 2017). Early work built delexicalized models to direct transfer, but at the expense of performance (McDonald et al., 2011; Rosa and Žabokrtský, 2015). With the development of cross-lingual word embedding techniques (Conneau et al., 2017; Devlin et al., 2019), the recent work has utilized it to retain lexical information (Ahmad et al., 2019b; Wang et al., 2019). Acknowledgments Data augmentation can enrich the word order that appears on the source language, thereby in- The authors wish to thank the reviewers for their helpful comments and suggestions, thank Peng Li creasing the intersection with the"
2021.emnlp-main.338,P13-2017,0,0.0181681,"Missing"
2021.emnlp-main.338,D15-1039,0,0.0427418,"Missing"
2021.emnlp-main.338,N19-1385,0,0.0512246,"ur experiments, we will select one of the re- ing blocks are directly obtained from order objec−→ ←− ordering signals to train the model and compare it tive (e.g., M and M ). Since we jointly perform with each other. In addition, we use single-head reordering and structured prediction. The data for self-attention to reduce computation because pre- learning word order is constrained by the corpus liminary experiments show that multiple heads are size of structured prediction task (e.g., treebanks). not helpful for reordering blocks. Michel et al. On the other hand, there are massive unlabelled (2019) has also shown that replacing multi-head sentences which can help build a more powerful with single-head does not hurt performance. reordering module. To use those unlabelled data, We cross stack reordering blocks with original one challenge is that directly feeding them into the Transformer blocks to build the complete encoder joint learning model could be problematic since the (Figure 3). The reordering blocks estimate prob- severe imbalance between structured prediction sigability p(os |ωs ) by learning linear word order on nals and reordering signals would make the model the source langua"
2021.emnlp-main.338,P15-2040,0,0.0128549,"al performances on three tasks without obviously hurting source language performance. Future work contains two parts: extending to multi-source transfer and extending to more structured prediction tasks such as NER which requires span-level reordering. There has been a lot of recent research on the cross-lingual transfer of structured prediction tasks, including dependency parsing (Wang and Eisner, 2018a; Ahmad et al., 2019b), and POS tagging (He et al., 2019; Kim et al., 2017). Early work built delexicalized models to direct transfer, but at the expense of performance (McDonald et al., 2011; Rosa and Žabokrtský, 2015). With the development of cross-lingual word embedding techniques (Conneau et al., 2017; Devlin et al., 2019), the recent work has utilized it to retain lexical information (Ahmad et al., 2019b; Wang et al., 2019). Acknowledgments Data augmentation can enrich the word order that appears on the source language, thereby in- The authors wish to thank the reviewers for their helpful comments and suggestions, thank Peng Li creasing the intersection with the target language’s and Yi Gao for their comments on writing. This word order. Tiedemann and Agic (2016); Wang and Eisner (2016, 2018a) create a"
2021.emnlp-main.338,K17-3009,0,0.0255242,"and three structured prediction models on the train set of Universal Dependencies (UD) English-EWT treebank (v2.2) (Nivre et al., 2018). We use the development set and test set of the UD English-EWT treebank to validate source language performance. Following Ahmad et al. (2019a)’s setup, we take 30 other languages as target languages, and use the development set and test set of their treebanks to evaluate target languages performance. For the reordering model, a Base train set is UD English, and an Extra set is automatically annotated raw texts (Ginter et al., 2017) generated by UDPipe v2.0 (Straka and Straková, 2017) from CommonCrawl and Wikipedia. Each sentence is automatic tokenization and syntactic annotations (include UPOS). The hyperparameters we used in word reordering task and downstream tasks are summarized in Appendix B. The statistics of the UD treebanks are summarized in Appendix C. 6.1 Performances of the Reordering Model Our Models and Baselines We explore the input features’ influence, order representations , and unlabeled data size to the reordering model. For input features, we utilize MUSE, mBERT, and optional upos features. For order representations, we utilize −→ ←− an undirected (M ) o"
2021.emnlp-main.338,W14-1614,0,0.0271603,"Missing"
2021.emnlp-main.338,Q16-1035,0,0.0206985,"t al., 2011; Rosa and Žabokrtský, 2015). With the development of cross-lingual word embedding techniques (Conneau et al., 2017; Devlin et al., 2019), the recent work has utilized it to retain lexical information (Ahmad et al., 2019b; Wang et al., 2019). Acknowledgments Data augmentation can enrich the word order that appears on the source language, thereby in- The authors wish to thank the reviewers for their helpful comments and suggestions, thank Peng Li creasing the intersection with the target language’s and Yi Gao for their comments on writing. This word order. Tiedemann and Agic (2016); Wang and Eisner (2016, 2018a) create a high-quality syn- research is funded by the NSFC (62076097) and the 2020 East China Normal University Future Scithetic treebank to increase source data. But data entists and Outstanding Scholars Incubation Proaugmentation requires expert knowledge to build gramme (WLKXJ2020). The corresponding autreebank and extra train time. It does not apply to thors are Tao Ji, Yuanbin Wu and Xiaoling Wang. a larger number of target languages. Annotation projection relies on cross-language annotation mapping using parallel corpus and automatic alignment References (Rasooli and Collins, 201"
2021.emnlp-main.338,Q18-1046,0,0.0122369,"with a bag of words input. The reordering module is distilled from a task-generic, unsupervised, and large-scale pre-trained reordering teacher. Experiments show that, our model can significantly improve cross-lingual performances on three tasks without obviously hurting source language performance. Future work contains two parts: extending to multi-source transfer and extending to more structured prediction tasks such as NER which requires span-level reordering. There has been a lot of recent research on the cross-lingual transfer of structured prediction tasks, including dependency parsing (Wang and Eisner, 2018a; Ahmad et al., 2019b), and POS tagging (He et al., 2019; Kim et al., 2017). Early work built delexicalized models to direct transfer, but at the expense of performance (McDonald et al., 2011; Rosa and Žabokrtský, 2015). With the development of cross-lingual word embedding techniques (Conneau et al., 2017; Devlin et al., 2019), the recent work has utilized it to retain lexical information (Ahmad et al., 2019b; Wang et al., 2019). Acknowledgments Data augmentation can enrich the word order that appears on the source language, thereby in- The authors wish to thank the reviewers for their helpfu"
2021.emnlp-main.338,D18-1163,0,0.0139835,"with a bag of words input. The reordering module is distilled from a task-generic, unsupervised, and large-scale pre-trained reordering teacher. Experiments show that, our model can significantly improve cross-lingual performances on three tasks without obviously hurting source language performance. Future work contains two parts: extending to multi-source transfer and extending to more structured prediction tasks such as NER which requires span-level reordering. There has been a lot of recent research on the cross-lingual transfer of structured prediction tasks, including dependency parsing (Wang and Eisner, 2018a; Ahmad et al., 2019b), and POS tagging (He et al., 2019; Kim et al., 2017). Early work built delexicalized models to direct transfer, but at the expense of performance (McDonald et al., 2011; Rosa and Žabokrtský, 2015). With the development of cross-lingual word embedding techniques (Conneau et al., 2017; Devlin et al., 2019), the recent work has utilized it to retain lexical information (Ahmad et al., 2019b; Wang et al., 2019). Acknowledgments Data augmentation can enrich the word order that appears on the source language, thereby in- The authors wish to thank the reviewers for their helpfu"
2021.emnlp-main.338,D19-1575,0,0.132284,"rders (e.g., SVO or SOV). To share annotations tured prediction. Current sentence encoders among them, we need to handle word order dis(e.g., RNN, Transformer with position emcrepancies carefully: if a model learned on the beddings) are usually word order sensitive. source language is tightly coupled with the source Even with uniform word form representations language word order, performances on target lan(MUSE, mBERT), word order discrepancies guages could be hurt as their word order could be may hurt the adaptation of models. This paper builds structured prediction models with incompatible (Wang et al., 2019). On the other side, bag-of-words inputs. It introduces a new reif one completely drops word order (e.g., bag-ofordering module to organize words following words), the source language (and target languages) the source language order, which learns taskperformances might be poor as order-sensitive feaspecific reordering strategies from a generaltures could be essential. Trade-offs have been made purpose order predictor model. Experiments by using weak word order information (e.g., relon zero-shot cross-lingual dependency parsing, ative positions instead of absolute positions (AhPOS tagging, and"
2021.emnlp-main.338,D19-1092,0,0.0263429,"Missing"
2021.emnlp-main.339,D16-1211,0,0.0132871,"so we can pack them into the batch dimension to obtain an O(1) training complexity. Hence, USE can uniformly extract full structure features efficiently. Comparing to Previous Encoders We divide previous work into three encoding methods: top-k, stack-LSTM, and binary vector. Top-k methods (Chen and Manning, 2014; Weiss et al., 2015) capture the conjunction of only few 1∼3 in-structure items. It extracts only partial structural information. Since the feature template is fixed, it is easy (l−1) Since ct contains the complete structural infor- to batchify. Stack-LSTM methods (Dyer et al., 2015; Ballesteros et al., 2016) can efficiently repmation, the lth layer’s USE module can interact resent all in-structure items, via the PUSH(·) and with other structures and output a more informative (l) (l) (l) (l) representation o∗ = USE(q∗ , K∗ , V∗ ). Then, POP(·) functions. But it loses the information of outside parts and subtree which cannot be treated we obtain a high layer configuration representation as a stack. Besides, Che et al. (2019) point out that by combining these output vectors: its batch computation is very inefficient. Binary (l) (l) (l) (l) (l) (l) ct = MLP(oσ,t ⊕ oβ,t ⊕ oα,t ⊕ oTarc ,t ⊕ oTrel ,t )."
2021.emnlp-main.339,K19-2007,0,0.0213531,"information. Since the feature template is fixed, it is easy (l−1) Since ct contains the complete structural infor- to batchify. Stack-LSTM methods (Dyer et al., 2015; Ballesteros et al., 2016) can efficiently repmation, the lth layer’s USE module can interact resent all in-structure items, via the PUSH(·) and with other structures and output a more informative (l) (l) (l) (l) representation o∗ = USE(q∗ , K∗ , V∗ ). Then, POP(·) functions. But it loses the information of outside parts and subtree which cannot be treated we obtain a high layer configuration representation as a stack. Besides, Che et al. (2019) point out that by combining these output vectors: its batch computation is very inefficient. Binary (l) (l) (l) (l) (l) (l) ct = MLP(oσ,t ⊕ oβ,t ⊕ oα,t ⊕ oTarc ,t ⊕ oTrel ,t ). Vector methods (Zhang et al., 2017) use two binary vectors to model whether each element is in a σ or 2 Note that we use action embedding matrix A instead of a β. It can efficiently encode some outside parts of X when encoding action list α. 3 stack and buffer but loss the information of inside Similar to Equation 2, we give the formulation for the other data structures in Appendix B. position. 4124 We compare existing"
2021.emnlp-main.339,D14-1082,0,0.0793963,"ng transition states with O(1) additional complexity (with respect to basic feature extractors). Experiments on the PTB and UD show that our proposed method significantly improves the test speed and achieves the best transition-based model, and is comparable to state-of-the-art methods. 1 1 Introduction configuration input classifier output buffer stack s-invariant subtree action s-dependent update action action Figure 1: An overview of our transition-based parser. efficiently. However, challenges appear when we try to have the cake and eat it. For example, traditional template-based methods (Chen and Manning, 2014) are fast, but only encode partial information of structures (e.g., few top items on stacks and buffers). Structure-based networks (e.g., StackRNN (Dyer et al., 2015)) rely on carefully designed network architecture to get a full encoding of structures (actually, they still miss some off-structure information, see our discussions in Section 4.2), but they are usually slow (e.g., not easy to batch). Furthermore, different structures have different ways of update (stacks are first-in-last-serve, buffers are first-in-first-serve), it also takes efforts to design different encoders and ways of fus"
2021.emnlp-main.339,P18-1130,0,0.197097,"ffer, and action list, which means that augmenting their information is helpful. Considering the performance gain and computational cost of adding heads, we finally use a total of 8 structural heads. The parser double attent to the stack, buffer and action list. Lexical Encoder UAS Dev LAS Glove + BiLSTM + Xformer 95.72 95.81 95.84 93.79 93.87 93.93 95.71 95.93 95.99 94.05 94.21 94.28 Bert + finetune M&H20 95.90 95.97 95.78 93.97 94.02 93.74 96.21 96.28 96.11 94.56 94.60 94.33 UAS Test LAS Table 2: Lexical encoder comparison on PTB. M&H20: Mohammadshahi and Henderson (2020). Speed Parser Type Ma et al. (2018) Dozat and Manning (2017) Ji et al. (2019) Zhang et al. (2020) T G G‡ G‡ 183 496 403 466 Our arc-hybrid parser T 918 Table 3: Parsing speed comparison on PTB test set. The ‡ indicates high-order graph-based parsers. independent Glove embeddings (Pennington et al., 2014) in the arc-hybrid system. We learn the context via BiLSTM or Transformer encoder. The results show that encoding context can further improve performance and the Transformer encoder is better than BiLSTM. The second part reports the use of contextual Bert networks (Devlin et al., 2019). The introduction of Bert networks and in p"
2021.emnlp-main.339,2020.findings-emnlp.294,0,0.272388,"d (UAS) In a fair comparison, our three unified structure and labeled attachment scores (LAS). For evaluencoding (USE) parsers all achieve significant imations on PTB, five punctuation symbols (“ ” : , provements on PTB. This demonstrates the benefit .) are excluded, while on UD, we use the official of complete structural information by our unified evaluation script. encoding. Hyper-parameters For structure-invariant part, Secondly, we compare with strong graph-based we directly adopt most parameter settings of Ji parsers. The second part of Table 1 contains two et al. (2019) and Zhang et al. (2020), including first-order parsers and two high-order parsers (in pretrained embeddings, BiLSTM, and CharCNN. the red cell). Our USE parsers beat the first-order For structure-dependent part, we use a total of 8 methods, but underperform the high-order methstructural heads, allocating two each for the stack, ods which capture high-order features by graph buffer and action list, one for the subtree’s edges neural networks and TreeCRF. However, speed exand one for the edges’ labels. Our pre-experiments periments show that USE is about 2 times faster show that stacking 6 layers of USE yields the bes"
2021.emnlp-main.339,W03-3017,0,0.0665993,"Missing"
2021.emnlp-main.339,W04-0308,0,0.0423912,"Missing"
2021.emnlp-main.339,J08-4003,0,0.0344169,"contain its lexical form and part-of-speech tag, while its structure-dependent view indicates that w is now sitting on the buffer and its distance to the buffer head is p. When w is detached from buffer and attached to the stack, its structure-dependent ∗ This work was conducted when Tao Ji was interning at view will switch to “sitting on the stack” while its Alibaba DAMO Academy. 1 https://github.com/AntNLP/trans-dep-parser. structure-invariant view stay unchanged. A unified 4121 Transition systems have been successfully applied in many fields of NLP, especially parsing (dependency parsing (Nivre, 2008), constituent parsing (Watanabe and Sumita, 2015), and semantic parsing (Yin and Neubig, 2018)). Basically, a transition system takes a series of actions which attach or detach some items (e.g., sentence words, intermediate outputs) to or from some structures (e.g., stacks, buffers, partial trees). Given a set of action series, a classifier is trained to predict the next action given a current configuration of structures in the transition system. The performances of the final system strongly depend on how well the classifier encodes those transition system configurations. Ideally, a good confi"
2021.emnlp-main.339,D19-1145,0,0.124754,"ing completeness and efficiency, we find that with structure indicators, it is relatively easy to encode a structure completely: one only needs to decompose the structure into identifiable subparts. In fact, we can use them to track some parts of structures which are not revealed in previous work (e.g., words have been popped out from stacks). It runs in the same manner as templated-based models, thus the decoding efficiency is guaranteed. We also note that using structure indicator is different from existing ways to include structure information into neural network models (Shaw et al., 2018; Wang et al., 2019; Shiv and Quirk, 2019): it encodes dynamical structures (changing with transition system running) rather than static structures (e.g., fixed parse trees). We can easily implement the unified structure encoding with existing multi-head attention networks (MHA, (Vaswani et al., 2017)). It is also easy to fuse encodings of different structures with multilayer MHA. We conduct experiments on the English Penn Treebank 3.0 and Universal Dependencies v2.2, show that the unified structure encoder is able to help us achieving state-of-the-art transitionbased parser (even competitive to the best graphba"
2021.emnlp-main.339,P15-1113,0,0.0211638,"of-speech tag, while its structure-dependent view indicates that w is now sitting on the buffer and its distance to the buffer head is p. When w is detached from buffer and attached to the stack, its structure-dependent ∗ This work was conducted when Tao Ji was interning at view will switch to “sitting on the stack” while its Alibaba DAMO Academy. 1 https://github.com/AntNLP/trans-dep-parser. structure-invariant view stay unchanged. A unified 4121 Transition systems have been successfully applied in many fields of NLP, especially parsing (dependency parsing (Nivre, 2008), constituent parsing (Watanabe and Sumita, 2015), and semantic parsing (Yin and Neubig, 2018)). Basically, a transition system takes a series of actions which attach or detach some items (e.g., sentence words, intermediate outputs) to or from some structures (e.g., stacks, buffers, partial trees). Given a set of action series, a classifier is trained to predict the next action given a current configuration of structures in the transition system. The performances of the final system strongly depend on how well the classifier encodes those transition system configurations. Ideally, a good configuration encoder should encode transition system"
2021.emnlp-main.339,P15-1032,0,0.0233944,"tructure). Vaswani et al. (2017) noted that a multi-head attention layer has a constant number (O(1)) of sequentially executed operations, which means that efficient GPU-based computing is possible. In training, the USE calculations at different moments are independent of each other, so we can pack them into the batch dimension to obtain an O(1) training complexity. Hence, USE can uniformly extract full structure features efficiently. Comparing to Previous Encoders We divide previous work into three encoding methods: top-k, stack-LSTM, and binary vector. Top-k methods (Chen and Manning, 2014; Weiss et al., 2015) capture the conjunction of only few 1∼3 in-structure items. It extracts only partial structural information. Since the feature template is fixed, it is easy (l−1) Since ct contains the complete structural infor- to batchify. Stack-LSTM methods (Dyer et al., 2015; Ballesteros et al., 2016) can efficiently repmation, the lth layer’s USE module can interact resent all in-structure items, via the PUSH(·) and with other structures and output a more informative (l) (l) (l) (l) representation o∗ = USE(q∗ , K∗ , V∗ ). Then, POP(·) functions. But it loses the information of outside parts and subtree w"
2021.emnlp-main.339,D17-1175,0,0.01276,"repmation, the lth layer’s USE module can interact resent all in-structure items, via the PUSH(·) and with other structures and output a more informative (l) (l) (l) (l) representation o∗ = USE(q∗ , K∗ , V∗ ). Then, POP(·) functions. But it loses the information of outside parts and subtree which cannot be treated we obtain a high layer configuration representation as a stack. Besides, Che et al. (2019) point out that by combining these output vectors: its batch computation is very inefficient. Binary (l) (l) (l) (l) (l) (l) ct = MLP(oσ,t ⊕ oβ,t ⊕ oα,t ⊕ oTarc ,t ⊕ oTrel ,t ). Vector methods (Zhang et al., 2017) use two binary vectors to model whether each element is in a σ or 2 Note that we use action embedding matrix A instead of a β. It can efficiently encode some outside parts of X when encoding action list α. 3 stack and buffer but loss the information of inside Similar to Equation 2, we give the formulation for the other data structures in Appendix B. position. 4124 We compare existing work with our USE encoder in terms of the coverage of structure features and GPU computing friendly (in Figure 4). Overall, USE does not lose any structural information and more efficient than previous feature ex"
2021.findings-acl.189,D18-1216,0,0.162103,"standard attention (LSTM-ATT) infers a correct sentiment label while pays more attention to the irrelevant word “movie”, making the result difﬁcult to explain. In general, the attention weights are only optimized to encode the task-relevant information while are not restricted to imitate human behavior. In order to enhance the interpretability of the attention mechanism, recent studies turn to integrate the human provided explanation signals into the attention models. Rei and Søgaard (2018) regularized the attention weights with a small amount of word-level annotations. Barrett et al. (2018); Bao et al. (2018) improved the explanation of attention by aligning explanations with human-provided rationales. These methods rely on additional labour consuming labelling for enhancing explanations, which is hard to extend to other datasets or tasks. In this paper, we aim to train a more efﬁcient and effective interpretable attention model without any pre-deﬁned annotations or pre-collected explanations. Speciﬁcally, we propose a framework consisting of a learner and a compressor, which enhances the performance and interpretability of the attention model for text classiﬁcation1 . The learner learns text repr"
2021.findings-acl.189,K18-1030,0,0.0939096,"term memory model with standard attention (LSTM-ATT) infers a correct sentiment label while pays more attention to the irrelevant word “movie”, making the result difﬁcult to explain. In general, the attention weights are only optimized to encode the task-relevant information while are not restricted to imitate human behavior. In order to enhance the interpretability of the attention mechanism, recent studies turn to integrate the human provided explanation signals into the attention models. Rei and Søgaard (2018) regularized the attention weights with a small amount of word-level annotations. Barrett et al. (2018); Bao et al. (2018) improved the explanation of attention by aligning explanations with human-provided rationales. These methods rely on additional labour consuming labelling for enhancing explanations, which is hard to extend to other datasets or tasks. In this paper, we aim to train a more efﬁcient and effective interpretable attention model without any pre-deﬁned annotations or pre-collected explanations. Speciﬁcally, we propose a framework consisting of a learner and a compressor, which enhances the performance and interpretability of the attention model for text classiﬁcation1 . The learn"
2021.findings-acl.189,N19-1423,0,0.0132863,"represents Kullback-Leibler divergence. Speciﬁcally, we regard ppyq as constant and then minimize Epθ py,zq rlog qφ py |zqs. Since we must ﬁrst sample r to sample y, z from pθ pr, y, zq, the lower bound of IpZ; Y q is computed as, IpZ; Y q ě Eppr,yq rEpθ pz|rq rlog qφ py |zqss IpZ;Rq upper bound hkkkkkkkkkkkkkkkkkkkkkikkkkkkkkkkkkkkkkkkkkkj hkkkkkkkkkkkkkkkkkkkkkikkkkkkkkkkkkkkkkkkkkkj pθ pz |rq pθ pz |rq Epprq rEpθ pz|rq rlog ss ´ Epprq rEpθ pz|rq rlog ss rψ pzq ppzq Experiment Setup We adopt two typical neural network models, attention-based LSTM (Hochreiter and Schmidhuber, 1997) and BERT (Devlin et al., 2019), to explore our VAT algorithm. 4.1 Datasets and Baselines Datasets To evaluate the effectiveness of our VAT model, we conduct the experiments over eight benchmark datasets: IMDB (Maas et al., 2011), Stanford Sentiment Treebank with (includes SST1 and its binary version SST-2) (Socher et al., 2013), Yelp (Zhang et al., 2015), AG News (Zhang et al., 2015), TREC (Li and Roth, 2002), subjective/objective classiﬁcation Subj (Pang and Lee, 2005) and Twitter (Rosenthal et al., 2015, 2014). The statistics information of these datasets are shown in Table 1. 2155 Class Length #train #dev #test IMDB 2 2"
2021.findings-acl.189,N19-1357,0,0.0691095,"ce but also the interpretability of the models (Mullenbach et al., 2018; Xie et al., 2017; Xu et al., 2015). Li et al. (2016) pointed the view: “Attention provides an important way to explain the workings of neural models”. Additionally, Wiegreffe and Pinter (2019) showed that attention mechanisms could help understand the inner workings of a model. The basic assumption of understanding of models with attention scores is that the inputs (e.g., words) with high attentive weights are essential for making decisions. However, as far as we know, it has not been formally veriﬁed. Existing research (Jain and Wallace, 2019) also shows that attention is not explicable, and there are a lot of controversy regarding to the result explanations (Wiegreffe and Pinter, 2019; Jain and Wallace, 2019). Moreover, we ﬁnd that though the attention mechanism can help improve the performance for text classiﬁcation in our experiments, it may focus on the irrelevant information. For example, in the sentence “A very funny movie.”, the long short-term memory model with standard attention (LSTM-ATT) infers a correct sentiment label while pays more attention to the irrelevant word “movie”, making the result difﬁcult to explain. In ge"
2021.findings-acl.189,D19-1276,0,0.0857347,"the performance and interpretability of the attention model for text classiﬁcation1 . The learner learns text representations by ﬁne-tuning 1 We focus on the task of text classiﬁcation, but our method can be easily extended to other NLP or CV tasks with attention mechanisms. 2152 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2152–2161 August 1–6, 2021. ©2021 Association for Computational Linguistics the encoder. Regarding to the compressor, we are motivated by the effectiveness of the information bottleneck (IB) (Tishby et al., 1999) to enhance performance (Li and Eisner, 2019) or detect important features (Bang et al., 2019; Chen and Ji, 2020; Jiang et al., 2020; Schulz et al., 2020), and present a Variational information bottleneck ATtention (VAT) mechanism using IB to keep the most relevant clues and forget the irrelevant ones for better attention explanations. In particular, IB is integrated into attention to minimize the mutual information (MI) with the input while preserving as much MI as possible with the output, which provides more accurate and reliable explanations by controlling the information ﬂow. To evaluate the effectiveness of our proposed approach, w"
2021.findings-acl.189,2020.emnlp-main.347,0,0.277583,"lassiﬁcation1 . The learner learns text representations by ﬁne-tuning 1 We focus on the task of text classiﬁcation, but our method can be easily extended to other NLP or CV tasks with attention mechanisms. 2152 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2152–2161 August 1–6, 2021. ©2021 Association for Computational Linguistics the encoder. Regarding to the compressor, we are motivated by the effectiveness of the information bottleneck (IB) (Tishby et al., 1999) to enhance performance (Li and Eisner, 2019) or detect important features (Bang et al., 2019; Chen and Ji, 2020; Jiang et al., 2020; Schulz et al., 2020), and present a Variational information bottleneck ATtention (VAT) mechanism using IB to keep the most relevant clues and forget the irrelevant ones for better attention explanations. In particular, IB is integrated into attention to minimize the mutual information (MI) with the input while preserving as much MI as possible with the output, which provides more accurate and reliable explanations by controlling the information ﬂow. To evaluate the effectiveness of our proposed approach, we adapt two advanced neural models (LSTM and BERT) within the frame"
2021.findings-acl.189,P11-1015,0,0.0834101,"nd of IpZ; Y q is computed as, IpZ; Y q ě Eppr,yq rEpθ pz|rq rlog qφ py |zqss IpZ;Rq upper bound hkkkkkkkkkkkkkkkkkkkkkikkkkkkkkkkkkkkkkkkkkkj hkkkkkkkkkkkkkkkkkkkkkikkkkkkkkkkkkkkkkkkkkkj pθ pz |rq pθ pz |rq Epprq rEpθ pz|rq rlog ss ´ Epprq rEpθ pz|rq rlog ss rψ pzq ppzq Experiment Setup We adopt two typical neural network models, attention-based LSTM (Hochreiter and Schmidhuber, 1997) and BERT (Devlin et al., 2019), to explore our VAT algorithm. 4.1 Datasets and Baselines Datasets To evaluate the effectiveness of our VAT model, we conduct the experiments over eight benchmark datasets: IMDB (Maas et al., 2011), Stanford Sentiment Treebank with (includes SST1 and its binary version SST-2) (Socher et al., 2013), Yelp (Zhang et al., 2015), AG News (Zhang et al., 2015), TREC (Li and Roth, 2002), subjective/objective classiﬁcation Subj (Pang and Lee, 2005) and Twitter (Rosenthal et al., 2015, 2014). The statistics information of these datasets are shown in Table 1. 2155 Class Length #train #dev #test IMDB 2 268 20,000 5,000 25,000 SST-1 5 18 8,544 1,1101 2,210 SST-2 2 19 6,920 872 1,821 Yelp 2 138 500,000 60,000 38,000 AG News 4 32 114,000 6,000 7,600 Trec 6 10 5,000 452 500 Subj 2 23 8,000 1,000 1,000"
2021.findings-acl.189,N18-1100,0,0.0291649,"with a Variational information bottleneck ATtention (VAT) mechanism. Extensive experiments on eight benchmark datasets show the great advantages of our proposed approach in terms of both performance and interpretability. 1 Introduction Attention mechanisms (Bahdanau et al., 2014) have achieved great success in various natural language processing (NLP) tasks. They are introduced to mimic the human eye focusing on important parts in the inputs when predicting labels. The existing studies show attention mechanisms can improve not only the performance but also the interpretability of the models (Mullenbach et al., 2018; Xie et al., 2017; Xu et al., 2015). Li et al. (2016) pointed the view: “Attention provides an important way to explain the workings of neural models”. Additionally, Wiegreffe and Pinter (2019) showed that attention mechanisms could help understand the inner workings of a model. The basic assumption of understanding of models with attention scores is that the inputs (e.g., words) with high attentive weights are essential for making decisions. However, as far as we know, it has not been formally veriﬁed. Existing research (Jain and Wallace, 2019) also shows that attention is not explicable, an"
2021.findings-acl.189,N18-1097,0,0.026584,"space into two-dimensional space. Accuracy AOPC Accuracy AOPC LSTM-base Random LSTM-ATT LSTM-VAT BERT-base Random BERT-ATT BERT-VAT IMDB 88.79 0.30 5.27 6.13 91.90 0.60 2.81 3.17 SST-1 45.20 5.97 12.94 14.34 51.44 33.26 33.98 34.03 SST-2 85.45 7.58 20.54 21.58 91.60 41.46 41.52 41.52 Yelp 95.10 1.02 6.64 7.12 96.07 3.60 4.73 6.64 AG News 91.91 1.87 5.99 6.59 93.52 44.20 52.22 54.70 Trec 90.00 19.40 31.00 37.20 96.60 65.80 71.60 72.20 Subj 89.00 1.50 2.10 6.30 96.50 45.70 45.70 45.80 Twitter 71.25 4.72 19.10 20.37 75.28 59.21 59.39 59.45 Table 3: The results of AOPC. perturbation curve (AOPC) (Nguyen, 2018; Samek et al., 2016) metric. It calculates the average change of accuracy over test data by deleting top K words via attentive weights. The larger the value of AOPC, the better the explanations of the models. (a) IMDB (LSTM) (b) IMDB (BERT) Figure 4: The inﬂuence of Top-K for LSTM/BERTbased models in terms of AOPC. servations show our VAT model can learn a better task-speciﬁc representation by enforcing the model to reduce the task-irrelevant information. 5.2 Quantitative Evaluation In this section, we evaluate our VAT model using two metrics, AOPC and post-hoc accuracy, which are widely used"
2021.findings-acl.189,P05-1015,0,0.274964,"θ pz|rq rlog ss rψ pzq ppzq Experiment Setup We adopt two typical neural network models, attention-based LSTM (Hochreiter and Schmidhuber, 1997) and BERT (Devlin et al., 2019), to explore our VAT algorithm. 4.1 Datasets and Baselines Datasets To evaluate the effectiveness of our VAT model, we conduct the experiments over eight benchmark datasets: IMDB (Maas et al., 2011), Stanford Sentiment Treebank with (includes SST1 and its binary version SST-2) (Socher et al., 2013), Yelp (Zhang et al., 2015), AG News (Zhang et al., 2015), TREC (Li and Roth, 2002), subjective/objective classiﬁcation Subj (Pang and Lee, 2005) and Twitter (Rosenthal et al., 2015, 2014). The statistics information of these datasets are shown in Table 1. 2155 Class Length #train #dev #test IMDB 2 268 20,000 5,000 25,000 SST-1 5 18 8,544 1,1101 2,210 SST-2 2 19 6,920 872 1,821 Yelp 2 138 500,000 60,000 38,000 AG News 4 32 114,000 6,000 7,600 Trec 6 10 5,000 452 500 Subj 2 23 8,000 1,000 1,000 Twitter 3 22 7,969 1,375 3,795 Table 1: The statistics information of the datasets, where Class is the number of the class, Length is average text length, and #train/#dev/#test counts the number of samples in the train/dev/test sets. LSTM-base LS"
2021.findings-acl.189,D14-1162,0,0.0851244,"sic models (LSTM/BERT-base) and attention-based models (LSTM/BERT-ATT). LSTM-base takes the max-pooling of the LSTM’s hidden vectors as text representation. For BERTbase, the “[CLS]” representation is obtained as the sentence representation. LSTM-ATT model is a standard attention-based LSTM model that has the same structure as the learner. We obtain the BERTATT by replacing the LSTM encoder with BERT in LSTM-ATT. Our models are marked with VAT (LSTM-VAT, BERT-VAT), which integrate VIB into attention-based neural models. 4.2 Implementation Details For LSTM-based models, we use GloVe embedding (Pennington et al., 2014) with 300-dimension to initialize the word embedding and ﬁne-tune it during the training. We randomly initialize all outof-vocabulary words and weights with the uniform distribution U p´0.1, 0.1q. For the BERT-based models, we ﬁne-tune pre-trained BERT-base model. The dimension of hidden state vectors of LSTM is 100 and the max sentence length is 256 in our experiments. Adam (Kingma and Ba, 2014) is utilized as the optimizer with learning rate 0.001 (for LSTM-based model) and 0.00001 (for BERTbased model). We also search different values β P t0.01, 0.1, 1, 10u. 5 Experiments First, we perform"
2021.findings-acl.189,D19-1002,0,0.0521488,"rformance and interpretability. 1 Introduction Attention mechanisms (Bahdanau et al., 2014) have achieved great success in various natural language processing (NLP) tasks. They are introduced to mimic the human eye focusing on important parts in the inputs when predicting labels. The existing studies show attention mechanisms can improve not only the performance but also the interpretability of the models (Mullenbach et al., 2018; Xie et al., 2017; Xu et al., 2015). Li et al. (2016) pointed the view: “Attention provides an important way to explain the workings of neural models”. Additionally, Wiegreffe and Pinter (2019) showed that attention mechanisms could help understand the inner workings of a model. The basic assumption of understanding of models with attention scores is that the inputs (e.g., words) with high attentive weights are essential for making decisions. However, as far as we know, it has not been formally veriﬁed. Existing research (Jain and Wallace, 2019) also shows that attention is not explicable, and there are a lot of controversy regarding to the result explanations (Wiegreffe and Pinter, 2019; Jain and Wallace, 2019). Moreover, we ﬁnd that though the attention mechanism can help improve"
2021.findings-acl.189,N18-1027,0,0.254065,"focus on the irrelevant information. For example, in the sentence “A very funny movie.”, the long short-term memory model with standard attention (LSTM-ATT) infers a correct sentiment label while pays more attention to the irrelevant word “movie”, making the result difﬁcult to explain. In general, the attention weights are only optimized to encode the task-relevant information while are not restricted to imitate human behavior. In order to enhance the interpretability of the attention mechanism, recent studies turn to integrate the human provided explanation signals into the attention models. Rei and Søgaard (2018) regularized the attention weights with a small amount of word-level annotations. Barrett et al. (2018); Bao et al. (2018) improved the explanation of attention by aligning explanations with human-provided rationales. These methods rely on additional labour consuming labelling for enhancing explanations, which is hard to extend to other datasets or tasks. In this paper, we aim to train a more efﬁcient and effective interpretable attention model without any pre-deﬁned annotations or pre-collected explanations. Speciﬁcally, we propose a framework consisting of a learner and a compressor, which e"
2021.findings-acl.189,S15-2078,0,0.154169,"iment Setup We adopt two typical neural network models, attention-based LSTM (Hochreiter and Schmidhuber, 1997) and BERT (Devlin et al., 2019), to explore our VAT algorithm. 4.1 Datasets and Baselines Datasets To evaluate the effectiveness of our VAT model, we conduct the experiments over eight benchmark datasets: IMDB (Maas et al., 2011), Stanford Sentiment Treebank with (includes SST1 and its binary version SST-2) (Socher et al., 2013), Yelp (Zhang et al., 2015), AG News (Zhang et al., 2015), TREC (Li and Roth, 2002), subjective/objective classiﬁcation Subj (Pang and Lee, 2005) and Twitter (Rosenthal et al., 2015, 2014). The statistics information of these datasets are shown in Table 1. 2155 Class Length #train #dev #test IMDB 2 268 20,000 5,000 25,000 SST-1 5 18 8,544 1,1101 2,210 SST-2 2 19 6,920 872 1,821 Yelp 2 138 500,000 60,000 38,000 AG News 4 32 114,000 6,000 7,600 Trec 6 10 5,000 452 500 Subj 2 23 8,000 1,000 1,000 Twitter 3 22 7,969 1,375 3,795 Table 1: The statistics information of the datasets, where Class is the number of the class, Length is average text length, and #train/#dev/#test counts the number of samples in the train/dev/test sets. LSTM-base LSTM-ATT LSTM-VAT BERT-base BERT-ATT B"
2021.findings-acl.189,S14-2009,0,0.0991451,"Missing"
2021.findings-acl.189,P17-1088,0,0.0167323,"mation bottleneck ATtention (VAT) mechanism. Extensive experiments on eight benchmark datasets show the great advantages of our proposed approach in terms of both performance and interpretability. 1 Introduction Attention mechanisms (Bahdanau et al., 2014) have achieved great success in various natural language processing (NLP) tasks. They are introduced to mimic the human eye focusing on important parts in the inputs when predicting labels. The existing studies show attention mechanisms can improve not only the performance but also the interpretability of the models (Mullenbach et al., 2018; Xie et al., 2017; Xu et al., 2015). Li et al. (2016) pointed the view: “Attention provides an important way to explain the workings of neural models”. Additionally, Wiegreffe and Pinter (2019) showed that attention mechanisms could help understand the inner workings of a model. The basic assumption of understanding of models with attention scores is that the inputs (e.g., words) with high attentive weights are essential for making decisions. However, as far as we know, it has not been formally veriﬁed. Existing research (Jain and Wallace, 2019) also shows that attention is not explicable, and there are a lot"
2021.findings-acl.189,D13-1170,0,0.00379002,"kkkkkkkkkkkkkkkkkkkkikkkkkkkkkkkkkkkkkkkkkj hkkkkkkkkkkkkkkkkkkkkkikkkkkkkkkkkkkkkkkkkkkj pθ pz |rq pθ pz |rq Epprq rEpθ pz|rq rlog ss ´ Epprq rEpθ pz|rq rlog ss rψ pzq ppzq Experiment Setup We adopt two typical neural network models, attention-based LSTM (Hochreiter and Schmidhuber, 1997) and BERT (Devlin et al., 2019), to explore our VAT algorithm. 4.1 Datasets and Baselines Datasets To evaluate the effectiveness of our VAT model, we conduct the experiments over eight benchmark datasets: IMDB (Maas et al., 2011), Stanford Sentiment Treebank with (includes SST1 and its binary version SST-2) (Socher et al., 2013), Yelp (Zhang et al., 2015), AG News (Zhang et al., 2015), TREC (Li and Roth, 2002), subjective/objective classiﬁcation Subj (Pang and Lee, 2005) and Twitter (Rosenthal et al., 2015, 2014). The statistics information of these datasets are shown in Table 1. 2155 Class Length #train #dev #test IMDB 2 268 20,000 5,000 25,000 SST-1 5 18 8,544 1,1101 2,210 SST-2 2 19 6,920 872 1,821 Yelp 2 138 500,000 60,000 38,000 AG News 4 32 114,000 6,000 7,600 Trec 6 10 5,000 452 500 Subj 2 23 8,000 1,000 1,000 Twitter 3 22 7,969 1,375 3,795 Table 1: The statistics information of the datasets, where Class is th"
2021.findings-acl.277,D19-5808,0,0.0145005,"st partition (left), on ParaRules test partitions (middle) and on Birds-Electricity dataset (right), after training on DU5 or partial DU5 (RC-k) training splits. points out that a reasoning system should not only answer queries but also generate a proof. However, PROVER adopts the multi-task learning framework in the training stage and cannot effectively capture the interactions between question answering and proof generation. Along this line, we explore more powerful joint models to achieve deep reasoning. QA and NLI There are bAbI (Weston et al., 2016), QuaRTz (Tafjord et al., 2019), ROPES (Lin et al., 2019) and Hotpot QA (Yang et al., 2018) (QA datasets) involved in rule reasoning. However, for those datasets, implicit rules (i.e., which multihop chains are valid) need to be inferred from the training data. In our task, the rules of reasoning are given in advance. Compared with the Natural Language Inference (MacCartney and Manning, 2014), our task can be regarded as its deductive subset. In particular, NLI allows for unsupported inferences (Dagan et al., 2013). 6 Conclusion In this work, we propose PROBR, a novel probabilistic graph reasoning framework for joint question answering and proof gen"
2021.findings-acl.277,2021.ccl-1.108,0,0.021698,"Missing"
2021.findings-acl.277,W09-3714,0,0.0303451,", 2005; Berant et al., 2013; Berant and Liang, 2014), researchers focus on developing theorem provers by combining the symbolic techniques with the differentiable learning from neural networks (Reed and de Freitas, 2016; Abdelaziz et al., 2020; Abboud et al., 2020), such as NLProlog (Weber et al., 2019), SAT solving (Selsam et al., 2019) and Neural programme (Neelakantan et al., 2016). To bypass this expensive and error-prone intermediate logical representation, reasoning over natural language statements in an end-to-end manner is promising. Text Reasoning over Natural Language Natural logic (MacCartney and Manning, 2009) focuses on semantic containment and monotonicity by incorporating semantic exclusion and implicativity. Subsequently, Clark et al. (2020) proposes to use a Transformer-based model to emulate deductive reasoning and achieves high accuracy on synthetically generated data. PROVER (Saha et al., 2020) 7 For more details, please refer to the supplementary materials. 3147 PROBR PROBR+GOLD PROBR+KL PROBR+GOLD+KL Figure 3: QA accuracy compared among PROBR, PROBR + Gold, PROBR + KL, and PROBR + Gold + KL on DU5 test partition (left), on ParaRules test partitions (middle) and on Birds-Electricity datase"
2021.findings-acl.277,2020.emnlp-main.9,0,0.466799,"of to prove or disprove the query. For example, in Figure 1, there are two facts, six rules and two queries, each of which is expressed by natural language. To predict the true/false of each query, starting from the facts, we need to reason deductively by applying given rules ∗ Equal contribution. until we can derive the truth value of the query. The process of deduction can be represented as a graph, whose node is either a fact, rule or special NAF node (explained in the Section 2.1). Generating answer and proof together makes a system easier to interpret and diagnose. Recent work by PROVER (Saha et al., 2020) first explored this problem through two modules: question answering and proof generation. It trains these two modules through implicit parameter sharing, and then uses integer linear programming (ILP) to enforce consistency constraints (only test time). It is difficult to ensure that the proof generation module contributes to the question answering module, because the proof is not explicitly involved in the answer prediction. Parameter sharing becomes more limited under few/zero-shot settings, as demonstrated in our experiments. We expect the proof to enhance the capability of question answer"
2021.findings-acl.277,D19-1608,0,0.0134592,"nd PROBR + Gold + KL on DU5 test partition (left), on ParaRules test partitions (middle) and on Birds-Electricity dataset (right), after training on DU5 or partial DU5 (RC-k) training splits. points out that a reasoning system should not only answer queries but also generate a proof. However, PROVER adopts the multi-task learning framework in the training stage and cannot effectively capture the interactions between question answering and proof generation. Along this line, we explore more powerful joint models to achieve deep reasoning. QA and NLI There are bAbI (Weston et al., 2016), QuaRTz (Tafjord et al., 2019), ROPES (Lin et al., 2019) and Hotpot QA (Yang et al., 2018) (QA datasets) involved in rule reasoning. However, for those datasets, implicit rules (i.e., which multihop chains are valid) need to be inferred from the training data. In our task, the rules of reasoning are given in advance. Compared with the Natural Language Inference (MacCartney and Manning, 2014), our task can be regarded as its deductive subset. In particular, NLI allows for unsupported inferences (Dagan et al., 2013). 6 Conclusion In this work, we propose PROBR, a novel probabilistic graph reasoning framework for joint questi"
2021.findings-acl.277,P19-1618,0,0.0627666,"Missing"
2021.findings-acl.277,D18-1259,0,0.0249411,"test partitions (middle) and on Birds-Electricity dataset (right), after training on DU5 or partial DU5 (RC-k) training splits. points out that a reasoning system should not only answer queries but also generate a proof. However, PROVER adopts the multi-task learning framework in the training stage and cannot effectively capture the interactions between question answering and proof generation. Along this line, we explore more powerful joint models to achieve deep reasoning. QA and NLI There are bAbI (Weston et al., 2016), QuaRTz (Tafjord et al., 2019), ROPES (Lin et al., 2019) and Hotpot QA (Yang et al., 2018) (QA datasets) involved in rule reasoning. However, for those datasets, implicit rules (i.e., which multihop chains are valid) need to be inferred from the training data. In our task, the rules of reasoning are given in advance. Compared with the Natural Language Inference (MacCartney and Manning, 2014), our task can be regarded as its deductive subset. In particular, NLI allows for unsupported inferences (Dagan et al., 2013). 6 Conclusion In this work, we propose PROBR, a novel probabilistic graph reasoning framework for joint question answering and proof generation. PROBR defines a joint dis"
D09-1159,H05-1045,0,0.0102432,"Missing"
D09-1159,W06-1651,0,0.0530414,"work on extracting opinion units including: opinion holder, subject, aspect and evaluation. Subject and aspect belong to product features, while evaluation is the opinion expression in our work. They converted the task to two kinds of relation extraction tasks and proposed a machine learning-based method which combines contextual clues and statistical clues. Their experimental results showed that the model using contextual clues improved the performance. However since the contextual information in a domain is specific, the model got by their approach can not easily converted to other domains. Choi et al. (2006) used an integer linear programming approach to jointly extract entities and relations in the context of opinion oriented information extraction. They identified expressions of opinions, sources of opinions and the linking relation that exists between them. The sources of opinions denote to the person or entity that holds the opinion. Another area related to our work is opinion expressions identification (Wilson et al., 2005a; Breck et al., 2007). They worked on identifying the words and phrases that express opinions in text. According to Wiebe et al. (2005), there are two types of opinion exp"
D09-1159,P04-1054,0,0.108616,"an that could be represented by a feature extraction-based approach, we define a new tree kernel over phrase dependency trees and incorporate this kernel within an SVM to extract relations between opinion expressions and product features. The potential relation set consists of the all combinations between candidate product features and candidate opinion expressions in a sentence. Given a phrase dependency parsing tree, we choose the subtree rooted at the lowest common parent(LCP) of opinion expression and product feature to represent the relation. Dependency tree kernels has been proposed by (Culotta and Sorensen, 2004). Their kernel is defined on lexical dependency tree by the convolution of similarities between all possible subtrees. However, if the convolution containing too many irrelevant subtrees, over-fitting may occur and decreases the performance of the classifier. In phrase dependency tree, local words in a same phrase are compacted, therefore it provides a way to treat “local dependencies” and “global dependencies” differently (Fig. 3). As a consequence, these two kinds of dependencies will not disturb each other in measuring similarity. Later experiments prove the validity of this statement. Phra"
D09-1159,C04-1200,0,0.59445,"lyzing this content are impossible tasks if they were to be manually done. However, advances in machine learning and natural language processing present us with a unique opportunity to automate the decoding of consumers’ opinions from online reviews. Previous works on mining opinions can be divided into two directions: sentiment classification and sentiment related information extraction. The former is a task of identifying positive and negative sentiments from a text which can be a passage, a sentence, a phrase and even a word (Somasundaran et al., 2008; Pang et al., 2002; Dave et al., 2003; Kim and Hovy, 2004; Takamura et al., 2005). The latter focuses on extracting the elements composing a sentiment text. The elements 1. I highly [recommend](1) the Canon SD500(1) to anybody looking for a compact camera that can take [good](2) pictures(2) . 2. This camera takes [amazing](3) image qualities(3) and its size(4) [cannot be beat](4) . The phrases underlined are the product features, marked with square brackets are opinion expressions. Product features and opinion expressions with identical superscript compose a relation. For the first sentence, an opinion relation exists between “the Canon SD500” and “"
D09-1159,D07-1114,0,0.71835,"product review. In practice, for a certain domain of product reviews, a language model is build on easily acquired unlabeled data. Each candidate NP or VP chunk in the output of shallow parser is scored by the model, and cut off if its score is less than a threshold. Opinion expressions are spans of text that express a comment or attitude of the opinion holder, which are usually evaluative or subjective phrases. We also analyze the labeled corpus for opinion expressions and observe that many opinion expressions are used in multiple domains, which is identical with the conclusion presented by Kobayashi et al. (2007). They collected 5,550 opinion expressions from various sources . The coverage of the dictionary is high in multiple domains. Motivated by those observations, we use a dictionary which contains 8221 opinion expressions to select candidates (Wilson et al., 2005b). An assumption we use to filter candidate opinion expressions is that opinion expressions tend to appear closely with product features, which is also used to extract product features by Hu and Liu (2004). In our experiments, the tree distance between product feature and opinion expression in a relation should be less than 5 in the phra"
D09-1159,W02-1011,0,0.023691,"s. Retrieving this information and analyzing this content are impossible tasks if they were to be manually done. However, advances in machine learning and natural language processing present us with a unique opportunity to automate the decoding of consumers’ opinions from online reviews. Previous works on mining opinions can be divided into two directions: sentiment classification and sentiment related information extraction. The former is a task of identifying positive and negative sentiments from a text which can be a passage, a sentence, a phrase and even a word (Somasundaran et al., 2008; Pang et al., 2002; Dave et al., 2003; Kim and Hovy, 2004; Takamura et al., 2005). The latter focuses on extracting the elements composing a sentiment text. The elements 1. I highly [recommend](1) the Canon SD500(1) to anybody looking for a compact camera that can take [good](2) pictures(2) . 2. This camera takes [amazing](3) image qualities(3) and its size(4) [cannot be beat](4) . The phrases underlined are the product features, marked with square brackets are opinion expressions. Product features and opinion expressions with identical superscript compose a relation. For the first sentence, an opinion relation"
D09-1159,H05-1043,0,0.956926,"Missing"
D09-1159,C08-1101,0,0.0140112,"d tracking customer opinions. Retrieving this information and analyzing this content are impossible tasks if they were to be manually done. However, advances in machine learning and natural language processing present us with a unique opportunity to automate the decoding of consumers’ opinions from online reviews. Previous works on mining opinions can be divided into two directions: sentiment classification and sentiment related information extraction. The former is a task of identifying positive and negative sentiments from a text which can be a passage, a sentence, a phrase and even a word (Somasundaran et al., 2008; Pang et al., 2002; Dave et al., 2003; Kim and Hovy, 2004; Takamura et al., 2005). The latter focuses on extracting the elements composing a sentiment text. The elements 1. I highly [recommend](1) the Canon SD500(1) to anybody looking for a compact camera that can take [good](2) pictures(2) . 2. This camera takes [amazing](3) image qualities(3) and its size(4) [cannot be beat](4) . The phrases underlined are the product features, marked with square brackets are opinion expressions. Product features and opinion expressions with identical superscript compose a relation. For the first sentence,"
D09-1159,P05-1017,0,0.272784,"are impossible tasks if they were to be manually done. However, advances in machine learning and natural language processing present us with a unique opportunity to automate the decoding of consumers’ opinions from online reviews. Previous works on mining opinions can be divided into two directions: sentiment classification and sentiment related information extraction. The former is a task of identifying positive and negative sentiments from a text which can be a passage, a sentence, a phrase and even a word (Somasundaran et al., 2008; Pang et al., 2002; Dave et al., 2003; Kim and Hovy, 2004; Takamura et al., 2005). The latter focuses on extracting the elements composing a sentiment text. The elements 1. I highly [recommend](1) the Canon SD500(1) to anybody looking for a compact camera that can take [good](2) pictures(2) . 2. This camera takes [amazing](3) image qualities(3) and its size(4) [cannot be beat](4) . The phrases underlined are the product features, marked with square brackets are opinion expressions. Product features and opinion expressions with identical superscript compose a relation. For the first sentence, an opinion relation exists between “the Canon SD500” and “recommend”, but not betw"
D09-1159,H05-2018,0,0.103904,"old. Opinion expressions are spans of text that express a comment or attitude of the opinion holder, which are usually evaluative or subjective phrases. We also analyze the labeled corpus for opinion expressions and observe that many opinion expressions are used in multiple domains, which is identical with the conclusion presented by Kobayashi et al. (2007). They collected 5,550 opinion expressions from various sources . The coverage of the dictionary is high in multiple domains. Motivated by those observations, we use a dictionary which contains 8221 opinion expressions to select candidates (Wilson et al., 2005b). An assumption we use to filter candidate opinion expressions is that opinion expressions tend to appear closely with product features, which is also used to extract product features by Hu and Liu (2004). In our experiments, the tree distance between product feature and opinion expression in a relation should be less than 5 in the phrase dependency parsing tree. 2.3 Relation Extraction This section describes our method on extracting relations between opinion expressions and product features using phrase dependency tree. Manually built patterns were used in previous works which have an obvio"
D09-1159,H05-1044,0,0.194139,"old. Opinion expressions are spans of text that express a comment or attitude of the opinion holder, which are usually evaluative or subjective phrases. We also analyze the labeled corpus for opinion expressions and observe that many opinion expressions are used in multiple domains, which is identical with the conclusion presented by Kobayashi et al. (2007). They collected 5,550 opinion expressions from various sources . The coverage of the dictionary is high in multiple domains. Motivated by those observations, we use a dictionary which contains 8221 opinion expressions to select candidates (Wilson et al., 2005b). An assumption we use to filter candidate opinion expressions is that opinion expressions tend to appear closely with product features, which is also used to extract product features by Hu and Liu (2004). In our experiments, the tree distance between product feature and opinion expression in a relation should be less than 5 in the phrase dependency parsing tree. 2.3 Relation Extraction This section describes our method on extracting relations between opinion expressions and product features using phrase dependency tree. Manually built patterns were used in previous works which have an obvio"
D09-1159,H01-1014,0,0.00472748,"roblem for dependency grammar. Fig. 2(a) shows the dependency representation of an example sentence. The root of the sentence is “enjoyed”. There are seven pairs of dependency relationships, depicted by seven arcs from heads to dependents. 2.1.2 Phrase Dependency Parsing Currently, the mainstream of dependency parsing is conducted on lexical elements: relations are built between single words. A major information loss of this word level dependency tree compared with constituent tree is that it doesn’t explicitly provide local structures and syntactic categories (i.e. NP, VP labels) of phrases (Xia and Palmer, 2001). On the other hand, dependency tree provides connections between distant words, which are useful in extracting long distance relations. Therefore, compromising between the two, we extend the dependency tree node with phrases. That implies a noun phrase “Cannon SD500 PowerShot” can be a dependent that modifies a verb phrase head “really enjoy using” with relation type “dobj”. The feasibility behind is that a phrase is a syntactic unit regardless of the length or syntactic category (Santorini and Kroch, 2007), and it is acceptable to substitute a single word by a phrase with same syntactic cate"
D09-1159,H05-2017,0,\N,Missing
D11-1123,P09-1079,0,0.0857641,"Missing"
D11-1123,N07-1030,0,0.0131716,"rative sentences and proposed learning approaches to identify them. Sentiment analysis of conditional sentences were studied by Narayanan et al. (2009). They aimed 1340 to determine whether opinions expressed on different topics in a conditional sentence are positive, negative or neutral. They analyzed the conditional sentences in both linguistic and computitional perspectives and used learning method to do it. They followed the feature-based sentiment analysis model (Hu and Liu, 2004), which also use flat frames to represent evaluations. Integer linear programming was used in many NLP tasks (Denis and Baldridge, 2007), for its power in both expressing and approximating various inference problems, especially in parsing (Riedel and Clarke, 2006; Martins et al., 2009). Martins etc. (2009) also applied ILP with flow formulation for maximum spanning tree, besides, they also handled dependency parse trees involving high order features(sibling, grandparent), and with projective constraint. 6 Conclusions This paper introduces a representation method for opinions in online reviews. Inspections on corpus show that the information ignored in previous sentiment representation can cause incorrect or incomplete mining r"
D11-1123,P10-1041,0,0.0347343,"Missing"
D11-1123,P06-2063,0,0.0289828,"Missing"
D11-1123,D07-1114,0,0.155196,"Results on vertices extraction with 10 folder cross validation. We use two criterion: 1) the vertex is correct if it is exactly same as ground truth(“E”), 2) the vertex is correct if it overlaps with ground truth(“O”). 5 Related Work Opinion mining has recently received considerable attentions. Large amount of work has been done on sentimental classification in different levels and sentiment related information extraction. Researches on different types of sentences such as comparative sentences (Jindal and Liu, 2006) and conditional sentences (Narayanan et al., 2009) have also been proposed. Kobayashi et al. (2007) presented their work on extracting opinion units including: opinion holder, subject, aspect and evaluation. They used slots to represent evaluations, converted the task to two kinds of relation extraction tasks and proposed a machine learning-based method which used both contextual and statistical clues. Jindal and Liu (2006) studied the problem of identifying comparative sentences. They analyzed different types of comparative sentences and proposed learning approaches to identify them. Sentiment analysis of conditional sentences were studied by Narayanan et al. (2009). They aimed 1340 to det"
D11-1123,P09-1039,0,0.115006,"connects to more than one opinion expression rarely occur comparing with those vertices which have a single parent. An explaination for this sparseness is that opinions in online reviews always concentrate in local context and have local semantic connections. 3.2.2 ILP Formulation Based on the property 3, we divide the inference algorithm into two steps: i) constructing G’s spanning tree (arborescence) with property 1 and 2; ii) finding additional non-tree edges as a post processing task. The first step is close to the works on ILP formulations of dependency parsing (Riedel and Clarke, 2006; Martins et al., 2009). In the second step, we use a heuristic method which greedily adds non-tree edges. A similar approximation method is also used in (Mcdonald and Pereira, 2006) for acyclic dependency graphs. Step 1. Find MST. Following the multicommodity flow formulation of maximum spanning tree (MST) problem in (Magnanti and Wolsey, 1994), the ILP for MST is: ∑ max. yij · score(xi , xj ) (3) i,j ∑ s.t. ∑ i i,j (4) yij = |V |− 1 fiju − ∑ ∑ k u f0k u fjk = δju ,1 ≤ u, j ≤ |V |(5) = 1, k fiju fiju (6) 1 ≤ u ≤ |V | ≤ yij , 1 ≤ u, j ≤ |V |, ≥ 0, 1 ≤ u, j ≤ |V |, yij ∈ { 0, 1}, 0 ≤ i ≤ |V | (7) 0 ≤ i ≤ |V | (8) 0 ≤"
D11-1123,E06-1011,0,0.0461743,"is that opinions in online reviews always concentrate in local context and have local semantic connections. 3.2.2 ILP Formulation Based on the property 3, we divide the inference algorithm into two steps: i) constructing G’s spanning tree (arborescence) with property 1 and 2; ii) finding additional non-tree edges as a post processing task. The first step is close to the works on ILP formulations of dependency parsing (Riedel and Clarke, 2006; Martins et al., 2009). In the second step, we use a heuristic method which greedily adds non-tree edges. A similar approximation method is also used in (Mcdonald and Pereira, 2006) for acyclic dependency graphs. Step 1. Find MST. Following the multicommodity flow formulation of maximum spanning tree (MST) problem in (Magnanti and Wolsey, 1994), the ILP for MST is: ∑ max. yij · score(xi , xj ) (3) i,j ∑ s.t. ∑ i i,j (4) yij = |V |− 1 fiju − ∑ ∑ k u f0k u fjk = δju ,1 ≤ u, j ≤ |V |(5) = 1, k fiju fiju (6) 1 ≤ u ≤ |V | ≤ yij , 1 ≤ u, j ≤ |V |, ≥ 0, 1 ≤ u, j ≤ |V |, yij ∈ { 0, 1}, 0 ≤ i ≤ |V | (7) 0 ≤ i ≤ |V | (8) 0 ≤ i, j ≤ |V |. (9) In this formulation, yij is an edge indicator variable that (xi , xj ) is a spanning tree edge when yij = 1, (xi , xj ) is a non-tree edge wh"
D11-1123,D09-1019,0,0.0509771,"5.1 47.9 57.2 60.2 F 50.3 52.1 63.7 65.6 Table 7: Results on vertices extraction with 10 folder cross validation. We use two criterion: 1) the vertex is correct if it is exactly same as ground truth(“E”), 2) the vertex is correct if it overlaps with ground truth(“O”). 5 Related Work Opinion mining has recently received considerable attentions. Large amount of work has been done on sentimental classification in different levels and sentiment related information extraction. Researches on different types of sentences such as comparative sentences (Jindal and Liu, 2006) and conditional sentences (Narayanan et al., 2009) have also been proposed. Kobayashi et al. (2007) presented their work on extracting opinion units including: opinion holder, subject, aspect and evaluation. They used slots to represent evaluations, converted the task to two kinds of relation extraction tasks and proposed a machine learning-based method which used both contextual and statistical clues. Jindal and Liu (2006) studied the problem of identifying comparative sentences. They analyzed different types of comparative sentences and proposed learning approaches to identify them. Sentiment analysis of conditional sentences were studied b"
D11-1123,W02-1011,0,0.0140594,"Missing"
D11-1123,W06-1616,0,0.0622205,"the cases that a modifier connects to more than one opinion expression rarely occur comparing with those vertices which have a single parent. An explaination for this sparseness is that opinions in online reviews always concentrate in local context and have local semantic connections. 3.2.2 ILP Formulation Based on the property 3, we divide the inference algorithm into two steps: i) constructing G’s spanning tree (arborescence) with property 1 and 2; ii) finding additional non-tree edges as a post processing task. The first step is close to the works on ILP formulations of dependency parsing (Riedel and Clarke, 2006; Martins et al., 2009). In the second step, we use a heuristic method which greedily adds non-tree edges. A similar approximation method is also used in (Mcdonald and Pereira, 2006) for acyclic dependency graphs. Step 1. Find MST. Following the multicommodity flow formulation of maximum spanning tree (MST) problem in (Magnanti and Wolsey, 1994), the ILP for MST is: ∑ max. yij · score(xi , xj ) (3) i,j ∑ s.t. ∑ i i,j (4) yij = |V |− 1 fiju − ∑ ∑ k u f0k u fjk = δju ,1 ≤ u, j ≤ |V |(5) = 1, k fiju fiju (6) 1 ≤ u ≤ |V | ≤ yij , 1 ≤ u, j ≤ |V |, ≥ 0, 1 ≤ u, j ≤ |V |, yij ∈ { 0, 1}, 0 ≤ i ≤ |V | ("
D11-1123,W03-0404,0,0.0879412,"Missing"
D11-1123,C08-1101,0,0.0508096,"Missing"
D11-1123,P05-1017,0,0.0851631,"Missing"
D11-1123,D09-1159,1,0.419347,"Missing"
D17-1134,D15-1262,0,0.269261,"text spans without explicit discourse marker (i.e., connective, e.g., because or but ) between them are logically connected to one another (e.g., cause or contrast). It is considered to be a crucial step for discourse analysis and language generation and helpful to many downstream NLP applications, e.g., QA, MT, sentiment analysis, machine comprehension, etc. With the release of PDTB 2.0 (Prasad et al., 2008), lots of work has been done for discourse relation identification on natural (i.e., genuine) discourse data (Pitler et al., 2009; Lin et al., 2009; Wang et al., 2010; Zhou et al., 2010; Braud and Denis, 2015; Fisher and Simmons, 2015) with the use of traditional NLP linguistically informed features and machine learning algorithms. Recently, more and more researchers resorted to neural networks for implicit discourse recognition (Zhang et al., 2015; Chen et al., 2016; Liu et al., 2016b; Qin et al., 2016a; Liu and Li, 2016; Braud and Denis, 2016; Wu et al., 2016). Meanwhile, to alleviate the shortage of labeled data, researchers explored multi-task learning with the aid of unannotated data for implicit discourse recognition either in traditional machine learning framework (Collobert and Weston, 200"
D17-1134,D16-1020,0,0.10315,"Missing"
D17-1134,P16-1163,0,0.391635,"many downstream NLP applications, e.g., QA, MT, sentiment analysis, machine comprehension, etc. With the release of PDTB 2.0 (Prasad et al., 2008), lots of work has been done for discourse relation identification on natural (i.e., genuine) discourse data (Pitler et al., 2009; Lin et al., 2009; Wang et al., 2010; Zhou et al., 2010; Braud and Denis, 2015; Fisher and Simmons, 2015) with the use of traditional NLP linguistically informed features and machine learning algorithms. Recently, more and more researchers resorted to neural networks for implicit discourse recognition (Zhang et al., 2015; Chen et al., 2016; Liu et al., 2016b; Qin et al., 2016a; Liu and Li, 2016; Braud and Denis, 2016; Wu et al., 2016). Meanwhile, to alleviate the shortage of labeled data, researchers explored multi-task learning with the aid of unannotated data for implicit discourse recognition either in traditional machine learning framework (Collobert and Weston, 2008; Lan et al., 2013) or recently in neural network framework (Wu et al., 2016; Liu et al., 2016b). In this work, we present a novel multi-task attention-based neural network to address implicit discourse relationship representation and recognition. It performs tw"
D17-1134,Q15-1024,0,0.123199,"ognition on natural (i.e., genuine) discourse data with the use of traditional NLP techniques to extract linguistically informed features and traditional machine learning algorithms (Pitler et al., 2009; Lin et al., 2009; Wang et al., 2010; Braud and Denis, 2015; Fisher and Simmons, 2015). Later, to make a full use of unlabelled data, several studies performed multi-task or unsupervised learning methods (Lan et al., 2013; Braud and Denis, 2015; Fisher and Simmons, 2015; Rutherford and Xue, 2015). Recently, with the development of deep learning, researchers resorted to neural networks methods (Ji and Eisenstein, 2015; Zhang et al., 2015; Chen et al., 2016; Liu et al., 2016b; Qin et al., 2016a; Liu and Li, 2016; Braud and Denis, 2016; Wu et al., 2016). 5.2 Multi-task learning Multi-task learning framework adopts traditional machine learning with human-selected effective knowledge and the shared part is integrated into the cost function to prefer the main task learning. (Collobert and Weston, 2008) proposed a multitask neural network trained jointly on the relevant tasks using weight-sharing (sharing the word embeddings with tasks). (Liu et al., 2016a) proposed the multi-task neural network by modifying the"
D17-1134,P13-1047,1,0.763423,"isher and Simmons, 2015) with the use of traditional NLP linguistically informed features and machine learning algorithms. Recently, more and more researchers resorted to neural networks for implicit discourse recognition (Zhang et al., 2015; Chen et al., 2016; Liu et al., 2016b; Qin et al., 2016a; Liu and Li, 2016; Braud and Denis, 2016; Wu et al., 2016). Meanwhile, to alleviate the shortage of labeled data, researchers explored multi-task learning with the aid of unannotated data for implicit discourse recognition either in traditional machine learning framework (Collobert and Weston, 2008; Lan et al., 2013) or recently in neural network framework (Wu et al., 2016; Liu et al., 2016b). In this work, we present a novel multi-task attention-based neural network to address implicit discourse relationship representation and recognition. It performs two types of representation learning at the same time. An attention-based neural network conducts discourse relationship representation learning through interaction between two discourse arguments. Meanwhile, a multi-task learning framework leverages knowledge from auxiliary task to enhance the performance of main task. Furthermore, these two types of learn"
D17-1134,D09-1036,0,0.0378244,"Missing"
D17-1134,D16-1130,0,0.582461,"analysis, machine comprehension, etc. With the release of PDTB 2.0 (Prasad et al., 2008), lots of work has been done for discourse relation identification on natural (i.e., genuine) discourse data (Pitler et al., 2009; Lin et al., 2009; Wang et al., 2010; Zhou et al., 2010; Braud and Denis, 2015; Fisher and Simmons, 2015) with the use of traditional NLP linguistically informed features and machine learning algorithms. Recently, more and more researchers resorted to neural networks for implicit discourse recognition (Zhang et al., 2015; Chen et al., 2016; Liu et al., 2016b; Qin et al., 2016a; Liu and Li, 2016; Braud and Denis, 2016; Wu et al., 2016). Meanwhile, to alleviate the shortage of labeled data, researchers explored multi-task learning with the aid of unannotated data for implicit discourse recognition either in traditional machine learning framework (Collobert and Weston, 2008; Lan et al., 2013) or recently in neural network framework (Wu et al., 2016; Liu et al., 2016b). In this work, we present a novel multi-task attention-based neural network to address implicit discourse relationship representation and recognition. It performs two types of representation learning at the same time. An"
D17-1134,P09-1077,0,0.0836555,"tion (or rhetorical relation) identification is to recognize how two adjacent text spans without explicit discourse marker (i.e., connective, e.g., because or but ) between them are logically connected to one another (e.g., cause or contrast). It is considered to be a crucial step for discourse analysis and language generation and helpful to many downstream NLP applications, e.g., QA, MT, sentiment analysis, machine comprehension, etc. With the release of PDTB 2.0 (Prasad et al., 2008), lots of work has been done for discourse relation identification on natural (i.e., genuine) discourse data (Pitler et al., 2009; Lin et al., 2009; Wang et al., 2010; Zhou et al., 2010; Braud and Denis, 2015; Fisher and Simmons, 2015) with the use of traditional NLP linguistically informed features and machine learning algorithms. Recently, more and more researchers resorted to neural networks for implicit discourse recognition (Zhang et al., 2015; Chen et al., 2016; Liu et al., 2016b; Qin et al., 2016a; Liu and Li, 2016; Braud and Denis, 2016; Wu et al., 2016). Meanwhile, to alleviate the shortage of labeled data, researchers explored multi-task learning with the aid of unannotated data for implicit discourse recognit"
D17-1134,prasad-etal-2008-penn,0,0.0261204,"r proposed model outperforms the state-of-the-art systems on benchmark corpora. 1 Introduction The task of implicit discourse relation (or rhetorical relation) identification is to recognize how two adjacent text spans without explicit discourse marker (i.e., connective, e.g., because or but ) between them are logically connected to one another (e.g., cause or contrast). It is considered to be a crucial step for discourse analysis and language generation and helpful to many downstream NLP applications, e.g., QA, MT, sentiment analysis, machine comprehension, etc. With the release of PDTB 2.0 (Prasad et al., 2008), lots of work has been done for discourse relation identification on natural (i.e., genuine) discourse data (Pitler et al., 2009; Lin et al., 2009; Wang et al., 2010; Zhou et al., 2010; Braud and Denis, 2015; Fisher and Simmons, 2015) with the use of traditional NLP linguistically informed features and machine learning algorithms. Recently, more and more researchers resorted to neural networks for implicit discourse recognition (Zhang et al., 2015; Chen et al., 2016; Liu et al., 2016b; Qin et al., 2016a; Liu and Li, 2016; Braud and Denis, 2016; Wu et al., 2016). Meanwhile, to alleviate the sh"
D17-1134,C16-1180,0,0.627907,", QA, MT, sentiment analysis, machine comprehension, etc. With the release of PDTB 2.0 (Prasad et al., 2008), lots of work has been done for discourse relation identification on natural (i.e., genuine) discourse data (Pitler et al., 2009; Lin et al., 2009; Wang et al., 2010; Zhou et al., 2010; Braud and Denis, 2015; Fisher and Simmons, 2015) with the use of traditional NLP linguistically informed features and machine learning algorithms. Recently, more and more researchers resorted to neural networks for implicit discourse recognition (Zhang et al., 2015; Chen et al., 2016; Liu et al., 2016b; Qin et al., 2016a; Liu and Li, 2016; Braud and Denis, 2016; Wu et al., 2016). Meanwhile, to alleviate the shortage of labeled data, researchers explored multi-task learning with the aid of unannotated data for implicit discourse recognition either in traditional machine learning framework (Collobert and Weston, 2008; Lan et al., 2013) or recently in neural network framework (Wu et al., 2016; Liu et al., 2016b). In this work, we present a novel multi-task attention-based neural network to address implicit discourse relationship representation and recognition. It performs two types of representation learning at"
D17-1134,D16-1246,0,0.534199,", QA, MT, sentiment analysis, machine comprehension, etc. With the release of PDTB 2.0 (Prasad et al., 2008), lots of work has been done for discourse relation identification on natural (i.e., genuine) discourse data (Pitler et al., 2009; Lin et al., 2009; Wang et al., 2010; Zhou et al., 2010; Braud and Denis, 2015; Fisher and Simmons, 2015) with the use of traditional NLP linguistically informed features and machine learning algorithms. Recently, more and more researchers resorted to neural networks for implicit discourse recognition (Zhang et al., 2015; Chen et al., 2016; Liu et al., 2016b; Qin et al., 2016a; Liu and Li, 2016; Braud and Denis, 2016; Wu et al., 2016). Meanwhile, to alleviate the shortage of labeled data, researchers explored multi-task learning with the aid of unannotated data for implicit discourse recognition either in traditional machine learning framework (Collobert and Weston, 2008; Lan et al., 2013) or recently in neural network framework (Wu et al., 2016; Liu et al., 2016b). In this work, we present a novel multi-task attention-based neural network to address implicit discourse relationship representation and recognition. It performs two types of representation learning at"
D17-1134,N15-1081,0,0.107539,"ask. 5 5.1 Related Work Implicit Discourse With the release of PDTB 2.0, a number of studies performed discourse relation recognition on natural (i.e., genuine) discourse data with the use of traditional NLP techniques to extract linguistically informed features and traditional machine learning algorithms (Pitler et al., 2009; Lin et al., 2009; Wang et al., 2010; Braud and Denis, 2015; Fisher and Simmons, 2015). Later, to make a full use of unlabelled data, several studies performed multi-task or unsupervised learning methods (Lan et al., 2013; Braud and Denis, 2015; Fisher and Simmons, 2015; Rutherford and Xue, 2015). Recently, with the development of deep learning, researchers resorted to neural networks methods (Ji and Eisenstein, 2015; Zhang et al., 2015; Chen et al., 2016; Liu et al., 2016b; Qin et al., 2016a; Liu and Li, 2016; Braud and Denis, 2016; Wu et al., 2016). 5.2 Multi-task learning Multi-task learning framework adopts traditional machine learning with human-selected effective knowledge and the shared part is integrated into the cost function to prefer the main task learning. (Collobert and Weston, 2008) proposed a multitask neural network trained jointly on the relevant tasks using weight-sh"
D17-1134,K16-2007,0,0.175529,"Missing"
D17-1134,P16-1044,0,0.0113504,"Wu et al., 2016) use bilingually-constrained synthetic implicit data for implicit discourse relation recognition a multi-task neural network. (Liu et al., 2016b) propose a convolutional neural network embedded multi-task learning system to improve the performance of implicit discourse identification. 5.3 Deep learning with Attention Recently deep learning with attention has been widely adopted by NLP researchers. (Zhou et al., 2016) proposed an attention-based Bi-LSTM for relation classification. (Wang et al., 2016c) proposed an attention-based LSTM for aspect-level sentiment classification. (Tan et al., 2016) proposed a attentive LSTMs for Question Answer Matching. (Wang et al., 2016a) proposed an inner attention based RNN (add attention information before RNN hidden representation) for Answer Selection in QA. (Wang et al., 2016b) proposed multi-level attention CNNs for relation classification. (Yin et al., 2016) proposed an attentive convolutional neural network for QA. 6 Concluding Remarks We present a novel multi-task attention-based neural network model for implicit discourse relationship representation and identification. Our method captures both the discourse relationships through interactio"
D17-1134,P16-1122,0,0.0143279,"ed system which can effectively use synthetic data for implicit discourse relation recognition. (Wu et al., 2016) use bilingually-constrained synthetic implicit data for implicit discourse relation recognition a multi-task neural network. (Liu et al., 2016b) propose a convolutional neural network embedded multi-task learning system to improve the performance of implicit discourse identification. 5.3 Deep learning with Attention Recently deep learning with attention has been widely adopted by NLP researchers. (Zhou et al., 2016) proposed an attention-based Bi-LSTM for relation classification. (Wang et al., 2016c) proposed an attention-based LSTM for aspect-level sentiment classification. (Tan et al., 2016) proposed a attentive LSTMs for Question Answer Matching. (Wang et al., 2016a) proposed an inner attention based RNN (add attention information before RNN hidden representation) for Answer Selection in QA. (Wang et al., 2016b) proposed multi-level attention CNNs for relation classification. (Yin et al., 2016) proposed an attentive convolutional neural network for QA. 6 Concluding Remarks We present a novel multi-task attention-based neural network model for implicit discourse relationship represent"
D17-1134,K16-2004,1,0.887562,"Missing"
D17-1134,P16-1123,0,0.0214013,"Missing"
D17-1134,P10-1073,0,0.0615245,"ation is to recognize how two adjacent text spans without explicit discourse marker (i.e., connective, e.g., because or but ) between them are logically connected to one another (e.g., cause or contrast). It is considered to be a crucial step for discourse analysis and language generation and helpful to many downstream NLP applications, e.g., QA, MT, sentiment analysis, machine comprehension, etc. With the release of PDTB 2.0 (Prasad et al., 2008), lots of work has been done for discourse relation identification on natural (i.e., genuine) discourse data (Pitler et al., 2009; Lin et al., 2009; Wang et al., 2010; Zhou et al., 2010; Braud and Denis, 2015; Fisher and Simmons, 2015) with the use of traditional NLP linguistically informed features and machine learning algorithms. Recently, more and more researchers resorted to neural networks for implicit discourse recognition (Zhang et al., 2015; Chen et al., 2016; Liu et al., 2016b; Qin et al., 2016a; Liu and Li, 2016; Braud and Denis, 2016; Wu et al., 2016). Meanwhile, to alleviate the shortage of labeled data, researchers explored multi-task learning with the aid of unannotated data for implicit discourse recognition either in traditional machine lea"
D17-1134,D16-1058,0,0.0138575,"ed system which can effectively use synthetic data for implicit discourse relation recognition. (Wu et al., 2016) use bilingually-constrained synthetic implicit data for implicit discourse relation recognition a multi-task neural network. (Liu et al., 2016b) propose a convolutional neural network embedded multi-task learning system to improve the performance of implicit discourse identification. 5.3 Deep learning with Attention Recently deep learning with attention has been widely adopted by NLP researchers. (Zhou et al., 2016) proposed an attention-based Bi-LSTM for relation classification. (Wang et al., 2016c) proposed an attention-based LSTM for aspect-level sentiment classification. (Tan et al., 2016) proposed a attentive LSTMs for Question Answer Matching. (Wang et al., 2016a) proposed an inner attention based RNN (add attention information before RNN hidden representation) for Answer Selection in QA. (Wang et al., 2016b) proposed multi-level attention CNNs for relation classification. (Yin et al., 2016) proposed an attentive convolutional neural network for QA. 6 Concluding Remarks We present a novel multi-task attention-based neural network model for implicit discourse relationship represent"
D17-1134,D16-1253,0,0.70684,"Missing"
D17-1134,C16-1164,0,0.0159977,"Missing"
D17-1134,D15-1266,0,0.09134,"tion and helpful to many downstream NLP applications, e.g., QA, MT, sentiment analysis, machine comprehension, etc. With the release of PDTB 2.0 (Prasad et al., 2008), lots of work has been done for discourse relation identification on natural (i.e., genuine) discourse data (Pitler et al., 2009; Lin et al., 2009; Wang et al., 2010; Zhou et al., 2010; Braud and Denis, 2015; Fisher and Simmons, 2015) with the use of traditional NLP linguistically informed features and machine learning algorithms. Recently, more and more researchers resorted to neural networks for implicit discourse recognition (Zhang et al., 2015; Chen et al., 2016; Liu et al., 2016b; Qin et al., 2016a; Liu and Li, 2016; Braud and Denis, 2016; Wu et al., 2016). Meanwhile, to alleviate the shortage of labeled data, researchers explored multi-task learning with the aid of unannotated data for implicit discourse recognition either in traditional machine learning framework (Collobert and Weston, 2008; Lan et al., 2013) or recently in neural network framework (Wu et al., 2016; Liu et al., 2016b). In this work, we present a novel multi-task attention-based neural network to address implicit discourse relationship representation and recognit"
D17-1134,P16-2034,0,0.00957092,"k for text classification tasks. (Lan et al., 2013) present a multi-task learning based system which can effectively use synthetic data for implicit discourse relation recognition. (Wu et al., 2016) use bilingually-constrained synthetic implicit data for implicit discourse relation recognition a multi-task neural network. (Liu et al., 2016b) propose a convolutional neural network embedded multi-task learning system to improve the performance of implicit discourse identification. 5.3 Deep learning with Attention Recently deep learning with attention has been widely adopted by NLP researchers. (Zhou et al., 2016) proposed an attention-based Bi-LSTM for relation classification. (Wang et al., 2016c) proposed an attention-based LSTM for aspect-level sentiment classification. (Tan et al., 2016) proposed a attentive LSTMs for Question Answer Matching. (Wang et al., 2016a) proposed an inner attention based RNN (add attention information before RNN hidden representation) for Answer Selection in QA. (Wang et al., 2016b) proposed multi-level attention CNNs for relation classification. (Yin et al., 2016) proposed an attentive convolutional neural network for QA. 6 Concluding Remarks We present a novel multi-tas"
D17-1134,C10-2172,1,0.40213,"ze how two adjacent text spans without explicit discourse marker (i.e., connective, e.g., because or but ) between them are logically connected to one another (e.g., cause or contrast). It is considered to be a crucial step for discourse analysis and language generation and helpful to many downstream NLP applications, e.g., QA, MT, sentiment analysis, machine comprehension, etc. With the release of PDTB 2.0 (Prasad et al., 2008), lots of work has been done for discourse relation identification on natural (i.e., genuine) discourse data (Pitler et al., 2009; Lin et al., 2009; Wang et al., 2010; Zhou et al., 2010; Braud and Denis, 2015; Fisher and Simmons, 2015) with the use of traditional NLP linguistically informed features and machine learning algorithms. Recently, more and more researchers resorted to neural networks for implicit discourse recognition (Zhang et al., 2015; Chen et al., 2016; Liu et al., 2016b; Qin et al., 2016a; Liu and Li, 2016; Braud and Denis, 2016; Wu et al., 2016). Meanwhile, to alleviate the shortage of labeled data, researchers explored multi-task learning with the aid of unannotated data for implicit discourse recognition either in traditional machine learning framework (Co"
D18-1249,P11-1056,0,0.448582,"nt learning paradigm based on minimum risk training for the joint entity relation extraction task. 2. implementing a strong and simple neuralnetwork-based entity relation extraction model which carries the proposed MRT algorithm. 1 1 Our implementation is available at https:// github.com/changzhisun/entrel-joint-mrt. 3. achieving state-of-the-art results on two benchmark datasets (ACE05 and NYT). 2 Related Work In many pipelined entity relation extraction systems, one first learns an entity model, then learns a relation model based on entities generated by the entity model (Miwa et al., 2009; Chan and Roth, 2011; Lin et al., 2016). Such systems are often flexible to incorporate different data sources and different learning algorithms. However, they may also suffer from error propagation and data inefficiency. To tackle the problem, many recent studies try to develop joint extraction algorithms. Parameter sharing is a basic strategy in joint learning paradigms. For example, in (Miwa and Bansal, 2016), the entity model is a sentence-level RNN, and the relation model is a dependency tree path RNN which takes hidden states of the entity model as features (i.e., the shared parameters). Our basic extractio"
D18-1249,N10-1112,0,0.0245376,"al. (2017) cannot handle entities which appear in multiple relations). On the other side, Li and Ji (2014) develop a joint decoding algorithm based on beam search. Zhang et al. (2017) study a globally normalized joint model. They retain capacities of submodels, while their decoding algorithms are inexact. Here, we introduce MRT to the task, which is a more lightweight setting of joint learning. Minimum risk training is a learning framework which tries to handle models with arbitrary discrepancy metrics (i.e., losses of a model output w.r.t. the true answer) (Och, 2003; Smith and Eisner, 2006; Gimpel and Smith, 2010). It has been successfully applied to many NLP tasks. Some recent work include (He and Deng, 2012; Shen et al., 2016) which apply MRT to (neural) ma2257 chine translation, (Xu et al., 2016) which develops a shift-reduce CCG parser to directly optimize F1, and (Ayana et al., 2016) which uses a MRT-based model for summarization. We note that most previous applications of MRT focus on a single job, while the joint entity relation extraction consists of two sub-tasks. Investigating MRT in joint learning scenarios is the main topic of this work. Finally, the sampling algorithm of solving MRT is sim"
D18-1249,P82-1020,0,0.858045,"Missing"
D18-1249,P16-1087,0,0.0831244,"odel can share some input features or internal hidden states. It has an advantage that no additional constraint is required on the two submodels. But the connections among sub-models are still not fully explored due to independent submodel decoders. For example, to get signals from relation annotations, the entity model needs to wait for the relation model to update the shared parameters. To further utilize the interaction between decoders, some complex joint decoding algorithms (e.g., simultaneously decoding entities and relations in beam search) have been carefully studied (Li and Ji, 2014; Katiyar and Cardie, 2016; Zhang et al., 2017; Zheng et al., 2017). In this paradigm, it is important (and hard) to make a good balance between the exactness of the joint decoding algorithm and capacities of individual sub-models. In this work, we propose a joint minimum risk training (MRT) (Och, 2003; Smith and Eisner, 2006) method for the entity and relation extraction 2256 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2256–2265 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics task. It provides a lightweight way to stre"
D18-1249,P17-1085,0,0.448424,"ipeline model and the joint model. In the pipeline setting, the task is broken down into independently learned components (an entity model and a relation model). Despite its flexibility, the pipeline ignores interactions between the two models. For example, the entity model doesn’t look at relation annotations which are useful for identifying entities (e.g., if an ORG-AFF relation exists, the entity model can only assign ORG and AFF to its entities). The joint setting, on the other hand, extracts entities One simple joint learning paradigm is through sharing parameters (Miwa and Bansal, 2016; Katiyar and Cardie, 2017). Typically, instead of training two independent models, the entity and relation model can share some input features or internal hidden states. It has an advantage that no additional constraint is required on the two submodels. But the connections among sub-models are still not fully explored due to independent submodel decoders. For example, to get signals from relation annotations, the entity model needs to wait for the relation model to update the shared parameters. To further utilize the interaction between decoders, some complex joint decoding algorithms (e.g., simultaneously decoding ent"
D18-1249,P17-1138,0,0.0277482,"Missing"
D18-1249,D16-1127,0,0.0227046,"optimize F1, and (Ayana et al., 2016) which uses a MRT-based model for summarization. We note that most previous applications of MRT focus on a single job, while the joint entity relation extraction consists of two sub-tasks. Investigating MRT in joint learning scenarios is the main topic of this work. Finally, the sampling algorithm of solving MRT is similar to the policy gradient algorithm in reinforcement learning (RL) (Sutton and Barto, 1998). Some recent NLP applications which share the key idea of MRT but are described with RL language also show promising results (e.g., dialog systems (Li et al., 2016), machine translation (Nguyen et al., 2017)). The idea of learning loss functions from data is similar to inverse reinforcement learning (Abbeel and Ng, 2004; Ratliff et al., 2006). 3 tity and ∗ ∈ Te represents different entity types. For example, for a person (PER) entity “Patrick McDowell”, we assign B-PER to “Patrick” and L-PER to “McDowell”. Given an input sentence s, the entity model predicts the tags of words ˆt = tˆ1 , tˆ2 , . . . , tˆ|s |by learning from the true tags t = t1 , t2 , . . . , t|s |. We use a bidirectional long short term memory (bi-LSTM) network (Hochreiter and Schmidhube"
D18-1249,P14-1038,0,0.455952,"ty and relation model can share some input features or internal hidden states. It has an advantage that no additional constraint is required on the two submodels. But the connections among sub-models are still not fully explored due to independent submodel decoders. For example, to get signals from relation annotations, the entity model needs to wait for the relation model to update the shared parameters. To further utilize the interaction between decoders, some complex joint decoding algorithms (e.g., simultaneously decoding entities and relations in beam search) have been carefully studied (Li and Ji, 2014; Katiyar and Cardie, 2016; Zhang et al., 2017; Zheng et al., 2017). In this paradigm, it is important (and hard) to make a good balance between the exactness of the joint decoding algorithm and capacities of individual sub-models. In this work, we propose a joint minimum risk training (MRT) (Och, 2003; Smith and Eisner, 2006) method for the entity and relation extraction 2256 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2256–2265 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics task. It provides"
D18-1249,P16-1200,0,0.0429372,"based on minimum risk training for the joint entity relation extraction task. 2. implementing a strong and simple neuralnetwork-based entity relation extraction model which carries the proposed MRT algorithm. 1 1 Our implementation is available at https:// github.com/changzhisun/entrel-joint-mrt. 3. achieving state-of-the-art results on two benchmark datasets (ACE05 and NYT). 2 Related Work In many pipelined entity relation extraction systems, one first learns an entity model, then learns a relation model based on entities generated by the entity model (Miwa et al., 2009; Chan and Roth, 2011; Lin et al., 2016). Such systems are often flexible to incorporate different data sources and different learning algorithms. However, they may also suffer from error propagation and data inefficiency. To tackle the problem, many recent studies try to develop joint extraction algorithms. Parameter sharing is a basic strategy in joint learning paradigms. For example, in (Miwa and Bansal, 2016), the entity model is a sentence-level RNN, and the relation model is a dependency tree path RNN which takes hidden states of the entity model as features (i.e., the shared parameters). Our basic extraction model is similar"
D18-1249,P16-1105,0,0.341187,"extraction task, the pipeline model and the joint model. In the pipeline setting, the task is broken down into independently learned components (an entity model and a relation model). Despite its flexibility, the pipeline ignores interactions between the two models. For example, the entity model doesn’t look at relation annotations which are useful for identifying entities (e.g., if an ORG-AFF relation exists, the entity model can only assign ORG and AFF to its entities). The joint setting, on the other hand, extracts entities One simple joint learning paradigm is through sharing parameters (Miwa and Bansal, 2016; Katiyar and Cardie, 2017). Typically, instead of training two independent models, the entity and relation model can share some input features or internal hidden states. It has an advantage that no additional constraint is required on the two submodels. But the connections among sub-models are still not fully explored due to independent submodel decoders. For example, to get signals from relation annotations, the entity model needs to wait for the relation model to update the shared parameters. To further utilize the interaction between decoders, some complex joint decoding algorithms (e.g.,"
D18-1249,D09-1013,0,0.394332,"proposing a new joint learning paradigm based on minimum risk training for the joint entity relation extraction task. 2. implementing a strong and simple neuralnetwork-based entity relation extraction model which carries the proposed MRT algorithm. 1 1 Our implementation is available at https:// github.com/changzhisun/entrel-joint-mrt. 3. achieving state-of-the-art results on two benchmark datasets (ACE05 and NYT). 2 Related Work In many pipelined entity relation extraction systems, one first learns an entity model, then learns a relation model based on entities generated by the entity model (Miwa et al., 2009; Chan and Roth, 2011; Lin et al., 2016). Such systems are often flexible to incorporate different data sources and different learning algorithms. However, they may also suffer from error propagation and data inefficiency. To tackle the problem, many recent studies try to develop joint extraction algorithms. Parameter sharing is a basic strategy in joint learning paradigms. For example, in (Miwa and Bansal, 2016), the entity model is a sentence-level RNN, and the relation model is a dependency tree path RNN which takes hidden states of the entity model as features (i.e., the shared parameters)"
D18-1249,P16-1218,0,0.032729,"build fe2 for e2 with another CNN. where θR = {θe1 , θe2 , θmiddle , W1 , W2 } contains parameters of the relation model (shared parameters with the entity model are omitted). Given an input sentence s, the training objective is to minimize For context features of the entity pair (e1 , e2 ), we build three feature vectors by looking at words between e1 and e2 (fmiddle ), words on the left of the pair (fleft ) and words on the right of the pair (fright ). For fmiddle , we run a CNN on words between e1 and e2 like the case of fe1 , fe2 . For fleft and fright , we use the “LSTM-Minus” method as (Wang and Chang, 2016; Zhang et al., 2017). Assume that the left context of (e1 , e2 ) is from sentence position 0 to i, then fleft = hi ⊕ (h0 − hi+1 ). Similarly, if the right context of (e1 , e2 ) is from j to |s |− 1, then fright = (h|s|−1 − hj−1 ) ⊕ hj . We also use a onehot feature fdist to describe the distance between e1 and e2 in the sentence. Finally, fe1 , fe2 , fmiddle , fleft , fright and fdist are concatenated to a single vector fe1 ,e2 . To get the Prel (ˆl|s, e1 , e2 ; θR ) = Softmax(W2 · ReLU(W1 · fe1 ,e2 )), (2) Lrel (θR ) = − ∑ log Prel (ˆl = l|s, e1 , e2 ; θR ) , ˆ E| ˆ − 1) |E|(| e1 ,e2 ∈"
D18-1249,D17-1153,0,0.0433919,"Missing"
D18-1249,P03-1021,0,0.290309,"annotations, the entity model needs to wait for the relation model to update the shared parameters. To further utilize the interaction between decoders, some complex joint decoding algorithms (e.g., simultaneously decoding entities and relations in beam search) have been carefully studied (Li and Ji, 2014; Katiyar and Cardie, 2016; Zhang et al., 2017; Zheng et al., 2017). In this paradigm, it is important (and hard) to make a good balance between the exactness of the joint decoding algorithm and capacities of individual sub-models. In this work, we propose a joint minimum risk training (MRT) (Och, 2003; Smith and Eisner, 2006) method for the entity and relation extraction 2256 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2256–2265 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics task. It provides a lightweight way to strengthen connections between the entity model and the relation model, and keeps their capacities unaffected. Given an input x and a loss function ∆(ˆ y, y) (measuring the difference between model output ˆ and the true y), MRT seeks a posterior P (ˆ y y|x) to minimize the expecte"
D18-1249,N16-1025,0,0.0519631,"Missing"
D18-1249,D17-1182,0,0.345808,"Missing"
D18-1249,P17-1113,0,0.329403,"hidden states. It has an advantage that no additional constraint is required on the two submodels. But the connections among sub-models are still not fully explored due to independent submodel decoders. For example, to get signals from relation annotations, the entity model needs to wait for the relation model to update the shared parameters. To further utilize the interaction between decoders, some complex joint decoding algorithms (e.g., simultaneously decoding entities and relations in beam search) have been carefully studied (Li and Ji, 2014; Katiyar and Cardie, 2016; Zhang et al., 2017; Zheng et al., 2017). In this paradigm, it is important (and hard) to make a good balance between the exactness of the joint decoding algorithm and capacities of individual sub-models. In this work, we propose a joint minimum risk training (MRT) (Och, 2003; Smith and Eisner, 2006) method for the entity and relation extraction 2256 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2256–2265 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics task. It provides a lightweight way to strengthen connections between the entity mod"
D18-1249,P16-1159,0,0.370465,"decoding algorithm based on beam search. Zhang et al. (2017) study a globally normalized joint model. They retain capacities of submodels, while their decoding algorithms are inexact. Here, we introduce MRT to the task, which is a more lightweight setting of joint learning. Minimum risk training is a learning framework which tries to handle models with arbitrary discrepancy metrics (i.e., losses of a model output w.r.t. the true answer) (Och, 2003; Smith and Eisner, 2006; Gimpel and Smith, 2010). It has been successfully applied to many NLP tasks. Some recent work include (He and Deng, 2012; Shen et al., 2016) which apply MRT to (neural) ma2257 chine translation, (Xu et al., 2016) which develops a shift-reduce CCG parser to directly optimize F1, and (Ayana et al., 2016) which uses a MRT-based model for summarization. We note that most previous applications of MRT focus on a single job, while the joint entity relation extraction consists of two sub-tasks. Investigating MRT in joint learning scenarios is the main topic of this work. Finally, the sampling algorithm of solving MRT is similar to the policy gradient algorithm in reinforcement learning (RL) (Sutton and Barto, 1998). Some recent NLP applic"
D18-1249,P06-2101,0,0.284037,", the entity model needs to wait for the relation model to update the shared parameters. To further utilize the interaction between decoders, some complex joint decoding algorithms (e.g., simultaneously decoding entities and relations in beam search) have been carefully studied (Li and Ji, 2014; Katiyar and Cardie, 2016; Zhang et al., 2017; Zheng et al., 2017). In this paradigm, it is important (and hard) to make a good balance between the exactness of the joint decoding algorithm and capacities of individual sub-models. In this work, we propose a joint minimum risk training (MRT) (Och, 2003; Smith and Eisner, 2006) method for the entity and relation extraction 2256 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2256–2265 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics task. It provides a lightweight way to strengthen connections between the entity model and the relation model, and keeps their capacities unaffected. Given an input x and a loss function ∆(ˆ y, y) (measuring the difference between model output ˆ and the true y), MRT seeks a posterior P (ˆ y y|x) to minimize the expected loss Eyˆ ∼P (ˆy|x) ∆(ˆ"
D19-1635,W19-3621,0,0.376517,"ociation graph. It turns out that both kinds of graphs exhibit “small world” properties, but they have different hub words and connecting mechanisms. We then run the same stereotype propagation algorithm and compare the derived gender bias scores. Third, as a case study, we use word-association-based scores as a benchmark to see the effectiveness of existing debias algorithms on word embeddings (Bolukbasi et al., 2016; Zhao et al., 2018). The results suggest that it is hard for both de-bias algorithms to remove gender stereotypes in word embeddings completely, which matches the conclusion in (Gonen and Goldberg, 2019). 2 Word Association Test Word association test is a simple and sometimes entertaining game in which participants are asked to respond with the first several words that come out in their mind (the response) after being presented with a word (the cue)2 . Table 1 lists some examples of the test. It is considered to be one of the most straightforward approaches for gaining insight into our semantic knowledge (Steyvers and Tenenbaum, 2005), and is also a common aphttps://en.wikipedia.org/wiki/Word_ Association R1 R2 R3 way extra i come than son mind path plus you go then daughter brain via special"
D19-1635,D16-1057,0,0.0399865,"Missing"
D19-1635,D14-1162,0,0.0851124,"e composition of these hub words. It turns out that most genderspecific words (except for he, she, daughter, aunt) in L are contained in the hub words of the word association graph, while none of them lie within those of the embedding-based graphs. Therefore, there is no wonder that L in the word association graph could distribute its gender information to other words in the graph more efficiently, leading to better performance in detecting gender bias of words. Co-occurrence Due to the fact that most commonly used word embedding models are trained on word co-occurrence (Mikolov et al., 2013; Pennington et al., 2014), we observe lots of connections with such type in bias paths on word embedding graphs, which also widely exists in those on the word association graph, such as birth-day and even-more. To summarize, frequency effects, neighborhood effects, and more concentrated hubs may work together to help spread gender stereotypes. It shows the value of introducing word association test into gender stereotype research. 5.5 Case Study: Do De-bias Methods Really Remove Gender Bias? Some works have proposed approaches to reduce gender stereotypes in word embeddings. They can mainly be divided into two categor"
D19-1635,N10-1119,0,0.029935,"e Influence of L Pearson’s r Under Various α 0.58 census data human judgments census 0.56 0.56 0.58 0.60 0.62 0.64 0.66 0.655 0.650 0.52 0.645 0.68 0.50 0.640 human judgments 0.54 0.660 0.54 census data human 0.665 Pearson’s r calculate the confidence intervals of correlations between our results and gender stereotypes in the real world in Figure 3. The results show that the words’ bias scores are stable with respect to reasonable settings of L. 5.3 Variants of Stereotype Propagation There are many variants of the random walk method in previous literature (Zhou et al., 2003; Zhu et al., 2003; Velikovich et al., 2010; Vicente et al., 2017). We experiment with another approach in (Zhu et al., 2003), to test whether bias scores of words are insensitive towards random walk algorithms. In this method, we consider only local consistency (without the second term of Equation 1). As a result, we find high consistency between their bias scores (Pearson’s r = 0.789, p = 0.0). We also examine the influence of hyper parameter α. Here, we conduct similar correlation analysis in Section 5.1 under various α (Figure 4). We find that stereotype propagation has a stable performance regarding different α, while a proper α ("
D19-1635,D18-1521,0,0.310971,"vanilla wordembedding-based scores. Second, as embeddings could also be applied for building graphs, we investigate such graphs and look into their differences with the word association graph. It turns out that both kinds of graphs exhibit “small world” properties, but they have different hub words and connecting mechanisms. We then run the same stereotype propagation algorithm and compare the derived gender bias scores. Third, as a case study, we use word-association-based scores as a benchmark to see the effectiveness of existing debias algorithms on word embeddings (Bolukbasi et al., 2016; Zhao et al., 2018). The results suggest that it is hard for both de-bias algorithms to remove gender stereotypes in word embeddings completely, which matches the conclusion in (Gonen and Goldberg, 2019). 2 Word Association Test Word association test is a simple and sometimes entertaining game in which participants are asked to respond with the first several words that come out in their mind (the response) after being presented with a word (the cue)2 . Table 1 lists some examples of the test. It is considered to be one of the most straightforward approaches for gaining insight into our semantic knowledge (Steyve"
E17-1097,N15-1146,0,0.0240204,". We aim to make all algorithms simple, fast and scalable for large-scale corpus. Our system is tested on Amazon review data which contains 15 different domains and 33 million reviews. The output database contains 72.5 million pairs of opinion relations. Extensive experiments have been conducted on various aspects of the algorithm, and the performances of the proposed unsupervised models are even competitive with previous supervised models. 2 Related Works Opinion relation extraction is an important task for fine-grained sentiment analysis. If human annotations are provided (e.g. MPQA corpus (Deng and Wiebe, 2015)), we could formulate the task into a supervised relation extraction problem as (Kobayashi et al., 2007; Johansson and Moschitti, 2013). Two types of models have been applied: pipeline models which first extract candidates of opinion expressions and targets then identify correct relations (Wu et al., 2009; Li et al., 2010; Yang and Cardie, 2012), and joint models which extract opinion expressions, targets and relations using a unified joint model (Yang and Cardie, 2013; Yang and Cardie, 2014). One consideration of applying supervised methods is their dependencies on the domains and human annot"
E17-1097,P15-1061,0,0.0341312,"orithms (Snow et al., 2004; Mintz et al., 2009). They use relations from WordNet or knowledge bases as distant supervision. Since we don’t have similar resources for opinion relation extraction, we use patterns to generate relations. Neural network classifiers are popular for relation extraction recently. Many of them focus on fully supervised settings, recurrent neural networks (RNN) and convolutional neural networks (CNN) (Vu et al., 2016; Zeng et al., 2015; Xu et al., 2015a; Xu et al., 2015b; Zhang and Wang, 2015), sequence models and tree models 1034 are investigated (Li et al., 2015; dos Santos et al., 2015). One similar network structure to our model is proposed in (Miwa and Bansal, 2016). They jointly extract entities and relations using two LSTM models. Another recent work (Jebbara and Cimiano, 2016) uses stacked RNNs and CNNs for aspect and opinion detection. Different from models there, we will learn representations for different lexical and syntactic features explicitly. Our formulation follows the features in traditional relation classifiers, which helps to interpret the learned vectors. A closely related task is aspect-based opinion mining (Zhao et al., 2010; Yu et al., 2011; Wang et al.,"
E17-1097,C08-1031,0,0.0342543,"ception”, “unit”), and (“touch your heart”, “The Passion of The Christ”). Extracting opinion bearing relations is usually the first step towards fine-gained analysis of opinion in texts, and plays an important role in other sentiment related applications (e.g., sentiment summarization). The goal of this paper is to extract opinion relations from open domain largescale opinion bearing texts. Previous works on fine-grained opinion information extraction have achieved notable success on many aspects: various domains were examined (Pontiki et al., 2015), different types of relations were studied (Ganapathibhotla and Liu, 2008; Narayanan et al., 2009; Wu et al., 2011), and both supervised and unsupervised (patternbased) algorithms were applied. But we have observed some difficulties when trying to use existing methods. The pattern based methods (both lexical patterns and syntactic patterns) are simple, fast, and scalable on large-scale datasets. However, the robustness of patterns is usually questionable in practice. For example, syntactic patterns are sensitive to errors in parse trees, which are common in user generated contents. Lexical patterns either have limited coverage (e.g., fixed set of patterns (Riloff a"
E17-1097,J13-3002,0,0.0219279,"hich contains 15 different domains and 33 million reviews. The output database contains 72.5 million pairs of opinion relations. Extensive experiments have been conducted on various aspects of the algorithm, and the performances of the proposed unsupervised models are even competitive with previous supervised models. 2 Related Works Opinion relation extraction is an important task for fine-grained sentiment analysis. If human annotations are provided (e.g. MPQA corpus (Deng and Wiebe, 2015)), we could formulate the task into a supervised relation extraction problem as (Kobayashi et al., 2007; Johansson and Moschitti, 2013). Two types of models have been applied: pipeline models which first extract candidates of opinion expressions and targets then identify correct relations (Wu et al., 2009; Li et al., 2010; Yang and Cardie, 2012), and joint models which extract opinion expressions, targets and relations using a unified joint model (Yang and Cardie, 2013; Yang and Cardie, 2014). One consideration of applying supervised methods is their dependencies on the domains and human annotations. Semi-supervised and unsupervised models are also applied for extracting opinion relations. Approaches include rule-based bootst"
E17-1097,klinger-cimiano-2014-usage,0,0.156827,"figurations We extract opinion relations on a subset of Amazon product review corpus provided by (McAuley et al., 2015), which contains 15 domains and 33 million reviews. The statistics of extracted relations are in Table 2. For quantitative evaluation, we select four domains (Cell Phones, Movie and TV, Food, Pet Supplies) for detailed analyses. We manually label all correct opinion relations in 1000 sentences, and select 200 sentences as the development set, the rest 800 as the test set 3 . Furthermore, to compare with previous supervised methods, we also conduct experiments on USAGE corpus (Klinger and Cimiano, 2014) which annotates 4481 opinion relations for 8 products. We use NLTK (Bird et al., 2009) for sentence splitting and word segmentation, Stanford parser 4 for getting POS tags, phrase chunks and dependency trees, and scikit-learn toolkit (Pedregosa et al., 2011) and TensorFlow 5 for machine learning algorithms. The general purpose opinion lexicon is from (Wilson et al., 2005). 4.2 Main Results Table 4 shows results on four domains. The methods for comparison are: • Adjacent is a simple baseline system from (Hu and Liu, 2004). It first identifies words in the general purpose opinion lexicon, then"
E17-1097,D07-1114,0,0.0320979,"on Amazon review data which contains 15 different domains and 33 million reviews. The output database contains 72.5 million pairs of opinion relations. Extensive experiments have been conducted on various aspects of the algorithm, and the performances of the proposed unsupervised models are even competitive with previous supervised models. 2 Related Works Opinion relation extraction is an important task for fine-grained sentiment analysis. If human annotations are provided (e.g. MPQA corpus (Deng and Wiebe, 2015)), we could formulate the task into a supervised relation extraction problem as (Kobayashi et al., 2007; Johansson and Moschitti, 2013). Two types of models have been applied: pipeline models which first extract candidates of opinion expressions and targets then identify correct relations (Wu et al., 2009; Li et al., 2010; Yang and Cardie, 2012), and joint models which extract opinion expressions, targets and relations using a unified joint model (Yang and Cardie, 2013; Yang and Cardie, 2014). One consideration of applying supervised methods is their dependencies on the domains and human annotations. Semi-supervised and unsupervised models are also applied for extracting opinion relations. Appr"
E17-1097,C10-1074,0,0.0363762,"thm, and the performances of the proposed unsupervised models are even competitive with previous supervised models. 2 Related Works Opinion relation extraction is an important task for fine-grained sentiment analysis. If human annotations are provided (e.g. MPQA corpus (Deng and Wiebe, 2015)), we could formulate the task into a supervised relation extraction problem as (Kobayashi et al., 2007; Johansson and Moschitti, 2013). Two types of models have been applied: pipeline models which first extract candidates of opinion expressions and targets then identify correct relations (Wu et al., 2009; Li et al., 2010; Yang and Cardie, 2012), and joint models which extract opinion expressions, targets and relations using a unified joint model (Yang and Cardie, 2013; Yang and Cardie, 2014). One consideration of applying supervised methods is their dependencies on the domains and human annotations. Semi-supervised and unsupervised models are also applied for extracting opinion relations. Approaches include rule-based bootstrapping (Qiu et al., 2011), graph propagation algorithms (Xu et al., 2013; Liu et al., 2014; Brody and Elhadad, 2010), integer programming (Lu et al., 2011), and probabilistic topic models"
E17-1097,D15-1278,0,0.0212734,"tantly supervised algorithms (Snow et al., 2004; Mintz et al., 2009). They use relations from WordNet or knowledge bases as distant supervision. Since we don’t have similar resources for opinion relation extraction, we use patterns to generate relations. Neural network classifiers are popular for relation extraction recently. Many of them focus on fully supervised settings, recurrent neural networks (RNN) and convolutional neural networks (CNN) (Vu et al., 2016; Zeng et al., 2015; Xu et al., 2015a; Xu et al., 2015b; Zhang and Wang, 2015), sequence models and tree models 1034 are investigated (Li et al., 2015; dos Santos et al., 2015). One similar network structure to our model is proposed in (Miwa and Bansal, 2016). They jointly extract entities and relations using two LSTM models. Another recent work (Jebbara and Cimiano, 2016) uses stacked RNNs and CNNs for aspect and opinion detection. Different from models there, we will learn representations for different lexical and syntactic features explicitly. Our formulation follows the features in traditional relation classifiers, which helps to interpret the learned vectors. A closely related task is aspect-based opinion mining (Zhao et al., 2010; Yu"
E17-1097,S14-2004,0,0.183539,"Missing"
E17-1097,P14-1030,0,0.0171181,"t candidates of opinion expressions and targets then identify correct relations (Wu et al., 2009; Li et al., 2010; Yang and Cardie, 2012), and joint models which extract opinion expressions, targets and relations using a unified joint model (Yang and Cardie, 2013; Yang and Cardie, 2014). One consideration of applying supervised methods is their dependencies on the domains and human annotations. Semi-supervised and unsupervised models are also applied for extracting opinion relations. Approaches include rule-based bootstrapping (Qiu et al., 2011), graph propagation algorithms (Xu et al., 2013; Liu et al., 2014; Brody and Elhadad, 2010), integer programming (Lu et al., 2011), and probabilistic topic models (Titov and McDonald, 2008; Mukherjee and Liu, 2012). Our model is inspired by previous distantly supervised algorithms (Snow et al., 2004; Mintz et al., 2009). They use relations from WordNet or knowledge bases as distant supervision. Since we don’t have similar resources for opinion relation extraction, we use patterns to generate relations. Neural network classifiers are popular for relation extraction recently. Many of them focus on fully supervised settings, recurrent neural networks (RNN) and"
E17-1097,S15-2082,0,0.0608991,"Missing"
E17-1097,P09-1113,0,0.724846,"rdie, 2013; Yang and Cardie, 2014). One consideration of applying supervised methods is their dependencies on the domains and human annotations. Semi-supervised and unsupervised models are also applied for extracting opinion relations. Approaches include rule-based bootstrapping (Qiu et al., 2011), graph propagation algorithms (Xu et al., 2013; Liu et al., 2014; Brody and Elhadad, 2010), integer programming (Lu et al., 2011), and probabilistic topic models (Titov and McDonald, 2008; Mukherjee and Liu, 2012). Our model is inspired by previous distantly supervised algorithms (Snow et al., 2004; Mintz et al., 2009). They use relations from WordNet or knowledge bases as distant supervision. Since we don’t have similar resources for opinion relation extraction, we use patterns to generate relations. Neural network classifiers are popular for relation extraction recently. Many of them focus on fully supervised settings, recurrent neural networks (RNN) and convolutional neural networks (CNN) (Vu et al., 2016; Zeng et al., 2015; Xu et al., 2015a; Xu et al., 2015b; Zhang and Wang, 2015), sequence models and tree models 1034 are investigated (Li et al., 2015; dos Santos et al., 2015). One similar network struc"
E17-1097,P16-1105,0,0.0208472,"or knowledge bases as distant supervision. Since we don’t have similar resources for opinion relation extraction, we use patterns to generate relations. Neural network classifiers are popular for relation extraction recently. Many of them focus on fully supervised settings, recurrent neural networks (RNN) and convolutional neural networks (CNN) (Vu et al., 2016; Zeng et al., 2015; Xu et al., 2015a; Xu et al., 2015b; Zhang and Wang, 2015), sequence models and tree models 1034 are investigated (Li et al., 2015; dos Santos et al., 2015). One similar network structure to our model is proposed in (Miwa and Bansal, 2016). They jointly extract entities and relations using two LSTM models. Another recent work (Jebbara and Cimiano, 2016) uses stacked RNNs and CNNs for aspect and opinion detection. Different from models there, we will learn representations for different lexical and syntactic features explicitly. Our formulation follows the features in traditional relation classifiers, which helps to interpret the learned vectors. A closely related task is aspect-based opinion mining (Zhao et al., 2010; Yu et al., 2011; Wang et al., 2015). Instead of locating the opinion expressions, aspect-based opinion mining di"
E17-1097,P12-1036,0,0.0202368,"d joint models which extract opinion expressions, targets and relations using a unified joint model (Yang and Cardie, 2013; Yang and Cardie, 2014). One consideration of applying supervised methods is their dependencies on the domains and human annotations. Semi-supervised and unsupervised models are also applied for extracting opinion relations. Approaches include rule-based bootstrapping (Qiu et al., 2011), graph propagation algorithms (Xu et al., 2013; Liu et al., 2014; Brody and Elhadad, 2010), integer programming (Lu et al., 2011), and probabilistic topic models (Titov and McDonald, 2008; Mukherjee and Liu, 2012). Our model is inspired by previous distantly supervised algorithms (Snow et al., 2004; Mintz et al., 2009). They use relations from WordNet or knowledge bases as distant supervision. Since we don’t have similar resources for opinion relation extraction, we use patterns to generate relations. Neural network classifiers are popular for relation extraction recently. Many of them focus on fully supervised settings, recurrent neural networks (RNN) and convolutional neural networks (CNN) (Vu et al., 2016; Zeng et al., 2015; Xu et al., 2015a; Xu et al., 2015b; Zhang and Wang, 2015), sequence models"
E17-1097,D09-1019,0,0.0283461,"your heart”, “The Passion of The Christ”). Extracting opinion bearing relations is usually the first step towards fine-gained analysis of opinion in texts, and plays an important role in other sentiment related applications (e.g., sentiment summarization). The goal of this paper is to extract opinion relations from open domain largescale opinion bearing texts. Previous works on fine-grained opinion information extraction have achieved notable success on many aspects: various domains were examined (Pontiki et al., 2015), different types of relations were studied (Ganapathibhotla and Liu, 2008; Narayanan et al., 2009; Wu et al., 2011), and both supervised and unsupervised (patternbased) algorithms were applied. But we have observed some difficulties when trying to use existing methods. The pattern based methods (both lexical patterns and syntactic patterns) are simple, fast, and scalable on large-scale datasets. However, the robustness of patterns is usually questionable in practice. For example, syntactic patterns are sensitive to errors in parse trees, which are common in user generated contents. Lexical patterns either have limited coverage (e.g., fixed set of patterns (Riloff and Wiebe, 2003)), or har"
E17-1097,J11-1002,0,0.140584,"Missing"
E17-1097,W03-1014,0,0.19536,"iu, 2008; Narayanan et al., 2009; Wu et al., 2011), and both supervised and unsupervised (patternbased) algorithms were applied. But we have observed some difficulties when trying to use existing methods. The pattern based methods (both lexical patterns and syntactic patterns) are simple, fast, and scalable on large-scale datasets. However, the robustness of patterns is usually questionable in practice. For example, syntactic patterns are sensitive to errors in parse trees, which are common in user generated contents. Lexical patterns either have limited coverage (e.g., fixed set of patterns (Riloff and Wiebe, 2003)), or hard-tocontrol noise (e.g., bootstrapping approaches (Qiu et al., 2011)). On the other hand, supervised models can achieve better performances than patterns on manually labeled datasets, but it is often difficult to obtain large number of annotations for the relation extraction task, and the trained models are 1033 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 1033–1043, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics also limited to specified domains. Thus, we s"
E17-1097,P08-1036,0,0.0501098,"Yang and Cardie, 2012), and joint models which extract opinion expressions, targets and relations using a unified joint model (Yang and Cardie, 2013; Yang and Cardie, 2014). One consideration of applying supervised methods is their dependencies on the domains and human annotations. Semi-supervised and unsupervised models are also applied for extracting opinion relations. Approaches include rule-based bootstrapping (Qiu et al., 2011), graph propagation algorithms (Xu et al., 2013; Liu et al., 2014; Brody and Elhadad, 2010), integer programming (Lu et al., 2011), and probabilistic topic models (Titov and McDonald, 2008; Mukherjee and Liu, 2012). Our model is inspired by previous distantly supervised algorithms (Snow et al., 2004; Mintz et al., 2009). They use relations from WordNet or knowledge bases as distant supervision. Since we don’t have similar resources for opinion relation extraction, we use patterns to generate relations. Neural network classifiers are popular for relation extraction recently. Many of them focus on fully supervised settings, recurrent neural networks (RNN) and convolutional neural networks (CNN) (Vu et al., 2016; Zeng et al., 2015; Xu et al., 2015a; Xu et al., 2015b; Zhang and Wan"
E17-1097,N16-1065,0,0.0589031,"Missing"
E17-1097,P15-1060,0,0.041169,"Missing"
E17-1097,H05-1044,0,0.0959387,"nion relations in 1000 sentences, and select 200 sentences as the development set, the rest 800 as the test set 3 . Furthermore, to compare with previous supervised methods, we also conduct experiments on USAGE corpus (Klinger and Cimiano, 2014) which annotates 4481 opinion relations for 8 products. We use NLTK (Bird et al., 2009) for sentence splitting and word segmentation, Stanford parser 4 for getting POS tags, phrase chunks and dependency trees, and scikit-learn toolkit (Pedregosa et al., 2011) and TensorFlow 5 for machine learning algorithms. The general purpose opinion lexicon is from (Wilson et al., 2005). 4.2 Main Results Table 4 shows results on four domains. The methods for comparison are: • Adjacent is a simple baseline system from (Hu and Liu, 2004). It first identifies words in the general purpose opinion lexicon, then finds the nearest noun or verb phrase to them as their opinion targets. 3 https://github.com/AntNLP/OpinionRelationCorpus http://nlp.stanford.edu/software/lex-parser.shtml 5 https://www.tensorflow.org/ Domain #Reviews #Sents #Relations Cell Phones Movie and TV Food Pet Supplies Automotive Digital Music Beauty Toys and Games Instruments Office Products Patio Baby Clothing S"
E17-1097,D09-1159,1,0.780249,"cts of the algorithm, and the performances of the proposed unsupervised models are even competitive with previous supervised models. 2 Related Works Opinion relation extraction is an important task for fine-grained sentiment analysis. If human annotations are provided (e.g. MPQA corpus (Deng and Wiebe, 2015)), we could formulate the task into a supervised relation extraction problem as (Kobayashi et al., 2007; Johansson and Moschitti, 2013). Two types of models have been applied: pipeline models which first extract candidates of opinion expressions and targets then identify correct relations (Wu et al., 2009; Li et al., 2010; Yang and Cardie, 2012), and joint models which extract opinion expressions, targets and relations using a unified joint model (Yang and Cardie, 2013; Yang and Cardie, 2014). One consideration of applying supervised methods is their dependencies on the domains and human annotations. Semi-supervised and unsupervised models are also applied for extracting opinion relations. Approaches include rule-based bootstrapping (Qiu et al., 2011), graph propagation algorithms (Xu et al., 2013; Liu et al., 2014; Brody and Elhadad, 2010), integer programming (Lu et al., 2011), and probabili"
E17-1097,D11-1123,1,0.742611,"n of The Christ”). Extracting opinion bearing relations is usually the first step towards fine-gained analysis of opinion in texts, and plays an important role in other sentiment related applications (e.g., sentiment summarization). The goal of this paper is to extract opinion relations from open domain largescale opinion bearing texts. Previous works on fine-grained opinion information extraction have achieved notable success on many aspects: various domains were examined (Pontiki et al., 2015), different types of relations were studied (Ganapathibhotla and Liu, 2008; Narayanan et al., 2009; Wu et al., 2011), and both supervised and unsupervised (patternbased) algorithms were applied. But we have observed some difficulties when trying to use existing methods. The pattern based methods (both lexical patterns and syntactic patterns) are simple, fast, and scalable on large-scale datasets. However, the robustness of patterns is usually questionable in practice. For example, syntactic patterns are sensitive to errors in parse trees, which are common in user generated contents. Lexical patterns either have limited coverage (e.g., fixed set of patterns (Riloff and Wiebe, 2003)), or hard-tocontrol noise"
E17-1097,P13-1173,0,0.0194606,"hich first extract candidates of opinion expressions and targets then identify correct relations (Wu et al., 2009; Li et al., 2010; Yang and Cardie, 2012), and joint models which extract opinion expressions, targets and relations using a unified joint model (Yang and Cardie, 2013; Yang and Cardie, 2014). One consideration of applying supervised methods is their dependencies on the domains and human annotations. Semi-supervised and unsupervised models are also applied for extracting opinion relations. Approaches include rule-based bootstrapping (Qiu et al., 2011), graph propagation algorithms (Xu et al., 2013; Liu et al., 2014; Brody and Elhadad, 2010), integer programming (Lu et al., 2011), and probabilistic topic models (Titov and McDonald, 2008; Mukherjee and Liu, 2012). Our model is inspired by previous distantly supervised algorithms (Snow et al., 2004; Mintz et al., 2009). They use relations from WordNet or knowledge bases as distant supervision. Since we don’t have similar resources for opinion relation extraction, we use patterns to generate relations. Neural network classifiers are popular for relation extraction recently. Many of them focus on fully supervised settings, recurrent neural"
E17-1097,D15-1203,0,0.027498,"al., 2011), and probabilistic topic models (Titov and McDonald, 2008; Mukherjee and Liu, 2012). Our model is inspired by previous distantly supervised algorithms (Snow et al., 2004; Mintz et al., 2009). They use relations from WordNet or knowledge bases as distant supervision. Since we don’t have similar resources for opinion relation extraction, we use patterns to generate relations. Neural network classifiers are popular for relation extraction recently. Many of them focus on fully supervised settings, recurrent neural networks (RNN) and convolutional neural networks (CNN) (Vu et al., 2016; Zeng et al., 2015; Xu et al., 2015a; Xu et al., 2015b; Zhang and Wang, 2015), sequence models and tree models 1034 are investigated (Li et al., 2015; dos Santos et al., 2015). One similar network structure to our model is proposed in (Miwa and Bansal, 2016). They jointly extract entities and relations using two LSTM models. Another recent work (Jebbara and Cimiano, 2016) uses stacked RNNs and CNNs for aspect and opinion detection. Different from models there, we will learn representations for different lexical and syntactic features explicitly. Our formulation follows the features in traditional relation class"
E17-1097,D10-1006,0,0.0325526,"gated (Li et al., 2015; dos Santos et al., 2015). One similar network structure to our model is proposed in (Miwa and Bansal, 2016). They jointly extract entities and relations using two LSTM models. Another recent work (Jebbara and Cimiano, 2016) uses stacked RNNs and CNNs for aspect and opinion detection. Different from models there, we will learn representations for different lexical and syntactic features explicitly. Our formulation follows the features in traditional relation classifiers, which helps to interpret the learned vectors. A closely related task is aspect-based opinion mining (Zhao et al., 2010; Yu et al., 2011; Wang et al., 2015). Instead of locating the opinion expressions, aspect-based opinion mining directly analyzes polarities of different opinion targets. The targets are usually constrained to be some predefined set. Shared tasks (SemEval2014, SemEval2015) have been held on the task, and various systems are proposed and evaluated (Pontiki et al., 2014; Pontiki et al., 2015). Comparing with aspect-based opinion mining, we will extract opinion expressions which are more informative, and we won’t constrain opinion target types which helps us to handle open domain texts. 3 The App"
E17-1097,D15-1062,0,0.181134,"babilistic topic models (Titov and McDonald, 2008; Mukherjee and Liu, 2012). Our model is inspired by previous distantly supervised algorithms (Snow et al., 2004; Mintz et al., 2009). They use relations from WordNet or knowledge bases as distant supervision. Since we don’t have similar resources for opinion relation extraction, we use patterns to generate relations. Neural network classifiers are popular for relation extraction recently. Many of them focus on fully supervised settings, recurrent neural networks (RNN) and convolutional neural networks (CNN) (Vu et al., 2016; Zeng et al., 2015; Xu et al., 2015a; Xu et al., 2015b; Zhang and Wang, 2015), sequence models and tree models 1034 are investigated (Li et al., 2015; dos Santos et al., 2015). One similar network structure to our model is proposed in (Miwa and Bansal, 2016). They jointly extract entities and relations using two LSTM models. Another recent work (Jebbara and Cimiano, 2016) uses stacked RNNs and CNNs for aspect and opinion detection. Different from models there, we will learn representations for different lexical and syntactic features explicitly. Our formulation follows the features in traditional relation classifiers, which hel"
E17-1097,D15-1206,0,0.170705,"babilistic topic models (Titov and McDonald, 2008; Mukherjee and Liu, 2012). Our model is inspired by previous distantly supervised algorithms (Snow et al., 2004; Mintz et al., 2009). They use relations from WordNet or knowledge bases as distant supervision. Since we don’t have similar resources for opinion relation extraction, we use patterns to generate relations. Neural network classifiers are popular for relation extraction recently. Many of them focus on fully supervised settings, recurrent neural networks (RNN) and convolutional neural networks (CNN) (Vu et al., 2016; Zeng et al., 2015; Xu et al., 2015a; Xu et al., 2015b; Zhang and Wang, 2015), sequence models and tree models 1034 are investigated (Li et al., 2015; dos Santos et al., 2015). One similar network structure to our model is proposed in (Miwa and Bansal, 2016). They jointly extract entities and relations using two LSTM models. Another recent work (Jebbara and Cimiano, 2016) uses stacked RNNs and CNNs for aspect and opinion detection. Different from models there, we will learn representations for different lexical and syntactic features explicitly. Our formulation follows the features in traditional relation classifiers, which hel"
E17-1097,D12-1122,0,0.0219912,"ormances of the proposed unsupervised models are even competitive with previous supervised models. 2 Related Works Opinion relation extraction is an important task for fine-grained sentiment analysis. If human annotations are provided (e.g. MPQA corpus (Deng and Wiebe, 2015)), we could formulate the task into a supervised relation extraction problem as (Kobayashi et al., 2007; Johansson and Moschitti, 2013). Two types of models have been applied: pipeline models which first extract candidates of opinion expressions and targets then identify correct relations (Wu et al., 2009; Li et al., 2010; Yang and Cardie, 2012), and joint models which extract opinion expressions, targets and relations using a unified joint model (Yang and Cardie, 2013; Yang and Cardie, 2014). One consideration of applying supervised methods is their dependencies on the domains and human annotations. Semi-supervised and unsupervised models are also applied for extracting opinion relations. Approaches include rule-based bootstrapping (Qiu et al., 2011), graph propagation algorithms (Xu et al., 2013; Liu et al., 2014; Brody and Elhadad, 2010), integer programming (Lu et al., 2011), and probabilistic topic models (Titov and McDonald, 20"
E17-1097,P13-1161,0,0.0209364,"tion extraction is an important task for fine-grained sentiment analysis. If human annotations are provided (e.g. MPQA corpus (Deng and Wiebe, 2015)), we could formulate the task into a supervised relation extraction problem as (Kobayashi et al., 2007; Johansson and Moschitti, 2013). Two types of models have been applied: pipeline models which first extract candidates of opinion expressions and targets then identify correct relations (Wu et al., 2009; Li et al., 2010; Yang and Cardie, 2012), and joint models which extract opinion expressions, targets and relations using a unified joint model (Yang and Cardie, 2013; Yang and Cardie, 2014). One consideration of applying supervised methods is their dependencies on the domains and human annotations. Semi-supervised and unsupervised models are also applied for extracting opinion relations. Approaches include rule-based bootstrapping (Qiu et al., 2011), graph propagation algorithms (Xu et al., 2013; Liu et al., 2014; Brody and Elhadad, 2010), integer programming (Lu et al., 2011), and probabilistic topic models (Titov and McDonald, 2008; Mukherjee and Liu, 2012). Our model is inspired by previous distantly supervised algorithms (Snow et al., 2004; Mintz et a"
E17-1097,Q14-1039,0,0.0194001,"mportant task for fine-grained sentiment analysis. If human annotations are provided (e.g. MPQA corpus (Deng and Wiebe, 2015)), we could formulate the task into a supervised relation extraction problem as (Kobayashi et al., 2007; Johansson and Moschitti, 2013). Two types of models have been applied: pipeline models which first extract candidates of opinion expressions and targets then identify correct relations (Wu et al., 2009; Li et al., 2010; Yang and Cardie, 2012), and joint models which extract opinion expressions, targets and relations using a unified joint model (Yang and Cardie, 2013; Yang and Cardie, 2014). One consideration of applying supervised methods is their dependencies on the domains and human annotations. Semi-supervised and unsupervised models are also applied for extracting opinion relations. Approaches include rule-based bootstrapping (Qiu et al., 2011), graph propagation algorithms (Xu et al., 2013; Liu et al., 2014; Brody and Elhadad, 2010), integer programming (Lu et al., 2011), and probabilistic topic models (Titov and McDonald, 2008; Mukherjee and Liu, 2012). Our model is inspired by previous distantly supervised algorithms (Snow et al., 2004; Mintz et al., 2009). They use rela"
E17-1097,D11-1013,0,0.0190978,"015; dos Santos et al., 2015). One similar network structure to our model is proposed in (Miwa and Bansal, 2016). They jointly extract entities and relations using two LSTM models. Another recent work (Jebbara and Cimiano, 2016) uses stacked RNNs and CNNs for aspect and opinion detection. Different from models there, we will learn representations for different lexical and syntactic features explicitly. Our formulation follows the features in traditional relation classifiers, which helps to interpret the learned vectors. A closely related task is aspect-based opinion mining (Zhao et al., 2010; Yu et al., 2011; Wang et al., 2015). Instead of locating the opinion expressions, aspect-based opinion mining directly analyzes polarities of different opinion targets. The targets are usually constrained to be some predefined set. Shared tasks (SemEval2014, SemEval2015) have been held on the task, and various systems are proposed and evaluated (Pontiki et al., 2014; Pontiki et al., 2015). Comparing with aspect-based opinion mining, we will extract opinion expressions which are more informative, and we won’t constrain opinion target types which helps us to handle open domain texts. 3 The Approach Given an in"
E17-1097,N10-1122,0,\N,Missing
K17-3025,P11-1068,0,0.0670787,"Missing"
K17-3025,L16-1262,0,0.0669038,"Missing"
K17-3025,W06-2920,0,0.290895,"Missing"
K17-3025,L16-1680,0,0.0593987,"Missing"
K17-3025,P15-1032,0,0.0127438,"CoNLL 2006 and CoNLL 2007, the focus of the CoNLL 2017 UD Shared Task is learning syntactic dependency parsers on a universal syntactic annotation standard. This shared task requires participants to parse raw texts from different languages, which vary both in typology and training set size. The CoNLL 2017 UD Shared Task provided universal dependencies description from LREC 2016 (Nivre et al., 2016), two datasets, which are UD version 2.0 datasets (Nivre et al., 2017b) and this task test datasets (Nivre et al., 2017a), two baseline models, which are UDPipe (Straka et al., 2016) and SyntaxNet (Weiss et al., 2015), and the evaluation platform TIRA (Potthast et al., 2014). 2 System Description We implement a transition-based projective parser following Kiperwasser and Goldberg (2016). The system consists of a BiLSTM feature extractor and an MLP classifier. We describe their model and our implementation in the following sections in detail. 2.1 Arc-Hybrid System In this work, we use the arc-hybrid transition system (Kuhlmann et al., 2011). In the arc-hybrid system, a configuration c = (α, β, A) consists of a stack α, a buffer β, and a set of dependency arcs A. Given n words sentence s = w1 , · · · , wn ,"
K17-3025,D14-1082,0,\N,Missing
K17-3025,D07-1096,0,\N,Missing
K18-2025,L16-1680,0,0.0487129,"Missing"
P13-1143,D09-1052,0,0.0915501,"The spelling correction candidates are given by a spell checker. We used GNU Aspell4 in our work. 1462 3 4 http://www.gnu.org/software/wdiff/ http://aspell.net 6.2.2 Weights As described in Section 3.2, the weight of each variable is a linear combination of the language model score, three classier condence scores, and three classier disagreement scores. We use the Web 1T 5-gram corpus (Brants and Franz, 2006) to compute the language model score for a sentence. Each of the three classiers (article, preposition, and noun number) is trained with the multi-class condence weighted algorithm (Crammer et al., 2009). The training data consists of all non-OCR papers in the ACL Anthology5 , minus the documents that overlap with the HOO 2011 data set. The features used for the classiers follow those in (Dahlmeier and Ng, 2012a), which include lexical and part-of-speech n-grams, lexical head words, web-scale n-gram counts, dependency heads and children, etc. Over 5 million training examples are extracted from the ACL Anthology for use as training data for the article and noun number classiers, and over 1 million training examples for the preposition classier. Finally, the language model score, classier c"
P13-1143,P11-1092,1,0.831643,"tical error correction systems (Murata and Nagao, 1993; Bond et al., 1995; Bond and Ikehara, 1996; Heine, 1998). However, as noted by (Han et al., 2006), rules usually have exceptions, and it is hard to utilize corpus statistics in handcrafted rules. As such, the machine learning approach has become the dominant approach in grammatical error correction. Previous work in the machine learning approach typically formulates the task as a classication problem. Article and preposition errors are the two main research topics (Knight and Chander, 1994; Han et al., 2006; Tetreault and Chodorow, 2008; Dahlmeier and Ng, 2011). Features used in classication include surrounding words, part-of-speech tags, language model scores (Gamon, 2010), and parse tree structures (Tetreault et al., 2010). Learning algorithms used include maximum entropy (Han et al., 2006; Tetreault and Chodorow, 2008), averaged perceptron, na¨ve Bayes (Rozovskaya and Roth, 2011), etc. Besides article and preposition errors, verb form errors also attract some attention recently (Liu et al., 2010; Tajiri et al., 2012). Several research efforts have started to deal with correcting different errors in an integrated manner (Gamon, 2011; Park and Le"
P13-1143,D12-1052,1,0.928624,"used in classication include surrounding words, part-of-speech tags, language model scores (Gamon, 2010), and parse tree structures (Tetreault et al., 2010). Learning algorithms used include maximum entropy (Han et al., 2006; Tetreault and Chodorow, 2008), averaged perceptron, na¨ve Bayes (Rozovskaya and Roth, 2011), etc. Besides article and preposition errors, verb form errors also attract some attention recently (Liu et al., 2010; Tajiri et al., 2012). Several research efforts have started to deal with correcting different errors in an integrated manner (Gamon, 2011; Park and Levy, 2011; Dahlmeier and Ng, 2012a). Gamon (2011) uses a high-order sequential labeling model to detect various errors. Park and Levy (2011) models grammatical error correction using a noisy channel model, where a predened generative model produces correct sentences and errors are added through a noise model. The work of (Dahlmeier and Ng, 2012a) is probably the closest to our current work. It uses a beamsearch decoder, which iteratively corrects an input sentence to arrive at the best corrected output. The difference between their work and our ILP approach is that the beam-search decoder returns an approximate solution to t"
P13-1143,N12-1067,1,0.913602,"used in classication include surrounding words, part-of-speech tags, language model scores (Gamon, 2010), and parse tree structures (Tetreault et al., 2010). Learning algorithms used include maximum entropy (Han et al., 2006; Tetreault and Chodorow, 2008), averaged perceptron, na¨ve Bayes (Rozovskaya and Roth, 2011), etc. Besides article and preposition errors, verb form errors also attract some attention recently (Liu et al., 2010; Tajiri et al., 2012). Several research efforts have started to deal with correcting different errors in an integrated manner (Gamon, 2011; Park and Levy, 2011; Dahlmeier and Ng, 2012a). Gamon (2011) uses a high-order sequential labeling model to detect various errors. Park and Levy (2011) models grammatical error correction using a noisy channel model, where a predened generative model produces correct sentences and errors are added through a noise model. The work of (Dahlmeier and Ng, 2012a) is probably the closest to our current work. It uses a beamsearch decoder, which iteratively corrects an input sentence to arrive at the best corrected output. The difference between their work and our ILP approach is that the beam-search decoder returns an approximate solution to t"
P13-1143,W11-2838,0,0.542561,"erimental results on the Helping Our Own shared task show that our method is competitive with state-of-the-art systems. 1 Introduction Grammatical error correction is an important task of natural language processing (NLP). It has many potential applications and may help millions of people who learn English as a second language (ESL). As a research eld, it faces the challenge of processing ungrammatical language, which is different from other NLP tasks. The task has received much attention in recent years, and was the focus of two shared tasks on grammatical error correction in 2011 and 2012 (Dale and Kilgarriff, 2011; Dale et al., 2012). To detect and correct grammatical errors, two different approaches are typically used  knowledge engineering or machine learning. The rst relies on handcrafting a set of rules. For example, the superlative adjective best is preceded by the article the. In contrast, the machine learning approach formulates the task as a classication problem based on learning from training data. For example, an article classier takes a noun phrase (NP) as input and predicts its article using class labels a/an, the, or ɛ (no article). Both approaches have their advantages and disadvantag"
P13-1143,1995.tmi-1.1,0,0.384786,"garriff, 2011). Experimental results show that the ILP formulation is competitive with stateof-the-art grammatical error correction systems. The remainder of this paper is organized as follows. Section 2 gives the related work. Section 3 introduces a basic ILP formulation. Sections 4 and 5 improve the basic ILP formulation with more constraints and second order variables, respectively. Section 6 presents the experimental results. Section 7 concludes the paper. 2 Related Work The knowledge engineering approach has been used in early grammatical error correction systems (Murata and Nagao, 1993; Bond et al., 1995; Bond and Ikehara, 1996; Heine, 1998). However, as noted by (Han et al., 2006), rules usually have exceptions, and it is hard to utilize corpus statistics in handcrafted rules. As such, the machine learning approach has become the dominant approach in grammatical error correction. Previous work in the machine learning approach typically formulates the task as a classication problem. Article and preposition errors are the two main research topics (Knight and Chander, 1994; Han et al., 2006; Tetreault and Chodorow, 2008; Dahlmeier and Ng, 2011). Features used in classication include surroundi"
P13-1143,N10-1019,0,0.169916,"ted by (Han et al., 2006), rules usually have exceptions, and it is hard to utilize corpus statistics in handcrafted rules. As such, the machine learning approach has become the dominant approach in grammatical error correction. Previous work in the machine learning approach typically formulates the task as a classication problem. Article and preposition errors are the two main research topics (Knight and Chander, 1994; Han et al., 2006; Tetreault and Chodorow, 2008; Dahlmeier and Ng, 2011). Features used in classication include surrounding words, part-of-speech tags, language model scores (Gamon, 2010), and parse tree structures (Tetreault et al., 2010). Learning algorithms used include maximum entropy (Han et al., 2006; Tetreault and Chodorow, 2008), averaged perceptron, na¨ve Bayes (Rozovskaya and Roth, 2011), etc. Besides article and preposition errors, verb form errors also attract some attention recently (Liu et al., 2010; Tajiri et al., 2012). Several research efforts have started to deal with correcting different errors in an integrated manner (Gamon, 2011; Park and Levy, 2011; Dahlmeier and Ng, 2012a). Gamon (2011) uses a high-order sequential labeling model to detect various error"
P13-1143,W11-1422,0,0.0493969,"Dahlmeier and Ng, 2011). Features used in classication include surrounding words, part-of-speech tags, language model scores (Gamon, 2010), and parse tree structures (Tetreault et al., 2010). Learning algorithms used include maximum entropy (Han et al., 2006; Tetreault and Chodorow, 2008), averaged perceptron, na¨ve Bayes (Rozovskaya and Roth, 2011), etc. Besides article and preposition errors, verb form errors also attract some attention recently (Liu et al., 2010; Tajiri et al., 2012). Several research efforts have started to deal with correcting different errors in an integrated manner (Gamon, 2011; Park and Levy, 2011; Dahlmeier and Ng, 2012a). Gamon (2011) uses a high-order sequential labeling model to detect various errors. Park and Levy (2011) models grammatical error correction using a noisy channel model, where a predened generative model produces correct sentences and errors are added through a noise model. The work of (Dahlmeier and Ng, 2012a) is probably the closest to our current work. It uses a beamsearch decoder, which iteratively corrects an input sentence to arrive at the best corrected output. The difference between their work and our ILP approach is that the beam-search"
P13-1143,P10-2065,0,0.0583004,"have exceptions, and it is hard to utilize corpus statistics in handcrafted rules. As such, the machine learning approach has become the dominant approach in grammatical error correction. Previous work in the machine learning approach typically formulates the task as a classication problem. Article and preposition errors are the two main research topics (Knight and Chander, 1994; Han et al., 2006; Tetreault and Chodorow, 2008; Dahlmeier and Ng, 2011). Features used in classication include surrounding words, part-of-speech tags, language model scores (Gamon, 2010), and parse tree structures (Tetreault et al., 2010). Learning algorithms used include maximum entropy (Han et al., 2006; Tetreault and Chodorow, 2008), averaged perceptron, na¨ve Bayes (Rozovskaya and Roth, 2011), etc. Besides article and preposition errors, verb form errors also attract some attention recently (Liu et al., 2010; Tajiri et al., 2012). Several research efforts have started to deal with correcting different errors in an integrated manner (Gamon, 2011; Park and Levy, 2011; Dahlmeier and Ng, 2012a). Gamon (2011) uses a high-order sequential labeling model to detect various errors. Park and Levy (2011) models grammatical error cor"
P13-1143,P98-1085,0,0.361985,"hat the ILP formulation is competitive with stateof-the-art grammatical error correction systems. The remainder of this paper is organized as follows. Section 2 gives the related work. Section 3 introduces a basic ILP formulation. Sections 4 and 5 improve the basic ILP formulation with more constraints and second order variables, respectively. Section 6 presents the experimental results. Section 7 concludes the paper. 2 Related Work The knowledge engineering approach has been used in early grammatical error correction systems (Murata and Nagao, 1993; Bond et al., 1995; Bond and Ikehara, 1996; Heine, 1998). However, as noted by (Han et al., 2006), rules usually have exceptions, and it is hard to utilize corpus statistics in handcrafted rules. As such, the machine learning approach has become the dominant approach in grammatical error correction. Previous work in the machine learning approach typically formulates the task as a classication problem. Article and preposition errors are the two main research topics (Knight and Chander, 1994; Han et al., 2006; Tetreault and Chodorow, 2008; Dahlmeier and Ng, 2011). Features used in classication include surrounding words, part-of-speech tags, languag"
P13-1143,D10-1104,0,0.0736214,"blem. Article and preposition errors are the two main research topics (Knight and Chander, 1994; Han et al., 2006; Tetreault and Chodorow, 2008; Dahlmeier and Ng, 2011). Features used in classication include surrounding words, part-of-speech tags, language model scores (Gamon, 2010), and parse tree structures (Tetreault et al., 2010). Learning algorithms used include maximum entropy (Han et al., 2006; Tetreault and Chodorow, 2008), averaged perceptron, na¨ve Bayes (Rozovskaya and Roth, 2011), etc. Besides article and preposition errors, verb form errors also attract some attention recently (Liu et al., 2010; Tajiri et al., 2012). Several research efforts have started to deal with correcting different errors in an integrated manner (Gamon, 2011; Park and Levy, 2011; Dahlmeier and Ng, 2012a). Gamon (2011) uses a high-order sequential labeling model to detect various errors. Park and Levy (2011) models grammatical error correction using a noisy channel model, where a predened generative model produces correct sentences and errors are added through a noise model. The work of (Dahlmeier and Ng, 2012a) is probably the closest to our current work. It uses a beamsearch decoder, which iteratively correc"
P13-1143,P09-1039,0,0.0213781,"s and errors are added through a noise model. The work of (Dahlmeier and Ng, 2012a) is probably the closest to our current work. It uses a beamsearch decoder, which iteratively corrects an input sentence to arrive at the best corrected output. The difference between their work and our ILP approach is that the beam-search decoder returns an approximate solution to the original inference problem, while ILP returns an exact solution to an approximate inference problem. Integer linear programming has been successfully applied to many NLP tasks, such as dependency parsing (Riedel and Clarke, 2006; Martins et al., 2009), semantic role labeling (Punyakanok et al., 2005), and event extraction (Riedel and McCallum, 2011). 3 Inference with First Order Variables The inference problem for grammatical error correction can be stated as follows: Given an input sentence, choose a set of corrections which results in the best output sentence. In this paper, this problem will be expressed and solved by integer linear programming (ILP). To express an NLP task in the framework of ILP requires the following steps: 1. Encode the output space of the NLP task using integer variables; 2. Express the inference objective as a l"
P13-1143,1993.tmi-1.18,0,0.102148,"hared task (Dale and Kilgarriff, 2011). Experimental results show that the ILP formulation is competitive with stateof-the-art grammatical error correction systems. The remainder of this paper is organized as follows. Section 2 gives the related work. Section 3 introduces a basic ILP formulation. Sections 4 and 5 improve the basic ILP formulation with more constraints and second order variables, respectively. Section 6 presents the experimental results. Section 7 concludes the paper. 2 Related Work The knowledge engineering approach has been used in early grammatical error correction systems (Murata and Nagao, 1993; Bond et al., 1995; Bond and Ikehara, 1996; Heine, 1998). However, as noted by (Han et al., 2006), rules usually have exceptions, and it is hard to utilize corpus statistics in handcrafted rules. As such, the machine learning approach has become the dominant approach in grammatical error correction. Previous work in the machine learning approach typically formulates the task as a classication problem. Article and preposition errors are the two main research topics (Knight and Chander, 1994; Han et al., 2006; Tetreault and Chodorow, 2008; Dahlmeier and Ng, 2011). Features used in classicatio"
P13-1143,P11-1094,0,0.0790867,"d Ng, 2011). Features used in classication include surrounding words, part-of-speech tags, language model scores (Gamon, 2010), and parse tree structures (Tetreault et al., 2010). Learning algorithms used include maximum entropy (Han et al., 2006; Tetreault and Chodorow, 2008), averaged perceptron, na¨ve Bayes (Rozovskaya and Roth, 2011), etc. Besides article and preposition errors, verb form errors also attract some attention recently (Liu et al., 2010; Tajiri et al., 2012). Several research efforts have started to deal with correcting different errors in an integrated manner (Gamon, 2011; Park and Levy, 2011; Dahlmeier and Ng, 2012a). Gamon (2011) uses a high-order sequential labeling model to detect various errors. Park and Levy (2011) models grammatical error correction using a noisy channel model, where a predened generative model produces correct sentences and errors are added through a noise model. The work of (Dahlmeier and Ng, 2012a) is probably the closest to our current work. It uses a beamsearch decoder, which iteratively corrects an input sentence to arrive at the best corrected output. The difference between their work and our ILP approach is that the beam-search decoder returns an a"
P13-1143,W06-1616,0,0.0338149,"produces correct sentences and errors are added through a noise model. The work of (Dahlmeier and Ng, 2012a) is probably the closest to our current work. It uses a beamsearch decoder, which iteratively corrects an input sentence to arrive at the best corrected output. The difference between their work and our ILP approach is that the beam-search decoder returns an approximate solution to the original inference problem, while ILP returns an exact solution to an approximate inference problem. Integer linear programming has been successfully applied to many NLP tasks, such as dependency parsing (Riedel and Clarke, 2006; Martins et al., 2009), semantic role labeling (Punyakanok et al., 2005), and event extraction (Riedel and McCallum, 2011). 3 Inference with First Order Variables The inference problem for grammatical error correction can be stated as follows: Given an input sentence, choose a set of corrections which results in the best output sentence. In this paper, this problem will be expressed and solved by integer linear programming (ILP). To express an NLP task in the framework of ILP requires the following steps: 1. Encode the output space of the NLP task using integer variables; 2. Express the inf"
P13-1143,D11-1001,0,0.0269282,"the closest to our current work. It uses a beamsearch decoder, which iteratively corrects an input sentence to arrive at the best corrected output. The difference between their work and our ILP approach is that the beam-search decoder returns an approximate solution to the original inference problem, while ILP returns an exact solution to an approximate inference problem. Integer linear programming has been successfully applied to many NLP tasks, such as dependency parsing (Riedel and Clarke, 2006; Martins et al., 2009), semantic role labeling (Punyakanok et al., 2005), and event extraction (Riedel and McCallum, 2011). 3 Inference with First Order Variables The inference problem for grammatical error correction can be stated as follows: Given an input sentence, choose a set of corrections which results in the best output sentence. In this paper, this problem will be expressed and solved by integer linear programming (ILP). To express an NLP task in the framework of ILP requires the following steps: 1. Encode the output space of the NLP task using integer variables; 2. Express the inference objective as a linear objective function; and 3. Introduce problem-specic constraints to rene the feasible output"
P13-1143,P11-1093,0,0.134979,"rammatical error correction. Previous work in the machine learning approach typically formulates the task as a classication problem. Article and preposition errors are the two main research topics (Knight and Chander, 1994; Han et al., 2006; Tetreault and Chodorow, 2008; Dahlmeier and Ng, 2011). Features used in classication include surrounding words, part-of-speech tags, language model scores (Gamon, 2010), and parse tree structures (Tetreault et al., 2010). Learning algorithms used include maximum entropy (Han et al., 2006; Tetreault and Chodorow, 2008), averaged perceptron, na¨ve Bayes (Rozovskaya and Roth, 2011), etc. Besides article and preposition errors, verb form errors also attract some attention recently (Liu et al., 2010; Tajiri et al., 2012). Several research efforts have started to deal with correcting different errors in an integrated manner (Gamon, 2011; Park and Levy, 2011; Dahlmeier and Ng, 2012a). Gamon (2011) uses a high-order sequential labeling model to detect various errors. Park and Levy (2011) models grammatical error correction using a noisy channel model, where a predened generative model produces correct sentences and errors are added through a noise model. The work of (Dahlme"
P13-1143,W11-2843,0,0.0382812,"= 1 (language model); λt = 1 (classier condence); and µt = −1 (classier disagreement). 6.2.3 Constraints In Section 4, three sets of constraints are introduced: modication count (MC), article-noun agreement (ANA), and dependency relation (DR) constraints. The values for the modication count parameters are set as follows: NArt = 3, NPrep = 2, NNoun = 2, and NSpell = 1. 6.3 Experimental Results We compare our ILP approach with two other systems: the beam search decoder of (Dahlmeier and Ng, 2012a) which achieves the best published performance to date on the HOO 2011 data set, and UI Run1 (Rozovskaya et al., 2011) which achieves the best performance among all participating systems at the HOO 2011 shared task. The results are given in Table 4. The HOO 2011 shared task provides two sets of gold-standard edits: the original gold-standard edits produced by the annotator, and the ofcial gold5 http://aclweb.org/anthology-new/ System Original Ofcial P R F P R F UI Run1 40.86 11.21 17.59 54.61 14.57 23.00 Beam search 30.28 19.17 23.48 33.59 20.53 25.48 ILP 20.54 27.93 23.67 21.99 29.04 25.03 Table 4: Comparison of three grammatical error correction systems. standard edits which incorporated corrections propo"
P13-1143,P12-2039,0,0.0772544,"preposition errors are the two main research topics (Knight and Chander, 1994; Han et al., 2006; Tetreault and Chodorow, 2008; Dahlmeier and Ng, 2011). Features used in classication include surrounding words, part-of-speech tags, language model scores (Gamon, 2010), and parse tree structures (Tetreault et al., 2010). Learning algorithms used include maximum entropy (Han et al., 2006; Tetreault and Chodorow, 2008), averaged perceptron, na¨ve Bayes (Rozovskaya and Roth, 2011), etc. Besides article and preposition errors, verb form errors also attract some attention recently (Liu et al., 2010; Tajiri et al., 2012). Several research efforts have started to deal with correcting different errors in an integrated manner (Gamon, 2011; Park and Levy, 2011; Dahlmeier and Ng, 2012a). Gamon (2011) uses a high-order sequential labeling model to detect various errors. Park and Levy (2011) models grammatical error correction using a noisy channel model, where a predened generative model produces correct sentences and errors are added through a noise model. The work of (Dahlmeier and Ng, 2012a) is probably the closest to our current work. It uses a beamsearch decoder, which iteratively corrects an input sentence t"
P13-1143,C08-1109,0,0.082898,"has been used in early grammatical error correction systems (Murata and Nagao, 1993; Bond et al., 1995; Bond and Ikehara, 1996; Heine, 1998). However, as noted by (Han et al., 2006), rules usually have exceptions, and it is hard to utilize corpus statistics in handcrafted rules. As such, the machine learning approach has become the dominant approach in grammatical error correction. Previous work in the machine learning approach typically formulates the task as a classication problem. Article and preposition errors are the two main research topics (Knight and Chander, 1994; Han et al., 2006; Tetreault and Chodorow, 2008; Dahlmeier and Ng, 2011). Features used in classication include surrounding words, part-of-speech tags, language model scores (Gamon, 2010), and parse tree structures (Tetreault et al., 2010). Learning algorithms used include maximum entropy (Han et al., 2006; Tetreault and Chodorow, 2008), averaged perceptron, na¨ve Bayes (Rozovskaya and Roth, 2011), etc. Besides article and preposition errors, verb form errors also attract some attention recently (Liu et al., 2010; Tajiri et al., 2012). Several research efforts have started to deal with correcting different errors in an integrated manner"
P13-1143,W12-2006,0,\N,Missing
P19-1131,D17-1209,0,0.025975,"learning(Sun and Wu, 2019), multiRecently, researches of graph neural networks (GNNs) have been receiving more and more attention because of the great expressive power of graphs (Cai et al., 2018; Battaglia et al., 2018; Zhou et al., 2018). Graph Convolutional Network (GCN) is one of the typical variants of GNN (Bruna et al., 2013; Defferrard et al., 2016; Kipf and Welling, 2017). It has been successfully applied to many NLP tasks such as text classification (Yao et al., 2018), semantic role labeling (Marcheggiani and Titov, 2017), relation extraction (Zhang et al., 2018) machine translation (Bastings et al., 2017) and knowledge base completion (Shang et al., 2018). We note that most previous applications of GCN focus on a single job, while the joint entity relation extraction consists of multiple sub-tasks. Investigating GCN in joint learning scenarios is the main topic of this work. A closely related work is (Christopoulou et al., 2018), which focuses on relation extraction with golden entities. Our work can be viewed as an end-to-end extension of their work. 6 Conclusion We propose a novel and concise joint model based on GCN to perform joint type inference for entity relation extraction task. Compar"
P19-1131,P11-1056,0,0.147557,"republican gurad]ORG-AFF-2:♥♣♠ [units]PHYS-1:♥♠|ORG-AFF-1:♥♣♠|ART-1:♥ ordered to use chemical [weapons]WEA:♥♣♠ ART-2:♥ once u.s. and allied troops cross it . Table 5: Examples from the ACE05 dataset with label annotations from “NN” model and “GCN” model for comparison. The ♥ is the gold standard, and the ♣, ♠ are the output of the “NN” ,“GCN” model respectively. 5 Related Work There have been extensive studies for entity relation extraction task. Early work employs a pipeline of methods that extracts entities first, and then determines their relations (Zelenko et al., 2003; Miwa et al., 2009; Chan and Roth, 2011; Lin et al., 2016). As pipeline approaches suffer from error propagation, researchers have proposed methods for joint entity relation extraction. Parameter sharing is a basic strategy for joint extraction. For example, Miwa and Bansal (2016) propose a neural method comprised of a sentencelevel RNN for extracting entities, and a dependency tree-based RNN to predict relations. Their relation model takes hidden states of the entity model as features (i.e., the shared parameters). Similarly, Katiyar and Cardie (2017) use a simplified relation model based on the entity RNN using the attention mech"
P19-1131,P18-2014,0,0.141109,"tifies a PHYS relation between “[units]PER ” and “[captial]GPE ”, while the “NN” does not find this relation even the entities are correct. However, both models do not identify the relation ART between “[units]PER ” and “[weapons]WEA ”. We think advanced improvement methods which use more powerful graph neural network might be helpful in this situation. 4.2 Golden Entity Results on ACE05 In order to compare with relation classification methods, we evaluate our models with golden entities on ACE05 corpus in Table 4. We use the same data split to compare with their model (Miwa and Bansal, 2016; Christopoulou et al., 2018). We do not tune hyperparameters extensively. For example, we use the same setting in both end-to-end and golden entity rather than tune parameters on each of them. The baseline systems are (Miwa and Bansal, 2016) and (Christopoulou et al., 2018). In general, our “NN” is competitive, comparing to the dependency tree-based state-of-the-art model (Miwa and Bansal, 2016). It shows that our CNN-based neural networks are able to extract more powerful features to help relation extraction task. After adding GCN, our GCN-based models achieve the better performance. This indicates that the proposed mod"
P19-1131,P82-1020,0,0.824741,"Missing"
P19-1131,P16-1087,0,0.0251959,"tree-based RNN to predict relations. Their relation model takes hidden states of the entity model as features (i.e., the shared parameters). Similarly, Katiyar and Cardie (2017) use a simplified relation model based on the entity RNN using the attention mechanism. These joint methods do joint learning through sharing parameters and they have no explicit interaction in type inference. To further explore interactions between the entity decoder and the relation decoder, many of them focus on some joint decoding algorithms. ILP-based joint decoder (Yang and Cardie, 2013), CRF-based joint decoder (Katiyar and Cardie, 2016), joint sequence labelling tag set (Zheng et al., 2017), beam search (Li and Ji, 2014), global normalization (Zhang et al., 2017), and transition system (Wang et al., 2018) are investigated. Different from models there, we propose a novel and concise joint model to handle joint type inference based on graph convolutional networks, which can capture information between multiple entity types and relation types explicitly9 . 9 In addition, transfer learning(Sun and Wu, 2019), multiRecently, researches of graph neural networks (GNNs) have been receiving more and more attention because of the great"
P19-1131,P17-1085,0,0.448054,"e first extracted by an entity model, and then these extracted entities are used as the inputs of a relation model. Pipeline models often ignore interactions between the two models and they suffer from error propagation. Joint models integrate information between entities and relations into a single model with the joint training, and have achieved better ∗ Work done while this author was an intern at Microsoft Research Asia. results than the pipeline models. In this paper, we focus on joint models. More and more joint methods have been applied to this task. Among them, Miwa and Bansal (2016); Katiyar and Cardie (2017) identify the entity with a sequence labelling model, and identify the relation type with a multi-class classifier. These joint methods do joint learning through sharing parameters and they have no explicit interaction in type inference. In addition, some complex joint decoding algorithms (e.g., simultaneously decoding entities and relations in beam search) have been carefully investigated, including Li and Ji (2014); Zhang et al. (2017); Zheng et al. (2017); Wang et al. (2018). They jointly handle span detection and type inference to achieve more interactions. By inspecting the performance of"
P19-1131,P14-1038,0,0.0836562,"el as features (i.e., the shared parameters). Similarly, Katiyar and Cardie (2017) use a simplified relation model based on the entity RNN using the attention mechanism. These joint methods do joint learning through sharing parameters and they have no explicit interaction in type inference. To further explore interactions between the entity decoder and the relation decoder, many of them focus on some joint decoding algorithms. ILP-based joint decoder (Yang and Cardie, 2013), CRF-based joint decoder (Katiyar and Cardie, 2016), joint sequence labelling tag set (Zheng et al., 2017), beam search (Li and Ji, 2014), global normalization (Zhang et al., 2017), and transition system (Wang et al., 2018) are investigated. Different from models there, we propose a novel and concise joint model to handle joint type inference based on graph convolutional networks, which can capture information between multiple entity types and relation types explicitly9 . 9 In addition, transfer learning(Sun and Wu, 2019), multiRecently, researches of graph neural networks (GNNs) have been receiving more and more attention because of the great expressive power of graphs (Cai et al., 2018; Battaglia et al., 2018; Zhou et al., 20"
P19-1131,P16-1200,0,0.055208,"AFF-2:♥♣♠ [units]PHYS-1:♥♠|ORG-AFF-1:♥♣♠|ART-1:♥ ordered to use chemical [weapons]WEA:♥♣♠ ART-2:♥ once u.s. and allied troops cross it . Table 5: Examples from the ACE05 dataset with label annotations from “NN” model and “GCN” model for comparison. The ♥ is the gold standard, and the ♣, ♠ are the output of the “NN” ,“GCN” model respectively. 5 Related Work There have been extensive studies for entity relation extraction task. Early work employs a pipeline of methods that extracts entities first, and then determines their relations (Zelenko et al., 2003; Miwa et al., 2009; Chan and Roth, 2011; Lin et al., 2016). As pipeline approaches suffer from error propagation, researchers have proposed methods for joint entity relation extraction. Parameter sharing is a basic strategy for joint extraction. For example, Miwa and Bansal (2016) propose a neural method comprised of a sentencelevel RNN for extracting entities, and a dependency tree-based RNN to predict relations. Their relation model takes hidden states of the entity model as features (i.e., the shared parameters). Similarly, Katiyar and Cardie (2017) use a simplified relation model based on the entity RNN using the attention mechanism. These joint"
P19-1131,D17-1159,0,0.0261108,"ation between multiple entity types and relation types explicitly9 . 9 In addition, transfer learning(Sun and Wu, 2019), multiRecently, researches of graph neural networks (GNNs) have been receiving more and more attention because of the great expressive power of graphs (Cai et al., 2018; Battaglia et al., 2018; Zhou et al., 2018). Graph Convolutional Network (GCN) is one of the typical variants of GNN (Bruna et al., 2013; Defferrard et al., 2016; Kipf and Welling, 2017). It has been successfully applied to many NLP tasks such as text classification (Yao et al., 2018), semantic role labeling (Marcheggiani and Titov, 2017), relation extraction (Zhang et al., 2018) machine translation (Bastings et al., 2017) and knowledge base completion (Shang et al., 2018). We note that most previous applications of GCN focus on a single job, while the joint entity relation extraction consists of multiple sub-tasks. Investigating GCN in joint learning scenarios is the main topic of this work. A closely related work is (Christopoulou et al., 2018), which focuses on relation extraction with golden entities. Our work can be viewed as an end-to-end extension of their work. 6 Conclusion We propose a novel and concise joint model ba"
P19-1131,P16-1105,0,0.140999,"two stages: entities are first extracted by an entity model, and then these extracted entities are used as the inputs of a relation model. Pipeline models often ignore interactions between the two models and they suffer from error propagation. Joint models integrate information between entities and relations into a single model with the joint training, and have achieved better ∗ Work done while this author was an intern at Microsoft Research Asia. results than the pipeline models. In this paper, we focus on joint models. More and more joint methods have been applied to this task. Among them, Miwa and Bansal (2016); Katiyar and Cardie (2017) identify the entity with a sequence labelling model, and identify the relation type with a multi-class classifier. These joint methods do joint learning through sharing parameters and they have no explicit interaction in type inference. In addition, some complex joint decoding algorithms (e.g., simultaneously decoding entities and relations in beam search) have been carefully investigated, including Li and Ji (2014); Zhang et al. (2017); Zheng et al. (2017); Wang et al. (2018). They jointly handle span detection and type inference to achieve more interactions. By in"
P19-1131,D09-1013,0,0.0833432,"♠ PER:♥♣♠ ORG:♥♣♠ [republican gurad]ORG-AFF-2:♥♣♠ [units]PHYS-1:♥♠|ORG-AFF-1:♥♣♠|ART-1:♥ ordered to use chemical [weapons]WEA:♥♣♠ ART-2:♥ once u.s. and allied troops cross it . Table 5: Examples from the ACE05 dataset with label annotations from “NN” model and “GCN” model for comparison. The ♥ is the gold standard, and the ♣, ♠ are the output of the “NN” ,“GCN” model respectively. 5 Related Work There have been extensive studies for entity relation extraction task. Early work employs a pipeline of methods that extracts entities first, and then determines their relations (Zelenko et al., 2003; Miwa et al., 2009; Chan and Roth, 2011; Lin et al., 2016). As pipeline approaches suffer from error propagation, researchers have proposed methods for joint entity relation extraction. Parameter sharing is a basic strategy for joint extraction. For example, Miwa and Bansal (2016) propose a neural method comprised of a sentencelevel RNN for extracting entities, and a dependency tree-based RNN to predict relations. Their relation model takes hidden states of the entity model as features (i.e., the shared parameters). Similarly, Katiyar and Cardie (2017) use a simplified relation model based on the entity RNN usi"
P19-1131,D14-1162,0,0.0870553,"s work in Table 1. In general, our “GCN” achieves the best entity performance 84.2 percent comparing with existing joint models. For relation performance, our “GCN” significantly outperforms all joint models except for (Sun et al., 2018) which uses more complex joint decoder. Comparing with our basic neural network “NN”, our “GCN” has large improvement both on entities and relations. Those observations demonstrate the effectiveness of our “GCN” for capturing information on multiple entity types and relation types from a sentence. 5 Our word embeddings is initialized with 100dimensional glove (Pennington et al., 2014) word embeddings. The dimensionality of the hidden units and node embedding are set to 128. For all CNN in our network, the kernel sizes are 2 and 3, and the output channels are 25. 1365 Model P Entity R F P Relation R F L&J (2014) Zhang (2017) Sun (2018) 85.2 83.9 76.9 83.2 80.8 83.5 83.6 65.4 64.9 39.8 55.1 49.5 57.5 59.6 M&B (2016) K&C (2017) NN GCN 82.9 84.0 85.7 86.1 83.9 81.3 82.1 82.4 83.4 82.6 83.9 84.2 57.2 55.5 65.6 68.1 54.0 51.8 50.7 52.3 55.6 53.6 57.2 59.1 Table 1: Results on the ACE05 test data. Li and Ji (2014) Zhang et al. (2017) and Sun et al. (2018) are joint decoding algori"
P19-1131,P17-1113,0,0.190846,"es hidden states of the entity model as features (i.e., the shared parameters). Similarly, Katiyar and Cardie (2017) use a simplified relation model based on the entity RNN using the attention mechanism. These joint methods do joint learning through sharing parameters and they have no explicit interaction in type inference. To further explore interactions between the entity decoder and the relation decoder, many of them focus on some joint decoding algorithms. ILP-based joint decoder (Yang and Cardie, 2013), CRF-based joint decoder (Katiyar and Cardie, 2016), joint sequence labelling tag set (Zheng et al., 2017), beam search (Li and Ji, 2014), global normalization (Zhang et al., 2017), and transition system (Wang et al., 2018) are investigated. Different from models there, we propose a novel and concise joint model to handle joint type inference based on graph convolutional networks, which can capture information between multiple entity types and relation types explicitly9 . 9 In addition, transfer learning(Sun and Wu, 2019), multiRecently, researches of graph neural networks (GNNs) have been receiving more and more attention because of the great expressive power of graphs (Cai et al., 2018; Battagli"
P19-1131,D18-1249,1,0.863882,"oncise joint model to handle the joint type inference problem based on graph convolutional network (GCN). 1362 In this work, we decompose the joint entity relation extraction task into two parts, namely, entity span detection and entity relation type deduction. We first treat entity span detection as a sequence labelling task (Section 3.1), and then construct an entity-relation bipartite graph (Section 3.2) to perform joint type inference on entity nodes and relation nodes (Section 3.3). All submodels share parameters and are trained jointly. Different from existing joint learning algorithms (Sun et al., 2018; Zhang et al., 2017; Katiyar and Cardie, 2017; Miwa and Bansal, 2016), we propose a concise joint model to perform joint type inference on entities and relations based on GCNs. It considers interactions among multiple entity types and relation types simultaneously in a sentence. 3.1 Entity Span Detection To extract entity spans from a sentence (Figure 2), we adopt the BILOU sequence tagging scheme: B, I, L and O denote the begin, inside, last and outside of a target span, U denotes a single word span. For example, for a person (PER) entity “Patrick McDowell”, we assign B to “Patrick” and L to"
P19-1131,P13-1161,0,0.0695515,"el RNN for extracting entities, and a dependency tree-based RNN to predict relations. Their relation model takes hidden states of the entity model as features (i.e., the shared parameters). Similarly, Katiyar and Cardie (2017) use a simplified relation model based on the entity RNN using the attention mechanism. These joint methods do joint learning through sharing parameters and they have no explicit interaction in type inference. To further explore interactions between the entity decoder and the relation decoder, many of them focus on some joint decoding algorithms. ILP-based joint decoder (Yang and Cardie, 2013), CRF-based joint decoder (Katiyar and Cardie, 2016), joint sequence labelling tag set (Zheng et al., 2017), beam search (Li and Ji, 2014), global normalization (Zhang et al., 2017), and transition system (Wang et al., 2018) are investigated. Different from models there, we propose a novel and concise joint model to handle joint type inference based on graph convolutional networks, which can capture information between multiple entity types and relation types explicitly9 . 9 In addition, transfer learning(Sun and Wu, 2019), multiRecently, researches of graph neural networks (GNNs) have been re"
P19-1131,D17-1182,0,0.112899,"Missing"
P19-1131,D18-1244,0,0.123934,"information from their neighborhood over the bipartite graph. It helps us to concisely capture information among entities and relations. For example, in Figure 1, to predict the PER (“Toefting”), our joint model can pool the information of PER-SOC, PHYS, PER (“teammates”) and GPE (captital). To further utilize the structure of the graph, we also propose assigning different weights on graph edges. In particular, we introduce a binary relation classification task, which is to determine whether the two entities form a valid relation. Different from previous GCN-based models (Shang et al., 2018; Zhang et al., 2018), the adjacency matrix of graph is based on the output of binary relation classification, which makes the proposed adjacency matrix more explanatory. To summarize, the main contributions of this work are 1 bipartite graph in a more efficient and interpretable way. • We show that the proposed joint model on ACE05 achieves best entity performance, and is competitive with the state-of-the-art in relation performance. 2 Background of GCN In this section, we briefly describe graph convolutional networks (GCNs). Given a graph with n nodes, the goal of GCNs is to learn structureaware node representat"
P19-1237,P16-1231,0,0.102021,"Missing"
P19-1237,D16-1211,0,0.0986309,"Missing"
P19-1237,P18-1026,0,0.117604,"as encoder to construct graph-to-sequence learning. To our knowledge, we are the first to investigate GNNs for dependency parsing task. GNN Parser UAS LAS 91.64 92.12 92.00 86.47 83.83 91.28 86.82 90.81 87.94 88.57 89.11 88.94 89.13 88.28 89.90 89.85 81.96 81.16 88.93 83.73 88.91 84.82 86.33 84.44 86.62 86.24 Table 5: UAS and LAS F1 scores on 12 UD 2.2 test sets from CoNLL 2018 shared task. than the baseline parser. For average performance, it achieves 0.24 percent UAS and 0.28 percent LAS improvement over the baseline parser. 4.2 Error Analysis Following McDonald and Nivre (2011); Ma et al. (2018), we characterize the errors made by the baseline biaffine parser and our GNN parser. Analysis shows that most of the gains come from the difficult cases (e.g. long sentences or longrange dependencies), which represents an encouraging sign of the proposed method’s benefits. Sentence Length. Figure 4 (a) shows the accuracy relative to sentence length. Our parser significantly improves the performance of the baseline parser on long sentence, but is slightly worse on short sentence (length ≤ 10). Dependency Length. Figure 4 (b) shows the precision and recall relative to dependency length. Our par"
P19-1237,K18-2005,0,0.0306511,"esults for a number of NLP tasks. By introducing context neighbors, the graph structure is added to the sequence modeling tool LSTMs, which improves The design of the node representation network is a key problem in neural graph-based parsers. Kiperwasser and Goldberg (2016b) use BiRNNs to obtain node representation with sentence-level information. To better characterize the direction of edge, Dozat and Manning (2017) feed BiRNNs outputs to two MLPs to distinguish word as head or dependent, and then construct a biaffine mapping for prediction. It also performs well on multilingual UD datasets (Che et al., 2018). Given a graph, a GNN can embed the node by recursively aggregating the node representations of its neighbors (Battaglia et al., 2018). Based on a biaffine mapping, GNNs can enhance the node representation by recursively integrating neighbors’ information. The message passing neural network (MPNN) (Gilmer et al., 2017) and the non-local neural network (NLNN) (Wang et al., 2018) are two popular GNN methods. Due to the convenience of self-attention in handling variable sentence length, we use a GAT-like network (Velikovi et al., 2018) belonging to NLNN. Then, we further explore its aggregating"
P19-1237,D14-1082,0,0.296643,"Missing"
P19-1237,D16-1238,0,0.175368,"Missing"
P19-1237,P15-1033,0,0.0404541,"Missing"
P19-1237,C96-1058,0,0.232364,"be T = (V, E), where the node set V contains all words and a synthetic root node 0, and the edge set E contains node pairs (i, j, r) which represents a dependency relation r between wi (the head) and wj (the dependent). Following the general graph-based dependency parsing framework, for every word pair (i, j), a function σ(i, j) assigns it a score which measures how possible is wi to be the head of wj . 2 We denote G to be the directed complete graph in which all nodes in V are connected with weights given by σ. The correct tree T is obtained from G using a decoder (e.g., dynamic programming (Eisner, 1996), maximum spanning tree (McDonald et al., 2005), and greedy algorithm (Zhang et al., 2017)). In neural-network-based models, the score function σ(i, j) usually relies on vector representations of nodes (words) i and j. How to get informative encodings of tree nodes is important for training the parser. Basically, we want the tree node encoder to explore both the surface form and deep structure of the sentence. To encode the surface form of s, we can use recurrent neural networks (Kiperwasser and Goldberg, 2016b). Specifically, we apply a bidirectional long short-term memory network (biLSTM, (H"
P19-1237,P82-1020,0,0.840467,"Missing"
P19-1237,Q16-1032,0,0.0829883,"out how possible they hold valid dependency relations, and then use decoders (e.g., greedy, maximum spanning tree) to generate a full parse tree from the scores. The score function is a key component in graph-based parses. Commonly, a neural network is assigned to learn low dimension vectors for words (i.e., nodes of parse trees), and the score function depends on vectors of the word pair (e.g., inner products). The main task of this paper is to explore effective encoding systems for dependency tree nodes. Two remarkable prior works on node representation are recurrent neural networks (RNNs) (Kiperwasser and Goldberg, 2016b) and biaffine mappings (Dozat and Manning, 2017). RNNs are powerful tools to collect sentence-level information, but the representations ignore features related to dependency structures. The biaffine mappings improve vanilla RNNs via a key observation: the representation of a word should be different regarding whether it is a head or a dependent (i.e., dependency tree edges are directional). Therefore, Dozat and Manning (2017) suggest distinguishing head and dependent vector of a word. Following this line of thought, it is natural to ask whether we can introduce more structured knowledge int"
P19-1237,P05-1012,0,0.148532,"ntains all words and a synthetic root node 0, and the edge set E contains node pairs (i, j, r) which represents a dependency relation r between wi (the head) and wj (the dependent). Following the general graph-based dependency parsing framework, for every word pair (i, j), a function σ(i, j) assigns it a score which measures how possible is wi to be the head of wj . 2 We denote G to be the directed complete graph in which all nodes in V are connected with weights given by σ. The correct tree T is obtained from G using a decoder (e.g., dynamic programming (Eisner, 1996), maximum spanning tree (McDonald et al., 2005), and greedy algorithm (Zhang et al., 2017)). In neural-network-based models, the score function σ(i, j) usually relies on vector representations of nodes (words) i and j. How to get informative encodings of tree nodes is important for training the parser. Basically, we want the tree node encoder to explore both the surface form and deep structure of the sentence. To encode the surface form of s, we can use recurrent neural networks (Kiperwasser and Goldberg, 2016b). Specifically, we apply a bidirectional long short-term memory network (biLSTM, (Hochreiter and Schmidhuber, 1997)). At each sent"
P19-1237,J11-1007,0,0.0623737,"ted GNNs (Beck et al., 2018) are used as encoder to construct graph-to-sequence learning. To our knowledge, we are the first to investigate GNNs for dependency parsing task. GNN Parser UAS LAS 91.64 92.12 92.00 86.47 83.83 91.28 86.82 90.81 87.94 88.57 89.11 88.94 89.13 88.28 89.90 89.85 81.96 81.16 88.93 83.73 88.91 84.82 86.33 84.44 86.62 86.24 Table 5: UAS and LAS F1 scores on 12 UD 2.2 test sets from CoNLL 2018 shared task. than the baseline parser. For average performance, it achieves 0.24 percent UAS and 0.28 percent LAS improvement over the baseline parser. 4.2 Error Analysis Following McDonald and Nivre (2011); Ma et al. (2018), we characterize the errors made by the baseline biaffine parser and our GNN parser. Analysis shows that most of the gains come from the difficult cases (e.g. long sentences or longrange dependencies), which represents an encouraging sign of the proposed method’s benefits. Sentence Length. Figure 4 (a) shows the accuracy relative to sentence length. Our parser significantly improves the performance of the baseline parser on long sentence, but is slightly worse on short sentence (length ≤ 10). Dependency Length. Figure 4 (b) shows the precision and recall relative to dependen"
P19-1237,E06-1011,0,0.237524,"Missing"
P19-1237,P15-1031,0,0.0305741,"collects information c i from the end of s to the position i: → → ← ← ← where xi is the input of a LSTM cell which includes a randomly initialized word embedding e(wi ), a pre-trained word embedding e′ (wi ) from Glove (Pennington et al., 2014) and a trainable embedding of wi ’s part-of-speech tag e(posi ), ( ) xi = e(wi ) + e′ (wi ) ⊕ e(posi ). Then, a context-dependent node representation of word i is the concatenation of the two hidden vectors, → ← ci = c i ⊕ c i . (1) With the node representations, we can define the score function σ using a multi-layer perceptron σ(i, j) = MLP(ci ⊕ cj ) (Pei et al., 2015), or using a normalized bilinear function (A, b1 , b2 are parameters), σ(i, j)= Softmaxi (c⊺i Acj + b⊺1 ci + b⊺2 cj ) ≜ P (i|j), 1 Following the convention of (Dozat and Manning, 2017), we use lowercase italic letters for scalars and indices, lowercase bold letters for vectors, uppercase italic letters for matrices. → c i = LSTM(xi , c i−1 ; θ ), c i = LSTM(xi , c i+1 ; θ ), 2 (2) We will focus on the unlabelled parsing when illustrating our parsing models. For predicting labels, we use the identical setting in (Dozat and Manning, 2017). 2476 x4 MST x4 x3 x3 x2 x2 x1 x1 Decoder RNN Encoder GNN"
P19-1237,Q17-1008,0,0.0293289,", our parser does not improve performance. For nl, our parser improves 0.22 UAS, although LAS is slightly lower 7 The results should not compare with the shared task’s official results. 8 https://github.com/facebookresearch/ fastText 2481 UD 2.2 Baseline Parser UAS LAS bg ca cs de en es fr it nl no ro ru Avg. 91.69 92.08 91.22 86.11 83.72 90.95 86.46 90.70 87.72 88.27 89.07 88.67 88.89 88.25 89.75 88.73 81.86 81.07 88.65 83.15 88.80 84.85 85.97 84.18 86.29 85.96 performance on text classification, POS tagging and NER tasks (Zhang et al., 2018a). Based on syntactic dependency trees, DAG LSTMs (Peng et al., 2017) and GCNs (Zhang et al., 2018b) are used to improve the performance of relation extraction task. Based on the AMR semantic graph representation, graph state LSTMs (Song et al., 2018), GCNs (Bastings et al., 2017) and gated GNNs (Beck et al., 2018) are used as encoder to construct graph-to-sequence learning. To our knowledge, we are the first to investigate GNNs for dependency parsing task. GNN Parser UAS LAS 91.64 92.12 92.00 86.47 83.83 91.28 86.82 90.81 87.94 88.57 89.11 88.94 89.13 88.28 89.90 89.85 81.96 81.16 88.93 83.73 88.91 84.82 86.33 84.44 86.62 86.24 Table 5: UAS and LAS F1 scores o"
P19-1237,Q16-1023,0,0.203845,"out how possible they hold valid dependency relations, and then use decoders (e.g., greedy, maximum spanning tree) to generate a full parse tree from the scores. The score function is a key component in graph-based parses. Commonly, a neural network is assigned to learn low dimension vectors for words (i.e., nodes of parse trees), and the score function depends on vectors of the word pair (e.g., inner products). The main task of this paper is to explore effective encoding systems for dependency tree nodes. Two remarkable prior works on node representation are recurrent neural networks (RNNs) (Kiperwasser and Goldberg, 2016b) and biaffine mappings (Dozat and Manning, 2017). RNNs are powerful tools to collect sentence-level information, but the representations ignore features related to dependency structures. The biaffine mappings improve vanilla RNNs via a key observation: the representation of a word should be different regarding whether it is a head or a dependent (i.e., dependency tree edges are directional). Therefore, Dozat and Manning (2017) suggest distinguishing head and dependent vector of a word. Following this line of thought, it is natural to ask whether we can introduce more structured knowledge int"
P19-1237,D14-1162,0,0.089215,"(Kiperwasser and Goldberg, 2016b). Specifically, we apply a bidirectional long short-term memory network (biLSTM, (Hochreiter and Schmidhuber, 1997)). At each sentence position i, a forward LSTM chain (with → → parameter θ ) computes a hidden state vector c i by collecting information from the beginning of s to the current position i. Similarly, a backward ← ← LSTM chain ( θ ) collects information c i from the end of s to the position i: → → ← ← ← where xi is the input of a LSTM cell which includes a randomly initialized word embedding e(wi ), a pre-trained word embedding e′ (wi ) from Glove (Pennington et al., 2014) and a trainable embedding of wi ’s part-of-speech tag e(posi ), ( ) xi = e(wi ) + e′ (wi ) ⊕ e(posi ). Then, a context-dependent node representation of word i is the concatenation of the two hidden vectors, → ← ci = c i ⊕ c i . (1) With the node representations, we can define the score function σ using a multi-layer perceptron σ(i, j) = MLP(ci ⊕ cj ) (Pei et al., 2015), or using a normalized bilinear function (A, b1 , b2 are parameters), σ(i, j)= Softmaxi (c⊺i Acj + b⊺1 ci + b⊺2 cj ) ≜ P (i|j), 1 Following the convention of (Dozat and Manning, 2017), we use lowercase italic letters for scalar"
P19-1237,D16-1180,0,0.235838,"Missing"
P19-1237,P18-1150,0,0.0200597,"ts. 8 https://github.com/facebookresearch/ fastText 2481 UD 2.2 Baseline Parser UAS LAS bg ca cs de en es fr it nl no ro ru Avg. 91.69 92.08 91.22 86.11 83.72 90.95 86.46 90.70 87.72 88.27 89.07 88.67 88.89 88.25 89.75 88.73 81.86 81.07 88.65 83.15 88.80 84.85 85.97 84.18 86.29 85.96 performance on text classification, POS tagging and NER tasks (Zhang et al., 2018a). Based on syntactic dependency trees, DAG LSTMs (Peng et al., 2017) and GCNs (Zhang et al., 2018b) are used to improve the performance of relation extraction task. Based on the AMR semantic graph representation, graph state LSTMs (Song et al., 2018), GCNs (Bastings et al., 2017) and gated GNNs (Beck et al., 2018) are used as encoder to construct graph-to-sequence learning. To our knowledge, we are the first to investigate GNNs for dependency parsing task. GNN Parser UAS LAS 91.64 92.12 92.00 86.47 83.83 91.28 86.82 90.81 87.94 88.57 89.11 88.94 89.13 88.28 89.90 89.85 81.96 81.16 88.93 83.73 88.91 84.82 86.33 84.44 86.62 86.24 Table 5: UAS and LAS F1 scores on 12 UD 2.2 test sets from CoNLL 2018 shared task. than the baseline parser. For average performance, it achieves 0.24 percent UAS and 0.28 percent LAS improvement over the baseline"
P19-1237,P18-1130,0,0.245361,"y tree node representations. Given a weighted graph, a GNN embeds a node by recursively aggregating node representations of its neighbours. For the parsing task, we build GNNs on weighted complete graphs which are readily obtained in graphbased parsers. The graphs could be fixed in prior or revised during the parsing process. By stacking multiple layers of GNNs, the representation of a node gradually collects various high-order information and bring global evidence into decoders’ final decision. Comparing with recent approximate highorder parsers (Kiperwasser and Goldberg, 2016b; Zheng, 2017; Ma et al., 2018), GNNs extract highorder information in a similar incremental manner: node representations of a GNN layer are computed based on outputs of former layers. However, the main difference is that, instead of extracting highorder features on only one intermediate tree, the update of GNN node vectors is able to inspect all intermediate trees. Thus, it may reduce the influence of a suboptimal intermediate parsing result. Comparing with the syntactic graph network 2475 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2475–2485 c Florence, Italy, July 28 - A"
P19-1237,L16-1680,0,0.0359525,"Missing"
P19-1237,D17-1159,0,0.062292,"anner: node representations of a GNN layer are computed based on outputs of former layers. However, the main difference is that, instead of extracting highorder features on only one intermediate tree, the update of GNN node vectors is able to inspect all intermediate trees. Thus, it may reduce the influence of a suboptimal intermediate parsing result. Comparing with the syntactic graph network 2475 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2475–2485 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics (Marcheggiani and Titov, 2017; Bastings et al., 2017; Zhang et al., 2018b) which runs GNNs on dependency trees given by external parsers, we use GNNs to build the parsing model. And instead of using different weight matrices for outgoing and ingoing edges, our way of handling directional edges is based on the separation of head and dependent representations, which requires new protocols for updating nodes. We discuss various configurations of GNNs, including strategies on neighbour vector aggregations, synchronized or asynchronized node vector update and graphs with different edge weights. Experiments on the benchmark Eng"
P19-1237,P16-1218,0,0.0784109,"Missing"
P19-1237,P15-1032,0,0.0588995,"Missing"
P19-1237,K18-2001,0,0.0383385,"Missing"
P19-1237,E17-1063,0,0.0232998,"and the edge set E contains node pairs (i, j, r) which represents a dependency relation r between wi (the head) and wj (the dependent). Following the general graph-based dependency parsing framework, for every word pair (i, j), a function σ(i, j) assigns it a score which measures how possible is wi to be the head of wj . 2 We denote G to be the directed complete graph in which all nodes in V are connected with weights given by σ. The correct tree T is obtained from G using a decoder (e.g., dynamic programming (Eisner, 1996), maximum spanning tree (McDonald et al., 2005), and greedy algorithm (Zhang et al., 2017)). In neural-network-based models, the score function σ(i, j) usually relies on vector representations of nodes (words) i and j. How to get informative encodings of tree nodes is important for training the parser. Basically, we want the tree node encoder to explore both the surface form and deep structure of the sentence. To encode the surface form of s, we can use recurrent neural networks (Kiperwasser and Goldberg, 2016b). Specifically, we apply a bidirectional long short-term memory network (biLSTM, (Hochreiter and Schmidhuber, 1997)). At each sentence position i, a forward LSTM chain (with"
P19-1237,P18-1030,0,0.0517714,"as encoder to construct graph-to-sequence learning. To our knowledge, we are the first to investigate GNNs for dependency parsing task. GNN Parser UAS LAS 91.64 92.12 92.00 86.47 83.83 91.28 86.82 90.81 87.94 88.57 89.11 88.94 89.13 88.28 89.90 89.85 81.96 81.16 88.93 83.73 88.91 84.82 86.33 84.44 86.62 86.24 Table 5: UAS and LAS F1 scores on 12 UD 2.2 test sets from CoNLL 2018 shared task. than the baseline parser. For average performance, it achieves 0.24 percent UAS and 0.28 percent LAS improvement over the baseline parser. 4.2 Error Analysis Following McDonald and Nivre (2011); Ma et al. (2018), we characterize the errors made by the baseline biaffine parser and our GNN parser. Analysis shows that most of the gains come from the difficult cases (e.g. long sentences or longrange dependencies), which represents an encouraging sign of the proposed method’s benefits. Sentence Length. Figure 4 (a) shows the accuracy relative to sentence length. Our parser significantly improves the performance of the baseline parser on long sentence, but is slightly worse on short sentence (length ≤ 10). Dependency Length. Figure 4 (b) shows the precision and recall relative to dependency length. Our par"
P19-1237,D18-1244,0,0.13902,"ed based on outputs of former layers. However, the main difference is that, instead of extracting highorder features on only one intermediate tree, the update of GNN node vectors is able to inspect all intermediate trees. Thus, it may reduce the influence of a suboptimal intermediate parsing result. Comparing with the syntactic graph network 2475 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2475–2485 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics (Marcheggiani and Titov, 2017; Bastings et al., 2017; Zhang et al., 2018b) which runs GNNs on dependency trees given by external parsers, we use GNNs to build the parsing model. And instead of using different weight matrices for outgoing and ingoing edges, our way of handling directional edges is based on the separation of head and dependent representations, which requires new protocols for updating nodes. We discuss various configurations of GNNs, including strategies on neighbour vector aggregations, synchronized or asynchronized node vector update and graphs with different edge weights. Experiments on the benchmark English Penn Treebank 3.0 and CoNLL2018 multil"
P19-1237,D17-1173,0,0.26325,"ing dependency tree node representations. Given a weighted graph, a GNN embeds a node by recursively aggregating node representations of its neighbours. For the parsing task, we build GNNs on weighted complete graphs which are readily obtained in graphbased parsers. The graphs could be fixed in prior or revised during the parsing process. By stacking multiple layers of GNNs, the representation of a node gradually collects various high-order information and bring global evidence into decoders’ final decision. Comparing with recent approximate highorder parsers (Kiperwasser and Goldberg, 2016b; Zheng, 2017; Ma et al., 2018), GNNs extract highorder information in a similar incremental manner: node representations of a GNN layer are computed based on outputs of former layers. However, the main difference is that, instead of extracting highorder features on only one intermediate tree, the update of GNN node vectors is able to inspect all intermediate trees. Thus, it may reduce the influence of a suboptimal intermediate parsing result. Comparing with the syntactic graph network 2475 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2475–2485 c Florence,"
S17-2028,D14-1162,0,0.104934,"Missing"
S17-2028,S16-1091,0,0.0605867,"Missing"
S17-2028,S16-1103,0,0.0274219,"Missing"
S17-2028,S15-2027,0,0.0319514,"r. Note that the stopwords are removed and each word is lemmatized so as to estimate sequence similarity more accurately. As a result, we get 5 features. Syntactic Parse Features: In order to model tree structured similarity between two sentences rather than sequence-based similarity, inspired by Moschitti (2006), we adopt tree kernels to calculate the similarity between two syntactic parse trees. In particular, we calculate the number of common substructures in three different kernel spaces, i.e., subtree (ST), subset tree (SST), partial tree (PT). Thus we get 3 features. Alignment Features: Sultan et al. (2015) used word aligner to align matching words across a pair of sentences, and then computes the proportion of aligned words as follows: Gradient Boosting Single Sentence Features Bag-of-Words BOW Features Bag-ofDependencyTripes Dependency Features XGBoost Kernels Pooled Word Embeddings Word Embedding Features E n s e m b l e Deep Learning Sentence Representation Averaging Word Vectors Neural Network Dense+Softmax Projected Averaging Word Vectors (ƶ-ƶ,ƶ×ƶ) Deep Averaging Network Long Short-Term Memory Network ƶ ƶ 6 6 Figure 1: The system architecture Traditional NLP Module is to extracts two kin"
S17-2028,S17-2001,0,0.145879,"Missing"
S17-2028,P15-1150,0,0.00769724,"y network (LSTM, Hochreiter and Schmidhuber (1997)) to capture long-distance dependencies information. In order to obtain the vector of sentence pair, given two single sentence vectors, we first use a element-wise subtraction and a multiplication and then concatenate the two values as the final vector of sentence pair representation. At last, we use a fully-connected neural network and output the probability of similarity based on a softmax function. Thus we obtain 4 deep learning based scores. To learn model parameters, we minimize the KL-divergence between the outputs and gold labels, as in Tai et al. (2015). We adopt Adam (Kingma and Ba, 2014) as optimization method and set learning rate of 0.01. Track 4b SP-EN-WMT, the performance is very poor. So we perform 10 − f old cross validation (CV) on Track 4b SP-EN-WMT. Preprocessing: All sentences are translated into English via Google Translator. The Stanford CoreNLP (Manning et al., 2014) is used for tokenization, lemmatization, POS tagging and dependency parsing. Evaluation: For Track 1 to Track 6, Pearson correlation coefficient is used to evaluate each individual test set. For Primary Track, since it is achieved by submitting results of all the"
S17-2028,S16-1089,0,0.0492112,"Missing"
S17-2028,S12-1060,0,0.0938589,"Missing"
S17-2028,P15-1162,0,0.0300464,"Missing"
S17-2028,S15-2021,1,0.845385,"n(S2 ) where na (S) and n(S) is the number of aligned and non-repeated words in sentence S. To assign appropriate weights to different words, we adopt two weighting methods: i) weighted by five POS tags (i.e., noun, verb, adjective, adverb and others; we first group words in two sentences into 5 POS categories, then for each POS category we compute the proportion of aligned words, and we get 5 features as a result. ii) weighted by IDF values (calculated in each dataset separately). Totally, we collect 7 alignment features. MT based Features: Following previous work in (Zhao et al., 2014) and (Zhao et al., 2015), we use MT evaluation metrics to measure the semantic equivalence of the given sentence pairs. Nine Traditional NLP Module In this section, we give the details of feature engineering and learning algorithms. 2.1.1 |S1 | |S2 | + )−1 |S1 ∩ S2 ||S1 ∩ S2 | Sentence Pair Matching Features Five types of sentence pair matching features are designed to directly calculate the similarity of two sentences based on the overlaps of character/word/sequence, syntactic structure, alignment and even MT metrics. 192 Type Measures Cosine distance, Manhanttan distance, linear kernel Euclidean distance, Chebyshev"
S17-2028,P14-5010,0,0.00383595,"ion. At last, we use a fully-connected neural network and output the probability of similarity based on a softmax function. Thus we obtain 4 deep learning based scores. To learn model parameters, we minimize the KL-divergence between the outputs and gold labels, as in Tai et al. (2015). We adopt Adam (Kingma and Ba, 2014) as optimization method and set learning rate of 0.01. Track 4b SP-EN-WMT, the performance is very poor. So we perform 10 − f old cross validation (CV) on Track 4b SP-EN-WMT. Preprocessing: All sentences are translated into English via Google Translator. The Stanford CoreNLP (Manning et al., 2014) is used for tokenization, lemmatization, POS tagging and dependency parsing. Evaluation: For Track 1 to Track 6, Pearson correlation coefficient is used to evaluate each individual test set. For Primary Track, since it is achieved by submitting results of all the secondary sub-tracks, a macro-averaged weighted sum of all correlations on sub-tracks is used for evaluation. 2.3 A series of comparison experiments on English STS 2016 training set have been performed to explore different features and algorithms. 4 Ensemble Module The NLP-based scores and the deep learning based scores are averaged"
S17-2028,S14-2044,1,0.134401,"1 ) + na (S2 ) n(S1 ) + n(S2 ) where na (S) and n(S) is the number of aligned and non-repeated words in sentence S. To assign appropriate weights to different words, we adopt two weighting methods: i) weighted by five POS tags (i.e., noun, verb, adjective, adverb and others; we first group words in two sentences into 5 POS categories, then for each POS category we compute the proportion of aligned words, and we get 5 features as a result. ii) weighted by IDF values (calculated in each dataset separately). Totally, we collect 7 alignment features. MT based Features: Following previous work in (Zhao et al., 2014) and (Zhao et al., 2015), we use MT evaluation metrics to measure the semantic equivalence of the given sentence pairs. Nine Traditional NLP Module In this section, we give the details of feature engineering and learning algorithms. 2.1.1 |S1 | |S2 | + )−1 |S1 ∩ S2 ||S1 ∩ S2 | Sentence Pair Matching Features Five types of sentence pair matching features are designed to directly calculate the similarity of two sentences based on the overlaps of character/word/sequence, syntactic structure, alignment and even MT metrics. 192 Type Measures Cosine distance, Manhanttan distance, linear kernel Eucli"
S17-2060,S16-1135,1,0.847975,"tor of one-hot forMatch, Topic Model based, and Lexical Semantic Similarity features, we also extracted Search Engine Extensional feature. For subtask C, we ranked the comments by multiplying the probability of the pair /relevant question õ comment0 being Good by the reciprocal rank of the related question. 2.1 Features Engineering All three subtasks can be regarded as an estimation task of sentence semantic measures which can be modeled by various types of features. Besides Word Match, Topic Model Based, Lexical Semantic Similarity, and Comment Information Features used in our previous work (Wu and Lan, 2016), we also extract three types of novel features, i.e., Meta Data Features, Google Ranking Feature, and Search Engine Extensional Features. The details of features are described as follows. Here we took the Q-Q pair for example. Word Matching Feature (WM): Inspired by the work of (Zhao et al., 2015), we adopt word matching feature in our system. This feature represents the the proportions of co-occurred words that between a given sentence pair. Given a QQ pair, this feature is expressed in the following nine measures:|Q0 ∩ Q1 |, |Q0 ∩ Q1 |/|Q0 |, |Q0 ∩ Q1 |/|Q1 |, |Q1 − Q0 |/|Q1 |, |Q0 − Q1 |/|"
S17-2060,P13-1171,0,0.504954,"as question retrieval), which is to retrieve the simi2 Systems Description For subtask A, we presented two different methods i.e., using traditional linguistic features and learning a CNN model to represent question and comment sentences. For subtask B, besides Word 365 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 365–369, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics lexical semantic similarity feature in our model. Two types of 300-dimensional vectors are pretrained on Qatar Living data with word2vec (Yih et al., 2013b) and Glove (Pennington et al., 2014) toolkits. We select the maximum, minimum and average values for each dimension of words vectors to make up a vector to represent the sentence. After obtained the vector representation of Q0 and Q1 , we also calculated the nine distance measures mentioned in TMB. Note that all above three types of features are adopted in both answer ranking and question retrieval tasks. Search Engine Extensional Feature (SEE): We first got two lists of 10 snippets returned by search engine (i.e., Google, Bing) with the subjects of original question Q0 and related question"
S17-2060,P08-1019,0,0.021664,"matching feature in our system. This feature represents the the proportions of co-occurred words that between a given sentence pair. Given a QQ pair, this feature is expressed in the following nine measures:|Q0 ∩ Q1 |, |Q0 ∩ Q1 |/|Q0 |, |Q0 ∩ Q1 |/|Q1 |, |Q1 − Q0 |/|Q1 |, |Q0 − Q1 |/|Q0 |, |Q0 ∩ Q1 |/|Q0 − Q1 |, |Q0 ∩ Q1 |/|Q1 − Q0 |, |Q0 ∩ Q1 |/|Q0 ∪Q1 |, 2∗|Q0 ∩Q1 |/(|Q0 |+|Q1 |), where |Q0 |and |Q1 |are the number of the words of Q0 and Q1 . Topic Model based Feature (TMB): Topic model based feature has been proved beneficial for question retrieval and answer ranking tasks by the work of (Duan et al., 2008; Qin et al., 2009). We use the GibbsLDA++ (Phan and Nguyen, 2007) Toolkit with 100,000 random sampling question and answer pairs from Qatar Living data to train the topic model. In training and test phase, Q0 and Q1 are transformed into an 100-dimensional topicbased vectors using pre-trained topic model. After that we calculate the cosine similarity, Manhattan distance and Euclidean distance between these two vectors and regard the scores as TMB feature. Inspired by the work of (Filice et al., 2016), we also adopt four kinds of nonlinear kernel functions to calculate the distance between two"
S17-2060,S16-1172,0,0.0948099,"eature has been proved beneficial for question retrieval and answer ranking tasks by the work of (Duan et al., 2008; Qin et al., 2009). We use the GibbsLDA++ (Phan and Nguyen, 2007) Toolkit with 100,000 random sampling question and answer pairs from Qatar Living data to train the topic model. In training and test phase, Q0 and Q1 are transformed into an 100-dimensional topicbased vectors using pre-trained topic model. After that we calculate the cosine similarity, Manhattan distance and Euclidean distance between these two vectors and regard the scores as TMB feature. Inspired by the work of (Filice et al., 2016), we also adopt four kinds of nonlinear kernel functions to calculate the distance between two vectors, i.e., ”polynomial”, ”rbf”, ”laplacian” and ”sigmoid”. Lexical Semantic Similarity Feature (LSS): Inspired by (Yih et al., 2013a), we included the 366 m for each comment. (2) comment ner feature, we extracted nine types of name entity information in the comment, i.e., ”Duration”, ”Location”, ”Person”, ”Organization”, ”Percent”, ”Ordinal”, ”Time”, ”Date”, and ”Money” with the CoreNLP tool, generating a nine-dimensional one-hot forming vector. (3) comment special characters feature, We extracte"
S17-2060,S17-2003,0,0.0340973,"Missing"
S17-2060,D14-1162,0,0.0821803,"is to retrieve the simi2 Systems Description For subtask A, we presented two different methods i.e., using traditional linguistic features and learning a CNN model to represent question and comment sentences. For subtask B, besides Word 365 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 365–369, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics lexical semantic similarity feature in our model. Two types of 300-dimensional vectors are pretrained on Qatar Living data with word2vec (Yih et al., 2013b) and Glove (Pennington et al., 2014) toolkits. We select the maximum, minimum and average values for each dimension of words vectors to make up a vector to represent the sentence. After obtained the vector representation of Q0 and Q1 , we also calculated the nine distance measures mentioned in TMB. Note that all above three types of features are adopted in both answer ranking and question retrieval tasks. Search Engine Extensional Feature (SEE): We first got two lists of 10 snippets returned by search engine (i.e., Google, Bing) with the subjects of original question Q0 and related question Q1 as query. Then we counted the frequ"
S17-2078,S17-2005,0,0.057566,"ated on sense vectors or cluster center vectors. For subtask 2, we established an unsupervised system to locate the pun by scoring each word in the sentence and we assumed that the word with the smallest score is the pun. 1 Introduction A pun is a form of wordplay in which one signifier (e.g., a word or phrase) suggests two or more meanings by exploiting polysemy, or phonological similarity to another signifier, for an intended humorous or rhetorical effect. The study of puns can be seen as a respectable research topic in traditional linguistics and the cognitive sciences. Semeval 2017 task 7(Miller et al., 2017) contains three subtasks, i.e., pun detection, pun location, and pun interpretation. And we participated in the first two subtasks. The detection and location of English puns are to determine whether or not a sentence contains a pun and which word is a pun respectively, which differ from traditional word sense disambiguation (WSD). WSD is to determine an exact meaning of the target word in the given context. However, WSD algorithms could provide the lexical-semantic understanding for pun detection and location. And we adopted a knowledge-based WSD algorithm to obtain possible senses1 for each"
S17-2086,D14-1162,0,0.0814337,"are binary features. The aforementioned features form a 9-dimensions feature. looking up pre-trained GoogleW2V, then grouped into 80 clusters. Thus we adopted 80-dimensions binary feature to mark whether the words of a certain cluster appeared in the tweet. Word Vector Features A lot of recent studies on NLP applications are reported to have good performance using word vectors, such as ducument classification (Sebastiani, 2002), parsing (Socher et al., 2013), and question answering (Lan et al., 2016a), We adopted two widely-used word vectors, i.e., GoogleW2V (Mikolov et al., 2013) and GloVe (Pennington et al., 2014). However, semantic word vectors find similar words with similar context rather than similar sentiment information. Several recent works focused on sentiment word vectors using neural network based models (Lan et al., 2016b). In this work, we also adopted two sentiment word vectors, one is SSWE (Tang et al., 2014) and the other is a home-made sentiment word vector from our previous work. To obtain the representation of a tweet, for each word in a tweet, we concatenated the maximum, minimum and mean of each dimension as a tweet vector [min-max-mean]. 2.2 Learning algorithms and Evaluation metri"
S17-2086,P14-1146,0,0.0364372,"ns are reported to have good performance using word vectors, such as ducument classification (Sebastiani, 2002), parsing (Socher et al., 2013), and question answering (Lan et al., 2016a), We adopted two widely-used word vectors, i.e., GoogleW2V (Mikolov et al., 2013) and GloVe (Pennington et al., 2014). However, semantic word vectors find similar words with similar context rather than similar sentiment information. Several recent works focused on sentiment word vectors using neural network based models (Lan et al., 2016b). In this work, we also adopted two sentiment word vectors, one is SSWE (Tang et al., 2014) and the other is a home-made sentiment word vector from our previous work. To obtain the representation of a tweet, for each word in a tweet, we concatenated the maximum, minimum and mean of each dimension as a tweet vector [min-max-mean]. 2.2 Learning algorithms and Evaluation metrics Based on above multiple features, we explored several learning algorithms to build classification models, e.g., Logistic Regression (LR), supplied in liblinear tools6 , Support Vector Machines (SVM), Decision Trees (DT), Random Forests (RF), AdaBoost (ADB), and Gradient Tree Boosting (GDB), implemented in sciki"
S17-2152,P14-5010,0,0.00533602,"ve samples in the training data. Then we calculate the weight (i.e., rf ) for each token in unigram, bigram and trigram as follows:  rf = max ln(2 + Since the differences between the spans and the title described in section 1, for subtask 2, we replace the target company with “TCOMPANY” and replace other company with “OCOMPNAY” in the title. For both subtasks, the subsequent preprocessing is the same. We firstly replace all URLs with “url” and transform the abbreviations, punctuation with a special format, slangs and elongated words to their normal format. Then, we use Stanford CoreNLP tools(Manning et al., 2014) for tokenization, POS tagging, named entity recognizing (NER) and parsing. Finally, the WordNet-based Lemmatizer implemented in NLTK1 is adopted to lemmatize words to their base forms with the aid of their POS tags. And the word stemmer based on the Porter stemming algorithm and implemented in NLTK is adopted to remove morphological affixes from lemmatized words. Feature Engineering We extract the following four types of features to construct supervised regression models for two subtasks, i.e., linguistic features, sentiment lexicon features, domain-specific features and word embedding featur"
S17-2152,S16-1073,1,0.649296,"e are no symbols and characters before and after the number). Keyword+Number: Based on the Number features, we defined 4-dimensional Keyword+Number features to indicate whether there 2.2.4 4 http://www.cs.uic.edu/liub/FBS/sentiment-analysis.html http://www.wjh.harvard.edu/inquirer/homecat.htm 6 http://mpqa.cs.pitt.edu/ 7 http://www2.imm.dtu.dk/pubdb/views/publication details .php?id=6010 8 http://sentiwordnet.isti.cnr.it/ 9 http://www.umiacs.umd.edu/saif/WebDocs/NRCHashtag-Sentiment-Lexicon-v0.1.zip 10 http://help.sentiment140.com/for-students/ 5 890 Word Embedding Features The previous work (Zhang and Lan, 2016; Jiang et al., 2016) on sentiment analysis task has proved the effectiveness of word embedding features. In this part, we utilize the Google word2vec to get the representation of the sentence. GoogleW2V: Unlike the word cluster features, the Google word2vec features (GoogleW2V) are extracted as follows: We firstly use the Google word2vec to get a 300-dimensional vector for each word in sentence. Then, the simple min, max, average pooling strategies are adopted to concatenate sentence vector representations with dimensionality of 900. 2.3 Learning Algorithms For both tasks, we explore 7 algori"
S17-2152,S13-2067,0,0.216212,"Missing"
S17-2152,S17-2089,0,0.157166,"al Microblogs and News task (i.e., Task 5) in SemEval-2017. This task includes two subtasks in microblogs and news headline domain respectively. To settle this problem, we extract four types of effective features, including linguistic features, sentiment lexicon features, domain-specific features and word embedding features. Then we employ these features to construct models by using ensemble regression algorithms. Our submissions rank 1st and rank 5th in subtask 1 and subtask 2 respectively. 1 Introduction SemEval-2017 Task 5 is Fine-Grained Sentiment Analysis on Financial Microblogs and News(Cortis et al., 2017), focusing on identifying positive (bullish; believing that the stock price will increase) and negative (bearish; believing that the stock price will decline) sentiment associated with stocks and companies from microblogs and news domains. Unlike previous sentiment analysis, in this task, the fine-grained sentiment analysis not only contains sentiment orientation (i.e., positive or negative of the sentiment score) but also sentiment strength (i.e., the value of the sentiment score) attached to a particular company or stock explicitly or implicitly expressed in given texts. Given a text instanc"
S17-2152,S16-1058,1,0.627075,"Missing"
S18-1035,S18-1001,0,0.0459373,"Missing"
S18-1035,L18-1030,0,0.0145841,"this tweet is an exclamation or question mark. Gradient Boosting Regressor (GBR) implemented in scikit-learn tools13 and XGBoost Regressor (XGB)14 . All these algorithms are used with default parameters. 3 Experiments 3.1 Dataset • Bag-of-Hashtags Hashtags reflect emotion orientation of tweets directly, so we constructed a vocabulary of hashtags appearing in the training set and development set, then adopted the bag-of-hashtags method for each tweet. The statistics of the English datasets provided by Semeval 2018 Task 1 are shown in Table 1 and 2. How the English data created is described in (Mohammad and Kiritchenko, 2018). Datasets train dev subtask 1 test subtask 2 • Emoticon We collected 67 emoticons from Internet11 , including 34 positive emoticons and 33 negative emoticons, then designed the following 4 binary features: anger 1,701 388 17,939 1,002 fear 2,252 689 17,923 986 joy 1,616 290 18,042 1,105 sadness 1,533 397 17,912 975 Table 1: The statistics of data sets for subtask 1 and 2. – to record whether the positive and negative emoticons are present in the tweet, respectively (1 for yes, 0 for no). – to record whether the last token is a positive or a negative emoticon. Subtask 3 4 5 • Intensity Words S"
S18-1035,S15-2094,1,0.744877,"ven a tweet, we calculated the following six scores: features, sentiment lexicon features, emotion lexicon features and domain-specific features. 2.2.1 Linguistic Features • Lemma unigram Considering there is similar emotion intensity expressed by “anger” and “angers”, we choose word lemma unigram features from tweets rather than word unigram features. • Negation Negation in a sentence often affects its sentiment orientation, and conveys its intensity of the sentiment. For example, a sentence with several negation words is more inclined to negative sentiment polarity. Following previous work (Zhang et al., 2015), we manually collected 29 negations2 and designed two binary features. One is to indicate whether there is any negation in the tweet and the other is to record whether this tweet contains more than one negation. – the ratio of positive words to all words. – the ratio of negative words to all words. – the maximum sentiment scores. – the minimum sentiment scores. – the sum of sentiment scores. – the sentiment score of the last word in tweet. • NER Given a tweet “@JackHoward the Christmas episode genuinely had me in tears of laughter”, it has useful information like person name and festival whic"
S18-1068,S18-1003,0,0.0242787,", we extract the following three types of features to capture effective information from the given tweets, i.e., linguistic features, sentiment lexicon features and tweet specific features. 2.1.1 Linguistic Features • N-grams: We extract 3 types of Bag-ofWords features as N-grams features, where N = 1,2,3 (i.e., unigram, bigram, and trigram features). Introduction Visual icons play a crucial role in providing information about the extra level of social media information. SemEval 2018 shared task for researchers to predict, given a tweet in English or Spanish, its most likely associated emoji (Barbieri et al., 2018, 2017) (Task 2, Multilingual Emoji Prediction), which is organized into two optional subtask (subtask 1 and subtask 2) respectively in English and Spanish. For subtask 1, we adopt a combination model to predict emojis, which consists of traditional Natural Language Processing (NLP) methods and deep learning methods. The results returned by the classifier with traditional NLP features, by the neural network model and by the combination model are voted to get the final result. For subtask 2, we only use deep learning model. 2 Traditional NLP Features • POS: Generally, the sentences carrying sub"
S18-1068,P15-1162,0,0.0893104,"Missing"
S18-1068,P14-5010,0,0.00448513,"Missing"
S18-1068,E17-2017,0,0.158018,"Missing"
S18-1165,S18-1117,0,0.051104,"abel positive positive negative negative Table 1: Examples from the training data. ing methods which use PMI features and WordNet features. In recent years, more and more studies have focused on word embeddings as an alternative to traditional hand-crafted features (Pennington et al., 2014; Tang et al., 2014). Therefore we use word embeddings to obtain the semantic similarity as word embedding features. Besides, we perform a series of experiments to explore the effectiveness of feature types and supervised machine learning algorithms. Introduction The Capturing Discriminative Attributes task (Paperno et al., 2018) in SemEval 2018 is to provide a standard testbed for semantic difference detection, which will benefit many other applications in Natural Language Processing (NLP), such as automatized lexicography and machine translation (Krebs and Paperno, 2016). The goal of this task is to predict whether a word is a discriminative attribute between two concepts. Specifically, given two concepts and an attribute, the task is to predict whether the first concept has this attribute but the second concept does not. For example, given the concepts apple and pineapple, participants are required to predict wheth"
S18-1165,D14-1162,0,0.0889431,"fference or not. We design and investigate several word embedding features, PMI features and WordNet features together with supervised machine learning methods to address this task. Officially released results show that our system ranks above average. 1 word2 pineapple chandelier coconut cucumber attribute seeds melts brine seeds label positive positive negative negative Table 1: Examples from the training data. ing methods which use PMI features and WordNet features. In recent years, more and more studies have focused on word embeddings as an alternative to traditional hand-crafted features (Pennington et al., 2014; Tang et al., 2014). Therefore we use word embeddings to obtain the semantic similarity as word embedding features. Besides, we perform a series of experiments to explore the effectiveness of feature types and supervised machine learning algorithms. Introduction The Capturing Discriminative Attributes task (Paperno et al., 2018) in SemEval 2018 is to provide a standard testbed for semantic difference detection, which will benefit many other applications in Natural Language Processing (NLP), such as automatized lexicography and machine translation (Krebs and Paperno, 2016). The goal of this ta"
S18-1165,P13-4028,0,0.0266012,"eatures Pointwise mutual information (PMI) (Church and Hanks, 1990) is a measure of association between two things used in information theory and statistics. And in NLP, this metric can be used to measure the correlation between two words. The higher the PMI, the stronger the correlation between the two words. So we obtain the PMI features of the given triple (word1 , word2 , attribute). We record the PMI value of word1 and attribute as well as the PMI value of word2 and attribute as PMI features. The PMI values we used are calculated using Wikimedia dumps1 and directly obtained from SEMILAR (Rus et al., 2013). As a result, we get four PMI features. 2.1.3 Word Embedding Features Word embedding is a continuous-valued vector representation for each word, which usually carries syntactic and semantic information. In this work, we employ two types of word embeddings which are pre-trained word vectors downloaded from Internet with dimensionality of 300: GoogleW2V (Mikolov et al., 2013) and GloVe (Pennington et al., 2014). The former is pretrained on News domain, available in Google2 . And the latter is pre-trained on tweets, available in GloVe3 . • WE similarity: 1 Given the triple (word1 , https://dumps"
S18-1165,P14-1146,0,0.0198255,"n and investigate several word embedding features, PMI features and WordNet features together with supervised machine learning methods to address this task. Officially released results show that our system ranks above average. 1 word2 pineapple chandelier coconut cucumber attribute seeds melts brine seeds label positive positive negative negative Table 1: Examples from the training data. ing methods which use PMI features and WordNet features. In recent years, more and more studies have focused on word embeddings as an alternative to traditional hand-crafted features (Pennington et al., 2014; Tang et al., 2014). Therefore we use word embeddings to obtain the semantic similarity as word embedding features. Besides, we perform a series of experiments to explore the effectiveness of feature types and supervised machine learning algorithms. Introduction The Capturing Discriminative Attributes task (Paperno et al., 2018) in SemEval 2018 is to provide a standard testbed for semantic difference detection, which will benefit many other applications in Natural Language Processing (NLP), such as automatized lexicography and machine translation (Krebs and Paperno, 2016). The goal of this task is to predict whe"
S18-1165,J90-1003,0,0.476851,"refore, we design the following features to record the semantic information. Given the triple (word1 , word2 , attribute), we first load all senses definitions of word1 , word2 and attribute. Then we implement four types of binary features: (1) whether attribute appears in the senses definitions of word1 , (2) whether attribute appears in the senses definitions of word2 , (3) whether word1 appears in the senses definitions of attribute and (4) whether word2 appears in the senses definitions of attribute. As a result, we get four features. 2.1.2 PMI Features Pointwise mutual information (PMI) (Church and Hanks, 1990) is a measure of association between two things used in information theory and statistics. And in NLP, this metric can be used to measure the correlation between two words. The higher the PMI, the stronger the correlation between the two words. So we obtain the PMI features of the given triple (word1 , word2 , attribute). We record the PMI value of word1 and attribute as well as the PMI value of word2 and attribute as PMI features. The PMI values we used are calculated using Wikimedia dumps1 and directly obtained from SEMILAR (Rus et al., 2013). As a result, we get four PMI features. 2.1.3 Wor"
S18-1165,W16-2509,0,0.0452318,"and-crafted features (Pennington et al., 2014; Tang et al., 2014). Therefore we use word embeddings to obtain the semantic similarity as word embedding features. Besides, we perform a series of experiments to explore the effectiveness of feature types and supervised machine learning algorithms. Introduction The Capturing Discriminative Attributes task (Paperno et al., 2018) in SemEval 2018 is to provide a standard testbed for semantic difference detection, which will benefit many other applications in Natural Language Processing (NLP), such as automatized lexicography and machine translation (Krebs and Paperno, 2016). The goal of this task is to predict whether a word is a discriminative attribute between two concepts. Specifically, given two concepts and an attribute, the task is to predict whether the first concept has this attribute but the second concept does not. For example, given the concepts apple and pineapple, participants are required to predict whether the attribute seeds characterizes the first concept but not the other. In other words, semantic difference detection is a binary classification task: given a triple (apple, pineapple, seeds), the task is to determine whether it exemplifies a sem"
S18-1175,D13-1020,0,0.180597,", this multiple-choice machine comprehension task can be expressed as a quadruple:&lt;D, Q, A, a&gt;. Where D represents a narrative text about everyday activities, Q represents a question for the content of the narrative text, A is the candidate answer choice set to the question(this task contains two candidate answers choice a0 and a1 ) and a represents the correct answer. The system is expected to select an answer from A that best answers Q according to the evidences in document D or commonsense knowledge. 2.2 Two Rule-based Baselines First of all, we implemented a rule-based system proposed in (Richardson et al., 2013), which used the sliding-window (SW) and word distance-based (WD) algorithms to calculate the answer scores according to the rules and return the highest-score answer. We also tried the improved SW and WD algorithms proposed in (Smith et al., 2015), and the system performance has improvement. Sliding-window and Word Distance-based algorithms are are described as follows: Sliding-Window: Given a data sample &lt;D, Q, a0 (or a1 ), a&gt;, firstly, we calculate the inverse word counts of each word in the document D. Then we set a window that slides word by word from the beginning of the document to the"
S18-1175,D15-1197,0,0.0214483,"e set to the question(this task contains two candidate answers choice a0 and a1 ) and a represents the correct answer. The system is expected to select an answer from A that best answers Q according to the evidences in document D or commonsense knowledge. 2.2 Two Rule-based Baselines First of all, we implemented a rule-based system proposed in (Richardson et al., 2013), which used the sliding-window (SW) and word distance-based (WD) algorithms to calculate the answer scores according to the rules and return the highest-score answer. We also tried the improved SW and WD algorithms proposed in (Smith et al., 2015), and the system performance has improvement. Sliding-window and Word Distance-based algorithms are are described as follows: Sliding-Window: Given a data sample &lt;D, Q, a0 (or a1 ), a&gt;, firstly, we calculate the inverse word counts of each word in the document D. Then we set a window that slides word by word from the beginning of the document to the end. When the window slides to a position, the sum of inverse word counts of all the words that appears in the question Q or the candidate choice a0 (or a1 ) is the score of the window at this moment. Until the window slides to the end of the passa"
S18-1175,P17-1171,0,0.0231758,"ate-of-art GatedAttention Reader which performs well on several datasets. When a sample data &lt;D, Q, A, a&gt; is given, the steps of the model processing this data sample are described below, Figure 1 shows the system. 2.3.1 Passage, Question and Choice Encoder First, each word in D, Q, and choices (two choices in candidate answer set A) is mapped to d-dimendional vector. The 300-dim GloVe embedding (Pennington et al., 2014) is used. For the input word vectors of D, we also include a 5-dim binary feature to indicates the overlap between the ducument and the question(or choices) which inspired by (Chen et al., 2017). Each dimension of the 5-dim binary match feature represent whether the word present in the query, in the choice a0 , in the choice a1 , in both question and 1 We use the following equations to estimate how many answers appear entirely in the document: if |answer word ∩ document word|/|answer word |= 1, it means the answer appears entirely in the document, where |A |means size of set A. Then we calculate |ansce |/|ansc |, where ansce means correct answers which entirely appeared in document, and ansc means correct answers. The percentage of the correct answers entirely appeared in document is"
S18-1175,L18-1564,0,0.0498242,"e-scale reading comprehension corpora has driven the development of technology for machine reading comprehension, and most of these machine comprehension datasets do not need commonsense knowledge to answer questions. The purpose of Machine Comprehension using Commonsense Knowledge task in Semeval 2018 is to provide a platform for finding a way for the machine to better understand the text and enable the machine answer questions based on the text, and encourage participants to make use any external resources (e.g., DeScript, narrative chains, Wikipedia, etc) to improve the system performance (Ostermann et al., 2018b). The task 11 is a multiple-choice machine comprehension, which requires a system read a narrative text about everyday activities (Ostermann et al., 2018a) and then answer multiple-choice questions based on this text. Some questions need to be answered according to the original text, and others can be answered by commonsense knowledge. Each question is associated with a set of two answers. Table 1 gives an example of the dataset. To address this machine comprehension task, we utilized rule-based methods and a deep learnTable 1: An example from the SemEval2018 task11 dataset. ing method. Our"
S18-1175,D14-1162,0,0.0813454,"that there is a limit to using the above method to improve system performance. Hence we used a deep learning approach to passage representations modeling. Inspired by (Lai et al., 2017), we use the state-of-art GatedAttention Reader which performs well on several datasets. When a sample data &lt;D, Q, A, a&gt; is given, the steps of the model processing this data sample are described below, Figure 1 shows the system. 2.3.1 Passage, Question and Choice Encoder First, each word in D, Q, and choices (two choices in candidate answer set A) is mapped to d-dimendional vector. The 300-dim GloVe embedding (Pennington et al., 2014) is used. For the input word vectors of D, we also include a 5-dim binary feature to indicates the overlap between the ducument and the question(or choices) which inspired by (Chen et al., 2017). Each dimension of the 5-dim binary match feature represent whether the word present in the query, in the choice a0 , in the choice a1 , in both question and 1 We use the following equations to estimate how many answers appear entirely in the document: if |answer word ∩ document word|/|answer word |= 1, it means the answer appears entirely in the document, where |A |means size of set A. Then we calcula"
S18-1184,D15-1075,0,0.0513053,"Missing"
S18-1184,P17-1152,0,0.137799,"Missing"
S18-1184,N18-1175,0,0.032383,"e window size, wj is the parameter of a filter, m is the number of the filters. We also adopt padding before the convolution operation. As a result, we obtain the spatial representations x0i ∈ Rm , which has the same length as the input sequence. After that, we utilize a bi-directional LSTM (BiLSTM) to obtain the temporal information. For each time step t, the LSTM unit computation corresponds to : it = σ(Wi x0t + Ui ht−1 + bi ) ft = ot = c˜t = σ(Wf x0t + Uf ht−1 + bf ) σ(Wo x0t + Uo ht−1 + bo ) tanh(Wc x0t + Uc ht−1 + bc ) ct = it c˜t + ft ct−1 ht = ot tanh(ct ) (3) (4) (7) (8) Inspired from Habernal et al. (2018a), we use an intra-temporal attention function to attend over specific parts of the input sequence. This kind of attention encourages the model to generate different representation according to the attention vector. Habernal et al. (2018a) have shown that such intra-temporal attention outperforms standard attention. We define va as the attention vector, and ht as the hidden states at time step t: ho = ReLU(Wo [attW0 , attW1 ]) pˆ = Softmax(Wp ho ) (10) ht = ht at (11) where at is the attention weights over the hidden states ht , is element-wise multiplication. (12) (13) As for the optimizatio"
S18-1184,S18-1121,0,0.0641325,"e window size, wj is the parameter of a filter, m is the number of the filters. We also adopt padding before the convolution operation. As a result, we obtain the spatial representations x0i ∈ Rm , which has the same length as the input sequence. After that, we utilize a bi-directional LSTM (BiLSTM) to obtain the temporal information. For each time step t, the LSTM unit computation corresponds to : it = σ(Wi x0t + Ui ht−1 + bi ) ft = ot = c˜t = σ(Wf x0t + Uf ht−1 + bf ) σ(Wo x0t + Uo ht−1 + bo ) tanh(Wc x0t + Uc ht−1 + bc ) ct = it c˜t + ft ct−1 ht = ot tanh(ct ) (3) (4) (7) (8) Inspired from Habernal et al. (2018a), we use an intra-temporal attention function to attend over specific parts of the input sequence. This kind of attention encourages the model to generate different representation according to the attention vector. Habernal et al. (2018a) have shown that such intra-temporal attention outperforms standard attention. We define va as the attention vector, and ht as the hidden states at time step t: ho = ReLU(Wo [attW0 , attW1 ]) pˆ = Softmax(Wp ho ) (10) ht = ht at (11) where at is the attention weights over the hidden states ht , is element-wise multiplication. (12) (13) As for the optimizatio"
S18-1184,W17-5301,0,0.0293064,"Missing"
W13-3601,de-marneffe-etal-2006-generating,0,0.067319,"Missing"
W13-3601,N10-1019,0,0.180097,"r determiners, prepositions, noun form, verb form, subject-verb agreement, pronouns, word choice, sentence structure, punctuation, capitalization, etc. Of all the error types, determiners and prepositions are among 1 Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 1–12, c Sofia, Bulgaria, August 8-9 2013. 2013 Association for Computational Linguistics the most frequent errors made by learners of English. Not surprisingly, much published research on grammatical error correction focuses on article and preposition errors (Han et al., 2006; Gamon, 2010; Rozovskaya and Roth, 2010; Tetreault et al., 2010; Dahlmeier and Ng, 2011b), with relatively less work on correcting word choice errors (Dahlmeier and Ng, 2011a). Article and preposition errors were also the only error types featured in the HOO 2012 shared task. Likewise, although all error types were included in the HOO 2011 shared task, almost all participating teams dealt with article and preposition errors only (besides spelling and punctuation errors). In the CoNLL-2013 shared task, it was felt that the community is now ready to deal with more error types, including noun number, verb fo"
W13-3601,P03-1054,0,0.00544378,"h purposes since June 20111 . All instances of grammatical errors are annotated in NUCLE, and the errors are classified into 27 error types (Dahlmeier et al., 2013). To help participating teams in their preparation for the shared task, we also performed automatic preprocessing of the NUCLE corpus and released the preprocessed form of NUCLE. The preprocessing operations performed on the NUCLE essays include sentence segmentation and word tokenization using the NLTK toolkit (Bird et al., 2009), and part-of-speech (POS) tagging, constituency and dependency tree parsing using the Stanford parser (Klein and Manning, 2003; de Marneffe et al., 2006). The error annotations, which are originally at the character level, are then mapped to error annotations at the word token level. Error annotations at the word token 1 level also facilitate scoring, as we will see in Section 4, since our scorer operates by matching tokens. Note that although we released our own preprocessed version of NUCLE, the participating teams were however free to perform their own preprocessing if they so preferred. 3.1.1 Revised version of NUCLE NUCLE release version 2.3 was used in the CoNLL-2013 shared task. In this version, 17 essays were"
W13-3601,D11-1010,1,0.122995,"agreement, pronouns, word choice, sentence structure, punctuation, capitalization, etc. Of all the error types, determiners and prepositions are among 1 Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 1–12, c Sofia, Bulgaria, August 8-9 2013. 2013 Association for Computational Linguistics the most frequent errors made by learners of English. Not surprisingly, much published research on grammatical error correction focuses on article and preposition errors (Han et al., 2006; Gamon, 2010; Rozovskaya and Roth, 2010; Tetreault et al., 2010; Dahlmeier and Ng, 2011b), with relatively less work on correcting word choice errors (Dahlmeier and Ng, 2011a). Article and preposition errors were also the only error types featured in the HOO 2012 shared task. Likewise, although all error types were included in the HOO 2011 shared task, almost all participating teams dealt with article and preposition errors only (besides spelling and punctuation errors). In the CoNLL-2013 shared task, it was felt that the community is now ready to deal with more error types, including noun number, verb form, and subject-verb agreement, besides articles/determiners and prepositio"
W13-3601,P11-1092,1,0.860095,"agreement, pronouns, word choice, sentence structure, punctuation, capitalization, etc. Of all the error types, determiners and prepositions are among 1 Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 1–12, c Sofia, Bulgaria, August 8-9 2013. 2013 Association for Computational Linguistics the most frequent errors made by learners of English. Not surprisingly, much published research on grammatical error correction focuses on article and preposition errors (Han et al., 2006; Gamon, 2010; Rozovskaya and Roth, 2010; Tetreault et al., 2010; Dahlmeier and Ng, 2011b), with relatively less work on correcting word choice errors (Dahlmeier and Ng, 2011a). Article and preposition errors were also the only error types featured in the HOO 2012 shared task. Likewise, although all error types were included in the HOO 2011 shared task, almost all participating teams dealt with article and preposition errors only (besides spelling and punctuation errors). In the CoNLL-2013 shared task, it was felt that the community is now ready to deal with more error types, including noun number, verb form, and subject-verb agreement, besides articles/determiners and prepositio"
W13-3601,D12-1052,1,0.806821,"admit some bad effect which is brought by the new technology, still the advantages of the new technologies cannot be simply discarded. The noun number error effect needs to be corrected (effect → effects). This necessitates the correction of a subject-verb agreement error (is → are). A pipeline system in which corrections for subjectverb agreement errors occur strictly before corrections for noun number errors would not be able to arrive at a fully corrected sentence for this example. The ability to correct multiple, interacting errors is thus necessary in our shared task. The recent work of (Dahlmeier and Ng, 2012a), for example, is designed to deal with multiple, interacting errors. Note that the essays in the training data and the test essays naturally contain grammatical errors of all types, beyond the five error types focused in our shared task. In the automatically corrected essays returned by a participating system, only corrections necessary to correct errors of the five types From past to the present, many important innovations have surfaced. There is an article/determiner error (past → the past) in this sentence. The error annotation, also called correction or edit, in SGML format is shown in"
W13-3601,D10-1094,0,0.289959,", prepositions, noun form, verb form, subject-verb agreement, pronouns, word choice, sentence structure, punctuation, capitalization, etc. Of all the error types, determiners and prepositions are among 1 Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 1–12, c Sofia, Bulgaria, August 8-9 2013. 2013 Association for Computational Linguistics the most frequent errors made by learners of English. Not surprisingly, much published research on grammatical error correction focuses on article and preposition errors (Han et al., 2006; Gamon, 2010; Rozovskaya and Roth, 2010; Tetreault et al., 2010; Dahlmeier and Ng, 2011b), with relatively less work on correcting word choice errors (Dahlmeier and Ng, 2011a). Article and preposition errors were also the only error types featured in the HOO 2012 shared task. Likewise, although all error types were included in the HOO 2011 shared task, almost all participating teams dealt with article and preposition errors only (besides spelling and punctuation errors). In the CoNLL-2013 shared task, it was felt that the community is now ready to deal with more error types, including noun number, verb form, and subject-verb agreem"
W13-3601,W13-3602,0,0.378187,"ion approach ANPSV AN ANPSV ANPSV ANPSV STEL SZEG TILB TOR UAB STAN SJT1 NTHU SAAR KOR NARA ANPSV R M R M T M L L M R M SV AN S ANP AP N SV ANPV A S ANPSV IITB M ANP HIT Approach T Error ANPSV Team CAMB and Manning, 2003). We also make use of the POS tags assigned in the preprocessed form of the test essays. We then assign an error type to a system edit based on the automatically determined POS tags, as follows: evaluated with alternative answers. Not surprisingly, the teams which submitted alternative answers tend to show the greatest improvements in their F1 measure. Overall, the UIUC team (Rozovskaya et al., 2013) achieves the best F1 measure, with a clear lead over the other teams in the shared task, under both evaluation settings (without and with alternative answers). For future research which uses the test data of the CoNLL-2013 shared task, we recommend that evaluation be carried out in the setting that does not use alternative answers, to ensure a fairer evaluation. This is because the scores of the teams which submitted alternative answers tend to be higher in a biased way when evaluated with alternative answers. Rank 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 Team UIUC NTHU UMC NARA HIT STEL CAM"
W13-3601,N12-1067,1,0.697277,"admit some bad effect which is brought by the new technology, still the advantages of the new technologies cannot be simply discarded. The noun number error effect needs to be corrected (effect → effects). This necessitates the correction of a subject-verb agreement error (is → are). A pipeline system in which corrections for subjectverb agreement errors occur strictly before corrections for noun number errors would not be able to arrive at a fully corrected sentence for this example. The ability to correct multiple, interacting errors is thus necessary in our shared task. The recent work of (Dahlmeier and Ng, 2012a), for example, is designed to deal with multiple, interacting errors. Note that the essays in the training data and the test essays naturally contain grammatical errors of all types, beyond the five error types focused in our shared task. In the automatically corrected essays returned by a participating system, only corrections necessary to correct errors of the five types From past to the present, many important innovations have surfaced. There is an article/determiner error (past → the past) in this sentence. The error annotation, also called correction or edit, in SGML format is shown in"
W13-3601,P10-2065,1,0.493158,"verb form, subject-verb agreement, pronouns, word choice, sentence structure, punctuation, capitalization, etc. Of all the error types, determiners and prepositions are among 1 Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 1–12, c Sofia, Bulgaria, August 8-9 2013. 2013 Association for Computational Linguistics the most frequent errors made by learners of English. Not surprisingly, much published research on grammatical error correction focuses on article and preposition errors (Han et al., 2006; Gamon, 2010; Rozovskaya and Roth, 2010; Tetreault et al., 2010; Dahlmeier and Ng, 2011b), with relatively less work on correcting word choice errors (Dahlmeier and Ng, 2011a). Article and preposition errors were also the only error types featured in the HOO 2012 shared task. Likewise, although all error types were included in the HOO 2011 shared task, almost all participating teams dealt with article and preposition errors only (besides spelling and punctuation errors). In the CoNLL-2013 shared task, it was felt that the community is now ready to deal with more error types, including noun number, verb form, and subject-verb agreement, besides articles/de"
W13-3601,W13-1703,1,0.717953,"ince there are five error types in our shared task compared to two in HOO 2012, there is a greater chance of encountering multiple, interacting errors in a sentence in our shared task. This increases the complexity of our shared task relative to that of HOO 2012. To illustrate, consider the following sentence: are made. The other errors are to be left uncorrected. 3 Data This section describes the training and test data released to each participating team in our shared task. 3.1 Training Data The training data provided in our shared task is the NUCLE corpus, the NUS Corpus of Learner English (Dahlmeier et al., 2013). As noted by (Leacock et al., 2010), the lack of a manually annotated and corrected corpus of English learner texts has been an impediment to progress in grammatical error correction, since it prevents comparative evaluations on a common benchmark test data set. NUCLE was created precisely to fill this void. It is a collection of 1,414 essays written by students at the National University of Singapore (NUS) who are non-native speakers of English. The essays were written in response to some prompts, and they cover a wide range of topics, such as environmental pollution, health care, etc. The g"
W13-3601,P11-1019,0,0.412215,"Missing"
W13-3601,W11-2838,0,0.650871,"the various approaches adopted by the participating teams, and present the evaluation results. 1 Introduction Grammatical error correction is the shared task of the Seventeenth Conference on Computational Natural Language Learning in 2013 (CoNLL2013). In this task, given an English essay written by a learner of English as a second language, the goal is to detect and correct the grammatical errors present in the essay, and return the corrected essay. This task has attracted much recent research interest, with two shared tasks Helping Our Own (HOO) 2011 and 2012 organized in the past two years (Dale and Kilgarriff, 2011; Dale et al., 2012). In contrast to previous CoNLL shared tasks which focused on particular subtasks of natural language processing, such as named entity recognition, semantic role labeling, dependency parsing, or coreference resolution, grammatical error correction aims at building a complete end-to-end application. This task is challenging since for many error types, current grammatical error correction systems do not achieve high performance and much research is still needed. Also, tackling this task has far-reaching impact, since it is estimated that hundreds of millions of people worldwi"
W13-3601,W12-2006,0,0.751371,"ted by the participating teams, and present the evaluation results. 1 Introduction Grammatical error correction is the shared task of the Seventeenth Conference on Computational Natural Language Learning in 2013 (CoNLL2013). In this task, given an English essay written by a learner of English as a second language, the goal is to detect and correct the grammatical errors present in the essay, and return the corrected essay. This task has attracted much recent research interest, with two shared tasks Helping Our Own (HOO) 2011 and 2012 organized in the past two years (Dale and Kilgarriff, 2011; Dale et al., 2012). In contrast to previous CoNLL shared tasks which focused on particular subtasks of natural language processing, such as named entity recognition, semantic role labeling, dependency parsing, or coreference resolution, grammatical error correction aims at building a complete end-to-end application. This task is challenging since for many error types, current grammatical error correction systems do not achieve high performance and much research is still needed. Also, tackling this task has far-reaching impact, since it is estimated that hundreds of millions of people worldwide are learning Engl"
W13-3601,W13-3604,0,\N,Missing
