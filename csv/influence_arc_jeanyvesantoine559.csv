2001.jeptalnrecital-long.18,A97-1012,0,\N,Missing
2001.jeptalnrecital-long.18,P92-1008,0,\N,Missing
2002.jeptalnrecital-long.14,2001.jeptalnrecital-long.18,1,0.803704,"Missing"
2002.jeptalnrecital-poster.12,antoine-etal-2002-predictive,1,0.841931,"Missing"
2002.jeptalnrecital-poster.12,P92-1008,0,0.0758789,"Missing"
2002.jeptalnrecital-poster.12,J99-4003,0,0.0229995,"Missing"
2002.jeptalnrecital-poster.12,P83-1019,0,0.3089,"Missing"
2003.jeptalnrecital-long.1,P97-1033,0,0.341007,"tanées qui, tout en perturbant la reconnaissance, gênent l’identification de la structure des énoncés : • hésitations Je voudrais un aller simple pour euh attendez Rosporden c&apos;est cela • réparations Savez-vous s&apos;il y a un hôtel un hôtel-restaurant dans la gare (répétition) Ce serait donc pour le retour non excusez-moi pour l&apos;aller (correction) • incises Je prends le premier train c&apos;est à dire je suis très pressé hein pour Paris Afin de contourner ces difficultés, certains systèmes cherchent à pré-traiter les énoncés oraux pour les corriger avant leur analyse. Les taux de correction rapportés (Heeman, 1997) restent cependant éloignés d’un traitement réellement robuste. 3 Compréhension sélective de la parole Une réponse aux difficultés de traitement du langage parlé consiste à profiter du caractère finalisé du dialogue pour développer des approches orientées par la tâche. Dans ce cas, la compréhension se limite à l’identification d’îlots-clés représentant des unités de sens pertinentes dans l’univers de la tâche ou utiles à la gestion du canal de communication. On parle alors de compréhension sélective restreinte au « sens utile » de l&apos;énoncé (Pérennou, 1996). Celui-ci est représenté par un ensem"
2003.jeptalnrecital-long.1,P98-2143,0,0.0873791,"Missing"
2003.jeptalnrecital-long.1,P98-2236,0,0.0191163,"laboratoire mettent en œuvre une stratégie d’analyse qui applique directement les principes du TAL robuste. Le domaine d’application retenu est le renseignement touristique. Il s’agit d’une tâche présentant une complexité relativement importante, comme en témoignent les ambiguïtés lexicales rencontrées dans nos corpus. 4.1 Caractéristiques communes des systèmes ROMUS et LOGUS Tout en reposant sur des techniques différentes, ROMUS et LOGUS suivent la même stratégie d’analyse incrémentale. Elle implique une succession d’étapes (figure 3) assez classique en TAL robuste et que l’on retrouve chez (Zechner, 1998) pour le langage oral. Nous avons toutefois apporté plusieurs modifications à cette stratégie générale pour répondre à la fois au caractère spontané de l’élocution et à la nature finalisée du dialogue mis en jeu. Séquence(s) de mots étiquetage liste de mots typés segmentation (chunks) liste de chunks typés dépendances sémanti ques structure sémantique Figure 3 — Architecture générique des systèmes de compréhension LOGUS et ROMUS Etiquetage — Cette étape consiste à associer à chaque mot une partie du discours. Plus précisément, on utilise un jeu de catégories syntaxiques associées éventuellemen"
2006.jeptalnrecital-poster.27,2002.jeptalnrecital-poster.1,1,0.751425,"Missing"
2006.jeptalnrecital-poster.27,J93-2001,0,0.148403,"Missing"
2006.jeptalnrecital-poster.27,W03-2501,0,0.0286339,"Missing"
2006.jeptalnrecital-poster.27,2004.jeptalnrecital-poster.20,1,0.835734,"Missing"
2008.jeptalnrecital-court.12,2003.jeptalnrecital-long.1,1,0.787255,"Missing"
2008.jeptalnrecital-court.12,P92-1008,0,0.311342,"Missing"
2008.jeptalnrecital-court.12,1997.iwpt-1.12,0,0.187069,"Missing"
2008.jeptalnrecital-court.12,P83-1019,0,0.407624,"Missing"
2009.jeptalnrecital-court.13,H92-1086,0,0.14575,"Missing"
2009.jeptalnrecital-court.13,N04-1026,0,0.0587363,"Missing"
2010.jeptalnrecital-court.17,N04-1026,0,0.0690582,"Missing"
2010.jeptalnrecital-court.17,W09-0507,1,0.812083,"Missing"
2010.jeptalnrecital-court.34,H05-1062,0,0.425948,"Missing"
2010.jeptalnrecital-court.34,nouvel-etal-2010-analysis,1,0.862545,"Missing"
2011.jeptalnrecital-court.1,S07-1013,0,0.0948878,"Missing"
2011.jeptalnrecital-court.1,W10-0212,0,0.0572838,"Missing"
2017.jeptalnrecital-court.1,F12-2024,1,0.858093,"Missing"
2017.jeptalnrecital-court.1,C86-1001,0,0.547911,"Missing"
2017.jeptalnrecital-court.1,2010.jeptalnrecital-invite.1,0,0.108359,"Missing"
2017.jeptalnrecital-court.1,L16-1262,0,0.0611543,"Missing"
2017.jeptalnrecital-court.1,W17-1704,1,0.869636,"Missing"
2017.jeptalnrecital-long.11,E14-1058,1,0.887039,"Missing"
2017.jeptalnrecital-long.11,W10-1806,0,0.0428931,"Missing"
2017.jeptalnrecital-long.11,C16-1286,0,0.025862,"Missing"
2017.jeptalnrecital-long.11,L16-1602,1,0.82506,"Missing"
2019.jeptalnrecital-court.19,rello-etal-2014-dyslist,0,0.0610316,"Missing"
2019.jeptalnrecital-court.19,D09-1093,0,0.0255099,"Missing"
2020.coling-main.296,bouamor-etal-2012-identifying,0,0.0316025,"iring up to millions of parameters. We also expect the method to generalize over many languages since the properties it exploits proved generic by the state-of-the-art studies. Details of this method and an illustration of the whole set of filters are exposed in the next section. Extraction and filtering techniques have been employed in the past (Constant et al., 2017, Sec. 3.2.1). Dictionary lookup was performed using lemmas, POS and distance filters prior to machine translation (Carpuat and Diab, 2010; Ramisch et al., 2013). In bilingual MWE lexicon discovery, filters can be applied before (Bouamor et al., 2012) or after extraction (Caseli et al., 2010; Semmar, 2018). Filters can be implemented using finite-state machines (Silberztein, 1997; Savary, 2009) and parameterized using classifiers (Pasquer et al., 2018b; Pasquer et al., 2020a). Our originality lies in (a) proposing a highly interpretable method based on on/off filters, (b) evaluating it on several categories of seen VMWEs in 11 languages, and (c) outperforming less interpretable state-of-the-art machine learning models. 4 Filter-Based Seen2020 System (5) LaDET lumièreNOUN.sing seraAUX faiteVERB sur ce drame. ‘The light will be done on this"
2020.coling-main.296,N10-1029,0,0.0376178,"e-of-the-art systems based on complex architectures, including advanced machine/deep learning techniques, and requiring up to millions of parameters. We also expect the method to generalize over many languages since the properties it exploits proved generic by the state-of-the-art studies. Details of this method and an illustration of the whole set of filters are exposed in the next section. Extraction and filtering techniques have been employed in the past (Constant et al., 2017, Sec. 3.2.1). Dictionary lookup was performed using lemmas, POS and distance filters prior to machine translation (Carpuat and Diab, 2010; Ramisch et al., 2013). In bilingual MWE lexicon discovery, filters can be applied before (Bouamor et al., 2012) or after extraction (Caseli et al., 2010; Semmar, 2018). Filters can be implemented using finite-state machines (Silberztein, 1997; Savary, 2009) and parameterized using classifiers (Pasquer et al., 2018b; Pasquer et al., 2020a). Our originality lies in (a) proposing a highly interpretable method based on on/off filters, (b) evaluating it on several categories of seen VMWEs in 11 languages, and (c) outperforming less interpretable state-of-the-art machine learning models. 4 Filter-"
2020.coling-main.296,J17-4005,1,0.912976,"Missing"
2020.coling-main.296,N19-1423,0,0.00895363,"WE-based F-score. It outperformed 6 other open track systems, notably those using complex neural architectures and contextual word embeddings. It scored best (F=0.65), across both tracks in Italian, and second, with less than 0.01 point F-score difference behind the best (open track) system in Polish, Portuguese and Swedish (global F=0.82, F=0.73 and F=0.71). Also for phenomenon-specific measures Seen2Seen scored second across both tracks on both discontinuous and seen VMWEs. The only (open) system which outperformed Seen2Seen is a deep learning system using a generic multilingual BERT model (Devlin et al., 2019) tuned for joined parsing and VMWE identification. It scored a bit less than 0.04 F-measure point higher in the general ranking. Together with Seen2Seen, we submitted another system, Seen2Unseen, which relies on the former for seen VMWEs and adds discovery methods to cover unseen VMWE (Pasquer et al., 2020b). 6 Interpretability and Generalization Our method proves encouraging. Not only does it outperform state-of-the-art systems, even those in the open track, but also it is interpretable. First, it is straightforward to identify the filters responsible for errors made by the system, enabling i"
2020.coling-main.296,J09-1005,0,0.119582,"Missing"
2020.coling-main.296,2020.lrec-1.497,0,0.051097,"Missing"
2020.coling-main.296,W18-4932,1,0.834419,"n illustration of the whole set of filters are exposed in the next section. Extraction and filtering techniques have been employed in the past (Constant et al., 2017, Sec. 3.2.1). Dictionary lookup was performed using lemmas, POS and distance filters prior to machine translation (Carpuat and Diab, 2010; Ramisch et al., 2013). In bilingual MWE lexicon discovery, filters can be applied before (Bouamor et al., 2012) or after extraction (Caseli et al., 2010; Semmar, 2018). Filters can be implemented using finite-state machines (Silberztein, 1997; Savary, 2009) and parameterized using classifiers (Pasquer et al., 2018b; Pasquer et al., 2020a). Our originality lies in (a) proposing a highly interpretable method based on on/off filters, (b) evaluating it on several categories of seen VMWEs in 11 languages, and (c) outperforming less interpretable state-of-the-art machine learning models. 4 Filter-Based Seen2020 System (5) LaDET lumièreNOUN.sing seraAUX faiteVERB sur ce drame. ‘The light will be done on this drama’⇒‘Light will be shed on this drama.’ (VID) (6) LaDET porteNOUN aAUX résolumentADV étéAUX ferméeVERB aux initiatives. ‘The door was firmly closed on initiatives.’ (VID) (7) Il fermeVERB laDET porteNO"
2020.coling-main.296,C18-1219,1,0.745152,"n illustration of the whole set of filters are exposed in the next section. Extraction and filtering techniques have been employed in the past (Constant et al., 2017, Sec. 3.2.1). Dictionary lookup was performed using lemmas, POS and distance filters prior to machine translation (Carpuat and Diab, 2010; Ramisch et al., 2013). In bilingual MWE lexicon discovery, filters can be applied before (Bouamor et al., 2012) or after extraction (Caseli et al., 2010; Semmar, 2018). Filters can be implemented using finite-state machines (Silberztein, 1997; Savary, 2009) and parameterized using classifiers (Pasquer et al., 2018b; Pasquer et al., 2020a). Our originality lies in (a) proposing a highly interpretable method based on on/off filters, (b) evaluating it on several categories of seen VMWEs in 11 languages, and (c) outperforming less interpretable state-of-the-art machine learning models. 4 Filter-Based Seen2020 System (5) LaDET lumièreNOUN.sing seraAUX faiteVERB sur ce drame. ‘The light will be done on this drama’⇒‘Light will be shed on this drama.’ (VID) (6) LaDET porteNOUN aAUX résolumentADV étéAUX ferméeVERB aux initiatives. ‘The door was firmly closed on initiatives.’ (VID) (7) Il fermeVERB laDET porteNO"
2020.coling-main.296,2020.mwe-1.16,1,0.896122,"hole set of filters are exposed in the next section. Extraction and filtering techniques have been employed in the past (Constant et al., 2017, Sec. 3.2.1). Dictionary lookup was performed using lemmas, POS and distance filters prior to machine translation (Carpuat and Diab, 2010; Ramisch et al., 2013). In bilingual MWE lexicon discovery, filters can be applied before (Bouamor et al., 2012) or after extraction (Caseli et al., 2010; Semmar, 2018). Filters can be implemented using finite-state machines (Silberztein, 1997; Savary, 2009) and parameterized using classifiers (Pasquer et al., 2018b; Pasquer et al., 2020a). Our originality lies in (a) proposing a highly interpretable method based on on/off filters, (b) evaluating it on several categories of seen VMWEs in 11 languages, and (c) outperforming less interpretable state-of-the-art machine learning models. 4 Filter-Based Seen2020 System (5) LaDET lumièreNOUN.sing seraAUX faiteVERB sur ce drame. ‘The light will be done on this drama’⇒‘Light will be shed on this drama.’ (VID) (6) LaDET porteNOUN aAUX résolumentADV étéAUX ferméeVERB aux initiatives. ‘The door was firmly closed on initiatives.’ (VID) (7) Il fermeVERB laDET porteNOUN.sing à une loi. ‘He"
2020.coling-main.296,2013.mtsummit-wmwumttt.8,0,0.0133367,"ed on complex architectures, including advanced machine/deep learning techniques, and requiring up to millions of parameters. We also expect the method to generalize over many languages since the properties it exploits proved generic by the state-of-the-art studies. Details of this method and an illustration of the whole set of filters are exposed in the next section. Extraction and filtering techniques have been employed in the past (Constant et al., 2017, Sec. 3.2.1). Dictionary lookup was performed using lemmas, POS and distance filters prior to machine translation (Carpuat and Diab, 2010; Ramisch et al., 2013). In bilingual MWE lexicon discovery, filters can be applied before (Bouamor et al., 2012) or after extraction (Caseli et al., 2010; Semmar, 2018). Filters can be implemented using finite-state machines (Silberztein, 1997; Savary, 2009) and parameterized using classifiers (Pasquer et al., 2018b; Pasquer et al., 2020a). Our originality lies in (a) proposing a highly interpretable method based on on/off filters, (b) evaluating it on several categories of seen VMWEs in 11 languages, and (c) outperforming less interpretable state-of-the-art machine learning models. 4 Filter-Based Seen2020 System ("
2020.coling-main.296,N19-1275,0,0.0479994,"Missing"
2020.coling-main.296,W19-5110,1,0.801388,"if a VMWE has previously been observed in a training corpus or in a lexicon, it can re-appear in morphosyntactically diverse forms. Examples (1–2) show two occurrences of a VMWE with variation in the components’ inflection (cutting vs. cut), word order, presence of discontinuities (were), and syntactic relations (obj vs. nsubj). (1) Some companies were cutting cornersobj to save costs. (2) The field would look uneven if cornersnsubj were cut. However, unrestricted variability is not a reasonable assumption either, since it may lead to literal or coincidental occurrences of VMWEs’ components (Savary et al., 2019b), as in (3) and (4), respectively.2 (3) Start with cutting one corner of the disinfectant bag. :::::: :::::: (4) If you ::: cut along these lines, you’ll get two acute ::::::: corners. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 Henceforth, the lexicalized components of a MWE, i.e. those always realized by the same lexemes, appear in bold. 2 Henceforth, literal and coincidental occurrences are highlighted with wavy underlining, following Savary et al. (2019b). 3333 Proceedings of the 28"
2020.coling-main.296,L18-1047,0,0.0242849,"generalize over many languages since the properties it exploits proved generic by the state-of-the-art studies. Details of this method and an illustration of the whole set of filters are exposed in the next section. Extraction and filtering techniques have been employed in the past (Constant et al., 2017, Sec. 3.2.1). Dictionary lookup was performed using lemmas, POS and distance filters prior to machine translation (Carpuat and Diab, 2010; Ramisch et al., 2013). In bilingual MWE lexicon discovery, filters can be applied before (Bouamor et al., 2012) or after extraction (Caseli et al., 2010; Semmar, 2018). Filters can be implemented using finite-state machines (Silberztein, 1997; Savary, 2009) and parameterized using classifiers (Pasquer et al., 2018b; Pasquer et al., 2020a). Our originality lies in (a) proposing a highly interpretable method based on on/off filters, (b) evaluating it on several categories of seen VMWEs in 11 languages, and (c) outperforming less interpretable state-of-the-art machine learning models. 4 Filter-Based Seen2020 System (5) LaDET lumièreNOUN.sing seraAUX faiteVERB sur ce drame. ‘The light will be done on this drama’⇒‘Light will be shed on this drama.’ (VID) (6) LaD"
2020.coling-main.296,W18-4930,0,0.0409915,"Missing"
2020.coling-main.296,W18-4931,0,0.061361,"Missing"
2020.jeptalnrecital-eternal.2,N13-2001,0,0.0460997,"Missing"
2020.jeptalnrecital-eternal.2,H05-1004,0,0.18507,"Missing"
2020.jeptalnrecital-eternal.2,P16-1060,0,0.0362345,"Missing"
2020.jeptalnrecital-eternal.2,M95-1025,0,0.878792,"Missing"
2020.jeptalnrecital-eternal.2,W12-4501,0,0.0631789,"Missing"
2020.jeptalnrecital-eternal.2,M95-1005,0,0.842886,"Missing"
2020.lrec-1.652,abeille-barrier-2004-enriching,0,0.225553,"Missing"
2020.lrec-1.652,W17-7413,1,0.814774,"r presents a constituency treebank: ODIL Syntax. This seemingly odd choice is motivated by specific annotation needs. Indeed, ODIL Syntax constitutes the first annotation layer of a larger ressource, ODIL Temp, which describes all the temporal entities and the temporal relations that can be found in the ODIL corpus. The temporal annotation of ODIL Temp relies on an extension of the TimeML standard (ISO, 2012). The main originality of this extended annotation scheme is to delimit temporal mentions not by their minimal chunk, but by the range of the constituency subtree that covers the mention (Antoine et al., 2017). This broader annotation is justified by theoretical reasons. For instance, the resolution of temporal abstract anaphora often needs the consideration of a whole clause or a whole speech turn (Zinsmeister and Dipper, 2010), which cannot be modelled by a lexical head-based annotation such as ISO TimeML. From a practical point of view, the pilot experiments we conducted have shown that the cognitive load required by the manual annotation of temporality is reduced if the phrase-based structure of the utterances is displayed to the annotator. It seems that temporal annotation requires a syntactic"
2020.lrec-1.652,candito-etal-2010-statistical,0,0.0957078,"Missing"
2020.lrec-1.652,2008.jeptalnrecital-long.17,0,0.0686395,"Missing"
2020.lrec-1.652,D11-1067,0,0.381677,"Missing"
2020.lrec-1.652,lacheret-etal-2014-rhapsodie,0,0.0225232,"Missing"
2020.lrec-1.652,L16-1602,1,0.44284,"Missing"
2020.lrec-1.652,nicolas-etal-2002-towards,1,0.594348,"orpora: speech transcripts Although it cannot be considered as a balanced corpus, the ODIL Syntax treebank aims at representing a certain variety of language registers and dialogue situations. It is based on extracts of three corpora of speech transcripts that were built during previous research projects (Table 2). These corpora present different degrees of spontaneity and interactivity. The ESLO corpus is a collection of sociolinguistic interviews with a restricted interactivity (Eshkol-Taravella et al., 2011). Conversely, OTG and Accueil UBS concern highly interactive Human-Human dialogues (Nicolas et al., 2002). These last two corpora differ by the media of interaction: direct conversation for the first and phone call for the latter. All of these corpora are freely distributed under a Creative Commons license (see Sec. 4.4). The figures in Table 2 show the word count of the raw corpus, with 12,355 words (for a total duration of a little more than one hour). The sidelining of phatic expressions prior to the syntactic annotation reduced the number of words that are included in the treebank to 10,295. 4.2. POS distribution As specified in Sec. 2.4, the tagset used for ODIL Syntax is the one used for th"
2020.lrec-1.652,2020.lrec-1.892,1,0.79248,"t is typically the case with expletive (e.g. non referential) pronouns like in the idiomatic structure il y a (‘there is’), as seen in the sentence displayed in Fig. 1. The distribution of each tag is given in Section 4.2, and compared to the distribution of the FTB. 3. The annotation of ODIL Syntax was carried out through an incremental process: the annotation is divided into five successive stages that combine automatic and manual procedures. All the annotation steps are conducted on Contemplata, a generic platform dedicated to treebank annotation that was developed during the ODIL project (Waszczuk et al., 2020). This incremental strategy relieves the cognitive load and has demonstrated its ability to limit the total workload of the annotation process, through appropriate calls of the automatic parsing. The five stages are described as follows: Cleft structure, ellipsis and parenthesis 1. Automatic pre-processing – the first step consists in a pre-processing of the speech transcripts to ease the parsing. We proceed in the sidelining of noises, interjections, and phatic expressions not carrying patent temporal information (for instance, oui (‘yes’), bonjour (‘good morning’)) whereas verbal expressions"
2020.lrec-1.892,P11-2023,0,0.051705,"Missing"
2020.lrec-1.892,D11-1067,0,0.403619,"Missing"
2020.lrec-1.892,L16-1602,1,0.610213,"has been applied on a large variety of languages (Italian, Korean, Romanian, Chinese, French...) with only slight idiomatic adaptations. The Temporal@ODIL project aims at building a new French corpus annotated with both temporal mentions and temporal relations. The primary motivation behind the project is to focus specifically on spontaneous speech, unlike the French Time Bank (Bittar et al., 2011) which targets written text. This leads Temporal@ODIL to review the ISOTimeML standard and propose enhancements, while preserving coherence and upward compatibility with the international standard (Lefeuvre-Halftermeyer et al., 2016). In that respect, the project meets a larger variety of needs in NLP and in corpus linguistics, in addition to offering a coverage of spoken language for temporality annotation. What makes the originality of this proposal is that temporal mentions are delimited not by their minimal chunk (lexical head) but by the range of the syntactic subtree that covers the temporal mention. This large-span annotation challenges the annotator’s ability to delimit the temporal mentions with a satisfactory reliability. To ease this delimitation, we adopt a solution that was investigated for multi-word express"
2020.lrec-1.892,2020.lrec-1.652,1,0.79248,"the role of adjudicator on any file, while keeping their role of annotator on other files. 4.2. Corpus Annotation Process This section describes the actual process of annotation using Contemplata, from the attribution of files to the adjudication of the annotation. At this point, we assume that a new project was created, that corpus samples were uploaded and that user accounts for each participant of the project were created using the Admin account. 4. a manual revision of constituent trees - this last step consists in the revision of the residual errors (if any) in the constituent trees. In (Wang et al., 2020), the syntactic annotation process is thoroughly depicted with five steps, adding the automatic pre-processing of files as a ‘step zero’. This step consists in the sidelining of irrelevant tokens (noises, interjections, and phatic expressions) as described in Sec. 3.1.2. They would appear greyed out in the Context window but are not shown in the trees (see Fig. 1). As soon as the syntactic annotation is validated, the annotators may proceed to a second layer of annotation (semantic relations, temporal entities...) depending on the objectives of the project. We are illustrating two possibilitie"
2020.mwe-1.16,J17-4005,1,0.866711,"Missing"
2020.mwe-1.16,2020.coling-main.296,1,0.778798,"commons.org/licenses/by/4.0/. 1 http://hdl.handle.net/11234/1-3367 2 VMWEs are represented as multisets (i.e. bags of elements with repetition allowed), since the same lemma and/or POS can occur twice, as in appeler un chat un chat ‘to call a cat a cat’⇒‘to call a spade a spade’. 124 Joint Workshop on Multiword Expressions and Electronic Lexicons, pages 124–129 Barcelona, Spain (Online), December 13, 2020. Seen2Seen in a nutshell Seen2Seen is a VMWE identification system dedicated to only those VMWEs which have been previously seen in the training data. Its detailed description is provided in Pasquer et al. (2020), but a brief overview is included here to make the current paper self-contained. Seen2Seen extracts lemma combinations of VMWEs seen in Train, looking for the same combinations (within one sentence) in Test, with an expected high recall. To improve precision, up to eight independent criteria can be used: (1) component lemmas should be disambiguated by their POS, (2) components should appear in specific orders (e.g. the determiner before the noun), (3) the order of “gap” words possibly occurring between components is also considered, (4) components should not be too far from each other in a se"
2020.mwe-1.16,W19-5110,1,0.598673,"seen F-score (i.e. not only for verb-noun constructions) was obtained for Hindi (42.66) and it reached 25.36 in French, which was our main focus. Despite the lower global MWE-based F1score of Seen2Unseen (63.02) compared to Seen2Seen (66.23), we describe the former (Sec. 2), analyse its interesting negative results (Sec. 3), and conclude with ideas for future work (Sec. 4). 2 System Description While describing the architecture of our system, we use the notions of a VMWE token (its occurrence in running text) and a VMWE type (abstraction over all occurrences of a given VMWE), as introduced by Savary et al. (2019b). We represent VMWE types as multisets of lemmas and POS.2 Our system uses a mixture of discovery and identification methods, as defined by Constant et al. (2017). Namely, VMWE discovery consists in generating lists of MWE types out of context, while VMWE identification marks VMWE tokens in running text. The system is freely available online (https://gitlab.com/ cpasquer/st_2020). This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 http://hdl.handle.net/11234/1-3367 2 VMWEs are represented as m"
antoine-etal-2000-obtaining,P97-1035,0,\N,Missing
antoine-etal-2000-obtaining,J96-2004,0,\N,Missing
antoine-etal-2002-predictive,H90-1020,0,\N,Missing
antoine-etal-2002-predictive,J99-4003,0,\N,Missing
antoine-etal-2002-predictive,antoine-etal-2000-obtaining,1,\N,Missing
antoine-etal-2008-automatic,A88-1019,0,\N,Missing
antoine-etal-2008-automatic,paroubek-etal-2006-data,0,\N,Missing
C18-1219,F12-2024,0,0.0517206,"inflection of the noun. Still, modification of the noun and passivization is exhibited by the former (8)–(9) but not by the latter (20). 4 Corpus We study VMWE variability on a corpus of French texts annotated with VMWEs for the PARSEME shared task.3 In addition to VMWE annotation (based on universal guidelines), the corpus contains POS, lemmas, morphological features and dependency structures, which we use in our experiments. The original release of the VMWE-annotated corpus contained two sub-corpora: the French part of the Universal Dependencies v1.4 corpus (Nivre et al., 2016) and Sequoia (Candito and Seddah, 2012). 3 http://multiword.sf.net/sharedtask2017, corpus at http://hdl.handle.net/11372/LRT-2282 2585 Corpus TrC TeC # Sentences 17,880 1,667 All POS patterns # Tokens # VMWEs 450,221 1,584 35,784 291 # occ. 4,462 500 All Verb-(Det-)Noun # VMWEs # occ. 854 2098 177 283 Verb-(Det-)Noun variants # VMWEs # occ. n/a n/a 86 132 Table 1: Number of different VMWE types (# VMWEs) and their token occurrences (# occ.) per POS and variability class in the training corpus (TrC) and in the test corpus (TeC). The tagsets and dependency trees used in both sub-corpora were incompatible. We homogenized them by repla"
C18-1219,P16-1016,0,0.0276659,"Missing"
C18-1219,J17-4005,1,0.755623,"Missing"
C18-1219,J09-1005,0,0.676032,"ocessors combined with rich finite-state patterns (Krstev et al., 2014) or unification meta-grammars (Jacquemin, 2001). Rule-based methods were often combined with statistical measures in the domain of multiword term extraction (Savary and Jacquemin, 2003), and explicitly addressed variation. But they can hardly distinguish literal from idiomatic readings, weakly cover discontinuous MWEs, and cannot generalize to MWEs absent from the lexicon and to new, initially uncovered, variation patterns. Sense disambiguation methods often focus on ambiguous known MWEs and neglect variant identification (Fazly et al., 2009). Sequence taggers learn identification models from annotated data based on regularities in sequences of tokens (Constant et al., 2013; Schneider et al., 2014). They are well suited for continuous seen MWEs and neutralize inflection when lemmas are available. They cope, however, badly with syntactic variation whenever it leads to discontinuities. MWE-aware parsers identify MWEs as a by-product of parsing a sentence (Green et al., 2013; Constant and Nivre, 2016). They often can deal with both morphological and syntactic variants, even though they are usually less accurate for highly discontinuo"
C18-1219,N15-1065,0,0.0281204,"ure, POS tags and lemmas with at least one annotated MWE in the training set. They find out that most systems perform better when the proportion of seen MWEs is high. This result suggests that MWE variant identification remains a hard problem and deserves being addressed explicitly by dedicated methods. 3 VMWE variation in French VMWEs are known to have specific lexical and morpho-syntactic variability profiles: they are more or less variable, but usually not as variable as regular phrases with the same syntactic structure. Therefore, methods used for detecting paraphrases of regular phrases (Fujita and Isabelle, 2015) cannot be straightforwardly applied to VMWEs. Different aspects of variability may be considered. Firstly, MWE-hood is, by nature, a lexical phenomenon, that is, a particular idiomatic reading is available only in presence of a combination of particular lexical units. Replacing one of them by a semantically close lexeme usually leads to the loss of idiomatic reading, e.g. (6) is an idiom but (7) can 2584 only be understood literally. Lexical variability is admitted by some VMWEs, but usually with a very restricted list of equivalents only, as in (11) and (12). In this work, unlike Fazly et al"
C18-1219,J13-1009,0,0.0728331,"Missing"
C18-1219,W17-1715,0,0.051104,"Missing"
C18-1219,L16-1262,0,0.0229078,"Missing"
C18-1219,N18-2068,1,0.909561,"both the literal and the idiomatic meaning can sometimes be preserved under syntactic variation as in (6) vs. (9). Secondly, some types of syntactic variation (e.g. passivization) tend to be exhibited less frequently by VMWEs than by non-VMWEs of the same syntactic structures (Fazly et al., 2009). Thirdly, some types of syntactic dependencies can be specific to some VMWEs, e.g. (18) involves a compulsory though non-lexicalized adjectival modifier of the noun. This also means that this VMWE admits insertions of external elements between its lexicalized components. Finally, as previously shown (Pasquer et al., 2018), syntactic features are particularly strong indicators for linguistically motivated similarity and flexibility measures of (French) VWMEs. The main challenge in modeling the variability profiles of VMWEs lies in the fact that VMWEs of the same syntactic structure may behave differently. For instance, the VMWEs in (6) and (19) both admit interrogation, insertions and inflection of the verb, and prohibit inflection of the noun. Still, modification of the noun and passivization is exhibited by the former (8)–(9) but not by the latter (20). 4 Corpus We study VMWE variability on a corpus of French"
C18-1219,Q14-1016,0,0.0776456,"Missing"
C18-1219,S16-1084,0,0.0159246,"f words exhibiting unexpected lexical, morphological, syntactic, semantic, pragmatic and/or statistical behavior (Baldwin and Kim, 2010). Most prominently, they are semantically non-compositional, that is, their meaning cannot be deduced from the meanings of their components and from their syntactic structure in a way deemed regular. For this reason, the presence of MWEs in texts calls for dedicated treatment, whose prerequisites include their automatic identification. The goal of MWE identification is, given some input running text, to identify all MWEs’ lexicalized components present in it (Schneider et al., 2016; Savary et al., 2017).1 Such systems face three main This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/ 1 A lexicalized component of a MWE, or component for short – marked in bold in examples – is one which is always realized by the same lexeme. For instance, the noun decision is lexicalized in to make a decision but the determiner a is not, because it can be replaced by other determiners (e.g. to make this/any/the decision). 2582 Proceedings of the 27th International Conference on Computational L"
C18-1219,I13-1024,0,0.0610946,"Missing"
C18-1219,C16-1042,1,0.868377,"d 1.b) or false positives (Fig.1.d). False negatives include occurrences without direct connection like complex determiners (Fig.1.f). The baseline is also sensitive to syntactic annotation errors, as in Fig.1.e (rˆole ‘role’ rather than cl´e ‘key’ should be the head of rˆole cl´e ‘key role’). Despite its simplicity, the baseline is already very strong, as shown in the first line of Tab. 2. It extracts 158 Verb-(Det-)Noun candidates, which – compared to the 132 STVs to be extracted – yield the F-score of 0.88. These results notably confirm previous observations that literal readings are rare (Waszczuk et al., 2016; Savary and Cordeiro, 2018). obj obl acl acl:relcl obj (a) Il joue son propre rôle (b) Chacun a son rôle à jouer (c) Elle jouera dans le film où elle tiendra le rôle de Dorothy ::::: ::: He plays his own role Everyone has a role to play She will-play in the movie where she will-hold the role of Dorothy obj compound obl obj nmod (d) Comme pour son rôle dans Aïda , l’ acteur joue avec son poids (e) Le tact joua un rôle clé (f) Il joue un tas de rôles ::: :::: As with his role in Aida , the actor plays with his weight The tact played a role key He plays a pile of roles Figure 1: True positive (a"
C96-1010,H92-1018,0,\N,Missing
D07-1053,P89-1010,0,0.123667,"re have been 507 attempts to integrate part-of-speech information (Fazly and Hirst, 2003) or more complex syntactic models (Schadle et al, 2004) to achieve a better prediction. In this paper, we will nevertheless limit our study to a standard 4-gram model as a baseline to make our results comparable. Our main aim is here to investigate the use of long-distance semantic dependencies to dynamically adapt the prediction to the current semantic context of communication. Similar work has been done by Li and Hirst (2005) and Matiasek and Baroni (2003), who exploit Pointwise Mutual Information (PMI; Church and Hanks, 1989). Trnka et al. (2005) dynamically interpolate a high number of topic-oriented models in order to adapt their predictions to the current topic of the text or conversation. Classically, word predictors are evaluated by an objective metric called Keystroke Saving Rate (ksr):  k  ksrn = 1 − p  ⋅ 100  ka  (1) with kp, ka being the number of keystrokes needed on the input device when typing a message with (kp) and without prediction (ka = number of characters in the text that has been entered, n = length of the prediction list, usually n = 5). As Trost et al. (2005) and Trnka et al. (2005),"
D07-1053,W03-2501,0,0.114665,"that such a model cannot take into account long-distance dependencies. There have been 507 attempts to integrate part-of-speech information (Fazly and Hirst, 2003) or more complex syntactic models (Schadle et al, 2004) to achieve a better prediction. In this paper, we will nevertheless limit our study to a standard 4-gram model as a baseline to make our results comparable. Our main aim is here to investigate the use of long-distance semantic dependencies to dynamically adapt the prediction to the current semantic context of communication. Similar work has been done by Li and Hirst (2005) and Matiasek and Baroni (2003), who exploit Pointwise Mutual Information (PMI; Church and Hanks, 1989). Trnka et al. (2005) dynamically interpolate a high number of topic-oriented models in order to adapt their predictions to the current topic of the text or conversation. Classically, word predictors are evaluated by an objective metric called Keystroke Saving Rate (ksr):  k  ksrn = 1 − p  ⋅ 100  ka  (1) with kp, ka being the number of keystrokes needed on the input device when typing a message with (kp) and without prediction (ka = number of characters in the text that has been entered, n = length of the predictio"
D07-1053,W03-2502,0,0.066093,"be seen in figure 1, the interface of the SIBYLLE system presents such a list of most probable words to the user. Several approaches can be used to carry out word prediction. Most of the commercial AAC systems make only use of a simple lexicon: in this approach, the context is not considered. On the other hand, stochastic language models can provide a list of word suggestions, depending on the n-1 (typically n = 3 or 4) last inserted words. It is obvious that such a model cannot take into account long-distance dependencies. There have been 507 attempts to integrate part-of-speech information (Fazly and Hirst, 2003) or more complex syntactic models (Schadle et al, 2004) to achieve a better prediction. In this paper, we will nevertheless limit our study to a standard 4-gram model as a baseline to make our results comparable. Our main aim is here to investigate the use of long-distance semantic dependencies to dynamically adapt the prediction to the current semantic context of communication. Similar work has been done by Li and Hirst (2005) and Matiasek and Baroni (2003), who exploit Pointwise Mutual Information (PMI; Church and Hanks, 1989). Trnka et al. (2005) dynamically interpolate a high number of top"
D07-1053,2005.jeptalnrecital-recital.1,1,0.670881,"me power (γ is normally between 3 and 8). After normalization we obtain a probability distribution which can be used for prediction purposes. It is calculated as follows: r ( cos( w , h ) − cos (h ) ) ( w h) = r r r ∑ (cos(w , h ) − cos (h )) r PLSA i r i k γ min k γ (3) min wi is a word in the vocabulary, h is the current conr r text (history) wi and h are their corresponding vecr tors in the LSA space; cosmin( h ) returns the lowest r cosine value measured for h ). The denominator then normalizes each similarity value to ensure that ∑P n k LSA 2.4 Measuring relation quality in an LSA space, Wandmacher (2005) pointed out that the reliability of LSA relations varies strongly between terms. He also showed that the entropy of a term does not correlate with relation quality (i.e. number of semantically related terms in an LSA-generated term cluster), but he found a medium correlation (Pearson coeff. = 0.56) between the number of semantically related terms and the average cosine similarity of the m nearest neighbors (density). The closer the nearest neighbors of a term vector are, the more probable it is to find semantically related terms for the given word. In turn, terms having a high density are mor"
D07-1053,J90-1003,0,\N,Missing
devillers-etal-2004-french,H92-1003,0,\N,Missing
devillers-etal-2004-french,P01-1066,0,\N,Missing
devillers-etal-2004-french,antoine-etal-2002-predictive,1,\N,Missing
devillers-etal-2004-french,antoine-etal-2000-obtaining,1,\N,Missing
E14-1058,H05-1073,0,0.0260567,"Missing"
E14-1058,J11-4004,0,0.0144221,"can be drawn from these figures. At first, low inter-coder agreements are observed on affective annotation, which is coherent with many other studies (Devillers and al., 2005; Callejas and Lopez-Cozar, 2008). Non-weighted metrics (multi-κ, multi-π, αb) range from 0.29 to 0.58, depending on the annotation scheme. This confirms that these annotation tasks are prone to high subjectivity. Higher levels of agreement may have been obtained if the annotators were trained with supervision. As said before, this would have reduced the spontaneity of judgment. Furthermore, a comprehensive meta-analysis (Bayerl and Paul, 2011) has shown that no difference may be found on data reliability between experts and novices. The reliability measures given by the weighted version of Krippendorff’s α on the two affective tasks are significantly higher: α values range from 0.57 to 0.80, which suggests a rather sufficient reliability. These results are not an artifact. They come from better disagreement estimation. For instance, the difference between a positive 554 and a negative annotation is more serious than between the positive and the neutral emotion, what a weighted metrics accounts for. Satisfactory measures are found o"
E14-1058,H92-1086,0,0.356791,"Missing"
E14-1058,P02-1053,0,0.0476499,"endorff, 2004) is greatly satisfied for the valence annotation (3 categories). It is approached on the complete annotation where we can assure 4 chance agreements per category. 4.3 Opinion corpus The second experiment concerns opinion annotation. Emotion detection can be related to a certain extent, with opinion mining (or sentiment analysis), whose aim is to detect the attitude of people in the texts they produce. A basic task in opinion mining consists in classifying the polarity of a given text, which should be either a sentence (Wilson and al., 2005), a speech turn or a complete document (Turney, 2002). Polarity plays the same role as valence does for affect analysis: it describes whether the expressed judgment is positive, negative, or neutral. One should also characterize the sentiment strength (Thelwall and al., 2010). This feature can be related to the notion of intensity used in emotional annotation. Both polarity and sentiment strength are considered in our annotation task. This experiment has been carried out on a corpus of film reviews. The reviews were relatively short texts written by ordinary people on dedicated French websites (www.senscritique.com and www.allocine.fr). They con"
E14-1058,W10-0212,0,0.0352745,"Missing"
E14-1058,H05-1044,0,0.0464068,"Missing"
E14-1058,J08-4004,0,\N,Missing
E14-1058,J96-2004,0,\N,Missing
E14-1058,muzerelle-etal-2014-ancor,1,\N,Missing
esteve-etal-2010-epac,bazillon-etal-2008-manual,1,\N,Missing
F13-1031,2010.jeptalnrecital-court.28,0,0.0960915,"Missing"
F13-1031,I11-1142,0,0.0319431,"Missing"
F13-1031,W10-2415,0,0.041731,"Missing"
F13-1031,I11-1058,0,0.0399825,"Missing"
F13-1031,C96-1079,0,0.113499,"Missing"
F13-1031,E99-1001,0,0.0605668,"Missing"
F14-2029,2008.jeptalnrecital-recital.2,0,0.0713565,"Missing"
F14-2029,P11-2023,0,0.0539537,"Missing"
F14-2029,gravier-etal-2012-etape,0,0.027931,"Missing"
F14-2029,2009.jeptalnrecital-court.23,0,0.0442801,"Missing"
F14-2029,J88-2006,0,0.0740919,"Missing"
L16-1602,2008.jeptalnrecital-recital.2,0,0.0706,"Missing"
L16-1602,P11-2023,0,0.132161,"Missing"
L16-1602,landragin-etal-2012-analec,0,0.0727265,"Missing"
L16-1602,teissedre-etal-2010-resources,1,0.888975,"Missing"
L16-1602,W11-0420,0,0.0512914,"Missing"
muzerelle-etal-2014-ancor,hendrickx-etal-2008-coreference,0,\N,Missing
muzerelle-etal-2014-ancor,J00-4005,0,\N,Missing
muzerelle-etal-2014-ancor,passonneau-2004-computing,0,\N,Missing
muzerelle-etal-2014-ancor,W07-1522,0,\N,Missing
muzerelle-etal-2014-ancor,J08-4004,0,\N,Missing
muzerelle-etal-2014-ancor,E14-1058,1,\N,Missing
muzerelle-etal-2014-ancor,nicolas-etal-2002-towards,1,\N,Missing
N18-2068,baldwin-etal-2004-road,0,0.0283401,"Jacquemin, 2001) and hinder straightforward search of MWE citation forms in a corpus (Nissim and Zaninello, 2013). They introduce discontinuities which challenge sequence labeling approaches. Even when employing parsers to cope with discontinuities, MWE recognizers can still fail to capture some syntactic transformations such as complex determiners, which can break a direct link between a verb and a noun in a dependency tree (pay a series of visits). These facts have important implications for downstream tasks and applications, e.g. parsers can heavily suffer from incorrectly identified MWEs (Baldwin et al., 2004). The restricted variability of MWEs as compared to their regular counterparts can also be seen as an advantage in their automatic discovery (Weller and Heid, 2010; Tsvetkov and Wintner, 2014; Buljan ˇ and Snajder, 2017). Substitution-based MWE discovery techniques based on lexico-semantic variability have been largely explored (Pearce, 2001; Farahmand and Henderson, 2016). Morphological and syntactic variability, however, have rarely been studied for MWE discovery (Ramisch et al., 2008) and even less so for in-context identification (Fazly et al., 2009). Given the importance of MWE variabilit"
N18-2068,W17-1727,0,0.0369668,"Missing"
N18-2068,J17-4005,1,0.880083,"Missing"
N18-2068,W16-1809,0,0.0681629,"Missing"
N18-2068,J14-2007,0,0.0428445,"ches. Even when employing parsers to cope with discontinuities, MWE recognizers can still fail to capture some syntactic transformations such as complex determiners, which can break a direct link between a verb and a noun in a dependency tree (pay a series of visits). These facts have important implications for downstream tasks and applications, e.g. parsers can heavily suffer from incorrectly identified MWEs (Baldwin et al., 2004). The restricted variability of MWEs as compared to their regular counterparts can also be seen as an advantage in their automatic discovery (Weller and Heid, 2010; Tsvetkov and Wintner, 2014; Buljan ˇ and Snajder, 2017). Substitution-based MWE discovery techniques based on lexico-semantic variability have been largely explored (Pearce, 2001; Farahmand and Henderson, 2016). Morphological and syntactic variability, however, have rarely been studied for MWE discovery (Ramisch et al., 2008) and even less so for in-context identification (Fazly et al., 2009). Given the importance of MWE variability (Constant et al., 2017) as well as its gradual nature, especially for VMWEs, we suggest that this phenomenon should be subject to measurement. This paper presents measures of VMWE variabili"
N18-2068,J09-1005,0,0.716401,"fer from incorrectly identified MWEs (Baldwin et al., 2004). The restricted variability of MWEs as compared to their regular counterparts can also be seen as an advantage in their automatic discovery (Weller and Heid, 2010; Tsvetkov and Wintner, 2014; Buljan ˇ and Snajder, 2017). Substitution-based MWE discovery techniques based on lexico-semantic variability have been largely explored (Pearce, 2001; Farahmand and Henderson, 2016). Morphological and syntactic variability, however, have rarely been studied for MWE discovery (Ramisch et al., 2008) and even less so for in-context identification (Fazly et al., 2009). Given the importance of MWE variability (Constant et al., 2017) as well as its gradual nature, especially for VMWEs, we suggest that this phenomenon should be subject to measurement. This paper presents measures of VMWE variability based on variant-to-variant similarity, taking synOne of the outstanding properties of multiword expressions (MWEs), especially verbal ones (VMWEs), important both in theoretical models and applications, is their idiosyncratic variability. Some MWEs are always continuous, while some others admit certain types of insertions. Components of some MWEs are rarely or ne"
N18-2068,weller-heid-2010-extraction,0,0.0227707,"equence labeling approaches. Even when employing parsers to cope with discontinuities, MWE recognizers can still fail to capture some syntactic transformations such as complex determiners, which can break a direct link between a verb and a noun in a dependency tree (pay a series of visits). These facts have important implications for downstream tasks and applications, e.g. parsers can heavily suffer from incorrectly identified MWEs (Baldwin et al., 2004). The restricted variability of MWEs as compared to their regular counterparts can also be seen as an advantage in their automatic discovery (Weller and Heid, 2010; Tsvetkov and Wintner, 2014; Buljan ˇ and Snajder, 2017). Substitution-based MWE discovery techniques based on lexico-semantic variability have been largely explored (Pearce, 2001; Farahmand and Henderson, 2016). Morphological and syntactic variability, however, have rarely been studied for MWE discovery (Ramisch et al., 2008) and even less so for in-context identification (Fazly et al., 2009). Given the importance of MWE variability (Constant et al., 2017) as well as its gradual nature, especially for VMWEs, we suggest that this phenomenon should be subject to measurement. This paper present"
nouvel-etal-2010-analysis,eshkol-etal-2010-eslo,1,\N,Missing
nouvel-etal-2010-analysis,antoine-etal-2008-automatic,1,\N,Missing
W09-0507,W05-1525,0,0.0686284,"Missing"
W09-0507,antoine-etal-2002-predictive,1,0.871674,"Missing"
W09-0507,P98-2236,0,0.102628,"Missing"
W09-0507,C98-2231,0,\N,Missing
W09-0507,devillers-etal-2004-french,1,\N,Missing
W12-0510,W98-1118,0,0.3994,"d systems Considering the complementary behaviors of knowledge-based and datadriven systems for NER, projects have been conducted to investigate how to conciliate both approaches. Work has been done to automatically induce symbolic knowledge (Hingston, 2002; Kushmerick et al., 1997) that may be used as NE taggers. But in most cases, hybridization for NER relies a much simpler principle: outputs of knowledge-based systems are considered as features by a machine learning algorithm. For instance, maximum entropy may be used when a high diversity of knowledge sources are to be taken into account (Borthwick et al., 1998). CRFs also have demonstrated their ability to merge symbolic and statistic processes in a machine learning framework (Zidouni et al., 2010). We propose an approach to combine knowledge-based and data-driven approaches in a modular way. Our first concern is to implement a module that automatically extracts knowledge that should be interoperable with the existing system’s transducers. This is done by focusing, in annotated corpora, more on ‘markers’ (tags) that are to be inserted between tokens (e.g. <pers>, </pers>, <org>, </org>, etc.), than on ‘labels’ assigned to each token, as transducer d"
W12-0510,H05-1062,0,0.0316939,"Missing"
W12-0510,C02-1054,0,0.0537739,"n its ability to reach outstanding levels of performances (Brun & Hag`ege, 2004; Brun & Hag`ege, 2009; van Shooten et al., 2009). Data-driven approaches A large diversity of data-driven approaches have been proposed during the last decade for NER. Generative models such as Hidden Markov Models or stochastic finite state transducers (Miller et al., 1998; Favre et al., 2005) benefit from their ability to take into account the sequential nature of language. On the other hand, discriminative classifiers such as 70 Support Vector Machines (SVMs) are very effective when a large variety of features (Isozaki & Kazawa, 2002) is used, but lack the ability to take a global decision over an entire sentence. Context Random Fields (CRFs) (Lafferty et al., 2001) have enabled NER to benefit from the advantages of both generative and discriminative approaches (McCallum & Li, 2003; Zidouni et al., 2010; B´echet & Charton, 2010). Besides, the robustness of data-driven / machine-learning approaches explains that the latter are more appropriate on noisy data such as ASR transcripts. Hybrid systems Considering the complementary behaviors of knowledge-based and datadriven systems for NER, projects have been conducted to invest"
W12-0510,W03-0430,0,0.0867824,"ve models such as Hidden Markov Models or stochastic finite state transducers (Miller et al., 1998; Favre et al., 2005) benefit from their ability to take into account the sequential nature of language. On the other hand, discriminative classifiers such as 70 Support Vector Machines (SVMs) are very effective when a large variety of features (Isozaki & Kazawa, 2002) is used, but lack the ability to take a global decision over an entire sentence. Context Random Fields (CRFs) (Lafferty et al., 2001) have enabled NER to benefit from the advantages of both generative and discriminative approaches (McCallum & Li, 2003; Zidouni et al., 2010; B´echet & Charton, 2010). Besides, the robustness of data-driven / machine-learning approaches explains that the latter are more appropriate on noisy data such as ASR transcripts. Hybrid systems Considering the complementary behaviors of knowledge-based and datadriven systems for NER, projects have been conducted to investigate how to conciliate both approaches. Work has been done to automatically induce symbolic knowledge (Hingston, 2002; Kushmerick et al., 1997) that may be used as NE taggers. But in most cases, hybridization for NER relies a much simpler principle: o"
W12-0510,nouvel-etal-2010-analysis,1,0.891669,"Corpora • CasEN-CRF: same as CRF, but the output of CasEN is added as a single feature (concatenation of CasEN features) 7 Experimentations 7.1 Corpora and Metrics For experimentations, we use the corpus that has been made available after the Ester2 evaluation campaign. Table 2 gives statistics on diverse subparts of this corpus. Unfortunately, many inconsistencies where noted for manual annotation, especially for ‘Ester2-Train’ part that won’t be used for training. There were fewer irregularities in other parts of the corpus. Although, manual corrections were done on half of the Test corpus (Nouvel et al., 2010) (Ester2-Test-corr in Table 2), to obtain a gold standard that we will use to evaluate our approach. The remaining part of the Test corpus (Ester2-Test-held in Table 2) merged with the Dev part constitute our training set (Ester2-Dev in Table 2), used as well to extract rules with mineXtract, to estimate stochastic model probabilities of mStruct and to learn CRF models. We evaluate systems using following metrics: • detect: rate of detection of the presence of any marker (binary decision) at any position Table 3: Performance of Systems 7.2 Comparing Hybridation with Systems First, we separatel"
W12-0510,M98-1002,0,\N,Missing
W12-0510,P10-1052,0,\N,Missing
W17-7413,I08-2111,0,0.0803105,"Missing"
W17-7413,2011.jeptalnrecital-long.17,0,0.0207918,"g tasks on linguistic data. Temporal annotation has benefitted from the normalization efforts of the ISO TC37/SC4 committee which has led to the definition of the ISO-TimeML standard (ISO 24617-1:2012), following the seminal proposal of Pustejovsky et al. (2003). While originally developed for English, ISO-TimeML has been applied on a large variety of languages (Italian, Korean, Romanian, Chinese...) with only slight idiomatic adaptations. This is a clear indication of its relevance and genericity. Only one French corpus (French Time Bank) has been annotated following the ISO-TimeML standard (Bittar et al., 2011). It was built on an extract of the French Tree Bank, FTB (Abeill´e et al., 2003). Its size is reasonable (15,876 words) for pilot linguistics studies but is too restricted for computational purposes. In addition, the syntactic information that is present in the FTB was not considered during the annotation phase. In this paper, we present Temporal@ODIL, a project which aims precisely at enlarging and deepening the seminal work conducted with the French Time Bank in two directions : - The temporal annotation is not conducted on written text but on speech transcripts. Several language registers"
W17-7413,muzerelle-etal-2014-ancor,1,0.81764,"ot linguistics studies but is too restricted for computational purposes. In addition, the syntactic information that is present in the FTB was not considered during the annotation phase. In this paper, we present Temporal@ODIL, a project which aims precisely at enlarging and deepening the seminal work conducted with the French Time Bank in two directions : - The temporal annotation is not conducted on written text but on speech transcripts. Several language registers are considered, ranging from socio-linguistic interviews to highly interactive dialogues. The annotation is conducted on ANCOR (Muzerelle et al., 2014), the largest French coreference corpus and one of the largest ones of spoken language. Temporal@ODIL provides a complementary annotation layer, allowing studies combining coreference and temporal data. It will concern 20,000 words, which doubles the size of existing French TimeML-based resources. - Temporal@ODIL proposes modifications to the ISO-TimeML standard and its main originality is to delimit temporal mentions not by their minimal chunk but by the range of the syntactic subtree that covers the temporal mention. In order to favor re-usability, we watch out carefully to maintain upward c"
W18-4932,J17-4005,1,0.854135,"Missing"
W18-4932,C18-1219,1,0.558536,"Missing"
wandmacher-antoine-2006-training,W03-2501,0,\N,Missing
wandmacher-antoine-2006-training,J93-2001,0,\N,Missing
