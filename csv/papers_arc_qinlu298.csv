2021.semeval-1.70,{P}oly{U} {CBS}-Comp at {S}em{E}val-2021 Task 1: Lexical Complexity Prediction ({LCP}),2021,-1,-1,5,1,1824,rong xiang,Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021),0,"In this contribution, we describe the system presented by the PolyU CBS-Comp Team at the Task 1 of SemEval 2021, where the goal was the estimation of the complexity of words in a given sentence context. Our top system, based on a combination of lexical, syntactic, word embeddings and Transformers-derived features and on a Gradient Boosting Regressor, achieves a top correlation score of 0.754 on the subtask 1 for single words and 0.659 on the subtask 2 for multiword expressions."
2020.starsem-1.4,Automatic Learning of Modality Exclusivity Norms with Crosslingual Word Embeddings,2020,-1,-1,3,0.338546,180,emmanuele chersoni,Proceedings of the Ninth Joint Conference on Lexical and Computational Semantics,0,"Collecting modality exclusivity norms for lexical items has recently become a common practice in psycholinguistics and cognitive research. However, these norms are available only for a relatively small number of languages and often involve a costly and time-consuming collection of ratings. In this work, we aim at learning a mapping between word embeddings and modality norms. Our experiments focused on crosslingual word embeddings, in order to predict modality association scores by training on a high-resource language and testing on a low-resource one. We ran two experiments, one in a monolingual and the other one in a crosslingual setting. Results show that modality prediction using off-the-shelf crosslingual embeddings indeed has moderate-to-high correlations with human ratings even when regression algorithms are trained on an English resource and tested on a completely unseen language."
2020.lrec-1.14,Affection Driven Neural Networks for Sentiment Analysis,2020,-1,-1,5,1,1824,rong xiang,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Deep neural network models have played a critical role in sentiment analysis with promising results in the recent decade. One of the essential challenges, however, is how external sentiment knowledge can be effectively utilized. In this work, we propose a novel affection-driven approach to incorporating affective knowledge into neural network models. The affective knowledge is obtained in the form of a lexicon under the Affect Control Theory (ACT), which is represented by vectors of three-dimensional attributes in Evaluation, Potency, and Activity (EPA). The EPA vectors are mapped to an affective influence value and then integrated into Long Short-term Memory (LSTM) models to highlight affective terms. Experimental results show a consistent improvement of our approach over conventional LSTM models by 1.0{\%} to 1.5{\%} in accuracy on three large benchmark datasets. Evaluations across a variety of algorithms have also proven the effectiveness of leveraging affective terms for deep model enhancement."
2020.lrec-1.701,{C}iron: a New Benchmark Dataset for {C}hinese Irony Detection,2020,-1,-1,6,1,1824,rong xiang,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Automatic Chinese irony detection is a challenging task, and it has a strong impact on linguistic research. However, Chinese irony detection often lacks labeled benchmark datasets. In this paper, we introduce Ciron, the first Chinese benchmark dataset available for irony detection for machine learning models. Ciron includes more than 8.7K posts, collected from Weibo, a micro blogging platform. Most importantly, Ciron is collected with no pre-conditions to ensure a much wider coverage. Evaluation on seven different machine learning classifiers proves the usefulness of Ciron as an important resource for Chinese irony detection."
2020.aacl-main.84,Sina {M}andarin Alphabetical Words:A Web-driven Code-mixing Lexical Resource,2020,-1,-1,5,1,1824,rong xiang,Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing,0,"Mandarin Alphabetical Word (MAW) is one indispensable component of Modern Chinese that demonstrates unique code-mixing idiosyncrasies influenced by language exchanges. Yet, this interesting phenomenon has not been properly addressed and is mostly excluded from the Chinese language system. This paper addresses the core problem of MAW identification and proposes to construct a large collection of MAWs from Sina Weibo (SMAW) using an automatic web-based technique which includes rule-based identification, informatics-based extraction, as well as Baidu search engine validation. A collection of 16,207 qualified SMAWs are obtained using this technique along with an annotated corpus of more than 200,000 sentences for linguistic research and applicable inquiries."
D19-5541,Improving Multi-label Emotion Classification by Integrating both General and Domain-specific Knowledge,2019,0,0,3,0,26597,wenhao ying,Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019),0,"Deep learning based general language models have achieved state-of-the-art results in many popular tasks such as sentiment analysis and QA tasks. Text in domains like social media has its own salient characteristics. Domain knowledge should be helpful in domain relevant tasks. In this work, we devise a simple method to obtain domain knowledge and further propose a method to integrate domain knowledge with general knowledge based on deep language models to improve performance of emotion classification. Experiments on Twitter data show that even though a deep language model fine-tuned by a target domain data has attained comparable results to that of previous state-of-the-art models, this fine-tuned model can still benefit from our extracted domain knowledge to obtain more improvement. This highlights the importance of making use of domain knowledge in domain-specific applications."
Y18-2004,Food-Related Sentiment Analysis for {C}antonese,2018,-1,-1,4,0,23830,natalia klyueva,"Proceedings of the 32nd Pacific Asia Conference on Language, Information and Computation: 25th Joint Workshop on Linguistics and Language Processing",0,None
W18-6214,Leveraging Writing Systems Change for Deep Learning Based {C}hinese Emotion Analysis,2018,0,1,3,1,1824,rong xiang,"Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",0,"Social media text written in Chinese communities contains mixed scripts including major text written in Chinese, an ideograph-based writing system, and some minor text using Latin letters, an alphabet-based writing system. This phenomenon is called writing systems changes (WSCs). Past studies have shown that WSCs can be used to express emotions, particularly where the social and political environment is more conservative. However, because WSCs can break the syntax of the major text, it poses more challenges in Natural Language Processing (NLP) tasks like emotion classification. In this work, we present a novel deep learning based method to include WSCs as an effective feature for emotion analysis. The method first identifies all WSCs points. Then representation of the major text is learned through an LSTM model whereas the minor text is learned by a separate CNN model. Emotions in the minor text are further highlighted through an attention mechanism before emotion classification. Performance evaluation shows that incorporating WSCs features using deep learning models can improve performance measured by F1-scores compared to the state-of-the-art model."
W18-6220,Dual Memory Network Model for Biased Product Review Classification,2018,0,4,3,1,16437,yunfei long,"Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",0,"In sentiment analysis (SA) of product reviews, both user and product information are proven to be useful. Current tasks handle user profile and product information in a unified model which may not be able to learn salient features of users and products effectively. In this work, we propose a dual user and product memory network (DUPMN) model to learn user profiles and product reviews using separate memory networks. Then, the two representations are used jointly for sentiment prediction. The use of separate models aims to capture user profiles and product information more effectively. Compared to state-of-the-art unified prediction models, the evaluations on three benchmark datasets, IMDB, Yelp13, and Yelp14, show that our dual learning model gives performance gain of 0.6{\%}, 1.2{\%}, and 0.9{\%}, respectively. The improvements are also deemed very significant measured by \textit{p-values}."
K17-1006,Leveraging Eventive Information for Better Metaphor Detection and Classification,2017,25,0,3,0,27577,ihsuan chen,Proceedings of the 21st Conference on Computational Natural Language Learning ({C}o{NLL} 2017),0,"Metaphor detection has been both challenging and rewarding in natural language processing applications. This study offers a new approach based on eventive information in detecting metaphors by leveraging the Chinese writing system, which is a culturally bound ontological system organized according to the basic concepts represented by radicals. As such, the information represented is available in all Chinese text without pre-processing. Since metaphor detection is another culturally based conceptual representation, we hypothesize that sub-textual information can facilitate the identification and classification of the types of metaphoric events denoted in Chinese text. We propose a set of syntactic conditions crucial to event structures to improve the model based on the classification of radical groups. With the proposed syntactic conditions, the model achieves a performance of 0.8859 in terms of F-scores, making 1.7{\%} of improvement than the same classifier with only Bag-of-word features. Results show that eventive information can improve the effectiveness of metaphor detection. Event information is rooted in every language, and thus this approach has a high potential to be applied to metaphor detection in other languages."
I17-2025,Are Manually Prepared Affective Lexicons Really Useful for Sentiment Analysis,2017,18,1,2,1,32861,minglei li,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"In this paper, we investigate the effectiveness of different affective lexicons through sentiment analysis of phrases. We examine how phrases can be represented through manually prepared lexicons, extended lexicons using computational methods, or word embedding. Comparative studies clearly show that word embedding using unsupervised distributional method outperforms manually prepared lexicons no matter what affective models are used in the lexicons. Our conclusion is that although different affective lexicons are cognitively backed by theories, they do not show any advantage over the automatically obtained word embedding."
I17-2043,Fake News Detection Through Multi-Perspective Speaker Profiles,2017,0,37,2,1,16437,yunfei long,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"Automatic fake news detection is an important, yet very challenging topic. Traditional methods using lexical features have only very limited success. This paper proposes a novel method to incorporate speaker profiles into an attention based LSTM model for fake news detection. Speaker profiles contribute to the model in two ways. One is to include them in the attention model. The other includes them as additional input data. By adding speaker profiles such as party affiliation, speaker title, location and credit history, our model outperforms the state-of-the-art method by 14.5{\%} in accuracy using a benchmark fake news detection dataset. This proves that speaker profiles provide valuable information to validate the credibility of news articles."
D17-1048,A Cognition Based Attention Model for Sentiment Analysis,2017,20,15,2,1,16437,yunfei long,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,0,"Attention models are proposed in sentiment analysis because some words are more important than others. However,most existing methods either use local context based text information or user preference information. In this work, we propose a novel attention model trained by cognition grounded eye-tracking data. A reading prediction model is first built using eye-tracking data as dependent data and other features in the context as independent data. The predicted reading time is then used to build a cognition based attention (CBA) layer for neural sentiment analysis. As a comprehensive model, We can capture attentions of words in sentences as well as sentences in documents. Different attention mechanisms can also be incorporated to capture other aspects of attentions. Evaluations show the CBA based method outperforms the state-of-the-art local context based attention methods significantly. This brings insight to how cognition grounded data can be brought into NLP tasks."
D17-1167,A Question Answering Approach for Emotion Cause Extraction,2017,0,19,5,1,3936,lin gui,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,0,"Emotion cause extraction aims to identify the reasons behind a certain emotion expressed in text. It is a much more difficult task compared to emotion classification. Inspired by recent advances in using deep memory networks for question answering (QA), we propose a new approach which considers emotion cause identification as a reading comprehension task in QA. Inspired by convolutional neural networks, we propose a new mechanism to store relevant context in different memory slots to model context information. Our proposed approach can extract both word level sequence features and lexical features. Performance evaluation shows that our method achieves the state-of-the-art performance on a recently released emotion cause dataset, outperforming a number of competitive baselines by at least 3.01{\%} in F-measure."
Y16-2013,Event Based Emotion Classification for News Articles,2016,0,1,3,1,32861,minglei li,"Proceedings of the 30th Pacific Asia Conference on Language, Information and Computation: Oral Papers",0,None
L16-1610,Syllable based {DNN}-{HMM} {C}antonese Speech to Text System,2016,18,0,5,0,35324,timothy wong,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"This paper reports our work on building up a Cantonese Speech-to-Text (STT) system with a syllable based acoustic model. This is a part of an effort in building a STT system to aid dyslexic students who have cognitive deficiency in writing skills but have no problem expressing their ideas through speech. For Cantonese speech recognition, the basic unit of acoustic models can either be the conventional Initial-Final (IF) syllables, or the Onset-Nucleus-Coda (ONC) syllables where finals are further split into nucleus and coda to reflect the intra-syllable variations in Cantonese. By using the Kaldi toolkit, our system is trained using the stochastic gradient descent optimization model with the aid of GPUs for the hybrid Deep Neural Network and Hidden Markov Model (DNN-HMM) with and without I-vector based speaker adaptive training technique. The input features of the same Gaussian Mixture Model with speaker adaptive training (GMM-SAT) to DNN are used in all cases. Experiments show that the ONC-based syllable acoustic modeling with I-vector based DNN-HMM achieves the best performance with the word error rate (WER) of 9.66{\%} and the real time factor (RTF) of 1.38812."
L16-1722,Nine Features in a Random Forest to Learn Taxonomical Semantic Relations,2016,3,12,4,0.727273,181,enrico santus,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"ROOT9 is a supervised system for the classification of hypernyms, co-hyponyms and random words that is derived from the already introduced ROOT13 (Santus et al., 2016). It relies on a Random Forest algorithm and nine unsupervised corpus-based features. We evaluate it with a 10-fold cross validation on 9,600 pairs, equally distributed among the three classes and involving several Parts-Of-Speech (i.e. adjectives, nouns and verbs). When all the classes are present, ROOT9 achieves an F1 score of 90.7{\%}, against a baseline of 57.2{\%} (vector cosine). When the classification is binary, ROOT9 achieves the following results against the baseline. hypernyms-co-hyponyms 95.7{\%} vs. 69.8{\%}, hypernyms-random 91.8{\%} vs. 64.1{\%} and co-hyponyms-random 97.8{\%} vs. 79.4{\%}. In order to compare the performance with the state-of-the-art, we have also evaluated ROOT9 in subsets of the Weeds et al. (2014) datasets, proving that it is in fact competitive. Finally, we investigated whether the system learns the semantic relation or it simply learns the prototypical hypernyms, as claimed by Levy et al. (2015). The second possibility seems to be the most likely, even though ROOT9 can be trained on negative examples (i.e., switched hypernyms) to drastically reduce this bias."
L16-1723,What a Nerd! Beating Students and Vector Cosine in the {ESL} and {TOEFL} Datasets,2016,4,6,4,0.727273,181,enrico santus,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"In this paper, we claim that Vector Cosine â which is generally considered one of the most efficient unsupervised measures for identifying word similarity in Vector Space Models â can be outperformed by a completely unsupervised measure that evaluates the extent of the intersection among the most associated contexts of two target words, weighting such intersection according to the rank of the shared contexts in the dependency ranked lists. This claim comes from the hypothesis that similar words do not simply occur in similar contexts, but they share a larger portion of their most relevant contexts compared to other related words. To prove it, we describe and evaluate APSyn, a variant of Average Precision that â independently of the adopted parameters â outperforms the Vector Cosine and the co-occurrence on the ESL and TOEFL test sets. In the best setting, APSyn reaches 0.73 accuracy on the ESL dataset and 0.70 accuracy in the TOEFL dataset, beating therefore the non-English US college applicants (whose average, as reported in the literature, is 64.50{\%}) and several state-of-the-art approaches."
D16-1170,Event-Driven Emotion Cause Extraction with Corpus Construction,2016,27,18,4,1,3936,lin gui,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,None
Y14-1018,Taking Antonymy Mask off in Vector Space,2014,36,13,2,0.727273,181,enrico santus,"Proceedings of the 28th Pacific Asia Conference on Language, Information and Computing",0,"Automatic detection of antonymy is an important task in Natural Language Processing (NLP) for Information Retrieval (IR), Ontology Learning (OL) and many other semantic applications. However, current unsupervised approaches to antonymy detection are still not fully effective because they cannot discriminate antonyms from synonyms. In this paper, we introduce APAnt, a new AveragePrecision-based measure for the unsupervised discrimination of antonymy from synonymy using Distributional Semantic Models (DSMs). APAnt makes use of Average Precision to estimate the extent and salience of the intersection among the most descriptive contexts of two target words. Evaluation shows that the proposed method is able to distinguish antonyms and synonyms with high accuracy across different parts of speech, including nouns, adjectives and verbs. APAnt outperforms the vector cosine and a baseline model implementing the cooccurrence hypothesis."
P14-2139,Cross-lingual Opinion Analysis via Negative Transfer Detection,2014,27,19,3,1,3936,lin gui,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Transfer learning has been used in opinion analysis to make use of available language resources for other resource scarce languages. However, the cumulative class noise in transfer learning adversely affects performance when more training data is used. In this paper, we propose a novel method in transductive transfer learning to identify noises through the detection of negative transfers. Evaluation on NLP&CC 2013 cross-lingual opinion analysis dataset shows that our approach outperforms the state-of-the-art systems. More significantly, our system shows a monotonic increase trend in performance improvement when more training data are used."
J14-3004,Feature-Frequency{--}Adaptive On-line Training for Fast and Accurate Natural Language Processing,2014,42,19,4,0,3749,xu sun,Computational Linguistics,0,"Training speed and accuracy are two major concerns of large-scale natural language processing systems. Typically, we need to make a tradeoff between speed and accuracy. It is trivial to improve the training speed via sacrificing accuracy or to improve the accuracy via sacrificing speed. Nevertheless, it is nontrivial to improve the training speed and the accuracy at the same time, which is the target of this work. To reach this target, we present a new training method, feature-frequencyxe2x80x94adaptive on-line training, for fast and accurate training of natural language processing systems. It is based on the core idea that higher frequency features should have a learning rate that decays faster. Theoretical analysis shows that the proposed method is convergent with a fast convergence rate. Experiments are conducted based on well-known benchmark tasks, including named entity recognition, word segmentation, phrase chunking, and sentiment analysis. These tasks consist of three structured classification tasks and one non-structured classification task, with binary features and real-valued features, respectively. Experimental results demonstrate that the proposed method is faster and at the same time more accurate than existing methods, achieving state-of-the-art scores on the tasks with different characteristics."
E14-4008,Chasing Hypernyms in Vector Spaces with Entropy,2014,20,58,3,0.727273,181,enrico santus,"Proceedings of the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics, volume 2: Short Papers",0,"In this paper, we introduce SLQS , a new entropy-based measure for the unsupervised identification of hypernymy and its directionality in Distributional Semantic Models (DSMs). SLQS is assessed through two tasks: (i.) identifying the hypernym in hyponym-hypernym pairs, and (ii.) discriminating hypernymy among various semantic relations. In both tasks, SLQS outperforms other state-of-the-art measures."
S13-1012,{P}oly{UCOMP}-{CORE}{\\_}{TYPED}: Computing Semantic Textual Similarity using Overlapped Senses,2013,10,2,2,0.952381,32405,jian xu,"Second Joint Conference on Lexical and Computational Semantics (*{SEM}), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity",0,"The Semantic Textual Similarity (STS) task aims to exam the degree of semantic equivalence between sentences (Agirre et al., 2012). This paper presents the work of the Hong Kong Polytechnic University (PolyUCOMP) team which has participated in the STS core and typed tasks of SemEval2013. For the STS core task, the PolyUCOMP system disambiguates words senses using contexts and then determine sentence similarity by counting the number of senses they shared. For the STS typed task, the string kernel (Lodhi et al., 2002) is used to compute similarity between two entities to avoid string variations in entities."
W12-6326,Explore {C}hinese Encyclopedic Knowledge to Disambiguate Person Names,2012,23,2,3,0,12452,jie liu,Proceedings of the Second {CIPS}-{SIGHAN} Joint Conference on {C}hinese Language Processing,0,"This paper presents the HITSZ-PolyU system in the CIPS-SIGHAN bakeoff 2012 Task 3, Chinese Personal Name Disambiguation. This system leveraged the Chinese encyclopedia Baidu Baike (Baike) as the external knowledge to disambiguate the person names. Three kinds of features are extracted from Baike. They are the entitiesxe2x80x99 texts in Baike, the entitiesxe2x80x99 work-of-art words and titles in the Baike. With these features, a Decision Tree (DT) based classifier is trained to link test names to nodes in the NameKB. Besides, the contextual information surrounding test names is used to verify whether test names are person name or not. Finally, a simple clustering approach is used to group NIL test names that have no links to the NameKB. Our proposed system attains 64.04% precision, 70.1% recall and 66.95% F-score."
S12-1075,{P}oly{UCOMP}: Combining Semantic Vectors with Skip bigrams for Semantic Textual Similarity,2012,11,2,2,0.952381,32405,jian xu,"*{SEM} 2012: The First Joint Conference on Lexical and Computational Semantics {--} Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation ({S}em{E}val 2012)",0,This paper presents the work of the Hong Kong Polytechnic University (PolyUCOMP) team which has participated in the Semantic Textual Similarity task of SemEval-2012. The PolyUCOMP system combines semantic vectors with skip bigrams to determine sentence similarity. The semantic vector is used to compute similarities between sentence pairs using the lexical database WordNet and the Wikipedia corpus. The use of skip bigram is to introduce the order of words in measuring sentence similarity.
xu-etal-2012-grammar,A Grammar-informed Corpus-based Sentence Database for Linguistic and Computational Studies,2012,3,0,4,0,16510,hongzhi xu,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"We adopt the corpus-informed approach to example sentence selections for the construction of a reference grammar. In the process, a database containing sentences that are carefully selected by linguistic experts including the full range of linguistic facts covered in an authoritative Chinese Reference Grammar is constructed and structured according to the reference grammar. A search engine system is developed to facilitate the process of finding the most typical examples the users need to study a linguistic problem or prove their hypotheses. The database can also be used as a training corpus by computational linguists to train models for Chinese word segmentation, POS tagging and sentence parsing."
Y11-1045,A Hybrid Extraction Model for {C}hinese Noun/Verb Synonymous bi-gram Collocations,2011,17,0,2,1,43957,wanyin li,"Proceedings of the 25th Pacific Asia Conference on Language, Information and Computation",0,"Statistical-based collocation extraction approaches suffer from (1) low precision rate because high co-occurrence bi-grams may be syntactically unrelated and are thus not true collocations; (2) low recall rate because some true collocations with low occurrences cannot be identified successfully by statistical-based models. To integrate both syntactic rules as well as semantic knowledge into a statistical model for collocation extraction is one way to achieve a high precision while keeping a reasonable recall. This paper designs a cascade system which employs a hybrid model by integrating both syntactic and semantic knowledge into a statistical model for Chinese synonymous noun/verb collocations extraction. The grammatically bounded noun/verb collocations are extracted first from a syntactic-rule based module, which is then inputted to a semantic-based module for further retrieval of low frequent bi-gram collocations."
S10-1067,{PKU}{\\_}{HIT}: An Event Detection System Based on Instances Expansion and Rich Syntactic Features,2010,6,0,4,0,27698,shiqi li,Proceedings of the 5th International Workshop on Semantic Evaluation,0,"This paper describes the PKU_HIT system on event detection in the SemEval-2010 Task. We construct three modules for the three sub-tasks of this evaluation. For target verb WSD, we build a Naive Bayesian classifier which uses additional training instances expanded from an untagged Chinese corpus automatically. For sentence SRL and event detection, we use a feature-based machine learning method which makes combined use of both constituent-based and dependency-based features. Experimental results show that the Macro Accuracy of the WSD module reaches 83.81% and F-Score of the SRL module is 55.71%."
C10-2076,Combining Constituent and Dependency Syntactic Views for {C}hinese Semantic Role Labeling,2010,30,3,2,0,27698,shiqi li,Coling 2010: Posters,0,"This paper presents a novel feature-based semantic role labeling (SRL) method which uses both constituent and dependency syntactic views. Comparing to the traditional SRL method relying on only one syntactic view, the method has a much richer set of syntactic features. First we select several important constituent-based and dependency-based features from existing studies as basic features. Then, we propose a statistical method to select discriminative combined features which are composed by the basic features. SRL is achieved by using the SVM classifier with both the basic features and the combined features. Experimental results on Chinese Proposition Bank (CPB) show that the method outperforms the traditional constituent-based or dependency-based SRL methods."
C10-2106,A Study on Position Information in Document Summarization,2010,11,34,3,0.869565,45324,you ouyang,Coling 2010: Posters,0,"Position information has been proved to be very effective in document summarization, especially in generic summarization. Existing approaches mostly consider the information of sentence positions in a document, based on a sentence position hypothesis that the importance of a sentence decreases with its distance from the beginning of the document. In this paper, we consider another kind of position information, i.e., the word position information, which is based on the ordinal positions of word appearances instead of sentence positions. An extractive summarization model is proposed to provide an evaluation framework for the position information. The resulting systems are evaluated on various data sets to demonstrate the effectiveness of the position information in different summarization tasks. Experimental results show that word position information is more effective and adaptive than sentence position information."
C10-2170,Sentence Ordering with Event-Enriched Semantics and Two-Layered Clustering for Multi-Document News Summarization,2010,15,11,3,0,34656,renxian zhang,Coling 2010: Posters,0,"We propose an event-enriched model to alleviate the semantic deficiency problem in the IR-style text processing and apply it to sentence ordering for multi-document news summarization. The ordering algorithm is built on event and entity coherence, both locally and globally. To accommodate the event-enriched model, a novel LSA-integrated two-layered clustering approach is adopted. The experimental result shows clear advantage of our model over event-agonistic models."
P09-5001,Fundamentals of {C}hinese Language Processing,2009,3,0,2,0,1504,churen huang,Tutorial Abstracts of {ACL}-{IJCNLP} 2009,0,"This tutorial gives an introduction to the fundamentals of Chinese language processing for text processing. Today, more and more Chinese information are available in electronic form and over the internet. Computer processing of Chinese text requires the understanding of both the language itself and the technology to handle them. This tutorial is targeted for both Chinese linguists who are interested in computational linguistics and computer scientists who are interested in research on processing Chinese."
P09-2029,An Integrated Multi-document Summarization Approach based on Word Hierarchical Representation,2009,6,14,3,0.869565,45324,you ouyang,Proceedings of the {ACL}-{IJCNLP} 2009 Conference Short Papers,0,"This paper introduces a novel hierarchical summarization approach for automatic multi-document summarization. By creating a hierarchical representation of the words in the input document set, the proposed approach is able to incorporate various objectives of multi-document summarization through an integrated framework. The evaluation is conducted on the DUC 2007 data set."
P09-2054,{C}hinese Term Extraction Using Different Types of Relevance,2009,13,8,3,1,43936,yuhang yang,Proceedings of the {ACL}-{IJCNLP} 2009 Conference Short Papers,0,"This paper presents a new term extraction approach using relevance between term candidates calculated by a link analysis based method. Different types of relevance are used separately or jointly for term verification. The proposed approach requires no prior domain knowledge and no adaptation for new domains. Consequently, the method can be used in any domain corpus and it is especially useful for resource-limited domains. Evaluations conducted on two different domains for Chinese term extraction show significant improvements over existing techniques and also verify the efficiency and relative domain independent nature of the approach."
P08-2023,A Novel Feature-based Approach to {C}hinese Entity Relation Extraction,2008,9,27,5,0.682824,1826,wenjie li,"Proceedings of ACL-08: HLT, Short Papers",0,"Relation extraction is the task of finding semantic relations between two entities from text. In this paper, we propose a novel feature-based Chinese relation extraction approach that explicitly defines and explores nine positional structures between two entities. We also suggest some correction and inference mechanisms based on relation hierarchy and co-reference information etc. The approach is effective when evaluated on the ACE 2005 Chinese data set."
chen-etal-2008-chinese,{C}hinese Core Ontology Construction from a Bilingual Term Bank,2008,17,1,2,1,48076,yirong chen,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"A core ontology is a mid-level ontology which bridges the gap between an upper ontology and a domain ontology. Automatic Chinese core ontology construction can help quickly model domain knowledge. A graph based core ontology construction algorithm (COCA) is proposed to automatically construct a core ontology from an English-Chinese bilingual term bank. This algorithm computes the mapping strength from a selected Chinese term to WordNet synset with association to an upper-level SUMO concept. The strength is measured using a graph model integrated with several mapping features from multiple information sources. The features include multiple translation feature between Chinese core term and WordNet, extended string feature and Part-of-Speech feature. Evaluation of COCA repeated on an English-Chinese bilingual Term bank with more than 130K entries shows that the algorithm is improved in performance compared with our previous research and can better serve the semi-automatic construction of mid-level ontology."
yang-etal-2008-chinese-term,{C}hinese Term Extraction Based on Delimiters,2008,10,2,2,1,43936,yuhang yang,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"Existing techniques extract term candidates by looking for internal and contextual information associated with domain specific terms. The algorithms always face the dilemma that fewer features are not enough to distinguish terms from non-terms whereas more features lead to more conflicts among selected features. This paper presents a novel approach for term extraction based on delimiters which are much more stable and domain independent. The proposed approach is not as sensitive to term frequency as that of previous works. This approach has no strict limit or hard rules and thus they can deal with all kinds of terms. It also requires no prior domain knowledge and no additional training to adapt to new domains. Consequently, the proposed approach can be applied to different domains easily and it is especially useful for resource-limited domains. Evaluations conducted on two different domains for Chinese term extraction show significant improvements over existing techniques which verifies its efficiency and domain independent nature. Experiments on new term extraction indicate that the proposed approach can also serve as an effective tool for domain lexicon expansion."
cui-etal-2008-corpus,Corpus Exploitation from {W}ikipedia for Ontology Construction,2008,10,16,2,0,48077,gaoying cui,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"Ontology construction usually requires a domain-specific corpus for building corresponding concept hierarchy. The domain corpus must have a good coverage of domain knowledge. Wikipedia(Wiki), the worldÂs largest online encyclopaedic knowledge source, is open-content, collaboratively edited, and free of charge. It covers millions of articles and still keeps on expanding continuously. These characteristics make Wiki a good candidate as domain corpus resource in ontology construction. However, the selected article collection must have considerable quality and quantity. In this paper, a novel approach is proposed to identify articles in Wiki as domain-specific corpus by using available classification information in Wiki pages. The main idea is to generate a domain hierarchy from the hyperlinked pages of Wiki. Only articles strongly linked to this hierarchy are selected as the domain corpus. The proposed approach makes use of linked category information in Wiki pages to produce the hierarchy as a directed graph for obtaining a set of pages in the same connected branch. Ranking and filtering are then done on these pages based on the classification tree generated by the traversal algorithm. The experiment and evaluation results show that Wiki is a good resource for acquiring a relative high quality domain-specific corpus for ontology construction."
zhang-etal-2008-exploiting,Exploiting the Role of Position Feature in {C}hinese Relation Extraction,2008,14,10,4,0,9167,peng zhang,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"Relation extraction is the task of finding pre-defined semantic relations between two entities or entity mentions from text. Many methods, such as feature-based and kernel-based methods, have been proposed in the literature. Among them, feature-based methods draw much attention from researchers. However, to the best of our knowledge, existing feature-based methods did not explicitly incorporate the position feature and no in-depth analysis was conducted in this regard. In this paper, we define and exploit nine types of position information between two named entity mentions and then use it along with other features in a multi-class classification framework for Chinese relation extraction. Experiments on the ACE 2005 data set show that the position feature is more effective than the other recognized features like entity type/subtype and character-based N-gram context. Most important, it can be easily captured and does not require as much effort as applying deep natural language processing."
I08-7003,Preliminary {C}hinese Term Classification for Ontology Construction,2008,18,1,2,0,48077,gaoying cui,Proceedings of the 6th Workshop on {A}sian Language Resources,0,"An ontology can be seen as a representation of concepts in a specific domain. Accordingly, ontology construction can be regarded as the process of organizing these concepts. If the terms which are used to label the concepts are classified before building an ontology, the work of ontology construction can proceed much more easily. Part-of-speech (PoS) tags usually carry some linguistic information of terms, so PoS tagging can be seen as a kind of preliminary classification to help constructing concept nodes in ontology because features or attributes related to concepts of different PoS types may be different. This paper presents a simple approach to tag domain terms for the convenience of ontology construction, referred to as Term PoS (TPoS) Tagging. The proposed approach makes use of segmentation and tagging results from a general PoS tagging software to predict tags for extracted domain specific terms. This approach needs no training and no context information. The experimental results show that the proposed approach achieves a precision of 95.41% for extracted terms and can be easily applied to different domains. Comparing with some existing approaches, our approach shows that for some specific tasks, simple method can obtain very good performance and is thus a better choice."
C08-1062,{PNR}2: Ranking Sentences with Positive and Negative Reinforcement for Query-Oriented Update Summarization,2008,20,38,3,0.682824,1826,wenjie li,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"Query-oriented update summarization is an emerging summarization task very recently. It brings new challenges to the sentence ranking algorithms that require not only to locate the important and query-relevant information, but also to capture the new information when document collections evolve. In this paper, we propose a novel graph based sentence ranking algorithm, namely PNR2, for update summarization. Inspired by the intuition that a sentence receives a positive influence from the sentences that correlate to it in the same collection, whereas a sentence receives a negative influence from the sentences that correlates to it in the different (perhaps previously read) collection, PNR2 models both the positive and the negative mutual reinforcement in the ranking process. Automatic evaluation on the DUC 2007 data set pilot task demonstrates the effectiveness of the algorithm."
C08-1130,{C}hinese Term Extraction Using Minimal Resources,2008,19,9,2,1,43936,yuhang yang,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"This paper presents a new approach for term extraction using minimal resources. A term candidate extraction algorithm is proposed to identify features of the relatively stable and domain independent term delimiters rather than that of the terms. For term verification, a link analysis based method is proposed to calculate the relevance between term candidates and the sentences in the domain specific corpus from which the candidates are extracted. The proposed approach requires no prior domain knowledge, no general corpora, no full segmentation and minimal adaptation for new domains. Consequently, the method can be used in any domain corpus and it is especially useful for resource-limited domains. Evaluations conducted on two different domains for Chinese term extraction show quite significant improvements over existing techniques and also verify the efficiency and relative domain independent nature of the approach. Experiments on new term extraction also indicate that the approach is quite effective for identifying new terms in a domain making it useful for domain knowledge update."
W07-1511,Annotating {C}hinese Collocations with Multi Information,2007,8,2,2,1,1816,ruifeng xu,Proceedings of the Linguistic Annotation Workshop,0,"This paper presents the design and construction of an annotated Chinese collocation bank as the resource to support systematic research on Chinese collocations. With the help of computational tools, the bi-gram and n-gram collocations corresponding to 3,643 head-words are manually identified. Furthermore, annotations for bi-gram collocations include dependency relation, chunking relation and classification of collocation types. Currently, the collocation bank annotated 23,581 bi-gram collocations and 2,752 n-gram collocations extracted from a 5-million-word corpus. Through statistical analysis on the collocation bank, some characteristics of Chinese bi-gram collocations are examined which is essential to collocation research, especially for Chinese."
P07-2047,Extractive Summarization Based on Event Term Clustering,2007,8,20,4,0,22033,maofu liu,Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,0,"Event-based summarization extracts and organizes summary sentences in terms of the events that the sentences describe. In this work, we focus on semantic relations among event terms. By connecting terms with relations, we build up event term graph, upon which relevant terms are grouped into clusters. We assume that each cluster represents a topic of documents. Then two summarization strategies are investigated, i.e. selecting one term as the representative of each topic so as to cover all the topics, or selecting all terms in one most significant topic so as to highlight the relevant information related to this topic. The selected terms are then responsible to pick out the most appropriate sentences describing them. The evaluation of clustering-based summarization on DUC 2001 document sets shows encouraging improvement over the well-known PageRank-based summarization."
Y06-1014,A Comparative Study of the Effect of Word Segmentation On {C}hinese Terminology Extraction,2006,11,0,2,0,49505,luning ji,"Proceedings of the 20th Pacific Asia Conference on Language, Information and Computation",0,"Automatic term extraction is the first step towards automatic or semi-automatic update of existing domain knowledge base. Most of the researches applied word segmentation as a preprocessing step to Chinese term extraction. However, segmentation ambiguity is unavoidable, especially in identifying unknown words for Chinese. In this paper, we discuss the effect and limitations of segmentation to Chinese terminology extraction. Detailed study shows that propagated errors caused by word segmentation have great impact on the result of terminology extraction. Based on our analysis and experiments, it is proven that character-based terminology extraction yields much better result than that using segmentation as a preprocessing step."
Y06-1015,{TC}tract-A Collocation Extraction Approach for Noun Phrases Using Shallow Parsing Rules and Statistic Models,2006,15,2,2,0,49506,wan li,"Proceedings of the 20th Pacific Asia Conference on Language, Information and Computation",0,"This paper presents a hybrid method for extracting Chinese noun phrase collocations that combines a statistical model with rule-based linguistic knowledge. The algorithm first extracts all the noun phrase collocations from a shallow parsed corpus by using syntactic knowledge in the form of phrase rules. It then removes pseudo collocations by using a set of statistic-based association measures (AMs) as filters. There are two main purposes for the design of this hybrid algorithm: (1) to maintain a reasonable recall while improving the precision, and (2) to investigate the proposed association measures on Chinese noun phrase collocations. The performance is compared with a pure statistical model and a pure rule-based method on a 60MB PoS tagged corpus. The experiment results show that the proposed hybrid method has a higher precision of 92.65% and recall of 47% based on 29 randomly selected noun headwords compared with the precision of 78.87% and recall of 27.19% of a statistics based extraction system. The F-score improvement is 55.7%."
P06-1047,Extractive Summarization using Inter- and Intra- Event Relevance,2006,14,58,3,0.782978,1826,wenjie li,Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,1,"Event-based summarization attempts to select and organize the sentences in a summary with respect to the events or the sub-events that the sentences describe. Each event has its own internal structure, and meanwhile often relates to other events semantically, temporally, spatially, causally or conditionally. In this paper, we define an event as one or more event terms along with the named entities associated, and present a novel approach to derive intra- and inter- event relevance using the information of internal association, semantic relatedness, distributional similarity and named entity clustering. We then apply PageRank ranking algorithm to estimate the significance of an event for inclusion in a summary from the event relevance derived. Experiments on the DUC 2001 test data shows that the relevance of the named entities involved in events achieves better result when their relevance is derived from the event terms they associate. It also reveals that the topic-specific relevance from documents themselves outperforms the semantic relevance from a general purpose knowledge base like Word-Net."
li-etal-2006-mining,Mining Implicit Entities in Queries,2006,3,2,3,0.854143,1884,wei li,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"Entities are pivotal in describing events and objects, and also very important in Document Summarization. In general only explicit entities which can be extracted by a Named Entity Recognizer are used in real applications. However, implicit entities hidden behind the phrases or words, e.g. entity referred by the phrase Âcross borderÂ, are proved to be helpful in Document Summarization. In our experiment, we extract the implicit entities from the web resources."
chen-etal-2006-study,A Study on Terminology Extraction Based on Classified Corpora,2006,10,7,2,1,48076,yirong chen,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"Algorithms for automatic term extraction in a specific domain should consider at least two issues, namely Unithood and Termhood (Kageura, 1996). Unithood refers to the degree of a string to occur as a word or a phrase. Termhood (Chen Yirong, 2005) refers to the degree of a word or a phrase to occur as a domain specific concept. Unlike unithood, study on termhood is not yet widely reported. In classified corpora, the class information provides the cue to the nature of data and can be used in termhood calculation. Three algorithms are provided and evaluated to investigate termhood based on classified corpora. The three algorithms are based on lexicon set computing, term frequency and document frequency, and the strength of the relation between a term and its document class respectively. Our objective is to investigate the effects of these different termhood measurement features. After evaluation, we can find which features are more effective and also, how we can improve these different features to achieve the best performance. Preliminary results show that the first measure can effectively filter out independent terms or terms of general use."
li-etal-2006-interaction,Interaction between Lexical Base and Ontology with Formal Concept Analysis,2006,8,0,2,1,5814,sujian li,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"An ontology describes conceptual knowledge in a specific domain. A lexical base collects a repository of words and gives independent definition of concepts. In this paper, we propose to use FCA as a tool to help constructing an ontology through an existing lexical base. We mainly address two issues. The first issue is how to select attributes to visualize the relations between lexical terms. The second issue is how to revise lexical definitions through analysing the relations in the ontology. Thus the focus is on the effect of interaction between a lexical base and an ontology for the purpose of good ontology construction. Finally, experiments have been conducted to verify our ideas."
xu-etal-2006-design,The Design and Construction of A {C}hinese Collocation Bank,2006,0,3,2,1,1816,ruifeng xu,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"This paper presents an annotated Chinese collocation bank developed at the Hong Kong Polytechnic University. The definition of collocation with good linguistic consistency and good computational operability is first discussed and the properties of collocations are then presented. Secondly, based on the combination of different properties, collocations are classified into four types. Thirdly, the annotation guideline is presented. Fourthly, the implementation issues for collocation bank construction are addressed including the annotation with categorization, dependency and contextual information. Currently, the collocation bank is completed for 3,643 headwords in a 5-million-word corpus."
O05-4006,The Design and Construction of the {P}oly{U} Shallow Treebank,2005,12,6,2,1,1816,ruifeng xu,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 10, Number 3, September 2005: Special Issue on Selected Papers from {ROCLING} {XVI}",0,"This paper presents the design and construction of the PolyU Treebank, a manually annotated Chinese shallow treebank. The PolyU Treebank is based on shallow annotation where only partial syntactical structures within sentences are annotated. Guided by the Phrase-Standard Grammar proposed by Peking University, the PolyU Treebank has been designed and constructed to provide a large amount of annotated data containing shallow syntactical information and limited semantic information for use in natural language processing (NLP) research. This paper describes the relevant design principles, annotation guidelines, and implementation issues, including the achievement of high quality annotation through the use of well-designed annotation workflow and effective post-annotation checking tools. Currently, the PolyU Treebank consists of a one-million-word annotated corpus and has been used in a number of NLP research projects with promising results."
O05-2006,Similarity Based {C}hinese Synonym Collocation Extraction,2005,12,10,2,1,43957,wanyin li,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 10, Number 1, March 2005",0,"Collocation extraction systems based on pure statistical methods suffer from two major problems. The first problem is their relatively low precision and recall rates. The second problem is their difficulty in dealing with sparse collocations. In order to improve performance, both statistical and lexicographic approaches should be considered. This paper presents a new method to extract synonymous collocations using semantic information. The semantic information is obtained by calculating similarities from HowNet. We have successfully extracted synonymous collocations which normally cannot be extracted using lexical statistics. Our evaluation conducted on a 60MB tagged corpus shows that we can extract synonymous collocations that occur with very low frequency and that the improvement in the recall rate is close to 100%. In addition, compared with a collocation extraction system based on the Xtract system for English, our algorithm can improve the precision rate by about 44%."
I05-7010,Experiments of Ontology Construction with Formal Concept Analysis,2005,0,10,2,1,5814,sujian li,Proceedings of {O}nto{L}ex 2005 - Ontologies and Lexical Resources,0,"Introduction Ontologies are constructs of domain-specific concepts, and their relationships are used to reason about or define that domain. While an ontology may be constructed either manually or semi-automatically, it is never a trivial task. Manual methods usually require that the concept architecture be constructed by experts who consult dictionaries and other text sources. For example, the Upper Cyc Ontology built by Cycorp was manually constructed with approximately 3,000 terms (Lenat, 1998). Automatic and semi-automatic methods require two separate steps in which the first step acquires domain-specific terms followed by the second step of identifying relations among them from available lexicons or corpora. As lexicons are a good resource and are helpful for ontology construction, Chapters 5 and 15 discuss the problems involving ontology construction and lexicons. To use the available corpus resource, a common approach for automatic acquisition employs heuristic rules (Hearst, 1992; Maedche and Staab, 2000). However, such a method can only acquire limited relations. One new approach in the automatic construction of ontologies (Cimiano et al ., 2004) is FCA (Formal Concept Analysis), a mathematical data analysis approach based on the lattice theory. Because formal concept lattices are a natural representation of hierarchies and classifications, FCA has evolved from a pure mathematical tool to an effective method in computer science (Stumme, 2002), such as in the automatic construction of an ontology (Cimiano et al ., 2004). The focus of this work is on how to use FCA to construct a domainspecific ontology based on different Chinese data sources."
I05-3012,Integrating Collocation Features in {C}hinese Word Sense Disambiguation,2005,21,8,2,1,43957,wanyin li,Proceedings of the Fourth {SIGHAN} Workshop on {C}hinese Language Processing,0,"The selection of features is critical in providing discriminative information for classifiers in Word Sense Disambiguation (WSD). Uninformative features will degrade the performance of classifiers. Based on the strong evidence that an ambiguous word expresses a unique sense in a given collocation, this paper reports our experiments on automatic WSD using collocation as local features based on the corpus extracted from Peoplexe2x80x99s Daily News (PDN) as well as the standard SENSEVAL-3 data set. Using the Naive Bayes classifier as our core algorithm, we have implemented a classifier using a feature set combining both local collocation features and topical features. The average precision on the PDN corpus has 3.2% improvement compared to 81.5% of the baseline system where collocation features are not considered. For the SENSEVAL-3 data, we have reached the precision rate of 37.6% by integrating collocation features into contextual features, to achieve 37% improvement over 26.7% of precision in the baseline system. Our experiments have shown that collocation features can be used to reduce the size of human tagged corpus."
I05-1037,A Preliminary Work on Classifying Time Granularities of Temporal Questions,2005,12,4,3,0.854143,1884,wei li,Second International Joint Conference on Natural Language Processing: Full Papers,0,"Temporal question classification assigns time granularities to temporal questions ac-cording to their anticipated answers. It is very important for answer extraction and verification in the literature of temporal question answering. Other than simply distinguishing between date and period, a more fine-grained classification hierarchy scaling down from millions of years to second is proposed in this paper. Based on it, a SNoW-based classifier, combining user preference, word N-grams, granularity of time expressions, special patterns as well as event types, is built to choose appropriate time granularities for the ambiguous temporal questions, such as When- and How long-like questions. Evaluation on 194 such questions achieves 83.5% accuracy, almost close to manually tagging accuracy 86.2%. Experiments reveal that user preferences make significant contributions to time granularity classification."
I05-1061,{CTEMP}: A {C}hinese Temporal Parser for Extracting and Normalizing Temporal Information,2005,12,28,3,1,48748,mingli wu,Second International Joint Conference on Natural Language Processing: Full Papers,0,"Temporal information is useful in many NLP applications, such as information extraction, question answering and summarization. In this paper, we present a temporal parser for extracting and normalizing temporal expressions from Chinese texts. An integrated temporal framework is proposed, which includes basic temporal concepts and the classification of temporal expressions. The identification of temporal expressions is fulfilled by powerful chart-parsing based on grammar rules and constraint rules. We evaluated the system on a substantial corpus and obtained promising results."
W04-1113,Using Synonym Relations in {C}hinese Collocation Extraction,2004,9,0,2,1,43957,wanyin li,Proceedings of the Third {SIGHAN} Workshop on {C}hinese Language Processing,0,"A challenging task in Chinese collocation extraction is to improve both the precision and recall rate. Most lexical statistical methods including Xtract face the problem of unable to extract collocations with lower frequencies than a given threshold. This paper presents a method where HowNet is used to find synonyms using a similarity function. Based on such synonym information, we have successfully extracted synonymous collocations which normally cannot be extracted using the lexical statistical approach. We applied synonyms mapping to each headword to extract more synonymous word bi-grams. Our evaluation over 60MB tagged corpus shows that we can extract synonymous collocations that occur with very low frequency, sometimes even for collocations that occur only once in the training set. Comparing to a collocation extraction system based on Xtract, we have reached the precision rate of 43% on word bi-grams for a set of 9 headwords, almost 50% improvement from precision rate of 30% in the Xtract system. Furthermore, it improves the recall rate of word bi-gram collocation extraction by 30%."
W04-1114,The Construction of A {C}hinese Shallow Treebank,2004,7,6,2,1,1816,ruifeng xu,Proceedings of the Third {SIGHAN} Workshop on {C}hinese Language Processing,0,"This paper presents the construction of a manually annotated Chinese shallow Treebank, named PolyU Treebank. Different from traditional Chinese Treebank based on full parsing, the PolyU Treebank is based on shallow parsing in which only partial syntactical structures are annotated. This Treebank can be used to support shallow parser training, testing and other natural language applications. Phrase-based Grammar, proposed by Peking University, is used to guide the design and implementation of the PolyU Treebank. The design principles include good resource sharing, low structural complexity, sufficient syntactic information and large data scale. The design issues, including corpus material preparation, standard for word segmentation and POS tagging, and the guideline for phrase bracketing and annotation, are presented in this paper. Well-designed workflow and effective semiautomatic and automatic annotation checking are used to ensure annotation accuracy and consistency. Currently, the PolyU Treebank has completed the annotation of a 1-million-word corpus. The evaluation shows that the accuracy of annotation is higher than 98%."
W02-1209,Decomposition for {ISO}/{IEC} 10646 Ideographic Characters,2002,0,2,1,1,1827,qin lu,{COLING}-02: The 3rd Workshop on {A}sian Language Resources and International Standardization,0,"Ideograph characters are often formed by some smaller functional units, which we call character components. These character components can be ideograph radicals, ideographs proper, or some pure components which must be used with others to form characters. Decomposition of ideographs can be used in many applications. It is particularly important in the study of Chinese character formation, phonetics and semantics. However, the way a character is decomposed depends on the definition of components as well as the decomposition rules. The 12 Ideographic Description Characters (IDCs) introduced in ISO 10646 are designed to describe characters using components. The Hong Kong SAR Government recently published two sets of glyph standards for ISO10646 characters. The standards, being the first of its kind, make use of character decomposition to specify a character glyph using its components. In this paper, we will first introduce the IDCs and how they can be used with components to describe two dimensional ideograph characters in a linear fashion. Next we will briefly discuss the basic references and character decomposition rules. We will then describe the data structure and algorithms to decompose Chinese characters into components and, vice versa. We have also implemented our database and algorithms as an internet application, called the Chinese Character Search System, available at website http://www.iso10646hk.net/. With this tool, people can easily search characters and components in ISO 10646."
