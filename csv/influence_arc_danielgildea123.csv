2003.mtsummit-papers.13,J00-1004,0,0.0221629,"form competitively with more traditional interlingua based approaches. In recent years, hybrid approaches, which aim at applying statistical models to structural data, have begun to emerge. However, such approaches have been faced with the problem of pervasive structural divergence between languages, due to both systematic differences between languages (Dorr, 1994) and the vagaries of loose translations in real corpora. Syntax-based statistical approaches to alignment began with (Wu, 1997), who introduced a polynomial-time solution for the alignment problem based on synchronous binary trees. (Alshawi et al., 2000) extended the tree-based approach by representing each production in parallel dependency trees as a finite-state transducer. Both these approaches learn the tree representations directly from parallel sentences, and do not make allowance for non-isomorphic structures. (Yamada and Knight, 2001, 2002) model translation as a sequence of operations transforming a syntactic tree in one language into the string of the second language, making use of the output of an automatic parser in one of the two parallel languages. This allows the model to make use of the syntactic information provided by treeba"
2003.mtsummit-papers.13,J90-2002,0,0.394016,"Missing"
2003.mtsummit-papers.13,J93-2003,0,0.0235932,"nce University of Pennsylvania Philadelphia, USA {yding, dgildea, mpalmer}@linc.cis.upenn.edu Abstract Structural divergence presents a challenge to the use of syntax in statistical machine translation. We address this problem with a new algorithm for alignment of loosely matched non-isomorphic dependency trees. The algorithm selectively relaxes the constraints of the two tree structures while keeping computational complexity polynomial in the length of the sentences. Experimentation with a large Chinese-English corpus shows an improvement in alignment results over the unstructured models of (Brown et al., 1993). 1 Introduction The statistical approach to machine translation, pioneered by (Brown et al., 1990, 1993), estimates word to word translation probabilities and sentence reordering probabilities directly from a large corpus of parallel sentences. Despite their lack of any internal representation of syntax or semantics, the ability of such systems to leverage large amounts of training data has enabled them to perform competitively with more traditional interlingua based approaches. In recent years, hybrid approaches, which aim at applying statistical models to structural data, have begun to emer"
2003.mtsummit-papers.13,J94-4004,0,0.0597229,"eordering probabilities directly from a large corpus of parallel sentences. Despite their lack of any internal representation of syntax or semantics, the ability of such systems to leverage large amounts of training data has enabled them to perform competitively with more traditional interlingua based approaches. In recent years, hybrid approaches, which aim at applying statistical models to structural data, have begun to emerge. However, such approaches have been faced with the problem of pervasive structural divergence between languages, due to both systematic differences between languages (Dorr, 1994) and the vagaries of loose translations in real corpora. Syntax-based statistical approaches to alignment began with (Wu, 1997), who introduced a polynomial-time solution for the alignment problem based on synchronous binary trees. (Alshawi et al., 2000) extended the tree-based approach by representing each production in parallel dependency trees as a finite-state transducer. Both these approaches learn the tree representations directly from parallel sentences, and do not make allowance for non-isomorphic structures. (Yamada and Knight, 2001, 2002) model translation as a sequence of operations"
2003.mtsummit-papers.13,W02-1039,0,0.0523757,"Missing"
2003.mtsummit-papers.13,P00-1056,0,0.0522059,"translation model (TM). During the construction of a machine translation pipeline, the alignment problem is usually handled as part of the TM and P( f |e) = P ( f , a |e) , ∑ a where a is any possible alignment between e and f . This approach requires a generative translation model. However, when the alignment problem is viewed on its own, a generative model is not necessary. In other words, we can simply maximize P ( a |e, f ) using a conditional model. More straightforwardly, the alignment problem can be defined as Definition (1), which is equivalent to the alignment problem definition in (Och and Ney, 2000): Here, f j and ea j are words in the source and target language sentences f and e , respectively. 0 Definition (1) For each f j ∈ f , find a labeling ea j , where ea j ∈ e ∪ {0} stands for the “empty symbol”, which means f j could be aligned to nothing. This definition does not allow multiple English words being aligned to a same foreign language word. 2.2 Algorithm Outline We introduce the framework of the alignment algorithm by first looking at how the IBM models handle alignment. In Model 1, all connections for each foreign position are assumed to be equally likely, which implies that the"
2003.mtsummit-papers.13,J97-3002,0,0.125784,"syntax or semantics, the ability of such systems to leverage large amounts of training data has enabled them to perform competitively with more traditional interlingua based approaches. In recent years, hybrid approaches, which aim at applying statistical models to structural data, have begun to emerge. However, such approaches have been faced with the problem of pervasive structural divergence between languages, due to both systematic differences between languages (Dorr, 1994) and the vagaries of loose translations in real corpora. Syntax-based statistical approaches to alignment began with (Wu, 1997), who introduced a polynomial-time solution for the alignment problem based on synchronous binary trees. (Alshawi et al., 2000) extended the tree-based approach by representing each production in parallel dependency trees as a finite-state transducer. Both these approaches learn the tree representations directly from parallel sentences, and do not make allowance for non-isomorphic structures. (Yamada and Knight, 2001, 2002) model translation as a sequence of operations transforming a syntactic tree in one language into the string of the second language, making use of the output of an automatic"
2003.mtsummit-papers.13,P01-1067,0,0.161015,"languages, due to both systematic differences between languages (Dorr, 1994) and the vagaries of loose translations in real corpora. Syntax-based statistical approaches to alignment began with (Wu, 1997), who introduced a polynomial-time solution for the alignment problem based on synchronous binary trees. (Alshawi et al., 2000) extended the tree-based approach by representing each production in parallel dependency trees as a finite-state transducer. Both these approaches learn the tree representations directly from parallel sentences, and do not make allowance for non-isomorphic structures. (Yamada and Knight, 2001, 2002) model translation as a sequence of operations transforming a syntactic tree in one language into the string of the second language, making use of the output of an automatic parser in one of the two parallel languages. This allows the model to make use of the syntactic information provided by treebanks and the automatic parsers derived from them. While we would like to use syntactic information in both languages, the problem of non-isomorphism grows when trees in both languages are required to match. The use of probabilistic tree substitution grammars for tree-to-tree alignment (Hajic e"
2003.mtsummit-papers.13,P02-1039,0,0.0376038,"Missing"
2003.mtsummit-papers.13,J03-4003,0,\N,Missing
2003.mtsummit-papers.13,P03-1011,1,\N,Missing
2008.amta-papers.16,2003.mtsummit-papers.6,0,0.920264,"achine translation. Most of this effort has been directed at the translation model rather than the language model; however, the level of syntactic divergence in parallel text makes it difficult to reliably learn syntax-based translation rules. Focusing on syntax-based approaches within the language model might be a more direct way to address the poor grammatically of most machine translation output. In previous work on syntax-based language modeling for machine translation, Wu and Wong (1998) included the score of an unlexicalized probabilistic context-free grammar (PCFG) in an ITG framework. Charniak et al. (2003) rescored a tree-tostring translation forest with a lexicalized parser and found more grammatical output despite a lower BLEU score (compared to an ngram version). More recently, Shen et al. (2008) used a trigram dependency model, with quite good results. Common to 2 Motivation From a grammatical standpoint, the output of most machine translation systems is very poor. Part of the reason may be that most systems do not incorporate syntax-based language models. The nominal task of the language model is to guide the search (decoding) procedure towards grammatical output, yet most systems use ngra"
2008.amta-papers.16,A00-2018,0,0.113697,"ith the unioned alignment and fewer than eight words on either side, along with the counts. Probabilities were set to relative frequency. We then combined the two tables by linearly interpolating them, experimenting with different weights until we found one that produced the best results on the development set. The ngram language model was trained on the English side of this parallel corpus using SRILM (Stolcke, 2002). The parsers were trained the 49,208 trees from the Penn Treebank plus parses of most of the English side of our parallel data, which was automatically parsed with the parser of Charniak (2000). With both parsers, we treated commas, colons, and periods as if they were just regular words in the vocabulary. Quotation marks were treated as an unknown word, and we used the same set of fifty bins (computed based on word surface features) for unknown words used in Petrov et al. (2006). Our development data consisted of all Chinese sentences with twenty or fewer words from the NIST 2002 evaluation (371 sentences). The parameters of our log-linear model (which includes weights for the ngram and parser models, along with a length bonus) were set with a hill-climbing procedure on the developm"
2008.amta-papers.16,J07-2003,0,0.0475281,"y different component models with overlapping responsibilities. Most recent syntax-based systems put much of what might be considered the language model’s responsibility into the translation model, by learning translation rules that produce structured target-language output from (flat) input phrases. An example is Galley et al. (2006), in which target language structures are linguistically motivated treebank parse fragments with an extended domain of locality, allowing, for example, a phrasal translation pair to specify for (optionally lexicalized) argument structure. In a different approach, Chiang (2007) used a generic (linguistically uninformed) grammar whose translation rules permitted lexicalized reorderings between the source and target languages. More recently, Shen et al. (2008) extended Chiang’s system to learn pieces of dependency structure along with the rules. These approaches (summarized in Figure 1) have shown significant progress, but there are reasons why we might want to construct target-side syntax independent of the translation channel. One problem faced by tree-to-string systems is syntactic divergence. Translation and its evaluation are not welldefined tasks; at a high leve"
2008.amta-papers.16,P97-1003,0,0.0932663,"lation channel. This paper is distinct from previous work in that we decouple the structure of the target language tree from that of the synchronous grammar tree. Instead of extracting fragments of parse tree along with the phrase table and learning syntax-motivated reorderings, we start with simple phrase pairs and build the target-language structure at decoding time. We compare two syntax-based language models in the context of a fairly simple syntactic translation model, as a means of isolating the contributions and potential of these language models. The first is the statistical parser of Collins (1997) (Model 1). The second is based on the dependency parser of Klein and Manning (2004). Our translation model is a generic binary bracketing transduction grammar (Wu, 1997), whose main purpose is to restrict the alignment space to something that can be explored in polynomial time. Most work in syntax-based machine translation has been in translation modeling, but there are many reasons why we may instead want to focus on the language model. We experiment with parsers as language models for machine translation in a simple translation model. This approach demands much more of the language models,"
2008.amta-papers.16,P06-1121,0,0.223698,"ii, 21-25 October 2008] TM xRS Hiero BTG nition community. Furthermore, the distinction between the duties of the language and translation models is somewhat dubious; the noisy-channel approach to MT permits a decomposition into these two models, but modern log-linear systems use many different component models with overlapping responsibilities. Most recent syntax-based systems put much of what might be considered the language model’s responsibility into the translation model, by learning translation rules that produce structured target-language output from (flat) input phrases. An example is Galley et al. (2006), in which target language structures are linguistically motivated treebank parse fragments with an extended domain of locality, allowing, for example, a phrasal translation pair to specify for (optionally lexicalized) argument structure. In a different approach, Chiang (2007) used a generic (linguistically uninformed) grammar whose translation rules permitted lexicalized reorderings between the source and target languages. More recently, Shen et al. (2008) extended Chiang’s system to learn pieces of dependency structure along with the rules. These approaches (summarized in Figure 1) have show"
2008.amta-papers.16,P03-1054,0,0.0272966,"ions. This model comprises three distributions, one each for assigning probabilities to (a) head labels, (b) sibling labels and head tags, and (c) sibling head words. 3.1.2 One potential problem with the use of the Collins parser is that it posits a great deal of hidden structure. The Treebank grammar, or the model parameterization used in the Collins parser, is not necessarily the most useful for machine translation; in fact, many researchers have shown that the treebank grammar isn’t even the best choice for building parsers that are evaluated on their ability to recover treebank structure (Klein and Manning, 2003; Matsuzaki et al., 2005; Petrov and Klein, 2007). It stands to reason that a different grammar might do better as a language model for MT. To explore this, we evaluated the dependency model of Klein and Manning (2004). This model imagines the generative process as adding left and right arguments to a lexical head word until a decision to stop is made, and then recursing on each of those arguments. We changed the model to use word-based (instead of tag-based) parameters, and backed off to unigram probabilities for the sibling distributions (we also ignored the constituent context model describ"
2008.amta-papers.16,P04-1061,0,0.0705305,"ple the structure of the target language tree from that of the synchronous grammar tree. Instead of extracting fragments of parse tree along with the phrase table and learning syntax-motivated reorderings, we start with simple phrase pairs and build the target-language structure at decoding time. We compare two syntax-based language models in the context of a fairly simple syntactic translation model, as a means of isolating the contributions and potential of these language models. The first is the statistical parser of Collins (1997) (Model 1). The second is based on the dependency parser of Klein and Manning (2004). Our translation model is a generic binary bracketing transduction grammar (Wu, 1997), whose main purpose is to restrict the alignment space to something that can be explored in polynomial time. Most work in syntax-based machine translation has been in translation modeling, but there are many reasons why we may instead want to focus on the language model. We experiment with parsers as language models for machine translation in a simple translation model. This approach demands much more of the language models, allowing us to isolate their strengths and weaknesses. We find that unmodified parse"
2008.amta-papers.16,N03-1017,0,0.0497998,"the well-known French/English translation pair {ne X pas / do not X}. BTG cannot handle this alignment, but it can compensate by learning instantiations of the phrase pair for different values of X. It is worth pointing out explicitly that BTG in this form provides very little information to the translation process. In contrast to channel-rich models discussed earlier, BTG has only one parameter governing reordering, which expresses a preference for the straight binary rule over the inverted binary rule. This distortion model is even less informative than that of phrase-based systems such as Koehn et al. (2003)2 . An uninformative translation model allows us to isolate the influence of the language model in assembling the translation hypotheses. 3.1 not too difficult to implement and train, and its basic features – lexicalization and rule markovization – are at the core of the best generative parsers. The exact model we used is the three-level backoff model given in Table 7.1 of Collins (1999), together with special handling of punctuation and conjuctions. This model comprises three distributions, one each for assigning probabilities to (a) head labels, (b) sibling labels and head tags, and (c) sibl"
2008.amta-papers.16,P06-1096,0,0.0593212,"Missing"
2008.amta-papers.16,P05-1010,0,0.0175614,"s three distributions, one each for assigning probabilities to (a) head labels, (b) sibling labels and head tags, and (c) sibling head words. 3.1.2 One potential problem with the use of the Collins parser is that it posits a great deal of hidden structure. The Treebank grammar, or the model parameterization used in the Collins parser, is not necessarily the most useful for machine translation; in fact, many researchers have shown that the treebank grammar isn’t even the best choice for building parsers that are evaluated on their ability to recover treebank structure (Klein and Manning, 2003; Matsuzaki et al., 2005; Petrov and Klein, 2007). It stands to reason that a different grammar might do better as a language model for MT. To explore this, we evaluated the dependency model of Klein and Manning (2004). This model imagines the generative process as adding left and right arguments to a lexical head word until a decision to stop is made, and then recursing on each of those arguments. We changed the model to use word-based (instead of tag-based) parameters, and backed off to unigram probabilities for the sibling distributions (we also ignored the constituent context model described in that paper). Parsi"
2008.amta-papers.16,P02-1040,0,0.0766231,"ures) for unknown words used in Petrov et al. (2006). Our development data consisted of all Chinese sentences with twenty or fewer words from the NIST 2002 evaluation (371 sentences). The parameters of our log-linear model (which includes weights for the ngram and parser models, along with a length bonus) were set with a hill-climbing procedure on the development data. These parameters were then used to produce results from our test data, which was the portion of the NIST 2003 evaluation dataset with no more than twenty words (347 sentences). Our evaluation metric was case-insensitive BLEU-4 (Papineni et al., 2002). Table 1 contains the results of runs on the development and test sets. 5 RUN D EV /10 D EV /4 T EST /4 No LM Bigram Trigram Collins + bigram Dep + bigram 17.35 25.09 26.18 25.00 24.49 13.12 19.18 20.08 18.92 18.73 15.41 18.62 21.55 18.13 18.52 Table 1: Results (BLEU-4 scores). The number following the slash indicates the number of human references used in computing the BLEU score. No post-processing was applied to the MT output. The weight for the parser score was only allowed to go as low at 0.1, which is why the parser + bigram models are able to score slightly below the bigram model alone"
2008.amta-papers.16,N07-1051,0,0.0194814,"ne each for assigning probabilities to (a) head labels, (b) sibling labels and head tags, and (c) sibling head words. 3.1.2 One potential problem with the use of the Collins parser is that it posits a great deal of hidden structure. The Treebank grammar, or the model parameterization used in the Collins parser, is not necessarily the most useful for machine translation; in fact, many researchers have shown that the treebank grammar isn’t even the best choice for building parsers that are evaluated on their ability to recover treebank structure (Klein and Manning, 2003; Matsuzaki et al., 2005; Petrov and Klein, 2007). It stands to reason that a different grammar might do better as a language model for MT. To explore this, we evaluated the dependency model of Klein and Manning (2004). This model imagines the generative process as adding left and right arguments to a lexical head word until a decision to stop is made, and then recursing on each of those arguments. We changed the model to use word-based (instead of tag-based) parameters, and backed off to unigram probabilities for the sibling distributions (we also ignored the constituent context model described in that paper). Parsing models The task of a s"
2008.amta-papers.16,P06-1055,0,0.0146268,"lts on the development set. The ngram language model was trained on the English side of this parallel corpus using SRILM (Stolcke, 2002). The parsers were trained the 49,208 trees from the Penn Treebank plus parses of most of the English side of our parallel data, which was automatically parsed with the parser of Charniak (2000). With both parsers, we treated commas, colons, and periods as if they were just regular words in the vocabulary. Quotation marks were treated as an unknown word, and we used the same set of fifty bins (computed based on word surface features) for unknown words used in Petrov et al. (2006). Our development data consisted of all Chinese sentences with twenty or fewer words from the NIST 2002 evaluation (371 sentences). The parameters of our log-linear model (which includes weights for the ngram and parser models, along with a length bonus) were set with a hill-climbing procedure on the development data. These parameters were then used to produce results from our test data, which was the portion of the NIST 2003 evaluation dataset with no more than twenty words (347 sentences). Our evaluation metric was case-insensitive BLEU-4 (Papineni et al., 2002). Table 1 contains the results"
2008.amta-papers.16,P08-1066,0,0.319863,"ably learn syntax-based translation rules. Focusing on syntax-based approaches within the language model might be a more direct way to address the poor grammatically of most machine translation output. In previous work on syntax-based language modeling for machine translation, Wu and Wong (1998) included the score of an unlexicalized probabilistic context-free grammar (PCFG) in an ITG framework. Charniak et al. (2003) rescored a tree-tostring translation forest with a lexicalized parser and found more grammatical output despite a lower BLEU score (compared to an ngram version). More recently, Shen et al. (2008) used a trigram dependency model, with quite good results. Common to 2 Motivation From a grammatical standpoint, the output of most machine translation systems is very poor. Part of the reason may be that most systems do not incorporate syntax-based language models. The nominal task of the language model is to guide the search (decoding) procedure towards grammatical output, yet most systems use ngrams, which do not model sentence structure and cannot handle long-distance dependencies. There are a number of reasons that researchers have stuck with ngrams. They work well, are easy to train, req"
2008.amta-papers.16,P06-1123,0,0.0388551,"Missing"
2008.amta-papers.16,P98-2230,0,0.0376621,"es. 1 Introduction In the past few years there has been a burgeoning interest in syntax-based approaches to statistical machine translation. Most of this effort has been directed at the translation model rather than the language model; however, the level of syntactic divergence in parallel text makes it difficult to reliably learn syntax-based translation rules. Focusing on syntax-based approaches within the language model might be a more direct way to address the poor grammatically of most machine translation output. In previous work on syntax-based language modeling for machine translation, Wu and Wong (1998) included the score of an unlexicalized probabilistic context-free grammar (PCFG) in an ITG framework. Charniak et al. (2003) rescored a tree-tostring translation forest with a lexicalized parser and found more grammatical output despite a lower BLEU score (compared to an ngram version). More recently, Shen et al. (2008) used a trigram dependency model, with quite good results. Common to 2 Motivation From a grammatical standpoint, the output of most machine translation systems is very poor. Part of the reason may be that most systems do not incorporate syntax-based language models. The nominal"
2008.amta-papers.16,P96-1021,0,0.259346,"Missing"
2008.amta-papers.16,J97-3002,0,0.610291,"xtracting fragments of parse tree along with the phrase table and learning syntax-motivated reorderings, we start with simple phrase pairs and build the target-language structure at decoding time. We compare two syntax-based language models in the context of a fairly simple syntactic translation model, as a means of isolating the contributions and potential of these language models. The first is the statistical parser of Collins (1997) (Model 1). The second is based on the dependency parser of Klein and Manning (2004). Our translation model is a generic binary bracketing transduction grammar (Wu, 1997), whose main purpose is to restrict the alignment space to something that can be explored in polynomial time. Most work in syntax-based machine translation has been in translation modeling, but there are many reasons why we may instead want to focus on the language model. We experiment with parsers as language models for machine translation in a simple translation model. This approach demands much more of the language models, allowing us to isolate their strengths and weaknesses. We find that unmodified parsers do not improve BLEU scores over ngram language models, and provide an analysis of t"
2008.amta-papers.16,J03-4003,0,\N,Missing
2008.amta-papers.16,J05-4003,0,\N,Missing
2008.amta-papers.16,C98-2225,0,\N,Missing
2011.iwslt-evaluation.20,P05-1033,0,0.0219964,"niversity of Rochester Rochester, NY 14627 Abstract We discuss learning latent annotations for synchronous context-free grammars (SCFG) for the purpose of improving machine translation. We show that learning annotations for nonterminals results in not only more accurate translation, but also faster SCFG decoding. 1. Introduction Synchronous context-free grammar (SCFG) is an expressive yet computationally expensive model for syntax-based machine translation. Practical variations of SCFG-based models usually extract rules from data under a set of constraints. The hierarchical phrase-based model [1] is a restricted form of synchronous context-free grammar. The grammar has only one nonterminal, and extraction is based on word alignments of sentence pairs. GHKM [2] represents a more general form of synchronous context-free grammar. The grammar is extracted under the constraints of both word alignments of sentence pairs and target-side parse trees. In this paper, we discuss adding annotations to nonterminals in synchronous context-free grammars extracted using the GHKM method [2] to improve machine translation performance. Nonterminal refinement is a well-studied problem in English parsing,"
2011.iwslt-evaluation.20,N04-1035,0,0.190985,"chine translation. We show that learning annotations for nonterminals results in not only more accurate translation, but also faster SCFG decoding. 1. Introduction Synchronous context-free grammar (SCFG) is an expressive yet computationally expensive model for syntax-based machine translation. Practical variations of SCFG-based models usually extract rules from data under a set of constraints. The hierarchical phrase-based model [1] is a restricted form of synchronous context-free grammar. The grammar has only one nonterminal, and extraction is based on word alignments of sentence pairs. GHKM [2] represents a more general form of synchronous context-free grammar. The grammar is extracted under the constraints of both word alignments of sentence pairs and target-side parse trees. In this paper, we discuss adding annotations to nonterminals in synchronous context-free grammars extracted using the GHKM method [2] to improve machine translation performance. Nonterminal refinement is a well-studied problem in English parsing, and there is a long, successful history of refining English nonterminals to discover distributional differences. The fact that the granularity of nonterminals in the"
2011.iwslt-evaluation.20,J93-2004,0,0.0361741,"eral form of synchronous context-free grammar. The grammar is extracted under the constraints of both word alignments of sentence pairs and target-side parse trees. In this paper, we discuss adding annotations to nonterminals in synchronous context-free grammars extracted using the GHKM method [2] to improve machine translation performance. Nonterminal refinement is a well-studied problem in English parsing, and there is a long, successful history of refining English nonterminals to discover distributional differences. The fact that the granularity of nonterminals in the Penn English Treebank [3] is too coarse for automatic parsing has been widely discussed, and addressing the issue by refining English nonterminals has been shown to improve monolingual parsing performance. There are many ways to refine the set of nonterminals in a Treebank. Many of the previous attempts essentially try to weaken the strong independence assumptions of probabilistic context-free grammars by annotating nonterminals in different ways. For example, Johnson [4] takes the approach of simply annotating each node with its parent’s label. The effect of this is to refine the distribution of each nonterminal over"
2011.iwslt-evaluation.20,J98-4004,0,0.036278,"of refining English nonterminals to discover distributional differences. The fact that the granularity of nonterminals in the Penn English Treebank [3] is too coarse for automatic parsing has been widely discussed, and addressing the issue by refining English nonterminals has been shown to improve monolingual parsing performance. There are many ways to refine the set of nonterminals in a Treebank. Many of the previous attempts essentially try to weaken the strong independence assumptions of probabilistic context-free grammars by annotating nonterminals in different ways. For example, Johnson [4] takes the approach of simply annotating each node with its parent’s label. The effect of this is to refine the distribution of each nonterminal over sequences of children according to its position in the tree; for example, a VP beneath an SBAR node will have a different distribution over its children from a VP beneath an S node. This simple technique alone produces a large improvement in English Treebank parsing. Klein and Manning [5] expanded this idea with a series of experiments wherein they manually refined nonterminals to different degrees, which resulted in parsing accuracy rivaling tha"
2011.iwslt-evaluation.20,P03-1054,0,0.00406657,"ally try to weaken the strong independence assumptions of probabilistic context-free grammars by annotating nonterminals in different ways. For example, Johnson [4] takes the approach of simply annotating each node with its parent’s label. The effect of this is to refine the distribution of each nonterminal over sequences of children according to its position in the tree; for example, a VP beneath an SBAR node will have a different distribution over its children from a VP beneath an S node. This simple technique alone produces a large improvement in English Treebank parsing. Klein and Manning [5] expanded this idea with a series of experiments wherein they manually refined nonterminals to different degrees, which resulted in parsing accuracy rivaling that of bilexicalized parsing models of the time. More recently, Petrov et al. [6] refined techniques originally proposed by Matsuzaki et al. [7] and Prescher [8] for automatically learning latent annotations, resulting in stateof-the-art parsing performance with cubic-time parsing algorithms. The main idea of these approaches is to learn latent annotation of nonterminals by splitting nonterminals and learning distributional differences o"
2011.iwslt-evaluation.20,P06-1055,0,0.225303,"The effect of this is to refine the distribution of each nonterminal over sequences of children according to its position in the tree; for example, a VP beneath an SBAR node will have a different distribution over its children from a VP beneath an S node. This simple technique alone produces a large improvement in English Treebank parsing. Klein and Manning [5] expanded this idea with a series of experiments wherein they manually refined nonterminals to different degrees, which resulted in parsing accuracy rivaling that of bilexicalized parsing models of the time. More recently, Petrov et al. [6] refined techniques originally proposed by Matsuzaki et al. [7] and Prescher [8] for automatically learning latent annotations, resulting in stateof-the-art parsing performance with cubic-time parsing algorithms. The main idea of these approaches is to learn latent annotation of nonterminals by splitting nonterminals and learning distributional differences of split nonterminals using the EM algorithm [9]. Improvements presented by Petrov et al. [6] include merging back frivolous splits and smoothing the parameters. There have been efforts to expand the idea of nonterminal annotation to synchro"
2011.iwslt-evaluation.20,P05-1010,0,0.023922,"minal over sequences of children according to its position in the tree; for example, a VP beneath an SBAR node will have a different distribution over its children from a VP beneath an S node. This simple technique alone produces a large improvement in English Treebank parsing. Klein and Manning [5] expanded this idea with a series of experiments wherein they manually refined nonterminals to different degrees, which resulted in parsing accuracy rivaling that of bilexicalized parsing models of the time. More recently, Petrov et al. [6] refined techniques originally proposed by Matsuzaki et al. [7] and Prescher [8] for automatically learning latent annotations, resulting in stateof-the-art parsing performance with cubic-time parsing algorithms. The main idea of these approaches is to learn latent annotation of nonterminals by splitting nonterminals and learning distributional differences of split nonterminals using the EM algorithm [9]. Improvements presented by Petrov et al. [6] include merging back frivolous splits and smoothing the parameters. There have been efforts to expand the idea of nonterminal annotation to synchronous context-free grammars in order to improve machine translat"
2011.iwslt-evaluation.20,N06-1031,0,0.0154128,"automatically learning latent annotations, resulting in stateof-the-art parsing performance with cubic-time parsing algorithms. The main idea of these approaches is to learn latent annotation of nonterminals by splitting nonterminals and learning distributional differences of split nonterminals using the EM algorithm [9]. Improvements presented by Petrov et al. [6] include merging back frivolous splits and smoothing the parameters. There have been efforts to expand the idea of nonterminal annotation to synchronous context-free grammars in order to improve machine translation. Huang and Knight [10] presented various linguistically motivated annotations to the Penn English Treebank nonterminals — some of which were motivated by ideas by Klein and Manning [5]. Their results show that annotating nonterminals in target-side parse trees and then extracting an SCFG grammar from the trees improves machine translation performance. Huang et al. [11] applied the idea by Petrov et al. [6] to learn a latent distribution for the single Hiero nonterminal and used distributional similarities as a soft constraint for rewriting rules. Wang et al. [12] used the same split-merge approach to learn latent a"
2011.iwslt-evaluation.20,J10-2004,0,0.139025,"der to improve machine translation. Huang and Knight [10] presented various linguistically motivated annotations to the Penn English Treebank nonterminals — some of which were motivated by ideas by Klein and Manning [5]. Their results show that annotating nonterminals in target-side parse trees and then extracting an SCFG grammar from the trees improves machine translation performance. Huang et al. [11] applied the idea by Petrov et al. [6] to learn a latent distribution for the single Hiero nonterminal and used distributional similarities as a soft constraint for rewriting rules. Wang et al. [12] used the same split-merge approach to learn latent annotations of the target-side parse tree before extracting an SCFG grammar. This approach results in improvement in machine translation performance. However, it is possible that the nonterminal refinements that help parsing may not be optimal for the purposes of machine translation. In particular, since the internal nodes of GHKM translation rules are irrelevant to the decoding process, there is no need to for the translation model to predict them accurately. The decoder must, however, choose which translation rules to combine, and refined n"
2011.iwslt-evaluation.20,D07-1078,0,0.30423,"lso discuss the exact process of labeling nonterminals with latent annotations. Figure 1 describes the entire process of extracting SCFG rules, learning latent annotations, and generating the final grammar. Table 1 summarizes the process. We will discuss each step in more detail and highlight the difference between our approach and the baseline in the rest of the section. Our baseline is the work by Wang et al. [12], which is summarized in Table 2. 2.1. Parse tree binarization The issue of binarization comes up multiple times when learning latent annotations for SCFGs. As shown by Wang et al. [13], binarization of target-side parse trees before extracting GHKM rules improves translation quality by breaking larger rules into smaller rules with more generalization power. Target-side tree binarization generates virtual tree nodes that may be frontiers, which relax the constituent boundary constraints imposed by parse trees during rule extraction. In both the baseline and our grammar refining approach, we left-binarize English parse trees. However, for comparison, we also do experiments where we use the original English parse trees without binarization. Parse tree binarization is the first"
2011.iwslt-evaluation.20,P11-2072,1,0.803416,"binarization. Parse tree binarization is the first step in Figure 1. 2.2. Marking frontier nodes To learn latent annotation for SCFG, we need to extract GHKM rules from each sentence and reconstruct them as a rule tree for each sentence. In practice, using parsed English sentences, Chinese terminals and their alignments, we marked frontier nodes as defined in [2] and removed nonfrontier nodes. We slightly modify the definition of frontier nodes to avoid extracting unary rules: a frontier node cannot have a parent who covers the same source-side span. The details of this change can be found in [14]. Steps 2 and 3 of Figure 1 show marking frontier nodes and removing nonfrontier nodes. There is one exception to the process of removing non-frontier nodes. To aid EM, we do not remove preterminals even when they are not frontier nodes. In Figure 1, VBD and JJ are removed because they are not frontier nodes. However, in actual grammar training, they were not removed. To differentiate from the original English parse tree, we henceforth refer to a tree with non-frontier nodes removed as a rule tree (e.g., the tree after step 3 in Figure 1) and a normal English parse tree as a parse tree. 2.3. R"
2011.iwslt-evaluation.20,P03-1021,0,0.00770389,"Missing"
2011.iwslt-evaluation.20,J07-2003,0,0.131268,"Missing"
2011.iwslt-evaluation.20,P02-1040,0,0.0823094,"Missing"
2011.iwslt-evaluation.20,W04-3250,0,0.0676028,"Missing"
2011.iwslt-evaluation.20,J09-4009,1,0.873981,"Missing"
2011.iwslt-evaluation.20,D10-1014,0,0.0154878,"Petrov et al. [6] include merging back frivolous splits and smoothing the parameters. There have been efforts to expand the idea of nonterminal annotation to synchronous context-free grammars in order to improve machine translation. Huang and Knight [10] presented various linguistically motivated annotations to the Penn English Treebank nonterminals — some of which were motivated by ideas by Klein and Manning [5]. Their results show that annotating nonterminals in target-side parse trees and then extracting an SCFG grammar from the trees improves machine translation performance. Huang et al. [11] applied the idea by Petrov et al. [6] to learn a latent distribution for the single Hiero nonterminal and used distributional similarities as a soft constraint for rewriting rules. Wang et al. [12] used the same split-merge approach to learn latent annotations of the target-side parse tree before extracting an SCFG grammar. This approach results in improvement in machine translation performance. However, it is possible that the nonterminal refinements that help parsing may not be optimal for the purposes of machine translation. In particular, since the internal nodes of GHKM translation rules"
2011.iwslt-evaluation.20,P06-1085,0,\N,Missing
2011.iwslt-evaluation.20,D08-1076,0,\N,Missing
2020.cl-4.2,P02-1001,0,0.24077,"de Pass for Commutative Semiring procedure O UTSIDE C OMMUTATIVE for Items X in reverse topological order do Compute Z(X) with Equation (7) Set F¬X (x) = Z(X) ⊗ x Commutative semirings include the sum-product semiring used for finding the total probability of all parses of a string, as well as the max-product and max-sum (Viterbi) semirings used for finding the score of the best parse. Other examples include: the K-best semiring used to find the scores for the k best parses (Mohri 2002), the expectation semiring used to compute expected feature values for EM or for training log-linear models (Eisner 2002), the variance semiring used in minimum risk training of log-linear models (Li and Eisner 2009), the entropy semiring used to compute the entropy of the distribution over parses (Hwa 2004; Cortes et al. 2006), the generalized entropy semiring used to compute the relative entropy between two grammars (Cohen, Simmons, and Smith 2011), and the k-best+residual semiring used to find the k best scores and total score simultaneously (Gimpel and Smith 2009). Gimpel and Smith (2009) also define “generalized” semirings for approximate inference that do not meet all the criteria that define a semiring, b"
2020.cl-4.2,W16-5901,0,0.0337925,"Missing"
2020.cl-4.2,J99-4004,0,0.281973,"specific item. Weighted deduction systems can be used with different semirings, or even more generally, with other classes of functions for computing the values of items bottom–up in the inside pass. In some cases, efficient inside computation is possible, but efficient outside computation is not. How can these cases be characterized? We give a very general characterization of the conditions for efficient outside computation in terms of function composition, as well as three more specific examples of sufficient conditions. The first of these conditions, commutative semirings, is discussed by Goodman (1999), while we believe the other two, extremal semirings and the sum of linear functions, to be novel formulations. We discuss general superior functions as a case where efficient outside computation is not possible. We conclude that, despite the emphasis in the literature on describing weighted deduction in terms of semirings, semirings are not the best abstraction for describing the requirements of the general inside–outside algorithm. 2. Weighted Deduction A , . . . , An A weighted deduction system (Nederhof 2003) has rules of the form 1 where C A1 , . . . , An are the items of the system that"
2020.cl-4.2,J04-3001,0,0.0973853,"Missing"
2020.cl-4.2,D09-1005,0,0.045702,"topological order do Compute Z(X) with Equation (7) Set F¬X (x) = Z(X) ⊗ x Commutative semirings include the sum-product semiring used for finding the total probability of all parses of a string, as well as the max-product and max-sum (Viterbi) semirings used for finding the score of the best parse. Other examples include: the K-best semiring used to find the scores for the k best parses (Mohri 2002), the expectation semiring used to compute expected feature values for EM or for training log-linear models (Eisner 2002), the variance semiring used in minimum risk training of log-linear models (Li and Eisner 2009), the entropy semiring used to compute the entropy of the distribution over parses (Hwa 2004; Cortes et al. 2006), the generalized entropy semiring used to compute the relative entropy between two grammars (Cohen, Simmons, and Smith 2011), and the k-best+residual semiring used to find the k best scores and total score simultaneously (Gimpel and Smith 2009). Gimpel and Smith (2009) also define “generalized” semirings for approximate inference that do not meet all the criteria that define a semiring, but that have a commutative ⊗ operator and thus admit outside computation with Algorithm 3. 3.2"
2020.cl-4.2,E09-1061,0,0.693769,"dition” for the rule, as in Nederhof (2003), in which case the weight w1 of the rule can be incorporated into the function FR . Weighted deduction systems can be used to express other parsing algorithms, including Earley parsing and dependency parsing (Eisner and Satta 1999). Beyond CFG, weighted deduction systems are used for parsing for tree adjoining grammars (Alonso et al. 1999), combinatory categorical grammars (Kuhlmann and Satta 2014), and general linear context-free rewriting systems ¨ 2005), as well as for machine translation (Melamed, Satta, and (Burden and Ljunglof Wellington 2004; Lopez 2009). In all of these applications, a set of general deduction rules is instantiated into a hypergraph for a specific input string. For example, given a sentence of length n, the general rule is shown in Figure 1(a). The goal item for CFG parsing with start symbol S and sentence length n [S; 0; n] is instantiated into a specific rule for each combination of i, j, k ∈ {0, . . . , n}. Each instantiated item is a vertex in the hypergraph, and each instantiated rule is a hyperedge from the antecedent vertices to the consequent vertex. The resulting hypergraphs are also known as parse forests. In this"
2020.cl-4.2,P05-1010,0,0.0872173,"Missing"
2020.cl-4.2,P92-1017,0,0.547491,"|E |) can be treated as a constant. In this case, efficient outside computation is equivalent to time linear in the size of the hypergraph. Our definition of efficient outside computation does not explicitly require a top– down or outside pass through the deduction system. It is possible in some settings to compute the total weight of an item without an outside pass. For example, in CYK parsing, one can first eliminate all items not consistent with a fixed item denoting a particular pair of nonterminal and span, and one can then compute the total weight of all remaining derivations bottom–up (Pereira and Schabes 1992), as shown in Algorithm 1. Algorithm 1 Bottom–Up Computation of Total Weight procedure B OTTOM U P T OTALW EIGHT for Items [A, i, j] do Remove all items [A0 , i0 , j0 ] such that i < i0 < j < j0 or i0 < i < j0 < j Compute inside value of goal V(G) = V([S, 0, n]) γ([A, i, j]) ← V(G) Restore all items previously removed 750 Gildea Efficient Outside Computation For CYK parsing |E |∈ O(n3 ) with respect to the sentence length n. The outer loop of Algorithm 1 has O(n2 ) iterations, and each inside pass is O(n3 ), for a total runtime of O(n5 ). Thus, using this method to compute the total weight for"
2020.cl-4.2,P06-1055,0,0.0173223,"esentation of F¬X (x) that we have derived results in the following corollary of Theorem 1. Corollary 2 Efficient outside computation is possible for any extremal semiring whose operations can be computed in time O(g(|E |)). 3.3 Sum of Linear Functions As an example of a setting where efficient outside computation is possible even though the inside functions are not semiring operations, we consider the case of vectors as item weights. Components of these vectors correspond to latent variables or refined nonterminals in the latent variable parsing models of Matsuzaki, Miyao, and Tsujii (2005), Petrov et al. (2006), and Cohen et al. (2012). To make this concrete, we take as our starting point the tensor formulation of the inside–outside algorithm given by Collins and Cohen (2012). Inside values for an 756 Gildea Efficient Outside Computation item are vectors, and the function for computing inside values bottom–up consists of applying a three-dimensional tensor Ta→b c specific to a CFG rule a → b c to two vectors representing the inside values for nonterminals b and c. The function for computing inside values takes two vectors as arguments, and returns a vector that is linear in each argument: X a→b c FR"
2020.cl-4.2,J95-2002,0,0.736244,"= M weight (D) C(X, D) D:X∈D where C(X, D) is an integer indicating the number of times that item X appears in derivation D, and the product weight (D) C(X, D) indicates repeated addition with the semiring ⊕ operation. Inside values can be computed by solving a set of equations of the form of Equation (2). The equations may be linear, if an item can appear at most once as the antecedent of a rule (this is the case for unary chains in CFGs), or nonlinear, if an item can appear more than once (as can happen with CFGs with epsilon productions). Methods for solving such equations are discussed by Stolcke (1995) and Goodman (1999), with detailed complexity analysis by Etessami and Yannakakis (2009). For commutative semirings, computing outside values once inside values are known involves solving a similar set of equations. The outside equations are always linear, because they have only one outside value on the right-hand side. For extremal semirings, derivations with cycles can always be discarded, as they have weight less 759 Computational Linguistics Volume 46, Number 4 than the same derivation with the cycle removed, assuming that the inside value is well-defined. For the sum of linear functions,"
2020.coling-main.181,W13-2322,0,0.0938947,"re how this path diversity affects performance across levels of AMR connectivity, demonstrating gains on AMRs of higher reentrancy counts and diameters. Analysis of generated sentences also supports high semantic coherence of our models for reentrant AMRs. Our best model achieves a 1.4 BLEU and 1.8 CHR F++ margin over a baseline that encodes only pairwise-unique shortest paths. 1 Introduction Text generation from sequences manifests in tasks such as story generation, sumplease-01 marization, and question-answering. Generation from semantic structures including Abstract Meaning Representation (Banarescu et al., 2013) instead involves arg0 arg1 graph inputs. An AMR is a rooted, tree-like structure that captures propositional meaning through labeled vertices and edges called concepts and relations. In this man setting a model must recover the original sentence annotated by the AMR, which (a) The man is pleased lacks syntactic specification (e.g., tense and plurality). The model must grapple with himself. with the denser connectivity of semantic graphs in addition to ambiguity in predicting the surface string. want-01 The arbitrary structure of edges in a semantic graph allows it to express meaning arg1 comp"
2020.coling-main.181,P18-1026,0,0.0786578,"apple with himself. with the denser connectivity of semantic graphs in addition to ambiguity in predicting the surface string. want-01 The arbitrary structure of edges in a semantic graph allows it to express meaning arg1 compactly, yet also presents a challenge in learning global representations. Given think-01 arg0 sequential inputs, a neural encoder can repeatedly apply recurrent or convolutional arg1 operations to encode full strings. This local function sharing does not transfer arg0 listen-01 easily to the varied neighborhoods of graphs, however. arg1 Previous neural AMR-to-text models (Beck et al., 2018; Song et al., 2018) have girl favored graph neural networks (GNNs) that iteratively update each vertex’s state arg0 as a function of those in its first-order neighborhood. To capture paths within the boy AMR, these models include gated recurrent units within the GNN vertex update (b) The boy wants the function. Since paths may contain word order information, encoding them faithgirl to think he is fully could help a model’s decoder flatten the input graph into a target sequence. listening to her. Though a GNN applies to arbitrary graphs, its number of stacked layers controls the length of enco"
2020.coling-main.181,2020.acl-main.640,0,0.0759791,"hus, the clique formed by vertex-level attention coefficients may obscure true graph structure. In spite of the obstacles, recent AMR-to-text models have surpassed GNN-based encoders by using Transformer-based ones. Wang et al. (2020) and Yao et al. (2020) apply a form of graph attention networks (Veliˇckovi´c et al., 2017) that mask out non-adjacent vertices in self-attention layers. Similar to GNNs, these encoders only propagate vertex state through local neighborhoods. Zhu et al. (2019) remove the constraint by exploring relation encoders (e.g., SAN- or CNN-based) between all vertex pairs. Cai and Lam (2020) similarly learn relation embeddings from pairwise shortest paths fed to the model. They maintain that richer modeling of paths injects global graph structure into self-attention between vertices. In this work, we propose to include a mixture of paths in the relation encoding of each vertex pair. This can be done by applying generalized shortest-paths algorithms—Floyd-Warshall and adjacency matrix multiplication. We draw inspiration from a generalized shortest-paths framework (Mohri, 2002) to learn relation embeddings. The benefits of this approach are threefold: (i) shortest paths need not be"
2020.coling-main.181,N19-1366,0,0.0319825,"unded, the grammars suffered from data sparsity due to the structural complexity of graphs relative to corpus size. Amid the dominance of attentive recurrent models, Konstas et al. (2017) linearized AMRs and trained sequence transduction models in both the parsing and generation directions. They applied a paired training procedure such that the generator could be trained on synthetic data produced by the parser (and vice versa). Gated GNNs (Beck et al., 2018; Song et al., 2018) soon replaced the sequential encoders, reducing the need to compensate for noisy linearizations with synthetic data. Damonte and Cohen (2019) evaluated the use of GNNs over simpler sequential and tree encoders, showing that encoding reentrancies boosts generation performance. They further found that stacking a recurrent layer above a GNN is superior to previous gated GNNs, verifying the importance of modeling AMR sequential dependencies. Recently, Transformer self-attention has been adapted to encode AMRs for generation tasks. As previously noted, the worst-case communication cost between vertices is constant in SANs as opposed to linear in GNNs. Yet treating the input as a clique has the side-effect of weakening a model’s grasp of"
2020.coling-main.181,N16-1087,0,0.147246,"oncept ‘man’ has two incoming edges from ‘please-01’. Dropping either path between the two concepts would alter the AMR’s meaning. Shortest paths may also inadequately express the relation between concept pairs. Figure 1b shows two possible paths between ‘want-01’ and ‘boy’. The longer path in fact describes the boy’s role in the subgraph rooted at ‘think-01’—without which the model may fail to produce the pronoun he. Our relation encoder avoids both illustrated issues since it considers all graph paths. 2 Related Work Initial work in AMR-to-text generation relied on grammar-based approaches. Flanigan et al. (2016) used tree transducers to map between spanning tree representations of AMRs and strings. These transducers generalize finite-state transducers (FSTs) to recursively rewrite trees as strings. Song et al. (2017) bypassed tree extraction and learned a synchronous node replacement grammar (SNRG) to align graph and string fragments directly. Though such approaches were well-founded, the grammars suffered from data sparsity due to the structural complexity of graphs relative to corpus size. Amid the dominance of attentive recurrent models, Konstas et al. (2017) linearized AMRs and trained sequence t"
2020.coling-main.181,P16-1154,0,0.0274681,"get tokens in tandem with all source tokens. 2006 As in the model by Cai and Lam (2020), we initialize the decoder state at each step j with global vertex embedding x0 described in §3.1. This injects global content learned from the AMR along the length of the predicted token sequence. In a previous gated RNN model (Song et al., 2017), the first decoder state similarly consisted of an average over encoder vertex states. Following the decoder initialization, stacked layers of multi-head attention proceed as before. During token prediction, the AMR-to-text decoder also includes a copy mechanism (Gu et al., 2016) that is common among neural text generation models. The mechanism addresses data sparsity of rare tokens (e.g., numbers, dates, named entities) by allowing the model to directly copy from AMR concept labels. The probability of drawing from the copy distribution as opposed to generating from the target vocabulary is found via a single feedforward layer: pcopy = softmax(Wc hj ) for state hj ∈ Rdt of token j and learned weights Wc ∈ R2×dt . Given target vocabulary distribution p0 (y |hj ) stemming from decoder attention, the final token distribution is computed as X αi,j , (2) p(y |hj ) = (1 − p"
2020.coling-main.181,P17-1014,0,0.0296039,"n relied on grammar-based approaches. Flanigan et al. (2016) used tree transducers to map between spanning tree representations of AMRs and strings. These transducers generalize finite-state transducers (FSTs) to recursively rewrite trees as strings. Song et al. (2017) bypassed tree extraction and learned a synchronous node replacement grammar (SNRG) to align graph and string fragments directly. Though such approaches were well-founded, the grammars suffered from data sparsity due to the structural complexity of graphs relative to corpus size. Amid the dominance of attentive recurrent models, Konstas et al. (2017) linearized AMRs and trained sequence transduction models in both the parsing and generation directions. They applied a paired training procedure such that the generator could be trained on synthetic data produced by the parser (and vice versa). Gated GNNs (Beck et al., 2018; Song et al., 2018) soon replaced the sequential encoders, reducing the need to compensate for noisy linearizations with synthetic data. Damonte and Cohen (2019) evaluated the use of GNNs over simpler sequential and tree encoders, showing that encoding reentrancies boosts generation performance. They further found that sta"
2020.coling-main.181,P02-1040,0,0.106444,"Missing"
2020.coling-main.181,W17-4770,0,0.0246882,"Missing"
2020.coling-main.181,P16-1162,0,0.0131119,"pairwise vertex distance) per AMR is 3.43 for both evaluation splits. The best model also featured relation embeddings of size 216 and a final relation projection of size 512. The inter-relation SAN employs hidden states of size 216 and eight attention heads. 2009 5.3 Results Performance on AMR-to-text generation from the LDC2017T10 corpus is reported relative to the baseline model results. We outperform all previous systems in §2 that primarily target the graph encoder except those of Zhu et al. (2019) and Yao et al. (2020). The former uses a different preprocessing method that includes BPE (Sennrich et al., 2016) applied to a shared concept-token vocabulary. Their ablation studies show that this method raises their baseline’s scores by 6.1 BLEU and 8.4 CHR F++ on LDC2015E86. Although these vocabulary changes may complement our proposed models, we do not include them since the authors do not release their preprocessing code. Yao et al. (2020) also applies BPE to AMR concept labels, without which their model suffers a 2.0 drop in BLEU on LDC2015E86. In addition, they apply Levi graph transformation and encode several views of the AMR graphs (e.g., reverse edges only, fully connected). Since their work i"
2020.coling-main.181,N18-2074,0,0.0307295,"Linguistics, pages 2004–2013 Barcelona, Spain (Online), December 8-13, 2020 Rather than preserving state between distant vertices using RNNs, another approach is to bring vertex pairs closer together. This idea has been popularized by Transformer (Vaswani et al., 2017) self-attention networks (SANs) on sequences. Relaxing the linear nature of RNNs, SANs support parallelized attention on any subset of tokens per encoding step. The SAN’s blindness to sequential positions seems ideal for unordered graph vertices. However, this lack of order also precludes the use of relative position embeddings (Shaw et al., 2018) to distinguish graph vertices within self-attention. Thus, the clique formed by vertex-level attention coefficients may obscure true graph structure. In spite of the obstacles, recent AMR-to-text models have surpassed GNN-based encoders by using Transformer-based ones. Wang et al. (2020) and Yao et al. (2020) apply a form of graph attention networks (Veliˇckovi´c et al., 2017) that mask out non-adjacent vertices in self-attention layers. Similar to GNNs, these encoders only propagate vertex state through local neighborhoods. Zhu et al. (2019) remove the constraint by exploring relation encode"
2020.coling-main.181,P17-2002,1,0.918959,"igure 1b shows two possible paths between ‘want-01’ and ‘boy’. The longer path in fact describes the boy’s role in the subgraph rooted at ‘think-01’—without which the model may fail to produce the pronoun he. Our relation encoder avoids both illustrated issues since it considers all graph paths. 2 Related Work Initial work in AMR-to-text generation relied on grammar-based approaches. Flanigan et al. (2016) used tree transducers to map between spanning tree representations of AMRs and strings. These transducers generalize finite-state transducers (FSTs) to recursively rewrite trees as strings. Song et al. (2017) bypassed tree extraction and learned a synchronous node replacement grammar (SNRG) to align graph and string fragments directly. Though such approaches were well-founded, the grammars suffered from data sparsity due to the structural complexity of graphs relative to corpus size. Amid the dominance of attentive recurrent models, Konstas et al. (2017) linearized AMRs and trained sequence transduction models in both the parsing and generation directions. They applied a paired training procedure such that the generator could be trained on synthetic data produced by the parser (and vice versa). Ga"
2020.coling-main.181,P18-1150,1,0.917035,"with the denser connectivity of semantic graphs in addition to ambiguity in predicting the surface string. want-01 The arbitrary structure of edges in a semantic graph allows it to express meaning arg1 compactly, yet also presents a challenge in learning global representations. Given think-01 arg0 sequential inputs, a neural encoder can repeatedly apply recurrent or convolutional arg1 operations to encode full strings. This local function sharing does not transfer arg0 listen-01 easily to the varied neighborhoods of graphs, however. arg1 Previous neural AMR-to-text models (Beck et al., 2018; Song et al., 2018) have girl favored graph neural networks (GNNs) that iteratively update each vertex’s state arg0 as a function of those in its first-order neighborhood. To capture paths within the boy AMR, these models include gated recurrent units within the GNN vertex update (b) The boy wants the function. Since paths may contain word order information, encoding them faithgirl to think he is fully could help a model’s decoder flatten the input graph into a target sequence. listening to her. Though a GNN applies to arbitrary graphs, its number of stacked layers controls the length of encoded paths. Tuning th"
2020.coling-main.181,2020.acl-main.712,0,0.0250881,"ed to linear in GNNs. Yet treating the input as a clique has the side-effect of weakening a model’s grasp of AMR structure and long-range paths. We build on the relation encoder developed by Cai and Lam (2020) to bias self-attention on all paths in an AMR. Our methods mediate between the efficiency gains of SAN-based graph encoders and the need to capture fine-grained AMR topology. Progress in past models attests to the utility of high resolution path encoding for the language modeling objective. 2005 New lines of work have modified aspects of AMR-to-text generation besides the graph encoder. Song et al. (2020) explore autoencoder-style loss terms that encourage decoder states to reconstruct the graph. Ribeiro et al. (2020) avoid graph encoding entirely, instead applying pretrained language models on linearized AMRs. These additions of input reconstruction and pretrained embedding are orthogonal to the graph encoder changes we introduce. 3 Graph Transformer Here we establish a base adaptation of the Transformer to a graph-to-sequence model. Due to discrepancies between sequences and graphs, we highlight efforts to make the encoder SAN structure-aware. 3.1 Encoder For sequence transduction let X = {x"
2020.coling-main.181,2020.tacl-1.2,0,0.160016,"orks (SANs) on sequences. Relaxing the linear nature of RNNs, SANs support parallelized attention on any subset of tokens per encoding step. The SAN’s blindness to sequential positions seems ideal for unordered graph vertices. However, this lack of order also precludes the use of relative position embeddings (Shaw et al., 2018) to distinguish graph vertices within self-attention. Thus, the clique formed by vertex-level attention coefficients may obscure true graph structure. In spite of the obstacles, recent AMR-to-text models have surpassed GNN-based encoders by using Transformer-based ones. Wang et al. (2020) and Yao et al. (2020) apply a form of graph attention networks (Veliˇckovi´c et al., 2017) that mask out non-adjacent vertices in self-attention layers. Similar to GNNs, these encoders only propagate vertex state through local neighborhoods. Zhu et al. (2019) remove the constraint by exploring relation encoders (e.g., SAN- or CNN-based) between all vertex pairs. Cai and Lam (2020) similarly learn relation embeddings from pairwise shortest paths fed to the model. They maintain that richer modeling of paths injects global graph structure into self-attention between vertices. In this work, we pr"
2020.coling-main.181,D19-1548,0,0.682876,"recludes the use of relative position embeddings (Shaw et al., 2018) to distinguish graph vertices within self-attention. Thus, the clique formed by vertex-level attention coefficients may obscure true graph structure. In spite of the obstacles, recent AMR-to-text models have surpassed GNN-based encoders by using Transformer-based ones. Wang et al. (2020) and Yao et al. (2020) apply a form of graph attention networks (Veliˇckovi´c et al., 2017) that mask out non-adjacent vertices in self-attention layers. Similar to GNNs, these encoders only propagate vertex state through local neighborhoods. Zhu et al. (2019) remove the constraint by exploring relation encoders (e.g., SAN- or CNN-based) between all vertex pairs. Cai and Lam (2020) similarly learn relation embeddings from pairwise shortest paths fed to the model. They maintain that richer modeling of paths injects global graph structure into self-attention between vertices. In this work, we propose to include a mixture of paths in the relation encoding of each vertex pair. This can be done by applying generalized shortest-paths algorithms—Floyd-Warshall and adjacency matrix multiplication. We draw inspiration from a generalized shortest-paths frame"
2020.iwpt-1.8,P83-1021,0,0.781842,"is relies on the ability of a WLP to decompose the value of a proof to a combination of the values of the sub-proofs. Specifically, given a derivation tree, a WLP description automatically provides algorithms for calculating the inside and outside values. We provide analogous algorithms for calculating the inside and outside values for partial-semiring WLPs. Our outside formulation addresses the noncommutative nature of tensors themselves, and could be extended to cases where the underlying semiring is non-commutative using the techniques presented by Goodman (1998). 2 “Parsing as deduction” (Pereira and Warren, 1983) is an established framework that allows a number of parsing algorithms to be written as declarative rules and deductive systems (Shieber et al., 1995), and their correctness to be rigorously stated (Sikkel, 1998). Goodman (1999) has extended the parsing as deduction framework to arbitrary semirings and showed that various different values of interest could be computed using the same algorithm by changing the semiring. This led to the development of Dyna, a toolkit for declaratively specifying weighted logic programs, allowing concise implementation of a number of NLP algorithms (Eisner et al."
2020.iwpt-1.8,H05-1036,0,0.300308,"tensors over semirings is no longer a semiring, we prove that if the set of tensors have certain matching dimensions for the set of grammar rules they are assigned to, then they fulfill all the desirable properties relevant for the semiring parsing framework. This paves the way to use WLPs with latent variables, naturally improving the expressivity of the statistical model represented by the underlying WLP. Introducing a semiring framework like ours makes it easier to seamlessly incorporate latent variables into any execution model for dynamic programming algorithms (or software such as Dyna, Eisner et al. 2005, and other Prolog-like/WLP-like solvers). We focus on CFG parsing, however the same latent variable techniques can be applied to any weighted deduction system, including systems for parsing TAG, CCG and LCFRS, and systems for Machine Translation (Lopez, 2009). The methods we present for inside and outside computation can be used to learn latent refinements of a specified grammar for any of these tasks with EM (Dempster et al., 1977; Matsuzaki et al., 2005), or used as a backbone to create spectral learning algorithms (Hsu et al., 2012; Bailly et al., 2009; Cohen et al., 2014). valid derivatio"
2020.iwpt-1.8,C18-1258,0,0.0480097,"Missing"
2020.iwpt-1.8,E09-1037,0,0.0286376,"). These utilize the algebraic structure to efficiently track quantities needed by the expectationmaximization algorithm for parameter estimation. Their framework allows working with parameters in the form of vectors in Rn for a fixed n, coupled with a scalar in R≥0 . The semiring value of a path is roughly calculated by the multiplication of the scalars and (appropriately weighted) addition of the vectors. This is in contrast with our framework where weights could be tensors of arbitrary rank rather than only vectors, and the values of paths are calculated via tensor multiplication. Finally, Gimpel and Smith (2009) extended the semiring framework to a more general algebraic structure with the purpose of incorporating nonlocal features. Their extension comes at the cost that the new algebraic structure does not obey all the semiring axioms. Our framework differs from theirs in that under reasonable conditions, tensors of semirings do behave fully like regular semirings. 4 CFG derivations can naturally be represented as trees. We will use the notation hr : T1 . . . Tk i to represent a tree that has the node r as its root and T1 , . . . , Tk as its direct subtrees. We will use DG to denote the set of all d"
2020.iwpt-1.8,J99-4004,0,0.12792,"ariables, and demonstrates their applicability on discontinuous constituent parsing. Given the usefulness of latent variables, it would be desirable to have a generic inference mechanism for any latent variable grammar. WLPs can represent inference algorithms for probabilistic grammars effectively. However, this does not trivially extend to latent-variable models because latent variables are often represented as vectors, matrices and higher-order tensors, and these taken together no longer form a semiring. This is because in the semiring framework, values for deduction items Semiring parsing (Goodman, 1999) is an elegant framework for describing parsers by using semiring weighted logic programs. In this paper we present a generalization of this concept: latent-variable semiring parsing. With our framework, any semiring weighted logic program can be latentified by transforming weights from scalar values of a semiring to rank-n arrays, or tensors, of semiring values, allowing the modeling of latent variables within the semiring parsing framework. Semiring is too strong a notion when dealing with tensors, and we have to resort to a weaker structure: a partial semiring.1 We prove that this generaliz"
2020.iwpt-1.8,D09-1005,0,0.0425864,"erminals, and denote this as A = ⇒ σ. We will denote the language that a grammar G defines by ∗ L(G) = {σ|S = ⇒ σ}. The semiring characterization of possible values to assign to WLPs gave rise to the formulation of a number of novel semirings. One novel semiring of interest for purposes of learning parameters is the generalized entropy semiring (Cohen et al., 2008) which can be used to calculate the KL-divergence between the distribution of derivations induced by two weighted logic programs. Other two semirings of interest are expectation and variance semirings introduced by Eisner (2002) and Li and Eisner (2009). These utilize the algebraic structure to efficiently track quantities needed by the expectationmaximization algorithm for parameter estimation. Their framework allows working with parameters in the form of vectors in Rn for a fixed n, coupled with a scalar in R≥0 . The semiring value of a path is roughly calculated by the multiplication of the scalars and (appropriately weighted) addition of the vectors. This is in contrast with our framework where weights could be tensors of arbitrary rank rather than only vectors, and the values of paths are calculated via tensor multiplication. Finally, G"
2020.iwpt-1.8,E09-1061,0,0.0383367,"aves the way to use WLPs with latent variables, naturally improving the expressivity of the statistical model represented by the underlying WLP. Introducing a semiring framework like ours makes it easier to seamlessly incorporate latent variables into any execution model for dynamic programming algorithms (or software such as Dyna, Eisner et al. 2005, and other Prolog-like/WLP-like solvers). We focus on CFG parsing, however the same latent variable techniques can be applied to any weighted deduction system, including systems for parsing TAG, CCG and LCFRS, and systems for Machine Translation (Lopez, 2009). The methods we present for inside and outside computation can be used to learn latent refinements of a specified grammar for any of these tasks with EM (Dempster et al., 1977; Matsuzaki et al., 2005), or used as a backbone to create spectral learning algorithms (Hsu et al., 2012; Bailly et al., 2009; Cohen et al., 2014). valid derivation should correspond to a sequence of well defined semiring operations. For CFGs, we give a straightforward condition that ensures this is the case. This essentially boils down to making sure that each non-terminal corresponds to a fixed vector space dimension."
2020.iwpt-1.8,P05-1010,0,0.0358787,"kes it easier to seamlessly incorporate latent variables into any execution model for dynamic programming algorithms (or software such as Dyna, Eisner et al. 2005, and other Prolog-like/WLP-like solvers). We focus on CFG parsing, however the same latent variable techniques can be applied to any weighted deduction system, including systems for parsing TAG, CCG and LCFRS, and systems for Machine Translation (Lopez, 2009). The methods we present for inside and outside computation can be used to learn latent refinements of a specified grammar for any of these tasks with EM (Dempster et al., 1977; Matsuzaki et al., 2005), or used as a backbone to create spectral learning algorithms (Hsu et al., 2012; Bailly et al., 2009; Cohen et al., 2014). valid derivation should correspond to a sequence of well defined semiring operations. For CFGs, we give a straightforward condition that ensures this is the case. This essentially boils down to making sure that each non-terminal corresponds to a fixed vector space dimension. For example, if A corresponds to a space of d1 dimensions, B to d2 and C to d3 , then a rule A → B C would have a tensor weight in d2 × d3 × d1 . As long as the weights are well defined, the standard"
2020.iwpt-1.8,J03-1006,0,0.174783,"ed descriptions of these semirings see Goodman (1999). Context-free Grammars Formally, a Context-Free Grammar (CFG) is a 4tuple hN, Σ, R, Si. The set of N denotes the nonterminals which will be denoted by uppercase letters A, B etc., and S is a non-terminal that is the special start symbol. The set of Σ denotes the terminals which will be denoted by lowercase letters a, b etc. R is the set of rules of the form A → α consisting of one non-terminal on the left hand side 2 Note that given a grammar G in a formalism F and a string α, it is possible to construct a CFG grammar c(G, w) from G and α (Nederhof, 2003). This construction is possible even for range concatenation grammars (Boullier, 2004) which span all languages that could be parsed in poly-time. 75 4.3 Weighted Logic Programming 4.4 In the context of parsing, Goodman (1999) presents a framework where a grammar G comes equipped with a function w that maps each rule in G to a semiring value. Then, a grammar derivation string E consisting of the successive applications of rules e1 ,Q . . . , en is defined to have the value n VG (E) = of a seni=1 w(ei ), and the value P tence σ ∈ L(G) is defined as VG = kj=1 VG (Ej ) where E1 , E2 , . . . , Ek"
2021.naacl-main.233,P99-1059,0,0.196508,"(w1 , w2 ): [A, i, j] [B, j, k] [S, i, k] Figure 1: A rule R for CFG parsing in weighted deduction notation for production S → A B. The goal item for CFG parsing with start symbol S and sentence length n is [S, 0, n]. Introduction efficient outside computation, allowing for a generWeighted deduction systems are used in a number alized inside-outside algorithm? In this paper, we of NLP applications, including parsing for contextanswer this question in the negative. We prove that free grammars (Shieber et al., 1995; Sikkel, 1997; a general algorithm for efficient outside computaNederhof, 2003; Eisner and Satta, 1999) and mation in this framework would imply the existence chine translation (Melamed et al., 2004; Lopez, of a subexponential time algorithm for satisfiability 2009). In these applications, the inside-outside alof boolean formulas in conjunctive normal form gorithm enables efficient calculation of the total (SAT), violating the Strong Exponential Time Hyweight of all derivations passing through a specific pothesis (SETH) (Impagliazzo and Paturi, 1999) item in the weighted deduction system by computwhich postulates that no such algorithms exist. This ing tables of inside and outside values. Goodm"
2021.naacl-main.233,2020.cl-4.2,1,0.8842,"s an instance of the minimum of superior functions framework1 that uses best-first search. Outside values are of particular interest for A* parsing because they can be used as admissible search heuristics (Pauls and Klein, 2009a), and to efficiently find the k best parses (Pauls and Klein, 2009b). When the function FR simply takes a product of its arguments, as in Pauls and Klein (2009b), efficient outside computation is possible. In this paper, we address the question of whether this is guaranteed by the minimum of superior functions framework or merely an artifact of this particular system. Gildea (2020) pointed out that there is no known efficient algorithm for outside computation in the minimum of superior functions framework. However, they did not present a formal hardness result. In this work, we prove that general efficient outside computation in this framework would yield a subquadratic time algorithm for the Orthogonal Vectors Problem, violating the Orthogonal Vectors Conjecture (Williams, 2005; Vassilevska Williams, 2015), which states that no such algorithms exist because their existence would violate the Strong Exponential Time Hypothesis (SETH) (Impagliazzo and Paturi, 1999) and yi"
2021.naacl-main.233,J99-4004,0,0.531919,"n to its left. This is exemplified in Figure 1, which shows an example rule for CFG parsing with items of the form [A, i, j], representing a subtree rooted by nonterminal A and spanning input tokens i + 1 through j. One item in the weighted deduction system is designated as the goal item, and the fundamental problem is to calculate the total weight of all derivations of this item, where the total weight is calculated using a generalized sum operation, writL ten . An extension of this is to calculate the total weight of all derivations of the goal item G that also contain item X, written γ(X) (Goodman, 1999): M γ(X) = weight(D) D:X,G∈D total weight of an item. They termed this “efficient outside computation.&quot; One important class of weighted deduction system is the minimum of superior functions (Knuth, 1977). In this framework, each rule weight function FR is a superior function, meaning that it is monotonically increasing in each argument and the result is always greater than or equal Lto each of its arguments. The generalized sum in this framework used for calculating total weight is the minimum operation: M γ(X) = weight(D) D:X,G∈D = min D:X,G∈D weight(D) Best-first search is possible in this f"
2021.naacl-main.233,N03-1016,0,0.227002,"ent and the result is always greater than or equal Lto each of its arguments. The generalized sum in this framework used for calculating total weight is the minimum operation: M γ(X) = weight(D) D:X,G∈D = min D:X,G∈D weight(D) Best-first search is possible in this framework using a generalization of Dijkstra’s algorithm (Nederhof, 2003). It is interesting to ask whether efficient outside computation is always possible within this framework, and even more generally, whether the conditions necessary for best-first search are sufficient for efficient outside computation. The A* parsing system of Klein and Manning (2003) is an instance of the minimum of superior functions framework1 that uses best-first search. Outside values are of particular interest for A* parsing because they can be used as admissible search heuristics (Pauls and Klein, 2009a), and to efficiently find the k best parses (Pauls and Klein, 2009b). When the function FR simply takes a product of its arguments, as in Pauls and Klein (2009b), efficient outside computation is possible. In this paper, we address the question of whether this is guaranteed by the minimum of superior functions framework or merely an artifact of this particular system"
2021.naacl-main.233,E09-1061,0,0.0675782,"Missing"
2021.naacl-main.233,P04-1084,0,0.126236,"n notation for production S → A B. The goal item for CFG parsing with start symbol S and sentence length n is [S, 0, n]. Introduction efficient outside computation, allowing for a generWeighted deduction systems are used in a number alized inside-outside algorithm? In this paper, we of NLP applications, including parsing for contextanswer this question in the negative. We prove that free grammars (Shieber et al., 1995; Sikkel, 1997; a general algorithm for efficient outside computaNederhof, 2003; Eisner and Satta, 1999) and mation in this framework would imply the existence chine translation (Melamed et al., 2004; Lopez, of a subexponential time algorithm for satisfiability 2009). In these applications, the inside-outside alof boolean formulas in conjunctive normal form gorithm enables efficient calculation of the total (SAT), violating the Strong Exponential Time Hyweight of all derivations passing through a specific pothesis (SETH) (Impagliazzo and Paturi, 1999) item in the weighted deduction system by computwhich postulates that no such algorithms exist. This ing tables of inside and outside values. Goodman result may be counterintuitive, because one might (1999) develops a generalized inside-outsi"
2021.naacl-main.233,J03-1006,0,0.621322,". 1 w1 : w2 : FR (w1 , w2 ): [A, i, j] [B, j, k] [S, i, k] Figure 1: A rule R for CFG parsing in weighted deduction notation for production S → A B. The goal item for CFG parsing with start symbol S and sentence length n is [S, 0, n]. Introduction efficient outside computation, allowing for a generWeighted deduction systems are used in a number alized inside-outside algorithm? In this paper, we of NLP applications, including parsing for contextanswer this question in the negative. We prove that free grammars (Shieber et al., 1995; Sikkel, 1997; a general algorithm for efficient outside computaNederhof, 2003; Eisner and Satta, 1999) and mation in this framework would imply the existence chine translation (Melamed et al., 2004; Lopez, of a subexponential time algorithm for satisfiability 2009). In these applications, the inside-outside alof boolean formulas in conjunctive normal form gorithm enables efficient calculation of the total (SAT), violating the Strong Exponential Time Hyweight of all derivations passing through a specific pothesis (SETH) (Impagliazzo and Paturi, 1999) item in the weighted deduction system by computwhich postulates that no such algorithms exist. This ing tables of inside"
2021.naacl-main.233,N09-1063,0,0.0464378,"first search is possible in this framework using a generalization of Dijkstra’s algorithm (Nederhof, 2003). It is interesting to ask whether efficient outside computation is always possible within this framework, and even more generally, whether the conditions necessary for best-first search are sufficient for efficient outside computation. The A* parsing system of Klein and Manning (2003) is an instance of the minimum of superior functions framework1 that uses best-first search. Outside values are of particular interest for A* parsing because they can be used as admissible search heuristics (Pauls and Klein, 2009a), and to efficiently find the k best parses (Pauls and Klein, 2009b). When the function FR simply takes a product of its arguments, as in Pauls and Klein (2009b), efficient outside computation is possible. In this paper, we address the question of whether this is guaranteed by the minimum of superior functions framework or merely an artifact of this particular system. Gildea (2020) pointed out that there is no known efficient algorithm for outside computation in the minimum of superior functions framework. However, they did not present a formal hardness result. In this work, we prove that ge"
2021.naacl-main.233,P09-1108,0,0.0474802,"first search is possible in this framework using a generalization of Dijkstra’s algorithm (Nederhof, 2003). It is interesting to ask whether efficient outside computation is always possible within this framework, and even more generally, whether the conditions necessary for best-first search are sufficient for efficient outside computation. The A* parsing system of Klein and Manning (2003) is an instance of the minimum of superior functions framework1 that uses best-first search. Outside values are of particular interest for A* parsing because they can be used as admissible search heuristics (Pauls and Klein, 2009a), and to efficiently find the k best parses (Pauls and Klein, 2009b). When the function FR simply takes a product of its arguments, as in Pauls and Klein (2009b), efficient outside computation is possible. In this paper, we address the question of whether this is guaranteed by the minimum of superior functions framework or merely an artifact of this particular system. Gildea (2020) pointed out that there is no known efficient algorithm for outside computation in the minimum of superior functions framework. However, they did not present a formal hardness result. In this work, we prove that ge"
C02-1132,P97-1003,0,0.124316,"Missing"
C02-1132,P95-1037,0,0.0416101,"systems. Our use of generative probabilistic models of argument structure also allows for language modeling applications independent of semantic interpretation. Language models based on head-modifier lexical dependencies in syntactic trees have been shown to have lower perplexity than n-gram language models and to reduce word-error rates for speech recognition (Chelba and Jelinek, 1999; Roark, 2001). Incorporating semantic classes and verb alternation behavior could improve such models’ performance. Automatically derived word clusters are used in the statistical parsers of Charniak (1997) and Magerman (1995). Incorporating alternation behavior into such models might improve parsing results as well. This paper focuses on evaluating probabilistic models of verb-argument structure in terms of how well they model unseen test data, as measured by perplexity. We will examine maximum likelihood bigram and trigram models, clustering models based on those of Rooth et al. (1999), as well as a new probabilistic model designed to capture alternations in verb-argument structure. 2 Capturing Alternation Behavior Automatic clustering of co-occurrences of verbs and their direct objects was first used to induce s"
C02-1132,H94-1020,0,0.045586,"Missing"
C02-1132,A00-2034,0,0.32493,"obabilistic models of verb argument structure trained on a corpus of verbs and their syntactic arguments. Models designed to represent patterns of verb alternation behavior are compared with generic clustering models in terms of the perplexity assigned to held-out test data. While the specialized models of alternation do not perform as well, closer examination reveals alternation behavior represented implicitly in the generic models. 1 Introduction Recent research into verb-argument structure has has attempted to acquire the syntactic alternation behavior of verbs directly from large corpora. McCarthy (2000), Merlo and Stevenson (2001), and Schulte im Walde (2000) have evaluated their systems’ accuracy against human judgments of verb classification, with the comprehensive verb classes of Levin (1993) often serving as a gold standard. Another area of research has focused on automatic clustering algorithms for verbs and their arguments with the goal of finding groups of semantically related words (Pereira et al., 1993; Rooth et al., 1999), without focusing specifically on alternation behavior. We aim to bring these strands of research together with a unified probabilistic model of verb argument str"
C02-1132,J94-2001,0,0.0449267,"best clustering model, which further reduces perplexity to 2.06 million. 6 Conclusion We have attempted to learn the mapping from syntactic position to semantic role in an unsupervised manner, and have evaluated the results in terms of our systems’ success as language model for unseen data. The models designed to explicit represent verb alternation behavior did not perform as well by this metric as other, simpler probability models. A perspective on this work can be gained by comparison with attempts at unsupervised learning of other natural language phenomena including partof-speech tagging (Merialdo, 1994) and syntactic dependencies (Carroll and Charniak, 1992; Paskin, 2001). While models trained using the Expectation Maximization algorithm do well at fitting the data, the results may not correspond to the human analyses they were intended to learn. Language does not exist in the abstract, but conveys information about the world, and the ultimate goal of grammar induction is not just to model strings but to extract this information. This suggests that although the probability models constrained to represent verb alternation behavior did not achieve the best perplexity results, they may be usefu"
C02-1132,J01-3003,0,0.311254,"s of verb argument structure trained on a corpus of verbs and their syntactic arguments. Models designed to represent patterns of verb alternation behavior are compared with generic clustering models in terms of the perplexity assigned to held-out test data. While the specialized models of alternation do not perform as well, closer examination reveals alternation behavior represented implicitly in the generic models. 1 Introduction Recent research into verb-argument structure has has attempted to acquire the syntactic alternation behavior of verbs directly from large corpora. McCarthy (2000), Merlo and Stevenson (2001), and Schulte im Walde (2000) have evaluated their systems’ accuracy against human judgments of verb classification, with the comprehensive verb classes of Levin (1993) often serving as a gold standard. Another area of research has focused on automatic clustering algorithms for verbs and their arguments with the goal of finding groups of semantically related words (Pereira et al., 1993; Rooth et al., 1999), without focusing specifically on alternation behavior. We aim to bring these strands of research together with a unified probabilistic model of verb argument structure incorporating alterna"
C02-1132,P93-1024,0,0.457247,"tly in the generic models. 1 Introduction Recent research into verb-argument structure has has attempted to acquire the syntactic alternation behavior of verbs directly from large corpora. McCarthy (2000), Merlo and Stevenson (2001), and Schulte im Walde (2000) have evaluated their systems’ accuracy against human judgments of verb classification, with the comprehensive verb classes of Levin (1993) often serving as a gold standard. Another area of research has focused on automatic clustering algorithms for verbs and their arguments with the goal of finding groups of semantically related words (Pereira et al., 1993; Rooth et al., 1999), without focusing specifically on alternation behavior. We aim to bring these strands of research together with a unified probabilistic model of verb argument structure incorporating alternation behavior. Unraveling the mapping between syntactic functions such as subject and object and semantic roles such as agent and patient is an important piece of the language understanding problem. Learning the alternation behavior of verbs automatically from unannotated text would significantly reduce the amount of labor needed to create text understanding systems, whether that labor"
C02-1132,J01-2004,0,0.0274644,"uld significantly reduce the amount of labor needed to create text understanding systems, whether that labor takes the form of writing lexical entries or of annotating semantic information to train statistical systems. Our use of generative probabilistic models of argument structure also allows for language modeling applications independent of semantic interpretation. Language models based on head-modifier lexical dependencies in syntactic trees have been shown to have lower perplexity than n-gram language models and to reduce word-error rates for speech recognition (Chelba and Jelinek, 1999; Roark, 2001). Incorporating semantic classes and verb alternation behavior could improve such models’ performance. Automatically derived word clusters are used in the statistical parsers of Charniak (1997) and Magerman (1995). Incorporating alternation behavior into such models might improve parsing results as well. This paper focuses on evaluating probabilistic models of verb-argument structure in terms of how well they model unseen test data, as measured by perplexity. We will examine maximum likelihood bigram and trigram models, clustering models based on those of Rooth et al. (1999), as well as a new"
C02-1132,P99-1014,0,0.584444,"els. 1 Introduction Recent research into verb-argument structure has has attempted to acquire the syntactic alternation behavior of verbs directly from large corpora. McCarthy (2000), Merlo and Stevenson (2001), and Schulte im Walde (2000) have evaluated their systems’ accuracy against human judgments of verb classification, with the comprehensive verb classes of Levin (1993) often serving as a gold standard. Another area of research has focused on automatic clustering algorithms for verbs and their arguments with the goal of finding groups of semantically related words (Pereira et al., 1993; Rooth et al., 1999), without focusing specifically on alternation behavior. We aim to bring these strands of research together with a unified probabilistic model of verb argument structure incorporating alternation behavior. Unraveling the mapping between syntactic functions such as subject and object and semantic roles such as agent and patient is an important piece of the language understanding problem. Learning the alternation behavior of verbs automatically from unannotated text would significantly reduce the amount of labor needed to create text understanding systems, whether that labor takes the form of wr"
C02-1132,C00-2108,0,0.393956,"orpus of verbs and their syntactic arguments. Models designed to represent patterns of verb alternation behavior are compared with generic clustering models in terms of the perplexity assigned to held-out test data. While the specialized models of alternation do not perform as well, closer examination reveals alternation behavior represented implicitly in the generic models. 1 Introduction Recent research into verb-argument structure has has attempted to acquire the syntactic alternation behavior of verbs directly from large corpora. McCarthy (2000), Merlo and Stevenson (2001), and Schulte im Walde (2000) have evaluated their systems’ accuracy against human judgments of verb classification, with the comprehensive verb classes of Levin (1993) often serving as a gold standard. Another area of research has focused on automatic clustering algorithms for verbs and their arguments with the goal of finding groups of semantically related words (Pereira et al., 1993; Rooth et al., 1999), without focusing specifically on alternation behavior. We aim to bring these strands of research together with a unified probabilistic model of verb argument structure incorporating alternation behavior. Unraveling the"
C02-1132,A00-2031,0,\N,Missing
C04-1055,swift-etal-2004-semi,1,\N,Missing
C04-1055,N04-1013,0,\N,Missing
C04-1055,J93-2004,0,\N,Missing
C04-1055,A00-2008,0,\N,Missing
C04-1055,W01-0521,1,\N,Missing
C04-1055,W04-0214,1,\N,Missing
C04-1055,J03-4003,0,\N,Missing
C04-1055,P03-1014,0,\N,Missing
C04-1055,P99-1010,0,\N,Missing
C04-1060,J90-2002,0,0.126615,"ranslation as a sequence of probabilistic operations transforming the syntactic parse tree of a sentence in one language into that of the other. The trees may be learned directly from parallel corpora (Wu, 1997), or provided by a parser trained on hand-annotated treebanks (Yamada and Knight, 2001). In this paper, we compare these approaches on Chinese-English and French-English datasets, and find that automatically derived trees result in better agreement with human-annotated word-level alignments for unseen test data. 1 Introduction Statistical approaches to machine translation, pioneered by Brown et al. (1990), estimate parameters for a probabilistic model of word-to-word correspondences and word re-orderings directly from large corpora of parallel bilingual text. In recent years, a number of syntactically motivated approaches to statistical machine translation have been proposed. These approaches assign a parallel tree structure to the two sides of each sentence pair, and model the translation process with reordering operations defined on the tree structure. The tree-based approach allows us to represent the fact that syntactic constituents tend to move as unit, as well as systematic differences i"
C04-1060,J93-2003,0,0.0331963,"Missing"
C04-1060,J94-4004,0,0.0553805,"ring model in the opposite direction, using Chinese trees and English strings. The Chinese training data was parsed with the Bikel (2002) parser, and used the Chinese Treebank parses for our test data. Results are shown in Table 3. Because the ITG is a symmetric, generative model, the ITG results in Table 3 are identical to those in Table 1. While the experiment does not show a significant improvement, it is possible that better parses for the training data might be equally important. Even when the automatic parser output is correct, the tree structure of the two languages may not correspond. Dorr (1994) categorizes sources of syntactic divergence between languages, and Fox (2002) analyzed a parallel French-English corpus, quantifying how often parse dependencies cross when projecting an English tree onto a French string. Even in this closely related language pair with generally similar word order, crossed dependencies were caused by such common occurrences as adverb modification of a verb, or the correspondence of “not” to “ne pas”. Galley et al. (2004) extract translation rules from a large parsed parallel corpus that extend in scope to tree fragments beyond a single node; we believe that a"
C04-1060,W02-1039,0,0.0693678,"The Chinese training data was parsed with the Bikel (2002) parser, and used the Chinese Treebank parses for our test data. Results are shown in Table 3. Because the ITG is a symmetric, generative model, the ITG results in Table 3 are identical to those in Table 1. While the experiment does not show a significant improvement, it is possible that better parses for the training data might be equally important. Even when the automatic parser output is correct, the tree structure of the two languages may not correspond. Dorr (1994) categorizes sources of syntactic divergence between languages, and Fox (2002) analyzed a parallel French-English corpus, quantifying how often parse dependencies cross when projecting an English tree onto a French string. Even in this closely related language pair with generally similar word order, crossed dependencies were caused by such common occurrences as adverb modification of a verb, or the correspondence of “not” to “ne pas”. Galley et al. (2004) extract translation rules from a large parsed parallel corpus that extend in scope to tree fragments beyond a single node; we believe that adding such larger-scale operations to the translation model is likely to signi"
C04-1060,N04-1035,0,0.0365642,"aining data might be equally important. Even when the automatic parser output is correct, the tree structure of the two languages may not correspond. Dorr (1994) categorizes sources of syntactic divergence between languages, and Fox (2002) analyzed a parallel French-English corpus, quantifying how often parse dependencies cross when projecting an English tree onto a French string. Even in this closely related language pair with generally similar word order, crossed dependencies were caused by such common occurrences as adverb modification of a verb, or the correspondence of “not” to “ne pas”. Galley et al. (2004) extract translation rules from a large parsed parallel corpus that extend in scope to tree fragments beyond a single node; we believe that adding such larger-scale operations to the translation model is likely to significantly improve the performance of syntactically supervised alignment. The syntactically supervised model has been found to outperform the IBM word-level alignment models of Brown et al. (1993) for translation by Yamada and Knight (2002). An evaluation for the alignment task, measuring agreement with human judges, also found the syntax-based model to outperform the IBM models."
C04-1060,P03-1011,1,0.805062,"to indicate that the syntactic structure in one language is given to the training procedure. It is important to note, however, that both algorithms are unsupervised in that they are not provided any hand-aligned training data. Rather, they both use Expectation Maximization to find an alignment model by iteratively improving the likelihood assigned to unaligned parallel sentences. Our evaluation is in terms of agreement with word-level alignments created by bilingual human annotators. We describe each of the models used in more detail in the next two sections, including the clone operation of Gildea (2003). The reader who is familiar with these models may proceed directly to our experiments in Section 4, and further discussion in Section 5. 2 The Inversion Transduction Grammar The Inversion Transduction Grammar of Wu (1997) can be thought as a a generative process which simultaneously produces strings in both languages through a series of synchronous context-free grammar productions. The grammar is restricted to binary rules, which can have the symbols in the right hand side appear in the same order in both languages, represented with square brackets: X → [Y Z] or the symbols may appear in reve"
C04-1060,P02-1050,0,0.0170174,"airs with a total of 276,113 Chinese words and 315,415 English words. The Chinese data were automatically segmented into tokens, and English capitalization was retained. We replace words occurring only once with an unknown word token, resulting in a Chinese vocabulary of 23,783 words and an English vocabulary of 27,075 words. Our hand-aligned data consisted of 48 sentence pairs also with less than 25 words in either language, for a total of 788 English words and 580 Chinese words. A separate development set of 49 sentence pairs was used to control overfitting. These sets were the data used by Hwa et al. (2002). The hand aligned test data consisted of 745 individual aligned word pairs. Words could be aligned oneto-many in either direction. This limits the performance achievable by our models; the IBM models allow one-to-many alignments in one direction only, while the tree-based models allow only one-to-one alignment unless the cloning operation is used. Our French-English experiments were based on data from the Canadian Hansards made available by Ulrich German. We used as training data 20,000 sentence pairs of no more than 25 words in either language. Our test data consisted of 447 sentence pairs o"
C04-1060,W03-0301,0,0.0367821,"ur French-English experiments were based on data from the Canadian Hansards made available by Ulrich German. We used as training data 20,000 sentence pairs of no more than 25 words in either language. Our test data consisted of 447 sentence pairs of no more than 30 words, hand aligned by Och and Ney (2000). A separate development set of 37 sentences was used to control overfitting. We used of vocabulary of words occurring at least 10 times in the entire Hansard corpus, resulting in 19,304 English words and 22,906 French words. Our test set is that used in the alignment evaluation organized by Mihalcea and Pederson (2003), though we retained sentence-initial capitalization, used a closed vocabulary, and restricted ourselves to a smaller training corpus. We parsed the English side of the data with the Collins parser. As an artifact of the parser’s probability model, it outputs sentence-final punctuation attached at the lowest level of the tree. We raised sentence-final punctuation to be a daughter of the tree’s root before training our parse-based model. As our Chinese-English test data did not include sentence-final punctuation, we also removed it from our French-English test set. We evaluate our translation m"
C04-1060,P00-1056,0,0.637948,"745 individual aligned word pairs. Words could be aligned oneto-many in either direction. This limits the performance achievable by our models; the IBM models allow one-to-many alignments in one direction only, while the tree-based models allow only one-to-one alignment unless the cloning operation is used. Our French-English experiments were based on data from the Canadian Hansards made available by Ulrich German. We used as training data 20,000 sentence pairs of no more than 25 words in either language. Our test data consisted of 447 sentence pairs of no more than 30 words, hand aligned by Och and Ney (2000). A separate development set of 37 sentences was used to control overfitting. We used of vocabulary of words occurring at least 10 times in the entire Hansard corpus, resulting in 19,304 English words and 22,906 French words. Our test set is that used in the alignment evaluation organized by Mihalcea and Pederson (2003), though we retained sentence-initial capitalization, used a closed vocabulary, and restricted ourselves to a smaller training corpus. We parsed the English side of the data with the Collins parser. As an artifact of the parser’s probability model, it outputs sentence-final punc"
C04-1060,J97-3002,0,0.687951,"ucture to the two sides of each sentence pair, and model the translation process with reordering operations defined on the tree structure. The tree-based approach allows us to represent the fact that syntactic constituents tend to move as unit, as well as systematic differences in word order in the grammars of the two languages. Furthermore, the tree structure allows us to make probabilistic independence assumptions that result in polynomial time algorithms for estimating a translation model from parallel training data, and for finding the highest probability translation given a new sentence. Wu (1997) modeled the reordering process with binary branching trees, where each production could be either in the same or in reverse order going from source to target language. The trees of Wu’s Inversion Transduction Grammar were derived by synchronously parsing a parallel corpus, using a grammar with lexical translation probabilities at the leaves and a simple grammar with a single nonterminal providing the tree structure. While this grammar did not represent traditional syntactic categories such as verb phrases and noun phrases, it served to restrict the word-level alignments considered by the syst"
C04-1060,C02-1145,0,0.0209827,"years old, will join the board as a nonexecutive director Nov. 29. contrasts dramatically with In the past when education on opposing Communists and on resisting Russia was stressed, retaking the mainland and unifying China became a slogan for the authoritarian system, which made the unification under the martial law a tool for oppressing the Taiwan people. a typical sentence from our corpus. While we did not have human-annotated goldstandard parses for our training data, we did have human annotated parses for the Chinese side of our test data, which was taken from the Penn Chinese Treebank (Xue et al., 2002). We trained a second tree-to-string model in the opposite direction, using Chinese trees and English strings. The Chinese training data was parsed with the Bikel (2002) parser, and used the Chinese Treebank parses for our test data. Results are shown in Table 3. Because the ITG is a symmetric, generative model, the ITG results in Table 3 are identical to those in Table 1. While the experiment does not show a significant improvement, it is possible that better parses for the training data might be equally important. Even when the automatic parser output is correct, the tree structure of the tw"
C04-1060,P01-1067,0,0.881871,"e this grammar did not represent traditional syntactic categories such as verb phrases and noun phrases, it served to restrict the word-level alignments considered by the system to those allowable by reordering operations on binary trees. This restriction corresponds to intuitions about the alignments that could be produced by systematic differences between the two language’s grammars, and allows for a polynomial time algorithm for finding the highest-probability alignment, and for re-estimation of the lexical translation and grammar probabilities using the Expectation Maximization algorithm. Yamada and Knight (2001) present an algorithm for estimating probabilistic parameters for a similar model which represents translation as a sequence of re-ordering operations over children of nodes in a syntactic tree, using automatic parser output for the initial tree structures. This gives the translation model more information about the structure of the source language, and further constrains the reorderings to match not just a possible bracketing as in Wu (1997), but the specific bracketing of the parse tree provided. In this paper, we make a direct comparison of a syntactically unsupervised alignment model, base"
C04-1060,P02-1039,0,0.0154414,"d order, crossed dependencies were caused by such common occurrences as adverb modification of a verb, or the correspondence of “not” to “ne pas”. Galley et al. (2004) extract translation rules from a large parsed parallel corpus that extend in scope to tree fragments beyond a single node; we believe that adding such larger-scale operations to the translation model is likely to significantly improve the performance of syntactically supervised alignment. The syntactically supervised model has been found to outperform the IBM word-level alignment models of Brown et al. (1993) for translation by Yamada and Knight (2002). An evaluation for the alignment task, measuring agreement with human judges, also found the syntax-based model to outperform the IBM models. However, a relatively small corpus was used to train both models (2121 Japanese-English sentence pairs), and the evaluations were performed on the same data for training, meaning that one or both models might be significantly overfitting. Zens and Ney (2003) provide a thorough analysis of alignment constraints from the perspective of decoding algorithms. They train the models of Wu Perplexity 700 0.55 600 0.5 0.45 500 AER 0.4 400 1 2 3 4 5 6 7 8 9 Itera"
C04-1060,P03-1019,0,0.657901,"j, k) β(X, l, n, i, k)+= P (hY Zi|X)β(Y, m, n, i, j)β(Z, l, m, j, k) end for end for end for A similar recursion is used to compute outside probabilities for each chart item, and the inside and outside probabilities are combined to derive expected counts for occurrence of each grammar rule, including the rules corresponding to individual lexical translations. In our experiments we use a grammar with a start symbol S, a single preterminal C, and two nonterminals A and B used to ensure that only one parse can generate any given word-level alignment (ignoring insertions and deletions) (Wu, 1997; Zens and Ney, 2003). The individual lexical translations produced by the grammar may include a NULL word on either side, in order to represent insertions and deletions. 3 The Tree-To-String Model The model of Yamada and Knight (2001) can be thought of as a generative process taking a tree in one language as input and producing a string in the other through a sequence of probabilistic operations. If we follow the process of an English sentence’s transformation into French, the English sentence is first given a syntactic tree representation by a statistical parser (Collins, 1999). As the first step in the translat"
C04-1060,J03-4003,0,\N,Missing
C08-1136,J04-4002,0,0.779057,"put is a permutation, but in machine translation it is common to work with word-level alignments that are many-to-many; in general any set of pairs of words, one from each language, is a valid alignment for a given bilingual sentence pair. In this paper, we consider a generalized concept of common intervals given such an alignment: a common interval is a pair of phrases such that no word pair in the alignment links a word inside the phrase to a word outside the phrase. Extraction of such phrases is a common feature of state-of-the-art phrase-based and syntax-based machine translation systems (Och and Ney, 2004a; Chiang, 2005). We generalize Uno and Yagiura’s algorithm to this setting, and demonstrate a linear time algorithm for a pair of aligned sequences. The output is a tree representation of possible phrases, which directly provides a set of minimal synchronous grammar 1081 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1081–1088 Manchester, August 2008 rules for an SCFG-based machine translation system. For phrase-based machine translation, one can also read all phrase pairs consistent with the original alignment off of the tree in time linear"
C08-1136,H05-1101,0,0.0302716,"he algorithm of the previous section outputs the normalized decomposition tree depicted in Figure 2. From this tree, it is straightforward to obtain 3 It can be shown further that in this region, f shifts up or is unchanged. Therefore any reductions in step (4) must be in region (a). C → e4 F (1) e6 , f1 F (1) f3 E → e3 , f4 F → e5 , f2 G → e1 , f6 Figure 5: Each node from the normalized decomposition tree of Figure 2 is converted into an SCFG rule. a set of maximally-decomposed SCFG rules. As an example, the tree of Figure 2 produces the rules shown in Figure 5. We adopt the SCFG notation of Satta and Peserico (2005). Each rule has a right-hand side sequence for both languages, separated by a comma. Superscript indices in the right-hand side of grammar rules such as: A → B (1) C (2) , C (2) B (1) indicate that the nonterminals with the same index are linked across the two languages, and will eventually be rewritten by the same rule application. The example above inverts the order of B and C when translating from the source language to the target language. The SCFG rule extraction proceeds as follows. Assign a nonterminal label to each node in the tree. Then for each node (S, T ) in the tree top-down, wher"
C08-1136,P06-1123,0,0.103616,"all the children of the nodes in the chain. Then, each of the subsequences {ηi , . . . , ηj |1 < i < j ≤ k} yields a valid phrase pair. In our example, the root of the tree of Figure 2 and its left child form such a chain, with three children; the subsequence {([3, 3], [4, 4]), ([4, 6], [1, 3])} yields the phrase ([3, 6], [1, 4]). In the case of unaligned words, we can also consider all combinations of their attachments, as discussed for SCFG rule extraction. 5 Experiments on Analyzing Word Alignments One application of our factorization algorithm is analyzing human-annotated word alignments. Wellington et al. (2006) argue for the necessity of discontinuous spans (i.e., for a formalism beyond Synchronous CFG) in order for synchronous parsing to cover human-annotated word alignment data under the constraint that rules have a rank of no more than two. In a related study, Zhang and Gildea (2007) analyze the rank of the Synchronous CFG derivation trees needed to parse the same data. The number of discontinuous spans and the rank determine the complexity of dynamic programming algorithms for synchronous parsing (alignment) or machine translation decoding. Both studies make simplifying assumptions on the alignm"
C08-1136,W07-0404,1,0.887063,"on interval is a set of numbers that are consecutive in both. The breakthrough algorithm of Uno and Yagiura (2000) computes all K common intervals of two length n permutations in O(n + K) time. This is achieved by designing data structures to index possible boundaries of common intervals as the computation proceeds, so that not all possible pairs of beginning and end points need to be considered. Landau et al. (2005) and Bui-Xuan et al. (2005) show that all common intervals can be encoded in O(n) space, and adapt Uno and Yagiura’s algorithm to produce this compact representation in O(n) time. Zhang and Gildea (2007) use similar techniques to factorize Synchronous Context Free Grammars in linear time. These previous algorithms assume that the input is a permutation, but in machine translation it is common to work with word-level alignments that are many-to-many; in general any set of pairs of words, one from each language, is a valid alignment for a given bilingual sentence pair. In this paper, we consider a generalized concept of common intervals given such an alignment: a common interval is a pair of phrases such that no word pair in the alignment links a word inside the phrase to a word outside the phr"
C08-1136,J93-2003,0,0.0412268,"Alignments and Phrase Pairs Let [x, y] denote the sequence of integers between x and y inclusive, and [x, y) the integers between x and y − 1 inclusive. An aligned sequence pair or simply an alignment is a tuple (E, F, A), where E = e1 · · · en and F = f1 · · · fm are strings, and A is a set of links (x, y), where 1 ≤ x ≤ n and 1 ≤ y ≤ m, connecting E and F . For most of this paper, since we are not concerned with the identity of the symbols in E and F , we will assume for simplicity that ei = i and fj = j, so that E = [1, n] and F = [1, m]. In the context of statistical machine translation (Brown et al., 1993), we may interpret E as an English sentence, F its translation in French, and A a representation of how the words correspond to each other in the two sentences. A pair of substrings [s, t] ⊂ E and [u, v] ⊂ F is a phrase pair (Och and Ney, 2004b) if and only if the subset of links emitted from [s, t] in E is equal to the subset of links emitted from [u, v] in F , and both are nonempty. Figure 1a shows an example of a many-tomany alignment, where E = [1, 6], F = [1, 7], and A = {(1, 6), (2, 5), (2, 7), (3, 4), (4, 1), (4, 3), (5, 2), (6, 1), (6, 3)}. The eight phrase pairs in this alignment are:"
C08-1136,P05-1033,1,0.933656,"analyze the maximum SCFG rule length needed to cover hand-aligned data from various language pairs. 1 Introduction Many recent syntax-based statistical machine translation systems fall into the general formalism of Synchronous Context-Free Grammars (SCFG), where the grammar rules are found by first aligning parallel text at the word level. From wordlevel alignments, such systems extract the grammar rules consistent either with the alignments and parse trees for one of languages (Galley et al., 2004), or with the the word-level alignments alone without reference to external syntactic analysis (Chiang, 2005), which is the scenario we address here. In this paper, we derive an optimal, linear-time algorithm for the problem of decomposing an arbitrary word-level alignment into SCFG rules such that each rule has at least one aligned word and is minimal in the sense that it cannot be further decomposed into smaller rules. Extracting minimal rules is of interest both because rules with fewer words are more likely to generalize to new data, c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some righ"
C08-1136,N04-1035,0,0.105225,"a Synchronous Context-Free Grammar (SCFG) with the simplest rules possible. We also use the algorithm to precisely analyze the maximum SCFG rule length needed to cover hand-aligned data from various language pairs. 1 Introduction Many recent syntax-based statistical machine translation systems fall into the general formalism of Synchronous Context-Free Grammars (SCFG), where the grammar rules are found by first aligning parallel text at the word level. From wordlevel alignments, such systems extract the grammar rules consistent either with the alignments and parse trees for one of languages (Galley et al., 2004), or with the the word-level alignments alone without reference to external syntactic analysis (Chiang, 2005), which is the scenario we address here. In this paper, we derive an optimal, linear-time algorithm for the problem of decomposing an arbitrary word-level alignment into SCFG rules such that each rule has at least one aligned word and is minimal in the sense that it cannot be further decomposed into smaller rules. Extracting minimal rules is of interest both because rules with fewer words are more likely to generalize to new data, c 2008. Licensed under the Creative Commons Attribution-"
C10-1081,P08-1024,0,0.0619879,"ment guarantees that a TTS template either covers parts of one source-side semantic role, or a few complete semantic roles. This advantage motivates us to use a TTS transducer as the MT system with which to demonstrate the use of the proposed semantic role features. Since it is hard to design a generative model to combine both the semantic role features and the TTS templates, we use a log-linear model to estimate the feature weights, by maximizing the conditional probabilities of the target strings given the source syntax trees. The log-linear model with latent variables has been discussed by Blunsom et al. (2008); we apply this technique to combine the TTS templates and the semantic role features. The remainder of the paper is organized as follows: Section 2 describes the semantic role features proposed for machine translation; Section 3 describes how semantic role features are used and trained in a TTS transducer; Section 4 presents the experimental results; and Section 5 gives the conclusion. 2 2.1 Semantic Role Features for Machine Translation Defining Semantic Roles There are two semantic standards with publicly available training data: PropBank (Palmer et al., 2005) and FrameNet (Johnson et al.,"
C10-1081,A00-2018,0,0.0198246,"s for the EM algorithm and to 0 4 Experiments for the log-linear model. The performance of these We train an English-to-Chinese translation system systems is shown in Table 1. We can see that the using the FBIS corpus, where 73,597 sentence EM algorithm, based only on TTS templates, is pairs are selected as the training data, and 500 slightly better than the baseline system. Adding sentence pairs with no more than 25 words on the semantic role features to the EM algorithm actuChinese side are selected for both the development ally hurts the performance, which is not surprising and test data.1 Charniak (2000)’s parser, trained on since the combination of the TTS templates and the Penn Treebank, is used to generate the English semantic role features does not yield a sound gensyntax trees. To compute the semantic roles for the erative model. The log-linear model based on TTS source trees, we use an in-house max-ent classifier templates achieves significantly better results than with features following Xue and Palmer (2004) and both the baseline system and the EM algorithm. Pradhan et al. (2004). The semantic role labeler Both improvements are significant at p < 0.05 is trained and tuned based on sec"
C10-1081,P98-1046,0,0.023413,"Missing"
C10-1081,W08-0306,0,0.0236163,"expected count of a feaThe baseline system in our experiments uses ture over all derivations given a pair of tree and the TTS templates generated by using GHKM string, can be computed using the modified inside- and the union of the two single-direction alignoutside algorithm described in Section 3.2, and ments generated by GIZA++. Unioning the two ECS 0 |T (fi ), the expected count of a feature over single-direction alignments yields better perforall possible target strings given the source tree, mance for the SSMT systems using TTS templates can be computed in a similar way to the partition (Fossum et al., 2008) than the two single-direction function described in Figure 7. With the objective alignments and the heuristic diagonal combination function and its derivatives, a variety of optimiza- (Koehn et al., 2003). The two single-direction tion methods can be used to obtain the best feature word alignments as well as the union are used to weights; we use LBFGS (Zhu et al., 1994) in our generate the initial TTS template set for both the experiments. To prevent the model from overfitting EM algorithm and the log-linear model. The inithe training data, a weighted Gaussian prior is used tial TTS templates"
C10-1081,N04-1035,0,0.0114253,"henever a semantic role is deleted in a TTS template’s RHS, the corresponding deletion penalty will be applied. 3.3 Training We describe two alternative methods for training the weights for the model’s features, including both the individual TTS templates and the semantic role features. The first method maximizes data likelihood as is standard in EM, while the second method maximizes conditional likelihood for a loglinear model following Blunsom et al. (2008). 3.3.1 Maximizing Data Likelihood The standard way to train a TTS translation model is to extract the minimum TTS templates using GHKM (Galley et al., 2004), and then normalize the frequency of the extracted TTS templates (Galley et al., 2004; Galley et al., 2006; Liu et al., 2006; Huang et al., 2006). The probability of the semantic features SRR and DR can be computed similarly, given that SRR and DR can be derived from the paired source/target sentences and the word alignments between them. We refer to this model as max-likelihood training and normalize the counts of TTS templates and semantic features based on their roots and predicates respectively. We wish to overcome noisy alignments from GIZA++ and learn better TTS rule probabilities by re"
C10-1081,P06-1121,0,0.108917,"pose semantic role features for a Tree-to-String transducer to model the reordering/deletion of source-side semantic roles. These semantic features, as well as the Tree-to-String templates, are trained based on a conditional log-linear model and are shown to significantly outperform systems trained based on Max-Likelihood and EM. We also show significant improvement in sentence fluency by using the semantic role features in the log-linear model, based on manual evaluation. 1 Introduction Syntax-based statistical machine translation (SSMT) has achieved significant progress during recent years (Galley et al., 2006; May and Knight, 2007; Liu et al., 2006; Huang et al., 2006), showing that deep linguistic knowledge, if used properly, can improve MT performance. Semantics-based SMT, as a natural extension to SSMT, has begun to receive more attention from researchers (Liu and Gildea, 2008; Wu and Fung, 2009). Semantic structures have two major advantages over syntactic structures in terms of helping machine translation. First of all, semantic roles tend to agree better between two languages than syntactic constituents (Fung et al., 2006). This property motivates the approach of using the consistency of sem"
C10-1081,N04-1014,0,0.0219137,"ring. A TTS template is composed of a left-hand side (LHS) and a right-hand side (RHS), where the LHS is a subtree pattern and the RHS is a sequence of variables and translated words. The variables in the RHS of a template correspond to the bottom level nonterminals in the LHS’s subtree pattern, and their relative order indicates the permutation desired at the point where the template is applied to translate one language to another. The variables are further transformed, and the recursive process goes on until there are no variables left. The formal description of a TTS transducer is given by Graehl and Knight (2004), and our baseline approach follows the Extended Tree-to-String Transducer defined by Huang et al. (2006). For a given derivation (decomposition into templates) of a syntax tree, the translation probability is computed as the product of the templates which generate both the source syntax trees and the target translations. P r(S |T, D∗ ) = Y P r(t) t∈D∗ Here, S denotes the target sentence, T denotes the source syntax tree, and D∗ denotes the derivation of T . In addition to the translation model, the 718 function DECODE(T ) for tree node v of T in bottom-up order do for template t applicable at"
C10-1081,W04-3250,0,0.139379,"Missing"
C10-1081,P06-1077,0,0.169497,"tring transducer to model the reordering/deletion of source-side semantic roles. These semantic features, as well as the Tree-to-String templates, are trained based on a conditional log-linear model and are shown to significantly outperform systems trained based on Max-Likelihood and EM. We also show significant improvement in sentence fluency by using the semantic role features in the log-linear model, based on manual evaluation. 1 Introduction Syntax-based statistical machine translation (SSMT) has achieved significant progress during recent years (Galley et al., 2006; May and Knight, 2007; Liu et al., 2006; Huang et al., 2006), showing that deep linguistic knowledge, if used properly, can improve MT performance. Semantics-based SMT, as a natural extension to SSMT, has begun to receive more attention from researchers (Liu and Gildea, 2008; Wu and Fung, 2009). Semantic structures have two major advantages over syntactic structures in terms of helping machine translation. First of all, semantic roles tend to agree better between two languages than syntactic constituents (Fung et al., 2006). This property motivates the approach of using the consistency of semantic roles to select MT outputs (Wu and"
C10-1081,D07-1038,0,0.0274016,"atures for a Tree-to-String transducer to model the reordering/deletion of source-side semantic roles. These semantic features, as well as the Tree-to-String templates, are trained based on a conditional log-linear model and are shown to significantly outperform systems trained based on Max-Likelihood and EM. We also show significant improvement in sentence fluency by using the semantic role features in the log-linear model, based on manual evaluation. 1 Introduction Syntax-based statistical machine translation (SSMT) has achieved significant progress during recent years (Galley et al., 2006; May and Knight, 2007; Liu et al., 2006; Huang et al., 2006), showing that deep linguistic knowledge, if used properly, can improve MT performance. Semantics-based SMT, as a natural extension to SSMT, has begun to receive more attention from researchers (Liu and Gildea, 2008; Wu and Fung, 2009). Semantic structures have two major advantages over syntactic structures in terms of helping machine translation. First of all, semantic roles tend to agree better between two languages than syntactic constituents (Fung et al., 2006). This property motivates the approach of using the consistency of semantic roles to select"
C10-1081,J05-1004,1,0.585784,"riables has been discussed by Blunsom et al. (2008); we apply this technique to combine the TTS templates and the semantic role features. The remainder of the paper is organized as follows: Section 2 describes the semantic role features proposed for machine translation; Section 3 describes how semantic role features are used and trained in a TTS transducer; Section 4 presents the experimental results; and Section 5 gives the conclusion. 2 2.1 Semantic Role Features for Machine Translation Defining Semantic Roles There are two semantic standards with publicly available training data: PropBank (Palmer et al., 2005) and FrameNet (Johnson et al., 2002). PropBank defines a set of semantic roles for the verbs in the Penn TreeBank using numbered roles. These roles are defined individually for each verb. For example, for the verb disappoint, the role name arg1 means experiencer, but for the verb wonder, role name arg1 means cause. FrameNet is motivated by the idea that a certain type of verbs can be gathered together to form a frame, and in the same frame, a set of semantic roles is defined and shared among the verbs. For example, the verbs boil, bake, and steam will be in frame apply heat, and they have the"
C10-1081,P02-1040,0,0.0870971,"reduces the computational cost, but also corresponds better to the TTS decoder, where the best MT output is 721 selected only among the possible candidates which can be generated from the input source tree using TTS templates. The derivative of the logarithm of the objective function (over the entire training corpus) w.r.t. a feature weight can be computed as: (n-gram language model, TTS templates, SRR, DR) weights of the transducer are tuned based on the development set using a grid-based line search, and the translation results are evaluated based on a single Chinese reference using BLEU-4 (Papineni et al., 2002). Huang et al. (2006) used characterbased BLEU as a way of normalizing inconsistent Q X ∂ log S,T P r(S |T ) Chinese word segmentation, but we avoid this prob= {ECD|S,T (fi ) − ECS 0 |T (fi )} ∂λi lem as the training, development, and test data are S,T from the same source. where ECD|S,T (fi ), the expected count of a feaThe baseline system in our experiments uses ture over all derivations given a pair of tree and the TTS templates generated by using GHKM string, can be computed using the modified inside- and the union of the two single-direction alignoutside algorithm described in Section 3.2"
C10-1081,N04-1030,0,0.0113272,"Moses (Koehn et al., 2007), and kept permuting the semantic roles of the MT output until they best matched the semantic roles in the source sentence. This approach shows the positive effect of applying semantic role constraints, but it requires re-tagging semantic roles for every permuted MT output and does not scale well to longer sentences. This paper explores ways of tightly integrating semantic role features (SRFs) into an MT system, rather than using them in post-processing or nbest re-ranking. Semantic role labeling (SRL) systems usually use sentence-wide features (Xue and Palmer, 2004; Pradhan et al., 2004; Toutanova et al., 2005); thus it is difficult to compute targetside semantic roles incrementally during decoding. Noticing that the source side semantic roles are easy to compute, we apply a compromise approach, where the target side semantic roles are generated by projecting the source side semantic roles using the word alignments between the source and target sentences. Since this approach does not perform true SRL on the target string, it cannot fully evaluate whether the source and target semantic structures are consistent. However, the approach does capture the semantic-level re-orderin"
C10-1081,P99-1014,0,0.0152784,"Missing"
C10-1081,2006.amta-papers.8,0,0.0547096,"o model the reordering/deletion of source-side semantic roles. These semantic features, as well as the Tree-to-String templates, are trained based on a conditional log-linear model and are shown to significantly outperform systems trained based on Max-Likelihood and EM. We also show significant improvement in sentence fluency by using the semantic role features in the log-linear model, based on manual evaluation. 1 Introduction Syntax-based statistical machine translation (SSMT) has achieved significant progress during recent years (Galley et al., 2006; May and Knight, 2007; Liu et al., 2006; Huang et al., 2006), showing that deep linguistic knowledge, if used properly, can improve MT performance. Semantics-based SMT, as a natural extension to SSMT, has begun to receive more attention from researchers (Liu and Gildea, 2008; Wu and Fung, 2009). Semantic structures have two major advantages over syntactic structures in terms of helping machine translation. First of all, semantic roles tend to agree better between two languages than syntactic constituents (Fung et al., 2006). This property motivates the approach of using the consistency of semantic roles to select MT outputs (Wu and Fung, 2009). Secondl"
C10-1081,N03-1017,0,0.0193932,"ide- and the union of the two single-direction alignoutside algorithm described in Section 3.2, and ments generated by GIZA++. Unioning the two ECS 0 |T (fi ), the expected count of a feature over single-direction alignments yields better perforall possible target strings given the source tree, mance for the SSMT systems using TTS templates can be computed in a similar way to the partition (Fossum et al., 2008) than the two single-direction function described in Figure 7. With the objective alignments and the heuristic diagonal combination function and its derivatives, a variety of optimiza- (Koehn et al., 2003). The two single-direction tion methods can be used to obtain the best feature word alignments as well as the union are used to weights; we use LBFGS (Zhu et al., 1994) in our generate the initial TTS template set for both the experiments. To prevent the model from overfitting EM algorithm and the log-linear model. The inithe training data, a weighted Gaussian prior is used tial TTS templates’ probabilities/weights are set to with the objective function. The variance of the their normalized counts based on the root of the Gaussian prior is tuned based on the development TTS template (Galley et"
C10-1081,P07-2045,0,0.00304724,"he readability of MT output. By skeleton, we mean the main structure of a sentence including the verbs and their arguments. In spite of the theoretical potential of the semantic roles, there has not been much success in using them to improve SMT systems. Liu and Gildea (2008) proposed a semantic role based Tree-to-String (TTS) transducer by adding semantic roles to the TTS templates. Their approach did not differentiate the semantic roles of different predicates, and did not always improve the TTS transducer’s performance. Wu and Fung (2009) took the output of a phrase-based SMT system Moses (Koehn et al., 2007), and kept permuting the semantic roles of the MT output until they best matched the semantic roles in the source sentence. This approach shows the positive effect of applying semantic role constraints, but it requires re-tagging semantic roles for every permuted MT output and does not scale well to longer sentences. This paper explores ways of tightly integrating semantic role features (SRFs) into an MT system, rather than using them in post-processing or nbest re-ranking. Semantic role labeling (SRL) systems usually use sentence-wide features (Xue and Palmer, 2004; Pradhan et al., 2004; Tout"
C10-1081,P05-1073,0,0.0100176,"007), and kept permuting the semantic roles of the MT output until they best matched the semantic roles in the source sentence. This approach shows the positive effect of applying semantic role constraints, but it requires re-tagging semantic roles for every permuted MT output and does not scale well to longer sentences. This paper explores ways of tightly integrating semantic role features (SRFs) into an MT system, rather than using them in post-processing or nbest re-ranking. Semantic role labeling (SRL) systems usually use sentence-wide features (Xue and Palmer, 2004; Pradhan et al., 2004; Toutanova et al., 2005); thus it is difficult to compute targetside semantic roles incrementally during decoding. Noticing that the source side semantic roles are easy to compute, we apply a compromise approach, where the target side semantic roles are generated by projecting the source side semantic roles using the word alignments between the source and target sentences. Since this approach does not perform true SRL on the target string, it cannot fully evaluate whether the source and target semantic structures are consistent. However, the approach does capture the semantic-level re-ordering of the sentences. We as"
C10-1081,N09-2004,0,0.280365,"ed based on Max-Likelihood and EM. We also show significant improvement in sentence fluency by using the semantic role features in the log-linear model, based on manual evaluation. 1 Introduction Syntax-based statistical machine translation (SSMT) has achieved significant progress during recent years (Galley et al., 2006; May and Knight, 2007; Liu et al., 2006; Huang et al., 2006), showing that deep linguistic knowledge, if used properly, can improve MT performance. Semantics-based SMT, as a natural extension to SSMT, has begun to receive more attention from researchers (Liu and Gildea, 2008; Wu and Fung, 2009). Semantic structures have two major advantages over syntactic structures in terms of helping machine translation. First of all, semantic roles tend to agree better between two languages than syntactic constituents (Fung et al., 2006). This property motivates the approach of using the consistency of semantic roles to select MT outputs (Wu and Fung, 2009). Secondly, the set of semantic roles of a predicate models the skeleton of a sentence, which is crucial to the readability of MT output. By skeleton, we mean the main structure of a sentence including the verbs and their arguments. In spite of"
C10-1081,W04-3212,0,0.0103042,"rase-based SMT system Moses (Koehn et al., 2007), and kept permuting the semantic roles of the MT output until they best matched the semantic roles in the source sentence. This approach shows the positive effect of applying semantic role constraints, but it requires re-tagging semantic roles for every permuted MT output and does not scale well to longer sentences. This paper explores ways of tightly integrating semantic role features (SRFs) into an MT system, rather than using them in post-processing or nbest re-ranking. Semantic role labeling (SRL) systems usually use sentence-wide features (Xue and Palmer, 2004; Pradhan et al., 2004; Toutanova et al., 2005); thus it is difficult to compute targetside semantic roles incrementally during decoding. Noticing that the source side semantic roles are easy to compute, we apply a compromise approach, where the target side semantic roles are generated by projecting the source side semantic roles using the word alignments between the source and target sentences. Since this approach does not perform true SRL on the target string, it cannot fully evaluate whether the source and target semantic structures are consistent. However, the approach does capture the sem"
C10-1081,C98-1046,0,\N,Missing
C10-1081,J08-3004,0,\N,Missing
C10-1081,W08-0308,1,\N,Missing
D09-1075,J93-2003,0,0.0181966,"Missing"
D09-1075,W08-0336,0,0.69833,"ems of both languages incorporate spaces between “words”, the granularity is too coarse compared with languages such as English. A single word in these languages is composed of several morphemes, which often correspond to separate words in English. These languages also form compound nouns more freely. Ideally, we want to find segmentations for source and target languages that create a one-toone mapping of words. However, this is not always straightforward for two major reasons. First, what the optimal tokenization for machine translation should be is not always clear. Zhang et al. (2008b) and Chang et al. (2008) show that getting the tokenization of one of the languages in In this paper, we explore unsupervised methods for tokenization, with the goal of automatically finding an appropriate tokenization for machine translation. We compare methods that have access to parallel corpora to methods that are trained solely using data from the source language. Unsupervised monolingual segmentation has been studied as a model of language acquisition (Goldwater et al., 2006), and as model of learning morphology in European languages (Goldsmith, 2001). Unsupervised segmentation using bilingual data has been att"
D09-1075,P96-1044,0,0.088803,"Missing"
D09-1075,P06-1085,0,0.248024,"d for two major reasons. First, what the optimal tokenization for machine translation should be is not always clear. Zhang et al. (2008b) and Chang et al. (2008) show that getting the tokenization of one of the languages in In this paper, we explore unsupervised methods for tokenization, with the goal of automatically finding an appropriate tokenization for machine translation. We compare methods that have access to parallel corpora to methods that are trained solely using data from the source language. Unsupervised monolingual segmentation has been studied as a model of language acquisition (Goldwater et al., 2006), and as model of learning morphology in European languages (Goldsmith, 2001). Unsupervised segmentation using bilingual data has been attempted for finding new translation pairs (Kikui and Yamamoto, 2002), and for finding good segmentation for Chinese in machine translation using Gibbs sampling (Xu et al., 2008). In this paper, further investigate the use of bilingual information to find tokenizations tailored for machine translation. We find a benefit not only for segmentation of languages with no space in the writing system (such as Chinese), but also for the smaller-scale tokenization prob"
D09-1075,D07-1031,0,0.00680609,"er for P (f |e) to control this situation. For both models, we also want a way to control the distribution of token length after tokenization. We address this problem by adding a length factor to our models. count(fi ) P (fi ) = P k count(fk ) We can compute these counts by making a single pass through the corpus. As in the bilingual model, we limit the maximum size of f for practical reasons and to prevent our model from learning unnecessarily long f . With P (f ), given a sequence of characters c, we can calculate the most likely segmentation using the Viterbi algorithm. 4.1 Beal (2003) and Johnson (2007) describe variational Bayes for hidden Markov model in detail, which can be directly applied to our bilingual model. With this Bayesian extension, the emission probability of our first model can be summarized as follows: ∗ s = argmax P (f ) s Our rationale for this model is that if a span of characters f = ci . . . cj is an independent token, it will occur often enough in different contexts that such a span of characters will have higher probability than other spans of characters that are not meaningful. For the rest of the paper, this model will be referred to as the monolingual model. 3.3 θe"
D09-1075,O03-4002,0,0.0299218,"was tokenized only using information from P (f ). We also trained MT systems using supervised tokenizations and tokenization requiring a minimal effort for the each language pair. For ChineseEnglish, the minimal effort tokenization is maximal tokenization where every Chinese character is segmented. Since a number of Chinese tokenizers are available, we have tried four different tokenizations for the supervised tokenizations. The first one is the LDC Chinese tokenizer available at the LDC website3 , which is compiled by Zhibiao Wu. The second tokenizer is a maxent-based tokenizer described by Xue (2003). The third and fourth tokenizations come from the CRF-based Stanford Chinese segmenter described by Chang et al. (2008). The difference between third and fourth tokenization comes from the different gold standard, the third one is based on Beijing University’s segmentation (pku) and the fourth one is based on Chinese Treebank (ctb). For Korean3 723 http://projects.ldc.upenn.edu/Chinese/LDC ch.htm Supervised Rule-based morphological analyzer LDC segmenter Xue’s segmenter Stanford segmenter (pku) Stanford segmenter (ctb) Unsupervised Splitting punctuation only Maximal (Character-based MT) Bilin"
D09-1075,W02-0704,0,0.0978816,"of the languages in In this paper, we explore unsupervised methods for tokenization, with the goal of automatically finding an appropriate tokenization for machine translation. We compare methods that have access to parallel corpora to methods that are trained solely using data from the source language. Unsupervised monolingual segmentation has been studied as a model of language acquisition (Goldwater et al., 2006), and as model of learning morphology in European languages (Goldsmith, 2001). Unsupervised segmentation using bilingual data has been attempted for finding new translation pairs (Kikui and Yamamoto, 2002), and for finding good segmentation for Chinese in machine translation using Gibbs sampling (Xu et al., 2008). In this paper, further investigate the use of bilingual information to find tokenizations tailored for machine translation. We find a benefit not only for segmentation of languages with no space in the writing system (such as Chinese), but also for the smaller-scale tokenization problem of normalizing between languages that include more or less information in a “word” as defined by the writing system, using Korean-English for our experiments. Here too, we find a benefit from using bil"
D09-1075,P08-1012,1,0.910711,"although the writing systems of both languages incorporate spaces between “words”, the granularity is too coarse compared with languages such as English. A single word in these languages is composed of several morphemes, which often correspond to separate words in English. These languages also form compound nouns more freely. Ideally, we want to find segmentations for source and target languages that create a one-toone mapping of words. However, this is not always straightforward for two major reasons. First, what the optimal tokenization for machine translation should be is not always clear. Zhang et al. (2008b) and Chang et al. (2008) show that getting the tokenization of one of the languages in In this paper, we explore unsupervised methods for tokenization, with the goal of automatically finding an appropriate tokenization for machine translation. We compare methods that have access to parallel corpora to methods that are trained solely using data from the source language. Unsupervised monolingual segmentation has been studied as a model of language acquisition (Goldwater et al., 2006), and as model of learning morphology in European languages (Goldsmith, 2001). Unsupervised segmentation using b"
D09-1075,W08-0335,0,0.418372,"although the writing systems of both languages incorporate spaces between “words”, the granularity is too coarse compared with languages such as English. A single word in these languages is composed of several morphemes, which often correspond to separate words in English. These languages also form compound nouns more freely. Ideally, we want to find segmentations for source and target languages that create a one-toone mapping of words. However, this is not always straightforward for two major reasons. First, what the optimal tokenization for machine translation should be is not always clear. Zhang et al. (2008b) and Chang et al. (2008) show that getting the tokenization of one of the languages in In this paper, we explore unsupervised methods for tokenization, with the goal of automatically finding an appropriate tokenization for machine translation. We compare methods that have access to parallel corpora to methods that are trained solely using data from the source language. Unsupervised monolingual segmentation has been studied as a model of language acquisition (Goldwater et al., 2006), and as model of learning morphology in European languages (Goldsmith, 2001). Unsupervised segmentation using b"
D09-1075,J99-1003,0,0.0181392,"al model, we pick the λ in the same manner as the monolingual model and run the Viterbi algorithm. After applying the length factor, what we have is a log-linear model for tokenization, with two feature functions with equal weights: the length factor and P (f ) learned from model. 5 Experiments 5.1 Data We tested our tokenization methods on two different language pairs: Chinese-English, and KoreanEnglish. For Chinese-English, we used FBIS newswire data. The Korean-English parallel data was collected from news websites and sentencealigned using two different tools described by Moore (2002) and Melamed (1999). We used subsets of each parallel corpus consisting of about 2M words and 60K sentences on the English side. For our development set and test set, Chinese-English had about 1000 sentences each with 10 reference translations taken from the NIST 2002 MT evaluation. For Korean-English, 2200 sentence pairs were randomly sampled from the parallel corpus, and held out from the training data. These were divided in half and used for test set and development set respectively. For all language pairs, very minimal tokenization — splitting off punctuation — was done on the English side. 5.2 Experimental"
D09-1075,moore-2002-fast,0,0.0233354,"from the bilingual model, we pick the λ in the same manner as the monolingual model and run the Viterbi algorithm. After applying the length factor, what we have is a log-linear model for tokenization, with two feature functions with equal weights: the length factor and P (f ) learned from model. 5 Experiments 5.1 Data We tested our tokenization methods on two different language pairs: Chinese-English, and KoreanEnglish. For Chinese-English, we used FBIS newswire data. The Korean-English parallel data was collected from news websites and sentencealigned using two different tools described by Moore (2002) and Melamed (1999). We used subsets of each parallel corpus consisting of about 2M words and 60K sentences on the English side. For our development set and test set, Chinese-English had about 1000 sentences each with 10 reference translations taken from the NIST 2002 MT evaluation. For Korean-English, 2200 sentence pairs were randomly sampled from the parallel corpus, and held out from the training data. These were divided in half and used for test set and development set respectively. For all language pairs, very minimal tokenization — splitting off punctuation — was done on the English side"
D09-1075,J03-1002,0,0.00287806,"00 sentences each with 10 reference translations taken from the NIST 2002 MT evaluation. For Korean-English, 2200 sentence pairs were randomly sampled from the parallel corpus, and held out from the training data. These were divided in half and used for test set and development set respectively. For all language pairs, very minimal tokenization — splitting off punctuation — was done on the English side. 5.2 Experimental setup We used Moses (Koehn et al., 2007) to train machine translation systems. Default parameters were used for all experiments except for the number of iterations for GIZA++ (Och and Ney, 2003). GIZA++ was run until the perplexity on development set stopped decreasing. For practical reasons, the maximum size of a token was set at three for Chinese, and four for Korean.2 Minimum error rate training (Och, 2003) was run on each system afterwards and BLEU score (Papineni et al., 2002) was calculated on the test sets. For the monolingual model, we tested two versions with the length factor φ1 , and φ2 . We picked λ and P (s) so that the number of tokens on source side (Chinese, and Korean) will be about the same 2 In the Korean writing system, one character is actually one syllable block"
D09-1075,P03-1021,0,0.00312658,"vided in half and used for test set and development set respectively. For all language pairs, very minimal tokenization — splitting off punctuation — was done on the English side. 5.2 Experimental setup We used Moses (Koehn et al., 2007) to train machine translation systems. Default parameters were used for all experiments except for the number of iterations for GIZA++ (Och and Ney, 2003). GIZA++ was run until the perplexity on development set stopped decreasing. For practical reasons, the maximum size of a token was set at three for Chinese, and four for Korean.2 Minimum error rate training (Och, 2003) was run on each system afterwards and BLEU score (Papineni et al., 2002) was calculated on the test sets. For the monolingual model, we tested two versions with the length factor φ1 , and φ2 . We picked λ and P (s) so that the number of tokens on source side (Chinese, and Korean) will be about the same 2 In the Korean writing system, one character is actually one syllable block. We do not decompose syllable blocks into individual consonants and vowels. as the number of tokens in the target side (English). For the bilingual model, as explained in the model section, we are learning P (f |e), bu"
D09-1075,P02-1040,0,0.107956,"ctively. For all language pairs, very minimal tokenization — splitting off punctuation — was done on the English side. 5.2 Experimental setup We used Moses (Koehn et al., 2007) to train machine translation systems. Default parameters were used for all experiments except for the number of iterations for GIZA++ (Och and Ney, 2003). GIZA++ was run until the perplexity on development set stopped decreasing. For practical reasons, the maximum size of a token was set at three for Chinese, and four for Korean.2 Minimum error rate training (Och, 2003) was run on each system afterwards and BLEU score (Papineni et al., 2002) was calculated on the test sets. For the monolingual model, we tested two versions with the length factor φ1 , and φ2 . We picked λ and P (s) so that the number of tokens on source side (Chinese, and Korean) will be about the same 2 In the Korean writing system, one character is actually one syllable block. We do not decompose syllable blocks into individual consonants and vowels. as the number of tokens in the target side (English). For the bilingual model, as explained in the model section, we are learning P (f |e), but only P (f ) is available for tokenizing any new data. We compared two c"
D09-1075,xia-etal-2000-developing,0,0.0210531,"or Chinese-English and Korean-English experiments and F-score of segmentation compared against Chinese Treebank standard. The highest unsupervised score is highlighted. English, the minimal effort tokenization splitting off punctuation and otherwise respecting the spacing in the Korean writing system. A Korean morphological analysis tool4 was used to create the supervised tokenization. For Chinese-English, since a gold standard for Chinese segmentation is available, we ran an additional evaluation of tokenization from each methods we have tested. We tokenized the raw text of Chinese Treebank (Xia et al., 2000) using all of the methods (supervised/unsupervised) we have described in this section except for the bilingual tokenization using P (f |e) because the English translation of the Chinese Treebank data was not available. We compared the result against the gold standard segmentation and calculated the F-score. 6 Results Results from Chinese-English and Korean-English experiments are presented in Table 1. Note that nature of data and number of references are different for the two language pairs, and therefore the BLEU scores are not comparable. For both language pairs, our models perform equally w"
D09-1075,C08-1128,0,0.277739,"inding an appropriate tokenization for machine translation. We compare methods that have access to parallel corpora to methods that are trained solely using data from the source language. Unsupervised monolingual segmentation has been studied as a model of language acquisition (Goldwater et al., 2006), and as model of learning morphology in European languages (Goldsmith, 2001). Unsupervised segmentation using bilingual data has been attempted for finding new translation pairs (Kikui and Yamamoto, 2002), and for finding good segmentation for Chinese in machine translation using Gibbs sampling (Xu et al., 2008). In this paper, further investigate the use of bilingual information to find tokenizations tailored for machine translation. We find a benefit not only for segmentation of languages with no space in the writing system (such as Chinese), but also for the smaller-scale tokenization problem of normalizing between languages that include more or less information in a “word” as defined by the writing system, using Korean-English for our experiments. Here too, we find a benefit from using bilingual information, with unsupervised segmentation rivaling and in some cases surpassing supervised segmentat"
D09-1075,J01-2001,0,\N,Missing
D09-1075,P07-2045,0,\N,Missing
D09-1075,D08-1076,0,\N,Missing
D09-1136,J93-2003,0,0.0201677,"Missing"
D09-1136,A00-2018,0,0.0108303,"HS and RHS. The outside score w out(g, t) is computed similarly, except that the IBM Model 1 scores are computed based on the words in the hosting template t’s LHS/RHS excluding the words in g’s LHS/RHS. The initial probabilities of the TTS templates are then computed by normalizing their initial counts using LHSN or ROOTN. 4 Experiments We train an English-to-Chinese translation system using the FBIS corpus, where 73,597 sentence pairs are selected as the training data, and 500 sentence pairs with no more than 25 words on the Chinese side are selected for both the development and test data.5 Charniak (2000)’s parser, trained on the Penn Treebank, is used to generate the English syntax trees. Modified Kneser-Ney trigram models are trained using SRILM (Stolcke, 2002) upon the Chinese portion of the training data. The trigram language model, as well as the TTS templates generated based on different methods, are used in the TTS transducer. The model weights of the transducer are tuned based on the development set using a grid-based line search, and the translation results are evaluated based on a single Chinese reference6 using BLEU-4 (Papineni et al., 2002). Huang et al. (2006) used character-based"
D09-1136,J07-2003,0,0.0659535,"ferable to the standard heuristics for extracting the phrase pairs from the many-to-one word-level alignments produced by the IBM series models (Brown et al., 1993) or the Hidden Markov Model (HMM) (Vogel et al., 1996). We wish to apply this direct, Bayesian approach to learn better translation rules for syntaxbased statistical MT (SSMT), by which we specifically refer to MT systems using Tree-to-String (TTS) translation templates derived from syntax trees (Liu et al., 2006; Huang et al., 2006; Galley et al., 2006; May and Knight, 2007), as opposed to formally syntactic systems such as Hiero (Chiang, 2007). The stumbling block preventing us from taking this approach is the extremely large space of possible TTS templates when no word alignments are given. Given a sentence pair and syntax tree over one side, there are an exponential number of potential TTS templates and a polynomial number of phrase pairs. In this paper, we explore methods for restricting the space of possible TTS templates under consideration, while still allowing good templates to emerge directly from the data as much as possible. We find an improvement in translation accuracy through, first, using constraints to limit the numb"
D09-1136,P07-1003,0,0.036683,"templates are then re-estimated using the EM algorithm described in Graehl and Knight (2004). This approach also generates TTS templates beyond the precomputed word alignments, but the freedom is only granted over unaligned target words, and most of the pre-computed word alignments remain unchanged. Other prior approaches towards improving TTS templates focus on improving the word alignment performance over the classic models such as IBM series models and Hidden Markov Model (HMM), which do not consider the syntactic structure of the aligning languages and produce syntax-violating alignments. DeNero and Klein (2007) use a syntaxbased distance in an HMM word alignment model to favor syntax-friendly alignments. Fossum et al. (2008) start from the GIZA++ alignment and incrementally delete bad links based on a discrimS S NP … … VP NN AUX issue has 事件 已经 NP … … … … VP NN AUX issue has 事件 已经 … … Figure 1: 5 small TTS templates are extracted based on the correct word alignments (left), but only 1 big TTS template (right) can be extracted when the cross-boundary noisy alignments are added in. inative model with syntactic features. This approach can only find a better subset of the GIZA++ alignment and requires a"
D09-1136,D08-1033,0,0.476967,"stacles to this approach are the very reasons that word-alignments are used for rule extraction: the huge space of possible rules, as well as controlling overfitting. By carefully controlling which portions of the original alignments are reanalyzed, and by using Bayesian inference during re-analysis, we show significant improvement over the baseline rules extracted from word-level alignments. 1 Introduction Non-parametric Bayesian methods have been successfully applied to directly learn phrase pairs from a bilingual corpus with little or no dependence on word alignments (Blunsom et al., 2008; DeNero et al., 2008). Because such approaches directly learn a generative model over phrase pairs, they are theoretically preferable to the standard heuristics for extracting the phrase pairs from the many-to-one word-level alignments produced by the IBM series models (Brown et al., 1993) or the Hidden Markov Model (HMM) (Vogel et al., 1996). We wish to apply this direct, Bayesian approach to learn better translation rules for syntaxbased statistical MT (SSMT), by which we specifically refer to MT systems using Tree-to-String (TTS) translation templates derived from syntax trees (Liu et al., 2006; Huang et al., 2"
D09-1136,W08-0306,0,0.0455217,"tes TTS templates beyond the precomputed word alignments, but the freedom is only granted over unaligned target words, and most of the pre-computed word alignments remain unchanged. Other prior approaches towards improving TTS templates focus on improving the word alignment performance over the classic models such as IBM series models and Hidden Markov Model (HMM), which do not consider the syntactic structure of the aligning languages and produce syntax-violating alignments. DeNero and Klein (2007) use a syntaxbased distance in an HMM word alignment model to favor syntax-friendly alignments. Fossum et al. (2008) start from the GIZA++ alignment and incrementally delete bad links based on a discrimS S NP … … VP NN AUX issue has 事件 已经 NP … … … … VP NN AUX issue has 事件 已经 … … Figure 1: 5 small TTS templates are extracted based on the correct word alignments (left), but only 1 big TTS template (right) can be extracted when the cross-boundary noisy alignments are added in. inative model with syntactic features. This approach can only find a better subset of the GIZA++ alignment and requires a parallel corpus with goldstandard word alignment for training the discriminative model. May and Knight (2007) facto"
D09-1136,P02-1040,0,0.0770656,"elected for both the development and test data.5 Charniak (2000)’s parser, trained on the Penn Treebank, is used to generate the English syntax trees. Modified Kneser-Ney trigram models are trained using SRILM (Stolcke, 2002) upon the Chinese portion of the training data. The trigram language model, as well as the TTS templates generated based on different methods, are used in the TTS transducer. The model weights of the transducer are tuned based on the development set using a grid-based line search, and the translation results are evaluated based on a single Chinese reference6 using BLEU-4 (Papineni et al., 2002). Huang et al. (2006) used character-based BLEU as a way of normalizing inconsistent Chinese word segmentation, but we avoid this problem as the training, development, and test data are from the same source. 5 The total 74,597 sentence pairs used in experiments are those in the FBIS corpus whose English part can be parsed using Charniak (2000)’s parser. 6 BLEU-4 scores based on a single reference are much lower than the ones based on multiple references. 1313 w/ Big w/o Big E2C 13.37 13.20 C2E 12.66 12.62 Union 14.55 14.53 Heuristic 14.28 14.21 0.1 0.3 0.5 temperature parameter β 0.7 0.8 0.9 1"
D09-1136,C96-2141,0,0.172922,"provement over the baseline rules extracted from word-level alignments. 1 Introduction Non-parametric Bayesian methods have been successfully applied to directly learn phrase pairs from a bilingual corpus with little or no dependence on word alignments (Blunsom et al., 2008; DeNero et al., 2008). Because such approaches directly learn a generative model over phrase pairs, they are theoretically preferable to the standard heuristics for extracting the phrase pairs from the many-to-one word-level alignments produced by the IBM series models (Brown et al., 1993) or the Hidden Markov Model (HMM) (Vogel et al., 1996). We wish to apply this direct, Bayesian approach to learn better translation rules for syntaxbased statistical MT (SSMT), by which we specifically refer to MT systems using Tree-to-String (TTS) translation templates derived from syntax trees (Liu et al., 2006; Huang et al., 2006; Galley et al., 2006; May and Knight, 2007), as opposed to formally syntactic systems such as Hiero (Chiang, 2007). The stumbling block preventing us from taking this approach is the extremely large space of possible TTS templates when no word alignments are given. Given a sentence pair and syntax tree over one side,"
D09-1136,N04-1035,0,0.100643,"Heuristic 14.28 14.21 0.1 0.3 0.5 temperature parameter β 0.7 0.8 0.9 1.0 1.0 1.0 1.0 9 10 21 20.5 Table 3: BLEU-4 scores (test set) of systems based on GIZA++ word alignments 20 19.5 BLEU-4 ≤5 14.27 ≤6 14.42 ≤7 14.43 ≤8 14.45 19 ≤∞ 14.55 18.5 MIXN-EM LHSN-VB LHSN-EM ROOTN-EM ROOTN-VB MIXN-VB 18 Table 4: BLEU-4 scores (test set) of the union alignment, using TTS templates up to a certain size, in terms of the number of leaves in their LHSs 17.5 17 1 4.1 2 3 4 5 6 iteration 7 8 Figure 8: BLEU-4 scores (development set) of annealing EM and annealing VB in each iteration. Baseline Systems GHKM (Galley et al., 2004) is used to generate the baseline TTS templates based on the word alignments computed using GIZA++ and different combination methods, including union and the diagonal growing heuristic (Koehn et al., 2003). We also tried combining alignments from GIZA++ based on intersection, but it is worse than both single-direction alignments, due to its low coverage of training corpus and the incomplete translations it generates. The baseline translation results based on ROOTN are shown in Table 4.1. The first two columns in the table show the results of the two single direction alignments. e2c and c2e den"
D09-1136,P06-1121,0,0.650936,"ause such approaches directly learn a generative model over phrase pairs, they are theoretically preferable to the standard heuristics for extracting the phrase pairs from the many-to-one word-level alignments produced by the IBM series models (Brown et al., 1993) or the Hidden Markov Model (HMM) (Vogel et al., 1996). We wish to apply this direct, Bayesian approach to learn better translation rules for syntaxbased statistical MT (SSMT), by which we specifically refer to MT systems using Tree-to-String (TTS) translation templates derived from syntax trees (Liu et al., 2006; Huang et al., 2006; Galley et al., 2006; May and Knight, 2007), as opposed to formally syntactic systems such as Hiero (Chiang, 2007). The stumbling block preventing us from taking this approach is the extremely large space of possible TTS templates when no word alignments are given. Given a sentence pair and syntax tree over one side, there are an exponential number of potential TTS templates and a polynomial number of phrase pairs. In this paper, we explore methods for restricting the space of possible TTS templates under consideration, while still allowing good templates to emerge directly from the data as much as possible. We f"
D09-1136,P06-1085,0,0.0194973,"EC(t) EC(t0 ) ; t :t .root=t.root else P r(t) = P 0 0 EC(t) EC(t0 ) ; t :t .lhs=t.lhs Figure 5: EM Algorithm For Estimating TTS Templates own strength and weakness, both of them are used in our EM algorithm: LHSN is used in all EM iterations except the last one to compute the expected counts of the TTS templates, and ROOTN is used in the last EM iteration to compute the final probabilities of the TTS templates. This two-stage normalization method is denoted as MIXN in this paper. Deterministic Annealing (Rose et al., 1992) is is used in our system to speed up the training process, similar to Goldwater et al. (2006). We start from a high temperature and gradually decrease the temperature to 1; we find that the initial high temperature can also help small templates to survive the initial iterations. The complete EM framework is sketched in Figure 3, where β is the inverse of the specified temperature, and EC denotes the expected count. 3.1 Bayesian Inference with the Dirichlet Process Prior Bayesian inference plus the Dirichlet Process (DP) have been shown to effectively prevent MT models from overfitting the training data (DeNero et al., 2008; Blunsom et al., 2008). A similar approach can be applied here"
D09-1136,N04-1014,0,0.0361484,"ssible TTS templates to overcome noisy word alignments. We use variational EM to approximate the inference of our Bayesian model and explore different normalization methods for the TTS templates. A two-stage normalization is proposed by combining LHSbased normalization with normalization based on the root of the LHS, and is shown to be the best model when used with variational EM. Galley et al. (2006) recompose the TTS templates by inserting unaligned target words and combining small templates into bigger ones. The recomposed templates are then re-estimated using the EM algorithm described in Graehl and Knight (2004). This approach also generates TTS templates beyond the precomputed word alignments, but the freedom is only granted over unaligned target words, and most of the pre-computed word alignments remain unchanged. Other prior approaches towards improving TTS templates focus on improving the word alignment performance over the classic models such as IBM series models and Hidden Markov Model (HMM), which do not consider the syntactic structure of the aligning languages and produce syntax-violating alignments. DeNero and Klein (2007) use a syntaxbased distance in an HMM word alignment model to favor s"
D09-1136,2006.amta-papers.8,0,0.14818,"o et al., 2008). Because such approaches directly learn a generative model over phrase pairs, they are theoretically preferable to the standard heuristics for extracting the phrase pairs from the many-to-one word-level alignments produced by the IBM series models (Brown et al., 1993) or the Hidden Markov Model (HMM) (Vogel et al., 1996). We wish to apply this direct, Bayesian approach to learn better translation rules for syntaxbased statistical MT (SSMT), by which we specifically refer to MT systems using Tree-to-String (TTS) translation templates derived from syntax trees (Liu et al., 2006; Huang et al., 2006; Galley et al., 2006; May and Knight, 2007), as opposed to formally syntactic systems such as Hiero (Chiang, 2007). The stumbling block preventing us from taking this approach is the extremely large space of possible TTS templates when no word alignments are given. Given a sentence pair and syntax tree over one side, there are an exponential number of potential TTS templates and a polynomial number of phrase pairs. In this paper, we explore methods for restricting the space of possible TTS templates under consideration, while still allowing good templates to emerge directly from the data as m"
D09-1136,N03-1017,0,0.0114618,".42 ≤7 14.43 ≤8 14.45 19 ≤∞ 14.55 18.5 MIXN-EM LHSN-VB LHSN-EM ROOTN-EM ROOTN-VB MIXN-VB 18 Table 4: BLEU-4 scores (test set) of the union alignment, using TTS templates up to a certain size, in terms of the number of leaves in their LHSs 17.5 17 1 4.1 2 3 4 5 6 iteration 7 8 Figure 8: BLEU-4 scores (development set) of annealing EM and annealing VB in each iteration. Baseline Systems GHKM (Galley et al., 2004) is used to generate the baseline TTS templates based on the word alignments computed using GIZA++ and different combination methods, including union and the diagonal growing heuristic (Koehn et al., 2003). We also tried combining alignments from GIZA++ based on intersection, but it is worse than both single-direction alignments, due to its low coverage of training corpus and the incomplete translations it generates. The baseline translation results based on ROOTN are shown in Table 4.1. The first two columns in the table show the results of the two single direction alignments. e2c and c2e denote the many English words to one Chinese word alignment and the many Chinese words to one English word alignment, respectively. The two rows show the results with and without the big templates, from which"
D09-1136,W04-3250,0,0.0373195,"Missing"
D09-1136,P06-1077,0,0.185911,"t al., 2008; DeNero et al., 2008). Because such approaches directly learn a generative model over phrase pairs, they are theoretically preferable to the standard heuristics for extracting the phrase pairs from the many-to-one word-level alignments produced by the IBM series models (Brown et al., 1993) or the Hidden Markov Model (HMM) (Vogel et al., 1996). We wish to apply this direct, Bayesian approach to learn better translation rules for syntaxbased statistical MT (SSMT), by which we specifically refer to MT systems using Tree-to-String (TTS) translation templates derived from syntax trees (Liu et al., 2006; Huang et al., 2006; Galley et al., 2006; May and Knight, 2007), as opposed to formally syntactic systems such as Hiero (Chiang, 2007). The stumbling block preventing us from taking this approach is the extremely large space of possible TTS templates when no word alignments are given. Given a sentence pair and syntax tree over one side, there are an exponential number of potential TTS templates and a polynomial number of phrase pairs. In this paper, we explore methods for restricting the space of possible TTS templates under consideration, while still allowing good templates to emerge directl"
D09-1136,D07-1038,0,0.687543,"directly learn a generative model over phrase pairs, they are theoretically preferable to the standard heuristics for extracting the phrase pairs from the many-to-one word-level alignments produced by the IBM series models (Brown et al., 1993) or the Hidden Markov Model (HMM) (Vogel et al., 1996). We wish to apply this direct, Bayesian approach to learn better translation rules for syntaxbased statistical MT (SSMT), by which we specifically refer to MT systems using Tree-to-String (TTS) translation templates derived from syntax trees (Liu et al., 2006; Huang et al., 2006; Galley et al., 2006; May and Knight, 2007), as opposed to formally syntactic systems such as Hiero (Chiang, 2007). The stumbling block preventing us from taking this approach is the extremely large space of possible TTS templates when no word alignments are given. Given a sentence pair and syntax tree over one side, there are an exponential number of potential TTS templates and a polynomial number of phrase pairs. In this paper, we explore methods for restricting the space of possible TTS templates under consideration, while still allowing good templates to emerge directly from the data as much as possible. We find an improvement in t"
D09-1136,W06-3601,0,\N,Missing
D09-1136,J08-3004,0,\N,Missing
D10-1058,J93-2003,0,0.21632,"ding word alignment models for statistical machine translation. We built a fertility hidden Markov model by adding fertility to the hidden Markov model. This model not only achieves lower alignment error rate than the hidden Markov model, but also runs faster. It is similar in some ways to IBM Model 4, but is much easier to understand. We use Gibbs sampling for parameter estimation, which is more principled than the neighborhood method used in IBM Model 4. 1 Introduction IBM models and the hidden Markov model (HMM) for word alignment are the most influential statistical word alignment models (Brown et al., 1993; Vogel et al., 1996; Och and Ney, 2003). There are three kinds of important information for word alignment models: lexicality, locality and fertility. IBM Model 1 uses only lexical information; IBM Model 2 and the hidden Markov model take advantage of both lexical and locality information; IBM Models 4 and 5 use all three kinds of information, and they remain the state of the art despite the fact that they were developed almost two decades ago. Recent experiments on large datasets have shown that the performance of the hidden Markov model is very close to IBM Model 4. Nevertheless, we believe"
D10-1058,P03-1012,0,0.153513,"all three kinds of information, and they remain the state of the art despite the fact that they were developed almost two decades ago. Recent experiments on large datasets have shown that the performance of the hidden Markov model is very close to IBM Model 4. Nevertheless, we believe that IBM Model 4 is essentially a better model because it exploits the fertility of words in the tarThere have been many years of research on word alignment. Our work is different from others in essential ways. Most other researchers take either the HMM alignments (Liang et al., 2006) or IBM Model 4 alignments (Cherry and Lin, 2003) as input and perform post-processing, whereas our model is a potential replacement for the HMM and IBM Model 4. Directly modeling fertility makes our model fundamentally different from others. Most models have limited ability to model fertility. Liang et al. (2006) learn the alignment in both translation directions jointly, essentially pushing the fertility towards 1. ITG models (Wu, 1997) assume the fertility to be either zero or one. It can model phrases, but the phrase has to be contiguous. There have been works that try to simulate fertility using the hidden Markov model (Toutanova et al."
D10-1058,D08-1033,0,0.0675607,"ster than the HMM, and has lower alignment error rate than the HMM. Parameter estimation for word alignment models that model fertility is more difficult than for models without fertility. Brown et al. (1993) and Och and Ney (2003) first compute the Viterbi alignments for simpler models, then consider only some neighbors of the Viterbi alignments for modeling fertility. If the optimal alignment is not in those neighbors, this method will not be able find the optimal alignment. We use the Markov Chain Monte Carlo (MCMC) method for training and decoding, which has nice probabilistic guarantees. DeNero et al. (2008) applied the Markov Chain Monte Carlo method to word alignment for machine translation; they do not model word fertility. 2 Statistical Word Alignment Models 2.1 Alignment and Fertility Given a source sentence f J1 = f1 , f2 , . . . , fJ and a target sentence eI1 = e1 , e2 , . . . , eI , we define the alignments between the two sentences as a subset of the Cartesian product of the word positions. Following Brown et al. (1993), we assume that each source word is aligned to exactly one target word. We denote as aJ1 = a1 , a2 , . . . , aJ the alignments between f1J and eI1 . When a word fj is not"
D10-1058,H05-1022,0,0.0353902,"t and perform post-processing, whereas our model is a potential replacement for the HMM and IBM Model 4. Directly modeling fertility makes our model fundamentally different from others. Most models have limited ability to model fertility. Liang et al. (2006) learn the alignment in both translation directions jointly, essentially pushing the fertility towards 1. ITG models (Wu, 1997) assume the fertility to be either zero or one. It can model phrases, but the phrase has to be contiguous. There have been works that try to simulate fertility using the hidden Markov model (Toutanova et al., 2002; Deng and Byrne, 2005), but we prefer to model fertility directly. Our model is a coherent generative model that combines the HMM and IBM Model 4. It is easier to understand than IBM Model 4 (see Section 3). Our model also removes several undesired properties in IBM Model 4. We use Gibbs sampling instead of a heuristic-based neighborhood method for parameter 596 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 596–605, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics estimation. Our distortion parameters are similar to IBM Mode"
D10-1058,P07-2045,0,0.00845604,"ster than the HMM. Figure 3 show the training time for different models. In fact, with just 1 sample for each alignment, our model archives lower AER than the HMM, and runs more than 5 times faster than the HMM. It is possible to use sampling instead of dynamic programming in the HMM to reduce the training time with no decrease in AER (often an increase). We conclude that the fertility HMM not only has better AER results, but also runs faster than the hidden Markov model. We also evaluate our model by computing the machine translation BLEU score (Papineni et al., 2002) using the Moses system (Koehn et al., 2007). The training data is the same as the above word alignment evaluation bitexts, with alignments for each model symmetrized using the grow-diag-final heuristic. Our test is 633 sentences of up to length 50, with four references. Results are shown in Table 2; we see that better word alignment results do not lead to better translations. Model HMM HMMF-30 IBM4 BLEU 19.55 19.26 18.77 Table 2: BLEU results 7 Conclusion We developed a fertility hidden Markov model that runs faster and has lower AER than the HMM. Our model is thus much faster than IBM Model 4. Our model is also easier to understand th"
D10-1058,N06-1014,0,0.0916645,"nd locality information; IBM Models 4 and 5 use all three kinds of information, and they remain the state of the art despite the fact that they were developed almost two decades ago. Recent experiments on large datasets have shown that the performance of the hidden Markov model is very close to IBM Model 4. Nevertheless, we believe that IBM Model 4 is essentially a better model because it exploits the fertility of words in the tarThere have been many years of research on word alignment. Our work is different from others in essential ways. Most other researchers take either the HMM alignments (Liang et al., 2006) or IBM Model 4 alignments (Cherry and Lin, 2003) as input and perform post-processing, whereas our model is a potential replacement for the HMM and IBM Model 4. Directly modeling fertility makes our model fundamentally different from others. Most models have limited ability to model fertility. Liang et al. (2006) learn the alignment in both translation directions jointly, essentially pushing the fertility towards 1. ITG models (Wu, 1997) assume the fertility to be either zero or one. It can model phrases, but the phrase has to be contiguous. There have been works that try to simulate fertilit"
D10-1058,P04-1066,0,0.54044,"th any word e, aj is 0. For convenience, we add an empty word ǫ to the target sentence at position 0 (i.e., e0 = ǫ). However, as we will see, we have to add more than one empty word for the HMM. In order to compute the “jump probability” in the HMM model, we need to know the position of the aligned target word for the previous source word. If the previous source word aligns to an empty word, we could use the position of the empty word to indicate the nearest previous source word that does not align to an empty word. For this reason, we use a 597 total of I + 1 empty words for the HMM model1 . Moore (2004) also suggested adding multiple empty words to the target sentence for IBM Model 1. After we add I + 1 empty words to the target sentence, the alignment is a mapping from source to target word positions: a : j → i, i = aj where j = 1, 2, . . . , J and i = 1, 2, . . . , 2I + 1. Words from position I + 1 to 2I + 1 in the target sentence are all empty words. We allow each source word to align with exactly one target word, but each target word may align with multiple source words. The fertility φi of a word ei at position i is defined as the number of aligned source words: φi = J X δ(aj , i) j=1 w"
D10-1058,J03-1002,0,0.291402,"al machine translation. We built a fertility hidden Markov model by adding fertility to the hidden Markov model. This model not only achieves lower alignment error rate than the hidden Markov model, but also runs faster. It is similar in some ways to IBM Model 4, but is much easier to understand. We use Gibbs sampling for parameter estimation, which is more principled than the neighborhood method used in IBM Model 4. 1 Introduction IBM models and the hidden Markov model (HMM) for word alignment are the most influential statistical word alignment models (Brown et al., 1993; Vogel et al., 1996; Och and Ney, 2003). There are three kinds of important information for word alignment models: lexicality, locality and fertility. IBM Model 1 uses only lexical information; IBM Model 2 and the hidden Markov model take advantage of both lexical and locality information; IBM Models 4 and 5 use all three kinds of information, and they remain the state of the art despite the fact that they were developed almost two decades ago. Recent experiments on large datasets have shown that the performance of the hidden Markov model is very close to IBM Model 4. Nevertheless, we believe that IBM Model 4 is essentially a bette"
D10-1058,P02-1040,0,0.0795587,"nly has lower AER than the HMM, it also runs faster than the HMM. Figure 3 show the training time for different models. In fact, with just 1 sample for each alignment, our model archives lower AER than the HMM, and runs more than 5 times faster than the HMM. It is possible to use sampling instead of dynamic programming in the HMM to reduce the training time with no decrease in AER (often an increase). We conclude that the fertility HMM not only has better AER results, but also runs faster than the hidden Markov model. We also evaluate our model by computing the machine translation BLEU score (Papineni et al., 2002) using the Moses system (Koehn et al., 2007). The training data is the same as the above word alignment evaluation bitexts, with alignments for each model symmetrized using the grow-diag-final heuristic. Our test is 633 sentences of up to length 50, with four references. Results are shown in Table 2; we see that better word alignment results do not lead to better translations. Model HMM HMMF-30 IBM4 BLEU 19.55 19.26 18.77 Table 2: BLEU results 7 Conclusion We developed a fertility hidden Markov model that runs faster and has lower AER than the HMM. Our model is thus much faster than IBM Model"
D10-1058,W02-1012,0,0.174578,"Missing"
D10-1058,C96-2141,0,0.935422,"models for statistical machine translation. We built a fertility hidden Markov model by adding fertility to the hidden Markov model. This model not only achieves lower alignment error rate than the hidden Markov model, but also runs faster. It is similar in some ways to IBM Model 4, but is much easier to understand. We use Gibbs sampling for parameter estimation, which is more principled than the neighborhood method used in IBM Model 4. 1 Introduction IBM models and the hidden Markov model (HMM) for word alignment are the most influential statistical word alignment models (Brown et al., 1993; Vogel et al., 1996; Och and Ney, 2003). There are three kinds of important information for word alignment models: lexicality, locality and fertility. IBM Model 1 uses only lexical information; IBM Model 2 and the hidden Markov model take advantage of both lexical and locality information; IBM Models 4 and 5 use all three kinds of information, and they remain the state of the art despite the fact that they were developed almost two decades ago. Recent experiments on large datasets have shown that the performance of the hidden Markov model is very close to IBM Model 4. Nevertheless, we believe that IBM Model 4 is"
D10-1058,J97-3002,0,0.0350847,"ny years of research on word alignment. Our work is different from others in essential ways. Most other researchers take either the HMM alignments (Liang et al., 2006) or IBM Model 4 alignments (Cherry and Lin, 2003) as input and perform post-processing, whereas our model is a potential replacement for the HMM and IBM Model 4. Directly modeling fertility makes our model fundamentally different from others. Most models have limited ability to model fertility. Liang et al. (2006) learn the alignment in both translation directions jointly, essentially pushing the fertility towards 1. ITG models (Wu, 1997) assume the fertility to be either zero or one. It can model phrases, but the phrase has to be contiguous. There have been works that try to simulate fertility using the hidden Markov model (Toutanova et al., 2002; Deng and Byrne, 2005), but we prefer to model fertility directly. Our model is a coherent generative model that combines the HMM and IBM Model 4. It is easier to understand than IBM Model 4 (see Section 3). Our model also removes several undesired properties in IBM Model 4. We use Gibbs sampling instead of a heuristic-based neighborhood method for parameter 596 Proceedings of the 20"
D10-1062,J93-2003,0,0.0198755,"y expected. Many treebanks include empty nodes in parse trees to represent non-local dependencies or dropped elements. Examples of the former include traces such as relative clause markers in the Penn Treebank (Bies et al., 1995). An example of the latter include dropped pronouns in the Korean Treebank (Han and Ryu, 2005) and the Chinese Treebank (Xue and Xia, 2000). In languages such as Chinese, Japanese, and Korean, pronouns are frequently or regularly dropped There are several different strategies to counter this problem. A special N ULL word is typically used when learning word alignment (Brown et al., 1993). Words that have non-existent counterparts can be aligned to the N ULL word. In phrase-based translation, the phrase learning system may be able to learn pronouns as a part of larger phrases. If the learned phrases include pronouns on the target side that are dropped from source side, the system may be able to insert pronouns even when they are missing from the source language. This is an often observed phenomenon in phrase-based translation systems. Explicit insertion of missing words can also be included in syntax-based translation models (Yamada and Knight, 2001). For the closely related p"
D10-1062,W08-0336,0,0.0178927,"at used the conditional random fields also corroborates this finding. For predicting dropped pronouns, the method using the CRFs did better than the others. This suggests that rather than tree structure, local context of words 642 Experiments Setup For Chinese-English, we used a subset of FBIS newswire data consisting of about 2M words and 60K sentences on the English side. For our development set and test set, we had about 1000 sentences each with 10 reference translations taken from the NIST 2002 MT evaluation. All Chinese data was re-segmented with the CRF-based Stanford Chinese segmenter (Chang et al., 2008) that is trained on the segmentation of the Chinese Treebank for consistency. The parser used in Section 3 was used to parse the training data so that null elements could be recovered from the trees. The same method for recovering null elements was applied to the trainBaseline Pattern CRF Parsing BLEU 23.73 23.99 24.69* 23.99 BP 1.000 0.998 1.000 1.000 *PRO* *pro* 0.62 0.55 0.56 0.31 0.44 0.42 Table 8: Final BLEU score result. The asterisk indicates statistical significance at p < 0.05 with 1000 iterations of paired bootstrap resampling. BP stands for the brevity penalty in BLEU. F1 scores for"
D10-1062,N06-1024,0,0.55593,"e also identified which empty categories maybe helpful for improving translation for different language pairs. In the next section, we focus on how we add these elements automatically to a corpus that is not annotated with empty elements for the purpose of preprocessing corpus for machine translation. 3 Recovering empty nodes There are a few previous works that have attempted restore empty nodes for parse trees using the Penn English Treebank. Johnson (2002) uses rather simple pattern matching to restore empty categories as well as their co-indexed antecedents with surprisingly good accuracy. Gabbard et al. (2006) present a more sophisticated algorithm that tries to recover empty categories in several steps. In each step, one or more empty categories are restored using patterns or classifiers (five maximum-entropy and two perceptron-based classifiers to be exact). What we are trying to achieve has obvious similarity to these previous works. However, there are several differences. First, we deal with different languages. Second, we are only trying to recover Chinese *PRO* 贯彻 *PRO* 逐步 形成 *PRO* 吸引 外资 作为 English Reference implementing have gradually formed attracting foreign investment System trained w/ nu"
D10-1062,P09-2059,0,0.029573,"learning system may be able to learn pronouns as a part of larger phrases. If the learned phrases include pronouns on the target side that are dropped from source side, the system may be able to insert pronouns even when they are missing from the source language. This is an often observed phenomenon in phrase-based translation systems. Explicit insertion of missing words can also be included in syntax-based translation models (Yamada and Knight, 2001). For the closely related problem of inserting grammatical function particles in English-to-Korean and English-to-Japanese machine translation, Hong et al. (2009) and Isozaki et al. (2010) employ preprocessing techniques to add special symbols to the English source text. In this paper, we examine a strategy of automatically inserting two types of empty elements from the Korean and Chinese treebanks as a preprocess636 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 636–645, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics Korean *T* (NP *pro*) (WHNP *op*) 0.47 0.88 0.40 *?* 0.006 trace of movement dropped subject or object empty operator in relative constructions v"
D10-1062,W10-1736,0,0.00998913,"able to learn pronouns as a part of larger phrases. If the learned phrases include pronouns on the target side that are dropped from source side, the system may be able to insert pronouns even when they are missing from the source language. This is an often observed phenomenon in phrase-based translation systems. Explicit insertion of missing words can also be included in syntax-based translation models (Yamada and Knight, 2001). For the closely related problem of inserting grammatical function particles in English-to-Korean and English-to-Japanese machine translation, Hong et al. (2009) and Isozaki et al. (2010) employ preprocessing techniques to add special symbols to the English source text. In this paper, we examine a strategy of automatically inserting two types of empty elements from the Korean and Chinese treebanks as a preprocess636 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 636–645, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics Korean *T* (NP *pro*) (WHNP *op*) 0.47 0.88 0.40 *?* 0.006 trace of movement dropped subject or object empty operator in relative constructions verb deletion, VP ellipsis,"
D10-1062,P02-1018,0,0.383678,"lation of one phrase. Experiments in this section showed that preprocessing the corpus to include some empty elements can improve translation results. We also identified which empty categories maybe helpful for improving translation for different language pairs. In the next section, we focus on how we add these elements automatically to a corpus that is not annotated with empty elements for the purpose of preprocessing corpus for machine translation. 3 Recovering empty nodes There are a few previous works that have attempted restore empty nodes for parse trees using the Penn English Treebank. Johnson (2002) uses rather simple pattern matching to restore empty categories as well as their co-indexed antecedents with surprisingly good accuracy. Gabbard et al. (2006) present a more sophisticated algorithm that tries to recover empty categories in several steps. In each step, one or more empty categories are restored using patterns or classifiers (five maximum-entropy and two perceptron-based classifiers to be exact). What we are trying to achieve has obvious similarity to these previous works. However, there are several differences. First, we deal with different languages. Second, we are only trying"
D10-1062,P07-2045,0,0.0130094,"ethod for recovering null elements was applied to the trainBaseline Pattern CRF Parsing BLEU 23.73 23.99 24.69* 23.99 BP 1.000 0.998 1.000 1.000 *PRO* *pro* 0.62 0.55 0.56 0.31 0.44 0.42 Table 8: Final BLEU score result. The asterisk indicates statistical significance at p < 0.05 with 1000 iterations of paired bootstrap resampling. BP stands for the brevity penalty in BLEU. F1 scores for recovering empty categories are repeated here for comparison. ing, development, and test sets to insert empty nodes for each experiment. The baseline system was also trained using the raw data. We used Moses (Koehn et al., 2007) to train machine translation systems. Default parameters were used for all experiments. The same number of GIZA++ (Och and Ney, 2003) iterations were used for all experiments. Minimum error rate training (Och, 2003) was run on each system afterwards and the BLEU score (Papineni et al., 2002) was calculated on the test set. 4.2 Results Table 8 summarizes our results. Generally, all systems produced BLEU scores that are better than the baseline, but the best BLEU score came from the system that used the CRF for null element insertion. The machine translation system that used training data from"
D10-1062,W04-3250,0,0.034368,"rwards and the BLEU score (Papineni et al., 2002) was calculated on the test set. 4.2 Results Table 8 summarizes our results. Generally, all systems produced BLEU scores that are better than the baseline, but the best BLEU score came from the system that used the CRF for null element insertion. The machine translation system that used training data from the method that was overall the best in predicting empty elements performed the best. The improvement is 0.96 points in BLEU score, which represents statistical significance at p < 0.002 based on 1000 iterations of paired bootstrap resampling (Koehn, 2004). Brevity penalties applied for calculating BLEU scores are presented to demonstrate that the baseline system is not penalized for producing shorter sentences compared other systems.3 The BLEU scores presented in Table 8 represent the best variations of each method we have tried for recovering empty elements. Although the difference was small, when the F1 score were same for two variations of a method, it seemed that we could get slightly better BLEU score with the variation that had higher recall for recovering empty ele3 We thank an anonymous reviewer for tipping us to examine the brevity pe"
D10-1062,J03-1002,0,0.00910306,"000 1.000 *PRO* *pro* 0.62 0.55 0.56 0.31 0.44 0.42 Table 8: Final BLEU score result. The asterisk indicates statistical significance at p < 0.05 with 1000 iterations of paired bootstrap resampling. BP stands for the brevity penalty in BLEU. F1 scores for recovering empty categories are repeated here for comparison. ing, development, and test sets to insert empty nodes for each experiment. The baseline system was also trained using the raw data. We used Moses (Koehn et al., 2007) to train machine translation systems. Default parameters were used for all experiments. The same number of GIZA++ (Och and Ney, 2003) iterations were used for all experiments. Minimum error rate training (Och, 2003) was run on each system afterwards and the BLEU score (Papineni et al., 2002) was calculated on the test set. 4.2 Results Table 8 summarizes our results. Generally, all systems produced BLEU scores that are better than the baseline, but the best BLEU score came from the system that used the CRF for null element insertion. The machine translation system that used training data from the method that was overall the best in predicting empty elements performed the best. The improvement is 0.96 points in BLEU score, wh"
D10-1062,P03-1021,0,0.0402686,"asterisk indicates statistical significance at p < 0.05 with 1000 iterations of paired bootstrap resampling. BP stands for the brevity penalty in BLEU. F1 scores for recovering empty categories are repeated here for comparison. ing, development, and test sets to insert empty nodes for each experiment. The baseline system was also trained using the raw data. We used Moses (Koehn et al., 2007) to train machine translation systems. Default parameters were used for all experiments. The same number of GIZA++ (Och and Ney, 2003) iterations were used for all experiments. Minimum error rate training (Och, 2003) was run on each system afterwards and the BLEU score (Papineni et al., 2002) was calculated on the test set. 4.2 Results Table 8 summarizes our results. Generally, all systems produced BLEU scores that are better than the baseline, but the best BLEU score came from the system that used the CRF for null element insertion. The machine translation system that used training data from the method that was overall the best in predicting empty elements performed the best. The improvement is 0.96 points in BLEU score, which represents statistical significance at p < 0.002 based on 1000 iterations of p"
D10-1062,P02-1040,0,0.0808631,"iterations of paired bootstrap resampling. BP stands for the brevity penalty in BLEU. F1 scores for recovering empty categories are repeated here for comparison. ing, development, and test sets to insert empty nodes for each experiment. The baseline system was also trained using the raw data. We used Moses (Koehn et al., 2007) to train machine translation systems. Default parameters were used for all experiments. The same number of GIZA++ (Och and Ney, 2003) iterations were used for all experiments. Minimum error rate training (Och, 2003) was run on each system afterwards and the BLEU score (Papineni et al., 2002) was calculated on the test set. 4.2 Results Table 8 summarizes our results. Generally, all systems produced BLEU scores that are better than the baseline, but the best BLEU score came from the system that used the CRF for null element insertion. The machine translation system that used training data from the method that was overall the best in predicting empty elements performed the best. The improvement is 0.96 points in BLEU score, which represents statistical significance at p < 0.002 based on 1000 iterations of paired bootstrap resampling (Koehn, 2004). Brevity penalties applied for calcu"
D10-1062,P06-1055,0,0.0120322,"ty node that has more than one child was annotated with information about the empty node, and the empty node was deleted. We annotated whether the deleted empty node was *pro* or *PRO* and where it was deleted. Adding where the child was necessary because, even though most empty nodes are the first child, there are many exceptions. We first extracted a plain context free grammar after modifying the trees and used the modified grammar to parse the test set and then tried to recover the empty elements. This approach did not work well. We then applied the latent annotation learning procedures of Petrov et al. (2006)2 to refine the nonterminals in the modified grammar. This has been shown to help parsing in many different situations. Although the state splitting procedure is designed to maximize the likelihood of of the parse trees, rather than specifically to predict the empty nodes, learning a refined grammar over modified trees was also effective in helping to predict empty nodes. Table 6 shows the dramatic improvement after each split, merge, and smoothing cycle. The gains leveled off after the sixth iteration and the sixth order grammar was used to run later experiments. Results Table 7 shows the res"
D10-1062,P01-1067,0,0.0128894,"d when learning word alignment (Brown et al., 1993). Words that have non-existent counterparts can be aligned to the N ULL word. In phrase-based translation, the phrase learning system may be able to learn pronouns as a part of larger phrases. If the learned phrases include pronouns on the target side that are dropped from source side, the system may be able to insert pronouns even when they are missing from the source language. This is an often observed phenomenon in phrase-based translation systems. Explicit insertion of missing words can also be included in syntax-based translation models (Yamada and Knight, 2001). For the closely related problem of inserting grammatical function particles in English-to-Korean and English-to-Japanese machine translation, Hong et al. (2009) and Isozaki et al. (2010) employ preprocessing techniques to add special symbols to the English source text. In this paper, we examine a strategy of automatically inserting two types of empty elements from the Korean and Chinese treebanks as a preprocess636 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 636–645, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computati"
D10-1062,D08-1076,0,\N,Missing
D14-1180,P09-1088,0,0.353392,"lit point, which we refer to as a cut, between two rules. The problem of extracting machine translation rules from word-aligned bitext is a similar problem in that we wish to automatically learn the best granularity for the rules with which to analyze each sentence. The problem of rule extraction is more complex, however, because the tree structure of the sentence is also unknown. In machine translation applications, most previous work on joint alignment and rule extraction models uses heuristic methods to extract rules from learned word alignment or bracketing structures (Zhang et al., 2008; Blunsom et al., 2009; DeNero et al., 2008; Levenberg et al., 2012). Chung et al. (2014) present a MCMC algorithm schedule to learn Hiero-style SCFG rules (Chiang, 2007) by sampling tree fragments from phrase decomposition forests, which represent all possible rules that are consistent with a set of fixed word alignments. Assuming fixed word alignments reduces the complexity of the sampling problem, and has generally been effective in most stateof-the-art machine translation systems. The algorithm for sampling rules from a forest is as follows: from the root of the phrase decomposition forest, one samples a cut va"
D14-1180,J14-1007,1,0.501463,"Missing"
D14-1180,P10-2042,0,0.0154806,"composition forest, one samples a cut variable, denoting whether the current node is a cut, and an edge variable, denoting which incoming hyperedge is chosen, at each node of the current tree in a top-down manner. This sampling schedule is efficient in that it only samples the current tree and will not waste time on updating variables that are unlikely to be used in any tree. As with many other token-based Gibbs Sampling applications, sampling one node at a time can result in slow mixing due to the strong coupling between variables. One general remedy is to sample blocks of coupled variables. Cohn and Blunsom (2010) and Yamangil and Shieber (2013) used blocked sampling algorithms that sample the whole tree structure associated with one sentence at a time for TSG and TAG learning. However, this kind of blocking does not deal with the coupling of variables correlated with the same type of structure across sentences. Liang et al. (2010) introduced a type-based sampling schedule which updates a block of variables of the same type jointly. The type of a variable is defined as the combination of new structural choices added when assigning different values to the variable. Type-based MCMC tackles the coupling i"
D14-1180,N09-1062,0,0.0562072,"While type-based MCMC has been shown to be effective in a number of NLP applications, our setting, where the tree structure of the sentence is itself a hidden variable, presents a number of challenges to type-based inference. We describe methods for defining variable types and efficiently indexing variables in order to overcome these challenges. These methods lead to improvements in both log likelihood and BLEU score in our experiments. 1 Introduction In previous work, sampling methods have been used to learn Tree Substitution Grammar (TSG) rules from derivation trees (Post and Gildea, 2009; Cohn et al., 2009) for TSG learning. Here, at each node in the derivation tree, there is a binary variable indicating whether the node is internal to a TSG rule or is a split point, which we refer to as a cut, between two rules. The problem of extracting machine translation rules from word-aligned bitext is a similar problem in that we wish to automatically learn the best granularity for the rules with which to analyze each sentence. The problem of rule extraction is more complex, however, because the tree structure of the sentence is also unknown. In machine translation applications, most previous work on join"
D14-1180,D08-1033,0,0.0909014,"fer to as a cut, between two rules. The problem of extracting machine translation rules from word-aligned bitext is a similar problem in that we wish to automatically learn the best granularity for the rules with which to analyze each sentence. The problem of rule extraction is more complex, however, because the tree structure of the sentence is also unknown. In machine translation applications, most previous work on joint alignment and rule extraction models uses heuristic methods to extract rules from learned word alignment or bracketing structures (Zhang et al., 2008; Blunsom et al., 2009; DeNero et al., 2008; Levenberg et al., 2012). Chung et al. (2014) present a MCMC algorithm schedule to learn Hiero-style SCFG rules (Chiang, 2007) by sampling tree fragments from phrase decomposition forests, which represent all possible rules that are consistent with a set of fixed word alignments. Assuming fixed word alignments reduces the complexity of the sampling problem, and has generally been effective in most stateof-the-art machine translation systems. The algorithm for sampling rules from a forest is as follows: from the root of the phrase decomposition forest, one samples a cut variable, denoting whet"
D14-1180,P13-1033,0,0.0137678,"riables but directly samples the real m. In our experiment, this one-stage sampling strategy gives us a 1.5 times overall speed up in comparison with the two-stage sampling schedule of Liang et al. (2010). 4.2 Parallel Implementation As our type-based sampler involves tedious bookkeeping and frequent conflict checking and mismatch of cut types, one iteration of the type-based sampler is slower than an iteration of the tokenbased sampler when run on a single processor. In order to speed up our sampling procedure, we used a parallel sampling strategy similar to that of Blunsom et al. (2009) and Feng and Cohn (2013), who use multiple processors to perform inexact Gibbs Sampling, and find equivalent performance in comparison with an exact Gibbs Sampler with significant speed up. In our application, we split the data into several subsets and assign each subset to a processor. Each processor performs typebased sampling on its subset using local counts and local bookkeeping, and communicates the update of the local counts after each iteration. All the updates are then aggregated to generate global counts and then we refresh the local counts of each processor. We do not communicate the update on the bookkeepi"
D14-1180,D10-1063,0,0.01496,"e tune the number of grammars included for averaging by comparing the BLEU score on the dev set and report the BLEU score result on the test with the same averaging of grammars. As each tree fragment sampled from the forest represents a unique translation rule, we do not need to explicitly extract the rules; we merely need to collect them and count them. However, the fragments sampled include purely non-lexical rules that do not conform to the rule constraints of Hiero, and rules that are not useful for translation. In order to get rid of this type of rule, we prune every rule that has scope (Hopkins and Langmead, 2010) greater than two. Whereas Hiero does not allow two adjacent nonterminals in the source side, our pruning criterion allows some rules of scope two that are not allowed by Hiero. For example, the following rule (only source side shown) has scope two but is not allowed by Hiero: −4.5 −5.0 −5.5 −6.0 parallel type-based non-parallel type-based −6.5 0 10 20 30 40 50 60 Iteration # Figure 5: parallelization result for type-based MCMC helps solve the slow mixing problem of tokenbased sampling methods. Another interesting observation is that, even though theoretically these two sampling methods should"
D14-1180,P02-1040,0,0.0905742,"18, LDC2002L27, LDC2003E07, LDC2003E14, LDC2004T08, LDC2005T06, LDC2005T10, LDC2005T34, LDC2006E26, LDC2005E83, LDC2006E34, LDC2006E85, LDC2006E92, LDC2006E24, LDC2006E92, LDC2006E24) The language model is trained on the English side of entire data (1.65M sentences, which is 39.3M words.) a 428-sentence test set with four references for testing.2 The development set and the test set have sentences with less than 30 words. A trigram language model was used for all experiments. We plotted the log likelihood graph to compare the convergence property of each sampling schedule and calculated BLEU (Papineni et al., 2002) for evaluation. 5.1 Experiment Settings We use the top-down token-based sampling algorithm of Chung et al. (2014) as our baseline. We use the same SCFG decoder for translation with both the baseline and the grammars sampled using our type-based MCMC sampler. The features included in our experiments are differently normalized rule counts and lexical weightings (Koehn et al., 2003) of each rule. Weights are tuned using Pairwise Ranking Optimization (Hopkins and May, 2011) using a grammar extracted by the standard heuristic method (Chiang, 2007) and the development set. The same weights are used"
D14-1180,P09-2012,1,0.904984,"a fixed word alignment. While type-based MCMC has been shown to be effective in a number of NLP applications, our setting, where the tree structure of the sentence is itself a hidden variable, presents a number of challenges to type-based inference. We describe methods for defining variable types and efficiently indexing variables in order to overcome these challenges. These methods lead to improvements in both log likelihood and BLEU score in our experiments. 1 Introduction In previous work, sampling methods have been used to learn Tree Substitution Grammar (TSG) rules from derivation trees (Post and Gildea, 2009; Cohn et al., 2009) for TSG learning. Here, at each node in the derivation tree, there is a binary variable indicating whether the node is internal to a TSG rule or is a split point, which we refer to as a cut, between two rules. The problem of extracting machine translation rules from word-aligned bitext is a similar problem in that we wish to automatically learn the best granularity for the rules with which to analyze each sentence. The problem of rule extraction is more complex, however, because the tree structure of the sentence is also unknown. In machine translation applications, most p"
D14-1180,P13-2106,0,0.0134893,"les a cut variable, denoting whether the current node is a cut, and an edge variable, denoting which incoming hyperedge is chosen, at each node of the current tree in a top-down manner. This sampling schedule is efficient in that it only samples the current tree and will not waste time on updating variables that are unlikely to be used in any tree. As with many other token-based Gibbs Sampling applications, sampling one node at a time can result in slow mixing due to the strong coupling between variables. One general remedy is to sample blocks of coupled variables. Cohn and Blunsom (2010) and Yamangil and Shieber (2013) used blocked sampling algorithms that sample the whole tree structure associated with one sentence at a time for TSG and TAG learning. However, this kind of blocking does not deal with the coupling of variables correlated with the same type of structure across sentences. Liang et al. (2010) introduced a type-based sampling schedule which updates a block of variables of the same type jointly. The type of a variable is defined as the combination of new structural choices added when assigning different values to the variable. Type-based MCMC tackles the coupling issue by assigning the same type"
D14-1180,C08-1136,1,0.882012,"Missing"
D14-1180,D11-1125,0,0.0215373,"ents. We plotted the log likelihood graph to compare the convergence property of each sampling schedule and calculated BLEU (Papineni et al., 2002) for evaluation. 5.1 Experiment Settings We use the top-down token-based sampling algorithm of Chung et al. (2014) as our baseline. We use the same SCFG decoder for translation with both the baseline and the grammars sampled using our type-based MCMC sampler. The features included in our experiments are differently normalized rule counts and lexical weightings (Koehn et al., 2003) of each rule. Weights are tuned using Pairwise Ranking Optimization (Hopkins and May, 2011) using a grammar extracted by the standard heuristic method (Chiang, 2007) and the development set. The same weights are used throughout our experiments. First we want to compare the DP likelihood of the baseline with our type-based MCMC sampler to see if type-based sampling would converge to a better sampling result. In order to verify if type-based MCMC really converges to a good optimum point, we use simulated annealing (Kirkpatrick et al., 1983) to search possible better optimum points. We sample from the real distribution modified by an annealing parameter β: z ∼ P (z)β We increase our β"
D14-1180,N03-1017,0,0.0853585,"h of the two languages. This complexity leads us to explore sampling algorithms instead of using dynamic programming. A span [i, j] is a set of contiguous word indices {i, i + 1, . . . , j − 1}. Given an aligned Chinese-English sentence pair, a phrase n is a pair of spans n = ([i1 , j1 ], [i2 , j2 ]) such that Chinese words in positions [i1 , j1 ] are aligned only to English words in positions [i2 , j2 ], and vice versa. A phrase forest H = hV, Ei is a hypergraph made of a set of hypernodes V and a set of hyperedges E. Each node n = ([i1 , j1 ], [i2 , j2 ]) ∈ V is a tight phrase as defined by Koehn et al. (2003), i.e., a phrase containing no unaligned words at its boundaries. A phrase n = ([i1 , j1 ], [i2 , j2 ]) covers n0 = ([i01 , j10 ], [i02 , j20 ]) if i1 ≤ i01 ∧ j10 ≤ j1 ∧ i2 ≤ i02 ∧ j20 ≤ j2 Each edge in E, written as T → n, is made of a 1736 1 rule: ([0 6], [0 7]) X ([0 1], [0 1]) X X ,X X X → 和 X2 有 X1 , have a X1 with X2 ([1 6], [1 7]) X ,I X X X ,X X X X ,X X ([4 6], [1 4]) X ([4 5], [1 2]) X X ([2 4], [4 6]) X X ,X aX X ([5 6], [3 4]) , have X Thus any valid SCFG rule can be formed by selecting a set of adjacent hyperedges from the forest and composing the minimal SCFG rules specified by e"
D14-1180,D12-1021,0,0.23041,"Missing"
D14-1180,N10-1082,0,\N,Missing
D14-1180,J07-2003,0,\N,Missing
D14-1188,P13-2074,1,0.813868,"which they used a semantic parser to rerank the translation hypotheses of a phrase-based system. Liu and Gildea (2010) used semantic features for a tree-to-string syntax based SMT system. Their features modeled deletion and reordering for source side semantic roles, and they improved the translation quality. Xiong et al. (2012) incorporated the semantic structures into phrasebased SMT by adding syntactic and semantic features to their translation model. They proposed two discriminative models which included features for predicate translation and argument reordering from source to target side. Bazrafshan and Gildea (2013) used semantic structures in a string-to-tree translation system by extracting translation rules enriched with semantic information, and showed that this can improve the translation quality. Li et al. (2013) used predicateargument structure reordering models for hierarchical phrase-based translation, and they used linguistically motivated constraints for phrase translation. In this paper, we experiment with methods for incorporating semantics in a string-to-tree MT system. These methods are designed to model the order of translation, as well as the completeness of the semantic structures. We e"
D14-1188,P11-2072,1,0.840589,", 2005, 2006). We used 392 sentences for the development set and 428 sentences for the test set. These sentences have lengths smaller than 30, and they each have 4 reference translations. We used our in-house stringto-tree decoder that uses Earley parsing. Other than the features that we presented for our new methods, we used a set of nine standard features. The rules for the baseline system were extracted using the GHKM method. Our baseline GHKM rules also include composed rules, where larger rules are constructed by combining two levels of the regular GHKM rules. We exclude any unary rules (Chung et al., 2011), and only keep rules that have scope up to 3 (Hopkins and Langmead, 2010). For the semantic language model, we used the SRILM package (Stolcke, 2002) and trained a tri-gram language model with the default GoodTuring smoothing. Our target side semantic role labeler uses a maximum entropy classifier to label parsed sentences. We used Sections 02-22 of the Penn TreeBank to 1 The data was randomly selected from the following sources: LDC2006E86, LDC2006E93, LDC2002E18, LDC2002L27, LDC2003E07, LDC2003E14, LDC2004T08, LDC2005T06, LDC2005T10, LDC2005T34, LDC2006E26, LDC2005E83, LDC2006E34, LDC2006E8"
D14-1188,P11-2031,0,0.0606931,"Missing"
D14-1188,2007.tmi-papers.10,0,0.0288936,"t al., 2004) rules that include one predicate and all of its core arguments that appear in the sentence. Rather than keeping the predicate and argument labels attached to the non-terminals, we remove those labels from our extracted semantic rules, to keep the non-terminals in the semantic rules consistent with the non-terminals of the baseline GHKM rules. This is also important when using both the source and the target semantic rules (i.e. Chinese and English rules), as it has been shown that there are cross lingual mismatches between Chinese and English semantic roles in bilingual sentences (Fung et al., 2007). We extract a complete semantic rule for each verbal predicate of each sentence pair in the training data. To extract the target side complete semantic rules, using the target side SRL anno1787 A → BC to space [B, i, j, (Agent, )] [C, j, k, (P RED bring, T heme, )] c0 (x1 x2 Destination) c1 c2 [A, i, k, (Agent, PRED bring,-*-, Theme, Destination)] c0 + c1 + c+ + LMcost(Agent, PRED bring,-*-, Theme, Destination) Figure 3: A deduction step in the semantic language model method. tated training data, we follow the general GHKM method, and modify it to ensure that each frontier node (Galley et al."
D14-1188,N04-1035,0,0.162625,"013) extracted translation rules that included a predicate and all of its arguments from the target side, and added those rules to the baseline rules of their string-to-tree MT system. Figure 1 shows an example of such rules, which we refer to as complete semantic rules. The new rules encourage the decoder to generate translations that include all of the semantic roles that appear in the source sentence. In this paper, we use the same idea to extract rules from the semantic structures of the source side. The complete semantic rules consist of the smallest fragments of the combination of GHKM (Galley et al., 2004) rules that include one predicate and all of its core arguments that appear in the sentence. Rather than keeping the predicate and argument labels attached to the non-terminals, we remove those labels from our extracted semantic rules, to keep the non-terminals in the semantic rules consistent with the non-terminals of the baseline GHKM rules. This is also important when using both the source and the target semantic rules (i.e. Chinese and English rules), as it has been shown that there are cross lingual mismatches between Chinese and English semantic roles in bilingual sentences (Fung et al.,"
D14-1188,J05-1004,1,0.10662,"g the semantic roles as the words in what we call the semantic language. Our experimental results show that the language model feature significantly improves translation quality. Semantic Role Labeling (SRL): We use semantic role labelers to annotate the training data that we use to extract the translation rules. For target side SRL, the role labels are attached to the nonterminal nodes in the syntactic parse of each sentence. For source side SRL, the labels annotate the spans from the source sentence that they cover. We train our semantic role labeler using two different standards: Propbank (Palmer et al., 2005) and VerbNet (Kipper Schuler, 2005). PropBank annotates the Penn Treebank with predicate-argument structures.It use generic labels (such as Arg0, Arg1, etc.) which are defined specifically for each verb. We trained a semantic role labeler on the annotated Penn Treebank data and used the classifier to tag our training data. VerbNet is a verb lexicon that categorizes English verbs into hierarchical classes, and annotates them with thematic roles for the arguments that they accept. Since the thematic roles use more meaningful labels (e.g. Agent, Patient, etc.), a language model trained on VerbNet"
D14-1188,P02-1040,0,0.0889934,"le, and ran a statistical significance test, comparing it to the merged top 3 results from our baseline system. The 3 runs were merged by duplicating each run 3 times, and arranging them in the file so that the significance testing compares each run with all the runs of the baseline. We performed significance testing using paired bootstrap resampling (Koehn, 2004). The difference is considered statistically significant if p < 0.05 using 1000 iterations of paired bootstrap resampling. 3.2 Results Our results are shown in Table 1. The second and the third columns contain the average BLEU score (Papineni et al., 2002) on the top three results on the development and test sets. The fourth column is the p-value for statistical significance testing against the baseline. The first row shows the results for our baseline. The second row contains the results for using the source (Chinese) side complete semantic rules of Section 2.1, and the third row is the results for combining both the source and the target side complete semantic rules. As noted before, in both of these experiments we also use the regular GHKM rules. The result show that the source side complete semantic rules improve the system (p = 0.048), and"
D14-1188,N09-2004,0,0.128002,"ment with translation rules which contain the core arguments for the predicates in the source side of a MT system, and observe that using these rules significantly improves the translation quality. We also present a new semantic feature that resembles a language model. Our results show that the language model feature can also significantly improve MT results. 1 Introduction In recent years, there have been increasing efforts to incorporate semantics in statistical machine translation (SMT), and the use of predicateargument structures has provided promising improvements in translation quality. Wu and Fung (2009) showed that shallow semantic parsing can improve the translation quality in a machine translation system. They introduced a two step model, in which they used a semantic parser to rerank the translation hypotheses of a phrase-based system. Liu and Gildea (2010) used semantic features for a tree-to-string syntax based SMT system. Their features modeled deletion and reordering for source side semantic roles, and they improved the translation quality. Xiong et al. (2012) incorporated the semantic structures into phrasebased SMT by adding syntactic and semantic features to their translation model"
D14-1188,W11-1003,0,0.048911,"Missing"
D14-1188,P12-1095,0,0.0196433,"hine translation (SMT), and the use of predicateargument structures has provided promising improvements in translation quality. Wu and Fung (2009) showed that shallow semantic parsing can improve the translation quality in a machine translation system. They introduced a two step model, in which they used a semantic parser to rerank the translation hypotheses of a phrase-based system. Liu and Gildea (2010) used semantic features for a tree-to-string syntax based SMT system. Their features modeled deletion and reordering for source side semantic roles, and they improved the translation quality. Xiong et al. (2012) incorporated the semantic structures into phrasebased SMT by adding syntactic and semantic features to their translation model. They proposed two discriminative models which included features for predicate translation and argument reordering from source to target side. Bazrafshan and Gildea (2013) used semantic structures in a string-to-tree translation system by extracting translation rules enriched with semantic information, and showed that this can improve the translation quality. Li et al. (2013) used predicateargument structure reordering models for hierarchical phrase-based translation,"
D14-1188,P02-1039,0,0.0375199,"emantic roles, we extract the semantic rules by ensuring that the span of the root (in the target side) of the extracted rule covers all of the spans of the roles in the predicate-argument structure. The semantic rules are then used together with the original GHKM rules. We add a binary feature to distinguish the semantic rules from the rest. We experiment with adding the semantic rules from the source side, and compare that with adding semantic rules of both the source and the target side. In all of the experiments in this paper, we use a string-to-tree decoder which uses a CYK style parser (Yamada and Knight, 2002). Figure 2 depicts a deduction step in the baseline decoder. The CFG rule in the first line is used to generate a new item A with span (i, k) using items B and C, which have spans (i, j) and (j, k) respectively. The cost of each item is shown on the right. For experimenting with complete semantic rules, in addition having more rules, the only other modification made to the baseline system is extending the feature vector to include the new feature. We do not modify the decoder in any significant way. 2.2 Semantic Language Model The semantic language model is designed to encourage the correct or"
D14-1188,D10-1063,0,0.0159141,"28 sentences for the test set. These sentences have lengths smaller than 30, and they each have 4 reference translations. We used our in-house stringto-tree decoder that uses Earley parsing. Other than the features that we presented for our new methods, we used a set of nine standard features. The rules for the baseline system were extracted using the GHKM method. Our baseline GHKM rules also include composed rules, where larger rules are constructed by combining two levels of the regular GHKM rules. We exclude any unary rules (Chung et al., 2011), and only keep rules that have scope up to 3 (Hopkins and Langmead, 2010). For the semantic language model, we used the SRILM package (Stolcke, 2002) and trained a tri-gram language model with the default GoodTuring smoothing. Our target side semantic role labeler uses a maximum entropy classifier to label parsed sentences. We used Sections 02-22 of the Penn TreeBank to 1 The data was randomly selected from the following sources: LDC2006E86, LDC2006E93, LDC2002E18, LDC2002L27, LDC2003E07, LDC2003E14, LDC2004T08, LDC2005T06, LDC2005T10, LDC2005T34, LDC2006E26, LDC2005E83, LDC2006E34, LDC2006E85, LDC2006E92, LDC2006E24, LDC2006E92, LDC2006E24 train the labeler, and s"
D14-1188,W04-3250,0,0.272069,"Missing"
D14-1188,N13-1060,0,0.027231,"Missing"
D14-1188,C10-1081,1,0.837369,"age model. Our results show that the language model feature can also significantly improve MT results. 1 Introduction In recent years, there have been increasing efforts to incorporate semantics in statistical machine translation (SMT), and the use of predicateargument structures has provided promising improvements in translation quality. Wu and Fung (2009) showed that shallow semantic parsing can improve the translation quality in a machine translation system. They introduced a two step model, in which they used a semantic parser to rerank the translation hypotheses of a phrase-based system. Liu and Gildea (2010) used semantic features for a tree-to-string syntax based SMT system. Their features modeled deletion and reordering for source side semantic roles, and they improved the translation quality. Xiong et al. (2012) incorporated the semantic structures into phrasebased SMT by adding syntactic and semantic features to their translation model. They proposed two discriminative models which included features for predicate translation and argument reordering from source to target side. Bazrafshan and Gildea (2013) used semantic structures in a string-to-tree translation system by extracting translation"
D14-1188,P03-1021,0,0.119649,"Missing"
D14-1188,D08-1076,0,\N,Missing
D16-1224,D15-1198,0,0.142364,"ations, such as statistical machine translation (SMT) (Jones et al., 2012). The task of AMR-to-text generation is to generate grammatical text containing the same semantic meaning as a given AMR graph. This task is important yet also challenging since each AMR graph usually has multiple corresponding sentences, and syntactic structure and function words are abstracted away when transforming a sentence into AMR (Banarescu et al., 2013). There has been work dealing with text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015). On the other hand, relatively little work has been done on AMR-to-text generation. One recent exception is Flanigan et al. (2016), who first generate a spanning tree for the input AMR graph, and then apply a tree transducer to generate the sentence. Here, we directly generate the sentence from an input AMR by treating AMR-to-text generation as a variant of the traveling salesman problem (TSP). Given an AMR as input, our method first cuts the graph into several rooted and connected fragments (sub-graphs), and then finds the translation for each fragment, before finally generating the sentence"
D16-1224,W13-2322,0,0.526909,"ing for a given AMR graph. We attack the task by first partitioning the AMR graph into smaller fragments, and then generating the translation for each fragment, before finally deciding the order by solving an asymmetric generalized traveling salesman problem (AGTSP). A Maximum Entropy classifier is trained to estimate the traveling costs, and a TSP solver is used to find the optimized solution. The final model reports a BLEU score of 22.44 on the SemEval-2016 Task8 dataset. 1 ARG0 go-01 ARG0 boy Figure 1: AMR graph for “The boy wants to go”. Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism encoding the meaning of a sentence as a rooted, directed graph. Shown in Figure 1, the nodes of an AMR graph (e.g. “boy”, “go-01” and “want01”) represent concepts, and the edges (e.g. “ARG0” and “ARG1”) represent relations between concepts. AMR jointly encodes a set of different semantic phenomena, which makes it useful in applications like question answering and semantics-based machine translation. AMR has served as an intermediate representation for various text-to-text NLP applications, such as statistical machine translation (SMT) (Jones et al., 2012). The task of"
D16-1224,C10-1012,0,0.0287772,"input AMR graph into a tree before linearization, we apply synchronous rules consisting of AMR graph fragments and text to directly transfer a AMR graph into a sentence. In addition to AMR parsing and generation, there has also been work using AMR as a semantic representation in machine translation (Jones et al., 2012). Our work also belongs to the task of text generation (Reiter and Dale, 1997). There has been work on generating natural language text from a bag of words (Wan et al., 2009; Zhang and Clark, 2015), surface syntactic trees (Zhang, 2013; Song et al., 2014), deep semantic graphs (Bohnet et al., 2010) and logical forms (White, 2004; White and Rajkumar, 2009). We are among the first to investigate generation from AMR, which is a different type of semantic representation. 5 Conclusion In conclusion, we showed that a TSP solver with a few real-valued features can be useful for AMR-totext generation. Our method is based on a set of graph to string rules, yet significantly better than a PBMT-based baseline. This shows that our rule induction algorithm is effective and that the TSP solver finds better solutions than beam search. Acknowledgments We are grateful for the help of Jeffrey Flanigan, L"
D16-1224,P13-2131,0,0.0614093,"tence for the whole AMR by ordering the translations. To cut the AMR and translate each fragment, we match the input AMR with rules, each consisting of a rooted, connected AMR fragment and a corresponding translation. These rules serve in a similar way to rules in SMT models. We learn the rules by a modified version of the sampling algorithm of Peng 2084 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2084–2089, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics et al. (2015), and use the rule matching algorithm of Cai and Knight (2013). For decoding the fragments and synthesizing the output, we define a cut to be a subset of matched rules without overlap that covers the AMR, and an ordered cut to be a cut with the rules being ordered. To generate a sentence for the whole AMR, we search for an ordered cut, and concatenate translations of all rules in the cut. TSP is used to traverse different cuts and determine the best order. Intuitively, our method is similar to phrase-based SMT, which first cuts the input sentence into phrases, then obtains the translation for each source phrase, before finally generating the target sente"
D16-1224,P14-1134,0,0.339027,"machine translation. AMR has served as an intermediate representation for various text-to-text NLP applications, such as statistical machine translation (SMT) (Jones et al., 2012). The task of AMR-to-text generation is to generate grammatical text containing the same semantic meaning as a given AMR graph. This task is important yet also challenging since each AMR graph usually has multiple corresponding sentences, and syntactic structure and function words are abstracted away when transforming a sentence into AMR (Banarescu et al., 2013). There has been work dealing with text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015). On the other hand, relatively little work has been done on AMR-to-text generation. One recent exception is Flanigan et al. (2016), who first generate a spanning tree for the input AMR graph, and then apply a tree transducer to generate the sentence. Here, we directly generate the sentence from an input AMR by treating AMR-to-text generation as a variant of the traveling salesman problem (TSP). Given an AMR as input, our method first cuts the graph into several rooted and connected fragment"
D16-1224,N16-1087,0,0.350788,"ammatical text containing the same semantic meaning as a given AMR graph. This task is important yet also challenging since each AMR graph usually has multiple corresponding sentences, and syntactic structure and function words are abstracted away when transforming a sentence into AMR (Banarescu et al., 2013). There has been work dealing with text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015). On the other hand, relatively little work has been done on AMR-to-text generation. One recent exception is Flanigan et al. (2016), who first generate a spanning tree for the input AMR graph, and then apply a tree transducer to generate the sentence. Here, we directly generate the sentence from an input AMR by treating AMR-to-text generation as a variant of the traveling salesman problem (TSP). Given an AMR as input, our method first cuts the graph into several rooted and connected fragments (sub-graphs), and then finds the translation for each fragment, before finally generating the sentence for the whole AMR by ordering the translations. To cut the AMR and translate each fragment, we match the input AMR with rules, eac"
D16-1224,C12-1083,0,0.362052,"on (AMR) (Banarescu et al., 2013) is a semantic formalism encoding the meaning of a sentence as a rooted, directed graph. Shown in Figure 1, the nodes of an AMR graph (e.g. “boy”, “go-01” and “want01”) represent concepts, and the edges (e.g. “ARG0” and “ARG1”) represent relations between concepts. AMR jointly encodes a set of different semantic phenomena, which makes it useful in applications like question answering and semantics-based machine translation. AMR has served as an intermediate representation for various text-to-text NLP applications, such as statistical machine translation (SMT) (Jones et al., 2012). The task of AMR-to-text generation is to generate grammatical text containing the same semantic meaning as a given AMR graph. This task is important yet also challenging since each AMR graph usually has multiple corresponding sentences, and syntactic structure and function words are abstracted away when transforming a sentence into AMR (Banarescu et al., 2013). There has been work dealing with text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015). On the other hand, relatively little work has been do"
D16-1224,N03-1017,0,0.0278261,"hose AMR fragments share common concepts. Otherwise the traveling cost is evaluated by a maximum entropy model, which will be discussed in detail in Section 2.4. 2.3 Rule Acquisition We extract rules from a corpus of (sentence, AMR) pairs using the method of Peng et al. (2015). Given 2086 an aligned (sentence, AMR) pair, a phrase-fragment pair is a pair ([i, j], f ), where [i, j] is a span of the sentence and f represents a connected and rooted AMR fragment. A fragment decomposition forest consists of all possible phrase-fragment pairs that satisfy the alignment agreement for phrase-based MT (Koehn et al., 2003). The rules that we use for generation are the result of applying an MCMC procedure to learn a set of likely phrase-fragment pairs from the forests containing all possible pairs. One difference from the work of Peng et al. (2015) is that, while they require the string side to be tight (does not include unaligned words on both sides), we expand the tight phrases to incorporate unaligned words on both sides. The intuition is that they do text-to-AMR parsing, which often involves discarding function words, while our task is AMR-to-text generation, and we need to be able to fill in these unaligned"
D16-1224,P02-1040,0,0.0987956,"have lower path length than others. 3 3.1 Dev 13.13 13.15 17.68 17.19 21.12 23.00 Test 16.94 14.93 18.09 17.75 22.44 23.00 Table 1: Main results. training instances, 1368 dev instances and 1371 test instances. Each instance consists of an AMR graph and a sentence representing the same meaning. Rules are extracted from the training data, and hyperparameters are tuned on the dev set. For tuning and testing, we filter out sentences that have more than 30 words, resulting in 1103 dev instances and 1055 test instances. We train a 4-gram language model (LM) with gigaword (LDC2011T07), and use BLEU (Papineni et al., 2002) as the evaluation metric. To solve the AGTSP, we use Or-tool3 . Our graph-to-string rules are reminiscent of phrase-to-string rules in phrase-based MT (PBMT). We compare our system to a baseline (PBMT) that first linearizes the input AMR graph by breadth first traversal, and then adopts the PBMT system from Moses4 to translate the linearized AMR into a sentence. To traverse the children of an AMR concept, we use the original order in the text file. The MT system is trained with the default setting on the same dataset and LM. We also compare with JAMRgen5 (Flanigan et al., 2016), which is trai"
D16-1224,K15-1004,1,0.941008,"intermediate representation for various text-to-text NLP applications, such as statistical machine translation (SMT) (Jones et al., 2012). The task of AMR-to-text generation is to generate grammatical text containing the same semantic meaning as a given AMR graph. This task is important yet also challenging since each AMR graph usually has multiple corresponding sentences, and syntactic structure and function words are abstracted away when transforming a sentence into AMR (Banarescu et al., 2013). There has been work dealing with text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015). On the other hand, relatively little work has been done on AMR-to-text generation. One recent exception is Flanigan et al. (2016), who first generate a spanning tree for the input AMR graph, and then apply a tree transducer to generate the sentence. Here, we directly generate the sentence from an input AMR by treating AMR-to-text generation as a variant of the traveling salesman problem (TSP). Given an AMR as input, our method first cuts the graph into several rooted and connected fragments (sub-graphs), and then finds the tra"
D16-1224,D15-1136,0,0.160292,"-to-text NLP applications, such as statistical machine translation (SMT) (Jones et al., 2012). The task of AMR-to-text generation is to generate grammatical text containing the same semantic meaning as a given AMR graph. This task is important yet also challenging since each AMR graph usually has multiple corresponding sentences, and syntactic structure and function words are abstracted away when transforming a sentence into AMR (Banarescu et al., 2013). There has been work dealing with text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015). On the other hand, relatively little work has been done on AMR-to-text generation. One recent exception is Flanigan et al. (2016), who first generate a spanning tree for the input AMR graph, and then apply a tree transducer to generate the sentence. Here, we directly generate the sentence from an input AMR by treating AMR-to-text generation as a variant of the traveling salesman problem (TSP). Given an AMR as input, our method first cuts the graph into several rooted and connected fragments (sub-graphs), and then finds the translation for each fragment, before finally ge"
D16-1224,N15-3006,0,0.10334,"sentation for various text-to-text NLP applications, such as statistical machine translation (SMT) (Jones et al., 2012). The task of AMR-to-text generation is to generate grammatical text containing the same semantic meaning as a given AMR graph. This task is important yet also challenging since each AMR graph usually has multiple corresponding sentences, and syntactic structure and function words are abstracted away when transforming a sentence into AMR (Banarescu et al., 2013). There has been work dealing with text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015). On the other hand, relatively little work has been done on AMR-to-text generation. One recent exception is Flanigan et al. (2016), who first generate a spanning tree for the input AMR graph, and then apply a tree transducer to generate the sentence. Here, we directly generate the sentence from an input AMR by treating AMR-to-text generation as a variant of the traveling salesman problem (TSP). Given an AMR as input, our method first cuts the graph into several rooted and connected fragments (sub-graphs), and then finds the translation for each fragment"
D16-1224,E09-1097,0,0.0297715,"(2016) and our work here study sentence generation from a given AMR graph. Different from Flanigan et al. (2016) who map a input AMR graph into a tree before linearization, we apply synchronous rules consisting of AMR graph fragments and text to directly transfer a AMR graph into a sentence. In addition to AMR parsing and generation, there has also been work using AMR as a semantic representation in machine translation (Jones et al., 2012). Our work also belongs to the task of text generation (Reiter and Dale, 1997). There has been work on generating natural language text from a bag of words (Wan et al., 2009; Zhang and Clark, 2015), surface syntactic trees (Zhang, 2013; Song et al., 2014), deep semantic graphs (Bohnet et al., 2010) and logical forms (White, 2004; White and Rajkumar, 2009). We are among the first to investigate generation from AMR, which is a different type of semantic representation. 5 Conclusion In conclusion, we showed that a TSP solver with a few real-valued features can be useful for AMR-totext generation. Our method is based on a set of graph to string rules, yet significantly better than a PBMT-based baseline. This shows that our rule induction algorithm is effective and th"
D16-1224,N15-1040,0,0.0534278,"Missing"
D16-1224,D09-1043,0,0.0313619,"apply synchronous rules consisting of AMR graph fragments and text to directly transfer a AMR graph into a sentence. In addition to AMR parsing and generation, there has also been work using AMR as a semantic representation in machine translation (Jones et al., 2012). Our work also belongs to the task of text generation (Reiter and Dale, 1997). There has been work on generating natural language text from a bag of words (Wan et al., 2009; Zhang and Clark, 2015), surface syntactic trees (Zhang, 2013; Song et al., 2014), deep semantic graphs (Bohnet et al., 2010) and logical forms (White, 2004; White and Rajkumar, 2009). We are among the first to investigate generation from AMR, which is a different type of semantic representation. 5 Conclusion In conclusion, we showed that a TSP solver with a few real-valued features can be useful for AMR-totext generation. Our method is based on a set of graph to string rules, yet significantly better than a PBMT-based baseline. This shows that our rule induction algorithm is effective and that the TSP solver finds better solutions than beam search. Acknowledgments We are grateful for the help of Jeffrey Flanigan, Lin Zhao, and Yifan He. This work was funded by NSF IIS-144"
D16-1224,P09-1038,0,0.143191,"4 System PBMT OnlyConceptRule OnlyInducedRule OnlyBigramLM All JAMR-gen Traveling cost Considering an AGTSP graph whose nodes are clustered into m groups, we define the traveling cost for a tour T in Equation 1: cost(ns , ne ) = − m X log p(“yes”|nTi , nTi+1 ) (1) i=0 where nT0 = ns , nTm+1 = ne and each nTi (i ∈ [1 . . . m]) belongs to a group that is different from all others. Here p(“yes”|nj , ni ) represents a learned score for a move from nj to ni . The choices before nTi are independent from choosing nTi+1 given nTi because of the Markovian property of the TSP problem. Previous methods (Zaslavskiy et al., 2009) evaluate traveling costs p(nTi+1 |nTi ) by using a language model. Inevitably some rules may only cover one translation word, making only bigram language models naturally applicable. Zaslavskiy et al. (2009) introduces a method for incorporating a trigram language model. However, as a result, the number of nodes in the AGTSP graph grows exponentially. To tackle the problem, we treat it as a local binary (“yes” or “no”) classification problem whether we should move to nj from ni . We train a maximum entropy model, where p(“yes”|ni , nj ) is defined as: p(“yes”|ni , nj ) = k hX i 1 exp λi fi (“"
D16-1224,J15-3005,1,0.852028,"k here study sentence generation from a given AMR graph. Different from Flanigan et al. (2016) who map a input AMR graph into a tree before linearization, we apply synchronous rules consisting of AMR graph fragments and text to directly transfer a AMR graph into a sentence. In addition to AMR parsing and generation, there has also been work using AMR as a semantic representation in machine translation (Jones et al., 2012). Our work also belongs to the task of text generation (Reiter and Dale, 1997). There has been work on generating natural language text from a bag of words (Wan et al., 2009; Zhang and Clark, 2015), surface syntactic trees (Zhang, 2013; Song et al., 2014), deep semantic graphs (Bohnet et al., 2010) and logical forms (White, 2004; White and Rajkumar, 2009). We are among the first to investigate generation from AMR, which is a different type of semantic representation. 5 Conclusion In conclusion, we showed that a TSP solver with a few real-valued features can be useful for AMR-totext generation. Our method is based on a set of graph to string rules, yet significantly better than a PBMT-based baseline. This shows that our rule induction algorithm is effective and that the TSP solver finds"
D18-1246,D17-1209,0,0.168044,"ia the dependency PREP OF NN path “exon-19 ! gene ! EGFR” is lost from the original subgraph. Second, using LSTMs on both DAGs, information of only ancestors and descendants can be incorporated for each word. Sibling information, which may also be important, is not included. A potential solution to the problems above is to model a graph as a whole, learning its representation without breaking it into two DAGs. Due to the existence of cycles, naive extension of tree LSTMs cannot serve this goal. Recently, graph convolutional networks (GCN) (Kipf and Welling, 2017; Marcheggiani and Titov, 2017; Bastings et al., 2017) and graph recurrent networks (GRN) (Song et al., 2018; Zhang et al., 2018) have been proposed for representing graph structures for NLP tasks. Such methods encode a given graph by hierarchically learning representations of neighboring nodes in the graphs via their connecting edges. While GCNs use CNN for information exchange, GRNs take gated recurrent steps to this end. For fair comparison with DAG LSTMs, we build a graph LSTM by extending Song et al. (2018), which strictly follow the configurations of Peng et al. (2017) such as the source of features and hyper parameter settings. In particul"
D18-1246,P10-1160,0,0.346512,"to be useful for detecting explicit facts, such as cause-effect (Hendrickx et al., 2009), and predicting the effectiveness of a medicine on a cancer caused by mutation of a certain gene in the biomedical domain (Quirk and Poon, 2017; Peng et al., 2017). While most existing work extracts relations within a sentence (Zelenko et al., 2003; Palmer et al., 2005; Zhao and Grishman, 2005; Jiang and Zhai, 2007; Plank and Moschitti, 2013; Li and Ji, 2014; Gormley et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2017), the task of cross-sentence relation extraction has received increasing attention (Gerber and Chai, 2010; Yoshikawa et al., 2011). Recently, Peng ⇤ Equal contribution et al. (2017) extend cross-sentence relation extraction by further detecting relations among several entity mentions (n-ary relation). Table 1 shows an example, which conveys the fact that cancers caused by the 858E mutation on EGFR gene can respond to the gefitinib medicine. The three entity mentions form a ternary relation yet appear in distinct sentences. Peng et al. (2017) proposed a graph-structured LSTM for n-ary relation extraction. As shown in Figure 1 (a), graphs are constructed from input sentences with dependency edges,"
D18-1246,N18-1082,0,0.0254715,"Missing"
D18-1246,D15-1205,0,0.0337129,"a central task in natural language processing, relation extraction has been investigated on news, web text and biomedical domains. It has been shown to be useful for detecting explicit facts, such as cause-effect (Hendrickx et al., 2009), and predicting the effectiveness of a medicine on a cancer caused by mutation of a certain gene in the biomedical domain (Quirk and Poon, 2017; Peng et al., 2017). While most existing work extracts relations within a sentence (Zelenko et al., 2003; Palmer et al., 2005; Zhao and Grishman, 2005; Jiang and Zhai, 2007; Plank and Moschitti, 2013; Li and Ji, 2014; Gormley et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2017), the task of cross-sentence relation extraction has received increasing attention (Gerber and Chai, 2010; Yoshikawa et al., 2011). Recently, Peng ⇤ Equal contribution et al. (2017) extend cross-sentence relation extraction by further detecting relations among several entity mentions (n-ary relation). Table 1 shows an example, which conveys the fact that cancers caused by the 858E mutation on EGFR gene can respond to the gefitinib medicine. The three entity mentions form a ternary relation yet appear in distinct sentences. Peng et al. (2017) proposed"
D18-1246,W09-2415,0,0.154489,"Missing"
D18-1246,N07-1015,0,0.0610435,"tion in EGFR gene respond to gefitinib treatment. Introduction As a central task in natural language processing, relation extraction has been investigated on news, web text and biomedical domains. It has been shown to be useful for detecting explicit facts, such as cause-effect (Hendrickx et al., 2009), and predicting the effectiveness of a medicine on a cancer caused by mutation of a certain gene in the biomedical domain (Quirk and Poon, 2017; Peng et al., 2017). While most existing work extracts relations within a sentence (Zelenko et al., 2003; Palmer et al., 2005; Zhao and Grishman, 2005; Jiang and Zhai, 2007; Plank and Moschitti, 2013; Li and Ji, 2014; Gormley et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2017), the task of cross-sentence relation extraction has received increasing attention (Gerber and Chai, 2010; Yoshikawa et al., 2011). Recently, Peng ⇤ Equal contribution et al. (2017) extend cross-sentence relation extraction by further detecting relations among several entity mentions (n-ary relation). Table 1 shows an example, which conveys the fact that cancers caused by the 858E mutation on EGFR gene can respond to the gefitinib medicine. The three entity mentions form a ternary rela"
D18-1246,P14-1038,0,0.0304269,"Introduction As a central task in natural language processing, relation extraction has been investigated on news, web text and biomedical domains. It has been shown to be useful for detecting explicit facts, such as cause-effect (Hendrickx et al., 2009), and predicting the effectiveness of a medicine on a cancer caused by mutation of a certain gene in the biomedical domain (Quirk and Poon, 2017; Peng et al., 2017). While most existing work extracts relations within a sentence (Zelenko et al., 2003; Palmer et al., 2005; Zhao and Grishman, 2005; Jiang and Zhai, 2007; Plank and Moschitti, 2013; Li and Ji, 2014; Gormley et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2017), the task of cross-sentence relation extraction has received increasing attention (Gerber and Chai, 2010; Yoshikawa et al., 2011). Recently, Peng ⇤ Equal contribution et al. (2017) extend cross-sentence relation extraction by further detecting relations among several entity mentions (n-ary relation). Table 1 shows an example, which conveys the fact that cancers caused by the 858E mutation on EGFR gene can respond to the gefitinib medicine. The three entity mentions form a ternary relation yet appear in distinct sentences. Peng"
D18-1246,P14-5010,0,0.00328524,", or a multi-class classification problem of detecting which relation holds for the entity mentions. Take Table 1 as an example. The binary classification task is to determine whether gefitinib would have an effect on this type of cancer, given a cancer patient with 858E mutation on gene EGFR. The multi-class classification task is to detect the exact drug effect: response, resistance, sensitivity, etc. 3 Baseline: Bi-directional DAG LSTM Peng et al. (2017) formulate the task as a graphstructured problem in order to adopt rich dependency and discourse features. In particular, Stanford parser (Manning et al., 2014) is used to assign syntactic structure to input sentences, and heads of two consecutive sentences are connected to represent discourse information, resulting in a graph structure. For each input graph G = (V, E), the nodes V are words within input sentences, and each edge e 2 E connects two words that either have a relation or are adjacent to each other. Each edge is denoted as a triple (i, j, l), where i and j are the indices of the source and target words, respectively, and the edge label l indicates either a dependency or discourse relation (such as “nsubj”) or a relative position (such as"
D18-1246,D17-1159,0,0.24825,"between “exon-19” and “EGFR” via the dependency PREP OF NN path “exon-19 ! gene ! EGFR” is lost from the original subgraph. Second, using LSTMs on both DAGs, information of only ancestors and descendants can be incorporated for each word. Sibling information, which may also be important, is not included. A potential solution to the problems above is to model a graph as a whole, learning its representation without breaking it into two DAGs. Due to the existence of cycles, naive extension of tree LSTMs cannot serve this goal. Recently, graph convolutional networks (GCN) (Kipf and Welling, 2017; Marcheggiani and Titov, 2017; Bastings et al., 2017) and graph recurrent networks (GRN) (Song et al., 2018; Zhang et al., 2018) have been proposed for representing graph structures for NLP tasks. Such methods encode a given graph by hierarchically learning representations of neighboring nodes in the graphs via their connecting edges. While GCNs use CNN for information exchange, GRNs take gated recurrent steps to this end. For fair comparison with DAG LSTMs, we build a graph LSTM by extending Song et al. (2018), which strictly follow the configurations of Peng et al. (2017) such as the source of features and hyper paramet"
D18-1246,P05-1061,0,0.240902,"comparisons with the binary relation extraction results. However, the performance gaps between GS GLSTM and Bidir DAG LSTM dramatically increase, showing the superiority of GS GLSTM over Bidir DAG LSTM in utilizing context information. 7 B INARY 50.7 71.7* Table 6: Average test accuracies for multi-class relation extraction with all instances (“Cross”). GS GLSTM 85 80 T ERNARY 51.7 71.1* Related Work N -ary relation extraction N -ary relation extractions can be traced back to MUC-7 (Chinchor, 1998), which focuses on entity-attribution relations. It has also been studied in biomedical domain (McDonald et al., 2005), but only the instances within a single sentence are considered. Previous work on cross-sentence relation extraction relies on either explicit co-reference annotation (Gerber and Chai, 2010; Yoshikawa et al., 2011), or the assumption that the whole document refers to a single coherent event (Wick et al., 2006; Swampillai and Stevenson, 2011). Both simplify the problem and reduce the need for learning better contextual representation of entity mentions. A notable exception is Quirk and Poon (2017), who adopt distant supervision and integrated contextual evidence of diverse types without relyin"
D18-1246,P16-1105,0,0.546468,"ral language processing, relation extraction has been investigated on news, web text and biomedical domains. It has been shown to be useful for detecting explicit facts, such as cause-effect (Hendrickx et al., 2009), and predicting the effectiveness of a medicine on a cancer caused by mutation of a certain gene in the biomedical domain (Quirk and Poon, 2017; Peng et al., 2017). While most existing work extracts relations within a sentence (Zelenko et al., 2003; Palmer et al., 2005; Zhao and Grishman, 2005; Jiang and Zhai, 2007; Plank and Moschitti, 2013; Li and Ji, 2014; Gormley et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2017), the task of cross-sentence relation extraction has received increasing attention (Gerber and Chai, 2010; Yoshikawa et al., 2011). Recently, Peng ⇤ Equal contribution et al. (2017) extend cross-sentence relation extraction by further detecting relations among several entity mentions (n-ary relation). Table 1 shows an example, which conveys the fact that cancers caused by the 858E mutation on EGFR gene can respond to the gefitinib medicine. The three entity mentions form a ternary relation yet appear in distinct sentences. Peng et al. (2017) proposed a graph-structured LST"
D18-1246,P15-1150,0,0.174145,"Missing"
D18-1246,W06-1671,0,0.0844934,"extraction with all instances (“Cross”). GS GLSTM 85 80 T ERNARY 51.7 71.1* Related Work N -ary relation extraction N -ary relation extractions can be traced back to MUC-7 (Chinchor, 1998), which focuses on entity-attribution relations. It has also been studied in biomedical domain (McDonald et al., 2005), but only the instances within a single sentence are considered. Previous work on cross-sentence relation extraction relies on either explicit co-reference annotation (Gerber and Chai, 2010; Yoshikawa et al., 2011), or the assumption that the whole document refers to a single coherent event (Wick et al., 2006; Swampillai and Stevenson, 2011). Both simplify the problem and reduce the need for learning better contextual representation of entity mentions. A notable exception is Quirk and Poon (2017), who adopt distant supervision and integrated contextual evidence of diverse types without relying on these assumptions. However, they only study binary relations. We follow Peng et al. (2017) by studying ternary cross-sentence relations. Graph encoder Liang et al. (2016) build a graph LSTM model for semantic object parsing, which aims to segment objects within an image into more fine-grained, semanticall"
D18-1246,J05-1004,1,0.140836,"An example showing that tumors with L858E mutation in EGFR gene respond to gefitinib treatment. Introduction As a central task in natural language processing, relation extraction has been investigated on news, web text and biomedical domains. It has been shown to be useful for detecting explicit facts, such as cause-effect (Hendrickx et al., 2009), and predicting the effectiveness of a medicine on a cancer caused by mutation of a certain gene in the biomedical domain (Quirk and Poon, 2017; Peng et al., 2017). While most existing work extracts relations within a sentence (Zelenko et al., 2003; Palmer et al., 2005; Zhao and Grishman, 2005; Jiang and Zhai, 2007; Plank and Moschitti, 2013; Li and Ji, 2014; Gormley et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2017), the task of cross-sentence relation extraction has received increasing attention (Gerber and Chai, 2010; Yoshikawa et al., 2011). Recently, Peng ⇤ Equal contribution et al. (2017) extend cross-sentence relation extraction by further detecting relations among several entity mentions (n-ary relation). Table 1 shows an example, which conveys the fact that cancers caused by the 858E mutation on EGFR gene can respond to the gefitinib medicine"
D18-1246,D18-1110,1,0.848544,"ng node states sequentially: for each input graph, a start node and a node sequence are chosen, which determines the order of recurrent state updates. In contrast, our graph LSTM do not need ordering of graph nodes, and is highly parallelizable. 2233 Graph convolutional networks (GCNs) and very recently graph recurrent networks (GRNs) have been used to model graph structures in NLP tasks, such as semantic role labeling (Marcheggiani and Titov, 2017), machine translation (Bastings et al., 2017), text generation (Song et al., 2018), text representation (Zhang et al., 2018) and semantic parsing (Xu et al., 2018b,a). In particular, Zhang et al. (2018) use GRN to represent raw sentences by building a graph structure of neighboring words and a sentence-level node, showing that the encoder outperforms BiLSTMs and Transformer (Vaswani et al., 2017) on classification and sequence labeling tasks; Song et al. (2018) build a GRN for encoding AMR graphs, showing that the representation is superior compared to BiLSTM on serialized AMR. Our work is in line with their work in the investigation of GRN on NLP. To our knowledge, we are the first to use GRN for representing dependency and discourse structures. Under"
D18-1246,Q17-1008,0,0.11233,"parallelization. On a standard benchmark, our model shows the best result in the literature. 1 Table 1: An example showing that tumors with L858E mutation in EGFR gene respond to gefitinib treatment. Introduction As a central task in natural language processing, relation extraction has been investigated on news, web text and biomedical domains. It has been shown to be useful for detecting explicit facts, such as cause-effect (Hendrickx et al., 2009), and predicting the effectiveness of a medicine on a cancer caused by mutation of a certain gene in the biomedical domain (Quirk and Poon, 2017; Peng et al., 2017). While most existing work extracts relations within a sentence (Zelenko et al., 2003; Palmer et al., 2005; Zhao and Grishman, 2005; Jiang and Zhai, 2007; Plank and Moschitti, 2013; Li and Ji, 2014; Gormley et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2017), the task of cross-sentence relation extraction has received increasing attention (Gerber and Chai, 2010; Yoshikawa et al., 2011). Recently, Peng ⇤ Equal contribution et al. (2017) extend cross-sentence relation extraction by further detecting relations among several entity mentions (n-ary relation). Table 1 shows an example, which co"
D18-1246,D14-1162,0,0.0965546,"or nonresponse”, “sensitivity”, “response”, “resistance” and “None”. We follow Peng et al. (2017) and binarize multi-class labels by grouping all relation classes as “Yes” and treat “None” as “No”. 6.2 Settings Following Peng et al. (2017), five-fold crossvalidation is used for evaluating the models,3 and the final test accuracy is calculated by averaging the test accuracies over all five folds. For each fold, we randomly separate 200 instances from the training set for development. The batch size is set as 8 for all experiments. Word embeddings are initialized with the 100-dimensional GloVe (Pennington et al., 2014) vectors, pretrained on 6 billion words from Wikipedia and web text. The edge label embeddings are 3-dimensional and randomly 2 The dataset is available at http://hanover.azurewebsites.net. 3 The released data has been separated into 5 portions, and we follow the exact split. 2230 Model Quirk and Poon (2017) Peng et al. (2017) - EMBED Peng et al. (2017) - FULL + multi-task Bidir DAG LSTM GS GLSTM Figure 3: Dev accuracies against transition steps for the graph state LSTM model. initialized. Pretrained word embeddings are not updated during training. The dimension of hidden vectors in LSTM units"
D18-1246,P13-1147,0,0.0141763,"ond to gefitinib treatment. Introduction As a central task in natural language processing, relation extraction has been investigated on news, web text and biomedical domains. It has been shown to be useful for detecting explicit facts, such as cause-effect (Hendrickx et al., 2009), and predicting the effectiveness of a medicine on a cancer caused by mutation of a certain gene in the biomedical domain (Quirk and Poon, 2017; Peng et al., 2017). While most existing work extracts relations within a sentence (Zelenko et al., 2003; Palmer et al., 2005; Zhao and Grishman, 2005; Jiang and Zhai, 2007; Plank and Moschitti, 2013; Li and Ji, 2014; Gormley et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2017), the task of cross-sentence relation extraction has received increasing attention (Gerber and Chai, 2010; Yoshikawa et al., 2011). Recently, Peng ⇤ Equal contribution et al. (2017) extend cross-sentence relation extraction by further detecting relations among several entity mentions (n-ary relation). Table 1 shows an example, which conveys the fact that cancers caused by the 858E mutation on EGFR gene can respond to the gefitinib medicine. The three entity mentions form a ternary relation yet appear in distinct"
D18-1246,E17-1110,0,0.142755,"ation by allowing more parallelization. On a standard benchmark, our model shows the best result in the literature. 1 Table 1: An example showing that tumors with L858E mutation in EGFR gene respond to gefitinib treatment. Introduction As a central task in natural language processing, relation extraction has been investigated on news, web text and biomedical domains. It has been shown to be useful for detecting explicit facts, such as cause-effect (Hendrickx et al., 2009), and predicting the effectiveness of a medicine on a cancer caused by mutation of a certain gene in the biomedical domain (Quirk and Poon, 2017; Peng et al., 2017). While most existing work extracts relations within a sentence (Zelenko et al., 2003; Palmer et al., 2005; Zhao and Grishman, 2005; Jiang and Zhai, 2007; Plank and Moschitti, 2013; Li and Ji, 2014; Gormley et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2017), the task of cross-sentence relation extraction has received increasing attention (Gerber and Chai, 2010; Yoshikawa et al., 2011). Recently, Peng ⇤ Equal contribution et al. (2017) extend cross-sentence relation extraction by further detecting relations among several entity mentions (n-ary relation). Table 1 shows"
D18-1246,P18-1150,1,0.937565,"s lost from the original subgraph. Second, using LSTMs on both DAGs, information of only ancestors and descendants can be incorporated for each word. Sibling information, which may also be important, is not included. A potential solution to the problems above is to model a graph as a whole, learning its representation without breaking it into two DAGs. Due to the existence of cycles, naive extension of tree LSTMs cannot serve this goal. Recently, graph convolutional networks (GCN) (Kipf and Welling, 2017; Marcheggiani and Titov, 2017; Bastings et al., 2017) and graph recurrent networks (GRN) (Song et al., 2018; Zhang et al., 2018) have been proposed for representing graph structures for NLP tasks. Such methods encode a given graph by hierarchically learning representations of neighboring nodes in the graphs via their connecting edges. While GCNs use CNN for information exchange, GRNs take gated recurrent steps to this end. For fair comparison with DAG LSTMs, we build a graph LSTM by extending Song et al. (2018), which strictly follow the configurations of Peng et al. (2017) such as the source of features and hyper parameter settings. In particular, the full input graph is modeled as a single state,"
D18-1246,D17-1182,1,0.86292,", relation extraction has been investigated on news, web text and biomedical domains. It has been shown to be useful for detecting explicit facts, such as cause-effect (Hendrickx et al., 2009), and predicting the effectiveness of a medicine on a cancer caused by mutation of a certain gene in the biomedical domain (Quirk and Poon, 2017; Peng et al., 2017). While most existing work extracts relations within a sentence (Zelenko et al., 2003; Palmer et al., 2005; Zhao and Grishman, 2005; Jiang and Zhai, 2007; Plank and Moschitti, 2013; Li and Ji, 2014; Gormley et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2017), the task of cross-sentence relation extraction has received increasing attention (Gerber and Chai, 2010; Yoshikawa et al., 2011). Recently, Peng ⇤ Equal contribution et al. (2017) extend cross-sentence relation extraction by further detecting relations among several entity mentions (n-ary relation). Table 1 shows an example, which conveys the fact that cancers caused by the 858E mutation on EGFR gene can respond to the gefitinib medicine. The three entity mentions form a ternary relation yet appear in distinct sentences. Peng et al. (2017) proposed a graph-structured LSTM for n-ary relation"
D18-1246,P18-1030,1,0.910776,"ginal subgraph. Second, using LSTMs on both DAGs, information of only ancestors and descendants can be incorporated for each word. Sibling information, which may also be important, is not included. A potential solution to the problems above is to model a graph as a whole, learning its representation without breaking it into two DAGs. Due to the existence of cycles, naive extension of tree LSTMs cannot serve this goal. Recently, graph convolutional networks (GCN) (Kipf and Welling, 2017; Marcheggiani and Titov, 2017; Bastings et al., 2017) and graph recurrent networks (GRN) (Song et al., 2018; Zhang et al., 2018) have been proposed for representing graph structures for NLP tasks. Such methods encode a given graph by hierarchically learning representations of neighboring nodes in the graphs via their connecting edges. While GCNs use CNN for information exchange, GRNs take gated recurrent steps to this end. For fair comparison with DAG LSTMs, we build a graph LSTM by extending Song et al. (2018), which strictly follow the configurations of Peng et al. (2017) such as the source of features and hyper parameter settings. In particular, the full input graph is modeled as a single state, with words in the gr"
D18-1246,P05-1052,0,0.0482608,"at tumors with L858E mutation in EGFR gene respond to gefitinib treatment. Introduction As a central task in natural language processing, relation extraction has been investigated on news, web text and biomedical domains. It has been shown to be useful for detecting explicit facts, such as cause-effect (Hendrickx et al., 2009), and predicting the effectiveness of a medicine on a cancer caused by mutation of a certain gene in the biomedical domain (Quirk and Poon, 2017; Peng et al., 2017). While most existing work extracts relations within a sentence (Zelenko et al., 2003; Palmer et al., 2005; Zhao and Grishman, 2005; Jiang and Zhai, 2007; Plank and Moschitti, 2013; Li and Ji, 2014; Gormley et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2017), the task of cross-sentence relation extraction has received increasing attention (Gerber and Chai, 2010; Yoshikawa et al., 2011). Recently, Peng ⇤ Equal contribution et al. (2017) extend cross-sentence relation extraction by further detecting relations among several entity mentions (n-ary relation). Table 1 shows an example, which conveys the fact that cancers caused by the 858E mutation on EGFR gene can respond to the gefitinib medicine. The three entity mentio"
D18-1246,R11-1004,0,0.0888552,"instances (“Cross”). GS GLSTM 85 80 T ERNARY 51.7 71.1* Related Work N -ary relation extraction N -ary relation extractions can be traced back to MUC-7 (Chinchor, 1998), which focuses on entity-attribution relations. It has also been studied in biomedical domain (McDonald et al., 2005), but only the instances within a single sentence are considered. Previous work on cross-sentence relation extraction relies on either explicit co-reference annotation (Gerber and Chai, 2010; Yoshikawa et al., 2011), or the assumption that the whole document refers to a single coherent event (Wick et al., 2006; Swampillai and Stevenson, 2011). Both simplify the problem and reduce the need for learning better contextual representation of entity mentions. A notable exception is Quirk and Poon (2017), who adopt distant supervision and integrated contextual evidence of diverse types without relying on these assumptions. However, they only study binary relations. We follow Peng et al. (2017) by studying ternary cross-sentence relations. Graph encoder Liang et al. (2016) build a graph LSTM model for semantic object parsing, which aims to segment objects within an image into more fine-grained, semantically meaningful parts. The nodes of"
D19-1020,D17-1209,0,0.142049,"endency forests is relatively rarely studied, with one exception being Tu et al. (2010), who use dependency forests to enhance long-range word-to-word dependencies for statistical machine translation. To our knowledge, we are the first to study the usefulness of dependency forests for relation extraction under a strong neural framework. Graph neural network Graph neural networks (GNNs) have been successful in encoding dependency trees for downstream tasks, such as semantic role labeling (Marcheggiani and Titov, 2017), semantic parsing (Xu et al., 2018), machine translation (Song et al., 2019; Bastings et al., 2017), relation extraction (Song et al., 2018b) and sentence ordering (Yin et al., 2019). In particular, Song et al. (2018b) showed that GNNs are more effective than DAG networks (Peng et al., 2017) for modeling syntactic trees in relation extraction, which cause loss of important structural information. We are the first to exploit GNNs for encoding search spaces in the form of dependency forests. 3 Task Formally, the input to our task is a sentence s = w1 , w2 , . . . , wN , where N is the number of words in the sentence and wi represents the i-th input word. s is annotated with boundary informati"
D19-1020,P18-1026,0,0.0260887,"nce. First, a dependency parser is used to label the syntactic structure of the input. Here our baseline system takes the standard approach, using the 1-best parser output tree DT as features. In contrast, our proposed model uses the most confident parser forest DF as features. Given DT or DF , the second step is to encode both s and DT /DF using a neural network, before making a prediction. We make use of the same graph neural network encoder structure to represent dependency syntax information for both the baseline and our model. In particular, a graph recurrent neural network architecture (Beck et al., 2018; Song et al., 2018a; Zhang et al., 2018a) is used, which has been shown effective in encoding graph structures (Song et al., 2019), giving competitive results with alternative graph networks such as graph convolutional neural networks (Marcheggiani and Titov, 2017; Bastings et al., 2017). 4 Baseline: D EP T REE As shown in Figure 2, our baseline model stacks a bidirectional LSTM layer to encode an input sentence w1 , . . . , wN with a graph recurrent network 209 In order to capture non-local interactions between words, the GRN layer adopts a message passing framework that performs iterative i"
D19-1020,H05-1091,0,0.867277,"the corresponding author nmod comp comp amod comp ... observed ... interaction of orexin receptor antagonist almorexant (a) nmod comp comp nmod comp amod comp ... observed ... interaction of orexin receptor antagonist almorexant (b) Figure 1: (a) 1-best dependency tree and (b) dependency forest for a medical-domain sentence, where edge label “comp” represents “compound”. Associated mentions are in different colors. Some irrelevant words and edges are omitted for simplicity. Previous work has shown that dependency syntax is important for guiding relation extraction (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Liu et al., 2015; Gormley et al., 2015; Xu et al., 2015a,b; Miwa and Bansal, 2016; Zhang et al., 2018b), especially in biological and medical domains (Quirk and Poon, 2017; Peng et al., 2017; Song et al., 2018b). Compared with sequential surface-level structures, such as POS tags, dependency trees help to model word-toword relations more easily by drawing direct connections between distant words that are syntactically correlated. Take the phrase “effect on the medicine” for example; “effect” and “medicine” are directly connected in a dependency tree, regardless of how many modifiers are adde"
D19-1020,W11-2905,0,0.0727527,"Missing"
D19-1020,W11-0216,0,0.0751446,"Missing"
D19-1020,C96-1058,0,0.0517257,"w recall given an imperfect parser. We investigate two algorithms to generate high-quality forests by judging “quality” from different perspectives: one focusing on arcs, and the other focusing on trees. E DGEWISE This algorithm focuses on the local relation of each individual edge and uses parser probabilities as confidence scores to assess edge qualities. Starting from the whole parser search space, it keeps all the edges with scores greater than a threshold . The time complexity is O(N 2 ), where N represents the sentence length.1 KB EST E ISNER This algorithm extends the Eisner algorithm (Eisner, 1996) with cube pruning (Huang and Chiang, 2005) for finding K highest-scored tree structures. The Eisner algorithm is a standard method for decoding 1-best trees for graph-based dependency parsing. Based on bottom-up dynamic programming, it stores the 1-best subtree for each span and takes O(N 3 ) time complexity for decoding a sentence of N words. KB EST E ISNER keeps a sorted list of K-best hypotheses for each span. Cube pruning (Huang and Chiang, 2005) is adopted to generate the Kbest list for each larger span from the K-best lists of its sub-spans. After the bottom-up decoding, we merge the fi"
D19-1020,D15-1205,1,0.779495,"Missing"
D19-1020,P19-1024,0,0.154261,"Missing"
D19-1020,W09-2415,0,0.0207045,"Missing"
D19-1020,W05-1506,0,0.102938,"r. We investigate two algorithms to generate high-quality forests by judging “quality” from different perspectives: one focusing on arcs, and the other focusing on trees. E DGEWISE This algorithm focuses on the local relation of each individual edge and uses parser probabilities as confidence scores to assess edge qualities. Starting from the whole parser search space, it keeps all the edges with scores greater than a threshold . The time complexity is O(N 2 ), where N represents the sentence length.1 KB EST E ISNER This algorithm extends the Eisner algorithm (Eisner, 1996) with cube pruning (Huang and Chiang, 2005) for finding K highest-scored tree structures. The Eisner algorithm is a standard method for decoding 1-best trees for graph-based dependency parsing. Based on bottom-up dynamic programming, it stores the 1-best subtree for each span and takes O(N 3 ) time complexity for decoding a sentence of N words. KB EST E ISNER keeps a sorted list of K-best hypotheses for each span. Cube pruning (Huang and Chiang, 2005) is adopted to generate the Kbest list for each larger span from the K-best lists of its sub-spans. After the bottom-up decoding, we merge the final K-bests by combining identical dependen"
D19-1020,D15-1137,0,0.031279,"2019) show that our method outperforms a strong baseline that uses 1-best dependency trees as features, giving the state-of-the-art accuracies in the literature. To our knowledge, we are the first to study dependency forests for medical information extraction, showing their advantages over 1-best tree structures. Our code is available at http://github.com/freesunshine/ dep-forest-re. 2 Related work Syntactic forests There have been previous studies leveraging constituent forests for machine translation (Mi et al., 2008; Ma et al., 2018; Zaremoodi and Haffari, 2018), sentiment analysis (Le and Zuidema, 2015) and text generation (Lu and Ng, 2011). However, the usefulness of dependency forests is relatively rarely studied, with one exception being Tu et al. (2010), who use dependency forests to enhance long-range word-to-word dependencies for statistical machine translation. To our knowledge, we are the first to study the usefulness of dependency forests for relation extraction under a strong neural framework. Graph neural network Graph neural networks (GNNs) have been successful in encoding dependency trees for downstream tasks, such as semantic role labeling (Marcheggiani and Titov, 2017), semant"
D19-1020,I05-1006,0,0.184,"Missing"
D19-1020,Q17-1029,1,0.895317,"Missing"
D19-1020,P15-2047,0,0.103796,"Missing"
D19-1020,D11-1149,0,0.0191059,"ms a strong baseline that uses 1-best dependency trees as features, giving the state-of-the-art accuracies in the literature. To our knowledge, we are the first to study dependency forests for medical information extraction, showing their advantages over 1-best tree structures. Our code is available at http://github.com/freesunshine/ dep-forest-re. 2 Related work Syntactic forests There have been previous studies leveraging constituent forests for machine translation (Mi et al., 2008; Ma et al., 2018; Zaremoodi and Haffari, 2018), sentiment analysis (Le and Zuidema, 2015) and text generation (Lu and Ng, 2011). However, the usefulness of dependency forests is relatively rarely studied, with one exception being Tu et al. (2010), who use dependency forests to enhance long-range word-to-word dependencies for statistical machine translation. To our knowledge, we are the first to study the usefulness of dependency forests for relation extraction under a strong neural framework. Graph neural network Graph neural networks (GNNs) have been successful in encoding dependency trees for downstream tasks, such as semantic role labeling (Marcheggiani and Titov, 2017), semantic parsing (Xu et al., 2018), machine"
D19-1020,P18-1249,0,0.0333378,"Missing"
D19-1020,D17-1159,0,0.500712,"iment analysis (Le and Zuidema, 2015) and text generation (Lu and Ng, 2011). However, the usefulness of dependency forests is relatively rarely studied, with one exception being Tu et al. (2010), who use dependency forests to enhance long-range word-to-word dependencies for statistical machine translation. To our knowledge, we are the first to study the usefulness of dependency forests for relation extraction under a strong neural framework. Graph neural network Graph neural networks (GNNs) have been successful in encoding dependency trees for downstream tasks, such as semantic role labeling (Marcheggiani and Titov, 2017), semantic parsing (Xu et al., 2018), machine translation (Song et al., 2019; Bastings et al., 2017), relation extraction (Song et al., 2018b) and sentence ordering (Yin et al., 2019). In particular, Song et al. (2018b) showed that GNNs are more effective than DAG networks (Peng et al., 2017) for modeling syntactic trees in relation extraction, which cause loss of important structural information. We are the first to exploit GNNs for encoding search spaces in the form of dependency forests. 3 Task Formally, the input to our task is a sentence s = w1 , w2 , . . . , wN , where N is the number of"
D19-1020,J93-2004,0,0.064101,"able 1: Statistics on forests generated with various (upper half) and K (lower half) on the development set. • D EP T REE: Our baseline using 1-best dependency trees, as shown in Section 4. 7.4 • E DGEWISE PS and E DGEWISE: Our models using the forests generated by our E DGEWISE algorithm with or without parser scores. • KB EST E ISNER PS and KB EST E ISNER: Our model using the forests generated by our KB EST E ISNER algorithm with or without parser scores, respectively. 7.3 Settings We take a state-of-the-art deep biaffine parser (Dozat and Manning, 2017), trained on the Penn Treebank (PTB) (Marcus and Marcinkiewicz, 1993) converted to Universal Dependency, to obtain 1-best trees and full search spaces for generating forests. Using standard PTB data split (02–21 for training, 22 for development and 23 for testing), it gives UAS and LAS scores of 95.7 and 94.6, respectively. For the other hyper-parameters, word embeddings are initialized with the 200-dimensional BioASQ vectors5 , pretrained on 10M abstracts of biomedical articles, and are fixed during training. The dimension of hidden vectors in Bi-LSTM is set to 200, and the number of message passing steps T is set to 2 based on Zhang et al. (2018b). We use Ada"
D19-1020,P08-2026,0,0.0861827,"Missing"
D19-1020,W16-3009,0,0.0314234,"Missing"
D19-1020,C10-1123,0,0.0261185,"ature. To our knowledge, we are the first to study dependency forests for medical information extraction, showing their advantages over 1-best tree structures. Our code is available at http://github.com/freesunshine/ dep-forest-re. 2 Related work Syntactic forests There have been previous studies leveraging constituent forests for machine translation (Mi et al., 2008; Ma et al., 2018; Zaremoodi and Haffari, 2018), sentiment analysis (Le and Zuidema, 2015) and text generation (Lu and Ng, 2011). However, the usefulness of dependency forests is relatively rarely studied, with one exception being Tu et al. (2010), who use dependency forests to enhance long-range word-to-word dependencies for statistical machine translation. To our knowledge, we are the first to study the usefulness of dependency forests for relation extraction under a strong neural framework. Graph neural network Graph neural networks (GNNs) have been successful in encoding dependency trees for downstream tasks, such as semantic role labeling (Marcheggiani and Titov, 2017), semantic parsing (Xu et al., 2018), machine translation (Song et al., 2019; Bastings et al., 2017), relation extraction (Song et al., 2018b) and sentence ordering"
D19-1020,P08-1023,0,0.0537829,"t al., 2017) and a recent dataset focused on phenotype-gene relations (PGR) (Sousa et al., 2019) show that our method outperforms a strong baseline that uses 1-best dependency trees as features, giving the state-of-the-art accuracies in the literature. To our knowledge, we are the first to study dependency forests for medical information extraction, showing their advantages over 1-best tree structures. Our code is available at http://github.com/freesunshine/ dep-forest-re. 2 Related work Syntactic forests There have been previous studies leveraging constituent forests for machine translation (Mi et al., 2008; Ma et al., 2018; Zaremoodi and Haffari, 2018), sentiment analysis (Le and Zuidema, 2015) and text generation (Lu and Ng, 2011). However, the usefulness of dependency forests is relatively rarely studied, with one exception being Tu et al. (2010), who use dependency forests to enhance long-range word-to-word dependencies for statistical machine translation. To our knowledge, we are the first to study the usefulness of dependency forests for relation extraction under a strong neural framework. Graph neural network Graph neural networks (GNNs) have been successful in encoding dependency trees f"
D19-1020,N18-1080,0,0.246962,"present an edge for simplicity, and p✏ is the parser probability for edge ✏. The edge probabilities are not adjusted during end-task training. 6 Training Relation loss Given a set of training instances, each containing a sentence s with two target mentions ⇠ and ⇣, and a dependency structure D (tree or forest), we train our models with a crossentropy loss between the gold-standard relations r and model distribution: lR = log p(r|s, ⇠, ⇣, D; ✓), (13) where ✓ represents the model parameters. Using additional NER loss For training on BioCreative VI CPR, we follow previous work (Liu et al., 2017; Verga et al., 2018) to take NER loss as additional supervision, though the mention boundaries are known during testing. lN ER = N 1 X log p(tn |s, D; ✓), N (14) n=1 where tn is the gold NE tag of wn with the “BIO” scheme. Both losses are conditionally independent given the deep features produced by our Experiments We conduct experiments on two medical benchmarks to test the usefulness of dependency forest. 7.1 Data BioCreative VI CPR (Krallinger et al., 2017) This task2 focuses on the relations between chemical compounds (such as drugs) and proteins (such as genes). The full corpus contains 1020, 612 and 800 ext"
D19-1020,P16-1105,0,0.0839479,"Missing"
D19-1020,Q17-1008,0,0.357248,"on. To our knowledge, we are the first to study the usefulness of dependency forests for relation extraction under a strong neural framework. Graph neural network Graph neural networks (GNNs) have been successful in encoding dependency trees for downstream tasks, such as semantic role labeling (Marcheggiani and Titov, 2017), semantic parsing (Xu et al., 2018), machine translation (Song et al., 2019; Bastings et al., 2017), relation extraction (Song et al., 2018b) and sentence ordering (Yin et al., 2019). In particular, Song et al. (2018b) showed that GNNs are more effective than DAG networks (Peng et al., 2017) for modeling syntactic trees in relation extraction, which cause loss of important structural information. We are the first to exploit GNNs for encoding search spaces in the form of dependency forests. 3 Task Formally, the input to our task is a sentence s = w1 , w2 , . . . , wN , where N is the number of words in the sentence and wi represents the i-th input word. s is annotated with boundary information (⇠1 : ⇠2 and ⇣1 : ⇣2 ) of target entity mentions (⇠ and ⇣). We focus on the classic binary relation extraction setting (Quirk and Poon, 2017), where the number of associated mentions is two."
D19-1020,D15-1062,0,0.231787,"Missing"
D19-1020,E17-1110,0,0.0869635,"wed that GNNs are more effective than DAG networks (Peng et al., 2017) for modeling syntactic trees in relation extraction, which cause loss of important structural information. We are the first to exploit GNNs for encoding search spaces in the form of dependency forests. 3 Task Formally, the input to our task is a sentence s = w1 , w2 , . . . , wN , where N is the number of words in the sentence and wi represents the i-th input word. s is annotated with boundary information (⇠1 : ⇠2 and ⇣1 : ⇣2 ) of target entity mentions (⇠ and ⇣). We focus on the classic binary relation extraction setting (Quirk and Poon, 2017), where the number of associated mentions is two. The output is a relation from a predefined relation set R = (r1 , . . . , rM , None), where “None” means that no relation holds for the entities. Two steps are taken for predicting the correct relation given an input sentence. First, a dependency parser is used to label the syntactic structure of the input. Here our baseline system takes the standard approach, using the 1-best parser output tree DT as features. In contrast, our proposed model uses the most confident parser forest DF as features. Given DT or DF , the second step is to encode bot"
D19-1020,W08-0504,0,0.0520198,"Missing"
D19-1020,C10-2133,0,0.0320316,"from parsing noise. Results on two biomedical benchmarks show that our method outperforms the standard tree-based methods, giving the state-of-the-art results in the literature. 1 Introduction The sheer amount of medical articles and their rapid growth prevent researchers from receiving comprehensive literature knowledge by direct reading. This can hamper both medical research and clinical diagnosis. NLP techniques have been used for automating the knowledge extraction process from the medical literature (Friedman et al., 2001; Yu and Agichtein, 2003; Hirschman et al., 2005; Xu et al., 2010; Sondhi et al., 2010; Abacha and Zweigenbaum, 2011). Along this line of work, a long-standing task is relation extraction, which mines factual knowledge from free text by labeling relations between entity mentions. As shown in Figure 1, the sub-clause “previously observed cytochrome P450 3A4 ( CYP3A4 ) interaction of the dual orexin receptor antagonist almorexant” contains two entities, namely “orexin receptor” and “almorexant”. There is an “adversary” relation between these two entities, denoted as“CPR:6”. ⇤ Yue Zhang is the corresponding author nmod comp comp amod comp ... observed ... interaction of orexin rec"
D19-1020,Q19-1002,1,0.928194,"e usefulness of dependency forests is relatively rarely studied, with one exception being Tu et al. (2010), who use dependency forests to enhance long-range word-to-word dependencies for statistical machine translation. To our knowledge, we are the first to study the usefulness of dependency forests for relation extraction under a strong neural framework. Graph neural network Graph neural networks (GNNs) have been successful in encoding dependency trees for downstream tasks, such as semantic role labeling (Marcheggiani and Titov, 2017), semantic parsing (Xu et al., 2018), machine translation (Song et al., 2019; Bastings et al., 2017), relation extraction (Song et al., 2018b) and sentence ordering (Yin et al., 2019). In particular, Song et al. (2018b) showed that GNNs are more effective than DAG networks (Peng et al., 2017) for modeling syntactic trees in relation extraction, which cause loss of important structural information. We are the first to exploit GNNs for encoding search spaces in the form of dependency forests. 3 Task Formally, the input to our task is a sentence s = w1 , w2 , . . . , wN , where N is the number of words in the sentence and wi represents the i-th input word. s is annotated"
D19-1020,P18-1150,1,0.94662,"with one exception being Tu et al. (2010), who use dependency forests to enhance long-range word-to-word dependencies for statistical machine translation. To our knowledge, we are the first to study the usefulness of dependency forests for relation extraction under a strong neural framework. Graph neural network Graph neural networks (GNNs) have been successful in encoding dependency trees for downstream tasks, such as semantic role labeling (Marcheggiani and Titov, 2017), semantic parsing (Xu et al., 2018), machine translation (Song et al., 2019; Bastings et al., 2017), relation extraction (Song et al., 2018b) and sentence ordering (Yin et al., 2019). In particular, Song et al. (2018b) showed that GNNs are more effective than DAG networks (Peng et al., 2017) for modeling syntactic trees in relation extraction, which cause loss of important structural information. We are the first to exploit GNNs for encoding search spaces in the form of dependency forests. 3 Task Formally, the input to our task is a sentence s = w1 , w2 , . . . , wN , where N is the number of words in the sentence and wi represents the i-th input word. s is annotated with boundary information (⇠1 : ⇠2 and ⇣1 : ⇣2 ) of target enti"
D19-1020,D18-1110,1,0.810915,"neration (Lu and Ng, 2011). However, the usefulness of dependency forests is relatively rarely studied, with one exception being Tu et al. (2010), who use dependency forests to enhance long-range word-to-word dependencies for statistical machine translation. To our knowledge, we are the first to study the usefulness of dependency forests for relation extraction under a strong neural framework. Graph neural network Graph neural networks (GNNs) have been successful in encoding dependency trees for downstream tasks, such as semantic role labeling (Marcheggiani and Titov, 2017), semantic parsing (Xu et al., 2018), machine translation (Song et al., 2019; Bastings et al., 2017), relation extraction (Song et al., 2018b) and sentence ordering (Yin et al., 2019). In particular, Song et al. (2018b) showed that GNNs are more effective than DAG networks (Peng et al., 2017) for modeling syntactic trees in relation extraction, which cause loss of important structural information. We are the first to exploit GNNs for encoding search spaces in the form of dependency forests. 3 Task Formally, the input to our task is a sentence s = w1 , w2 , . . . , wN , where N is the number of words in the sentence and wi repres"
D19-1020,D15-1206,0,0.201939,"Missing"
D19-1020,C18-1120,0,0.0200441,"focused on phenotype-gene relations (PGR) (Sousa et al., 2019) show that our method outperforms a strong baseline that uses 1-best dependency trees as features, giving the state-of-the-art accuracies in the literature. To our knowledge, we are the first to study dependency forests for medical information extraction, showing their advantages over 1-best tree structures. Our code is available at http://github.com/freesunshine/ dep-forest-re. 2 Related work Syntactic forests There have been previous studies leveraging constituent forests for machine translation (Mi et al., 2008; Ma et al., 2018; Zaremoodi and Haffari, 2018), sentiment analysis (Le and Zuidema, 2015) and text generation (Lu and Ng, 2011). However, the usefulness of dependency forests is relatively rarely studied, with one exception being Tu et al. (2010), who use dependency forests to enhance long-range word-to-word dependencies for statistical machine translation. To our knowledge, we are the first to study the usefulness of dependency forests for relation extraction under a strong neural framework. Graph neural network Graph neural networks (GNNs) have been successful in encoding dependency trees for downstream tasks, such as semantic role labe"
D19-1020,D18-1246,1,0.942065,"with one exception being Tu et al. (2010), who use dependency forests to enhance long-range word-to-word dependencies for statistical machine translation. To our knowledge, we are the first to study the usefulness of dependency forests for relation extraction under a strong neural framework. Graph neural network Graph neural networks (GNNs) have been successful in encoding dependency trees for downstream tasks, such as semantic role labeling (Marcheggiani and Titov, 2017), semantic parsing (Xu et al., 2018), machine translation (Song et al., 2019; Bastings et al., 2017), relation extraction (Song et al., 2018b) and sentence ordering (Yin et al., 2019). In particular, Song et al. (2018b) showed that GNNs are more effective than DAG networks (Peng et al., 2017) for modeling syntactic trees in relation extraction, which cause loss of important structural information. We are the first to exploit GNNs for encoding search spaces in the form of dependency forests. 3 Task Formally, the input to our task is a sentence s = w1 , w2 , . . . , wN , where N is the number of words in the sentence and wi represents the i-th input word. s is annotated with boundary information (⇠1 : ⇠2 and ⇣1 : ⇣2 ) of target enti"
D19-1020,P18-1030,1,0.867698,"to label the syntactic structure of the input. Here our baseline system takes the standard approach, using the 1-best parser output tree DT as features. In contrast, our proposed model uses the most confident parser forest DF as features. Given DT or DF , the second step is to encode both s and DT /DF using a neural network, before making a prediction. We make use of the same graph neural network encoder structure to represent dependency syntax information for both the baseline and our model. In particular, a graph recurrent neural network architecture (Beck et al., 2018; Song et al., 2018a; Zhang et al., 2018a) is used, which has been shown effective in encoding graph structures (Song et al., 2019), giving competitive results with alternative graph networks such as graph convolutional neural networks (Marcheggiani and Titov, 2017; Bastings et al., 2017). 4 Baseline: D EP T REE As shown in Figure 2, our baseline model stacks a bidirectional LSTM layer to encode an input sentence w1 , . . . , wN with a graph recurrent network 209 In order to capture non-local interactions between words, the GRN layer adopts a message passing framework that performs iterative information exchange between directly con"
D19-1020,N19-1152,0,0.0439742,"Missing"
D19-1020,D18-1244,0,0.376206,"to label the syntactic structure of the input. Here our baseline system takes the standard approach, using the 1-best parser output tree DT as features. In contrast, our proposed model uses the most confident parser forest DF as features. Given DT or DF , the second step is to encode both s and DT /DF using a neural network, before making a prediction. We make use of the same graph neural network encoder structure to represent dependency syntax information for both the baseline and our model. In particular, a graph recurrent neural network architecture (Beck et al., 2018; Song et al., 2018a; Zhang et al., 2018a) is used, which has been shown effective in encoding graph structures (Song et al., 2019), giving competitive results with alternative graph networks such as graph convolutional neural networks (Marcheggiani and Titov, 2017; Bastings et al., 2017). 4 Baseline: D EP T REE As shown in Figure 2, our baseline model stacks a bidirectional LSTM layer to encode an input sentence w1 , . . . , wN with a graph recurrent network 209 In order to capture non-local interactions between words, the GRN layer adopts a message passing framework that performs iterative information exchange between directly con"
E17-1035,D15-1198,0,0.418719,", they have not fulfilled their promise on the AMR parsing task due to the data sparsity issue. In this paper, we describe a sequence-to-sequence model for AMR parsing and present different ways to tackle the data sparsity problem. We show that our methods achieve significant improvement over a baseline neural attention model and our results are also competitive against state-of-the-art systems that do not use extra linguistic resources. 1 ARG0 ARG1 person ARG2 name name op1 genius “Ryan” Figure 1: An example of AMR graph representing the meaning of: “Ryan’s description of himself: a genius.” Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015). Most of these parsers have used external resources such as dependency parses, semantic lexicons, etc., to tackle the sparsity issue. Recently, Sutskever et al. (2014) introduced a neural network model for solving the general sequence-to-sequence problem, and Bahdanau et al. (2014) proposed a related model with an attention mechanism that is capable of handling long sequences. Both models achieve state-of-the-art results on large scale machine translation tasks. However, sequence-to-sequence models mostly work well for large scale parallel data, usually"
E17-1035,W13-2322,0,0.101641,"usually involving millions of sentence pairs. Vinyals et al. (2015) present a method which linearizes parse trees into a sequence structure and therefore a sequence-to-sequence model can be applied to the constituent parsing task. Competitive results have been achieved with an attention model on the Penn Treebank dataset, with only 40K annotated sentences. AMR parsing is a much harder task in that the target vocabulary size is much larger, while the size of the dataset is much smaller. While for constituent parsing we only need to predict nonIntroduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism where the meaning of a sentence is encoded as a rooted, directed graph. Figure 1 shows an example of an AMR in which the nodes represent the AMR concepts and the edges represent the relations between the concepts they connect. AMR concepts consist of predicate senses, named entity annotations, and in some cases, simply lemmas of English words. AMR relations consist of core semantic roles drawn from the Propbank (Palmer et al., 2005) as well as very fine-grained semantic relations defined specifically for AMR. These properties render the AMR representation useful in app"
E17-1035,S16-1176,0,0.542997,"4; Wang et al., 2015b; *Both authors contribute equally. 366 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 366–375, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics two RNNs: a forward RNN and a backward RNN. The forward RNN can be seen as a recurrent function defined as follows: terminal labels and the output vocabulary is limited to 128 symbols, AMR parsing has both concepts and relation labels, and the target vocabulary size consists of tens of thousands of symbols. Barzdins and Gosko (2016) applied a similar approach where AMR graphs are linearized using depth-first search and both concepts and relations are treated as tokens (see Figure 3). Due to the data sparsity issue, their AMR parsing results are significantly lower than state-of-the-art models when using the neural attention model. In this paper, we present a method which linearizes AMR graphs in a way that captures the interaction of concepts and relations. To overcome the data sparsity issue for the target vocabulary, we propose a categorization strategy which first maps low frequency concepts and entity subgraphs to a"
E17-1035,P06-1055,0,0.0217691,"ere the original graph structure can be reconstructed with the linearization result. Even though we call it a sequence, its core idea is actually generating a graph structure from top-down. In practice, they found that using the attention model is more data efficient and works well on the parsing task. They also reversed the input sentence and normalized the part-of-speech tags. After decoding, the output parse tree is recovered from the output sequence of the decoder in a postprocessing procedure. Overall, the sequence-tosequence model is able to match the performance of the Berkeley Parser (Petrov et al., 2006). 3 AMR Linearization Barzdins and Gosko (2016) present a similar linearization procedure where the depth-first traversal result of an AMR graph is used as the AMR sequence (see Figure 3). The bracketing structure of AMR is hard to maintain because the prediction of relation (with left parenthesis) and the prediction of an isolated right parenthesis are not correlated. As a result, the output AMR sequences usually have parentheses that do not match. We present a linearization strategy which captures the bracketing structure of AMR and the connection between relations and concepts. Figure 3 sho"
E17-1035,P13-2131,0,0.0620234,"e input sequence and AMR concepts/relations. Second, as argued by Liu et al. (2016), the sequenceto-sequence model tries to calculate the attention vector and predict the current output label simultaneously. This makes it impossible for the learned 5 Experiments We evaluate our system on the released dataset (LDC2015E86) for SemEval 2016 task 8 on meaning representation parsing (May, 2016). The dataset contains 16,833 training, 1,368 development and 1,371 test sentences which mainly cover domains like newswire, discussion forum, etc. All parsing results are measured by Smatch (version 2.0.2) (Cai and Knight, 2013). 5.1 Experiment Settings We first preprocess the input sentences with tokenization and lemmatization. Then we extract 370 the named entities using the Illinois Named Entity Tagger (Ratinov and Roth, 2009). For training all the neural AMR parsing systems, we use 256 for both hidden layer size and word embedding size. Stochastic gradient descent is used to optimize the cross-entropy loss function and we set the drop out rate to be 0.5. We train our model for 150 epochs with initial learning rate of 0.5 and learning rate decay factor 0.95 if the model doesn’t improve for the 3 last epochs. 5.2 6"
E17-1035,D14-1048,0,0.40301,"Missing"
E17-1035,N16-1101,0,0.0164067,")? is the character-level neural AMR parser. F 0.52 0.43 Table 5 gives the comparison of our system to some of the teams participating in SemEval16 Task 8. Since a large portion of the teams extend on the state-of-the-art system CAMR (Wang et al., 2015b; Wang et al., 2015a; Wang et al., 2016), here we just pick typical teams that represent different approaches. We can see from the table that our system fails to outperform the stateTable 3: Supervised attention impact on development set Because we have relations in the AMR graph, the alignment problem here is different from the 372 parameters. Firat et al. (2016) builds a multilingual neural system where the attention mechanism can be shared between different language pairs. Our work could be seen as parallel efforts to handle the sparsity problem since we focus on the input/output categorization and external alignment, which are both handy for low-resource languages. In this paper, we haven’t used any syntactic parser. However, as shown in previous works (Flanigan et al., 2014; Wang et al., 2015b; Artzi et al., 2015; Pust et al., 2015), using dependency features helps improve the parsing performance significantly because of the linguistic similarity"
E17-1035,D15-1136,0,0.64323,"illed their promise on the AMR parsing task due to the data sparsity issue. In this paper, we describe a sequence-to-sequence model for AMR parsing and present different ways to tackle the data sparsity problem. We show that our methods achieve significant improvement over a baseline neural attention model and our results are also competitive against state-of-the-art systems that do not use extra linguistic resources. 1 ARG0 ARG1 person ARG2 name name op1 genius “Ryan” Figure 1: An example of AMR graph representing the meaning of: “Ryan’s description of himself: a genius.” Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015). Most of these parsers have used external resources such as dependency parses, semantic lexicons, etc., to tackle the sparsity issue. Recently, Sutskever et al. (2014) introduced a neural network model for solving the general sequence-to-sequence problem, and Bahdanau et al. (2014) proposed a related model with an attention mechanism that is capable of handling long sequences. Both models achieve state-of-the-art results on large scale machine translation tasks. However, sequence-to-sequence models mostly work well for large scale parallel data, usually involving millions"
E17-1035,P14-1134,0,0.742379,"notations, and in some cases, simply lemmas of English words. AMR relations consist of core semantic roles drawn from the Propbank (Palmer et al., 2005) as well as very fine-grained semantic relations defined specifically for AMR. These properties render the AMR representation useful in applications like question answering and semanticsbased machine translation. The task of AMR graph parsing is to map natural language strings to AMR semantic graphs. Recently, a sizable new corpus of English/AMR pairs (LDC2015E86) has been released. Different parsers have been developed to tackle this problem (Flanigan et al., 2014; Wang et al., 2015b; *Both authors contribute equally. 366 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 366–375, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics two RNNs: a forward RNN and a backward RNN. The forward RNN can be seen as a recurrent function defined as follows: terminal labels and the output vocabulary is limited to 128 symbols, AMR parsing has both concepts and relation labels, and the target vocabulary size consists of tens of thousands of symbols. B"
E17-1035,W09-1119,0,0.039536,"sly. This makes it impossible for the learned 5 Experiments We evaluate our system on the released dataset (LDC2015E86) for SemEval 2016 task 8 on meaning representation parsing (May, 2016). The dataset contains 16,833 training, 1,368 development and 1,371 test sentences which mainly cover domains like newswire, discussion forum, etc. All parsing results are measured by Smatch (version 2.0.2) (Cai and Knight, 2013). 5.1 Experiment Settings We first preprocess the input sentences with tokenization and lemmatization. Then we extract 370 the named entities using the Illinois Named Entity Tagger (Ratinov and Roth, 2009). For training all the neural AMR parsing systems, we use 256 for both hidden layer size and word embedding size. Stochastic gradient descent is used to optimize the cross-entropy loss function and we set the drop out rate to be 0.5. We train our model for 150 epochs with initial learning rate of 0.5 and learning rate decay factor 0.95 if the model doesn’t improve for the 3 last epochs. 5.2 60 55 50 Smatch 45 35 30 25 20 Baseline Model We first inspect the influence of utilizing categorization on the input and output sequence. Table 1 shows the Smatch evaluation score on development set. P 0.4"
E17-1035,P82-1020,0,0.847308,"Missing"
E17-1035,S16-1166,0,0.128355,"et al., 2014; Vinyals et al., 2015). However, we only have 16k sentences in the AMR training data and our output vocabulary size is quite large, which makes it hard for the model to learn a useful alignment between the input sequence and AMR concepts/relations. Second, as argued by Liu et al. (2016), the sequenceto-sequence model tries to calculate the attention vector and predict the current output label simultaneously. This makes it impossible for the learned 5 Experiments We evaluate our system on the released dataset (LDC2015E86) for SemEval 2016 task 8 on meaning representation parsing (May, 2016). The dataset contains 16,833 training, 1,368 development and 1,371 test sentences which mainly cover domains like newswire, discussion forum, etc. All parsing results are measured by Smatch (version 2.0.2) (Cai and Knight, 2013). 5.1 Experiment Settings We first preprocess the input sentences with tokenization and lemmatization. Then we extract 370 the named entities using the Illinois Named Entity Tagger (Ratinov and Roth, 2009). For training all the neural AMR parsing systems, we use 256 for both hidden layer size and word embedding size. Stochastic gradient descent is used to optimize the"
E17-1035,P15-2141,1,0.951473,"cases, simply lemmas of English words. AMR relations consist of core semantic roles drawn from the Propbank (Palmer et al., 2005) as well as very fine-grained semantic relations defined specifically for AMR. These properties render the AMR representation useful in applications like question answering and semanticsbased machine translation. The task of AMR graph parsing is to map natural language strings to AMR semantic graphs. Recently, a sizable new corpus of English/AMR pairs (LDC2015E86) has been released. Different parsers have been developed to tackle this problem (Flanigan et al., 2014; Wang et al., 2015b; *Both authors contribute equally. 366 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 366–375, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics two RNNs: a forward RNN and a backward RNN. The forward RNN can be seen as a recurrent function defined as follows: terminal labels and the output vocabulary is limited to 128 symbols, AMR parsing has both concepts and relation labels, and the target vocabulary size consists of tens of thousands of symbols. Barzdins and Gosko ("
E17-1035,J05-1004,1,0.206863,"e dataset is much smaller. While for constituent parsing we only need to predict nonIntroduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism where the meaning of a sentence is encoded as a rooted, directed graph. Figure 1 shows an example of an AMR in which the nodes represent the AMR concepts and the edges represent the relations between the concepts they connect. AMR concepts consist of predicate senses, named entity annotations, and in some cases, simply lemmas of English words. AMR relations consist of core semantic roles drawn from the Propbank (Palmer et al., 2005) as well as very fine-grained semantic relations defined specifically for AMR. These properties render the AMR representation useful in applications like question answering and semanticsbased machine translation. The task of AMR graph parsing is to map natural language strings to AMR semantic graphs. Recently, a sizable new corpus of English/AMR pairs (LDC2015E86) has been released. Different parsers have been developed to tackle this problem (Flanigan et al., 2014; Wang et al., 2015b; *Both authors contribute equally. 366 Proceedings of the 15th Conference of the European Chapter of the Assoc"
E17-1035,S16-1183,1,0.893498,"Missing"
E17-1035,N15-1040,1,0.873586,"cases, simply lemmas of English words. AMR relations consist of core semantic roles drawn from the Propbank (Palmer et al., 2005) as well as very fine-grained semantic relations defined specifically for AMR. These properties render the AMR representation useful in applications like question answering and semanticsbased machine translation. The task of AMR graph parsing is to map natural language strings to AMR semantic graphs. Recently, a sizable new corpus of English/AMR pairs (LDC2015E86) has been released. Different parsers have been developed to tackle this problem (Flanigan et al., 2014; Wang et al., 2015b; *Both authors contribute equally. 366 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 366–375, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics two RNNs: a forward RNN and a backward RNN. The forward RNN can be seen as a recurrent function defined as follows: terminal labels and the output vocabulary is limited to 128 symbols, AMR parsing has both concepts and relation labels, and the target vocabulary size consists of tens of thousands of symbols. Barzdins and Gosko ("
E17-1035,K15-1004,1,0.929731,"on the AMR parsing task due to the data sparsity issue. In this paper, we describe a sequence-to-sequence model for AMR parsing and present different ways to tackle the data sparsity problem. We show that our methods achieve significant improvement over a baseline neural attention model and our results are also competitive against state-of-the-art systems that do not use extra linguistic resources. 1 ARG0 ARG1 person ARG2 name name op1 genius “Ryan” Figure 1: An example of AMR graph representing the meaning of: “Ryan’s description of himself: a genius.” Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015). Most of these parsers have used external resources such as dependency parses, semantic lexicons, etc., to tackle the sparsity issue. Recently, Sutskever et al. (2014) introduced a neural network model for solving the general sequence-to-sequence problem, and Bahdanau et al. (2014) proposed a related model with an attention mechanism that is capable of handling long sequences. Both models achieve state-of-the-art results on large scale machine translation tasks. However, sequence-to-sequence models mostly work well for large scale parallel data, usually involving millions of sentence pairs. V"
E17-1035,D16-1163,0,0.0293668,"Missing"
E17-1035,S16-1181,1,\N,Missing
J02-3001,P98-1013,0,0.372385,"Missing"
J02-3001,A00-2031,0,0.0493528,"iven approach to information extraction, Riloff (1993) builds a dictionary of patterns for filling slots in a specific domain such as terrorist attacks, and Riloff and Schmelzenbach (1998) extend this technique to derive automatically entire “case frames” for words in the domain. These last systems make use of a limited amount of hand labor to accept or reject automatically generated hypotheses. They show promise for a more sophisticated approach to generalizing beyond the relatively small number of frames considered in the tasks. More recently, a domain-independent system has been trained by Blaheta and Charniak (2000) on the function tags, such as Manner and Temporal, included in the Penn Treebank corpus. Some of these tags correspond to FrameNet semantic roles, but the Treebank tags do not include all the arguments of most predicates. In this article, we aim to develop a statistical system for automatically learning to identify all semantic roles for a wide variety of predicates in unrestricted text. 251 Computational Linguistics Volume 28, Number 3 4. Probability Estimation for Roles In this section we describe the first, basic version of our statistically trained system for automatically identifying fra"
J02-3001,W98-1505,0,0.0480626,"nique is based on the expectation that words with similar semantics will tend to co-occur with the same other sets of words. For example, nouns describing foods will tend to occur as direct objects of verbs such as eat devour, and savor. The clustering algorithm attempts to find such patterns of co-occurrence from the counts of grammatical relations between pairs of specific words in the corpus, without the use of any external knowledge or semantic representation. We extracted verb–direct object relations from an automatically parsed version of the British National Corpus, using the parser of Carroll and Rooth (1998).4 Clustering 4 We are indebted to Mats Rooth and Sabine Schulte im Walde for providing us with the parsed corpus. 267 Computational Linguistics Volume 28, Number 3 was performed using the probabilistic model of co-occurrence described in detail by Hofmann and Puzicha (1998). (For other natural language processing [NLP] applications of the probabilistic clustering algorithm, see, e.g., Rooth [1995], Rooth et al. [1999]; for application to language modeling, see Gildea and Hofmann [1999]. According to this model, the two observed variables, in this case the verb and the head noun of its object,"
J02-3001,P97-1003,0,0.094406,"Missing"
J02-3001,P99-1001,0,0.188406,"e. For example, Lapata and Brew (1999) and others have shown that the different syntactic subcategorization frames of a verb such as serve can be used to help disambiguate a particular instance of the word. Adding semantic role subcategorization information to this syntactic information could extend this idea to 246 Gildea and Jurafsky Automatic Labeling of Semantic Roles use richer semantic knowledge. Semantic roles could also act as an important intermediate representation in statistical machine translation or automatic text summarization and in the emerging field of text data mining (TDM) (Hearst 1999). Finally, incorporating semantic roles into probabilistic models of language may eventually yield more accurate parsers and better language models for speech recognition. This article describes an algorithm for identifying the semantic roles filled by constituents in a sentence. We apply statistical techniques that have been successful for the related problems of syntactic parsing, part-of-speech tagging, and word sense disambiguation, including probabilistic parsing and statistical classification. Our statistical algorithms are trained on a hand-labeled data set: the FrameNet database (Baker"
J02-3001,W00-2021,0,0.0773412,"eneralizations relevant to argument linking can be made. Our results for unseen predicates within the same frame are encouraging, indicating that the predicates are semantically similar in ways that result in similar argument structure, as the semantically based theories of linking advocated by Levin (1993) and Levin and Rappaport Hovav (1996) would predict. We hope that corpus-based systems such as ours can provide a way of testing and elaborating such theories in the future. We believe that some level of skeletal representation of the relevant aspects of a word’s meaning, along the lines of Kipper et al. (2000) and of the frame hierarchy being developed by the FrameNet project, could be used in the future to help a statistical system generalize from similar words for which training data are available. 10. Conclusion Our system is able to label semantic roles automatically with fairly high accuracy, indicating promise for applications in various natural language tasks. Semantic roles do not seem to be simple functions of a sentence’s syntactic tree structure, and lexical statistics were found to be extremely valuable, as has been the case in other natural language processing applications. Although le"
J02-3001,W99-0632,0,0.07496,"me. Such common frames might allow a question-answering system to take a question like (5) and discover that (6) is relevant in constructing an answer to the question: (5) Which party sent absentee ballots to voters? (6) Both Democratic and Republican voters received absentee ballots from their party. This shallow semantic level of interpretation has additional uses outside of generalizing information extraction, question answering, and semantic dialogue systems. One such application is in word sense disambiguation, where the roles associated with a word can be cues to its sense. For example, Lapata and Brew (1999) and others have shown that the different syntactic subcategorization frames of a verb such as serve can be used to help disambiguate a particular instance of the word. Adding semantic role subcategorization information to this syntactic information could extend this idea to 246 Gildea and Jurafsky Automatic Labeling of Semantic Roles use richer semantic knowledge. Semantic roles could also act as an important intermediate representation in statistical machine translation or automatic text summarization and in the emerging field of text data mining (TDM) (Hearst 1999). Finally, incorporating s"
J02-3001,H94-1020,0,0.0601571,"f the string, the part of speech of the target word and, as the last element, the phrase type or syntactic category of the sentence constituent marked as a frame element. After some experimentation, we settled on a version of the path feature that collapses the various part-of-speech tags for verbs, including past-tense verb (VBD), third-person singular present-tense verb (VBZ), other present-tense verb (VBP), and past participle (VBN), into a single verb tag denoted “VB.” Our path feature is dependent on the syntactic representation used, which in our case is the Treebank-2 annotation style (Marcus et al. 1994), as our parser is trained on this later version of the Treebank data. Figure 4 shows the annotation for the sentence “They expect him to cut costs throughout the organization,” which exhibits 254 Gildea and Jurafsky Automatic Labeling of Semantic Roles S VP NP PRP NP VB DT He ate some NN pancakes Figure 3 In this example, the path from the target word ate to the frame element He can be represented as VB↑VP↑S↓NP, with ↑ indicating upward movement in the parse tree and ↓ downward movement. The NP corresponding to He is found as described in Section 4.1.1. Figure 4 Treebank annotation of raising"
J02-3001,J93-2004,0,0.0651427,"Missing"
J02-3001,A00-2034,0,0.0507751,"matic clustering described above can be seen as an imperfect method of deriving semantic classes from the vocabulary, and we might expect a hand-developed set of classes to do better. We tested this hypothesis using WordNet (Fellbaum 1998), a freely available semantic hierarchy. The basic technique, when presented with a head word for which no training examples had been seen, was to ascend the type hierarchy until reaching a level for which training data are available. To do this, counts of training data were percolated up the semantic hierarchy in a technique similar to that of, for example, McCarthy (2000). For each training example, the count # (r, s, pt, t) was incremented in a table indexed by the semantic role r, WordNet sense s, phrase type pt, and target word t, for each WordNet sense s above the head word h in the hypernym hierarchy. In fact, the WordNet hierarchy is not a tree, but rather includes multiple inheritance. For example, person has as hypernyms both life form and causal agent. In such cases, we simply took the first hypernym listed, effectively converting the structure into a tree. A further complication is that several WordNet senses are possible for a given head word. We si"
J02-3001,A00-2030,0,0.176136,"stem. In this section, we explore 276 Gildea and Jurafsky Automatic Labeling of Semantic Roles the interaction between semantic roles and syntactic parsing by integrating the parser with the semantic-role probability model. This allows the semantic-role assignment to affect the syntactic attachment decisions made by the parser, with the hope of improving the accuracy of the complete system. Although most statistical parsing work measures performance in terms of syntactic trees without semantic information, an assignment of role fillers has been incorporated into a statistical parsing model by Miller et al. (2000) for the domain-specific templates of the Message Understanding Conference (Defense Advanced Research Projects Agency 1998) task. A key finding of Miller et al.’s work was that a system developed by annotating role fillers in text and training a statistical system performed at the same level as one based on writing a large system of rules, which requires much more highly skilled labor to design. 8.1 Incorporating Roles into the Parsing Model We use as the baseline of all our parsing experiments the model described in Collins (1999). The algorithm is a form of chart parsing, which uses dynamic"
J02-3001,P93-1024,0,0.118089,"Missing"
J02-3001,P99-1014,0,0.205967,"Missing"
J02-3001,C69-0201,0,0.146482,"the from airport, to airport, or depart time discussed above, or verb-specific roles such as eater and eaten for the verb eat. The opposite end of the spectrum consists of theories with only two “proto-roles” or “macroroles”: Proto-Agent and Proto-Patient (Van Valin 1993; Dowty 1991). In between lie many theories with approximately 10 roles, such as Fillmore’s (1971) list of nine: Agent, Experiencer, Instrument, Object, Source, Goal, Location, Time, and Path.1 1 There are scores of other theories with slightly different sets of roles, including those of Fillmore (1968), Jackendoff (1972), and Schank (1972); see Somers (1987) for an excellent summary. 247 Computational Linguistics Domain: Volume 28, Number 3 Communication Frame: Frame: Conversation Frame Elements: Protagonist−1 Protagonist−2 Protagonists Topic Medium debate−v tiff−n banter−v dispute−n converse−v gossip−v discussion−n Cognition Questioning Frame Elements: Frame: argue−v Domain: Speaker Addressee Message Topic Medium Statement Frame Elements: Speaker Addressee Message Topic Medium Frame: Judgment Frame Elements: blame−v admire−v admiration−n Judge Evaluee Reason Role Frame: Categorization Frame Elements: Cognizer Item Category Cri"
J02-3001,A00-1010,0,0.0747477,"ersity of Colorado, Boulder, CO 80309. E-mail: jurafsky@colorado.edu c 2002 Association for Computational Linguistics  Computational Linguistics Volume 28, Number 3 far away as it once was, thanks to the development of large semantic databases such as WordNet (Fellbaum 1998) and progress in domain-independent machine learning algorithms. Current information extraction and dialogue understanding systems, however, are still based on domain-specific frame-and-slot templates. Systems for booking airplane information use domain-specific frames with slots like orig city, dest city, or depart time (Stallard 2000). Systems for studying mergers and acquisitions use slots like products, relationship, joint venture company, and amount (Hobbs et al. 1997). For natural language understanding tasks to proceed beyond these specific domains, we need semantic frames and semantic understanding systems that do not require a new set of slots for each new application domain. In this article we describe a shallow semantic interpreter based on semantic roles that are less domain specific than to airport or joint venture company. These roles are defined at the level of semantic frames of the type introduced by Fillmor"
J02-3001,H89-1033,0,0.0892135,"to appear in tasks in which understanding and semantics play a greater role. For example, there has been widespread commercial deployment of simple speech-based natural language understanding systems that answer questions about flight arrival times, give directions, report on bank balances, or perform simple financial transactions. More sophisticated research systems generate concise summaries of news articles, answer fact-based questions, and recognize complex semantic and dialogue structure. But the challenges that lie ahead are still similar to the challenge that the field has faced since Winograd (1972): moving away from carefully hand-crafted, domaindependent systems toward robustness and domain independence. This goal is not as ∗ Currently at Institute for Research in Cognitive Science, University of Pennsylvania, 3401 Walnut Street, Suite 400A, Philadelphia, PA 19104. E-mail: dgildea@cis.upenn.edu † Departments of Linguistics and Computer Science, University of Colorado, Boulder, CO 80309. E-mail: jurafsky@colorado.edu c 2002 Association for Computational Linguistics  Computational Linguistics Volume 28, Number 3 far away as it once was, thanks to the development of large semantic databa"
J02-3001,W98-1106,0,\N,Missing
J02-3001,J03-4003,0,\N,Missing
J02-3001,C98-1013,0,\N,Missing
J02-3001,P96-1008,0,\N,Missing
J05-1004,P98-1013,0,0.547446,"Missing"
J05-1004,J99-2004,0,0.0793722,"iscuss the criteria used to define the sets of semantic roles used in the annotation process and to analyze the frequency of syntactic/semantic alternations in the corpus. We describe an automatic system for semantic role tagging trained on the corpus and discuss the effect on its performance of various types of information, including a comparison of full syntactic parsing with a flat representation and the contribution of the empty ‘‘trace’’ categories of the treebank. 1. Introduction Robust syntactic parsers, made possible by new statistical techniques (Ratnaparkhi 1997; Collins 1999, 2000; Bangalore and Joshi 1999; Charniak 2000) and by the availability of large, hand-annotated training corpora (Marcus, Santorini, and Marcinkiewicz 1993; Abeille´ 2003), have had a major impact on the field of natural language processing in recent years. However, the syntactic analyses produced by these parsers are a long way from representing the full meaning of the sentences that are parsed. As a simple example, in the sentences (1) John broke the window. (2) The window broke. a syntactic analysis will represent the window as the verb’s direct object in the first sentence and its subject in the second but does not ind"
J05-1004,J93-2002,0,0.0764722,"Missing"
J05-1004,A97-1052,0,0.0690349,"Missing"
J05-1004,W04-2412,0,0.0762594,"Missing"
J05-1004,A00-2018,0,0.0611182,"o define the sets of semantic roles used in the annotation process and to analyze the frequency of syntactic/semantic alternations in the corpus. We describe an automatic system for semantic role tagging trained on the corpus and discuss the effect on its performance of various types of information, including a comparison of full syntactic parsing with a flat representation and the contribution of the empty ‘‘trace’’ categories of the treebank. 1. Introduction Robust syntactic parsers, made possible by new statistical techniques (Ratnaparkhi 1997; Collins 1999, 2000; Bangalore and Joshi 1999; Charniak 2000) and by the availability of large, hand-annotated training corpora (Marcus, Santorini, and Marcinkiewicz 1993; Abeille´ 2003), have had a major impact on the field of natural language processing in recent years. However, the syntactic analyses produced by these parsers are a long way from representing the full meaning of the sentences that are parsed. As a simple example, in the sentences (1) John broke the window. (2) The window broke. a syntactic analysis will represent the window as the verb’s direct object in the first sentence and its subject in the second but does not indicate that it pl"
J05-1004,W03-1006,0,0.0731698,"Missing"
J05-1004,P98-1046,1,0.0921239,"Missing"
J05-1004,W03-1005,0,0.0602322,"Missing"
J05-1004,W03-1008,1,0.0985942,"Missing"
J05-1004,J02-3001,1,0.544914,"ver those involving complex syntactic structure not immediately relevant to the lexical predicate itself. Only sentences in which the lexical predicate was used ‘‘in frame’’ were annotated. A word with multiple distinct senses would generally be analyzed as belonging to different frames in each sense but may only be found in the FrameNet corpus in the sense for which a frame has been defined. It is interesting to note that the semantic frames are a helpful way of generalizing between predicates; words in the same frame have been found frequently to share the same syntactic argument structure (Gildea and Jurafsky 2002). A more complete description of the FrameNet project can be found in Baker, Fillmore, and Lowe (1998) and Johnson et al. (2002), and the ramifications for automatic classification are discussed more thoroughly in Gildea and Jurafsky (2002). In contrast with FrameNet, PropBank is aimed at providing data for training statistical systems and has to provide an annotation for every clause in the Penn Treebank, no matter how complex or unexpected. Similarly to FrameNet, PropBank also attempts to label semantically related verbs consistently, relying primarily on VerbNet classes for determining sema"
J05-1004,P02-1031,1,0.09693,"bs et al. (1997) built finite-state recognizers for various entities, which were then cascaded to form recognizers for higher-level relations, while Ray and Craven (2001) used low-level ‘‘chunks’’ from a general-purpose syntactic analyzer as observations in a trained hidden Markov model. Such an approach has a large advantage in speed, as the extensive search of modern statistical parsers is avoided. It is also possible that this approach may be more robust to error than parsers. Our experiments working with a flat, ‘‘chunked’’ representation of the input sentence, described in more detail in Gildea and Palmer (2002), test this finite-state hypothesis. In the chunked representation, base-level constituent boundaries and labels are present, but there are no dependencies between constituents, as shown by the following sample sentence: (48) [NP Big investment banks] [VP refused to step] [ADVP up] [PP to] [NP the plate] [VP to support] [NP the beleaguered floor traders] [PP by] [VP buying] [NP bigblocks] [PP of] [NP stock], [NP traders] [VP say]. (wsj_2300) Our chunks were derived from the treebank trees using the conversion described by Tjong Kim Sang and Buchholz (2000). Thus, the experiments were carried o"
J05-1004,hajicova-kucerova-2002-argument,0,0.24424,"cate was used ‘‘in frame’’ were annotated. A word with multiple distinct senses would generally be analyzed as belonging to different frames in each sense but may only be found in the FrameNet corpus in the sense for which a frame has been defined. It is interesting to note that the semantic frames are a helpful way of generalizing between predicates; words in the same frame have been found frequently to share the same syntactic argument structure (Gildea and Jurafsky 2002). A more complete description of the FrameNet project can be found in Baker, Fillmore, and Lowe (1998) and Johnson et al. (2002), and the ramifications for automatic classification are discussed more thoroughly in Gildea and Jurafsky (2002). In contrast with FrameNet, PropBank is aimed at providing data for training statistical systems and has to provide an annotation for every clause in the Penn Treebank, no matter how complex or unexpected. Similarly to FrameNet, PropBank also attempts to label semantically related verbs consistently, relying primarily on VerbNet classes for determining semantic relatedness. However, there is much less emphasis on the definition of the semantics of the class that the verbs are associ"
J05-1004,P02-1043,0,0.064046,"Missing"
J05-1004,P02-1018,0,0.0648451,"Missing"
J05-1004,W04-2606,0,0.149793,"Missing"
J05-1004,P03-1009,0,0.0622622,"Missing"
J05-1004,J93-2004,0,0.0909159,"Missing"
J05-1004,A00-2034,0,0.0767001,"Missing"
J05-1004,J01-3003,0,0.140055,"frame element’s syntactic relation to the predicate. 6. A Quantitative Analysis of the Semantic-Role Labels The stated aim of PropBank is the training of statistical systems. It also provides a rich resource for a distributional analysis of semantic features of language that have hitherto been somewhat inaccessible. We begin this section with an overview of general characteristics of the syntactic realization of the different semantic-role labels and then attempt to measure the frequency of syntactic alternations with respect to verb class membership. We base this analysis on previous work by Merlo and Stevenson (2001). In the following section we discuss the performance of a system trained to automatically assign the semantic-role labels. 6.1 Associating Role Labels with Specific Syntactic Constructions We begin by simply counting the frequency of occurrence of roles in specific syntactic positions. In all the statistics given in this section, we do not consider past- or presentparticiple uses of the predicates, thus excluding any passive-voice sentences. The syntactic positions used are based on a few heuristic rules: Any NP under an S node in the treebank is considered a syntactic subject, and any NP und"
J05-1004,W04-2807,1,0.0979165,"Missing"
J05-1004,H01-1010,1,0.12072,"Missing"
J05-1004,W97-0301,0,0.0793095,"tive statistics to be calculated. We discuss the criteria used to define the sets of semantic roles used in the annotation process and to analyze the frequency of syntactic/semantic alternations in the corpus. We describe an automatic system for semantic role tagging trained on the corpus and discuss the effect on its performance of various types of information, including a comparison of full syntactic parsing with a flat representation and the contribution of the empty ‘‘trace’’ categories of the treebank. 1. Introduction Robust syntactic parsers, made possible by new statistical techniques (Ratnaparkhi 1997; Collins 1999, 2000; Bangalore and Joshi 1999; Charniak 2000) and by the availability of large, hand-annotated training corpora (Marcus, Santorini, and Marcinkiewicz 1993; Abeille´ 2003), have had a major impact on the field of natural language processing in recent years. However, the syntactic analyses produced by these parsers are a long way from representing the full meaning of the sentences that are parsed. As a simple example, in the sentences (1) John broke the window. (2) The window broke. a syntactic analysis will represent the window as the verb’s direct object in the first sentence"
J05-1004,C00-2108,0,0.0697808,"Missing"
J05-1004,P02-1029,0,0.072459,"Missing"
J05-1004,P03-1002,0,0.127172,"r, still have a long way to go. Our results using hand-annotated parse trees including traces show that improvements in parsing should translate directly into more accurate semantic representations. There has already been a demonstration that a preliminary version of these data can be used to simplify the effort involved in developing information extraction (IE) systems. Researchers were able to construct a reasonable IE system by simply mapping specific Arg labels for a set of verbs to template slots, completely avoiding the necessity of building explicit regular expression pattern matchers (Surdeanu et al. 2003). There is equal hope for advantages for machine translation, and proposition banks in Chinese (Xue and Palmer 2003) and Korean are already being built, focusing where possible on parallel data. The general approach ports well to new languages, with the major effort continuing to go into the creation of frames files for verbs. There are many directions for future work. Our preliminary linguistic analyses have merely scratched the surface of what is possible with the current annotation, and yet it is only a first approximation at capturing the richness of semantic representation. Annotation of"
J05-1004,W00-0726,0,0.128223,"Missing"
J05-1004,W04-3212,1,0.16398,"nguistics Volume 31, Number 1 The broad-coverage annotation has proven to be suitable for training automatic taggers, and in addition to ourselves there is a growing body of researchers engaged in this task. Chen and Rambow (2003) make use of extracted tree-adjoining grammars. Most recently, the Gildea and Palmer (2002) scores presented here have been improved markedly through the use of support-vector machines as well as additional features for named entity tags, headword POS tags, and verb clusters for back-off (Pradhan et al. 2003) and using maximum-entropy classifiers (He and Gildea 2004, Xue and Palmer 2004). This group also used Charniak’s parser instead of Collins’s and tested the system on TDT data. The performance on a new genre is lower, as would be expected. Despite the complex relationship between syntactic and semantic structures, we find that statistical parsers, although computationally expensive, do a good job of providing information relevant for this level of semantic interpretation. In addition to the constituent structure, the headword information, produced as a side product, is an important feature. Automatic parsers, however, still have a long way to go. Our results using hand-an"
J05-1004,W04-2704,1,\N,Missing
J05-1004,X98-1014,0,\N,Missing
J05-1004,J06-2001,0,\N,Missing
J05-1004,W03-1707,1,\N,Missing
J05-1004,J03-4003,0,\N,Missing
J05-1004,J08-2004,0,\N,Missing
J05-1004,C98-1046,1,\N,Missing
J05-1004,C98-1013,0,\N,Missing
J05-1004,P93-1032,0,\N,Missing
J05-1004,W05-0620,0,\N,Missing
J05-1004,J06-3002,0,\N,Missing
J09-4009,P05-1033,0,0.869126,"mars and present a linear-time algorithm for binarizing synchronous rules when possible. In our large-scale experiments, we found that almost all rules are binarizable and the resulting binarized rule set significantly improves the speed and accuracy of a state-of-the-art syntaxbased machine translation system. We also discuss the more general, and computationally more difficult, problem of finding good parsing strategies for non-binarizable rules, and present an approximate polynomial-time algorithm for this problem. 1. Introduction Several recent syntax-based models for machine translation (Chiang 2005; Galley et al. 2004) can be seen as instances of the general framework of synchronous grammars and tree transducers. In this framework, both alignment (synchronous parsing) and decoding can be thought of as parsing problems, whose complexity is in general exponential in the number of nonterminals on the right-hand side of a grammar rule. To alleviate this problem, we investigate bilingual binarization as a technique to factor each synchronous grammar rule into a series of binary rules. Although monolingual context-free grammars (CFGs) can always be binarized, this is not the case ∗ Informatio"
J09-4009,P03-2041,0,0.106836,"reorderings of nonterminals are denoted by variables xi . In the treetransducer formalism of Rounds (1970), the right-hand (target) side subtree can have multiple levels, as in the first rule above. This system can model non-isomorphic transformations on English parse trees to “fit” another language, learning, for example, that the (V S O) structure in Arabic should be transformed into a (S (V O)) structure in English, by looking at two-level tree fragments (Knight and Graehl 2005). From a synchronous rewriting point of view, this is more akin to synchronous tree substitution grammar (STSG) (Eisner 2003; Shieber 2004) (see Figure 3). This larger locality captures more linguistic phenomena and leads to better parameter estimation. By creating a 563 Computational Linguistics Volume 35, Number 4 Figure 3 Two equivalent representations of the first rule in Example (5): (a) tree transducer; (b) Synchronous Tree Subsitution Grammar (STSG). The ↓ arrows denote substitution sites, which correspond to variables in tree transducers. nonterminal for each right-hand-side tree, we can convert the transducer representation to an SCFG with the same generative capacity. We can again create a projected CFG w"
J09-4009,N04-1035,1,0.843738,"ent a linear-time algorithm for binarizing synchronous rules when possible. In our large-scale experiments, we found that almost all rules are binarizable and the resulting binarized rule set significantly improves the speed and accuracy of a state-of-the-art syntaxbased machine translation system. We also discuss the more general, and computationally more difficult, problem of finding good parsing strategies for non-binarizable rules, and present an approximate polynomial-time algorithm for this problem. 1. Introduction Several recent syntax-based models for machine translation (Chiang 2005; Galley et al. 2004) can be seen as instances of the general framework of synchronous grammars and tree transducers. In this framework, both alignment (synchronous parsing) and decoding can be thought of as parsing problems, whose complexity is in general exponential in the number of nonterminals on the right-hand side of a grammar rule. To alleviate this problem, we investigate bilingual binarization as a technique to factor each synchronous grammar rule into a series of binary rules. Although monolingual context-free grammars (CFGs) can always be binarized, this is not the case ∗ Information Science Institute,"
J09-4009,N07-1019,1,0.850542,"Missing"
J09-4009,W05-1506,1,0.824427,"ecting the constraints from the other side. This scheme generalizes to the case where we have n nonterminals in a SCFG rule, and the decoder conservatively assumes nothing can be done on language model scoring (because target-language spans are non-contiguous in general) until the real nonterminal has been recognized. In other words, target-language boundary words 2 An alternative to integrated decoding is rescoring, where one first computes the k-best translations according to the TM only, and then reranks the k-best list with the language model costs. This method runs very fast in practice (Huang and Chiang 2005), but often produces a considerable number of search errors because the true best translation is often outside of the k-best list, especially for longer sentences. 562 Huang et al. Binarization of Synchronous Context-Free Grammars from each child nonterminal of the rule will be cached in all virtual nonterminals derived from this rule. In the case of m-gram integrated decoding, we have to maintain 2(m − 1) boundary words for each child nonterminal, which leads to a prohibitive overall complexity of O(|w|3+2n(m−1) ), which is exponential in rule size (Huang, Zhang, and Gildea 2005). Aggressive"
J09-4009,W05-1507,1,0.919163,"Missing"
J09-4009,P05-1057,0,0.0471912,"Missing"
J09-4009,N03-1021,0,0.294945,"real examples of non-binarizable cases verified by native speakers. In the final, theoretical, sections of this article, we investigate the general problem of finding the most efficient synchronous parsing or decoding strategy for arbitrary synchronous context-free grammar (SCFG) rules, including non-binarizable cases. Although this problem is believed to be NP-complete, we prove two results that substantially reduce the search space over strategies. We also present an optimal algorithm that runs tractably in practice and a polynomial-time algorithm that is a good approximation of the former. Melamed (2003) discusses binarization of multi-text grammars on a theoretical level, showing the importance and difficulty of binarization for efficient synchronous parsing. One way around this difficulty is to stipulate that all rules must be binary from the outset, as in Inversion Transduction Grammar (ITG) (Wu 1997) and the binary SCFG employed by the Hiero system (Chiang 2005) to model the hierarchical phrases. In contrast, the rule extraction method of Galley et al. (2004) aims to incorporate more syntactic information by providing parse trees for the target language and extracting tree transducer rule"
J09-4009,W03-0301,0,0.0464804,"Missing"
J09-4009,J03-1006,0,0.1198,"Missing"
J09-4009,J04-4002,0,0.117917,"e 13 Comparing the two binarization methods in terms of translation quality against search effort. Table 2 Machine translation results for syntax-based systems vs. the phrase-based Alignment Template System. System BLEU monolingual binarization synchronous binarization alignment-template system 36.25 38.44 37.00 decoding is used as a measure of the size of search space, or time efficiency. Our system is consistently faster and more accurate than the baseline system. We also compare the top result of our synchronous binarization system with the state-of-the-art alignment-template system (ATS) (Och and Ney 2004). The results are shown in Table 2. Our system has a promising improvement over the ATS system, which is trained on a larger data set but tuned independently. A larger-scale system based on our best result performs very well in the 2006 NIST MT Evaluation (ISI Machine Translation Team 2006), achieving the best overall BLEU scores in the Chineseto-English track among all participants.4 The readers are referred to Galley et al. (2004) for details of the decoder and the overall system. 6. One-Sided Binarization In this section and the following section, we discuss techniques for handling rules th"
J09-4009,C69-0101,0,0.65771,"th source- and target-sides, so that we can generate a binary-branching SCFG: (4) S PP-VP → → NP 1 PP-VP 2 , VP 1 PP 2 , NP 1 PP-VP 2 PP 2 VP 1 In this case m-gram integrated decoding can be done in O(|w|3+4(m−1) ) time, which is a much lower-order polynomial and no longer depends on rule size (Wu 1996), allowing the search to be much faster and more accurate, as is evidenced in the Hiero system of Chiang (2005), which restricts the hierarchical phrases to form binary-branching SCFG rules. Some recent syntax-based MT systems (Galley et al. 2004) have adopted the formalism of tree transducers (Rounds 1970), modeling translation as a set of rules for a transducer that takes a syntax tree in one language as input and transforms it into a tree (or string) in the other language. The same decoding algorithms are used for machine translation in this formalism, and the following example shows that the same issues of binarization arise. Suppose we have the following transducer rules: (5) S(x1 :NP x2 :PP x3 :VP) NP( / B`aow¯eier) VP( / jux´ ˇ ıng le hu`ıt´an) PP( / yuˇ Sh¯al´ong)  >L     → → → → S(x1 VP(x3 x2 )) NP(NNP(Powell)) VP(VBD(held) NP(DT(a) NPS(meeting))) PP(TO(with) NP(NNP(Sharon))) w"
J09-4009,H05-1101,0,0.806039,"binarizing a tree-transducer rule, and consider only the alignment (or permutation) of the nonterminal variables. Again, rightmost binarization is preferable for the first rule. In SCFG-based frameworks, the problem of finding a word-level alignment between two sentences is an instance of the synchronous parsing problem: Given two strings and a synchronous grammar, find a parse tree that generates both input strings. The benefit of binary grammars also applies in this case. Wu (1997) shows that parsing a binary-branching SCFG is in O(|w|6 ), while parsing SCFG with arbitrary rules is NP-hard (Satta and Peserico 2005). For example, in Figure 2, the complexity of synchronous parsing for the original grammar (a) is O(|w|8 ), because we have to maintain four indices on either side, giving a total of eight; parsing the monolingually binarized grammar (b) involves seven indices, three on the Chinese side and four on the English side. In contrast, the synchronously binarized version (c) requires only 3 + 3 = 6 indices, which can be thought of as “CKY in two dimensions.” An efficient alignment algorithm is guaranteed if a binarization is found, and the same binarization can be used for decoding and alignment. We"
J09-4009,W04-3312,0,0.0458485,"of nonterminals are denoted by variables xi . In the treetransducer formalism of Rounds (1970), the right-hand (target) side subtree can have multiple levels, as in the first rule above. This system can model non-isomorphic transformations on English parse trees to “fit” another language, learning, for example, that the (V S O) structure in Arabic should be transformed into a (S (V O)) structure in English, by looking at two-level tree fragments (Knight and Graehl 2005). From a synchronous rewriting point of view, this is more akin to synchronous tree substitution grammar (STSG) (Eisner 2003; Shieber 2004) (see Figure 3). This larger locality captures more linguistic phenomena and leads to better parameter estimation. By creating a 563 Computational Linguistics Volume 35, Number 4 Figure 3 Two equivalent representations of the first rule in Example (5): (a) tree transducer; (b) Synchronous Tree Subsitution Grammar (STSG). The ↓ arrows denote substitution sites, which correspond to variables in tree transducers. nonterminal for each right-hand-side tree, we can convert the transducer representation to an SCFG with the same generative capacity. We can again create a projected CFG which will be ex"
J09-4009,C90-3045,0,0.379979,"arizable SCFGs, and are mainly of theoretical interest. Algorithms 1–3 make fewer and fewer assumptions on the strategy space, and produce parsing strategies closer and closer to the optimal. Algorithm 4 further improves Algorithm 3. Section Algorithm Assumptions of Strategy Space Complexity 3–4 6 Alg. 1 (synchronous) Alg. 2 (one-sided, CKY) Alg. 3 (optimal) ⇒ Alg. 4 (best-first) Contiguous on both sides Contiguous on one side O(n) O(n3 ) O(3n ) O(9k n2k ) 7.2 No assumptions systems improve. Synchronous grammars that go beyond the power of SCFG (and therefore binary SCFG) have been defined by Shieber and Schabes (1990) and Rambow and Satta (1999), and motivated for machine translation by Melamed (2003), although previous work has not given algorithms for finding efficient and optimal parsing strategies for general SCFGs, which we believe is an important problem. In the remainder of this section and the next section, we will present a series of algorithms that produce increasingly faster parsing strategies, by gradually relaxing the strong “continuity” constraint made by the synchronous binarization technique. As that technique requires continuity on both languages, we will first study a relaxation where bin"
J09-4009,P06-1123,0,0.0494632,"Missing"
J09-4009,P96-1021,0,0.347375,"NP and PP into an intermediate state which contains a gap on the English side. (c) This scheme groups PP and VP into an intermediate state which is contiguous on both sides. These two binarizations are no different in the translation-model-only decoding described previously, just as in monolingual parsing. However, in the source-channel approach to machine translation, we need to combine probabilities from the translation model (TM) (an SCFG) with the language model (an n-gram), which has been shown to be very important for translation quality (Chiang 2005). To do bigram-integrated decoding (Wu 1996), we need to augment each chart item (X, i, j) with twoÃ target! u ··· v language boundary words u and v to produce a bigram-item which we denote i X j .2 Now the two binarizations have very different effects. In the first case, we first combine NP with PP. This step is written as follows in the weighted deduction notation of Nederhof (2003): ¶ µ ¶ µ Powell ··· Powell with ··· Sharon :q :p NP PP 2 4 2 µ1 ¶ Powell ··· Powell ··· with ··· Sharon : pq NP-PP 1 4 where p and q are the scores of antecedent items. This situation is unpleasant because in the target language NP and PP are not contiguou"
J09-4009,J97-3002,0,0.790505,"e cases. Although this problem is believed to be NP-complete, we prove two results that substantially reduce the search space over strategies. We also present an optimal algorithm that runs tractably in practice and a polynomial-time algorithm that is a good approximation of the former. Melamed (2003) discusses binarization of multi-text grammars on a theoretical level, showing the importance and difficulty of binarization for efficient synchronous parsing. One way around this difficulty is to stipulate that all rules must be binary from the outset, as in Inversion Transduction Grammar (ITG) (Wu 1997) and the binary SCFG employed by the Hiero system (Chiang 2005) to model the hierarchical phrases. In contrast, the rule extraction method of Galley et al. (2004) aims to incorporate more syntactic information by providing parse trees for the target language and extracting tree transducer rules that apply to the parses. This approach results in rules with many nonterminals, making good binarization techniques critical. We explain how synchronous rule binarization interacts with n-gram language models and affects decoding for machine translation in Section 2. We define binarization formally in"
J09-4009,W07-0404,1,0.868562,"g one nonterminal at a time. The optimal grouping of nonterminals is shown on the right. time O(|w|10 ) by adding one nonterminal at a time. All permutations of less than eight elements can be optimally parsed by adding one element at a time. 7.4 Discontinuous Parsing Is Necessary Only for Non-Decomposable Permutations In this subsection, we show that an optimal parsing strategy can be found by first factoring an SCFG rule into a sequence of shorter SCFG rules, if possible, and then considering each of the new rules independently. The first step can be done efficiently using the algorithms of Zhang and Gildea (2007). The second step can be done in time O(9kc · n2kc ) using Algorithm 4, where kc is the complexity of the longest SCFG rule after factorizations, implying that kc ≤ (n + 4). We show that this two-step process is optimal, by proving that the optimal parsing strategy for the initial rule will not need to build subsets of children that cross the boundaries of the factorization into shorter SCFG rules. Figure 19 shows a permutation that contains permutations of fewer numbers within itself so that the entire permutation can be decomposed hierarchically. We prove that if there is a contiguous block"
J09-4009,N06-1033,1,0.518184,"present a decoding strategy for these rules in Section 6. Section 7 gives a solution to the general theoretical problem of finding optimal decoding and synchronous parsing strategies for arbitrary SCFGs, and presents complexity results on the nonbinarizable rules from our Chinese–English data. These final two sections are of primarily theoretical interest, as nonbinarizable rules have not been shown to benefit real-world machine translation systems. However, the algorithms presented may become relevant as machine translation systems improve. 1 A preliminary version of Section 1–5 appeared in Zhang et al. (2006). 560 Huang et al. Binarization of Synchronous Context-Free Grammars 2. Motivation Consider the following Chinese sentence and its English translation: (1)    >L  B`aow¯eier yuˇ Sh¯al´ong jux´ ˇ ıng le Powell with Sharon hold [past.] “Powell held a meeting with Sharon”  hu`ıt´an meeting Suppose we have the following SCFG, where superscripts indicate reorderings (formal definitions of SCFGs with a more flexible notation can be found in Section 3): (2) S NP VP PP → → → → NP 1 PP 2 VP 3 , / B`aow¯eier, / jux´ ˇ ıng le hu`ıt´an, / yuˇ Sh¯al´ong,  >L     NP 1 VP 3 PP 2 Powell held"
J09-4009,W07-0405,1,\N,Missing
J09-4009,P06-1121,1,\N,Missing
J09-4009,W90-0102,0,\N,Missing
J11-1008,J07-2003,0,0.0416891,"word h. Placing an edge between each pair of variable nodes that share a factor, we get Figure 7b. If we compute the optimal tree decomposition for this graph, shown in Figure 7c, each of the two nodes corresponds to one of the factored rules in Figure 3b. The largest node of the tree decomposition has four variables, giving the O(n4 ) algorithm of Eisner and Satta (1999). 3.3 SCFG Parsing Strategies SCFGs generalize CFGs to generate two strings with isomorphic hierarchical structure simultaneously, and have become widely used as statistical models of machine translation (Galley et al. 2004; Chiang 2007). We write SCFG rules as productions with one Figure 7 Treewidth applied to bilexicalized parsing. 239 Computational Linguistics Volume 37, Number 1 lefthand side nonterminal and two righthand side strings. Nonterminals in the two strings are linked with superscript indices; symbols with the same index must be further rewritten synchronously. For example, X → A(1) B(2) C(3) D(4) , A(1) B(2) C(3) D(4) (1) is a rule with four children and no reordering, whereas X → A(1) B(2) C(3) D(4) , B(2) D(4) A(1) C(3) (2) expresses a more complex reordering. In general, we can take indices in the ﬁrst right"
J11-1008,P99-1059,0,0.185448,"le. We refer to this process as factorization. One straightforward example of rule factorization is the binarization of a CFG, as shown in Figure 2. Given a deduction rule for a CFG rule with r nonterminals on the righthand side, and a total of r + 1 variables, an equivalent set of rules can be produced, each with three variables, storing intermediate results that indicate that a substring of the original rule’s righthand side has been recognized. This type of rule factorization produces an O(n3 ) parser for any input CFG. Another well-known instance of rule factorization is the hook trick of Eisner and Satta (1999), which reduces the complexity of parsing for bilexicalized CFGs from O(n5 ) to O(n4 ). The basic rule for bilexicalized parsing combines two CFG constituents marked with lexical heads as shown in Figure 3a. Here items with type C indicate constituents, with [C, x0 , h, x1 ] indicating a constituent extending from position x0 to position x1 , headed by the word at position h. The item [D, m → h] is used to indicate the weight assigned by the grammar to a bilexical dependency headed by the word at a) w1 : w2 : w3 : w4 : w0 w1 w2 w3 w4 : [A, x0 , x1 ] [B, x1 , x2 ] [C, x2 , x3 ] [D, x3 , x4 ] [S"
J11-1008,W00-2011,0,0.0241606,"w2 : Grammar Factorization by Tree Decomposition [D, m → h] [C, x0 , h, x1 ] [C, x1 , m, x2 ] [C, x0 , h, x2 ] [D, m → h] [C, x1 , m, x2 ] [H, h, x1 , x2 ] wh : w1 : wh w1 : [H, h, x1 , x2 ] [C, x0 , h, x1 ] [C, x0 , h, x2 ] Figure 3 Rule factorization for bilexicalized parsing. position h with the word at position m as a modiﬁer. The deduction rule is broken into two steps, one which includes the weight for the bilexical grammar rule, and another which identiﬁes the boundaries of the new constituent, as shown in Figure 3b. The hook trick has also been applied to Tree Adjoining Grammar (TAG; Eisner and Satta 2000), and has been generalized to improve the complexity of machine translation decoding under synchronous context-free grammars (SCFGs) with an n-gram language model (Huang, Zhang, and Gildea 2005). Rule factorization has also been studied in the context of parsing for SCFGs. Unlike monolingual CFGs, SCFGs cannot always be binarized; depending on the permutation between nonterminals in the two languages, it may or may not be possible to reduce the rank, or number of nonterminals on the righthand side, of a rule. Algorithms for ﬁnding the optimal rank reduction of a speciﬁc rule are given by Zhang"
J11-1008,N04-1035,0,0.0783873,"and x2 and its head word h. Placing an edge between each pair of variable nodes that share a factor, we get Figure 7b. If we compute the optimal tree decomposition for this graph, shown in Figure 7c, each of the two nodes corresponds to one of the factored rules in Figure 3b. The largest node of the tree decomposition has four variables, giving the O(n4 ) algorithm of Eisner and Satta (1999). 3.3 SCFG Parsing Strategies SCFGs generalize CFGs to generate two strings with isomorphic hierarchical structure simultaneously, and have become widely used as statistical models of machine translation (Galley et al. 2004; Chiang 2007). We write SCFG rules as productions with one Figure 7 Treewidth applied to bilexicalized parsing. 239 Computational Linguistics Volume 37, Number 1 lefthand side nonterminal and two righthand side strings. Nonterminals in the two strings are linked with superscript indices; symbols with the same index must be further rewritten synchronously. For example, X → A(1) B(2) C(3) D(4) , A(1) B(2) C(3) D(4) (1) is a rule with four children and no reordering, whereas X → A(1) B(2) C(3) D(4) , B(2) D(4) A(1) C(3) (2) expresses a more complex reordering. In general, we can take indices in"
J11-1008,N10-1118,1,0.947732,"le factorization has also been applied to Linear Context-Free Rewriting Systems (LCFRS), which generalize CFG, TAG, and SCFG to deﬁne a rewriting system where nonterminals may have arbitrary fan-out, which indicates the number of continuous spans that a nonterminal accounts for in the string (Vijay-Shankar, Weir, and Joshi 1987). Recent work has examined the problem of factorization of LCFRS rules in order to ´ reduce rank without increasing grammar fan-out (Gomez-Rodr´ ıguez et al. 2009), as well as factorization with the goal of directly minimizing the parsing complexity of the new grammar (Gildea 2010). We deﬁne factorization as a process which applies to rules of the input grammar independently. Individual rules are replaced with an equivalent set of new rules, which must derive the same set of consequent items as the original rule given the same antecedent items. While new intermediate items of distinct types may be produced, the set of items and weights derived by the original weighted deduction system is unchanged. This deﬁnition of factorization is broad enough to include all of the previous examples, but does not include, for example, the fold/unfold operation applied to grammars by J"
J11-1008,N07-1019,1,0.670669,"Missing"
J11-1008,N09-1061,0,0.223662,"7). The complexity of synchronous parsing for a rule of rank r is O(n2r+2 ), so reducing rank improves parsing complexity. Rule factorization has also been applied to Linear Context-Free Rewriting Systems (LCFRS), which generalize CFG, TAG, and SCFG to deﬁne a rewriting system where nonterminals may have arbitrary fan-out, which indicates the number of continuous spans that a nonterminal accounts for in the string (Vijay-Shankar, Weir, and Joshi 1987). Recent work has examined the problem of factorization of LCFRS rules in order to ´ reduce rank without increasing grammar fan-out (Gomez-Rodr´ ıguez et al. 2009), as well as factorization with the goal of directly minimizing the parsing complexity of the new grammar (Gildea 2010). We deﬁne factorization as a process which applies to rules of the input grammar independently. Individual rules are replaced with an equivalent set of new rules, which must derive the same set of consequent items as the original rule given the same antecedent items. While new intermediate items of distinct types may be produced, the set of items and weights derived by the original weighted deduction system is unchanged. This deﬁnition of factorization is broad enough to incl"
J11-1008,J99-4004,0,0.0614108,"ree Rewriting Systems. We show that any polynomial-time algorithm for this problem would imply an improved approximation algorithm for the well-studied treewidth problem on general graphs. 1. Introduction In this article, we describe meta-algorithms for parsing: algorithms for ﬁnding the optimal parsing algorithm for a given grammar, with the constraint that rules in the grammar are considered independently of one another. In order to have a common representation for our algorithms to work with, we represent parsing algorithms as weighted deduction systems (Shieber, Schabes, and Pereira 1995; Goodman 1999; Nederhof 2003). Weighted deduction systems consist of axioms and rules for building items or partial results. Items are identiﬁed by square brackets, with their weights written to the left. Figure 1 shows a rule for deducing a new item when parsing a context free grammar (CFG) with the rule S → A B. The item below the line, called the consequent, can be derived if the two items above the line, called the antecedents, have been derived. Items have types, corresponding to grammar nonterminals in this example, and variables, whose values range over positions in the string to be parsed. We restr"
J11-1008,W05-1507,1,0.857563,"Missing"
J11-1008,J09-4009,1,0.877137,"r, optimal factorization is generally feasible whenever parsing with the unfactorized grammar is feasible. This is because, for an input rule with  variables, parsing is O(n ) in the sentence length n. The treewidth of this rule is at most  − 1, and can be computed in time O(+1 ); generally we expect n to be greater than . One may also wish to accept only rules having treewidth k and disregard the remainder, for example, when factorizing rules automatically 236 Gildea Grammar Factorization by Tree Decomposition extracted from word-aligned bitext (Wellington, Waxmonsky, and Melamed 2006; Huang et al. 2009) or from dependency treebanks (Kuhlmann and Nivre 2006; Gildea 2010). In this setting, the rules having treewidth k can be identiﬁed in time O(k+2 ) using the simple algorithm of Arnborg, Corneil, and Proskurowski (1987), (where again  is the number of variables in the input rules), or in time O( ) using the algorithm of Bodlaender (1996). 2.2 Cyclic Dependencies Although this article primarily addresses the case where there are no cyclic dependencies between rule instantiations, we note here that our techniques carry over to the cyclic case under certain conditions. If there are cycles in"
J11-1008,P07-1022,0,0.0241742,"). We deﬁne factorization as a process which applies to rules of the input grammar independently. Individual rules are replaced with an equivalent set of new rules, which must derive the same set of consequent items as the original rule given the same antecedent items. While new intermediate items of distinct types may be produced, the set of items and weights derived by the original weighted deduction system is unchanged. This deﬁnition of factorization is broad enough to include all of the previous examples, but does not include, for example, the fold/unfold operation applied to grammars by Johnson (2007) and Eisner and Blatz (2007). Rule factorization corresponds to the unfold operation of fold/unfold. If we allow unrestricted transformations of the input deduction system, ﬁnding the most efﬁcient equivalent system is undecidable; this follows from the fact that it is undecidable whether a CFG generates the set of all strings (Bar-Hillel, Perles, and Shamir 1961), and would therefore be recognizable in constant time. Whereas the fold/unfold operation of Johnson (2007) and Eisner and Blatz (2007) speciﬁes a narrower class of 233 Computational Linguistics Volume 37, Number 1 grammar transformat"
J11-1008,P06-2066,0,0.0257088,"enever parsing with the unfactorized grammar is feasible. This is because, for an input rule with  variables, parsing is O(n ) in the sentence length n. The treewidth of this rule is at most  − 1, and can be computed in time O(+1 ); generally we expect n to be greater than . One may also wish to accept only rules having treewidth k and disregard the remainder, for example, when factorizing rules automatically 236 Gildea Grammar Factorization by Tree Decomposition extracted from word-aligned bitext (Wellington, Waxmonsky, and Melamed 2006; Huang et al. 2009) or from dependency treebanks (Kuhlmann and Nivre 2006; Gildea 2010). In this setting, the rules having treewidth k can be identiﬁed in time O(k+2 ) using the simple algorithm of Arnborg, Corneil, and Proskurowski (1987), (where again  is the number of variables in the input rules), or in time O( ) using the algorithm of Bodlaender (1996). 2.2 Cyclic Dependencies Although this article primarily addresses the case where there are no cyclic dependencies between rule instantiations, we note here that our techniques carry over to the cyclic case under certain conditions. If there are cycles in the rule dependencies, but the semiring meets Knuth’s"
J11-1008,E09-1055,0,0.053045,"e more than one span in each language. These intermediate deduction steps do, however, correspond to LCFRS rules. We now turn to examine LCFRS in more detail. 240 Gildea Grammar Factorization by Tree Decomposition Figure 8 Treewidth applied to the SCFG rule of Equation (2). 4. LCFRS Parsing Strategies LCFRS provides a generalization of a number of widely used formalisms in natural language processing, including CFG, TAG, SCFG, and synchronous TAG. LCFRS has also been used to model non-projective dependency grammars, and the LCFRS rules extracted from dependency treebanks can be quite complex (Kuhlmann and Satta 2009), making factorization important. Similarly, LCFRS can model translation relations beyond the power of SCFG (Melamed, Satta, and Wellington 2004), and grammars extracted from word-aligned bilingual corpora can also be quite complex (Wellington, Waxmonsky, and Melamed 2006). An algorithm for factorization of LCFRS rules is presented by Gildea (2010), exploiting speciﬁc properties of LCFRS. The tree decomposition method achieves the same results without requiring analysis speciﬁc to LCFRS. In this section, we examine the complexity of rule factorization for general LCFRS grammars. The problem of"
J11-1008,P04-1084,0,0.060589,"Missing"
J11-1008,J03-1006,0,0.120199,"Systems. We show that any polynomial-time algorithm for this problem would imply an improved approximation algorithm for the well-studied treewidth problem on general graphs. 1. Introduction In this article, we describe meta-algorithms for parsing: algorithms for ﬁnding the optimal parsing algorithm for a given grammar, with the constraint that rules in the grammar are considered independently of one another. In order to have a common representation for our algorithms to work with, we represent parsing algorithms as weighted deduction systems (Shieber, Schabes, and Pereira 1995; Goodman 1999; Nederhof 2003). Weighted deduction systems consist of axioms and rules for building items or partial results. Items are identiﬁed by square brackets, with their weights written to the left. Figure 1 shows a rule for deducing a new item when parsing a context free grammar (CFG) with the rule S → A B. The item below the line, called the consequent, can be derived if the two items above the line, called the antecedents, have been derived. Items have types, corresponding to grammar nonterminals in this example, and variables, whose values range over positions in the string to be parsed. We restrict ourselves to"
J11-1008,P92-1012,0,0.65578,"sB  A → gA () gA () = a B → gB () gB () = b Here, all nonterminals have fan-out = 1, reﬂected in the fact that all tuples deﬁning the productions’ functions contain just one string. As CFG is equivalent to LCFRS with fanout = 1, SCFG and TAG can be represented as LCFRS with fan-out = 2. Higher values of fan-out allow strictly more powerful grammars (Rambow and Satta 1999). Polynomialtime parsing is possible for any ﬁxed LCFRS grammar, but the degree of the polynomial depends on the grammar. Parsing general LCFRS grammars, where the grammar is considered part of the input, is NP-complete (Satta 1992). 4.1 Graphs Derived from LCFRS Rules Given an LCFRS rule as deﬁned previously, a weighted deduction rule for a bottom– up parser can be derived by creating an antecedent for each righthand nonterminal, a consequent for the lefthand side, and variables for all the boundaries of the nonterminals in the rule. A nonterminal of fan-out f has 2f boundaries. Each boundary 242 Gildea Grammar Factorization by Tree Decomposition variable will occur exactly twice in the deduction rule: either in two antecedents, if two nonterminals on the rule’s righthand side are adjacent, or once in an antecedent and"
J11-1008,P87-1015,0,0.878501,"Missing"
J11-1008,P06-1123,0,0.0513662,"Missing"
J11-1008,J97-3002,0,0.26282,"binarizable rule of Equation (2), the resulting factor graph is shown in Figure 8a, where variables x0 , . . . , x4 indicate position variables in one language of the synchronous grammar, and y0 , . . . , y4 are positions in the other language. The optimal tree decomposition for this rule is shown in Figure 8c. For this permutation, the optimal parsing algorithm takes time O(n8 ), because the largest node in the tree decomposition of Figure 8c includes eight position variables. This result is intermediate between the O(n6 ) for binarizable SCFGs, also known as Inversion Transduction Grammars (Wu 1997), and the O(n10 ) that we would achieve by recognizing the rule in a single deduction step. ˇ Gildea and Stefankoviˇ c (2007) use a combinatorial argument to show that as the number of nonterminals r in an SCFG rule grows, the parsing complexity grows as Ω(ncr ) for some constant c. In other words, some very difﬁcult permutations exist of all lengths. It is interesting to note that although applying the tree decomposition technique to long CFG rules results in a deduction system equivalent to a binarized CFG, the individual deduction steps in the best parsing strategy for an SCFG rule do not i"
J11-1008,W07-0404,1,0.844703,"2000), and has been generalized to improve the complexity of machine translation decoding under synchronous context-free grammars (SCFGs) with an n-gram language model (Huang, Zhang, and Gildea 2005). Rule factorization has also been studied in the context of parsing for SCFGs. Unlike monolingual CFGs, SCFGs cannot always be binarized; depending on the permutation between nonterminals in the two languages, it may or may not be possible to reduce the rank, or number of nonterminals on the righthand side, of a rule. Algorithms for ﬁnding the optimal rank reduction of a speciﬁc rule are given by Zhang and Gildea (2007). The complexity of synchronous parsing for a rule of rank r is O(n2r+2 ), so reducing rank improves parsing complexity. Rule factorization has also been applied to Linear Context-Free Rewriting Systems (LCFRS), which generalize CFG, TAG, and SCFG to deﬁne a rewriting system where nonterminals may have arbitrary fan-out, which indicates the number of continuous spans that a nonterminal accounts for in the string (Vijay-Shankar, Weir, and Joshi 1987). Recent work has examined the problem of factorization of LCFRS rules in order to ´ reduce rank without increasing grammar fan-out (Gomez-Rodr´ ıg"
J12-3008,J07-2003,0,0.0121416,"uch as those provided by STSGs are ultimately tools to translate a string ∗ Computer Science Department, University of Rochester, Rochester NY 14627. E-mail: gildea@cs.rochester.edu. Submission received: 3 May 2011; revised submission received: 1 October 2011; accepted for publication: 28 October 2011. © 2012 Association for Computational Linguistics Computational Linguistics Volume 38, Number 3 in one natural language into a string in another. Whereas MBOTs originate in the tree transducer literature and are deﬁned to take a tree as input, MT systems such as those of Galley et al. (2006) and Chiang (2007) ﬁnd a parse of the source language sentence as part of the translation process, and the decoding algorithm, introduced by Yamada and Knight (2002), has more in common with CYK parsing than with simulating a tree transducer. In this article, we investigate the power of MBOTs, and of compositions of STSGs in particular, in terms of the set of string translations that they generate. We relate MBOTs and compositions of STSGs to existing grammatical formalisms deﬁned on strings through ﬁve main results, which we outline subsequently. The ﬁrst four results serve to situate general MBOTs among strin"
J12-3008,P06-1121,0,0.130759,"ee Transducers from this point of view. 1. Introduction Many current approaches to syntax-based statistical machine translation fall under the theoretical framework of synchronous tree substitution grammars (STSGs). Tree substitution grammars (TSGs) generalize context-free grammars (CFGs) in that each rule expands a nonterminal to produce an arbitrarily large tree fragment, rather than a fragment of depth one as in a CFG. Synchronous TSGs generate tree fragments in the source and target languages in parallel, with each rule producing a tree fragment in either language. Systems such as that of Galley et al. (2006) extract STSG rules from parallel bilingual text that has been automatically parsed in one language, and the STSG nonterminals correspond to nonterminals in these parse trees. Chiang’s (2007) Hiero system produces simpler STSGs with a single nonterminal. STSGs have the advantage that they can naturally express many re-ordering and restructuring operations necessary for machine translation (MT). They have the disadvantage, however, that they are not closed under composition (Maletti et al. 2009). Therefore, if one wishes to construct an MT system as a pipeline of STSG operations, the result may"
J12-3008,J09-4009,1,0.834143,"e binarized and synchronously parsed in time O(n6 ), but tabular parsing for the LCFRS resulting from composition has higher complexity. Thus, composing STSGs generally increases the complexity of synchronous parsing. The problem of language-model–integrated decoding with synchronous grammars is closely related to that of synchronous parsing; both problems can be seen as intersecting the grammar with a ﬁxed source-language string and a ﬁnite-state machine constraining the target-language string. The widely used decoding algorithms for SCFG (Yamada and Knight 2002; Zollmann and Venugopal 2006; Huang et al. 2009) search for the highest-scoring translation when combining scores from a weighted SCFG and a weighted ﬁnite-state language model. As with SCFG, language-model–integrated decoding for weighted MBOTs can be performed by adding n-gram language model state to each candidate target language span. This, as with synchronous parsing, gives an algorithm which is polynomial in the length of the input sentence for a ﬁxed MBOT, but with an exponent that depends on the complexity of the MBOT. Furthermore, Theorem 5 indicates that SCFG-based decoding techniques cannot be applied off the shelf to composition"
J12-3008,N10-1130,0,0.35178,"l text that has been automatically parsed in one language, and the STSG nonterminals correspond to nonterminals in these parse trees. Chiang’s (2007) Hiero system produces simpler STSGs with a single nonterminal. STSGs have the advantage that they can naturally express many re-ordering and restructuring operations necessary for machine translation (MT). They have the disadvantage, however, that they are not closed under composition (Maletti et al. 2009). Therefore, if one wishes to construct an MT system as a pipeline of STSG operations, the result may not be expressible as an STSG. Recently, Maletti (2010) has argued that multi bottom–up tree transducers (MBOTs) (Lilin 1981; Arnold and Dauchet 1982; Engelfriet, Lilin, and Maletti 2009) provide a useful representation for natural language processing applications because they generalize STSGs, but have the added advantage of being closed under composition. MBOTs generalize traditional bottom–up tree transducers in that they allow transducer states to pass more than one output subtree up to subsequent transducer operations. The number of subtrees taken by a state is called its rank. MBOTs are linear and non-deleting; that is, operations cannot cop"
J12-3008,N03-1021,0,0.0270631,"the translations produced by MBOTs are equivalent to a speciﬁc restricted form of LCFRS, which we call 1-m-LCFRS. From the construction relating MBOTs and 1-m-LCFRSs follow results about the source and target sides of the translations produced by MBOTs. In particular, our third result is that the translations produced by MBOTs are context-free within the source language, and hence are strictly less powerful than LCFRSs. This implies that MBOTs are not as general as STAGs, for example. Similarly, MBOTs are not as general as the generalized multitext grammars proposed for machine translation by Melamed (2003), which retain the full power of LCFRSs in each language (Melamed, Satta, and Wellington 2004). Our fourth result is that the output of an MBOT, when viewed as a string language, does retain the full power of LCFRSs. This fact is mentioned by Engelfriet, Lilin, and Maletti (2009, page 586), although no explicit construction is given. Our ﬁnal result speciﬁcally addresses the string translations that result from compositions of STSGs, with the goal of better understanding the complexity of using such compositions in machine translation systems. We show that the translations produced by composit"
J12-3008,P04-1084,0,0.0576384,"Missing"
J12-3008,P92-1012,0,0.171092,"ne, reﬂected in the fact that all tuples deﬁning the productions’ functions contain just one string. Just as CFG is equivalent to LCFRS with fan-out 1, SCFG and TAG can be represented as LCFRS with fan-out 2. Higher values of fan-out allow strictly more powerful grammars (Rambow and Satta 1999). Polynomial-time parsing is possible for any ﬁxed LCFRS grammar, but the degree of 676 Gildea On the String Translations Produced by Multi Bottom–Up Tree Transducers the polynomial depends on the grammar. Parsing general LCFRS grammars, where the grammar is considered part of the input, is NP-complete (Satta 1992). Following Melamed, Satta, and Wellington (2004), we represent translation in LCFRS by using a special symbol # to separate the strings of the two languages. Our LCFRS grammars will only generate strings of the form s#t, where s and t are strings not containing the symbol #, and we will identify s as the source string and t as the target string. We use the notation trans(LCFRS) to denote the set of translations that can be produced by taking the string language of some LCFRS and splitting each string into a pair at the location of the # symbol. 3. Translations Produced by General MBOTs In thi"
J12-3008,H05-1101,0,0.0186702,"omposition of STSGs. In discussing the complexity of synchronous parsing problems, we distinguish the case where the grammar is considered part of the input, and the case where the grammar is ﬁxed, and only the source and target strings are considered part of the input. For SCFGs, synchronous parsing is NP-complete when the grammar is considered part of the input and can have arbitrary rank. For any ﬁxed grammar, however, synchronous parsing is possible in time polynomial in the lengths of the source and target strings, with the degree of the polynomial depending on the rank of the ﬁxed SCFG (Satta and Peserico 2005). Because MBOTs subsume SCFGs, the problem of recognizing whether a string pair belongs to the translation produced by an arbitrary MBOT, when the MBOT is considered part of the input, is also NP-complete. Given our construction for converting an MBOT to an LCFRS, we can use standard LCFRS tabular parsing techniques to determine whether a given string pair belongs to the translation deﬁned by the yield of a ﬁxed MBOT. As with arbitrary-rank SCFG, LCFRS parsing is polynomial in the length of the input string pair, but the degree of the polynomial depends on the complexity of the MBOT. To be pre"
J12-3008,J94-1004,0,0.413186,"ng formalisms, and the ﬁfth result addresses MBOTs resulting from compositions of STSGs in particular. Our ﬁrst result is that the translations produced by MBOTs are a subset of those produced by linear context-free rewriting systems (LCFRSs) (Vijay-Shankar, Weir, and Joshi 1987). LCFRS provides a very general framework that subsumes CFG, tree adjoining grammar (TAG; Joshi, Levy, and Takahashi 1975; Joshi and Schabes 1997), and more complex systems, as well as synchronous context-free grammar (SCFG) (Aho and Ullman 1972) and synchronous tree adjoining grammar (STAG) (Shieber and Schabes 1990; Schabes and Shieber 1994) in the context of translation. LCFRS allows grammar nonterminals to generate more than one span in the ﬁnal string; the number of spans produced by an LCFRS nonterminal corresponds to the rank of an MBOT state. Our second result states that the translations produced by MBOTs are equivalent to a speciﬁc restricted form of LCFRS, which we call 1-m-LCFRS. From the construction relating MBOTs and 1-m-LCFRSs follow results about the source and target sides of the translations produced by MBOTs. In particular, our third result is that the translations produced by MBOTs are context-free within the s"
J12-3008,C90-3045,0,0.514347,"e general MBOTs among string formalisms, and the ﬁfth result addresses MBOTs resulting from compositions of STSGs in particular. Our ﬁrst result is that the translations produced by MBOTs are a subset of those produced by linear context-free rewriting systems (LCFRSs) (Vijay-Shankar, Weir, and Joshi 1987). LCFRS provides a very general framework that subsumes CFG, tree adjoining grammar (TAG; Joshi, Levy, and Takahashi 1975; Joshi and Schabes 1997), and more complex systems, as well as synchronous context-free grammar (SCFG) (Aho and Ullman 1972) and synchronous tree adjoining grammar (STAG) (Shieber and Schabes 1990; Schabes and Shieber 1994) in the context of translation. LCFRS allows grammar nonterminals to generate more than one span in the ﬁnal string; the number of spans produced by an LCFRS nonterminal corresponds to the rank of an MBOT state. Our second result states that the translations produced by MBOTs are equivalent to a speciﬁc restricted form of LCFRS, which we call 1-m-LCFRS. From the construction relating MBOTs and 1-m-LCFRSs follow results about the source and target sides of the translations produced by MBOTs. In particular, our third result is that the translations produced by MBOTs ar"
J12-3008,P87-1015,0,0.771455,"Missing"
J12-3008,J97-3002,0,0.191634,"ly in the source side of the sentential form, we refer to the sequence of indices in the target side as the preterminal permutation of a derivation. For example, the preterminal permutation of the derivation in Figure 10 is (3,2,1). The permutation of any sentential form of an SCFG of rank r can be produced by composing permutations of length no greater than r, by induction over the length of the derivation. Thus, while the permutation (3,2,1) of our example can be produced by composing permutations of length 2, the preterminal permutation (2,4,1,3) can never be produced by an SCFG of rank 2 (Wu 1997). In fact, this restriction also applies to subsequences of the preterminal permutation. Lemma 3 Let π be a preterminal permutation produced by an SCFG derivation containing rules of maximum rank r, and let π be a permutation obtained from π by removing some elements and renumbering the remaining elements with a strictly increasing function. Then π falls within the class of compositions of permutations of length r. Proof From each rule in the derivation producing preterminal permutation π, construct a new rule by removing any nonterminals whose indices were removed from π. The resulting sequ"
J12-3008,P02-1039,0,0.254883,"ter NY 14627. E-mail: gildea@cs.rochester.edu. Submission received: 3 May 2011; revised submission received: 1 October 2011; accepted for publication: 28 October 2011. © 2012 Association for Computational Linguistics Computational Linguistics Volume 38, Number 3 in one natural language into a string in another. Whereas MBOTs originate in the tree transducer literature and are deﬁned to take a tree as input, MT systems such as those of Galley et al. (2006) and Chiang (2007) ﬁnd a parse of the source language sentence as part of the translation process, and the decoding algorithm, introduced by Yamada and Knight (2002), has more in common with CYK parsing than with simulating a tree transducer. In this article, we investigate the power of MBOTs, and of compositions of STSGs in particular, in terms of the set of string translations that they generate. We relate MBOTs and compositions of STSGs to existing grammatical formalisms deﬁned on strings through ﬁve main results, which we outline subsequently. The ﬁrst four results serve to situate general MBOTs among string formalisms, and the ﬁfth result addresses MBOTs resulting from compositions of STSGs in particular. Our ﬁrst result is that the translations prod"
J12-3008,W06-3119,0,0.0126652,"ics Volume 38, Number 3 can be binarized and synchronously parsed in time O(n6 ), but tabular parsing for the LCFRS resulting from composition has higher complexity. Thus, composing STSGs generally increases the complexity of synchronous parsing. The problem of language-model–integrated decoding with synchronous grammars is closely related to that of synchronous parsing; both problems can be seen as intersecting the grammar with a ﬁxed source-language string and a ﬁnite-state machine constraining the target-language string. The widely used decoding algorithms for SCFG (Yamada and Knight 2002; Zollmann and Venugopal 2006; Huang et al. 2009) search for the highest-scoring translation when combining scores from a weighted SCFG and a weighted ﬁnite-state language model. As with SCFG, language-model–integrated decoding for weighted MBOTs can be performed by adding n-gram language model state to each candidate target language span. This, as with synchronous parsing, gives an algorithm which is polynomial in the length of the input sentence for a ﬁxed MBOT, but with an exponent that depends on the complexity of the MBOT. Furthermore, Theorem 5 indicates that SCFG-based decoding techniques cannot be applied off the"
J12-3008,W90-0102,0,\N,Missing
J14-1007,P09-1088,0,0.726616,"bilingual sentences for which a forest of possible minimal SCFG rules has been constructed from fixed word alignments. The construction of this forest and its properties are described in Section 3. We make the assumption that the alignments produced by a word-level model are correct in order to simplify the computation necessary for rule learning. This approach seems safe given that the pipeline of alignment followed by rule extraction has generally remained the state of the art despite attempts to learn joint models of alignment and rule decomposition (DeNero, Bouchard-Cote, and Klein 2008; Blunsom et al. 2009; Blunsom and Cohn 2010a). We apply our sampling algorithm to learn the granularity of rule decomposition in a Bayesian framework, comparing sampling algorithms in Section 4. The end-to-end machine translation experiments of Section 5 show that our algorithm is able to achieve performance equivalent to the standard technique of extracting all rules, but results in a significantly smaller grammar. 204 Chung et al. Sampling Tree Fragments from Forests 2. Sampling Trees from Forests As a motivating example, consider the small example forest of Figure 1. This forest contains a total of five trees,"
J14-1007,N10-1028,0,0.240304,"for which a forest of possible minimal SCFG rules has been constructed from fixed word alignments. The construction of this forest and its properties are described in Section 3. We make the assumption that the alignments produced by a word-level model are correct in order to simplify the computation necessary for rule learning. This approach seems safe given that the pipeline of alignment followed by rule extraction has generally remained the state of the art despite attempts to learn joint models of alignment and rule decomposition (DeNero, Bouchard-Cote, and Klein 2008; Blunsom et al. 2009; Blunsom and Cohn 2010a). We apply our sampling algorithm to learn the granularity of rule decomposition in a Bayesian framework, comparing sampling algorithms in Section 4. The end-to-end machine translation experiments of Section 5 show that our algorithm is able to achieve performance equivalent to the standard technique of extracting all rules, but results in a significantly smaller grammar. 204 Chung et al. Sampling Tree Fragments from Forests 2. Sampling Trees from Forests As a motivating example, consider the small example forest of Figure 1. This forest contains a total of five trees, one under the hyperedg"
J14-1007,D10-1117,0,0.051572,"Missing"
J14-1007,J93-2003,0,0.0647876,"Missing"
J14-1007,P05-1033,0,0.504252,"recise probability model used for our experiments in Section 5.2, discussing a technique to speed-up the model’s burn-in in Section 5.3, and describing our experiments in Section 5.4. 5.1 Approach A typical pipeline for training current statistical machine translation systems consists of the following three steps: word alignment, rule extraction, and tuning of feature weights. Word alignment is most often performed using the models of Brown et al. (1993) and Vogel, Ney, and Tillmann (1996). Phrase extraction is performed differently for phrase-based (Koehn, Och, and Marcu 2003), hierarchical (Chiang 2005), and syntaxbased (Galley et al. 2004) translation models, whereas tuning algorithms are generally independent of the translation model (Och 2003; Chiang, Marton, and Resnik 2008; Hopkins and May 2011). Recently, a number of efforts have been made to combine the word alignment and rule extraction steps into a joint model, with the hope both of avoiding some of the errors of the word-level alignment, and of automatically learning the decomposition of sentence pairs into rules (DeNero, Bouchard-Cote, and Klein 2008; Blunsom et al. 2009; Blunsom and Cohn 2010a; Neubig et al. 2011). This approach"
J14-1007,J07-2003,0,0.944658,"introduced by the forest structure, a complication that does not arise when the tree structure is fixed. In order to simplify the presentation of the algorithm, we first set aside the complication of large, TSG-style rules, and describe an algorithm for sampling trees from forests while avoiding computation of inside probabilities. This algorithm is then generalized to learn the composed rules of TSG in Section 2.3. As an application of our technique, we present machine translation experiments in the remainder of the article. We learn Hiero-style Synchronous Context-Free Grammar (SCFG) rules (Chiang 2007) from bilingual sentences for which a forest of possible minimal SCFG rules has been constructed from fixed word alignments. The construction of this forest and its properties are described in Section 3. We make the assumption that the alignments produced by a word-level model are correct in order to simplify the computation necessary for rule learning. This approach seems safe given that the pipeline of alignment followed by rule extraction has generally remained the state of the art despite attempts to learn joint models of alignment and rule decomposition (DeNero, Bouchard-Cote, and Klein 2"
J14-1007,D08-1024,0,0.0367942,"Missing"
J14-1007,P10-2042,0,0.108517,"bability for each node, as in the first phase of the algorithm of Johnson, Griffiths, and Goldwater (2007), becomes difficult because of the exponential number of TSG rules that can apply at any node in the forest. Not only is the number of possible TSG rules that can apply given a fixed tree structure exponentially large in the size of the tree, but the number of possible tree structures under a node is also exponentially large. This problem is particularly acute during grammar learning, as opposed to sampling according to a fixed grammar, because any tree fragment is a valid potential rule. Cohn and Blunsom (2010) address the large number of valid unseen rules by decomposing the prior over TSG rules into an equivalent probabilistic context-free grammar; however, this technique only applies to certain priors. In general, algorithms that match all possible rules are likely to be prohibitively slow, as well as unwieldy to implement. In this article, we design a sampling algorithm that avoids explicitly computing inside probabilities for each node in the forest. In Section 2, we derive a general algorithm for sampling tree fragments from forests. We avoid computing inside probabilities, as in the TSG sampl"
J14-1007,N09-1062,0,0.402648,"Missing"
J14-1007,D08-1033,0,0.244668,"Missing"
J14-1007,P06-1121,0,0.0480158,"mix of terminals and nonterminals, and we use the rules found by our sampler directly for decoding. Because word alignments are fixed in our model, any improvements we observe in translation quality indicate that our model learns how SCFG rules interplay with each other, rather than fixing word alignment errors. The problem of rule decomposition is not only relevant to the Hiero model. Translation models that make use of monolingual parsing, such as string-to-tree (Galley et al. 2004), and tree-to-string (Liu, Liu, and Lin 2006), are all known to benefit greatly from learning composed rules (Galley et al. 2006). In the particular case of Hiero rule extraction, although there is no explicit rule composition step, the extracted rules are in fact “composed rules” in the sense of string-to-tree or tree-tostring rule extraction, because they can be further decomposed into smaller SCFG rules that are also consistent with word alignments. Although our experiments only include the Hiero model, the method presented in this article is also applicable to string-totree and tree-to-string models, because the phrase decomposition forest presented in Section 3 can be extended to rule learning and extraction of oth"
J14-1007,N04-1035,0,0.300002,"or our experiments in Section 5.2, discussing a technique to speed-up the model’s burn-in in Section 5.3, and describing our experiments in Section 5.4. 5.1 Approach A typical pipeline for training current statistical machine translation systems consists of the following three steps: word alignment, rule extraction, and tuning of feature weights. Word alignment is most often performed using the models of Brown et al. (1993) and Vogel, Ney, and Tillmann (1996). Phrase extraction is performed differently for phrase-based (Koehn, Och, and Marcu 2003), hierarchical (Chiang 2005), and syntaxbased (Galley et al. 2004) translation models, whereas tuning algorithms are generally independent of the translation model (Och 2003; Chiang, Marton, and Resnik 2008; Hopkins and May 2011). Recently, a number of efforts have been made to combine the word alignment and rule extraction steps into a joint model, with the hope both of avoiding some of the errors of the word-level alignment, and of automatically learning the decomposition of sentence pairs into rules (DeNero, Bouchard-Cote, and Klein 2008; Blunsom et al. 2009; Blunsom and Cohn 2010a; Neubig et al. 2011). This approach treats both word alignment and rule de"
J14-1007,D10-1063,0,0.0921764,"el. Keeping track of table assignments during inference requires a lot of book-keeping. In order to simplify the implementation, instead of explicitly keeping track of the number of tables for each rule, we estimate the number of tables using the following equations (Huang and Renals 2010): Tr = ndr T = ndr r:|r|= 222 Chung et al. Sampling Tree Fragments from Forests In order to encourage learning rules with smaller parsing complexity and rules with mixed terminals and nonterminals, which are useful for replicating re-orderings that are seen in the data, we made use of the concept of scope (Hopkins and Langmead 2010) in our definition of rule length. The scope of a rule is defined as the number of pairs of adjacent nonterminals in the source language right-hand side plus the number of nonterminals at the beginning or end of the source language right-hand side. For example, X → f1 X1 X2 f2 X3 , X1 e1 X2 X3 e2 has scope 2 because X1 and X2 are adjacent in the source language and X3 is at the end of the source language right-hand side. The target side of the rule is irrelevant. The intuition behind this definition is that it measures the number of free indices into the source language string required during"
J14-1007,D11-1125,0,0.138676,"Missing"
J14-1007,N07-1018,0,0.0970821,"Missing"
J14-1007,W04-3250,0,0.100756,"(0 to 70) Pitman-Yor Pitman-Yor Pitman-Yor Pitman-Yor scope < 3 scope < 3 scope < 3 Hiero 212K 313K 885K 785K 19.9 23.9 26.2 25.6 19.1 23.3 24.5 25.1 489 1,214 1,488 532 Sampled averaged (0 to 70) Dirichlet scope < 3 774K 24.6 23.8 930 225 Computational Linguistics Volume 40, Number 1 different samples, we get a larger number of rules than from a single sample, but still only a quarter as many rules as in the Hiero baseline. The translation results with eight samples are comparable to the Hiero baseline (not significantly different according to 1,000 iterations of paired bootstrap resampling [Koehn 2004]). Translation results are better with the sampled grammar than with the no-singleton method of reducing grammar size, while the sampled grammar was smaller than the no-singleton rule set. Thus, averaging samples seems to produce a good trade-off between grammar size and quality. The filtering applied to the final rule set affects both the grammar size and decoding speed, because rules with different terminal/nonterminal patterns have varying decoding complexities. We experimented with two methods of filtering the final grammar: retaining rules of scope no greater than three, and the more res"
J14-1007,N03-1017,0,0.170658,"Missing"
J14-1007,D12-1021,0,0.561847,"Missing"
J14-1007,N09-1069,0,0.0205886,"tation tasks in natural language processing, including word segmentation, TSG learning, and learning machine translation rules, as a way of controlling the overfitting produced when Expectation Maximization would tend to prefer longer segments. However, it is important to note that the Bayesian priors in most cases control the size and number of the clusters, but do not explicitly control the size of rules. In many cases, this type of Bayesian prior alone is not strong enough to overcome the preference for longer, less generalizable rules. For example, some previous work in word segmentation (Liang and Klein 2009; Naradowsky and Toutanova 2011) adopts a “length penalty” to remedy this situation. Because we have the prior knowledge that longer rules are less likely to generalize and are therefore less likely to be a good rule, we adopt a similar scheme to control the length of rules in our model. 221 Computational Linguistics Volume 40, Number 1 In order to explicitly control the length of our rules, we generate a rule r in two stages. First, we draw the length of a rule |r |=  from a probability distribution defined over positive integers. We use a Poisson distribution:  −λ P(; λ ) = λ e ! Because"
J14-1007,P06-1077,0,0.0856457,"Missing"
J14-1007,P11-1090,0,0.0180393,"l language processing, including word segmentation, TSG learning, and learning machine translation rules, as a way of controlling the overfitting produced when Expectation Maximization would tend to prefer longer segments. However, it is important to note that the Bayesian priors in most cases control the size and number of the clusters, but do not explicitly control the size of rules. In many cases, this type of Bayesian prior alone is not strong enough to overcome the preference for longer, less generalizable rules. For example, some previous work in word segmentation (Liang and Klein 2009; Naradowsky and Toutanova 2011) adopts a “length penalty” to remedy this situation. Because we have the prior knowledge that longer rules are less likely to generalize and are therefore less likely to be a good rule, we adopt a similar scheme to control the length of rules in our model. 221 Computational Linguistics Volume 40, Number 1 In order to explicitly control the length of our rules, we generate a rule r in two stages. First, we draw the length of a rule |r |=  from a probability distribution defined over positive integers. We use a Poisson distribution:  −λ P(; λ ) = λ e ! Because of the factorial in the denomin"
J14-1007,P11-1064,0,0.0645305,"u 2003), hierarchical (Chiang 2005), and syntaxbased (Galley et al. 2004) translation models, whereas tuning algorithms are generally independent of the translation model (Och 2003; Chiang, Marton, and Resnik 2008; Hopkins and May 2011). Recently, a number of efforts have been made to combine the word alignment and rule extraction steps into a joint model, with the hope both of avoiding some of the errors of the word-level alignment, and of automatically learning the decomposition of sentence pairs into rules (DeNero, Bouchard-Cote, and Klein 2008; Blunsom et al. 2009; Blunsom and Cohn 2010a; Neubig et al. 2011). This approach treats both word alignment and rule decomposition as hidden variables in an EM-style algorithm. While these efforts have been able to match the performance of systems based on two successive steps for word alignment and rule extraction, they have generally not improved performance enough to become widely adopted. One possible reason for this is the added complexity and in particular the increased computation time when compared to the standard pipeline. The accuracy of word-level alignments from the standard GIZA++ package has proved hard to beat, in particular when large amount"
J14-1007,P03-1021,0,0.0608996,"ng our experiments in Section 5.4. 5.1 Approach A typical pipeline for training current statistical machine translation systems consists of the following three steps: word alignment, rule extraction, and tuning of feature weights. Word alignment is most often performed using the models of Brown et al. (1993) and Vogel, Ney, and Tillmann (1996). Phrase extraction is performed differently for phrase-based (Koehn, Och, and Marcu 2003), hierarchical (Chiang 2005), and syntaxbased (Galley et al. 2004) translation models, whereas tuning algorithms are generally independent of the translation model (Och 2003; Chiang, Marton, and Resnik 2008; Hopkins and May 2011). Recently, a number of efforts have been made to combine the word alignment and rule extraction steps into a joint model, with the hope both of avoiding some of the errors of the word-level alignment, and of automatically learning the decomposition of sentence pairs into rules (DeNero, Bouchard-Cote, and Klein 2008; Blunsom et al. 2009; Blunsom and Cohn 2010a; Neubig et al. 2011). This approach treats both word alignment and rule decomposition as hidden variables in an EM-style algorithm. While these efforts have been able to match the p"
J14-1007,P02-1040,0,0.0904425,"red in BLEU. We therefore used stratified sampling throughout our experiments. 223 Computational Linguistics Volume 40, Number 1 5.4 Experiments We used a Chinese–English parallel corpus available from LDC,1 composed of newswire text. The corpus consists of 41K sentence pairs, which is 1M words on the English side. We used a 392-sentence development set with four references for parameter tuning, and a 428-sentence test set with four references for testing.2 The development set and the test set have sentences with less than 30 words. A trigram language model was used for all experiments. BLEU (Papineni et al. 2002) was calculated for evaluation. 5.4.1 Baseline. For our baseline system, we extract Hiero translation rules using the heuristic method (Chiang 2007), with the standard Hiero rule extraction constraints. We use our in-house SCFG decoder for translation with both the Hiero baseline and our sampled grammars. Our features for all experiments include differently normalized rule counts and lexical weightings (Koehn, Och, and Marcu 2003) of each rule. Weights are tuned using Pairwise Ranking Optimization (Hopkins and May 2011) using the baseline grammar and development set, then used throughout the e"
J14-1007,P09-2012,1,0.913292,"ias introduced by unbalanced forests, and we present experiments using the algorithm to learn Synchronous Context-Free Grammar rules for machine translation. In this application, the forests being sampled represent the set of Hiero-style rules that are consistent with fixed input word-level alignments. We demonstrate equivalent machine translation performance to standard techniques but with much smaller grammars. 1. Introduction Recent work on learning Tree Substitution Grammars (TSGs) has developed procedures for sampling TSG rules from known derived trees (Cohn, Goldwater, and Blunsom 2009; Post and Gildea 2009). Here one samples binary variables at each node in the tree, indicating whether the node is internal to a TSG rule or is a split point between two rules. We consider the problem of learning TSGs in cases where the tree structure is not known, but rather where possible tree structures are represented in a forest. For example, we may wish to learn from text where treebank annotation is unavailable, ∗ Computer Science Dept., University of Rochester, Rochester NY 14627. E-mail: chung@cs.rochester.edu. ∗∗ Computer Science Dept., University of Rochester, Rochester NY 14627. E-mail: lfang@cs.rochest"
J14-1007,W11-2167,0,0.027214,"Missing"
J14-1007,P06-1124,0,0.138298,"mposed rules” in the sense of string-to-tree or tree-tostring rule extraction, because they can be further decomposed into smaller SCFG rules that are also consistent with word alignments. Although our experiments only include the Hiero model, the method presented in this article is also applicable to string-totree and tree-to-string models, because the phrase decomposition forest presented in Section 3 can be extended to rule learning and extraction of other syntax-based MT models. 5.2 Model In this section, we describe a generative model based on the Pitman-Yor process (Pitman and Yor 1997; Teh 2006) over derivation trees consisting of composed rules. Bayesian methods have been applied to a number of segmentation tasks in natural language processing, including word segmentation, TSG learning, and learning machine translation rules, as a way of controlling the overfitting produced when Expectation Maximization would tend to prefer longer segments. However, it is important to note that the Bayesian priors in most cases control the size and number of the clusters, but do not explicitly control the size of rules. In many cases, this type of Bayesian prior alone is not strong enough to overcom"
J14-1007,J97-3002,0,0.527566,"ecoding. The following is the full list of the Hiero constraints, taken verbatim from Chiang (2007): r r r r r r If there are multiple initial phrase pairs containing the same set of alignments, only the smallest is kept. That is, unaligned words are not allowed at the edges of phrases. Initial phrases are limited to a length of 10 words on either side. Rules are limited to five nonterminals plus terminals on the French side. Rules can have at most two nonterminals, which simplifies the decoder implementation. This also makes our grammar weakly equivalent to an inversion transduction grammar (Wu 1997), although the conversion would create a very large number of new nonterminal symbols. It is prohibited for nonterminals to be adjacent on the French side, a major cause of spurious ambiguity. A rule must have at least one pair of aligned words, so that translation decisions are always based on some lexical evidence. Of these constraints, the differences between the Hiero constraints and scope filtering are: First, the Hiero constraints limit the number of nonterminals in a rule to no more than two. Second, the Hiero constraints do not allow two adjacent nonterminals in the source side of a ru"
J14-1007,C08-1136,1,0.903155,"fficient to train and efficient to decode. Our approach of beginning with fixed word alignments is similar to that of Sankaran, Haffari, and Sarkar (2011), although their sampling algorithm reanalyzes individual phrases extracted with Hiero heuristics rather than entire sentences, and produces rules with no more than one nonterminal on the right-hand side. Most previous works on joint word alignment and rule extraction models were evaluated indirectly by resorting to heuristic methods to extract rules from learned word alignment or bracketing structures (DeNero, Bouchard-Cote, and Klein 2008; Zhang et al. 2008; Blunsom et al. 2009; Levenberg, Dyer, and Blunsom 2012), and do not directly learn the SCFG rules that are used during decoding. In this article, we work with lexicalized translation rules with a mix of terminals and nonterminals, and we use the rules found by our sampler directly for decoding. Because word alignments are fixed in our model, any improvements we observe in translation quality indicate that our model learns how SCFG rules interplay with each other, rather than fixing word alignment errors. The problem of rule decomposition is not only relevant to the Hiero model. Translation m"
J14-1007,P08-1012,1,0.807454,"fficient to train and efficient to decode. Our approach of beginning with fixed word alignments is similar to that of Sankaran, Haffari, and Sarkar (2011), although their sampling algorithm reanalyzes individual phrases extracted with Hiero heuristics rather than entire sentences, and produces rules with no more than one nonterminal on the right-hand side. Most previous works on joint word alignment and rule extraction models were evaluated indirectly by resorting to heuristic methods to extract rules from learned word alignment or bracketing structures (DeNero, Bouchard-Cote, and Klein 2008; Zhang et al. 2008; Blunsom et al. 2009; Levenberg, Dyer, and Blunsom 2012), and do not directly learn the SCFG rules that are used during decoding. In this article, we work with lexicalized translation rules with a mix of terminals and nonterminals, and we use the rules found by our sampler directly for decoding. Because word alignments are fixed in our model, any improvements we observe in translation quality indicate that our model learns how SCFG rules interplay with each other, rather than fixing word alignment errors. The problem of rule decomposition is not only relevant to the Hiero model. Translation m"
J14-1007,C96-2141,0,\N,Missing
J14-1007,D08-1076,0,\N,Missing
J16-2002,J07-2003,0,0.060943,"tion algorithms for finding the treewidth of general graphs. 1. Introduction Synchronous context-free grammars (SCFGs) generalize context-free grammars (CFGs) to generate two strings simultaneously. The formalism dates from the early days of automata theory; it was developed under the name syntax-direct translation schemata to model compilers for programming languages (Lewis and Stearns 1968; Aho and Ullman 1969). SCFGs are widely used today to model the patterns of re-ordering between natural languages, and they form the basis of many state-of-the-art statistical machine translation systems (Chiang 2007). Despite the fact that SCFGs are a very natural extension of CFGs, and that the parsing problem for CFGs is rather well understood nowadays, our knowledge of the parsing problem for SCFGs is quite limited, with many questions still left unanswered. In this article we tackle one of these open problems. Unlike CFGs, SCFGs do not admit any canonical form in which rules are bounded in length (Aho and Ullman 1972), as for instance in the well-known Chomsky normal form for CFGs. A consequence of this fact is that the computational complexity of parsing with SCFG depends on the grammar. More precise"
J16-2002,P11-1046,1,0.593005,"Missing"
J16-2002,J16-2002,1,0.0513221,"Missing"
J16-2002,N10-1118,1,0.860575,"ous rules and to the re-ordering of the nonterminals across the two components. Let us call fan-out the number of substrings generated by a parse tree (this notion will be formally defined later). It is well-known among parsing practitioners that the 211 Computational Linguistics Volume 42, Number 2 fan-out affects the number of stored edges for a given input string, and is directly connected to the space and time performance of the algorithm. A binary parsing strategy of fan-out ϕ has space complexity O(n2ϕ) and time complexity at most O(n3ϕ) where n is the sentence length (Seki et al. 1991; Gildea 2010). If we adopt the appropriate parsing strategy, we can reduce the fan-out, resulting in asymptotic improvement in the space and time complexity of our algorithm. To illustrate this claim we discuss a simple example. Example 3 Consider the synchronous rule 1 2 3 4 5 6 7 8 5 7 3 1 8 6 2 4 s : [A → B B B B B B B B A→B B B B B B B B ] (1) The permutation associated with this rule is schematically visualized in Figure 1. A naive parsing strategy for rule s would be to collect the nonterminals B k one at a time and in ascending order of k. For instance, at the first step we combine B 1 and B 2 , con"
J16-2002,J11-1008,1,0.78433,"ide. In the case of an SCFG rule, each nonterminal has four endpoints, two on the English side and two on the Chinese side. If the rule has n linked nonterminals, including the left-hand side linked nonterminal, the dependency graph consists of 2n vertices and n cliques of size four. For instance, the SCFG rule [S → A 1 B 2 C 3 D 4 , S → B 2 D 4 A 1 C 3 ] is shown in the right part of Figure 17. A tree decomposition of the dependency graph corresponds directly to a parsing strategy, and the treewidth of the graph plus one is the exponent in the time complexity of the optimal parsing strategy (Gildea 2011). Each cluster of vertices in the tree decomposition corresponds to a combination step in a parsing strategy. The running intersection property of a tree decomposition ensures that each endpoint in the parsing rule has a consistent value at each step. Treewidth depends on the number of vertices in the largest cluster of the tree decomposition, which in turn determines the largest number of endpoints involved in any combination step of the parsing strategy. Our hardness result in this section is a reduction from treewidth of general graphs. Given an input graph G to the treewidth problem, we co"
J16-2002,N07-1019,1,0.714637,"Missing"
J16-2002,N09-1061,1,0.907213,"Missing"
J16-2002,J09-4009,1,0.923653,"also consider the problem of finding the parsing strategy with the lowest time complexity, and we show that it would require progress on long-standing open problems in graph theory either to find a polynomial algorithm or to show NP-hardness. The parsing complexity of SCFG rules increases with the increase of the number of nonterminals in the rule itself. Practical machine translation systems usually confine themselves to binary rules, that is, rules having no more than two right-hand side nonterminals, because of the complexity issues and because binary rules seem to be adequate empirically (Huang et al. 2009). Longer rules are of theoretical interest because of the naturalness and generality of the SCFG formalism. Longer rules may also be of practical interest as machine translation systems improve. For a fixed SCFG, complexity can be reduced by factoring the parsing of a grammar rule into a sequence of smaller steps, which we refer to as a parsing strategy. Each step of a parsing strategy collects nonterminals from the right-hand side of an SCFG rule into a subset, indicating that a portion of the SCFG rule has been matched to a subsequence of the two input strings, as we explain precisely in Sec"
J16-2002,P87-1015,0,0.837407,"Missing"
J16-2002,N04-1035,0,\N,Missing
J16-2002,N10-1035,1,\N,Missing
J16-2002,J93-2003,0,\N,Missing
J16-2002,H05-1101,1,\N,Missing
J16-2002,W90-0102,0,\N,Missing
J16-2002,C90-3045,0,\N,Missing
J16-2002,P10-1054,1,\N,Missing
J16-2002,N03-1017,0,\N,Missing
J16-2002,J94-1004,0,\N,Missing
J16-3003,N13-1052,1,0.884109,"Missing"
J16-3003,P81-1022,0,0.746306,"e parsed in time O(n4.76 ). It also shows that inversion transduction grammars can be parsed in time O(n5.76 ). In addition, binary LCFRS subsumes many other formalisms and types of grammars, for some of which we also improve the asymptotic complexity of parsing. 1. Introduction The problem of grammar recognition is a decision problem of determining whether a string belongs to a language induced by a grammar. For context-free grammars (CFGs), recognition can be done using parsing algorithms such as the CKY algorithm (Kasami 1965; Younger 1967; Cocke and Schwartz 1970) or the Earley algorithm (Earley 1970). The asymptotic complexity of these chart-parsing algorithms is cubic in the length of the sentence. In a major breakthrough, Valiant (1975) showed that context-free grammar recognition is no more complex than Boolean matrix multiplication for a matrix of size m × m where m is linear in the length of the sentence, n. With current state-of-the-art results in matrix multiplication, this means that CFG recognition can be done with an asymptotic complexity of O(n2.38 ). ∗ School of Informatics, University of Edinburgh, Edinburgh, EH8 9AB, United Kingdom. E-mail: scohen@inf.ed.ac.uk. ∗∗ Department"
J16-3003,H05-1036,0,0.091604,"Missing"
J16-3003,P99-1059,0,0.174207,"d it is a condition on the set of LCFRS grammar rules that is satisfied with many practical grammars. In cases where the grammar is balanced, our algorithm can be used as a subroutine so that it parses the binary LCFRS in time O(nωd+1 ). A similar procedure was applied by Nakanishi et al. (1998) for multiple component context-free grammars. See more discussion of this in Section 7.5. Our results focus on the asymptotic complexity as a function of string length. We do not give explicit grammar constants. For other work that focuses on reducing the grammar constant in parsing, see, for example, Eisner and Satta (1999), Dunlop, Bodenstab, and Roark (2010), and Cohen, Satta, and Collins (2013). For a discussion of the optimality of the grammar constants in Valiant’s algorithm, see, for example, Abboud, Backurs, and Williams (2015). 2. Background and Notation This section provides background on LCFRS, and establishes notation used in the remainder of the paper. A reference table of notation is also provided in Appendix A. For an integer n, let [n] denote the set of integers {1, . . . , n}. Let [n]0 = [n] ∪ {0}. For a set X, we denote by X+ the set of all sequences of length 1 or more of elements from X. A spa"
J16-3003,J11-1008,1,0.84548,"nonterminals and the rank of the rules. With tabular parsing, we can actually refer to the parsing complexity of a specific rule in the grammar. Its complexity is O(np ), where the parsing complexity p is the total fan-out of all nonterminals in the rule. For binary rules of the form A → B C, p = ϕ(A) + ϕ(B) + ϕ(C). To optimize the tabular algorithm time complexity of parsing with a binary LCFRS, equivalent to another non-binary LCFRS, we would want to minimize the time complexity it takes to parse each rule. As such, our goal is to minimize ϕ(A) + ϕ(B) + ϕ(C) in the resulting binary grammar. Gildea (2011) has shown that this metric corresponds to the tree width of a dependency graph that is constructed from the grammar. It is not known whether finding the optimal binarization of an LCFRS is an NP-complete problem, but Gildea shows that a polynomial time algorithm would imply improved approximation algorithms for the treewidth of general graphs. In general, the optimal binarization for tabular parsing may not by the same as the optimal binarization for parsing with our algorithm based on matrix multiplication. In order to optimize the complexity of our algorithm, we want to minimize d, which is"
J16-3003,J09-4009,1,0.801115,"re details in Section 7.3. 7.2 General Recognition for Synchronous Parsing Similarly to LCFRS, the rank of an SCFG is the maximal number of nonterminals that appear in the right-hand side of a rule. Any SCFG can be binarized into an LCFRS grammar. However, when the SCFG rank is arbitrary, this means that the fan-out of the LCFRS grammar can be larger than 2. This happens because binarization creates intermediate nonterminals that span several substrings, denoting binarization steps of the rule. These substrings are eventually combined into two spans, to yield the language of the SCFG grammar (Huang et al. 2009). Our algorithm does not always improve the asymptotic complexity of SCFG parsing over tabular methods. For example, Figure 10 shows the combination of spans for the rule [S → A B C D, B D A C], along with a binarization into three simpler LCFRS rules. A na¨ıve tabular algorithm for this rule would have the asymptotic complexity of O(n10 ), but the binarization shown in Figure 10 reduces this to O(n8 ). Our algorithm gives a complexity of O(n9.52 ), as the second step in the binarization shown consists of a rule with d = 4. 7.3 Generalization to Weighted Logic Programs Weighted logic programs"
J16-3003,C10-1061,0,0.0805117,"Missing"
J16-3003,P92-1012,0,0.483558,"widely used in practice. Bened´ı and S´anchez (2007) show speed improvement when parsing natural language sentences using Strassen’s algorithm as the matrix multiplication subroutine for Valiant’s algorithm for CFG parsing. This indicates that similar speed-ups may be possible in practice using our algorithm for LCFRS parsing. 1.2 Main Result Our main result is a matrix multiplication algorithm for unbalanced, single-initial binary LCFRS with asymptotic complexity M(nd ) = O(nωd ) where d is the maximal 1 Without placing a bound on f , the problem of recognition of LCFRS languages is NP-hard (Satta 1992). 422 Cohen and Gildea Parsing LCFRS with Fast Matrix Multiplication number of combination points in all grammar rules. The constant d can be easily determined from the grammar at hand:    ϕ(A) + ϕ(B) − ϕ(C),  d = max max ϕ(A) − ϕ(B) + ϕ(C),   A→B C −ϕ(A) + ϕ(B) + ϕ(C) where A → B C ranges over rules in the grammar and ϕ(A) is the fan-out of nonterminal A. Single-initial grammars are defined in Section 2, and include common formalisms such as tree-adjoining grammars. Any LCFRS can be converted to single-initial form by increasing its fan-out by at most one. The notion of unbalanced gramm"
J16-3003,J94-2002,0,0.44688,"eir (1994) reduced tree-adjoining grammars to combinatory categorial grammars. The TAGs they tackle are in “normal form,” such that the auxiliary trees are binary (all TAGs can be reduced to normal form TAGs). Such TAGs can be converted to weakly equivalent CCG (but not necessarily strongly equivalent), and as such, our algorithm applies to TAGs as well. As mentioned earlier, this finding supports the finding of Rajasekaran and Yooseph (1998), who show that TAG can be recognized in time O(M(n2 )). For an earlier discussion connections between TAG parsing and Boolean matrix multiplication, see Satta (1994). 5 General indexed grammars copy the stack to multiple nonterminals on the right-hand side. 448 Cohen and Gildea Parsing LCFRS with Fast Matrix Multiplication 6.2 Synchronous Context-Free Grammars SCFGs are widely used in machine translation to model the simultaneous derivation of translationally equivalent strings in two natural languages, and are equivalent to the syntax-directed translation schemata of Aho and Ullman (1969). SCFGs are a subclass of LCFRS where each nonterminal has fan-out 2: one span in one language and one span in the other. Because the first span of the l.h.s. nontermina"
J16-3003,C90-3045,0,0.251844,"ve a bound of O(n2ω+1 ) for ITG, which is O(n5.76 ) using the current state of the art for matrix multiplication. We achieve even greater gains for the case of multi-language synchronous parsing. Generalizing ITG to allow two nonterminals on the right-hand side of a rule in each of k languages, we have an LCFRS with fan-out k. Traditional tabular parsing has an asymptotic complexity of O(n3k ), whereas our algorithm has the complexity of O(nωk+1 ). Another interesting case of a synchronous formalism that our algorithm improves the best well-known result for is that of binary synchronous TAGs (Shieber and Schabes 1990)—that is, a TAG in which all auxiliary trees are binary. This formalism can be reduced to a binary LCFRS. A tabular algorithm for such grammar has the asymptotic complexity of O(n12 ). With our algorithm, d = 4 for this formalism, and as such its asymptotic complexity in that case is O(n9.52 ). 7. Discussion and Open Problems In this section, we discuss some extensions to our algorithm and open problems. 7.1 Turning Recognition into Parsing The algorithm we presented focuses on recognition: Given a string and a grammar, it can decide whether the string is in the language of the grammar or not."
J16-3003,J97-3002,0,0.450389,"ning that we reduce parsing complexity from O(n3f ) to O(nωf ), and that, in general, the savings in the exponent is larger for more complex grammars. LCFRS is a broad family of grammars. As such, we are able to support the findings of Rajasekaran and Yooseph (1998), who showed that tree-adjoining grammar (TAG) recognition can be done in time O(M(n2 )) = O(n4.76 ) (TAG can be reduced to LCFRS with d = 2). As a result, combinatory categorial grammars, head grammars, and linear indexed grammars can be recognized in time O(M(n2 )). In addition, we show that inversion transduction grammars (ITGs; Wu 1997) can be parsed in time O(nM(n2 )) = O(n5.76 ), improving the best asymptotic complexity previously known for ITGs. 1.1 Matrix Multiplication State of the Art Our algorithm reduces the problem of LCFRS parsing to Boolean matrix multiplication. Let M(n) be the complexity of multiplying two such n × n matrices. These matrices can be na¨ıvely multiplied in O(n3 ) time by computing for each output cell the dot product between the corresponding row and column in the input matrices (each such product is an O(n) operation). Strassen (1969) discovered a way to do the same multiplication in O(n2.8704 )"
J16-3003,W90-0102,0,\N,Missing
J18-1003,P92-1005,0,0.697689,"a sentence using an unscoped LF of the following general form: Every(x, Child(x, y), ), A(y, Politician(y), ), Run(x) (4) To scope this LF, at each step, a quantifier is picked and the main predication (i.e., Run(x)) or the partially scoped formula built so far is fused to its body hole: Step 1. Every(x, Child(x, y), Run(x)) Step 2. A(y, Politician(y), Every(x, Child(x, y), Run(x))) (5) By picking quantifiers in different orders, different scopings are generated. Next, the notion of constraints was introduced into the domain of scope underspecified semantics. For example, Quasi Logical Form (Alshawi and Crouch 1992) allows for constraints such as A > Every to be used to force one quantifier to rest within the scope of another. By inventing some machinery that allows for Discourse Representation Theory (Kamp 1981) to support scope underspecification, Underspecified Discourse Representation Theory (or UDRT) (Reyle 1993) takes the notion of constraint-based underspecification to a new level. UDRT introduces a complex system of constraints, that, among other things, can define a maximum and a minimum range for the scope of a quantifier relative to other scope bearing elements. It is fair to say that Reyle’s"
J18-1003,copestake-flickinger-2000-open,0,0.050146,"Missing"
J18-1003,P01-1019,0,0.265156,"Missing"
J18-1003,P04-1032,0,0.0628044,"ort towards defining a notion of well-formedness within the context of these frameworks. The goal has been to define a subset of URs, the so-called well-formed UR, for which the satisfiability problem becomes tractable. For example, Niehren and Thater (2003) defined the notion of (weak) net to characterize such a subset. Well-formedness was also intended to bridge the gap between these underspecification formalisms; the hope was that the differences between these formalisms disappear and they become equivalent once restricted to well-formed structures. As seen in Niehren and Thater (2003) and Fuchss et al. (2004), the problem with those efforts on defining a notion of well-formedness is that their satisfaction of both properties was only empirically supported, and hence the correctness of those statements has remained a conjecture. In better words, first, there was no mathematical proof to show that nets enforce the equivalence of qeq vs. dominance relations, the two different types of constraint used in MRS vs. Dominance Constraints/Hole Semantics, and second, although they were proved to be tractable, there was no convincing linguistic justification as to why nets cover all URs corresponding to cohe"
J18-1003,J03-1004,0,0.0812889,"Missing"
J18-1003,J87-1005,0,0.819072,"Missing"
J18-1003,P03-1047,0,0.0848282,"Missing"
J18-1003,S12-1022,1,0.733146,"Missing"
J18-1003,J82-1003,0,0.498732,"Missing"
J18-1003,D09-1152,0,0.0307979,"Missing"
J18-1004,W06-2922,0,0.540022,", Italy. E-mail: satta@dei.unipd.it. † Computer Science Department, University of Rochester, Rochester NY 14627. E-mail: xpeng@cs.rochester.edu.s Submission received: 19 December 2016; revised version received: 26 June 2017; accepted for publication: 7 July 2017. doi:10.1162/COLI a 00308 © 2017 Association for Computational Linguistics Published under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics Volume 44, Number 1 a number of extensions of stack-based transition systems to handle non-projective ´ trees (e.g., Attardi 2006; Nivre 2009; Choi and McCallum 2013; Gomez-Rodr´ ıguez and Nivre 2013; Pitler and McDonald 2015). Stack-based transition systems can produce general graphs rather than trees. Perhaps the simplest way to generate graphs is to shift one word at a time onto the stack, and then consider building all possible arcs between each word on the stack and the next word in the buffer. This is essentially the algorithm of Covington (2001), generalized to produce graphs rather than non-projective trees. This algorithm was also cast as a stack-based transition system by Nivre (2008). The algorithm runs in ti"
J18-1004,W13-2322,0,0.438888,"lgorithm for computing the minimal cache size needed to parse a given data set. In general, a graph’s relative treewidth with respect to an input order may be much higher than its absolute treewidth. However, if relative treewidth with respect to the real English word order is low, and not significantly higher than the absolute treewidth, this indicates that the word order provides valuable information about the graph structure to be predicted, and that efficient parsing is possible by making use of this information. We test this hypothesis with experiments on Abstract Meaning Representation (Banarescu et al. 2013), a semantic formalism where the meaning of a sentence is encoded as a directed graph. We find that, for English sentences, these structures have low relative treewidth with respect to the English word order, and can thus be parsed efficiently using a transition-based parser with small cache size. In order to compare across a wider variety of the semantic representations that have been 86 Gildea, Satta, and Peng Cache Transition Systems for Graph Parsing proposed (Kuhlmann and Oepen 2016), we also experiment with three sets of semantic dependencies from the Semeval 2015 semantic dependency par"
J18-1004,P13-1104,0,0.10716,"Missing"
J18-1004,E17-1051,1,0.909736,"Missing"
J18-1004,S14-2080,0,0.0199521,"graphs, with the motivation of representing semantically motivated predicate-argument relations and anaphoric references. This is done by dropping the constraint of a single head per word, and by using post-processing transformations that introduce non-projectivity. Titov et al. (2009) and Henderson et al. (2013) present a transition system for synchronous syntactic-semantic parsing, with the motivation of modeling the syntax/semantic interface. On the semantic side, their system mainly captures the predicate-argument structure and semantic role labeling. Their model has then been adapted by Du et al. (2014) for semantic-only parsing. Later, Wang, Xue, and Pradhan (2015) proposed a transition system for AMR parsing. Unlike traditional stack-based transition parsers that process input strings, 114 Gildea, Satta, and Peng Cache Transition Systems for Graph Parsing this system takes as input a dependency tree and processes its edges using a stack, applying tree-to-graph transformations that produce a directed acyclic graph. Similarly to Sagae and Tsujii (2008), the system presented by Damonte, Cohen, and Satta (2017) extends standard approaches for transition-based dependency parsing to AMR parsing,"
J18-1004,P14-1134,0,0.111776,"t of methods for unsupervised learning, as for example the inside-outside algorithm (Charniak 1993). Although we have treated the input buffer as an ordering of the vertices of the final graph, this is a simplification of the problem setting of semantic parsing for NLP. Given as input a sequence of English words, the parser must also predict which words correspond to zero, one, or more vertices of the final graph, and possibly insert vertices not corresponding to any English word. This could be accomplished either by preprocessing the input string with a separate concept identification phase (Flanigan et al. 2014), or by extending the actions of the transition system to include moves inserting new vertices into the graph. We have not included moves inserting new vertices, in order to simplify our exposition, but such moves would not fundamentally alter the correspondence between parsing runs and tree decompositions described in this article. The correspondence between runs of our parser and tree decompositions of the output graph allows for a precise characterization of the class of graphs covered, as well as simple and efficient algorithms for providing an oracle sequence of parser moves, and for dete"
J18-1004,Q14-1010,1,0.864703,"Missing"
J18-1004,J13-4002,0,0.242801,"Missing"
J18-1004,hajic-etal-2012-announcing,0,0.0434206,"Missing"
J18-1004,J13-4006,0,0.0213456,"rammars. We conclude this section with a discussion of other transition-based systems explicitly designed for graph parsing, as opposed to tree parsing. Sagae and Tsujii (2008) have possibly been the first authors to extend the stack-based transition framework for dependency tree parsing to directed acyclic graphs, with the motivation of representing semantically motivated predicate-argument relations and anaphoric references. This is done by dropping the constraint of a single head per word, and by using post-processing transformations that introduce non-projectivity. Titov et al. (2009) and Henderson et al. (2013) present a transition system for synchronous syntactic-semantic parsing, with the motivation of modeling the syntax/semantic interface. On the semantic side, their system mainly captures the predicate-argument structure and semantic role labeling. Their model has then been adapted by Du et al. (2014) for semantic-only parsing. Later, Wang, Xue, and Pradhan (2015) proposed a transition system for AMR parsing. Unlike traditional stack-based transition parsers that process input strings, 114 Gildea, Satta, and Peng Cache Transition Systems for Graph Parsing this system takes as input a dependency"
J18-1004,P10-1110,0,0.0618603,"Missing"
J18-1004,C12-1083,0,0.0396289,"se order, indicates that the real English word order provides valuable information that our parsing framework can exploit. 7. Comparison with Other Formalisms In this section we compare our cache transition parser with existing formalisms that have been used for graph-based parsing, as well as to similar transition-based systems for dependency tree parsing. 7.1 Connection to Hyperedge Replacement Grammars Hyperedge Replacement Grammars (HRGs) are a general graph rewriting formalism (Drewes, Kreowski, and Habel 1997) that has been applied by a number of authors to semantic graphs such as AMRs (Jones et al. 2012; Jones, Goldwater, and Johnson 2013; Peng, Song, and Gildea 2015). Our parsing formalism can be related to HRG through the concept of tree decomposition. 110 Gildea, Satta, and Peng Cache Transition Systems for Graph Parsing 1 → S 2 X 1 1 2 → X 2 1 2 X 1 1 2 2 → X Figure 12 An HRG that generates cycles of any size. Squares indicate grammar nonterminals with numbered ports. Number above vertices in a rule’s right-hand side correspond to the ports of the lefthand side nonterminal. An example derivation is shown in Figure 13. HRGs contain rules that rewrite a nonterminal hyperedge into a graph f"
J18-1004,W13-1810,0,0.339879,"Missing"
J18-1004,P11-1068,1,0.936355,"Missing"
J18-1004,J16-4009,0,0.178254,"Missing"
J18-1004,S16-1166,0,0.102428,"s encoded as a rooted, directed graph. Figure 8 shows an example of an AMR graph in which the nodes represent the AMR concepts and the edges represent the relations between the concepts they connect. AMR concepts consist of predicate senses, named entity annotations, and in some cases, simply lemmas of English words. AMR relations consist of core semantic roles drawn from the Propbank (Palmer, Gildea, and Kingsbury 2005) as well as very fine-grained semantic relations defined specifically for AMR. We use the training set of LDC2015E86 for SemEval 2016 task 8 on meaning representation parsing (May 2016), which contains 16,833 sentences. This data set covers various domains including newswire and Web discussion forums. For each graph, we derive a vertex order corresponding to the English word order by using the automatically generated alignments provided with the data set, which want-01 ARG1 ARG0 ARG1 like-01 ARG0 person name name op1 “John” person name name op1 “Mary” Figure 8 An example AMR graph for the sentence “John wants Mary to like him.” 106 Gildea, Satta, and Peng Cache Transition Systems for Graph Parsing 6000 5000 4000 3000 2000 1000 0 0 1 2 3 4 5 6 7 >=8 Figure 9 The distribution"
J18-1004,J08-4003,0,0.156637,"ions: They take as input a sentence and produce as output a graph representation of the semantics of the sentence itself. At the same time, recent years have seen a general trend from chart-based syntactic parsers toward stack-based transition systems, as the accuracy of transition systems has increased, and as speed has become increasingly important for real-world applications. On the syntactic side, stack-based transition systems for projective dependency parsing run in time O(n), where n is the sentence length; for a general overview of these systems, see, for instance, the presentation of Nivre (2008). There have also been ∗ Computer Science Department, University of Rochester, Rochester NY 14627. E-mail: gildea@cs.rochester.edu. ∗∗ Dipartimento di Ingegneria dell’Informazione, Universit`a di Padova, Via Gradenigo 6/A, 35131 Padova, Italy. E-mail: satta@dei.unipd.it. † Computer Science Department, University of Rochester, Rochester NY 14627. E-mail: xpeng@cs.rochester.edu.s Submission received: 19 December 2016; revised version received: 26 June 2017; accepted for publication: 7 July 2017. doi:10.1162/COLI a 00308 © 2017 Association for Computational Linguistics Published under a Creative"
J18-1004,P09-1040,0,0.365254,"l: satta@dei.unipd.it. † Computer Science Department, University of Rochester, Rochester NY 14627. E-mail: xpeng@cs.rochester.edu.s Submission received: 19 December 2016; revised version received: 26 June 2017; accepted for publication: 7 July 2017. doi:10.1162/COLI a 00308 © 2017 Association for Computational Linguistics Published under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics Volume 44, Number 1 a number of extensions of stack-based transition systems to handle non-projective ´ trees (e.g., Attardi 2006; Nivre 2009; Choi and McCallum 2013; Gomez-Rodr´ ıguez and Nivre 2013; Pitler and McDonald 2015). Stack-based transition systems can produce general graphs rather than trees. Perhaps the simplest way to generate graphs is to shift one word at a time onto the stack, and then consider building all possible arcs between each word on the stack and the next word in the buffer. This is essentially the algorithm of Covington (2001), generalized to produce graphs rather than non-projective trees. This algorithm was also cast as a stack-based transition system by Nivre (2008). The algorithm runs in time O(n2 ), a"
J18-1004,S15-2153,0,0.168651,"if we reverse the vertex order (reversed string order), the 108 Gildea, Satta, and Peng Cache Transition Systems for Graph Parsing relative treewidth is 3.08. This number is slightly larger than using the string order. The reason might be that English is more likely to have relation arcs going from left to right. If we randomize the vertex order, the relative treewidth becomes 4.84. We also evaluate the coverage of our algorithm on semantic graph-based representations other than AMR. We consider the set of semantic graphs in the Broad-Coverage Semantic Dependency Parsing task of SemEval 2015 (Oepen et al. 2015), which uses three distinct graph representations for English semantic dependencies. r r r DELPH-IN MRS-Derived Bi-Lexical Dependencies (DM): These semantic dependencies are derived from the annotation of Sections 00-21 of the WSJ Corpus with gold-standard HPSG analyses provided by the LinGO English Resource Grammar (Flickinger 2000; Flickinger, Zhang, and Kordoni 2012). Among other layers of linguistic analysis, this representation also includes logical-form meaning representations in the framework of Minimal Recursion Semantics (MRS) (Copestake et al. 2005). Enju Predicate-Argument Structure"
J18-1004,J05-1004,1,0.381751,"Missing"
J18-1004,K15-1004,1,0.937101,"Missing"
J18-1004,N15-1068,0,0.383383,"chester, Rochester NY 14627. E-mail: xpeng@cs.rochester.edu.s Submission received: 19 December 2016; revised version received: 26 June 2017; accepted for publication: 7 July 2017. doi:10.1162/COLI a 00308 © 2017 Association for Computational Linguistics Published under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics Volume 44, Number 1 a number of extensions of stack-based transition systems to handle non-projective ´ trees (e.g., Attardi 2006; Nivre 2009; Choi and McCallum 2013; Gomez-Rodr´ ıguez and Nivre 2013; Pitler and McDonald 2015). Stack-based transition systems can produce general graphs rather than trees. Perhaps the simplest way to generate graphs is to shift one word at a time onto the stack, and then consider building all possible arcs between each word on the stack and the next word in the buffer. This is essentially the algorithm of Covington (2001), generalized to produce graphs rather than non-projective trees. This algorithm was also cast as a stack-based transition system by Nivre (2008). The algorithm runs in time O(n2 ), and requires the system to discriminate the arcs to be built from a large set of possi"
J18-1004,D14-1048,0,0.0840109,"Missing"
J18-1004,C08-1095,0,0.146218,"s the input order of the tokens, making it possible to produce non-projective trees. The cache parser can then be viewed as a generalization of the two-register transition systems. This is because in a cache parser one can move tokens in and out of the cache repeatedly, as already discussed. This is not possible in a register transition system. It would be interesting then to explore the use of our cache parsers for non-projective dependency grammars. We conclude this section with a discussion of other transition-based systems explicitly designed for graph parsing, as opposed to tree parsing. Sagae and Tsujii (2008) have possibly been the first authors to extend the stack-based transition framework for dependency tree parsing to directed acyclic graphs, with the motivation of representing semantically motivated predicate-argument relations and anaphoric references. This is done by dropping the constraint of a single head per word, and by using post-processing transformations that introduce non-projectivity. Titov et al. (2009) and Henderson et al. (2013) present a transition system for synchronous syntactic-semantic parsing, with the motivation of modeling the syntax/semantic interface. On the semantic s"
J18-1004,N15-1040,0,0.146164,"Missing"
J18-1004,D16-1065,0,0.0117091,"parsing. Unlike traditional stack-based transition parsers that process input strings, 114 Gildea, Satta, and Peng Cache Transition Systems for Graph Parsing this system takes as input a dependency tree and processes its edges using a stack, applying tree-to-graph transformations that produce a directed acyclic graph. Similarly to Sagae and Tsujii (2008), the system presented by Damonte, Cohen, and Satta (2017) extends standard approaches for transition-based dependency parsing to AMR parsing, allowing re-entrancies. Similar extensions of transition-based systems to AMR parsing also appear in Zhou et al. (2016) and Ribeyre, de La Clergerie, and Seddah (2015). All of these approaches are based on the idea of extending the transition inventory of standard transition-based dependency parsing systems in order to produce graph representations. On a theoretical perspective, what is missing from these proposals is a mathematical characterizaton of the set of graphs that can be produced and, with few exceptions, a precise description of the oracle algorithms that are used to produce training data from the gold graphs. Furthermore, all of these proposals still retain the stack and buffer architecture of the"
J18-1005,P13-1023,0,0.134502,"nt of many systems, chiefly string-toAMR parsers like JAMR (Flanigan et al. 2014) and CAMR (Wang, Xue, and Pradhan 2015a,b), as well as many other systems submitted to the AMR Parsing task at SemEval 2016 (May 2016). AMRs have also been used for generation (Flanigan et al. 2016), summarization (Liu et al. 2015), and entity detection and linking (Li et al. 2015; Pan et al. 2015). But the AMR Bank is by no means the only resource of its kind. Others include the ¨ Prague Dependency Treebank (Bohmov´ a et al. 2003), DeepBank (Oepen and Lønning 2006), and Universal Conceptual Cognitive Annotation (Abend and Rappoport 2013). By and large, these resources are based on, or equivalent to, graphs, in which vertices stand for entities and edges stand for semantic relations among them. The Semantic Dependency Parsing task at SemEval 2014 and 2015 (Oepen et al. 2014, 2015) converted several such resources into a unified graph format and invited participants to map from sentences to these semantic graphs. The unification of various kinds of semantic annotation into a single representation, semantic graphs, and the creation of large, broad-coverage collections of these representations are very positive developments for r"
J18-1005,P99-1070,0,0.343387,"Missing"
J18-1005,W13-2322,0,0.530691,"process. This was made possible because of a single representation (phrase structure or dependency trees) that captures all of these phenomena; because of corpora annotated with these representations, like the Penn Treebank (Marcus, Marcinkiewicz, and Santorini 1993); and because of formalisms, like context-free grammars, which can model these representations practically (Charniak 1997; Collins 1997; Petrov et al. 2006). In a similar way, more recent work in semantic processing consolidates various semantics-related tasks into one. For example, the Abstract Meaning Representation (AMR) Bank (Banarescu et al. 2013) began as an effort to unify the various annotation layers of OntoNotes. It has driven the development of many systems, chiefly string-toAMR parsers like JAMR (Flanigan et al. 2014) and CAMR (Wang, Xue, and Pradhan 2015a,b), as well as many other systems submitted to the AMR Parsing task at SemEval 2016 (May 2016). AMRs have also been used for generation (Flanigan et al. 2016), summarization (Liu et al. 2015), and entity detection and linking (Li et al. 2015; Pan et al. 2015). But the AMR Bank is by no means the only resource of its kind. Others include the ¨ Prague Dependency Treebank (Bohmov"
J18-1005,P91-1034,0,0.10154,"Missing"
J18-1005,J99-1004,0,0.0256352,"is concave (Boyd and Vandenberghe 2004), gradient ascent is guaranteed to converge to the unique global maximum. We may also wish to learn a distribution over the DAGs themselves—for example, in order to provide a prior over semantic structures. A natural choice would be to adopt a similar log-linear framework: δ (ρ ) pM (D, ρ ) = X [[M]] (D0 ) D0 where δ(ρ ) is a log-linear combination of weights and per-transition features as before. Here, the normalization ranges over all possible DAGs. For some values of the weight vector, this sum may diverge, as in weighted context-free grammars (CFGs; Chi 1999), meaning that the corresponding probability distribution is not defined. More importantly, estimating the normalization constant is computationally difficult, whereas in the case of weighted CFGs it can be estimated relatively easily with an iterative numerical algorithm (Abney, McAllester, and Pereira 1999; Smith and Johnson 2007). A similar problem arises in Exponential Random Graph Models (Frank and Strauss 1986); the most common solution is to use Markov chain Monte Carlo (MCMC) methods (Snijders 2002). To train a model over DAGs, we can perform gradient ascent on the log likelihood: X LL"
J18-1005,P97-1003,0,0.185191,"parkhi 1996), noun-phrase chunking (Ramshaw and Marcus 1995), prepositional phrase attachment (Collins and Brooks 1995), and so on. As the field matured, these tasks were increasingly synthesized into a single process. This was made possible because of a single representation (phrase structure or dependency trees) that captures all of these phenomena; because of corpora annotated with these representations, like the Penn Treebank (Marcus, Marcinkiewicz, and Santorini 1993); and because of formalisms, like context-free grammars, which can model these representations practically (Charniak 1997; Collins 1997; Petrov et al. 2006). In a similar way, more recent work in semantic processing consolidates various semantics-related tasks into one. For example, the Abstract Meaning Representation (AMR) Bank (Banarescu et al. 2013) began as an effort to unify the various annotation layers of OntoNotes. It has driven the development of many systems, chiefly string-toAMR parsers like JAMR (Flanigan et al. 2014) and CAMR (Wang, Xue, and Pradhan 2015a,b), as well as many other systems submitted to the AMR Parsing task at SemEval 2016 (May 2016). AMRs have also been used for generation (Flanigan et al. 2016),"
J18-1005,W95-0103,0,0.261819,"ther stream aims for broader coverage, and historically tackled shallower, piecemeal tasks, like semantic role labeling (Gildea and Jurafsky 2000), word sense disambiguation (Brown et al. 1991), coreference resolution (Soon, Ng, and Lim 2001), and so on. Correspondingly, resources like OntoNotes (Hovy et al. 2006) provided separate resources for each of these tasks. This piecemeal situation parallels that of early work on syntactic parsing, which focused on subtasks like part-of-speech tagging (Ratnaparkhi 1996), noun-phrase chunking (Ramshaw and Marcus 1995), prepositional phrase attachment (Collins and Brooks 1995), and so on. As the field matured, these tasks were increasingly synthesized into a single process. This was made possible because of a single representation (phrase structure or dependency trees) that captures all of these phenomena; because of corpora annotated with these representations, like the Penn Treebank (Marcus, Marcinkiewicz, and Santorini 1993); and because of formalisms, like context-free grammars, which can model these representations practically (Charniak 1997; Collins 1997; Petrov et al. 2006). In a similar way, more recent work in semantic processing consolidates various seman"
J18-1005,P02-1001,0,0.117651,"iently using the forward-backward algorithm; for DAG automata, the expectation can be computed analogously. Algorithm 2 provides the bottom–up procedure for computing a chart of inside weights. If we compute weights in the derivation forest semiring (Goodman 1999), in which ⊗ creates an “and” node and ⊕ creates an “or” node, the resulting and/or graph has the same structure as a CFG parse forest generated by CKY, so we can simply run the inside-outside algorithm (Lari and Young 1990) on it to obtain the desired expectations. Alternatively, we could compute weights in the expectation semiring (Eisner 2002; Chiang 2012). Because the log-likelihood LL is concave (Boyd and Vandenberghe 2004), gradient ascent is guaranteed to converge to the unique global maximum. We may also wish to learn a distribution over the DAGs themselves—for example, in order to provide a prior over semantic structures. A natural choice would be to adopt a similar log-linear framework: δ (ρ ) pM (D, ρ ) = X [[M]] (D0 ) D0 where δ(ρ ) is a log-linear combination of weights and per-transition features as before. Here, the normalization ranges over all possible DAGs. For some values of the weight vector, this sum may diverge,"
J18-1005,N16-1087,0,0.0374856,"niak 1997; Collins 1997; Petrov et al. 2006). In a similar way, more recent work in semantic processing consolidates various semantics-related tasks into one. For example, the Abstract Meaning Representation (AMR) Bank (Banarescu et al. 2013) began as an effort to unify the various annotation layers of OntoNotes. It has driven the development of many systems, chiefly string-toAMR parsers like JAMR (Flanigan et al. 2014) and CAMR (Wang, Xue, and Pradhan 2015a,b), as well as many other systems submitted to the AMR Parsing task at SemEval 2016 (May 2016). AMRs have also been used for generation (Flanigan et al. 2016), summarization (Liu et al. 2015), and entity detection and linking (Li et al. 2015; Pan et al. 2015). But the AMR Bank is by no means the only resource of its kind. Others include the ¨ Prague Dependency Treebank (Bohmov´ a et al. 2003), DeepBank (Oepen and Lønning 2006), and Universal Conceptual Cognitive Annotation (Abend and Rappoport 2013). By and large, these resources are based on, or equivalent to, graphs, in which vertices stand for entities and edges stand for semantic relations among them. The Semantic Dependency Parsing task at SemEval 2014 and 2015 (Oepen et al. 2014, 2015) conver"
J18-1005,P14-1134,0,0.0479906,"se representations, like the Penn Treebank (Marcus, Marcinkiewicz, and Santorini 1993); and because of formalisms, like context-free grammars, which can model these representations practically (Charniak 1997; Collins 1997; Petrov et al. 2006). In a similar way, more recent work in semantic processing consolidates various semantics-related tasks into one. For example, the Abstract Meaning Representation (AMR) Bank (Banarescu et al. 2013) began as an effort to unify the various annotation layers of OntoNotes. It has driven the development of many systems, chiefly string-toAMR parsers like JAMR (Flanigan et al. 2014) and CAMR (Wang, Xue, and Pradhan 2015a,b), as well as many other systems submitted to the AMR Parsing task at SemEval 2016 (May 2016). AMRs have also been used for generation (Flanigan et al. 2016), summarization (Liu et al. 2015), and entity detection and linking (Li et al. 2015; Pan et al. 2015). But the AMR Bank is by no means the only resource of its kind. Others include the ¨ Prague Dependency Treebank (Bohmov´ a et al. 2003), DeepBank (Oepen and Lønning 2006), and Universal Conceptual Cognitive Annotation (Abend and Rappoport 2013). By and large, these resources are based on, or equival"
J18-1005,P00-1065,1,0.624731,"Y-NC-ND 4.0) license Computational Linguistics Volume 44, Number 1 1. Introduction Statistical models of natural language semantics are making rapid progress. At the risk of oversimplifying, work in this area can be divided into two streams. One stream, semantic parsing (Mooney 2007), aims to map from sentences to logical forms that can be executed (for example, to query a knowledge base); work in this stream tends to be on small, narrow-domain data sets like GeoQuery. The other stream aims for broader coverage, and historically tackled shallower, piecemeal tasks, like semantic role labeling (Gildea and Jurafsky 2000), word sense disambiguation (Brown et al. 1991), coreference resolution (Soon, Ng, and Lim 2001), and so on. Correspondingly, resources like OntoNotes (Hovy et al. 2006) provided separate resources for each of these tasks. This piecemeal situation parallels that of early work on syntactic parsing, which focused on subtasks like part-of-speech tagging (Ratnaparkhi 1996), noun-phrase chunking (Ramshaw and Marcus 1995), prepositional phrase attachment (Collins and Brooks 1995), and so on. As the field matured, these tasks were increasingly synthesized into a single process. This was made possible"
J18-1005,J99-4004,0,0.32362,"i=1 = N X i=1 148 ρ on Di Φ(ρi ) − Eρ|Di [Φ(ρ )]  since ∂δ(ρ ) = δ (ρ ) Φ (ρ ) ∂w (3) Chiang et al. Weighted DAG Automata for Semantic Graphs Unfortunately, we cannot derive a closed-form solution for the zeros of Equation (3). We therefore use gradient ascent. In CRF training for finite automata, the expectation in Equation (3) is computed efficiently using the forward-backward algorithm; for DAG automata, the expectation can be computed analogously. Algorithm 2 provides the bottom–up procedure for computing a chart of inside weights. If we compute weights in the derivation forest semiring (Goodman 1999), in which ⊗ creates an “and” node and ⊕ creates an “or” node, the resulting and/or graph has the same structure as a CFG parse forest generated by CKY, so we can simply run the inside-outside algorithm (Lari and Young 1990) on it to obtain the desired expectations. Alternatively, we could compute weights in the expectation semiring (Eisner 2002; Chiang 2012). Because the log-likelihood LL is concave (Boyd and Vandenberghe 2004), gradient ascent is guaranteed to converge to the unique global maximum. We may also wish to learn a distribution over the DAGs themselves—for example, in order to pro"
J18-1005,N06-2015,0,0.0282756,"rsimplifying, work in this area can be divided into two streams. One stream, semantic parsing (Mooney 2007), aims to map from sentences to logical forms that can be executed (for example, to query a knowledge base); work in this stream tends to be on small, narrow-domain data sets like GeoQuery. The other stream aims for broader coverage, and historically tackled shallower, piecemeal tasks, like semantic role labeling (Gildea and Jurafsky 2000), word sense disambiguation (Brown et al. 1991), coreference resolution (Soon, Ng, and Lim 2001), and so on. Correspondingly, resources like OntoNotes (Hovy et al. 2006) provided separate resources for each of these tasks. This piecemeal situation parallels that of early work on syntactic parsing, which focused on subtasks like part-of-speech tagging (Ratnaparkhi 1996), noun-phrase chunking (Ramshaw and Marcus 1995), prepositional phrase attachment (Collins and Brooks 1995), and so on. As the field matured, these tasks were increasingly synthesized into a single process. This was made possible because of a single representation (phrase structure or dependency trees) that captures all of these phenomena; because of corpora annotated with these representations,"
J18-1005,P99-1069,0,0.188893,"d Let w ∈ R be a vector of feature weights, which are the parameters to be estimated. Then we can parameterize δ in terms of the features and feature weights: δ(t) = exp w · Φ(t) so that δ(ρ ) = exp w · Φ(ρ ) X exp w · Φ(ρ ) [[M]] (D) = run ρ on D To obtain a probability model of runs of M on D, we simply renormalize the run weights: δ (ρ ) [[M]] (D) Assume a set of training examples {(Di , ρi ) |1 ≤ i ≤ N}, where each example consists of a DAG Di and an associated run ρi . We can train the model by analogy with conditional random fields (CRFs), which are log-linear models on finite automata (Johnson et al. 1999; Lafferty, McCallum, and Pereira 2001). The training procedure is essentially gradient ascent on the log-likelihood, which is pM (ρ |D) = LL = N X log pM (ρi |Di ) i=1 = N X log δ(ρi ) − log [[M]] (Di )  i=1 The gradient of LL is: ∂LL = ∂w N  X i=1  ∂ log δ(ρ ) − ∂ log [[M]] (D ) i i ∂w ∂w N  X  1 1 ∂ ∂ = δ(ρi ) − [[M]] (Di ) δ(ρi ) ∂w [[M]] (Di ) ∂w i=1   N X X 1 ∂ δ ( ρ )  1 ∂ δ(ρi ) − = ∂w δ(ρi ) ∂w [[M]] (Di ) ρ on Di i=1   N X X δ ( ρ ) Φ(ρi ) − = Φ ( ρ ) [[M]] (Di ) i=1 = N X i=1 148 ρ on Di Φ(ρi ) − Eρ|Di [Φ(ρ )]  since ∂δ(ρ ) = δ (ρ ) Φ (ρ ) ∂w (3) Chiang et al. Weighted"
J18-1005,W15-4502,0,0.0159569,"ic processing consolidates various semantics-related tasks into one. For example, the Abstract Meaning Representation (AMR) Bank (Banarescu et al. 2013) began as an effort to unify the various annotation layers of OntoNotes. It has driven the development of many systems, chiefly string-toAMR parsers like JAMR (Flanigan et al. 2014) and CAMR (Wang, Xue, and Pradhan 2015a,b), as well as many other systems submitted to the AMR Parsing task at SemEval 2016 (May 2016). AMRs have also been used for generation (Flanigan et al. 2016), summarization (Liu et al. 2015), and entity detection and linking (Li et al. 2015; Pan et al. 2015). But the AMR Bank is by no means the only resource of its kind. Others include the ¨ Prague Dependency Treebank (Bohmov´ a et al. 2003), DeepBank (Oepen and Lønning 2006), and Universal Conceptual Cognitive Annotation (Abend and Rappoport 2013). By and large, these resources are based on, or equivalent to, graphs, in which vertices stand for entities and edges stand for semantic relations among them. The Semantic Dependency Parsing task at SemEval 2014 and 2015 (Oepen et al. 2014, 2015) converted several such resources into a unified graph format and invited participants to"
J18-1005,N15-1114,0,0.0541523,"Missing"
J18-1005,J93-2004,0,0.0616988,"Missing"
J18-1005,S16-1166,0,0.0130652,"ch can model these representations practically (Charniak 1997; Collins 1997; Petrov et al. 2006). In a similar way, more recent work in semantic processing consolidates various semantics-related tasks into one. For example, the Abstract Meaning Representation (AMR) Bank (Banarescu et al. 2013) began as an effort to unify the various annotation layers of OntoNotes. It has driven the development of many systems, chiefly string-toAMR parsers like JAMR (Flanigan et al. 2014) and CAMR (Wang, Xue, and Pradhan 2015a,b), as well as many other systems submitted to the AMR Parsing task at SemEval 2016 (May 2016). AMRs have also been used for generation (Flanigan et al. 2016), summarization (Liu et al. 2015), and entity detection and linking (Li et al. 2015; Pan et al. 2015). But the AMR Bank is by no means the only resource of its kind. Others include the ¨ Prague Dependency Treebank (Bohmov´ a et al. 2003), DeepBank (Oepen and Lønning 2006), and Universal Conceptual Cognitive Annotation (Abend and Rappoport 2013). By and large, these resources are based on, or equivalent to, graphs, in which vertices stand for entities and edges stand for semantic relations among them. The Semantic Dependency Parsin"
J18-1005,S15-2153,0,0.0866557,"Missing"
J18-1005,S14-2008,0,0.118213,"neration (Flanigan et al. 2016), summarization (Liu et al. 2015), and entity detection and linking (Li et al. 2015; Pan et al. 2015). But the AMR Bank is by no means the only resource of its kind. Others include the ¨ Prague Dependency Treebank (Bohmov´ a et al. 2003), DeepBank (Oepen and Lønning 2006), and Universal Conceptual Cognitive Annotation (Abend and Rappoport 2013). By and large, these resources are based on, or equivalent to, graphs, in which vertices stand for entities and edges stand for semantic relations among them. The Semantic Dependency Parsing task at SemEval 2014 and 2015 (Oepen et al. 2014, 2015) converted several such resources into a unified graph format and invited participants to map from sentences to these semantic graphs. The unification of various kinds of semantic annotation into a single representation, semantic graphs, and the creation of large, broad-coverage collections of these representations are very positive developments for research in semantic processing. What is still missing—in our view—is a formal framework for creating, combining, and using models involving graphs that parallels those for strings and trees. Finite string automata and transducers served as"
J18-1005,N15-1119,0,0.0128014,"nsolidates various semantics-related tasks into one. For example, the Abstract Meaning Representation (AMR) Bank (Banarescu et al. 2013) began as an effort to unify the various annotation layers of OntoNotes. It has driven the development of many systems, chiefly string-toAMR parsers like JAMR (Flanigan et al. 2014) and CAMR (Wang, Xue, and Pradhan 2015a,b), as well as many other systems submitted to the AMR Parsing task at SemEval 2016 (May 2016). AMRs have also been used for generation (Flanigan et al. 2016), summarization (Liu et al. 2015), and entity detection and linking (Li et al. 2015; Pan et al. 2015). But the AMR Bank is by no means the only resource of its kind. Others include the ¨ Prague Dependency Treebank (Bohmov´ a et al. 2003), DeepBank (Oepen and Lønning 2006), and Universal Conceptual Cognitive Annotation (Abend and Rappoport 2013). By and large, these resources are based on, or equivalent to, graphs, in which vertices stand for entities and edges stand for semantic relations among them. The Semantic Dependency Parsing task at SemEval 2014 and 2015 (Oepen et al. 2014, 2015) converted several such resources into a unified graph format and invited participants to map from sentences"
J18-1005,K15-1004,1,0.920031,"Missing"
J18-1005,P06-1055,0,0.0700356,"noun-phrase chunking (Ramshaw and Marcus 1995), prepositional phrase attachment (Collins and Brooks 1995), and so on. As the field matured, these tasks were increasingly synthesized into a single process. This was made possible because of a single representation (phrase structure or dependency trees) that captures all of these phenomena; because of corpora annotated with these representations, like the Penn Treebank (Marcus, Marcinkiewicz, and Santorini 1993); and because of formalisms, like context-free grammars, which can model these representations practically (Charniak 1997; Collins 1997; Petrov et al. 2006). In a similar way, more recent work in semantic processing consolidates various semantics-related tasks into one. For example, the Abstract Meaning Representation (AMR) Bank (Banarescu et al. 2013) began as an effort to unify the various annotation layers of OntoNotes. It has driven the development of many systems, chiefly string-toAMR parsers like JAMR (Flanigan et al. 2014) and CAMR (Wang, Xue, and Pradhan 2015a,b), as well as many other systems submitted to the AMR Parsing task at SemEval 2016 (May 2016). AMRs have also been used for generation (Flanigan et al. 2016), summarization (Liu et"
J18-1005,W12-4209,0,0.402951,"lack a similar framework for learning and inferring semantic representations. Two such formalisms have recently been proposed for NLP: one is hyperedge replacement graph grammars, or HRGs (Bauderon and Courcelle 1987; Habel and Kreowski 1987; Habel 1992; Drewes, Kreowski, and Habel 1997), applied to AMR 120 Chiang et al. Weighted DAG Automata for Semantic Graphs ¨ parsing by various authors (Chiang et al. 2013; Peng, Song, and Gildea 2015; Bjorklund, Drewes, and Ericson 2016). The other formalism is directed acyclic graph (DAG) automata, defined by Kamimura and Slutzki (1981) and extended by Quernheim and Knight (2012). In this article, we study DAG automata in depth, with the goal of enabling efficient algorithms for natural language processing applications. After some background on the use of graph-based representations in natural language processing in Section 2, we define our variant of DAG automata in Section 3. We then show the following properties of our formalism: r r r Path languages are regular, as is desirable for a formal model of AMRs (Section 4.1). The class of hyperedge-replacement languages is closed under intersection with languages recognized by DAG automata (Section 4.2). Emptiness is dec"
J18-1005,W95-0107,0,0.44496,"o be on small, narrow-domain data sets like GeoQuery. The other stream aims for broader coverage, and historically tackled shallower, piecemeal tasks, like semantic role labeling (Gildea and Jurafsky 2000), word sense disambiguation (Brown et al. 1991), coreference resolution (Soon, Ng, and Lim 2001), and so on. Correspondingly, resources like OntoNotes (Hovy et al. 2006) provided separate resources for each of these tasks. This piecemeal situation parallels that of early work on syntactic parsing, which focused on subtasks like part-of-speech tagging (Ratnaparkhi 1996), noun-phrase chunking (Ramshaw and Marcus 1995), prepositional phrase attachment (Collins and Brooks 1995), and so on. As the field matured, these tasks were increasingly synthesized into a single process. This was made possible because of a single representation (phrase structure or dependency trees) that captures all of these phenomena; because of corpora annotated with these representations, like the Penn Treebank (Marcus, Marcinkiewicz, and Santorini 1993); and because of formalisms, like context-free grammars, which can model these representations practically (Charniak 1997; Collins 1997; Petrov et al. 2006). In a similar way, more re"
J18-1005,W96-0213,0,0.301856,"wledge base); work in this stream tends to be on small, narrow-domain data sets like GeoQuery. The other stream aims for broader coverage, and historically tackled shallower, piecemeal tasks, like semantic role labeling (Gildea and Jurafsky 2000), word sense disambiguation (Brown et al. 1991), coreference resolution (Soon, Ng, and Lim 2001), and so on. Correspondingly, resources like OntoNotes (Hovy et al. 2006) provided separate resources for each of these tasks. This piecemeal situation parallels that of early work on syntactic parsing, which focused on subtasks like part-of-speech tagging (Ratnaparkhi 1996), noun-phrase chunking (Ramshaw and Marcus 1995), prepositional phrase attachment (Collins and Brooks 1995), and so on. As the field matured, these tasks were increasingly synthesized into a single process. This was made possible because of a single representation (phrase structure or dependency trees) that captures all of these phenomena; because of corpora annotated with these representations, like the Penn Treebank (Marcus, Marcinkiewicz, and Santorini 1993); and because of formalisms, like context-free grammars, which can model these representations practically (Charniak 1997; Collins 1997"
J18-1005,J07-4003,0,0.0362617,"pM (D, ρ ) = X [[M]] (D0 ) D0 where δ(ρ ) is a log-linear combination of weights and per-transition features as before. Here, the normalization ranges over all possible DAGs. For some values of the weight vector, this sum may diverge, as in weighted context-free grammars (CFGs; Chi 1999), meaning that the corresponding probability distribution is not defined. More importantly, estimating the normalization constant is computationally difficult, whereas in the case of weighted CFGs it can be estimated relatively easily with an iterative numerical algorithm (Abney, McAllester, and Pereira 1999; Smith and Johnson 2007). A similar problem arises in Exponential Random Graph Models (Frank and Strauss 1986); the most common solution is to use Markov chain Monte Carlo (MCMC) methods (Snijders 2002). To train a model over DAGs, we can perform gradient ascent on the log likelihood: X LL = log pM (ρi , Di ) i ∂LL = ∂w X Φ(ρi ) − ED0 ,ρ [Φ(ρ )] i by using MCMC to estimate the second expectation. Finally, we may wish to learn a distribution over DAGs by learning the states in an unsupervised manner, either because it is not practical to annotate states by hand, or because we wish to automatically find the set of stat"
J18-1005,J01-4004,0,0.292972,"Missing"
J18-1005,P87-1015,0,0.781806,"Missing"
J18-1005,P15-2141,0,0.0234808,"Missing"
J18-1005,N15-1040,0,0.0381581,"Missing"
J18-3006,P17-1042,0,0.0631219,"Missing"
J18-3006,N10-1083,0,0.0763564,"Missing"
J18-3006,J93-2003,0,0.0817864,"Missing"
J18-3006,P93-1001,0,0.498419,"ised learning tasks, such as unsupervised part-of-speech induction (Haghighi and Klein 2006; Berg-Kirkpatrick et al. 2010), word alignment (Dyer et al. 2011; Ammar, Dyer, and Smith 2014), and grammar induction (Berg-Kirkpatrick et al. 2010). Many pairs of related languages share vocabulary or grammatical structure due to borrowing or inheritance: the English aquatic and Spanish agua share the Latin root aqua, and the English beige was borrowed from French. As a result, orthographic features provide crucial information for determining word-level translations for closely related language pairs. Church (1993) leveraged orthographic similarity for character alignment. Haghighi, Berg-Kirkpatrick, and Klein (2008) proposed a generative model for inducing a bilingual lexicon from monolingual text by exploiting orthographic and contextual similarities among the words in two different languages. The model proposed by Haghighi et al. learns a one-to-one mapping between the words in two languages by analyzing type-level features only, while ignoring the token-level n-gram frequencies. We propose a decipherment model that unifies the type-level feature-based approach of Haghighi et al. with token-level EM-"
J18-3006,D12-1025,0,0.0229234,"en the same as the concatenation of the translations of individual unigrams (consider the shared use of postnominal adjectives in the French maison bleu and Spanish casa azul). Although this certainly is not always true, we assume that it is common enough to provide a useful signal. The goal of decipherment algorithms is to leverage such statistical similarities across languages, and effectively learn word-level translation probabilities from monolingual data. Existing decipherment methods are predominantly based on probabilistic generative models (Koehn and Knight 2000; Ravi and Knight 2011; Dou and Knight 2012; Nuhn and Ney 2014). These models primarily focus on the statistical similarities between the n-gram frequencies in the source and the target language, and rely on the expectation maximization (EM) algorithm (Dempster, Laird, and Rubin 1977) or its faster approximations. However, there can be many other types of statistical and linguistic similarities across languages beyond n-gram frequencies (similarities in spelling, word-length distribution, syntactic structure, etc.). Unfortunately, existing generative models do not allow incorporating such a wide range of linguistically motivated featur"
J18-3006,D14-1061,0,0.603217,"Missing"
J18-3006,P11-1042,0,0.0134706,"ons. However, there can be many other types of statistical and linguistic similarities across languages beyond n-gram frequencies (similarities in spelling, word-length distribution, syntactic structure, etc.). Unfortunately, existing generative models do not allow incorporating such a wide range of linguistically motivated features. Previous research has shown the effectiveness of incorporating linguistically motivated features for many different unsupervised learning tasks, such as unsupervised part-of-speech induction (Haghighi and Klein 2006; Berg-Kirkpatrick et al. 2010), word alignment (Dyer et al. 2011; Ammar, Dyer, and Smith 2014), and grammar induction (Berg-Kirkpatrick et al. 2010). Many pairs of related languages share vocabulary or grammatical structure due to borrowing or inheritance: the English aquatic and Spanish agua share the Latin root aqua, and the English beige was borrowed from French. As a result, orthographic features provide crucial information for determining word-level translations for closely related language pairs. Church (1993) leveraged orthographic similarity for character alignment. Haghighi, Berg-Kirkpatrick, and Klein (2008) proposed a generative model for induci"
J18-3006,P08-1088,0,0.124648,"Missing"
J18-3006,N06-1041,0,0.0340667,"Missing"
J18-3006,W99-0906,0,0.295536,"Missing"
J18-3006,P14-2123,0,0.0142172,"ncatenation of the translations of individual unigrams (consider the shared use of postnominal adjectives in the French maison bleu and Spanish casa azul). Although this certainly is not always true, we assume that it is common enough to provide a useful signal. The goal of decipherment algorithms is to leverage such statistical similarities across languages, and effectively learn word-level translation probabilities from monolingual data. Existing decipherment methods are predominantly based on probabilistic generative models (Koehn and Knight 2000; Ravi and Knight 2011; Dou and Knight 2012; Nuhn and Ney 2014). These models primarily focus on the statistical similarities between the n-gram frequencies in the source and the target language, and rely on the expectation maximization (EM) algorithm (Dempster, Laird, and Rubin 1977) or its faster approximations. However, there can be many other types of statistical and linguistic similarities across languages beyond n-gram frequencies (similarities in spelling, word-length distribution, syntactic structure, etc.). Unfortunately, existing generative models do not allow incorporating such a wide range of linguistically motivated features. Previous researc"
J18-3006,P13-1154,0,0.046593,"Missing"
J18-3006,P15-2090,0,0.110909,"Missing"
J18-3006,P95-1050,0,0.514478,"gram frequencies. We propose a decipherment model that unifies the type-level feature-based approach of Haghighi et al. with token-level EM-based approaches such as Koehn and Knight (2000) and Ravi and Knight (2011). In addition to orthographic similarity, we also often observe similarity in the distribution of word lengths across different languages. Linguists have long noted the 526 Naim, Riley, and Gildea Feature-Based Decipherment for Machine Translation relationship between word frequency and length (Zipf 1949), so the tendency of words and their translations to have similar frequencies (Rapp 1995) may apply to length as well. Our feature-rich log-linear model can easily incorporate such length-based similarity features. One of the key challenges with the proposed latent variable log-linear model is the high computational complexity of training, as it requires normalizing globally via summing over all possible observations and latent variables. As a result, an exact implementation is impractical even for the moderate vocabulary size of most lowresource languages. To address this challenge, we perform approximate inference using Markov chain Monte Carlo (MCMC) sampling for scalable train"
J18-3006,P13-1036,0,0.0342314,"Missing"
J18-3006,P11-1002,0,0.664298,"ir monolingual corpora tend to be small. However, these monolingual corpora can often be downloaded from the Internet, and are much easier to obtain or produce than parallel corpora. Leveraging useful information from monolingual corpora can be extremely helpful for learning translation models for low- and no-resource language pairs. Decipherment algorithms (so-called because of the assumption that one language is a cipher for the other) aim to exploit such monolingual corpora in order to learn translation model parameters, when parallel data are limited or unavailable (Koehn and Knight 2000; Ravi and Knight 2011; Dou, Vaswani, and Knight 2014). The key intuition is that similar words and n-grams tend to have similar distributional properties across languages. For example, if a bigram appears frequently in the monolingual source corpus, its translation is likely to appear frequently in the monolingual target corpus, and vice versa. This is especially true when the corpora share similar topics and context. Furthermore, for many such language pairs, we observe similar monotonic word ordering—that is, the translation of a bigram is often the same as the concatenation of the translations of individual uni"
J18-3006,P10-1107,0,0.0686784,"Missing"
J18-3006,P17-1179,0,0.0564945,"Missing"
J18-3006,D16-1163,0,0.0336483,"parallel data, the accuracy of standard word alignment algorithms drops significantly. This is also true of supervised neural methods: Even with hundreds of thousands of parallel training sentences, neural methods only achieve modest results Submission received: 9 October 2017; revised version received: 16 March 2018; accepted for publication: 15 May 2018. doi:10.1162/COLI a 00326 © 2018 Association for Computational Linguistics Published under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics Volume 44, Number 3 (Zoph et al. 2016). Low- and no-resource languages generally do not have parallel corpora, and even their monolingual corpora tend to be small. However, these monolingual corpora can often be downloaded from the Internet, and are much easier to obtain or produce than parallel corpora. Leveraging useful information from monolingual corpora can be extremely helpful for learning translation models for low- and no-resource language pairs. Decipherment algorithms (so-called because of the assumption that one language is a cipher for the other) aim to exploit such monolingual corpora in order to learn translation mod"
J18-3006,P91-1022,0,\N,Missing
J18-3006,J98-4003,0,\N,Missing
J19-2005,W13-2322,0,0.0236108,"ho introduce a number of heuristics to find tree decompositions of semantic graphs of natural language sentences. They extract corresponding HRG rules and analyze the characteristics of the resulting grammars. Unlike the methods we present in this article, the methods of Jones, Goldwater, and Johnson do not refer to the natural language string corresponding to the semantic graph, and therefore are not applicable to string-to-graph parsing. In this article, we experiment with semantic data sets annotated according to the linguistic formalism called Abstract Meaning Representation, described by Banarescu et al. (2013). A general discussion of alternative formalisms and linguistic graph banks can be found in Kuhlmann and Oepen (2016). 2. Tree Decompositions and Parse Trees The graphs that we use in this article have directed arcs, because this is a standard requirement for semantic representation of natural language sentences. We denote a directed graph as G = (V, E), where V is the set of vertices and E is the set of edges—that is, ordered pairs of the form (v, u) with v, u ∈ V. A tree decomposition of G is a special tree where each node is associated with a subset of V. Because a tree is a particular kind"
J19-2005,P13-1091,0,0.027257,"luated on the task of extracting semantic dependency graphs from text. Considering the already mentioned graph parsing problem, where the input consists of a graph and a graph-rewriting grammar, one of the first algorithms for parsing based on HRG has been proposed by Lautemann (1990). For general HRGs, this algorithm runs in exponential time. This complexity result comes as no surprise, since it is known that graph parsing for HRG is an NP-hard problem, even for fixed grammars (Aalbersberg, Rozenberg, and Ehrenfeucht 1986; Lange and Welzl 1987). In the context of natural language processing, Chiang et al. (2013) proposed an optimized version of Lautemann, also providing a fine-grained complexity analysis that is missing in the original article. The running time of the optimized algorithm is an exponential function of both the treewidth of the input graph (to be defined in Section 2) and of the maximum degree of its nodes. Polynomial time parsing algorithms for subclasses of HRG have also been investigated in the graph grammar literature. A predictive top–down parsing algorithm has been presented by Drewes, Hoffmann, and Minas (2015), inspired by the LL(1) parsing method for CFGs, and working for a re"
J19-2005,P96-1025,0,0.0429396,", and h in a partial analysis can range from 0 to n, the length of w, for a total number of partial analyses in O(n3 ). Furthermore, the left/right tree attachment operations can be implemented in constant time for fixed values of i, k, j, h, and h0 . Then the overall time used by the algorithm when combining trees is in O(n5 ). We conclude that our parsing algorithm runs in polynomial time and space. What we have briefly summarized here is a well-known adaptation of the standard Cocke-Kasami-Younger (CKY) algorithm for parsing using CFGs (Aho and Ullman 1972), as applied to lexicalized CFGs (Collins 1996) and to dependency grammars (Eisner 1996). Let us now turn to string-to-graph parsing. When applying dynamic programming, we can exploit ideas similar to those presented for the string-to-tree case, but we need to make some generalizations. The input to the algorithm is now a sequence of h (a) j i [i, j; h] h h h0 ⇒ (b) i k [i, k; h0 ] j k [k, j; h] j i [i, j; h] Figure 1 Graphical representation for (a) the partial analysis [i, j; h] and (b) the combination of partial analyses [i, k; h0 ] and [k, j; h], implementing left attachment and producing partial analysis [i, j; h]. 342 Gildea, Satta,"
J19-2005,E17-1051,1,0.89149,"Missing"
J19-2005,C96-1058,0,0.0189606,"om 0 to n, the length of w, for a total number of partial analyses in O(n3 ). Furthermore, the left/right tree attachment operations can be implemented in constant time for fixed values of i, k, j, h, and h0 . Then the overall time used by the algorithm when combining trees is in O(n5 ). We conclude that our parsing algorithm runs in polynomial time and space. What we have briefly summarized here is a well-known adaptation of the standard Cocke-Kasami-Younger (CKY) algorithm for parsing using CFGs (Aho and Ullman 1972), as applied to lexicalized CFGs (Collins 1996) and to dependency grammars (Eisner 1996). Let us now turn to string-to-graph parsing. When applying dynamic programming, we can exploit ideas similar to those presented for the string-to-tree case, but we need to make some generalizations. The input to the algorithm is now a sequence of h (a) j i [i, j; h] h h h0 ⇒ (b) i k [i, k; h0 ] j k [k, j; h] j i [i, j; h] Figure 1 Graphical representation for (a) the partial analysis [i, j; h] and (b) the combination of partial analyses [i, k; h0 ] and [k, j; h], implementing left attachment and producing partial analysis [i, j; h]. 342 Gildea, Satta, and Peng Ordered Tree Decomposition for H"
J19-2005,P14-1134,0,0.02064,"tices at the left and right boundaries of the span of the subgraph itself. This condition rules out crossing arcs and is only possible because of the projective restriction on the processed graphs. If we ignore the fact that Kuhlmann and Johnsson and Schluter use a grammarless approach, the parsing algorithm we present in this article can be viewed as a generalization of the work by those authors, since we relax the condition d = 2 and we do not impose any restriction on the position of the attachment vertices. Other grammarless algorithms for dependency semantic parsing have been reported by Flanigan et al. (2014), who use maximum weight spanning techniques, and by Damonte, Cohen, and Satta (2017) and Gildea, Satta, and Peng (2018), who use special transition systems combined with greedy methods. 347 Computational Linguistics Volume 45, Number 2 The connection between tree decomposition of a graph and HRG rules, which we use in this article, was first made by Lautemann (1988). In the context of natural language processing, the same idea has been previously exploited by Jones, Goldwater, and Johnson (2013), who introduce a number of heuristics to find tree decompositions of semantic graphs of natural la"
J19-2005,J18-1004,1,0.801701,"Missing"
J19-2005,J76-4004,0,0.590919,"s investigating string-to-graph parsing. This article provides a contribution in this direction. 340 Gildea, Satta, and Peng Ordered Tree Decomposition for HRG Rule Extraction String-to-tree parsing is rather well understood. If we assume that when generating a string our trees do not have crossing arcs,1 then string-to-tree parsing can be efficiently solved using dynamic programming algorithms, also called chart-based methods or tabular methods in the natural language processing community. For an introduction to dynamic programming methods for string-to-tree parsing, we refer the reader to ¨ Graham and Harrison (1976) and Nederhof and Satta (2004) for CFGs, and to Kubler, McDonald, and Nivre (2009, Chapter 5) for projective dependency grammars. Dynamic programming is perhaps the most general way to attack string-to-tree parsing, and other alternative methods such as greedy algorithms or beam search algorithms can be derived as special cases of dynamic programming, as discussed by Huang and Sagae ´ (2010) and Kuhlmann, Gomez-Rodr´ ıguez, and Satta (2011). In most cases of interest, dynamic programming provides polynomial space and time solutions for non-crossing string-to-tree parsing as well as for unsuper"
J19-2005,P10-1110,0,0.0878263,"Missing"
J19-2005,P16-1025,0,0.0276898,"emantic role labeling, and co-reference resolution), we now witness the attempt to incorporate all these different aspects into a single process of semantic parsing. This trend is attested to by the wide variety of semantic representations that are being proposed (Kuhlmann and Oepen 2016) and by the number of semantically annotated data sets being produced and distributed. As a follow-up, semantic parsing is now being exploited in machine translation (Jones et al. 2012), text summarization (Liu et al. 2015; Dohare and Karnick 2017), sentence compression (Takase et al. 2016), event extraction (Huang et al. 2016; Rao et al. 2017; Wang et al. 2017), and question answering (Khashabi et al. 2018). It seems therefore that the field is returning to the task of semantic parsing after a long hiatus, since semantic analysis was already part of the agenda in formalisms such as head-driven phrase structure grammars in the era when grammars were generally manually developed rather than learned from data. Crucially, semantic representations are graph structures, as opposed to tree structures. There is therefore a need for formal grammars that generate graphs. Graph grammars have been investigated in the formal l"
J19-2005,C12-1083,0,0.222335,"cal natural language processing, semantic analysis had been mainly investigated by means of separated tasks (such as named entity recognition, semantic role labeling, and co-reference resolution), we now witness the attempt to incorporate all these different aspects into a single process of semantic parsing. This trend is attested to by the wide variety of semantic representations that are being proposed (Kuhlmann and Oepen 2016) and by the number of semantically annotated data sets being produced and distributed. As a follow-up, semantic parsing is now being exploited in machine translation (Jones et al. 2012), text summarization (Liu et al. 2015; Dohare and Karnick 2017), sentence compression (Takase et al. 2016), event extraction (Huang et al. 2016; Rao et al. 2017; Wang et al. 2017), and question answering (Khashabi et al. 2018). It seems therefore that the field is returning to the task of semantic parsing after a long hiatus, since semantic analysis was already part of the agenda in formalisms such as head-driven phrase structure grammars in the era when grammars were generally manually developed rather than learned from data. Crucially, semantic representations are graph structures, as oppose"
J19-2005,W13-1810,0,0.0603293,"Missing"
J19-2005,P04-1061,0,0.0294878,"Missing"
J19-2005,P11-1068,1,0.813397,"Missing"
J19-2005,Q15-1040,0,0.0219803,"g, meaning that in the definition of the former problem the search space need not be defined by a generative grammar. Dependency semantic parsing has been a prominent shared task at recent editions of the International Workshops on Semantic Evaluation (SemEval). See Oepen et al. (2014) and Oepen et al. (2015) for a quick overview of the different approaches that have been evaluated. Two grammarless algorithms for dependency semantic parsing are worth discussing here, because they are related to the dynamic programming approach that we use in Section 5 to solve string-to-graph parsing for HRG. Kuhlmann and Jonsson (2015) and Schluter (2015) have independently derived essentially the same algorithm for finding a maximum weight projective directed acyclic graph, given an input vertex sequence w and using a first-order model (a model using only first order patterns). The term projective is a generalization to graphs of the same concept as used in dependency trees: Informally, a projective graph does not have two arcs that cross each other, with respect to the ordering in w. The algorithm is based on the already mentioned CKY algorithm for text-to-string parsing under a CFG (Aho and Ullman 1972). It explores proj"
J19-2005,J16-4009,0,0.0786897,"tention toward semantic analysis of sentences with the conviction that, along with syntactic information, semantic information will greatly improve end-user applications. Whereas, in statistical natural language processing, semantic analysis had been mainly investigated by means of separated tasks (such as named entity recognition, semantic role labeling, and co-reference resolution), we now witness the attempt to incorporate all these different aspects into a single process of semantic parsing. This trend is attested to by the wide variety of semantic representations that are being proposed (Kuhlmann and Oepen 2016) and by the number of semantically annotated data sets being produced and distributed. As a follow-up, semantic parsing is now being exploited in machine translation (Jones et al. 2012), text summarization (Liu et al. 2015; Dohare and Karnick 2017), sentence compression (Takase et al. 2016), event extraction (Huang et al. 2016; Rao et al. 2017; Wang et al. 2017), and question answering (Khashabi et al. 2018). It seems therefore that the field is returning to the task of semantic parsing after a long hiatus, since semantic analysis was already part of the agenda in formalisms such as head-drive"
J19-2005,N15-1114,0,0.0672862,"Missing"
J19-2005,S16-1166,0,0.037782,"Missing"
J19-2005,S15-2153,0,0.0288086,"The weight of a graph G is then computed by summing up the weight of each occurrence of a pattern in G, where different occurrences might overlap in case of patterns of order higher than 1. In this respect, we may therefore view semantic dependency parsing as the grammarless version of string-to-graph parsing, meaning that in the definition of the former problem the search space need not be defined by a generative grammar. Dependency semantic parsing has been a prominent shared task at recent editions of the International Workshops on Semantic Evaluation (SemEval). See Oepen et al. (2014) and Oepen et al. (2015) for a quick overview of the different approaches that have been evaluated. Two grammarless algorithms for dependency semantic parsing are worth discussing here, because they are related to the dynamic programming approach that we use in Section 5 to solve string-to-graph parsing for HRG. Kuhlmann and Jonsson (2015) and Schluter (2015) have independently derived essentially the same algorithm for finding a maximum weight projective directed acyclic graph, given an input vertex sequence w and using a first-order model (a model using only first order patterns). The term projective is a generaliz"
J19-2005,S14-2008,0,0.0247619,"d a k-th order pattern. The weight of a graph G is then computed by summing up the weight of each occurrence of a pattern in G, where different occurrences might overlap in case of patterns of order higher than 1. In this respect, we may therefore view semantic dependency parsing as the grammarless version of string-to-graph parsing, meaning that in the definition of the former problem the search space need not be defined by a generative grammar. Dependency semantic parsing has been a prominent shared task at recent editions of the International Workshops on Semantic Evaluation (SemEval). See Oepen et al. (2014) and Oepen et al. (2015) for a quick overview of the different approaches that have been evaluated. Two grammarless algorithms for dependency semantic parsing are worth discussing here, because they are related to the dynamic programming approach that we use in Section 5 to solve string-to-graph parsing for HRG. Kuhlmann and Jonsson (2015) and Schluter (2015) have independently derived essentially the same algorithm for finding a maximum weight projective directed acyclic graph, given an input vertex sequence w and using a first-order model (a model using only first order patterns). The term p"
J19-2005,K15-1004,1,0.892111,"Missing"
J19-2005,P18-1171,1,0.86951,"Missing"
J19-2005,D14-1048,0,0.046467,"Missing"
J19-2005,W17-2315,0,0.0284204,"Missing"
J19-2005,S15-1031,0,0.0233415,"n of the former problem the search space need not be defined by a generative grammar. Dependency semantic parsing has been a prominent shared task at recent editions of the International Workshops on Semantic Evaluation (SemEval). See Oepen et al. (2014) and Oepen et al. (2015) for a quick overview of the different approaches that have been evaluated. Two grammarless algorithms for dependency semantic parsing are worth discussing here, because they are related to the dynamic programming approach that we use in Section 5 to solve string-to-graph parsing for HRG. Kuhlmann and Jonsson (2015) and Schluter (2015) have independently derived essentially the same algorithm for finding a maximum weight projective directed acyclic graph, given an input vertex sequence w and using a first-order model (a model using only first order patterns). The term projective is a generalization to graphs of the same concept as used in dependency trees: Informally, a projective graph does not have two arcs that cross each other, with respect to the ordering in w. The algorithm is based on the already mentioned CKY algorithm for text-to-string parsing under a CFG (Aho and Ullman 1972). It explores projective graphs whose"
J19-2005,D16-1112,0,0.0210111,"ks (such as named entity recognition, semantic role labeling, and co-reference resolution), we now witness the attempt to incorporate all these different aspects into a single process of semantic parsing. This trend is attested to by the wide variety of semantic representations that are being proposed (Kuhlmann and Oepen 2016) and by the number of semantically annotated data sets being produced and distributed. As a follow-up, semantic parsing is now being exploited in machine translation (Jones et al. 2012), text summarization (Liu et al. 2015; Dohare and Karnick 2017), sentence compression (Takase et al. 2016), event extraction (Huang et al. 2016; Rao et al. 2017; Wang et al. 2017), and question answering (Khashabi et al. 2018). It seems therefore that the field is returning to the task of semantic parsing after a long hiatus, since semantic analysis was already part of the agenda in formalisms such as head-driven phrase structure grammars in the era when grammars were generally manually developed rather than learned from data. Crucially, semantic representations are graph structures, as opposed to tree structures. There is therefore a need for formal grammars that generate graphs. Graph grammars h"
J19-2005,P05-1073,0,0.0127583,"Missing"
J96-4003,J95-4011,0,0.0207904,"cal rules as finitestate transducers that accept underlying forms as input and generate surface forms as output. Johnson (1972) first observed that traditional phonological rewrite rules can be expressed as regular (finite-state) relations if one accepts the constraint that no rule may reapply directly to its own output. This means that finite-state transducers (FSTs) can be used to represent phonological rules, greatly simplifying the problem of parsing the output of phonological rules in order to obtain the underlying, lexical forms (Koskenniemi 1983; Karttunen 1993; Pulman and Hepple 1993; Bird 1995; Bird and Ellison 1994). The fact that the weaker generative capacity of FSTs makes them easier to 498 Gildea and Jurafsky Learning Bias and Phonological-Rule Induction learn than arbitrary context-sensitive rules has allowed the development of a number of learning algorithms including those for deterministic finite-state automata (FSAs) (Freund et al. 1993), deterministic transducers (Oncina, Garcia, and Vidal 1993), as well as nondeterministic (stochastic) FSAs (Stolcke and Omohundro 1993; Stolcke and Omohundro 1994; Ron, Singer, and Tishby 1994). Like the empiricist models discussed above,"
J96-4003,J94-1003,0,0.0278125,"s finitestate transducers that accept underlying forms as input and generate surface forms as output. Johnson (1972) first observed that traditional phonological rewrite rules can be expressed as regular (finite-state) relations if one accepts the constraint that no rule may reapply directly to its own output. This means that finite-state transducers (FSTs) can be used to represent phonological rules, greatly simplifying the problem of parsing the output of phonological rules in order to obtain the underlying, lexical forms (Koskenniemi 1983; Karttunen 1993; Pulman and Hepple 1993; Bird 1995; Bird and Ellison 1994). The fact that the weaker generative capacity of FSTs makes them easier to 498 Gildea and Jurafsky Learning Bias and Phonological-Rule Induction learn than arbitrary context-sensitive rules has allowed the development of a number of learning algorithms including those for deterministic finite-state automata (FSAs) (Freund et al. 1993), deterministic transducers (Oncina, Garcia, and Vidal 1993), as well as nondeterministic (stochastic) FSAs (Stolcke and Omohundro 1993; Stolcke and Omohundro 1994; Ron, Singer, and Tishby 1994). Like the empiricist models discussed above, these algorithms are al"
J96-4003,J92-4003,0,0.0880468,"onology (e.g., optimality constraints). Second, we assume that a cognitive model of automaton induction would be more stochastic and hence more robust than the OSTIA algorithm underlying our work. 1 Rather, our model is intended to suggest the kind of biases that may be added to empiricist induction models to build a learning model for phonological rules that is cognitively and computationally plausible. Furthermore, our model is not necessarily nativist; these biases may be innate, but they may also be the product of some other earlier learning algorithm, as the results of Ellison (1992) and Brown et al. (1992) suggest (see Section 5.2). So our results suggest that assuming in the system some very general and fundamental properties of phonological knowledge (whether innate or previously learned) and learning others empirically may provide a basis for future learning models. Ellison (1994), for example, has shown how to map the optimality constraints of Prince and Smolensky (1993) to finite-state automata; given this result, models of 1 Although our assumption of the simultaneous presentation of surface and underlying forms to the learner may seem at first glance to be unnatural as well, it is quite"
J96-4003,J94-3007,0,0.0180628,"Missing"
J96-4003,C94-2163,0,0.0820543,"tate transducers that accept underlying forms as input and generate surface forms as output. Johnson (1972) first observed that traditional phonological rewrite rules can be expressed as regular (finite-state) relations if one accepts the constraint that no rule may reapply directly to its own output. This means that finite-state transducers (FSTs) can be used to represent phonological rules, greatly simplifying the problem of parsing the output of phonological rules in order to obtain the underlying, lexical forms (Koskenniemi 1983; Karttunen 1993; Pulman and Hepple 1993; Bird 1995; Bird and Ellison 1994). The fact that the weaker generative capacity of FSTs makes them easier to 498 Gildea and Jurafsky Learning Bias and Phonological-Rule Induction learn than arbitrary context-sensitive rules has allowed the development of a number of learning algorithms including those for deterministic finite-state automata (FSAs) (Freund et al. 1993), deterministic transducers (Oncina, Garcia, and Vidal 1993), as well as nondeterministic (stochastic) FSAs (Stolcke and Omohundro 1993; Stolcke and Omohundro 1994; Ron, Singer, and Tishby 1994). Like the empiricist models discussed above, these algorithms are al"
J96-4003,P84-1070,0,0.572124,"in the transducer system. 8. Related Work Recent work in the machine learning of phonology includes algorithms for learning both segmental and nonsegmental information. Nonsegmental approaches include those of Daelemans, Gillis, and Durieux (1994) for learning stress systems, as well as approaches to learning morphology such as Gasser&apos;s (1993) system for inducing Semitic morphology, and Ellison&apos;s (1992) extensive work on syllabicity, sonority, and harmony. Since our approach learns only segmental structure, a more relevant comparison is with other algorithms for inducing segmental structure. Johnson (1984) gives one of the first computational algorithms for phonological rule induction. His algorithm works for rules of the form (15) a --* b/C where C is the feature matrix of the segments around a. Johnson&apos;s algorithm sets up a system of constraint equations that C must satisfy, by considering both the positive contexts, i.e., all the contexts Ci in which a b occurs on the surface, as well as all the negative contexts Cj in which an a occurs on the surface. The set of all positive and negative contexts will not generally determine a unique rule, but will determine a set of possible rules. Johnson"
J96-4003,J94-3001,0,0.0163909,"te rules. For example, in American English an underlying t is realized as a flap (a tap of the tongue on the alveolar ridge) after a stressed vowel and zero or more r&apos;s, and before an unstressed vowel. In the rewrite-rule formalism of Chomsky and Halle (1968), this rule would be represented as in (1). (1) t --~ d x / Q r* __ V Since Johnson&apos;s (1972) work, researchers have proposed a number of different ways to represent such phonological rules by transducers. The most popular method is the two-level formalism of Koskenniemi (1983), based on Johnson (1972) and the (belatedly published) work of Kaplan and Kay (1994), and various implementations and extensions (summarized and contrasted in Karttunen [1993]). The basic intuition of two-level phonology is that a rule that rewrites an underlying string as a surface string can be implemented as a transducer that reads from an underlying tape and writes to a surface tape. Figure 1 shows an example of a transducer that implements the flapping rule in (1). Each arc has an input symbol and an output symbol, separated by a colon. A single symbol (such as t or V) is a shorthand for a symbol that is the same in the input and output (i.e., t : t or V:V). Either the i"
K15-1004,W13-2322,0,0.191247,"f starting with a dependency tree instead of the original sentence is that the dependency structure is more linguistically similar to an AMR graph and provides more direct feature information within limited context. Hyperedge replacement grammar (HRG) is a context-free rewriting formalism for generating graphs (Drewes et al., 1997). Its synchronous counterpart, SHRG, can be used for transforming a graph from/to another structured representation such as a string or tree structure. HRG has great potential for applications in natural language unIntroduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism where the meaning of a sentence is encoded as a rooted, directed graph. Figure 1 shows an example of the edge-labeled representation of an AMR graph where the edges are labeled while the nodes are not. The label of the leaf edge going out of a node represents the concept of the node, and the label of a non-leaf edge shows the relation between the concepts of the two nodes it connects to. This formalism is based on propositional logic and neo-Davidsonian event representations (Parsons, 1990; Davidson, 1967). AMR does not encode quantifiers, tense and modality, but it jo"
K15-1004,P13-2131,0,0.330005,"elow it. And there is no explicit order of composition with each branch. Another constraint we have tried is to attach all unaligned edges to the head node concept. The problem with this constraint is that it is very hard to generalize and introduces a lot of additional redundant relation edges. As for sampling, we initialize all cut variables in the forest as 1 (except for nodes that are marked as nosample cut, which indicates we initialize it with 0 and keep it fixed) and uniformly sample an incoming edge for each node. We evaluate the performance of our SHRG-based parser using Smatch v1.0 (Cai and Knight, 2013), which evaluates the precision, recall and F 1 of the concepts and relations all together. Table 1 shows the dev results of our sampled grammar using different lexical rules that maps substrings to graph fragments. Concept id only is the result of using the concepts identified by Flanigan et al. (2014). From second line, we replace the concept identification result with the lexical rules we have extracted from the training data (except for named entities and time expressions). +MCMC shows the result using additional alignments identified using our sampling approach. We can see that using the"
K15-1004,P13-1091,0,0.30562,"tations (Parsons, 1990; Davidson, 1967). AMR does not encode quantifiers, tense and modality, but it jointly encodes a set of selected semantic phenomena which renders it useful in applications like question answering and semantics-based machine translation. 32 Proceedings of the 19th Conference on Computational Language Learning, pages 32–41, c Beijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics derstanding and generation, and also semanticsbased machine translation. Given a graph as input, finding its derivation of HRG rules is NP-complete (Drewes et al., 1997). Chiang et al. (2013) describe in detail a graph recognition algorithm and present an optimization scheme which enables the parsing algorithm to run in polynomial time when the treewidth and degree of the graph are bounded. However, there is still no real system available for parsing large graphs. An SHRG can be used for AMR graph parsing where each SHRG rule consists of a pair of a CFG rule and an HRG rule, which can generate strings and AMR graphs in parallel. Jones et al. (2012) present a Syntactic Semantic Algorithm that learns SHRG by matching minimal parse constituents to aligned graph fragments and incremen"
K15-1004,D14-1048,0,0.0444466,"based parser and present preliminary results of a graph-grammar-based approach. 1 ARG0 ARG0 girl ARG1 boy Figure 1: An example of AMR graph representing the meaning of: “The boy wants the girl to believe him” The task of AMR graph parsing is to map natural language strings to AMR semantic graphs. Flanigan et al. (2014) propose a two-stage parsing algorithm which first maps meaningful continuous spans on the string side to concept fragments on the graph side, and then in the second stage adds additional edges to make all these fragments connected. Concept identification (Flanigan et al., 2014; Pourdamghani et al., 2014) can be considered as an important first step to relate components of the string to components in the graph. Wang et al. (2015) also present a two-stage procedure where they first use a dependency parser trained on a large corpus to generate a dependency tree for each sentence. In the second step, a transition-based algorithm is used to greedily modify the dependency tree into an AMR graph. The benefit of starting with a dependency tree instead of the original sentence is that the dependency structure is more linguistically similar to an AMR graph and provides more direct feature information w"
K15-1004,J14-1007,1,0.906562,"Missing"
K15-1004,N15-1040,0,0.558357,"raph representing the meaning of: “The boy wants the girl to believe him” The task of AMR graph parsing is to map natural language strings to AMR semantic graphs. Flanigan et al. (2014) propose a two-stage parsing algorithm which first maps meaningful continuous spans on the string side to concept fragments on the graph side, and then in the second stage adds additional edges to make all these fragments connected. Concept identification (Flanigan et al., 2014; Pourdamghani et al., 2014) can be considered as an important first step to relate components of the string to components in the graph. Wang et al. (2015) also present a two-stage procedure where they first use a dependency parser trained on a large corpus to generate a dependency tree for each sentence. In the second step, a transition-based algorithm is used to greedily modify the dependency tree into an AMR graph. The benefit of starting with a dependency tree instead of the original sentence is that the dependency structure is more linguistically similar to an AMR graph and provides more direct feature information within limited context. Hyperedge replacement grammar (HRG) is a context-free rewriting formalism for generating graphs (Drewes"
K15-1004,N09-1062,0,0.0250222,"nce, AMR graph) pair for “The boy wants the girl to believe him” sents the current node is internal to an SHRG rule, while 1 represents the current node is the boundary of two SHRG rules. Let all the edge variables form the random vector Y and all the cut variables form the random vector Z. Given an assignment y to the edge variables and assignment z to the cut variables, our desired distribution is proportional to the product of weights of the rules specified by the assignment: MCMC sampling Sampling methods have been used to learn Tree Substitution Grammar (TSG) rules from derivation trees (Cohn et al., 2009; Post and Gildea, 2009) for TSG learning. The basic intuition is to automatically learn the best granularity for the rules with which to analyze our data. Our problem, however, is different in that we need to sample rules from a compact forest representation. We need to sample one tree from the forest, and then sample one derivation from this tree structure, where each tree fragment represents one rule in the derivation. Sampling tree fragments from forests is described in detail in Chung et al. (2014) and Peng and Gildea (2014). We formulate the rule sampling procedure with two types of vari"
K15-1004,P14-1134,0,0.241304,"phrase-based machine translation and come up with an efficient algorithm to learn graph grammars from string-graph pairs. We propose an effective approximation strategy to resolve the complexity issue of graph compositions. We also show some useful strategies to overcome existing problems in an SHRG-based parser and present preliminary results of a graph-grammar-based approach. 1 ARG0 ARG0 girl ARG1 boy Figure 1: An example of AMR graph representing the meaning of: “The boy wants the girl to believe him” The task of AMR graph parsing is to map natural language strings to AMR semantic graphs. Flanigan et al. (2014) propose a two-stage parsing algorithm which first maps meaningful continuous spans on the string side to concept fragments on the graph side, and then in the second stage adds additional edges to make all these fragments connected. Concept identification (Flanigan et al., 2014; Pourdamghani et al., 2014) can be considered as an important first step to relate components of the string to components in the graph. Wang et al. (2015) also present a two-stage procedure where they first use a dependency parser trained on a large corpus to generate a dependency tree for each sentence. In the second s"
K15-1004,C12-1083,0,0.0439477,"Missing"
K15-1004,N03-1017,0,0.0118933,"d word. After this alignment, there are also left-over edges that are not aligned from any substrings, which are called unaligned edges. Given an aligned string, AMR graph pair, a phrase-fragment pair n is a pair ([i, j], f ) which defines a pair of a phrase [i, j] and a fragment f such that words in positions [i, j] are only aligned to concepts in the fragment f and vice versa (with unaligned words and edges omitted). A fragment forest H = hV, Ei is a hypergraph made of a set of hypernodes V and hyperedges E. Each node n = ([i, j], f ) is tight on the string side similar to the definition by Koehn et al. (2003), i.e., n contains no unaligned words at its boundaries. Note here we do not have the constraint that f should be connected or single rooted, but we will deal with these constraints separately in the sampling procedure. We define two phrases [i1 , j1 ], [i2 , j2 ] to be adjacent if word indices {j1 , j1 + 1, . . . , i2 − 1} are all unaligned. We also define two fragments f1 = hV1 , E1 i, f2 = hV2 , E2 i to be disjoint if E1 ∩ E2 = ∅. And f1 and f2 are adjacent if they are disjoint and f = hV1 ∪ V2 , E1 ∪ E2 i is connected. We also define the compose operation of two nodes: it takes two nodes n"
K15-1004,P03-1021,0,0.0290647,"[X1-1, n] | (. :a/and :op1 [X1-1, 1] :op2 [X1-1, 2] · · · :opn [X1-1, n]) where the HRG side is a :a/and coordinate structure of X1-1s connected with relation :ops. 5 Precision 0.37 0.57 0.60 0.59 Experiments We use the same newswire section of LDC2013E117 as Flanigan et al. (2014), which 39 JAMR Wang et al. Our approach Precision 0.67 0.64 0.59 Recall 0.58 0.62 0.57 F-score 0.62 0.63 0.58 a graph language model into our CFG decoder, which should also help improve the performance. All the weights of the local features mentioned in Section 4.2 are tuned by hand. We have tried tuning with MERT (Och, 2003), but the computation of smatch score for the k-best list has become a major overhead. This issue might come from the NP-Completeness of the problem smatch tries to evaluate, unlike the simple counting of N-grams in BLEU (Papineni et al., 2001). Parallelization might be a consideration for tuning smatch score with MERT. Table 2: Comparisons of smatch score results alignments of length 6 on the string side from our constructed forest. We can see that using this alignment table further improves the smatch score. This is because the larger phrase-fragment pairs can make better use of the dependen"
K15-1004,D14-1180,1,0.858979,"to learn Tree Substitution Grammar (TSG) rules from derivation trees (Cohn et al., 2009; Post and Gildea, 2009) for TSG learning. The basic intuition is to automatically learn the best granularity for the rules with which to analyze our data. Our problem, however, is different in that we need to sample rules from a compact forest representation. We need to sample one tree from the forest, and then sample one derivation from this tree structure, where each tree fragment represents one rule in the derivation. Sampling tree fragments from forests is described in detail in Chung et al. (2014) and Peng and Gildea (2014). We formulate the rule sampling procedure with two types of variables: an edge variable en representing which incoming hyperedge is chosen at a given node n in the forest (allowing us to sample one tree from a forest) and a cut variable zn representing whether node n in forest is a boundary between two SHRG rules or is internal to an SHRG rule (allowing us to sample rules from a tree). Figure 5 shows one sampled derivation from the forest. We have sampled one tree from the forest using the edge variables. We also have a 0-1 variable at each node in this tree where 0 reprePt (Y = y, Z = z) ∝ Y"
K15-1004,P09-2012,1,0.751774,"r for “The boy wants the girl to believe him” sents the current node is internal to an SHRG rule, while 1 represents the current node is the boundary of two SHRG rules. Let all the edge variables form the random vector Y and all the cut variables form the random vector Z. Given an assignment y to the edge variables and assignment z to the cut variables, our desired distribution is proportional to the product of weights of the rules specified by the assignment: MCMC sampling Sampling methods have been used to learn Tree Substitution Grammar (TSG) rules from derivation trees (Cohn et al., 2009; Post and Gildea, 2009) for TSG learning. The basic intuition is to automatically learn the best granularity for the rules with which to analyze our data. Our problem, however, is different in that we need to sample rules from a compact forest representation. We need to sample one tree from the forest, and then sample one derivation from this tree structure, where each tree fragment represents one rule in the derivation. Sampling tree fragments from forests is described in detail in Chung et al. (2014) and Peng and Gildea (2014). We formulate the rule sampling procedure with two types of variables: an edge variable"
K15-1004,J07-2003,0,\N,Missing
K15-1004,D08-1076,0,\N,Missing
N04-1021,J00-1004,0,0.0540744,"Missing"
N04-1021,J93-2003,0,0.0210066,"e different types of features, including features based on syntactic analyses of the source and target sentences, which we hope will address the grammaticality of the translations, as well as lower-level features. As we work on n-best lists, we can easily use global sentence-level features. We begin by describing our baseline system and the n-best rescoring framework within which we conducted our experiments. We then present a selection of new features, progressing from word-level features to those based Anoop Sarkar Simon Fraser U. As an alternative to the often used source-channel approach (Brown et al., 1993), we directly model the posterior probability P r(eI1 |f1J ) (Och and Ney, 2002) using a log-linear combination of feature functions. In this framework, we have a set of M feature functions hm (eI1 , f1J ), m = 1, . . . , M . For each feature function, there exists a model parameter λm , m = 1, . . . , M . The direct translation probability is given by: PM exp[ m=1 λm hm (eI1 , f1J )] I J P r(e1 |f1 ) = P (2) PM J 0I e0I exp[ m=1 λm hm (e 1 , f1 )] 1 We obtain the following decision rule: eˆI1 = argmax eI1 M nX o λm hm (eI1 , f1J ) (3) m=1 The standard criterion for training such a log-linear"
N04-1021,P03-1011,1,0.218398,"of words in tree fragment and k for maximum height of with some probability, to transform one tree into another. tree fragment. We proceed from left to right in the ChiHowever, when training the model, trees for both the nese sentence and incrementally grow a pair of subtrees, source and target languages are provided, in our case one subtree in Chinese and the other in English, such that from the Chinese and English parsers. each word in the Chinese subtree is aligned to a word in We began with the tree-to-tree alignment model prethe English subtree. We grow this pair of subtrees unsented by Gildea (2003). The model was extended to hantil we can no longer grow either subtree without violatdle dependency trees, and to make use of the word-level ing the two parameter values n and k. Note that these alignments produced by the baseline MT system. The aligned subtree pairs have properties similar to alignment probability assigned by the tree-to-tree alignment model, templates. They can rearrange in complex ways between given the word-level alignment with which the candidate source and target. Figure 2 shows how subtree-pairs for translation was generated, was used as a feature in our parameters n ="
N04-1021,P03-1021,1,0.129951,"nsisting of S sentence pairs {(fs , es ) : s = 1, . . . , S}. However, this does not guarantee optimal performance on the metric of translation quality by which our system will ultimately be evaluated. For this reason, we optimize the parameters directly against the BLEU metric on held-out data. This is a more difficult optimization problem, as the search space is no longer convex. Figure 1: Example segmentation of Chinese sentence and its English translation into alignment templates. However, certain properties of the BLEU metric can be exploited to speed up search, as described in detail by Och (2003). We use this method of optimizing feature weights throughout this paper. 2.1 Baseline MT System: Alignment Templates Our baseline MT system is the alignment template system described in detail by Och, Tillmann, and Ney (1999) and Och and Ney (2004). In the following, we give a short description of this baseline model. The probability model of the alignment template system for translating a sentence can be thought of in distinct stages. First, the source sentence words f1J are grouped to phrases f˜1K . For each phrase f˜ an alignment template z is chosen and the sequence of chosen alignment te"
N04-1021,P02-1038,1,0.203592,"the source and target sentences, which we hope will address the grammaticality of the translations, as well as lower-level features. As we work on n-best lists, we can easily use global sentence-level features. We begin by describing our baseline system and the n-best rescoring framework within which we conducted our experiments. We then present a selection of new features, progressing from word-level features to those based Anoop Sarkar Simon Fraser U. As an alternative to the often used source-channel approach (Brown et al., 1993), we directly model the posterior probability P r(eI1 |f1J ) (Och and Ney, 2002) using a log-linear combination of feature functions. In this framework, we have a set of M feature functions hm (eI1 , f1J ), m = 1, . . . , M . For each feature function, there exists a model parameter λm , m = 1, . . . , M . The direct translation probability is given by: PM exp[ m=1 λm hm (eI1 , f1J )] I J P r(e1 |f1 ) = P (2) PM J 0I e0I exp[ m=1 λm hm (e 1 , f1 )] 1 We obtain the following decision rule: eˆI1 = argmax eI1 M nX o λm hm (eI1 , f1J ) (3) m=1 The standard criterion for training such a log-linear model is to maximize the probability of the parallel training corpus consisting"
N04-1021,J04-4002,1,0.260908,"ters directly against the BLEU metric on held-out data. This is a more difficult optimization problem, as the search space is no longer convex. Figure 1: Example segmentation of Chinese sentence and its English translation into alignment templates. However, certain properties of the BLEU metric can be exploited to speed up search, as described in detail by Och (2003). We use this method of optimizing feature weights throughout this paper. 2.1 Baseline MT System: Alignment Templates Our baseline MT system is the alignment template system described in detail by Och, Tillmann, and Ney (1999) and Och and Ney (2004). In the following, we give a short description of this baseline model. The probability model of the alignment template system for translating a sentence can be thought of in distinct stages. First, the source sentence words f1J are grouped to phrases f˜1K . For each phrase f˜ an alignment template z is chosen and the sequence of chosen alignment templates is reordered (according to π1K ). Then, every phrase f˜ produces its translation e˜ (using the corresponding alignment template z). Finally, the sequence of phrases e˜K 1 constitutes the sequence of words eI1 . Our baseline system incorporat"
N04-1021,W99-0604,1,0.236444,"Missing"
N04-1021,W03-1002,0,0.0153043,"of the AT touches the upper right corner of the previous AT and the first word in the current AT immediately follows the last word in the previous AT. The total probability is the product over all alignment templates i, either P (ATi is right-continuous) or 1 − P (ATi is right-continuous). In both models, the probabilities P have been estimated from the full training data (train). 5 Shallow Syntactic Feature Functions By shallow syntax, we mean the output of the part-ofspeech tagger and chunkers. We hope that such features can combine the strengths of tag- and chunk-based translation systems (Schafer and Yarowsky, 2003) with our baseline system. 5.1 Projected POS Language Model This feature uses Chinese POS tag sequences as surrogates for Chinese words to model movement. Chinese words are too sparse to model movement, but an attempt to model movement using Chinese POS may be more successful. We hope that this feature will compensate for a weak model of word movement in the baseline system. Chinese POS sequences are projected to English using the word alignment. Relative positions are indicated for each Chinese tag. The feature function was also tried without the relative positions: CD +0 M +1 NN +3 NN -1 NN"
N04-1021,C96-2141,0,0.254737,"Missing"
N04-1021,P98-2230,0,0.0760244,"Missing"
N04-1021,P01-1067,1,0.0551165,"ical human translations. One reason for that is that the MT output uses fewer unseen words and typically more frequent words which lead to a higher language model probability. We also performed experiments to balance this effect by dividing the parser probability by the word unigram probability and using this ’normalized parser probability’ as a feature function, but also this did not yield improvements. 6.2 Tree-to-String Alignment A tree-to-string model is one of several syntaxbased translation models used. The model is a conditional probability p(f |T (e)). Here, we used a model defined by Yamada and Knight (2001) and Yamada and Knight (2002). Internally, the model performs three types of operations on each node of a parse tree. First, it reorders the child nodes, such as changing VP → VB NP PP into VP → NP PP VB. Second, it inserts an optional word at each node. Third, it translates the leaf English words into Chinese words. These operations are stochastic and their probabilities are assumed to depend only on the node, and are independent of other operations on the node, or other nodes. The probability of each operation is automatically obtained by a training algorithm, using about 780,000 English par"
N04-1021,C98-2225,0,\N,Missing
N06-1033,P03-2041,0,0.286386,"juxing le huitan PP(TO(with), NP(NNP(Sharon))) → yu Shalong where the reorderings of nonterminals are denoted by variables xi . Notice that the first rule has a multi-level lefthand side subtree. This system can model nonisomorphic transformations on English parse trees to “fit” another language, for example, learning that 258 the (S (V O)) structure in English should be transformed into a (V S O) structure in Arabic, by looking at two-level tree fragments (Knight and Graehl, 2005). From a synchronous rewriting point of view, this is more akin to synchronous tree substitution grammar (STSG) (Eisner, 2003). This larger locality is linguistically motivated and leads to a better parameter estimation. By imagining the left-hand-side trees as special nonterminals, we can virtually create an SCFG with the same generative capacity. The technical details will be explained in Section 3.2. In general, if we are given an arbitrary synchronous rule with many nonterminals, what are the good decompositions that lead to a binary grammar? Figure 2 suggests that a binarization is good if every virtual nonterminal has contiguous spans on both sides. We formalize this idea in the next section. 2 Synchronous Bina"
N06-1033,N04-1035,1,0.655736,"re-orderings by binarizing synchronous rules when possible and show that the resulting rule set significantly improves the speed and accuracy of a state-of-the-art syntax-based machine translation system. 1 • We examine the effect of this binarization method on end-to-end machine translation quality, compared to a more typical baseline method. • We examine cases of non-binarizable rules in a large, empirically-derived rule set, and we investigate the effect on translation quality when excluding such rules. Introduction Several recent syntax-based models for machine translation (Chiang, 2005; Galley et al., 2004) can be seen as instances of the general framework of synchronous grammars and tree transducers. In this framework, both alignment (synchronous parsing) and decoding can be thought of as parsing problems, whose complexity is in general exponential in the number of nonterminals on the right hand side of a grammar rule. To alleviate this problem, we investigate bilingual binarization to factor the synchronous grammar to a smaller branching factor, although it is not guaranteed to be successful for any synchronous rule with arbitrary permutation. In particular: Melamed (2003) discusses binarizati"
N06-1033,W05-1507,1,0.837659,"e n nonterminals in a SCFG rule, and the decoder conservatively assumes nothing can be done on language model scoring (because target-language spans are non-contiguous in general) until the real nonterminal has been recognized. In other words, targetlanguage boundary words from each child nonterminal of the rule will be cached in all virtual nonterminals derived from this rule. In the case of m-gram integrated decoding, we have to maintain 2(m − 1) boundary words for each child nonterminal, which leads to a prohibitive overall complexity of O(|w|3+2n(m−1) ), which is exponential in rule size (Huang et al., 2005). Aggressive pruning must be used to make it tractable in practice, which in general introduces many search errors and adversely affects translation quality. In the second case, however:  with ··· Sharon 2  PP held 2 4 ··· VPP-VP  :r Sharon 7   held ··· meeting 4 VP 7  :s : rs · Pr(with |meeting) Here since PP and VP are contiguous (but swapped) in the target-language, we can include the source (Chinese) NP NP PP VP VP PP target (English) English boundary words VPP-VP VPP-VP Sharon PP with meeting held Powell Powell VP NP 1 2 4 7 Chinese indices Figure 2: The alignment pattern (left) and"
N06-1033,N03-1021,0,0.0662649,"(Chiang, 2005; Galley et al., 2004) can be seen as instances of the general framework of synchronous grammars and tree transducers. In this framework, both alignment (synchronous parsing) and decoding can be thought of as parsing problems, whose complexity is in general exponential in the number of nonterminals on the right hand side of a grammar rule. To alleviate this problem, we investigate bilingual binarization to factor the synchronous grammar to a smaller branching factor, although it is not guaranteed to be successful for any synchronous rule with arbitrary permutation. In particular: Melamed (2003) discusses binarization of multitext grammars on a theoretical level, showing the importance and difficulty of binarization for efficient synchronous parsing. One way around this difficulty is to stipulate that all rules must be binary from the outset, as in inversion-transduction grammar (ITG) (Wu, 1997) and the binary synchronous context-free grammar (SCFG) employed by the Hiero system (Chiang, 2005) to model the hierarchical phrases. In contrast, the rule extraction method of Galley et al. (2004) aims to incorporate more syntactic information by providing parse trees for the target language"
N06-1033,J04-4002,0,0.236624,"Missing"
N06-1033,H05-1101,0,0.681458,"VPP-VP → NP(1) VP(1) (2) , NP(1) VPP-VP VPP-VP (2) (2) PP , PP VP(1) (2) In this case m-gram integrated decoding can be done in O(|w|3+4(m−1) ) time which is much lowerorder polynomial and no longer depends on rule size (Wu, 1996), allowing the search to be much faster and more accurate facing pruning, as is evidenced in the Hiero system of Chiang (2005) where he restricts the hierarchical phrases to be a binary SCFG. The benefit of binary grammars also lies in synchronous parsing (alignment). Wu (1997) shows that parsing a binary SCFG is in O(|w|6 ) while parsing SCFG is NP-hard in general (Satta and Peserico, 2005). The same reasoning applies to tree transducer rules. Suppose we have the following tree-to-string rules, following Galley et al. (2004): (3) S(x0 :NP, VP(x2 :VP, x1 :PP)) → x0 x1 x2 NP(NNP(Powell)) → Baoweier VP(VBD(held), NP(DT(a) NPS(meeting))) → juxing le huitan PP(TO(with), NP(NNP(Sharon))) → yu Shalong where the reorderings of nonterminals are denoted by variables xi . Notice that the first rule has a multi-level lefthand side subtree. This system can model nonisomorphic transformations on English parse trees to “fit” another language, for example, learning that 258 the (S (V O)) struct"
N06-1033,C90-3045,0,0.251755,"Missing"
N06-1033,P96-1021,0,0.41387,"s production. language model score by adding Pr(with |meeting), and the resulting item again has two boundary words. Later we add Pr(held | Powell) whenthe Powell ··· Powell resulting item is combined with to NP 1 2 form an S item. As illustrated in Figure 2, VPP-VP has contiguous spans on both source and target sides, so that we can generate a binary-branching SCFG: (2) S→ VPP-VP → NP(1) VP(1) (2) , NP(1) VPP-VP VPP-VP (2) (2) PP , PP VP(1) (2) In this case m-gram integrated decoding can be done in O(|w|3+4(m−1) ) time which is much lowerorder polynomial and no longer depends on rule size (Wu, 1996), allowing the search to be much faster and more accurate facing pruning, as is evidenced in the Hiero system of Chiang (2005) where he restricts the hierarchical phrases to be a binary SCFG. The benefit of binary grammars also lies in synchronous parsing (alignment). Wu (1997) shows that parsing a binary SCFG is in O(|w|6 ) while parsing SCFG is NP-hard in general (Satta and Peserico, 2005). The same reasoning applies to tree transducer rules. Suppose we have the following tree-to-string rules, following Galley et al. (2004): (3) S(x0 :NP, VP(x2 :VP, x1 :PP)) → x0 x1 x2 NP(NNP(Powell)) → Baow"
N06-1033,J97-3002,0,0.851118,"s on the right hand side of a grammar rule. To alleviate this problem, we investigate bilingual binarization to factor the synchronous grammar to a smaller branching factor, although it is not guaranteed to be successful for any synchronous rule with arbitrary permutation. In particular: Melamed (2003) discusses binarization of multitext grammars on a theoretical level, showing the importance and difficulty of binarization for efficient synchronous parsing. One way around this difficulty is to stipulate that all rules must be binary from the outset, as in inversion-transduction grammar (ITG) (Wu, 1997) and the binary synchronous context-free grammar (SCFG) employed by the Hiero system (Chiang, 2005) to model the hierarchical phrases. In contrast, the rule extraction method of Galley et al. (2004) aims to incorporate more syntactic information by providing parse trees for the target language and extracting tree transducer rules that apply to the parses. This approach results in rules with many nonterminals, making good binarization techniques critical. Suppose we have the following SCFG, where superscripts indicate reorderings (formal definitions of 256 Proceedings of the Human Language Tech"
N06-1033,W90-0102,0,\N,Missing
N06-1033,P05-1033,0,\N,Missing
N07-1006,W05-0909,0,0.221946,"iously proposed metrics. We further improve performance by combining individual evaluation metrics using maximum correlation training, which is shown to be better than the classification-based framework. 1 • Metrics based on syntactic similarities such as the head-word chain metric (HWCM) (Liu and Gildea, 2005). Such metrics try to improve fluency evaluation performance for MT, but they heavily depend on automatic parsers, which are designed for well-formed sentences and cannot generate robust parse trees for MT outputs. • Metrics based on word alignment between MT outputs and the references (Banerjee and Lavie, 2005). Such metrics do well in adequacy evaluation, but are not as good in fluency evaluation, because of their unigram basis (Liu and Gildea, 2006). Introduction Evaluation has long been a stumbling block in the development of machine translation systems, due to the simple fact that there are many correct translations for a given sentence. The most commonly used metric, BLEU, correlates well over large test sets with human judgments (Papineni et al., 2002), but does not perform as well on sentence-level evaluation (Blatz et al., 2003). Later approaches to improve sentence-level evaluation performa"
N07-1006,H05-1093,0,0.130102,"nce-level evaluation performance can be summarized as falling into four types: • Metrics based on common loose sequences of MT outputs and references (Lin and Och, 2004; Liu and Gildea, 2006). Such metrics were • Combination of metrics based on machine learning. Kulesza and Shieber (2004) used SVMs to combine several metrics. Their method is based on the assumption that higher classification accuracy in discriminating human- from machine-generated translations will yield closer correlation with human judgment. This assumption may not always hold, particularly when classification is difficult. Lita et al. (2005) proposed a log-linear model to combine features, but they only did preliminary experiments based on 2 features. Following the track of previous work, to improve evaluation performance, one could either propose new metrics, or find more effective ways to combine the metrics. We explore both approaches. Much work has been done on computing MT scores based 41 Proceedings of NAACL HLT 2007, pages 41–48, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics on the pair of MT output/reference, and we aim to investigate whether some other information could be used in the MT eva"
N07-1006,W05-0904,1,0.95193,"rdering metrics, and discriminative unigram precision, as well as a method of learning linear feature weights to directly maximize correlation with human judgments. By aligning both the hypothesis and the reference with the sourcelanguage sentence, we achieve better correlation with human judgments than previously proposed metrics. We further improve performance by combining individual evaluation metrics using maximum correlation training, which is shown to be better than the classification-based framework. 1 • Metrics based on syntactic similarities such as the head-word chain metric (HWCM) (Liu and Gildea, 2005). Such metrics try to improve fluency evaluation performance for MT, but they heavily depend on automatic parsers, which are designed for well-formed sentences and cannot generate robust parse trees for MT outputs. • Metrics based on word alignment between MT outputs and the references (Banerjee and Lavie, 2005). Such metrics do well in adequacy evaluation, but are not as good in fluency evaluation, because of their unigram basis (Liu and Gildea, 2006). Introduction Evaluation has long been a stumbling block in the development of machine translation systems, due to the simple fact that there a"
N07-1006,P06-2070,1,0.888539,"wn to be better than the classification-based framework. 1 • Metrics based on syntactic similarities such as the head-word chain metric (HWCM) (Liu and Gildea, 2005). Such metrics try to improve fluency evaluation performance for MT, but they heavily depend on automatic parsers, which are designed for well-formed sentences and cannot generate robust parse trees for MT outputs. • Metrics based on word alignment between MT outputs and the references (Banerjee and Lavie, 2005). Such metrics do well in adequacy evaluation, but are not as good in fluency evaluation, because of their unigram basis (Liu and Gildea, 2006). Introduction Evaluation has long been a stumbling block in the development of machine translation systems, due to the simple fact that there are many correct translations for a given sentence. The most commonly used metric, BLEU, correlates well over large test sets with human judgments (Papineni et al., 2002), but does not perform as well on sentence-level evaluation (Blatz et al., 2003). Later approaches to improve sentence-level evaluation performance can be summarized as falling into four types: • Metrics based on common loose sequences of MT outputs and references (Lin and Och, 2004; Li"
N07-1006,P03-1021,0,0.0231414,"ve different importance in MT evaluation, e.g., subprecision of nouns, verbs, and adjectives should be important for evaluating adequacy, and sub-precision in determiners and conjunctions should mean more in evaluating fluency. Along the direction of feature combination, since indirect weight training using SVMs, based on reducing classification error, cannot always yield good performance, we train the weights by directly optimizing the evaluation performance, i.e., maximizing the correlation with the human judgment. This type of direct optimization is known as Minimum Error 42 Rate Training (Och, 2003) in the MT community, and is an essential component in building the stateof-art MT systems. It would seem logical to apply similar methods to MT evaluation. What is more, Maximum Correlation Training (MCT) enables us to train the weights based on human fluency judgments and adequacy judgments respectively, and thus makes it possible to make a fluency-oriented or adequacy-oriented metric. It surpasses previous MT metrics’ approach, where a a single metric evaluates both fluency and adequacy. The rest of the paper is organized as follows: Section 2 gives a brief recap of n-gram precision-based m"
N07-1006,P02-1040,0,0.0725308,"ed sentences and cannot generate robust parse trees for MT outputs. • Metrics based on word alignment between MT outputs and the references (Banerjee and Lavie, 2005). Such metrics do well in adequacy evaluation, but are not as good in fluency evaluation, because of their unigram basis (Liu and Gildea, 2006). Introduction Evaluation has long been a stumbling block in the development of machine translation systems, due to the simple fact that there are many correct translations for a given sentence. The most commonly used metric, BLEU, correlates well over large test sets with human judgments (Papineni et al., 2002), but does not perform as well on sentence-level evaluation (Blatz et al., 2003). Later approaches to improve sentence-level evaluation performance can be summarized as falling into four types: • Metrics based on common loose sequences of MT outputs and references (Lin and Och, 2004; Liu and Gildea, 2006). Such metrics were • Combination of metrics based on machine learning. Kulesza and Shieber (2004) used SVMs to combine several metrics. Their method is based on the assumption that higher classification accuracy in discriminating human- from machine-generated translations will yield closer co"
N07-1006,2004.tmi-1.8,0,\N,Missing
N07-1006,P04-1077,0,\N,Missing
N07-1006,C04-1046,0,\N,Missing
N07-1006,D08-1076,0,\N,Missing
N07-1019,H05-1101,0,0.16562,"n to be explored. It has been known since the early days of automata theory (Aho and Ullman, 1972) that the languages of string pairs generated by a synchronous grammar can be arranged in an infinite hierarchy, with each rule size ≥ 4 producing languages not possible with grammars restricted to smaller rules. For any grammar with maximum rule size n, a fairly straightforward dynamic programming strategy yields an O(N n+4 ) algorithm for parsing sentences of length N . However, this is often not the best achievable complexity, and the exact bounds of the best possible algorithms are not known. Satta and Peserico (2005) showed that a permutation can be defined for any length n such that √ tabular parsing strategies must take at least c Ω(N n ), that is, the exponent of the algorithm is proportional to the square root of the rule length. In this paper, we improve this result, showing that in the worst case the exponent grows linearly with the rule length. Using a probabilistic argument, we show that the number of easily parsable permutations grows slowly enough that most permutations must be difficult, where by difficult we mean that the exponent in the complexity is greater than a constant factor times the r"
N10-1118,N09-1061,0,0.66519,"show how to apply factorization in order to minimize the parsing complexity of the resulting grammar, and study the relationship between rank, fanout, and parsing complexity. We show that it is always possible to obtain optimum parsing complexity with rank two. However, among transformed grammars of rank two, minimum parsing complexity is not always possible with minimum fan-out. Applying our factorization algorithm to LCFRS rules extracted from dependency treebanks allows us to find the most efficient parsing strategy for the syntactic phenomena found in non-projective trees. 1 Introduction Gómez-Rodríguez et al. (2009a) recently examined the problem of transforming arbitrary grammars in the Linear Context-Free Rewriting System (LCFRS) formalism (Vijay-Shankar et al., 1987) in order to reduce the rank of a grammar to 2 while minimizing its fan-out. The work was motivated by the desire to develop efficient chart-parsing algorithms for non-projective dependency trees (Kuhlmann and Nivre, 2006) that do not rely on the independence assumptions of spanning tree algorithms (McDonald et al., 2005). Efficient parsing algorithms for general LCFRS are also relevant in the context of Synchronous Context-Free Grammars"
N10-1118,E09-1034,0,0.133803,"show how to apply factorization in order to minimize the parsing complexity of the resulting grammar, and study the relationship between rank, fanout, and parsing complexity. We show that it is always possible to obtain optimum parsing complexity with rank two. However, among transformed grammars of rank two, minimum parsing complexity is not always possible with minimum fan-out. Applying our factorization algorithm to LCFRS rules extracted from dependency treebanks allows us to find the most efficient parsing strategy for the syntactic phenomena found in non-projective trees. 1 Introduction Gómez-Rodríguez et al. (2009a) recently examined the problem of transforming arbitrary grammars in the Linear Context-Free Rewriting System (LCFRS) formalism (Vijay-Shankar et al., 1987) in order to reduce the rank of a grammar to 2 while minimizing its fan-out. The work was motivated by the desire to develop efficient chart-parsing algorithms for non-projective dependency trees (Kuhlmann and Nivre, 2006) that do not rely on the independence assumptions of spanning tree algorithms (McDonald et al., 2005). Efficient parsing algorithms for general LCFRS are also relevant in the context of Synchronous Context-Free Grammars"
N10-1118,P06-2066,0,0.483024,"Applying our factorization algorithm to LCFRS rules extracted from dependency treebanks allows us to find the most efficient parsing strategy for the syntactic phenomena found in non-projective trees. 1 Introduction Gómez-Rodríguez et al. (2009a) recently examined the problem of transforming arbitrary grammars in the Linear Context-Free Rewriting System (LCFRS) formalism (Vijay-Shankar et al., 1987) in order to reduce the rank of a grammar to 2 while minimizing its fan-out. The work was motivated by the desire to develop efficient chart-parsing algorithms for non-projective dependency trees (Kuhlmann and Nivre, 2006) that do not rely on the independence assumptions of spanning tree algorithms (McDonald et al., 2005). Efficient parsing algorithms for general LCFRS are also relevant in the context of Synchronous Context-Free Grammars (SCFGs) as a formalism for machine translation, as well as the desire to handle even more general synchronous grammar formalisms which allow nonterminals to cover discontinuous spans in either language (Melamed et al., 2004; Wellington et al., 2006). LCFRS provides a very general formalism which subsumes SCFGs, the Multitext Grammars of Melamed et al. (2004), as well as mildly"
N10-1118,E09-1055,0,0.321734,"rs. This allows us to arrange LCFRS grammars into total ordering over generative capacity, that is a one-dimensional hierarchy, rather than a two-dimensional grid. It also gives a way of categorizing generative capacity that is more closely tied to algorithmic complexity. A number of recent papers have examined dynamic programming algorithms for parsing non-projective dependency structures by exploring how well various categories of polynomially-parsable grammars cover the structures found in dependency treebanks for various languages (Kuhlmann and Nivre, 2006; Gómez-Rodríguez et al., 2009b). Kuhlmann and Satta (2009) give an algorithm for extracting LCFRS rules from dependency structures. One rule is extracted for each word in the dependency tree. The rank of the rule is the number of children that the word has in the dependency tree, as shown by the example in Figure 4. The fan-out of the symbol corresponding to a word is the number of continuous intervals in the sentence formed by the word and its descendants in the tree. Projec774 Experiments complexity 20 18 16 15 13 12 11 10 9 8 7 6 5 4 3 arabic czech danish dutch 2 48 250 10942 4 3 178 1132 18269 265202 11 93 1026 18306 7 12 362 411 6678 39362 germa"
N10-1118,H05-1066,0,0.182552,"Missing"
N10-1118,P04-1084,0,0.0600171,"2 while minimizing its fan-out. The work was motivated by the desire to develop efficient chart-parsing algorithms for non-projective dependency trees (Kuhlmann and Nivre, 2006) that do not rely on the independence assumptions of spanning tree algorithms (McDonald et al., 2005). Efficient parsing algorithms for general LCFRS are also relevant in the context of Synchronous Context-Free Grammars (SCFGs) as a formalism for machine translation, as well as the desire to handle even more general synchronous grammar formalisms which allow nonterminals to cover discontinuous spans in either language (Melamed et al., 2004; Wellington et al., 2006). LCFRS provides a very general formalism which subsumes SCFGs, the Multitext Grammars of Melamed et al. (2004), as well as mildly context-sensitive monolingual formalisms such as Tree Adjoining Grammar (Joshi and Schabes, 1997). Thus, work on transforming general LCFRS grammars promises to be widely applicable in both understanding how these formalisms interrelate, and, from a more practical viewpoint, deriving efficient parsing algorithms for them. In this paper, we focus on the problem of transforming an LCFRS grammar into an equivalent grammar for which straightfo"
N10-1118,P87-1015,0,0.914374,"sing complexity. We show that it is always possible to obtain optimum parsing complexity with rank two. However, among transformed grammars of rank two, minimum parsing complexity is not always possible with minimum fan-out. Applying our factorization algorithm to LCFRS rules extracted from dependency treebanks allows us to find the most efficient parsing strategy for the syntactic phenomena found in non-projective trees. 1 Introduction Gómez-Rodríguez et al. (2009a) recently examined the problem of transforming arbitrary grammars in the Linear Context-Free Rewriting System (LCFRS) formalism (Vijay-Shankar et al., 1987) in order to reduce the rank of a grammar to 2 while minimizing its fan-out. The work was motivated by the desire to develop efficient chart-parsing algorithms for non-projective dependency trees (Kuhlmann and Nivre, 2006) that do not rely on the independence assumptions of spanning tree algorithms (McDonald et al., 2005). Efficient parsing algorithms for general LCFRS are also relevant in the context of Synchronous Context-Free Grammars (SCFGs) as a formalism for machine translation, as well as the desire to handle even more general synchronous grammar formalisms which allow nonterminals to c"
N10-1118,P06-1123,0,0.0409221,"fan-out. The work was motivated by the desire to develop efficient chart-parsing algorithms for non-projective dependency trees (Kuhlmann and Nivre, 2006) that do not rely on the independence assumptions of spanning tree algorithms (McDonald et al., 2005). Efficient parsing algorithms for general LCFRS are also relevant in the context of Synchronous Context-Free Grammars (SCFGs) as a formalism for machine translation, as well as the desire to handle even more general synchronous grammar formalisms which allow nonterminals to cover discontinuous spans in either language (Melamed et al., 2004; Wellington et al., 2006). LCFRS provides a very general formalism which subsumes SCFGs, the Multitext Grammars of Melamed et al. (2004), as well as mildly context-sensitive monolingual formalisms such as Tree Adjoining Grammar (Joshi and Schabes, 1997). Thus, work on transforming general LCFRS grammars promises to be widely applicable in both understanding how these formalisms interrelate, and, from a more practical viewpoint, deriving efficient parsing algorithms for them. In this paper, we focus on the problem of transforming an LCFRS grammar into an equivalent grammar for which straightforward application of dynam"
N10-1118,J09-4009,1,\N,Missing
N12-1062,D08-1024,0,0.394772,"a binary classifier. In this work, we use linear regression and show that our approach is as effective as using a binary classifier and converges faster. 1 Introduction Since its introduction, the minimum error rate training (MERT) (Och, 2003) method has been the most popular method used for parameter tuning in machine translation. Although MERT has nice properties such as simplicity, effectiveness and speed, it is known to not scale well for systems with large numbers of features. One alternative that has been used for large numbers of features is the Margin Infused Relaxed Algorithm (MIRA) (Chiang et al., 2008). MIRA works well with a large number of features, but the optimization problem is much more complicated than MERT. MIRA also involves some modifications to the decoder itself to produce hypotheses with high scores against gold translations. Hopkins and May (2011) introduced the method of pairwise ranking optimization (PRO), which casts the problem of tuning as a ranking problem between pairs of translation candidates. The problem is solved by doing a binary classification between “correctly ordered” and “incorrectly ordered” pairs. Hopkins and May (2011) use the maximum entropy classifier Meg"
N12-1062,J07-2003,0,0.121081,"extracted a general SCFG (GHKM) grammar using standard methods (Galley et al., 2004; Wang et al., 2010) from the parallel corpus with a modification to preclude any unary rules (Chung et al., 2011). All rules over scope 3 are pruned (Hopkins and Langmead, 2010). A set of nine standard features was used for the experiments, which includes globally normalized count of rules, lexical weighting (Koehn et al., 2003), and length penalty. Our in-house decoder was used for experiments with a trigram language model. The decoder is capable of both CNF parsing and Earley-style parsing with cube-pruning (Chiang, 2007). We implemented linear regression tuning using 1 We randomly sampled our data from various different sources (LDC2006E86, LDC2006E93, LDC2002E18, LDC2002L27, LDC2003E07, LDC2003E14, LDC2004T08, LDC2005T06, LDC2005T10, LDC2005T34, LDC2006E26, LDC2005E83, LDC2006E34, LDC2006E85, LDC2006E92, LDC2006E24, LDC2006E92, LDC2006E24) The language model is trained on the English side of entire data (1.65M sentences, which is 39.3M words.) 545 Average of max BLEU dev test 27.7 (0.91) 26.4 (0.82) 26.9 (1.05) 25.6 (0.84) Regression PRO Max BLEU dev test 29.0 27.6 28.0 27.2 Table 1: Average of maximum BLEU"
N12-1062,P11-2072,1,0.773309,"is 6.3M words on the English side. The corpus derives from newswire texts available from LDC.1 We used a 392sentence development set with four references for parameter tuning, and a 428-sentence test set with four references for testing. They are drawn from the newswire portion of NIST evaluations (2004, 2005, 2006). The development set and the test set only had sentences with less than 30 words for decoding speed. We extracted a general SCFG (GHKM) grammar using standard methods (Galley et al., 2004; Wang et al., 2010) from the parallel corpus with a modification to preclude any unary rules (Chung et al., 2011). All rules over scope 3 are pruned (Hopkins and Langmead, 2010). A set of nine standard features was used for the experiments, which includes globally normalized count of rules, lexical weighting (Koehn et al., 2003), and length penalty. Our in-house decoder was used for experiments with a trigram language model. The decoder is capable of both CNF parsing and Earley-style parsing with cube-pruning (Chiang, 2007). We implemented linear regression tuning using 1 We randomly sampled our data from various different sources (LDC2006E86, LDC2006E93, LDC2002E18, LDC2002L27, LDC2003E07, LDC2003E14, L"
N12-1062,N04-1035,0,0.0692895,"rallel corpus with the English side parsed for our experiments. The corpus consists of 250K sentence pairs, which is 6.3M words on the English side. The corpus derives from newswire texts available from LDC.1 We used a 392sentence development set with four references for parameter tuning, and a 428-sentence test set with four references for testing. They are drawn from the newswire portion of NIST evaluations (2004, 2005, 2006). The development set and the test set only had sentences with less than 30 words for decoding speed. We extracted a general SCFG (GHKM) grammar using standard methods (Galley et al., 2004; Wang et al., 2010) from the parallel corpus with a modification to preclude any unary rules (Chung et al., 2011). All rules over scope 3 are pruned (Hopkins and Langmead, 2010). A set of nine standard features was used for the experiments, which includes globally normalized count of rules, lexical weighting (Koehn et al., 2003), and length penalty. Our in-house decoder was used for experiments with a trigram language model. The decoder is capable of both CNF parsing and Earley-style parsing with cube-pruning (Chiang, 2007). We implemented linear regression tuning using 1 We randomly sampled"
N12-1062,D10-1063,0,0.0120032,"om newswire texts available from LDC.1 We used a 392sentence development set with four references for parameter tuning, and a 428-sentence test set with four references for testing. They are drawn from the newswire portion of NIST evaluations (2004, 2005, 2006). The development set and the test set only had sentences with less than 30 words for decoding speed. We extracted a general SCFG (GHKM) grammar using standard methods (Galley et al., 2004; Wang et al., 2010) from the parallel corpus with a modification to preclude any unary rules (Chung et al., 2011). All rules over scope 3 are pruned (Hopkins and Langmead, 2010). A set of nine standard features was used for the experiments, which includes globally normalized count of rules, lexical weighting (Koehn et al., 2003), and length penalty. Our in-house decoder was used for experiments with a trigram language model. The decoder is capable of both CNF parsing and Earley-style parsing with cube-pruning (Chiang, 2007). We implemented linear regression tuning using 1 We randomly sampled our data from various different sources (LDC2006E86, LDC2006E93, LDC2002E18, LDC2002L27, LDC2003E07, LDC2003E14, LDC2004T08, LDC2005T06, LDC2005T10, LDC2005T34, LDC2006E26, LDC20"
N12-1062,D11-1125,0,0.705465,"e most popular method used for parameter tuning in machine translation. Although MERT has nice properties such as simplicity, effectiveness and speed, it is known to not scale well for systems with large numbers of features. One alternative that has been used for large numbers of features is the Margin Infused Relaxed Algorithm (MIRA) (Chiang et al., 2008). MIRA works well with a large number of features, but the optimization problem is much more complicated than MERT. MIRA also involves some modifications to the decoder itself to produce hypotheses with high scores against gold translations. Hopkins and May (2011) introduced the method of pairwise ranking optimization (PRO), which casts the problem of tuning as a ranking problem between pairs of translation candidates. The problem is solved by doing a binary classification between “correctly ordered” and “incorrectly ordered” pairs. Hopkins and May (2011) use the maximum entropy classifier MegaM (Daum´e III, 2004) to do the binary classification. Their method compares well to the results of MERT, scales better for high dimensional feature spaces, and is simpler than MIRA. In this paper, we use the same idea for tuning, but, instead of using a classifie"
N12-1062,N03-1017,0,0.0296513,"erences for testing. They are drawn from the newswire portion of NIST evaluations (2004, 2005, 2006). The development set and the test set only had sentences with less than 30 words for decoding speed. We extracted a general SCFG (GHKM) grammar using standard methods (Galley et al., 2004; Wang et al., 2010) from the parallel corpus with a modification to preclude any unary rules (Chung et al., 2011). All rules over scope 3 are pruned (Hopkins and Langmead, 2010). A set of nine standard features was used for the experiments, which includes globally normalized count of rules, lexical weighting (Koehn et al., 2003), and length penalty. Our in-house decoder was used for experiments with a trigram language model. The decoder is capable of both CNF parsing and Earley-style parsing with cube-pruning (Chiang, 2007). We implemented linear regression tuning using 1 We randomly sampled our data from various different sources (LDC2006E86, LDC2006E93, LDC2002E18, LDC2002L27, LDC2003E07, LDC2003E14, LDC2004T08, LDC2005T06, LDC2005T10, LDC2005T34, LDC2006E26, LDC2005E83, LDC2006E34, LDC2006E85, LDC2006E92, LDC2006E24, LDC2006E92, LDC2006E24) The language model is trained on the English side of entire data (1.65M se"
N12-1062,C04-1072,0,0.258278,"to tune the weights, PRO uses the entire k-best list to learn the ranking between the pairs, which can help prevent overfitting. Let g(e) be a scoring function that maps each translation candidate e to a number (score) using a set of reference translations. The most commonly used gold scoring function in machine translation is the BLEU score, which is calculated for the entire corpus, rather than for individual sentences. To use BLEU as our gold scoring function, we need to modify it to make it decomposable for single sentences. One way to do this is to use a variation of BLEU called BLEU+1 (Lin and Och, 2004), which is a smoothed version of the BLEU score. We assume that our machine translation system scores translations by using a scoring function which is a linear combination of the features: h(e) = wT x(e) (1) where w is the weight vector and x is the feature vector. The goal of tuning as ranking is learning weights such that for every two candidate translations e1 and e2 , the following inequality holds: g(e1 ) > g(e2 ) ⇔ h(e1 ) > h(e2 ) (2) Using Equation 1, we can rewrite Equation 2: g(e1 ) > g(e2 ) ⇔ wT (x(e1 ) − x(e2 )) > 0 (3) This problem can be viewed as a binary classification problem"
N12-1062,P03-1021,0,0.237797,"Missing"
N12-1062,J10-2004,0,0.0150147,"e English side parsed for our experiments. The corpus consists of 250K sentence pairs, which is 6.3M words on the English side. The corpus derives from newswire texts available from LDC.1 We used a 392sentence development set with four references for parameter tuning, and a 428-sentence test set with four references for testing. They are drawn from the newswire portion of NIST evaluations (2004, 2005, 2006). The development set and the test set only had sentences with less than 30 words for decoding speed. We extracted a general SCFG (GHKM) grammar using standard methods (Galley et al., 2004; Wang et al., 2010) from the parallel corpus with a modification to preclude any unary rules (Chung et al., 2011). All rules over scope 3 are pruned (Hopkins and Langmead, 2010). A set of nine standard features was used for the experiments, which includes globally normalized count of rules, lexical weighting (Koehn et al., 2003), and length penalty. Our in-house decoder was used for experiments with a trigram language model. The decoder is capable of both CNF parsing and Earley-style parsing with cube-pruning (Chiang, 2007). We implemented linear regression tuning using 1 We randomly sampled our data from variou"
N12-1062,D08-1076,0,\N,Missing
N13-1004,J93-2003,0,0.0759257,"nal Linguistics Turkish-English pair both on hand-aligned data and by running end-to-end machine translation experiments. To evaluate our results, we created gold word alignments for 75 Turkish-English sentences. We obtain significant improvement of AER and BLEU scores over IBM Model 4. Section 2.1 introduces the concept of morpheme alignment in terms of its relation to word alignment. Section 2.2 presents the derivation of the EM algorithm and Section 3 presents the results of our experiments. 2 Two-level Alignment Model (TAM) 2.1 Morpheme Alignment Following the standard alignment models of Brown et al. (1993), we assume one-to-many alignment for both words and morphemes. A word alignment aw (or only a) is a function mapping a set of word positions in a source language sentence to a set of word positions in a target language sentence. A morpheme alignment am is a function mapping a set of morpheme positions in a source language sentence to a set of morpheme positions in a target language sentence. A morpheme position is a pair of integers (j, k), which defines a word position j and a relative morpheme position k in the word at position j. The alignments below are depicted in Figures 1 and 2. aw (1)"
N13-1004,D09-1075,1,0.906515,"account the morpheme, the smallest unit of syntax, beyond merely splitting words. Since morphology has not been addressed explicitly in word alignment models, researchers have resorted to tweaking SMT systems by manipulating the content and the form of what should be the so-called “word”. Since the word is the smallest unit of translation from the standpoint of word alignment models, the central focus of research on translating morphologically rich languages has been decomposition of morphologically complex words into tokens of the right granularity and representation for machine translation. Chung and Gildea (2009) and Naradowsky and Toutanova (2011) use unsupervised methods to find Kemal Oflazer Computer Science Carnegie Mellon University PO Box 24866, Doha, Qatar word segmentations that create a one-to-one mapping of words in both languages. Al-Onaizan et al. ˇ (1999), Cmejrek et al. (2003), and Goldwater and McClosky (2005) manipulate morphologically rich languages by selective lemmatization. Lee (2004) attempts to learn the probability of deleting or merging Arabic morphemes for Arabic to English translation. Niessen and Ney (2000) split German compound nouns, and merge German phrases that correspon"
N13-1004,E03-1004,0,0.650851,"Missing"
N13-1004,H05-1085,0,0.40777,"smallest unit of translation from the standpoint of word alignment models, the central focus of research on translating morphologically rich languages has been decomposition of morphologically complex words into tokens of the right granularity and representation for machine translation. Chung and Gildea (2009) and Naradowsky and Toutanova (2011) use unsupervised methods to find Kemal Oflazer Computer Science Carnegie Mellon University PO Box 24866, Doha, Qatar word segmentations that create a one-to-one mapping of words in both languages. Al-Onaizan et al. ˇ (1999), Cmejrek et al. (2003), and Goldwater and McClosky (2005) manipulate morphologically rich languages by selective lemmatization. Lee (2004) attempts to learn the probability of deleting or merging Arabic morphemes for Arabic to English translation. Niessen and Ney (2000) split German compound nouns, and merge German phrases that correspond to a single English word. Alternatively, Yeniterzi and Oflazer (2010) manipulate words of the morphologically poor side of a language pair to mimic having a morphological structure similar to the richer side via exploiting syntactic structure, in order to improve the similarity of words on both sides of the transla"
N13-1004,D07-1031,0,0.0816608,"miss+VB–VBG+JJ person+NN–NNS search+NN unit+NN within+IN the+DT minister+NN– y+N|N. of+IN the+DT interior+NN .+. (e) In+IN November+NNP 1996+CD the+DT Turkish+JJ authorities+NNS set+VBD up+RP a+DT missing+JJ persons+NNS search+NN unit+NN within+IN the+DT Ministry+NNP of+IN the+DT Interior+NNP .+. Figure 3: Turkish-English data examples amounts to a small change to the M step of the original EM algorithm. We introduce Dirichlet priors α to perform an inexact normalization by applying the function f (v) = exp(ψ(v)) to the expected counts collected in the E step, where ψ is the digamma function (Johnson, 2007). θx|y = f (E[c(x|y)] + α) P f ( j E[c(xj |y)] + α) We set α to 10−20 , a very low value, to have the effect of anti-smoothing, as low values of α cause the algorithm to favor words which co-occur frequently and to penalize words that co-occur rarely. 3 Experimental Setup 3.1 Data We trained our model on a Turkish-English parallel corpus of approximately 50K sentences, which have a maximum of 80 morphemes. Our parallel data consists mainly of documents in international relations and legal documents from sources such as the Turkish Ministry of Foreign Affairs, EU, etc. We followed a heavily sup"
N13-1004,N03-1017,0,0.0315582,"assed GIZA++ in the Moses toolkit (Och and Ney, 2003). We also ran GIZA++ (IBM Model 1–4) on the data. We translated 1000 sentence test sets. 4 Results and Discussion We evaluated the performance of our model in two different ways. First, we evaluated against gold word alignments for 75 Turkish-English sentences. Second, we used the word Viterbi alignments of our algorithm to obtain BLEU scores. Table 2 shows the AER (Och and Ney, 2003) of the word alignments of the Turkish-English pair and the translation performance of the word alignments learned by our models. We report the grow-diagfinal (Koehn et al., 2003) of the Viterbi alignments. In Table 2, results obtained with different versions of the English data are represented as follows: ‘Der’ stands for derivational morphology, ‘Inf’ for inflectional morphology, and ‘POS’ for part-of-speech 38 tags. ‘Der+Inf’ corresponds to the example sentence in line (d) in Figure 3, and ‘POS’ to line (e). ‘DIR’ stands for models with Dirichlet priors, and ‘NO DIR’ stands for models without Dirichlet priors. All reported results are of the HMM extension of respective models. Table 2 shows that using Dirichlet priors hurts the AER performance of the word-and-morphe"
N13-1004,W04-3250,0,0.068395,"stands for models with Dirichlet priors, and ‘NO DIR’ stands for models without Dirichlet priors. All reported results are of the HMM extension of respective models. Table 2 shows that using Dirichlet priors hurts the AER performance of the word-and-morpheme model in all experiment settings, and benefits the morpheme-only model in the POS tagged experiment settings. In order to reduce the effect of nondeterminism, we run Moses three times per experiment setting, and report the highest BLEU scores obtained. Since the BLEU scores we obtained are close, we did a significance test on the scores (Koehn, 2004). Table 2 visualizes the partition of the BLEU scores into statistical significance groups. If two scores within the same column have the same background color, or the border between their cells is removed, then the difference between their scores is not statistically significant. For example, the best BLEU scores, which are in bold, have white background. All scores in a given experiment setting without white background are significantly worse than the best score in that experiment setting, unless there is no border separating them from the best score. In all experiment settings, the TAM Mode"
N13-1004,N04-4015,0,0.12299,"arch on translating morphologically rich languages has been decomposition of morphologically complex words into tokens of the right granularity and representation for machine translation. Chung and Gildea (2009) and Naradowsky and Toutanova (2011) use unsupervised methods to find Kemal Oflazer Computer Science Carnegie Mellon University PO Box 24866, Doha, Qatar word segmentations that create a one-to-one mapping of words in both languages. Al-Onaizan et al. ˇ (1999), Cmejrek et al. (2003), and Goldwater and McClosky (2005) manipulate morphologically rich languages by selective lemmatization. Lee (2004) attempts to learn the probability of deleting or merging Arabic morphemes for Arabic to English translation. Niessen and Ney (2000) split German compound nouns, and merge German phrases that correspond to a single English word. Alternatively, Yeniterzi and Oflazer (2010) manipulate words of the morphologically poor side of a language pair to mimic having a morphological structure similar to the richer side via exploiting syntactic structure, in order to improve the similarity of words on both sides of the translation. We present an alignment model that assumes internal structure for words, an"
N13-1004,P04-1066,0,0.0443788,"Missing"
N13-1004,P11-1090,0,0.018095,"allest unit of syntax, beyond merely splitting words. Since morphology has not been addressed explicitly in word alignment models, researchers have resorted to tweaking SMT systems by manipulating the content and the form of what should be the so-called “word”. Since the word is the smallest unit of translation from the standpoint of word alignment models, the central focus of research on translating morphologically rich languages has been decomposition of morphologically complex words into tokens of the right granularity and representation for machine translation. Chung and Gildea (2009) and Naradowsky and Toutanova (2011) use unsupervised methods to find Kemal Oflazer Computer Science Carnegie Mellon University PO Box 24866, Doha, Qatar word segmentations that create a one-to-one mapping of words in both languages. Al-Onaizan et al. ˇ (1999), Cmejrek et al. (2003), and Goldwater and McClosky (2005) manipulate morphologically rich languages by selective lemmatization. Lee (2004) attempts to learn the probability of deleting or merging Arabic morphemes for Arabic to English translation. Niessen and Ney (2000) split German compound nouns, and merge German phrases that correspond to a single English word. Alternat"
N13-1004,C00-2162,0,0.282407,"f the right granularity and representation for machine translation. Chung and Gildea (2009) and Naradowsky and Toutanova (2011) use unsupervised methods to find Kemal Oflazer Computer Science Carnegie Mellon University PO Box 24866, Doha, Qatar word segmentations that create a one-to-one mapping of words in both languages. Al-Onaizan et al. ˇ (1999), Cmejrek et al. (2003), and Goldwater and McClosky (2005) manipulate morphologically rich languages by selective lemmatization. Lee (2004) attempts to learn the probability of deleting or merging Arabic morphemes for Arabic to English translation. Niessen and Ney (2000) split German compound nouns, and merge German phrases that correspond to a single English word. Alternatively, Yeniterzi and Oflazer (2010) manipulate words of the morphologically poor side of a language pair to mimic having a morphological structure similar to the richer side via exploiting syntactic structure, in order to improve the similarity of words on both sides of the translation. We present an alignment model that assumes internal structure for words, and we can legitimately talk about words and their morphemes in line with the linguistic conception of these terms. Our model avoids t"
N13-1004,J03-1002,0,0.160536,"heme alignments, weighted by their probability. The morpheme count function below collects evidence from a word pair (e, f ) in a sentence pair (e, f ) as follows: For all words ej of the sentence e and for all word alignments aw (j), for all morphemes ekj of the word ej and for all morpheme alignments am (j, k), we collect counts for a particular input morpheme g and an output morpheme h iff ej = e and faw (j) = f and h = ekj and g = fam (j,k) . cm (h|g; e, f , aw , am ) = X X T (e|f ) |f | P 2lf − 1 words for the HMM model, the positions > lf stand for null positions between the words of f (Och and Ney, 2003). We do not allow null to null jumps. In sum, we enforce the following constraints: t(h|g) P (i + lf + 1|i0 ) = p(null|i0 ) |f | P P (i + lf + 1|i0 + lf + 1) = 0 1≤j≤|e| 1≤k≤|e| T (e|fi ) t(h|f i ) s.t. s.t. k i=0 i=1 e=ej h=ej f =faw (j) g=f am (j,k) P (i|i0 + lf + 1) = p(i|i0 ) The left part of the morpheme count function is the same as the word-counts in Eqn. 5. Since it does not contain h or g, it needs to be computed only once for each word. The right part of the equation is familiar from the IBM Model 1 counts. In the HMM extension of TAM, we perform forward-backward training using the w"
N13-1004,C96-2141,0,0.876455,"P (i|i0 + lf + 1) = p(i|i0 ) The left part of the morpheme count function is the same as the word-counts in Eqn. 5. Since it does not contain h or g, it needs to be computed only once for each word. The right part of the equation is familiar from the IBM Model 1 counts. In the HMM extension of TAM, we perform forward-backward training using the word counts in Eqn. 5 as the emission probabilities. We calculate the posterior word translation probabilities for each ej and fi such that 1 ≤ j ≤ le and 1 ≤ i ≤ 2lf − 1 as follows: 2.3 HMM Extension γj (i) = We implemented TAM with the HMM extension (Vogel et al., 1996) at the word level. We redefine p(e|f ) as follows: |e| R(e, f ) XY p(s(j ) |C (faw (j −1 ) )) t(ej |faw (j) ) aw j=1 R(ej , faw (j) ) |ej | XY ! t(ekj |fam (j,k) ) αj (i)βj (i) 2lP f −1 where α is the forward and β is the backward probabilities of the HMM. The HMM word counts, in turn, are the posterior word translation probabilities obtained from the forward-backward training: X cw (e|f ; e, f , aw ) = γj (aw (j)) 1≤j≤|e| s.t. e=ej f =faw (j) am k=1 where the distortion probability depends on the relative jump width s(j) = aw (j − 1) − aw (j), as opposed to absolute positions. The distortion"
N13-1004,P10-1047,1,0.87694,"supervised methods to find Kemal Oflazer Computer Science Carnegie Mellon University PO Box 24866, Doha, Qatar word segmentations that create a one-to-one mapping of words in both languages. Al-Onaizan et al. ˇ (1999), Cmejrek et al. (2003), and Goldwater and McClosky (2005) manipulate morphologically rich languages by selective lemmatization. Lee (2004) attempts to learn the probability of deleting or merging Arabic morphemes for Arabic to English translation. Niessen and Ney (2000) split German compound nouns, and merge German phrases that correspond to a single English word. Alternatively, Yeniterzi and Oflazer (2010) manipulate words of the morphologically poor side of a language pair to mimic having a morphological structure similar to the richer side via exploiting syntactic structure, in order to improve the similarity of words on both sides of the translation. We present an alignment model that assumes internal structure for words, and we can legitimately talk about words and their morphemes in line with the linguistic conception of these terms. Our model avoids the problem of collapsing words and morphemes into one single category. We adopt a twolevel representation of alignment: the first level invo"
N13-1020,E06-1032,0,\N,Missing
N13-1020,N03-2021,0,\N,Missing
N13-1020,P02-1040,0,\N,Missing
N15-1017,P06-1009,0,0.0317446,"show that unsupervised alignment is feasible, they considered the mappings between nouns and blobs only, and ignored the verbs and other relations in the sentences. Moreover, incorporating domain knowledge is not straightforward in these generative models. 2.2 Discriminative Word Alignment In machine translation, alignment of the words in source language with the words in target language has traditionally been done using the IBM word alignment models (Brown et al., 1993), which are generative models, and typically trained using Expectation Maximization (Dempster et al., 1977). Early attempts (Blunsom and Cohn, 2006; Taskar et al., 2005) towards discriminative word alignment relied on supervised hand-aligned parallel corpora. Dyer et al. (2011) first applied a latent variable conditional random field (LCRF) to perform unsupervised discriminative word alignment. They treated the words’ alignments as latent variables, and formulated the task as predicting the target sentence, given the source sentence. We apply similar latent variable discriminative models for unsupervised alignment of sentences with video segments. 3 Problem Formulation and Notations The input to our system is a dataset containing N pairs"
N15-1017,J93-2003,0,0.0625682,"lab experiment videos with associated text protocols, without any direct supervision. They proposed a hierarchical generative model to infer the alignment between each video segment with corresponding protocol sentence, and also the mapping of each blob with corresponding noun in that sentence. First, it models the generation of each video segment from one of the sentences in the protocol using a Hidden Markov Model (HMM) (Rabiner, 1989; Vogel et al., 1996). Next, each tracked object or blob in a video segment is generated from one of the nouns in the corresponding sentence using IBM Model 1 (Brown et al., 1993), a generative model frequently used in machine translation. The IBM Model 1 probabilities are incorporated as emission probabilities in HMM. The transition probabilities are parameterized using the jump size, i.e., the difference between the alignments of two consecutive video segments. They also extended IBM Model 1 by introducing latent variables for each noun, allowing some of the nonobject nouns to be unobserved in the video. While the alignment results are encouraging, and show that unsupervised alignment is feasible, they considered the mappings between nouns and blobs only, and ignored"
N15-1017,P05-1022,0,0.0138272,"ocols are not necessarily unique, as we have multiple videos of different people carrying out the same protocol. 166 xi = Xi,1 hi = 1 2 yi = Yi,1 Yi,2 Xi,2 Xi,3 2 3 Yi,3 Yi,4 Figure 2: Each Xi,m is a sentence in the protocol, consisting of the nouns and verbs in the sentence, and each Yi,n is a video chunk represented by the set of blobs touched by hands in that chunk. The alignment hi = [1, 2, 2, 3] maps each video chunk to the corresponding sentence. We apply similar data preprocessing as Naim et al. (2014). First, we parse each protocol sentence using the two-stage Charniak-Johnson parser (Charniak and Johnson, 2005), and extract the head nouns and verbs from each sentence. Let mi be the number of sentences in the protocol xi . We represent xi as a sequence of sets xi = [Xi,1 , . . . , Xi,mi ], where Xi,m is the set of nouns and verbs in the mth sentence of xi . Each video yi is segmented into a sequence of chunks, each one second long. For each video chunk, we determine the set of objects touched by the participant’s hands using automated image segmentation and tracking. We ignore the chunks over which no object is touched by a hand. Let ni be the number of chunks in yi . We represent the video yi as a s"
N15-1017,W02-1001,0,0.043586,"utational Linguistics tion to its corresponding video segment, and to align nouns in each instruction to their corresponding objects in the video. We extend this generative alignment framework by applying several discriminative models with latent variables. Discriminative models are attractive as they can easily incorporate domain knowledge by adding many diverse, overlapping, and complex features. By incorporating a large number of features and regularizing their weights properly, discriminative models have been shown to outperform generative models in many natural language processing tasks (Collins, 2002; Dyer et al., 2011; Yu et al., 2013). Similar to Naim et al. (2014), we applied our algorithm to align the natural language instructions for biological experiments in “wet laboratories” with recorded videos of people performing these experiments. Typically, each wetlab experiment has a protocol written in natural language, describing the sequence of steps necessary for that experiment. However, these instructions are often incomplete, and do not spell out implicit assumptions and knowledge, causing the results to be difficult to reproduce (Begley and Ellis, 2012). Given a set of such wetlab e"
N15-1017,P11-1042,0,0.203753,"istics tion to its corresponding video segment, and to align nouns in each instruction to their corresponding objects in the video. We extend this generative alignment framework by applying several discriminative models with latent variables. Discriminative models are attractive as they can easily incorporate domain knowledge by adding many diverse, overlapping, and complex features. By incorporating a large number of features and regularizing their weights properly, discriminative models have been shown to outperform generative models in many natural language processing tasks (Collins, 2002; Dyer et al., 2011; Yu et al., 2013). Similar to Naim et al. (2014), we applied our algorithm to align the natural language instructions for biological experiments in “wet laboratories” with recorded videos of people performing these experiments. Typically, each wetlab experiment has a protocol written in natural language, describing the sequence of steps necessary for that experiment. However, these instructions are often incomplete, and do not spell out implicit assumptions and knowledge, causing the results to be difficult to reproduce (Begley and Ellis, 2012). Given a set of such wetlab experiment protocols"
N15-1017,W13-1302,0,0.0200422,"Missing"
N15-1017,Q13-1016,0,0.0233218,"blobs, and ignored verbs, we incorporated the co-occurrences of verbs with blobs as features in our model. Finally, we propose a constrained variant of the standard LSP and LSSVM update rule, which provided better alignment accuracy and more stable convergence on our datasets. 165 2 2.1 Background Research Unsupervised Grounded Language Learning Most existing grounded language learning algorithms for integrating language with vision rely on either a fully supervised (Kollar et al., 2010; Matuszek et al., 2012) or a weakly supervised training stage (Yu and Ballard, 2004; Kate and Mooney, 2007; Krishnamurthy and Kollar, 2013; Yu and Siskind, 2013; Krishnamoorthy et al., 2013; Rohrbach et al., 2013; Tellex et al., 2013). The fully supervised methods assume that each sentence in the training data is manually paired with the corresponding image or video segment, and furthermore, each word or phrase in a sentence is already mapped to its corresponding blob or action in the image or video segment. Given the detailed annotations, these methods train a set of classifiers to recognize perceptual representations for commonly used words or phrases. After the initial fully supervised training stage, these methods can learn"
N15-1017,P06-1096,0,0.0370273,"Missing"
N15-1017,H05-1010,0,0.0253354,"lignment is feasible, they considered the mappings between nouns and blobs only, and ignored the verbs and other relations in the sentences. Moreover, incorporating domain knowledge is not straightforward in these generative models. 2.2 Discriminative Word Alignment In machine translation, alignment of the words in source language with the words in target language has traditionally been done using the IBM word alignment models (Brown et al., 1993), which are generative models, and typically trained using Expectation Maximization (Dempster et al., 1977). Early attempts (Blunsom and Cohn, 2006; Taskar et al., 2005) towards discriminative word alignment relied on supervised hand-aligned parallel corpora. Dyer et al. (2011) first applied a latent variable conditional random field (LCRF) to perform unsupervised discriminative word alignment. They treated the words’ alignments as latent variables, and formulated the task as predicting the target sentence, given the source sentence. We apply similar latent variable discriminative models for unsupervised alignment of sentences with video segments. 3 Problem Formulation and Notations The input to our system is a dataset containing N pairs of observations {(xi"
N15-1017,C96-2141,0,0.135175,"extend to new domains, as this may require collecting new annotated data. Recently, Naim et al. (2014) proposed a fully unsupervised approach for aligning wetlab experiment videos with associated text protocols, without any direct supervision. They proposed a hierarchical generative model to infer the alignment between each video segment with corresponding protocol sentence, and also the mapping of each blob with corresponding noun in that sentence. First, it models the generation of each video segment from one of the sentences in the protocol using a Hidden Markov Model (HMM) (Rabiner, 1989; Vogel et al., 1996). Next, each tracked object or blob in a video segment is generated from one of the nouns in the corresponding sentence using IBM Model 1 (Brown et al., 1993), a generative model frequently used in machine translation. The IBM Model 1 probabilities are incorporated as emission probabilities in HMM. The transition probabilities are parameterized using the jump size, i.e., the difference between the alignments of two consecutive video segments. They also extended IBM Model 1 by introducing latent variables for each noun, allowing some of the nonobject nouns to be unobserved in the video. While t"
N15-1017,P13-1006,0,0.168923,"orporated the co-occurrences of verbs with blobs as features in our model. Finally, we propose a constrained variant of the standard LSP and LSSVM update rule, which provided better alignment accuracy and more stable convergence on our datasets. 165 2 2.1 Background Research Unsupervised Grounded Language Learning Most existing grounded language learning algorithms for integrating language with vision rely on either a fully supervised (Kollar et al., 2010; Matuszek et al., 2012) or a weakly supervised training stage (Yu and Ballard, 2004; Kate and Mooney, 2007; Krishnamurthy and Kollar, 2013; Yu and Siskind, 2013; Krishnamoorthy et al., 2013; Rohrbach et al., 2013; Tellex et al., 2013). The fully supervised methods assume that each sentence in the training data is manually paired with the corresponding image or video segment, and furthermore, each word or phrase in a sentence is already mapped to its corresponding blob or action in the image or video segment. Given the detailed annotations, these methods train a set of classifiers to recognize perceptual representations for commonly used words or phrases. After the initial fully supervised training stage, these methods can learn the meaning of new wor"
N15-1017,D13-1112,1,0.889793,"corresponding video segment, and to align nouns in each instruction to their corresponding objects in the video. We extend this generative alignment framework by applying several discriminative models with latent variables. Discriminative models are attractive as they can easily incorporate domain knowledge by adding many diverse, overlapping, and complex features. By incorporating a large number of features and regularizing their weights properly, discriminative models have been shown to outperform generative models in many natural language processing tasks (Collins, 2002; Dyer et al., 2011; Yu et al., 2013). Similar to Naim et al. (2014), we applied our algorithm to align the natural language instructions for biological experiments in “wet laboratories” with recorded videos of people performing these experiments. Typically, each wetlab experiment has a protocol written in natural language, describing the sequence of steps necessary for that experiment. However, these instructions are often incomplete, and do not spell out implicit assumptions and knowledge, causing the results to be difficult to reproduce (Begley and Ellis, 2012). Given a set of such wetlab experiment protocols and associated vi"
N18-2090,P16-1014,0,0.046067,"wer, which encodes all words and the word order. Attentive-matching synthesizes a vector by computing a weighted sum of all answer states against the passage state, then compares the vector with the passage state. It also considers all words in the answer but without word order. Finally, max-attentive-matching only considers the most relevant answer state to the passage state. Pvocab = softmax(V1 [st ; ct ] + b1 ), where V1 and b1 are model parameters, and the number of rows in V1 is the size of the vocabulary. Since many passage words also appear in the question, we adopt the copy mechanism (Gulcehre et al., 2016; Gu et al., 2016), which integrates the attention over input words into the final vocabulary distribution. The probability distribution is defined as the interpolation: Pf inal = gt Pvocab + (1 − gt )Pattn , where gt is the switch for controlling generating a word from the vocabulary or directly copying it from the passage. Pvocab is the vocabulary probability distribution as defined above, and Pattn is calculated based on the current attention distribution by merging probabilities of duplicated words. Finally, gt is defined as: Multi-perspective matching These strategies require a function f"
N18-2090,D16-1264,0,0.134063,"ng et al., 2017; Wang et al., 2017a; Yuan et al., 2017). These methods can neglect rich potential ∗ We investigate explicit interaction between the target answer and the passage, so that contextual information can be better considered by the encoder. In particular, matching is used between the target answer and the passage for collecting relevant contextual information. We adopt the multiperspective context matching (MPCM) algorithm (Wang et al., 2017b), which takes two texts as input before producing a vector of numbers, representing similarity under different perspectives. Results on SQuAD (Rajpurkar et al., 2016) show that our model gives better BLEU scores than the state of the art. Furthermore, the questions generated by our model help to improve a strong extractive QA system. Our code is available at https://github.com/freesunshine0316/MPQG. Work done during an internship at IBM. 569 Proceedings of NAACL-HLT 2018, pages 569–574 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics 2 Baseline: sequence-to-sequence Our baseline is a sequence-to-sequence model (Bahdanau et al., 2015) with the copy mechanism (Gulcehre et al., 2016; Gu et al., 2016). It uses an LSTM"
N18-2090,N10-1086,0,0.479507,"born. This can be easily determined by leveraging the contextual information of “10 july 1856 – 7 january 1943”, while it is relatively hard when only the answer position information is adopted. Introduction The task of natural question generation (NQG) is to generate a fluent and relevant question given a passage and a target answer. Recently NQG has received increasing attention from both the industrial and academic communities because of its values for improving QA systems by automatically increasing the training data. It can also be used for educational purposes such as language learning (Heilman and Smith, 2010). One example is shown in Table 1, where a question “when was nikola tesla born ?” is generated given a passage and a fact “1856”. Existing work for NQG uses a sequence-to-sequence model (Sutskever et al., 2014), which takes a passage as input for generating a question. They either entirely ignore the target answer (Du et al., 2017), or directly hard-code answer positions (Zhou et al., 2017; Yang et al., 2017; Subramanian et al., 2017; Tang et al., 2017; Wang et al., 2017a; Yuan et al., 2017). These methods can neglect rich potential ∗ We investigate explicit interaction between the target ans"
N18-2090,D17-1090,0,0.106253,"Missing"
N18-2090,P17-1123,0,0.132216,"tly NQG has received increasing attention from both the industrial and academic communities because of its values for improving QA systems by automatically increasing the training data. It can also be used for educational purposes such as language learning (Heilman and Smith, 2010). One example is shown in Table 1, where a question “when was nikola tesla born ?” is generated given a passage and a fact “1856”. Existing work for NQG uses a sequence-to-sequence model (Sutskever et al., 2014), which takes a passage as input for generating a question. They either entirely ignore the target answer (Du et al., 2017), or directly hard-code answer positions (Zhou et al., 2017; Yang et al., 2017; Subramanian et al., 2017; Tang et al., 2017; Wang et al., 2017a; Yuan et al., 2017). These methods can neglect rich potential ∗ We investigate explicit interaction between the target answer and the passage, so that contextual information can be better considered by the encoder. In particular, matching is used between the target answer and the passage for collecting relevant contextual information. We adopt the multiperspective context matching (MPCM) algorithm (Wang et al., 2017b), which takes two texts as input be"
N18-2090,P16-1154,0,0.116662,"words and the word order. Attentive-matching synthesizes a vector by computing a weighted sum of all answer states against the passage state, then compares the vector with the passage state. It also considers all words in the answer but without word order. Finally, max-attentive-matching only considers the most relevant answer state to the passage state. Pvocab = softmax(V1 [st ; ct ] + b1 ), where V1 and b1 are model parameters, and the number of rows in V1 is the size of the vocabulary. Since many passage words also appear in the question, we adopt the copy mechanism (Gulcehre et al., 2016; Gu et al., 2016), which integrates the attention over input words into the final vocabulary distribution. The probability distribution is defined as the interpolation: Pf inal = gt Pvocab + (1 − gt )Pattn , where gt is the switch for controlling generating a word from the vocabulary or directly copying it from the passage. Pvocab is the vocabulary probability distribution as defined above, and Pattn is calculated based on the current attention distribution by merging probabilities of duplicated words. Finally, gt is defined as: Multi-perspective matching These strategies require a function fm to match two vec"
N18-2090,P17-1096,0,0.0263172,"ic communities because of its values for improving QA systems by automatically increasing the training data. It can also be used for educational purposes such as language learning (Heilman and Smith, 2010). One example is shown in Table 1, where a question “when was nikola tesla born ?” is generated given a passage and a fact “1856”. Existing work for NQG uses a sequence-to-sequence model (Sutskever et al., 2014), which takes a passage as input for generating a question. They either entirely ignore the target answer (Du et al., 2017), or directly hard-code answer positions (Zhou et al., 2017; Yang et al., 2017; Subramanian et al., 2017; Tang et al., 2017; Wang et al., 2017a; Yuan et al., 2017). These methods can neglect rich potential ∗ We investigate explicit interaction between the target answer and the passage, so that contextual information can be better considered by the encoder. In particular, matching is used between the target answer and the passage for collecting relevant contextual information. We adopt the multiperspective context matching (MPCM) algorithm (Wang et al., 2017b), which takes two texts as input before producing a vector of numbers, representing similarity under different pe"
N18-2090,W17-2603,0,0.0998808,"asing the training data. It can also be used for educational purposes such as language learning (Heilman and Smith, 2010). One example is shown in Table 1, where a question “when was nikola tesla born ?” is generated given a passage and a fact “1856”. Existing work for NQG uses a sequence-to-sequence model (Sutskever et al., 2014), which takes a passage as input for generating a question. They either entirely ignore the target answer (Du et al., 2017), or directly hard-code answer positions (Zhou et al., 2017; Yang et al., 2017; Subramanian et al., 2017; Tang et al., 2017; Wang et al., 2017a; Yuan et al., 2017). These methods can neglect rich potential ∗ We investigate explicit interaction between the target answer and the passage, so that contextual information can be better considered by the encoder. In particular, matching is used between the target answer and the passage for collecting relevant contextual information. We adopt the multiperspective context matching (MPCM) algorithm (Wang et al., 2017b), which takes two texts as input before producing a vector of numbers, representing similarity under different perspectives. Results on SQuAD (Rajpurkar et al., 2016) show that our model gives bette"
P00-1065,P98-1013,0,0.0933979,"more accurate parsers and better language models for speech recognition. This paper proposes an algorithm for automatic semantic analysis, assigning a semantic role to constituents in a sentence. Our approach to semantic analysis is to treat the problem of semantic role labeling like the similar problems of parsing, part of speech tagging, and word sense disambiguation. We apply statistical techniques that have been successful for these tasks, including probabilistic parsing and statistical classi cation. Our statistical algorithms are trained on a hand-labeled dataset: the FrameNet database (Baker et al., 1998). The FrameNet database de nes a tagset of semantic roles called frame elements, and includes roughly 50,000 sentences from the British National Corpus which have been hand-labeled with these frame elements. The next section describes the set of frame elements/semantic roles used by our system. In the rest of this paper we report on our current system, as well as a number of preliminary experiments on extensions to the system. 2 Semantic Roles Historically, two types of semantic roles have been studied: abstract roles such as Agent and Patient, and roles speci c to individual verbs such as Eat"
P00-1065,A00-2031,0,0.0145381,"ilo (1993) builds a dictionary of patterns for lling slots in a speci c domain such as terrorist attacks, and Rilo and Schmelzenbach (1998) extend this technique to automatically derive entire case frames for words in the domain. These last systems make use of a limited amount of hand labor to accept or reject automatically generated hypotheses. They show promise for a more sophisticated approach to generalize beyond the relatively small number of frames considered in the tasks. More recently, a domain independent system has been trained on general function tags such as Manner and Temporal by Blaheta and Charniak (2000). 4 Methodology We divide the task of labeling frame elements into two subtasks: that of identifying the boundaries of the frame elements in the sentences, and that of labeling each frame element, given its boundaries, with the correct role. We rst give results for a system which Domain: Communication Frame: Frame: Conversation Frame Elements: Protagonist−1 Protagonist−2 Protagonists Topic Medium debate−v tiff−n dispute−n converse−v gossip−v discussion−n Speaker Addressee Message Topic Medium Statement Frame Elements: confer−v Cognition Questioning Frame Elements: Frame: talk−v Domain: Speaker"
P00-1065,W98-1505,0,0.0161728,"Missing"
P00-1065,P97-1003,0,0.0374372,"nd Pat had an argument about politics Kim and Pat argued in French Kim and pat had an argument in French Medium Table 1: Examples of semantic roles, or frame elements, for target words argue&quot; and argument&quot; from the conversation&quot; frame labels roles using human-annotated boundaries, returning to the question of automatically identifying the boundaries in Section 5.3. 4.1 Features Used in Assigning Semantic Roles The system is a statistical one, based on training a classi er on a labeled training set, and testing on an unlabeled test set. The system is trained by rst using the Collins parser (Collins, 1997) to parse the 36,995 training sentences, matching annotated frame elements to parse constituents, and extracting various features from the string of words and the parse tree. During testing, the parser is run on the test sentences and the same features extracted. Probabilities for each possible semantic role r are then computed from the features. The probability computation will be described in the next section; the features include: Phrase Type: This feature indicates the syntactic type of the phrase expressing the semantic roles: examples include noun phrase (NP), verb phrase (VP), and claus"
P00-1065,P99-1001,0,0.0270721,"e the roles associated with a word can be cues to its sense. For example, Lapata and Brew (1999) and others have shown that the di erent syntactic subcatgorization frames of a verb like serve&quot; can be used to help disambiguate a particular instance of the word serve&quot;. Adding semantic role subcategorization information to this syntactic information could extend this idea to use richer semantic knowledge. Semantic roles could also act as an important intermediate representation in statistical machine translation or automatic text summarization and in the emerging eld of Text Data Mining (TDM) (Hearst, 1999). Finally, incorporating semantic roles into probabilistic models of language should yield more accurate parsers and better language models for speech recognition. This paper proposes an algorithm for automatic semantic analysis, assigning a semantic role to constituents in a sentence. Our approach to semantic analysis is to treat the problem of semantic role labeling like the similar problems of parsing, part of speech tagging, and word sense disambiguation. We apply statistical techniques that have been successful for these tasks, including probabilistic parsing and statistical classi cation"
P00-1065,W99-0632,0,0.0104052,"emplates to extract facts about, for example, nancial news or interesting political events. A shallow semantic level of representation is a more domain-independent, robust level of representation. Identifying these roles, for example, could allow a system to determine that in the sentence The rst one crashed&quot; the subject is the vehicle, but in the sentence The rst one crashed it&quot; the subject is the agent, which would help in information extraction in this domain. Another application is in wordsense disambiguation, where the roles associated with a word can be cues to its sense. For example, Lapata and Brew (1999) and others have shown that the di erent syntactic subcatgorization frames of a verb like serve&quot; can be used to help disambiguate a particular instance of the word serve&quot;. Adding semantic role subcategorization information to this syntactic information could extend this idea to use richer semantic knowledge. Semantic roles could also act as an important intermediate representation in statistical machine translation or automatic text summarization and in the emerging eld of Text Data Mining (TDM) (Hearst, 1999). Finally, incorporating semantic roles into probabilistic models of language shoul"
P00-1065,P98-2127,0,0.141466,"Missing"
P00-1065,P96-1008,0,0.00852743,"tations of uni cation-based grammars such as HPSG (Pollard and Sag, 1994), rely on handdeveloped grammars which must anticipate each way in which semantic roles may be realized syntactically. Writing such grammars is time-consuming, and typically such systems have limited coverage. Data-driven techniques have recently been applied to template-based semantic interpretation in limited domains by shallow&quot; systems that avoid complex feature structures, and often perform only shallow syntactic analysis. For example, in the context of the Air Traveler Information System (ATIS) for spoken dialogue, Miller et al. (1996) computed the probability that a constituent such as Atlanta&quot; lled a semantic slot such as Destination in a semantic frame for air travel. In a data-driven approach to information extraction, Rilo (1993) builds a dictionary of patterns for lling slots in a speci c domain such as terrorist attacks, and Rilo and Schmelzenbach (1998) extend this technique to automatically derive entire case frames for words in the domain. These last systems make use of a limited amount of hand labor to accept or reject automatically generated hypotheses. They show promise for a more sophisticated approach to gen"
P00-1065,W98-1106,0,0.010611,"applied to template-based semantic interpretation in limited domains by shallow&quot; systems that avoid complex feature structures, and often perform only shallow syntactic analysis. For example, in the context of the Air Traveler Information System (ATIS) for spoken dialogue, Miller et al. (1996) computed the probability that a constituent such as Atlanta&quot; lled a semantic slot such as Destination in a semantic frame for air travel. In a data-driven approach to information extraction, Rilo (1993) builds a dictionary of patterns for lling slots in a speci c domain such as terrorist attacks, and Rilo and Schmelzenbach (1998) extend this technique to automatically derive entire case frames for words in the domain. These last systems make use of a limited amount of hand labor to accept or reject automatically generated hypotheses. They show promise for a more sophisticated approach to generalize beyond the relatively small number of frames considered in the tasks. More recently, a domain independent system has been trained on general function tags such as Manner and Temporal by Blaheta and Charniak (2000). 4 Methodology We divide the task of labeling frame elements into two subtasks: that of identifying the boundar"
P00-1065,A00-2030,0,\N,Missing
P00-1065,J93-2004,0,\N,Missing
P00-1065,A00-2034,0,\N,Missing
P00-1065,J03-4003,0,\N,Missing
P00-1065,H94-1020,0,\N,Missing
P00-1065,C98-1013,0,\N,Missing
P00-1065,P99-1014,0,\N,Missing
P00-1065,C98-2122,0,\N,Missing
P00-1065,W00-2021,0,\N,Missing
P00-1065,A00-1010,0,\N,Missing
P00-1065,P93-1024,0,\N,Missing
P02-1031,P98-1013,0,0.174814,"Missing"
P02-1031,P97-1003,0,0.54256,"icate, semantic arguments often have multiple syntactic realizations, as shown by the following paraphrases: (1) John will meet with Mary. John will meet Mary. John and Mary will meet. (2) The door opened. Mary opened the door. Correctly identifying the semantic roles of the sentence constituents is a crucial part of interpreting text, and in addition to forming an important part of the information extraction problem, can serve as an intermediate step in machine translation or automatic summarization. In this paper, we examine how the information provided by modern statistical parsers such as Collins (1997) and Charniak (1997) contributes to solving this problem. We measure the e ect of parser accuracy on semantic role prediction from parse trees, and determine whether a complete tree is indeed necessary for accurate role prediction. Gildea and Jurafsky (2002) describe a statistical system trained on the data from the FrameNet project to automatically assign semantic roles. The system rst passed sentences through an automatic parser, extracted syntactic features from the parses, and estimated probabilities for semantic roles from the syntactic and lexical features. Both training and test sentenc"
P02-1031,W00-0726,0,0.0435352,"Missing"
P02-1031,J02-3001,1,0.903105,"ing the semantic roles of the sentence constituents is a crucial part of interpreting text, and in addition to forming an important part of the information extraction problem, can serve as an intermediate step in machine translation or automatic summarization. In this paper, we examine how the information provided by modern statistical parsers such as Collins (1997) and Charniak (1997) contributes to solving this problem. We measure the e ect of parser accuracy on semantic role prediction from parse trees, and determine whether a complete tree is indeed necessary for accurate role prediction. Gildea and Jurafsky (2002) describe a statistical system trained on the data from the FrameNet project to automatically assign semantic roles. The system rst passed sentences through an automatic parser, extracted syntactic features from the parses, and estimated probabilities for semantic roles from the syntactic and lexical features. Both training and test sentences were automatically parsed, as no hand-annotated parse trees were available for the corpus. While the errors introduced by the parser no doubt negatively affected the results obtained, there was no direct way of quantifying this e ect. Of the systems evalu"
P02-1031,kingsbury-palmer-2002-treebank,1,0.505,"semantic phenomena including quanti cation, anaphora, aspect and modality (e.g. Alshawi (1992)), to simpler nite-state or statistical systems such as Hobbs et al. (1997) and Miller et al. (1998). Much of the evaluation of these systems has been conducted on extracting relations for speci c semantic domains such as corporate acquisitions or terrorist events in the framework of the DARPA Message Understanding Conferences. Recently, attention has turned to creating corpora annotated for argument structure for a broader range of predicates. The Propbank project at the University of Pennsylvania (Kingsbury and Palmer, 2002) and the FrameNet project at the International Computer Science Institute (Baker et al., 1998) share the goal of documenting the syntactic realization of arguments of the predicates of the general English lexicon by annotating a corpus with semantic roles. Even for a single predicate, semantic arguments often have multiple syntactic realizations, as shown by the following paraphrases: (1) John will meet with Mary. John will meet Mary. John and Mary will meet. (2) The door opened. Mary opened the door. Correctly identifying the semantic roles of the sentence constituents is a crucial part of in"
P02-1031,C98-1013,0,\N,Missing
P02-1031,A97-1029,0,\N,Missing
P03-1011,J00-1004,0,0.408768,"ted in the target language, and statistical approaches, pioneered by Brown et al. (1990), which estimate parameters for a model of word-to-word correspondences and word re-orderings directly from large corpora of parallel bilingual text. Only recently have hybrid approaches begun to emerge, which apply probabilistic models to a structured representation of the source text. Wu (1997) showed that restricting word-level alignments between sentence pairs to observe syntactic bracketing constraints significantly reduces the complexity of the alignment problem and allows a polynomial-time solution. Alshawi et al. (2000) also induce parallel tree structures from unbracketed parallel text, modeling the generation of each node’s children with a finite-state transducer. Yamada and Knight (2001) present an algorithm for estimating probabilistic parameters for a similar model which represents translation as a sequence of re-ordering operations over children of nodes in a syntactic tree, using automatic parser output for the initial tree structures. The use of explicit syntactic information for the target language in this model has led to excellent translation results (Yamada and Knight, 2002), and raises the prosp"
P03-1011,J85-4001,0,0.306459,"slated) subtree from the English sentences to occur, with some cost, at any point in the resulting French sentence. For example, in the case of the input tree A B X 3 Z Y a clone operation making a copy of node 3 as a new child of B would produce the tree: A X estimated using the dynamic programming method above, keeping counts for the expected number of times each node has been cloned, at no increase in computational complexity. Without such an assumption, the parameter estimation becomes a problem of parsing with crossing dependencies, which is exponential in the length of the input string (Barton, 1985). B Z Z Y This operation, combined with the deletion of the original node Z, produces the alignment (XZY) that was disallowed by the original tree reordering model. Figure 1 shows an example from our Korean-English corpus where the clone operation allows the model to handle a case of wh-movement in the English sentence that could not be realized by any reordering of subtrees of the Korean parse. The probability of adding a clone of original node εi as a child of node εj is calculated in two steps: first, the choice of whether to insert a clone under εj , with probability Pins (clone|εj ), and"
P03-1011,J90-2002,0,0.647673,"-ordering nodes in syntactic trees in order to allow alignments not conforming to the original tree structure, while keeping computational complexity polynomial in the sentence length. This is done by adding a new subtree cloning operation to either tree-to-string or tree-to-tree alignment algorithms. 1 Introduction Systems for automatic translation between languages have been divided into transfer-based approaches, which rely on interpreting the source string into an abstract semantic representation from which text is generated in the target language, and statistical approaches, pioneered by Brown et al. (1990), which estimate parameters for a model of word-to-word correspondences and word re-orderings directly from large corpora of parallel bilingual text. Only recently have hybrid approaches begun to emerge, which apply probabilistic models to a structured representation of the source text. Wu (1997) showed that restricting word-level alignments between sentence pairs to observe syntactic bracketing constraints significantly reduces the complexity of the alignment problem and allows a polynomial-time solution. Alshawi et al. (2000) also induce parallel tree structures from unbracketed parallel tex"
P03-1011,J93-2003,0,0.0434916,"and Ney (2000) differentiate between sure and possible hand-annotated alignments, our gold standard alignments come in only one variety. IBM Model 1 IBM Model 2 IBM Model 3 Tree-to-String Tree-to-String, Clone Tree-to-String, Clone Pins = .5 Tree-to-Tree Tree-to-Tree, Clone Alignment Error Rate .37 .35 .43 .42 .36 .32 .49 .36 Table 2: Alignment error rate on Korean-English corpus where A is the set of word pairs aligned by the automatic system, and G the set aligned in the gold standard. We provide a comparison of the tree-based models with the sequence of successively more complex models of Brown et al. (1993). Results are shown in Table 2. The error rates shown in Table 2 represent the minimum over training iterations; training was stopped for each model when error began to increase. IBM Models 1, 2, and 3 refer to Brown et al. (1993). “Tree-to-String” is the model of Yamada and Knight (2001), and “Tree-to-String, Clone” allows the node cloning operation of Section 2.1. “Tree-to-Tree” indicates the model of Section 3, while “Tree-to-Tree, Clone” adds the node cloning operation of Section 3.1. Model 2 is initialized from the parameters of Model 1, and Model 3 is initialized from Model 2. The lexica"
P03-1011,J94-4004,0,0.113203,"The use of explicit syntactic information for the target language in this model has led to excellent translation results (Yamada and Knight, 2002), and raises the prospect of training a statistical system using syntactic information for both sides of the parallel corpus. Tree-to-tree alignment techniques such as probabilistic tree substitution grammars (Hajiˇc et al., 2002) can be trained on parse trees from parallel treebanks. However, real bitexts generally do not exhibit parse-tree isomorphism, whether because of systematic differences between how languages express a concept syntactically (Dorr, 1994), or simply because of relatively free translations in the training material. In this paper, we introduce “loosely” tree-based alignment techniques to address this problem. We present analogous extensions for both tree-to-string and tree-to-tree models that allow alignments not obeying the constraints of the original syntactic tree (or tree pair), although such alignments are dispreferred because they incur a cost in probability. This is achieved by introducing a clone operation, which copies an entire subtree of the source language syntactic structure, moving it anywhere in the target languag"
P03-1011,J94-2001,0,0.00772036,"d uniformly. Figure 1 shows the viterbi alignment produced by the “Tree-to-String, Clone” system on one sentence from our test set. We found better agreement with the human alignments when fixing Pins (left) in the Tree-to-String model to a constant rather than letting it be determined through the EM training. While the model learned by EM tends to overestimate the total number of aligned word pairs, fixing a higher probability for insertions results in fewer total aligned pairs and therefore a better trade-off between precision and recall. As seen for other tasks (Carroll and Charniak, 1992; Merialdo, 1994), the likelihood criterion used in EM training may not be optimal when evaluating a system against human labeling. The approach of optimizing a small number of metaparameters has been applied to machine translation by Och and Ney (2002). It is likely that the IBM models could similarly be optimized to minimize alignment error – an open question is whether the optimization with respect to alignment error will correspond to optimization for translation accuracy. Within the strict EM framework, we found roughly equivalent performance between the IBM models and the two tree-based models when makin"
P03-1011,P00-1056,0,0.475981,"Missing"
P03-1011,P02-1038,0,0.0327289,"el to a constant rather than letting it be determined through the EM training. While the model learned by EM tends to overestimate the total number of aligned word pairs, fixing a higher probability for insertions results in fewer total aligned pairs and therefore a better trade-off between precision and recall. As seen for other tasks (Carroll and Charniak, 1992; Merialdo, 1994), the likelihood criterion used in EM training may not be optimal when evaluating a system against human labeling. The approach of optimizing a small number of metaparameters has been applied to machine translation by Och and Ney (2002). It is likely that the IBM models could similarly be optimized to minimize alignment error – an open question is whether the optimization with respect to alignment error will correspond to optimization for translation accuracy. Within the strict EM framework, we found roughly equivalent performance between the IBM models and the two tree-based models when making use of the cloning operation. For both the tree-tostring and tree-to-tree models, the cloning operation improved results, indicating that adding the flexibility to handle structural divergence is important when using syntax-based mode"
P03-1011,J97-3002,0,0.866213,"troduction Systems for automatic translation between languages have been divided into transfer-based approaches, which rely on interpreting the source string into an abstract semantic representation from which text is generated in the target language, and statistical approaches, pioneered by Brown et al. (1990), which estimate parameters for a model of word-to-word correspondences and word re-orderings directly from large corpora of parallel bilingual text. Only recently have hybrid approaches begun to emerge, which apply probabilistic models to a structured representation of the source text. Wu (1997) showed that restricting word-level alignments between sentence pairs to observe syntactic bracketing constraints significantly reduces the complexity of the alignment problem and allows a polynomial-time solution. Alshawi et al. (2000) also induce parallel tree structures from unbracketed parallel text, modeling the generation of each node’s children with a finite-state transducer. Yamada and Knight (2001) present an algorithm for estimating probabilistic parameters for a similar model which represents translation as a sequence of re-ordering operations over children of nodes in a syntactic t"
P03-1011,P01-1067,0,0.903246,"-orderings directly from large corpora of parallel bilingual text. Only recently have hybrid approaches begun to emerge, which apply probabilistic models to a structured representation of the source text. Wu (1997) showed that restricting word-level alignments between sentence pairs to observe syntactic bracketing constraints significantly reduces the complexity of the alignment problem and allows a polynomial-time solution. Alshawi et al. (2000) also induce parallel tree structures from unbracketed parallel text, modeling the generation of each node’s children with a finite-state transducer. Yamada and Knight (2001) present an algorithm for estimating probabilistic parameters for a similar model which represents translation as a sequence of re-ordering operations over children of nodes in a syntactic tree, using automatic parser output for the initial tree structures. The use of explicit syntactic information for the target language in this model has led to excellent translation results (Yamada and Knight, 2002), and raises the prospect of training a statistical system using syntactic information for both sides of the parallel corpus. Tree-to-tree alignment techniques such as probabilistic tree substitut"
P03-1011,P02-1039,0,0.323258,"olynomial-time solution. Alshawi et al. (2000) also induce parallel tree structures from unbracketed parallel text, modeling the generation of each node’s children with a finite-state transducer. Yamada and Knight (2001) present an algorithm for estimating probabilistic parameters for a similar model which represents translation as a sequence of re-ordering operations over children of nodes in a syntactic tree, using automatic parser output for the initial tree structures. The use of explicit syntactic information for the target language in this model has led to excellent translation results (Yamada and Knight, 2002), and raises the prospect of training a statistical system using syntactic information for both sides of the parallel corpus. Tree-to-tree alignment techniques such as probabilistic tree substitution grammars (Hajiˇc et al., 2002) can be trained on parse trees from parallel treebanks. However, real bitexts generally do not exhibit parse-tree isomorphism, whether because of systematic differences between how languages express a concept syntactically (Dorr, 1994), or simply because of relatively free translations in the training material. In this paper, we introduce “loosely” tree-based alignmen"
P03-1011,J03-4003,0,\N,Missing
P05-1059,J00-1004,0,0.0201849,"t sentences for which full EM is feasible, but pruning seems to have a negative impact on longer sentences. 1 Introduction The Inversion Transduction Grammar (ITG) of Wu (1997) is a syntactically motivated algorithm for producing word-level alignments of pairs of translationally equivalent sentences in two languages. The algorithm builds a synchronous parse tree for both sentences, and assumes that the trees have the same underlying structure but that the ordering of constituents may differ in the two languages. This probabilistic, syntax-based approach has inspired much subsequent reasearch. Alshawi et al. (2000) use hierarchical finite-state transducers. In the tree-to-string model of Yamada and Knight (2001), a parse tree for one sentence of a translation pair is projected onto the other string. Melamed (2003) presents algorithms for synchronous parsing with more complex grammars, discussing how to parse grammars with greater than binary branching and lexicalization of synchronous grammars. Despite being one of the earliest probabilistic syntax-based translation models, ITG remains stateof-the art. Zens and Ney (2003) found that the constraints of ITG were a better match to the decoding task than th"
P05-1059,J93-2003,0,0.0169638,"Missing"
P05-1059,N03-1016,0,0.039107,"t strings to make the algorithm feasible. Our technique involves computing an estimate of how likely each of the n4 cells in the chart is before considering all ways of building the cell by combining smaller subcells. Our figure of merit for a cell involves an estimate of both the inside probability of the cell (how likely the words within the box in both dimensions are to align) and the outside probability (how likely the words outside the box in both dimensions are to align). In including an estimate of the outside probability, our technique is related to A* methods for monolingual parsing (Klein and Manning, 2003), although our estimate is not guaranteed to be lower than complete outside probabity assigned by ITG. Figure 3(a) displays the tic-tac-toe pattern for the inside and outside components of a particular cell. We use IBM Model 1 as our estimate of both the inside and 478 outside probabilities. In the Model 1 estimate of the outside probability, source and target words can align using any combination of points from the four outside corners of the tic-tac-toe pattern. Thus in Figure 3(a), there is one solid cell (corresponding to the Model 1 Viterbi alignment) in each column, falling either in the"
P05-1059,N03-1021,0,0.0585084,"rithm for producing word-level alignments of pairs of translationally equivalent sentences in two languages. The algorithm builds a synchronous parse tree for both sentences, and assumes that the trees have the same underlying structure but that the ordering of constituents may differ in the two languages. This probabilistic, syntax-based approach has inspired much subsequent reasearch. Alshawi et al. (2000) use hierarchical finite-state transducers. In the tree-to-string model of Yamada and Knight (2001), a parse tree for one sentence of a translation pair is projected onto the other string. Melamed (2003) presents algorithms for synchronous parsing with more complex grammars, discussing how to parse grammars with greater than binary branching and lexicalization of synchronous grammars. Despite being one of the earliest probabilistic syntax-based translation models, ITG remains stateof-the art. Zens and Ney (2003) found that the constraints of ITG were a better match to the decoding task than the heuristics used in the IBM decoder of Berger et al. (1996). Zhang and Gildea (2004) found ITG to outperform the tree-to-string model for word-level alignment, as measured against human gold-standard al"
P05-1059,P00-1056,0,0.473219,"led the pruning techniques for the LITG with the beam ratio for the tic-tac-toe pruning as 10−5 and the number k for the top-k pruning as 25. We ran the experiments on sentences up to 25 words long in both languages. The resulting training corpus had 18,773 sentence pairs with a total of 276,113 Chinese words and 315,415 English words. We evaluate our translation models in terms of agreement with human-annotated word-level alignments between the sentence pairs. For scoring the Viterbi alignments of each system against goldstandard annotated alignments, we use the alignment error rate (AER) of Och and Ney (2000), which measures agreement at the level of pairs of words: AER = 1 − P (hY (∗)Zi |X(∗)) P (hY Z(∗)i |X(∗)) where ∗ stands for any lexical pair. For instance, P ([Y (e/f )Z] |X(e/f )) = (1 − λ)PEM ([Y (e/f )Z] |X(e/f )) + λP ([Y (∗)Z] |X(∗)) where λ = 1/(1 + Expected Counts(X(e/f ))) 480 |A ∩ GP |+ |A ∩ GS | |A |+ |GS | where A is the set of word pairs aligned by the automatic system, GS is the set marked in the gold standard as “sure”, and GP is the set marked as “possible” (including the “sure” pairs). In our Chinese-English data, only one type of alignment was marked, meaning that GP = GS ."
P05-1059,J97-3002,0,0.800639,"the two languages, whereas the rules with pointed brackets expand the left hand side symbol into the two right hand side symbols in reverse order in the two languages. One special case of ITG is the bracketing ITG that has only one nonterminal that instantiates exactly one straight rule and one inverted rule. The ITG we apply in our experiments has more structural labels than the primitive bracketing grammar: it has a start symbol S, a single preterminal C, and two intermediate nonterminals A and B used to ensure that only one parse can generate any given word-level alignment, as discussed by Wu (1997) and Zens and Ney (2003). As an example, Figure 1 shows the alignment and the corresponding parse tree for the sentence pair Je les vois / I see them using the unambiguous bracketing ITG. A stochastic ITG can be thought of as a stochastic CFG extended to the space of bitext. The independence assumptions typifying S-CFGs are also valid for S-ITGs. Therefore, the probability of an S-ITG parse is calculated as the product of the probabilities of all the instances of rules in the parse tree. For instance, the probability of the parse in Figure 1 is: X → X(e/f ) The word pair e/f is representative"
P05-1059,P01-1067,0,0.0308662,"sentences. 1 Introduction The Inversion Transduction Grammar (ITG) of Wu (1997) is a syntactically motivated algorithm for producing word-level alignments of pairs of translationally equivalent sentences in two languages. The algorithm builds a synchronous parse tree for both sentences, and assumes that the trees have the same underlying structure but that the ordering of constituents may differ in the two languages. This probabilistic, syntax-based approach has inspired much subsequent reasearch. Alshawi et al. (2000) use hierarchical finite-state transducers. In the tree-to-string model of Yamada and Knight (2001), a parse tree for one sentence of a translation pair is projected onto the other string. Melamed (2003) presents algorithms for synchronous parsing with more complex grammars, discussing how to parse grammars with greater than binary branching and lexicalization of synchronous grammars. Despite being one of the earliest probabilistic syntax-based translation models, ITG remains stateof-the art. Zens and Ney (2003) found that the constraints of ITG were a better match to the decoding task than the heuristics used in the IBM decoder of Berger et al. (1996). Zhang and Gildea (2004) found ITG to"
P05-1059,P03-1019,0,0.487169,"his probabilistic, syntax-based approach has inspired much subsequent reasearch. Alshawi et al. (2000) use hierarchical finite-state transducers. In the tree-to-string model of Yamada and Knight (2001), a parse tree for one sentence of a translation pair is projected onto the other string. Melamed (2003) presents algorithms for synchronous parsing with more complex grammars, discussing how to parse grammars with greater than binary branching and lexicalization of synchronous grammars. Despite being one of the earliest probabilistic syntax-based translation models, ITG remains stateof-the art. Zens and Ney (2003) found that the constraints of ITG were a better match to the decoding task than the heuristics used in the IBM decoder of Berger et al. (1996). Zhang and Gildea (2004) found ITG to outperform the tree-to-string model for word-level alignment, as measured against human gold-standard alignments. One explanation for this result is that, while a tree representation is helpful for modeling translation, the trees assigned by the traditional monolingual parsers (and the treebanks on which they are trained) may not be optimal for translation of a specific language pair. ITG has the advantage of being"
P05-1059,C04-1060,1,0.854847,"ing model of Yamada and Knight (2001), a parse tree for one sentence of a translation pair is projected onto the other string. Melamed (2003) presents algorithms for synchronous parsing with more complex grammars, discussing how to parse grammars with greater than binary branching and lexicalization of synchronous grammars. Despite being one of the earliest probabilistic syntax-based translation models, ITG remains stateof-the art. Zens and Ney (2003) found that the constraints of ITG were a better match to the decoding task than the heuristics used in the IBM decoder of Berger et al. (1996). Zhang and Gildea (2004) found ITG to outperform the tree-to-string model for word-level alignment, as measured against human gold-standard alignments. One explanation for this result is that, while a tree representation is helpful for modeling translation, the trees assigned by the traditional monolingual parsers (and the treebanks on which they are trained) may not be optimal for translation of a specific language pair. ITG has the advantage of being entirely data-driven – the trees are derived from an expectation maximization procedure given only the original strings as input. In this paper, we extend ITG to condi"
P06-2036,P05-1033,0,0.0371941,"Science Dept. University of Rochester Rochester, NY 14627 Introduction Synchronous Context-Free Grammars (SCFGs) are a generalization of the Context-Free Grammar (CFG) formalism to simultaneously produce strings in two languages. SCFGs have a wide range of applications, including machine translation, word and phrase alignments, and automatic dictionary construction. Variations of SCFGs go back to Aho and Ullman (1972)’s Syntax-Directed Translation Schemata, but also include the Inversion Transduction Grammars in Wu (1997), which restrict grammar rules to be binary, the synchronous grammars in Chiang (2005), which use only a single nonterminal symbol, and the Multitext Grammars in Melamed (2003), which allow independent rewriting, as well as other tree-based models such as Yamada and Knight (2001) and Galley et al. (2004). When viewed as a rewriting system, an SCFG generates a set of string pairs, representing some translation relation. We are concerned here with the time complexity of parsing such a pair, according to the grammar. Assume then a pair with each 279 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 279–286, c Sydney, July 2006. 2006 Association for Computat"
P06-2036,N04-1035,0,0.0453813,"in two languages. SCFGs have a wide range of applications, including machine translation, word and phrase alignments, and automatic dictionary construction. Variations of SCFGs go back to Aho and Ullman (1972)’s Syntax-Directed Translation Schemata, but also include the Inversion Transduction Grammars in Wu (1997), which restrict grammar rules to be binary, the synchronous grammars in Chiang (2005), which use only a single nonterminal symbol, and the Multitext Grammars in Melamed (2003), which allow independent rewriting, as well as other tree-based models such as Yamada and Knight (2001) and Galley et al. (2004). When viewed as a rewriting system, an SCFG generates a set of string pairs, representing some translation relation. We are concerned here with the time complexity of parsing such a pair, according to the grammar. Assume then a pair with each 279 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 279–286, c Sydney, July 2006. 2006 Association for Computational Linguistics 1,2 1,2 2,1 2 1 7 3,1,4,2 1,2 3 7 rank of a CFG, in this paper we focus on each single synchronous rule and factorize it into synchronous rules of lower rank. If we view the bijective relation associat"
P06-2036,N03-1021,0,0.0211477,"t-Free Grammars (SCFGs) are a generalization of the Context-Free Grammar (CFG) formalism to simultaneously produce strings in two languages. SCFGs have a wide range of applications, including machine translation, word and phrase alignments, and automatic dictionary construction. Variations of SCFGs go back to Aho and Ullman (1972)’s Syntax-Directed Translation Schemata, but also include the Inversion Transduction Grammars in Wu (1997), which restrict grammar rules to be binary, the synchronous grammars in Chiang (2005), which use only a single nonterminal symbol, and the Multitext Grammars in Melamed (2003), which allow independent rewriting, as well as other tree-based models such as Yamada and Knight (2001) and Galley et al. (2004). When viewed as a rewriting system, an SCFG generates a set of string pairs, representing some translation relation. We are concerned here with the time complexity of parsing such a pair, according to the grammar. Assume then a pair with each 279 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 279–286, c Sydney, July 2006. 2006 Association for Computational Linguistics 1,2 1,2 2,1 2 1 7 3,1,4,2 1,2 3 7 rank of a CFG, in this paper we focus"
P06-2036,H05-1101,1,0.929841,"Rochester Rochester, NY 14627 Giorgio Satta Dept. of Information Eng’g University of Padua I-35131 Padua, Italy string having a maximum length of N , and consider an SCFG G of size |G|, with a bound of n nonterminals in the right-hand side of each rule in a single dimension, which we call below the rank of G. As an upper bound, parsing can be carried out in time O(|G |N n+4 ) by a dynamic programming algorithm maintaining continuous spans in one dimension. As a lower bound, parsing strategies with discontinuous√spans in both dimensions can take time Ω(|G |N c n ) for unfriendly permutations (Satta and Peserico, 2005). A natural question to ask then is: What if we could reduce the rank of G, preserving the generated translation? As in the case of CFGs, one way of doing this would be to factorize each single rule into several rules of rank strictly smaller than n. It is not difficult to see that this would result in a new grammar of size at most 2 · |G|. In the time complexities reported above, we see that such a size increase would be more than compensated by the reduction in the degree of the polynomial in N . We thus conclude that a reduction in the rank of an SCFG would result in more efficient parsing"
P06-2036,J97-3002,0,0.221999,"problem about recognizing permutations that can be factored. 1 Hao Zhang Computer Science Dept. University of Rochester Rochester, NY 14627 Introduction Synchronous Context-Free Grammars (SCFGs) are a generalization of the Context-Free Grammar (CFG) formalism to simultaneously produce strings in two languages. SCFGs have a wide range of applications, including machine translation, word and phrase alignments, and automatic dictionary construction. Variations of SCFGs go back to Aho and Ullman (1972)’s Syntax-Directed Translation Schemata, but also include the Inversion Transduction Grammars in Wu (1997), which restrict grammar rules to be binary, the synchronous grammars in Chiang (2005), which use only a single nonterminal symbol, and the Multitext Grammars in Melamed (2003), which allow independent rewriting, as well as other tree-based models such as Yamada and Knight (2001) and Galley et al. (2004). When viewed as a rewriting system, an SCFG generates a set of string pairs, representing some translation relation. We are concerned here with the time complexity of parsing such a pair, according to the grammar. Assume then a pair with each 279 Proceedings of the COLING/ACL 2006 Main Confere"
P06-2036,P01-1067,0,0.0784028,"multaneously produce strings in two languages. SCFGs have a wide range of applications, including machine translation, word and phrase alignments, and automatic dictionary construction. Variations of SCFGs go back to Aho and Ullman (1972)’s Syntax-Directed Translation Schemata, but also include the Inversion Transduction Grammars in Wu (1997), which restrict grammar rules to be binary, the synchronous grammars in Chiang (2005), which use only a single nonterminal symbol, and the Multitext Grammars in Melamed (2003), which allow independent rewriting, as well as other tree-based models such as Yamada and Knight (2001) and Galley et al. (2004). When viewed as a rewriting system, an SCFG generates a set of string pairs, representing some translation relation. We are concerned here with the time complexity of parsing such a pair, according to the grammar. Assume then a pair with each 279 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 279–286, c Sydney, July 2006. 2006 Association for Computational Linguistics 1,2 1,2 2,1 2 1 7 3,1,4,2 1,2 3 7 rank of a CFG, in this paper we focus on each single synchronous rule and factorize it into synchronous rules of lower rank. If we view the bi"
P06-2036,N06-1033,1,0.761511,"torizing a permutation of arity n into the composition of several permutations of arity k < n. Such factorization can be represented as a tree of composed permutations, called in what follows a permutation tree. A permutation tree can be converted into a set of k-ary SCFG rules equivalent to the input rule. For example, the input rule: 4,1,3,5,2 5 8 6 1 4 2,4,1,3 6 3 8 2 5 4 Figure 1: Two permutation trees. The permutations associated with the leaves can be produced by composing the permutations at the internal nodes. spans in one dimension. Previous work on this problem has been presented in Zhang et al. (2006), where a method is provided for casting an SCFG to a form with rank k = 2. If generalized to any value of k, that algorithm would run in time O(n2 ). We thus improve existing factorization methods by almost a factor of n. We also solve an open problem mentioned by Albert et al. (2003), who pose the question of whether irreducible, or simple, permutations can be recognized in time less than Θ(n2 ). [ X → A(1) B (2) C (3) D(4) E (5) F (6) G(7) H (8) , X → B (2) A(1) C (3) D(4) G(7) E (5) H (8) F (6) ] yields the permutation tree of Figure 1(left). Introducing a new grammar nonterminal Xi for ea"
P06-2070,J93-2003,0,0.00679935,"Missing"
P06-2070,W04-3250,0,0.128696,"Missing"
P06-2070,P04-1077,0,0.225584,"Missing"
P06-2070,W05-0904,1,0.937552,"n. 1 Introduction Evaluation has long been a stumbling block in the development of machine translation systems, due to the simple fact that there are many correct translations for a given sentence. Human evaluation of system output is costly in both time and money, leading to the rise of automatic evaluation metrics in recent years. In the 2003 Johns Hopkins Workshop on Speech and Language Engineering, experiments on MT evaluation showed that BLEU and NIST do not correlate well with human judgments at the sentence level, even when they correlate well over large test sets (Blatz et al., 2003). Liu and Gildea (2005) also pointed out that due to the limited references for every MT output, using the overlapping ratio of n-grams longer than 2 did not improve sentence level evaluation performance of BLEU. The problem leads 1 METEOR and ROUGE both compute the score based on the best reference 539 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 539–546, c Sydney, July 2006. 2006 Association for Computational Linguistics • Computing the string alignment score based on the gaps in the common sequence. Though ROUGE-W also takes into consider the gaps in the common sequence between the MT"
P06-2070,W05-0909,0,0.279416,"Missing"
P06-2070,N03-1024,0,0.0638828,"Missing"
P06-2070,P05-1074,0,0.0186333,"e MT output words matching the references. We use a different stochastic approach in SIA to achieve the same purpose. The string alignment has a good dynamic framework which allows the stochastic word matching to be easily incorporated into it. The stochastic string alignment can be implemented by simply replacing the function COM PUTE SCORE with the function of Figure 5. The function similarity(word1, word2) returns a ratio which reflects how similar the two words are. Now we consider how to compute the similarity ratio of two words. Our method is motivated by the phrase extraction method of Bannard and Callison-Burch (2005), which computes the similarity ratio of two words by looking at their relationship with words in another language. Given a bilingual parallel corpus with aligned sentences, say English and French, the probability of an English word given a French word can be computed by training word alignment models such as IBM Model4. Then for every English word e, we have a set of conditional probabilities given each French word: p(e|f1 ), p(e|f2 ), ... , p(e|fN ). If we consider these probabilities as a vector, the similarities of two English words can be obtained by computing the dot product of their cor"
P06-2070,P02-1040,0,0.0785808,"RD-NET used in METEOR and ROUGE. Instead of using exact matching, we use a soft matching based on the similarity between two words, which is trained in a bilingual corpus. The corpus is aligned in the word level using IBM Model4 (Brown et al., 1993). Stochastic word matching is a uniform replacement for both morphological processing and synonym matching. More importantly, it can be easily adapted for different kinds of languages, as long as there are bilingual parallel corpora available (which is always true for statistical machine translation). fessional human translation, the better it is” (Papineni et al., 2002). For every hypothesis, BLEU computes the fraction of n-grams which also appear in the reference sentences, as well as a brevity penalty. NIST uses a similar strategy to BLEU but further considers that n-grams with different frequency should be treated differently in the evaluation (Doddington, 2002). BLEU and NIST have been shown to correlate closely with human judgments in ranking MT systems with different qualities (Papineni et al., 2002; Doddington, 2002). ROUGE-W is based on the weighted longest common subsequence (LCS) between the MT output and the reference. The common subsequences in R"
P06-2070,N03-1003,0,0.0225982,"tables as input, which record the unavailable positions in the MT output and the reference. These positions have already been used in the prior best alignments and should not be considered in the ongoing alignment. It also returns the aligned positions of the best alignment. The pseudocode for GET ALIGN SCORE 1 is shown in Figure 7. The computation of the length penalty is similar to BLEU: it is set to 1 if length of the MT output is longer than the arithmetic mean of length of the p(ei |fk )p(ej |fk ) (3) Paraphrasing methods based on monolingual parallel corpora such as (Pang et al., 2003; Barzilay and Lee, 2003) can also be used to compute the similarity ratio of two words, but they don’t have as rich training resources as the bilingual methods do. 2 Although the marginalized probability (over all French words) of an English word given the other English word P ( N k=1 p(ei |fk )p(fk |ej )) is a more intuitive way of measuring the similarity, the dot product of the vectors p(e|f ) described above performed slightly better in our experiments. 542 function GET ALIGN SCORE IN MULTIPLE REFS (mt, ref 1 , ..., ref N , α) . Iteratively Compute the Alignment Score Based on Multiple References and the Decay Fa"
P06-2070,C04-1046,0,\N,Missing
P06-2122,P04-1060,0,0.0538395,"Missing"
P06-2122,P05-1057,0,0.0260683,"ults on English side of the test data set. The dependency results on Chinese are similar. The gold standard dependencies were extracted from Collins’ parser output on the sentences. The LITG and BLITG dependencies were extracted from the Viterbi synchronous trees by following the head words. For comparison, we also included two base-line results. ITG-lh is unlexicalized ITG with left-head assumption, meaning the head words always come from the left branches. ITG-rh is ITG with righthead assumption. To make more confident conclusions, we also did tests on a larger hand-aligned data set used in Liu et al. (2005). We used 165 sentence pairs that are up to 25 words in length on both sides. 5 Discussion The BLITG model has two components, namely the dependency model on the upper levels of the tree structure and the word-level translation model at the bottom. We hope that the two components will mutually improve one another. The current experiments indicate clearly that the word level alignment does help inducing dependency structures on both sides. The precision and recall on the dependency retrieval sub-task are almost doubled for both languages from LITG which only has a kind of uni-lexical dependency"
P06-2122,N03-1021,0,0.26991,"ficulty in statistical machine translation is the trade-off between representational power and computational complexity. Real-world corpora for language pairs such as Chinese-English have complex reordering relationships that are not captured by current phrase-based MT systems, despite their state-of-the-art performance measured in competitive evaluations. Synchronous grammar formalisms that are capable of modeling such complex relationships while maintaining the context-free property in each language have been proposed for many years, (Aho and Ullman, 1972; Wu, 1997; Yamada and Knight, 2001; Melamed, 2003; Chiang, 2005), but have not been scaled to large corpora and long sentences until recently. In Synchronous Context Free Grammars, there are two sources of complexity, grammar branching factor and lexicalization. In this paper we focus on the second issue, constraining the grammar to the binary-branching Inversion Transduction Grammar of Wu (1997). Lexicalization seems likely to help models predict alignment patterns between languages, and has been proposed by Melamed (2003) and implemented by Alshawi et al. (2000) and Zhang and Gildea (2005). However, each piece of lexical information consid"
P06-2122,P00-1056,0,0.0593994,"t comparison, we measured the performance of five word aligners, including IBM models, ITG, the lexical ITG (LITG) of Zhang and Gildea (2005), and our bilexical ITG (BLITG), on a hand-aligned bilingual corpus. All the models were trained using the same amount of data. We ran the experiments on sentences up to 25 words long in both languages. The resulting training corpus had 18,773 sentence pairs with a total of 276,113 Chinese words and 315,415 English words. For scoring the Viterbi alignments of each system against gold-standard annotated alignments, we use the alignment error rate (AER) of Och and Ney (2000), which measures agreement at the level of pairs of words: The two levels of distributions are interpolated using a technique inspired by Witten-Bell smoothing (Chen and Goodman, 1996). We use the expected count of the left hand side lexical nonterminal to adjust the weight for the EM-trained bilexical probability. For example, P ([Y (e) Z(e0 )] |X(e)) = (1 − λ)PEM ([Y (e) Z(e0 )] |X(e)) + λP ([Y (∗) Z(e0 )] |X(∗)) where λ = 1/(1 + Expected Counts(X(e))) 4 Experiments First of all, we are interested in finding out how much speedup can be achieved by doing the hook trick for EM. We implemented"
P06-2122,J97-3002,0,0.752045,"available. Introduction A major difficulty in statistical machine translation is the trade-off between representational power and computational complexity. Real-world corpora for language pairs such as Chinese-English have complex reordering relationships that are not captured by current phrase-based MT systems, despite their state-of-the-art performance measured in competitive evaluations. Synchronous grammar formalisms that are capable of modeling such complex relationships while maintaining the context-free property in each language have been proposed for many years, (Aho and Ullman, 1972; Wu, 1997; Yamada and Knight, 2001; Melamed, 2003; Chiang, 2005), but have not been scaled to large corpora and long sentences until recently. In Synchronous Context Free Grammars, there are two sources of complexity, grammar branching factor and lexicalization. In this paper we focus on the second issue, constraining the grammar to the binary-branching Inversion Transduction Grammar of Wu (1997). Lexicalization seems likely to help models predict alignment patterns between languages, and has been proposed by Melamed (2003) and implemented by Alshawi et al. (2000) and Zhang and Gildea (2005). However,"
P06-2122,P01-1067,0,0.0566379,"Introduction A major difficulty in statistical machine translation is the trade-off between representational power and computational complexity. Real-world corpora for language pairs such as Chinese-English have complex reordering relationships that are not captured by current phrase-based MT systems, despite their state-of-the-art performance measured in competitive evaluations. Synchronous grammar formalisms that are capable of modeling such complex relationships while maintaining the context-free property in each language have been proposed for many years, (Aho and Ullman, 1972; Wu, 1997; Yamada and Knight, 2001; Melamed, 2003; Chiang, 2005), but have not been scaled to large corpora and long sentences until recently. In Synchronous Context Free Grammars, there are two sources of complexity, grammar branching factor and lexicalization. In this paper we focus on the second issue, constraining the grammar to the binary-branching Inversion Transduction Grammar of Wu (1997). Lexicalization seems likely to help models predict alignment patterns between languages, and has been proposed by Melamed (2003) and implemented by Alshawi et al. (2000) and Zhang and Gildea (2005). However, each piece of lexical inf"
P06-2122,J00-1004,0,0.0244868,"een proposed for many years, (Aho and Ullman, 1972; Wu, 1997; Yamada and Knight, 2001; Melamed, 2003; Chiang, 2005), but have not been scaled to large corpora and long sentences until recently. In Synchronous Context Free Grammars, there are two sources of complexity, grammar branching factor and lexicalization. In this paper we focus on the second issue, constraining the grammar to the binary-branching Inversion Transduction Grammar of Wu (1997). Lexicalization seems likely to help models predict alignment patterns between languages, and has been proposed by Melamed (2003) and implemented by Alshawi et al. (2000) and Zhang and Gildea (2005). However, each piece of lexical information considered by a model multiplies the number of states of dynamic programming algorithms for inference, meaning In order to better understand the model, we analyze its performance in terms of both agreement with human-annotated alignments, and agreement with the dependencies produced by monolingual parsers. We find that within-language bilexicalization does not improve alignment over crosslanguage bilexicalization, but does improve recovery of dependencies. We find that the hook trick significantly speeds training, even in"
P06-2122,P03-1019,0,0.0183984,"head-modifier relations in either language into account. However, modeling complete bilingual bilexical dependencies as theorized in Melamed (2003) implies a huge parameter space of O(|V |2 |T |2 ), where |V |and |T |are the vocabulary sizes of the two languages. So, instead of modeling cross-language word translations and within-language word dependencies in P (S → A) · P (A → [C B]) · P (C → I/Je) · P (B → hC Ci) · P (C → see/vois) · P (C → them/les) The structural constraint of ITG, which is that only binary permutations are allowed on each level, has been demonstrated to be reasonable by Zens and Ney (2003) and Zhang and Gildea (2004). However, in the space of ITG-constrained 954 S S A A(see/vois) C B C I/Je S B(see/vois) C(I/Je) C(see/vois) see/vois them/les I/Je C(them/les) B(see) C(I) C C C A(see) C(see) see/vois C(them) them/les Figure 1: Parses for an example sentence pair under unlexicalized ITG (left), cross-language bilexicalization (center), and head-modifier bilexicaliztion (right). Thick lines indicate head child; crossbar indicates inverted production. a joint fashion, we factor them apart. We lexicalize the dependencies in the synchronous tree using words from only one language and"
P06-2122,P96-1041,0,0.0205626,"hand-aligned bilingual corpus. All the models were trained using the same amount of data. We ran the experiments on sentences up to 25 words long in both languages. The resulting training corpus had 18,773 sentence pairs with a total of 276,113 Chinese words and 315,415 English words. For scoring the Viterbi alignments of each system against gold-standard annotated alignments, we use the alignment error rate (AER) of Och and Ney (2000), which measures agreement at the level of pairs of words: The two levels of distributions are interpolated using a technique inspired by Witten-Bell smoothing (Chen and Goodman, 1996). We use the expected count of the left hand side lexical nonterminal to adjust the weight for the EM-trained bilexical probability. For example, P ([Y (e) Z(e0 )] |X(e)) = (1 − λ)PEM ([Y (e) Z(e0 )] |X(e)) + λP ([Y (∗) Z(e0 )] |X(∗)) where λ = 1/(1 + Expected Counts(X(e))) 4 Experiments First of all, we are interested in finding out how much speedup can be achieved by doing the hook trick for EM. We implemented both versions in C++ and turned off pruning for both. We ran the two inside-outside parsing algorithms on a small test set of 46 sentence pairs that are no longer than 25 words in both"
P06-2122,C04-1060,1,0.850492,"in either language into account. However, modeling complete bilingual bilexical dependencies as theorized in Melamed (2003) implies a huge parameter space of O(|V |2 |T |2 ), where |V |and |T |are the vocabulary sizes of the two languages. So, instead of modeling cross-language word translations and within-language word dependencies in P (S → A) · P (A → [C B]) · P (C → I/Je) · P (B → hC Ci) · P (C → see/vois) · P (C → them/les) The structural constraint of ITG, which is that only binary permutations are allowed on each level, has been demonstrated to be reasonable by Zens and Ney (2003) and Zhang and Gildea (2004). However, in the space of ITG-constrained 954 S S A A(see/vois) C B C I/Je S B(see/vois) C(I/Je) C(see/vois) see/vois them/les I/Je C(them/les) B(see) C(I) C C C A(see) C(see) see/vois C(them) them/les Figure 1: Parses for an example sentence pair under unlexicalized ITG (left), cross-language bilexicalization (center), and head-modifier bilexicaliztion (right). Thick lines indicate head child; crossbar indicates inverted production. a joint fashion, we factor them apart. We lexicalize the dependencies in the synchronous tree using words from only one language and translate the words into the"
P06-2122,P05-1033,0,0.0800153,"istical machine translation is the trade-off between representational power and computational complexity. Real-world corpora for language pairs such as Chinese-English have complex reordering relationships that are not captured by current phrase-based MT systems, despite their state-of-the-art performance measured in competitive evaluations. Synchronous grammar formalisms that are capable of modeling such complex relationships while maintaining the context-free property in each language have been proposed for many years, (Aho and Ullman, 1972; Wu, 1997; Yamada and Knight, 2001; Melamed, 2003; Chiang, 2005), but have not been scaled to large corpora and long sentences until recently. In Synchronous Context Free Grammars, there are two sources of complexity, grammar branching factor and lexicalization. In this paper we focus on the second issue, constraining the grammar to the binary-branching Inversion Transduction Grammar of Wu (1997). Lexicalization seems likely to help models predict alignment patterns between languages, and has been proposed by Melamed (2003) and implemented by Alshawi et al. (2000) and Zhang and Gildea (2005). However, each piece of lexical information considered by a model"
P06-2122,P05-1059,1,0.844585,"s, (Aho and Ullman, 1972; Wu, 1997; Yamada and Knight, 2001; Melamed, 2003; Chiang, 2005), but have not been scaled to large corpora and long sentences until recently. In Synchronous Context Free Grammars, there are two sources of complexity, grammar branching factor and lexicalization. In this paper we focus on the second issue, constraining the grammar to the binary-branching Inversion Transduction Grammar of Wu (1997). Lexicalization seems likely to help models predict alignment patterns between languages, and has been proposed by Melamed (2003) and implemented by Alshawi et al. (2000) and Zhang and Gildea (2005). However, each piece of lexical information considered by a model multiplies the number of states of dynamic programming algorithms for inference, meaning In order to better understand the model, we analyze its performance in terms of both agreement with human-annotated alignments, and agreement with the dependencies produced by monolingual parsers. We find that within-language bilexicalization does not improve alignment over crosslanguage bilexicalization, but does improve recovery of dependencies. We find that the hook trick significantly speeds training, even in the presence of pruning. Se"
P06-2122,P99-1059,0,0.203941,"benefits from the dynamic programming “hook trick”. The model produces improved dependency structure for both languages. 1 In this paper we compare two approaches to lexicalization, both of which incorporate bilexical probabilities. One model uses bilexical probabilities across languages, while the other uses bilexical probabilities within one language. We compare results on word-level alignment, and investigate the implications of the choice of lexicalization on the specifics of our alignment algorithms. The new model, which bilexicalizes within languages, allows us to use the “hook trick” (Eisner and Satta, 1999) and therefore reduces complexity. We describe the application of the hook trick to estimation with Expectation Maximization (EM). Despite the theoretical benefits of the hook trick, it is not widely used in statistical monolingual parsers, because the savings do not exceed those obtained with simple pruning. We speculate that the advantages may be greater in an EM setting, where parameters to guide pruning are not (initially) available. Introduction A major difficulty in statistical machine translation is the trade-off between representational power and computational complexity. Real-world co"
P07-1024,W05-1504,0,0.0688752,"Missing"
P07-1024,H05-1066,0,0.100457,"Missing"
P07-1024,P03-1021,0,0.00427503,"d iterating through the set of weights to be set. The objective function describing the total dependency length of the corpus is piecewise constant, as the dependency length will not change until one weight crosses another, causing two dependents to reverse order, at which point the total length will discontinuously jump. Nondifferentiability implies that methods based on gradient ascent will not apply. This setting is reminiscent of the problem of optimizing feature weights for reranking of candidate machine translation outputs, and we employ an optimization technique similar to that used by Och (2003) for machine translation. Because the objective function only changes at points where one weight crosses another’s value, the set of segments of weight values with different values of the objective function can be exhaustively enumerated. In fact, the only significant points are the values of other weights for dependency types which occur in the corpus attached to the same head 2 We omit details due to space. 189 as the dependency being optimized. We build a table of interacting dependencies as a preprocessing step on the data, and then when optimizing a weight, consider the sequence of values"
P07-1024,J03-4003,0,\N,Missing
P07-1024,D08-1076,0,\N,Missing
P08-1012,J93-2003,0,0.0547987,"models toward generalizable, parsimonious parameter sets, leading to significant improvements in word alignment. This preference for sparse solutions together with effective pruning methods forms a phrase alignment regimen that produces better end-to-end translations than standard word alignment approaches. 1 Introduction Most state-of-the-art statistical machine translation systems are based on large phrase tables extracted from parallel text using word-level alignments. These word-level alignments are most often obtained using Expectation Maximization on the conditional generative models of Brown et al. (1993) and Vogel et al. (1996). As these word-level alignment models restrict the word alignment complexity by requiring each target word to align to zero or one source words, results are improved by aligning both source-to-target as well as target-to-source, then heuristically combining these alignments. Finally, the set of phrases consistent with the word alignments are extracted from every sentence pair; these form the basis of the decoding process. While this approach has been very successful, poor wordlevel alignments are nonetheless a common source of error in machine translation systems. A na"
P08-1012,W07-0403,0,0.541593,"ly speaking, the goal of this section is the same as the previous section, namely, to limit the set of phrase pairs that needs to be considered in the training process. The tic-tac-toe pruning relies on IBM model 1 for scoring a given aligned area. In this part, we use word-based ITG alignments as anchor points in the alignment space to pin down the potential phrases. The scope of iterative phrasal ITG training, therefore, is limited to determining the boundaries of the phrases anchored on the given one-toone word alignments. The heuristic method is based on the NonCompositional Constraint of Cherry and Lin (2007). Cherry and Lin (2007) use GIZA++ intersections which have high precision as anchor points in the 102 bitext space to constraint ITG phrases. We use ITG Viterbi alignments instead. The benefit is two-fold. First of all, we do not have to run a GIZA++ aligner. Second, we do not need to worry about non-ITG word alignments, such as the (2, 4, 1, 3) permutation patterns. GIZA++ does not limit the set of permutations allowed during translation, so it can produce permutations that are not reachable using an ITG. Formally, given a word-based ITG alignment, the bootstrapping algorithm finds all the p"
P08-1012,P05-1033,0,0.0486337,"recision as anchor points in the 102 bitext space to constraint ITG phrases. We use ITG Viterbi alignments instead. The benefit is two-fold. First of all, we do not have to run a GIZA++ aligner. Second, we do not need to worry about non-ITG word alignments, such as the (2, 4, 1, 3) permutation patterns. GIZA++ does not limit the set of permutations allowed during translation, so it can produce permutations that are not reachable using an ITG. Formally, given a word-based ITG alignment, the bootstrapping algorithm finds all the phrase pairs according to the definition of Och and Ney (2004) and Chiang (2005) with the additional constraint that each phrase pair contains at most one word link. Mathematically, let e(i, j) count the number of word links that are emitted from the substring ei...j , and f (l, m) count the number of word links emitted from the substring fl...m . The non-compositional phrase pairs satisfy e(i, j) = f (l, m) ≤ 1. Figure 3 (a) shows all possible non-compositional phrases given the Viterbi word alignment of the example sentence pair. 6 Summary of the Pipeline We summarize the pipeline of our system, demonstrating the interactions between the three main contributions of this"
P08-1012,P07-1094,0,0.0064648,"r non-zero values. Our second approach was to constrain the search space using simpler alignment models, which has the further benefit of significantly speeding up training. First we train a lower level word alignment model, then we place hard constraints on the phrasal alignment space using confident word links from this simpler model. Combining the two approaches, we have a staged training procedure going from the simplest unconstrained word based model to a constrained Bayesian word-level ITG model, and finally proceeding to a constrained Bayesian phrasal model. 3 Variational Bayes for ITG Goldwater and Griffiths (2007) and Johnson (2007) show that modifying an HMM to include a sparse prior over its parameters and using Bayesian estimation leads to improved accuracy for unsupervised part-of-speech tagging. In this section, we describe a Bayesian estimator for ITG: we select parameters that optimize the probability of the data given a prior. The traditional estimation method for word alignment models is the EM algorithm (Brown et al., 1993) which iteratively updates parameters to maximize the likelihood of the data. The drawback of maximum likelihood is obvious for phrase-based models. If we do not put any co"
P08-1012,D07-1031,0,0.0815598,"ach was to constrain the search space using simpler alignment models, which has the further benefit of significantly speeding up training. First we train a lower level word alignment model, then we place hard constraints on the phrasal alignment space using confident word links from this simpler model. Combining the two approaches, we have a staged training procedure going from the simplest unconstrained word based model to a constrained Bayesian word-level ITG model, and finally proceeding to a constrained Bayesian phrasal model. 3 Variational Bayes for ITG Goldwater and Griffiths (2007) and Johnson (2007) show that modifying an HMM to include a sparse prior over its parameters and using Bayesian estimation leads to improved accuracy for unsupervised part-of-speech tagging. In this section, we describe a Bayesian estimator for ITG: we select parameters that optimize the probability of the data given a prior. The traditional estimation method for word alignment models is the EM algorithm (Brown et al., 1993) which iteratively updates parameters to maximize the likelihood of the data. The drawback of maximum likelihood is obvious for phrase-based models. If we do not put any constraint on the dis"
P08-1012,koen-2004-pharaoh,0,0.0360953,"translation tables using five iterations of Model 1. These values were used to perform tic-tac-toe pruning with τb = 1 × 10−3 and τs = 1 × 10−6 . Over the pruned charts, we ran 10 iterations of word-based ITG using EM or VB. The charts were then pruned further by applying the non-compositional constraint from the Viterbi alignment links of that model. Finally we ran 10 iterations of phrase-based ITG over the residual charts, using EM or VB, and extracted the Viterbi alignments. For translation, we used the standard phrasal decoding approach, based on a re-implementation of the Pharaoh system (Koehn, 2004). The output of the word alignment systems (GIZA++ or ITG) were fed to a standard phrase extraction procedure that extracted all phrases of length up to 7 and estimated the conditional probabilities of source given target and target given source using relative frequencies. Thus our phrasal ITG learns only the minimal non-compositional phrases; the standard phrase-extraction algorithm learns larger combinations of these minimal units. In addition the phrases were annotated with lexical weights using the IBM Model 1 tables. The decoder also used a trigram language model trained on the target sid"
P08-1012,W02-1018,0,0.18838,"ues is unite the word-level and phrase-level models into one learning procedure. Ideally, such a procedure would remedy the deficiencies of word-level alignment models, including the strong restrictions on the form of the alignment, and the strong independence assumption between words. Furthermore it would obviate the need for heuristic combination of word alignments. A unified procedure may also improve the identification of non-compositional phrasal translations, and the attachment decisions for unaligned words. In this direction, Expectation Maximization at the phrase level was proposed by Marcu and Wong (2002), who, however, experienced two major difficulties: computational complexity and controlling overfitting. Computational complexity arises from the exponentially large number of decompositions of a sentence pair into phrase pairs; overfitting is a problem because as EM attempts to maximize the likelihood of its training data, it prefers to directly explain a sentence pair with a single phrase pair. In this paper, we attempt to address these two issues in order to apply EM above the word level. 97 Proceedings of ACL-08: HLT, pages 97–105, c Columbus, Ohio, USA, June 2008. 2008 Association for Co"
P08-1012,E03-1035,1,0.44657,"ee (either as terminals or non-terminals) serves to not only speed up ITG parsing, but also to provide a kind of initialization hint to the training procedures, encouraging it to focus on promising regions of the alignment space. Given a bitext cell defined by the four boundary indices (i, j, l, m) as shown in Figure 1a, we prune based on a figure of merit V (i, j, l, m) approximating the utility of that cell in a full ITG parse. The figure of merit considers the Model 1 scores of not only the words inside a given cell, but also all the words not included in the source and target spans, as in Moore (2003) and Vogel (2005). Like Zhang and Gildea (2005), it is used to prune bitext cells rather than score phrases. The total score is the product of the Model 1 probabilities for each column; “inside” columns in the range [l, m] are scored according to the sum (or maximum) of Model 1 probabilities for [i, j], and “outside” columns use the sum (or maximum) of all probabilities not in the range [i, j]. Our pruning differs from Zhang and Gildea (2005) in two major ways. First, we perform pruning using both directions of the IBM Model 1 scores; instead of a single figure of merit V , we have two: VF and"
P08-1012,J03-1002,0,0.0141074,"7.2 End-to-end Evaluation Given an unlimited amount of time, we would tune the prior to maximize end-to-end performance, using an objective function such as BLEU. Unfortunately these experiments are very slow. Since we observed monotonic increases in alignment performance with smaller values of αC , we simply fixed the prior at a very small value (10−100 ) for all translation experiments. We do compare VB against EM in terms of final BLEU scores in the translation experiments to ensure that this sparse prior has a significant impact on the output. We also trained a baseline model with GIZA++ (Och and Ney, 2003) following a regimen of 5 iterations of Model 1, 5 iterations of HMM, and 5 iterations of Model 4. We computed Chinese-toEnglish and English-to-Chinese word translation tables using five iterations of Model 1. These values were used to perform tic-tac-toe pruning with τb = 1 × 10−3 and τs = 1 × 10−6 . Over the pruned charts, we ran 10 iterations of word-based ITG using EM or VB. The charts were then pruned further by applying the non-compositional constraint from the Viterbi alignment links of that model. Finally we ran 10 iterations of phrase-based ITG over the residual charts, using EM or VB"
P08-1012,J04-4002,0,0.0390477,"tions which have high precision as anchor points in the 102 bitext space to constraint ITG phrases. We use ITG Viterbi alignments instead. The benefit is two-fold. First of all, we do not have to run a GIZA++ aligner. Second, we do not need to worry about non-ITG word alignments, such as the (2, 4, 1, 3) permutation patterns. GIZA++ does not limit the set of permutations allowed during translation, so it can produce permutations that are not reachable using an ITG. Formally, given a word-based ITG alignment, the bootstrapping algorithm finds all the phrase pairs according to the definition of Och and Ney (2004) and Chiang (2005) with the additional constraint that each phrase pair contains at most one word link. Mathematically, let e(i, j) count the number of word links that are emitted from the substring ei...j , and f (l, m) count the number of word links emitted from the substring fl...m . The non-compositional phrase pairs satisfy e(i, j) = f (l, m) ≤ 1. Figure 3 (a) shows all possible non-compositional phrases given the Viterbi word alignment of the example sentence pair. 6 Summary of the Pipeline We summarize the pipeline of our system, demonstrating the interactions between the three main con"
P08-1012,P03-1021,0,0.03586,"d all phrases of length up to 7 and estimated the conditional probabilities of source given target and target given source using relative frequencies. Thus our phrasal ITG learns only the minimal non-compositional phrases; the standard phrase-extraction algorithm learns larger combinations of these minimal units. In addition the phrases were annotated with lexical weights using the IBM Model 1 tables. The decoder also used a trigram language model trained on the target side of the training data, as well as word count, phrase count, and distortion penalty features. Minimum Error Rate training (Och, 2003) over BLEU was used to optimize the weights for each of these models over the development test data. We used the NIST 2002 evaluation datasets for tuning and evaluation; the 10-reference development set was used for minimum error rate training, and the 4-reference test set was used for evaluation. We trained several phrasal translation systems, varying only the word alignment (or phrasal alignment) method. Table 1 compares the four systems: the GIZA++ baseline, the ITG word-based model, the ITG multiword model using EM training, and the ITG multiword model using VB training. ITG-mwm-VB is our"
P08-1012,C96-2141,0,0.614103,"ble, parsimonious parameter sets, leading to significant improvements in word alignment. This preference for sparse solutions together with effective pruning methods forms a phrase alignment regimen that produces better end-to-end translations than standard word alignment approaches. 1 Introduction Most state-of-the-art statistical machine translation systems are based on large phrase tables extracted from parallel text using word-level alignments. These word-level alignments are most often obtained using Expectation Maximization on the conditional generative models of Brown et al. (1993) and Vogel et al. (1996). As these word-level alignment models restrict the word alignment complexity by requiring each target word to align to zero or one source words, results are improved by aligning both source-to-target as well as target-to-source, then heuristically combining these alignments. Finally, the set of phrases consistent with the word alignments are extracted from every sentence pair; these form the basis of the decoding process. While this approach has been very successful, poor wordlevel alignments are nonetheless a common source of error in machine translation systems. A natural solution to severa"
P08-1012,2005.mtsummit-papers.33,0,0.067956,"minals or non-terminals) serves to not only speed up ITG parsing, but also to provide a kind of initialization hint to the training procedures, encouraging it to focus on promising regions of the alignment space. Given a bitext cell defined by the four boundary indices (i, j, l, m) as shown in Figure 1a, we prune based on a figure of merit V (i, j, l, m) approximating the utility of that cell in a full ITG parse. The figure of merit considers the Model 1 scores of not only the words inside a given cell, but also all the words not included in the source and target spans, as in Moore (2003) and Vogel (2005). Like Zhang and Gildea (2005), it is used to prune bitext cells rather than score phrases. The total score is the product of the Model 1 probabilities for each column; “inside” columns in the range [l, m] are scored according to the sum (or maximum) of Model 1 probabilities for [i, j], and “outside” columns use the sum (or maximum) of all probabilities not in the range [i, j]. Our pruning differs from Zhang and Gildea (2005) in two major ways. First, we perform pruning using both directions of the IBM Model 1 scores; instead of a single figure of merit V , we have two: VF and VB . Only those"
P08-1012,J97-3002,0,0.894319,"arning small noncompositional phrases. We address the tendency of EM to overfit by using Bayesian methods, where sparse priors assign greater mass to parameter vectors with fewer non-zero values therefore favoring shorter, more frequent phrases. We test our model by extracting longer phrases from our model’s alignments using traditional phrase extraction, and find that a phrase table based on our system improves MT results over a phrase table extracted from traditional word-level alignments. 2 Phrasal Inversion Transduction Grammar We use a phrasal extension of Inversion Transduction Grammar (Wu, 1997) as the generative framework. Our ITG has two nonterminals: X and C, where X represents compositional phrase pairs that can have recursive structures and C is the preterminal over terminal phrase pairs. There are three rules with X on the left-hand side: X → [X X], X → hX Xi, X → C. The first two rules are the straight rule and inverted rule respectively. They split the left-hand side constituent which represents a phrase pair into two smaller phrase pairs on the right-hand side and order them according to one of the two possible permutations. The rewriting process continues until the third ru"
P08-1012,P05-1059,1,0.873,"nals) serves to not only speed up ITG parsing, but also to provide a kind of initialization hint to the training procedures, encouraging it to focus on promising regions of the alignment space. Given a bitext cell defined by the four boundary indices (i, j, l, m) as shown in Figure 1a, we prune based on a figure of merit V (i, j, l, m) approximating the utility of that cell in a full ITG parse. The figure of merit considers the Model 1 scores of not only the words inside a given cell, but also all the words not included in the source and target spans, as in Moore (2003) and Vogel (2005). Like Zhang and Gildea (2005), it is used to prune bitext cells rather than score phrases. The total score is the product of the Model 1 probabilities for each column; “inside” columns in the range [l, m] are scored according to the sum (or maximum) of Model 1 probabilities for [i, j], and “outside” columns use the sum (or maximum) of all probabilities not in the range [i, j]. Our pruning differs from Zhang and Gildea (2005) in two major ways. First, we perform pruning using both directions of the IBM Model 1 scores; instead of a single figure of merit V , we have two: VF and VB . Only those spans that pass the pruning th"
P08-1025,P05-1033,0,0.0881751,"s. Thus, we are focusing on Inversion Transduction Grammars (Wu, 1997) which are an important subclass of SCFG. Formally, the rules in our grammar include preterminal unary rules: X → e/f for pairing up words or phrases in the two languages and binary production rules with straight or inverted orders that are responsible for building up upperlevel synchronous structures. They are straight rules written: X → [Y Z] and inverted rules written: X → hY Zi. Most practical non-binary SCFGs can be binarized using the synchronous binarization technique by Zhang et al. (2006). The Hiero-style rules of (Chiang, 2005), which are not strictly binary but binary only on nonterminals: X → yu X (1) you X (2) ; have X (2) with X (1) can be handled similarly through either offline binarization or allowing a fixed maximum number of gap words between the right hand side nonterminals in the decoder. For these reasons, the parsing problems for more realistic synchronous CFGs such as in Chiang (2005) and Galley et al. (2006) are formally equivalent to ITG. Therefore, we believe our focus on ITG 210 for the search efficiency issue is likely to generalize to other SCFG-based methods. Without an n-gram language model, de"
P08-1025,J07-2003,0,0.511542,"i, j, u1,..,n−1 , v1,..,n−1 ]) + α(X[i, j, u1 , vn−1 ]) + hBB (X, i, j, u1,...,n , v1,...,n ) (1) where β is the Viterbi inside cost and α is the Viterbi outside cost, to globally prioritize the n-gram integrated states on the agenda for exploration. The complexity of n-gram integrated decoding for SCFG has been tackled using other methods. The hook trick of Huang et al. (2005) factorizes the dynamic programming steps and lowers the asymptotic complexity of the n-gram integrated decoding, but has not been implemented in large-scale systems where massive pruning is present. The cube-pruning by Chiang (2007) and the lazy cube-pruning of Huang and Chiang (2007) turn the computation of beam pruning of CYK decoders into a top-k selection problem given two columns of translation hypotheses that need to be combined. The insight for doing the expansion top-down lazily is that there is no need to uniformly explore every cell. The algorithm starts with requesting the first best hypothesis from the root. The request translates into requests for the k-bests of some of its children and grandchildren and so on, because re-ranking at each node is needed to get the top ones. Venugopal et al. (2007) also take a"
P08-1025,P06-1121,0,0.0497188,"s written: X → [Y Z] and inverted rules written: X → hY Zi. Most practical non-binary SCFGs can be binarized using the synchronous binarization technique by Zhang et al. (2006). The Hiero-style rules of (Chiang, 2005), which are not strictly binary but binary only on nonterminals: X → yu X (1) you X (2) ; have X (2) with X (1) can be handled similarly through either offline binarization or allowing a fixed maximum number of gap words between the right hand side nonterminals in the decoder. For these reasons, the parsing problems for more realistic synchronous CFGs such as in Chiang (2005) and Galley et al. (2006) are formally equivalent to ITG. Therefore, we believe our focus on ITG 210 for the search efficiency issue is likely to generalize to other SCFG-based methods. Without an n-gram language model, decoding using SCFG is not much different from CFG parsing. At each time a CFG rule is applied on the input string, we apply the synchronized CFG rule for the output language. From a dynamic programming point of view, the DP states are X[i, j], where X ranges over all possible nonterminals and i and j range over 0 to the input string length |w|. Each state stores the best translations obtainable. When"
P08-1025,P96-1024,0,0.032353,"since each synchronous constituent in the tree adds a new 4-gram to the translation at the point where its children are concatenated, the additional pass approximately maximizes BLEU. Kumar and Byrne (2004) proposed the framework of Minimum Bayesian Risk (MBR) decoding that minimizes the expected loss given a loss function. Their MBR decoding is a reranking pass over an nbest list of translations returned by the decoder. Our algorithm is another dynamic programming decoding pass on the trigram forest, and is similar to the parsing algorithm for maximizing expected labelled recall presented by Goodman (1996). where α is the outside probability and β is the inside probability. We approximate β and α using the Viterbi probabilities. Since decoding from bottom up in the trigram pass already gives us the inside Viterbi scores, we only have to visit the nodes in the reverse order once we reach the root to compute the Viterbi outside scores. The outside-pass Algorithm 1 for bigram decoding can be generalized to the trigram case. We want to maximize over all translations (synchronous trees) T in the forest after the trigram decoding pass according to X EC([X, i, j, u, u′ , v ′ , v]). max T [X,i,j,u,u′ ,"
P08-1025,W05-1507,1,0.95154,"] can interact with each other by “peeping into” the leading and trailing n − 1 words on the output side for each state. Different boundary words differentiate the spanparameterized states. Thus, to preserve the dynamic programming property, we need to refine the states by adding the boundary words into the parameterization. The LM -integrated states are represented as X[i, j, u1,..,n−1 , v1,..,n−1 ]. Since the number of variables involved at each DP step has increased to 3 + 4(n − 1), the decoding algorithm is asymptotically O(|w|3+4(n−1) ). Although it is possible to use the “hook” trick of Huang et al. (2005) to factorize the DP operations to reduce the complexity to O(|w|3+3(n−1) ), when n is greater than 2, the complexity is still prohibitive. 3 Multi-pass LM-Integrated Decoding In this section, we describe a multi-pass progressive decoding technique that gradually augments the LM -integrated states from lower orders to higher orders. For instance, a bigram-integrated state [X, i, j, u, v] is said to be a coarse-level state of a trigram-integrate state [X, i, j, u, u′ , v ′ , v], because the latter state refines the previous by specifying more inner words. Progressive search has been used for HM"
P08-1025,N03-1016,0,0.0422729,"ive pass for the following n-gram pass. We need to do insideoutside parsing as coarse-to-fine parsers do. However, we use the outside probability or cost information differently. We do not combine the inside and outside costs of a simpler model to prune the space for a more complex model. Instead, for a given finergained state, we combine its true inside cost with the outside cost of its coarse-level counter-part to estimate its worthiness of being explored. The use of the outside cost from a coarser-level as the outside estimate makes our method naturally fall in the framework of A* parsing. Klein and Manning (2003) describe an A* parsing framework for monolingual parsing and admissible outside estimates that are computed using inside/outside parsing algorithm on simplified PCFGs compared to the original PCFG. Zhang and Gildea (2006) describe A* for ITG and develop admissible heuristics for both alignment and decoding. Both have shown the effectiveness of A* in situations where the outside estimate approximates the true cost closely such as when the sentences are short. For decoding long sentences, it is difficult to come up with good admissible (or inadmissible) heuristics. If we can afford a bigram dec"
P08-1025,N04-1022,0,0.0875274,"u, u′ ]) + rule(X → hY Zi) + bigram(u′ , v ′ )} ′ α(Z[k, j, u, u ]) = max {α(Z[k, j, u, u′ ]), α(X[i, j, u, v]) + β(Y [i, k, v ′ , v]) + rule(X → hY Zi) + bigram(u′ , v ′ )} end if end for end for we deal with the mismatch by introducing another decoding pass that maximizes the expected count of synchronous constituents in the tree corresponding to the translation returned. BLEU is based on n-gram precision, and since each synchronous constituent in the tree adds a new 4-gram to the translation at the point where its children are concatenated, the additional pass approximately maximizes BLEU. Kumar and Byrne (2004) proposed the framework of Minimum Bayesian Risk (MBR) decoding that minimizes the expected loss given a loss function. Their MBR decoding is a reranking pass over an nbest list of translations returned by the decoder. Our algorithm is another dynamic programming decoding pass on the trigram forest, and is similar to the parsing algorithm for maximizing expected labelled recall presented by Goodman (1996). where α is the outside probability and β is the inside probability. We approximate β and α using the Viterbi probabilities. Since decoding from bottom up in the trigram pass already gives us"
P08-1025,H05-1101,0,0.0154863,"egin by introducing Synchronous Context Free Grammars and their decoding algorithms when an n-gram language model is integrated into the grammatical search space. A synchronous CFG (SCFG) is a set of contextfree rewriting rules for recursively generating string pairs. Each synchronous rule is a pair of CFG rules 209 Proceedings of ACL-08: HLT, pages 209–217, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics with the nonterminals on the right hand side of one CFG rule being one-to-one mapped to the other CFG rule via a permutation π. We adopt the SCFG notation of Satta and Peserico (2005). Superscript indices in the right-hand side of grammar rules: (1) (π(1)) (π(n)) X → X1 ...Xn(n) , Xπ(1) ...Xπ(n) indicate that the nonterminals with the same index are linked across the two languages, and will eventually be rewritten by the same rule application. Each Xi is a variable which can take the value of any nonterminal in the grammar. In this paper, we focus on binary SCFGs and without loss of generality assume that only the preterminal unary rules can generate terminal string pairs. Thus, we are focusing on Inversion Transduction Grammars (Wu, 1997) which are an important subclass o"
P08-1025,N07-1063,0,0.0938584,". The cube-pruning by Chiang (2007) and the lazy cube-pruning of Huang and Chiang (2007) turn the computation of beam pruning of CYK decoders into a top-k selection problem given two columns of translation hypotheses that need to be combined. The insight for doing the expansion top-down lazily is that there is no need to uniformly explore every cell. The algorithm starts with requesting the first best hypothesis from the root. The request translates into requests for the k-bests of some of its children and grandchildren and so on, because re-ranking at each node is needed to get the top ones. Venugopal et al. (2007) also take a two-pass decoding approach, with the first pass leaving the language model boundary words out of the dynamic programming state, such that only one hypothesis is retained for each span and grammar symbol. 4 Decoding to Maximize BLEU The ultimate goal of efficient decoding to find the translation that has a highest evaluation score using the least time possible. Section 3 talks about utilizing the outside cost of a lower-order model to estimate the outside cost of a higher-order model, boosting the search for the higher-order model. By doing so, we hope the intrinsic metric of our m"
P08-1025,J97-3002,0,0.0265446,"SCFG notation of Satta and Peserico (2005). Superscript indices in the right-hand side of grammar rules: (1) (π(1)) (π(n)) X → X1 ...Xn(n) , Xπ(1) ...Xπ(n) indicate that the nonterminals with the same index are linked across the two languages, and will eventually be rewritten by the same rule application. Each Xi is a variable which can take the value of any nonterminal in the grammar. In this paper, we focus on binary SCFGs and without loss of generality assume that only the preterminal unary rules can generate terminal string pairs. Thus, we are focusing on Inversion Transduction Grammars (Wu, 1997) which are an important subclass of SCFG. Formally, the rules in our grammar include preterminal unary rules: X → e/f for pairing up words or phrases in the two languages and binary production rules with straight or inverted orders that are responsible for building up upperlevel synchronous structures. They are straight rules written: X → [Y Z] and inverted rules written: X → hY Zi. Most practical non-binary SCFGs can be binarized using the synchronous binarization technique by Zhang et al. (2006). The Hiero-style rules of (Chiang, 2005), which are not strictly binary but binary only on nonter"
P08-1025,W06-1627,1,0.941301,"ts of a simpler model to prune the space for a more complex model. Instead, for a given finergained state, we combine its true inside cost with the outside cost of its coarse-level counter-part to estimate its worthiness of being explored. The use of the outside cost from a coarser-level as the outside estimate makes our method naturally fall in the framework of A* parsing. Klein and Manning (2003) describe an A* parsing framework for monolingual parsing and admissible outside estimates that are computed using inside/outside parsing algorithm on simplified PCFGs compared to the original PCFG. Zhang and Gildea (2006) describe A* for ITG and develop admissible heuristics for both alignment and decoding. Both have shown the effectiveness of A* in situations where the outside estimate approximates the true cost closely such as when the sentences are short. For decoding long sentences, it is difficult to come up with good admissible (or inadmissible) heuristics. If we can afford a bigram decoding pass, the outside cost from a bigram model is conceivably a 211 very good estimate of the outside cost using a trigram model since a bigram language model and a trigram language model must be strongly correlated. Alt"
P08-1025,N06-1033,1,0.857507,"l unary rules can generate terminal string pairs. Thus, we are focusing on Inversion Transduction Grammars (Wu, 1997) which are an important subclass of SCFG. Formally, the rules in our grammar include preterminal unary rules: X → e/f for pairing up words or phrases in the two languages and binary production rules with straight or inverted orders that are responsible for building up upperlevel synchronous structures. They are straight rules written: X → [Y Z] and inverted rules written: X → hY Zi. Most practical non-binary SCFGs can be binarized using the synchronous binarization technique by Zhang et al. (2006). The Hiero-style rules of (Chiang, 2005), which are not strictly binary but binary only on nonterminals: X → yu X (1) you X (2) ; have X (2) with X (1) can be handled similarly through either offline binarization or allowing a fixed maximum number of gap words between the right hand side nonterminals in the decoder. For these reasons, the parsing problems for more realistic synchronous CFGs such as in Chiang (2005) and Galley et al. (2006) are formally equivalent to ITG. Therefore, we believe our focus on ITG 210 for the search efficiency issue is likely to generalize to other SCFG-based meth"
P09-2012,P01-1010,0,0.879883,"0 150 100 50 0 0 2 4 6 8 subtree height 10 12 2.2 Collapsed Gibbs sampling with a DP prior2 14 For an excellent introduction to collapsed Gibbs sampling with a DP prior, we refer the reader to Appendix A of Goldwater et al. (2009), which we follow closely here. Our training data is a set of parse trees T that we assume was produced by an unknown TSG g with probability Pr(T |g). Using Bayes’ rule, we can compute the probability of a particular hypothesized grammar as Figure 1: Subtree count (thousands) across heights for the “all subtrees” grammar () and the superior “minimal subset” () from Bod (2001). events can simply be counted in the training data. In contrast, there are no treebanks annotated with TSG derivations, and a treebank parse tree of n nodes is ambiguous among 2n possible derivations. One solution would be to manually annotate a treebank with TSG derivations, but in addition to being expensive, this task requires one to know what the grammar actually is. Part of the thinking motivating TSGs is to let the data determine the best set of subtrees. One approach to grammar-learning is DataOriented Parsing (DOP), whose strategy is to simply take all subtrees in the training data as"
P09-2012,C02-1126,0,0.0180372,"nd CFGs (and their probabilistic counterparts, which concern us here) by allowing nonterminals to be rewritten as subtrees of arbitrary size. Although nonterminal rewrites are still context-free, in practice TSGs can loosen the independence assumptions of CFGs because larger rules capture more context. This is simpler than the complex independence and backoff decisions of Markovized grammars. Furthermore, subtrees with terminal symbols can be viewed as learning dependencies among the words in the subtree, obviating the need for the manual specification (Magerman, 1995) or automatic inference (Chiang and Bikel, 2002) of lexical dependencies. Following standard notation for PCFGs, the probability of a derivation d in the grammar is given as Y Pr(r) Pr(d) = Introduction Tree substition grammars (TSGs) have potential advantages over regular context-free grammars (CFGs), but there is no obvious way to learn these grammars. In particular, learning procedures are not able to take direct advantage of manually annotated corpora like the Penn Treebank, which are not marked for derivations and thus assume a standard CFG. Since different TSG derivations can produce the same parse tree, learning procedures must guess"
P09-2012,N09-1062,0,0.470075,"oduct of the PCFG rules r ∈ t that constitute it and a geometric distribution Pr$ over the number of those rules, thus encoding a preference for smaller subtrees.3 The parameter α contributes to the probability that previously unseen subtrees will be sampled. All DPs share parameters p$ and α. An entire grammar is then given as g = {gX : X ∈ N }. We emphasize that no head information is used by the sampler. Rather than explicitly consider each segmentation of the parse trees (which would define a TSG and its associated parameters), we use a collapsed Gibbs sampler to integrate over all possi2 Cohn et al. (2009) and O’Donnell et al. (2009) independently developed similar models. 3 GX (t) = 0 unless root(t) = X. 2P R . P +R 46 3 Experiments S1 NP ADVP VP NN RB VBZ Someone always makes 3.1 Setup We used the standard split for the Wall Street Journal portion of the Treebank, training on sections 2 to 21, and reporting results on sentences with no more than forty words from section 23. We compare with three other grammars. S2 NP VP PRP VB you quit • A standard Treebank PCFG. • A “spinal” TSG, produced by extracting n lexicalized subtrees from each length n sentence in the training data. Each subtree is d"
P09-2012,D08-1033,0,0.0676377,"Missing"
P09-2012,P95-1037,0,0.0792182,"ed ones on parsing accuracy. 1 TSGs extend CFGs (and their probabilistic counterparts, which concern us here) by allowing nonterminals to be rewritten as subtrees of arbitrary size. Although nonterminal rewrites are still context-free, in practice TSGs can loosen the independence assumptions of CFGs because larger rules capture more context. This is simpler than the complex independence and backoff decisions of Markovized grammars. Furthermore, subtrees with terminal symbols can be viewed as learning dependencies among the words in the subtree, obviating the need for the manual specification (Magerman, 1995) or automatic inference (Chiang and Bikel, 2002) of lexical dependencies. Following standard notation for PCFGs, the probability of a derivation d in the grammar is given as Y Pr(r) Pr(d) = Introduction Tree substition grammars (TSGs) have potential advantages over regular context-free grammars (CFGs), but there is no obvious way to learn these grammars. In particular, learning procedures are not able to take direct advantage of manually annotated corpora like the Penn Treebank, which are not marked for derivations and thus assume a standard CFG. Since different TSG derivations can produce the"
P09-2012,E93-1006,0,\N,Missing
P09-2012,C96-2215,0,\N,Missing
P11-1046,N07-1019,1,0.914196,"Missing"
P11-1046,P06-2036,1,0.855315,"iven strategies for an LCFRS production with a head and n modifiers. Choosing among these possible strategies affects both the time and the space complexity of parsing. In this paper we have shown that optimizing the choice according to either metric is NP-hard. To our knowledge, our results are the first NP-hardness results for a grammar factorization problem. SCFGs and STAGs are specific instances of LCFRSs. Grammar factorization for synchronous models is an important component of current machine translation systems (Zhang et al., 2006), and algorithms for factorization have been studied by Gildea et al. (2006) for SCFGs and by Nesson et al. (2008) for STAGs. These algorithms do not result in what we refer as head-driven strategies, although, as machine translation systems improve, lexicalized rules may become important in this setting as well. However, the results we have presented in this paper do not carry over to the above mentioned synchronous models, since the fan-out of these models is bounded by two, while in our reductions in Section 3 we freely use unbounded values for this parameter. Thus the computational complexity of optimizing the choice of the parsing strategy for SCFGs is still an o"
P11-1046,N10-1118,1,0.786484,"w recent papers have investigated this trade-off taking gen450 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 450–459, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics eral LCFRS rules as input. G´omez-Rodr´ıguez et al. (2009) present an algorithm for binarization of LCFRSs while keeping fan-out as small as possible. The algorithm is exponential in the resulting fan-out, and G´omez-Rodr´ıguez et al. (2009) mention as an important open question whether polynomialtime algorithms to minimize fan-out are possible. Gildea (2010) presents a related method for binarizing rules while keeping the time complexity of parsing as small as possible. Binarization turns out to be possible with no penalty in time complexity, but, again, the factorization algorithm is exponential in the resulting time complexity. Gildea (2011) shows that a polynomial time algorithm for factorizing LCFRSs in order to minimize time complexity would imply an improved approximation algorithm for the well-studied graph-theoretic property known as treewidth. However, whether the problem of factorizing LCFRSs in order to minimize time complexity is NP-h"
P11-1046,J11-1008,1,0.931244,"ıguez et al. (2009) present an algorithm for binarization of LCFRSs while keeping fan-out as small as possible. The algorithm is exponential in the resulting fan-out, and G´omez-Rodr´ıguez et al. (2009) mention as an important open question whether polynomialtime algorithms to minimize fan-out are possible. Gildea (2010) presents a related method for binarizing rules while keeping the time complexity of parsing as small as possible. Binarization turns out to be possible with no penalty in time complexity, but, again, the factorization algorithm is exponential in the resulting time complexity. Gildea (2011) shows that a polynomial time algorithm for factorizing LCFRSs in order to minimize time complexity would imply an improved approximation algorithm for the well-studied graph-theoretic property known as treewidth. However, whether the problem of factorizing LCFRSs in order to minimize time complexity is NP-hard is still an open question in the above works. Similar questions have arisen in the context of machine translation, as the SCFGs used to model translation are also instances of LCFRSs, as already mentioned. For SCFG, Satta and Peserico (2005) showed that the exponent in the time complexi"
P11-1046,N09-1061,1,0.878802,"Missing"
P11-1046,N10-1035,1,0.838515,"Missing"
P11-1046,J09-4009,1,0.873338,"Ss in order to minimize time complexity is NP-hard is still an open question in the above works. Similar questions have arisen in the context of machine translation, as the SCFGs used to model translation are also instances of LCFRSs, as already mentioned. For SCFG, Satta and Peserico (2005) showed that the exponent in the time complexity of parsing algorithms must grow at least as fast as the square root of the rule rank, and Gildea and ˇ Stefankoviˇ c (2007) tightened this bound to be linear in the rank. However, neither paper provides an algorithm for finding the best parsing strategy, and Huang et al. (2009) mention that whether finding the optimal parsing strategy for an SCFG rule is NPhard is an important problem for future work. In this paper, we investigate the problem of rule binarization for LCFRSs in the context of headdriven parsing strategies. Head-driven strategies begin with one rhs symbol, and add one nonterminal at a time. This rules out any factorization in which two subsets of nonterminals of size greater than one are combined in a single step. Head-driven strategies allow for the techniques of lexicalization and Markovization that are widely used in (projective) statistical parsin"
P11-1046,C10-1061,0,0.219426,"hankar et al., 1987) constitute a very general grammatical formalism which subsumes contextfree grammars (CFGs) and tree adjoining grammars (TAGs), as well as the synchronous context-free grammars (SCFGs) and synchronous tree adjoining grammars (STAGs) used as models in machine translation.1 LCFRSs retain the fundamental property of CFGs that grammar nonterminals rewrite independently, but allow nonterminals to generate discontinuous phrases, that is, to generate more than one span in the string being produced. This important feature has been recently exploited by Maier and Søgaard (2008) and Kallmeyer and Maier (2010) for modeling phrase structure treebanks with discontinuous constituents, and by Kuhlmann and Satta (2009) for modeling non-projective dependency treebanks. The rules of a LCFRS can be analyzed in terms of the properties of rank and fan-out. Rank is the 1 To be more precise, SCFGs and STAGs generate languages composed by pair of strings, while LCFRSs generate string languages. We can abstract away from this difference by assuming concatenation of components in a string pair. number of nonterminals on the right-hand side (rhs) of a rule, while fan-out is the number of spans of the string genera"
P11-1046,E09-1055,1,0.894676,"FGs) and tree adjoining grammars (TAGs), as well as the synchronous context-free grammars (SCFGs) and synchronous tree adjoining grammars (STAGs) used as models in machine translation.1 LCFRSs retain the fundamental property of CFGs that grammar nonterminals rewrite independently, but allow nonterminals to generate discontinuous phrases, that is, to generate more than one span in the string being produced. This important feature has been recently exploited by Maier and Søgaard (2008) and Kallmeyer and Maier (2010) for modeling phrase structure treebanks with discontinuous constituents, and by Kuhlmann and Satta (2009) for modeling non-projective dependency treebanks. The rules of a LCFRS can be analyzed in terms of the properties of rank and fan-out. Rank is the 1 To be more precise, SCFGs and STAGs generate languages composed by pair of strings, while LCFRSs generate string languages. We can abstract away from this difference by assuming concatenation of components in a string pair. number of nonterminals on the right-hand side (rhs) of a rule, while fan-out is the number of spans of the string generated by the nonterminal in the lefthand side (lhs) of the rule. CFGs are equivalent to LCFRSs with fan-out"
P11-1046,P08-1069,1,0.908781,"n with a head and n modifiers. Choosing among these possible strategies affects both the time and the space complexity of parsing. In this paper we have shown that optimizing the choice according to either metric is NP-hard. To our knowledge, our results are the first NP-hardness results for a grammar factorization problem. SCFGs and STAGs are specific instances of LCFRSs. Grammar factorization for synchronous models is an important component of current machine translation systems (Zhang et al., 2006), and algorithms for factorization have been studied by Gildea et al. (2006) for SCFGs and by Nesson et al. (2008) for STAGs. These algorithms do not result in what we refer as head-driven strategies, although, as machine translation systems improve, lexicalized rules may become important in this setting as well. However, the results we have presented in this paper do not carry over to the above mentioned synchronous models, since the fan-out of these models is bounded by two, while in our reductions in Section 3 we freely use unbounded values for this parameter. Thus the computational complexity of optimizing the choice of the parsing strategy for SCFGs is still an open problem. Finally, our results for"
P11-1046,P10-1054,1,0.771437,"e and space complexity that are dependent on the rank and fan-out of the grammar rules. Whenever it is possible, binarization of LCFRS rules, or reduction of rank to two, is therefore important for parsing, as it reduces the time complexity needed for dynamic programming. This has lead to a number of binarization algorithms for LCFRSs, as well as factorization algorithms that factor rules into new rules with smaller rank, without necessarily reducing rank all the way to two. Kuhlmann and Satta (2009) present an algorithm for binarizing certain LCFRS rules without increasing their fan-out, and Sagot and Satta (2010) show how to reduce rank to the lowest value possible for LCFRS rules of fan-out two, again without increasing fan-out. G´omez-Rodr´ıguez et al. (2010) show how to factorize well-nested LCFRS rules of arbitrary fan-out for efficient parsing. In general there may be a trade-off required between rank and fan-out, and a few recent papers have investigated this trade-off taking gen450 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 450–459, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics eral LCFRS rules as input."
P11-1046,H05-1101,1,0.86926,"lgorithm is exponential in the resulting time complexity. Gildea (2011) shows that a polynomial time algorithm for factorizing LCFRSs in order to minimize time complexity would imply an improved approximation algorithm for the well-studied graph-theoretic property known as treewidth. However, whether the problem of factorizing LCFRSs in order to minimize time complexity is NP-hard is still an open question in the above works. Similar questions have arisen in the context of machine translation, as the SCFGs used to model translation are also instances of LCFRSs, as already mentioned. For SCFG, Satta and Peserico (2005) showed that the exponent in the time complexity of parsing algorithms must grow at least as fast as the square root of the rule rank, and Gildea and ˇ Stefankoviˇ c (2007) tightened this bound to be linear in the rank. However, neither paper provides an algorithm for finding the best parsing strategy, and Huang et al. (2009) mention that whether finding the optimal parsing strategy for an SCFG rule is NPhard is an important problem for future work. In this paper, we investigate the problem of rule binarization for LCFRSs in the context of headdriven parsing strategies. Head-driven strategies"
P11-1046,P87-1015,0,0.882354,"tta Dip. di Ingegneria dell’Informazione Universit`a di Padova Abstract We study the problem of finding the best headdriven parsing strategy for Linear ContextFree Rewriting System productions. A headdriven strategy must begin with a specified righthand-side nonterminal (the head) and add the remaining nonterminals one at a time in any order. We show that it is NP-hard to find the best head-driven strategy in terms of either the time or space complexity of parsing. 1 Andrea Marino Dip. di Sistemi e Informatica Universit`a di Firenze Introduction Linear Context-Free Rewriting Systems (LCFRSs) (Vijay-Shankar et al., 1987) constitute a very general grammatical formalism which subsumes contextfree grammars (CFGs) and tree adjoining grammars (TAGs), as well as the synchronous context-free grammars (SCFGs) and synchronous tree adjoining grammars (STAGs) used as models in machine translation.1 LCFRSs retain the fundamental property of CFGs that grammar nonterminals rewrite independently, but allow nonterminals to generate discontinuous phrases, that is, to generate more than one span in the string being produced. This important feature has been recently exploited by Maier and Søgaard (2008) and Kallmeyer and Maier"
P11-1046,N06-1033,1,0.817626,"to parsers with discontinuous spans. However, there are n! possible head-driven strategies for an LCFRS production with a head and n modifiers. Choosing among these possible strategies affects both the time and the space complexity of parsing. In this paper we have shown that optimizing the choice according to either metric is NP-hard. To our knowledge, our results are the first NP-hardness results for a grammar factorization problem. SCFGs and STAGs are specific instances of LCFRSs. Grammar factorization for synchronous models is an important component of current machine translation systems (Zhang et al., 2006), and algorithms for factorization have been studied by Gildea et al. (2006) for SCFGs and by Nesson et al. (2008) for STAGs. These algorithms do not result in what we refer as head-driven strategies, although, as machine translation systems improve, lexicalized rules may become important in this setting as well. However, the results we have presented in this paper do not carry over to the above mentioned synchronous models, since the fan-out of these models is bounded by two, while in our reductions in Section 3 we freely use unbounded values for this parameter. Thus the computational complex"
P11-1046,P97-1003,0,\N,Missing
P11-2070,P09-2036,0,0.0517091,"er of right-hand-side nonterminals 6 7 Figure 1: Rule Statistics pruning, we show that different strategies do have a significant effect in translation quality. Other works investigating alternative binarization methods mostly focus on the effect of nonterminal sharing. Xiao et al. (2009) also proposed a CYKlike algorithm for synchronous binarization. Apparently the lack of virtual nonterminal sharing in their decoder caused heavy competition between virtual nonterminals, and they created a cost function to “diversify” binarization trees, which is equivalent to minimizing nonterminal sharing. DeNero et al. (2009b) used a greedy method to maximize virtual nonterminal sharing on the source side during the -LM parsing phase. They show that effective source-side binarization can improve the efficiency of parsing SCFG. However, their method works only on the source side, and synchronous binarization is put off to the +LM decoding phase (DeNero et al., 2009a). Although these ideas all lead to faster decoding and reduced search errors, there can be conflicts in the constraints each of them has on the form of rules and accommodating all of them can be a challenge. In this paper, we present a cubic time algor"
P11-2070,N09-1026,0,0.0575274,"er of right-hand-side nonterminals 6 7 Figure 1: Rule Statistics pruning, we show that different strategies do have a significant effect in translation quality. Other works investigating alternative binarization methods mostly focus on the effect of nonterminal sharing. Xiao et al. (2009) also proposed a CYKlike algorithm for synchronous binarization. Apparently the lack of virtual nonterminal sharing in their decoder caused heavy competition between virtual nonterminals, and they created a cost function to “diversify” binarization trees, which is equivalent to minimizing nonterminal sharing. DeNero et al. (2009b) used a greedy method to maximize virtual nonterminal sharing on the source side during the -LM parsing phase. They show that effective source-side binarization can improve the efficiency of parsing SCFG. However, their method works only on the source side, and synchronous binarization is put off to the +LM decoding phase (DeNero et al., 2009a). Although these ideas all lead to faster decoding and reduced search errors, there can be conflicts in the constraints each of them has on the form of rules and accommodating all of them can be a challenge. In this paper, we present a cubic time algor"
P11-2070,P81-1022,0,0.273794,"ide and early language model integration on the target language side. We also examine how different strategies of target-side terminal attachment during binarization can significantly affect translation quality. 1 Introduction Synchronous context-free grammars (SCFG) are behind most syntax-based machine translation models. Efficient machine translation decoding with an SCFG requires converting the grammar into a binarized form, either explicitly, as in synchronous binarization (Zhang et al., 2006), where virtual nonterminals are generated for binarization, or implicitly, as in Earley parsing (Earley, 1970), where dotted items are used. Given a source-side binarized SCFG with terminal set T and nonterminal set N , the time complexity of decoding a sentence of length n with a m-gram language model is (Venugopal et al., 2007): O(n3 (|N |· |T |2(m−1) )K ) where K is the maximum number of right-hand-side nonterminals. SCFG binarization serves two important goals: • Parsing complexity for unbinarized SCFG grows exponentially with the number of nonterminals on the right-hand side of grammar rules. Binarization ensures cubic time decoding in terms of input sentence length. In this paper, we examine a C"
P11-2070,N04-1035,0,0.0903139,"for the original unbinarized rule and applying the heuristic to its binarized rules, this still grants no benefit over late terminal attachment. We show in our experiment that late target-side terminal attachment significantly outperforms early target side terminal attachment. 3 Experiments 3.1 Setup The first binarization is generated by attaching the target-side terminals as low as possible in a postit is defined as an expectation over input strings, instead of an expectation over trees. 404 We test our binarization algorithm on an ChineseEnglish translation task. We extract a GHKM grammar (Galley et al., 2004) from a parallel corpus with the parsed English side with some modification so terms of BLEU score, decoding speed, or model score when comparing translation results that used grammars that employed nonterminal sharing maximization and ones that did not. In the rest of this section, all the results we discuss use nonterminal sharing maximization as a part of the cost function. We then compare the effects of early target-side terminal attachment and late attachment. Figure 2 shows model scores of each decoder run with varying bin sizes, and Figure 3 shows BLEU scores for corresponding runs of t"
P11-2070,J99-4004,0,0.0203156,"nals created by binarizations always have contiguous spans on both sides (Huang, 2007). 402 Even with the synchronous binarization constraint, many possible binarizations exist. Analysis of our Chinese-English parallel corpus has shown that the majority of synchronously binarizable rules with arity smaller than 4 are monotonic, i.e., the target-side nonterminal permutation is either strictly increasing or decreasing (See Figure 1). For monotonic rules, any source-side binarization is also a permissible synchronous binarization. The binarization problem can be formulated as a semiring parsing (Goodman, 1999) problem. We define a cost function that considers different binarization criteria. A CYK-like algorithm can be used to find the best binarization tree according to the cost function. Consider an SCFG rule X → hγ, αi, where γ and α stand for the source side and the target side. Let B(γ) be the set of all possible binarization trees for γ. With the cost function c defined over hyperedges in a binarization tree t, the optimal binarization tree tˆ is X c(h) tˆ = argmin t∈B(γ) h∈t where c(h) is the cost of a hyperedge h in t. The optimization problem can be solved by Algorithm 1. hi, k, ji denotes"
P11-2070,D10-1063,0,0.369773,"nbinarized SCFG grows exponentially with the number of nonterminals on the right-hand side of grammar rules. Binarization ensures cubic time decoding in terms of input sentence length. In this paper, we examine a CYK-like synchronous binarization algorithm that integrates a novel criterion in a unified semiring parsing framework. The criterion we present has explicit consideration of source-side terminals. In general, terminals in a rule have a lower probability of being matched given a sentence, and therefore have the effect of “anchoring” a rule and limiting its possible application points. Hopkins and Langmead (2010) formalized this concept as the scope of a rule. A rule of scope of k can be parsed in O(nk ). The scope of a rule can be calculated by counting the number of adjacent nonterminal pairs and boundary nonterminals. For example, A → w1 BCw2 D has scope two. Building on the concept of scope, we define a cost function that estimates the expected number of hyperedges to be built when a particular binarization tree is applied to unseen data. This effectively puts hard-to-match derivations at the bottom of the binarization tree, which enables the decoder to decide early on whether an unbinarized rule"
P11-2070,J09-4009,1,0.835346,"bracketing of nonterminals is not violated. The following example is taken from Zhang et al. (2006): ADJP → RB 负责 PP 的 NN, RB responsible for the NN PP With the source-side binarization fixed, we can produce distinct binarized rules by choosing different ways of attaching target-side terminals: ADJP ADJP → [RB 负责]1 h [PP 的]3 NN i2 , [RB]1 h resp. for the NN [PP]3 i2 → [RB 负责]1 h [PP 的]3 NN i2 , [RB]1 resp. for the h NN [PP]3 i2 order traversal of the binarization tree. The conventional wisdom is that early consideration of targetside terminals promotes early language model score integration (Huang et al., 2009). The second binarization, on the contrary, attaches the target-side terminals as high as possible in the binarization tree. We argue that this late target-side terminal attachment is in fact better for two reasons. First, as in the example above, compare the following two rules resulting from early attachment of target terminals and late attachment of target terminals: hi2 → []3 NN, resp. for the NN []3 hi2 → []3 NN, NN []3 The former has a much smaller chance of sharing the same target side with other binarized rules because on the target side, many nonterminals will be attached without any"
P11-2070,W07-0405,0,0.0191208,"off to the +LM decoding phase (DeNero et al., 2009a). Although these ideas all lead to faster decoding and reduced search errors, there can be conflicts in the constraints each of them has on the form of rules and accommodating all of them can be a challenge. In this paper, we present a cubic time algorithm to find the best binarization tree, given the conflicting constraints. 2 The Binarization Algorithm An SCFG rule is synchronously binarizable if when simultaneously binarizing source and target sides, virtual nonterminals created by binarizations always have contiguous spans on both sides (Huang, 2007). 402 Even with the synchronous binarization constraint, many possible binarizations exist. Analysis of our Chinese-English parallel corpus has shown that the majority of synchronously binarizable rules with arity smaller than 4 are monotonic, i.e., the target-side nonterminal permutation is either strictly increasing or decreasing (See Figure 1). For monotonic rules, any source-side binarization is also a permissible synchronous binarization. The binarization problem can be formulated as a semiring parsing (Goodman, 1999) problem. We define a cost function that considers different binarizatio"
P11-2070,N07-1063,0,0.0218963,"1 Introduction Synchronous context-free grammars (SCFG) are behind most syntax-based machine translation models. Efficient machine translation decoding with an SCFG requires converting the grammar into a binarized form, either explicitly, as in synchronous binarization (Zhang et al., 2006), where virtual nonterminals are generated for binarization, or implicitly, as in Earley parsing (Earley, 1970), where dotted items are used. Given a source-side binarized SCFG with terminal set T and nonterminal set N , the time complexity of decoding a sentence of length n with a m-gram language model is (Venugopal et al., 2007): O(n3 (|N |· |T |2(m−1) )K ) where K is the maximum number of right-hand-side nonterminals. SCFG binarization serves two important goals: • Parsing complexity for unbinarized SCFG grows exponentially with the number of nonterminals on the right-hand side of grammar rules. Binarization ensures cubic time decoding in terms of input sentence length. In this paper, we examine a CYK-like synchronous binarization algorithm that integrates a novel criterion in a unified semiring parsing framework. The criterion we present has explicit consideration of source-side terminals. In general, terminals in"
P11-2070,J97-3002,0,0.037769,"e guarantees that we will find a tree that is a synchronously binarized if one exists. 2.2 Early Source-Side Terminal Matching When a rule is being applied while parsing a sentence, terminals in the rule have less chance of being matched. We can exploit this fact by taking terminals into account during binarization and placing terminals lower in the binarization tree. Consider the following SCFG rule: VP → PP 提出 JJ NN, propose a JJ NN PP The synchronous binarization algorithm of Zhang et al. (2006) binarizes the rule1 by finding the rightmost binarizable points on the source side: 1 We follow Wu (1997) and use square brackets for straight rules and pointed brackets for inverted rules. We also mark brackets with indices to represent virtual nonterminals. 403 VP → PP [提出 [JJ NN]1 ]2 , [[propose a JJ NN]1 ]2 PP The source side of the first binarized rule “[]1 → JJ NN, propose a JJ NN” contains a very frequent nonterminal sequence “JJ NN”. If one were to parse with the binarized rule, and if the virtual nonterminal []1 has been built, the parser needs to continue following the binarization tree in order to determine whether the original rule would be matched. Furthermore, having two consecutive"
P11-2070,D09-1038,0,0.0255014,"Missing"
P11-2070,N06-1033,1,0.914592,"resent an SCFG binarization algorithm that combines the strengths of early terminal matching on the source language side and early language model integration on the target language side. We also examine how different strategies of target-side terminal attachment during binarization can significantly affect translation quality. 1 Introduction Synchronous context-free grammars (SCFG) are behind most syntax-based machine translation models. Efficient machine translation decoding with an SCFG requires converting the grammar into a binarized form, either explicitly, as in synchronous binarization (Zhang et al., 2006), where virtual nonterminals are generated for binarization, or implicitly, as in Earley parsing (Earley, 1970), where dotted items are used. Given a source-side binarized SCFG with terminal set T and nonterminal set N , the time complexity of decoding a sentence of length n with a m-gram language model is (Venugopal et al., 2007): O(n3 (|N |· |T |2(m−1) )K ) where K is the maximum number of right-hand-side nonterminals. SCFG binarization serves two important goals: • Parsing complexity for unbinarized SCFG grows exponentially with the number of nonterminals on the right-hand side of grammar r"
P11-2070,P11-2072,1,\N,Missing
P11-2070,J07-2003,0,\N,Missing
P11-2072,P05-1033,0,0.611791,"nt of Computer Science University of Rochester Rochester, NY 14627 Abstract We discuss some of the practical issues that arise from decoding with general synchronous context-free grammars. We examine problems caused by unary rules and we also examine how virtual nonterminals resulting from binarization can best be handled. We also investigate adding more flexibility to synchronous context-free grammars by adding glue rules and phrases. 1 Introduction Synchronous context-free grammar (SCFG) is widely used for machine translation. There are many different ways to extract SCFGs from data. Hiero (Chiang, 2005) represents a more restricted form of SCFG, while GHKM (Galley et al., 2004) uses a general form of SCFG. In this paper, we discuss some of the practical issues that arise from decoding general SCFGs that are seldom discussed in the literature. We focus on parsing grammars extracted using the method put forth by Galley et al. (2004), but the solutions to these issues are applicable to other general forms of SCFG with many nonterminals. The GHKM grammar extraction method produces a large number of unary rules. Unary rules are the rules that have exactly one nonterminal and no terminals on the s"
P11-2072,J07-2003,0,0.444586,"lel corpus with the English side parsed. The corpus consists of 250K sentence pairs, which is 6.3M words on the English side. Terminal-aware synchronous binarization (Fang et al., 2011) was applied to all GHKM grammars that are not scopefiltered. MERT (Och, 2003) was used to tune parameters. We used a 392-sentence development set with four references for parameter tuning, and a 428sentence test set with four references for testing. Our in-house decoder was used for experiments with a trigram language model. The decoder is capable of both CNF parsing and Earley-style parsing with cube-pruning (Chiang, 2007). For the experiment that incorporated phrases, the phrase pairs were extracted from the same corpus with the same set of alignments. We have limited the maximum size of phrases to be four. 5.2 Results Our result is summarized in Table 2. The baseline GHKM grammar with monotonic glue rules yielded a worse result than the no-unary grammar with the same glue rules. The difference is statistically significant at p < 0.05 based on 1000 iterations of paired bootstrap resampling (Koehn, 2004). Compared to using monotonic glue rules, using ABC glue rules brought slight improvements for both the no-un"
P11-2072,P81-1022,0,0.737767,"unary productions. This causes unary derivations to go on forever. The solution is to set a maximum length for unary chains, or to ban negative unary productions outright. 3 Issues with binarization 3.1 Filtering and binarization Synchronous binarization (Zhang et al., 2006) is an effective method to reduce SCFG parsing complexity and allow early language model integration. However, it creates virtual nonterminals which require special attention at parsing time. Alternatively, we can filter rules that have more than scope-3 to parse in O(n3 ) time with unbinarized rules. This requires Earley (Earley, 1970) style parsing, which does implicit binarization at decoding time. Scopefiltering may filter out unnecessarily long rules that may never be applied, but it may also throw out rules with useful contextual information. In addition, scope-filtering does not accommodate early language model state integration. We compare the two with an experiment. For the rest of the section, we discuss issues created by virtual nonterminals. 3.2 Handling virtual nonterminals One aspect of grammar binarization that is rarely mentioned is how to assign probabilities to binarized grammar rules. The naïve solution is"
P11-2072,P11-2070,1,0.773549,"alized rules to be extracted. 416 Baseline + monotonic glue rules No-unary + monotonic glue rules No-unary + ABC glue rules No-unary (scope-filtered) + monotonic No-unary (scope-filtered) + ABC glue rules No-unary + ABC glue rules + phrases BLEU 20.99 23.83 23.94 23.99 24.09 23.43 Table 2: BLEU score results for Chinese-English with different settings 5 Experiments 5.1 Setup We extracted a GHKM grammar from a ChineseEnglish parallel corpus with the English side parsed. The corpus consists of 250K sentence pairs, which is 6.3M words on the English side. Terminal-aware synchronous binarization (Fang et al., 2011) was applied to all GHKM grammars that are not scopefiltered. MERT (Och, 2003) was used to tune parameters. We used a 392-sentence development set with four references for parameter tuning, and a 428sentence test set with four references for testing. Our in-house decoder was used for experiments with a trigram language model. The decoder is capable of both CNF parsing and Earley-style parsing with cube-pruning (Chiang, 2007). For the experiment that incorporated phrases, the phrase pairs were extracted from the same corpus with the same set of alignments. We have limited the maximum size of ph"
P11-2072,N04-1035,0,0.9489,"stract We discuss some of the practical issues that arise from decoding with general synchronous context-free grammars. We examine problems caused by unary rules and we also examine how virtual nonterminals resulting from binarization can best be handled. We also investigate adding more flexibility to synchronous context-free grammars by adding glue rules and phrases. 1 Introduction Synchronous context-free grammar (SCFG) is widely used for machine translation. There are many different ways to extract SCFGs from data. Hiero (Chiang, 2005) represents a more restricted form of SCFG, while GHKM (Galley et al., 2004) uses a general form of SCFG. In this paper, we discuss some of the practical issues that arise from decoding general SCFGs that are seldom discussed in the literature. We focus on parsing grammars extracted using the method put forth by Galley et al. (2004), but the solutions to these issues are applicable to other general forms of SCFG with many nonterminals. The GHKM grammar extraction method produces a large number of unary rules. Unary rules are the rules that have exactly one nonterminal and no terminals on the source side. They may be problematic for decoders since they may create cycle"
P11-2072,P06-1121,0,0.189929,"he phrase extraction methods used in phrase-based models, since, in GHKM grammar extraction, phrase segmentation is constrained by parse trees. This may be a good thing, but it suffers from loss of flexibility, and it also cannot use non-constituent phrases. We use the method of Koehn et al. (2003) to extract phrases, and, for each phrase, we add a rule with the glue nonterminal as the left-hand side and the phrase pair as the right-hand side. We experiment to see whether adding phrases is beneficial. There have been other efforts to extend GHKM grammar to allow more flexible rule extraction. Galley et al. (2006) introduce composed rules where minimal GHKM rules are fused to form larger rules. Zollmann and Venugopal (2006) introduce a model that allows more generalized rules to be extracted. 416 Baseline + monotonic glue rules No-unary + monotonic glue rules No-unary + ABC glue rules No-unary (scope-filtered) + monotonic No-unary (scope-filtered) + ABC glue rules No-unary + ABC glue rules + phrases BLEU 20.99 23.83 23.94 23.99 24.09 23.43 Table 2: BLEU score results for Chinese-English with different settings 5 Experiments 5.1 Setup We extracted a GHKM grammar from a ChineseEnglish parallel corpus wit"
P11-2072,D10-1063,0,0.235313,"ary production chains that contain duplicated dynamic programming states. In later sections, we discuss why unary rules are problematic and investigate two possible solutions. GHKM grammars often have rules with many right-hand-side nonterminals and require binarization to ensure O(n3 ) time parsing. However, binarization creates a large number of virtual nonterminals. We discuss the challenges of, and possible solutions to, issues arising from having a large number of virtual nonterminals. We also compare binarizing the grammar with filtering rules according to scope, a concept introduced by Hopkins and Langmead (2010). By explicitly considering the effect of anchoring terminals on input sentences, scope3 rules encompass a much larger set of rules than Chomsky normal form but they can still be parsed in O(n3 ) time. Unlike phrase-based machine translation, GHKM grammars are less flexible in how they can segment sentence pairs into phrases because they are restricted not only by alignments between words in sentence pairs, but also by target-side parse trees. In general, GHKM grammars suffer more from data sparsity than phrasal rules. To alleviate this issue, we discuss adding glue rules and phrases extracted"
P11-2072,N03-1016,0,0.017944,"omparison. For our experiments, we use an outside estimate as a heuristic for a virtual item. Consider the following rule binarization (only the source side shown): A → BCD : − log(p) ⇒ V → BC : 0 A → VD : − log(p) 415 A → BCD is the orginal rule and − log(p) is the cost of the rule. In decoding time, when a chart item is generated from the binarized rule V → BC, we add − log(p) to its total cost as an optimistic estimate of the cost to build the original unbinarized rule. The heuristic is used only for pruning purposes, and it does not change the real cost. The idea is similar to A* parsing (Klein and Manning, 2003). One complication is that a binarized rule can arise from multiple different unbinarized rules. In this case, we pick the lowest cost among the unbinarized rules as the heuristic. Another approach for handling virtual nonterminals would be giving virtual items separate bins and avoiding pruning them at all. This is usually not practical for GHKM grammars, because of the large number of nonterminals. 4 Adding flexibility 4.1 Glue rules Because of data sparsity, an SCFG extracted from data may fail to parse sentences at test time. For example, consider the following rules: NP → JJ NN, JJ NN JJ"
P11-2072,N03-1017,0,0.0330525,"ty. Although it is reasonable to believe that reordering should always have evidence in data, as with GHKM rules, we may wish to reorder based on evidence from the language model. In our experiments, we compare the ABC glue rules with the monotonic glue rules. 4.2 Adding phrases GHKM grammars are more restricted than the phrase extraction methods used in phrase-based models, since, in GHKM grammar extraction, phrase segmentation is constrained by parse trees. This may be a good thing, but it suffers from loss of flexibility, and it also cannot use non-constituent phrases. We use the method of Koehn et al. (2003) to extract phrases, and, for each phrase, we add a rule with the glue nonterminal as the left-hand side and the phrase pair as the right-hand side. We experiment to see whether adding phrases is beneficial. There have been other efforts to extend GHKM grammar to allow more flexible rule extraction. Galley et al. (2006) introduce composed rules where minimal GHKM rules are fused to form larger rules. Zollmann and Venugopal (2006) introduce a model that allows more generalized rules to be extracted. 416 Baseline + monotonic glue rules No-unary + monotonic glue rules No-unary + ABC glue rules No"
P11-2072,W04-3250,0,0.230396,"Missing"
P11-2072,P03-1021,0,0.0265677,"glue rules No-unary + ABC glue rules No-unary (scope-filtered) + monotonic No-unary (scope-filtered) + ABC glue rules No-unary + ABC glue rules + phrases BLEU 20.99 23.83 23.94 23.99 24.09 23.43 Table 2: BLEU score results for Chinese-English with different settings 5 Experiments 5.1 Setup We extracted a GHKM grammar from a ChineseEnglish parallel corpus with the English side parsed. The corpus consists of 250K sentence pairs, which is 6.3M words on the English side. Terminal-aware synchronous binarization (Fang et al., 2011) was applied to all GHKM grammars that are not scopefiltered. MERT (Och, 2003) was used to tune parameters. We used a 392-sentence development set with four references for parameter tuning, and a 428sentence test set with four references for testing. Our in-house decoder was used for experiments with a trigram language model. The decoder is capable of both CNF parsing and Earley-style parsing with cube-pruning (Chiang, 2007). For the experiment that incorporated phrases, the phrase pairs were extracted from the same corpus with the same set of alignments. We have limited the maximum size of phrases to be four. 5.2 Results Our result is summarized in Table 2. The baselin"
P11-2072,J97-3002,0,0.324325,"sequence c1 c3 and c2 c3 but not c1 c2 c3 , if we have not seen “NP → JJ JJ NN” at training time. Because SCFGs neither model adjunction, nor are they markovized, with a small amount of data, such problems can occur. Therefore, we may opt to add glue rules as used in Hiero (Chiang, 2005): S → C, C S → S C, S C where S is the goal state and C is the glue nonterminal that can produce any nonterminals. We refer to these glue rules as the monotonic glue rules. We rely on GHKM rules for reordering when we use the monotonic glue rules. However, we can also allow glue rules to reorder constituents. Wu (1997) presents a better-constrained grammar designed to only produce tail-recursive parses. See Table 1 for the complete set of rules. We refer to these rules as ABC glue rules. These rules always generate leftS→A S→B S→C A → [A B] A → [B B] A → [C B] A → [A C] A → [B C] A → [C C] B→hBAi B→hAAi B→hCAi B→hBCi B→hACi B→hCCi Table 1: The ABC Grammar. We follow the convention of Wu (1997) that square brackets stand for straight rules and angle brackets stand for inverted rules. heavy derivations, weeding out ambiguity and making search more efficient. We learn probabilities of ABC glue rules by using e"
P11-2072,N06-1033,1,0.89532,", this is often not a problem: if all unary derivations have positive costs and a priority queue is used to expand unary derivations, only the best K unary items will be generated, where K is the pruning constant. • Ban negative cost unary rules When tuning feature weights, an optimizer may try feature weights that may give negative costs to unary productions. This causes unary derivations to go on forever. The solution is to set a maximum length for unary chains, or to ban negative unary productions outright. 3 Issues with binarization 3.1 Filtering and binarization Synchronous binarization (Zhang et al., 2006) is an effective method to reduce SCFG parsing complexity and allow early language model integration. However, it creates virtual nonterminals which require special attention at parsing time. Alternatively, we can filter rules that have more than scope-3 to parse in O(n3 ) time with unbinarized rules. This requires Earley (Earley, 1970) style parsing, which does implicit binarization at decoding time. Scopefiltering may filter out unnecessarily long rules that may never be applied, but it may also throw out rules with useful contextual information. In addition, scope-filtering does not accommo"
P11-2072,W06-3119,0,0.03561,"mentation is constrained by parse trees. This may be a good thing, but it suffers from loss of flexibility, and it also cannot use non-constituent phrases. We use the method of Koehn et al. (2003) to extract phrases, and, for each phrase, we add a rule with the glue nonterminal as the left-hand side and the phrase pair as the right-hand side. We experiment to see whether adding phrases is beneficial. There have been other efforts to extend GHKM grammar to allow more flexible rule extraction. Galley et al. (2006) introduce composed rules where minimal GHKM rules are fused to form larger rules. Zollmann and Venugopal (2006) introduce a model that allows more generalized rules to be extracted. 416 Baseline + monotonic glue rules No-unary + monotonic glue rules No-unary + ABC glue rules No-unary (scope-filtered) + monotonic No-unary (scope-filtered) + ABC glue rules No-unary + ABC glue rules + phrases BLEU 20.99 23.83 23.94 23.99 24.09 23.43 Table 2: BLEU score results for Chinese-English with different settings 5 Experiments 5.1 Setup We extracted a GHKM grammar from a ChineseEnglish parallel corpus with the English side parsed. The corpus consists of 250K sentence pairs, which is 6.3M words on the English side."
P11-2072,D08-1076,0,\N,Missing
P12-2060,J93-2003,0,0.109499,"of Rochester Rochester, NY 14627 Abstract Bayesian approaches have been shown to reduce the amount of overfitting that occurs when running the EM algorithm, by placing prior probabilities on the model parameters. We apply one such Bayesian technique, variational Bayes, to the IBM models of word alignment for statistical machine translation. We show that using variational Bayes improves the performance of the widely used GIZA++ software, as well as improving the overall performance of the Moses machine translation system in terms of BLEU score. 1 Introduction The IBM Models of word alignment (Brown et al., 1993), along with the Hidden Markov Model (HMM) (Vogel et al., 1996), serve as the starting point for most current state-of-the-art machine translation systems, both phrase-based and syntax-based (Koehn et al., 2007; Chiang, 2005; Galley et al., 2004). Both the IBM Models and the HMM are trained using the EM algorithm (Dempster et al., 1977). Recently, Bayesian techniques have become widespread in applications of EM to natural language processing tasks, as a very general method of controlling overfitting. For instance, Johnson (2007) showed the benefits of such techniques when applied to HMMs for u"
P12-2060,P05-1033,0,0.057855,"Bayesian technique, variational Bayes, to the IBM models of word alignment for statistical machine translation. We show that using variational Bayes improves the performance of the widely used GIZA++ software, as well as improving the overall performance of the Moses machine translation system in terms of BLEU score. 1 Introduction The IBM Models of word alignment (Brown et al., 1993), along with the Hidden Markov Model (HMM) (Vogel et al., 1996), serve as the starting point for most current state-of-the-art machine translation systems, both phrase-based and syntax-based (Koehn et al., 2007; Chiang, 2005; Galley et al., 2004). Both the IBM Models and the HMM are trained using the EM algorithm (Dempster et al., 1977). Recently, Bayesian techniques have become widespread in applications of EM to natural language processing tasks, as a very general method of controlling overfitting. For instance, Johnson (2007) showed the benefits of such techniques when applied to HMMs for unsupervised part of speech tagging. In machine translation, Blunsom et al. (2008) and DeNero et al. (2008) use Bayesian techniques to learn bilingual phrase pairs. In this setting, which involves finding a segmentation of th"
P12-2060,D08-1033,0,0.218359,"Missing"
P12-2060,N04-1035,0,0.0709631,"nique, variational Bayes, to the IBM models of word alignment for statistical machine translation. We show that using variational Bayes improves the performance of the widely used GIZA++ software, as well as improving the overall performance of the Moses machine translation system in terms of BLEU score. 1 Introduction The IBM Models of word alignment (Brown et al., 1993), along with the Hidden Markov Model (HMM) (Vogel et al., 1996), serve as the starting point for most current state-of-the-art machine translation systems, both phrase-based and syntax-based (Koehn et al., 2007; Chiang, 2005; Galley et al., 2004). Both the IBM Models and the HMM are trained using the EM algorithm (Dempster et al., 1977). Recently, Bayesian techniques have become widespread in applications of EM to natural language processing tasks, as a very general method of controlling overfitting. For instance, Johnson (2007) showed the benefits of such techniques when applied to HMMs for unsupervised part of speech tagging. In machine translation, Blunsom et al. (2008) and DeNero et al. (2008) use Bayesian techniques to learn bilingual phrase pairs. In this setting, which involves finding a segmentation of the input sentences into"
P12-2060,D07-1031,0,0.179738,"BLEU score. 1 Introduction The IBM Models of word alignment (Brown et al., 1993), along with the Hidden Markov Model (HMM) (Vogel et al., 1996), serve as the starting point for most current state-of-the-art machine translation systems, both phrase-based and syntax-based (Koehn et al., 2007; Chiang, 2005; Galley et al., 2004). Both the IBM Models and the HMM are trained using the EM algorithm (Dempster et al., 1977). Recently, Bayesian techniques have become widespread in applications of EM to natural language processing tasks, as a very general method of controlling overfitting. For instance, Johnson (2007) showed the benefits of such techniques when applied to HMMs for unsupervised part of speech tagging. In machine translation, Blunsom et al. (2008) and DeNero et al. (2008) use Bayesian techniques to learn bilingual phrase pairs. In this setting, which involves finding a segmentation of the input sentences into phrasal units, it is particularly important to control the tendency of EM to choose longer phrases, which explain the training data well but are unlikely to generalize. However, most state-of-the-art machine translation systems today are built on the basis of wordlevel alignments of the"
P12-2060,P07-2045,0,0.00445123,"s. We apply one such Bayesian technique, variational Bayes, to the IBM models of word alignment for statistical machine translation. We show that using variational Bayes improves the performance of the widely used GIZA++ software, as well as improving the overall performance of the Moses machine translation system in terms of BLEU score. 1 Introduction The IBM Models of word alignment (Brown et al., 1993), along with the Hidden Markov Model (HMM) (Vogel et al., 1996), serve as the starting point for most current state-of-the-art machine translation systems, both phrase-based and syntax-based (Koehn et al., 2007; Chiang, 2005; Galley et al., 2004). Both the IBM Models and the HMM are trained using the EM algorithm (Dempster et al., 1977). Recently, Bayesian techniques have become widespread in applications of EM to natural language processing tasks, as a very general method of controlling overfitting. For instance, Johnson (2007) showed the benefits of such techniques when applied to HMMs for unsupervised part of speech tagging. In machine translation, Blunsom et al. (2008) and DeNero et al. (2008) use Bayesian techniques to learn bilingual phrase pairs. In this setting, which involves finding a segm"
P12-2060,P11-2032,0,0.180125,"ith probability 0.1, then the sentence will have a higher probability if EM assigns the rare word and its actual translation a probability of t(f1 |e1 ) = 0.5, and assigns the rare word’s translation to f2 a probability of t(f2 |e1 ) = 0.5, than if it assigns a probability of 1 to the correct translation t(f1 |e1 ). Moore suggests a number of solutions to this issue, including add-n smoothing and initializing the probabilities based on a heuristic rather than choosing uniform probabilities. When combined, his solutions cause a significant decrease in alignment error rate (AER). More recently, Mermer and Saraclar (2011) have added a Bayesian prior to IBM Model 1 using Gibbs sampling for inference, showing improvements in BLEU scores. In this paper, we describe the results of incorpo306 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 306–310, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics rating variational Bayes (VB) into the widely used GIZA++ software for word alignment. We use VB both because it converges more quickly than Gibbs sampling, and because it can be applied in a fairly straightforward manner to all of the"
P12-2060,P04-1066,0,0.1322,"lingual phrase pairs. In this setting, which involves finding a segmentation of the input sentences into phrasal units, it is particularly important to control the tendency of EM to choose longer phrases, which explain the training data well but are unlikely to generalize. However, most state-of-the-art machine translation systems today are built on the basis of wordlevel alignments of the type generated by GIZA++ from the IBM Models and the HMM. Overfitting is also a problem in this context, and improving these word alignment systems could be of broad utility in machine translation research. Moore (2004) discusses details of how EM overfits the data when training IBM Model 1. He discovers that the EM algorithm is particularly susceptible to overfitting in the case of rare words, due to the “garbage collection” phenomenon. Suppose a sentence contains an English word e1 that occurs nowhere else in the data, and its French translation f1 . Suppose that same sentence also contains a word e2 which occurs frequently in the overall data but whose translation in this sentence, f2 , co-occurs with it infrequently. If the translation t(f2 |e2 ) occurs with probability 0.1, then the sentence will have a"
P12-2060,P00-1056,0,0.203584,"rovided by Yang Liu, also with sure alignments only. For computing BLEU scores, we used single reference datasets for FrenchEnglish and German-English, and four references for Chinese-English. For minimum error rate training, we used 1000 sentences for French-English, 2000 sentences for German-English, and 1274 sentences for Chinese-English. Our test sets contained 1000 sentences each for French-English and German-English, and 686 sentences for ChineseEnglish. For scoring the Viterbi alignments of each system against gold-standard annotated alignments, we use the alignment error rate (AER) of Och and Ney (2000), which measures agreement at the level of pairs of words. We ran our code on ten thousand sentence pairs to determine the best value of α for the translation probabilities t(f |e). For our training, we ran GIZA++ for five iterations each of Model 1, the HMM, Model 3, and Model 4. Variational Bayes was only used for Model 1. Figure 1 shows how VB, and different values of α in particular, affect the performance of GIZA++ in terms of AER. We discover that, after all training is complete, VB improves the performance of the overall system, lowering AER (Figure 1) for all three language pairs. We f"
P12-2060,C96-2141,0,0.909808,"have been shown to reduce the amount of overfitting that occurs when running the EM algorithm, by placing prior probabilities on the model parameters. We apply one such Bayesian technique, variational Bayes, to the IBM models of word alignment for statistical machine translation. We show that using variational Bayes improves the performance of the widely used GIZA++ software, as well as improving the overall performance of the Moses machine translation system in terms of BLEU score. 1 Introduction The IBM Models of word alignment (Brown et al., 1993), along with the Hidden Markov Model (HMM) (Vogel et al., 1996), serve as the starting point for most current state-of-the-art machine translation systems, both phrase-based and syntax-based (Koehn et al., 2007; Chiang, 2005; Galley et al., 2004). Both the IBM Models and the HMM are trained using the EM algorithm (Dempster et al., 1977). Recently, Bayesian techniques have become widespread in applications of EM to natural language processing tasks, as a very general method of controlling overfitting. For instance, Johnson (2007) showed the benefits of such techniques when applied to HMMs for unsupervised part of speech tagging. In machine translation, Blu"
P13-1007,A88-1008,0,0.613809,"ork treats a conjunction of NPs as separate NPs. However, similar to plurals, NP conjunctions (disjunctions) introduce an extra scopal element: a universal (existential). We are working on an annotation scheme for NP conjunctions, so we have left this for after the annotations become available. 70 Implicit universals only (pairs with at least one id) P+ R+ der to evade scope disambiguation, yet be able to perform entailment, Koller and Thater (2010) propose an algorithm to calculate the weakest readings20 from a scope-underspecified representation. Early efforts on automatic QSD (Moran, 1988; Hurum, 1988) were based on heuristics, manually formed into rules with manually assigned weights for resolving conflicts. To the best of our knowledge, there have been four major efforts on statistical QSD for English: Higgins and Sadock (2003), Galen and MacCartney (2004), Srinivasan and Yates (2009), and Manshadi and Allen (2011a). The first three only scope two scopal terms in a sentence, where the scopal term is an NP with an explicit quantification. MA11 is the first to scope any number of NPs in a sentence with no restriction on the type of quantification. Besides ignoring negation and implicit univ"
P13-1007,P92-1005,0,0.405946,") optimizing a well justified criterion, by using automatically generated features instead of hand-annotated dependencies, and by boosting the performance by a large margin with the help of a rich feature vector. This work can be improved in many directions, among which are scoping more elements such as other scopal operators and implicit entities, deploying more complex learning models, and developing models which require less supervision. Related work Since automatic QSD is in general challenging, traditionally quantifier scoping is left underspecified in deep linguistic processing systems (Alshawi and Crouch, 1992; Bos, 1996; Copestake et al., 2001). Some efforts have been made to move underspecification frameworks towards weighted constraint-based graphs in order to produce the most preferred reading (Koller et al., 2008), but the source of these types of constraint are often discourse, pragmatics, world knowledge, etc., and hence, they are hard to obtain automatically. In orAcknowledgement We need to thank William de Beaumont and Jonathan Gordon for their comments on the paper and Omid Bakhshandeh for his assistance. This work was supported in part by NSF grant 1012205, and ONR grant N000141110417. 1"
P13-1007,D08-1073,0,0.0342002,"universals and negations are easier to scope, even for the human annotators.17 There are several reasons for poor performance with negations as well. First, the number of negations in the corpus is small, therefore the data is very sparse. Second, the RPC model does not work well for negations. Scoping a negation relative to an NP chunk, with which it has a long distance dependency, often depends on the scope of the elements in between. Third, scoping negation usually requires a deep semantic analysis. In order to see how well our approximation algorithm is working, similar to the approach of Chambers and Jurafsky (2008), we tried an ILP solver18 for DAGs with at most 8 nodes to find the optimum solution, but we found the difference insignificant. In fact, the approximation algorithm finds the optimum solution in all but one case.19 5 6 Summary and future work We develop the first statistical QSD model addressing the interaction of quantifiers with negation and the implicit universal of plurals, defining a baseline for this task on QuanText data (Manshadi et al., 2012). In addition, our work improves upon Manshadi and Allen (2011a)’s work by (approximately) optimizing a well justified criterion, by using auto"
P13-1007,P03-1054,0,0.0134095,"nks only (no negation) Where ni is the number of scopal terms introduced by sentence i. Out of the 4500 samples, around 1800 involve at least one implicit universal (i.e., id), but only 120 samples contain a negation. We evaluate the performance of the system for implicit universals and negation both separately and in the context of full scope disambiguation. We split the corpus at random into three sets of 50, 100, and 350 sentences, as development, test, and train sets respectively.13 To extract part-of-speech tags, phrase structure trees, and typed dependencies, we use the Stanford parser (Klein and Manning, 2003; de Marneffe et al., 2006) on both train and test sets. Since we are using SVM, we have passed the confidence levels through a softmax function to convert them λ before applying the algointo probabilities Pu,v rithm of Section 3. We take MA11’s system as the baseline. However, in order to have a fair comparison, we have used the output of the Stanford parser to automatically generate the same features that MA11 have hand-annotated.14 In order to run the baseline system on implicit universals, we take the feature vector of a plural NP and add a feature to indicate that this feature vector repr"
P13-1007,P10-1004,0,0.0149296,"to part-of-speech tags and untyped dependency relations. 15 SV M M ulticlass from SVM-light (Joachims, 1999). 16 In all experiments, we ignore NP conjunctions. Previous work treats a conjunction of NPs as separate NPs. However, similar to plurals, NP conjunctions (disjunctions) introduce an extra scopal element: a universal (existential). We are working on an annotation scheme for NP conjunctions, so we have left this for after the annotations become available. 70 Implicit universals only (pairs with at least one id) P+ R+ der to evade scope disambiguation, yet be able to perform entailment, Koller and Thater (2010) propose an algorithm to calculate the weakest readings20 from a scope-underspecified representation. Early efforts on automatic QSD (Moran, 1988; Hurum, 1988) were based on heuristics, manually formed into rules with manually assigned weights for resolving conflicts. To the best of our knowledge, there have been four major efforts on statistical QSD for English: Higgins and Sadock (2003), Galen and MacCartney (2004), Srinivasan and Yates (2009), and Manshadi and Allen (2011a). The first three only scope two scopal terms in a sentence, where the scopal term is an NP with an explicit quantifica"
P13-1007,P01-1019,0,0.0162985,"on, by using automatically generated features instead of hand-annotated dependencies, and by boosting the performance by a large margin with the help of a rich feature vector. This work can be improved in many directions, among which are scoping more elements such as other scopal operators and implicit entities, deploying more complex learning models, and developing models which require less supervision. Related work Since automatic QSD is in general challenging, traditionally quantifier scoping is left underspecified in deep linguistic processing systems (Alshawi and Crouch, 1992; Bos, 1996; Copestake et al., 2001). Some efforts have been made to move underspecification frameworks towards weighted constraint-based graphs in order to produce the most preferred reading (Koller et al., 2008), but the source of these types of constraint are often discourse, pragmatics, world knowledge, etc., and hence, they are hard to obtain automatically. In orAcknowledgement We need to thank William de Beaumont and Jonathan Gordon for their comments on the paper and Omid Bakhshandeh for his assistance. This work was supported in part by NSF grant 1012205, and ONR grant N000141110417. 17 Trivially, we have taken the relat"
P13-1007,P08-1026,0,0.0643141,"Missing"
P13-1007,W08-1301,0,0.0412108,"Missing"
P13-1007,P11-1060,0,0.0165486,"d corpora, resulting in the lack of work on extensive 1. Three words start with a capital letter. A deep understanding of this sentence, requires deciding whether each word in the set, referred to by Three words, starts with a potentially distinct capital letter (as in Apple, Orange, Banana) or there is a unique capital letter which each word starts with (as in Apple, Adam, Athens). By treating the NP Three words as a single atomic entity, earlier work on automatic QSD has overlooked this problem. In general, every plural NP potentially introduces an implicit universal, ranging 1 For example, Liang et al. (2011) in their state-of-the-art statistical semantic parser within the domain of natural language queries to databases, explicitly devise quantifier scoping in the semantic model. 64 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 64–72, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics over the collection of entities introduced by the plural.2 Scoping this implicit universal is just as important. While explicit universals may not occur very often in natural language, the usage of plurals is very common. Plurals form 18"
P13-1007,de-marneffe-etal-2006-generating,0,0.029263,"Missing"
P13-1007,W11-1108,1,0.947536,"antly improve the performance of the previous model using a rich set of automatically generated features. 1 Introduction The sentence there is one faculty member in every graduate committee is ambiguous with respect to quantifier scoping, since there are at least two possible readings: If one has wide scope, there is a unique faculty member on every committee. If every has wide scope, there can be different faculty members on each committee. Over the past decade there has been some work on statistical quantifier scope disambiguation (QSD) (Higgins and Sadock, 2003; Galen and MacCartney, 2004; Manshadi and Allen, 2011a). However, the extent of the work has been quite limited for several reasons. First, in the past two decades, the main focus of the NLP community has been on shallow text processing. As a deep processing task, QSD is not essential for many NLP applications that do not require deep understanding. Second, there has been a lack of comprehensive scope-disambiguated corpora, resulting in the lack of work on extensive 1. Three words start with a capital letter. A deep understanding of this sentence, requires deciding whether each word in the set, referred to by Three words, starts with a potential"
P13-1007,P11-2025,1,0.911758,"Missing"
P13-1007,manshadi-etal-2012-annotation,1,0.910505,"se limitations and scope an arbitrary number of NPs in a sentence with no restriction on the type of quantification. However, although their corpus annotates the scope of negations and the implicit universal of plurals, their QSD system does not handle those. As a step towards comprehensive automatic QSD, in this paper we present our work on automatic scoping of the implicit universal of plurals and negations. For data, we use a new revision of MA11’s corpus, first introduced in Manshadi et al. (2011b). The new revision, called QuanText, carries a more detailed, fine-grained scope annotation (Manshadi et al., 2012). The performance of our model defines a baseline for future efforts on (comprehensive) QSD over QuanText. In addition to addressing plurality and negation, this work improves upon MA11’s in two directions. • We theoretically justify MA11’s ternaryclassification approach, formulating it as a general framework for learning to build partial orders. An n log n algorithm is then given to find a guaranteed approximation within a fixed ratio of the optimal solution from a set of pairwise preferences (Sect. 3.1). • We replace MA11’s hand-annotated features with a set of automatically generated lingui"
P13-1007,P88-1005,0,0.569371,"s. Previous work treats a conjunction of NPs as separate NPs. However, similar to plurals, NP conjunctions (disjunctions) introduce an extra scopal element: a universal (existential). We are working on an annotation scheme for NP conjunctions, so we have left this for after the annotations become available. 70 Implicit universals only (pairs with at least one id) P+ R+ der to evade scope disambiguation, yet be able to perform entailment, Koller and Thater (2010) propose an algorithm to calculate the weakest readings20 from a scope-underspecified representation. Early efforts on automatic QSD (Moran, 1988; Hurum, 1988) were based on heuristics, manually formed into rules with manually assigned weights for resolving conflicts. To the best of our knowledge, there have been four major efforts on statistical QSD for English: Higgins and Sadock (2003), Galen and MacCartney (2004), Srinivasan and Yates (2009), and Manshadi and Allen (2011a). The first three only scope two scopal terms in a sentence, where the scopal term is an NP with an explicit quantification. MA11 is the first to scope any number of NPs in a sentence with no restriction on the type of quantification. Besides ignoring negation and"
P13-1007,W10-1809,0,0.0294866,"scoping, i and j are incomparable. This happens if both orders are equivalent (as in two existentials) or when the two chunks have no scope interaction. Since a partial order can be represented by a Directed Acyclic Graph (DAG), we use DAGs to represent scopings. For example, G1 in Figure 1 represents the scoping in (4). 2.1 Evaluation metrics Given the gold standard DAG Gg = (V, Eg ) and the predicted DAG Gp = (V, Ep ), a similarity measure may be defined based on the ratio of the number of pairs (of nodes) labeled correctly to the 2 Although plurals carry different types of quantification (Herbelot and Copestake, 2010), almost always there exists an implicit universal. The importance of scoping this universal, however, may vary based on the type of quantification. 65 2 2 1 1 3 2 Our framework 3.1 3 3 4 1 4 2 1 Since we defined QSD as a partial ordering, automatic QSD would become the problem of learning to build partial orders. The machine learning community has studied the problem of learning total orders (ranking) in depth (Cohen et al., 1999; Furnkranz and Hullermeier, 2003; Hullermeier et al., 2008). Many ranking systems create partial orders as output when the confidence level for the relative order of"
P13-1007,W95-0107,0,0.133641,"set of pairwise preferences (Sect. 3.1). • We replace MA11’s hand-annotated features with a set of automatically generated linguistic features. Our rich set of features significantly improves the performance of the QSD model, even though we give up the goldstandard dependency features (Sect. 3.3). 2 Task definition In QuanText, scope-bearing elements (or, as we call them, scopal terms) of each sentence have been identified using labeled chunks, as in (3). 3. Replace [1/ every line] in [2/ the file] ending in [3/ punctuation] with [4/ a blank line] . NP chunks follow the definition of baseNP (Ramshaw and Marcus, 1995) and hence are flat. Outscoping relations are used to specify the relative scope of scopal terms. The relation i > j means that chunk i outscopes (or has wide scope over) chunk j. Equivalently, chunk j is said to have narrow scope with respect to i. Each sentence is annotated with its most preferred scoping (according to the annotators’ judgement), represented as a partial order: 4. SI : (2 > 1 > 4; 1 > 3) If neither i > j nor j > i is entailed from the scoping, i and j are incomparable. This happens if both orders are equivalent (as in two existentials) or when the two chunks have no scope in"
P13-1007,J03-1004,0,0.419339,"hich works very well in practice. Finally, we significantly improve the performance of the previous model using a rich set of automatically generated features. 1 Introduction The sentence there is one faculty member in every graduate committee is ambiguous with respect to quantifier scoping, since there are at least two possible readings: If one has wide scope, there is a unique faculty member on every committee. If every has wide scope, there can be different faculty members on each committee. Over the past decade there has been some work on statistical quantifier scope disambiguation (QSD) (Higgins and Sadock, 2003; Galen and MacCartney, 2004; Manshadi and Allen, 2011a). However, the extent of the work has been quite limited for several reasons. First, in the past two decades, the main focus of the NLP community has been on shallow text processing. As a deep processing task, QSD is not essential for many NLP applications that do not require deep understanding. Second, there has been a lack of comprehensive scope-disambiguated corpora, resulting in the lack of work on extensive 1. Three words start with a capital letter. A deep understanding of this sentence, requires deciding whether each word in the se"
P13-1007,D09-1152,0,0.0935619,"ection ic. The outscoping relation 1d > 2 in (6) states that every line in the collection, denoted by 1c, starts with its own punctuation character. Similarly, 1d > 3 indicates that every line has its own next non-blank line. Figure 4(a) shows a DAG for the scoping in (6). In (7) we have a sentence containing a negation. In QuanText, negation chunks are labeled with an uppercase “N” followed by a number. 3.3 Feature selection Previous work has shown that the lexical item of quantifiers and syntactic clues (often extracted from phrase structure trees) are good at predicting quantifier scoping. Srinivasan and Yates (2009) use the semantics of the head noun in a quantified NP to predict the scoping. MA11 also find the lexical item of the head noun to be a good predictor. In this paper, we introduce a new set of syntactic features which we found very informative: the “type” dependency features of de Marneffe et al. (2006). Adopting this new set of features, we outperform MA11’s system by a large margin. Another point to mention here is that the features that are predictive of the relative scope of quantifiers are not necessarily as helpful when determining the scope of negation and vice versa. Therefore we do no"
P13-2074,P06-1055,0,0.0174569,"Missing"
P13-2074,2010.amta-papers.7,0,0.0261602,"get sentence. Wu et al. (2010) use a head-driven phrase structure grammar (HPSG) parser to add semantic representations to their translation rules. In this paper, we use semantic role labels to enrich a string-to-tree translation system, and show that this approach can increase the BLEU (Papineni et al., 2002) score of the translations. We extract GHKM-style (Galley et al., 2004) translation rules from training data where the target side has been parsed and labeled with semantic roles. Our general method of adding information to the syntactic tree is similar to the “tree grafting” approach of Baker et al. (2010), although we focus on predicate-argument structure, rather than named entity tags and modality. We modify the rule extraction procedure of Galley et al. (2004) to produce rules representing the overall predicateargument structure of each verb, allowing us to model alternations in the mapping from syntax to semantics of the type described by Levin (1993). We experiment with adding semantic role information to a string-to-tree machine translation system based on the rule extraction procedure of Galley et al. (2004). We compare methods based on augmenting the set of nonterminals by adding semant"
P13-2074,J07-2003,0,0.050664,"ST evaluation (2004, 2005, 2006). The development set and the test set only had sentences with less than 30 words for decoding speed. A set of nine standard features, which include globally normalized count of rules, lexical weighting (Koehn et al., 2003), length penalty, and number of rules used, was used for the experiments. In all of our experiments, we used the split-merge parsing method of Petrov et al. on the training corpus, and mapped the semantic roles from the original trees to the result of the split-merge parser. We used a syntax-based decoder with Earley parsing and cube pruning (Chiang, 2007). We used the Minimum Error Rate Training (Och, 2003) to tune the decoding parameters for the development set and tested the best weights that were found on the test set. We ran three sets of experiments: Baseline experiments, where we did not do any semantic role labeling prior to rule extraction and only extracted regular GHKM rules, experiments with our method of Section 2.4 (Method 1), and a set of experiments with our method of Section 2.5 (Method 2). Table 1 contains the numbers of the GHKM translation rules used by our three method. The rules were filtered by the development and the tes"
P13-2074,N04-1030,0,0.1274,"Missing"
P13-2074,P11-2072,1,0.875797,"after inserting semantic roles. “Lending” is the predicate, “everybody” is argument 0, and “a hand” is argument 1 for the predicate. S-8 NP-7-ARG1 2.2 String-to-Tree Translation We adopt the GHKM framework of Galley et al. (2004) using the parses produced by the splitmerge parser of Petrov et al. (2006) as the English trees. As shown by Wang et al. (2010), the refined nonterminals produced by the split-merge method can aid machine translation. Furthermore, in all of our experiments, we exclude unary rules during extraction by ensuring that no rules will have the same span in the source side (Chung et al., 2011). 1 NP-7-ARG1 victimized 1 受 by NP-7-ARG0 NP-7-ARG0 2 2 Figure 2: A complete semantic rule. these new labels for rule extraction. We only label the core arguments of each predicate, to make sure that the rules are not too specific to the training data. We attach each semantic label to the root of the subtree that it is labeling. Figure 1 shows an example target tree after attaching the semantic roles. We then run a GHKM rule extractor on the labeled training corpus and use the semantically enriched rules with a syntax-based decoder. 2.3 Using Semantic Role Labels in SMT To incorporate semantic"
P13-2074,D11-1012,0,0.051191,"Missing"
P13-2074,P05-1073,0,0.06099,"Missing"
P13-2074,J12-1005,0,0.05026,"Missing"
P13-2074,J10-2004,0,0.0469163,"Missing"
P13-2074,N04-1035,0,0.571645,"rst model defined features based on the context of a verbal predicate, to predict the target translation for that verb. Their second model predicted the reordering direction between a predicate and its arguments from the source to the target sentence. Wu et al. (2010) use a head-driven phrase structure grammar (HPSG) parser to add semantic representations to their translation rules. In this paper, we use semantic role labels to enrich a string-to-tree translation system, and show that this approach can increase the BLEU (Papineni et al., 2002) score of the translations. We extract GHKM-style (Galley et al., 2004) translation rules from training data where the target side has been parsed and labeled with semantic roles. Our general method of adding information to the syntactic tree is similar to the “tree grafting” approach of Baker et al. (2010), although we focus on predicate-argument structure, rather than named entity tags and modality. We modify the rule extraction procedure of Galley et al. (2004) to produce rules representing the overall predicateargument structure of each verb, allowing us to model alternations in the mapping from syntax to semantics of the type described by Levin (1993). We ex"
P13-2074,N09-2004,0,0.156466,"entire predicate-argument structure. Our results demonstrate that the second approach is effective in increasing the quality of translations. 1 Introduction Statistical machine translation (SMT) has made considerable advances in using syntactic properties of languages in both the training and the decoding of translation systems. Over the past few years, many researchers have started to realize that incorporating semantic features of languages can also be effective in increasing the quality of translations, as they can model relationships that often are not derivable from syntactic structures. Wu and Fung (2009) demonstrated the promise of using features based on semantic predicateargument structure in machine translation, using these feature to re-rank machine translation output. In general, re-ranking approaches are limited by the set of translation hypotheses, leading to a desire to incorporate semantic features into the translation model used during MT decoding. Liu and Gildea (2010) introduced two types of semantic features for tree-to-string machine translation. These features model the reorderings and deletions of the semantic roles in the source sentence during decoding. They showed that addi"
P13-2074,P00-1065,1,0.760912,"Missing"
P13-2074,P10-1034,0,0.0184512,"ester Rochester, NY 14627 Abstract trained on parse trees, they are constrained by the tree structures and are generally outperformed by string-to-tree systems. Xiong et al. (2012) integrated two discriminative feature-based models into a phrase-based SMT system, which used the semantic predicateargument structure of the source language. Their first model defined features based on the context of a verbal predicate, to predict the target translation for that verb. Their second model predicted the reordering direction between a predicate and its arguments from the source to the target sentence. Wu et al. (2010) use a head-driven phrase structure grammar (HPSG) parser to add semantic representations to their translation rules. In this paper, we use semantic role labels to enrich a string-to-tree translation system, and show that this approach can increase the BLEU (Papineni et al., 2002) score of the translations. We extract GHKM-style (Galley et al., 2004) translation rules from training data where the target side has been parsed and labeled with semantic roles. Our general method of adding information to the syntactic tree is similar to the “tree grafting” approach of Baker et al. (2010), although"
P13-2074,N03-1017,0,0.00416124,"1300589 1340314 1349070 1416491 1426159 more than 250K sentence pairs, which consist of 6.3M English words. The corpus was drawn from the newswire texts available from LDC.1 We used a 392-sentence development set with four references for parameter tuning, and a 428-sentence test set with four references for testing. They are drawn from the newswire portion of NIST evaluation (2004, 2005, 2006). The development set and the test set only had sentences with less than 30 words for decoding speed. A set of nine standard features, which include globally normalized count of rules, lexical weighting (Koehn et al., 2003), length penalty, and number of rules used, was used for the experiments. In all of our experiments, we used the split-merge parsing method of Petrov et al. on the training corpus, and mapped the semantic roles from the original trees to the result of the split-merge parser. We used a syntax-based decoder with Earley parsing and cube pruning (Chiang, 2007). We used the Minimum Error Rate Training (Och, 2003) to tune the decoding parameters for the development set and tested the best weights that were found on the test set. We ran three sets of experiments: Baseline experiments, where we did no"
P13-2074,P12-1095,0,0.122877,"Missing"
P13-2074,C10-1081,1,0.832807,"arted to realize that incorporating semantic features of languages can also be effective in increasing the quality of translations, as they can model relationships that often are not derivable from syntactic structures. Wu and Fung (2009) demonstrated the promise of using features based on semantic predicateargument structure in machine translation, using these feature to re-rank machine translation output. In general, re-ranking approaches are limited by the set of translation hypotheses, leading to a desire to incorporate semantic features into the translation model used during MT decoding. Liu and Gildea (2010) introduced two types of semantic features for tree-to-string machine translation. These features model the reorderings and deletions of the semantic roles in the source sentence during decoding. They showed that addition of these semantic features helps improve the quality of translations. Since tree-to-string systems are 2 Semantic Roles for String-to-Tree Translation 2.1 Semantic Role Labeling Semantic Role Labeling (SRL) is the task of identifying the arguments of the predicates in a sentence, and classifying them into different argument labels. Semantic roles can provide a level 419 Proce"
P13-2074,W04-3212,0,0.0958059,"Missing"
P13-2074,P03-1021,0,0.0183021,"d the test set only had sentences with less than 30 words for decoding speed. A set of nine standard features, which include globally normalized count of rules, lexical weighting (Koehn et al., 2003), length penalty, and number of rules used, was used for the experiments. In all of our experiments, we used the split-merge parsing method of Petrov et al. on the training corpus, and mapped the semantic roles from the original trees to the result of the split-merge parser. We used a syntax-based decoder with Earley parsing and cube pruning (Chiang, 2007). We used the Minimum Error Rate Training (Och, 2003) to tune the decoding parameters for the development set and tested the best weights that were found on the test set. We ran three sets of experiments: Baseline experiments, where we did not do any semantic role labeling prior to rule extraction and only extracted regular GHKM rules, experiments with our method of Section 2.4 (Method 1), and a set of experiments with our method of Section 2.5 (Method 2). Table 1 contains the numbers of the GHKM translation rules used by our three method. The rules were filtered by the development and the test to increase the decoding speed. The increases in th"
P13-2074,J05-1004,1,0.453869,"Missing"
P13-2074,P02-1040,0,0.0935663,"the semantic predicateargument structure of the source language. Their first model defined features based on the context of a verbal predicate, to predict the target translation for that verb. Their second model predicted the reordering direction between a predicate and its arguments from the source to the target sentence. Wu et al. (2010) use a head-driven phrase structure grammar (HPSG) parser to add semantic representations to their translation rules. In this paper, we use semantic role labels to enrich a string-to-tree translation system, and show that this approach can increase the BLEU (Papineni et al., 2002) score of the translations. We extract GHKM-style (Galley et al., 2004) translation rules from training data where the target side has been parsed and labeled with semantic roles. Our general method of adding information to the syntactic tree is similar to the “tree grafting” approach of Baker et al. (2010), although we focus on predicate-argument structure, rather than named entity tags and modality. We modify the rule extraction procedure of Galley et al. (2004) to produce rules representing the overall predicateargument structure of each verb, allowing us to model alternations in the mappin"
P13-2074,D08-1076,0,\N,Missing
P14-2039,N13-1020,1,\N,Missing
P17-2002,2006.amta-papers.8,0,0.0358799,"istinguish different non-terminal instances. Figure 2 shows an example derivation process for the sentence “the boy wants to go” given the rule set in Table 1. Given the start symbol S, which is first replaced with X1 , rule (c) is applied to generate “X2 to go” and its AMR counterpart. Then rule (b) is used to generate “X3 wants” and its AMR counterpart from X2 . Finally, rule (a) is used to generate “the boy” and its AMR counterpart from X3 . Our graph-to-string rules are inspired by synchronous grammars for machine translation (Wu, 1997; Yamada and Knight, 2002; Gildea, 2003; Chiang, 2005; Huang et al., 2006; Liu et al., 2006; Shen et al., 2008; Xie et al., 2011; Meng et al., 2013). Synchronous Node Replacement Grammar Grammar Definition A synchronous node replacement grammar (NRG) is a rewriting formalism: G = hN, Σ, ∆, P, Si, where N is a finite set of nonterminals, Σ and ∆ are finite sets of terminal symbols for the source and target sides, respectively. S ∈ N is the start symbol, and P is a finite set of productions. Each instance of P takes the form Xi → (hF, Ei, ∼), where Xi ∈ N is a nonterminal node, F is a rooted, connected AMR fragment with edge labels over Σ and node labels over N ∪ Σ,"
P17-2002,D15-1198,0,0.0124762,"s. Encoding many semantic phenomena into a graph structure, AMR is useful for NLP tasks such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015). AMR-to-text generation is challenging as function words and syntactic structures are abstracted away, making an AMR graph correspond to multiple realizations. Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016; Peng et al., 2017), there has been little work on AMR-to-text generation (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016). 7 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 7–13 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2002 ARG1 (c) AMR: #S# want-01 (root) #X2# #X1# go-01 String: {#S#} {#X1#} #X3# {#X2# to go} ARG1 (a) ARG0 ARG0 want-01 ARG1 (b) go-01 ARG0 {#X3#"
P17-2002,C12-1083,0,0.0608432,"Missing"
P17-2002,W13-2322,0,0.227385,"nto a sentence as a traveling salesman problem, using local features and a language model to rank candidate sentences. However, their method does not learn hierarchical structural correspondences between AMR graphs and strings. We propose to leverage the advantages of hierarchical rules without suffering from graph-to-tree errors by directly learning graph-to-string rules. As shown in Figure 1, we learn a synchronous node replacement grammar (NRG) from a corpus of aligned AMR and sentence pairs. At test time, we apply a graph transducer to collapse input Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism encoding the meaning of a sentence as a rooted, directed graph. AMR uses a graph to represent meaning, where nodes (such as “boy”, “want-01”) represent concepts, and edges (such as “ARG0”, “ARG1”) represent relations between concepts. Encoding many semantic phenomena into a graph structure, AMR is useful for NLP tasks such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015). AMR-to-text generation is challenging as function words and syn"
P17-2002,N03-1017,0,0.0322761,"rule list R (Lines 5 and 9), which will be further normalized to obtain the final induced rule set. 2.3 t where g denotes the input AMR, fi (·, ·) and wi represent a feature and the corresponding weight, respectively. The feature set that we adopt includes phrase-to-graph and graph-to-phrase translation probabilities and their corresponding lexicalized translation probabilities (section 3.1), language model score, word count, rule count, reordering model score (section 3.2) and moving distance (section 3.3). The language model score, word count and phrase count features are adopted from SMT (Koehn et al., 2003; Chiang, 2005). We perform bottom-up search to transduce input AMRs to surface strings. Each hypothesis contains the current AMR graph, translations of collapsed subgraphs, the feature vector and the current model score. Beam search is adopted, where hypotheses with the same number of collapsed edges and nodes are put into the same beam. Concept Rules and Glue Rules In addition to induced rules, we adopt concept rules (Song et al., 2016) and graph glue rules to ensure existence of derivations. For a concept rule, F is a single node in the input AMR graph, and E is a morphological string of th"
P17-2002,P05-1033,0,0.0721117,"ubscripts to distinguish different non-terminal instances. Figure 2 shows an example derivation process for the sentence “the boy wants to go” given the rule set in Table 1. Given the start symbol S, which is first replaced with X1 , rule (c) is applied to generate “X2 to go” and its AMR counterpart. Then rule (b) is used to generate “X3 wants” and its AMR counterpart from X2 . Finally, rule (a) is used to generate “the boy” and its AMR counterpart from X3 . Our graph-to-string rules are inspired by synchronous grammars for machine translation (Wu, 1997; Yamada and Knight, 2002; Gildea, 2003; Chiang, 2005; Huang et al., 2006; Liu et al., 2006; Shen et al., 2008; Xie et al., 2011; Meng et al., 2013). Synchronous Node Replacement Grammar Grammar Definition A synchronous node replacement grammar (NRG) is a rewriting formalism: G = hN, Σ, ∆, P, Si, where N is a finite set of nonterminals, Σ and ∆ are finite sets of terminal symbols for the source and target sides, respectively. S ∈ N is the start symbol, and P is a finite set of productions. Each instance of P takes the form Xi → (hF, Ei, ∼), where Xi ∈ N is a nonterminal node, F is a rooted, connected AMR fragment with edge labels over Σ and node"
P17-2002,W15-4502,0,0.0386307,"to collapse input Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism encoding the meaning of a sentence as a rooted, directed graph. AMR uses a graph to represent meaning, where nodes (such as “boy”, “want-01”) represent concepts, and edges (such as “ARG0”, “ARG1”) represent relations between concepts. Encoding many semantic phenomena into a graph structure, AMR is useful for NLP tasks such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015). AMR-to-text generation is challenging as function words and syntactic structures are abstracted away, making an AMR graph correspond to multiple realizations. Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016; Peng et al., 2017), there has been little work on AMR-to-text generation (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016). 7 Proceedings of the 55th Annual Meeting of the"
P17-2002,P06-1077,0,0.016854,"non-terminal instances. Figure 2 shows an example derivation process for the sentence “the boy wants to go” given the rule set in Table 1. Given the start symbol S, which is first replaced with X1 , rule (c) is applied to generate “X2 to go” and its AMR counterpart. Then rule (b) is used to generate “X3 wants” and its AMR counterpart from X2 . Finally, rule (a) is used to generate “the boy” and its AMR counterpart from X3 . Our graph-to-string rules are inspired by synchronous grammars for machine translation (Wu, 1997; Yamada and Knight, 2002; Gildea, 2003; Chiang, 2005; Huang et al., 2006; Liu et al., 2006; Shen et al., 2008; Xie et al., 2011; Meng et al., 2013). Synchronous Node Replacement Grammar Grammar Definition A synchronous node replacement grammar (NRG) is a rewriting formalism: G = hN, Σ, ∆, P, Si, where N is a finite set of nonterminals, Σ and ∆ are finite sets of terminal symbols for the source and target sides, respectively. S ∈ N is the start symbol, and P is a finite set of productions. Each instance of P takes the form Xi → (hF, Ei, ∼), where Xi ∈ N is a nonterminal node, F is a rooted, connected AMR fragment with edge labels over Σ and node labels over N ∪ Σ, E is a correspondi"
P17-2002,D13-1108,1,0.889096,"Missing"
P17-2002,N16-1087,0,0.26532,"versity of Technology and Design Abstract This paper addresses the task of AMR-totext generation by leveraging synchronous node replacement grammar. During training, graph-to-string rules are learned using a heuristic extraction algorithm. At test time, a graph transducer is applied to collapse input AMRs and generate output sentences. Evaluated on a standard benchmark, our method gives the state-of-the-art result. 1 #X3# #X3# ARG1 #X2# go-01 #X2# ARG0 ARG0 #X1# ARG0 boy want-01 ARG0 want-01 ARG0 ARG1 #X1# go-01 ARG1 go-01 the boy wants to go Figure 1: Graph-to-string derivation. Introduction Flanigan et al. (2016) transform a given AMR graph into a spanning tree, before translating it to a sentence using a tree-to-string transducer. Their method leverages existing machine translation techniques, capturing hierarchical correspondences between the spanning tree and the surface string. However, it suffers from error propagation since the output is constrained given a spanning tree due to the projective correspondence between them. Information loss in the graph-to-tree transformation step cannot be recovered. Song et al. (2016) directly generate sentences using graphfragment-to-string rules. They cast the"
P17-2002,P14-1134,0,0.0267127,"oy”, “want-01”) represent concepts, and edges (such as “ARG0”, “ARG1”) represent relations between concepts. Encoding many semantic phenomena into a graph structure, AMR is useful for NLP tasks such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015). AMR-to-text generation is challenging as function words and syntactic structures are abstracted away, making an AMR graph correspond to multiple realizations. Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016; Peng et al., 2017), there has been little work on AMR-to-text generation (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016). 7 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 7–13 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2002 ARG1 (c) AMR: #S# want-01 (root) #X2#"
P17-2002,P03-1021,0,0.0915657,"want-01 (root) #X2# #X1# go-01 String: {#S#} {#X1#} #X3# {#X2# to go} ARG1 (a) ARG0 ARG0 want-01 ARG1 (b) go-01 ARG0 {#X3# wants to go} ARG0 boy go-01 ARG0 {the boy wants to go} Figure 2: Example deduction procedure ID. (a) (b) (c) (d) F (b / boy) (w / want-01 :ARG0 (X / #X#)) (X / #X# :ARG1 (g / go-01 :ARG0 X)) (w / want-01 :ARG0 (b / boy)) E the boy 1 #X# wants 2 3 #X# to go 4 5 6 the boy wants 7 8 Table 1: Example rule set 9 10 11 AMR graphs and generate output strings according to the learned grammar. Our system makes use of a log-linear model with real-valued features, tuned using MERT (Och, 2003), and beam search decoding. It gives a BLEU score of 25.62 on LDC2015E86, which is the state-of-the-art on this dataset. 2 2.1 12 13 Data: training corpus C Result: rule instances R R ← []; for (Sent, AM R, ∼) in C do Rcur ← F RAGMENT E XTRACT(Sent,AM R,∼); for ri in Rcur do R.APPEND(ri ) ; for rj in Rcur /{ri } do if ri .C ONTAINS(rj ) then rij ← ri .COLLAPSE(rj ); R.APPEND(rij ) ; end end end end Algorithm 1: Rule extraction (2005), we use only one nonterminal X in addition to S, and use subscripts to distinguish different non-terminal instances. Figure 2 shows an example derivation process"
P17-2002,P03-1011,1,0.658507,"o S, and use subscripts to distinguish different non-terminal instances. Figure 2 shows an example derivation process for the sentence “the boy wants to go” given the rule set in Table 1. Given the start symbol S, which is first replaced with X1 , rule (c) is applied to generate “X2 to go” and its AMR counterpart. Then rule (b) is used to generate “X3 wants” and its AMR counterpart from X2 . Finally, rule (a) is used to generate “the boy” and its AMR counterpart from X3 . Our graph-to-string rules are inspired by synchronous grammars for machine translation (Wu, 1997; Yamada and Knight, 2002; Gildea, 2003; Chiang, 2005; Huang et al., 2006; Liu et al., 2006; Shen et al., 2008; Xie et al., 2011; Meng et al., 2013). Synchronous Node Replacement Grammar Grammar Definition A synchronous node replacement grammar (NRG) is a rewriting formalism: G = hN, Σ, ∆, P, Si, where N is a finite set of nonterminals, Σ and ∆ are finite sets of terminal symbols for the source and target sides, respectively. S ∈ N is the start symbol, and P is a finite set of productions. Each instance of P takes the form Xi → (hF, Ei, ∼), where Xi ∈ N is a nonterminal node, F is a rooted, connected AMR fragment with edge labels o"
P17-2002,P02-1040,0,0.101213,"connected by edge l. 3.3 Dev 21.12 23.00 25.24 16.75 23.99 23.48 25.09 Experiments Setup We use LDC2015E86 as our experimental dataset, which contains 16833 training, 1368 dev and 1371 test instances. Each instance contains a sentence, an AMR graph and the alignment generated by a heuristic aligner. Rules are extracted from the training data, and model parameters are tuned on the dev set. For tuning and testing, we filter out sentences with more than 30 words, resulting in 1103 dev instances and 1055 test instances. We train a 4-gram language model (LM) on gigaword (LDC2011T07), and use BLEU (Papineni et al., 2002) as the evaluation metric. MERT is used (Och, 2003) to tune model parameters on k-best outputs on the devset, where k is set 50. We investigate the effectiveness of rules and features by ablation tests: “NoInducedRule” does not adopt induced rules, “NoConceptRule” does not adopt concept rules, “NoMovingDistance” does not adopt the moving distance feature, and “NoReorderModel” disables the reordering model. Given an AMR graph, if NoConceptRule cannot produce a legal derivation, we concatenate 4.3 Grammar analysis We have shown the effectiveness of our synchronous node replacement grammar (SNRG)"
P17-2002,P16-1001,0,0.0154203,"aph structure, AMR is useful for NLP tasks such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015). AMR-to-text generation is challenging as function words and syntactic structures are abstracted away, making an AMR graph correspond to multiple realizations. Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016; Peng et al., 2017), there has been little work on AMR-to-text generation (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016). 7 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 7–13 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2002 ARG1 (c) AMR: #S# want-01 (root) #X2# #X1# go-01 String: {#S#} {#X1#} #X3# {#X2# to go} ARG1 (a) ARG0 ARG0 want-01 ARG1 (b) go-01 ARG0 {#X3# wants to go} ARG0 boy go-01 ARG0 {the boy want"
P17-2002,K15-1004,1,0.839012,"ges (such as “ARG0”, “ARG1”) represent relations between concepts. Encoding many semantic phenomena into a graph structure, AMR is useful for NLP tasks such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015). AMR-to-text generation is challenging as function words and syntactic structures are abstracted away, making an AMR graph correspond to multiple realizations. Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016; Peng et al., 2017), there has been little work on AMR-to-text generation (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016). 7 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 7–13 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2002 ARG1 (c) AMR: #S# want-01 (root) #X2# #X1# go-01 String: {#S#} {#X1#} #X3#"
P17-2002,P15-1143,0,0.0161628,"antic phenomena into a graph structure, AMR is useful for NLP tasks such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015). AMR-to-text generation is challenging as function words and syntactic structures are abstracted away, making an AMR graph correspond to multiple realizations. Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016; Peng et al., 2017), there has been little work on AMR-to-text generation (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016). 7 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 7–13 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2002 ARG1 (c) AMR: #S# want-01 (root) #X2# #X1# go-01 String: {#S#} {#X1#} #X3# {#X2# to go} ARG1 (a) ARG0 ARG0 want-01 ARG1 (b) go-01 ARG0 {#X3# wants to go} ARG0 boy go"
P17-2002,E17-1035,1,0.853477,"s such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015). AMR-to-text generation is challenging as function words and syntactic structures are abstracted away, making an AMR graph correspond to multiple realizations. Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016; Peng et al., 2017), there has been little work on AMR-to-text generation (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016). 7 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 7–13 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2002 ARG1 (c) AMR: #S# want-01 (root) #X2# #X1# go-01 String: {#S#} {#X1#} #X3# {#X2# to go} ARG1 (a) ARG0 ARG0 want-01 ARG1 (b) go-01 ARG0 {#X3# wants to go} ARG0 boy go-01 ARG0 {the boy wants to go} Figure 2: Example deduction pr"
P17-2002,W16-6603,0,0.200621,"), summarization (Takase et al., 2016) and event detection (Li et al., 2015). AMR-to-text generation is challenging as function words and syntactic structures are abstracted away, making an AMR graph correspond to multiple realizations. Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016; Peng et al., 2017), there has been little work on AMR-to-text generation (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016). 7 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 7–13 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2002 ARG1 (c) AMR: #S# want-01 (root) #X2# #X1# go-01 String: {#S#} {#X1#} #X3# {#X2# to go} ARG1 (a) ARG0 ARG0 want-01 ARG1 (b) go-01 ARG0 {#X3# wants to go} ARG0 boy go-01 ARG0 {the boy wants to go} Figure 2: Example deduction procedure ID. (a) (b) (c) (d) F (b / boy) (w / want-01 :ARG0 (X / #X#)) (X / #X# :ARG1 (g / go-01 :ARG0 X)) (w / want-01 :ARG0"
P17-2002,D16-1065,0,0.0438864,"useful for NLP tasks such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015). AMR-to-text generation is challenging as function words and syntactic structures are abstracted away, making an AMR graph correspond to multiple realizations. Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016; Peng et al., 2017), there has been little work on AMR-to-text generation (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016). 7 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 7–13 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2002 ARG1 (c) AMR: #S# want-01 (root) #X2# #X1# go-01 String: {#S#} {#X1#} #X3# {#X2# to go} ARG1 (a) ARG0 ARG0 want-01 ARG1 (b) go-01 ARG0 {#X3# wants to go} ARG0 boy go-01 ARG0 {the boy wants to go} Figure 2:"
P17-2002,D15-1136,0,0.0142351,"ons between concepts. Encoding many semantic phenomena into a graph structure, AMR is useful for NLP tasks such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015). AMR-to-text generation is challenging as function words and syntactic structures are abstracted away, making an AMR graph correspond to multiple realizations. Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016; Peng et al., 2017), there has been little work on AMR-to-text generation (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016). 7 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 7–13 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2002 ARG1 (c) AMR: #S# want-01 (root) #X2# #X1# go-01 String: {#S#} {#X1#} #X3# {#X2# to go} ARG1 (a) ARG0 ARG0 want-01 ARG1"
P17-2002,P08-1066,0,0.00990465,"ances. Figure 2 shows an example derivation process for the sentence “the boy wants to go” given the rule set in Table 1. Given the start symbol S, which is first replaced with X1 , rule (c) is applied to generate “X2 to go” and its AMR counterpart. Then rule (b) is used to generate “X3 wants” and its AMR counterpart from X2 . Finally, rule (a) is used to generate “the boy” and its AMR counterpart from X3 . Our graph-to-string rules are inspired by synchronous grammars for machine translation (Wu, 1997; Yamada and Knight, 2002; Gildea, 2003; Chiang, 2005; Huang et al., 2006; Liu et al., 2006; Shen et al., 2008; Xie et al., 2011; Meng et al., 2013). Synchronous Node Replacement Grammar Grammar Definition A synchronous node replacement grammar (NRG) is a rewriting formalism: G = hN, Σ, ∆, P, Si, where N is a finite set of nonterminals, Σ and ∆ are finite sets of terminal symbols for the source and target sides, respectively. S ∈ N is the start symbol, and P is a finite set of productions. Each instance of P takes the form Xi → (hF, Ei, ∼), where Xi ∈ N is a nonterminal node, F is a rooted, connected AMR fragment with edge labels over Σ and node labels over N ∪ Σ, E is a corresponding target string ov"
P17-2002,D16-1224,1,0.913448,"o-01 the boy wants to go Figure 1: Graph-to-string derivation. Introduction Flanigan et al. (2016) transform a given AMR graph into a spanning tree, before translating it to a sentence using a tree-to-string transducer. Their method leverages existing machine translation techniques, capturing hierarchical correspondences between the spanning tree and the surface string. However, it suffers from error propagation since the output is constrained given a spanning tree due to the projective correspondence between them. Information loss in the graph-to-tree transformation step cannot be recovered. Song et al. (2016) directly generate sentences using graphfragment-to-string rules. They cast the task of finding a sequence of disjoint rules to transduce an AMR graph into a sentence as a traveling salesman problem, using local features and a language model to rank candidate sentences. However, their method does not learn hierarchical structural correspondences between AMR graphs and strings. We propose to leverage the advantages of hierarchical rules without suffering from graph-to-tree errors by directly learning graph-to-string rules. As shown in Figure 1, we learn a synchronous node replacement grammar (N"
P17-2002,D16-1112,0,0.0090019,"At test time, we apply a graph transducer to collapse input Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism encoding the meaning of a sentence as a rooted, directed graph. AMR uses a graph to represent meaning, where nodes (such as “boy”, “want-01”) represent concepts, and edges (such as “ARG0”, “ARG1”) represent relations between concepts. Encoding many semantic phenomena into a graph structure, AMR is useful for NLP tasks such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015). AMR-to-text generation is challenging as function words and syntactic structures are abstracted away, making an AMR graph correspond to multiple realizations. Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016; Peng et al., 2017), there has been little work on AMR-to-text generation (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016). 7 Proceedi"
P17-2002,W15-3504,0,0.363273,"Missing"
P17-2002,N15-3006,0,0.0208597,", “ARG1”) represent relations between concepts. Encoding many semantic phenomena into a graph structure, AMR is useful for NLP tasks such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015). AMR-to-text generation is challenging as function words and syntactic structures are abstracted away, making an AMR graph correspond to multiple realizations. Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016; Peng et al., 2017), there has been little work on AMR-to-text generation (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016). 7 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 7–13 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2002 ARG1 (c) AMR: #S# want-01 (root) #X2# #X1# go-01 String: {#S#} {#X1#} #X3# {#X2# to go} ARG1 (a) ARG0"
P17-2002,N15-1040,0,0.0145351,"nt concepts, and edges (such as “ARG0”, “ARG1”) represent relations between concepts. Encoding many semantic phenomena into a graph structure, AMR is useful for NLP tasks such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015). AMR-to-text generation is challenging as function words and syntactic structures are abstracted away, making an AMR graph correspond to multiple realizations. Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016; Peng et al., 2017), there has been little work on AMR-to-text generation (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016). 7 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 7–13 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2002 ARG1 (c) AMR: #S# want-01 (root) #X2# #X1# go-01 String:"
P17-2002,J97-3002,0,0.0460913,"nly one nonterminal X in addition to S, and use subscripts to distinguish different non-terminal instances. Figure 2 shows an example derivation process for the sentence “the boy wants to go” given the rule set in Table 1. Given the start symbol S, which is first replaced with X1 , rule (c) is applied to generate “X2 to go” and its AMR counterpart. Then rule (b) is used to generate “X3 wants” and its AMR counterpart from X2 . Finally, rule (a) is used to generate “the boy” and its AMR counterpart from X3 . Our graph-to-string rules are inspired by synchronous grammars for machine translation (Wu, 1997; Yamada and Knight, 2002; Gildea, 2003; Chiang, 2005; Huang et al., 2006; Liu et al., 2006; Shen et al., 2008; Xie et al., 2011; Meng et al., 2013). Synchronous Node Replacement Grammar Grammar Definition A synchronous node replacement grammar (NRG) is a rewriting formalism: G = hN, Σ, ∆, P, Si, where N is a finite set of nonterminals, Σ and ∆ are finite sets of terminal symbols for the source and target sides, respectively. S ∈ N is the start symbol, and P is a finite set of productions. Each instance of P takes the form Xi → (hF, Ei, ∼), where Xi ∈ N is a nonterminal node, F is a rooted, co"
P17-2002,D11-1020,0,0.0167829,"ws an example derivation process for the sentence “the boy wants to go” given the rule set in Table 1. Given the start symbol S, which is first replaced with X1 , rule (c) is applied to generate “X2 to go” and its AMR counterpart. Then rule (b) is used to generate “X3 wants” and its AMR counterpart from X2 . Finally, rule (a) is used to generate “the boy” and its AMR counterpart from X3 . Our graph-to-string rules are inspired by synchronous grammars for machine translation (Wu, 1997; Yamada and Knight, 2002; Gildea, 2003; Chiang, 2005; Huang et al., 2006; Liu et al., 2006; Shen et al., 2008; Xie et al., 2011; Meng et al., 2013). Synchronous Node Replacement Grammar Grammar Definition A synchronous node replacement grammar (NRG) is a rewriting formalism: G = hN, Σ, ∆, P, Si, where N is a finite set of nonterminals, Σ and ∆ are finite sets of terminal symbols for the source and target sides, respectively. S ∈ N is the start symbol, and P is a finite set of productions. Each instance of P takes the form Xi → (hF, Ei, ∼), where Xi ∈ N is a nonterminal node, F is a rooted, connected AMR fragment with edge labels over Σ and node labels over N ∪ Σ, E is a corresponding target string over N ∪ ∆ and ∼ den"
P17-2002,P02-1039,0,\N,Missing
P17-2002,P13-2131,0,\N,Missing
P18-1150,W13-2322,0,0.343839,"le to model non-local semantic information, a sequence LSTM can lose information from the AMR graph structure, and thus faces challenges with large graphs, which result in long sequences. We introduce a neural graph-to-sequence model, using a novel LSTM structure for directly encoding graph-level semantics. On a standard benchmark, our model shows superior results to existing methods in the literature. 1 :ARG0 :ARG1 :ARG2 person :name name genius :op1 ""Ryan"" Figure 1: An example of AMR graph meaning “Ryan’s description of himself: a genius.” Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism that encodes the meaning of a sentence as a rooted, directed graph. Figure 1 shows an AMR graph in which the nodes (such as “describe-01” and “person”) represent the concepts, and edges (such as “:ARG0” and “:name”) represent the relations between concepts they connect. AMR has been proven helpful on other NLP tasks, such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015). The task of AMR-to-text generation is to produce a text with the"
P18-1150,D17-1209,0,0.0587751,"to provide research and institutes in the center . G2S: the agreement provides the staff of research centers and funding . G2S+CP: the agreement provides the staff of the research center and the funding . Table 3: Example system outputs. graphs by breadth-first traversal, and then use a phrase-based machine translation system2 to generate results by translating linearized sequences. Prior work using graph neural networks for NLP include the use graph convolutional networks (GCN) (Kipf and Welling, 2017) for semantic role labeling (Marcheggiani and Titov, 2017) and neural machine translation (Bastings et al., 2017). Both GCN and the graph LSTM update node states by exchanging information between neighboring nodes within each iteration. However, our graph state LSTM adopts gated operations for making updates, while GCN uses a linear transformation. Intuitively, the former has better learning power than the later. Another major difference is that our graph state LSTM keeps a cell vector for each node to remember all history. The contrast 1623 2 http://www.statmt.org/moses/ between our model with GCN is reminiscent of the contrast between RNN and CNN. We leave empirical comparison of their effectiveness to"
P18-1150,S16-1186,0,0.439676,"al, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015). The task of AMR-to-text generation is to produce a text with the same meaning as a given input AMR graph. The task is challenging as word tenses and function words are abstracted away when constructing AMR graphs from texts. The translation from AMR nodes to text phrases can be far from literal. For example, shown in Figure 1, “Ryan” is represented as “(p / person :name (n / name :op1 “Ryan”))”, and “description of” is represented as “(d / describe-01 :ARG1 )”. While initial work used statistical approaches (Flanigan et al., 2016b; Pourdamghani et al., 2016; Song et al., 2017; Lampouras and Vlachos, 2017; Mille et al., 2017; Gruzitis et al., 2017), recent research has demonstrated the success of deep learning, and in particular the sequence-to-sequence model (Sutskever et al., 2014), which has achieved the state-of-the-art results on AMR-to-text generation (Konstas et al., 2017). One limitation of sequence-to-sequence models, however, is that they require serialization of input AMR graphs, which adds to the challenge of representing graph structure information, especially when the graph is large. In particular, closel"
P18-1150,N16-1087,0,0.285942,"al, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015). The task of AMR-to-text generation is to produce a text with the same meaning as a given input AMR graph. The task is challenging as word tenses and function words are abstracted away when constructing AMR graphs from texts. The translation from AMR nodes to text phrases can be far from literal. For example, shown in Figure 1, “Ryan” is represented as “(p / person :name (n / name :op1 “Ryan”))”, and “description of” is represented as “(d / describe-01 :ARG1 )”. While initial work used statistical approaches (Flanigan et al., 2016b; Pourdamghani et al., 2016; Song et al., 2017; Lampouras and Vlachos, 2017; Mille et al., 2017; Gruzitis et al., 2017), recent research has demonstrated the success of deep learning, and in particular the sequence-to-sequence model (Sutskever et al., 2014), which has achieved the state-of-the-art results on AMR-to-text generation (Konstas et al., 2017). One limitation of sequence-to-sequence models, however, is that they require serialization of input AMR graphs, which adds to the challenge of representing graph structure information, especially when the graph is large. In particular, closel"
P18-1150,S17-2159,0,0.0529915,"n is to produce a text with the same meaning as a given input AMR graph. The task is challenging as word tenses and function words are abstracted away when constructing AMR graphs from texts. The translation from AMR nodes to text phrases can be far from literal. For example, shown in Figure 1, “Ryan” is represented as “(p / person :name (n / name :op1 “Ryan”))”, and “description of” is represented as “(d / describe-01 :ARG1 )”. While initial work used statistical approaches (Flanigan et al., 2016b; Pourdamghani et al., 2016; Song et al., 2017; Lampouras and Vlachos, 2017; Mille et al., 2017; Gruzitis et al., 2017), recent research has demonstrated the success of deep learning, and in particular the sequence-to-sequence model (Sutskever et al., 2014), which has achieved the state-of-the-art results on AMR-to-text generation (Konstas et al., 2017). One limitation of sequence-to-sequence models, however, is that they require serialization of input AMR graphs, which adds to the challenge of representing graph structure information, especially when the graph is large. In particular, closely-related nodes, such as parents, children and siblings can be far away after serialization. It can be difficult for a l"
P18-1150,P16-1154,0,0.593108,"tional Linguistics (Long Papers), pages 1616–1626 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics To capture non-local information, the encoder performs graph state transition by information exchange between connected nodes, with a graph state consisting of all node states. Multiple recurrent transition steps are taken so that information can propagate non-locally, and LSTM (Hochreiter and Schmidhuber, 1997) is used to avoid gradient diminishing and bursting in the recurrent process. The decoder is an attention-based LSTM model with a copy mechanism (Gu et al., 2016; Gulcehre et al., 2016), which helps copy sparse tokens (such as numbers and named entities) from the input. Trained on a standard dataset (LDC2015E86), our model surpasses a strong sequence-tosequence baseline by 2.3 BLEU points, demonstrating the advantage of graph-to-sequence models for AMR-to-text generation compared to sequence-to-sequence models. Our final model achieves a BLEU score of 23.3 on the test set, which is 1.3 points higher than the existing state of the art (Konstas et al., 2017) trained on the same dataset. When using gigaword sentences as additional training data, our mode"
P18-1150,P16-1014,0,0.452163,"s (Long Papers), pages 1616–1626 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics To capture non-local information, the encoder performs graph state transition by information exchange between connected nodes, with a graph state consisting of all node states. Multiple recurrent transition steps are taken so that information can propagate non-locally, and LSTM (Hochreiter and Schmidhuber, 1997) is used to avoid gradient diminishing and bursting in the recurrent process. The decoder is an attention-based LSTM model with a copy mechanism (Gu et al., 2016; Gulcehre et al., 2016), which helps copy sparse tokens (such as numbers and named entities) from the input. Trained on a standard dataset (LDC2015E86), our model surpasses a strong sequence-tosequence baseline by 2.3 BLEU points, demonstrating the advantage of graph-to-sequence models for AMR-to-text generation compared to sequence-to-sequence models. Our final model achieves a BLEU score of 23.3 on the test set, which is 1.3 points higher than the existing state of the art (Konstas et al., 2017) trained on the same dataset. When using gigaword sentences as additional training data, our model is consistently better"
P18-1150,C12-1083,0,0.109094,"Missing"
P18-1150,N03-1017,0,0.00861135,"We perform a similar experiment for the Seq2seq+copy baseline by only executing singledirectional LSTM for the encoder. We observe BLEU scores of 11.8 and 12.7 using only forward or backward LSTM, respectively. This is consistent with our graph model in that execution using only one direction leads to a huge performance drop. The contrast is also reminiscent of using the normal input versus the reversed input in neural machine translation (Sutskever et al., 2014). 5.5 model trained with the anonymized data. PBMT (Pourdamghani et al., 2016) adopts a phrase-based model for machine translation (Koehn et al., 2003) on the input of linearized AMR graph, SNRG (Song et al., 2017) uses synchronous node replacement grammar for parsing the AMR graph while generating the text, and Tree2Str (Flanigan et al., 2016b) converts AMR graphs into trees by splitting the re-entrances before using a tree transducer to generate the results. Graph2seq+charLSTM+copy achieves a BLEU score of 23.3, which is 1.3 points better than MSeq2seq+Anon trained on the same AMR corpus. In addition, our model without character LSTM is still 0.7 BLEU points higher than MSeq2seq+Anon. Note that MSeq2seq+Anon relies on anonymization, which"
P18-1150,P17-1014,0,0.218578,"s can be far from literal. For example, shown in Figure 1, “Ryan” is represented as “(p / person :name (n / name :op1 “Ryan”))”, and “description of” is represented as “(d / describe-01 :ARG1 )”. While initial work used statistical approaches (Flanigan et al., 2016b; Pourdamghani et al., 2016; Song et al., 2017; Lampouras and Vlachos, 2017; Mille et al., 2017; Gruzitis et al., 2017), recent research has demonstrated the success of deep learning, and in particular the sequence-to-sequence model (Sutskever et al., 2014), which has achieved the state-of-the-art results on AMR-to-text generation (Konstas et al., 2017). One limitation of sequence-to-sequence models, however, is that they require serialization of input AMR graphs, which adds to the challenge of representing graph structure information, especially when the graph is large. In particular, closely-related nodes, such as parents, children and siblings can be far away after serialization. It can be difficult for a linear recurrent neural network to automatically induce their original connections from bracketed string forms. To address this issue, we introduce a novel graph-to-sequence model, where a graph-state LSTM is used to encode AMR structure"
P18-1150,S17-2096,0,0.214613,"Missing"
P18-1150,W15-4502,0,0.0367213,"nius.” Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism that encodes the meaning of a sentence as a rooted, directed graph. Figure 1 shows an AMR graph in which the nodes (such as “describe-01” and “person”) represent the concepts, and edges (such as “:ARG0” and “:name”) represent the relations between concepts they connect. AMR has been proven helpful on other NLP tasks, such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015). The task of AMR-to-text generation is to produce a text with the same meaning as a given input AMR graph. The task is challenging as word tenses and function words are abstracted away when constructing AMR graphs from texts. The translation from AMR nodes to text phrases can be far from literal. For example, shown in Figure 1, “Ryan” is represented as “(p / person :name (n / name :op1 “Ryan”))”, and “description of” is represented as “(d / describe-01 :ARG1 )”. While initial work used statistical approaches (Flanigan et al., 2016b; Pourdamghani et al., 2016; Song et al., 2017; Lampouras and"
P18-1150,D17-1159,0,0.0610451,"rovide staff and funding for the research center . S2S: agreed to provide research and institutes in the center . G2S: the agreement provides the staff of research centers and funding . G2S+CP: the agreement provides the staff of the research center and the funding . Table 3: Example system outputs. graphs by breadth-first traversal, and then use a phrase-based machine translation system2 to generate results by translating linearized sequences. Prior work using graph neural networks for NLP include the use graph convolutional networks (GCN) (Kipf and Welling, 2017) for semantic role labeling (Marcheggiani and Titov, 2017) and neural machine translation (Bastings et al., 2017). Both GCN and the graph LSTM update node states by exchanging information between neighboring nodes within each iteration. However, our graph state LSTM adopts gated operations for making updates, while GCN uses a linear transformation. Intuitively, the former has better learning power than the later. Another major difference is that our graph state LSTM keeps a cell vector for each node to remember all history. The contrast 1623 2 http://www.statmt.org/moses/ between our model with GCN is reminiscent of the contrast between RNN and CNN."
P18-1150,P05-1012,0,0.0220157,"y incoming or outgoing edges are used. From the results, we can see that there is a huge drop when state transition is performed only with incoming or outgoing edges. Using edges of one direction, the node states only contain information of ancestors or descendants. On the other hand, node states contain information of ancestors, descendants, and siblings if edges of both directions are used. From the results, we can conclude that not only the ancestors and descendants, but also the siblings are important for modeling the AMR graphs. This is similar to observations on syntactic parsing tasks (McDonald et al., 2005), where sibling features are adopted. We perform a similar experiment for the Seq2seq+copy baseline by only executing singledirectional LSTM for the encoder. We observe BLEU scores of 11.8 and 12.7 using only forward or backward LSTM, respectively. This is consistent with our graph model in that execution using only one direction leads to a huge performance drop. The contrast is also reminiscent of using the normal input versus the reversed input in neural machine translation (Sutskever et al., 2014). 5.5 model trained with the anonymized data. PBMT (Pourdamghani et al., 2016) adopts a phrase-"
P18-1150,S17-2158,0,0.0361727,"MR-to-text generation is to produce a text with the same meaning as a given input AMR graph. The task is challenging as word tenses and function words are abstracted away when constructing AMR graphs from texts. The translation from AMR nodes to text phrases can be far from literal. For example, shown in Figure 1, “Ryan” is represented as “(p / person :name (n / name :op1 “Ryan”))”, and “description of” is represented as “(d / describe-01 :ARG1 )”. While initial work used statistical approaches (Flanigan et al., 2016b; Pourdamghani et al., 2016; Song et al., 2017; Lampouras and Vlachos, 2017; Mille et al., 2017; Gruzitis et al., 2017), recent research has demonstrated the success of deep learning, and in particular the sequence-to-sequence model (Sutskever et al., 2014), which has achieved the state-of-the-art results on AMR-to-text generation (Konstas et al., 2017). One limitation of sequence-to-sequence models, however, is that they require serialization of input AMR graphs, which adds to the challenge of representing graph structure information, especially when the graph is large. In particular, closely-related nodes, such as parents, children and siblings can be far away after serialization. It"
P18-1150,P02-1040,0,0.103679,"onstas et al. (2017) only works on the anonymized data. For training on both sampled data and LDC2015E86, we also follow the method of Konstas et al. (2017), which is fine-tuning the model on the AMR corpus after every epoch of pretraining on the gigaword data. 5.2 Settings We extract a vocabulary from the training set, which is shared by both the encoder and the decoder. The word embeddings are initialized from Glove pretrained word embeddings (Pennington et al., 2014) on Common Crawl, and are not updated during training. Following existing work, we evaluate the results with the BLEU metric (Papineni et al., 2002). For model hyperparameters, we set the graph state transition number as 9 according to development experiments. Each node takes information from at most 10 neighbors. The hidden vector sizes for both encoder and decoder are set to 300 (They are set to 600 for experiments using largescale automatic data). Both character embeddings and hidden layer sizes for character LSTMs are set 100, and at most 20 characters are taken for each graph node or linearized token. Data We use a standard AMR corpus (LDC2015E86) as our experimental dataset, which contains 16,833 instances for training, 1368 for dev"
P18-1150,Q17-1008,0,0.0361108,"te LSTM adopts gated operations for making updates, while GCN uses a linear transformation. Intuitively, the former has better learning power than the later. Another major difference is that our graph state LSTM keeps a cell vector for each node to remember all history. The contrast 1623 2 http://www.statmt.org/moses/ between our model with GCN is reminiscent of the contrast between RNN and CNN. We leave empirical comparison of their effectiveness to future work. In this work our main goal is to show that graph LSTM encoding of AMR is superior compared with sequence LSTM. Closest to our work, Peng et al. (2017) modeled syntactic and discourse structures using DAG LSTM, which can be viewed as extensions to tree LSTMs (Tai et al., 2015). The state update follows the sentence order for each node, and has sequential nature. Our state update is in parallel. In addition, Peng et al. (2017) split input graphs into separate DAGs before their method can be used. To our knowledge, we are the first to apply an LSTM structure to encode AMR graphs. The recurrent information exchange mechanism in our state transition process is remotely related to the idea of loopy belief propagation (LBP) (Murphy et al., 1999)."
P18-1150,D14-1162,0,0.0936009,"19.9 20.6 20.4 22.2 22.1 22.8 Time 35.4s 37.4s 39.7s 11.2s 11.1s 9.2s 16.3s Table 1: D EV BLEU scores and decoding times. AMRs, as the AMR parser of Konstas et al. (2017) only works on the anonymized data. For training on both sampled data and LDC2015E86, we also follow the method of Konstas et al. (2017), which is fine-tuning the model on the AMR corpus after every epoch of pretraining on the gigaword data. 5.2 Settings We extract a vocabulary from the training set, which is shared by both the encoder and the decoder. The word embeddings are initialized from Glove pretrained word embeddings (Pennington et al., 2014) on Common Crawl, and are not updated during training. Following existing work, we evaluate the results with the BLEU metric (Papineni et al., 2002). For model hyperparameters, we set the graph state transition number as 9 according to development experiments. Each node takes information from at most 10 neighbors. The hidden vector sizes for both encoder and decoder are set to 300 (They are set to 600 for experiments using largescale automatic data). Both character embeddings and hidden layer sizes for character LSTMs are set 100, and at most 20 characters are taken for each graph node or line"
P18-1150,W16-6603,0,0.403474,"(Takase et al., 2016) and event detection (Li et al., 2015). The task of AMR-to-text generation is to produce a text with the same meaning as a given input AMR graph. The task is challenging as word tenses and function words are abstracted away when constructing AMR graphs from texts. The translation from AMR nodes to text phrases can be far from literal. For example, shown in Figure 1, “Ryan” is represented as “(p / person :name (n / name :op1 “Ryan”))”, and “description of” is represented as “(d / describe-01 :ARG1 )”. While initial work used statistical approaches (Flanigan et al., 2016b; Pourdamghani et al., 2016; Song et al., 2017; Lampouras and Vlachos, 2017; Mille et al., 2017; Gruzitis et al., 2017), recent research has demonstrated the success of deep learning, and in particular the sequence-to-sequence model (Sutskever et al., 2014), which has achieved the state-of-the-art results on AMR-to-text generation (Konstas et al., 2017). One limitation of sequence-to-sequence models, however, is that they require serialization of input AMR graphs, which adds to the challenge of representing graph structure information, especially when the graph is large. In particular, closely-related nodes, such as par"
P18-1150,P17-1099,0,0.0409756,"tely related to the idea of loopy belief propagation (LBP) (Murphy et al., 1999). However, there are two major differences. First, messages between LSTM states are gated neural node values, rather than probabilities in LBP. Second, while the goal of LBP is to estimate marginal probabilities, the goal of information exchange between graph states in our LSTM is to find neural representation features, which are directly optimized by a task objective. In addition to NMT (Gulcehre et al., 2016), the copy mechanism has been shown effective on tasks such as dialogue (Gu et al., 2016), summarization (See et al., 2017) and question generation (Song et al., 2018). We investigate the copy mechanism on AMR-to-text generation. 7 Conclusion We introduced a novel graph-to-sequence model for AMR-to-text generation. Compared to sequence-to-sequence models, which require linearization of AMR before decoding, a graph LSTM is leveraged to directly model full AMR structure. Allowing high parallelization, the graph encoder is more efficient than the sequence encoder. In our experiments, the graph model outperforms a strong sequence-to-sequence model, achieving the best performance. Acknowledgments We thank the anonymize"
P18-1150,P17-2002,1,0.902838,"event detection (Li et al., 2015). The task of AMR-to-text generation is to produce a text with the same meaning as a given input AMR graph. The task is challenging as word tenses and function words are abstracted away when constructing AMR graphs from texts. The translation from AMR nodes to text phrases can be far from literal. For example, shown in Figure 1, “Ryan” is represented as “(p / person :name (n / name :op1 “Ryan”))”, and “description of” is represented as “(d / describe-01 :ARG1 )”. While initial work used statistical approaches (Flanigan et al., 2016b; Pourdamghani et al., 2016; Song et al., 2017; Lampouras and Vlachos, 2017; Mille et al., 2017; Gruzitis et al., 2017), recent research has demonstrated the success of deep learning, and in particular the sequence-to-sequence model (Sutskever et al., 2014), which has achieved the state-of-the-art results on AMR-to-text generation (Konstas et al., 2017). One limitation of sequence-to-sequence models, however, is that they require serialization of input AMR graphs, which adds to the challenge of representing graph structure information, especially when the graph is large. In particular, closely-related nodes, such as parents, children and"
P18-1150,N18-2090,1,0.825928,"opagation (LBP) (Murphy et al., 1999). However, there are two major differences. First, messages between LSTM states are gated neural node values, rather than probabilities in LBP. Second, while the goal of LBP is to estimate marginal probabilities, the goal of information exchange between graph states in our LSTM is to find neural representation features, which are directly optimized by a task objective. In addition to NMT (Gulcehre et al., 2016), the copy mechanism has been shown effective on tasks such as dialogue (Gu et al., 2016), summarization (See et al., 2017) and question generation (Song et al., 2018). We investigate the copy mechanism on AMR-to-text generation. 7 Conclusion We introduced a novel graph-to-sequence model for AMR-to-text generation. Compared to sequence-to-sequence models, which require linearization of AMR before decoding, a graph LSTM is leveraged to directly model full AMR structure. Allowing high parallelization, the graph encoder is more efficient than the sequence encoder. In our experiments, the graph model outperforms a strong sequence-to-sequence model, achieving the best performance. Acknowledgments We thank the anonymized reviewers for the insightful comments, and"
P18-1150,P15-1150,0,0.208715,"Missing"
P18-1150,D16-1112,0,0.0316661,"aning “Ryan’s description of himself: a genius.” Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism that encodes the meaning of a sentence as a rooted, directed graph. Figure 1 shows an AMR graph in which the nodes (such as “describe-01” and “person”) represent the concepts, and edges (such as “:ARG0” and “:name”) represent the relations between concepts they connect. AMR has been proven helpful on other NLP tasks, such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015). The task of AMR-to-text generation is to produce a text with the same meaning as a given input AMR graph. The task is challenging as word tenses and function words are abstracted away when constructing AMR graphs from texts. The translation from AMR nodes to text phrases can be far from literal. For example, shown in Figure 1, “Ryan” is represented as “(p / person :name (n / name :op1 “Ryan”))”, and “description of” is represented as “(d / describe-01 :ARG1 )”. While initial work used statistical approaches (Flanigan et al., 2016b; Pourdamghani et al., 2"
P18-1150,W15-3504,0,0.0515809,"Missing"
P18-1150,P16-1008,0,0.0232013,". ; aN ] (4) where N is the number of input tokens. The decoder yields an output sequence w1 , w2 , . . . , wM by calculating a sequence of hidden states s1 , s2 . . . , sM recurrently. While generating the t-th word, the decoder considers five factors: (1) the attention memory A; (2) the previous hidden state of the LSTM model st 1 ; (3) the embedding of the current input (previously generated word) et ; (4) the previous context vector µt 1 , which is calculated with attention from A; and (5) the previous coverage vector γt 1 , which is the accumulation of all attention distributions so far (Tu et al., 2016). When t = 1, we initialize µ0 and γ0 as zero vectors, set e1 to the embedding of the start token “<s>”, and s0 as the average of all encoder states. For each time-step t, the decoder feeds the concatenation of the embedding of the current input et and the previous context vector µt 1 into the 1617 In order to capture non-local interaction between nodes, we allow information exchange between nodes through a sequence of state transitions, leading to a sequence of states g0 , g1 , . . . , gt , . . . , where gt = {hjt }|vj ∈V . The initial state g0 consists of a set of initial node states hj0 = h"
P18-1171,P17-1183,0,0.0238709,"ossible output. The transition system can also provide better local context information than the linearized graph representation, which is important for neural AMR parsing given the limited amount of data. More specifically, we use bi-LSTM to encode two levels of input information for AMR parsing: word level and concept level, each refined with more general category information such as lemmatization, POS tags, and concept categories. We also want to make better use of the complex transition system to address the data sparsity issue for neural AMR parsing. We extend the hard attention model of Aharoni and Goldberg (2017), which deals with the nearly-monotonic alignment in the morphological inflection task, to the more general scenario of transition systems where the input buffer is processed from left-to-right. When we process the buffer in this ordered manner, the sequence of target transition actions are also strictly aligned left-to-right according to the input order. On the decoder side, we augment the prediction of output action with embedding features from the current transition state. Our experiments show that encoding information from the transition state significantly improves sequenceto-sequence mod"
P18-1171,D15-1198,0,0.11179,"which the nodes represent the AMR concepts and the edges represent the relations between the concepts. AMR has been used in various applications such as text summarization (Liu et al., 2015), sentence compression (Takase et al., 2016), and event extraction (Huang et al., 2016). 1 The implementation of our parser is available at https://github.com/xiaochang13/CacheTransition-Seq2seq The task of AMR graph parsing is to map natural language strings to AMR semantic graphs. Different parsers have been developed to tackle this problem (Flanigan et al., 2014; Wang et al., 2015b,a; Peng et al., 2015; Artzi et al., 2015; Pust et al., 2015; van Noord and Bos, 2017). On the other hand, due to the limited amount of labeled data and the large output vocabulary, the sequence-to-sequence model has not been very successful on AMR parsing. Peng et al. (2017) propose a linearization approach that encodes labeled graphs as sequences. To address the data sparsity issue, low-frequency entities and tokens are mapped to special categories to reduce the vocabulary size for the neural models. Konstas et al. (2017) use self-training on a huge amount of unlabeled text to lower the out-of-vocabulary rate. However, the final pe"
P18-1171,D17-1130,0,0.232718,"step by step, as shown by the system 1842 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1842–1852 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics of Wang and Xue (2017), which is currently the top performing system. This raises the question of whether the advantages of neural and transitionbased system can be combined, as for example with the syntactic parser of Dyer et al. (2015), who use stack LSTMs to capture action history information in the transition state of the transition system. Ballesteros and Al-Onaizan (2017) apply stack-LSTM to transition-based AMR parsing and achieve competitive results, which shows that local transition state information is important for predicting transition actions. Instead of linearizing the target AMR graph to a sequence structure, Buys and Blunsom (2017) propose a sequence-to-action-sequence approach where the reference AMR graph is replaced with an action derivation sequence by running a deterministic oracle algorithm on the training sentence, AMR graph pairs. They use a separate alignment probability to explicitly model the hard alignment from graph nodes to sentence tok"
P18-1171,W13-2322,0,0.138186,"ch decoder state. We present a monotonic hard attention model for the transition framework to handle the strictly left-to-right alignment between each transition state and the current buffer input focus. We evaluate our neural transition model on the AMR parsing task, and our parser outperforms other sequence-to-sequence approaches and achieves competitive results in comparison with the best-performing models.1 1 ARG1 ARG0 go-01 person ARG0 name name op1 “John” Figure 1: An example of AMR graph representing the meaning of: “John wants to go” Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism where the meaning of a sentence is encoded as a rooted, directed graph. Figure 1 shows an example of an AMR in which the nodes represent the AMR concepts and the edges represent the relations between the concepts. AMR has been used in various applications such as text summarization (Liu et al., 2015), sentence compression (Takase et al., 2016), and event extraction (Huang et al., 2016). 1 The implementation of our parser is available at https://github.com/xiaochang13/CacheTransition-Seq2seq The task of AMR graph parsing is to map natural language strings to AMR semanti"
P18-1171,P17-1112,0,0.681502,"currently the top performing system. This raises the question of whether the advantages of neural and transitionbased system can be combined, as for example with the syntactic parser of Dyer et al. (2015), who use stack LSTMs to capture action history information in the transition state of the transition system. Ballesteros and Al-Onaizan (2017) apply stack-LSTM to transition-based AMR parsing and achieve competitive results, which shows that local transition state information is important for predicting transition actions. Instead of linearizing the target AMR graph to a sequence structure, Buys and Blunsom (2017) propose a sequence-to-action-sequence approach where the reference AMR graph is replaced with an action derivation sequence by running a deterministic oracle algorithm on the training sentence, AMR graph pairs. They use a separate alignment probability to explicitly model the hard alignment from graph nodes to sentence tokens in the buffer. Gildea et al. (2018) propose a special transition framework called a cache transition system to generate the set of semantic graphs. They adapt the stack-based parsing system by adding a working set, which they refer to as a cache, to the traditional stack"
P18-1171,P13-2131,0,0.0629132,"subgraph each category is collapsed from, and map each category to its original subgraph representation. We also use heuristic rules to generate the target-side AMR subgraph representation for NE, DATE, and NUMBER based on the source side tokens. 5 Experiments We evaluate our system on the released dataset (LDC2015E86) for SemEval 2016 task 8 on meaning representation parsing (May, 2016). The dataset contains 16,833 training, 1,368 development, and 1,371 test sentences which mainly cover domains like newswire, discussion forum, etc. All parsing results are measured by Smatch (version 2.0.2) (Cai and Knight, 2013). 5.1 Experiment Settings We categorize the training data using the automatic alignment and dump a template for date entities and frequent phrases from the multiple to one alignment. We also generate an alignment table from tokens or phrases to their candidate targetside subgraphs. For the dev and test data, we first extract the named entities using the Illinois Named Entity Tagger (Ratinov and Roth, 2009) and extract date entities by matching spans with the date template. We further categorize the dataset with the categories we have defined. After categorization, we use Stanford CoreNLP (Mann"
P18-1171,P17-1193,0,0.0167447,", which encodes both word and concept sequence information, can accurately predict the reentrancy through the two arc decisions shown in Figure 5. When desire-01 and live01 are shifted into the cache respectively, the transition system makes a left-going arc from each of them to the same concept i, thus creating the reentrancy as desired. 6 we are focused on AMR parsing in this paper, in future work our cache transition system and the presented sequence-to-sequence models can be potentially applied to other semantic graph parsing tasks (Oepen et al., 2015; Du et al., 2015; Zhang et al., 2016; Cao et al., 2017). Acknowledgments We gratefully acknowledge the assistance of Hao Zhang from Google, New York for the monotonic hard attention idea and the helpful comments and suggestions. Conclusion References In this paper, we have presented a sequence-toaction-sequence approach for cache transition systems and applied it to AMR parsing. To address the data sparsity issue for neural AMR parsing, we show that the transition state features are very helpful in constraining the possible output and improving the performance of sequence-to-sequence models. We also show that the monotonic hard attention model can"
P18-1171,E17-1051,1,0.937526,"hard attention only attends to one position of the input, it performs slightly better than the soft attention model, while the time complexity is lower. Impact of Different Cache Sizes The cache size of the transition system can be optimized as a trade-off between coverage of AMR graphs and the prediction accuracy. While larger cache size increases the coverage of AMR graphs, it complicates the prediction procedure with more cache decisions to make. From Table 3 we can see that 1848 System Soft Soft+feats Hard+feats P 0.55 0.69 0.70 R 0.51 0.63 0.64 F 0.53 0.66 0.67 System Peng et al. (2018) Damonte et al. (2017) JAMR Ours Table 2: Impact of various components for the sequence-to-sequence model (dev). Cache Size 4 5 6 P 0.69 0.70 0.69 R 0.63 0.64 0.64 F 0.66 0.67 0.66 Table 3: Impact of cache size for the sequenceto-sequence model, hard attention (dev). the hard attention model performs best with cache size 5. The soft attention model also achieves best performance with the same cache size. Comparison with other Parsers Table 4 shows the comparison with other AMR parsers. The first three systems are some competitive neural models. We can see that our parser significantly outperforms the sequence-to-ac"
P18-1171,S15-2154,0,0.0200108,"t if they are distant. Our classifier, which encodes both word and concept sequence information, can accurately predict the reentrancy through the two arc decisions shown in Figure 5. When desire-01 and live01 are shifted into the cache respectively, the transition system makes a left-going arc from each of them to the same concept i, thus creating the reentrancy as desired. 6 we are focused on AMR parsing in this paper, in future work our cache transition system and the presented sequence-to-sequence models can be potentially applied to other semantic graph parsing tasks (Oepen et al., 2015; Du et al., 2015; Zhang et al., 2016; Cao et al., 2017). Acknowledgments We gratefully acknowledge the assistance of Hao Zhang from Google, New York for the monotonic hard attention idea and the helpful comments and suggestions. Conclusion References In this paper, we have presented a sequence-toaction-sequence approach for cache transition systems and applied it to AMR parsing. To address the data sparsity issue for neural AMR parsing, we show that the transition state features are very helpful in constraining the possible output and improving the performance of sequence-to-sequence models. We also show that"
P18-1171,P15-1033,0,0.0292887,"parsers model graph structures directly. One approach to modeling graph structures is to use a transition system to build graphs step by step, as shown by the system 1842 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1842–1852 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics of Wang and Xue (2017), which is currently the top performing system. This raises the question of whether the advantages of neural and transitionbased system can be combined, as for example with the syntactic parser of Dyer et al. (2015), who use stack LSTMs to capture action history information in the transition state of the transition system. Ballesteros and Al-Onaizan (2017) apply stack-LSTM to transition-based AMR parsing and achieve competitive results, which shows that local transition state information is important for predicting transition actions. Instead of linearizing the target AMR graph to a sequence structure, Buys and Blunsom (2017) propose a sequence-to-action-sequence approach where the reference AMR graph is replaced with an action derivation sequence by running a deterministic oracle algorithm on the traini"
P18-1171,S16-1186,0,0.307711,"numbers (NUMBER) and phrases (PHRASE). The phrases are extracted based on the multiple-to-one alignment in the training data. One example phrase is more than which aligns to a single concept more-than. We first collapse spans and subgraphs into these categories based on the alignment from the JAMR aligner (Flanigan et al., 2014), which greedily aligns a span of words to AMR subgraphs using a set of heuristics. This categorization procedure enables the parser to capture mappings from continuous spans on the sentence side to connected subgraphs on the AMR side. We use the semi-Markov model from Flanigan et al. (2016) as the concept identifier, which jointly segments the sentence into a sequence of spans and maps each span to a subgraph. During decoding, our output has categories, and we need to map 4 For example, verbalization of “teacher” as “(person :ARG0-of teach-01)”, or “minister” as “(person :ARG0-of (have-org-role-91 :ARG2 minister))”. 1847 Peng et al. (2018) Soft+feats Hard+feats ShiftOrPop 0.87 0.93 0.94 PushIndex 0.87 0.84 0.85 ArcBinary 0.83 0.91 0.93 ArcLabel 0.81 0.75 0.77 Table 1: Performance breakdown of each transition phase. each category to the corresponding AMR concept or subgraph. We s"
P18-1171,P14-1134,0,0.551749,"rooted, directed graph. Figure 1 shows an example of an AMR in which the nodes represent the AMR concepts and the edges represent the relations between the concepts. AMR has been used in various applications such as text summarization (Liu et al., 2015), sentence compression (Takase et al., 2016), and event extraction (Huang et al., 2016). 1 The implementation of our parser is available at https://github.com/xiaochang13/CacheTransition-Seq2seq The task of AMR graph parsing is to map natural language strings to AMR semantic graphs. Different parsers have been developed to tackle this problem (Flanigan et al., 2014; Wang et al., 2015b,a; Peng et al., 2015; Artzi et al., 2015; Pust et al., 2015; van Noord and Bos, 2017). On the other hand, due to the limited amount of labeled data and the large output vocabulary, the sequence-to-sequence model has not been very successful on AMR parsing. Peng et al. (2017) propose a linearization approach that encodes labeled graphs as sequences. To address the data sparsity issue, low-frequency entities and tokens are mapped to special categories to reduce the vocabulary size for the neural models. Konstas et al. (2017) use self-training on a huge amount of unlabeled te"
P18-1171,J18-1004,1,0.920933,"o transition-based AMR parsing and achieve competitive results, which shows that local transition state information is important for predicting transition actions. Instead of linearizing the target AMR graph to a sequence structure, Buys and Blunsom (2017) propose a sequence-to-action-sequence approach where the reference AMR graph is replaced with an action derivation sequence by running a deterministic oracle algorithm on the training sentence, AMR graph pairs. They use a separate alignment probability to explicitly model the hard alignment from graph nodes to sentence tokens in the buffer. Gildea et al. (2018) propose a special transition framework called a cache transition system to generate the set of semantic graphs. They adapt the stack-based parsing system by adding a working set, which they refer to as a cache, to the traditional stack and buffer. Peng et al. (2018) apply the cache transition system to AMR parsing and design refined action phases, each modeled with a separate feedforward neural network, to deal with some practical implementation issues. In this paper, we propose a sequence-to-actionsequence approach for AMR parsing with cache transition systems. We want to take advantage of t"
P18-1171,P16-1025,0,0.0429727,"ARG1 ARG0 go-01 person ARG0 name name op1 “John” Figure 1: An example of AMR graph representing the meaning of: “John wants to go” Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism where the meaning of a sentence is encoded as a rooted, directed graph. Figure 1 shows an example of an AMR in which the nodes represent the AMR concepts and the edges represent the relations between the concepts. AMR has been used in various applications such as text summarization (Liu et al., 2015), sentence compression (Takase et al., 2016), and event extraction (Huang et al., 2016). 1 The implementation of our parser is available at https://github.com/xiaochang13/CacheTransition-Seq2seq The task of AMR graph parsing is to map natural language strings to AMR semantic graphs. Different parsers have been developed to tackle this problem (Flanigan et al., 2014; Wang et al., 2015b,a; Peng et al., 2015; Artzi et al., 2015; Pust et al., 2015; van Noord and Bos, 2017). On the other hand, due to the limited amount of labeled data and the large output vocabulary, the sequence-to-sequence model has not been very successful on AMR parsing. Peng et al. (2017) propose a linearization"
P18-1171,P17-1014,0,0.290237,"arsers have been developed to tackle this problem (Flanigan et al., 2014; Wang et al., 2015b,a; Peng et al., 2015; Artzi et al., 2015; Pust et al., 2015; van Noord and Bos, 2017). On the other hand, due to the limited amount of labeled data and the large output vocabulary, the sequence-to-sequence model has not been very successful on AMR parsing. Peng et al. (2017) propose a linearization approach that encodes labeled graphs as sequences. To address the data sparsity issue, low-frequency entities and tokens are mapped to special categories to reduce the vocabulary size for the neural models. Konstas et al. (2017) use self-training on a huge amount of unlabeled text to lower the out-of-vocabulary rate. However, the final performance still falls behind the best-performing models. The best performing AMR parsers model graph structures directly. One approach to modeling graph structures is to use a transition system to build graphs step by step, as shown by the system 1842 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1842–1852 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics of Wang and Xue (2017), wh"
P18-1171,N15-1114,0,0.0368503,"s and achieves competitive results in comparison with the best-performing models.1 1 ARG1 ARG0 go-01 person ARG0 name name op1 “John” Figure 1: An example of AMR graph representing the meaning of: “John wants to go” Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism where the meaning of a sentence is encoded as a rooted, directed graph. Figure 1 shows an example of an AMR in which the nodes represent the AMR concepts and the edges represent the relations between the concepts. AMR has been used in various applications such as text summarization (Liu et al., 2015), sentence compression (Takase et al., 2016), and event extraction (Huang et al., 2016). 1 The implementation of our parser is available at https://github.com/xiaochang13/CacheTransition-Seq2seq The task of AMR graph parsing is to map natural language strings to AMR semantic graphs. Different parsers have been developed to tackle this problem (Flanigan et al., 2014; Wang et al., 2015b,a; Peng et al., 2015; Artzi et al., 2015; Pust et al., 2015; van Noord and Bos, 2017). On the other hand, due to the limited amount of labeled data and the large output vocabulary, the sequence-to-sequence model"
P18-1171,P14-5010,0,0.00734709,"Missing"
P18-1171,S16-1166,0,0.0868203,"0.84 0.85 ArcBinary 0.83 0.91 0.93 ArcLabel 0.81 0.75 0.77 Table 1: Performance breakdown of each transition phase. each category to the corresponding AMR concept or subgraph. We save a table Q which shows the original subgraph each category is collapsed from, and map each category to its original subgraph representation. We also use heuristic rules to generate the target-side AMR subgraph representation for NE, DATE, and NUMBER based on the source side tokens. 5 Experiments We evaluate our system on the released dataset (LDC2015E86) for SemEval 2016 task 8 on meaning representation parsing (May, 2016). The dataset contains 16,833 training, 1,368 development, and 1,371 test sentences which mainly cover domains like newswire, discussion forum, etc. All parsing results are measured by Smatch (version 2.0.2) (Cai and Knight, 2013). 5.1 Experiment Settings We categorize the training data using the automatic alignment and dump a template for date entities and frequent phrases from the multiple to one alignment. We also generate an alignment table from tokens or phrases to their candidate targetside subgraphs. For the dev and test data, we first extract the named entities using the Illinois Named"
P18-1171,J08-4003,0,0.0317682,"rent from Peng et al. (2018) in two ways: the PushIndex phase is initiated before making all the arc decisions; the newly introduced concept is placed at the last cache position instead of the leftmost buffer position, which essentially increases the cache size by 1. Given the sentence “John wants to go” and the recognized concept sequence “Per want-01 go-01” (person name category Per for “John”), our cache transition parser can construct the AMR graph shown in Figure 1 using the run shown in Figure 2 with cache size of 3. 2.1 Oracle Extraction Algorithm We use the following oracle algorithm (Nivre, 2008) to derive the sequence of actions that leads to the gold AMR graph for a cache transition parser with cache size m. The correctness of the oracle is shown by Gildea et al. (2018). Let EG be the set of edges of the gold graph G. We maintain the set of vertices that is not yet shifted into the cache as S, which is initialized with all vertices in G. The vertices are ordered according to their aligned position in the word sequence and the unaligned vertices are listed according to their order in the depth-first traversal of the graph. The oracle algorithm can look into 1844 Figure 3: Sequence-to"
P18-1171,S15-2153,0,0.127115,"hat are close and not if they are distant. Our classifier, which encodes both word and concept sequence information, can accurately predict the reentrancy through the two arc decisions shown in Figure 5. When desire-01 and live01 are shifted into the cache respectively, the transition system makes a left-going arc from each of them to the same concept i, thus creating the reentrancy as desired. 6 we are focused on AMR parsing in this paper, in future work our cache transition system and the presented sequence-to-sequence models can be potentially applied to other semantic graph parsing tasks (Oepen et al., 2015; Du et al., 2015; Zhang et al., 2016; Cao et al., 2017). Acknowledgments We gratefully acknowledge the assistance of Hao Zhang from Google, New York for the monotonic hard attention idea and the helpful comments and suggestions. Conclusion References In this paper, we have presented a sequence-toaction-sequence approach for cache transition systems and applied it to AMR parsing. To address the data sparsity issue for neural AMR parsing, we show that the transition state features are very helpful in constraining the possible output and improving the performance of sequence-to-sequence models."
P18-1171,K15-1004,1,0.889524,"ample of an AMR in which the nodes represent the AMR concepts and the edges represent the relations between the concepts. AMR has been used in various applications such as text summarization (Liu et al., 2015), sentence compression (Takase et al., 2016), and event extraction (Huang et al., 2016). 1 The implementation of our parser is available at https://github.com/xiaochang13/CacheTransition-Seq2seq The task of AMR graph parsing is to map natural language strings to AMR semantic graphs. Different parsers have been developed to tackle this problem (Flanigan et al., 2014; Wang et al., 2015b,a; Peng et al., 2015; Artzi et al., 2015; Pust et al., 2015; van Noord and Bos, 2017). On the other hand, due to the limited amount of labeled data and the large output vocabulary, the sequence-to-sequence model has not been very successful on AMR parsing. Peng et al. (2017) propose a linearization approach that encodes labeled graphs as sequences. To address the data sparsity issue, low-frequency entities and tokens are mapped to special categories to reduce the vocabulary size for the neural models. Konstas et al. (2017) use self-training on a huge amount of unlabeled text to lower the out-of-vocabulary rate. H"
P18-1171,N15-1040,0,0.330941,". Figure 1 shows an example of an AMR in which the nodes represent the AMR concepts and the edges represent the relations between the concepts. AMR has been used in various applications such as text summarization (Liu et al., 2015), sentence compression (Takase et al., 2016), and event extraction (Huang et al., 2016). 1 The implementation of our parser is available at https://github.com/xiaochang13/CacheTransition-Seq2seq The task of AMR graph parsing is to map natural language strings to AMR semantic graphs. Different parsers have been developed to tackle this problem (Flanigan et al., 2014; Wang et al., 2015b,a; Peng et al., 2015; Artzi et al., 2015; Pust et al., 2015; van Noord and Bos, 2017). On the other hand, due to the limited amount of labeled data and the large output vocabulary, the sequence-to-sequence model has not been very successful on AMR parsing. Peng et al. (2017) propose a linearization approach that encodes labeled graphs as sequences. To address the data sparsity issue, low-frequency entities and tokens are mapped to special categories to reduce the vocabulary size for the neural models. Konstas et al. (2017) use self-training on a huge amount of unlabeled text to lower the out"
P18-1171,J16-3001,0,0.0600652,"tant. Our classifier, which encodes both word and concept sequence information, can accurately predict the reentrancy through the two arc decisions shown in Figure 5. When desire-01 and live01 are shifted into the cache respectively, the transition system makes a left-going arc from each of them to the same concept i, thus creating the reentrancy as desired. 6 we are focused on AMR parsing in this paper, in future work our cache transition system and the presented sequence-to-sequence models can be potentially applied to other semantic graph parsing tasks (Oepen et al., 2015; Du et al., 2015; Zhang et al., 2016; Cao et al., 2017). Acknowledgments We gratefully acknowledge the assistance of Hao Zhang from Google, New York for the monotonic hard attention idea and the helpful comments and suggestions. Conclusion References In this paper, we have presented a sequence-toaction-sequence approach for cache transition systems and applied it to AMR parsing. To address the data sparsity issue for neural AMR parsing, we show that the transition state features are very helpful in constraining the possible output and improving the performance of sequence-to-sequence models. We also show that the monotonic hard"
P18-1171,E17-1035,1,0.839076,"and event extraction (Huang et al., 2016). 1 The implementation of our parser is available at https://github.com/xiaochang13/CacheTransition-Seq2seq The task of AMR graph parsing is to map natural language strings to AMR semantic graphs. Different parsers have been developed to tackle this problem (Flanigan et al., 2014; Wang et al., 2015b,a; Peng et al., 2015; Artzi et al., 2015; Pust et al., 2015; van Noord and Bos, 2017). On the other hand, due to the limited amount of labeled data and the large output vocabulary, the sequence-to-sequence model has not been very successful on AMR parsing. Peng et al. (2017) propose a linearization approach that encodes labeled graphs as sequences. To address the data sparsity issue, low-frequency entities and tokens are mapped to special categories to reduce the vocabulary size for the neural models. Konstas et al. (2017) use self-training on a huge amount of unlabeled text to lower the out-of-vocabulary rate. However, the final performance still falls behind the best-performing models. The best performing AMR parsers model graph structures directly. One approach to modeling graph structures is to use a transition system to build graphs step by step, as shown by"
P18-1171,D14-1162,0,0.0817312,", . . . , a∗t−1 , X; θ), (6) t=1 where X represents the input word and concept sequences, and θ is the model parameters. Adam (Kingma and Ba, 2014) with a learning rate of 0.001 is used as the optimizer, and the model that yields the best performance on the dev set is selected to evaluate on the test set. Dropout with rate 0.3 is used during training. Beam search with a beam size of 10 is used for decoding. Both training and decoding use a Tesla K20X GPU. Hidden state sizes for both encoder and decoder are set to 100. The word embeddings are initialized from Glove pretrained word embeddings (Pennington et al., 2014) on Common Crawl, and are not updated during training. The embeddings for POS tags and features are randomly initialized, with the sizes of 20 and 50, respectively. 4.2 Preprocessing and Postprocessing As the AMR data is very sparse, we collapse some subgraphs or spans into categories based on the alignment. We define some special categories such as named entities (NE), dates (DATE), single rooted subgraphs involving multiple concepts (MULT)4 , numbers (NUMBER) and phrases (PHRASE). The phrases are extracted based on the multiple-to-one alignment in the training data. One example phrase is mor"
P18-1171,D15-1136,0,0.229429,"esent the AMR concepts and the edges represent the relations between the concepts. AMR has been used in various applications such as text summarization (Liu et al., 2015), sentence compression (Takase et al., 2016), and event extraction (Huang et al., 2016). 1 The implementation of our parser is available at https://github.com/xiaochang13/CacheTransition-Seq2seq The task of AMR graph parsing is to map natural language strings to AMR semantic graphs. Different parsers have been developed to tackle this problem (Flanigan et al., 2014; Wang et al., 2015b,a; Peng et al., 2015; Artzi et al., 2015; Pust et al., 2015; van Noord and Bos, 2017). On the other hand, due to the limited amount of labeled data and the large output vocabulary, the sequence-to-sequence model has not been very successful on AMR parsing. Peng et al. (2017) propose a linearization approach that encodes labeled graphs as sequences. To address the data sparsity issue, low-frequency entities and tokens are mapped to special categories to reduce the vocabulary size for the neural models. Konstas et al. (2017) use self-training on a huge amount of unlabeled text to lower the out-of-vocabulary rate. However, the final performance still fal"
P18-1171,W09-1119,0,0.0693426,"ntains 16,833 training, 1,368 development, and 1,371 test sentences which mainly cover domains like newswire, discussion forum, etc. All parsing results are measured by Smatch (version 2.0.2) (Cai and Knight, 2013). 5.1 Experiment Settings We categorize the training data using the automatic alignment and dump a template for date entities and frequent phrases from the multiple to one alignment. We also generate an alignment table from tokens or phrases to their candidate targetside subgraphs. For the dev and test data, we first extract the named entities using the Illinois Named Entity Tagger (Ratinov and Roth, 2009) and extract date entities by matching spans with the date template. We further categorize the dataset with the categories we have defined. After categorization, we use Stanford CoreNLP (Manning et al., 2014) to get the POS tags and dependencies of the categorized dataset. We run the oracle algorithm separately for training and dev data (with alignment) to get the statistics of individual phases. We use a cache size of 5 in our experiments. 5.2 Results Individual Phase Accuracy We first evaluate the prediction accuracy of individual phases on the dev oracle data assuming gold prediction histor"
P18-1171,D16-1112,0,0.0456677,"parison with the best-performing models.1 1 ARG1 ARG0 go-01 person ARG0 name name op1 “John” Figure 1: An example of AMR graph representing the meaning of: “John wants to go” Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism where the meaning of a sentence is encoded as a rooted, directed graph. Figure 1 shows an example of an AMR in which the nodes represent the AMR concepts and the edges represent the relations between the concepts. AMR has been used in various applications such as text summarization (Liu et al., 2015), sentence compression (Takase et al., 2016), and event extraction (Huang et al., 2016). 1 The implementation of our parser is available at https://github.com/xiaochang13/CacheTransition-Seq2seq The task of AMR graph parsing is to map natural language strings to AMR semantic graphs. Different parsers have been developed to tackle this problem (Flanigan et al., 2014; Wang et al., 2015b,a; Peng et al., 2015; Artzi et al., 2015; Pust et al., 2015; van Noord and Bos, 2017). On the other hand, due to the limited amount of labeled data and the large output vocabulary, the sequence-to-sequence model has not been very successful on AMR parsing."
P18-1171,D17-1129,0,0.670836,". Konstas et al. (2017) use self-training on a huge amount of unlabeled text to lower the out-of-vocabulary rate. However, the final performance still falls behind the best-performing models. The best performing AMR parsers model graph structures directly. One approach to modeling graph structures is to use a transition system to build graphs step by step, as shown by the system 1842 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1842–1852 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics of Wang and Xue (2017), which is currently the top performing system. This raises the question of whether the advantages of neural and transitionbased system can be combined, as for example with the syntactic parser of Dyer et al. (2015), who use stack LSTMs to capture action history information in the transition state of the transition system. Ballesteros and Al-Onaizan (2017) apply stack-LSTM to transition-based AMR parsing and achieve competitive results, which shows that local transition state information is important for predicting transition actions. Instead of linearizing the target AMR graph to a sequence s"
P18-1171,P15-2141,0,0.0742167,". Figure 1 shows an example of an AMR in which the nodes represent the AMR concepts and the edges represent the relations between the concepts. AMR has been used in various applications such as text summarization (Liu et al., 2015), sentence compression (Takase et al., 2016), and event extraction (Huang et al., 2016). 1 The implementation of our parser is available at https://github.com/xiaochang13/CacheTransition-Seq2seq The task of AMR graph parsing is to map natural language strings to AMR semantic graphs. Different parsers have been developed to tackle this problem (Flanigan et al., 2014; Wang et al., 2015b,a; Peng et al., 2015; Artzi et al., 2015; Pust et al., 2015; van Noord and Bos, 2017). On the other hand, due to the limited amount of labeled data and the large output vocabulary, the sequence-to-sequence model has not been very successful on AMR parsing. Peng et al. (2017) propose a linearization approach that encodes labeled graphs as sequences. To address the data sparsity issue, low-frequency entities and tokens are mapped to special categories to reduce the vocabulary size for the neural models. Konstas et al. (2017) use self-training on a huge amount of unlabeled text to lower the out"
P18-2062,P08-1088,0,0.322675,"Missing"
P18-2062,D16-1250,0,0.0522238,"icant accuracy gains for related languages. 1 Introduction Over the past few years, new methods for bilingual lexicon induction have been proposed that are applicable to low-resource language pairs, for which very little sentence-aligned parallel data is available. Parallel data can be very expensive to create, so methods that require less of it or that can utilize more readily available data are desirable. One prevalent strategy involves creating multilingual word embeddings, where each language’s vocabulary is embedded in the same latent space (Vuli´c and Moens, 2013; Mikolov et al., 2013a; Artetxe et al., 2016); however, many of these methods still require a strong cross-lingual signal in the form of a large seed dictionary. More recent work has focused on reducing that constraint. Vuli´c and Moens (2016) and Vulic and Korhonen (2016) use document-aligned data to learn bilingual embeddings instead of a seed dictionary. Artetxe et al. (2017) use a very small, automatically-generated seed lexicon of identical numerals as the initialization in an iterative self-learning framework to learn a linear mapping between monolingual embedding spaces; Zhang et al. (2017) use an adversarial training method to le"
P18-2062,P16-1024,0,0.0885809,"tence-aligned parallel data is available. Parallel data can be very expensive to create, so methods that require less of it or that can utilize more readily available data are desirable. One prevalent strategy involves creating multilingual word embeddings, where each language’s vocabulary is embedded in the same latent space (Vuli´c and Moens, 2013; Mikolov et al., 2013a; Artetxe et al., 2016); however, many of these methods still require a strong cross-lingual signal in the form of a large seed dictionary. More recent work has focused on reducing that constraint. Vuli´c and Moens (2016) and Vulic and Korhonen (2016) use document-aligned data to learn bilingual embeddings instead of a seed dictionary. Artetxe et al. (2017) use a very small, automatically-generated seed lexicon of identical numerals as the initialization in an iterative self-learning framework to learn a linear mapping between monolingual embedding spaces; Zhang et al. (2017) use an adversarial training method to learn a similar mapping. Lample et al. (2018a) use a series of techniques to align monolingual embedding spaces in a completely unsupervised 2 Background This work is directly based on the work of Artetxe et al. (2017). Following"
P18-2062,P17-1042,0,0.184572,"al. (2011) and Berg-Kirkpatrick et al. (2010) investigate using linguistic features for word alignment, and Haghighi et al. (2008) use linguistic features for unsupervised bilingual lexicon induction. These features can help identify words with common ancestry (such as the English-Italian pair agile-agile) and borrowed words (macaronimaccheroni). The addition of linguistic features led to increased performance in these earlier models, especially for related languages, yet these features have not been applied to more modern methods. In this work, we extend the modern embeddingbased approach of Artetxe et al. (2017) with orthographic information in order to leverage similarities between related languages for increased accuracy in bilingual lexicon induction. Recent embedding-based methods in bilingual lexicon induction show good results, but do not take advantage of orthographic features, such as edit distance, which can be helpful for pairs of related languages. This work extends embedding-based methods to incorporate these features, resulting in significant accuracy gains for related languages. 1 Introduction Over the past few years, new methods for bilingual lexicon induction have been proposed that a"
P18-2062,D13-1168,0,0.0541167,"Missing"
P18-2062,N10-1083,0,0.113958,"Missing"
P18-2062,P17-1179,0,0.0586647,"nd Moens, 2013; Mikolov et al., 2013a; Artetxe et al., 2016); however, many of these methods still require a strong cross-lingual signal in the form of a large seed dictionary. More recent work has focused on reducing that constraint. Vuli´c and Moens (2016) and Vulic and Korhonen (2016) use document-aligned data to learn bilingual embeddings instead of a seed dictionary. Artetxe et al. (2017) use a very small, automatically-generated seed lexicon of identical numerals as the initialization in an iterative self-learning framework to learn a linear mapping between monolingual embedding spaces; Zhang et al. (2017) use an adversarial training method to learn a similar mapping. Lample et al. (2018a) use a series of techniques to align monolingual embedding spaces in a completely unsupervised 2 Background This work is directly based on the work of Artetxe et al. (2017). Following their work, let X ∈ R|Vs |×d and Z ∈ R|Vt |×d be the word embedding matrices of two distinct languages, referred to respectively as the source and target, such that each row corresponds to the d-dimensional embedding of a single word. We refer to the ith row of one of 390 Proceedings of the 56th Annual Meeting of the Association"
P18-2062,P11-1042,0,0.0551595,"27 Abstract way; their method is used by Lample et al. (2018b) as the initialization for a completely unsupervised machine translation system. These recent advances in unsupervised bilingual lexicon induction show promise for use in low-resource contexts. However, none of them make use of linguistic features of the languages themselves (with the arguable exception of syntactic/semantic information encoded in the word embeddings). This is in contrast to work that predates many of these embedding-based methods that leveraged linguistic features such as edit distance and orthographic similarity: Dyer et al. (2011) and Berg-Kirkpatrick et al. (2010) investigate using linguistic features for word alignment, and Haghighi et al. (2008) use linguistic features for unsupervised bilingual lexicon induction. These features can help identify words with common ancestry (such as the English-Italian pair agile-agile) and borrowed words (macaronimaccheroni). The addition of linguistic features led to increased performance in these earlier models, especially for related languages, yet these features have not been applied to more modern methods. In this work, we extend the modern embeddingbased approach of Artetxe et"
P19-1446,W13-2322,0,0.389057,"Missing"
P19-1446,P17-1112,0,0.0342968,"ing Representation (AMR) (Banarescu et al., 2013) is a semantic formalism where the meaning of a sentence is encoded as a rooted, directed graph. Figure 1 shows two AMR graphs in which the nodes (such as “girl” and “leave-11”) represent AMR concepts and the edges (such as “ARG0” and “ARG1”) represent relations between the concepts. The task of parsing sentences into AMRs has received increasing attention, due to the demand for better natural language understanding. Despite the large amount of work on AMR parsing (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015; Buys and Blunsom, 2017; Konstas et al., 2017; Wang and Xue, 2017; Ballesteros and Al-Onaizan, 2017; Lyu and Titov, 2018; Peng et al., 2018; Groschwitz et al., 2018; Guo and Lu, 2018), little attention has been paid to evaluating the parsing results, leaving S MATCH Figure 2: Average, minimal and maximal S MATCH scores over 100 runs on 100 sentences. The running time increases from 6.6 seconds (r=1) to 21.0 (r = 4). (Cai and Knight, 2013) as the only overall performance metric. Damonte et al. (2017) developed a suite of fine-grained performance measures based on the node mappings of S MATCH (see below). S MATCH suff"
P19-1446,P13-2131,0,0.16234,"demand for better natural language understanding. Despite the large amount of work on AMR parsing (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015; Buys and Blunsom, 2017; Konstas et al., 2017; Wang and Xue, 2017; Ballesteros and Al-Onaizan, 2017; Lyu and Titov, 2018; Peng et al., 2018; Groschwitz et al., 2018; Guo and Lu, 2018), little attention has been paid to evaluating the parsing results, leaving S MATCH Figure 2: Average, minimal and maximal S MATCH scores over 100 runs on 100 sentences. The running time increases from 6.6 seconds (r=1) to 21.0 (r = 4). (Cai and Knight, 2013) as the only overall performance metric. Damonte et al. (2017) developed a suite of fine-grained performance measures based on the node mappings of S MATCH (see below). S MATCH suffers from two major drawbacks: first, it is based on greedy hill-climbing to find a one-to-one node mapping between two AMRs (finding the exact best mapping is NP-complete). The search errors weaken its robustness as a metric. To enhance robustness, the hill-climbing search is executed multiple times with random restarts. This decreases efficiency and, more importantly, does not eliminate search errors. Figure 2 show"
P19-1446,W10-1703,0,0.0808578,"Missing"
P19-1446,W14-3346,0,0.130265,"Missing"
P19-1446,J18-1005,1,0.742255,"ing the properties of being rooted and acyclic. Both the original and inverse relations carry the same semantic meaning. Following S MATCH, we unify both types of relations by reverting all inverse relations to their original ones, before calculating S EM B LEU scores. Efficiency As an important factor, the efficiency of S EM B LEU largely depends on the number of extracted n-grams. One potential problem is that there can be a large number of extracted ngrams for very dense graphs. For a fully connected graph with N nodes, there are O(N n ) possible ngrams. Luckily, AMRs are tree-like graphs (Chiang et al., 2018) that are very sparse. For a tree with N nodes, the number of n-grams is bounded by O(n · N ), which is linear in the tree scale. As tree-like graphs, we expect the number of n-grams Comparison with S MATCH In general, S MATCH breaks down the problem of comparing two AMRs into comparing the smallest units: nodes and edges. It treats each AMR as a bag of nodes and edges, and then calculates an F1 score regarding the correctly mapped nodes and edges. Given two AMRs, S MATCH searches for one-to-one mappings between the graph nodes by maximizing the overall F1 score, and the edgeto-edge mappings a"
P19-1446,E17-1051,0,0.0503911,"large amount of work on AMR parsing (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015; Buys and Blunsom, 2017; Konstas et al., 2017; Wang and Xue, 2017; Ballesteros and Al-Onaizan, 2017; Lyu and Titov, 2018; Peng et al., 2018; Groschwitz et al., 2018; Guo and Lu, 2018), little attention has been paid to evaluating the parsing results, leaving S MATCH Figure 2: Average, minimal and maximal S MATCH scores over 100 runs on 100 sentences. The running time increases from 6.6 seconds (r=1) to 21.0 (r = 4). (Cai and Knight, 2013) as the only overall performance metric. Damonte et al. (2017) developed a suite of fine-grained performance measures based on the node mappings of S MATCH (see below). S MATCH suffers from two major drawbacks: first, it is based on greedy hill-climbing to find a one-to-one node mapping between two AMRs (finding the exact best mapping is NP-complete). The search errors weaken its robustness as a metric. To enhance robustness, the hill-climbing search is executed multiple times with random restarts. This decreases efficiency and, more importantly, does not eliminate search errors. Figure 2 shows the means and error bounds of S MATCH scores as a function o"
P19-1446,P14-1134,0,0.488908,"Missing"
P19-1446,P18-1170,0,0.0322099,"Missing"
P19-1446,W04-3250,0,0.496815,"Missing"
P19-1446,P17-1014,0,0.0570761,"(Banarescu et al., 2013) is a semantic formalism where the meaning of a sentence is encoded as a rooted, directed graph. Figure 1 shows two AMR graphs in which the nodes (such as “girl” and “leave-11”) represent AMR concepts and the edges (such as “ARG0” and “ARG1”) represent relations between the concepts. The task of parsing sentences into AMRs has received increasing attention, due to the demand for better natural language understanding. Despite the large amount of work on AMR parsing (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015; Buys and Blunsom, 2017; Konstas et al., 2017; Wang and Xue, 2017; Ballesteros and Al-Onaizan, 2017; Lyu and Titov, 2018; Peng et al., 2018; Groschwitz et al., 2018; Guo and Lu, 2018), little attention has been paid to evaluating the parsing results, leaving S MATCH Figure 2: Average, minimal and maximal S MATCH scores over 100 runs on 100 sentences. The running time increases from 6.6 seconds (r=1) to 21.0 (r = 4). (Cai and Knight, 2013) as the only overall performance metric. Damonte et al. (2017) developed a suite of fine-grained performance measures based on the node mappings of S MATCH (see below). S MATCH suffers from two major dra"
P19-1446,P18-1037,0,0.19416,"tence is encoded as a rooted, directed graph. Figure 1 shows two AMR graphs in which the nodes (such as “girl” and “leave-11”) represent AMR concepts and the edges (such as “ARG0” and “ARG1”) represent relations between the concepts. The task of parsing sentences into AMRs has received increasing attention, due to the demand for better natural language understanding. Despite the large amount of work on AMR parsing (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015; Buys and Blunsom, 2017; Konstas et al., 2017; Wang and Xue, 2017; Ballesteros and Al-Onaizan, 2017; Lyu and Titov, 2018; Peng et al., 2018; Groschwitz et al., 2018; Guo and Lu, 2018), little attention has been paid to evaluating the parsing results, leaving S MATCH Figure 2: Average, minimal and maximal S MATCH scores over 100 runs on 100 sentences. The running time increases from 6.6 seconds (r=1) to 21.0 (r = 4). (Cai and Knight, 2013) as the only overall performance metric. Damonte et al. (2017) developed a suite of fine-grained performance measures based on the node mappings of S MATCH (see below). S MATCH suffers from two major drawbacks: first, it is based on greedy hill-climbing to find a one-to-one nod"
P19-1446,P02-1040,0,0.106943,"Missing"
P19-1446,K15-1004,1,0.853345,"ction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism where the meaning of a sentence is encoded as a rooted, directed graph. Figure 1 shows two AMR graphs in which the nodes (such as “girl” and “leave-11”) represent AMR concepts and the edges (such as “ARG0” and “ARG1”) represent relations between the concepts. The task of parsing sentences into AMRs has received increasing attention, due to the demand for better natural language understanding. Despite the large amount of work on AMR parsing (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015; Buys and Blunsom, 2017; Konstas et al., 2017; Wang and Xue, 2017; Ballesteros and Al-Onaizan, 2017; Lyu and Titov, 2018; Peng et al., 2018; Groschwitz et al., 2018; Guo and Lu, 2018), little attention has been paid to evaluating the parsing results, leaving S MATCH Figure 2: Average, minimal and maximal S MATCH scores over 100 runs on 100 sentences. The running time increases from 6.6 seconds (r=1) to 21.0 (r = 4). (Cai and Knight, 2013) as the only overall performance metric. Damonte et al. (2017) developed a suite of fine-grained performance measures based on the node mappings of S MATCH ("
P19-1446,P18-1171,1,0.775362,"rooted, directed graph. Figure 1 shows two AMR graphs in which the nodes (such as “girl” and “leave-11”) represent AMR concepts and the edges (such as “ARG0” and “ARG1”) represent relations between the concepts. The task of parsing sentences into AMRs has received increasing attention, due to the demand for better natural language understanding. Despite the large amount of work on AMR parsing (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015; Buys and Blunsom, 2017; Konstas et al., 2017; Wang and Xue, 2017; Ballesteros and Al-Onaizan, 2017; Lyu and Titov, 2018; Peng et al., 2018; Groschwitz et al., 2018; Guo and Lu, 2018), little attention has been paid to evaluating the parsing results, leaving S MATCH Figure 2: Average, minimal and maximal S MATCH scores over 100 runs on 100 sentences. The running time increases from 6.6 seconds (r=1) to 21.0 (r = 4). (Cai and Knight, 2013) as the only overall performance metric. Damonte et al. (2017) developed a suite of fine-grained performance measures based on the node mappings of S MATCH (see below). S MATCH suffers from two major drawbacks: first, it is based on greedy hill-climbing to find a one-to-one node mapping between t"
P19-1446,D15-1136,0,0.108417,"Missing"
P19-1446,D17-1129,0,0.0389543,"13) is a semantic formalism where the meaning of a sentence is encoded as a rooted, directed graph. Figure 1 shows two AMR graphs in which the nodes (such as “girl” and “leave-11”) represent AMR concepts and the edges (such as “ARG0” and “ARG1”) represent relations between the concepts. The task of parsing sentences into AMRs has received increasing attention, due to the demand for better natural language understanding. Despite the large amount of work on AMR parsing (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015; Buys and Blunsom, 2017; Konstas et al., 2017; Wang and Xue, 2017; Ballesteros and Al-Onaizan, 2017; Lyu and Titov, 2018; Peng et al., 2018; Groschwitz et al., 2018; Guo and Lu, 2018), little attention has been paid to evaluating the parsing results, leaving S MATCH Figure 2: Average, minimal and maximal S MATCH scores over 100 runs on 100 sentences. The running time increases from 6.6 seconds (r=1) to 21.0 (r = 4). (Cai and Knight, 2013) as the only overall performance metric. Damonte et al. (2017) developed a suite of fine-grained performance measures based on the node mappings of S MATCH (see below). S MATCH suffers from two major drawbacks: first, it is"
P19-1446,D18-1198,0,0.0146831,"AMR graphs in which the nodes (such as “girl” and “leave-11”) represent AMR concepts and the edges (such as “ARG0” and “ARG1”) represent relations between the concepts. The task of parsing sentences into AMRs has received increasing attention, due to the demand for better natural language understanding. Despite the large amount of work on AMR parsing (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015; Buys and Blunsom, 2017; Konstas et al., 2017; Wang and Xue, 2017; Ballesteros and Al-Onaizan, 2017; Lyu and Titov, 2018; Peng et al., 2018; Groschwitz et al., 2018; Guo and Lu, 2018), little attention has been paid to evaluating the parsing results, leaving S MATCH Figure 2: Average, minimal and maximal S MATCH scores over 100 runs on 100 sentences. The running time increases from 6.6 seconds (r=1) to 21.0 (r = 4). (Cai and Knight, 2013) as the only overall performance metric. Damonte et al. (2017) developed a suite of fine-grained performance measures based on the node mappings of S MATCH (see below). S MATCH suffers from two major drawbacks: first, it is based on greedy hill-climbing to find a one-to-one node mapping between two AMRs (finding the exact best mapping is N"
P19-1446,D15-1198,0,\N,Missing
P19-1446,D17-1130,0,\N,Missing
P95-1002,P84-1070,0,0.0968189,"Missing"
P95-1002,J94-3001,0,\N,Missing
P95-1002,J94-3007,0,\N,Missing
Q19-1002,P17-2021,0,0.0274704,"ledge can significantly improve a strong attention-based sequenceto-sequence neural translation model. 1 Introduction It is intuitive that semantic representations ought to be relevant to machine translation, given that the task is to produce a target language sentence with the same meaning as the source language input. Semantic representations formed the core of the earliest symbolic machine translation systems, and have been applied to statistical but non-neural systems as well. Leveraging syntax for neural machine translation (NMT) has been an active research topic (Stahlberg et al., 2016; Aharoni and Goldberg, 2017; Li et al., 2017; Chen et al., 2017; Bastings et al., 2017; Wu et al., 2017; Chen et al., 2018). On the other hand, exploring semantics for NMT has so far received relatively little attention. Recently, Marcheggiani et al. (2018) exploited semantic role labeling (SRL) for NMT, showing that the predicate–argument information 1 That is, flipping arguments corresponding to different roles. 19 Transactions of the Association for Computational Linguistics, vol. 7, pp. 19–31, 2019. Action Editor: Philipp Koehn. Submission batch: 6/2018; Revision batch: 10/2018; Published 3/2019. c 2019 Association"
Q19-1002,D15-1198,0,0.0207633,"MRs capture more relations, such as the relation between John and wife (represented by the subgraph within dotted lines). In addition, AMRs directly capture entity relations and abstract away inflections and function words. As a result, they can serve as a source of knowledge for machine translation that is orthogonal to the textual input. Furthermore, structural information from AMR graphs can help reduce data sparsity when training data is not sufficient for large-scale training. Recent advances in AMR parsing keep pushing the boundary of state-of-the-art performance (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015; Flanigan et al., 2016; Buys and Blunsom, 2017; Konstas et al., 2017; Wang and Xue, 2017; Lyu and Titov, 2018; It is intuitive that semantic representations can be useful for machine translation, mainly because they can help in enforcing meaning preservation and handling data sparsity (many sentences correspond to one meaning) of machine translation models. On the other hand, little work has been done on leveraging semantics for neural machine translation (NMT). In this work, we study the usefulness of AMR (abstract meaning representation) on NMT. Experim"
Q19-1002,J12-2006,0,0.0211864,"en training data are not sufficient. To our knowledge, we are the first to investigate AMR for NMT. Our code and parallel data (training/dev/test) with automatically parsed AMRs are available at https://github.com/freesunshine0316/semantic-nmt. 2 Figure 1: (a) A sentence with semantic roles annotations; (b) the corresponding AMR graph of that sentence. Related Work Most previous work on exploring semantics for statistical machine translation (SMT) studies the usefulness of predicate–argument structure from semantic role labeling (Wong and Mooney, 2006; Wu and Fung, 2009; Liu and Gildea, 2010; Baker et al., 2012). Jones et al. (2012) first convert Prolog expressions into graphical meaning representations, leveraging synchronous hyperedge replacement grammar to parse the input graphs while generating the outputs. Their graphical meaning representation is different from AMR under a strict definition, and their experimental data are limited to 880 sentences. We are the first to investigate AMR on a largescale machine translation task. Recently, Marcheggiani et al. (2018) investigated SRL on NMT. The predicate–argument structures are encoded via graph convolutional network (GCN) layers (Kipf and Welling,"
Q19-1002,W13-2322,0,0.484662,"ow@gmail.com 4 jssu@xmu.edu.cn Abstract from SRL can improve the performance of an attention-based sequence-to-sequence model by alleviating the ‘‘argument switching’’ problem,1 one frequent and severe issue faced by NMT systems (Isabelle et al., 2017). Figure 1(a) shows one example of semantic role information, which only captures the relations between a predicate (gave) and its arguments (John, wife, and present). Other important information, such as the relation between John and wife, cannot be incorporated. In this paper, we explore the usefulness of abstract meaning representation (AMR) (Banarescu et al., 2013) as a semantic representation for NMT. AMR is a semantic formalism that encodes the meaning of a sentence as a rooted, directed graph. Figure 1(b) shows an AMR graph, in which the nodes (such as give-01 and John) represent the concepts and edges (such as :ARG0 and :ARG1) represent the relations between concepts they connect. Comparing with semantic roles, AMRs capture more relations, such as the relation between John and wife (represented by the subgraph within dotted lines). In addition, AMRs directly capture entity relations and abstract away inflections and function words. As a result, they"
Q19-1002,S16-1186,0,0.244245,"ohn and wife (represented by the subgraph within dotted lines). In addition, AMRs directly capture entity relations and abstract away inflections and function words. As a result, they can serve as a source of knowledge for machine translation that is orthogonal to the textual input. Furthermore, structural information from AMR graphs can help reduce data sparsity when training data is not sufficient for large-scale training. Recent advances in AMR parsing keep pushing the boundary of state-of-the-art performance (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015; Flanigan et al., 2016; Buys and Blunsom, 2017; Konstas et al., 2017; Wang and Xue, 2017; Lyu and Titov, 2018; It is intuitive that semantic representations can be useful for machine translation, mainly because they can help in enforcing meaning preservation and handling data sparsity (many sentences correspond to one meaning) of machine translation models. On the other hand, little work has been done on leveraging semantics for neural machine translation (NMT). In this work, we study the usefulness of AMR (abstract meaning representation) on NMT. Experiments on a standard English-toGerman dataset show that incorpo"
Q19-1002,D17-1209,0,0.0409234,"eto-sequence neural translation model. 1 Introduction It is intuitive that semantic representations ought to be relevant to machine translation, given that the task is to produce a target language sentence with the same meaning as the source language input. Semantic representations formed the core of the earliest symbolic machine translation systems, and have been applied to statistical but non-neural systems as well. Leveraging syntax for neural machine translation (NMT) has been an active research topic (Stahlberg et al., 2016; Aharoni and Goldberg, 2017; Li et al., 2017; Chen et al., 2017; Bastings et al., 2017; Wu et al., 2017; Chen et al., 2018). On the other hand, exploring semantics for NMT has so far received relatively little attention. Recently, Marcheggiani et al. (2018) exploited semantic role labeling (SRL) for NMT, showing that the predicate–argument information 1 That is, flipping arguments corresponding to different roles. 19 Transactions of the Association for Computational Linguistics, vol. 7, pp. 19–31, 2019. Action Editor: Philipp Koehn. Submission batch: 6/2018; Revision batch: 10/2018; Published 3/2019. c 2019 Association for Computational Linguistics. Distributed under a CC-BY 4."
Q19-1002,P14-1134,0,0.0836165,"with semantic roles, AMRs capture more relations, such as the relation between John and wife (represented by the subgraph within dotted lines). In addition, AMRs directly capture entity relations and abstract away inflections and function words. As a result, they can serve as a source of knowledge for machine translation that is orthogonal to the textual input. Furthermore, structural information from AMR graphs can help reduce data sparsity when training data is not sufficient for large-scale training. Recent advances in AMR parsing keep pushing the boundary of state-of-the-art performance (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015; Flanigan et al., 2016; Buys and Blunsom, 2017; Konstas et al., 2017; Wang and Xue, 2017; Lyu and Titov, 2018; It is intuitive that semantic representations can be useful for machine translation, mainly because they can help in enforcing meaning preservation and handling data sparsity (many sentences correspond to one meaning) of machine translation models. On the other hand, little work has been done on leveraging semantics for neural machine translation (NMT). In this work, we study the usefulness of AMR (abstract meaning representat"
Q19-1002,P18-1026,0,0.190312,"se layer: − ← − → s 0 = W 1 [ h 0 ; h N ] + b1 , and Transformer (Vaswani et al., 2017) on classification and sequence labeling tasks; Song et al. (2018) build a GRN for encoding AMR graphs for text generation, showing that the representation is superior compared to BiLSTM on serialized AMR. We extend Song et al. (2018) by investigating the usefulness of AMR for neural machine translation. To our knowledge, we are the first to use GRN for machine translation. In addition to GRNs and GCNs, there have been other graph neural networks, such as graph gated neural network (GGNN) (Li et al., 2015b; Beck et al., 2018). Because our main concern is to empirically investigate the effectiveness of AMR for NMT, we leave it to future work to compare GCN, GGNN, and GRN for our task. 3 where W 1 and b1 are model parameters. For each decoding step m, the decoder feeds the concatenation of the embedding of the current input eym and the previous context vector ζ m−1 into the LSTM model to update its hidden state: Baseline: Attention-Based BiLSTM sm = LSTM(sm−1 , [eym ; ζ m−1 ]). We take the attention-based sequence-to-sequence model of Bahdanau et al. (2015) as the baseline, but use LSTM cells (Hochreiter and Schmidh"
Q19-1002,P18-1170,0,0.0223019,"2018; Zhang et al., 2018) for modeling AMRs rather than GCN, which is formally consistent with the RNN sentence encoder. Since there is no one-to-one correspondence between AMR nodes and source words, we adopt a doubly attentive LSTM decoder, which is another major difference from Marcheggiani et al. (2018). GRNs have recently been used to model graph structures in NLP tasks. In particular, Zhang et al. (2018) use a GRN model to represent raw sentences by building a graph structure of neighboring words and a sentence-level node, showing that the encoder outperforms BiLSTMs Peng et al., 2018; Groschwitz et al., 2018; Guo and Lu, 2018), and have made it possible for automatically generated AMRs to benefit downstream tasks, such as question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016), and event detection (Li et al., 2015a). However, to our knowledge, no existing work has exploited AMR for enhancing NMT. We fill in this gap, taking an attention-based sequence-to-sequence system as our baseline, which is similar to Bahdanau et al. (2015). To leverage knowledge within an AMR graph, we adopt a graph recurrent network (GRN) (Song et al., 2018; Zhang et al., 2018) as the AMR encoder. I"
Q19-1002,P17-1112,0,0.0195771,"ed by the subgraph within dotted lines). In addition, AMRs directly capture entity relations and abstract away inflections and function words. As a result, they can serve as a source of knowledge for machine translation that is orthogonal to the textual input. Furthermore, structural information from AMR graphs can help reduce data sparsity when training data is not sufficient for large-scale training. Recent advances in AMR parsing keep pushing the boundary of state-of-the-art performance (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015; Flanigan et al., 2016; Buys and Blunsom, 2017; Konstas et al., 2017; Wang and Xue, 2017; Lyu and Titov, 2018; It is intuitive that semantic representations can be useful for machine translation, mainly because they can help in enforcing meaning preservation and handling data sparsity (many sentences correspond to one meaning) of machine translation models. On the other hand, little work has been done on leveraging semantics for neural machine translation (NMT). In this work, we study the usefulness of AMR (abstract meaning representation) on NMT. Experiments on a standard English-toGerman dataset show that incorporating AMR as additional"
Q19-1002,D18-1198,0,0.0365651,") for modeling AMRs rather than GCN, which is formally consistent with the RNN sentence encoder. Since there is no one-to-one correspondence between AMR nodes and source words, we adopt a doubly attentive LSTM decoder, which is another major difference from Marcheggiani et al. (2018). GRNs have recently been used to model graph structures in NLP tasks. In particular, Zhang et al. (2018) use a GRN model to represent raw sentences by building a graph structure of neighboring words and a sentence-level node, showing that the encoder outperforms BiLSTMs Peng et al., 2018; Groschwitz et al., 2018; Guo and Lu, 2018), and have made it possible for automatically generated AMRs to benefit downstream tasks, such as question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016), and event detection (Li et al., 2015a). However, to our knowledge, no existing work has exploited AMR for enhancing NMT. We fill in this gap, taking an attention-based sequence-to-sequence system as our baseline, which is similar to Bahdanau et al. (2015). To leverage knowledge within an AMR graph, we adopt a graph recurrent network (GRN) (Song et al., 2018; Zhang et al., 2018) as the AMR encoder. In particular, a ful"
Q19-1002,P17-1177,0,0.0237591,"ntion-based sequenceto-sequence neural translation model. 1 Introduction It is intuitive that semantic representations ought to be relevant to machine translation, given that the task is to produce a target language sentence with the same meaning as the source language input. Semantic representations formed the core of the earliest symbolic machine translation systems, and have been applied to statistical but non-neural systems as well. Leveraging syntax for neural machine translation (NMT) has been an active research topic (Stahlberg et al., 2016; Aharoni and Goldberg, 2017; Li et al., 2017; Chen et al., 2017; Bastings et al., 2017; Wu et al., 2017; Chen et al., 2018). On the other hand, exploring semantics for NMT has so far received relatively little attention. Recently, Marcheggiani et al. (2018) exploited semantic role labeling (SRL) for NMT, showing that the predicate–argument information 1 That is, flipping arguments corresponding to different roles. 19 Transactions of the Association for Computational Linguistics, vol. 7, pp. 19–31, 2019. Action Editor: Philipp Koehn. Submission batch: 6/2018; Revision batch: 10/2018; Published 3/2019. c 2019 Association for Computational Linguistics. Distr"
Q19-1002,P82-1020,0,0.739452,"Missing"
Q19-1002,D17-1263,0,0.0258882,"feng Song,1 Daniel Gildea,1 Yue Zhang,2 Zhiguo Wang,3 and Jinsong Su4 1 Department of Computer Science, University of Rochester, Rochester, NY 14627 2 School of Engineering, Westlake University, China 3 IBM T.J. Watson Research Center, Yorktown Heights, NY 10598 4 Xiamen University, Xiamen, China 1 {lsong10,gildea}@cs.rochester.edu 2 yue.zhang@wias.org.cn 3 zgw.tomorrow@gmail.com 4 jssu@xmu.edu.cn Abstract from SRL can improve the performance of an attention-based sequence-to-sequence model by alleviating the ‘‘argument switching’’ problem,1 one frequent and severe issue faced by NMT systems (Isabelle et al., 2017). Figure 1(a) shows one example of semantic role information, which only captures the relations between a predicate (gave) and its arguments (John, wife, and present). Other important information, such as the relation between John and wife, cannot be incorporated. In this paper, we explore the usefulness of abstract meaning representation (AMR) (Banarescu et al., 2013) as a semantic representation for NMT. AMR is a semantic formalism that encodes the meaning of a sentence as a rooted, directed graph. Figure 1(b) shows an AMR graph, in which the nodes (such as give-01 and John) represent the co"
Q19-1002,D14-1179,0,0.0368049,"Missing"
Q19-1002,C12-1083,0,0.0622695,"not sufficient. To our knowledge, we are the first to investigate AMR for NMT. Our code and parallel data (training/dev/test) with automatically parsed AMRs are available at https://github.com/freesunshine0316/semantic-nmt. 2 Figure 1: (a) A sentence with semantic roles annotations; (b) the corresponding AMR graph of that sentence. Related Work Most previous work on exploring semantics for statistical machine translation (SMT) studies the usefulness of predicate–argument structure from semantic role labeling (Wong and Mooney, 2006; Wu and Fung, 2009; Liu and Gildea, 2010; Baker et al., 2012). Jones et al. (2012) first convert Prolog expressions into graphical meaning representations, leveraging synchronous hyperedge replacement grammar to parse the input graphs while generating the outputs. Their graphical meaning representation is different from AMR under a strict definition, and their experimental data are limited to 880 sentences. We are the first to investigate AMR on a largescale machine translation task. Recently, Marcheggiani et al. (2018) investigated SRL on NMT. The predicate–argument structures are encoded via graph convolutional network (GCN) layers (Kipf and Welling, 2017), which are laid"
Q19-1002,W14-3348,0,0.0650882,"Missing"
Q19-1002,P14-5010,0,0.00592251,"Missing"
Q19-1002,P17-4012,0,0.114376,"Missing"
Q19-1002,N18-2078,0,0.264809,"s to produce a target language sentence with the same meaning as the source language input. Semantic representations formed the core of the earliest symbolic machine translation systems, and have been applied to statistical but non-neural systems as well. Leveraging syntax for neural machine translation (NMT) has been an active research topic (Stahlberg et al., 2016; Aharoni and Goldberg, 2017; Li et al., 2017; Chen et al., 2017; Bastings et al., 2017; Wu et al., 2017; Chen et al., 2018). On the other hand, exploring semantics for NMT has so far received relatively little attention. Recently, Marcheggiani et al. (2018) exploited semantic role labeling (SRL) for NMT, showing that the predicate–argument information 1 That is, flipping arguments corresponding to different roles. 19 Transactions of the Association for Computational Linguistics, vol. 7, pp. 19–31, 2019. Action Editor: Philipp Koehn. Submission batch: 6/2018; Revision batch: 10/2018; Published 3/2019. c 2019 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. cantly improves a strong attention-based sequenceto-sequence baseline (25.5 vs 23.7 BLEU). When trained with small-scale (226K) data, the improvement increases"
Q19-1002,W04-3250,0,0.44934,"Missing"
Q19-1002,P17-1014,0,0.345035,"n dotted lines). In addition, AMRs directly capture entity relations and abstract away inflections and function words. As a result, they can serve as a source of knowledge for machine translation that is orthogonal to the textual input. Furthermore, structural information from AMR graphs can help reduce data sparsity when training data is not sufficient for large-scale training. Recent advances in AMR parsing keep pushing the boundary of state-of-the-art performance (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015; Flanigan et al., 2016; Buys and Blunsom, 2017; Konstas et al., 2017; Wang and Xue, 2017; Lyu and Titov, 2018; It is intuitive that semantic representations can be useful for machine translation, mainly because they can help in enforcing meaning preservation and handling data sparsity (many sentences correspond to one meaning) of machine translation models. On the other hand, little work has been done on leveraging semantics for neural machine translation (NMT). In this work, we study the usefulness of AMR (abstract meaning representation) on NMT. Experiments on a standard English-toGerman dataset show that incorporating AMR as additional knowledge can signifi"
Q19-1002,P02-1040,0,0.103866,"Missing"
Q19-1002,P17-1064,0,0.0168608,"ove a strong attention-based sequenceto-sequence neural translation model. 1 Introduction It is intuitive that semantic representations ought to be relevant to machine translation, given that the task is to produce a target language sentence with the same meaning as the source language input. Semantic representations formed the core of the earliest symbolic machine translation systems, and have been applied to statistical but non-neural systems as well. Leveraging syntax for neural machine translation (NMT) has been an active research topic (Stahlberg et al., 2016; Aharoni and Goldberg, 2017; Li et al., 2017; Chen et al., 2017; Bastings et al., 2017; Wu et al., 2017; Chen et al., 2018). On the other hand, exploring semantics for NMT has so far received relatively little attention. Recently, Marcheggiani et al. (2018) exploited semantic role labeling (SRL) for NMT, showing that the predicate–argument information 1 That is, flipping arguments corresponding to different roles. 19 Transactions of the Association for Computational Linguistics, vol. 7, pp. 19–31, 2019. Action Editor: Philipp Koehn. Submission batch: 6/2018; Revision batch: 10/2018; Published 3/2019. c 2019 Association for Computational"
Q19-1002,K15-1004,1,0.859168,"relation between John and wife (represented by the subgraph within dotted lines). In addition, AMRs directly capture entity relations and abstract away inflections and function words. As a result, they can serve as a source of knowledge for machine translation that is orthogonal to the textual input. Furthermore, structural information from AMR graphs can help reduce data sparsity when training data is not sufficient for large-scale training. Recent advances in AMR parsing keep pushing the boundary of state-of-the-art performance (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015; Flanigan et al., 2016; Buys and Blunsom, 2017; Konstas et al., 2017; Wang and Xue, 2017; Lyu and Titov, 2018; It is intuitive that semantic representations can be useful for machine translation, mainly because they can help in enforcing meaning preservation and handling data sparsity (many sentences correspond to one meaning) of machine translation models. On the other hand, little work has been done on leveraging semantics for neural machine translation (NMT). In this work, we study the usefulness of AMR (abstract meaning representation) on NMT. Experiments on a standard English-toGerman da"
Q19-1002,W15-4502,0,0.34139,", which is another major difference from Marcheggiani et al. (2018). GRNs have recently been used to model graph structures in NLP tasks. In particular, Zhang et al. (2018) use a GRN model to represent raw sentences by building a graph structure of neighboring words and a sentence-level node, showing that the encoder outperforms BiLSTMs Peng et al., 2018; Groschwitz et al., 2018; Guo and Lu, 2018), and have made it possible for automatically generated AMRs to benefit downstream tasks, such as question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016), and event detection (Li et al., 2015a). However, to our knowledge, no existing work has exploited AMR for enhancing NMT. We fill in this gap, taking an attention-based sequence-to-sequence system as our baseline, which is similar to Bahdanau et al. (2015). To leverage knowledge within an AMR graph, we adopt a graph recurrent network (GRN) (Song et al., 2018; Zhang et al., 2018) as the AMR encoder. In particular, a full AMR graph is considered as a single state, with nodes in the graph being its substates. State transitions are performed on the graph recurrently, allowing substates to exchange information through edges. At each r"
Q19-1002,P18-1171,1,0.850218,"a GRN (Song et al., 2018; Zhang et al., 2018) for modeling AMRs rather than GCN, which is formally consistent with the RNN sentence encoder. Since there is no one-to-one correspondence between AMR nodes and source words, we adopt a doubly attentive LSTM decoder, which is another major difference from Marcheggiani et al. (2018). GRNs have recently been used to model graph structures in NLP tasks. In particular, Zhang et al. (2018) use a GRN model to represent raw sentences by building a graph structure of neighboring words and a sentence-level node, showing that the encoder outperforms BiLSTMs Peng et al., 2018; Groschwitz et al., 2018; Guo and Lu, 2018), and have made it possible for automatically generated AMRs to benefit downstream tasks, such as question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016), and event detection (Li et al., 2015a). However, to our knowledge, no existing work has exploited AMR for enhancing NMT. We fill in this gap, taking an attention-based sequence-to-sequence system as our baseline, which is similar to Bahdanau et al. (2015). To leverage knowledge within an AMR graph, we adopt a graph recurrent network (GRN) (Song et al., 2018; Zhang et al., 20"
Q19-1002,C10-1081,1,0.785947,"viate data sparsity when training data are not sufficient. To our knowledge, we are the first to investigate AMR for NMT. Our code and parallel data (training/dev/test) with automatically parsed AMRs are available at https://github.com/freesunshine0316/semantic-nmt. 2 Figure 1: (a) A sentence with semantic roles annotations; (b) the corresponding AMR graph of that sentence. Related Work Most previous work on exploring semantics for statistical machine translation (SMT) studies the usefulness of predicate–argument structure from semantic role labeling (Wong and Mooney, 2006; Wu and Fung, 2009; Liu and Gildea, 2010; Baker et al., 2012). Jones et al. (2012) first convert Prolog expressions into graphical meaning representations, leveraging synchronous hyperedge replacement grammar to parse the input graphs while generating the outputs. Their graphical meaning representation is different from AMR under a strict definition, and their experimental data are limited to 880 sentences. We are the first to investigate AMR on a largescale machine translation task. Recently, Marcheggiani et al. (2018) investigated SRL on NMT. The predicate–argument structures are encoded via graph convolutional network (GCN) layer"
Q19-1002,D15-1136,0,0.0408605,"ations, such as the relation between John and wife (represented by the subgraph within dotted lines). In addition, AMRs directly capture entity relations and abstract away inflections and function words. As a result, they can serve as a source of knowledge for machine translation that is orthogonal to the textual input. Furthermore, structural information from AMR graphs can help reduce data sparsity when training data is not sufficient for large-scale training. Recent advances in AMR parsing keep pushing the boundary of state-of-the-art performance (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015; Flanigan et al., 2016; Buys and Blunsom, 2017; Konstas et al., 2017; Wang and Xue, 2017; Lyu and Titov, 2018; It is intuitive that semantic representations can be useful for machine translation, mainly because they can help in enforcing meaning preservation and handling data sparsity (many sentences correspond to one meaning) of machine translation models. On the other hand, little work has been done on leveraging semantics for neural machine translation (NMT). In this work, we study the usefulness of AMR (abstract meaning representation) on NMT. Experiments on a standard"
Q19-1002,P18-1037,0,0.181496,"y capture entity relations and abstract away inflections and function words. As a result, they can serve as a source of knowledge for machine translation that is orthogonal to the textual input. Furthermore, structural information from AMR graphs can help reduce data sparsity when training data is not sufficient for large-scale training. Recent advances in AMR parsing keep pushing the boundary of state-of-the-art performance (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015; Flanigan et al., 2016; Buys and Blunsom, 2017; Konstas et al., 2017; Wang and Xue, 2017; Lyu and Titov, 2018; It is intuitive that semantic representations can be useful for machine translation, mainly because they can help in enforcing meaning preservation and handling data sparsity (many sentences correspond to one meaning) of machine translation models. On the other hand, little work has been done on leveraging semantics for neural machine translation (NMT). In this work, we study the usefulness of AMR (abstract meaning representation) on NMT. Experiments on a standard English-toGerman dataset show that incorporating AMR as additional knowledge can significantly improve a strong attention-based s"
Q19-1002,P16-1162,0,0.0975416,"ns around 4.5 million sentence pairs for training. In addition, we use a subset of the full dataset (News Commentary v11 [NC-v11], containing around 243,000 sentence pairs) for development and additional experiments. For all experiments, we use newstest2013 and newstest2016 as the development and test sets, respectively. To preprocess the data, the tokenizer from Moses4 is used to tokenize both the English and German sides. The training sentence pairs where either side is longer than 50 words are filtered out after tokenization. To deal with rare and compound words, byte-pair encoding (BPE)5 (Sennrich et al., 2016) is applied to both sides. In particular, 8,000 and 16,000 BPE merges are used on the News Commentary v11 subset and the full training set, respectively. On the other hand, JAMR6 (Flanigan et al., 2016) is adopted to parse the English sentences into AMRs before BPE is applied. The statistics of the training data and vocabularies after preprocessing are shown in Tables 1 and 2, respectively. For the experiments with the full training set, we used the top 40K where el and ei are the embeddings of edge label l and source node vi , and W 4 and b4 are model parameters. 4.2 Training Incorporating AM"
Q19-1002,2006.amta-papers.25,0,0.0803469,"Missing"
Q19-1002,D17-1129,0,0.0841465,"dition, AMRs directly capture entity relations and abstract away inflections and function words. As a result, they can serve as a source of knowledge for machine translation that is orthogonal to the textual input. Furthermore, structural information from AMR graphs can help reduce data sparsity when training data is not sufficient for large-scale training. Recent advances in AMR parsing keep pushing the boundary of state-of-the-art performance (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015; Flanigan et al., 2016; Buys and Blunsom, 2017; Konstas et al., 2017; Wang and Xue, 2017; Lyu and Titov, 2018; It is intuitive that semantic representations can be useful for machine translation, mainly because they can help in enforcing meaning preservation and handling data sparsity (many sentences correspond to one meaning) of machine translation models. On the other hand, little work has been done on leveraging semantics for neural machine translation (NMT). In this work, we study the usefulness of AMR (abstract meaning representation) on NMT. Experiments on a standard English-toGerman dataset show that incorporating AMR as additional knowledge can significantly improve a str"
Q19-1002,N06-1056,0,0.151113,"Missing"
Q19-1002,P18-1150,1,0.928432,"ir graphical meaning representation is different from AMR under a strict definition, and their experimental data are limited to 880 sentences. We are the first to investigate AMR on a largescale machine translation task. Recently, Marcheggiani et al. (2018) investigated SRL on NMT. The predicate–argument structures are encoded via graph convolutional network (GCN) layers (Kipf and Welling, 2017), which are laid on top of regular BiRNN or CNN layers. Our work is in line with exploring semantic information, but different in exploiting AMR rather than SRL for NMT. In addition, we leverage a GRN (Song et al., 2018; Zhang et al., 2018) for modeling AMRs rather than GCN, which is formally consistent with the RNN sentence encoder. Since there is no one-to-one correspondence between AMR nodes and source words, we adopt a doubly attentive LSTM decoder, which is another major difference from Marcheggiani et al. (2018). GRNs have recently been used to model graph structures in NLP tasks. In particular, Zhang et al. (2018) use a GRN model to represent raw sentences by building a graph structure of neighboring words and a sentence-level node, showing that the encoder outperforms BiLSTMs Peng et al., 2018; Grosc"
Q19-1002,N09-2004,0,0.0443451,"n from AMR can alleviate data sparsity when training data are not sufficient. To our knowledge, we are the first to investigate AMR for NMT. Our code and parallel data (training/dev/test) with automatically parsed AMRs are available at https://github.com/freesunshine0316/semantic-nmt. 2 Figure 1: (a) A sentence with semantic roles annotations; (b) the corresponding AMR graph of that sentence. Related Work Most previous work on exploring semantics for statistical machine translation (SMT) studies the usefulness of predicate–argument structure from semantic role labeling (Wong and Mooney, 2006; Wu and Fung, 2009; Liu and Gildea, 2010; Baker et al., 2012). Jones et al. (2012) first convert Prolog expressions into graphical meaning representations, leveraging synchronous hyperedge replacement grammar to parse the input graphs while generating the outputs. Their graphical meaning representation is different from AMR under a strict definition, and their experimental data are limited to 880 sentences. We are the first to investigate AMR on a largescale machine translation task. Recently, Marcheggiani et al. (2018) investigated SRL on NMT. The predicate–argument structures are encoded via graph convolution"
Q19-1002,P16-2049,0,0.0266319,"g AMR as additional knowledge can significantly improve a strong attention-based sequenceto-sequence neural translation model. 1 Introduction It is intuitive that semantic representations ought to be relevant to machine translation, given that the task is to produce a target language sentence with the same meaning as the source language input. Semantic representations formed the core of the earliest symbolic machine translation systems, and have been applied to statistical but non-neural systems as well. Leveraging syntax for neural machine translation (NMT) has been an active research topic (Stahlberg et al., 2016; Aharoni and Goldberg, 2017; Li et al., 2017; Chen et al., 2017; Bastings et al., 2017; Wu et al., 2017; Chen et al., 2018). On the other hand, exploring semantics for NMT has so far received relatively little attention. Recently, Marcheggiani et al. (2018) exploited semantic role labeling (SRL) for NMT, showing that the predicate–argument information 1 That is, flipping arguments corresponding to different roles. 19 Transactions of the Association for Computational Linguistics, vol. 7, pp. 19–31, 2019. Action Editor: Philipp Koehn. Submission batch: 6/2018; Revision batch: 10/2018; Published"
Q19-1002,D16-1112,0,0.0991011,"Missing"
Q19-1002,P18-1030,1,0.718086,"g representation is different from AMR under a strict definition, and their experimental data are limited to 880 sentences. We are the first to investigate AMR on a largescale machine translation task. Recently, Marcheggiani et al. (2018) investigated SRL on NMT. The predicate–argument structures are encoded via graph convolutional network (GCN) layers (Kipf and Welling, 2017), which are laid on top of regular BiRNN or CNN layers. Our work is in line with exploring semantic information, but different in exploiting AMR rather than SRL for NMT. In addition, we leverage a GRN (Song et al., 2018; Zhang et al., 2018) for modeling AMRs rather than GCN, which is formally consistent with the RNN sentence encoder. Since there is no one-to-one correspondence between AMR nodes and source words, we adopt a doubly attentive LSTM decoder, which is another major difference from Marcheggiani et al. (2018). GRNs have recently been used to model graph structures in NLP tasks. In particular, Zhang et al. (2018) use a GRN model to represent raw sentences by building a graph structure of neighboring words and a sentence-level node, showing that the encoder outperforms BiLSTMs Peng et al., 2018; Groschwitz et al., 2018; G"
S16-1183,W13-2322,0,0.172838,"Missing"
S16-1183,P13-2131,0,0.143244,"se ARG relations to connect disconnected components to make the result graph connected. The features used are described in Peng et al. (2015). 6 Experiments We evaluate our parser on the LDC2015E86 dataset, which includes 16833 training, 1368 dev, and 1371 test sentences. During the sampling procedure, all the cut variables in the derivation forest are initialized as 1 and an incoming hyperedge is sampled uniformly for each node. We run the sampler for 160 iterations and combine the grammar dumped every 10th iteration. The performance of our SHRG-based parser is evaluated using Smatch v2.0.2 (Cai and Knight, 2013), which evaluates the precision, recall, and F 1 of the concepts and relations all together. Table 3 shows the results on the dev and test set. We also report smatch score on the shared task evaluation data, which includes 1053 sentences. The smatch score on the evaluation data is 0.50. This score is much lower than the performance on the dev and test data. The reason might be that the evaluation data is much harder and includes more noise, which can break down the structure of the learned grammar. The results show that SHRG-based parsing can be a viable approach for AMR parsing. Currently our"
S16-1183,J14-1007,1,0.890632,"Missing"
S16-1183,P14-1134,0,0.698732,"ed English/AMR pairs is provided to learn this mapping. Hyperedge replacement grammar (HRG) is a context-free rewriting formalism for generating graphs (Drewes et al., 1997). Its synchronous counterpart, SHRG, can be used for transforming a graph from/to another structured representation such as a string or tree structure. Therefore, an SHRG-based approach can be used for AMR parsing. Previous approaches usually first map the components of the sentence to components of the graph. Then different supervised algorithms are used to assemble these graph components to generate a complete AMR graph (Flanigan et al., 2014; Wang et al., 2015b; Wang et al., 2015a). Previously, we have developed a system that learns SHRG rules from sentence/AMR graph pairs (Peng et al., 2015), with automatic alignments extracted from JAMR (Flanigan et al., 2014). During the decoding procedure, we also use the concept identification results from Flanigan et al. (2014). The system is evaluated on the newswire section of LDC2013E117, which has around 4000 sentenceAMR pairs as training data. In this paper, we extend this system by using the alignments from Ulf Hermjakob’s automatic aligner and building a perceptron-based concept iden"
S16-1183,D14-1180,1,0.825507,"MC algorithm to learn a grammar of larger rules, by sampling both which minimal rules are used, and how minimal rules combine to form larger composed rules. We sample two types of variables: an edge variable en representing which incoming hyperedge is chosen at a given node n in the forest (allowing us to sample one tree from a forest) and a cut variable zn representing whether node n in the forest is a boundary between two SHRG rules or is internal to an SHRG rule (allowing us to sample rules/fragments from a tree). We use an MCMC algorithm to sample from top-down and one variable at a time (Peng and Gildea, 2014). Sampling tree fragments from forests is described in detail in Chung et al. (2014). Table 2 shows some examples of the sampled SHRG rules. 5 AMR parsing In Section 4 we have described how we extract the SHRG from the training data. Now given a new sentence, we first use the perceptron algorithm to identify the graph fragments each span in the sentence aligns to. Then we use the Earley algorithm to decode the sentence and recover its AMR graph. 5.1 Concept Identification First we identify the segmentation of the sentence. We use the Illinois Named Entity Tagger (NER) (Ratinov and Roth, 2009)"
S16-1183,K15-1004,1,0.927429,"es et al., 1997). Its synchronous counterpart, SHRG, can be used for transforming a graph from/to another structured representation such as a string or tree structure. Therefore, an SHRG-based approach can be used for AMR parsing. Previous approaches usually first map the components of the sentence to components of the graph. Then different supervised algorithms are used to assemble these graph components to generate a complete AMR graph (Flanigan et al., 2014; Wang et al., 2015b; Wang et al., 2015a). Previously, we have developed a system that learns SHRG rules from sentence/AMR graph pairs (Peng et al., 2015), with automatic alignments extracted from JAMR (Flanigan et al., 2014). During the decoding procedure, we also use the concept identification results from Flanigan et al. (2014). The system is evaluated on the newswire section of LDC2013E117, which has around 4000 sentenceAMR pairs as training data. In this paper, we extend this system by using the alignments from Ulf Hermjakob’s automatic aligner and building a perceptron-based concept identifier where the boundary information of the mapped frag1185 Proceedings of SemEval-2016, pages 1185–1189, c San Diego, California, June 16-17, 2016. 2016"
S16-1183,W09-1119,0,0.0477485,"Missing"
S16-1183,P15-2141,0,0.391263,"provided to learn this mapping. Hyperedge replacement grammar (HRG) is a context-free rewriting formalism for generating graphs (Drewes et al., 1997). Its synchronous counterpart, SHRG, can be used for transforming a graph from/to another structured representation such as a string or tree structure. Therefore, an SHRG-based approach can be used for AMR parsing. Previous approaches usually first map the components of the sentence to components of the graph. Then different supervised algorithms are used to assemble these graph components to generate a complete AMR graph (Flanigan et al., 2014; Wang et al., 2015b; Wang et al., 2015a). Previously, we have developed a system that learns SHRG rules from sentence/AMR graph pairs (Peng et al., 2015), with automatic alignments extracted from JAMR (Flanigan et al., 2014). During the decoding procedure, we also use the concept identification results from Flanigan et al. (2014). The system is evaluated on the newswire section of LDC2013E117, which has around 4000 sentenceAMR pairs as training data. In this paper, we extend this system by using the alignments from Ulf Hermjakob’s automatic aligner and building a perceptron-based concept identifier where the bo"
S16-1183,N15-1040,0,0.118081,"provided to learn this mapping. Hyperedge replacement grammar (HRG) is a context-free rewriting formalism for generating graphs (Drewes et al., 1997). Its synchronous counterpart, SHRG, can be used for transforming a graph from/to another structured representation such as a string or tree structure. Therefore, an SHRG-based approach can be used for AMR parsing. Previous approaches usually first map the components of the sentence to components of the graph. Then different supervised algorithms are used to assemble these graph components to generate a complete AMR graph (Flanigan et al., 2014; Wang et al., 2015b; Wang et al., 2015a). Previously, we have developed a system that learns SHRG rules from sentence/AMR graph pairs (Peng et al., 2015), with automatic alignments extracted from JAMR (Flanigan et al., 2014). During the decoding procedure, we also use the concept identification results from Flanigan et al. (2014). The system is evaluated on the newswire section of LDC2013E117, which has around 4000 sentenceAMR pairs as training data. In this paper, we extend this system by using the alignments from Ulf Hermjakob’s automatic aligner and building a perceptron-based concept identifier where the bo"
S16-1183,J07-2003,0,\N,Missing
S16-2009,N09-1004,0,0.0251889,"ense Embedding Learning for Word Sense Induction 1 Linfeng Song1 , Zhiguo Wang2 , Haitao Mi2 and Daniel Gildea1 Department of Computer Science, University of Rochester, Rochester, NY 14627 2 IBM T.J. Watson Research Center, Yorktown Heights, NY 10598 Abstract are utilized in WSD tasks. WSI has been successfully applied to many NLP tasks such as machine translation (Xiong and Zhang, 2014), information retrieval (Navigli and Crisafulli, 2010) and novel sense detection (Lau et al., 2012). However, existing methods usually represent each instance with discrete hand-crafted features (Bordag, 2006; Chen et al., 2009; Van de Cruys and Apidianaki, 2011; Purandare and Pedersen, 2004), which are designed manually and require linguistic knowledge. Most previous methods require learning a specific model for each polysemous word, which limits their usability for downstream applications and loses the chance to jointly learn senses for multiple words. There is a great advance in recent distributed semantics, such as word embedding (Mikolov et al., 2013; Pennington et al., 2014) and sense embedding (Reisinger and Mooney, 2010; Huang et al., 2012; Jauhar et al., 2015; Rothe and Sch¨utze, 2015; Chen et al., 2014; Ti"
S16-2009,D14-1110,0,0.0217553,"06; Chen et al., 2009; Van de Cruys and Apidianaki, 2011; Purandare and Pedersen, 2004), which are designed manually and require linguistic knowledge. Most previous methods require learning a specific model for each polysemous word, which limits their usability for downstream applications and loses the chance to jointly learn senses for multiple words. There is a great advance in recent distributed semantics, such as word embedding (Mikolov et al., 2013; Pennington et al., 2014) and sense embedding (Reisinger and Mooney, 2010; Huang et al., 2012; Jauhar et al., 2015; Rothe and Sch¨utze, 2015; Chen et al., 2014; Tian et al., 2014). Comparing with word embedding, sense embedding methods learn distributed representations for senses of a polysemous word, which is similar to the sense centroid of WSI tasks. In this work, we point out that the WSI task and the sense embedding task are highly interrelated, and propose to jointly learn sense centroids (embeddings) of all polysemous words for the WSI task. Concretely, our method induces several sense centroids (embedding) for each polysemous word in training stage. In testing stage, our method represents each instance as a contextual vector, and induces its"
S16-2009,C14-1123,0,0.0578541,"w, and vc is the representation vector of the instance. Methodology Word Sense Induction WSI is generally considered as an unsupervised clustering task under the distributional hypothesis (Harris, 1954) that the word meaning is reflected by the set of contexts in which it appears. Existing WSI methods can be roughly divided into featurebased or Bayesian. Feature-based methods first represent each instance as a context vector, then utilize a clustering algorithm on the context vectors to induce all the senses. Bayesian methods (Brody and Lapata, 2009; Yao and Van Durme, 2011; Lau et al., 2012; Goyal and Hovy, 2014; Wang et al., 2015), on the other hand, discover senses based on topic models. They adopt either the LDA (Blei et al., 2003) or HDP (Teh et al., 2006) model by viewing each target word as a corpus and the contexts as pseudo-documents, where a context includes all words within a window centred by the target word. For sense induction, they first extract pseudo-documents for the target word, then train topic model, finally pick the most probable topic for each test pseudo-document as the sense. All of the existing WSI methods have two important factors: 1) how to group similar instances (cluster"
S16-2009,N15-1070,0,0.0201635,"with discrete hand-crafted features (Bordag, 2006; Chen et al., 2009; Van de Cruys and Apidianaki, 2011; Purandare and Pedersen, 2004), which are designed manually and require linguistic knowledge. Most previous methods require learning a specific model for each polysemous word, which limits their usability for downstream applications and loses the chance to jointly learn senses for multiple words. There is a great advance in recent distributed semantics, such as word embedding (Mikolov et al., 2013; Pennington et al., 2014) and sense embedding (Reisinger and Mooney, 2010; Huang et al., 2012; Jauhar et al., 2015; Rothe and Sch¨utze, 2015; Chen et al., 2014; Tian et al., 2014). Comparing with word embedding, sense embedding methods learn distributed representations for senses of a polysemous word, which is similar to the sense centroid of WSI tasks. In this work, we point out that the WSI task and the sense embedding task are highly interrelated, and propose to jointly learn sense centroids (embeddings) of all polysemous words for the WSI task. Concretely, our method induces several sense centroids (embedding) for each polysemous word in training stage. In testing stage, our method represents each ins"
S16-2009,P14-1023,0,0.236732,"rd sense disambiguation (WSD) assumes there exists an already-known sense inventory, and the sense of a word type is disambiguated according to the sense inventory. Therefore, clustering methods are generally applied in WSI tasks, while classification methods 85 Proceedings of the Fifth Joint Conference on Lexical and Computational Semantics (*SEM 2016), pages 85–90, Berlin, Germany, August 11-12, 2016. ods separately train a specific VSM for each word. No methods have shown distributional vectors can keep knowledge for multiple words while showing competitive performance. tributional models (Baroni et al., 2014), and (2) a general model for the whole vocabulary is jointly trained to induce sense centroids under the mutlitask learning framework (Caruana, 1997). Evaluated on SemEval-2010 WSI dataset, our method outperforms all participants and most of the recent state-of-the-art methods. 2 2.1 2.2 Sense Embedding for WSI As mentioned in Section 1, sense embedding methods learn a distributed representation for each sense of a polysemous word. There are two key factors for sense embedding learning: (1) how to decide the number of senses for each polysemous word and (2) how to learn an embedding represent"
S16-2009,S10-1079,0,0.0264613,"raditional methods for WSI tasks, the advantages of our method include: 1) WSI models for all the polysemous words are trained jointly under the multi-task learning framework; 2) distributed sense embeddings are taken as the knowledge representations which are trained discriminatively, and usually have better performance than traditional count-based distributional models (Baroni et al., 2014). To verify the two statements, we carefully designed comparative experiments described in the next section. 3 3.1 3.2 Comparing on SemEval-2010 We compare our methods with the following systems: (1) UoY (Korkontzelos and Manandhar, 2010) which is the best system in the SemEval2010 WSI competition; (2) NMFlib (Van de Cruys and Apidianaki, 2011) which adopts non-negative matrix factorization to factor a matrix and then conducts word sense clustering on the test set; (3) NB (Choe and Charniak, 2013) which adopts naive Bayes with the generative story that a context is generated by picking a sense and then all context words given the sense; and (4) Spectral (Goyal and Hovy, 2014) which applies spectral clustering on a set of distributional context vectors. Experimental results are shown in Table 1. Let us see the results on superv"
S16-2009,E06-1018,0,0.0790095,"Missing"
S16-2009,W15-1504,0,0.0378227,"Missing"
S16-2009,E09-1013,0,0.0267862,"where µ(wt , k) is the vector for the k-th sense centroid of word w, and vc is the representation vector of the instance. Methodology Word Sense Induction WSI is generally considered as an unsupervised clustering task under the distributional hypothesis (Harris, 1954) that the word meaning is reflected by the set of contexts in which it appears. Existing WSI methods can be roughly divided into featurebased or Bayesian. Feature-based methods first represent each instance as a context vector, then utilize a clustering algorithm on the context vectors to induce all the senses. Bayesian methods (Brody and Lapata, 2009; Yao and Van Durme, 2011; Lau et al., 2012; Goyal and Hovy, 2014; Wang et al., 2015), on the other hand, discover senses based on topic models. They adopt either the LDA (Blei et al., 2003) or HDP (Teh et al., 2006) model by viewing each target word as a corpus and the contexts as pseudo-documents, where a context includes all words within a window centred by the target word. For sense induction, they first extract pseudo-documents for the target word, then train topic model, finally pick the most probable topic for each test pseudo-document as the sense. All of the existing WSI methods have"
S16-2009,E12-1060,0,0.0146655,"centroid of word w, and vc is the representation vector of the instance. Methodology Word Sense Induction WSI is generally considered as an unsupervised clustering task under the distributional hypothesis (Harris, 1954) that the word meaning is reflected by the set of contexts in which it appears. Existing WSI methods can be roughly divided into featurebased or Bayesian. Feature-based methods first represent each instance as a context vector, then utilize a clustering algorithm on the context vectors to induce all the senses. Bayesian methods (Brody and Lapata, 2009; Yao and Van Durme, 2011; Lau et al., 2012; Goyal and Hovy, 2014; Wang et al., 2015), on the other hand, discover senses based on topic models. They adopt either the LDA (Blei et al., 2003) or HDP (Teh et al., 2006) model by viewing each target word as a corpus and the contexts as pseudo-documents, where a context includes all words within a window centred by the target word. For sense induction, they first extract pseudo-documents for the target word, then train topic model, finally pick the most probable topic for each test pseudo-document as the sense. All of the existing WSI methods have two important factors: 1) how to group simi"
S16-2009,N10-1013,0,0.196952,"-PPMI on both SR and VM. CRP-PPMI has higher PF mainly because it induces fewer number of senses. The above results prove that using sense embeddings have better performance than using count-based distributional models. Besides, SE-WSI-fix is significantly better than WE-Kmeans on every metric. As WEKmeans and SE-WSI-fix learn sense centroids in the same vectors space, while the latter performs joint learning. Therefore, the joint learning is better than learning separately. As for the unsupervised evaluations, SE-WSIfix achieves a good V-Measure score (VM) with a few induced senses. Pedersen (2010) points out that bad models can increase VM by increasing the number of clusters, but doing this will harm performance on both Paired F-score (PF) and SR. Even though UoY, NMFlib and NB show better VM, they (especially UoY) induced more senses than SE-WSI-fix. SE-WSI-fix has higher PF than all others, and higher SR than UoY and NMFlib . 4 Related Work K˚ageb¨ack et al. (2015) proposed two methods to utilize distributed representations for the WSI task. The first method learned centroid vectors by clustering all pre-computed context vectors of each target word. The other method simply adopted M"
S16-2009,D15-1200,0,0.170573,"lity distribution among all the senses for each instance, which can be seen as soft clustering algorithms. As for knowledge representation, existing WSI methods use the vector space model (VSM) to represent each context. In feature-based models, each instance is represented as a vector of values, where a value can be the count of a feature or the co-occurrence between two words. In Bayesian methods, the vectors are represented as co-occurrences between documents and senses or between senses and words. Overall existing methst = arg max sim(µ(wt , k), vc ) k=1,..,K (1) Another group of methods (Li and Jurafsky, 2015) employs non-parametric algorithms to dynamically decide the number of senses for each word, and each instance is assigned to a sense following a probability distribution in Equation 2, where St is the set of already generated senses for wt , and γ is a constant probability for generating a new sense for wt . ( p(k|µ(wt , k), vc ) ∀ k ∈ St st ∼ γ for new sense (2) From the above discussions, we can obviously notice that WSI task and sense embedding task are inter-related. The two factors in sense embedding learning can be aligned to the two factors of WSI task. Concretely, deciding the number"
S16-2009,P15-1173,0,0.0663194,"Missing"
S16-2009,S10-1011,0,0.223826,"Missing"
S16-2009,D10-1012,0,0.0843825,"Missing"
S16-2009,C14-1016,0,0.0236297,"09; Van de Cruys and Apidianaki, 2011; Purandare and Pedersen, 2004), which are designed manually and require linguistic knowledge. Most previous methods require learning a specific model for each polysemous word, which limits their usability for downstream applications and loses the chance to jointly learn senses for multiple words. There is a great advance in recent distributed semantics, such as word embedding (Mikolov et al., 2013; Pennington et al., 2014) and sense embedding (Reisinger and Mooney, 2010; Huang et al., 2012; Jauhar et al., 2015; Rothe and Sch¨utze, 2015; Chen et al., 2014; Tian et al., 2014). Comparing with word embedding, sense embedding methods learn distributed representations for senses of a polysemous word, which is similar to the sense centroid of WSI tasks. In this work, we point out that the WSI task and the sense embedding task are highly interrelated, and propose to jointly learn sense centroids (embeddings) of all polysemous words for the WSI task. Concretely, our method induces several sense centroids (embedding) for each polysemous word in training stage. In testing stage, our method represents each instance as a contextual vector, and induces its sense by finding th"
S16-2009,D14-1113,0,0.547899,"itask learning framework (Caruana, 1997). Evaluated on SemEval-2010 WSI dataset, our method outperforms all participants and most of the recent state-of-the-art methods. 2 2.1 2.2 Sense Embedding for WSI As mentioned in Section 1, sense embedding methods learn a distributed representation for each sense of a polysemous word. There are two key factors for sense embedding learning: (1) how to decide the number of senses for each polysemous word and (2) how to learn an embedding representation for each sense. To decide the number of senses in factor (1), one group of methods (Huang et al., 2012; Neelakantan et al., 2014) set a fixed number K of senses for each word, and each instance is assigned to the most probable sense according to Equation 1, where µ(wt , k) is the vector for the k-th sense centroid of word w, and vc is the representation vector of the instance. Methodology Word Sense Induction WSI is generally considered as an unsupervised clustering task under the distributional hypothesis (Harris, 1954) that the word meaning is reflected by the set of contexts in which it appears. Existing WSI methods can be roughly divided into featurebased or Bayesian. Feature-based methods first represent each insta"
S16-2009,P11-1148,0,0.044133,"Missing"
S16-2009,S10-1081,0,0.0243244,"than CRP-PPMI on both SR and VM. CRP-PPMI has higher PF mainly because it induces fewer number of senses. The above results prove that using sense embeddings have better performance than using count-based distributional models. Besides, SE-WSI-fix is significantly better than WE-Kmeans on every metric. As WEKmeans and SE-WSI-fix learn sense centroids in the same vectors space, while the latter performs joint learning. Therefore, the joint learning is better than learning separately. As for the unsupervised evaluations, SE-WSIfix achieves a good V-Measure score (VM) with a few induced senses. Pedersen (2010) points out that bad models can increase VM by increasing the number of clusters, but doing this will harm performance on both Paired F-score (PF) and SR. Even though UoY, NMFlib and NB show better VM, they (especially UoY) induced more senses than SE-WSI-fix. SE-WSI-fix has higher PF than all others, and higher SR than UoY and NMFlib . 4 Related Work K˚ageb¨ack et al. (2015) proposed two methods to utilize distributed representations for the WSI task. The first method learned centroid vectors by clustering all pre-computed context vectors of each target word. The other method simply adopted M"
S16-2009,Q15-1005,0,0.0201342,"sentation vector of the instance. Methodology Word Sense Induction WSI is generally considered as an unsupervised clustering task under the distributional hypothesis (Harris, 1954) that the word meaning is reflected by the set of contexts in which it appears. Existing WSI methods can be roughly divided into featurebased or Bayesian. Feature-based methods first represent each instance as a context vector, then utilize a clustering algorithm on the context vectors to induce all the senses. Bayesian methods (Brody and Lapata, 2009; Yao and Van Durme, 2011; Lau et al., 2012; Goyal and Hovy, 2014; Wang et al., 2015), on the other hand, discover senses based on topic models. They adopt either the LDA (Blei et al., 2003) or HDP (Teh et al., 2006) model by viewing each target word as a corpus and the contexts as pseudo-documents, where a context includes all words within a window centred by the target word. For sense induction, they first extract pseudo-documents for the target word, then train topic model, finally pick the most probable topic for each test pseudo-document as the sense. All of the existing WSI methods have two important factors: 1) how to group similar instances (clustering algorithm) and 2"
S16-2009,D14-1162,0,0.0881563,"sense detection (Lau et al., 2012). However, existing methods usually represent each instance with discrete hand-crafted features (Bordag, 2006; Chen et al., 2009; Van de Cruys and Apidianaki, 2011; Purandare and Pedersen, 2004), which are designed manually and require linguistic knowledge. Most previous methods require learning a specific model for each polysemous word, which limits their usability for downstream applications and loses the chance to jointly learn senses for multiple words. There is a great advance in recent distributed semantics, such as word embedding (Mikolov et al., 2013; Pennington et al., 2014) and sense embedding (Reisinger and Mooney, 2010; Huang et al., 2012; Jauhar et al., 2015; Rothe and Sch¨utze, 2015; Chen et al., 2014; Tian et al., 2014). Comparing with word embedding, sense embedding methods learn distributed representations for senses of a polysemous word, which is similar to the sense centroid of WSI tasks. In this work, we point out that the WSI task and the sense embedding task are highly interrelated, and propose to jointly learn sense centroids (embeddings) of all polysemous words for the WSI task. Concretely, our method induces several sense centroids (embedding) for"
S16-2009,P14-1137,0,0.0424369,"Missing"
S16-2009,W04-2406,0,0.0636251,"feng Song1 , Zhiguo Wang2 , Haitao Mi2 and Daniel Gildea1 Department of Computer Science, University of Rochester, Rochester, NY 14627 2 IBM T.J. Watson Research Center, Yorktown Heights, NY 10598 Abstract are utilized in WSD tasks. WSI has been successfully applied to many NLP tasks such as machine translation (Xiong and Zhang, 2014), information retrieval (Navigli and Crisafulli, 2010) and novel sense detection (Lau et al., 2012). However, existing methods usually represent each instance with discrete hand-crafted features (Bordag, 2006; Chen et al., 2009; Van de Cruys and Apidianaki, 2011; Purandare and Pedersen, 2004), which are designed manually and require linguistic knowledge. Most previous methods require learning a specific model for each polysemous word, which limits their usability for downstream applications and loses the chance to jointly learn senses for multiple words. There is a great advance in recent distributed semantics, such as word embedding (Mikolov et al., 2013; Pennington et al., 2014) and sense embedding (Reisinger and Mooney, 2010; Huang et al., 2012; Jauhar et al., 2015; Rothe and Sch¨utze, 2015; Chen et al., 2014; Tian et al., 2014). Comparing with word embedding, sense embedding m"
S16-2009,W11-1102,0,0.0490206,"Missing"
S16-2009,P12-1092,0,\N,Missing
S16-2009,D13-1148,0,\N,Missing
W01-0521,J93-2001,0,0.00733771,"f the parser&apos;s probability model are particularly tuned to one corpus, and which are more general? Our investigation of these questions leads us to a surprising result about parsing the WSJ corpus: over a third of the model&apos;s parameters can be eliminated with little impact on performance. Aside from cross-corpus considerations, this is an important nding if a lightweight parser is desired or memory usage is a consideration. 2 Previous Comparisons of Corpora A great deal of work has been done outside of the parsing community analyzing the variations between corpora and di erent genres of text. Biber (1993) investigated variation in a number syntactic features over genres, or registers, of language. Of particular importance to statistical parsers is the investigation of frequencies for verb subcategorizations such as Roland and Jurafsky (1998). Roland et al. (2000) nd that subcategorization frequencies for certain verbs vary signi cantly between the Wall Street Journal corpus and the mixed-genre Brown corpus, but that they vary less so between genre-balanced British and American corpora. Argument structure is essentially the task that automatic parsers attempt to solve, and the frequencies of va"
W01-0521,A00-2018,0,0.123993,"Missing"
W01-0521,P99-1065,0,0.115937,"past several years have seen great progress in the eld of natural language parsing, through the use of statistical methods trained using large corpora of hand-parsed training data. The techniques of Charniak (1997), Collins (1997), and Ratnaparkhi (1997) achieved roughly comparable results using the same sets of training and test data. In each case, the corpus used was the Penn Treebank&apos;s hand-annotated parses of Wall Street Journal articles. Relatively few quantitative parsing results have been reported on other corpora (though see Stolcke et al. (1996) for results on Switchboard, as well as Collins et al. (1999) for results on Czech and Hwa (1999) for bootstrapping from WSJ to ATIS). The inclusion of parses for the Brown corpus in the Penn Treebank allows us to compare parser performance across corpora. In this paper we examine the following questions:  To what extent is the performance of statistical parsers on the WSJ task due to its relatively uniform style, and how might such parsers fare on the more varied Brown corpus?  Can training data from one corpus be applied to parsing another?  What aspects of the parser&apos;s probability model are particularly tuned to one corpus, and which are more gene"
W01-0521,P96-1025,0,0.19984,"Missing"
W01-0521,P97-1003,0,0.588467,"r performance, and how portable parsing models are across corpora. We examine these questions by comparing results for the Brown and WSJ corpora, and also consider which parts of the parser&apos;s probability model are particularly tuned to the corpus on which it was trained. This leads us to a technique for pruning parameters to reduce the size of the parsing model. 1 Introduction The past several years have seen great progress in the eld of natural language parsing, through the use of statistical methods trained using large corpora of hand-parsed training data. The techniques of Charniak (1997), Collins (1997), and Ratnaparkhi (1997) achieved roughly comparable results using the same sets of training and test data. In each case, the corpus used was the Penn Treebank&apos;s hand-annotated parses of Wall Street Journal articles. Relatively few quantitative parsing results have been reported on other corpora (though see Stolcke et al. (1996) for results on Switchboard, as well as Collins et al. (1999) for results on Czech and Hwa (1999) for bootstrapping from WSJ to ATIS). The inclusion of parses for the Brown corpus in the Penn Treebank allows us to compare parser performance across corpora. In this paper"
W01-0521,P99-1010,0,0.0196761,"the eld of natural language parsing, through the use of statistical methods trained using large corpora of hand-parsed training data. The techniques of Charniak (1997), Collins (1997), and Ratnaparkhi (1997) achieved roughly comparable results using the same sets of training and test data. In each case, the corpus used was the Penn Treebank&apos;s hand-annotated parses of Wall Street Journal articles. Relatively few quantitative parsing results have been reported on other corpora (though see Stolcke et al. (1996) for results on Switchboard, as well as Collins et al. (1999) for results on Czech and Hwa (1999) for bootstrapping from WSJ to ATIS). The inclusion of parses for the Brown corpus in the Penn Treebank allows us to compare parser performance across corpora. In this paper we examine the following questions:  To what extent is the performance of statistical parsers on the WSJ task due to its relatively uniform style, and how might such parsers fare on the more varied Brown corpus?  Can training data from one corpus be applied to parsing another?  What aspects of the parser&apos;s probability model are particularly tuned to one corpus, and which are more general? Our investigation of these ques"
W01-0521,P95-1037,0,0.199298,"Missing"
W01-0521,W97-0301,0,0.00726156,"ow portable parsing models are across corpora. We examine these questions by comparing results for the Brown and WSJ corpora, and also consider which parts of the parser&apos;s probability model are particularly tuned to the corpus on which it was trained. This leads us to a technique for pruning parameters to reduce the size of the parsing model. 1 Introduction The past several years have seen great progress in the eld of natural language parsing, through the use of statistical methods trained using large corpora of hand-parsed training data. The techniques of Charniak (1997), Collins (1997), and Ratnaparkhi (1997) achieved roughly comparable results using the same sets of training and test data. In each case, the corpus used was the Penn Treebank&apos;s hand-annotated parses of Wall Street Journal articles. Relatively few quantitative parsing results have been reported on other corpora (though see Stolcke et al. (1996) for results on Switchboard, as well as Collins et al. (1999) for results on Czech and Hwa (1999) for bootstrapping from WSJ to ATIS). The inclusion of parses for the Brown corpus in the Penn Treebank allows us to compare parser performance across corpora. In this paper we examine the followin"
W01-0521,P98-2184,0,0.0266526,"arameters can be eliminated with little impact on performance. Aside from cross-corpus considerations, this is an important nding if a lightweight parser is desired or memory usage is a consideration. 2 Previous Comparisons of Corpora A great deal of work has been done outside of the parsing community analyzing the variations between corpora and di erent genres of text. Biber (1993) investigated variation in a number syntactic features over genres, or registers, of language. Of particular importance to statistical parsers is the investigation of frequencies for verb subcategorizations such as Roland and Jurafsky (1998). Roland et al. (2000) nd that subcategorization frequencies for certain verbs vary signi cantly between the Wall Street Journal corpus and the mixed-genre Brown corpus, but that they vary less so between genre-balanced British and American corpora. Argument structure is essentially the task that automatic parsers attempt to solve, and the frequencies of various structures in training data are re ected in a statistical parser&apos;s probability model. The variation in verb argument structure found by previous research caused us to wonder to what extent a model trained on one corpus would be useful"
W01-0521,W00-0905,0,0.0317266,"with little impact on performance. Aside from cross-corpus considerations, this is an important nding if a lightweight parser is desired or memory usage is a consideration. 2 Previous Comparisons of Corpora A great deal of work has been done outside of the parsing community analyzing the variations between corpora and di erent genres of text. Biber (1993) investigated variation in a number syntactic features over genres, or registers, of language. Of particular importance to statistical parsers is the investigation of frequencies for verb subcategorizations such as Roland and Jurafsky (1998). Roland et al. (2000) nd that subcategorization frequencies for certain verbs vary signi cantly between the Wall Street Journal corpus and the mixed-genre Brown corpus, but that they vary less so between genre-balanced British and American corpora. Argument structure is essentially the task that automatic parsers attempt to solve, and the frequencies of various structures in training data are re ected in a statistical parser&apos;s probability model. The variation in verb argument structure found by previous research caused us to wonder to what extent a model trained on one corpus would be useful in parsing another. Th"
W01-0521,1993.iwpt-1.22,0,0.0394903,"s to have relatively little impact if it is not matched to the test material. The more varied nature of the Brown corpus also seems to impact results, as all the results on Brown are lower than the WSJ result. 5 The E ect of Lexical Dependencies The parsers cited above all use some variety of lexical dependency feature to capture statistics on the cooccurrence of pairs of words being found in parentchild relations within the parse tree. These word pair relations, also called lexical bigrams (Collins, 1996), are reminiscent of dependency grammars such as Melcuk (1988) and the link grammar of Sleator and Temperley (1993). In Collins&apos; Model 1, the word pair statistics occur in the distribution Pcw (ChwjP; H; Hht; Hhw; ; C; Cht) where Hhw represent the head word of a parent node in the tree and Chw the head word of its (non-head) child. (The head word of a parent is the same as the head word of its head child.) Because this is the only part of the model that involves pairs of words, it is also where the bulk of the parameters are found. The large number of possible pairs of words in the vocabulary make the training data necessarily sparse. In order to avoid assigning zero probability to unseen events, it is ne"
W01-0521,J03-4003,0,\N,Missing
W01-0521,C98-2179,0,\N,Missing
W03-1008,P02-1043,1,0.469137,"res. The PropBank (Kingsbury and Palmer, 2002) and the FrameNet (Baker et al., 1998) projects both document the variation in syntactic realization of the arguments of predicates in general English text. Gildea and Palmer (2002) developed a system to predict semantic roles (as defined in PropBank) from sentences and their parse trees as determined by the statistical parser of Collins (1999). In this paper, we examine how the syntactic representations used by different statistical parsers affect the performance of such a system. We compare a parser based on Combinatory Categorial Grammar (CCG) (Hockenmaier and Steedman, 2002b) with the Collins parser. As the CCG parser is trained and tested on a corpus of CCG derivations that have been obtained by automatic conversion from the Penn Treebank, we are able to compare performance using both goldstandard and automatic parses for both CCG and the traditional Treebank representation. The Treebankparser returns skeletal phrase-structure trees without the traces or functional tags in the original Penn Treebank, whereas the CCG parser returns wordword dependencies that correspond to the underlying predicate-argument structure, including longrange dependencies arising throu"
W03-1008,P02-1018,0,0.0264799,"Missing"
W03-1008,kingsbury-palmer-2002-treebank,0,0.112779,"e semantic roles of sentence constituents is a crucial part of interpreting text, and in addition to forming an important part of the information extraction problem, can serve as an intermediate step in machine translation or automatic summarization. Even for a single predicate, semantic arguments can have multiple syntactic realizations, as shown by the following paraphrases: (1) (2) John will meet with Mary. John will meet Mary. John and Mary will meet. The door opened. Mary opened the door. Recently, attention has turned to creating corpora annotated with argument structures. The PropBank (Kingsbury and Palmer, 2002) and the FrameNet (Baker et al., 1998) projects both document the variation in syntactic realization of the arguments of predicates in general English text. Gildea and Palmer (2002) developed a system to predict semantic roles (as defined in PropBank) from sentences and their parse trees as determined by the statistical parser of Collins (1999). In this paper, we examine how the syntactic representations used by different statistical parsers affect the performance of such a system. We compare a parser based on Combinatory Categorial Grammar (CCG) (Hockenmaier and Steedman, 2002b) with the Coll"
W03-1008,P98-1013,0,0.0402206,"crucial part of interpreting text, and in addition to forming an important part of the information extraction problem, can serve as an intermediate step in machine translation or automatic summarization. Even for a single predicate, semantic arguments can have multiple syntactic realizations, as shown by the following paraphrases: (1) (2) John will meet with Mary. John will meet Mary. John and Mary will meet. The door opened. Mary opened the door. Recently, attention has turned to creating corpora annotated with argument structures. The PropBank (Kingsbury and Palmer, 2002) and the FrameNet (Baker et al., 1998) projects both document the variation in syntactic realization of the arguments of predicates in general English text. Gildea and Palmer (2002) developed a system to predict semantic roles (as defined in PropBank) from sentences and their parse trees as determined by the statistical parser of Collins (1999). In this paper, we examine how the syntactic representations used by different statistical parsers affect the performance of such a system. We compare a parser based on Combinatory Categorial Grammar (CCG) (Hockenmaier and Steedman, 2002b) with the Collins parser. As the CCG parser is train"
W03-1008,J02-3001,1,0.53218,"are available, which can be thought of as choosing the topmost distributions available from a backoff lattice, shown in Figure 3. P(r |h, pt, p) P(r |pt, path, p) P(r |h, p) P(r |h) P(r |pt, p) P(r |pt, pos, v, p) P(r |pt, pos, v) P(r |p) Figure 3: Backoff lattice with more specific distributions towards the top. The probabilities P (ri |Fi , p) are combined with the probabilities P ({r1..n }|p) for a set of roles appearing in a sentence given a predicate, using the following formula: P (r1..n |F1..n , p) ≈ P ({r1..n }|p) Y P (ri |Fi , p) i P (ri |p) This approach, described in more detail in Gildea and Jurafsky (2002), allows interaction between the role assignments for individual constituents while making certain independence assumptions necessary for efficient probability estimation. In particular, we assume that sets of roles appear independent of their linear order, and that the features F of a constituents are independent of other constituents’ features given the constituent’s role. 5.2 The model for CCG derivations In the CCG version, we replace the features above with corresponding features based on both the sentence’s CCG derivation tree (shown in Figure 1) and the CCG predicate-argument relations"
W03-1008,P02-1031,1,0.632647,"termediate step in machine translation or automatic summarization. Even for a single predicate, semantic arguments can have multiple syntactic realizations, as shown by the following paraphrases: (1) (2) John will meet with Mary. John will meet Mary. John and Mary will meet. The door opened. Mary opened the door. Recently, attention has turned to creating corpora annotated with argument structures. The PropBank (Kingsbury and Palmer, 2002) and the FrameNet (Baker et al., 1998) projects both document the variation in syntactic realization of the arguments of predicates in general English text. Gildea and Palmer (2002) developed a system to predict semantic roles (as defined in PropBank) from sentences and their parse trees as determined by the statistical parser of Collins (1999). In this paper, we examine how the syntactic representations used by different statistical parsers affect the performance of such a system. We compare a parser based on Combinatory Categorial Grammar (CCG) (Hockenmaier and Steedman, 2002b) with the Collins parser. As the CCG parser is trained and tested on a corpus of CCG derivations that have been obtained by automatic conversion from the Penn Treebank, we are able to compare per"
W03-1008,hockenmaier-steedman-2002-acquiring,1,\N,Missing
W03-1008,J03-4003,0,\N,Missing
W03-1008,C98-1013,0,\N,Missing
W04-3228,J90-2002,0,0.249265,"y of Rochester Rochester, NY 14627 Abstract Given a parallel parsed corpus, statistical treeto-tree alignment attempts to match nodes in the syntactic trees for a given sentence in two languages. We train a probabilistic tree transduction model on a large automatically parsed Chinese-English corpus, and evaluate results against human-annotated word level alignments. We find that a constituent-based model performs better than a similar probability model trained on the same trees converted to a dependency representation. 1 Introduction Statistical approaches to machine translation, pioneered by Brown et al. (1990), estimate parameters for a probabilistic model of word-to-word correspondences and word re-orderings directly from large corpora of parallel bilingual text. In recent years, a number of syntactically motivated approaches to statistical machine translation have been proposed. These approaches assign a parallel tree structure to the two sides of each sentence pair, and model the translation process with reordering operations defined on the tree structure. The tree-based approach allows us to represent the fact that syntactic constituents tend to move as unit, as well as systematic differences i"
W04-3228,J93-2003,0,0.0195757,"Missing"
W04-3228,P03-2041,0,0.0301991,"represents translation as a sequence of re-ordering operations over children of nodes in a syntactic tree, using automatic parser output for the initial tree structures. This gives the translation model more information about the structure of the source language, and further constrains the reorderings to match not just a possible bracketing as in Wu (1997), but the specific bracketing of the parse tree provided. Recent models of alignment have attempted to exploit syntactic information from both languages by aligning a pair of parse trees for the same sentence in either language node by node. Eisner (2003) presented such a system for transforming semantic-level dependecy trees into syntactic-level dependency trees for text generation. Gildea (2003) trained a system on parallel constituent trees from the Korean-English Treebank, evaluating agreement with hand-annotated word alignments. Ding and Palmer (2004) align parallel dependency trees with a divide and conquer strategy, choosing a highly likely word-pair as a splitting point in each tree. In addition to providing a deeper level of representation for the transformations of the translation model to work with, tree-to-tree models have the adva"
W04-3228,W02-1039,0,0.0530879,"can be computed under the same dynamic how to reorder the remaining nodes from source to programming assumptions as the basic tree-to-tree target tree, O((2m)!). Thus overall complexity of model. As with the tree-to-string cloning operation, the algorithm is O(|T |2 m2 42m (2m)!), quadratic in this independence assumption is essential to keep the size of the input sentences, but exponential in the complexity polynomial in the size of the input sentences. 3 Dependency Tree-to-Tree Alignments Dependencies were found to be more consistent than constituent structure between French and English by Fox (2002), though this study used a tree representation on the English side only. We wish to investigate whether dependency trees are also more suited to tree-to-tree alignment. Figure 1 shows a typical Xinhua newswire sentence with the Chinese parser output, and the sentence’s English translation with its parse tree. The conversion to dependency representation is shown below the original parse trees. Examination of the trees shows both cases where the dependency representation is more similar across the two languages, as well as its potential pitfalls. The initial noun phrase, “14 Chinese open border"
W04-3228,N04-1035,0,0.0720012,"Missing"
W04-3228,P03-1011,1,0.938936,"initial tree structures. This gives the translation model more information about the structure of the source language, and further constrains the reorderings to match not just a possible bracketing as in Wu (1997), but the specific bracketing of the parse tree provided. Recent models of alignment have attempted to exploit syntactic information from both languages by aligning a pair of parse trees for the same sentence in either language node by node. Eisner (2003) presented such a system for transforming semantic-level dependecy trees into syntactic-level dependency trees for text generation. Gildea (2003) trained a system on parallel constituent trees from the Korean-English Treebank, evaluating agreement with hand-annotated word alignments. Ding and Palmer (2004) align parallel dependency trees with a divide and conquer strategy, choosing a highly likely word-pair as a splitting point in each tree. In addition to providing a deeper level of representation for the transformations of the translation model to work with, tree-to-tree models have the advantage that they are much less computationally costly to train than models which must induce tree structure on one or both sides of the translatio"
W04-3228,P02-1050,0,0.0475451,"urselves to sentences of no more than 25 words in either language, resulting in a training corpus of 18,773 sentence pairs with a total of 276,113 Chinese words and 315,415 English words. The Chinese data were automatically segmented into tokens, and English capitalization was retained. We replace words occurring only once with an unknown word token, resulting in a Chinese vocabulary of 23,783 words and an English vocabulary of 27,075 words. Chinese data was parsed using the parser of Bikel (2002), and English data was parsed using Collins (1999). Our hand-aligned test data were those used in Hwa et al. (2002), and consisted of 48 sentence pairs also with less than 25 words in either language, for a total of 788 English words and 580 Chinese words. The hand aligned data consisted of 745 individual aligned word pairs. Words could be aligned one-to-many in either direction. This limits the performance achievable by our models; the IBM models allow one-to-many alignments in one direction only, while the tree-based models allow only one-to-one alignment unless the cloning operation is used. A separate set of 49 hand-aligned sentence pairs was used to control overfitting in training our models. We evalu"
W04-3228,P03-1056,0,0.0274869,"dependency representation is shown below the original parse trees. Examination of the trees shows both cases where the dependency representation is more similar across the two languages, as well as its potential pitfalls. The initial noun phrase, “14 Chinese open border cities” has two subphrases with a level of constituent structure (the QP and the lower NP) not found in the English parse. In this case, the difference in constituent structure derives primarily from differences in the annotation style between the original English and Chinese treebanks (Marcus et al., 1993; Xue and Xia, 2000; Levy and Manning, 2003). These differences disappear in the constituent representation. In general, the number of levels of constituent structure in a tree can be relatively arbitrary, while it is easier for people (whether professional syntacticians or not) to agree on the word-to-word dependencies. In some cases, differences in the number of level may be handled by the tree-to-tree model, for example by grouping the subject NP and its base NP child together as a single elementary tree. However, this introduces unnecessary variability into the alignment process. In cases with large difference in the depths of the t"
W04-3228,J93-2004,0,0.0240639,"n with its parse tree. The conversion to dependency representation is shown below the original parse trees. Examination of the trees shows both cases where the dependency representation is more similar across the two languages, as well as its potential pitfalls. The initial noun phrase, “14 Chinese open border cities” has two subphrases with a level of constituent structure (the QP and the lower NP) not found in the English parse. In this case, the difference in constituent structure derives primarily from differences in the annotation style between the original English and Chinese treebanks (Marcus et al., 1993; Xue and Xia, 2000; Levy and Manning, 2003). These differences disappear in the constituent representation. In general, the number of levels of constituent structure in a tree can be relatively arbitrary, while it is easier for people (whether professional syntacticians or not) to agree on the word-to-word dependencies. In some cases, differences in the number of level may be handled by the tree-to-tree model, for example by grouping the subject NP and its base NP child together as a single elementary tree. However, this introduces unnecessary variability into the alignment process. In cases"
W04-3228,P00-1056,0,0.0494078,"er direction. This limits the performance achievable by our models; the IBM models allow one-to-many alignments in one direction only, while the tree-based models allow only one-to-one alignment unless the cloning operation is used. A separate set of 49 hand-aligned sentence pairs was used to control overfitting in training our models. We evaluate our translation models in terms of agreement with human-annotated word-level alignments between the sentence pairs. For scoring the viterbi alignments of each system against goldstandard annotated alignments, we use the alignment error rate (AER) of Och and Ney (2000), which measures agreement at the level of pairs of words:1 AER = 1 − 2|A ∩ G| |A |+ |G| where A is the set of word pairs aligned by the automatic system, and G the set aligned in the gold standard. For a better understanding of how the models 1 While Och and Ney (2000) differentiate between sure and possible hand-annotated alignments, our gold standard alignments come in only one variety. differ, we break this figure down into precision: P= and recall: |A ∩ G| |A| |A ∩ G| |G| Since none of the systems presented in this comparison make use of hand-aligned data, they may differ in the overall p"
W04-3228,J97-3002,0,0.139507,"ucture to the two sides of each sentence pair, and model the translation process with reordering operations defined on the tree structure. The tree-based approach allows us to represent the fact that syntactic constituents tend to move as unit, as well as systematic differences in word order in the grammars of the two languages. Furthermore, the tree structure allows us to make probabilistic independence assumptions that result in polynomial time algorithms for estimating a translation model from parallel training data, and for finding the highest probability translation given a new sentence. Wu (1997) modeled the reordering process with binary branching trees, where each production could be either in the same or in reverse order going from source to target language. The trees of Wu’s Inversion Transduction Grammar were derived by synchronously parsing a parallel corpus, using a grammar with lexical translation probabilities at the leaves and a simple grammar with a single nonterminal providing the tree structure. While this grammar did not represent traditional syntactic categories such as verb phrases and noun phrases, it served to restrict the word-level alignments considered by the syst"
W04-3228,P01-1067,0,0.0791731,"re each production could be either in the same or in reverse order going from source to target language. The trees of Wu’s Inversion Transduction Grammar were derived by synchronously parsing a parallel corpus, using a grammar with lexical translation probabilities at the leaves and a simple grammar with a single nonterminal providing the tree structure. While this grammar did not represent traditional syntactic categories such as verb phrases and noun phrases, it served to restrict the word-level alignments considered by the system to those allowable by reordering operations on binary trees. Yamada and Knight (2001) present an algorithm for estimating probabilistic parameters for a similar model which represents translation as a sequence of re-ordering operations over children of nodes in a syntactic tree, using automatic parser output for the initial tree structures. This gives the translation model more information about the structure of the source language, and further constrains the reorderings to match not just a possible bracketing as in Wu (1997), but the specific bracketing of the parse tree provided. Recent models of alignment have attempted to exploit syntactic information from both languages b"
W05-0904,J00-1004,0,0.0209921,"easures, which compute overlap in the same way as does BLEU. For example, if the same subtree occurs 10 times in both the hypothesis and the reference, this contributes a term of 100 to the dot product, rather than 10 in the clipped count used by BLEU and by our subtree metric STM. 2.1 Dependency-Based Metrics Dependency trees consist of trees of head-modifier relations with a word at each node, rather than just at the leaves. Dependency trees were found to correspond better across translation pairs than constituent trees by Fox (2002), and form the basis of the machine translation systems of Alshawi et al. (2000) reference: hypothesis: S NP NP VP PRON V NP ART S VP PRON N V NP PRON Figure 2: Examples for the Computation of STM and Lin (2004). We derived dependency trees from the constituent trees by applying the deterministic headword extraction rules used by the parser of Collins (1999). For the example of the reference syntax tree in Figure 2, the whole tree with the root S represents a sentence; and the subtree NP→ART N represents a noun phrase. Then for every node in the syntax tree, we can determine its headword by its syntactic structure; from the subtree NP→ART N, for example, the headword sele"
W05-0904,2003.mtsummit-papers.6,0,0.018036,"25 Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation c and/or Summarization, pages 25–32, Ann Arbor, June 2005. 2005 Association for Computational Linguistics reference: S NP VP PRON V NP ART hypothesis 1: N S NP PRON hypothesis 2: S NP VP V NP ART ART N NP VP PRON V N Figure 1: Syntax Trees of the Examples often contains roughly the correct words and concepts, but does not form a coherent sentence. Often the intended meaning can be inferred; often it cannot. Evidence that we are reaching the limits of ngram based evaluation was provided by Charniak et al. (2003), who found that a syntax-based language model improved the fluency and semantic accuracy of their system, but lowered their BLEU score. With the progress of MT research in recent years, we are not satisfied with the getting correct words in the translations; we also expect them to be wellformed and more readable. This presents new challenges to MT evaluation. As discussed above, the existing word-based metrics can not give a clear evaluation for the hypothesis’ fluency. For example, in the BLEU metric, the overlapping fractions of n-grams with more than one word are considered as a kind of me"
W05-0904,W02-1039,0,0.0153655,"product also weights individual features differently than our other measures, which compute overlap in the same way as does BLEU. For example, if the same subtree occurs 10 times in both the hypothesis and the reference, this contributes a term of 100 to the dot product, rather than 10 in the clipped count used by BLEU and by our subtree metric STM. 2.1 Dependency-Based Metrics Dependency trees consist of trees of head-modifier relations with a word at each node, rather than just at the leaves. Dependency trees were found to correspond better across translation pairs than constituent trees by Fox (2002), and form the basis of the machine translation systems of Alshawi et al. (2000) reference: hypothesis: S NP NP VP PRON V NP ART S VP PRON N V NP PRON Figure 2: Examples for the Computation of STM and Lin (2004). We derived dependency trees from the constituent trees by applying the deterministic headword extraction rules used by the parser of Collins (1999). For the example of the reference syntax tree in Figure 2, the whole tree with the root S represents a sentence; and the subtree NP→ART N represents a noun phrase. Then for every node in the syntax tree, we can determine its headword by it"
W05-0904,2004.tmi-1.8,0,0.354606,"oduces the notion of information weights, which indicate that rarely occurring n-grams count more than those frequently occurring ones in the evaluation (Doddington, 2002). BLEU and NIST have been shown to correlate closely with human judgments in ranking MT systems with different qualities (Papineni et al., 2002; Doddington, 2002). In the 2003 Johns Hopkins Workshop on Speech and Language Engineering, experiments on MT evaluation showed that BLEU and NIST do not correlate well with human judgments at the sentence level, even when they correlate well over large test sets (Blatz et al., 2003). Kulesza and Shieber (2004) use a machine learning approach to improve the correlation at the sentence level. Their method, based on the assumption that higher classification accuracy in discriminating human- from machine-generated translations will yield closer correlation with human judgments, uses support vector machine (SVM) based learning to weight multiple metrics such as BLEU, NIST, and WER (minimal word error rate). The SVM is trained for differentiating the MT hypothesis and the professional human translations, and then the distance from the hypothesis’s metric vector to the hyper-plane of the trained SVM is ta"
W05-0904,C04-1090,0,0.00724965,"eference, this contributes a term of 100 to the dot product, rather than 10 in the clipped count used by BLEU and by our subtree metric STM. 2.1 Dependency-Based Metrics Dependency trees consist of trees of head-modifier relations with a word at each node, rather than just at the leaves. Dependency trees were found to correspond better across translation pairs than constituent trees by Fox (2002), and form the basis of the machine translation systems of Alshawi et al. (2000) reference: hypothesis: S NP NP VP PRON V NP ART S VP PRON N V NP PRON Figure 2: Examples for the Computation of STM and Lin (2004). We derived dependency trees from the constituent trees by applying the deterministic headword extraction rules used by the parser of Collins (1999). For the example of the reference syntax tree in Figure 2, the whole tree with the root S represents a sentence; and the subtree NP→ART N represents a noun phrase. Then for every node in the syntax tree, we can determine its headword by its syntactic structure; from the subtree NP→ART N, for example, the headword selection rules chose the headword of NP to be word corresponding to the POS N in the subtree, and the other child, which corresponds t"
W05-0904,P03-1021,0,0.00387597,"om length 1 to N for i = 1 to N for every node n in T if i == 1 add n’s word to n’s 1 word headword chains; else for every direct child c of n for every i-1 words headword chain hc of c newchain = joint(n’s word, hc); add newchain to the i words headword chains of n; endfor endfor endif endfor endfor Figure 3: Algorithm for Extracting the Headword Chains The human judgments, on a scale of 1 to 5, were collected at the 2003 Johns Hopkins Speech and Language Summer Workshop, which tells the overall quality of the MT hypotheses. The translations were generated by the alignment template system of Och (2003). This testing set is called JHU testing set in this paper. The other set of testing data is from MT evaluation workshop at ACL05. Three sets of human translations (E01, E03, E04) are selected as the references, and the outputs of seven MT systems (E9 E11 E12 E14 E15 E17 E22) are used for testing the performance of our syntactic metrics. Each set of MT translations contains 929 English sentences, each of which is associated with human judgments for its fluency and adequacy. The fluency and adequacy scores both range from 1 to 5. 3.1 Sentence-level Evaluation Our syntactic metrics are motivated"
W05-0904,P02-1040,0,0.101864,"n computing similarity between output and reference. Our results show that adding syntactic information to the evaluation metric improves both sentence-level and corpus-level correlation with human judgments. 1 Introduction Evaluation has long been a stumbling block in the development of machine translation systems, due to the simple fact that there are many correct translations for a given sentence. Human evaluation of system output is costly in both time and money, leading to the rise of automatic evaluation metrics in recent years. The most commonly used automatic evaluation metrics, BLEU (Papineni et al., 2002) and NIST (Doddington, 2002), are based on the assumption that “The closer a machine translation is to a professional human translation, the better it is” (Papineni et al., 2002). For every hypothesis, BLEU computes the fraction of n-grams which also appear in the reference sentences, as well as a brevity penalty. NIST uses a similar strategy to BLEU but further considers that n-grams with different frequency should be treated differently in the evaluation. It introduces the notion of information weights, which indicate that rarely occurring n-grams count more than those frequently occurring o"
W05-0904,J03-4003,0,\N,Missing
W05-0904,C04-1046,0,\N,Missing
W05-0904,N04-1021,1,\N,Missing
W05-1507,P97-1003,0,0.0620211,"m number of interacting variables is 4, implying that the algorithmic complexity is O(n4 ) after binarizing the factors cleverly. The intermediate result max [β(B[i, k, h0 ]) · P (A[h] → B[h0 ]C[h])] 0 h ,B A C[h] k can be represented pictorially as i . The same trick works for the second max term in Equation 1. The intermediate result coming from binarizing the second term can be visualized as A[h] → B[h]C[h0 ] or A[h] → B[h0 ]C[h] A depending on which child is the head child that agrees with the parent on head word selection. Bilexical CFG is at the heart of most modern statistical parsers (Collins, 1997; Charniak, 1997), because the statistics associated with word-specific rules are more informative for disambiguation purposes. If we use A[i, j, h] to represent a lexicalized constituent, β(·) to represent the Viterbi score function applicable to any constituent, and P (·) to represent the rule probability function applicable to any rule, Figure 2 shows the equation for the dynamic programming computation of the Viterbi parse. The two terms of the outermost max operator are symmetric cases for heads coming from left and right. Containing five free variables i,j,k,h0 ,h, ranging over 1 to n, t"
W05-1507,P99-1059,0,0.0890971,"Bilexical Parsing A traditional CFG generates words at the bottom of a parse tree and uses nonterminals as abstract representations of substrings to build higher level tree nodes. Nonterminals can be made more specific to the actual substrings they are covering by associating a representative word from the nonterminal’s yield. When the maximum number of lexicalized nonterminals in any rule is two, a CFG is bilexical. A typical bilexical CFG in Chomsky normal form has two types of rule templates: instantiated in n5 possible ways, implying that the complexity of the parsing algorithm is O(n5 ). Eisner and Satta (1999) pointed out we don’t have to enumerate k and h0 simultaneously. The trick, shown in mathematical form in Figure 2 (bottom) is very simple. When maximizing over h0 , j is irrelevant. After getting the intermediate result of maximizing over h0 , we have one less free variable than before. Throughout the two steps, the maximum number of interacting variables is 4, implying that the algorithmic complexity is O(n4 ) after binarizing the factors cleverly. The intermediate result max [β(B[i, k, h0 ]) · P (A[h] → B[h0 ]C[h])] 0 h ,B A C[h] k can be represented pictorially as i . The same trick works"
W05-1507,N03-1021,0,0.0689172,"se A[i, j, h] to represent a lexicalized constituent, β(·) to represent the Viterbi score function applicable to any constituent, and P (·) to represent the rule probability function applicable to any rule, Figure 2 shows the equation for the dynamic programming computation of the Viterbi parse. The two terms of the outermost max operator are symmetric cases for heads coming from left and right. Containing five free variables i,j,k,h0 ,h, ranging over 1 to n, the length of input sentence, both terms can be 67 B[h] k j. The shape of the intermediate results gave rise to the nickname of “hook”. Melamed (2003) discussed the applicability of the hook trick for parsing bilexical multitext grammars. The analysis of the hook trick in this section shows that it is essentially an algebraic manipulation. We will formulate the ITG Viterbi decoding algorithm in a dynamic programming equation in the following section and apply the same algebraic manipulation to produce hooks that are suitable for ITG decoding. 4 Hook Trick for ITG Decoding We start from the bigram case, in which each decoding constituent keeps a left boundary word and X X [Y u11 u12 &lt;Y Z] v11 v12 u21 u22 s S u21 u22 v21 v22 t Z> v21 v22 u11"
W05-1507,P96-1021,0,0.532503,"g the chart with an item for each possible translation of each foreign word in f , and then applying ITG rules from the bottom up. However, ITG’s independence assumptions are too strong to use the ITG probability alone for machine translation. In particular, the context-free assumption that each foreign word’s translation is chosen independently will lead to simply choosing each foreign word’s single most probable English translation with no reordering. In practice it is beneficial to combine the probability given by ITG with a local m-gram language model for English: and q e 2.1 ITG Decoding Wu (1996) presented a polynomial-time algorithm for decoding ITG combined with an m-gram lan66 e∗ = argmax max P (e, f, q)Plm (e)α e q with some constant language model weight α. The language model will lead to more fluent output by influencing both the choice of English words and the reordering, through the choice of straight or inverted rules. While the use of a language model complicates the CKY-based algorithm for finding the best translation, a dynamic programming solution is still possible. We extend the algorithm by storing in each chart item the English boundary words that will affect the m-gra"
W05-1507,J97-3002,0,0.829471,"en translation and monolingual parsing with lexicalized grammars. Chart items in translation must be augmented with words from the output language in order to capture language model state. This can be thought of as a form of lexicalization with some similarity to that of head-driven lexicalized grammars, despite being unrelated to any notion of syntactic head. We show 1 We speak of m-gram language models to avoid confusion with n, which here is the length of the input sentence for translation. Machine Translation using Inversion Transduction Grammar The Inversion Transduction Grammar (ITG) of Wu (1997) is a type of context-free grammar (CFG) for generating two languages synchronously. To model the translational equivalence within a sentence pair, ITG employs a synchronous rewriting mechanism to relate two sentences recursively. To deal with the syntactic divergence between two languages, ITG allows the inversion of rewriting order going from one language to another at any recursive level. ITG in Chomsky normal form consists of unary production rules that are responsible for generating word pairs: X → e/f X → e/ X → /f where e is a source language word, f is a foreign language word, and"
W05-1526,C04-1055,1,0.772927,"alogue. We show that our augmented parser produces significantly fewer constituents than the baseline system and achieves comparable bracketing accuracy, even yielding slight improvements for longer sentences. 1 Experiments Introduction Unification parsers have problems with efficiency and selecting the best parse. Lexically-conditioned statistics as used by Collins (1999) may provide a solution. They have been used in three ways: as a postprocess for parse selection (Toutanova et al., 2005; Riezler et al., 2000; Riezler et al., 2002), a preprocess to find more probable bracketing structures (Swift et al., 2004), and online to rank each constituent produced, as in Tsuruoka et al. (2004) and this experiment. The TRIPS parser (Allen et al., 1996) is a unification parser using an HPSG-inspired grammar and hand-tuned weights for each rule. In our augmented system (Aug-TRIPS), we replaced these weights with a lexically-conditioned model based on the adaptation of Collins used by Bikel (2002), allowing more efficiency and (in some cases) better selection. Aug-TRIPS retains the same grammar and lexicon as TRIPS, but uses its statistical model to determine the order in which unifications are attempted. (trai"
W05-1526,W04-0214,1,0.849773,"Missing"
W05-1526,P00-1061,0,\N,Missing
W05-1526,W00-1320,0,\N,Missing
W05-1526,J03-4003,0,\N,Missing
W05-1526,P96-1009,1,\N,Missing
W05-1526,P02-1035,0,\N,Missing
W06-1627,J93-2003,0,0.00603012,"ents are possible, and these constraints have been shown to be a good match for real bitext data (Zens and Ney, 2003). A major motivation for the introduction of ITG was the existence of polynomial-time algorithms both for alignment and translation. Alignment, whether for training a translation model using EM or for finding the Viterbi alignment of test data, is O(n6 ) (Wu, 1997), while translation (decoding) is O(n7 ) using a bigram language model, and O(n11 ) with trigrams. While polynomial-time algorithms are a major improvement over the NPcomplete problems posed by the alignment models of Brown et al. (1993), the degree of these polyno2 Inversion Transduction Grammar An Inversion Transduction Grammar can generate pairs of sentences in two languages by recursively applying context-free bilingual production rules. Most work on ITG has focused on the 2-normal form, which consists of unary production rules that are responsible for generating word pairs: X → e/f 224 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 224–231, c Sydney, July 2006. 2006 Association for Computational Linguistics source node to the destination node. The cost in this s"
W06-1627,W98-1115,0,0.0284701,"anslation decoding. We also combine the dynamic programming hook trick with A* search for decoding. These techniques make it possible to find optimal alignments much more quickly, and make it possible to find optimal translations for the first time. Even in the presence of pruning, we are able to achieve higher BLEU scores with the same amount of computation. 1 Our search heuristics are a conservative estimate of the outside probability of a bitext cell in the complete synchronous parse. Some estimate of this outside probability is a common element of modern statistical (monolingual) parsers (Charniak et al., 1998; Collins, 1999), and recent work has developed heuristics that are admissible for A* search, guaranteeing that the optimal parse will be found (Klein and Manning, 2003). We extend this type of outside probability estimate to include both word translation and n-gram language model probabilities. These measures have been used to guide search in word- or phrase-based MT systems (Wang and Waibel, 1997; Och et al., 2001), but in such models optimal search is generally not practical even with good heuristics. In this paper, we show that the same assumptions that make ITG polynomial-time can be used"
W06-1627,P99-1059,0,0.0376819,"abilities. Figure 2 is the picture of the outside translations and bigrams of a particular translation hypothesis X[i, j, u, v]. Our heuristic involves precomputing two values for each word in the input string, involving forward- and backward-looking language model probabilities. For the forward looking value hf at input position n, we take a maximum over the set of words Sn that the input word tn can be translated as:   0 hf (n) = max Pt (s |tn ) max Plm (s |s) 0 4.2 Combining the Hook Trick with A* The hook trick is a factorization technique for dynamic programming. For bilexical parsing, Eisner and Satta (1999) pointed out we can reduce the complexity of parsing from O(n5 ) to O(n4 ) by combining the non-head constituents with the bilexical rules first, and then combining the resultant hook constituents with the head constituents. By doing so, the maximal number of interactive variables ranging over n is reduced from 5 to 4. For ITG decoding, we can apply a similar factors∈Sn s ∈S ization trick. We describe the bigram-integrated where: [ decoding case here, and refer to Huang et al. S= Sn (2005) for more detailed discussion. Figure 3 n shows how to decompose the expression for the is the set of all"
W06-1627,W05-1507,1,0.909662,"Missing"
W06-1627,W01-1408,0,0.045366,"side probability of a bitext cell in the complete synchronous parse. Some estimate of this outside probability is a common element of modern statistical (monolingual) parsers (Charniak et al., 1998; Collins, 1999), and recent work has developed heuristics that are admissible for A* search, guaranteeing that the optimal parse will be found (Klein and Manning, 2003). We extend this type of outside probability estimate to include both word translation and n-gram language model probabilities. These measures have been used to guide search in word- or phrase-based MT systems (Wang and Waibel, 1997; Och et al., 2001), but in such models optimal search is generally not practical even with good heuristics. In this paper, we show that the same assumptions that make ITG polynomial-time can be used to efficiently compute heuristics which guarantee us that we will find the optimal alignment or translation, while significantly speeding the search. Introduction The Inversion Transduction Grammar (ITG) of Wu (1997) is a syntactically motivated algorithm for producing word-level alignments of pairs of translationally equivalent sentences in two languages. The algorithm builds a synchronous parse tree for both sente"
W06-1627,P97-1047,0,0.028531,"ive estimate of the outside probability of a bitext cell in the complete synchronous parse. Some estimate of this outside probability is a common element of modern statistical (monolingual) parsers (Charniak et al., 1998; Collins, 1999), and recent work has developed heuristics that are admissible for A* search, guaranteeing that the optimal parse will be found (Klein and Manning, 2003). We extend this type of outside probability estimate to include both word translation and n-gram language model probabilities. These measures have been used to guide search in word- or phrase-based MT systems (Wang and Waibel, 1997; Och et al., 2001), but in such models optimal search is generally not practical even with good heuristics. In this paper, we show that the same assumptions that make ITG polynomial-time can be used to efficiently compute heuristics which guarantee us that we will find the optimal alignment or translation, while significantly speeding the search. Introduction The Inversion Transduction Grammar (ITG) of Wu (1997) is a syntactically motivated algorithm for producing word-level alignments of pairs of translationally equivalent sentences in two languages. The algorithm builds a synchronous parse"
W06-1627,P96-1021,0,0.116518,"n from the null target string of [i, i] into source language words as many times as necessary, the decoder can translate an input sentence into a longer output sentence. When there is the null symbol in the bag of candidate words, the decoder can choose to translate a word into null to decrease the output length. Both insertions and deletions are special cases of the bitext parsing items. nature of the cells, we can compute values for the inside and outside components of each cell using dynamic programming in O(n4 ) time (Zhang and Gildea, 2005). 4 A* Decoding The of ITG decoding algorithm of Wu (1996) can be viewed as a variant of the Viterbi parsing algorithm for alignment selection. The task of standard alignment is to find word level links between two fixed-order strings. In the decoding situation, while the input side is a fixed sequence of words, the output side is a bag of words to be linked with the input words and then reordered. Under the ITG constraint, if the target language substring [i, j] is translated into s1 in the source language and the target substring [j, k] is translated into s2 , then s1 and s2 must be consecutive in the source language as well and two possible orderi"
W06-1627,J97-3002,0,0.524612,"ility estimate to include both word translation and n-gram language model probabilities. These measures have been used to guide search in word- or phrase-based MT systems (Wang and Waibel, 1997; Och et al., 2001), but in such models optimal search is generally not practical even with good heuristics. In this paper, we show that the same assumptions that make ITG polynomial-time can be used to efficiently compute heuristics which guarantee us that we will find the optimal alignment or translation, while significantly speeding the search. Introduction The Inversion Transduction Grammar (ITG) of Wu (1997) is a syntactically motivated algorithm for producing word-level alignments of pairs of translationally equivalent sentences in two languages. The algorithm builds a synchronous parse tree for both sentences, and assumes that the trees have the same underlying structure but that the ordering of constituents may differ in the two languages. ITG imposes constraints on which alignments are possible, and these constraints have been shown to be a good match for real bitext data (Zens and Ney, 2003). A major motivation for the introduction of ITG was the existence of polynomial-time algorithms both"
W06-1627,P03-1019,0,0.0398662,"translation, while significantly speeding the search. Introduction The Inversion Transduction Grammar (ITG) of Wu (1997) is a syntactically motivated algorithm for producing word-level alignments of pairs of translationally equivalent sentences in two languages. The algorithm builds a synchronous parse tree for both sentences, and assumes that the trees have the same underlying structure but that the ordering of constituents may differ in the two languages. ITG imposes constraints on which alignments are possible, and these constraints have been shown to be a good match for real bitext data (Zens and Ney, 2003). A major motivation for the introduction of ITG was the existence of polynomial-time algorithms both for alignment and translation. Alignment, whether for training a translation model using EM or for finding the Viterbi alignment of test data, is O(n6 ) (Wu, 1997), while translation (decoding) is O(n7 ) using a bigram language model, and O(n11 ) with trigrams. While polynomial-time algorithms are a major improvement over the NPcomplete problems posed by the alignment models of Brown et al. (1993), the degree of these polyno2 Inversion Transduction Grammar An Inversion Transduction Grammar can"
W06-1627,J03-4003,0,\N,Missing
W06-1627,P05-1059,1,\N,Missing
W06-1627,N03-1016,0,\N,Missing
W07-0404,P05-1033,0,0.0886606,"al. (2006) discuss methods for binarizing SCFGs, ignoring the nonbinarizable grammars; in Section 2 we discuss the generalized problem of factoring to k-ary grammars for any k and formalize the problem as permutation factorization in Section 3. Introduction A number of recent syntax-based approaches to statistical machine translation make use of Synchronous Context Free Grammar (SCFG) as the underlying model of translational equivalence. Wu (1997)’s Inversion Transduction Grammar, as well as tree-transformation models of translation such as Yamada and Knight (2001), Galley et al. (2004), and Chiang (2005) all fall into this category. A crucial question for efficient computation in approaches based on SCFG is the length of the grammar rules. Grammars with longer rules can represent a larger set of reorderings between languages (Aho In Section 4, we describe an O(k · n) left-toright shift-reduce algorithm for analyzing permutations that can be k-arized. Its time complexity becomes O(n2 ) when k is not specified beforehand and the minimal k is to be discovered. Instead of linearly shifting in one number at a time, Gildea et al. (2006) employ a balanced binary tree as the control structure, produc"
W07-0404,N04-1035,0,0.0767373,"ear in the rules. Zhang et al. (2006) discuss methods for binarizing SCFGs, ignoring the nonbinarizable grammars; in Section 2 we discuss the generalized problem of factoring to k-ary grammars for any k and formalize the problem as permutation factorization in Section 3. Introduction A number of recent syntax-based approaches to statistical machine translation make use of Synchronous Context Free Grammar (SCFG) as the underlying model of translational equivalence. Wu (1997)’s Inversion Transduction Grammar, as well as tree-transformation models of translation such as Yamada and Knight (2001), Galley et al. (2004), and Chiang (2005) all fall into this category. A crucial question for efficient computation in approaches based on SCFG is the length of the grammar rules. Grammars with longer rules can represent a larger set of reorderings between languages (Aho In Section 4, we describe an O(k · n) left-toright shift-reduce algorithm for analyzing permutations that can be k-arized. Its time complexity becomes O(n2 ) when k is not specified beforehand and the minimal k is to be discovered. Instead of linearly shifting in one number at a time, Gildea et al. (2006) employ a balanced binary tree as the contro"
W07-0404,P06-2036,1,0.402249,"anslation such as Yamada and Knight (2001), Galley et al. (2004), and Chiang (2005) all fall into this category. A crucial question for efficient computation in approaches based on SCFG is the length of the grammar rules. Grammars with longer rules can represent a larger set of reorderings between languages (Aho In Section 4, we describe an O(k · n) left-toright shift-reduce algorithm for analyzing permutations that can be k-arized. Its time complexity becomes O(n2 ) when k is not specified beforehand and the minimal k is to be discovered. Instead of linearly shifting in one number at a time, Gildea et al. (2006) employ a balanced binary tree as the control structure, producing an algorithm similar in spirit to merge-sort with a reduced time complexity of O(n log n). However, both algorithms rely on reduction tests on emerging spans which involve redundancies with the spans that have already been tested. 25 Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 25–32, c Rochester, New York, April 2007. 2007 Association for Computational Linguistics Uno and Yagiura (2000) describe a clever algorithm for the problem of finding all common intervals o"
W07-0404,H05-1101,0,0.0898736,"les as much as possible. This paper focuses on converting an SCFG to the equivalent grammar with smallest possible maximum rule size. The algorithm processes each rule in the input grammar independently, and determines whether the rule can be factored into smaller SCFG rules by analyzing the rule’s permutation π. As an example, given the input rule: [ X → A(1) B (2) C (3) D(4) E (5) F (6) G(7) , X → E (5) G(7) D(4) F (6) C (3) A(1) B (2) ] we consider the associated permutation: We begin by describing the synchronous CFG formalism, which is more rigorously defined by Aho and Ullman (1972) and Satta and Peserico (2005). We adopt the SCFG notation of Satta and Peserico (2005). Superscript indices in the right-hand side of grammar rules: (5, 7, 4, 6, 3, 1, 2) We determine that this permutation can be factored into the following permutation tree: (2,1) (π(1)) (1,2) (2,1) (2,4,1,3) (1) (1) 3 1 2 (π(n)) X → X1 ...Xn(n) , Xπ(1) ...Xπ(n) 5 indicate that the nonterminals with the same index are linked across the two languages, and will eventually be rewritten by the same rule application. Each Xi is a variable which can take the value of any nonterminal in the grammar. We say an SCFG is n-ary if and only if the max"
W07-0404,P06-1123,0,0.711946,"hop on Syntax and Structure in Statistical Translation, pages 25–32, c Rochester, New York, April 2007. 2007 Association for Computational Linguistics Uno and Yagiura (2000) describe a clever algorithm for the problem of finding all common intervals of two permutations in time O(n + K), where K is the number of common intervals, which can itself be Ω(n2 ). In Section 5, we adapt their approach to the problem of factoring SCFGs, and show that, given this problem definition, running time can be improved to O(n), the optimum given the time needed to read the input permutation. The methodology in Wellington et al. (2006) measures the complexity of word alignment using the number of gaps that are necessary for their synchronous parser which allows discontinuous spans to succeed in parsing. In Section 6, we provide a more direct measurement using the minimal branching factor yielded by the permutation factorization algorithm. 2 Synchronous CFG and Synchronous Parsing n-ary SCFG, the parsing complexity can be as high as O(N n+4 ). The reason is even if we binarize on one side to maintain 3 indices, for many unfriendly permutations, at most n + 1 boundary variables in the other language are necessary. The fact th"
W07-0404,J97-3002,0,0.927362,"with maximum rule length n into a simpler grammar with a maximum of k nonterminals in any one rule, if not all n! permutations appear in the rules. Zhang et al. (2006) discuss methods for binarizing SCFGs, ignoring the nonbinarizable grammars; in Section 2 we discuss the generalized problem of factoring to k-ary grammars for any k and formalize the problem as permutation factorization in Section 3. Introduction A number of recent syntax-based approaches to statistical machine translation make use of Synchronous Context Free Grammar (SCFG) as the underlying model of translational equivalence. Wu (1997)’s Inversion Transduction Grammar, as well as tree-transformation models of translation such as Yamada and Knight (2001), Galley et al. (2004), and Chiang (2005) all fall into this category. A crucial question for efficient computation in approaches based on SCFG is the length of the grammar rules. Grammars with longer rules can represent a larger set of reorderings between languages (Aho In Section 4, we describe an O(k · n) left-toright shift-reduce algorithm for analyzing permutations that can be k-arized. Its time complexity becomes O(n2 ) when k is not specified beforehand and the minimal"
W07-0404,P01-1067,0,0.0514932,"ot all n! permutations appear in the rules. Zhang et al. (2006) discuss methods for binarizing SCFGs, ignoring the nonbinarizable grammars; in Section 2 we discuss the generalized problem of factoring to k-ary grammars for any k and formalize the problem as permutation factorization in Section 3. Introduction A number of recent syntax-based approaches to statistical machine translation make use of Synchronous Context Free Grammar (SCFG) as the underlying model of translational equivalence. Wu (1997)’s Inversion Transduction Grammar, as well as tree-transformation models of translation such as Yamada and Knight (2001), Galley et al. (2004), and Chiang (2005) all fall into this category. A crucial question for efficient computation in approaches based on SCFG is the length of the grammar rules. Grammars with longer rules can represent a larger set of reorderings between languages (Aho In Section 4, we describe an O(k · n) left-toright shift-reduce algorithm for analyzing permutations that can be k-arized. Its time complexity becomes O(n2 ) when k is not specified beforehand and the minimal k is to be discovered. Instead of linearly shifting in one number at a time, Gildea et al. (2006) employ a balanced bin"
W07-0404,N06-1033,1,0.868868,"related problem of finding all common intervals of two permutations, we achieve a linear time algorithm for the permutation factorization problem. We also use the algorithm to analyze the maximum SCFG rule length needed to cover hand-aligned data from various language pairs. 1 However, parsing complexity depends not only on rule length, but also on the specific permutations represented by the individual rules. It may be possible to factor an SCFG with maximum rule length n into a simpler grammar with a maximum of k nonterminals in any one rule, if not all n! permutations appear in the rules. Zhang et al. (2006) discuss methods for binarizing SCFGs, ignoring the nonbinarizable grammars; in Section 2 we discuss the generalized problem of factoring to k-ary grammars for any k and formalize the problem as permutation factorization in Section 3. Introduction A number of recent syntax-based approaches to statistical machine translation make use of Synchronous Context Free Grammar (SCFG) as the underlying model of translational equivalence. Wu (1997)’s Inversion Transduction Grammar, as well as tree-transformation models of translation such as Yamada and Knight (2001), Galley et al. (2004), and Chiang (200"
W08-0308,A00-2018,0,0.0257794,"is in an agent role. Table 1 shows the TREE-based weights of the 4 translation templates, computed based on our training corpus. This shows that the difference caused by the roles of NP is significant. 4 Experiment We used 74,597 pairs of English and Chinese sentences in the FBIS data set as our experimental data, which are further divided into 500 test sentence pairs, 500 development sentence pairs and 73597 training sentence pairs. The test set and development set are selected as those sentences having fewer than 25 words on the Chinese side. The translation is from English to Chinese, and Charniak (2000)’s parser, trained on the Penn Treebank, is used to generate the syntax trees for the English side. The weights of the MT components are optimized based on the development set using a gridbased line search. The Chinese sentence from the selected pair is used as the single reference to tune and evaluate the MT system with word-based BLEU-4 (Papineni et al., 2002). Huang et al. (2006) used character-based BLEU as a way of normalizing inconsistent Chinese word segmentation, but we avoid this problem as the training, development, and test data are from the same source. 4.1 Syntax-Based System The"
W08-0308,P07-1003,0,0.0400644,"to the TTS transducer and introduced an EM algorithm to estimate the probability of TTS templates based on a bilingual corpus with one side parsed. Liu et al. (2006) and Huang et al. (2006) then used the TTS transducer on the task of Chineseto-English and English-to-Chinese translation, respectively, and achieved decent performance. Despite the progress SSMT has achieved, it is still a developing field with many problems unsolved. For example, the word alignment computed by GIZA++ and used as a basis to extract the TTS templates in most SSMT systems has been observed to be a problem for SSMT (DeNero and Klein, 2007; May and Knight, 2007), due to the fact that the word-based alignment models are not aware of the syntactic structure of the sentences and could produce many syntax-violating word alignments. Approaches have been proposed recently towards getting better word alignment and thus better TTS templates, such as encoding syntactic structure information into the HMM-based word alignment model DeNero and Klein (2007), and build62 Proceedings of the Third Workshop on Statistical Machine Translation, pages 62–69, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics ing a syn"
W08-0308,N04-1035,0,0.832527,"target language based on TTS templates. In synchronous SSMT, TTS templates are used similar to the context free grammar used in the standard CYK parser, thus the syntax is part of the output and can be thought of as a constraint on the translation process. In the TTS transducer, since the parse tree is given, syntax can be thought of as an additional feature of the input to be used in the translation. The idea of synchronous SSMT can be traced back to Wu (1997)’s Stochastic Inversion Transduction Grammars. A systematic method for extracting TTS templates from parallel corpora was proposed by Galley et al. (2004), and later binarized by Zhang et al. (2006) for high efficiency and accuracy. In the other track, the TTS transducer originated from the tree transducer proposed by Rounds (1970) and Thatcher (1970) independently. Graehl and Knight (2004) generalized the tree transducer to the TTS transducer and introduced an EM algorithm to estimate the probability of TTS templates based on a bilingual corpus with one side parsed. Liu et al. (2006) and Huang et al. (2006) then used the TTS transducer on the task of Chineseto-English and English-to-Chinese translation, respectively, and achieved decent perfor"
W08-0308,P06-1121,0,0.374744,"s, a syntactic alignment framework integrating the insertion of unaligned target words, and subtree-based ngram model addressing the tree decomposition probability. Empirical results show that these methods improve the performance of a TTS transducer based on the standard BLEU4 metric. We also experiment with semantic labels in a TTS transducer, and achieve improvement over our baseline system. 1 Introduction Syntax-based statistical machine translation (SSMT) has achieved significant progress during recent years, with two threads developing simultaneously: the synchronous parsing-based SSMT (Galley et al., 2006; May and Knight, 2007) and the tree-to-string (TTS) transducer (Liu et al., 2006; Huang et al., 2006). Synchronous SSMT here denotes the systems which accept a source sentence as the input and generate the translation and the syntactic structure for both the source and the translation simultaneously. Such systems are sometimes also called TTS transducers, but in this paper, TTS transducer refers to the system which starts with the syntax tree of a source sentence and recursively transforms the tree to the target language based on TTS templates. In synchronous SSMT, TTS templates are used simi"
W08-0308,N04-1014,0,0.437706,"e translation process. In the TTS transducer, since the parse tree is given, syntax can be thought of as an additional feature of the input to be used in the translation. The idea of synchronous SSMT can be traced back to Wu (1997)’s Stochastic Inversion Transduction Grammars. A systematic method for extracting TTS templates from parallel corpora was proposed by Galley et al. (2004), and later binarized by Zhang et al. (2006) for high efficiency and accuracy. In the other track, the TTS transducer originated from the tree transducer proposed by Rounds (1970) and Thatcher (1970) independently. Graehl and Knight (2004) generalized the tree transducer to the TTS transducer and introduced an EM algorithm to estimate the probability of TTS templates based on a bilingual corpus with one side parsed. Liu et al. (2006) and Huang et al. (2006) then used the TTS transducer on the task of Chineseto-English and English-to-Chinese translation, respectively, and achieved decent performance. Despite the progress SSMT has achieved, it is still a developing field with many problems unsolved. For example, the word alignment computed by GIZA++ and used as a basis to extract the TTS templates in most SSMT systems has been ob"
W08-0308,2006.amta-papers.8,0,0.0768274,"sed ngram model addressing the tree decomposition probability. Empirical results show that these methods improve the performance of a TTS transducer based on the standard BLEU4 metric. We also experiment with semantic labels in a TTS transducer, and achieve improvement over our baseline system. 1 Introduction Syntax-based statistical machine translation (SSMT) has achieved significant progress during recent years, with two threads developing simultaneously: the synchronous parsing-based SSMT (Galley et al., 2006; May and Knight, 2007) and the tree-to-string (TTS) transducer (Liu et al., 2006; Huang et al., 2006). Synchronous SSMT here denotes the systems which accept a source sentence as the input and generate the translation and the syntactic structure for both the source and the translation simultaneously. Such systems are sometimes also called TTS transducers, but in this paper, TTS transducer refers to the system which starts with the syntax tree of a source sentence and recursively transforms the tree to the target language based on TTS templates. In synchronous SSMT, TTS templates are used similar to the context free grammar used in the standard CYK parser, thus the syntax is part of the output"
W08-0308,koen-2004-pharaoh,0,0.0338285,"us systems with the syntactic alignment and subtree bigram improvements added incrementally. from Chinese to English obtained from GIZA++. We have tried using alignment in the reverse direction and the union of both directions, but neither of them is better than the Chinese-to-English alignment. The reason, based on the empirical result, is simply that the Chinese-to-English alignments lead to the maximum number of templates using GHKM. A modified Kneser-Ney bigram model of the Chinese sentence is trained using SRILM (Stolcke, 2002) using the training set. For comparison, results for Pharaoh (Koehn, 2004), trained and tuned under the same condition, are also shown in Table 2. The phrases used in Pharaoh are extracted as the pair of longest continuous spans in English and Chinese based on the union of the alignments in both direction. We tried using alignments of different directions with Pharaoh, and find that the union gives the maximum number of phrase pairs and the best BLEU scores. The results show that the TTS transducers all outperform Pharaoh, and among them, the one with CFG normalization works better than the other two. We tried the three normalization methods in the syntactic alignme"
W08-0308,P06-1077,0,0.153674,"ds, and subtree-based ngram model addressing the tree decomposition probability. Empirical results show that these methods improve the performance of a TTS transducer based on the standard BLEU4 metric. We also experiment with semantic labels in a TTS transducer, and achieve improvement over our baseline system. 1 Introduction Syntax-based statistical machine translation (SSMT) has achieved significant progress during recent years, with two threads developing simultaneously: the synchronous parsing-based SSMT (Galley et al., 2006; May and Knight, 2007) and the tree-to-string (TTS) transducer (Liu et al., 2006; Huang et al., 2006). Synchronous SSMT here denotes the systems which accept a source sentence as the input and generate the translation and the syntactic structure for both the source and the translation simultaneously. Such systems are sometimes also called TTS transducers, but in this paper, TTS transducer refers to the system which starts with the syntax tree of a source sentence and recursively transforms the tree to the target language based on TTS templates. In synchronous SSMT, TTS templates are used similar to the context free grammar used in the standard CYK parser, thus the syntax"
W08-0308,P07-1089,0,0.0244427,"of both the source subtree and the target string, while CFG, as something of a compromise between TREE and ROOT, hopefully can achieve a combined effect of both of them. Compared with TREE, CFG favors the one-level context-free grammar like templates and gives penalty to the templates bigger (in terms of the depth of the source subtree) than that. It makes sense considering that the big templates, due to their sparseness in the corpus, are often assigned unduly large probabilities by TREE. 3.2 Such normalization, denoted here as TREE, is used in most tree-to-string template-based MT systems (Liu et al., 2007; Liu et al., 2006; Huang et al., 2006). Galley et al. (2006) proposed an alteration in synchronous SSMT which addresses the probability of both the source subtree and the target string #(t) #(t0 : root(t0 ) = root(t)) Syntactic Word Alignment The idea of building a syntax-based word alignment model has been explored by May and Knight (2007), with an algorithm working from the root tree node down to the leaves, recursively replacing the variables in the matched tree-to-string templates until there are no such variables left. The TTS templates they use are initially gathered using GHKM 1. Run G"
W08-0308,D07-1038,0,0.43031,"ent framework integrating the insertion of unaligned target words, and subtree-based ngram model addressing the tree decomposition probability. Empirical results show that these methods improve the performance of a TTS transducer based on the standard BLEU4 metric. We also experiment with semantic labels in a TTS transducer, and achieve improvement over our baseline system. 1 Introduction Syntax-based statistical machine translation (SSMT) has achieved significant progress during recent years, with two threads developing simultaneously: the synchronous parsing-based SSMT (Galley et al., 2006; May and Knight, 2007) and the tree-to-string (TTS) transducer (Liu et al., 2006; Huang et al., 2006). Synchronous SSMT here denotes the systems which accept a source sentence as the input and generate the translation and the syntactic structure for both the source and the translation simultaneously. Such systems are sometimes also called TTS transducers, but in this paper, TTS transducer refers to the system which starts with the syntax tree of a source sentence and recursively transforms the tree to the target language based on TTS templates. In synchronous SSMT, TTS templates are used similar to the context free"
W08-0308,J04-4002,0,0.111373,"a given derivation (decomposition) of a syntax tree, the translation probability is computed as the product of the templates which generate both 64 S Since the n-gram model tends to favor short translations, a penalty is added to the translation templates with fewer RHS symbols than LHS leaf symbols: P enalty(t) = exp(|t.RHS |− |t.LHSLeaf |) where |t.RHS |denotes the number of symbols in the RHS of t, and |t.LHSLeaf |denotes the number of leaves in the LHS of t. The length penalty is analogous to the length feature widely used in loglinear models for MT (Huang et al., 2006; Liu et al., 2006; Och and Ney, 2004). Here we distribute the penalty into TTS templates for the convenience of DP, so that we don’t have to generate the N -best list and do re-ranking. To speed up the decoding, standard beam search is used. In Figure 3, BinaryCombine denotes the targetsize binarization (Huang et al., 2006) combination. The translation candidates of the template’s variables, as well as its terminals, are combined pairwise in the order they appear in the RHS of the template. fi denotes a combined translation, whose probability is equal to the product of the probabilities of the component translations, the probabil"
W08-0308,J05-1004,1,0.294089,"ally, we compare three ways of normalizing the TTS templates based on the tree pattern, the root of the tree pattern, and the firstlevel expansion of the tree pattern respectively, in the context of hard counting and EM estimation; we present a syntactic alignment framework integrating both the template re-estimation and insertion of unaligned target words; we use a subtree-based n-gram model to address the decomposition of the syntax trees in TTS transducer (or the syntactic language model for synchronous SSMT); we use a statistical classifier to label the semantic roles defined by PropBank (Palmer et al., 2005) and try different ways of using the semantic features in a TTS transducer. We chose the TTS transducer instead of synchronous SSMT for two reasons. First, the decoding algorithm for the TTS transducer has lower computational complexity, which makes it easier to integrate a complex decomposition model. Second, the TTS Transducer can be easily integrated with semantic role features since the syntax tree is present, and it’s not clear how to do this in a synchronous SSMT system. The remainder of the paper will focus on introducing the improved TTS transducer and is organized as follows: Section"
W08-0308,P02-1040,0,0.0755713,"sentence pairs, 500 development sentence pairs and 73597 training sentence pairs. The test set and development set are selected as those sentences having fewer than 25 words on the Chinese side. The translation is from English to Chinese, and Charniak (2000)’s parser, trained on the Penn Treebank, is used to generate the syntax trees for the English side. The weights of the MT components are optimized based on the development set using a gridbased line search. The Chinese sentence from the selected pair is used as the single reference to tune and evaluate the MT system with word-based BLEU-4 (Papineni et al., 2002). Huang et al. (2006) used character-based BLEU as a way of normalizing inconsistent Chinese word segmentation, but we avoid this problem as the training, development, and test data are from the same source. 4.1 Syntax-Based System The decoding algorithm described in Figure 3 is used with the different normalization methods described in Section 3.1 and the results are summarized in Table 2. The TTS templates are extracted using GHKM based on the many-to-one alignment TREE ROOT CFG PHARAOH Baseline dev test 12.29 8.90 12.41 9.66 13.27 9.69 9.04 7.84 Syntactic Alignment dev test 13.25 9.65 13.72"
W08-0308,C69-0101,0,0.662372,"utput and can be thought of as a constraint on the translation process. In the TTS transducer, since the parse tree is given, syntax can be thought of as an additional feature of the input to be used in the translation. The idea of synchronous SSMT can be traced back to Wu (1997)’s Stochastic Inversion Transduction Grammars. A systematic method for extracting TTS templates from parallel corpora was proposed by Galley et al. (2004), and later binarized by Zhang et al. (2006) for high efficiency and accuracy. In the other track, the TTS transducer originated from the tree transducer proposed by Rounds (1970) and Thatcher (1970) independently. Graehl and Knight (2004) generalized the tree transducer to the TTS transducer and introduced an EM algorithm to estimate the probability of TTS templates based on a bilingual corpus with one side parsed. Liu et al. (2006) and Huang et al. (2006) then used the TTS transducer on the task of Chineseto-English and English-to-Chinese translation, respectively, and achieved decent performance. Despite the progress SSMT has achieved, it is still a developing field with many problems unsolved. For example, the word alignment computed by GIZA++ and used as a basis t"
W08-0308,P05-1073,0,0.0219754,"explicit modeling for the decomposition of a syntax tree in the TTS transducer (or the probability of the syntactic tree in a synchronous SSMT). Most systems simply use a uniform model (Liu et al., 2006; Huang et al., 2006) or implicitly consider it with a joint model producing both syntax trees and the translations (Galley et al., 2006). 3. Use of semantics. Using semantic features in a SSMT is a natural step along the way towards generating more refined models across languages. The statistical approach to semantic role labeling has been well studied (Xue and Palmer, 2004; Ward et al., 2004; Toutanova et al., 2005), but there is no work attempting to use such information in SSMT, to our limited knowledge. 63 This paper proposes novel methods towards solving these problems. Specifically, we compare three ways of normalizing the TTS templates based on the tree pattern, the root of the tree pattern, and the firstlevel expansion of the tree pattern respectively, in the context of hard counting and EM estimation; we present a syntactic alignment framework integrating both the template re-estimation and insertion of unaligned target words; we use a subtree-based n-gram model to address the decomposition of th"
W08-0308,N04-1030,0,0.292938,"SSMT). There is no explicit modeling for the decomposition of a syntax tree in the TTS transducer (or the probability of the syntactic tree in a synchronous SSMT). Most systems simply use a uniform model (Liu et al., 2006; Huang et al., 2006) or implicitly consider it with a joint model producing both syntax trees and the translations (Galley et al., 2006). 3. Use of semantics. Using semantic features in a SSMT is a natural step along the way towards generating more refined models across languages. The statistical approach to semantic role labeling has been well studied (Xue and Palmer, 2004; Ward et al., 2004; Toutanova et al., 2005), but there is no work attempting to use such information in SSMT, to our limited knowledge. 63 This paper proposes novel methods towards solving these problems. Specifically, we compare three ways of normalizing the TTS templates based on the tree pattern, the root of the tree pattern, and the firstlevel expansion of the tree pattern respectively, in the context of hard counting and EM estimation; we present a syntactic alignment framework integrating both the template re-estimation and insertion of unaligned target words; we use a subtree-based n-gram model to addres"
W08-0308,J97-3002,0,0.0553762,"is paper, TTS transducer refers to the system which starts with the syntax tree of a source sentence and recursively transforms the tree to the target language based on TTS templates. In synchronous SSMT, TTS templates are used similar to the context free grammar used in the standard CYK parser, thus the syntax is part of the output and can be thought of as a constraint on the translation process. In the TTS transducer, since the parse tree is given, syntax can be thought of as an additional feature of the input to be used in the translation. The idea of synchronous SSMT can be traced back to Wu (1997)’s Stochastic Inversion Transduction Grammars. A systematic method for extracting TTS templates from parallel corpora was proposed by Galley et al. (2004), and later binarized by Zhang et al. (2006) for high efficiency and accuracy. In the other track, the TTS transducer originated from the tree transducer proposed by Rounds (1970) and Thatcher (1970) independently. Graehl and Knight (2004) generalized the tree transducer to the TTS transducer and introduced an EM algorithm to estimate the probability of TTS templates based on a bilingual corpus with one side parsed. Liu et al. (2006) and Huan"
W08-0308,W04-3212,0,0.0443201,"model in synchronous SSMT). There is no explicit modeling for the decomposition of a syntax tree in the TTS transducer (or the probability of the syntactic tree in a synchronous SSMT). Most systems simply use a uniform model (Liu et al., 2006; Huang et al., 2006) or implicitly consider it with a joint model producing both syntax trees and the translations (Galley et al., 2006). 3. Use of semantics. Using semantic features in a SSMT is a natural step along the way towards generating more refined models across languages. The statistical approach to semantic role labeling has been well studied (Xue and Palmer, 2004; Ward et al., 2004; Toutanova et al., 2005), but there is no work attempting to use such information in SSMT, to our limited knowledge. 63 This paper proposes novel methods towards solving these problems. Specifically, we compare three ways of normalizing the TTS templates based on the tree pattern, the root of the tree pattern, and the firstlevel expansion of the tree pattern respectively, in the context of hard counting and EM estimation; we present a syntactic alignment framework integrating both the template re-estimation and insertion of unaligned target words; we use a subtree-based n-g"
W08-0308,N06-1033,1,0.819241,"ynchronous SSMT, TTS templates are used similar to the context free grammar used in the standard CYK parser, thus the syntax is part of the output and can be thought of as a constraint on the translation process. In the TTS transducer, since the parse tree is given, syntax can be thought of as an additional feature of the input to be used in the translation. The idea of synchronous SSMT can be traced back to Wu (1997)’s Stochastic Inversion Transduction Grammars. A systematic method for extracting TTS templates from parallel corpora was proposed by Galley et al. (2004), and later binarized by Zhang et al. (2006) for high efficiency and accuracy. In the other track, the TTS transducer originated from the tree transducer proposed by Rounds (1970) and Thatcher (1970) independently. Graehl and Knight (2004) generalized the tree transducer to the TTS transducer and introduced an EM algorithm to estimate the probability of TTS templates based on a bilingual corpus with one side parsed. Liu et al. (2006) and Huang et al. (2006) then used the TTS transducer on the task of Chineseto-English and English-to-Chinese translation, respectively, and achieved decent performance. Despite the progress SSMT has achieve"
W08-0308,J08-3004,0,\N,Missing
W09-3815,N03-1016,0,0.0262451,"· xr ∈ P do for i ∈ (2 · · · r) do κ[xi−1 , xi ]++ return κ NP 0.6/1.0 0.4/0.67 〈a:〈〈JJ:NN〉:NN〉〉 PP the a 1.0/1.0 〈〈JJ:NN〉:NN〉 1.0/1.0 〈JJ:NN〉 NN 1.0/0.6 JJ Figure 2: A greedy binarization algorithm. The rank of a grammar is the rank of its largest rule. Our implementation updates the counts in κ more efficiently, but we present it this way for clarity. NN Figure 3: The binarized rules of Figure 1 arranged in a shared hypergraph forest. Each hyperedge is labeled with its weight before/after pushing. q, shifting a constant amount of weight d(q)−1 from q’s outgoing edges to its incoming edges. Klein and Manning (2003) describe an encoding of context-free grammar rule binarization that permits weight pushing to be applied. Their approach, however, works only with left or right binarizations whose rules can be encoded as an FST. We propose a form of weight pushing that works for arbitrary binarizations. Weight pushing across a grammar can be viewed as generalizing pushing from weighted transducers to a certain kind of weighted hypergraph. To begin, we use the following definition of a hypergraph: nonfinal hyperedges have a probability of 1, and final hyperedges have a probability equal to the that of the ori"
W09-3815,J97-2003,0,0.243152,"ecause of the way the probability of a rule is distributed among its binarized pieces: The standard approach is to place all of that probability on the top-level binarized rule, and to set the probabilities of lower binarized pieces to 1.0. Because these rules are reconstructed from the bottom up, pruning procedures do not have a good estimate of the complete cost of a rule until the entire original rule has been reconstructed. It is preferable to have this information earlier on, especially for larger rules. In this paper we adapt the technique of weight pushing for finite state transducers (Mohri, 1997) to arbitrary binarizations of context-free grammar rules. Weight pushing takes the probability (or, more generally, the weight) of a rule in the original grammar and pushes it down across the rule’s binarized pieces. This helps the parser make betWe apply the idea of weight pushing (Mohri, 1997) to CKY parsing with fixed context-free grammars. Applied after rule binarization, weight pushing takes the weight from the original grammar rule and pushes it down across its binarized pieces, allowing the parser to make better pruning decisions earlier in the parsing process. This process can be view"
W09-3815,P09-2012,1,0.900638,"was produced by inducing a TSG derivation on each of the trees in the training data, from which subtree counts were read directly. These derivations were induced using a collapsed Gibbs sampler, which sampled from the posterior of a Dirichlet process (DP) defined over the subtree rewrites of each nonterminal. The DP describes a generative process that prefers small subtrees but occasionally produces larger ones; when used for inference, it essentially discovers TSG derivations that contain larger subtrees only if they are frequent in the training data, which discourages model overfitting. See Post and Gildea (2009) for more detail. We ran the sampler for 100 iterations with a stop probability of 0.7 and the DP parameter α = 100, accumulating subtree counts from the derivation state at the end of all the iterations, which corresponds to the (100, 0.7, ≤ 100) grammar from that paper. All four grammar were learned from all sentences in sections 2 to 21 of the Wall Street Journal portion of the Penn Treebank. All trees were preprocessed to remove empty nodes and nontermi• level of binarization (0,1,2+) The level of binarization refers to the height of a nonterminal in the subtree created by binarizing a CFG"
W09-3815,C04-1024,0,0.024843,"inarization (depicted) produces only 5 rules due to the fact that two of them are shared. Since the complexity of parsing with CKY is a function of the grammar size as well as the input sentence length, and since in practice parsing requires significant pruning, having a smaller grammar with maximal shared substructure among the rules is desirable. We investigate two kinds of binarization in this paper. The first is right binarization, in which nonterminal pairs are collapsed beginning from the two rightmost children and moving leftward. The second is a greedy binarization, similar to that of Schmid (2004), in which the most frequently occurring (grammar-wide) nonterminal pair is collapsed in turn, according to the algorithm given in Figure 2. Binarization must ensure that the product of the probabilities of the binarized pieces is the same as that of the original rule. The easiest way to do this is to assign each newly-created binarized rule a probability of 1.0, and give the top-level rule the complete probability of the original rule. In the following subsection, we describe a better way. 2.2 Weight pushing Spreading the weight of an original rule across its binarized pieces is complicated b"
W09-3815,P01-1010,0,0.382362,"rs to hypergraphs. We examine its effect on parsing efficiency with various binarization schemes applied to tree substitution grammars from previous work. We find that weight pushing produces dramatic improvements in efficiency, especially with small amounts of time and with large grammars. 1 Introduction Fixed grammar-parsing refers to parsing that employs grammars comprising a finite set of rules that is fixed before inference time. This is in contrast to markovized grammars (Collins, 1999; Charniak, 2000), variants of tree-adjoining grammars (Chiang, 2000), or grammars with wildcard rules (Bod, 2001), all of which allow the construction and use of rules not seen in the training data. Fixed grammars must be binarized (either explicitly or implicitly) in order to maintain the O(n3 |G|) (n the sentence length, |G |the grammar size) complexity of algorithms such as the CKY algorithm. Recently, Song et al. (2008) explored different methods of binarization of a PCFG read directly from the Penn Treebank (the Treebank PCFG), 89 Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 89–98, c Paris, October 2009. 2009 Association for Computational Linguistics ter pru"
W09-3815,D08-1018,0,0.025794,"Missing"
W09-3815,N06-1033,1,0.884574,"oth the number of rules and new nonterminals introduced, and subsequently on parsing time. This variation occurs because different binarization schemes produce different amounts of shared rules, which are rules produced during the binarization process from more than one rule in the original grammar. Increasing sharing reduces the amount of state that the parser must explore. Binarization has also been investigated in the context of parsing-based approaches to machine translation, where it has been shown that paying careful attention to the binarization scheme can produce much faster decoders (Zhang et al., 2006; Huang, 2007; DeNero et al., 2009). The choice of binarization scheme will not affect parsing results if the parser is permitted to explore the whole search space. In practice, however, this space is too large, so parsers use pruning to discard unlikely hypotheses. This presents a problem for bottom-up parsing algorithms because of the way the probability of a rule is distributed among its binarized pieces: The standard approach is to place all of that probability on the top-level binarized rule, and to set the probabilities of lower binarized pieces to 1.0. Because these rules are reconstruc"
W09-3815,A00-2018,0,0.219823,"Missing"
W09-3815,P00-1058,0,0.264512,"ed as generalizing weight pushing from transducers to hypergraphs. We examine its effect on parsing efficiency with various binarization schemes applied to tree substitution grammars from previous work. We find that weight pushing produces dramatic improvements in efficiency, especially with small amounts of time and with large grammars. 1 Introduction Fixed grammar-parsing refers to parsing that employs grammars comprising a finite set of rules that is fixed before inference time. This is in contrast to markovized grammars (Collins, 1999; Charniak, 2000), variants of tree-adjoining grammars (Chiang, 2000), or grammars with wildcard rules (Bod, 2001), all of which allow the construction and use of rules not seen in the training data. Fixed grammars must be binarized (either explicitly or implicitly) in order to maintain the O(n3 |G|) (n the sentence length, |G |the grammar size) complexity of algorithms such as the CKY algorithm. Recently, Song et al. (2008) explored different methods of binarization of a PCFG read directly from the Penn Treebank (the Treebank PCFG), 89 Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 89–98, c Paris, October 2009. 2009 Asso"
W09-3815,N09-1026,0,0.0210377,"onterminals introduced, and subsequently on parsing time. This variation occurs because different binarization schemes produce different amounts of shared rules, which are rules produced during the binarization process from more than one rule in the original grammar. Increasing sharing reduces the amount of state that the parser must explore. Binarization has also been investigated in the context of parsing-based approaches to machine translation, where it has been shown that paying careful attention to the binarization scheme can produce much faster decoders (Zhang et al., 2006; Huang, 2007; DeNero et al., 2009). The choice of binarization scheme will not affect parsing results if the parser is permitted to explore the whole search space. In practice, however, this space is too large, so parsers use pruning to discard unlikely hypotheses. This presents a problem for bottom-up parsing algorithms because of the way the probability of a rule is distributed among its binarized pieces: The standard approach is to place all of that probability on the top-level binarized rule, and to set the probabilities of lower binarized pieces to 1.0. Because these rules are reconstructed from the bottom up, pruning pro"
W09-3815,W07-0405,0,0.0195718,"les and new nonterminals introduced, and subsequently on parsing time. This variation occurs because different binarization schemes produce different amounts of shared rules, which are rules produced during the binarization process from more than one rule in the original grammar. Increasing sharing reduces the amount of state that the parser must explore. Binarization has also been investigated in the context of parsing-based approaches to machine translation, where it has been shown that paying careful attention to the binarization scheme can produce much faster decoders (Zhang et al., 2006; Huang, 2007; DeNero et al., 2009). The choice of binarization scheme will not affect parsing results if the parser is permitted to explore the whole search space. In practice, however, this space is too large, so parsers use pruning to discard unlikely hypotheses. This presents a problem for bottom-up parsing algorithms because of the way the probability of a rule is distributed among its binarized pieces: The standard approach is to place all of that probability on the top-level binarized rule, and to set the probabilities of lower binarized pieces to 1.0. Because these rules are reconstructed from the"
W09-3815,J03-4003,0,\N,Missing
W10-1406,P01-1010,0,0.0873773,"iteration (after which performance began to fall). With the empty elements, we have achieved accuracy scores that are on par with the best accuracy scores obtained parsing the English Treebank. 6 Tree substitution grammars We have shown that coarse labels and the prevalence of N ULL elements in Korean both contribute to parsing difficulty. We now turn to grammar formalisms that allow us to work with larger fragments of parse trees than the height-one rules of standard contextfree grammars. Tree substitution grammars (TSGs) have been shown to improve upon the standard English Treebank grammar (Bod, 2001) in parser accuracy, and more recently, techniques for inferring TSG subtrees in a Bayesian framework have enabled learning more efficiently representable grammars, permitting some interesting analysis (O’Donnell et al., 2009; Cohn et al., 2009; Post and Gildea, 2009). In this section, we try parsing the KTB with TSGs. We experiment with different methods of learning TSGs to see whether they can reveal any insights into the difficulties parsing Korean. 6.1 Head rules TSGs present some difficulties in learning and representation, but a simple extraction heuristic called a spinal grammar has bee"
W10-1406,P00-1058,0,0.057207,"re recently, techniques for inferring TSG subtrees in a Bayesian framework have enabled learning more efficiently representable grammars, permitting some interesting analysis (O’Donnell et al., 2009; Cohn et al., 2009; Post and Gildea, 2009). In this section, we try parsing the KTB with TSGs. We experiment with different methods of learning TSGs to see whether they can reveal any insights into the difficulties parsing Korean. 6.1 Head rules TSGs present some difficulties in learning and representation, but a simple extraction heuristic called a spinal grammar has been shown to be very useful (Chiang, 2000; Sangati and Zuidema, 2009; Post and Gildea, 2009). Spinal subtrees are extracted from a parse tree by using a set of head rules to maximally project each lexical item (a word or morpheme). Each node in the parse tree having a different head from its parent becomes the root of a new subtree, which induces a spinal TSG derivation in the parse tree (see Figure 1). A probabilistic grammar is derived by taking counts from these trees, smoothing them with counts of all depth-one rules from the same training set, and setting rule probabilities to relative frequency. This heuristic requires a set of"
W10-1406,N09-1062,0,0.0933868,"that coarse labels and the prevalence of N ULL elements in Korean both contribute to parsing difficulty. We now turn to grammar formalisms that allow us to work with larger fragments of parse trees than the height-one rules of standard contextfree grammars. Tree substitution grammars (TSGs) have been shown to improve upon the standard English Treebank grammar (Bod, 2001) in parser accuracy, and more recently, techniques for inferring TSG subtrees in a Bayesian framework have enabled learning more efficiently representable grammars, permitting some interesting analysis (O’Donnell et al., 2009; Cohn et al., 2009; Post and Gildea, 2009). In this section, we try parsing the KTB with TSGs. We experiment with different methods of learning TSGs to see whether they can reveal any insights into the difficulties parsing Korean. 6.1 Head rules TSGs present some difficulties in learning and representation, but a simple extraction heuristic called a spinal grammar has been shown to be very useful (Chiang, 2000; Sangati and Zuidema, 2009; Post and Gildea, 2009). Spinal subtrees are extracted from a parse tree by using a set of head rules to maximally project each lexical item (a word or morpheme). Each node in t"
W10-1406,P97-1003,0,0.2432,"y taking counts from these trees, smoothing them with counts of all depth-one rules from the same training set, and setting rule probabilities to relative frequency. This heuristic requires a set of head rules, which we present in Table 6. As an evaluation of our rules, we list in Table 7 the accuracy results for parsing with spinal grammars extracted using the head rules we developed as well as with two head rule heuristics (head-left and head-right). As a point of comparison, we provide the same results for English, using the standard Magerman/Collins head rules for English (Magerman, 1995; Collins, 1997). Function tags were retained for Korean but not for English. We observe a number of things from Table 7. First, the relative performance of the head-left and 54 NT S VV VX ADJP CV LV NP VJ VP ⋆ RC SFN EFN EFN EFN EFN EFN EFN EFN EFN ⋆ rule second rightmost child rightmost XSV rightmost VJ or CO rightmost VJ rightmost VV rightmost VV rightmost CO rightmost XSV or XSJ rightmost VX, XSV, or VV rightmost child Table 6: Head rules for the Korean Treebank. NT is the nonterminal whose head is being determined, RC identifies the label of its rightmost child. The default is to take the rightmost child"
W10-1406,A00-2016,0,0.0309149,"orphological and syntactic information. The corpus contains roughly 5K sentences, 132K words, and 14K unique morphemes. The syntactic bracketing rules are mostly the same as the previous version of the treebank (Han et al., 2001) and the phrase structure annotation schemes used are very similar to the ones used in Penn English treebank. The Korean Treebank is constructed over text that has been morphologically analyzed; not only is the text tokenized into morphemes, but all allomorphs are neutralized. To our knowledge, there have been only a few papers focusing on syntactic parsing of Korean. Hermjakob (2000) implemented a shift-reduce parser for Korean trained on very limited (1K sentences) data, and Sarkar and Han (2002) used an earlier version of the Treebank to train a lexicalized tree adjoining grammar. In this paper, we conduct a range of experiments using the Korean Treebank 2.0 (hereafter, KTB) as our training data and provide analyses that reveal insights into parsing morphologically rich languages like Korean. We try to provide comparisons with English parsing using parsers trained on a similar amount of data wherever applicable. 2 Difficulties parsing Korean There are several challenges"
W10-1406,J98-4004,0,0.189834,"side nonterminal is present three or more times on its righthand side. There are only three instances of such “triple+recursive” NPs among the ∼40K trees in the training portion of the PTB, each occurring only once. NP → NP NP NP , CC NP NP → NP NP NP CC NP NP → NP NP NP NP . The KTB is an eighth of the size of this, but has fifteen instances of such NPs (listed here with their frequencies): 1 We thank one of our anonymous reviewers for bringing this to our attention. 51 Nonterminal granularity There are many ways to refine the set of nonterminals in a Treebank. A simple approach suggested by Johnson (1998) is to simply annotate each node with its parent’s label. The effect of this is to refine the distribution of each nonterminal over sequences of children according to its position in the sentence; for example, a VP beneath an SBAR node will have a different distribution over children than a VP beneath an S node. This simple technique alone produces a large improvement in English Treebank parsing. Klein and Manning (2003) expanded this idea with a series of experiments wherein they manually refined nonterminals to different degrees, which resulted in parsing accuracy rivaling that of bilexicali"
W10-1406,W04-3306,0,0.0293379,"a even greater problem than in the English Treebank. We also find that Korean’s relatively free word order does not impact parsing results as much as one might expect, but in fact the prevalence of zero pronouns accounts for a large portion of the difference between Korean and English parsing scores. 1 Introduction Korean is a head-final, agglutinative, and morphologically productive language. The language presents multiple challenges for syntactic parsing. Like some other head-final languages such as German, Japanese, and Hindi, Korean exhibits long-distance scrambling (Rambow and Lee, 1994; Kallmeyer and Yoon, 2004). Compound nouns are formed freely (Park et al., 2004), and verbs have well over 400 paradigmatic endings (Martin, 1992). Korean Treebank 2.0 (LDC2006T09) (Han and Ryu, 2005) is a subset of a Korean newswire corpus (LDC2000T45) annotated with morphological and syntactic information. The corpus contains roughly 5K sentences, 132K words, and 14K unique morphemes. The syntactic bracketing rules are mostly the same as the previous version of the treebank (Han et al., 2001) and the phrase structure annotation schemes used are very similar to the ones used in Penn English treebank. The Korean Treeba"
W10-1406,P03-1054,0,0.0364175,"ur anonymous reviewers for bringing this to our attention. 51 Nonterminal granularity There are many ways to refine the set of nonterminals in a Treebank. A simple approach suggested by Johnson (1998) is to simply annotate each node with its parent’s label. The effect of this is to refine the distribution of each nonterminal over sequences of children according to its position in the sentence; for example, a VP beneath an SBAR node will have a different distribution over children than a VP beneath an S node. This simple technique alone produces a large improvement in English Treebank parsing. Klein and Manning (2003) expanded this idea with a series of experiments wherein they manually refined nonterminals to different degrees, which resulted in parsing accuracy rivaling that of bilexicalized parsing models of the time. More recently, Petrov et al. (2006) refined techniques originally proposed by Matsuzaki et al. (2005) and Prescher SBJ OBJ COMP ADV VOC LV subject with nominative case marker complement with accusative case marker complement with adverbial postposition NP that function as adverbial phrase noun with vocative case maker NP coupled with “light” verb construction Table 2: Function tags in the"
W10-1406,I05-2034,0,0.0266992,"Missing"
W10-1406,P00-1048,0,0.0259375,"Missing"
W10-1406,P95-1037,0,0.251065,"mar is derived by taking counts from these trees, smoothing them with counts of all depth-one rules from the same training set, and setting rule probabilities to relative frequency. This heuristic requires a set of head rules, which we present in Table 6. As an evaluation of our rules, we list in Table 7 the accuracy results for parsing with spinal grammars extracted using the head rules we developed as well as with two head rule heuristics (head-left and head-right). As a point of comparison, we provide the same results for English, using the standard Magerman/Collins head rules for English (Magerman, 1995; Collins, 1997). Function tags were retained for Korean but not for English. We observe a number of things from Table 7. First, the relative performance of the head-left and 54 NT S VV VX ADJP CV LV NP VJ VP ⋆ RC SFN EFN EFN EFN EFN EFN EFN EFN EFN ⋆ rule second rightmost child rightmost XSV rightmost VJ or CO rightmost VJ rightmost VV rightmost VV rightmost CO rightmost XSV or XSJ rightmost VX, XSV, or VV rightmost child Table 6: Head rules for the Korean Treebank. NT is the nonterminal whose head is being determined, RC identifies the label of its rightmost child. The default is to take the"
W10-1406,P05-1010,0,0.0263646,"each nonterminal over sequences of children according to its position in the sentence; for example, a VP beneath an SBAR node will have a different distribution over children than a VP beneath an S node. This simple technique alone produces a large improvement in English Treebank parsing. Klein and Manning (2003) expanded this idea with a series of experiments wherein they manually refined nonterminals to different degrees, which resulted in parsing accuracy rivaling that of bilexicalized parsing models of the time. More recently, Petrov et al. (2006) refined techniques originally proposed by Matsuzaki et al. (2005) and Prescher SBJ OBJ COMP ADV VOC LV subject with nominative case marker complement with accusative case marker complement with adverbial postposition NP that function as adverbial phrase noun with vocative case maker NP coupled with “light” verb construction Table 2: Function tags in the Korean treebank model F1 Korean 52.78 coarse w/ function tags 56.18 English (small) 72.20 coarse w/ function tags 70.50 English (standard) 71.61 coarse w/ function tags 72.82 F1≤40 56.55 60.21 73.29 71.78 72.74 74.05 Table 3: Parser scores for Treebank PCFGs in Korean and English with and without function ta"
W10-1406,P06-1055,0,0.755068,"26 73.29 72.74 types 6.6K 5.5K 7.5K 23K tokens 194K 96K 147K 950K Table 1: Parser scores for Treebank PCFGs in Korean and English. For English, we vary the size of the training data to provide a better point of comparison against Korean. Types and tokens denote vocabulary sizes (which for Korean is the mean over the folds). was set to all words occurring more than once in its training data, with a handful of count one tokens replacing unknown words based on properties of the word’s surface form (all Korean words were placed in a single bin, and English words were binned following the rules of Petrov et al. (2006)). We report scores on the development set. We report parser accuracy scores using the standard F1 metric, which balances precision and recall of the labeled constituents recovered by the parser: 2P R/(P + R). Throughout the paper, all evaluation occurs against gold standard trees that contain no N ULL elements or nonterminal function tags or annotations, which in some cases requires the removal of those elements from parse trees output by the parser. 3.2 Treebank grammars We begin by presenting in Table 1 scores for the standard Treebank grammar, obtained by reading a standard context-free gr"
W10-1406,P09-2012,1,0.934853,"and the prevalence of N ULL elements in Korean both contribute to parsing difficulty. We now turn to grammar formalisms that allow us to work with larger fragments of parse trees than the height-one rules of standard contextfree grammars. Tree substitution grammars (TSGs) have been shown to improve upon the standard English Treebank grammar (Bod, 2001) in parser accuracy, and more recently, techniques for inferring TSG subtrees in a Bayesian framework have enabled learning more efficiently representable grammars, permitting some interesting analysis (O’Donnell et al., 2009; Cohn et al., 2009; Post and Gildea, 2009). In this section, we try parsing the KTB with TSGs. We experiment with different methods of learning TSGs to see whether they can reveal any insights into the difficulties parsing Korean. 6.1 Head rules TSGs present some difficulties in learning and representation, but a simple extraction heuristic called a spinal grammar has been shown to be very useful (Chiang, 2000; Sangati and Zuidema, 2009; Post and Gildea, 2009). Spinal subtrees are extracted from a parse tree by using a set of head rules to maximally project each lexical item (a word or morpheme). Each node in the parse tree having a d"
W10-1406,W07-2460,0,0.122936,"Missing"
W10-1406,E09-1080,0,0.0126512,"echniques for inferring TSG subtrees in a Bayesian framework have enabled learning more efficiently representable grammars, permitting some interesting analysis (O’Donnell et al., 2009; Cohn et al., 2009; Post and Gildea, 2009). In this section, we try parsing the KTB with TSGs. We experiment with different methods of learning TSGs to see whether they can reveal any insights into the difficulties parsing Korean. 6.1 Head rules TSGs present some difficulties in learning and representation, but a simple extraction heuristic called a spinal grammar has been shown to be very useful (Chiang, 2000; Sangati and Zuidema, 2009; Post and Gildea, 2009). Spinal subtrees are extracted from a parse tree by using a set of head rules to maximally project each lexical item (a word or morpheme). Each node in the parse tree having a different head from its parent becomes the root of a new subtree, which induces a spinal TSG derivation in the parse tree (see Figure 1). A probabilistic grammar is derived by taking counts from these trees, smoothing them with counts of all depth-one rules from the same training set, and setting rule probabilities to relative frequency. This heuristic requires a set of head rules, which we prese"
W10-1406,W02-2207,0,0.526071,"rphemes. The syntactic bracketing rules are mostly the same as the previous version of the treebank (Han et al., 2001) and the phrase structure annotation schemes used are very similar to the ones used in Penn English treebank. The Korean Treebank is constructed over text that has been morphologically analyzed; not only is the text tokenized into morphemes, but all allomorphs are neutralized. To our knowledge, there have been only a few papers focusing on syntactic parsing of Korean. Hermjakob (2000) implemented a shift-reduce parser for Korean trained on very limited (1K sentences) data, and Sarkar and Han (2002) used an earlier version of the Treebank to train a lexicalized tree adjoining grammar. In this paper, we conduct a range of experiments using the Korean Treebank 2.0 (hereafter, KTB) as our training data and provide analyses that reveal insights into parsing morphologically rich languages like Korean. We try to provide comparisons with English parsing using parsers trained on a similar amount of data wherever applicable. 2 Difficulties parsing Korean There are several challenges in parsing Korean compared to languages like English. At the root of many of these challenges is the fact that it i"
W10-1406,2004.jeptalnrecital-long.24,0,\N,Missing
W13-2262,D07-1031,0,0.0342055,"e and f . In other words, the sum of the posterior probabilities in each column of the small trellis is the same. Therefore, we collect word translation counts only from the last morphemes of the words in e. Variational Bayes In order to prevent overfitting, we use the Variational Bayes extension of the EM algorithm (Beal, 2003). This amounts to a small change to the M step of the original EM algorithm. We introduce Dirichlet priors α to perform an inexact normalization by applying the function f (v) = exp(ψ(v)) to the expected counts collected in the E step, where ψ is the digamma function (Johnson, 2007). The M-step update for a multinomial parameter θx|y becomes: θx|y = 499 f (E[c(x|y)] + α) P f ( j E[c(xj |y)] + α) TAM-HMM Multi-rate WordMorph HMM Morph only BLEU WORD IBM 4 Baseline TR to EN 30.82 29.48 29.98 29.13 27.91 EN to TR 23.09 22.55 22.54 21.95 21.82 0.254 0.255 0.256 0.375 0.370 AER Table 1: AER and BLEU Scores We set α to 10−20 , a very low value, to have the effect of anti-smoothing, as low values of α cause the algorithm to favor words which co-occur frequently and to penalize words that co-occur rarely. We used Dirichlet priors on morpheme translation probabilities. 4 4.1 of T"
W13-2262,N03-1017,0,0.00817684,"ly marked by an overt morpheme. For English, we use partof-speech tagged data. The number of English words is 1,033,726 and the size of the English vocabulary is 28,647. The number of Turkish words is 812,374, the size of the Turkish vocabulary is 57,249. The number of Turkish morphemes is 1,484,673 and the size of the morpheme vocabulary is 16,713. 4.2 We evaluated the performance of our model in two different ways. First, we evaluated against gold word alignments for 75 Turkish-English sentences. Table 1 shows the AER (Och and Ney, 2003) of the word alignments; we report the growdiag-final (Koehn et al., 2003) of the Viterbi alignments. Second, we used the Moses toolkit (Koehn et al., 2007) to train machine translation systems from the Viterbi alignments of our various models, and evaluated the results with BLEU (Papineni et al., 2002). In order to reduce the effect of nondeterminism, we run Moses three times per experiment setting, and report the highest BLEU scores obtained. Since the BLEU scores we obtained are close, we did a significance test on the scores (Koehn, 2004). In Table 1, the colors partition the table into equivalence classes: If two scores within the same row have different backgr"
W13-2262,P07-2045,0,0.0088783,"number of English words is 1,033,726 and the size of the English vocabulary is 28,647. The number of Turkish words is 812,374, the size of the Turkish vocabulary is 57,249. The number of Turkish morphemes is 1,484,673 and the size of the morpheme vocabulary is 16,713. 4.2 We evaluated the performance of our model in two different ways. First, we evaluated against gold word alignments for 75 Turkish-English sentences. Table 1 shows the AER (Och and Ney, 2003) of the word alignments; we report the growdiag-final (Koehn et al., 2003) of the Viterbi alignments. Second, we used the Moses toolkit (Koehn et al., 2007) to train machine translation systems from the Viterbi alignments of our various models, and evaluated the results with BLEU (Papineni et al., 2002). In order to reduce the effect of nondeterminism, we run Moses three times per experiment setting, and report the highest BLEU scores obtained. Since the BLEU scores we obtained are close, we did a significance test on the scores (Koehn, 2004). In Table 1, the colors partition the table into equivalence classes: If two scores within the same row have different background colors, then the difference between their scores is statistically significant"
W13-2262,W04-3250,0,0.00986126,"ish-English sentences. Table 1 shows the AER (Och and Ney, 2003) of the word alignments; we report the growdiag-final (Koehn et al., 2003) of the Viterbi alignments. Second, we used the Moses toolkit (Koehn et al., 2007) to train machine translation systems from the Viterbi alignments of our various models, and evaluated the results with BLEU (Papineni et al., 2002). In order to reduce the effect of nondeterminism, we run Moses three times per experiment setting, and report the highest BLEU scores obtained. Since the BLEU scores we obtained are close, we did a significance test on the scores (Koehn, 2004). In Table 1, the colors partition the table into equivalence classes: If two scores within the same row have different background colors, then the difference between their scores is statistically significant. The best scores in the leftmost column were obtained from multi-rate HMMs with Dirichlet priors only during the TAM 1 training. On the contrary, the best scores for TAM-HMM and the baseline-HMM were obtained with Dirichlet priors both during the TAM 1 and the TAM-HMM Experiments We initialized our implementation of the single level ‘word-only’ model, which we call ‘baseline’ in Table 1,"
W13-2262,N04-4015,0,0.0753661,"Missing"
W13-2262,J93-2003,0,0.0535186,"Missing"
W13-2262,D09-1075,1,0.832868,"Missing"
W13-2262,C00-2162,0,0.105832,"Missing"
W13-2262,E03-1004,0,0.0408877,"Missing"
W13-2262,J03-1002,0,0.0724201,"MM differs from multi-rate HMM only by the lack of morpheme-level sequence modeling, and has complexity O(m2 n3 ). For the HMM to work correctly, we must handle jumping to and jumping from null positions. We learn the probabilities of jumping to a null position from the data. To compute the transition probability from a null position, we keep track of the nearest previous source word (or morpheme) that does not align to null, and use the position of the previous non-null word to calculate the jump width. In order to keep track of the previous nonnull word, we insert a null word between words (Och and Ney, 2003). Similarly, we insert a null morpheme after every non-null morpheme. M(p, q) = Class(fpq ) Fourth, as the arrow from faw (0) to fam (0,0) in Figure 4 shows, there is a conditional dependence on the word class that the morpheme is in: W(r) = Class(fr ) Putting together these components, the morpheme transitions are formulated as follows: p(am (j, k) = (r, s) |am (prev(j, k)) = (p, q)) ∝  p J (p, q, r, s)|M(p, q), W(r) δ(p, q, r, s) (2) The block diagonal matrix Ab consists of morpheme transition probabilities. 3.1.2 Word transitions In the multi-rate HMM, word transition probabilities have tw"
W13-2262,P02-1040,0,0.0868218,"urkish vocabulary is 57,249. The number of Turkish morphemes is 1,484,673 and the size of the morpheme vocabulary is 16,713. 4.2 We evaluated the performance of our model in two different ways. First, we evaluated against gold word alignments for 75 Turkish-English sentences. Table 1 shows the AER (Och and Ney, 2003) of the word alignments; we report the growdiag-final (Koehn et al., 2003) of the Viterbi alignments. Second, we used the Moses toolkit (Koehn et al., 2007) to train machine translation systems from the Viterbi alignments of our various models, and evaluated the results with BLEU (Papineni et al., 2002). In order to reduce the effect of nondeterminism, we run Moses three times per experiment setting, and report the highest BLEU scores obtained. Since the BLEU scores we obtained are close, we did a significance test on the scores (Koehn, 2004). In Table 1, the colors partition the table into equivalence classes: If two scores within the same row have different background colors, then the difference between their scores is statistically significant. The best scores in the leftmost column were obtained from multi-rate HMMs with Dirichlet priors only during the TAM 1 training. On the contrary, t"
W13-2262,C96-2141,0,0.371781,"orithm. TAMs can align rarely occurring words through their frequently occurring morphemes. In other words, they use morpheme probabilities to smooth rare word probabilities. Eyig¨oz et al. (2013) introduced TAM 1, which is analogous to IBM Model 1, in that the first level is a bag of words in a pair of sentences, and the second level is a bag of morphemes. By introducing distortion probabilities at the word level, Eyig¨oz et al. (2013) defined the HMM extension of TAM 1, the TAM-HMM. TAM-HMM was shown to be superior to its single-level counterpart, i.e., the HMM-based word alignment model of Vogel et al. (1996). The alignment example in Figure 1 shows a Turkish word aligned to an English phrase. The morphemes of the Turkish word are aligned to the English words. As the example shows, morphologically rich languages exhibit complex reordering phenomena at the morpheme level, which is left unutilized in TAM-HMMs. In this paper, we add morpheme sequence modeling to TAMs to capture morpheme level distortions. The example also shows that the Turkish morpheme orWe apply multi-rate HMMs, a tree structured HMM model, to the word-alignment problem. Multi-rate HMMs allow us to model reordering at both the morp"
W13-2262,P10-1047,1,0.869521,"Missing"
W13-2262,J08-4010,1,\N,Missing
W13-2262,J08-3003,1,\N,Missing
W13-2262,H05-1085,0,\N,Missing
W13-2262,N13-1004,1,\N,Missing
W17-4729,E17-3017,0,0.0437391,"Missing"
W17-4729,N12-1047,0,0.0369556,"baseline is an encoder-decoder model with attention and dropout implemented with Nematus (Sennrich et al., 2017) and AmuNMT (Junczys-Dowmunt et al., 2016). This baseline system without pre-tokenization or language model scoring achieves 17.32 uncased BLEU on news-test2017 and 19.78 after sourcesegmentation with the BPE algorithm. We used beam search with a beam width of 8 to approximately find the most likely translations given a source sentence before introducing features proposed by our language models and reranking with the default Moses (Koehn et al., 2007) implementation of K-best MIRA (Cherry and Foster, 2012). Both language models were trained on the English news data. Our unigrampruned 5-gram language model was trained with KenLM (Heafield, 2011), and our RNN-based language model was trained with RNNLM (Mikolov et al., 2011) with a hidden layer size of 300. 3.4 4.1 Error Analysis Error analysis on the validation set shows that the two main sources of errors produced by the baseline are missing and incorrect words. These issues are addressed in our model by applying morphological segmentation in combination with BPE and adding new backtranslated data to the training set. Our model’s translation er"
W17-4729,P16-1009,0,0.0722059,"ations based on the context information provided by the encoder (Bahdanau et al., 2014). More specifically, the decoder RNN tries to find a sequence of tokens in the target language that maximizes the following probability: If we simply apply the Chinese morphological analyzer to segment Chinese sentences into individual words and feed the words into our encoder, overfitting will occur; some words are so rare, that they only appear altogether with others. Thus, we enforced a thresholded on frequent words and applied the byte-pair-encoding (BPE) algorithm proposed by Gage (1994) and applied by Sennrich et al. (2016b) to NMT to further reduce the sparsity of our language data and to reduce the number of rare and out-of-vocabulary tokens. 2.3 (1) log p(Y |X) = TY X t=1 log p(yt |y1 , . . . , yt−1 , X) (3) Each hidden state st in the decoder is updated by st = φy (iy (yt−1 ), st−1 , ct ), Encoder (4) where iy is the continuous embedding of a token in the target language. ct is a context vector related to the t-th output token, such that The encoder reads a sequence of source language tokens X = (x1 , . . . , xTX ), and outputs a sequence of hidden states H = (h1 , . . . , hTX ). A bidirectional recurrent n"
W17-4729,W14-4012,0,0.128333,"Missing"
W17-4729,P16-1162,0,0.266034,"ations based on the context information provided by the encoder (Bahdanau et al., 2014). More specifically, the decoder RNN tries to find a sequence of tokens in the target language that maximizes the following probability: If we simply apply the Chinese morphological analyzer to segment Chinese sentences into individual words and feed the words into our encoder, overfitting will occur; some words are so rare, that they only appear altogether with others. Thus, we enforced a thresholded on frequent words and applied the byte-pair-encoding (BPE) algorithm proposed by Gage (1994) and applied by Sennrich et al. (2016b) to NMT to further reduce the sparsity of our language data and to reduce the number of rare and out-of-vocabulary tokens. 2.3 (1) log p(Y |X) = TY X t=1 log p(yt |y1 , . . . , yt−1 , X) (3) Each hidden state st in the decoder is updated by st = φy (iy (yt−1 ), st−1 , ct ), Encoder (4) where iy is the continuous embedding of a token in the target language. ct is a context vector related to the t-th output token, such that The encoder reads a sequence of source language tokens X = (x1 , . . . , xTX ), and outputs a sequence of hidden states H = (h1 , . . . , hTX ). A bidirectional recurrent n"
W17-4729,P16-1159,0,0.0305431,"language. ct is a context vector related to the t-th output token, such that The encoder reads a sequence of source language tokens X = (x1 , . . . , xTX ), and outputs a sequence of hidden states H = (h1 , . . . , hTX ). A bidirectional recurrent neural network (BiRNN) (Bahdanau et al., 2014) consisting of a forward recurrent neural network (RNN) and a backward ct = TX X l=1 311 hl · atl (5) sub-word tokens extracted by the morphological analyzer and BPE algorithms, and the target sentence Y is represented as a sequence of sub-words. 2.6 Minimum Risk Tuning We applied minimum risk training (Shen et al., 2016) to tune the model parameters post convergence of the cross-entropy loss by minimizing the expected risk for sentence-level BLEU scores where the risk is defined to be Figure 2: Illustration of Attention Mechanism from Luong et al. (2015). R(θ) = and Ey|x(s) ;θ [∆(y, y(s) )] (9) s=1 exp(etl ) atl = PT X k=1 exp(etk ) (6) = etk = falign (st−1 , hk ). X P (y|x(s) ; θ)∆(y, y(s) ) (10) for candidate translations Y (x(s) ) for x(s) . Details regarding methods to solve this problem can be found in Shen et al. (2016). 3 Experimental Settings In this section, we describe the details of the experimenta"
W17-4729,W11-2123,0,0.0139935,"2016). This baseline system without pre-tokenization or language model scoring achieves 17.32 uncased BLEU on news-test2017 and 19.78 after sourcesegmentation with the BPE algorithm. We used beam search with a beam width of 8 to approximately find the most likely translations given a source sentence before introducing features proposed by our language models and reranking with the default Moses (Koehn et al., 2007) implementation of K-best MIRA (Cherry and Foster, 2012). Both language models were trained on the English news data. Our unigrampruned 5-gram language model was trained with KenLM (Heafield, 2011), and our RNN-based language model was trained with RNNLM (Mikolov et al., 2011) with a hidden layer size of 300. 3.4 4.1 Error Analysis Error analysis on the validation set shows that the two main sources of errors produced by the baseline are missing and incorrect words. These issues are addressed in our model by applying morphological segmentation in combination with BPE and adding new backtranslated data to the training set. Our model’s translation error rate (0.716) is strictly lower than that of our baseline’s output (0.743). We attribute this reduction in error rate to our system being"
W18-2504,W12-3202,0,0.726029,"handled through volunteer efforts coordinated by the Anthology editor. Running a key service for the computational linguistics community that needs to be continuously available and updated frequently is one of the main issues in administering the Anthology. We discuss this issue along with the challenges of running a large scale project on a volunteer basis and its resulting technical debt. As we look towards the future, previous research has shown that it can also be used as a data source to characterize the work and workings of the ACL community (Bird et al., 2008; Vogel and Jurafsky, 2012; Anderson et al., 2012). Extensions to the Anthology that build on this information could make the Anthology an even more valuable resource for the community. We will discuss two possibl eextensions – anonymous pre-prints and support for finding relevant submission reviewers by linking au2 https://creativecommons.org/licenses/ by-nc-sa/3.0/ 3 https://creativecommons.org/licenses/ by/4.0/ thors in the Anthology with their research interests and community connections. Beyond being useful in itself, work on such challenges has the potential to motivate the ACL community to further support the Anthology. 2 Current State"
W18-2504,bird-etal-2008-acl,1,0.930936,"e maintenance of the code and the website is handled through volunteer efforts coordinated by the Anthology editor. Running a key service for the computational linguistics community that needs to be continuously available and updated frequently is one of the main issues in administering the Anthology. We discuss this issue along with the challenges of running a large scale project on a volunteer basis and its resulting technical debt. As we look towards the future, previous research has shown that it can also be used as a data source to characterize the work and workings of the ACL community (Bird et al., 2008; Vogel and Jurafsky, 2012; Anderson et al., 2012). Extensions to the Anthology that build on this information could make the Anthology an even more valuable resource for the community. We will discuss two possibl eextensions – anonymous pre-prints and support for finding relevant submission reviewers by linking au2 https://creativecommons.org/licenses/ by-nc-sa/3.0/ 3 https://creativecommons.org/licenses/ by/4.0/ thors in the Anthology with their research interests and community connections. Beyond being useful in itself, work on such challenges has the potential to motivate the ACL community"
W18-2504,buitelaar-etal-2014-hot,0,0.0232717,"institution, contributors that work on Anthology-related system administration and development tasks have been recruited in response to calls for volunteers at the main ACL conferences. In contrast, new features have been developed by researchers using the ACL Anthology as a resource in their own work, unconnected with the daily operation of the Anthology. Such research deliverables include, for example, the creation of a corpus of research papers (Bird et al., 2008), an author citation network (Radev et al., 2013) or a 4 https://www.softconf.com/ faceted search engine (Sch¨afer et al., 2012; Buitelaar et al., 2014). These factors, in combination with the multiple, changing responsibilities and shifting research interests of community members, mean that new volunteers join and leave the Anthology team in unpredictable and sporadic patterns. Preserving knowledge about the Anthology’s operational workflow is thus one of the most important challenges for the Anthology. The Anthology editor has played a key role ensuring the continuity of the entire project. This position has so far always been filled for multiple years, longer than the normal time frame for an ACL officer. The role has been critical in ensu"
W18-2504,W12-3209,1,0.776602,"lenge of reviewer matching we encourage the community to rally towards. 1 Introduction The ACL Anthology1 is a service offered by the Association for Computational Linguistics (ACL) allowing open access to the proceedings of all ACL sponsored conferences and journal articles. As a community goodwill gesture, it also hosts third-party computational linguistics literature from sister organizations and their national venues. It offers both text and faceted search of the indexed papers, author-specific pages, and can incorporate third-party metadata and services that can be embedded within pages (Bysani and Kan, 2012). As of this paper, it hosts over 1 https://aclanthology.info/ Min-Yen Kan School of Computing National University of Singapore kanmy@comp.nus.edu.sg 43,000 computational linguistics and natural language processing papers, along with their metadata. Over 4,500 daily requests are served by the Anthology. The code for the Anthology is available at https://github.com/acl-org/ acl-anthology under the Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License2 . Slightly different from the Anthology source code, ACL also licenses its papers with a more liberal license, supporti"
W18-2504,P11-4002,0,0.0607328,"Missing"
W18-2504,W12-3210,0,0.0498541,"Missing"
W18-2504,W12-3204,0,0.606146,"e code and the website is handled through volunteer efforts coordinated by the Anthology editor. Running a key service for the computational linguistics community that needs to be continuously available and updated frequently is one of the main issues in administering the Anthology. We discuss this issue along with the challenges of running a large scale project on a volunteer basis and its resulting technical debt. As we look towards the future, previous research has shown that it can also be used as a data source to characterize the work and workings of the ACL community (Bird et al., 2008; Vogel and Jurafsky, 2012; Anderson et al., 2012). Extensions to the Anthology that build on this information could make the Anthology an even more valuable resource for the community. We will discuss two possibl eextensions – anonymous pre-prints and support for finding relevant submission reviewers by linking au2 https://creativecommons.org/licenses/ by-nc-sa/3.0/ 3 https://creativecommons.org/licenses/ by/4.0/ thors in the Anthology with their research interests and community connections. Beyond being useful in itself, work on such challenges has the potential to motivate the ACL community to further support the An"
W18-6553,P09-1091,0,0.489263,"al network, observing significantly better results compared to LSTM language models on this task. 1 Introduction Linearization is the task of finding the grammatical order for a given set of words. Syntactic linearization systems generate output sentences along with their syntactic trees. Depending on how much syntactic information is available during decoding, recent work on syntactic linearization can be classified into abstract word ordering (Wan et al., 2009; Zhang et al., 2012; de Gispert et al., 2014), where no syntactic information is available during decoding, full tree linearization (He et al., 2009; Bohnet et al., 2010; Song et al., 2014), where full tree information is available, and partial tree linearization (Zhang, 2013), where partial syntactic information is given as input. Linearization has been adapted to tasks such as machine translation (Zhang et al., 2014), and is potentially helpful for many NLG applications, such as cooking recipe generation (Kiddon et al., 2016), dialogue response generation (Wen et al., 2015), and question generation (Serban et al., 2016). 431 Proceedings of The 11th International Natural Language Generation Conference, pages 431–440, c Tilburg, The Nethe"
W18-6553,D16-1032,0,0.0287327,"yntactic linearization can be classified into abstract word ordering (Wan et al., 2009; Zhang et al., 2012; de Gispert et al., 2014), where no syntactic information is available during decoding, full tree linearization (He et al., 2009; Bohnet et al., 2010; Song et al., 2014), where full tree information is available, and partial tree linearization (Zhang, 2013), where partial syntactic information is given as input. Linearization has been adapted to tasks such as machine translation (Zhang et al., 2014), and is potentially helpful for many NLG applications, such as cooking recipe generation (Kiddon et al., 2016), dialogue response generation (Wen et al., 2015), and question generation (Serban et al., 2016). 431 Proceedings of The 11th International Natural Language Generation Conference, pages 431–440, c Tilburg, The Netherlands, November 5-8, 2018. 2018 Association for Computational Linguistics Liu et al., 2015; Liu and Zhang, 2015) on syntactic linearization uses dependency grammar. We follow this line of works. On the other hand, linearization with other syntactic grammars, such as context free grammar (de Gispert et al., 2014) and combinatory categorial grammar (White and Rajkumar, 2009; Zhang an"
W18-6553,D15-1043,1,0.952914,"ial tree linearization (Zhang, 2013), where partial syntactic information is given as input. Linearization has been adapted to tasks such as machine translation (Zhang et al., 2014), and is potentially helpful for many NLG applications, such as cooking recipe generation (Kiddon et al., 2016), dialogue response generation (Wen et al., 2015), and question generation (Serban et al., 2016). 431 Proceedings of The 11th International Natural Language Generation Conference, pages 431–440, c Tilburg, The Netherlands, November 5-8, 2018. 2018 Association for Computational Linguistics Liu et al., 2015; Liu and Zhang, 2015) on syntactic linearization uses dependency grammar. We follow this line of works. On the other hand, linearization with other syntactic grammars, such as context free grammar (de Gispert et al., 2014) and combinatory categorial grammar (White and Rajkumar, 2009; Zhang and Clark, 2011), has also been studied. and the gap can go up to 11 BLEU points by integrating the LSTM language model as features. The integrated system also outperforms the LSTM language model by 1 BLEU point using beam search, which shows that syntactic information is useful for a neural linearization system. 2 Related work"
W18-6553,N15-1012,1,0.926688,"ently, Schmaltz et al. (2016) report new state-of-the-art results by leveraging a neural language model without using syntactic information. In their experiments, the neural language model, which is less sparse and captures long-range dependencies, outperforms previous discrete syntactic systems. A research question that naturally arises from this result is whether syntactic information is helpful for a neural linearization system. We empirically answer this question by comparing a neural transition-based syntactic linearizer with the neural language model of Schmaltz et al. (2016). Following Liu et al. (2015), our linearizer works incrementally given a set of words, using a stack to store partially built dependency trees, and a set to maintain unordered incoming words. At each step, it either shifts a word onto the stack, or reduces the top two partial trees on the stack. We leverage a feed forward neural network, which takes stack features as input and predicts the next action (such as S HIFT, L EFTA RC and R IGHTA RC). Hence our method can be regarded as an extension of the parser of Chen and Manning (2014), adding word ordering functionalities. In addition, we investigate two methods for integr"
W18-6553,P14-2128,1,0.86206,"latility . the debate between the stock and futures markets is prepared for wall street will cause another situation about whether de-linkage crash undoubtedly properly renewed friday . the wall street futures markets undoubtedly will cause renewed debate about whether the stock situation is properly prepared for an other crash between friday and de-linkage . the de-linkage between the stock and futures markets friday will undoubtedly cause renewed debate about whether wall street is prope rly prepared for another crash situation . Table 5: Output samples. 0.8 The results are consistent with (Ma et al., 2014) in that both increasing beam size and using richer features are solutions for error propagation. Synl ×LSTM-512 Synl ×LSTM-1 LSTM-512 LSTM-1 0.7 0.6 S YN×LSTM is better than S YN+LSTM. In fact, S YN×LSTM can be considered as interpolation with α being automatically calculated under different states. Finally, S YNl ×LSTM is better than S YN×LSTM except under greedy search, showing that word-to-word dependency features may be sufficient for this task. BLEU 0.5 0.4 0.3 0.2 0.1 10 As for the decoding times, S YNl ×LSTM shows a moderate time growth along increasing beam size, which is roughly 1.5"
W18-6553,P18-1026,0,0.0565936,"Missing"
W18-6553,C10-1012,0,0.143374,"ving significantly better results compared to LSTM language models on this task. 1 Introduction Linearization is the task of finding the grammatical order for a given set of words. Syntactic linearization systems generate output sentences along with their syntactic trees. Depending on how much syntactic information is available during decoding, recent work on syntactic linearization can be classified into abstract word ordering (Wan et al., 2009; Zhang et al., 2012; de Gispert et al., 2014), where no syntactic information is available during decoding, full tree linearization (He et al., 2009; Bohnet et al., 2010; Song et al., 2014), where full tree information is available, and partial tree linearization (Zhang, 2013), where partial syntactic information is given as input. Linearization has been adapted to tasks such as machine translation (Zhang et al., 2014), and is potentially helpful for many NLG applications, such as cooking recipe generation (Kiddon et al., 2016), dialogue response generation (Wen et al., 2015), and question generation (Serban et al., 2016). 431 Proceedings of The 11th International Natural Language Generation Conference, pages 431–440, c Tilburg, The Netherlands, November 5-8,"
W18-6553,J08-4003,0,0.0315241,"terpolated probability distribution is chosen, before both systems advancing to a new state using the action. The interpolated conditional probability is: 6 Following Chen and Manning (2014), we set the training objective as maximizing the loglikelihood of each successive action conditioned on the dependency tree, which can be gold or automatically parsed. To train our linearizer, we first generate training examples {(si , ti )}m i=1 from the training sentences and their gold parse trees, where si is a state, and ti ∈ T is the corresponding oracle transition. We use the “arc standard” oracle (Nivre, 2008), which always prefers S HIFT over LEFTA RC. The final training objective is to minimize the cross-entropy loss, plus an L2regularization term: p(a|si , hi ; θ1 , θ2 ) = log p(a|si ; θ1 ) + α log p(a|hi ; θ2 ), (8) where si and θ1 are the state and parameters of the linearizer, hi and θ2 are the state and parameters of the LSTM language model, and α is the interpolation hyper parameter. The action spaces of the two systems are different because the actions of the LSTM language model correspond only to the shift actions of the linearizer. To match the probability distributions, we expand the di"
W18-6553,P02-1040,0,0.10215,"orward neural network, which takes stack features as input and predicts the next action (such as S HIFT, L EFTA RC and R IGHTA RC). Hence our method can be regarded as an extension of the parser of Chen and Manning (2014), adding word ordering functionalities. In addition, we investigate two methods for integrating neural language models: interpolating the log probabilities of both models and integrating the neural language model as a feature. On standard benchmarks, our syntactic linearizer gives results that are higher than the LSTM language model of Schmaltz et al. (2016) by 7 BLEU points (Papineni et al., 2002) using greedy search, The task of linearization is to find a grammatical order given a set of words. Traditional models use statistical methods. Syntactic linearization systems, which generate a sentence along with its syntactic tree, have shown state-of-the-art performance. Recent work shows that a multilayer LSTM language model outperforms competitive statistical syntactic linearization systems without using syntax. In this paper, we study neural syntactic linearization, building a transition-based syntactic linearizer leveraging a feed forward neural network, observing significantly better"
W18-6553,D14-1082,0,0.635996,"-based syntactic linearizer with the neural language model of Schmaltz et al. (2016). Following Liu et al. (2015), our linearizer works incrementally given a set of words, using a stack to store partially built dependency trees, and a set to maintain unordered incoming words. At each step, it either shifts a word onto the stack, or reduces the top two partial trees on the stack. We leverage a feed forward neural network, which takes stack features as input and predicts the next action (such as S HIFT, L EFTA RC and R IGHTA RC). Hence our method can be regarded as an extension of the parser of Chen and Manning (2014), adding word ordering functionalities. In addition, we investigate two methods for integrating neural language models: interpolating the log probabilities of both models and integrating the neural language model as a feature. On standard benchmarks, our syntactic linearizer gives results that are higher than the LSTM language model of Schmaltz et al. (2016) by 7 BLEU points (Papineni et al., 2002) using greedy search, The task of linearization is to find a grammatical order given a set of words. Traditional models use statistical methods. Syntactic linearization systems, which generate a sent"
W18-6553,D16-1255,0,0.596497,"with discriminative features. Recently, Schmaltz et al. (2016) report new state-of-the-art results by leveraging a neural language model without using syntactic information. In their experiments, the neural language model, which is less sparse and captures long-range dependencies, outperforms previous discrete syntactic systems. A research question that naturally arises from this result is whether syntactic information is helpful for a neural linearization system. We empirically answer this question by comparing a neural transition-based syntactic linearizer with the neural language model of Schmaltz et al. (2016). Following Liu et al. (2015), our linearizer works incrementally given a set of words, using a stack to store partially built dependency trees, and a set to maintain unordered incoming words. At each step, it either shifts a word onto the stack, or reduces the top two partial trees on the stack. We leverage a feed forward neural network, which takes stack features as input and predicts the next action (such as S HIFT, L EFTA RC and R IGHTA RC). Hence our method can be regarded as an extension of the parser of Chen and Manning (2014), adding word ordering functionalities. In addition, we inves"
W18-6553,E14-1028,0,0.0515525,"Missing"
W18-6553,E12-1075,1,0.923719,"this paper, we study neural syntactic linearization, building a transition-based syntactic linearizer leveraging a feed forward neural network, observing significantly better results compared to LSTM language models on this task. 1 Introduction Linearization is the task of finding the grammatical order for a given set of words. Syntactic linearization systems generate output sentences along with their syntactic trees. Depending on how much syntactic information is available during decoding, recent work on syntactic linearization can be classified into abstract word ordering (Wan et al., 2009; Zhang et al., 2012; de Gispert et al., 2014), where no syntactic information is available during decoding, full tree linearization (He et al., 2009; Bohnet et al., 2010; Song et al., 2014), where full tree information is available, and partial tree linearization (Zhang, 2013), where partial syntactic information is given as input. Linearization has been adapted to tasks such as machine translation (Zhang et al., 2014), and is potentially helpful for many NLG applications, such as cooking recipe generation (Kiddon et al., 2016), dialogue response generation (Wen et al., 2015), and question generation (Serban et"
W18-6553,P16-1056,0,0.0600418,"Missing"
W18-6553,D11-1106,1,0.961405,"., 2016), dialogue response generation (Wen et al., 2015), and question generation (Serban et al., 2016). 431 Proceedings of The 11th International Natural Language Generation Conference, pages 431–440, c Tilburg, The Netherlands, November 5-8, 2018. 2018 Association for Computational Linguistics Liu et al., 2015; Liu and Zhang, 2015) on syntactic linearization uses dependency grammar. We follow this line of works. On the other hand, linearization with other syntactic grammars, such as context free grammar (de Gispert et al., 2014) and combinatory categorial grammar (White and Rajkumar, 2009; Zhang and Clark, 2011), has also been studied. and the gap can go up to 11 BLEU points by integrating the LSTM language model as features. The integrated system also outperforms the LSTM language model by 1 BLEU point using beam search, which shows that syntactic information is useful for a neural linearization system. 2 Related work 3 Previous work (White, 2005; White and Rajkumar, 2009; Zhang and Clark, 2011; Zhang, 2013) on syntactic linearization uses best-first search, which adopts a priority queue to store partial hypotheses and a chart to store input words. At each step, it pops the highest-scored hypothesis"
W18-6553,P18-1030,1,0.876063,"Missing"
W18-6553,P11-2033,1,0.776233,"on-like models. Second, we investigate a light version of the system, which only uses word features, while previous works all rely on POS tags and arc labels, limiting their usability on low-resource domains and languages. Schmaltz et al. (2016) are the first to adopt neural networks on this task, while only using surface features. To our knowledge, we are the first to leverage both neural networks and syntactic features. The contrast between our method and the method of Chen and Manning (2014) is reminiscent of the contrast between the method of Liu et al. (2015) and the dependency parser of Zhang and Nivre (2011). Comparing with the dependency parsing task, which assumes that POS tags are available as input, the search space of syntactic linearization is much larger. Recent work (Zhang, 2013; Song et al., 2014; Task Given an input bag-of-words x = {x1 , x2 , ..., xn }, the goal is to output the correct permutation y, which recovers the original sentence, from the set of all possible permutations Y. A linearizer can be seen as a scoring function f over Y, which is trained to output its highest scoring permutation yˆ = argmaxy0 ∈Y f (x, y 0 ) as close as possible to the correct permutation y. 3.1 Baseli"
W18-6553,P18-1150,1,0.887521,"Missing"
W18-6553,D14-1021,1,0.83641,"ir syntactic trees. Depending on how much syntactic information is available during decoding, recent work on syntactic linearization can be classified into abstract word ordering (Wan et al., 2009; Zhang et al., 2012; de Gispert et al., 2014), where no syntactic information is available during decoding, full tree linearization (He et al., 2009; Bohnet et al., 2010; Song et al., 2014), where full tree information is available, and partial tree linearization (Zhang, 2013), where partial syntactic information is given as input. Linearization has been adapted to tasks such as machine translation (Zhang et al., 2014), and is potentially helpful for many NLG applications, such as cooking recipe generation (Kiddon et al., 2016), dialogue response generation (Wen et al., 2015), and question generation (Serban et al., 2016). 431 Proceedings of The 11th International Natural Language Generation Conference, pages 431–440, c Tilburg, The Netherlands, November 5-8, 2018. 2018 Association for Computational Linguistics Liu et al., 2015; Liu and Zhang, 2015) on syntactic linearization uses dependency grammar. We follow this line of works. On the other hand, linearization with other syntactic grammars, such as contex"
W18-6553,P13-1043,1,0.818051,"on the training set. we use ten-fold jackknifing to construct WSJ training data with different accuracies. More specifically, the data is first randomly split into ten equalsize subsets, and then each subset is automatically parsed with a constituent parser trained on the other subsets, before the results are finally converted to dependency trees using Penn2Malt. In order to obtain datasets with different parsing accuracies, we randomly sample a small number of sentences from each training subset and choose different training iterations, as shown in Table 4. In our experiments, we use ZPar3 (Zhu et al., 2013) for automatic constituent parsing. Our syntactic linearizer is implemented with Keras.4 We randomly initialize Ew , Et , El , W1 and W2 within (−0.01, 0.01), and use default setting for other parameters. The hyper-parameters and parameters which achieve the best performance on the development set are chosen for final evaluation. Our vocabulary comes from SENNA5 , which has 130,000 words. The activation functions tanh and softmax are added on top of the hidden and output layers, respectively. We use Adagrad (Duchi et al., 2011) with an initial learning rate of 0.01, regularization parameter λ"
W18-6553,E09-1097,0,0.732492,"using syntax. In this paper, we study neural syntactic linearization, building a transition-based syntactic linearizer leveraging a feed forward neural network, observing significantly better results compared to LSTM language models on this task. 1 Introduction Linearization is the task of finding the grammatical order for a given set of words. Syntactic linearization systems generate output sentences along with their syntactic trees. Depending on how much syntactic information is available during decoding, recent work on syntactic linearization can be classified into abstract word ordering (Wan et al., 2009; Zhang et al., 2012; de Gispert et al., 2014), where no syntactic information is available during decoding, full tree linearization (He et al., 2009; Bohnet et al., 2010; Song et al., 2014), where full tree information is available, and partial tree linearization (Zhang, 2013), where partial syntactic information is given as input. Linearization has been adapted to tasks such as machine translation (Zhang et al., 2014), and is potentially helpful for many NLG applications, such as cooking recipe generation (Kiddon et al., 2016), dialogue response generation (Wen et al., 2015), and question ge"
W18-6553,D15-1199,0,0.0691019,"Missing"
W18-6553,W05-1104,0,0.0491195,"earization uses dependency grammar. We follow this line of works. On the other hand, linearization with other syntactic grammars, such as context free grammar (de Gispert et al., 2014) and combinatory categorial grammar (White and Rajkumar, 2009; Zhang and Clark, 2011), has also been studied. and the gap can go up to 11 BLEU points by integrating the LSTM language model as features. The integrated system also outperforms the LSTM language model by 1 BLEU point using beam search, which shows that syntactic information is useful for a neural linearization system. 2 Related work 3 Previous work (White, 2005; White and Rajkumar, 2009; Zhang and Clark, 2011; Zhang, 2013) on syntactic linearization uses best-first search, which adopts a priority queue to store partial hypotheses and a chart to store input words. At each step, it pops the highest-scored hypothesis from the priority queue, expanding it by combination with the words in the chart, before finally putting all new hypotheses back into the priority queue. As the search space is huge, a timeout threshold is set, beyond which the search terminates and the current best hypothesis is taken as the result. Liu et al. (2015) adapt the transition-"
W18-6553,D09-1043,0,0.03745,"e generation (Kiddon et al., 2016), dialogue response generation (Wen et al., 2015), and question generation (Serban et al., 2016). 431 Proceedings of The 11th International Natural Language Generation Conference, pages 431–440, c Tilburg, The Netherlands, November 5-8, 2018. 2018 Association for Computational Linguistics Liu et al., 2015; Liu and Zhang, 2015) on syntactic linearization uses dependency grammar. We follow this line of works. On the other hand, linearization with other syntactic grammars, such as context free grammar (de Gispert et al., 2014) and combinatory categorial grammar (White and Rajkumar, 2009; Zhang and Clark, 2011), has also been studied. and the gap can go up to 11 BLEU points by integrating the LSTM language model as features. The integrated system also outperforms the LSTM language model by 1 BLEU point using beam search, which shows that syntactic information is useful for a neural linearization system. 2 Related work 3 Previous work (White, 2005; White and Rajkumar, 2009; Zhang and Clark, 2011; Zhang, 2013) on syntactic linearization uses best-first search, which adopts a priority queue to store partial hypotheses and a chart to store input words. At each step, it pops the h"
