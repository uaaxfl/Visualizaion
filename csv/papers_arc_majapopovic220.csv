2021.mtsummit-research.14,On nature and causes of observed {MT} errors,2021,-1,-1,1,1,5059,maja popovic,Proceedings of Machine Translation Summit XVIII: Research Track,0,This work describes analysis of nature and causes of MT errors observed by different evaluators under guidance of different quality criteria: adequacy and comprehension and and a not specified generic mixture of adequacy and fluency. We report results for three language pairs and two domains and eleven MT systems. Our findings indicate that and despite the fact that some of the identified phenomena depend on domain and/or language and the following set of phenomena can be considered as generally challenging for modern MT systems: rephrasing groups of words and translation of ambiguous source words and translating noun phrases and and mistranslations. Furthermore and we show that the quality criterion also has impact on error perception. Our findings indicate that comprehension and adequacy can be assessed simultaneously by different evaluators and so that comprehension and as an important quality criterion and can be included more often in human evaluations.
2021.inlg-1.31,A Reproduction Study of an Annotation-based Human Evaluation of {MT} Outputs,2021,-1,-1,1,1,5059,maja popovic,Proceedings of the 14th International Conference on Natural Language Generation,0,"In this paper we report our reproduction study of the Croatian part of an annotation-based human evaluation of machine-translated user reviews (Popovic, 2020). The work was carried out as part of the ReproGen Shared Task on Reproducibility of Human Evaluation in NLG. Our aim was to repeat the original study exactly, except for using a different set of evaluators. We describe the experimental design, characterise differences between original and reproduction study, and present the results from each study, along with analysis of the similarity between them. For the six main evaluation results of Major/Minor/All Comprehension error rates and Major/Minor/All Adequacy error rates, we find that (i) 4/6 system rankings are the same in both studies, (ii) the relative differences between systems are replicated well for Major Comprehension and Adequacy (Pearson{'}s {\textgreater} 0.9), but not for the corresponding Minor error rates (Pearson{'}s 0.36 for Adequacy, 0.67 for Comprehension), and (iii) the individual system scores for both types of Minor error rates had a higher degree of reproducibility than the corresponding Major error rates. We also examine inter-annotator agreement and compare the annotations obtained in the original and reproduction studies."
2021.gebnlp-1.11,Generating Gender Augmented Data for {NLP},2021,-1,-1,2,0,6301,nishtha jain,Proceedings of the 3rd Workshop on Gender Bias in Natural Language Processing,0,"Gender bias is a frequent occurrence in NLP-based applications, especially pronounced in gender-inflected languages. Bias can appear through associations of certain adjectives and animate nouns with the natural gender of referents, but also due to unbalanced grammatical gender frequencies of inflected words. This type of bias becomes more evident in generating conversational utterances where gender is not specified within the sentence, because most current NLP applications still work on a sentence-level context. As a step towards more inclusive NLP, this paper proposes an automatic and generalisable re-writing approach for short conversational sentences. The rewriting method can be applied to sentences that, without extra-sentential context, have multiple equivalent alternatives in terms of gender. The method can be applied both for creating gender balanced outputs as well as for creating gender balanced training data. The proposed approach is based on a neural machine translation system trained to {`}translate{'} from one gender alternative to another. Both the automatic and manual analysis of the approach show promising results with respect to the automatic generation of gender alternatives for conversational sentences in Spanish."
2021.conll-1.18,Agree to Disagree: Analysis of Inter-Annotator Disagreements in Human Evaluation of Machine Translation Output,2021,-1,-1,1,1,5059,maja popovic,Proceedings of the 25th Conference on Computational Natural Language Learning,0,"This work describes an analysis of inter-annotator disagreements in human evaluation of machine translation output. The errors in the analysed texts were marked by multiple annotators under guidance of different quality criteria: adequacy, comprehension, and an unspecified generic mixture of adequacy and fluency. Our results show that different criteria result in different disagreements, and indicate that a clear definition of quality criterion can improve the inter-annotator agreement. Furthermore, our results show that for certain linguistic phenomena which are not limited to one or two words (such as word ambiguity or gender) but span over several words or even entire phrases (such as negation or relative clause), disagreements do not necessarily represent {``}errors{''} or {``}noise{''} but are rather inherent to the evaluation process. {\%}These disagreements are caused by differences in error perception and/or the fact that there is no single correct translation of a text so that multiple solutions are possible. On the other hand, for some other phenomena (such as omission or verb forms) agreement can be easily improved by providing more precise and detailed instructions to the evaluators."
2020.wmt-1.51,Neural Machine Translation between similar {S}outh-{S}lavic languages,2020,-1,-1,1,1,5059,maja popovic,Proceedings of the Fifth Conference on Machine Translation,0,"This paper describes the ADAPT-DCU machine translation systems built for the WMT 2020 shared task on Similar Language Translation. We explored several set-ups for NMT for Croatian{--}Slovenian and Serbian{--}Slovenian language pairs in both translation directions. Our experiments focus on different amounts and types of training data: we first apply basic filtering on the OpenSubtitles training corpora, then we perform additional cleaning of remaining misaligned segments based on character n-gram matching. Finally, we make use of additional monolingual data by creating synthetic parallel data through back-translation. Automatic evaluation shows that multilingual systems with joint Serbian and Croatian data are better than bilingual, as well as that character-based cleaning leads to improved scores while using less data. The results also confirm once more that adding back-translated data further improves the performance, especially when the synthetic data is similar to the desired domain of the development and test set. This, however, might come at a price of prolonged training time, especially for multitarget systems."
2020.vardial-1.10,Neural Machine Translation for translating into {C}roatian and {S}erbian,2020,-1,-1,1,1,5059,maja popovic,"Proceedings of the 7th Workshop on NLP for Similar Languages, Varieties and Dialects",0,"In this work, we systematically investigate different set-ups for training of neural machine translation (NMT) systems for translation into Croatian and Serbian, two closely related South Slavic languages. We explore English and German as source languages, different sizes and types of training corpora, as well as bilingual and multilingual systems. We also explore translation of English IMDb user movie reviews, a domain/genre where only monolingual data are available. First, our results confirm that multilingual systems with joint target languages perform better. Furthermore, translation performance from English is much better than from German, partly because German is morphologically more complex and partly because the corpus consists mostly of parallel human translations instead of original text and its human translation. The translation from German should be further investigated systematically. For translating user reviews, creating synthetic in-domain parallel data through back- and forward-translation and adding them to a small out-of-domain parallel corpus can yield performance comparable with a system trained on a full out-of-domain corpus. However, it is still not clear what is the optimal size of synthetic in-domain data, especially for forward-translated data where the target language is machine translated. More detailed research including manual evaluation and analysis is needed in this direction."
2020.lrec-1.461,On Context Span Needed for Machine Translation Evaluation,2020,-1,-1,2,0,5000,sheila castilho,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Despite increasing efforts to improve evaluation of machine translation (MT) by going beyond the sentence level to the document level, the definition of what exactly constitutes a {``}document level{''} is still not clear. This work deals with the context span necessary for a more reliable MT evaluation. We report results from a series of surveys involving three domains and 18 target languages designed to identify the necessary context span as well as issues related to it. Our findings indicate that, despite the fact that some issues and spans are strongly dependent on domain and on the target language, a number of common patterns can be observed so that general guidelines for context-aware MT evaluation can be drawn."
2020.eamt-1.39,On the differences between human translations,2020,-1,-1,1,1,5059,maja popovic,Proceedings of the 22nd Annual Conference of the European Association for Machine Translation,0,"Many studies have confirmed that translated texts exhibit different features than texts originally written in the given language. This work explores texts translated by different translators taking into account expertise and native language. A set of computational analyses was conducted on three language pairs, English-Croatian, German-French and English-Finnish, and the results show that each of the factors has certain influence on the features of the translated texts, especially on sentence length and lexical richness. The results also indicate that for translations used for machine translation evaluation, it is important to specify these factors, especially if comparing machine translation quality with human translation quality is involved."
2020.eamt-1.52,{QR}ev: Machine Translation of User Reviews: What Influences the Translation Quality?,2020,-1,-1,1,1,5059,maja popovic,Proceedings of the 22nd Annual Conference of the European Association for Machine Translation,0,"This project aims to identify the important aspects of translation quality of user reviews which will represent a starting point for developing better automatic MT metrics and challenge test sets, and will be also helpful for developing MT systems for this genre. We work on two types of reviews: Amazon products and IMDb movies, written in English and translated into two closely related target languages, Croatian and Serbian."
2020.conll-1.19,Relations between comprehensibility and adequacy errors in machine translation output,2020,-1,-1,1,1,5059,maja popovic,Proceedings of the 24th Conference on Computational Natural Language Learning,0,"This work presents a detailed analysis of translation errors perceived by readers as comprehensibility and/or adequacy issues. The main finding is that good comprehensibility, similarly to good fluency, can mask a number of adequacy errors. Of all major adequacy errors, 30{\%} were fully comprehensible, thus fully misleading the reader to accept the incorrect information. Another 25{\%} of major adequacy errors were perceived as almost comprehensible, thus being potentially misleading. Also, a vast majority of omissions (about 70{\%}) is hidden by comprehensibility. Further analysis of misleading translations revealed that the most frequent error types are ambiguity, mistranslation, noun phrase error, word-by-word translation, untranslated word, subject-verb agreement, and spelling error in the source text. However, none of these error types appears exclusively in misleading translations, but are also frequent in fully incorrect (incomprehensible inadequate) and discarded correct (incomprehensible adequate) translations. Deeper analysis is needed to potentially detect underlying phenomena specifically related to misleading translations."
2020.coling-main.444,Informative Manual Evaluation of Machine Translation Output,2020,-1,-1,1,1,5059,maja popovic,Proceedings of the 28th International Conference on Computational Linguistics,0,"This work proposes a new method for manual evaluation of Machine Translation (MT) output based on marking actual issues in the translated text. The novelty is that the evaluators are not assigning any scores, nor classifying errors, but marking all problematic parts (words, phrases, sentences) of the translation. The main advantage of this method is that the resulting annotations do not only provide overall scores by counting words with assigned tags, but can be further used for analysis of errors and challenging linguistic phenomena, as well as inter-annotator disagreements. Detailed analysis and understanding of actual problems are not enabled by typical manual evaluations where the annotators are asked to assign overall scores or to rank two or more translations. The proposed method is very general: it can be applied on any genre/domain and language pair, and it can be guided by various types of quality criteria. Also, it is not restricted to MT output, but can be used for other types of generated text."
W19-7602,Challenge Test Sets for {MT} Evaluation,2019,-1,-1,1,1,5059,maja popovic,Proceedings of Machine Translation Summit XVII: Tutorial Abstracts,0,"Most of the test sets used for the evaluation of MT systems reflect the frequency distribution of different phenomena found in naturally occurring data ({''}standard{''} or {''}natural{''} test sets). However, to better understand particular strengths and weaknesses of MT systems, especially those based on neural networks, it is necessary to apply more focused evaluation procedures. Therefore, another type of test sets ({''}challenge{''} test sets, also called {''}test suites{''}) is being increasingly employed in order to highlight points of difficulty which are relevant to model development, training, or using of the given system. This tutorial will be useful for anyone (researchers, developers, users, translators) interested in detailed evaluation and getting a better understanding of machine translation (MT) systems and models. The attendees will learn about the motivation and linguistic background of challenge test sets and a range of testing possibilities applied to the state-of-the-art MT systems, as well as a number of practical aspects and challenges."
W19-6712,On reducing translation shifts in translations intended for {MT} evaluation,2019,0,0,1,1,5059,maja popovic,"Proceedings of Machine Translation Summit XVII: Translator, Project and User Tracks",0,None
W19-6609,Automatic error classification with multiple error labels,2019,0,0,1,1,5059,maja popovic,Proceedings of Machine Translation Summit XVII: Research Track,0,None
W19-5353,Evaluating Conjunction Disambiguation on {E}nglish-to-{G}erman and {F}rench-to-{G}erman {WMT} 2019 Translation Hypotheses,2019,-1,-1,1,1,5059,maja popovic,"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)",0,"We present a test set for evaluating an MT system{'}s capability to translate ambiguous conjunctions depending on the sentence structure. We concentrate on the English conjunction {``}but{''} and its French equivalent {``}mais{''} which can be translated into two different German conjunctions. We evaluate all English-to-German and French-to-German submissions to the WMT 2019 shared translation task. The evaluation is done mainly automatically, with additional fast manual inspection of unclear cases. All systems almost perfectly recognise the target conjunction {``}aber{''}, whereas accuracies for the other target conjunction {``}sondern{''} range from 78{\%} to 97{\%}, and the errors are mostly caused by replacing it with the alternative conjunction {``}aber{''}. The best performing system for both language pairs is a multilingual Transformer {``}TartuNLP{''} system trained on all WMT 2019 language pairs which use the Latin script, indicating that the multilingual approach is beneficial for conjunction disambiguation. As for other system features, such as using synthetic back-translated data, context-aware, hybrid, etc., no particular (dis)advantages can be observed. Qualitative manual inspection of translation hypotheses shown that highly ranked systems generally produce translations with high adequacy and fluency, meaning that these systems are not only capable of capturing the right conjunction whereas the rest of the translation hypothesis is poor. On the other hand, the low ranked systems generally exhibit lower fluency and poor adequacy."
W19-3715,Building {E}nglish-to-{S}erbian Machine Translation System for {IMD}b Movie Reviews,2019,0,0,2,0,22365,pintu lohar,Proceedings of the 7th Workshop on Balto-Slavic Natural Language Processing,0,"This paper reports the results of the first experiment dealing with the challenges of building a machine translation system for user-generated content involving a complex South Slavic language. We focus on translation of English IMDb user movie reviews into Serbian, in a low-resource scenario. We explore potentials and limits of (i) phrase-based and neural machine translation systems trained on out-of-domain clean parallel data from news articles (ii) creating additional synthetic in-domain parallel corpus by machine-translating the English IMDb corpus into Serbian. Our main findings are that morphology and syntax are better handled by the neural approach than by the phrase-based approach even in this low-resource mismatched domain scenario, however the situation is different for the lexical aspect, especially for person names. This finding also indicates that in general, machine translation of person names into Slavic languages (especially those which require/allow transcription) should be investigated more systematically."
R19-1107,Combining {PBSMT} and {NMT} Back-translated Data for Efficient {NMT},2019,21,1,2,0,13873,alberto poncelas,Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019),0,"Neural Machine Translation (NMT) models achieve their best performance when large sets of parallel data are used for training. Consequently, techniques for augmenting the training set have become popular recently. One of these methods is back-translation, which consists on generating synthetic sentences by translating a set of monolingual, target-language sentences using a Machine Translation (MT) model. Generally, NMT models are used for back-translation. In this work, we analyze the performance of models when the training data is extended with synthetic data using different MT approaches. In particular we investigate back-translated data generated not only by NMT but also by Statistical Machine Translation (SMT) models and combinations of both. The results reveal that the models achieve the best performances when the training set is augmented with back-translated data created by merging different MT approaches."
R19-1111,Are ambiguous conjunctions problematic for machine translation?,2019,0,0,1,1,5059,maja popovic,Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019),0,"The translation of ambiguous words still poses challenges for machine translation. In this work, we carry out a systematic quantitative analysis regarding the ability of different machine translation systems to disambiguate the source language conjunctions {``}but{''} and {``}and{''}. We evaluate specialised test sets focused on the translation of these two conjunctions. The test sets contain source languages that do not distinguish different variants of the given conjunction, whereas the target languages do. In total, we evaluate the conjunction {``}but{''} on 20 translation outputs, and the conjunction {``}and{''} on 10. All machine translation systems almost perfectly recognise one variant of the target conjunction, especially for the source conjunction {``}but{''}. The other target variant, however, represents a challenge for machine translation systems, with accuracy varying from 50{\%} to 95{\%} for {``}but{''} and from 20{\%} to 57{\%} for {``}and{''}. The major error for all systems is replacing the correct target variant with the opposite one."
R19-1131,Automated Text Simplification as a Preprocessing Step for Machine Translation into an Under-resourced Language,2019,0,0,2,0.167023,24998,sanja vstajner,Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019),0,"In this work, we investigate the possibility of using fully automatic text simplification system on the English source in machine translation (MT) for improving its translation into an under-resourced language. We use the state-of-the-art automatic text simplification (ATS) system for lexically and syntactically simplifying source sentences, which are then translated with two state-of-the-art English-to-Serbian MT systems, the phrase-based MT (PBMT) and the neural MT (NMT). We explore three different scenarios for using the ATS in MT: (1) using the raw output of the ATS; (2) automatically filtering out the sentences with low grammaticality and meaning preservation scores; and (3) performing a minimal manual correction of the ATS output. Our results show improvement in fluency of the translation regardless of the chosen scenario, and difference in success of the three scenarios depending on the MT approach used (PBMT or NMT) with regards to improving translation fluency and post-editing effort."
W18-7006,Improving Machine Translation of {E}nglish Relative Clauses with Automatic Text Simplification,2018,0,2,2,0.167023,24998,sanja vstajner,Proceedings of the 1st Workshop on Automatic Text Adaptation ({ATA}),0,None
W18-0541,Complex Word Identification Using Character n-grams,2018,0,1,1,1,5059,maja popovic,Proceedings of the Thirteenth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"This paper investigates the use of character n-gram frequencies for identifying complex words in English, German and Spanish texts. The approach is based on the assumption that complex words are likely to contain different character sequences than simple words. The multinomial Naive Bayes classifier was used with n-grams of different lengths as features, and the best results were obtained for the combination of 2-grams and 4-grams. This variant was submitted to the Complex Word Identification Shared Task 2018 for all texts and achieved F-scores between 70{\%} and 83{\%}. The system was ranked in the middle range for all English texts, as third of fourteen submissions for German, and as tenth of seventeen submissions for Spanish. The method is not very convenient for the cross-language task, achieving only 59{\%} on the French text."
L18-1073,A Multilingual Wikified Data Set of Educational Material,2018,0,0,11,0,16715,iris hendrickx,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
W17-4770,chr{F}++: words helping character n-grams,2017,0,18,1,1,5059,maja popovic,Proceedings of the Second Conference on Machine Translation,0,None
W16-4806,Language Related Issues for Machine Translation between Closely Related {S}outh {S}lavic Languages,2016,0,2,1,1,5059,maja popovic,"Proceedings of the Third Workshop on {NLP} for Similar Languages, Varieties and Dialects ({V}ar{D}ial3)",0,"Machine translation between closely related languages is less challenging and exibits a smaller number of translation errors than translation between distant languages, but there are still obstacles which should be addressed in order to improve such systems. This work explores the obstacles for machine translation systems between closely related South Slavic languages, namely Croatian, Serbian and Slovenian. Statistical systems for all language pairs and translation directions are trained using parallel texts from different domains, however mainly on spoken language i.e. subtitles. For translation between Serbian and Croatian, a rule-based system is also explored. It is shown that for all language pairs and translation systems, the main obstacles are differences between structural properties."
W16-4813,Enlarging Scarce In-domain {E}nglish-{C}roatian Corpus for {SMT} of {MOOC}s Using {S}erbian,2016,6,0,1,1,5059,maja popovic,"Proceedings of the Third Workshop on {NLP} for Similar Languages, Varieties and Dialects ({V}ar{D}ial3)",0,"Massive Open Online Courses have been growing rapidly in size and impact. Yet the language barrier constitutes a major growth impediment in reaching out all people and educating all citizens. A vast majority of educational material is available only in English, and state-of-the-art machine translation systems still have not been tailored for this peculiar genre. In addition, a mere collection of appropriate in-domain training material is a challenging task. In this work, we investigate statistical machine translation of lecture subtitles from English into Croatian, which is morphologically rich and generally weakly supported, especially for the educational domain. We show that results comparable with publicly available systems trained on much larger data can be achieved if a small in-domain training set is used in combination with additional in-domain corpus originating from the closely related Serbian language."
W16-3410,Potential and Limits of Using Post-edits as Reference Translations for {MT} Evaluation,2016,-1,-1,1,1,5059,maja popovic,Proceedings of the 19th Annual Conference of the {E}uropean Association for Machine Translation,0,None
W16-3411,Can Text Simplification Help Machine Translation?,2016,0,9,2,0.167023,24998,sanja vstajner,Proceedings of the 19th Annual Conference of the {E}uropean Association for Machine Translation,0,"This article explores the use of text simplification as a pre-processing step for statistical machine translation of grammatically complex under-resourced languages. Our experiments on English-to-Serbian translation show that this approach can improve grammaticality (fluency) of the translation output and reduce technical post-editing effort (number of post-edit operations). Furthermore, the use of more aggressive text simplification methods (which do not only simplify the given sentence but also discard irrelevant information thus producing syntactically very simple sentences) also improves meaning preservation (adequacy) of the translation output."
W16-2341,chr{F} deconstructed: beta parameters and n-gram weights,2016,-1,-1,1,1,5059,maja popovic,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,None
L16-1005,{PE}2rr Corpus: Manual Error Annotation of Automatically Pre-annotated {MT} Post-edits,2016,12,2,1,1,5059,maja popovic,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"We present a freely available corpus containing source language texts from different domains along with their automatically generated translations into several distinct morphologically rich languages, their post-edited versions, and error annotations of the performed post-edit operations. We believe that the corpus will be useful for many different applications. The main advantage of the approach used for creation of the corpus is the fusion of post-editing and error classification tasks, which have usually been seen as two independent tasks, although naturally they are not. We also show benefits of coupling automatic and manual error classification which facilitates the complex manual error annotation task as well as the development of automatic error classification tools. In addition, the approach facilitates annotation of language pair related issues."
L16-1296,Tools and Guidelines for Principled Machine Translation Development,2016,2,1,6,0,20841,nora aranberri,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"This work addresses the need to aid Machine Translation (MT) development cycles with a complete workflow of MT evaluation methods. Our aim is to assess, compare and improve MT system variants. We hereby report on novel tools and practices that support various measures, developed in order to support a principled and informed approach of MT development. Our toolkit for automatic evaluation showcases quick and detailed comparison of MT system variants through automatic metrics and n-gram feedback, along with manual evaluation via edit-distance, error annotation and task-based feedback."
2016.eamt-2.20,{T}ra{MOOC} (Translation for Massive Open Online Courses): providing reliable {MT} for {MOOC}s,2016,0,1,15,0,12066,valia kordoni,Proceedings of the 19th Annual Conference of the European Association for Machine Translation: Projects/Products,0,None
W15-5702,Towards Deeper {MT} - A Hybrid System for {G}erman,2015,-1,-1,3,0.976326,5140,eleftherios avramidis,Proceedings of the 1st Deep Machine Translation Workshop,0,None
W15-4913,Identifying main obstacles for statistical machine translation of morphologically rich {S}outh {S}lavic languages,2015,11,2,1,1,5059,maja popovic,Proceedings of the 18th Annual Conference of the {E}uropean Association for Machine Translation,0,"The best way to improve a statistical machine translation system is to identify concrete problems causing translation errors and address them. Many of these problems are related to the characteristics of the involved languages and differences between them. This work explores the main obstacles for statistical machine translation systems involving two morphologically rich and under-resourced languages, namely Serbian and Slovenian. Systems are trained for translations from and into English and German using parallel texts from different domains, including both written and spoken language. It is shown that for all translation directions structural properties concerning multi-noun collocations and exact phrase boundaries are the most difficult for the systems, followed by negation, preposition and local word order differences. For translation into English and German, articles and pronouns are the most problematic, as well as disambiguation of certain frequent functional words. For translation into Serbian and Slovenian, cases and verb inflections are most difficult. In addition, local word order involving verbs is often incorrect and verb parts are often missing, especially when translating from German."
W15-4914,Poor man{'}s lemmatisation for automatic error classification,2015,9,0,1,1,5059,maja popovic,Proceedings of the 18th Annual Conference of the {E}uropean Association for Machine Translation,0,This publication has emanated from research supported by QTLEAP project xe2x80x93 ECs FP7 (FP7/2007-n 2013) under grant agreement number 610516:n xe2x80x9cQTLEAP: Quality Translation by Deep Language Engineering Approachesxe2x80x9d and by a researchn grant from Science Foundation Ireland (SFI) undern Grant Number SFI/12/RC/2289. We are grateful ton the reviewers for their valuable feedbac
W15-3004,{DFKI}{'}s experimental hybrid {MT} system for {WMT} 2015,2015,15,4,2,0.976326,5140,eleftherios avramidis,Proceedings of the Tenth Workshop on Statistical Machine Translation,0,"DFKI participated in the shared translation task of WMT 2015 with the GermanEnglish language pair in each translation direction. The submissions were generated using an experimental hybrid system based on three systems: a statistical Moses system, a commercial rule-based system, and a serial coupling of the two where the output of the rule-based system is further translated by Moses trained on parallel text consisting of the rule-based output and the original target language. The outputs of three systems are combined using two methods: (a) an empirical selection mechanism based on grammatical features (primary submission) and (b) IBM1 models based on POS 4-grams (contrastive submission)."
W15-3049,chr{F}: character n-gram {F}-score for automatic {MT} evaluation,2015,9,96,1,1,5059,maja popovic,Proceedings of the Tenth Workshop on Statistical Machine Translation,0,"We propose the use of character n-gram F-score for automatic evaluation of machine translation output. Character ngrams have already been used as a part of more complex metrics, but their individual potential has not been investigated yet. We report system-level correlations with human rankings for 6-gram F1-score (CHRF) on the WMT12, WMT13 and WMT14 data as well as segment-level correlation for 6gram F1 (CHRF) and F3-scores (CHRF3) on WMT14 data for all available target languages. The results are very promising, especially for the CHRF3 score xe2x80x93 for translation from English, this variant showed the highest segment-level correlations outperforming even the best metrics on the WMT14 shared evaluation task."
2015.eamt-1.14,Identifying main obstacles for statistical machine translation of morphologically rich {S}outh {S}lavic languages,2015,11,2,1,1,5059,maja popovic,Proceedings of the 18th Annual Conference of the European Association for Machine Translation,0,"The best way to improve a statistical machine translation system is to identify concrete problems causing translation errors and address them. Many of these problems are related to the characteristics of the involved languages and differences between them. This work explores the main obstacles for statistical machine translation systems involving two morphologically rich and under-resourced languages, namely Serbian and Slovenian. Systems are trained for translations from and into English and German using parallel texts from different domains, including both written and spoken language. It is shown that for all translation directions structural properties concerning multi-noun collocations and exact phrase boundaries are the most difficult for the systems, followed by negation, preposition and local word order differences. For translation into English and German, articles and pronouns are the most problematic, as well as disambiguation of certain frequent functional words. For translation into Serbian and Slovenian, cases and verb inflections are most difficult. In addition, local word order involving verbs is often incorrect and verb parts are often missing, especially when translating from German."
2015.eamt-1.15,Poor man{'}s lemmatisation for automatic error classification,2015,9,0,1,1,5059,maja popovic,Proceedings of the 18th Annual Conference of the European Association for Machine Translation,0,This publication has emanated from research supported by QTLEAP project xe2x80x93 ECs FP7 (FP7/2007-n 2013) under grant agreement number 610516:n xe2x80x9cQTLEAP: Quality Translation by Deep Language Engineering Approachesxe2x80x9d and by a researchn grant from Science Foundation Ireland (SFI) undern Grant Number SFI/12/RC/2289. We are grateful ton the reviewers for their valuable feedbac
W14-5104,Correlating decoding events with errors in Statistical Machine Translation,2014,23,1,2,0.976326,5140,eleftherios avramidis,Proceedings of the 11th International Conference on Natural Language Processing,0,None
W14-4210,Exploring cross-language statistical machine translation for closely related {S}outh {S}lavic languages,2014,9,4,1,1,5059,maja popovic,Proceedings of the {EMNLP}{'}2014 Workshop on Language Technology for Closely Related Languages and Language Variants,0,"This work investigates the use of crosslanguage resources for statistical machine translation (SMT) between English and two closely related South Slavic languages, namely Croatian and Serbian. The goal is to explore the effects of translating from and into one language using an SMT system trained on another. For translation into English, a loss due to cross-translation is about 13% of BLEU and for the other translation direction about 15%. The performance decrease for both languages in both translation directions is mainly due to lexical divergences. Several language adaptation methods are explored, and it is shown that very simple lexical transformations already can yield a small improvement, and that the most promising adaptation method is using a Croatian-Serbian SMT system trained on a very small corpus."
avramidis-etal-2014-taraxu,"The tara{X{\\\U}} corpus of human-annotated machine translations""",2014,7,2,4,0.976326,5140,eleftherios avramidis,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"Human translators are the key to evaluating machine translation (MT) quality and also to addressing the so far unanswered question when and how to use MT in professional translation workflows. This paper describes the corpus developed as a result of a detailed large scale human evaluation consisting of three tightly connected tasks: ranking, error classification and post-editing."
2014.eamt-1.38,Using a new analytic measure for the annotation and analysis of {MT} errors on real data,2014,7,12,3,0,28494,arle lommel,Proceedings of the 17th Annual conference of the European Association for Machine Translation,0,"This work presents the new flexible Multidimensional Quality Metrics (MQM) framework and uses it to analyze the performance of state-of-the-art machine translation systems, focusing on xe2x80x9cnearly acceptablexe2x80x9d translated sentences. A selection of WMT news data and xe2x80x9ccustomerxe2x80x9d data provided by language service providers (LSPs) in four language pairs was annotated using MQM issue types and examined in terms of the types of errors found in it. Despite criticisms of WMT data by the LSPs, an examination of the resulting errors and patterns for both types of data shows that they are strikingly consistent, with more variation between language pairs and system types than between text types. These results validate the use of WMT data in an analytic approach to assessing quality and show that analytic approaches represent a useful addition to more traditional assessment methodologies such as BLEU or METEOR."
2014.eamt-1.41,"Relations between different types of post-editing operations, cognitive effort and temporal effort",2014,-1,-1,1,1,5059,maja popovic,Proceedings of the 17th Annual conference of the European Association for Machine Translation,0,None
W13-2240,Selecting Feature Sets for Comparative and Time-Oriented Quality Estimation of Machine Translation Output,2013,22,3,2,0.909091,5140,eleftherios avramidis,Proceedings of the Eighth Workshop on Statistical Machine Translation,0,None
2013.mtsummit-wptp.2,What can we learn about the selection mechanism for post-editing?,2013,-1,-1,1,1,5059,maja popovic,Proceedings of the 2nd Workshop on Post-editing Technology and Practice,0,None
2013.mtsummit-posters.4,A {CCG}-based Quality Estimation Metric for Statistical Machine Translation Learning from Human Judgments of Machine Translation Output,2013,-1,-1,1,1,5059,maja popovic,Proceedings of Machine Translation Summit XIV: Posters,0,None
2013.mtsummit-posters.5,Learning from Human Judgments of Machine Translation Output,2013,10,7,1,1,5059,maja popovic,Proceedings of Machine Translation Summit XIV: Posters,0,"Human translators are the key to evaluating machine translation (MT) quality and also to addressing the so far unanswered question when and how to use MT in professional translation workflows. Usually, human judgments come in the form of ranking outputs of different translation systems and recently, post-edits of MT output have come into focus. This paper describes the results of a detailed large scale human evaluation consisting of three tightly connected tasks: ranking, error classification and post-editing. Translation outputs from three domains and six translation directions generated by five distinct translation systems have been analysed with the goal of getting relevant insights for further improvement of MT quality and applicability."
W12-3105,{T}error{C}at: a Translation Error Categorization-based {MT} Quality Metric,2012,13,14,3,0,13955,mark fishel,Proceedings of the Seventh Workshop on Statistical Machine Translation,0,"We present TerrorCat, a submission to the WMT'12 metrics shared task. TerrorCat uses frequencies of automatically obtained translation error categories as base for pairwise comparison of translation hypotheses, which is in turn used to generate a score for every translation. The metric shows high overall correlation with human judgements on the system level and more modest results on the level of individual sentences."
W12-3106,Class error rates for evaluation of machine translation output,2012,8,11,1,1,5059,maja popovic,Proceedings of the Seventh Workshop on Statistical Machine Translation,0,"We investigate the use of error classification results for automatic evaluation of machine translation output. Five basic error classes are taken into account: morphological errors, syntactic (reordering) errors, missing words, extra words and lexical errors. In addition, linear combinations of these categories are investigated. Correlations between the class error rates and human judgments are calculated on the data of the third, fourth, fifth and sixth shared tasks of the Statistical Machine Translation Workshop. Machine translation outputs in five different European languages are used: English, Spanish, French, German and Czech. The results show that the following combinations are the most promising: the sum of all class error rates, the weighted sum optimised for translation into English and the weighted sum optimised for translation from English."
W12-3116,Morpheme- and {POS}-based {IBM}1 and language model scores for translation quality estimation,2012,11,16,1,1,5059,maja popovic,Proceedings of the Seventh Workshop on Statistical Machine Translation,0,"We present a method we used for the quality estimation shared task of WMT 2012 involving IBM 1 and language model scores calculated on morphemes and POS tags. The IBM 1 scores calculated on morphemes and POS-4grams of the source sentence and obtained translation output are shown to be competitive with the classic evaluation metrics for ranking of translation systems. Since these scores do not require any reference translations, they can be used as features for the quality estimation task presenting a connection between the source language and the obtained target language. In addition, target language model scores of morphemes and POS tags are investigated as estimates for the obtained target language quality."
avramidis-etal-2012-involving,Involving Language Professionals in the Evaluation of Machine Translation,2012,17,12,4,0.783852,5140,eleftherios avramidis,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"Significant breakthroughs in machine translation only seem possible if human translators are taken into the loop. While automatic evaluation and scoring mechanisms such as BLEU have enabled the fast development of systems, it is not clear how systems can meet real-world (quality) requirements in industrial translation scenarios today. The taraX{\""U} project paves the way for wide usage of hybrid machine translation outputs through various feedback loops in system development. In a consortium of research and industry partners, the project integrates human translators into the development process for rating and post-editing of machine translation outputs thus collecting feedback for possible improvements."
berka-etal-2012-automatic,Automatic {MT} Error Analysis: Hjerson Helping Addicter,2012,13,9,4,0,42951,jan berka,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"We present a complex, open source tool for detailed machine translation error analysis providing the user with automatic error detection and classification, several monolingual alignment algorithms as well as with training and test corpus browsing. The tool is the result of a merge of automatic error detection and classification of Hjerson (Popovi{\'c}, 2011) and Addicter (Zeman et al., 2011) into the pipeline and web visualization of Addicter. It classifies errors into categories similar to those of Vilar et al. (2006), such as: morphological, reordering, missing words, extra words and lexical errors. The graphical user interface shows alignments in both training corpus and test data; the different classes of errors are colored. Also, the summary of errors can be displayed to provide an overall view of the MT system's weaknesses. The tool was developed in Linux, but it was tested on Windows too."
fishel-etal-2012-terra,{T}erra: a Collection of Translation Error-Annotated Corpora,2012,19,13,3,0,13955,mark fishel,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"Recently the first methods of automatic diagnostics of machine translation have emerged; since this area of research is relatively young, the efforts are not coordinated. We present a collection of translation error-annotated corpora, consisting of automatically produced translations and their detailed manual translation error analysis. Using the collected corpora we evaluate the available state-of-the-art methods of MT diagnostics and assess, how well the methods perform, how they compare to each other and whether they can be useful in practice."
W11-2104,Evaluate with Confidence Estimation: Machine ranking of translation outputs using grammatical features,2011,20,22,2,0.783852,5140,eleftherios avramidis,Proceedings of the Sixth Workshop on Statistical Machine Translation,0,"We present a pilot study on an evaluation method which is able to rank translation outputs with no reference translation, given only their source sentence. The system employs a statistical classifier trained upon existing human rankings, using several features derived from analysis of both the source and the target sentences. Development experiments on one language pair showed that the method has considerably good correlation with human ranking when using features obtained from a PCFG parser."
W11-2109,Evaluation without references: {IBM}1 scores as evaluation metrics,2011,9,15,1,1,5059,maja popovic,Proceedings of the Sixth Workshop on Statistical Machine Translation,0,"Current metrics for evaluating machine translation quality have the huge drawback that they require human-quality reference translations. We propose a truly automatic evaluation metric based on ibm1 lexicon probabilities which does not need any reference translations. Several variants of ibm1 scores are systematically explored in order to find the most promising directions. Correlations between the new metrics and human judgments are calculated on the data of the third, fourth and fifth shared tasks of the Statistical Machine Translation Workshop. Five different European languages are taken into account: English, Spanish, French, German and Czech. The results show that the ibm1 scores are competitive with the classic evaluation metrics, the most promising being ibm1 scores calculated on morphemes and pos-4grams."
W11-2110,Morphemes and {POS} tags for n-gram based evaluation metrics,2011,7,17,1,1,5059,maja popovic,Proceedings of the Sixth Workshop on Statistical Machine Translation,0,"We propose the use of morphemes for automatic evaluation of machine translation output, and systematically investigate a set of F score and bleu score based metrics calculated on words, morphemes and pos tags along with all corresponding combinations. Correlations between the new metrics and human judgments are calculated on the data of the third, fourth and fifth shared tasks of the Statistical Machine Translation Workshop. Machine translation outputs in five different European languages are used: English, Spanish, French, German and Czech. The results show that the F scores which take into account morphemes and POS tags are the most promising metrics."
J11-4002,Towards Automatic Error Analysis of Machine Translation Output,2011,23,61,1,1,5059,maja popovic,Computational Linguistics,0,"Evaluation and error analysis of machine translation output are important but difficult tasks. In this article, we propose a framework for automatic error analysis and classification based on the identification of actual erroneous words using the algorithms for computation of Word Error Rate (WER) and Position-independent word Error Rate (PER), which is just a very first step towards development of automatic evaluation measures that provide more specific information of certain translation problems. The proposed approach enables the use of various types of linguistic knowledge in order to classify translation errors in many different ways. This work focuses on one possible set-up, namely, on five error categories: inflectional errors, errors due to wrong word order, missing words, extra words, and incorrect lexical choices. For each of the categories, we analyze the contribution of various POS classes. We compared the results of automatic error analysis with the results of human error analysis in order to investigate two possible applications: estimating the contribution of each error type in a given translation output in order to identify the main sources of errors for a given translation system, and comparing different translation outputs using the introduced error categories in order to obtain more information about advantages and disadvantages of different systems and possibilites for improvements, as well as about advantages and disadvantages of applied methods for improvements. We used Arabic-English Newswire and Broadcast News and Chinese-English Newswire outputs created in the framework of the GALE project, several Spanish and English European Parliament outputs generated during the TC-Star project, and three German-English outputs generated in the framework of the fourth Machine Translation Workshop. We show that our results correlate very well with the results of a human error analysis, and that all our metrics except the extra words reflect well the differences between different versions of the same translation system as well as the differences between different translation systems."
2011.iwslt-evaluation.13,{DFKI}{'}s {SC} and {MT} submissions to {IWSLT} 2011,2011,26,4,3,0.176471,5800,david vilar,Proceedings of the 8th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"We describe DFKI{'}s submission to the System Combination and Machine Translation tracks of the 2011 IWSLT Evaluation Campaign. We focus on a sentence selection mechanism which chooses the (hopefully) best sentence among a set of candidates. The rationale behind it is to take advantage of the strengths of each system, especially given an heterogeneous dataset like the one in this evaluation campaign, composed of TED Talks of very different topics. We focus on using features that correlate well with human judgement and, while our primary system still focus on optimizing the BLEU score on the development set, our goal is to move towards optimizing directly the correlation with human judgement. This kind of system is still under development and was used as a secondary submission."
2011.eamt-1.36,From Human to Automatic Error Classification for Machine Translation Output,2011,13,18,1,1,5059,maja popovic,Proceedings of the 15th Annual conference of the European Association for Machine Translation,0,"Future improvement of machine translation systems requires reliable automatic evaluation and error classification measures to avoid time and money consuming human classification. In this article, we propose a new method for automatic error classification and systematically compare its results to those obtained by humans. We show that the proposed automatic measures correlate well with human judgments across different error classes as well as across different translation outputs on four out of five commonly used error classes."
W09-0402,Syntax-Oriented Evaluation Measures for Machine Translation Output,2009,6,31,1,1,5059,maja popovic,Proceedings of the Fourth Workshop on Statistical Machine Translation,0,"We explored novel automatic evaluation measures for machine translation output oriented to the syntactic structure of the sentence: the Bleu score on the detailed Part-of-Speech (pos) tags as well as the precision, recall and F-measure obtained on pos n-grams. We also introduced F-measure based on both word and pos n-grams. Correlations between the new metrics and human judgments were calculated on the data of the first, second and third shared task of the Statistical Machine Translation Workshop. Machine translation outputs in four different European languages were taken into account: English, Spanish, French and German. The results show that the new measures correlate very well with the human judgements and that they are competitive with the widely used BLEU, METEOR and TER metrics."
W09-0410,The {RWTH} Machine Translation System for {WMT} 2009,2009,40,10,1,1,5059,maja popovic,Proceedings of the Fourth Workshop on Statistical Machine Translation,0,"RWTH participated in the shared translation task of the Fourth Workshop of Statistical Machine Translation (WMT 2009) with the German-English, French-English and Spanish-English pair in each translation direction. The submissions were generated using a phrase-based and a hierarchical statistical machine translation systems with appropriate morpho-syntactic enhancements. pos-based reorderings of the source language for the phrase-based systems and splitting of German compounds for both systems were applied. For some tasks, a system combination was used to generate a final hypothesis. An additional English hypothesis was produced by combining all three final systems for translation into English."
W07-0707,Word Error Rates: Decomposition over {POS} classes and Applications for Error Analysis,2007,16,66,1,1,5059,maja popovic,Proceedings of the Second Workshop on Statistical Machine Translation,0,"Evaluation and error analysis of machine translation output are important but difficult tasks. In this work, we propose a novel method for obtaining more details about actual translation errors in the generated output by introducing the decomposition of Word Error Rate (Wer) and Position independent word Error Rate (Per) over different Part-of-Speech (Pos) classes. Furthermore, we investigate two possible aspects of the use of these decompositions for automatic error analysis: estimation of inflectional errors and distribution of missing words over Pos classes. The obtained results are shown to correspond to the results of a human error analysis. The results obtained on the European Parliament Plenary Session corpus in Spanish and English give a better overview of the nature of translation errors as well as ideas of where to put efforts for possible improvements of the translation system."
W06-3101,Morpho-syntactic Information for Automatic Error Analysis of Statistical Machine Translation Output,2006,16,33,1,1,5059,maja popovic,Proceedings on the Workshop on Statistical Machine Translation,0,"Evaluation of machine translation output is an important but difficult task. Over the last years, a variety of automatic evaluation measures have been studied, some of them like Word Error Rate (WER), Position Independent Word Error Rate (PER) and BLEU and NIST scores have become widely used tools for comparing different systems as well as for evaluating improvements within one system. However, these measures do not give any details about the nature of translation errors. Therefore some analysis of the generated output is needed in order to identify the main problems and to focus the research efforts. On the other hand, human evaluation is a time consuming and expensive task. In this paper, we investigate methods for using of morpho-syntactic information for automatic evaluation: standard error measures WER and PER are calculated on distinct word classes and forms in order to get a better idea about the nature of translation errors and possibilities for improvements."
popovic-ney-2006-pos,{POS}-based Word Reorderings for Statistical Machine Translation,2006,6,82,1,1,5059,maja popovic,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"Translation In this work we investigate new possibilities for improving the quality of statistical machine translation (SMT) by applying word reorderings of the source language sentences based on Part-of-Speech tags. Results are presented on the European Parliament corpus containing about 700k sentences and 15M running words. In order to investigate sparse training data scenarios, we also report results obtained on about 1{\textbackslash}{\%} of the original corpus. The source languages are Spanish and English and target languages are Spanish, English and German. We propose two types of reorderings depending on the language pair and the translation direction: local reorderings of nouns and adjectives for translation from and into Spanish and long-range reorderings of verbs for translation into German. For our best translation system, we achieve up to 2{\textbackslash}{\%} relative reduction of WER and up to 7{\textbackslash}{\%} relative increase of BLEU score. Improvements can be seen both on the reordered sentences as well as on the rest of the test corpus. Local reorderings are especially important for the translation systems trained on the small corpus whereas long-range reorderings are more effective for the larger corpus."
2006.iwslt-papers.7,{AER}: do we need to {``}improve{''} our alignments?,2006,17,31,2,0.176471,5800,david vilar,Proceedings of the Third International Workshop on Spoken Language Translation: Papers,0,"Currently most statistical machine translation systems make use of alignments as a first step in the process of training the actual translation models. Several researchers have investigated how to improve the alignment quality, with the (intuitive) assumption that better alignments increase the translation quality. In this paper we will investigate this assumption and show that this is not always the case."
W05-0806,Augmenting a Small Parallel Text with Morpho-Syntactic Language,2005,0,4,1,1,5059,maja popovic,Proceedings of the {ACL} Workshop on Building and Using Parallel Texts,0,None
2005.eamt-1.29,Exploiting phrasal lexica and additional morpho-syntactic language resources for statistical machine translation with scarce training data,2005,7,12,1,1,5059,maja popovic,Proceedings of the 10th EAMT Conference: Practical applications of machine translation,0,"In this work, the use of a phrasal lexicon for statistical machine translation is proposed, and the relation between data acquisition costs and translation quality for different types and sizes of language resources has been analyzed. The language pairs are Spanish-English and Catalan-English, and the translation is performed in all directions. The phrasal lexicon is used to increase as well as to replace the original training corpus. The augmentation of the phrasal lexicon with the help of additional monolingual language resources containing morpho-syntactic information has been investigated for the translation with scarce training material. Using the augmented phrasal lexicon as additional training data, a reasonable translation quality can be achieved with only 1000 sentence pairs from the desired domain."
W04-3235,Error Measures and {B}ayes Decision Rules Revisited with Applications to {POS} Tagging,2004,12,4,2,0,3262,hermann ney,Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing,0,"Starting from first principles, we re-visit the statistical approach and study two forms of the Bayes decision rule: the common rule for minimizing the number of string errors and a novel rule for minimizing the number of symbols errors. The Bayes decision rule for minimizing the number of string errors is widely used, e.g. in speech recognition, POS tagging and machine translation, but its justification is rarely questioned. To minimize the number of symbol errors as is more suitable for a task like POS tagging, we show that another form of the Bayes decision rule can be derived. The major purpose of this paper is to show that the form of the Bayes decision rule should not be taken for granted (as it is done in virtually all statistical NLP work), but should be adapted to the error measure being used. We present first experimental results for POS tagging tasks."
popovic-ney-2004-towards,Towards the Use of Word Stems and Suffixes for Statistical Machine Translation,2004,9,61,1,1,5059,maja popovic,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"In this paper we present methods for improving the quality of translation from an inflected language into English by making use of part-of-speech tags and word stems and suffixes in the source language. Results for translations from Spanish and Catalan into English are presented on the LC-STAR trilingual corpus which consists of spontaneously spoken dialogues in the domain of travelling and appointment scheduling. Results for translation from Serbian into English are presented on the Assimil language course, the bilingual corpus from unrestricted domain. We achieve up to 5% relative reduction of error rates for Spanish and Catalan and about 8% for Serbian."
C04-1045,Improving Word Alignment Quality using Morpho-syntactic Information,2004,9,17,2,0,3262,hermann ney,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"In this paper, we present an approach to include morpho-syntactic dependencies into the training of the statistical alignment models. Existing statistical translation systems usually treat different derivations of the same base form as they were independent of each other. We propose a method which explicitly takes into account such in-terdependencies during the EM training of the statistical alignment models. The evaluation is done by comparing the obtained Viterbi alignments with a manually annotated reference alignment. The improvements of the alignment quality compared to the, to our knowledge, best system are reported on the German-English Verbmobil corpus."
2004.iwslt-papers.7,Statistical machine translation of spontaneous speech with scarce resources,2004,15,7,2,0,5736,evgeny matusov,Proceedings of the First International Workshop on Spoken Language Translation: Papers,0,This paper deals with the task of statistical machine translation of spontaneous speech using a limited amount of training data. We propose a method for selecting relevant additional training data from other sources that may come from other domains. We present two ways to solve the data sparseness problem by including morphological information into the EM training of word alignments. We show that the use of part-of-speech information for harmonizing word order between source and target sentences yields significant improvements in the BLEU score.
