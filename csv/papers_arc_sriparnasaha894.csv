2021.naacl-main.456,Towards Sentiment and Emotion aided Multi-modal Speech Act Classification in {T}witter,2021,-1,-1,3,0,4601,tulika saha,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Speech Act Classification determining the communicative intent of an utterance has been investigated widely over the years as a standalone task. This holds true for discussion in any fora including social media platform such as Twitter. But the emotional state of the tweeter which has a considerable effect on the communication has not received the attention it deserves. Closely related to emotion is sentiment, and understanding of one helps understand the other. In this work, we firstly create a new multi-modal, emotion-TA ({`}TA{'} means tweet act, i.e., speech act in Twitter) dataset called \textit{EmoTA} collected from open-source Twitter dataset. We propose a Dyadic Attention Mechanism (DAM) based multi-modal, adversarial multi-tasking framework. DAM incorporates intra-modal and inter-modal attention to fuse multiple modalities and learns generalized features across all the tasks. Experimental results indicate that the proposed framework boosts the performance of the primary task, i.e., TA classification (TAC) by benefitting from the two secondary tasks, i.e., Sentiment and Emotion Analysis compared to its uni-modal and single task TAC (tweet act classification) variants."
2021.findings-acl.328,Multimodal Graph-based Transformer Framework for Biomedical Relation Extraction,2021,-1,-1,4,0,8270,sriram pingali,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,0,None
2020.semeval-1.214,{EL}-{BERT} at {S}em{E}val-2020 Task 10: A Multi-Embedding Ensemble Based Approach for Emphasis Selection in Visual Media,2020,-1,-1,2,0,15304,chandresh kanani,Proceedings of the Fourteenth Workshop on Semantic Evaluation,0,"In visual media, text emphasis is the strengthening of words in a text to convey the intent of the author. Text emphasis in visual media is generally done by using different colors, backgrounds, or font for the text; it helps in conveying the actual meaning of the message to the readers. Emphasis selection is the task of choosing candidate words for emphasis, it helps in automatically designing posters and other media contents with written text. If we consider only the text and do not know the intent, then there can be multiple valid emphasis selections. We propose the use of ensembles for emphasis selection to improve over single emphasis selection models. We show that the use of multi-embedding helps in enhancing the results for base models. To show the efficacy of proposed approach we have also done a comparison of our results with state-of-the-art models."
2020.semeval-1.253,{C}yber{T}ronics at {S}em{E}val-2020 Task 12: Multilingual Offensive Language Identification over Social Media,2020,-1,-1,2,0,15355,sayanta paul,Proceedings of the Fourteenth Workshop on Semantic Evaluation,0,"The SemEval-2020 Task 12 (OffensEval) challenge focuses on detection of signs of offensiveness using posts or comments over social media. This task has been organized for several languages, e.g., Arabic, Danish, English, Greek and Turkish. It has featured three related sub-tasks for English language: sub-task A was to discriminate between offensive and non-offensive posts, the focus of sub-task B was on the type of offensive content in the post and finally, in sub-task C, proposed systems had to identify the target of the offensive posts. The corpus for each of the languages is developed using the posts and comments over Twitter, a popular social media platform. We have participated in this challenge and submitted results for different languages. The current work presents different machine learning and deep learning techniques and analyzes their performance for offensiveness prediction which involves various classifiers and feature engineering schemes. The experimental analysis on the training set shows that SVM using language specific pre-trained word embedding (Fasttext) outperforms the other methods. Our system achieves a macro-averaged F1 score of 0.45 for Arabic language, 0.43 for Greek language and 0.54 for Turkish language."
2020.sdp-1.27,"{IIITBH}-{IITP}@{CL}-{S}ci{S}umm20, {CL}-{L}ay{S}umm20, {L}ong{S}umm20",2020,-1,-1,3,0,13721,saichethan reddy,Proceedings of the First Workshop on Scholarly Document Processing,0,"In this paper, we present the IIIT Bhagalpur and IIT Patna team{'}s effort to solve the three shared tasks namely, CL-SciSumm 2020, CL-LaySumm 2020, LongSumm 2020 at SDP 2020. The theme of these tasks is to generate medium-scale, lay and long summaries, respectively, for scientific articles. For the first two tasks, unsupervised systems are developed, while for the third one, we develop a supervised system.The performances of all the systems were evaluated on the associated datasets with the shared tasks in term of well-known ROUGE metric."
2020.sdp-1.30,"{IITP}-{AI}-{NLP}-{ML}@ {CL}-{S}ci{S}umm 2020, {CL}-{L}ay{S}umm 2020, {L}ong{S}umm 2020",2020,-1,-1,4,0,15466,santosh mishra,Proceedings of the First Workshop on Scholarly Document Processing,0,"The publication rate of scientific literature increases rapidly, which poses a challenge for researchers to keep themselves updated with new state-of-the-art. Scientific document summarization solves this problem by summarizing the essential fact and findings of the document. In the current paper, we present the participation of IITP-AI-NLP-ML team in three shared tasks, namely, CL-SciSumm 2020, LaySumm 2020, LongSumm 2020, which aims to generate medium, lay, and long summaries of the scientific articles, respectively. To solve CL-SciSumm 2020 and LongSumm 2020 tasks, three well-known clustering techniques are used, and then various sentence scoring functions, including textual entailment, are used to extract the sentences from each cluster for a summary generation. For LaySumm 2020, an encoder-decoder based deep learning model has been utilized. Performances of our developed systems are evaluated in terms of ROUGE measures on the associated datasets with the shared task."
2020.icon-main.25,Semantic Extractor-Paraphraser based Abstractive Summarization,2020,-1,-1,4,0,19127,anubhav jangra,Proceedings of the 17th International Conference on Natural Language Processing (ICON),0,"The anthology of spoken languages today is inundated with textual information, necessitating the development of automatic summarization models. In this manuscript, we propose an extractor-paraphraser based abstractive summarization system that exploits semantic overlap as opposed to its predecessors that focus more on syntactic information overlap. Our model outperforms the state-of-the-art baselines in terms of ROUGE, METEOR and word mover similarity (WMS), establishing the superiority of the proposed system via extensive ablation experiments. We have also challenged the summarization capabilities of the state of the art Pointer Generator Network (PGN), and through thorough experimentation, shown that PGN is more of a paraphraser, contrary to the prevailing notion of a summarizer; illustrating it{'}s incapability to accumulate information across multiple sentences."
2020.icon-main.42,A Multi-modal Personality Prediction System,2020,-1,-1,3,0,19157,chanchal suman,Proceedings of the 17th International Conference on Natural Language Processing (ICON),0,"Automatic prediction of personality traits has many real-life applications, e.g., in forensics, recommender systems, personalized services etc.. In this work, we have proposed a solution framework for solving the problem of predicting the personality traits of a user from videos. Ambient, facial and the audio features are extracted from the video of the user. These features are used for the final output prediction. The visual and audio modalities are combined in two different ways: averaging of predictions obtained from the individual modalities, and concatenation of features in multi-modal setting. The dataset released in Chalearn-16 is used for evaluating the performance of the system. Experimental results illustrate that it is possible to obtain better performance with a hand full of images, rather than using all the images present in the video"
2020.icon-main.43,{D}-Coref: A Fast and Lightweight Coreference Resolution Model using {D}istil{BERT},2020,-1,-1,3,0,19157,chanchal suman,Proceedings of the 17th International Conference on Natural Language Processing (ICON),0,"Smart devices are often deployed in some edge-devices, which require quality solutions in limited amount of memory usage. In most of the user-interaction based smart devices, coreference resolution is often required. Keeping this in view, we have developed a fast and lightweight coreference resolution model which meets the minimum memory requirement and converges faster. In order to generate the embeddings for solving the task of coreference resolution, DistilBERT, a light weight BERT module is utilized. DistilBERT consumes less memory (only 60{\%} of memory in comparison to BERT-based heavy model) and it is suitable for deployment in edge devices. DistilBERT embedding helps in 60{\%} faster convergence with an accuracy compromise of 2.59{\%}, and 6.49{\%} with respect to its base model and current state-of-the-art, respectively."
2020.icon-main.62,Annotated Corpus of Tweets in {E}nglish from Various Domains for Emotion Detection,2020,-1,-1,4,0,15366,soumitra ghosh,Proceedings of the 17th International Conference on Natural Language Processing (ICON),0,"Emotion recognition is a very well-attended problem in Natural Language Processing (NLP). Most of the existing works on emotion recognition focus on the general domain and in some cases to specific domains like fairy tales, blogs, weather, Twitter etc. But emotion analysis systems in the domains of security, social issues, technology, politics, sports, etc. are very rare. In this paper, we create a benchmark setup for emotion recognition in these specialised domains. First, we construct a corpus of 18,921 tweets in English annotated with Paul Ekman{'}s six basic emotions (Anger, Disgust, Fear, Happiness, Sadness, Surprise) and a non-emotive class Others. Thereafter, we propose a deep neural framework to perform emotion recognition in an end-to-end setting. We build various models based on Convolutional Neural Network (CNN), Bi-directional Long Short Term Memory (Bi-LSTM), Bi-directional Gated Recurrent Unit (Bi-GRU). We propose a Hierarchical Attention-based deep neural network for Emotion Detection (HAtED). We also develop multiple systems by considering different sets of emotion classes for each system and report the detailed comparative analysis of the results. Experiments show the hierarchical attention-based model achieves best results among the considered baselines with accuracy of 69{\%}."
2020.acl-main.402,Towards Emotion-aided Multi-modal Dialogue Act Classification,2020,-1,-1,3,0,4601,tulika saha,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"The task of Dialogue Act Classification (DAC) that purports to capture communicative intent has been studied extensively. But these studies limit themselves to text. Non-verbal features (change of tone, facial expressions etc.) can provide cues to identify DAs, thus stressing the benefit of incorporating multi-modal inputs in the task. Also, the emotional state of the speaker has a substantial effect on the choice of the dialogue act, since conversations are often influenced by emotions. Hence, the effect of emotion too on automatic identification of DAs needs to be studied. In this work, we address the role of \textit{both} multi-modality and emotion recognition (ER) in DAC. DAC and ER help each other by way of multi-task learning. One of the major contributions of this work is a new dataset- multimodal Emotion aware Dialogue Act dataset called EMOTyDA, collected from open-sourced dialogue datasets. To demonstrate the utility of EMOTyDA, we build an attention based (self, inter-modal, inter-task) multi-modal, multi-task Deep Neural Network (DNN) for joint learning of DAs and emotions. We show empirically that multi-modality and multi-tasking achieve better performance of DAC compared to uni-modal and single task DAC variants."
2020.acl-main.570,"Amalgamation of protein sequence, structure and textual information for improving protein-protein interaction identification",2020,-1,-1,2,0,8272,pratik dutta,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"An in-depth exploration of protein-protein interactions (PPI) is essential to understand the metabolism in addition to the regulations of biological entities like proteins, carbohydrates, and many more. Most of the recent PPI tasks in BioNLP domain have been carried out solely using textual data. In this paper, we argue that incorporating multimodal cues can improve the automatic identification of PPI. As a first step towards enabling the development of multimodal approaches for PPI identification, we have developed two multi-modal datasets which are extensions and multi-modal versions of two popular benchmark PPI corpora (BioInfer and HRPD50). Besides, existing textual modalities, two new modalities, 3D protein structure and underlying genomic sequence, are also added to each instance. Further, a novel deep multi-modal architecture is also implemented to efficiently predict the protein interactions from the developed datasets. A detailed experimental analysis reveals the superiority of the multi-modal approach in comparison to the strong baselines including unimodal approaches and state-of the-art methods over both the generated multi-modal datasets. The developed multi-modal datasets are available for use at https://github.com/sduttap16/MM{\_}PPI{\_}NLP."
P19-1516,A Unified Multi-task Adversarial Learning Framework for Pharmacovigilance Mining,2019,0,1,3,1,8271,shweta yadav,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"The mining of adverse drug reaction (ADR) has a crucial role in the pharmacovigilance. The traditional ways of identifying ADR are reliable but time-consuming, non-scalable and offer a very limited amount of ADR relevant information. With the unprecedented growth of information sources in the forms of social media texts (Twitter, Blogs, Reviews etc.), biomedical literature, and Electronic Medical Records (EMR), it has become crucial to extract the most pertinent ADR related information from these free-form texts. In this paper, we propose a neural network inspired multi- task learning framework that can simultaneously extract ADRs from various sources. We adopt a novel adversarial learning-based approach to learn features across multiple ADR information sources. Unlike the other existing techniques, our approach is capable to extracting fine-grained information (such as {`}Indications{'}, {`}Symptoms{'}, {`}Finding{'}, {`}Disease{'}, {`}Drug{'}) which provide important cues in pharmacovigilance. We evaluate our proposed approach on three publicly available real- world benchmark pharmacovigilance datasets, a Twitter dataset from PSB 2016 Social Me- dia Shared Task, CADEC corpus and Medline ADR corpus. Experiments show that our unified framework achieves state-of-the-art performance on individual tasks associated with the different benchmark datasets. This establishes the fact that our proposed approach is generic, which enables it to achieve high performance on the diverse datasets."
N18-2044,Multi-Task Learning Framework for Mining Crowd Intelligence towards Clinical Treatment,2018,0,6,3,1,8271,shweta yadav,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",0,"In recent past, social media has emerged as an active platform in the context of healthcare and medicine. In this paper, we present a study where medical user{'}s opinions on health-related issues are analyzed to capture the medical sentiment at a blog level. The medical sentiments can be studied in various facets such as medical condition, treatment, and medication that characterize the overall health status of the user. Considering these facets, we treat analysis of this information as a multi-task classification problem. In this paper, we adopt a novel adversarial learning approach for our multi-task learning framework to learn the sentiment{'}s strengths expressed in a medical blog. Our evaluation shows promising results for our target tasks."
L18-1442,Medical Sentiment Analysis using Social Media: Towards building a Patient Assisted System,2018,0,5,3,1,8271,shweta yadav,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
C18-1321,Incorporating Deep Visual Features into Multiobjective based Multi-view Search Results Clustering,2018,0,1,3,0,30937,sayantan mitra,Proceedings of the 27th International Conference on Computational Linguistics,0,"Current paper explores the use of multi-view learning for search result clustering. A web-snippet can be represented using multiple views. Apart from textual view cued by both the semantic and syntactic information, a complimentary view extracted from images contained in the web-snippets is also utilized in the current framework. A single consensus partitioning is finally obtained after consulting these two individual views by the deployment of a multiobjective based clustering technique. Several objective functions including the values of a cluster quality measure measuring the goodness of partitionings obtained using different views and an agreement-disagreement index, quantifying the amount of oneness among multiple views in generating partitionings are optimized simultaneously using AMOSA. In order to detect the number of clusters automatically, concepts of variable length solutions and a vast range of permutation operators are introduced in the clustering process. Finally, a set of alternative partitioning are obtained on the final Pareto front by the proposed multi-view based multiobjective technique. Experimental results by the proposed approach on several benchmark test datasets of SRC with respect to different performance metrics evidently establish the power of visual and text-based views in achieving better search result clustering."
P17-2104,Temporal Orientation of Tweets for Predicting Income of Users,2017,7,4,4,0.714286,15356,mohammed hasanuzzaman,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Automatically estimating a user{'}s socio-economic profile from their language use in social media can significantly help social science research and various downstream applications ranging from business to politics. The current paper presents the first study where user cognitive structure is used to build a predictive model of income. In particular, we first develop a classifier using a weakly supervised learning framework to automatically time-tag tweets as past, present, or future. We quantify a user{'}s overall temporal orientation based on their distribution of tweets, and use it to build a predictive model of income. Our analysis uncovers a correlation between future temporal orientation and income. Finally, we measure the predictive power of future temporal orientation on income by performing regression."
E17-1109,Entity Extraction in Biomedical Corpora: An Approach to Evaluate Word Embedding Features with {PSO} based Feature Selection,2017,30,7,3,1,8271,shweta yadav,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers",0,"Text mining has drawn significant attention in recent past due to the rapid growth in biomedical and clinical records. Entity extraction is one of the fundamental components for biomedical text mining. In this paper, we propose a novel approach of feature selection for entity extraction that exploits the concept of deep learning and Particle Swarm Optimization (PSO). The system utilizes word embedding features along with several other features extracted by studying the properties of the datasets. We obtain an interesting observation that compact word embedding features as determined by PSO are more effective compared to the entire word embedding feature set for entity extraction. The proposed system is evaluated on three benchmark biomedical datasets such as GENIA, GENETAG, and AiMed. The effectiveness of the proposed approach is evident with significant performance gains over the baseline models as well as the other existing systems. We observe improvements of 7.86{\%}, 5.27{\%} and 7.25{\%} F-measure points over the baseline models for GENIA, GENETAG, and AiMed dataset respectively."
W16-6311,Improving Document Ranking using Query Expansion and Classification Techniques for Mixed Script Information Retrieval,2016,0,0,5,0,15327,subham kumar,Proceedings of the 13th International Conference on Natural Language Processing,0,None
W16-6325,A Recurrent Neural Network Architecture for De-identifying Clinical Records,2016,0,7,4,0,33377,shweta,Proceedings of the 13th International Conference on Natural Language Processing,0,None
W16-4205,Semi-supervised Clustering of Medical Text,2016,10,0,3,0,33603,pracheta sahoo,Proceedings of the Clinical Natural Language Processing Workshop ({C}linical{NLP}),0,"Semi-supervised clustering is an attractive alternative for traditional (unsupervised) clustering in targeted applications. By using the information of a small annotated dataset, semi-supervised clustering can produce clusters that are customized to the application domain. In this paper, we present a semi-supervised clustering technique based on a multi-objective evolutionary algorithm (NSGA-II-clus). We apply this technique to the task of clustering medical publications for Evidence Based Medicine (EBM) and observe an improvement of the results against unsupervised and other semi-supervised clustering techniques."
W16-4206,Deep Learning Architecture for Patient Data De-identification in Clinical Records,2016,0,14,3,1,8271,shweta yadav,Proceedings of the Clinical Natural Language Processing Workshop ({C}linical{NLP}),0,"Rapid growth in Electronic Medical Records (EMR) has emerged to an expansion of data in the clinical domain. The majority of the available health care information is sealed in the form of narrative documents which form the rich source of clinical information. Text mining of such clinical records has gained huge attention in various medical applications like treatment and decision making. However, medical records enclose patient Private Health Information (PHI) which can reveal the identities of the patients. In order to retain the privacy of patients, it is mandatory to remove all the PHI information prior to making it publicly available. The aim is to de-identify or encrypt the PHI from the patient medical records. In this paper, we propose an algorithm based on deep learning architecture to solve this problem. We perform de-identification of seven PHI terms from the clinical records. Experiments on benchmark datasets show that our proposed approach achieves encouraging performance, which is better than the baseline model developed with Conditional Random Field."
S14-2052,{IITP}: A Supervised Approach for Disorder Mention Detection and Disambiguation,2014,4,0,3,1,28345,utpal sikdar,Proceedings of the 8th International Workshop on Semantic Evaluation ({S}em{E}val 2014),0,"In this paper we briefly describe our supervised machine learning approach for disorder mention detection system that we submitted as part of our participation in the SemEval-2014 Shared task. The main goal of this task is to build a system that automatically identifies mentions of clinical conditions from the clinical texts. The main challenge lies due in the fact that the same mention of concept may be represented in many surface forms. We develop the system based on the supervised machine learning algorithms, namely Conditional Random Field and Support Vector Machine. One appealing characteristics of our system is that most of the features for learning are extracted automatically from the given training or test datasets without using deep domain specific resources and/or tools. We submitted three runs, and best performing system is based on Conditional Random Field. For task A, it shows the precision, recall and F-measure values of 50.00%, 47.90% and 48.90%, respectively under the strict matching criterion. When the matching criterion is relaxed, it shows the precision, recall and F-measure of 81.50%, 79.70% and 80.60%, respectively. For task B, we obtain the accuracies of 33.30% and 69.60% for the relaxed and strict matches, respectively."
C14-1011,Multi-Objective Search Results Clustering,2014,24,3,2,0,40218,sudipta acharya,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"Most web search results clustering (SRC) strategies have predominantly studied the definition of adapted representation spaces to the detriment of new clustering techniques to improve perfor-mance. In this paper, we define SRC as a multi-objective optimization (MOO) problem to take advantage of most recent works in clustering. In particular, we define two objective functions (compactness and separability), which are simultaneously optimized using a MOO-based simu-lated annealing technique called AMOSA. The proposed algorithm is able to automatically detect the number of clusters for any query and outperforms all state-of-the-art text-based solutions in terms of F xcexb2 -measure and F b 3 -measure over two gold standard data sets."
U13-1008,Multi-Objective Optimization for Clustering of Medical Publications,2013,25,3,2,0.644938,363,asif ekbal,Proceedings of the Australasian Language Technology Association Workshop 2013 ({ALTA} 2013),0,"Clustering the results of a search can help a multi-document summarizer present a summary for evidence based medicine (EBM). In this work, we introduce a clustering technique that is based on multiobjective (MOO) optimization. MOO is a technique that shows promise in the areas of machine learning and natural language processing. In our approach we show how MOO based semi-supervised clustering technique can be effectively used for EBM."
I13-1099,Adapting a State-of-the-art Anaphora Resolution System for Resource-poor Language,2013,16,7,3,1,28345,utpal sikdar,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"In this paper we present our work on adapting a state-of-the-art anaphora resolution system for a resource poor language, namely Bengali. Performance of any anaphoric resolver greatly depends on the quality of a high accurate mention detector. We develop a number of models for mention detection based on heuristics and machine learning. Our experiments show that, a language-dependent system can attain reasonably good performance when re-trained on a new language with a proper subset of features. The system yields the MUC recall, precision and F-measure values of 57.80%, 79.00% and 66.70%, respectively. Our experiments with other available scorers show the F-measure values of 59.47%, 49.83%, 31.81% and 70.82% for BCUB, CEAFM, CEAFE and BLANC, respectively."
C12-1151,Differential Evolution Based Feature Selection and Classifier Ensemble for Named Entity Recognition,2012,24,14,3,1,28345,utpal sikdar,Proceedings of {COLING} 2012,0,"In this paper, we propose a differential evolution (DE) based two-stage evolutionary approach for named entity recognition (NER). The first stage concerns with the problem of relevant feature selection for NER within the frameworks of two popular machine learning algorithms, namely Conditional Random Field (CRF) and Support Vector Machine (SVM). The solutions of the final best population provides different diverse set of classifiers; some are effective with respect to recall whereas some are effective with respect to precision. In the second stage we propose a novel technique for classifier ensemble for combining these classifiers. The approach is very general and can be applied for any classification problem. Currently we evaluate the proposed algorithm for NER in three popular Indian languages, namely Bengali, Hindi and Telugu. In order to maintain the domain-independence property the features are selected and developed mostly without using any deep domain knowledge and/or language dependent resources. Experimental results show that the proposed two stage technique attains the final F-measure values of 88.89%, 88.09% and 76.63% for Bengali, Hindi and Telugu, respectively. The key contributions of this work are two-fold, viz. (i). proposal of differential evolution (DE) based feature selection and classifier ensemble methods that can be applied to any classification problem; and (ii). scope of the development of language independent NER systems in a resource-poor"
W11-1908,Multi-metric optimization for coreference: The {U}ni{TN} / {IITP} / {E}ssex submission to the 2011 {CONLL} Shared Task,2011,17,10,2,0,28598,olga uryupina,Proceedings of the Fifteenth Conference on Computational Natural Language Learning: Shared Task,0,"Because there is no generally accepted metric for measuring the performance of anaphora resolution systems, a combination of metrics was proposed to evaluate submissions to the 2011 CONLL Shared Task (Pradhan et al., 2011). We investigate therefore Multi-objective function Optimization (moo) techniques based on Genetic Algorithms to optimize models according to multiple metrics simultaneously."
I11-1011,Single and multi-objective optimization for feature selection in anaphora resolution,2011,22,9,1,1,4603,sriparna saha,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"There is no generally accepted metric for measuring the performance of anaphora resolution systems, and the existing metricsxe2x80x94MUC, B3, CEAF, Blanc, among othersxe2x80x94tend to reward significantly different behaviors. Systems optimized according to one metric tend to perform poorly with respect to other ones, making it very difficult to compare anaphora resolution systems, as clearly shown by the results of the SEMEVAL 2010 Multilingual Coreference task. One solution would be to find a single completely satisfactory metric, but itxe2x80x99s not clear whether this is possible and at any rate it is not going to happen any time soon. An alternative is to optimize models according to multiple metrics simultaneously. In this paper, we show, first of all, that this is possible to develop such models using Multi-objective Optimization (MOO) techniques based on Genetic Algorithms. Secondly, we show that optimizing according to multiple metrics simultaneously may result in better results with respect to each individual metric than optimizing according to that metric only."
Y10-1015,Finding Appropriate Subset of Votes Per Classifier Using Multiobjective Optimization: Application to Named Entity Recognition,2010,11,0,2,0,363,asif ekbal,"Proceedings of the 24th Pacific Asia Conference on Language, Information and Computation",0,"In this paper, we report a multiobjective optimization (MOO) based technique to select the appropriate subset of votes per classifier in an ensemble system. We hypothesize that the reliability of prediction of each classifier differs among the various output classes. Thus, it is necessary to find out the subset of classes for which any particular classifier is most suitable. Rather than optimizing a single measure of classification quality, we simultaneously optimize two different measures of classification quality using the search capability of MOO. We use our proposed technique to solve the problem of Named Entity Recognition (NER). Maximum Entropy (ME) model is used as a base to build a number of classifiers depending upon the various representations of the contextual, orthographic word-level and semantically motivated features. Evaluation results with a resource constrained language like Bengali yield the recall, precision and F-measure values of 87.98%, 93.00%, and 90.42%, respectively. Experimental results suggest that the use of semantic feature can significantly improve the overall system performance. Results also reveal that the classifier ensemble identified by the proposed MOO based approach performs better in comparison to the individual classifiers, two different baseline ensembles and the classifier ensemble identified by a single objective genetic algorithm (GA) based approach."
Y10-1019,Feature Subset Selection Using Genetic Algorithm for Named Entity Recognition,2010,15,7,2,0,25303,md hasanuzzaman,"Proceedings of the 24th Pacific Asia Conference on Language, Information and Computation",0,"In this paper, genetic algorithm (GA) is utilized to search for the appropriate feature combination for constructing a maximum entropy (ME) based classifier for named entity recognition (NER). Features are encoded in the chromosomes. The ME classifier is evaluated for the 3-fold cross validation with the features, encoded in a particular chromosome, and its average F-measure value is used as the fitness value of the corresponding chromosome. The proposed technique is evaluated for determining the suitable feature combinations for NER in three resource-constrained languages, namely Bengali, Hindi and Telugu. Evaluation results show the effectiveness of the proposed approach with the overall recall, precision and F-measure values of 71.27%, 83.95% and 77.09%, respectively for Bengali, 74.72%, 87.15% and 80.46%, respectively for Hindi and 60.91%, 94.15% and 73.97%, respectively for Telugu."
ekbal-saha-2010-maximum,Maximum Entropy Classifier Ensembling using Genetic Algorithm for {NER} in {B}engali,2010,22,5,2,0,363,asif ekbal,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"In this paper, we propose classifier ensemble selection for Named Entity Recognition (NER) as a single objective optimization problem. Thereafter, we develop a method based on genetic algorithm (GA) to solve this problem. Our underlying assumption is that rather than searching for the best feature set for a particular classifier, ensembling of several classifiers which are trained using different feature representations could be a more fruitful approach. Maximum Entropy (ME) framework is used to generate a number of classifiers by considering the various combinations of the available features. In the proposed approach, classifiers are encoded in the chromosomes. A single measure of classification quality, namely F-measure is used as the objective function. Evaluation results on a resource constrained language like Bengali yield the recall, precision and F-measure values of 71.14{\%}, 84.07{\%} and 77.11{\%}, respectively. Experiments also show that the classifier ensemble identified by the proposed GA based approach attains higher performance than all the individual classifiers and two different conventional baseline ensembles."
