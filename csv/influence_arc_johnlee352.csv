2006.amta-papers.24,N03-1017,0,0.00499491,"tences (speech transcriptions), which we do not want the SMT system to capture in its translation models. English Rules English Grammar English Sentence Language Generation Language Interlingua Understanding Bilingual Corpus Language Generation Chinese Rules Figure 1: Schematic of technique to automatically generate a synthetic bilingual corpus, for training a statistical translation system. Once this bilingual corpus is prepared, it can be used to train a statistical machine translation (SMT) system. For this we made use of a state-of-theart phrase-based MT system developed by Philipp Koehn (Koehn et al., 2003; Koehn, 2004). 3.2 Formal Translation In parallel, we have developed a linguistic method to translate the generated Chinese corpus back into English. We can use the English grammar rules as a reference to generate comparable Chinese grammar rules, such that there is considerable uniformity in the resulting meaning representations. Ideally, the two languages would produce an identical “interlingua” for sentences with equivalent meaning, and the generation rules would be agnostic to the input language. Our interlingual representation, in contrast to many other interlingual approaches, captures"
2006.amta-papers.24,koen-2004-pharaoh,0,0.019129,"n detailing our experimental results, we conclude with a discussion of related research and a look to the future. 3 Approach 3.1 Phrase-based SMT Given a corpus of English sentences within the flight domain and a reasonably high quality Englishto-Chinese translation system (Wang and Seneff, 2006b), we can easily generate a parallel EnglishChinese corpus, which can be used to train a statistical machine translation (SMT) system. This allows us to quickly develop a reverse Chinese-to-English translation capability, using publicly available SMT tools for training and decoding (Och and Ney, 2003; Koehn, 2004). As illustrated in Figure 1, we can automatically generate a corpus of English-Chinese pairs from the same interlingual representation by parsing our English corpus and then paraphrasing each utterance into both English and Chinese. It is our belief that the English paraphrase is preferred over the original English sentence because it is likely to be more consistent with the Chinese paraphrase. Furthermore, the English paraphrases usually remove disfluencies present in the original sentences (speech transcriptions), which we do not want the SMT system to capture in its translation models. Eng"
2006.amta-papers.24,P98-1116,0,0.00905462,"rkhi, 2000; Rayner and Carter, 1997), probably due to the existence of large English speech corpora obtained from spoken dialogue systems (such as ATIS and the Darpa funded Commmunicator Program). (Rayner and Carter, 1997) advocate combining rule-based and statistical methods to achieve robust and efficient performance within a linguistically motivated framework. Their system translates from English into Swedish and French, and uses statistical methods for word-sense disambiguation. They argue for utilizing expertise to develop generic grammar rules covering the core linguistic constructions (Langkilde and Knight, 1998) were pioneers in introducing the idea of using a linguistic generation system to overgenerate a set of candidate hypotheses for later selection by a statistical evaluation. Their Nitrogen language generation system processes from an underspecified semantic input, which they call an “AMR” (abstract meaning represention). It outputs a word graph representing a very large list of alternative realizations of the syntactic sugar missing from the AMR. An n-gram language model then selects the most plausible realization. As in our work, they generate both singular and plural forms of underspecified"
2006.amta-papers.24,J03-1002,0,0.00282882,"pus. After a section detailing our experimental results, we conclude with a discussion of related research and a look to the future. 3 Approach 3.1 Phrase-based SMT Given a corpus of English sentences within the flight domain and a reasonably high quality Englishto-Chinese translation system (Wang and Seneff, 2006b), we can easily generate a parallel EnglishChinese corpus, which can be used to train a statistical machine translation (SMT) system. This allows us to quickly develop a reverse Chinese-to-English translation capability, using publicly available SMT tools for training and decoding (Och and Ney, 2003; Koehn, 2004). As illustrated in Figure 1, we can automatically generate a corpus of English-Chinese pairs from the same interlingual representation by parsing our English corpus and then paraphrasing each utterance into both English and Chinese. It is our belief that the English paraphrase is preferred over the original English sentence because it is likely to be more consistent with the Chinese paraphrase. Furthermore, the English paraphrases usually remove disfluencies present in the original sentences (speech transcriptions), which we do not want the SMT system to capture in its translati"
2006.amta-papers.24,A00-2026,0,0.0768196,"a set of “keyword rules” map semantic and syntactic roles to grammatical constructs. However, because their representation is missing explicit syntactic information, the hypothesized syntactic organization is encapsulated in the (alternative) rules rather than directly in the AMR. Their notion of an “AMR recasting rule” is similar to our corrective rule rewrite mechanism, and allows an original AMR to be cast into related AMR’s. 7 Related Research A number of projects have involved language translation in the flight domain, particularly involving spoken language translation (Gao et al., 2002; Ratnaparkhi, 2000; Rayner and Carter, 1997), probably due to the existence of large English speech corpora obtained from spoken dialogue systems (such as ATIS and the Darpa funded Commmunicator Program). (Rayner and Carter, 1997) advocate combining rule-based and statistical methods to achieve robust and efficient performance within a linguistically motivated framework. Their system translates from English into Swedish and French, and uses statistical methods for word-sense disambiguation. They argue for utilizing expertise to develop generic grammar rules covering the core linguistic constructions (Langkilde"
2006.amta-papers.24,W00-0303,1,0.78795,"ction with the system. In this languagelearning mode, the student would be able to obtain translation assistance at any time over the course of a dialogue, by simply speaking a sentence in their native language with equivalent meaning. The system is then tasked with the challenging requirement to provide a fluent translation of their utterance. Two application domains where we have invested considerable previous effort, both towards multilingual dialogue interaction and towards translation assistance between English and Chinese, are the weather domain (Zue et al., 2000) and the flight domain (Seneff and Polifroni, 2000). We have previously reported on various strategies for achieving high quality and enhancing coverage and robustness for bidirectional speech translation in the weather domain (Wang and Seneff, 2006a; Lee and Seneff, 2005) and for translation from English to Chinese in the flight domain (Wang and Seneff, 2006b). This paper is focused on the specific (new) task of translating from Chinese to English in the flight domain. We have found in general, as might be expected, that the flight domain is considerably more difficult than the weather domain, due to the much larger number of attributes that"
2006.amta-papers.24,J92-1004,1,0.422595,"ed Chinese corpus back into English. We can use the English grammar rules as a reference to generate comparable Chinese grammar rules, such that there is considerable uniformity in the resulting meaning representations. Ideally, the two languages would produce an identical “interlingua” for sentences with equivalent meaning, and the generation rules would be agnostic to the input language. Our interlingual representation, in contrast to many other interlingual approaches, captures both syntactic and semantic information within a hierarchical structure. Through the device of a trace mechanism (Seneff, 1992) we are able to achieve a strong degree of parallelism in the meaning representations derived from both languages. For example, wh-marked NP’s in English are restored to their deep-structure location in the clause, as are temporals and locatives in Chinese. Figure 2 shows examples of the interlingual meaning representation automatically derived from a parse analysis for an English sentence and a Chinese sentence of equivalent meaning. As seen in the figure, syntactic structure is encoded in the hierarchy, with structural “frames” representing three principal linguistic categories: “{c }” = cla"
2006.amta-papers.24,C98-1112,0,\N,Missing
2019.ccnlg-1.6,N19-1423,0,0.00722978,"ng 332 retrieved from the Brown Corpus and 288 from the British Academic Written English (BAWE) Corpus (Nesi, 2008). The sentences contain 73 distinct verbs and 19 distinct adjectives, each with an average of 2.67 noun candidates. 4 Approach The noun generation algorithm used by Lee et al. (2018) considers only the word frequency statistics of the noun candidates. It therefore always chooses the same noun candidate for a verb (or adjective), even if the sentential context warrants a different choice due to word sense, register or fluency considerations. To remove this limitation, we use BERT (Devlin et al., 2019), a state-of-the-art neural language model based on the “Transformer” architecture (Vaswani et al., 2017). BERT has been shown to be effective in a wide range of natural language processing tasks. The model is bi-directional, i.e., trained to predict the identity of a masked word based on the words both before and after it. We consider the suitability of each noun candidate in the verb-to-noun and adjective-to-noun mappings as the masked word. In each sentence in our dataset, we mask the target noun and ask BERT for its word predictions for the masked position.3 Among the noun candidates, we i"
2019.ccnlg-1.6,N13-1092,0,0.0338298,"Missing"
2019.ccnlg-1.6,N03-1013,0,0.130422,"ies on automatic manipulation of clausal structure have mostly concentrated on syntactic simplification, typically by splitting a complex sentence into two or more simple sentences (Siddharthan, 2002; Alu´ısio et al., 2008; Narayan and Gardent, 2014). More recent research has also attempted semi-automatic nominalization (Lee et al., 2018), which aims to paraphrase a complex clause into a simplex clause by transforming verb or adjectival phrases into noun phrases. Noun generation is a core task in the nominalization pipeline (Table 2). Resources such as NOMLEX (Meyers et al., 1998) and CATVAR (Habash and Dorr, 2003) have greatly facilitated this task by providing lists of related nouns, verbs and adjectives. However, straightforward look-up in these lists does not suffice since a word may have multiple nominalized forms with similar meaning. For example, the verb “dominate” can be transformed into “domination”, “dominance”, “dominion”, as well as the gerund form “dominating”. We will henceforth refer to these as the “noun candidates”. As shown in Table 1, in the context of the clause “The British dominated India”, “domination” would be preferred (i.e., “British domination of India”); in the context of th"
2019.ccnlg-1.6,W18-6706,1,0.900002,"ne, the text exhibits a “metaphorical reconstrual”, and the sentences are clausally simpler and lexically denser (e.g., the nominal group “Her death through ignorance of the rules”). 1 This example and the next are both taken from Halliday and Matthiessen (2014). Previous studies on automatic manipulation of clausal structure have mostly concentrated on syntactic simplification, typically by splitting a complex sentence into two or more simple sentences (Siddharthan, 2002; Alu´ısio et al., 2008; Narayan and Gardent, 2014). More recent research has also attempted semi-automatic nominalization (Lee et al., 2018), which aims to paraphrase a complex clause into a simplex clause by transforming verb or adjectival phrases into noun phrases. Noun generation is a core task in the nominalization pipeline (Table 2). Resources such as NOMLEX (Meyers et al., 1998) and CATVAR (Habash and Dorr, 2003) have greatly facilitated this task by providing lists of related nouns, verbs and adjectives. However, straightforward look-up in these lists does not suffice since a word may have multiple nominalized forms with similar meaning. For example, the verb “dominate” can be transformed into “domination”, “dominance”, “do"
2019.ccnlg-1.6,W98-0604,0,0.650312,"Matthiessen (2014). Previous studies on automatic manipulation of clausal structure have mostly concentrated on syntactic simplification, typically by splitting a complex sentence into two or more simple sentences (Siddharthan, 2002; Alu´ısio et al., 2008; Narayan and Gardent, 2014). More recent research has also attempted semi-automatic nominalization (Lee et al., 2018), which aims to paraphrase a complex clause into a simplex clause by transforming verb or adjectival phrases into noun phrases. Noun generation is a core task in the nominalization pipeline (Table 2). Resources such as NOMLEX (Meyers et al., 1998) and CATVAR (Habash and Dorr, 2003) have greatly facilitated this task by providing lists of related nouns, verbs and adjectives. However, straightforward look-up in these lists does not suffice since a word may have multiple nominalized forms with similar meaning. For example, the verb “dominate” can be transformed into “domination”, “dominance”, “dominion”, as well as the gerund form “dominating”. We will henceforth refer to these as the “noun candidates”. As shown in Table 1, in the context of the clause “The British dominated India”, “domination” would be preferred (i.e., “British dominati"
2019.ccnlg-1.6,P14-1041,0,0.0206081,"e complex clause “Because she didn’t know the rules, she died”1 ). Towards the other end of the cline, the text exhibits a “metaphorical reconstrual”, and the sentences are clausally simpler and lexically denser (e.g., the nominal group “Her death through ignorance of the rules”). 1 This example and the next are both taken from Halliday and Matthiessen (2014). Previous studies on automatic manipulation of clausal structure have mostly concentrated on syntactic simplification, typically by splitting a complex sentence into two or more simple sentences (Siddharthan, 2002; Alu´ısio et al., 2008; Narayan and Gardent, 2014). More recent research has also attempted semi-automatic nominalization (Lee et al., 2018), which aims to paraphrase a complex clause into a simplex clause by transforming verb or adjectival phrases into noun phrases. Noun generation is a core task in the nominalization pipeline (Table 2). Resources such as NOMLEX (Meyers et al., 1998) and CATVAR (Habash and Dorr, 2003) have greatly facilitated this task by providing lists of related nouns, verbs and adjectives. However, straightforward look-up in these lists does not suffice since a word may have multiple nominalized forms with similar meanin"
2020.coling-main.196,C10-2011,0,0.432379,"ng the typical LS pipeline (Paetzold and Specia, 2016), is closely related to ours. In the first step, the system identifies the non-academic target words with a binary classifier. The second step, Substitution Generation, proposes candidate substitutions for each target word. Finally, the system ranks these candidates to determine the optimal substitution. The candidate substitutions were harvested from PPDB (Pavlick et al., 2015) and WordNet (Fellbaum, 1998), and ranked with the TF-Ranking deep learning model (Pasumarthi et al., 2019). Automatically mined pairs of informal and formal words (Brooke et al., 2010) have been shown to be useful for natural language generation (Sheikha and Inkpen, 2011). Lexical formality analysis may also be beneficial for academic LS since academic vocabulary tends to be more formal. Although informalformal text pairs are now publicly available in large volume (Pavlick and Tetreault, 2016), they have not yet been exploited in academic LS. This work is licensed under a Creative Commons Attribution 4.0 International Licence. http://creativecommons.org/licenses/by/4.0/. Licence details: 2163 Proceedings of the 28th International Conference on Computational Linguistics, pag"
2020.coling-main.196,N19-1423,0,0.0172417,"Missing"
2020.coling-main.196,E14-1057,0,0.0743248,"Missing"
2020.coling-main.196,L16-1491,0,0.0217838,"ng its meaning (McCarthy and Navigli, 2009). Our task — academic LS — may be viewed as a special form of LS, with the additional requirement that the substitution be typical for academic discourse. We describe and evaluate a writing assistance system for academic word usage that incorporates lexical formality analysis. Experimental results show that the incorporation of formality analysis helps raise the performance in identifying suitable substitutions. 2 Previous work The system reported in Yimam et al. (2020), which performs academic lexical substitution (LS) using the typical LS pipeline (Paetzold and Specia, 2016), is closely related to ours. In the first step, the system identifies the non-academic target words with a binary classifier. The second step, Substitution Generation, proposes candidate substitutions for each target word. Finally, the system ranks these candidates to determine the optimal substitution. The candidate substitutions were harvested from PPDB (Pavlick et al., 2015) and WordNet (Fellbaum, 1998), and ranked with the TF-Ranking deep learning model (Pasumarthi et al., 2019). Automatically mined pairs of informal and formal words (Brooke et al., 2010) have been shown to be useful for"
2020.coling-main.196,N15-1023,0,0.155066,"y substitutions may be superfluous. One solution is to train a classifier to determine if a non-academic word requires revision (Yimam et al., 2020). While target word selection is an important task, it is not the focus of this study. We will assume as target words all non-academic words that are considered “informal” (e.g., the word “left” in Table 1), since informal words are likely to require revision in the academic context. For this purpose, we compiled a list of informal words, which include the 396 informal words from Brooke et al. (2010), and 4,198 words with human formality judgment (Pavlick and Nenkova, 2015). We will henceforth use the term informal word to refer to a word in this list. 3.3 Evaluation data We derived our evaluation data from the 2,474 sentences in the Concepts in Context (CoInCo) corpus (Kremer et al., 2014). As an “all-words” lexical substitution dataset, every word in the text that can be replaced with another (i.e., a “target word”) is manually annotated with the possible substitutions. We retained only those target words that are both non-academic and informal (cf. Section 3.2). For example, neither the word “university” nor “hydroponics” in Table 1 is considered a target wor"
2020.coling-main.196,P15-2070,0,0.063824,"Missing"
2020.coling-main.196,W11-2826,0,0.026873,"In the first step, the system identifies the non-academic target words with a binary classifier. The second step, Substitution Generation, proposes candidate substitutions for each target word. Finally, the system ranks these candidates to determine the optimal substitution. The candidate substitutions were harvested from PPDB (Pavlick et al., 2015) and WordNet (Fellbaum, 1998), and ranked with the TF-Ranking deep learning model (Pasumarthi et al., 2019). Automatically mined pairs of informal and formal words (Brooke et al., 2010) have been shown to be useful for natural language generation (Sheikha and Inkpen, 2011). Lexical formality analysis may also be beneficial for academic LS since academic vocabulary tends to be more formal. Although informalformal text pairs are now publicly available in large volume (Pavlick and Tetreault, 2016), they have not yet been exploited in academic LS. This work is licensed under a Creative Commons Attribution 4.0 International Licence. http://creativecommons.org/licenses/by/4.0/. Licence details: 2163 Proceedings of the 28th International Conference on Computational Linguistics, pages 2163–2168 Barcelona, Spain (Online), December 8-13, 2020 Input: Output: She She left"
2020.coling-main.196,2020.lrec-1.722,1,0.913115,"er, a user study demonstrate that students were able to use the system to improve text quality. 1 Introduction While most research on automatic writing assistance has focused on grammatical error correction (Ng et al., 2014), there has been increasing attention on analyzing academic writing, with respect to the writing style expected in the genre (Bailey, 2011). Examples of recent efforts include identification of argumentation structure (Zhang et al., 2017), automatic assistance for nominalization and sentence restructuring (Lee et al., 2019), and lexical substitution of academic vocabulary (Yimam et al., 2020). This paper focuses on the latter, an example of which is shown in Table 1. Lexical substitution (LS) is the subfield of natural language processing that aims to replace target words in a text without changing its meaning (McCarthy and Navigli, 2009). Our task — academic LS — may be viewed as a special form of LS, with the additional requirement that the substitution be typical for academic discourse. We describe and evaluate a writing assistance system for academic word usage that incorporates lexical formality analysis. Experimental results show that the incorporation of formality analysis"
2020.coling-main.196,P17-1144,0,0.0282089,"that lexical formality analysis can improve the quality of the suggestions, in comparison to a baseline that relies on the masked language model only. Further, a user study demonstrate that students were able to use the system to improve text quality. 1 Introduction While most research on automatic writing assistance has focused on grammatical error correction (Ng et al., 2014), there has been increasing attention on analyzing academic writing, with respect to the writing style expected in the genre (Bailey, 2011). Examples of recent efforts include identification of argumentation structure (Zhang et al., 2017), automatic assistance for nominalization and sentence restructuring (Lee et al., 2019), and lexical substitution of academic vocabulary (Yimam et al., 2020). This paper focuses on the latter, an example of which is shown in Table 1. Lexical substitution (LS) is the subfield of natural language processing that aims to replace target words in a text without changing its meaning (McCarthy and Navigli, 2009). Our task — academic LS — may be viewed as a special form of LS, with the additional requirement that the substitution be typical for academic discourse. We describe and evaluate a writing as"
2020.coling-main.196,P19-1328,0,0.0490267,"Missing"
2020.coling-main.309,N15-3022,0,0.0172162,"as pedagogical tools for translation in the technical domain. In a user study, novice translators revised Chinese translations of English patents through bilingual concordancing. Results show that concordancing with an in-domain bilingual corpus can yield greater improvement in translation quality of technical terms than a general-domain bilingual corpus. 1 Introduction Text corpora are increasingly used in language pedagogy, with many studies showing their effectiveness in data-driven language learning (Boulton, 2017), language exercise generation (Susanti et al., 2018) and assisted writing (Chang and Chang, 2015), among many other tasks. Bilingual corpora, however, remain underused in translation pedagogy. Most research has focused on exploiting bilingual data for training machine translation (MT) systems and for developing translation memory (TM) in computer-assisted translation tools. Professional translators generally favor dictionaries and search engines over corpora (Man et al., 2020). While these tools may be sufficient when translators work in a familiar domain, more domainspecific examples are often needed to handle unfamiliar terms and collocations in specialized areas. Complementing MT and T"
2020.coling-main.309,Y12-1012,0,0.0216573,"rough Sketch Engine (Kilgarriff et al., 2008). As a general-domain corpus, OPUS2 has more limited coverage of technical terms, and their translation may not necessarily reflect the terminology adopted in patents. In-domain version The student produced a final version via parallel concordancing with in-domain examples in PatentLex (Section 4). The search interface for PatentLex supported bilingual keyword search in the KWIC format.1 5.3 Translation quality assessment We evaluated the quality of the above translation versions both automatically and manually. In automatic evaluation, we followed Li et al. (2012) in using the BLEU score (Papineni et al., 2002). Manual evaluation focused on “technical terms”, broadly defined as noun phrases that bear a scientific or technical concept. Three human judges, all native speakers of Chinese with a bachelor’s or master’s degree in linguistics or language studies, performed the evaluation. Gold data. One of the judges identified all technical terms that appeared in the ten English abstracts, and then aligned them to their counterpart in the gold Chinese translations. After review by the other two judges, the list consisted of a total of 200 technical terms, am"
2020.coling-main.309,2009.mtsummit-wpt.3,1,0.799414,"Missing"
2020.coling-main.309,P02-1040,0,0.112979,"08). As a general-domain corpus, OPUS2 has more limited coverage of technical terms, and their translation may not necessarily reflect the terminology adopted in patents. In-domain version The student produced a final version via parallel concordancing with in-domain examples in PatentLex (Section 4). The search interface for PatentLex supported bilingual keyword search in the KWIC format.1 5.3 Translation quality assessment We evaluated the quality of the above translation versions both automatically and manually. In automatic evaluation, we followed Li et al. (2012) in using the BLEU score (Papineni et al., 2002). Manual evaluation focused on “technical terms”, broadly defined as noun phrases that bear a scientific or technical concept. Three human judges, all native speakers of Chinese with a bachelor’s or master’s degree in linguistics or language studies, performed the evaluation. Gold data. One of the judges identified all technical terms that appeared in the ten English abstracts, and then aligned them to their counterpart in the gold Chinese translations. After review by the other two judges, the list consisted of a total of 200 technical terms, amounting to an average of 20 terms per abstract."
2020.coling-main.309,W19-8714,1,0.697915,", 2009), a large-scale collection of comparable Chinese-English patents which will be used in our study. 4 Data General-domain corpus OPUS2, a large collection of freely available multilingual data (Tiedemann, 2009), served as the general-domain parallel corpus in our study. It includes text from political and administrative sources, user-provided movie subtitles, as well as software localisation, news providers, translated descriptions of medical products, religious texts and multilingual wikis and other websites. In-domain corpus Composed of over 300K Chinese and English patents, PatentLex (Tsou et al., 2019) has served as the dataset for patent MT shared tasks such as those organized by NTCIR workshops. Our in-domain bilingual data consisted of over 30 million parallel bilingual sentences extracted from PatentLex. 5 5.1 Evaluation Set-up Subjects Our subjects were 31 students in a postgraduate course in Linguistics at City University of Hong Kong. All students were native speakers of Chinese who were proficient in English, having met the university’s admissions requirement for IELTS. Our study focused on English-to-Chinese translation. This translation direction avoids L2-related errors in the ta"
2020.framenet-1.8,W14-1821,0,0.0488107,"Missing"
2020.framenet-1.8,W12-3716,0,0.018836,". Introduction such as those in FrameNet, Chinese Framenet (You and Liu, 2005), or Mandarin VerbNet (Liu, 2016; Liu and Chang, 2016; Liu, 2018; Liu, 2019). Based on Mandarin VerbNet, a verbal semantic database that adopts a frame-based approach, this paper investigates the correlation between verb frames and text difficulty. FrameNet (https://framenet.icsi.berkeley.edu) and other similar resources have supported a large range of natural language processing (NLP) tasks including semantic role labeling (Gildea and Jurafsky, 2002), information extraction (Fader et al., 2011), sentiment analysis (Ruppenhofer and Rehbein, 2012) and language learning (Carri´on, 2006; Xu and Li, 2011). However, they have yet to be exploited for analyzing text difficulty, which is also known as readability assessment. Given any text, the system is to predict its reading difficulty, by estimating the age or school grade (e.g., Grades 1 to 13) required for readers to understand the text; by assigning it a difficulty score, such as Lexile (Stenner, 1996); or by locating it on a proficiency scale, such as the six-level scale in the Common European Framework of Reference for Language (2001). Previous research on automatic readability assess"
2020.framenet-1.8,D16-1192,0,0.100226,"rocess. Likewise, sentences (2a) and (2b) are semantically similar, but the reason construction ‘because [he] missed the exam’ in the latter may make it harder to read than the former. Finally, sentence (3b) is likely more challenging to understand than (3a) due to a metaphorical usage. Semantic analysis can be expected to improve the readability assessment for such sentences. While some existing assessment models already incorporate semantic features, they are mostly limited to anaphora patterns, word senses and semantic categories of individual words (Pil´an et al., 2014; Sung et al., 2015; Schumacher et al., 2016). Salient features may potentially be derived from semantic frames, 2. Research Questions We hypothesize that the verb usage patterns encoded in verb frames can be associated with different levels of reading difficulty. The distribution of frame-related attributes in a text may therefore be correlated with readability. More precisely, this paper tests the following hypotheses: • H1: Non-core frame elements are more frequently used in more difficult texts (Section 5); • H2: Core frame elements are more frequently omitted in more difficult texts (Section 6); • H3: For verbs that can take either"
2020.framenet-1.8,W11-2308,0,0.0810024,"Missing"
2020.framenet-1.8,D11-1142,0,0.0239359,"erb frames, frame elements, readability 1. Introduction such as those in FrameNet, Chinese Framenet (You and Liu, 2005), or Mandarin VerbNet (Liu, 2016; Liu and Chang, 2016; Liu, 2018; Liu, 2019). Based on Mandarin VerbNet, a verbal semantic database that adopts a frame-based approach, this paper investigates the correlation between verb frames and text difficulty. FrameNet (https://framenet.icsi.berkeley.edu) and other similar resources have supported a large range of natural language processing (NLP) tasks including semantic role labeling (Gildea and Jurafsky, 2002), information extraction (Fader et al., 2011), sentiment analysis (Ruppenhofer and Rehbein, 2012) and language learning (Carri´on, 2006; Xu and Li, 2011). However, they have yet to be exploited for analyzing text difficulty, which is also known as readability assessment. Given any text, the system is to predict its reading difficulty, by estimating the age or school grade (e.g., Grades 1 to 13) required for readers to understand the text; by assigning it a difficulty score, such as Lexile (Stenner, 1996); or by locating it on a proficiency scale, such as the six-level scale in the Common European Framework of Reference for Language (2001"
2020.framenet-1.8,D12-1043,0,0.0694675,"Missing"
2020.framenet-1.8,J02-3001,0,0.520227,"n of text readability. Keywords: Mandarin VerbNet, verb frames, frame elements, readability 1. Introduction such as those in FrameNet, Chinese Framenet (You and Liu, 2005), or Mandarin VerbNet (Liu, 2016; Liu and Chang, 2016; Liu, 2018; Liu, 2019). Based on Mandarin VerbNet, a verbal semantic database that adopts a frame-based approach, this paper investigates the correlation between verb frames and text difficulty. FrameNet (https://framenet.icsi.berkeley.edu) and other similar resources have supported a large range of natural language processing (NLP) tasks including semantic role labeling (Gildea and Jurafsky, 2002), information extraction (Fader et al., 2011), sentiment analysis (Ruppenhofer and Rehbein, 2012) and language learning (Carri´on, 2006; Xu and Li, 2011). However, they have yet to be exploited for analyzing text difficulty, which is also known as readability assessment. Given any text, the system is to predict its reading difficulty, by estimating the age or school grade (e.g., Grades 1 to 13) required for readers to understand the text; by assigning it a difficulty score, such as Lexile (Stenner, 1996); or by locating it on a proficiency scale, such as the six-level scale in the Common Europ"
2020.framenet-1.8,C12-1065,0,0.0232561,"ability Less difficult Remarks Clause argument for ‘worry’ More difficult Noun argument for ‘worry’ Less difficult Clause argument for ‘regret’ More difficult Less difficult Use of reason construction to express cause for regret No metaphorical usage More difficult Metaphorical usage with ‘put’ Table 1: Sentences with varying reading difficulty due to semantic complexity, despite similar lexical and syntactic complexity. 4. More recent work in NLP has made use of n-gram language models (Collins-Thompson and Callan, 2004; Petersen and Ostendorf, 2009), inflectional and derivational morphology (Hancke et al., 2012), verbal morphology, verb tense and mood-based features (Dell’Orletta et al., 2011; Franc¸ois and Fairon, 2012). Psycholinguistic properties, such as the concreteness, imageability and meaningfulness of words (Wilson, 1988), and the age of acquisition (Kuperman et al., 2012), have also been shown to be helpful. 3.2. 4.1. Mandarin VerbNet Mandarin VerbNet is a verbal semantic database with annotation of frame-based constructional features (Liu and Chiang, 2008). In addition to frame elements, its frames make use of a schema-based meaning representation and constructional patterns. Adopting a hy"
2020.framenet-1.8,W08-0909,0,0.0520909,"simple words, it can still be difficult to understand because of complicated syntactic structure. Early models often use sentence length and clause length as proxies for syntactic complexity. More recent ones incorporate part-of-speech (POS) features, including the frequency of coordination and subordination; the nominal ratio and the pronoun/noun ratio (Pil´an et al., 2014); the number of different kinds of pronouns and conjunctions (Sung et al., 2015); and more generally, the percentage and diversity of POS tags (Vajjala and Meurers, 2014). Parse tree depth, parse scores, subtree patterns (Heilman et al., 2008; Schumacher et al., 2016) and dependency distance (Liu, 2008) have also been found to be useful. 3.3. Data This section first presents Mandarin VerbNet and the verbs to be analyzed (Section 4.1), and then describes the corpus of graded texts on which our analysis is based (Section 4.2). 4.2. Corpus of Graded Text We performed our analysis on a corpus of Chineselanguage textbooks constructed at Ludong University, China.1 The 5-million-character corpus consists of more than 6000 articles, taken from 368 textbooks spanning the twelve grades in the curriculum for Chinese language in mainland Chin"
2020.lrec-1.41,Q17-1010,0,0.00535843,"ctively and possibly also due to timepressure. Setting a canonical example of language use appears to be less prioritized, potentially because feedback was directed towards advanced L2 speakers in the source corpus. Spelling errors Apart from incomplete sentences, openended teacher comments also contain spelling errors occasionally, both of which can have a negative impact on the accuracy of NLP processing pipelines typically trained on more formal genres (e.g. newspapers, novels etc.). Word embeddings with sub-word information could efficiently handle the problem of out-of-vocabulary tokens (Bojanowski et al., 2017). Orthographic conventions As also the examples in Section 5.1. show, teachers may use, for instance, different orthographic conventions for quoting, employing as delimiters single, double quotes or dashes. Locating errors We have also found some individual variation in pinpointing the exact location of an error, especially for errors spanning multiple tokens and an entire sentence. Relying on token-level alignment information may therefore not be sufficient for automatically locating errors while processing such data. Terminology There might not always be a consensus in the community about th"
2020.lrec-1.41,P14-1071,0,0.196423,"Missing"
2020.lrec-1.41,D19-1316,0,0.159417,"urrence of an error, explanation about why it occurred and suggestions about how to correct it can help language learners revise incorrectly used linguistic elements and it can contribute to avoiding repeated future occurrences of these errors. Despite numerous investigations on the kinds of feedback that can best promote learning, this question remains an open debate in the area of Second Language Acquisition (SLA) (Lightbown and Spada, 1990; Lyster and Ranta, 1997; Sheen and Ellis, 2011). The application of computational linguistic methods to answer this question remains much less explored (Nagata, 2019). Since these methods allow for the automatic processing of larger amounts of texts, it has a good potential to contribute to this debate with empirical evidence. To promote research on feedback in the above mentioned areas, we collect and annotate a set of teacher feedback and pairs of original and revised student-written sentences from a corpus of teacher-corrected academic essays authored by non-native speakers of English1 . We annotate two types of information: (i) feedback directness, whether the feedback explicitly contained the correction (direct) or not (indirect), and (ii) revision ou"
2020.lrec-1.41,W18-0513,0,0.058768,"Missing"
2020.lrec-1.41,W18-0506,0,0.0637114,"Missing"
2020.lrec-1.722,K18-1022,0,0.0587655,"Missing"
2020.lrec-1.722,bird-etal-2008-acl,0,0.119488,"Missing"
2020.lrec-1.722,J08-4005,0,0.0564611,"Missing"
2020.lrec-1.722,N19-1423,0,0.00750394,"compilation process. Similarly, some of the errors from the system’s prediction are to be attributed to the annotation process of the test set. For example, in the sentence ”They included support for marine reserves and money for fisheries management reform.”, reserves is annotated as informal while the system identified it as formal. In general, while bootstrapping the academic resource compilation and the informal word identification tasks, a minimal intervention of human annotators would enhance the overall system. Furthermore, integration of a BERT or other contextualized embedding model (Devlin et al., 2019) could also help to improve the performance of the system. Contextualized word embeddings provide word vector representations based on their context. As the vector representation of words varies as per the context, they implicitly provide a model for word sense disambiguation (WSD). 7 Conclusion and Future Direction In the realm of academic text writing, we explored how to compile academic resources, automatically identify informal words (words that are less formal for academic writing), and provide better substitutes. We have used a generic approach to compile the academic resources, which ca"
2020.lrec-1.722,D17-1091,0,0.0341901,"Missing"
2020.lrec-1.722,L18-1039,0,0.0336358,"Missing"
2020.lrec-1.722,C18-1039,0,0.0173605,"nd Section 7 respectively. 5896 2 Previous Work In this section, we review previous work in lexical substitution, a closely related task, and discuss how the academic text rewriting system potentially differs. In essence, our system is similar to lexical substitution (LS) and text simplification tasks, in such a way that both focus on the rewriting of an original text towards a given goal. Lexical substitution system mainly focuses on rewriting texts by replacing some of the words or phrases without ˇ altering the original meaning (Szarvas et al., 2013; Stajner and Saggion, 2018). The work by Guo et al. (2018) targeted text simplification based on the sequence-to-sequence deep neural network model, where its entailment and paraphrasing capabilities are improved via multi-task learning. While the complex word identification (CWI) task focuses on identifying lexical units that pose difficulties to understand the sentence (Yimam et al., 2017b; Yimam et al., 2017a; Yimam et al., 2018; Paetzold and Specia, 2016), our informal word identification (IWI) component focuses on identifying words that are not fitting or adhering to the academic style of writing. The work by Riedl et al. (2014) focuses on the l"
2020.lrec-1.722,D18-1541,0,0.0326254,"Missing"
2020.lrec-1.722,E14-1057,0,0.261959,"Missing"
2020.lrec-1.722,W18-6706,1,0.750418,"arson1 . However, these resources are 1) limited to a certain domain and target writers (mostly L2 learners and students), 2) their vocabulary is fixed, thus requiring manual work for an extension, and 3) the resources are limited to uni-gram and bi-gram lists. In this work, we build academic resources that are more generic, which can be built from existing reference corpora. In addition to uni-gram and bi-gram resources, we also design a system that can produce resources up to a length of four words (quad-grams). As far as we know, the only system available to academic writing is the work of Lee et al. (2018), which addresses a different aspect, which is a sentence restructuring based on nominalizing verbal expressions. 3 Building Academic Resources In this section, we will first discuss the existing academic resources, how they are built and their limitations. Then, we will present our approach that describes the process of building academic resources from different reference corpora. Finally, we will discuss the quality of the collected resources against two evaluation measures, namely comparing with the existing resources and manually evaluating the academic fitness of resources. 1 Academic col"
2020.lrec-1.722,S07-1009,0,0.166276,"Missing"
2020.lrec-1.722,P16-2024,0,0.0646835,"Missing"
2020.lrec-1.722,P15-2070,0,0.155618,"Missing"
2020.lrec-1.722,D14-1162,0,0.0836199,"Missing"
2020.lrec-1.722,D14-1066,0,0.0193169,"2018). The work by Guo et al. (2018) targeted text simplification based on the sequence-to-sequence deep neural network model, where its entailment and paraphrasing capabilities are improved via multi-task learning. While the complex word identification (CWI) task focuses on identifying lexical units that pose difficulties to understand the sentence (Yimam et al., 2017b; Yimam et al., 2017a; Yimam et al., 2018; Paetzold and Specia, 2016), our informal word identification (IWI) component focuses on identifying words that are not fitting or adhering to the academic style of writing. The work by Riedl et al. (2014) focuses on the lexical substitution task, particularly for medical documents. They have relied on Distributional Thesaurus (DT), computed on medical texts to generate synonyms for target words. Existing resources for academic writing are limited to a precompiled list of words such as the Corpus Of Contemporary American English (COCA) (Gardner and Davies, 2013) and the New Academic Word List 1.0 (NAWL) (Browne et al., 2013) vocabulary lists. Regarding phrases (multiword expressions) for academic writing, the only available resources are the academic bi-grams compiled by Pearson1 . However, the"
2020.lrec-1.722,P15-4018,1,0.88784,"Missing"
2020.lrec-1.722,W17-5703,0,0.0432254,"Missing"
2020.lrec-1.722,C18-3005,0,0.111521,"cademic resources might vary from domain to domain as some words or phrases are extensively used in one domain over the other. The first step in building an academic writing aid tool is to collect resources that determines whether a given phrase follows the style of writing in academia. This involves analyzing a given sentence and determining if the lexemes of the sentences are well-selected academic words and phrases or not. To evaluate the resources compiled, we have to build a system, analogous to the lexical substitution and text simpliˇ fication tasks, for example, (Szarvas et al., 2013; Stajner and Saggion, 2018), that consists of informal word identification, academic candidate generation, and candidate paraphrase ranking components (see Figure 1). While it is possible to follow the same approaches as the lexical substitution and text simplification approaches for academic text rewriting tasks, the main challenge for the academic paraphrasing task is the collection of resources for academic texts. The following are the main objectives of building academic resources: 1. Identify suitable academic and non-academic datasets that are to be used to build academic resources. 2. Design a generic, domain-ind"
2020.lrec-1.722,N13-1133,1,0.906611,"style. Moreover, the academic resources might vary from domain to domain as some words or phrases are extensively used in one domain over the other. The first step in building an academic writing aid tool is to collect resources that determines whether a given phrase follows the style of writing in academia. This involves analyzing a given sentence and determining if the lexemes of the sentences are well-selected academic words and phrases or not. To evaluate the resources compiled, we have to build a system, analogous to the lexical substitution and text simpliˇ fication tasks, for example, (Szarvas et al., 2013; Stajner and Saggion, 2018), that consists of informal word identification, academic candidate generation, and candidate paraphrase ranking components (see Figure 1). While it is possible to follow the same approaches as the lexical substitution and text simplification approaches for academic text rewriting tasks, the main challenge for the academic paraphrasing task is the collection of resources for academic texts. The following are the main objectives of building academic resources: 1. Identify suitable academic and non-academic datasets that are to be used to build academic resources. 2."
2020.lrec-1.722,N03-1033,0,0.138259,"Missing"
2020.lrec-1.722,C18-1028,1,0.881095,"Missing"
2020.lrec-1.722,W16-1801,1,0.862553,"ions. Pearson has published a set of academic bi-grams7 . Words like best, almost, and way are not by themselves academic, but they can be combined with other 6 http://www.newgeneralservicelist.org/ nawl-new-academic-word-list 7 Academic collocation list: https://pearsonpte. com/organizations/resea 4.3.2 Paraphrase Generation, Selection, and Ranking Given an informal word, this step generates a list of substitution candidates. While there are different approaches to generate candidates for target words, such as using existing paraphrase resources like WordNet and Distributional thesaurus (see Yimam et al. (2016)), we depend solely on the CoInCo (Kremer et al., 2014), WordNet (Miller, 1995), and the paraphrase database (PPDB) (Pavlick et al., 2015) resources to generate candidates. Once the candidates are generated, all of the candidates, which must be academic words are retained for the paraphrase ranking component. Given a list of academic substitution candidates, the paraphrase ranking component finds the one that fits best in the context. The detailed approach is presented in Section 4.4. 5899 Figure 2: Architecture of the system. CoInCo annotation IWI dataset Pacific First Financial Corp said[par"
2020.lrec-1.722,I17-2068,1,0.904094,"Missing"
2020.lrec-1.722,yimam-etal-2017-multilingual,1,0.933203,"on the rewriting of an original text towards a given goal. Lexical substitution system mainly focuses on rewriting texts by replacing some of the words or phrases without ˇ altering the original meaning (Szarvas et al., 2013; Stajner and Saggion, 2018). The work by Guo et al. (2018) targeted text simplification based on the sequence-to-sequence deep neural network model, where its entailment and paraphrasing capabilities are improved via multi-task learning. While the complex word identification (CWI) task focuses on identifying lexical units that pose difficulties to understand the sentence (Yimam et al., 2017b; Yimam et al., 2017a; Yimam et al., 2018; Paetzold and Specia, 2016), our informal word identification (IWI) component focuses on identifying words that are not fitting or adhering to the academic style of writing. The work by Riedl et al. (2014) focuses on the lexical substitution task, particularly for medical documents. They have relied on Distributional Thesaurus (DT), computed on medical texts to generate synonyms for target words. Existing resources for academic writing are limited to a precompiled list of words such as the Corpus Of Contemporary American English (COCA) (Gardner and Da"
2020.lrec-1.722,W18-0507,1,0.910658,"7 respectively. 5896 2 Previous Work In this section, we review previous work in lexical substitution, a closely related task, and discuss how the academic text rewriting system potentially differs. In essence, our system is similar to lexical substitution (LS) and text simplification tasks, in such a way that both focus on the rewriting of an original text towards a given goal. Lexical substitution system mainly focuses on rewriting texts by replacing some of the words or phrases without ˇ altering the original meaning (Szarvas et al., 2013; Stajner and Saggion, 2018). The work by Guo et al. (2018) targeted text simplification based on the sequence-to-sequence deep neural network model, where its entailment and paraphrasing capabilities are improved via multi-task learning. While the complex word identification (CWI) task focuses on identifying lexical units that pose difficulties to understand the sentence (Yimam et al., 2017b; Yimam et al., 2017a; Yimam et al., 2018; Paetzold and Specia, 2016), our informal word identification (IWI) component focuses on identifying words that are not fitting or adhering to the academic style of writing. The work by Riedl et al. (2014) focuses on the l"
2020.lrec-1.722,S16-1085,0,\N,Missing
2020.sltu-1.50,1997.mtsummit-papers.13,0,0.0614063,"thews and Yip, 2011). Less frequently used in formal written communication than Mandarin, the dominant variety in mainland China, Cantonese is supported by relatively few resources for natural language processing. Although Cantonese and Mandarin are genetically related, having both developed from Middle Chinese, they are mutually unintelligible in their spoken form and have significant differences in their written form (Wong and Lee, 2018). The differences between the major Chinese varieties have been described as being “at least on the order of the different languages of the Romance family” (Hannas, 1997). The rest of the paper is organized as follows. After an overview of corpus design in the next section, we describe the content and role of the domain-independent subcorpus (Section 3) and the domain-specific subcorpus (Section 4).1 We then present a case study on constructing a chatbot for test anxiety (Section 5). 2. Corpus Design Our corpus, which consists of post-reply sentence pairs, is designed to support the construction of virtual counsellors in any domain. Given a user input, a chatbot can identify the example post in the corpus that is semantically most similar, and then retrieve th"
2020.sltu-1.50,P13-4012,0,0.145468,"eneral, and mental health assistance in particular. While chatbots may not qualify to replace human counsellors, users have been found to be more willing to disclose information to a chatbot than to a human (Lucas et al., 2014). Woebot, for example, has been shown to be effective in reducing symptoms of depression (Fitzpatrick et al., 2017). In the “guided conversation” format, virtual counsellors ask pre-determined questions and allow users to choose from suggested responses (Fitzpatrick et al., 2017; Casas et al., 2018). Other chatbots are designed to handle free-form input from users. PAL (Liu et al., 2013) and TeenChat (Huang et al., 2015) are two examples among those operating in Chinese. Targeting teenagers and young adults, these two chatbots offer advice on social topics such as family relations and love affairs. In response to user input, it selects the most relevant answer from a knowledge base, taking into account user characteristics such as gender, marital status and age. Since no single virtual counsellor can adequately address all mental health issues or serve all user populations, counselling services at schools or mental health organizations may be interested in developing their ow"
2020.sltu-1.50,N18-1202,0,0.0131381,"in the sentence with the word2vec vector embeddings. We then used WMD to measure the dissimilarity between two sentences, as expressed by the minimum cumulative distance that the embedded words of one sentence need to travel to match those in the other sentence (Kusner et al., 2015). The symptom statement that yields the shortest distance is returned. ELMo. Shown to improve the state-of-the-art in a variety of NLP tasks, ELMo is a contextualized word representation that models not only the characteristics of word usage, but also how the usage varies across linguistic contexts, i.e., polysemy (Peters et al., 2018). The word vectors are learned functions of the internal states of a deep bidirectional language model, pre-trained on a large text corpus. 5.3. Symptom Detection Accuracy We conducted a leave-one-out evaluation on the symptom statements in our domain-specific subcorpus (Section 5.1) to measure accuracy in symptom detection. The chatbot automatically determined the symptom for each of the 436 statements, by retrieving the most similar statement among those in the remainder of the subcorpus, with respect to the similarity measures described in Section 5.2. As shown in Table 4, Word Mover’s Dist"
2021.bea-1.6,L16-1138,0,0.0346998,"Missing"
2021.bea-1.6,N16-1119,0,0.0507562,"Missing"
2021.bea-1.6,D17-1027,0,0.0201178,"ning to the semantic component associated with F . This method can be problematic, however, since the dominant meaning of a component may differ from those of the family members. For example, members of the family associated with the component y`e are semantically related to “head”, but as a standalone character y`e means ‘page’ in modern Chinese (Table 1). We instead measure semantic similarity between characters. Using a set of 7.6 million sentences from Chinese Wikipedia, we trained context-free embeddings ~c and w ~ for each character c and word w with the joint learning model proposed by Yu et al. (2017). We compare two methods for generating the representation v(c) for a character c: Figure 1: The “Averaged word vectors” method (top-k word vectors) assigns higher scores to similar character pairs (rated 0.5 or over) than dissimilar pairs (rated below 0.5); in contrast, the “Character vector” method does not clearly distinguish the similar and dissimilar pairs. Character vector The baseline directly uses the character embeddings, i.e., v(c) = ~c. There were 2,285 characters distributed in 600 character families. We randomly selected ten character families for evaluation. For each family, we c"
2021.bea-1.6,N19-1423,0,0.0209462,"Missing"
2021.bea-1.6,S12-1049,0,0.0182134,"(c) (Section 4.1), with the settings k = {5, 10, 15}. Averaged word vectors The meaning of some characters may be more clearly expressed within words. From the Wikipedia dataset, we retrieve the k most frequent words w1 , . . . , wk that contain the character c. We then average the word vectors of these k words, i.e. definP ~i . ing v(c) = k1 i=1,...,k w 4.3 We presented two human judges with all character pairs within the 20 character sets. Both native speakers of Chinese, the judges rated each pair according to the annotation scheme of the SemEval2012 shared task on Chinese word similarity (Jin and Wu, 2012). The similarity score ranged from 0 (not at all related) to 5 (identical). We computed the correlation between the averaged human scores and the cosine similarity cos(v(ci ), v(cj )) as generated by the two methods described in Section 4.1. The “Character vector” method attained a Pearson correlation coefficient of only 0.09.4 The “Averaged word vectors” method achieved a coefficient of 0.805 , outperforming the “Character vector” method. This coefficient was obtained at k = 5, i.e., averaging the 5 most frequent words. Performance degraded at higher values of k, likely because of increased s"
2021.emnlp-main.632,N03-1013,0,0.0625339,"phrase.4 5 Approach The system takes as input the original sentence and the strings NV , p, O, and Ma or Mn , (Table 1). These strings were provided, rather than automatically extracted, so that errors in automatic parsing would not confound experimental results. 3 Accessible at https://github.com/NominalizationParaphrase E.g., paraphrasing “nuclear attack on Japan” as “attack Japan with nuclear weapons”. 4 Paraphrase generation Verb (V ) Candidates include all verbs that are derivationally related to NV in the WordNet database. If there is none, we use all verbs in the entry of NV in CatVar (Habash and Dorr, 2003). Arguments (M , O) The argument M is constructed from either Mn or Ma . For the former, candidates include the singular and plural forms of Mn . For the latter, candidates include the singular and plural forms of its pertainyms in WordNet. The argument O remains unchanged from the input. We place all permutations of candidates for V , M , and O in the word orders of all paraphrase types shown in Table 1, as well as their passive voice equivalents.5 The arguments M and O can be preceded with a determiner, and also with a preposition except when at the sentence-initial position. We used the T5"
2021.emnlp-main.632,E17-1006,0,0.0133966,"rase (Hendrickx et al., 2013). As modifier (e.g., “fracture treatment”). However, adunsupervised methods has recently been found to jective modifiers (“surgical treatment”) are also perform well in NCI (Ponkiya et al., 2020), we prevalent and have been analyzed by linguists along with nominal modifiers under the term “complex 1 e.g., “regional electricity consumption” or “consumption nominal” (Levy, 1978). While there has been preof electricity in the region” 2 vious work on predicting the attribute of an adjece.g., “the region’s electricity consumption” or “its electricity consumption” tive (Hartung et al., 2017) and the use of pertainyms 8024 Type MVO VOM OVM VMO MOV Ma 92 75 50 57 1 Mn 44 57 35 36 2 5.1 We first generate all possible word choices for the verb and its arguments that will appear in candidate paraphrases: Table 2: Distribution of paraphrase types (see examples in Table 1) in our dataset (Section 4), which includes nominalizations with nominal modifiers (Mn ) and with adjectival modifiers (Ma ). The majority baseline uses MVO for Ma inputs and VOM for Mn inputs. in question answering (Greenwood, 2004), neither has been evaluated on paraphrase generation in the general domain. 4 Dataset"
2021.emnlp-main.632,S13-2025,0,0.0291056,"roach assigns rectly make use of SRL output. To others, such an abstract label to describe the relation between as machine translation and text simplification, a the head noun and the noun modifier (Tratz and paraphrase of the nominalization can potentially be Hovy, 2010). Another, similar to ours, generates a paraphrase that links the two nouns with prepo- more immediately applicable. sitions and verbs (Butnariu et al., 2010; Nakov Previous studies have only addressed nominaland Hearst, 2013; Ponkiya et al., 2020), or in a izations with one argument, typically a nominal free-form paraphrase (Hendrickx et al., 2013). As modifier (e.g., “fracture treatment”). However, adunsupervised methods has recently been found to jective modifiers (“surgical treatment”) are also perform well in NCI (Ponkiya et al., 2020), we prevalent and have been analyzed by linguists along with nominal modifiers under the term “complex 1 e.g., “regional electricity consumption” or “consumption nominal” (Levy, 1978). While there has been preof electricity in the region” 2 vious work on predicting the attribute of an adjece.g., “the region’s electricity consumption” or “its electricity consumption” tive (Hartung et al., 2017) and the"
2021.emnlp-main.632,J02-3004,0,0.359174,"ds, we obtained the strongest performance by using a pre-trained contextualized language model to re-rank paraphrase candidates identified by a textual entailment model. 1 Introduction with surgery”, may have its PP argument (“with surgery”) mapped to the subject, and its prenominal modifier (“fracture”) mapped to the object. Table 1 shows a range of possible mappings. Our task is distinguished from previous research in terms of both the input and output. In the field of nominalization disambiguation, most studies have focused on deverbal nouns with one argument, typically a nominal modifier (Lapata, 2002; Nicholson and Baldwin, 2008). We extend the scope by addressing both nominal and adjectival modifiers, as well as PP argument. In terms of the output, most previous work assigned semantic role labels to arguments of the deverbal noun; in contrast, we generate paraphrases in predicate-argument form (Table 1). This approach has the advantage of avoiding commitment to a particular semantic theory, and can facilitate direct application to various NLP downstream tasks. The paraphrases may improve performance in machine translation, for example, for sentences that do not use nominalization in the"
2021.emnlp-main.632,S10-1007,0,0.0292805,"ome downstream NLP tasks, such as semantic parsing into meaning Research on noun compound interpretation has representations (Samuel and Straka, 2020), can ditaken two main approaches. One approach assigns rectly make use of SRL output. To others, such an abstract label to describe the relation between as machine translation and text simplification, a the head noun and the noun modifier (Tratz and paraphrase of the nominalization can potentially be Hovy, 2010). Another, similar to ours, generates a paraphrase that links the two nouns with prepo- more immediately applicable. sitions and verbs (Butnariu et al., 2010; Nakov Previous studies have only addressed nominaland Hearst, 2013; Ponkiya et al., 2020), or in a izations with one argument, typically a nominal free-form paraphrase (Hendrickx et al., 2013). As modifier (e.g., “fracture treatment”). However, adunsupervised methods has recently been found to jective modifiers (“surgical treatment”) are also perform well in NCI (Ponkiya et al., 2020), we prevalent and have been analyzed by linguists along with nominal modifiers under the term “complex 1 e.g., “regional electricity consumption” or “consumption nominal” (Levy, 1978). While there has been preo"
2021.emnlp-main.632,2021.ccl-1.108,0,0.023589,"Missing"
2021.emnlp-main.632,C08-1084,0,0.0370503,"whether an input can be paraphrased, a task that we leave to future work. likewise pursue this direction. Our work can be viewed as a special case in NCI, where the head noun is required to be a deverbal noun. Given the properties of the deverbal noun, the paraphrase in this paper takes a different form, making use of the underlying verb to form a clause. 3.2 Nominal semantic role labeling Previous work on nominalization interpretation has mostly focused on nominal semantic role labeling (SRL), which assigns abstract labels (e.g., agent, patient) to arguments of nominalizations (Lapata, 2002; Padó et al., 2008; Kilicoglu et al., 2010). 3 Previous work SRL can be performed with a classifier, trained Our research is most closely related to noun com- on features such as syntactic structure and corpus pound interpretation (Section 3.1) and nominal se- frequencies of verb-argument pairs (Lapata, 2002; mantic role labeling (Section 3.2). Pradhan et al., 2004). Compared to SRL, paraphrasing a nominalization requires the generation 3.1 Noun compound interpretation of the verb and its arguments. Some downstream NLP tasks, such as semantic parsing into meaning Research on noun compound interpretation has rep"
2021.emnlp-main.632,2020.findings-emnlp.386,0,0.0337639,"nterpretation has representations (Samuel and Straka, 2020), can ditaken two main approaches. One approach assigns rectly make use of SRL output. To others, such an abstract label to describe the relation between as machine translation and text simplification, a the head noun and the noun modifier (Tratz and paraphrase of the nominalization can potentially be Hovy, 2010). Another, similar to ours, generates a paraphrase that links the two nouns with prepo- more immediately applicable. sitions and verbs (Butnariu et al., 2010; Nakov Previous studies have only addressed nominaland Hearst, 2013; Ponkiya et al., 2020), or in a izations with one argument, typically a nominal free-form paraphrase (Hendrickx et al., 2013). As modifier (e.g., “fracture treatment”). However, adunsupervised methods has recently been found to jective modifiers (“surgical treatment”) are also perform well in NCI (Ponkiya et al., 2020), we prevalent and have been analyzed by linguists along with nominal modifiers under the term “complex 1 e.g., “regional electricity consumption” or “consumption nominal” (Levy, 1978). While there has been preof electricity in the region” 2 vious work on predicting the attribute of an adjece.g., “the"
2021.emnlp-main.632,N04-4036,0,0.0841,"m a clause. 3.2 Nominal semantic role labeling Previous work on nominalization interpretation has mostly focused on nominal semantic role labeling (SRL), which assigns abstract labels (e.g., agent, patient) to arguments of nominalizations (Lapata, 2002; Padó et al., 2008; Kilicoglu et al., 2010). 3 Previous work SRL can be performed with a classifier, trained Our research is most closely related to noun com- on features such as syntactic structure and corpus pound interpretation (Section 3.1) and nominal se- frequencies of verb-argument pairs (Lapata, 2002; mantic role labeling (Section 3.2). Pradhan et al., 2004). Compared to SRL, paraphrasing a nominalization requires the generation 3.1 Noun compound interpretation of the verb and its arguments. Some downstream NLP tasks, such as semantic parsing into meaning Research on noun compound interpretation has representations (Samuel and Straka, 2020), can ditaken two main approaches. One approach assigns rectly make use of SRL output. To others, such an abstract label to describe the relation between as machine translation and text simplification, a the head noun and the noun modifier (Tratz and paraphrase of the nominalization can potentially be Hovy, 201"
2021.emnlp-main.632,D19-1410,0,0.0165802,"a preposition except when at the sentence-initial position. We used the T5 model (Raffel et al., 2020) to generate up to two words — a preposition followed by a determiner; a preposition alone; a determiner alone; or the empty string. 5.2 Paraphrase selection We then select the best candidate paraphrase with the following approaches: Sentence similarity selects the candidate paraphrase whose sentence embedding is most similar to that of the original nominalization, according to cosine similarity. We obtained sentence embeddings with the pre-trained stsb-roberta-large model6 from SentenceBERT (Reimers and Gurevych, 2019). Language model (LM) selects the candidate paraphrase that yields the highest language model score. We evaluated the log-probability score based on GPT-2 (117M), and the pseudo-log-likelihood score based on DistilBERT (Salazar et al., 2020).7 5 The passive paraphrases are “O Vprt by M ” (for MVO), “M Vprt by O” (for OVM), “O Vprt p M ” (for VOM) and “M Vprt p O” (for VMO), where Vprt represents hbei followed by the past participle of V , and p represents any preposition except “by”. 6 https://huggingface.co/sentence-transformers/stsbroberta-large 7 Both from https://github.com/awslabs/mlm-sco"
2021.emnlp-main.632,2020.acl-main.240,0,0.0203304,"Missing"
2021.emnlp-main.632,2020.conll-shared.5,0,0.139909,"al., 2010). 3 Previous work SRL can be performed with a classifier, trained Our research is most closely related to noun com- on features such as syntactic structure and corpus pound interpretation (Section 3.1) and nominal se- frequencies of verb-argument pairs (Lapata, 2002; mantic role labeling (Section 3.2). Pradhan et al., 2004). Compared to SRL, paraphrasing a nominalization requires the generation 3.1 Noun compound interpretation of the verb and its arguments. Some downstream NLP tasks, such as semantic parsing into meaning Research on noun compound interpretation has representations (Samuel and Straka, 2020), can ditaken two main approaches. One approach assigns rectly make use of SRL output. To others, such an abstract label to describe the relation between as machine translation and text simplification, a the head noun and the noun modifier (Tratz and paraphrase of the nominalization can potentially be Hovy, 2010). Another, similar to ours, generates a paraphrase that links the two nouns with prepo- more immediately applicable. sitions and verbs (Butnariu et al., 2010; Nakov Previous studies have only addressed nominaland Hearst, 2013; Ponkiya et al., 2020), or in a izations with one argument,"
2021.emnlp-main.632,P10-1070,0,0.0350709,"Missing"
2021.latechclfl-1.10,I05-3004,0,0.0900354,"Yang, 1999), accuracy in adverbial identification would be affected (Xing et al., 2020). Research on linguistic variation in Chinese often addressed DE as possessive and attributive marker, but not as adverbial marker (Zhang, 2012b). Because of the ambiguity with the DE suffix, most quantitative analyses on DI- vs. DE-adverbial usage required manual annotation or was restricted to relatively small sample sizes (Tan, 2004; Zhang, 2012a; Ho, 2015). Previous research has utilized both rule-based and machine learning approaches to study Chinese numeral classifiers that form quantity noun phrases (Guo and Zhong, 2005; Peinelt et al., 2017). 4.1 Proposed method We investigate a parser-based approach in the following three settings, using the HanLP Chinese parser (He, 2020) for word segmentation, POS tagging and dependency parsing.3 POS only This baseline predicts “adverbial” if the base word is tagged as “adverb” (d in HanLP, e.g., gaoxing/d DE/u). Note that the DEadverbial in Figure 1a would be falsely rejected. If the word segmentor combines the base word and suffix as one word, the POS tag of the word is also required to be “adverb” (e.g. gaoxing-DE/d). POS+base This second baseline aims to improve reca"
2021.latechclfl-1.10,I17-3011,0,0.0126634,"in adverbial identification would be affected (Xing et al., 2020). Research on linguistic variation in Chinese often addressed DE as possessive and attributive marker, but not as adverbial marker (Zhang, 2012b). Because of the ambiguity with the DE suffix, most quantitative analyses on DI- vs. DE-adverbial usage required manual annotation or was restricted to relatively small sample sizes (Tan, 2004; Zhang, 2012a; Ho, 2015). Previous research has utilized both rule-based and machine learning approaches to study Chinese numeral classifiers that form quantity noun phrases (Guo and Zhong, 2005; Peinelt et al., 2017). 4.1 Proposed method We investigate a parser-based approach in the following three settings, using the HanLP Chinese parser (He, 2020) for word segmentation, POS tagging and dependency parsing.3 POS only This baseline predicts “adverbial” if the base word is tagged as “adverb” (d in HanLP, e.g., gaoxing/d DE/u). Note that the DEadverbial in Figure 1a would be falsely rejected. If the word segmentor combines the base word and suffix as one word, the POS tag of the word is also required to be “adverb” (e.g. gaoxing-DE/d). POS+base This second baseline aims to improve recall with more relaxed PO"
2021.latechclfl-1.10,2020.emnlp-demos.6,0,0.0250162,"Missing"
2021.nlp4posimpact-1.1,Q16-1033,0,0.06017,"Missing"
2021.nlp4posimpact-1.1,N19-1423,0,0.0109534,"nd in the input text, i.e., the previous utterances, so that the question would not seem redundant. Question generation models have been deployed to engage users in a conversation (Mostafazadeh et al., 2016), but the research was focused on images. Template-based approaches, as exemplified by ELIZA (Weizenbaum, 1983), can also transform the user’s statements into questions. These templates are labor-intensive to Text summarization models, which condense an input text into a shorter version, can generate short summaries or headlines (Rush et al., 2015). Pretrained language models such as BERT (Devlin et al., 2019) have been shown to boost the quality of summarization, among many other NLP tasks. Among the best-performing models is BertSum, which uses a document-level BERT-based encoder to express the semantics of the input text document and obtain sentence representations (Liu and Lapata, 2019). Its fine-tuning schedule adopts different optimizers for the encoder and the decoder, and has been shown to improve performance by alleviating the mismatch between them. Compared to open-domain dialogs, a human counsellor more often gives shorter replies and reflects the points made by the counsellee. Summariza"
2021.nlp4posimpact-1.1,P17-1123,0,0.0121081,"response generation has exploited models from machine translation (Ritter et al., 2011) and question answering (Liu et al., 2013), there has been less effort in leveraging those from other NLP tasks such as text summarization and question generation. This section reviews research in these two fields. 2.1 2.2 Text summarization Question generation A question generation model composes a question from an input text. Neural question generation algorithms have recently attained state-of-the-art performance. For example, a sequence-to-sequence model with an attention mechanism has been proposed by Du et al. (2017). Answer separation techniques have further improved question quality (Kim et al., 2019). Question generation is slightly different in the dialog context in that the answer should generally not be found in the input text, i.e., the previous utterances, so that the question would not seem redundant. Question generation models have been deployed to engage users in a conversation (Mostafazadeh et al., 2016), but the research was focused on images. Template-based approaches, as exemplified by ELIZA (Weizenbaum, 1983), can also transform the user’s statements into questions. These templates are lab"
2021.nlp4posimpact-1.1,D11-1054,0,0.0580314,"ents in the counselling domain. Generic summarization models, however, likely need to be fine-tuned since restatements are not identical to summaries. In Table 1(c), for instance, the perspective changes from first person to second person (‘I’ll get a headache’ → ‘You’ll get a headache’); empathetic words are also inserted to diagnose the counsellee’s emotion (‘You worry ...’). To our knowledge, this is the first reported evaluation on applying a summarization model to counselling dialog generation. Previous work While chatbot response generation has exploited models from machine translation (Ritter et al., 2011) and question answering (Liu et al., 2013), there has been less effort in leveraging those from other NLP tasks such as text summarization and question generation. This section reviews research in these two fields. 2.1 2.2 Text summarization Question generation A question generation model composes a question from an input text. Neural question generation algorithms have recently attained state-of-the-art performance. For example, a sequence-to-sequence model with an attention mechanism has been proposed by Du et al. (2017). Answer separation techniques have further improved question quality (K"
2021.nlp4posimpact-1.1,D15-1044,0,0.0303351,"dialog context in that the answer should generally not be found in the input text, i.e., the previous utterances, so that the question would not seem redundant. Question generation models have been deployed to engage users in a conversation (Mostafazadeh et al., 2016), but the research was focused on images. Template-based approaches, as exemplified by ELIZA (Weizenbaum, 1983), can also transform the user’s statements into questions. These templates are labor-intensive to Text summarization models, which condense an input text into a shorter version, can generate short summaries or headlines (Rush et al., 2015). Pretrained language models such as BERT (Devlin et al., 2019) have been shown to boost the quality of summarization, among many other NLP tasks. Among the best-performing models is BertSum, which uses a document-level BERT-based encoder to express the semantics of the input text document and obtain sentence representations (Liu and Lapata, 2019). Its fine-tuning schedule adopts different optimizers for the encoder and the decoder, and has been shown to improve performance by alleviating the mismatch between them. Compared to open-domain dialogs, a human counsellor more often gives shorter re"
2021.nlp4posimpact-1.1,P16-1094,0,0.0246328,"l, with the in-domain manual dataset augmented with a large-scale, automatically mined open-domain dataset. 1 • Text summarization and question generation are NLP tasks that are potentially relevant to the counselling domain. Can we adapt models designed for these tasks to produce highquality restatements and questions for a counsellor chatbot? • Dialog data for domain-specific tasks such as counselling is often limited. Can we leverage open-domain dialog data to improve restatement and question generation? Introduction Advances in dialog modeling have facilitated chatbot use in many domains (Li et al., 2016; Zhou et al., 2020; Wang et al., 2020a). They are now also increasingly deployed for mental health assistance, including counselling (Fitzpatrick et al., 2017). Dialogs in counselling share some common characteristics with those in other domains. Advice generation, for example, can be implemented with a Q&A model that retrieves counselling materials from a knowledge base (Liu et al., 2013; Huang et al., 2015). Empathetic language — words that reflect the feelings of one’s interlocutors — is conducive to establishing rapport with the counsellee. Research in empathetic response generation has l"
2021.nlp4posimpact-1.1,W04-1013,0,0.0909831,"tion by using global information of the source context (Lin et al., 2018). Similar to above, we fine-tuned the pre-trained model with our postreply pairs.5 DialoGPT (ceiling) Same as above, the algorithm uses DialoGPT rather than BertSum+ . Oracle Retrieval To gauge the maximum performance of a retrieval-based paradigm, this algorithm selects the highest-scoring reply in the training set in terms of ROUGE-L. 5 All results are based on 5-fold cross-validation on the manual dataset (Section 3.1). Following previous research, our evaluation metrics include BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004). In addition, we report results with METEOR (Banerjee and Lavie, 2005) and BertScore (Zhang et al., 2019). We further fine-tuned the DialoGPT, mT5, BertSum and Global Encoding models with the automatically mined dataset (Section 3.2). The resulting models are denoted as DialoGPT+ , mT5+ , BertSum+ , and Global Encoding+ . 4.2 Experimental results Interleaving restatements and questions A conversation becomes monotonous and even irritating if the counsellor repeatedly gives restatements or asks questions. Using DialoGPT and BertSum+ , the two strongest models for question generation (Table 5),"
2021.nlp4posimpact-1.1,P18-2027,0,0.0133027,"ther the BertSum+ model for restatements or the BertSum+ model for questions. BertSum+ (ceiling) Designed to measure the maximum performance of BertSum+ , this algorithm identifies the subset of posts for which BertSum+ generates the highestscoring questions in terms of ROUGE-L. It replies to these posts with the generated questions, and to the remainder with restatements. Global Encoding The Global Encoding framework, which has shown competitive result in text summarization, seeks to improve the representations of the source-side information by using global information of the source context (Lin et al., 2018). Similar to above, we fine-tuned the pre-trained model with our postreply pairs.5 DialoGPT (ceiling) Same as above, the algorithm uses DialoGPT rather than BertSum+ . Oracle Retrieval To gauge the maximum performance of a retrieval-based paradigm, this algorithm selects the highest-scoring reply in the training set in terms of ROUGE-L. 5 All results are based on 5-fold cross-validation on the manual dataset (Section 3.1). Following previous research, our evaluation metrics include BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004). In addition, we report results with METEOR (Banerjee and Lavi"
2021.nlp4posimpact-1.1,D19-1012,0,0.0435805,"Missing"
2021.nlp4posimpact-1.1,D19-1387,0,0.241171,"her domains. Advice generation, for example, can be implemented with a Q&A model that retrieves counselling materials from a knowledge base (Liu et al., 2013; Huang et al., 2015). Empathetic language — words that reflect the feelings of one’s interlocutors — is conducive to establishing rapport with the counsellee. Research in empathetic response generation has led to systems that can recognize the emotional Our experiments compare a number of summarization, question generation and dialog models for the single-turn reply generation task. We obtained the strongest model by fine-tuning BertSum (Liu and Lapata, 2019), a state-of-the-art summarization model, with an in-domain, manually annotated dataset augmented with a large-scale, automatically mined open-domain dataset. After summarizing previous work (Section 2) and presenting our dataset (Section 3), we describe our approach for restatement and question generation (Section 4). We then report experimen1 Proceedings of the 1st Workshop on NLP for Positive Impact, pages 1–7 Bangkok, Thailand (online), August 5, 2021. ©2021 Association for Computational Linguistics Post (a) 每逢測驗都一定會夜晚唔食飯 專心溫習 同自己講 我一定唔可以輸 Before a test, I skip dinner to study and I say to"
2021.nlp4posimpact-1.1,2020.acl-main.470,0,0.0593281,"Missing"
2021.nlp4posimpact-1.1,P13-4012,0,0.2017,"as counselling is often limited. Can we leverage open-domain dialog data to improve restatement and question generation? Introduction Advances in dialog modeling have facilitated chatbot use in many domains (Li et al., 2016; Zhou et al., 2020; Wang et al., 2020a). They are now also increasingly deployed for mental health assistance, including counselling (Fitzpatrick et al., 2017). Dialogs in counselling share some common characteristics with those in other domains. Advice generation, for example, can be implemented with a Q&A model that retrieves counselling materials from a knowledge base (Liu et al., 2013; Huang et al., 2015). Empathetic language — words that reflect the feelings of one’s interlocutors — is conducive to establishing rapport with the counsellee. Research in empathetic response generation has led to systems that can recognize the emotional Our experiments compare a number of summarization, question generation and dialog models for the single-turn reply generation task. We obtained the strongest model by fine-tuning BertSum (Liu and Lapata, 2019), a state-of-the-art summarization model, with an in-domain, manually annotated dataset augmented with a large-scale, automatically mine"
2021.nlp4posimpact-1.1,2020.acl-demos.30,0,0.0354927,"dataset We recruited 10 undergraduate students to collect Cantonese social media posts with content concerning loneliness, academic and test anxiety. For each of the 6,294 posts collected, human annotators marked a text span as their “target phrase”, and composed a restatement and/or question for that phrase. As shown in Table 2, the dataset contains 12,634 post-restatement pairs and 9,036 postquestion pairs. There are on average 2.2 gold restatements per post, and 1.6 gold questions per post. 3.2 Approach DialoGPT We used GPT2 for Chinese chitchat1 , a dialog model that is based on DialoGPT (Zhang et al., 2020) and trained on GPT2-Chinese (Du, 2019). We fine-tuned the pre-trained model with our post-reply pairs (Section 3.1).2 mT5 Competitive question generation models can be built by fine-tuning the Google T5 model (Pan et al., 2021). Adopting a similar approach with mT5 (Xue et al., 2021), a multilingual variant of T5, we fine-tuned the mT5-base model with our post-reply pairs.3 Automatically mined dataset This dataset was automatically mined from the LCCC dataset (Wang et al., 2020b), which consists of 6.8 million Mandarin dialogs; and from 89K post-reply pairs crawled from Cantonese discussion f"
2021.nlp4posimpact-1.1,P16-1170,0,0.0178708,"from an input text. Neural question generation algorithms have recently attained state-of-the-art performance. For example, a sequence-to-sequence model with an attention mechanism has been proposed by Du et al. (2017). Answer separation techniques have further improved question quality (Kim et al., 2019). Question generation is slightly different in the dialog context in that the answer should generally not be found in the input text, i.e., the previous utterances, so that the question would not seem redundant. Question generation models have been deployed to engage users in a conversation (Mostafazadeh et al., 2016), but the research was focused on images. Template-based approaches, as exemplified by ELIZA (Weizenbaum, 1983), can also transform the user’s statements into questions. These templates are labor-intensive to Text summarization models, which condense an input text into a shorter version, can generate short summaries or headlines (Rush et al., 2015). Pretrained language models such as BERT (Devlin et al., 2019) have been shown to boost the quality of summarization, among many other NLP tasks. Among the best-performing models is BertSum, which uses a document-level BERT-based encoder to express"
2021.nlp4posimpact-1.1,2021.naacl-main.469,0,0.0263266,"eir “target phrase”, and composed a restatement and/or question for that phrase. As shown in Table 2, the dataset contains 12,634 post-restatement pairs and 9,036 postquestion pairs. There are on average 2.2 gold restatements per post, and 1.6 gold questions per post. 3.2 Approach DialoGPT We used GPT2 for Chinese chitchat1 , a dialog model that is based on DialoGPT (Zhang et al., 2020) and trained on GPT2-Chinese (Du, 2019). We fine-tuned the pre-trained model with our post-reply pairs (Section 3.1).2 mT5 Competitive question generation models can be built by fine-tuning the Google T5 model (Pan et al., 2021). Adopting a similar approach with mT5 (Xue et al., 2021), a multilingual variant of T5, we fine-tuned the mT5-base model with our post-reply pairs.3 Automatically mined dataset This dataset was automatically mined from the LCCC dataset (Wang et al., 2020b), which consists of 6.8 million Mandarin dialogs; and from 89K post-reply pairs crawled from Cantonese discussion forums in Hong Kong. We used two methods to generate post-reply pairs: Extraction. To produce post-restatement pairs, we identified the longest common string of the 1 https://github.com/yangjianxin1/GPT2-chitchat We used AdamW wi"
2021.nlp4posimpact-1.1,P02-1040,0,0.109982,"tations of the source-side information by using global information of the source context (Lin et al., 2018). Similar to above, we fine-tuned the pre-trained model with our postreply pairs.5 DialoGPT (ceiling) Same as above, the algorithm uses DialoGPT rather than BertSum+ . Oracle Retrieval To gauge the maximum performance of a retrieval-based paradigm, this algorithm selects the highest-scoring reply in the training set in terms of ROUGE-L. 5 All results are based on 5-fold cross-validation on the manual dataset (Section 3.1). Following previous research, our evaluation metrics include BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004). In addition, we report results with METEOR (Banerjee and Lavie, 2005) and BertScore (Zhang et al., 2019). We further fine-tuned the DialoGPT, mT5, BertSum and Global Encoding models with the automatically mined dataset (Section 3.2). The resulting models are denoted as DialoGPT+ , mT5+ , BertSum+ , and Global Encoding+ . 4.2 Experimental results Interleaving restatements and questions A conversation becomes monotonous and even irritating if the counsellor repeatedly gives restatements or asks questions. Using DialoGPT and BertSum+ , the two strongest models for question"
C12-2061,W09-2307,0,0.116204,"Missing"
C12-2061,W02-1806,0,0.0742568,"Missing"
C12-2061,N12-1020,1,0.768913,"2002), and the Sheffield Corpus of Chinese (Hu et al., 2005). Linguistic annotations, if available in these corpora, are limited to part-of-speech (POS) tags. With this constraint, most previous corpus-based studies focused on character frequency distribution (Zhū, 2004; Zh ng, 2004; Qín, 2005), including a concordance for the Complete Tang Poems (Shǐ, 1990). In terms of syntactic annotations, only two treebanks are currently available: a constituent treebank on 1000 sentences from the pre-Qin period (Huang et al., 2002), and a dependency treebank on a small subset of the Complete Tang Poems (Lee & Kong, 2012). This latter treebank will be used as training data to automatically produce dependency trees for the entire Complete Tang Poems, on which our word analysis will be based. In Chinese, 全唐詩 Quántángshī, (or Ch “an T ang Shi). The anthology was compiled by a team of scholars in 1705. Our digital version is downloaded from http://www.xysa.com/quantangshi/t-index.htm 1 622 2.2 Studies on the Complete Tang Poems Research on syntactic and semantic issues in the Complete Tang Poems is a venerable subfield in Classical Chinese philology, with a vast literature. We seek to demonstrate a new route of in"
C12-2061,W06-2932,0,0.0919996,"Missing"
C12-2061,W09-3811,0,0.0419791,"Missing"
C16-2003,C12-1049,0,0.0485993,"Missing"
C16-2003,N07-1058,0,0.0975668,"Missing"
C16-2003,W08-0910,0,0.0405035,"Missing"
C16-2003,P14-5010,0,0.00316995,"ist. In Review Mode, when the user successfully completes an exercise on a target word, that word is removed from the list. The user can also directly edit the “to-learn” list and/or the proficiency level in the Settings page (Section 2.4). 4 Implementation 4.1 Text Database We used Solr, a high performance search server that supports full-text search, to construct our database. We extracted a total of 524,543 articles from Chinese Wikipedia to be included in the database. On average, each article has 370 characters and 12 sentences. We segmented all texts with the Stanford Chinese segmenter (Manning et al., 2014). CC-CEDICT, a Chinese to English dictionary with 114,291 entries, supplies English translations for Chinese words in the texts. 4.2 Fill-in-the-blank Exercises For each target word, the app generates fill-in-the-blank items (Section 2.3). Each item consists of a carrier sentence with a blank, and four choices for the blank. The generation process is as follows: • Carrier sentence selection: The sentence must contain the target word, and must be between 10 and 20 words long. Further, other words in the sentence must not be more difficult (i.e., have lower frequency) than the target word. Withi"
C16-2003,W08-0911,0,0.0487519,"Missing"
C16-2020,W09-2105,0,0.041835,"Missing"
C16-2020,P11-2087,0,0.0211459,"h low literacy skills (Arnaldo Candido Jr. and Erick Maziero and Caroline Gasperin and Thiago A. S. Pardo and Lucia Specia and Sandra M. Aluisio, 2009) or language disabilities (John Carroll and Guido Minnen and Darren Pearce and Yvonne Canning and Siobhan Devlin and John Tait, 1999; Luz Rello and Ricardo Baeza-Yates, 2014). To cater to these target reader populations, language teachers, linguists and other editors are often called upon to manually adapt a text. To automate this time-consuming task, there has been much effort in developing systems for lexical simplification (Zhu et al., 2010; Biran et al., 2011) and syntactic simplification (Siddharthan, 2002; Siddharthan and Angrosh, 2014). The performance of the state-of-the-art systems has improved significantly (Horn et al., 2014; Siddharthan and Angrosh, 2014). Nonetheless, one cannot expect any single system, trained on a particular dataset, to simplify arbitrary texts in a way that would suit all readers — for example, the kinds of English words and structures suitable for a native speaker in Grade 6 are unlikely to be suitable for a non-native speaker in Grade 4. Hence, human effort is generally needed for modifying the system output. To supp"
C16-2020,N07-4002,0,0.022389,"arbitrary texts in a way that would suit all readers — for example, the kinds of English words and structures suitable for a native speaker in Grade 6 are unlikely to be suitable for a non-native speaker in Grade 4. Hence, human effort is generally needed for modifying the system output. To support human post-editing, a number of researchers have developed specialized editors for text simplification. While the editor described in Max (2006) shares similar goals as ours, it requires human intervention in much of the simplification process. The Automatic Text Adaptation tool suggests synonyms (Burstein et al., 2007), but does not perform syntactic simplification. Conversely, the SimpliThis work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 93 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: System Demonstrations, pages 93–97, Osaka, Japan, December 11-17 2016. fica tool, developed for Brazilian Portuguese, does not perform lexical simplification. Other packages for lexical simplification, such as LEXenstein (Paetzold and Specia, 2015), are not designed for post-editing. T"
C16-2020,W11-2123,0,0.015102,"y the Natural Language Toolkit (Bird et al., 2009), nor words in our stoplist, which are already simple. In terms of the three-step framework described above, we use the word2vec model1 to retrieve candidates for substitution in the first step. We trained the model with all sentences from Wikipedia. For each target word, the model returns a list of the most similar words; we extract the top 20 in this list that are included in the user-supplied vocabulary list. In the next step, substitution selection, we re-rank these 20 words with a language model. We trained a trigram model with the kenlm (Heafield, 2011), again using all sentences from Wikipedia. We then place the 10 words with the highest probabilities in a drop-down list in our editor2 ; for example, Figure 1 shows the ten candidates offered for the word “municipal”. If none of the candidates are appropriate, the user can easily revert to the original word, which is also included in the drop-down list; alternatively, the user can click on the text to directly edit it. 2.2 Evaluation We evaluated the performance of our algorithm on the Mechanical Turk Lexical Simplification Data Set (Horn et al., 2014). This dataset contains 500 manually ann"
C16-2020,P14-2075,0,0.0890361,"(John Carroll and Guido Minnen and Darren Pearce and Yvonne Canning and Siobhan Devlin and John Tait, 1999; Luz Rello and Ricardo Baeza-Yates, 2014). To cater to these target reader populations, language teachers, linguists and other editors are often called upon to manually adapt a text. To automate this time-consuming task, there has been much effort in developing systems for lexical simplification (Zhu et al., 2010; Biran et al., 2011) and syntactic simplification (Siddharthan, 2002; Siddharthan and Angrosh, 2014). The performance of the state-of-the-art systems has improved significantly (Horn et al., 2014; Siddharthan and Angrosh, 2014). Nonetheless, one cannot expect any single system, trained on a particular dataset, to simplify arbitrary texts in a way that would suit all readers — for example, the kinds of English words and structures suitable for a native speaker in Grade 6 are unlikely to be suitable for a non-native speaker in Grade 4. Hence, human effort is generally needed for modifying the system output. To support human post-editing, a number of researchers have developed specialized editors for text simplification. While the editor described in Max (2006) shares similar goals as ou"
C16-2020,E99-1042,0,0.267383,"Missing"
C16-2020,O13-1007,0,0.167462,"s.” The rewriting process involves both syntactic and lexical simplification. The former decomposes the complex sentence, extracting the participial phrase “carrying numerous books” and turning it into a separate sentence. The latter replaces the word “professor” with the simpler word “teacher”, and “numerous” with “many”. It is well known that sentences with difficult vocabulary, passive voice or complex structures, such as relative and subordinated clauses, can be challenging to understand. Text simplification has been found to be beneficial for language learners (Shirzadi, 2014), children (Kajiwara et al., 2013), and adults with low literacy skills (Arnaldo Candido Jr. and Erick Maziero and Caroline Gasperin and Thiago A. S. Pardo and Lucia Specia and Sandra M. Aluisio, 2009) or language disabilities (John Carroll and Guido Minnen and Darren Pearce and Yvonne Canning and Siobhan Devlin and John Tait, 1999; Luz Rello and Ricardo Baeza-Yates, 2014). To cater to these target reader populations, language teachers, linguists and other editors are often called upon to manually adapt a text. To automate this time-consuming task, there has been much effort in developing systems for lexical simplification (Zh"
C16-2020,P14-5010,0,0.00364157,"sentence, it identifies relative clauses, adverbial clauses, coordinated clauses, subordinated clauses, participial phrases and appositive phrases; it then splits the sentence into two simpler ones. Further, it transforms passive voice into active voice when the agent is explicitly mentioned. Examples of these constructs and their simplifications are listed in Table 1. 3.1 Algorithm The system follows the three-step framework of analysis, transformation and regeneration, as laid out in Siddharthan (2002). In the analysis step, it parses the input sentence with the Stanford dependency parser (Manning et al., 2014). In the transformation step, it scans the parse tree of the input sentence to match subtree patterns that have been manually crafted for each of the seven constructs in Table 1. In Figure 1, the input sentence matches the subtree pattern for coordination; it is therefore split into two shorter sentences, S1 =“City of Faizabad ... India.” and S2 =“and situated ... river Ghaghra”. Since S1 then matches the pattern for appositive phrase, the phrase “the headquarters of Faizabad District” is taken out to form its own sentence. If the user finds a sentence split to be inappropriate, he or she can"
C16-2020,P15-4015,0,0.0161147,"c Text Adaptation tool suggests synonyms (Burstein et al., 2007), but does not perform syntactic simplification. Conversely, the SimpliThis work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 93 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: System Demonstrations, pages 93–97, Osaka, Japan, December 11-17 2016. fica tool, developed for Brazilian Portuguese, does not perform lexical simplification. Other packages for lexical simplification, such as LEXenstein (Paetzold and Specia, 2015), are not designed for post-editing. To fill this gap, we developed a customizable, browser-based editor for simplifying English text. Besides performing automatic lexical and syntactic simplification, it facilitates user post-editing, for example in choosing candidate substitutions or undoing sentence splits. Importantly, the user can supply a vocabulary list tailored for a target reader population. This list serves to specify which words are considered “simple,” thus guiding the system in tailoring lexical substitution for the target readers. 2 Lexical Simplification The lexical simplificati"
C16-2020,E14-1076,0,0.0191548,"line Gasperin and Thiago A. S. Pardo and Lucia Specia and Sandra M. Aluisio, 2009) or language disabilities (John Carroll and Guido Minnen and Darren Pearce and Yvonne Canning and Siobhan Devlin and John Tait, 1999; Luz Rello and Ricardo Baeza-Yates, 2014). To cater to these target reader populations, language teachers, linguists and other editors are often called upon to manually adapt a text. To automate this time-consuming task, there has been much effort in developing systems for lexical simplification (Zhu et al., 2010; Biran et al., 2011) and syntactic simplification (Siddharthan, 2002; Siddharthan and Angrosh, 2014). The performance of the state-of-the-art systems has improved significantly (Horn et al., 2014; Siddharthan and Angrosh, 2014). Nonetheless, one cannot expect any single system, trained on a particular dataset, to simplify arbitrary texts in a way that would suit all readers — for example, the kinds of English words and structures suitable for a native speaker in Grade 6 are unlikely to be suitable for a non-native speaker in Grade 4. Hence, human effort is generally needed for modifying the system output. To support human post-editing, a number of researchers have developed specialized edito"
C16-2020,C10-1152,0,0.0296976,"3), and adults with low literacy skills (Arnaldo Candido Jr. and Erick Maziero and Caroline Gasperin and Thiago A. S. Pardo and Lucia Specia and Sandra M. Aluisio, 2009) or language disabilities (John Carroll and Guido Minnen and Darren Pearce and Yvonne Canning and Siobhan Devlin and John Tait, 1999; Luz Rello and Ricardo Baeza-Yates, 2014). To cater to these target reader populations, language teachers, linguists and other editors are often called upon to manually adapt a text. To automate this time-consuming task, there has been much effort in developing systems for lexical simplification (Zhu et al., 2010; Biran et al., 2011) and syntactic simplification (Siddharthan, 2002; Siddharthan and Angrosh, 2014). The performance of the state-of-the-art systems has improved significantly (Horn et al., 2014; Siddharthan and Angrosh, 2014). Nonetheless, one cannot expect any single system, trained on a particular dataset, to simplify arbitrary texts in a way that would suit all readers — for example, the kinds of English words and structures suitable for a native speaker in Grade 6 are unlikely to be suitable for a non-native speaker in Grade 4. Hence, human effort is generally needed for modifying the s"
C18-1019,E99-1042,0,0.62867,"results show that even a simple CWI model, based on graded vocabulary lists, can help reduce the number of unnecessary simplifications and complex words in the output for learners of English at different proficiency levels. 1 Introduction Lexical simplification (LS) is the task of replacing difficult words with simple words in a text, while preserving its meaning and grammaticality. It aims to produce output text that is easier to understand for readers with special needs, such as language learners, children (Kajiwara et al., 2013), and those with language disabilities (Devlin and Tait, 1998; Carroll et al., 1999). Table 1 shows an example input sentence to an LS system, and the ranked list of possible substitutions for the target word, i.e., the word that should be simplified. Most LS systems first perform complex word identification (CWI) to detect target words (i.e., “avoid” in this case), and then find appropriate substitutions for them (i.e., “prevent”, “stop”, etc., in order of preference). CWI is thus an important first step in the LS pipeline. On the one hand, an overly conservative CWI model would fail to detect many complex words, leaving them unsimplified and limiting the utility of the LS s"
C18-1019,C12-1049,0,0.290124,"Missing"
C18-1019,D14-1143,0,0.386774,"Missing"
C18-1019,P15-2011,0,0.14717,"Missing"
C18-1019,P14-2075,0,0.611464,"ion metrics. Section 6 presents experimental results and discusses the extent to which LS systems can benefit from personalized CWI. Finally, Section 7 concludes. 2 Previous work Most lexical simplification (LS) systems adopt a pipeline architecture (Shardlow, 2014; Paetzold and Specia, 2016b). The pipeline typically begins with Complex Word Identification (CWI) to find target words to be simplified. A Substitution Generation component then generates candidate replacements for these complex words. These substitutions can be learned, for example, from standard Wikipedia ˘ and Simple Wikipedia (Horn et al., 2014), or with word embedding models (Glava˘s and Stajner, 2015; Paetzold and Specia, 2016c). The Substitution Selection step then discards candidates that may distort the meaning of the text or affect its grammaticality, and retains those that best fits the context. Lastly, Substitution Ranking determines the best output by ranking the remaining candidates by simplicity. LS research has mostly adopted the user-independent approach. We now review previous work in two components of the pipeline to which we will attempt to add personalization: CWI (Section 2.1) and Substitution Ranking (Section 2.2)."
C18-1019,O13-1007,0,0.190944,"g complex word identification (CWI) models to personalize an LS system. Experimental results show that even a simple CWI model, based on graded vocabulary lists, can help reduce the number of unnecessary simplifications and complex words in the output for learners of English at different proficiency levels. 1 Introduction Lexical simplification (LS) is the task of replacing difficult words with simple words in a text, while preserving its meaning and grammaticality. It aims to produce output text that is easier to understand for readers with special needs, such as language learners, children (Kajiwara et al., 2013), and those with language disabilities (Devlin and Tait, 1998; Carroll et al., 1999). Table 1 shows an example input sentence to an LS system, and the ranked list of possible substitutions for the target word, i.e., the word that should be simplified. Most LS systems first perform complex word identification (CWI) to detect target words (i.e., “avoid” in this case), and then find appropriate substitutions for them (i.e., “prevent”, “stop”, etc., in order of preference). CWI is thus an important first step in the LS pipeline. On the one hand, an overly conservative CWI model would fail to detec"
C18-1019,L16-1491,0,0.652186,"ep in the LS pipeline. On the one hand, an overly conservative CWI model would fail to detect many complex words, leaving them unsimplified and limiting the utility of the LS system. On the other hand, an overly aggressive CWI model would be prone to misidentify simple words as complex, leading to unnecessary simplifications and increasing the risk of substitution errors. In an error analysis on LS systems, CWI-related error categories turned out to be among the most frequent (Shardlow, 2014). CWI has been receiving increasing attention in recent years, including a recent SemEval shared task (Paetzold and Specia, 2016b). Since the test set was annotated by a single learner, however, CWI performance on language learners at different levels of vocabulary proficiency continues to be under-explored. Indeed, most LS evaluations assume one best substitution or one fixed ranked list of substitutions (cf. Table 1), and do not take into account variations in vocabulary knowledge among users. This “onesize-fits-all” approach is suboptimal since word complexity is in the eye of the beholder: a word that is complex for a low-proficiency user may be perfectly familiar to a high-proficiency user, or even to a low-profic"
C18-1019,W16-4912,0,0.0686318,"ep in the LS pipeline. On the one hand, an overly conservative CWI model would fail to detect many complex words, leaving them unsimplified and limiting the utility of the LS system. On the other hand, an overly aggressive CWI model would be prone to misidentify simple words as complex, leading to unnecessary simplifications and increasing the risk of substitution errors. In an error analysis on LS systems, CWI-related error categories turned out to be among the most frequent (Shardlow, 2014). CWI has been receiving increasing attention in recent years, including a recent SemEval shared task (Paetzold and Specia, 2016b). Since the test set was annotated by a single learner, however, CWI performance on language learners at different levels of vocabulary proficiency continues to be under-explored. Indeed, most LS evaluations assume one best substitution or one fixed ranked list of substitutions (cf. Table 1), and do not take into account variations in vocabulary knowledge among users. This “onesize-fits-all” approach is suboptimal since word complexity is in the eye of the beholder: a word that is complex for a low-proficiency user may be perfectly familiar to a high-proficiency user, or even to a low-profic"
C18-1019,E17-2006,0,0.166755,"potentially be trained on graded text corpora, such as the Newsela corpus (Xu et al., 2015). This approach thus offers more coarse-grained personalization, akin to graded readers, and not every user necessarily fits neatly into one of the predetermined levels. 2.2 Substitution ranking Given a set of candidates from the Substitution Selection step, the Substitution Ranking step chooses the simplest candidate. Most current approaches impose the same notion of simplicity on all users. Recent systems have applied machine learning approaches, such as the SVM (Horn et al., 2014) and neural models (Paetzold and Specia, 2017), on a range of features including word frequencies in large corpora and human rankings in LS datasets. This step can potentially be enhanced with CWI to filter out candidates that are complex words. 3 Data In this section, we first describe our dataset of language learners (Section 3.1), and then explain how we used it to create personalized versions of an existing, user-independent dataset of lexical simplification (Section 3.2). 3.1 User dataset Our user dataset was annotated by 15 learners of English as a foreign language who were native speakers of Japanese (Ehara et al., 2010). Each lear"
C18-1019,shardlow-2014-open,0,0.450815,"iate substitutions for them (i.e., “prevent”, “stop”, etc., in order of preference). CWI is thus an important first step in the LS pipeline. On the one hand, an overly conservative CWI model would fail to detect many complex words, leaving them unsimplified and limiting the utility of the LS system. On the other hand, an overly aggressive CWI model would be prone to misidentify simple words as complex, leading to unnecessary simplifications and increasing the risk of substitution errors. In an error analysis on LS systems, CWI-related error categories turned out to be among the most frequent (Shardlow, 2014). CWI has been receiving increasing attention in recent years, including a recent SemEval shared task (Paetzold and Specia, 2016b). Since the test set was annotated by a single learner, however, CWI performance on language learners at different levels of vocabulary proficiency continues to be under-explored. Indeed, most LS evaluations assume one best substitution or one fixed ranked list of substitutions (cf. Table 1), and do not take into account variations in vocabulary knowledge among users. This “onesize-fits-all” approach is suboptimal since word complexity is in the eye of the beholder:"
C18-1019,yimam-etal-2017-multilingual,0,0.0647556,"determines the best output by ranking the remaining candidates by simplicity. LS research has mostly adopted the user-independent approach. We now review previous work in two components of the pipeline to which we will attempt to add personalization: CWI (Section 2.1) and Substitution Ranking (Section 2.2). 2.1 Complex word identification The complex word identification (CWI) task classifies words in a text as either “complex” or “noncomplex”. Complex words are those that are difficult for a non-native speaker to understand; noncomplex words are those that are not (Paetzold and Specia, 2016b; Yimam et al., 2017). In the 2016 SemEval CWI shared task, the best team, which combined various lexicon-based, threshold-based and machine learning voter sub-systems, achieved a precision of 0.147 and recall of 0.769 (Paetzold and Specia, 2016b). Overall, word frequencies were found to give the most reliable prediction for word complexity. The shared task was not designed to test performance on users at different proficiency levels, since the test set was annotated by a single learner. To date, most CWI research has taken the user-independent approach, with only a few published studies on personalized CWI. Zeng"
C18-1292,C12-1049,0,0.275956,"the 2016 SemEval shared task for CWI in English (Paetzold and Specia, 2016). The best team, which combined various lexicon-based, threshold-based and machine learning voter subsystems, achieved a precision of 0.147 and recall of 0.769. This approach has yet to be evaluated on language learners at different levels of proficiency, since the entire test set was annotated by one learner. Personalized models for CWI adjust their predictions based on user characteristics. Zeng et al. (2005) showed that demographic features can help improve CWI performance for individual users in the medical domain. Ehara et al. (2012; 2014) performed CWI for individual learners with a two-step approach. In the first step, an active learning approach selects the k most informative nodes, or words, to be annotated by the user (see Section 3.1 for details). The user rates their knowledge of these k words on a five-point scale (Table 1). In the second step, the system uses Local and Global Consistency (Zhou et al., 2004), a label propagation algorithm, to train an independent classifier for each user. The algorithm performs binary classification on the nodes to predict whether a user knows a word or not. The assumption behind"
C18-1292,D14-1143,0,0.724338,"al student. This paper describes a personalized text retrieval algorithm that helps language learners select the most suitable reading material in terms of vocabulary complexity. The user first rates their knowledge of a small set of words, chosen by a graph-based active learning model. The system trains a complex word identification model (CWI) on this set. Most current CWI approaches assume vocabulary knowledge to be a dichotomy, labeling each word either as “non-complex”, if the user is familiar with the word, or as “complex”, if the user is not familiar with it (Paetzold and Specia, 2016; Ehara et al., 2014). There are, however, many dimensions of vocabulary knowledge (Richards, 1976). As our first contribution, we take one step in this direction by attempting to identify not only known and unknown words, but also challenging words. Further, we apply CWI to the text retrieval task, and measure its effect on the vocabulary complexity of retrieved texts from the perspective of a language learner. Specifically, the algorithm attempts to find texts with a minimum proportion of non-complex words and a maximum number of challenging words for the learner. As our second contribution, we show that the alg"
C18-1292,N07-1058,0,0.111377,"Missing"
C18-1292,P13-3015,0,0.0700496,"Missing"
C18-1292,W15-0614,0,0.0682405,"Missing"
C18-1292,W18-0507,0,0.0147929,"tried to learn the word before but forgot its meaning Probably know, or able to guess, the word’s meaning Absolutely know the word’s meaning Label (Ehara) complex complex complex Label (this study) complex complex complex complex challenging non-complex non-complex Table 1: Five-point scale for rating vocabulary knowledge, and the mapping to CWI labels used by Ehara et al. (2012; 2014) and in this study. Recently, CWI for other languages has begun to receive more attention. The latest CWI shared task, for example, featured multilingual datasets, covering English, German, Spanish, and French (Yimam et al., 2018). To the best of our knowledge, there has so far been only one reported study on CWI for Chinese as a foreign language (CFL) (Lee and Yeung, 2018). They followed Ehara et al. (2012; 2014) in selecting 50 most informative words to be rated by CFL learners. They then used a support vector machine (SVM) classifier to train a CWI classifier for each individual learner based on his/her 50-word 3449 training set. The classifier used features based on word difficulty levels in two assessment scales for CFL, namely the Test of Chinese as a Foreign Language (TOCFL) (Zeng, 2014) and the Hanyu Shuiping K"
I11-1174,W08-0336,0,0.0443152,"Missing"
I11-1174,Y96-1018,0,0.0150915,"nese communities. Although considered the “most widely known and influential variety of Chinese other than Mandarin” (Matthews & Yip, 1994), Cantonese currently has rather limited linguistic resources. This paucity may be due to its unofficial status, as opposed to Mandarin, which is the official language of China. Furthermore, as a primarily spoken language, it does not traditionally have any standard written form. This paper presents the first parallel corpus of transcribed Cantonese speech and its equivalent written Mandarin. The corpus is expected to be useful for language 1 For example, (Chen et al., 1996), (Xue et al., 2005), and (Tsou & Kwong, 2006), among many others learners, linguists and developers of natural language processing applications. The corpus provides students with authentic, parallel examples of sentences in both languages, which are not mutually intelligible. Native speakers of Cantonese must learn Mandarin for use in writing and official communication; conversely, many Mandarin speakers living in Hong Kong also want to learn Cantonese. The corpus also serves as a repository for linguistic research. In particular, it facilitates research in comparative grammar, by lending sta"
I11-1174,tsou-kwong-2006-toward,0,0.0178999,"st widely known and influential variety of Chinese other than Mandarin” (Matthews & Yip, 1994), Cantonese currently has rather limited linguistic resources. This paucity may be due to its unofficial status, as opposed to Mandarin, which is the official language of China. Furthermore, as a primarily spoken language, it does not traditionally have any standard written form. This paper presents the first parallel corpus of transcribed Cantonese speech and its equivalent written Mandarin. The corpus is expected to be useful for language 1 For example, (Chen et al., 1996), (Xue et al., 2005), and (Tsou & Kwong, 2006), among many others learners, linguists and developers of natural language processing applications. The corpus provides students with authentic, parallel examples of sentences in both languages, which are not mutually intelligible. Native speakers of Cantonese must learn Mandarin for use in writing and official communication; conversely, many Mandarin speakers living in Hong Kong also want to learn Cantonese. The corpus also serves as a repository for linguistic research. In particular, it facilitates research in comparative grammar, by lending statistical evidence, and potentially demonstrati"
I11-1174,2010.jec-1.7,0,0.020205,"us also serves as a repository for linguistic research. In particular, it facilitates research in comparative grammar, by lending statistical evidence, and potentially demonstrating exceptions or other differences yet unnoticed. Finally, it can be exploited as training material for natural language processing systems, such as cross-lingual spoken document retrieval (Meng & Hui, 2001), and especially machine translation (MT) systems. For example, MT systems may be trained to automatically generate Chinese subtitles for Cantonese television programs, as has been done for Scandinavian languages (Volk et al., 2010). 2 Previous Work Cantonese grammar has been well studied (Matthews & Yip, 1994; Cheung, 2007), and a few monolingual corpora for Cantonese have been compiled (Lee & Wong, 1998; Leung & Law, 2001; Wong, 2006). While the present corpus may also be used simply as Cantonese data, its primary contribution is as parallel data between Cantonese and Mandarin. The main difference between Cantonese and Mandarin is in phonology and vocabulary; indeed, various bilingual dictionaries and lexical comparisons are already available (Zhang & Yang, 2008). In terms of syntax, although the “grammatical structure"
I11-1174,prokopidis-etal-2008-condensing,0,\N,Missing
I17-2055,D12-1072,0,0.629359,"Missing"
I17-2055,ruppenhofer-etal-2010-speaker,0,0.0326799,"form the rule-based approach. We present the first study that evaluates both speaker and listener identification for direct speech in literary texts. Our approach consists of two steps: identification of speakers and listeners near the quotes, and dialogue chain segmentation. Evaluation results show that this approach outperforms a rule-based approach that is stateof-the-art on a corpus of literary texts. 1 2 Previous Work Among rule-based approaches on speaker identification, most rely on speech verbs to locate the speakers (Pouliquen et al., 2007; Glass and Bangay, 2007; Liang et al., 2010; Ruppenhofer et al., 2010). For machine learning approaches, Elson and McKeown (2010) treated the task as classification, using features such as the distance between quotes and speakers, the presence of punctuation marks, etc. O’Keefe et al. (2012) reformulated the task as a sequence labelling task. In the news domain, their statistical model outperformed a rulebasd approach; in the literary domain, however, the rule-based approach achieved the best performance. This rule-based approach will be compared with our proposed approach in our experiments. Similar to our approach, He et al. (2013) parsed the sentences near th"
I17-2055,P10-1015,0,0.789105,"rocessing step that extracts all name mentions in the text and clusters them into character aliases, and so cannot be directly compared with our approach, which does not require preprocessing and thus can be more easily applied to larger datasets. Introduction A literary work can be analysed in terms of its conversational network, often encoded as a graph whose nodes represent characters, and whose edges indicate dialogue interactions between characters. Such a network has been drawn for Hamlet (Moretti, 2011), Classical Greek tragedies (Rydberg-Cox, 2011), as well as a set of British novels (Elson et al., 2010). To automatically construct these networks, it is necessary to identify the speakers and listeners of quoted speech. Past research on quote attribution has mostly focused on speaker identification (O’Keefe et al., 2012; He et al., 2013). In the only previous study that attempts both speaker and listener identification (Elson et al., 2010), there was no formal evaluation on the listeners. Listener identification can be expected to be challenging, since they are more often implicit. This paper presents the first evaluation on both speaker and listener identification, with two main contributions"
I17-2055,P05-1045,0,0.0237435,"text into dialogue chains. Each quote can be a continuation of a dialogue, i.e., its speaker (listener) is the listener (speaker) of the preceding quote; otherwise, it is the beginning of a new chain. As shown in 4.1 Baselines Speaker and listener identification Distance baseline. We re-implemented the rulebased approach that was state-of-the-art for literary texts (O’Keefe et al., 2012), achieving the best 326 6 performance on a re-annotated version of the Elson et al. (2010) dataset. We take as entities all pronouns and all words tagged as person and organization by the Stanford NER tagger (Finkel et al., 2005). We compiled a list of quotative verbs by retrieving the verbs closest to the quotes in the training set.1 Dependency baseline (Dep). We parsed the sentence that contains the quote, excluding the words within the quote and replacing the trailing comma, if any, with a full-stop. If a quotative verb is modified by a word with the dependency relation ‘nsubj’, that word is extracted as the speaker; if it is modified by a word with ‘dobj’ or ‘nmod’, that word is extracted as the listener. 4.2 We used the Stanford parser (Manning et al., 2014) for POS tagging and dependency parsing, and CRF++ (Kudo"
I17-2055,P13-1129,0,0.561245,"Liang et al., 2010; Ruppenhofer et al., 2010). For machine learning approaches, Elson and McKeown (2010) treated the task as classification, using features such as the distance between quotes and speakers, the presence of punctuation marks, etc. O’Keefe et al. (2012) reformulated the task as a sequence labelling task. In the news domain, their statistical model outperformed a rulebasd approach; in the literary domain, however, the rule-based approach achieved the best performance. This rule-based approach will be compared with our proposed approach in our experiments. Similar to our approach, He et al. (2013) parsed the sentences near the quotation. Their method, however, includes a manual preprocessing step that extracts all name mentions in the text and clusters them into character aliases, and so cannot be directly compared with our approach, which does not require preprocessing and thus can be more easily applied to larger datasets. Introduction A literary work can be analysed in terms of its conversational network, often encoded as a graph whose nodes represent characters, and whose edges indicate dialogue interactions between characters. Such a network has been drawn for Hamlet (Moretti, 201"
I17-2055,L16-1168,1,0.619246,"Missing"
I17-2055,P14-5010,0,\N,Missing
I17-2073,D16-1114,0,0.0135107,"ognitive disabilities and even machines. Approaches to lexical simplification generally follow a standard pipeline consisting of two main steps: generation and ranking. In the generation step, a set of possible substitutions for the target word is commonly created by querying semantic databases such as Wordnet (Devlin and Tait, 1998), learning substitution rules from sentence-aligned parallel corpora of complex-simple texts (Horn et al., 2014; Paetzold and Specia, 2017), and learning word embeddings from a large corpora to obtain similar words of ˇ the complex word (Glavaˇs and Stajner, 2015; Kim et al., 2016; Paetzold and Specia, 2016a, 2017). In the ranking step, features that discriminate a substitution candidate from other substitution candidates are leveraged and the candidates are ranked with respect to their simplicity and contextual fitness. 2 2.1 Method Task Definition We focus on the ranking step of the standard lexical simplification pipeline. Given a dataset of tar∗ This research was conducted while the first author was a Post Doctoral Fellow at the City University of Hong Kong. 430 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 430–435,"
I17-2073,N15-1092,1,0.925286,"e features at different levels of abstractions that capture nuances that discriminate the candidates. In this paper, we propose to use a Deep Structured Similarity Model (DSSM) (Huang et al., 2013) to rank substitution candidates. The DSSM exploits a deep architecture by using a deep neural network (DNN), that can effectively capture contextual features to perform semantic matching between two sentences. It has been successfully applied to several natural language processing (NLP) tasks, such as machine translation (Gao et al., 2014), web search ranking (Huang et al., 2013; Shen et al., 2014; Liu et al., 2015), question answering (Yih et al., 2014), and image captioning (Fang et al., 2015). To the best of our knowledge, this is the first time this model is applied to lexical simplification. We adapt the original DSSM architecture and objective function to our specific task. Our evaluation on two standard datasets for lexical simplification shows that this method outperforms state-of-the-art approaches that use supervised machine learning-based methods. We explore the application of a Deep Structured Similarity Model (DSSM) to ranking in lexical simplification. Our results show that the DSSM can eff"
I17-2073,E17-2006,0,0.126615,"riants, while retaining its meaning and grammaticality. The goal is to make the text easier to understand for children, language learners, people with cognitive disabilities and even machines. Approaches to lexical simplification generally follow a standard pipeline consisting of two main steps: generation and ranking. In the generation step, a set of possible substitutions for the target word is commonly created by querying semantic databases such as Wordnet (Devlin and Tait, 1998), learning substitution rules from sentence-aligned parallel corpora of complex-simple texts (Horn et al., 2014; Paetzold and Specia, 2017), and learning word embeddings from a large corpora to obtain similar words of ˇ the complex word (Glavaˇs and Stajner, 2015; Kim et al., 2016; Paetzold and Specia, 2016a, 2017). In the ranking step, features that discriminate a substitution candidate from other substitution candidates are leveraged and the candidates are ranked with respect to their simplicity and contextual fitness. 2 2.1 Method Task Definition We focus on the ranking step of the standard lexical simplification pipeline. Given a dataset of tar∗ This research was conducted while the first author was a Post Doctoral Fellow at"
I17-2073,P15-4015,0,0.322504,"he semantic representation of T and nonlinear projection W s constructs the semantic representation of S. Finally, the cosine similarity is adopted to measure the relevance between the T and S. At last, the posterior probabilities over all candidates are computed. T T Sq S Sq R(T, S) = cosine(T Sq , S Sq ) = T Sq S Sq (2) 2.3 Features for DSSM As baseline features, we use the same n-gram probability features as in Paetzold and Specia (2017), who also employ a neural network to rank substitution candidates. As in Paetzold and Specia (2017), the features were extracted using the SubIMDB corpus (Paetzold and Specia, 2015). We also experiment with additional features that have been reported as useful in this task. For each target word and a substitution candidate word we also compute: cosine similarity, word length, and alignment probability in the Compared to other latent semantic models, such as Latent Semantic Analysis (Deerwester et al., 1990) and its extensions, Deep Structured Similarity Model (also called Deep Semantic Similarity Model) or DSSM (Huang et al., 2013) can cap1 (1) 431 sentence-aligned Normal-Simple Wikipedia corpus (Kauchak, 2013). The cosine similarity feature is computed using the SubIMDB"
I17-2073,W16-4912,0,0.0240763,"ond, WA 98052, USA 3 Department of Linguistics and Translation, City University of Hong Kong kanash@wni.com, xiaodl@microsoft.com, jsylee@cityu.edu.hk Abstract The ranking step is challenging because the substitution candidates usually have similar meaning to the target word, and thus share similar context features. State-of-the-art approaches to ranking in lexical simplification exploit supervised machine learning-based methods that rely mostly on surface features, such as word frequency, word length and n-gram probability, for training the model (Horn et al., 2014; Bingel and Søgaard, 2016; Paetzold and Specia, 2016a, 2017). Moreover, deep architectures are not explored in these models. Surface features and shallow architectures might not be able to explore the features at different levels of abstractions that capture nuances that discriminate the candidates. In this paper, we propose to use a Deep Structured Similarity Model (DSSM) (Huang et al., 2013) to rank substitution candidates. The DSSM exploits a deep architecture by using a deep neural network (DNN), that can effectively capture contextual features to perform semantic matching between two sentences. It has been successfully applied to several n"
I17-2073,P14-1066,0,0.0272188,"ls. Surface features and shallow architectures might not be able to explore the features at different levels of abstractions that capture nuances that discriminate the candidates. In this paper, we propose to use a Deep Structured Similarity Model (DSSM) (Huang et al., 2013) to rank substitution candidates. The DSSM exploits a deep architecture by using a deep neural network (DNN), that can effectively capture contextual features to perform semantic matching between two sentences. It has been successfully applied to several natural language processing (NLP) tasks, such as machine translation (Gao et al., 2014), web search ranking (Huang et al., 2013; Shen et al., 2014; Liu et al., 2015), question answering (Yih et al., 2014), and image captioning (Fang et al., 2015). To the best of our knowledge, this is the first time this model is applied to lexical simplification. We adapt the original DSSM architecture and objective function to our specific task. Our evaluation on two standard datasets for lexical simplification shows that this method outperforms state-of-the-art approaches that use supervised machine learning-based methods. We explore the application of a Deep Structured Similarity Model (DSSM"
I17-2073,L16-1491,0,0.113327,"ond, WA 98052, USA 3 Department of Linguistics and Translation, City University of Hong Kong kanash@wni.com, xiaodl@microsoft.com, jsylee@cityu.edu.hk Abstract The ranking step is challenging because the substitution candidates usually have similar meaning to the target word, and thus share similar context features. State-of-the-art approaches to ranking in lexical simplification exploit supervised machine learning-based methods that rely mostly on surface features, such as word frequency, word length and n-gram probability, for training the model (Horn et al., 2014; Bingel and Søgaard, 2016; Paetzold and Specia, 2016a, 2017). Moreover, deep architectures are not explored in these models. Surface features and shallow architectures might not be able to explore the features at different levels of abstractions that capture nuances that discriminate the candidates. In this paper, we propose to use a Deep Structured Similarity Model (DSSM) (Huang et al., 2013) to rank substitution candidates. The DSSM exploits a deep architecture by using a deep neural network (DNN), that can effectively capture contextual features to perform semantic matching between two sentences. It has been successfully applied to several n"
I17-2073,P15-2011,0,0.0486796,"Missing"
I17-2073,Q15-1021,0,0.0324083,"mber 27 – December 1, 2017 2017 AFNLP get words, their sentential contexts and substitution candidates for the target words, the goal is to train a model that accurately ranks the candidates based on their simplicity and semantic matching. For generating substitution candidates, we utilize the method proposed by Paetzold and Specia (2017), which was recently shown to be the state-of-art method for generating substitution candidates. They exploit a hybrid substitution generation approach where candidates are first extracted from 550,644 simple-complex aligned sentences from the Newsela corpus (Xu et al., 2015). Then, these candidates are complemented with candidates generated with a retrofitted word embedding model. The word embedding model is retrofitted over WordNet’s synonym pairs (for details, please refer to Paetzold and Specia (2017)). For ranking substitution candidates, we use a DSSM, which we elaborate in the next section. 2.2 ture fine-grained local and global contextual features more effectively. The DSSM is trained by optimizing a similarity-driven objective, by projecting the whole sentence to a continuous semantic space. In addition, it is is built upon characters (rather than words)"
I17-2073,P14-2075,0,0.276677,"2 Microsoft Research, One Microsoft Way, Redmond, WA 98052, USA 3 Department of Linguistics and Translation, City University of Hong Kong kanash@wni.com, xiaodl@microsoft.com, jsylee@cityu.edu.hk Abstract The ranking step is challenging because the substitution candidates usually have similar meaning to the target word, and thus share similar context features. State-of-the-art approaches to ranking in lexical simplification exploit supervised machine learning-based methods that rely mostly on surface features, such as word frequency, word length and n-gram probability, for training the model (Horn et al., 2014; Bingel and Søgaard, 2016; Paetzold and Specia, 2016a, 2017). Moreover, deep architectures are not explored in these models. Surface features and shallow architectures might not be able to explore the features at different levels of abstractions that capture nuances that discriminate the candidates. In this paper, we propose to use a Deep Structured Similarity Model (DSSM) (Huang et al., 2013) to rank substitution candidates. The DSSM exploits a deep architecture by using a deep neural network (DNN), that can effectively capture contextual features to perform semantic matching between two sen"
I17-2073,P14-2105,0,0.0379564,"actions that capture nuances that discriminate the candidates. In this paper, we propose to use a Deep Structured Similarity Model (DSSM) (Huang et al., 2013) to rank substitution candidates. The DSSM exploits a deep architecture by using a deep neural network (DNN), that can effectively capture contextual features to perform semantic matching between two sentences. It has been successfully applied to several natural language processing (NLP) tasks, such as machine translation (Gao et al., 2014), web search ranking (Huang et al., 2013; Shen et al., 2014; Liu et al., 2015), question answering (Yih et al., 2014), and image captioning (Fang et al., 2015). To the best of our knowledge, this is the first time this model is applied to lexical simplification. We adapt the original DSSM architecture and objective function to our specific task. Our evaluation on two standard datasets for lexical simplification shows that this method outperforms state-of-the-art approaches that use supervised machine learning-based methods. We explore the application of a Deep Structured Similarity Model (DSSM) to ranking in lexical simplification. Our results show that the DSSM can effectively capture fine-grained features"
I17-3012,O13-3001,0,0.0474464,"Missing"
I17-3012,P16-4002,0,0.0163745,"other languages (Pitler and Nenkova, 2008; Sato et al., 2008). Among those that target language learners, most give a holistic score on the overall difficulty level of the text (François and Fairon, 2012; Pilán et al., 2014), but do not specifically indicate the difficult words or grammatical constructions. Hence, while these systems can help identify suitable reading material for language learners (Brown and Eskenazi, 2004), they are not designed to facilitate editing of language teaching materials, which is the goal of our system. Targeting learners of English as a foreign language, FLAIR (Chinkina et al., 2016) can detect 87 linguistic forms in the official English curriculum in a German state. The system attains an average precision and recall of 0.94 and 0.90 in detecting grammar points. Most systems for CFL determine the difficulty level of a text on the basis of vocabulary difficulty alone. ChineseTA (Chu, 2005), for example, estimate vocabulary difficulty on the basis of word frequencies interpolated from varIntroduction Reading is critical to foreign language acquisition (Krashen, 2005). While language textbooks provide a convenient source of reading materials, these materials are limited in q"
I17-3012,P14-5010,0,0.00260289,"shrimps painted by Qibaishi in early times.” 5 NR 6+ 3 1 6+ 1 5 6+ 5 3 1 1 Parenthetical Relative clause with subject Adverb of Verbal predicate expression and predicate degree Table 1: Vocabulary and grammar difficulty level of an example sentence, according to the HSK scale. “NR” refers to a proper noun; 6+ is the vocabulary level attributed to words not found in the HSK vocabulary lists. Lv 1 2 3 Vocab. items 150 150 600 Gram. points 35 58 68 Lv 4 5 6 Vocab. items 1200 2500 5000 Gram. points 38 39 28 performs word segmentation, POS tagging and dependency parsing using the Stanford Parser (Manning et al., 2014). It then offers difficulty assessment in terms of vocabulary and grammar (Section 3.1), and guides the user in editing the sentence towards the target difficulty level (Section 3.2). Table 2: Number of vocabulary items and grammar points at each HSK level 3.1 Difficulty assessment det The HSK guidelines provide a vocabulary list and a set of grammar points for each level; as shown in Table 2), there are a total of 9,600 vocabulary items and 266 grammar points. For vocabulary assessment, the system matches each word with these lists, but does not assess the difficulty level of proper nouns, ex"
I17-3012,W14-1821,0,0.0385928,"Missing"
I17-3012,D08-1020,0,0.0630872,"Missing"
I17-3012,sato-etal-2008-automatic,0,0.0927623,"Missing"
L16-1168,W12-2513,0,0.100437,"erances in a text, as well as the quotative verbs that reports the utterances. We measure inter-annotator agreement on this annotation task. We then present statistics on a manually annotated corpus that consists of books from the New Testament. Finally, we visualize the corpus as a conversational network. Keywords: direct speech, coreference, corpus annotation 1. possible. A summary of the elements and their attributes are provided in Table 1. Introduction There is increasing interest in analyzing social networks in literary texts. Such networks have been constructed for Alice in Wonderland (Agarwal et al., 2012), Les Misérables (Newman and Girvan, 2004), Biographies of Eminent Monks (Bingenheimer et al., 2011), a set of British novels (Elson et al., 2010), Hamlet (Moretti, 2011) and various Classical Greek tragedies (Rydberg-Cox, 2011), to name just a few examples. Social networks can visualize the protagonists, closely related characters and communities in the text. They also support the analysis of text structures and properties, such as the perspective holder (Agarwal et al., 2012) and the density of dialog interactions (Elson et al., 2010). While characters can socially interact in a variety of w"
L16-1168,P10-1015,0,0.681244,"present statistics on a manually annotated corpus that consists of books from the New Testament. Finally, we visualize the corpus as a conversational network. Keywords: direct speech, coreference, corpus annotation 1. possible. A summary of the elements and their attributes are provided in Table 1. Introduction There is increasing interest in analyzing social networks in literary texts. Such networks have been constructed for Alice in Wonderland (Agarwal et al., 2012), Les Misérables (Newman and Girvan, 2004), Biographies of Eminent Monks (Bingenheimer et al., 2011), a set of British novels (Elson et al., 2010), Hamlet (Moretti, 2011) and various Classical Greek tragedies (Rydberg-Cox, 2011), to name just a few examples. Social networks can visualize the protagonists, closely related characters and communities in the text. They also support the analysis of text structures and properties, such as the perspective holder (Agarwal et al., 2012) and the density of dialog interactions (Elson et al., 2010). While characters can socially interact in a variety of ways, their conversations — who talked to whom, and what they talked about — are key indicators of their relationships. This paper proposes a schem"
L16-1168,M98-1029,0,\N,Missing
L16-1265,W09-2307,0,0.0903911,"Missing"
L16-1265,W02-1806,0,0.0773954,"ddhist Chinese material consists only of four sutras (Lee and Kong, 2014). We trained a dependency parser on this small treebank, and then automatically parsed the entire Chinese Buddhist Canon. In this paper, we start with an overview of existing treebanks for ancient Chinese (Section 2). We then report the procedure for constructing this treebank, and evaluate its accuracy (Section 3). Finally, as a case To the best of our knowledge, only three treebanks are available to-date for ancient Chinese. First, a constituent-based treebank has been constructed on 1,000 sentences from pre-Qin texts (Huang et al., 2002). Second, a dependency treebank has been annotated for 32,000 characters of Tang poems, selected from the works of three poets from the 8th century CE (Lee and Kong, 2012). Third, and most related to this work, is a dependency treebank of four sutras, numbering about 50,000 characters, taken from the Chinese Buddhist Canon (Lee and Kong, 2014). Written in medieval Chinese, the Canon consists of translations of Buddhist texts from Indic languages, produced from the 2nd to the 11th centuries CE. Figure 1: Dependency tree for the sentence, “Ananda bowed and addressed Buddha, saying, ‘…’”. 1679 3."
L16-1265,N12-1020,1,0.843498,"Chinese Buddhist Canon. In this paper, we start with an overview of existing treebanks for ancient Chinese (Section 2). We then report the procedure for constructing this treebank, and evaluate its accuracy (Section 3). Finally, as a case To the best of our knowledge, only three treebanks are available to-date for ancient Chinese. First, a constituent-based treebank has been constructed on 1,000 sentences from pre-Qin texts (Huang et al., 2002). Second, a dependency treebank has been annotated for 32,000 characters of Tang poems, selected from the works of three poets from the 8th century CE (Lee and Kong, 2012). Third, and most related to this work, is a dependency treebank of four sutras, numbering about 50,000 characters, taken from the Chinese Buddhist Canon (Lee and Kong, 2014). Written in medieval Chinese, the Canon consists of translations of Buddhist texts from Indic languages, produced from the 2nd to the 11th centuries CE. Figure 1: Dependency tree for the sentence, “Ananda bowed and addressed Buddha, saying, ‘…’”. 1679 3. in medieval Chinese text contain more than two syllables. Therefore, we followed Peng et al. (2004) and Tseng et al. (2005) in adopting a 2-tag set for word segmentation."
L16-1265,C04-1081,0,0.0102937,"oems, selected from the works of three poets from the 8th century CE (Lee and Kong, 2012). Third, and most related to this work, is a dependency treebank of four sutras, numbering about 50,000 characters, taken from the Chinese Buddhist Canon (Lee and Kong, 2014). Written in medieval Chinese, the Canon consists of translations of Buddhist texts from Indic languages, produced from the 2nd to the 11th centuries CE. Figure 1: Dependency tree for the sentence, “Ananda bowed and addressed Buddha, saying, ‘…’”. 1679 3. in medieval Chinese text contain more than two syllables. Therefore, we followed Peng et al. (2004) and Tseng et al. (2005) in adopting a 2-tag set for word segmentation. Three Buddhist lexicons — the Soothill-Hodous Dictionary of Chinese Buddhist Terms (Soothill-Hodous & Lewis, 1995), the Person and Place Authority Databases from Dharma Drum Buddhist College (DDBC, 2008a; 2008b) — served as dictionaries. Treebank Construction 3.1 Textual material Our treebank is based on a digital version of the Tripiṭaka Koreana, the Korean Edition of the Chinese Buddhist Canon (Lancaster, 2010). This edition is derived from the most complete set of available printing blocks, those currently stored at Hae"
L16-1265,I05-3027,0,0.059642,"works of three poets from the 8th century CE (Lee and Kong, 2012). Third, and most related to this work, is a dependency treebank of four sutras, numbering about 50,000 characters, taken from the Chinese Buddhist Canon (Lee and Kong, 2014). Written in medieval Chinese, the Canon consists of translations of Buddhist texts from Indic languages, produced from the 2nd to the 11th centuries CE. Figure 1: Dependency tree for the sentence, “Ananda bowed and addressed Buddha, saying, ‘…’”. 1679 3. in medieval Chinese text contain more than two syllables. Therefore, we followed Peng et al. (2004) and Tseng et al. (2005) in adopting a 2-tag set for word segmentation. Three Buddhist lexicons — the Soothill-Hodous Dictionary of Chinese Buddhist Terms (Soothill-Hodous & Lewis, 1995), the Person and Place Authority Databases from Dharma Drum Buddhist College (DDBC, 2008a; 2008b) — served as dictionaries. Treebank Construction 3.1 Textual material Our treebank is based on a digital version of the Tripiṭaka Koreana, the Korean Edition of the Chinese Buddhist Canon (Lancaster, 2010). This edition is derived from the most complete set of available printing blocks, those currently stored at Haein Monastery in Korea (L"
L16-1265,O97-3006,0,0.439863,"er the past decade, there has been growing interest in building treebanks for historical texts, not only for facilitating their reading but also for studying the historical languages in which they were written. The sacred texts of many major religious, for example, have been syntactically analysed: treebanks are now available for the Hebrew Bible (Wu & Lowery, 2006), the New Testament in Greek (Haug & Jøhndal, 2008), and the Qur’an in Classical Arabic (Dukes & Buckwalter, 2010). 2. Previous Work Among large, diachronic corpora in ancient Chinese are the Academia Sinica Ancient Chinese Corpus (Wei et al., 1997) and the Sheffield Corpus of Chinese (Hu et al., 2005), both covering a wide range of time and genres. Although word-segmented and POS-tagged, they have not been syntactically analyzed. With about 50 million characters, the sheer volume of the Chinese Buddhist Canon makes it difficult for any individual to perform manual analysis over the entire corpus. Although digitized versions of the Canon have enabled n-gram and other lexical analyses (Lancaster, 2010), it remains difficult to examine patterns in part-of-speech (POS) and sentence structures without syntactic annotations. To date, the only"
L16-1265,wu-lowery-2006-hebrew,0,0.0271066,"between the speaker and the listener. Keywords: dependency treebank; Chinese Buddhist canon; quotative verbs 1. study, we examine the verbs used by Buddha and other characters in the treebank, focusing on quotative verbs (Section 4). Introduction Over the past decade, there has been growing interest in building treebanks for historical texts, not only for facilitating their reading but also for studying the historical languages in which they were written. The sacred texts of many major religious, for example, have been syntactically analysed: treebanks are now available for the Hebrew Bible (Wu & Lowery, 2006), the New Testament in Greek (Haug & Jøhndal, 2008), and the Qur’an in Classical Arabic (Dukes & Buckwalter, 2010). 2. Previous Work Among large, diachronic corpora in ancient Chinese are the Academia Sinica Ancient Chinese Corpus (Wei et al., 1997) and the Sheffield Corpus of Chinese (Hu et al., 2005), both covering a wide range of time and genres. Although word-segmented and POS-tagged, they have not been syntactically analyzed. With about 50 million characters, the sheer volume of the Chinese Buddhist Canon makes it difficult for any individual to perform manual analysis over the entire cor"
L16-1265,W06-0127,0,\N,Missing
L18-1647,W13-1703,0,0.189639,"97 L1 sentences. We report the most overused and underused syntactic relations by the CFL learners, and discuss the underlying learner errors. Keywords: learner corpus, parallel treebank, Chinese as a foreign language 1. Introduction Learner corpora, which consist of texts written by nonnative speakers, are increasingly used in quantitative studies in second language acquisition. Some of these corpora have been annotated to answer various research questions. To support analysis of grammatical mistakes made by learners, a number of them have been error-tagged (e.g., Yannakoudakis et al., 2011; Dahlmeier et al., 2013; Lee et al., 2016). To better characterize learner syntax, others have been part-of-speech (POS) tagged (e.g., DíazNegrillo et al., 2010; Reznicek et al., 2013), and syntactically analysed with constituent trees (e.g., Nagata and Sakaguchi, 2016) and dependency trees (e.g., Ragheb and Dickinson, 2014; Berzak et al., 2016). Building on learner treebanks, Lee et al. (2017b) proposed to use “L1-L2 parallel treebanks” — parse trees of nonnative sentences (“L2 sentences”) aligned to their target hypotheses (“L1 sentences”) — to facilitate analyses of learner language. Figure 1 shows an example tre"
L18-1647,W17-0408,1,0.601513,"on. Some of these corpora have been annotated to answer various research questions. To support analysis of grammatical mistakes made by learners, a number of them have been error-tagged (e.g., Yannakoudakis et al., 2011; Dahlmeier et al., 2013; Lee et al., 2016). To better characterize learner syntax, others have been part-of-speech (POS) tagged (e.g., DíazNegrillo et al., 2010; Reznicek et al., 2013), and syntactically analysed with constituent trees (e.g., Nagata and Sakaguchi, 2016) and dependency trees (e.g., Ragheb and Dickinson, 2014; Berzak et al., 2016). Building on learner treebanks, Lee et al. (2017b) proposed to use “L1-L2 parallel treebanks” — parse trees of nonnative sentences (“L2 sentences”) aligned to their target hypotheses (“L1 sentences”) — to facilitate analyses of learner language. Figure 1 shows an example tree pair. It includes the parse tree of the learner sentence and of its target target hypothesis, both annotated in the Universal Dependencies (UD) scheme for Chinese (Leung et al., 2016; Lee et al., 2017), as well as word alignments between the two sentences. Such a treebank has the potential to enhance Contrastive Interlanguage Analysis (CIA) (Granger, 2015) and Error An"
L18-1647,W17-6306,1,0.790775,"on. Some of these corpora have been annotated to answer various research questions. To support analysis of grammatical mistakes made by learners, a number of them have been error-tagged (e.g., Yannakoudakis et al., 2011; Dahlmeier et al., 2013; Lee et al., 2016). To better characterize learner syntax, others have been part-of-speech (POS) tagged (e.g., DíazNegrillo et al., 2010; Reznicek et al., 2013), and syntactically analysed with constituent trees (e.g., Nagata and Sakaguchi, 2016) and dependency trees (e.g., Ragheb and Dickinson, 2014; Berzak et al., 2016). Building on learner treebanks, Lee et al. (2017b) proposed to use “L1-L2 parallel treebanks” — parse trees of nonnative sentences (“L2 sentences”) aligned to their target hypotheses (“L1 sentences”) — to facilitate analyses of learner language. Figure 1 shows an example tree pair. It includes the parse tree of the learner sentence and of its target target hypothesis, both annotated in the Universal Dependencies (UD) scheme for Chinese (Leung et al., 2016; Lee et al., 2017), as well as word alignments between the two sentences. Such a treebank has the potential to enhance Contrastive Interlanguage Analysis (CIA) (Granger, 2015) and Error An"
L18-1647,W16-5403,1,0.839211,"syntactically analysed with constituent trees (e.g., Nagata and Sakaguchi, 2016) and dependency trees (e.g., Ragheb and Dickinson, 2014; Berzak et al., 2016). Building on learner treebanks, Lee et al. (2017b) proposed to use “L1-L2 parallel treebanks” — parse trees of nonnative sentences (“L2 sentences”) aligned to their target hypotheses (“L1 sentences”) — to facilitate analyses of learner language. Figure 1 shows an example tree pair. It includes the parse tree of the learner sentence and of its target target hypothesis, both annotated in the Universal Dependencies (UD) scheme for Chinese (Leung et al., 2016; Lee et al., 2017), as well as word alignments between the two sentences. Such a treebank has the potential to enhance Contrastive Interlanguage Analysis (CIA) (Granger, 2015) and Error Analysis (EA) by supporting a greater range of automatic, quantitative studies. For CIA, they would enable comparisons between native and interlanguages not only on the lexical level but also on the syntactic level. For EA, parallel parse trees would give more fine-grained characterization of the syntactic environment in which learner errors occur. This paper reports on the construction of an L1-L2 parallel tr"
L18-1647,P11-1121,0,0.0325671,"r dependency treebanks for learner language are the Treebank of Learner English (TLE) (Berzak et al., 2016) and the project on Syntactically Annotating Learner Language of English (SALLE) (Ragheb and Dickinson, 2014). They both contain English texts written by non-native speakers. TLE annotates a subset of sentences from the Cambridge FCE corpus (Yannakoudakis et al., 2011), while SALLE has been applied on essays written by university students. A phrase-structure treebank for learner English (Nagata and Sakaguchi, 2016) has also been constructed for the texts in the Konan-JIEM Learner Corpus (Nagata et al., 2011). None of these treebanks, however, are L1-L2 parallel treebanks: they either do not provide explicit target 4106 hypotheses (Ragheb and Dickinson, 2014; Nagata and Sakaguchi, 2016), or have not yet provided parse trees for the target hypotheses (Berzak et al., 2016). involve rewriting the sentence as a whole. Even when performing the same kind of edits, individual annotators may come up with multiple valid target hypotheses. As interest grows in learning Chinese as a foreign language (CFL), a number of large CFL corpora have been compiled and annotated (e.g., Zhang, 2009; Wang et al., 2015; L"
L18-1647,P16-1173,0,0.126994,"corpora, which consist of texts written by nonnative speakers, are increasingly used in quantitative studies in second language acquisition. Some of these corpora have been annotated to answer various research questions. To support analysis of grammatical mistakes made by learners, a number of them have been error-tagged (e.g., Yannakoudakis et al., 2011; Dahlmeier et al., 2013; Lee et al., 2016). To better characterize learner syntax, others have been part-of-speech (POS) tagged (e.g., DíazNegrillo et al., 2010; Reznicek et al., 2013), and syntactically analysed with constituent trees (e.g., Nagata and Sakaguchi, 2016) and dependency trees (e.g., Ragheb and Dickinson, 2014; Berzak et al., 2016). Building on learner treebanks, Lee et al. (2017b) proposed to use “L1-L2 parallel treebanks” — parse trees of nonnative sentences (“L2 sentences”) aligned to their target hypotheses (“L1 sentences”) — to facilitate analyses of learner language. Figure 1 shows an example tree pair. It includes the parse tree of the learner sentence and of its target target hypothesis, both annotated in the Universal Dependencies (UD) scheme for Chinese (Leung et al., 2016; Lee et al., 2017), as well as word alignments between the two"
L18-1647,L16-1262,0,0.052287,"Missing"
L18-1647,Q16-1013,0,0.016536,"s, with diverse topics such as “The most memorable trip”, “Experience of cultural differences”, “My family members”, etc. Based on scanned copies of their essays, we manually transcribed them in digital format. So far, we have collected a total of 27 essays, which consist of 600 sentences. 3.2 Target hypotheses It is well known that there can be multiple target hypotheses for each learner sentence (Reznicek et al., 2013). One can focus on “minimal edits”, which aim to make the sentence grammatically correct with the shortest edit distance; one can also perform “fluency edits”, as advocated by Sakaguchi et al. (2016), which aim to make the sentence not only grammatical correct but also native-like, often 4. Dependency annotation Our learner corpus is annotated in the form of an L1-L2 parallel dependency treebank. The treebank consists of sentences written by CFL learners (“L2 sentences”); their target hypothesis (“L1 sentences”); the parse trees of the L1 and L2 sentences; and word alignment between the L1 sentences and L2 sentences. Figure 1 shows an example of a tree pair in our treebank. We performed manual word segmentation, POS tagging, and dependency annotations on all sentences. Both the L1 and L2"
L18-1647,W15-0614,0,0.0239798,"Nagata et al., 2011). None of these treebanks, however, are L1-L2 parallel treebanks: they either do not provide explicit target 4106 hypotheses (Ragheb and Dickinson, 2014; Nagata and Sakaguchi, 2016), or have not yet provided parse trees for the target hypotheses (Berzak et al., 2016). involve rewriting the sentence as a whole. Even when performing the same kind of edits, individual annotators may come up with multiple valid target hypotheses. As interest grows in learning Chinese as a foreign language (CFL), a number of large CFL corpora have been compiled and annotated (e.g., Zhang, 2009; Wang et al., 2015; Lee et al., 2016). Lee et al. (2017a) reported the first attempts to perform dependency annotation on CFL texts. For our treebank, a native speaker of Mandarin Chinese performed corrections on the 600 L2 sentences to produce a “minimal edit” target hypothesis for each sentence. An L2 sentence may be split up, or several L2 sentences may be combined. There are a total of 697 L1 sentences (target hypotheses) in the treebank. In future work, we plan to include “minimal edit” target hypotheses from other annotators, as well as “fluency edit” hypotheses, and study how multiple hypotheses affects"
L18-1647,P11-1019,0,0.136974,"ts of 600 L2 sentences and 697 L1 sentences. We report the most overused and underused syntactic relations by the CFL learners, and discuss the underlying learner errors. Keywords: learner corpus, parallel treebank, Chinese as a foreign language 1. Introduction Learner corpora, which consist of texts written by nonnative speakers, are increasingly used in quantitative studies in second language acquisition. Some of these corpora have been annotated to answer various research questions. To support analysis of grammatical mistakes made by learners, a number of them have been error-tagged (e.g., Yannakoudakis et al., 2011; Dahlmeier et al., 2013; Lee et al., 2016). To better characterize learner syntax, others have been part-of-speech (POS) tagged (e.g., DíazNegrillo et al., 2010; Reznicek et al., 2013), and syntactically analysed with constituent trees (e.g., Nagata and Sakaguchi, 2016) and dependency trees (e.g., Ragheb and Dickinson, 2014; Berzak et al., 2016). Building on learner treebanks, Lee et al. (2017b) proposed to use “L1-L2 parallel treebanks” — parse trees of nonnative sentences (“L2 sentences”) aligned to their target hypotheses (“L1 sentences”) — to facilitate analyses of learner language. Figur"
lee-haug-2010-porting,kingsbury-palmer-2002-treebank,0,\N,Missing
lee-haug-2010-porting,kinyon-prolo-2002-identifying,0,\N,Missing
lee-haug-2010-porting,miltsakaki-etal-2004-penn,0,\N,Missing
lee-haug-2010-porting,J93-2004,0,\N,Missing
lee-haug-2010-porting,W99-0502,0,\N,Missing
lee-haug-2010-porting,brants-2000-inter,0,\N,Missing
lee-haug-2010-porting,W03-2404,0,\N,Missing
lee-haug-2010-porting,J07-3004,0,\N,Missing
lee-haug-2010-porting,W07-0905,0,\N,Missing
N04-2006,P98-1085,0,0.459375,"Missing"
N04-2006,W00-0708,0,0.257705,"Missing"
N04-2006,J03-4003,0,\N,Missing
N04-2006,C98-1082,0,\N,Missing
N07-2024,P97-1003,0,0.0241485,"ence translations, of Chinese and Arabic newspaper articles. Ent Entropy3 from a trigram language model trained on 4.4 million English sentences with the SRILM toolkit (Stolcke, 2002). The trigrams are intended to detect local mistakes. JLE (Japanese Learners of English Corpus) Transcripts of Japanese examinees in the Standard Speaking Test. False starts and disfluencies were then cleaned up, and grammatical mistakes tagged (Izumi et al., 2003). The speaking style is more formal than spontaneous English, due to the examination setting. Parse Parse score from Model 2 of the statistical parser (Collins, 1997), normalized by the number of words. We hypothesize that nonnative sentences are more likely to receive lower scores. 3.2 Machine Learning Framework SVM-Light (Joachims, 1999), an implementation of Support Vector Machines (SVM), is used for the classification task. For the ranking task, we utilize the ranking mode of SVM-Light. In this mode, the SVM algorithm is adapted for learning ranking functions, originally used for ranking web pages with respect to a 1 Except spelling mistakes, which we consider to be a separate problem that should be dealt with in a pre-processing step. 2 The nature of"
N07-2024,P01-1020,0,0.0307522,"eir writing. Classifying a sentence into discrete categories can be difficult: a sentence that seems fluent to one judge might not be good enough to another. An alternative is to rank sentences by their relative fluency. This would be useful when a non-native speaker is unsure which one of several possible ways of writing a sentence is the best. Related Research Previous research has paid little attention to ranking sentences by fluency. As for classification, one line of research in MT evaluation is to evaluate the fluency of an output sentence without its reference translations, such as in (Corston-Oliver et al., 2001) and (Gamon et al., 2005). Our task here is similar, but is applied on non-native sentences, arguably more challenging than MT output. Evaluation of non-native writing has encompassed both the document and sentence levels. At the document level, automatic essay scorers, such as (Burstein et al., 2004) and (Ishioka and Kameda, 2006), can provide holistic scores that correlate well with those of human judges. At the sentence level, which is the focus of this paper, previous work follows two trends. Some researchers explicitly focus on individual classes of er93 Proceedings of NAACL HLT 2007, Com"
N07-2024,2005.eamt-1.15,0,0.0481903,"e into discrete categories can be difficult: a sentence that seems fluent to one judge might not be good enough to another. An alternative is to rank sentences by their relative fluency. This would be useful when a non-native speaker is unsure which one of several possible ways of writing a sentence is the best. Related Research Previous research has paid little attention to ranking sentences by fluency. As for classification, one line of research in MT evaluation is to evaluate the fluency of an output sentence without its reference translations, such as in (Corston-Oliver et al., 2001) and (Gamon et al., 2005). Our task here is similar, but is applied on non-native sentences, arguably more challenging than MT output. Evaluation of non-native writing has encompassed both the document and sentence levels. At the document level, automatic essay scorers, such as (Burstein et al., 2004) and (Ishioka and Kameda, 2006), can provide holistic scores that correlate well with those of human judges. At the sentence level, which is the focus of this paper, previous work follows two trends. Some researchers explicitly focus on individual classes of er93 Proceedings of NAACL HLT 2007, Companion Volume, pages 93–9"
N07-2024,P06-1030,0,0.0136139,"s the best. Related Research Previous research has paid little attention to ranking sentences by fluency. As for classification, one line of research in MT evaluation is to evaluate the fluency of an output sentence without its reference translations, such as in (Corston-Oliver et al., 2001) and (Gamon et al., 2005). Our task here is similar, but is applied on non-native sentences, arguably more challenging than MT output. Evaluation of non-native writing has encompassed both the document and sentence levels. At the document level, automatic essay scorers, such as (Burstein et al., 2004) and (Ishioka and Kameda, 2006), can provide holistic scores that correlate well with those of human judges. At the sentence level, which is the focus of this paper, previous work follows two trends. Some researchers explicitly focus on individual classes of er93 Proceedings of NAACL HLT 2007, Companion Volume, pages 93–96, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics rors, e.g., mass vs count nouns in (Brockett et al., 2006) and (Nagata et al., 2006). Others implicitly do so with hand-crafted rules, via templates (Heidorn, 2000) or mal-rules in context-free grammars, such as (Michaud et al.,"
N07-2024,P06-1031,0,0.0208375,"has encompassed both the document and sentence levels. At the document level, automatic essay scorers, such as (Burstein et al., 2004) and (Ishioka and Kameda, 2006), can provide holistic scores that correlate well with those of human judges. At the sentence level, which is the focus of this paper, previous work follows two trends. Some researchers explicitly focus on individual classes of er93 Proceedings of NAACL HLT 2007, Companion Volume, pages 93–96, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics rors, e.g., mass vs count nouns in (Brockett et al., 2006) and (Nagata et al., 2006). Others implicitly do so with hand-crafted rules, via templates (Heidorn, 2000) or mal-rules in context-free grammars, such as (Michaud et al., 2000) and (Bender et al., 2004). Typically, however, non-native writing exhibits a wide variety of errors, in grammar, style and word collocations. In this research, we allow unrestricted classes of errors1 , and in this regard our goal is closest to that of (Tomokiyo and Jones, 2001). However, they focus on non-native speech, and assume the availability of non-native training data. 3 query (Joachims, 2002). In our context, given a set of English sent"
N07-2024,N01-1031,0,0.0319142,"07, Companion Volume, pages 93–96, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics rors, e.g., mass vs count nouns in (Brockett et al., 2006) and (Nagata et al., 2006). Others implicitly do so with hand-crafted rules, via templates (Heidorn, 2000) or mal-rules in context-free grammars, such as (Michaud et al., 2000) and (Bender et al., 2004). Typically, however, non-native writing exhibits a wide variety of errors, in grammar, style and word collocations. In this research, we allow unrestricted classes of errors1 , and in this regard our goal is closest to that of (Tomokiyo and Jones, 2001). However, they focus on non-native speech, and assume the availability of non-native training data. 3 query (Joachims, 2002). In our context, given a set of English sentences with similar semantic content, say s1 , . . . , sn , and a ranking based on their fluency, the learning algorithm estimates the weights w ~ to satisfy the inequalities: w ~ · Φ(sj ) &gt; w ~ · Φ(sk ) where sj is more fluent than sk , and where Φ maps a sentence to a feature vector. This is in contrast to standard SVMs, which learn a hyperplane boundary between native and non-native sentences from the inequalities: Experimen"
N07-2024,P03-2026,0,\N,Missing
N07-2024,P06-1032,0,\N,Missing
N12-1020,W09-2307,0,0.601645,"pendency Treebank 191 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 191–199, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics (Hajic, 1998). The most widely used treebank for Modern Chinese, the Penn Chinese Treebank (Xue et al., 2005), belongs to the former kind. Rather than encoding constituency information, dependency grammars give information about grammatical relations between words. Modern Chinese has been analyzed in this framework, for example at Stanford University (Chang et al., 2009). The dependency relations follow the design principles of those initially applied to English (de Marneffe and Manning, 2008), with a few added relations to accommodate Chinese-specific features, such as the “ba”-construction. Their POS tagset is borrowed from that of the Penn Chinese Treebank. 2.2 ern Chinese, our principle is to borrow from existing annotation framework as much as possible. For example, our POS tagset is based on that of the Penn Chinese Treebank, after a slight revision of its 33 tags (Lee, 2012). This approach not only gives users a familiar point of reference, and also ma"
N12-1020,W02-1806,0,0.633043,"ing a Classical Chinese treebank are word segmentation and part-of-speech tagging. In this section, we first summarize existing POS tagging frameworks, then describe the only current treebank of Classical Chinese. Word boundaries and parts-of-speech tags have been added to the Academia Sinica Ancient Chinese Corpus (Wei et al., 1997) and the Sheffield Corpus of Chinese (Hu et al., 2005). Since there is not yet a scholarly consensus on word segmentation in Chinese (Feng 1998), it is not surprising that there are wide-ranging levels of granularity of the POS tagsets. They range from 21 tags in (Huang et al., 2002), 26 in the Peking University corpus (Yu et al., 2002), 46 in the Academia Sinica Balanced Corpus (Chen et al., 1996), to 111 in the Sheffield Corpus of Chinese (Hu et al., 2005). This treebank uses a system of nested POS tags (Lee, 2012), which accommodates different policies for word segmentation and maximize interoperability between corpora. The only previous syntactic treebank for Classical Chinese is a constituent-based one (Huang et al., 2002), composed of 1000 sentences from preTsin Classical Chinese. No word segmentation was performed for this treebank. 3 Treebank design Although Class"
N12-1020,C08-1048,0,0.0238843,"enjoys perhaps the most elevated status in the Classical Chinese tradition. The Tang Dynasty is considered the golden age of shi, one of the five subgenres of Chinese poetry7. The Complete Shi Poetry of the Tang (Peng, 1960), originally compiled in 1705, consists of nearly 50,000 poems by more than two thousand poets. This book is treasured by scholars and the public alike. Even today, Chinese people informally compose couplets (see section 5), in the style of shi poetry, to celebrate special occasions such as birthdays. Indeed, NLP techniques have been applied to generate them automatically (Jiang and Zhou, 2008). 4.1 Material This treebank contains the complete works, a total of over 32,000 characters in 521 poems, by two Chinese poets in the 8th century CE, Wang Wei and Meng Haoran. Wang, also known as the PoetBuddha (shifo 詩佛), is considered one of the three most prominent Tang poets. Meng is often asso5 From Meng Haoran 《與張折衝遊耆闍寺》 From top to bottom, Meng Haoran 《登龍興寺閣》,《歲暮 歸南山》, and Du Fu 杜甫《八陣圖》 6 195 7 The other four genres are ci, fu, qu, and sao. ciated with Wang due to the similarity of his poems in style and content. Aside from the dependency relations, word boundaries and POS tags, the tre"
N12-1020,W12-1011,1,0.398243,"been analyzed in this framework, for example at Stanford University (Chang et al., 2009). The dependency relations follow the design principles of those initially applied to English (de Marneffe and Manning, 2008), with a few added relations to accommodate Chinese-specific features, such as the “ba”-construction. Their POS tagset is borrowed from that of the Penn Chinese Treebank. 2.2 ern Chinese, our principle is to borrow from existing annotation framework as much as possible. For example, our POS tagset is based on that of the Penn Chinese Treebank, after a slight revision of its 33 tags (Lee, 2012). This approach not only gives users a familiar point of reference, and also makes the treebank interoperable with existing Modern Chinese resources. Interoperability allows the potential of bootstrapping with Modern Chinese data, as well as contrastive studies for the two languages. Classical Chinese Like its modern counterpart, two pre-requisites for constructing a Classical Chinese treebank are word segmentation and part-of-speech tagging. In this section, we first summarize existing POS tagging frameworks, then describe the only current treebank of Classical Chinese. Word boundaries and pa"
N12-1020,W08-1301,0,\N,Missing
N12-1020,prasad-etal-2008-penn,0,\N,Missing
P06-2113,W02-1001,0,0.0204116,"mption in Eq. (2), these expectations can be computed using a forward-backward-like dynamic programming algorithm. For CRFs, whose features do not depend on hidden state sequences, the first expectation is simply the feature counts given the observation and label sequences. In this work, we applied stochastic gradient descent (SGD) (Kushner and Yin, 1997) for parameter optimization. In our experiments on several different tasks, it is faster than LBFGS (Nocedal and Wright, 1999), a quasiNewton optimization algorithm. 3.3 CRFs and Perceptron Learning Perceptron training for conditional models (Collins, 2002) is an approximation to the SGD algorithm, using feature counts from the Viterbi label sequence in lieu of expected feature counts. It eliminates the need of a forward-backward algorithm to collect the expected counts, hence greatly speeds up model training. This algorithm can be viewed as using the minimum margin of a training example (i.e., the difference in the log conditional probability of the reference label sequence and the Viterbi label sequence) as the objective function instead of the conditional probability: Here again, o is the observation and l is its reference label sequence. In"
P06-2113,H90-1020,0,\N,Missing
P07-1011,P04-1022,1,0.832643,"Missing"
P07-1011,W05-0904,0,0.0123293,"e ratio of the number of erroneous LCs to the number of collocations in each sentence. Perplexity from Language Model (PLM) Perplexity measures are extracted from a trigram language model trained on a general English corpus using the SRILM-SRI Language Modeling Toolkit (Stolcke, 2002). We calculate two values for each sentence: lexicalized trigram perplexity and part of speech (POS) trigram perplexity. The erroneous sentences would have higher perplexity. Syntactic Score (SC) Some erroneous sentences often contain words and concepts that are locally correct but cannot form coherent sentences (Liu and Gildea, 2005). To measure the coherence of sentences, we use a statistical parser Toolkit (Collins, 1997) to assign each sentence a parser’s score that is the related log probability of parsing. We assume that erroneous sentences with undesirable sentence structures are more likely to receive lower scores. Function Word Density (FWD) We consider the density of function words (Corston-Oliver et al., 2001), i.e. the ratio of function words to content words. This is inspired by the work (Corston-Oliver et al., 2001) showing that function word density can be effective in distinguishing between human references"
P07-1011,P06-1032,0,0.446282,"high quality; Also, it is difficult to produce and maintain a large number of non-conflicting rules to cover a wide range of grammatical errors. Moreover, ESL writers of different first-language backgrounds and skill levels may make different errors, and thus different sets of rules may be required. Worse still, it is hard to write rules for some grammatical errors, for example, detecting errors concerning the articles and singular plural usage (Nagata et al., 2006). Instead of asking experts to write hand-crafted rules, statistical approaches (Chodorow and Leacock, 2000; Izumi et al., 2003; Brockett et al., 2006; Nagata et al., 2006) build statistical models to identify sentences containing errors. However, existing 81 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 81–88, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics statistical approaches focus on some pre-defined errors and the reported results are not attractive. Moreover, these approaches, e.g., (Izumi et al., 2003; Brockett et al., 2006) usually need errors to be specified and tagged in the training sentences, which requires expert help to be recruited and is ti"
P07-1011,P06-1031,0,0.0698495,"al errors in the writing of English learners. However, it could be expensive to write rules manually. Linguistic experts are needed to write rules of high quality; Also, it is difficult to produce and maintain a large number of non-conflicting rules to cover a wide range of grammatical errors. Moreover, ESL writers of different first-language backgrounds and skill levels may make different errors, and thus different sets of rules may be required. Worse still, it is hard to write rules for some grammatical errors, for example, detecting errors concerning the articles and singular plural usage (Nagata et al., 2006). Instead of asking experts to write hand-crafted rules, statistical approaches (Chodorow and Leacock, 2000; Izumi et al., 2003; Brockett et al., 2006; Nagata et al., 2006) build statistical models to identify sentences containing errors. However, existing 81 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 81–88, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics statistical approaches focus on some pre-defined errors and the reported results are not attractive. Moreover, these approaches, e.g., (Izumi et al., 2003;"
P07-1011,J93-2003,0,0.00558954,"pose a new approach to detecting erroneous sentences by integrating pattern discovery with supervised learning models. Experimental results show that our techniques are promising. 1 Gao Cong Introduction Detecting erroneous/correct sentences has the following applications. First, it can provide feedback for writers of English as a Second Language (ESL) as to whether a sentence contains errors. Second, it can be applied to control the quality of parallel bilingual sentences mined from the Web, which are critical sources for a wide range of applications, such as statistical machine translation (Brown et al., 1993) and cross-lingual information retrieval (Nie et al., 1999). Third, it can be used to evaluate machine translation results. As demonstrated in (CorstonOliver et al., 2001; Gamon et al., 2005), the better human reference translations can be distinguished from machine translations by a classification model, the worse the machine translation system is. ∗ Work done while the author was a visiting student at MSRA † Work done while the author was a visiting student at MSRA The previous work on identifying erroneous sentences mainly aims to find errors from the writing of ESL learners. The common mis"
P07-1011,P98-1032,0,0.105684,"oyed to identify and correct writing errors (Brockett et al., 2006). This method must collect a large number of parallel corpora (pairs of erroneous sentences and their corrections) and performance depends on SMT techniques that are not yet mature. The work in (Nagata et al., 2006) focuses on a type of error, namely mass vs. count nouns. In contrast to existing statistical methods, our technique needs neither errors tagged nor parallel corpora, and is not limited to a specific type of grammatical error. There are also studies on automatic essay scoring at document-level. For example, E-rater (Burstein et al., 1998), developed by the ETS, and Intelligent Essay Assessor (Foltz et al., 1999). The evaluation criteria for documents are different from those for sentences. A document is evaluated mainly by its organization, topic, diversity of vocabulary, and grammar while a sentence is done by grammar, sentence structure, and lexical choice. Another related work is Machine Translation (MT) evaluation. Classification models are employed in (Corston-Oliver et al., 2001; Gamon et al., 2005) to evaluate the well-formedness of machine translation outputs. The writers of ESL and MT normally make different mistakes:"
P07-1011,A00-2019,0,0.804432,"Linguistic experts are needed to write rules of high quality; Also, it is difficult to produce and maintain a large number of non-conflicting rules to cover a wide range of grammatical errors. Moreover, ESL writers of different first-language backgrounds and skill levels may make different errors, and thus different sets of rules may be required. Worse still, it is hard to write rules for some grammatical errors, for example, detecting errors concerning the articles and singular plural usage (Nagata et al., 2006). Instead of asking experts to write hand-crafted rules, statistical approaches (Chodorow and Leacock, 2000; Izumi et al., 2003; Brockett et al., 2006; Nagata et al., 2006) build statistical models to identify sentences containing errors. However, existing 81 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 81–88, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics statistical approaches focus on some pre-defined errors and the reported results are not attractive. Moreover, these approaches, e.g., (Izumi et al., 2003; Brockett et al., 2006) usually need errors to be specified and tagged in the training sentences, which req"
P07-1011,P97-1003,0,0.0417279,"rom Language Model (PLM) Perplexity measures are extracted from a trigram language model trained on a general English corpus using the SRILM-SRI Language Modeling Toolkit (Stolcke, 2002). We calculate two values for each sentence: lexicalized trigram perplexity and part of speech (POS) trigram perplexity. The erroneous sentences would have higher perplexity. Syntactic Score (SC) Some erroneous sentences often contain words and concepts that are locally correct but cannot form coherent sentences (Liu and Gildea, 2005). To measure the coherence of sentences, we use a statistical parser Toolkit (Collins, 1997) to assign each sentence a parser’s score that is the related log probability of parsing. We assume that erroneous sentences with undesirable sentence structures are more likely to receive lower scores. Function Word Density (FWD) We consider the density of function words (Corston-Oliver et al., 2001), i.e. the ratio of function words to content words. This is inspired by the work (Corston-Oliver et al., 2001) showing that function word density can be effective in distinguishing between human references and machine outputs. In this paper, we calculate the densities of seven kinds of function w"
P07-1011,P01-1020,0,0.163624,"nd is not limited to a specific type of grammatical error. There are also studies on automatic essay scoring at document-level. For example, E-rater (Burstein et al., 1998), developed by the ETS, and Intelligent Essay Assessor (Foltz et al., 1999). The evaluation criteria for documents are different from those for sentences. A document is evaluated mainly by its organization, topic, diversity of vocabulary, and grammar while a sentence is done by grammar, sentence structure, and lexical choice. Another related work is Machine Translation (MT) evaluation. Classification models are employed in (Corston-Oliver et al., 2001; Gamon et al., 2005) to evaluate the well-formedness of machine translation outputs. The writers of ESL and MT normally make different mistakes: in general, ESL writers can write overall grammatically correct sentences with some local mistakes while MT outputs normally produce locally well-formed phrases with overall grammatically wrong sentences. Hence, the manual features designed for MT evaluation are not applicable to detect erroneous sentences from ESL learners. LSPs differ from the traditional sequential patterns, e.g., (Agrawal and Srikant, 1995; Pei et al., 2001) in that LSPs are atta"
P07-1011,2005.eamt-1.15,0,0.196679,"Introduction Detecting erroneous/correct sentences has the following applications. First, it can provide feedback for writers of English as a Second Language (ESL) as to whether a sentence contains errors. Second, it can be applied to control the quality of parallel bilingual sentences mined from the Web, which are critical sources for a wide range of applications, such as statistical machine translation (Brown et al., 1993) and cross-lingual information retrieval (Nie et al., 1999). Third, it can be used to evaluate machine translation results. As demonstrated in (CorstonOliver et al., 2001; Gamon et al., 2005), the better human reference translations can be distinguished from machine translations by a classification model, the worse the machine translation system is. ∗ Work done while the author was a visiting student at MSRA † Work done while the author was a visiting student at MSRA The previous work on identifying erroneous sentences mainly aims to find errors from the writing of ESL learners. The common mistakes (Yukio et al., 2001; Gui and Yang, 2003) made by ESL learners include spelling, lexical collocation, sentence structure, tense, agreement, verb formation, wrong PartOf-Speech (POS), art"
P07-1011,H05-1006,0,0.137165,"use of hand-crafted rules, e.g., template rules (Heidorn, 2000) and mal-rules in context-free grammars (Michaud et al., 2000; Bender et al., 2004). As discussed in Section 1, manual rule based methods have some shortcomings. The second category uses statistical techniques to detect erroneous sentences. An unsupervised method (Chodorow and Leacock, 2000) is employed to detect grammatical errors by inferring negative evidence from TOEFL administrated by ETS. The method (Izumi et al., 2003) aims to detect omission-type and replacement-type errors and transformation-based leaning is employed in (Shi and Zhou, 2005) to learn rules to detect errors for speech recognition outputs. They also require specifying error tags that can tell the specific errors and their corrections in the training corpus. The phrasal Statistical Machine Translation (SMT) technique is employed to identify and correct writing errors (Brockett et al., 2006). This method must collect a large number of parallel corpora (pairs of erroneous sentences and their corrections) and performance depends on SMT techniques that are not yet mature. The work in (Nagata et al., 2006) focuses on a type of error, namely mass vs. count nouns. In contr"
P07-1011,C98-1032,0,\N,Missing
P07-1011,P03-2026,0,\N,Missing
P07-1060,P02-1020,0,0.556199,"computational model of text reuse tailored for ancient literary texts, available to us often only in small and noisy samples. The model takes into account source alternation patterns, so as to be able to align even sentences with low surface similarity. We demonstrate its ability to characterize text reuse in the Greek New Testament. 1 Introduction Text reuse is the transformation of a source text into a target text in order to serve a different purpose. Past research has addressed a variety of text-reuse applications, including: journalists turning a news agency text into a newspaper story (Clough et al., 2002); editors adapting an encyclopedia entry to an abridged version (Barzilay and Elhadad, 2003); and plagiarizers disguising their sources by removing surface similarities (Uzuner et al., 2005). A common assumption in the recovery of text reuse is the conservation of some degree of lexical similarity from the source sentence to the derived sentence. A simple approach, then, is to define a lexical similarity measure and estimate a score threshold; given a sentence in the target text, if the highest-scoring sentence in the source text is above the threshold, then the former is considered to be deri"
P07-1060,W02-1001,0,0.0169099,"al data, but our model does not depend on its validity. 2 We do not consider the Gospel of Matthew or Q in this study. Verses from Luke 9:51 to the end of chapter 21 are also not considered, since their sources are difficult to ascertain (Bovon, 2002). 3 Obtained through Peter Ballard (personal communication) 4 Approach target verse and a candidate source verse, then: For each verse in the target text (a “target verse”), we would like to determine whether it is derived from a verse in the source text (a “source verse”) and, if so, which one. Following the framework of global linear models in (Collins, 2002), we cast this task as learning a mapping F from input verses x ∈ X to a text-reuse hypothesis y ∈ Y ∪ {}. X is the set of verses in the target text. In our case, xtrain = (x1 , . . . , x458 ) is the sequence of verses in Ltrain , and xtest is that of Ltest . Y is the set of verses in the source text. Say the sequence y = (y1 , . . . , yn ) is the text-reuse hypothesis for x = (x1 , . . . , xn ). If yi is , then xi is not derived from the source text; otherwise, yi is the source verse for xi . The set of candidates GEN(x) contains all possible sequences for y, and Θ is the parameter vector."
P07-1060,W99-0625,0,0.0275806,"nt text-reuse hypotheses: The B model is on the hypothesis in (Bovon, 2002), and the J model, on (Jeremias, 1966). These two models then predict the text-reuse in Ltest . 8 9 10 11 Luke Hypothesis Ltrain.B Ltrain.J Ltest.B Ltest.J 12 13 14 15 16 17 18 19 20 21 22 3 23 24 1 23 4 5 6 7 8 9 10 1112 1314 Mark 15 16 Figure 1: A dot-plot of the cosine similarity measure between the Gospel of Luke and the Gospel of Mark. The number on the axes represent chapters. The thick diagonal lines reflect regions of high lexical similarity between the two gospels. At the level of short passages or sentences, (Hatzivassiloglou et al., 1999) goes beyond N -gram, taking advantage of WordNet synonyms, as well as ordering and distance between shared words. (Barzilay and Elhadad, 2003) shows that the simple cosine similarity score can be effective when used in conjunction with paragraph clustering. A more detailed comparison with this work follows in §4.2. In the humanities, reused material in the writings of Plutarch (Helmbold and O’Neil, 1959) and Clement (van den Hoek, 1996) have been manually classified as quotations, reminiscences, references or paraphrases. Studies on the Synoptics have been limited to N -gram overlap, notably"
P07-1060,W03-1004,0,0.338128,"us often only in small and noisy samples. The model takes into account source alternation patterns, so as to be able to align even sentences with low surface similarity. We demonstrate its ability to characterize text reuse in the Greek New Testament. 1 Introduction Text reuse is the transformation of a source text into a target text in order to serve a different purpose. Past research has addressed a variety of text-reuse applications, including: journalists turning a news agency text into a newspaper story (Clough et al., 2002); editors adapting an encyclopedia entry to an abridged version (Barzilay and Elhadad, 2003); and plagiarizers disguising their sources by removing surface similarities (Uzuner et al., 2005). A common assumption in the recovery of text reuse is the conservation of some degree of lexical similarity from the source sentence to the derived sentence. A simple approach, then, is to define a lexical similarity measure and estimate a score threshold; given a sentence in the target text, if the highest-scoring sentence in the source text is above the threshold, then the former is considered to be derived from the latter. Obviously, the effectiveness of this basic approach depends on the degr"
P07-1060,W05-0207,0,0.0127675,"to be able to align even sentences with low surface similarity. We demonstrate its ability to characterize text reuse in the Greek New Testament. 1 Introduction Text reuse is the transformation of a source text into a target text in order to serve a different purpose. Past research has addressed a variety of text-reuse applications, including: journalists turning a news agency text into a newspaper story (Clough et al., 2002); editors adapting an encyclopedia entry to an abridged version (Barzilay and Elhadad, 2003); and plagiarizers disguising their sources by removing surface similarities (Uzuner et al., 2005). A common assumption in the recovery of text reuse is the conservation of some degree of lexical similarity from the source sentence to the derived sentence. A simple approach, then, is to define a lexical similarity measure and estimate a score threshold; given a sentence in the target text, if the highest-scoring sentence in the source text is above the threshold, then the former is considered to be derived from the latter. Obviously, the effectiveness of this basic approach depends on the degree of lexical similarity: source sentences that are quoted verbatim are easier to identify than th"
P08-1021,W07-1604,0,0.189277,"ed to verbs are among the most frequent categories. Table 2 shows some sentences with these errors. A system that automatically detects and corrects misused verb forms would be both an educational and practical tool for students of English. It may also potentially improve the performance of machine translation and natural language generation systems, especially when the source and target languages employ very different verb systems. Research on automatic grammar correction has been conducted on a number of different parts-ofspeech, such as articles (Knight and Chander, 1994) and prepositions (Chodorow et al., 2007). Errors in verb forms have been covered as part of larger systems such as (Heidorn, 2000), but we believe that their specific research challenges warrant more detailed examination. We build on the basic approach of templatematching on parse trees in two ways. To improve recall, irregularities in parse trees caused by verb form errors are considered; to improve precision, n-gram counts are utilized to filter proposed corrections. We start with a discussion on the scope of our 174 Proceedings of ACL-08: HLT, pages 174–182, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Lin"
P08-1021,P97-1003,0,0.0433905,"ities are introduced by the other primary verbs2 . The verb “have” can function as an auxiliary in the perfect aspect (EDperf ) as well as a main verb. The versatile “do” can serve as “do”support or add emphasis (BASEdo ), or simply act as a main verb. 3.2 Automatic Parsing The ambiguities discussed above may be expected to cause degradation in automatic parsing performance. In other words, sentences containing verb form errors are more likely to yield an “incorrect” parse tree, sometimes with significant differences. For example, the sentence “My father is *work in the laboratory” is parsed (Collins, 1997) as: This is work not play. (main verb) (S (NP My father) (VP is (NP work)) (PP in the laboratory)) My father is working in the lab. (INGprog ) A solution is worked out. (EDpass ) These different roles clearly affect the forms required for the verbs (if any) that follow. Dis176 2 The abbreviations ’s (is or has) and ’d (would or had) compound the ambiguities. The progressive form “working” is substituted with its bare form, which happens to be also a noun. The parser, not unreasonably, identifies “work” as a noun. Correcting the verb form error in this sentence, then, necessitates considering"
P08-1021,A97-1004,0,0.03598,"Implications on our evaluation procedure are discussed in §5.4. 5.3 Evaluation Metric For each verb in the input sentence, a change in verb form may be hypothesized. There are five possible outcomes for this hypothesis, as enumerated in Table 4. To penalize “false alarms”, a strict definition is used for false positives — even when the hypothesized correction yields a good sentence, it is still considered a false positive so long as the original sentence is acceptable. It can sometimes be difficult to determine which words should be considered verbs, as they are not 4 Obtained by segmenting (Reynar and Ratnaparkhi, 1997) the interviewee turns, and discarding sentences with only one word. The HKUST corpus was processed likewise. 5 Specifically, those tagged with the “v fml”, “v fin” (covering auxiliary agreement and complementation) and “v agr” (subject-verb agreement) types; those with semantic errors (see §2.1), i.e. “v tns” (tense), are excluded. 6 Provided by Prof. John Milton, personal communication. 178 clearly demarcated in our evaluation corpora. We will thus apply the outcomes in Table 4 at the sentence level; that is, the output sentence is considered a true positive only if the original sentence con"
P08-1021,P03-2026,0,\N,Missing
P11-1089,P08-1043,0,0.427248,"amantis lovers omnis all Figure 1: Dependency tree for the sentence “Una dies omnis potuit praecurrere amantis”. The word omnis is an adjective modifying the noun amantis. This information is key to the morphological disambiguation of both words, as shown in Table 1. such as Turkish (Hakkani-T¨ur et al., 2000) and Czech (Hajiˇc et al., 2001), did not consider syntactic relationships between words. In the literature on data-driven parsing, two recent studies attempted joint inference on morphology and syntax, and both considered phrase-structure trees for Modern Hebrew (Cohen and Smith, 2007; Goldberg and Tsarfaty, 2008). The primary focus of morphological processing in Modern Hebrew is splitting orthographic words into morphemes: clitics such as prepositions, pronouns, and the definite article must be separated from the core word. Each of the resulting morphemes is then tagged with an atomic “part-of-speech” to indicate word class and some morphological features. Similarly, the English POS tags in the Penn Treebank combine word class information with morphological attributes such as “plural” or “past tense”. Cohen and Smith (2007) separately train a discriminative conditional random field (CRF) for segmentat"
P11-1089,P96-1024,0,0.0375614,"number of belief propagation iterations and the number of training rounds, were tuned on the development set. 5.3 Decoding The output of the joint model is the assignment to the TAG and L INK variables. Loopy belief propagation (BP) was used to calculate the posterior probabilities of these variables. For TAG, we emit the tag with the highest posterior probability as computed by sum-product BP. We produced head attachments by first calculating the posteriors of the L INK variables with BP and then passing them to an edgefactored tree decoder. This is equivalent to minimum Bayes risk decoding (Goodman, 1996), which is used by Cohen and Smith (2007) and Smith and Eisner (2008). This MBR decoding procedure enforces the hard constraint that the output be a tree but sums over possible morphological assignments.5 5.4 Reducing Model Complexity In principle, the joint model should consider every possible combination of morphological attributes for every word. In practice, to reduce the complexity of the model, we used a pre-existing morphological database, M ORPHEUS (Crane, 1991), to constrain the range of possible values of the attributes listed in Table 2; more precisely, we add a hard constraint, req"
P11-1089,C00-1042,0,0.21484,"Missing"
P11-1089,H05-1066,0,0.0604475,"Missing"
P11-1089,P05-1012,0,0.660921,"d to a TAG variable. The factor fires when Ta,i,v is true. The features consist of the value v of the morphological attribute concerned, combined with the word identity of wi , with backoff using all suffixes of the word. The CASEUNIGRAM factors shown in Figure 2 are examples of this family of factors. • TAG -B IGRAM: There are O(nm2 ) of such binary factors, each connected to the TAG variables of a pair of neighboring words. The factor fires when Ta,i,v1 and Ta,i+1,v2 are both true. The CASE-BIGRAM factors shown in Figure 2 are examples of this family of factors. Beyond these basic features, McDonald et al. (2005) also utilize POS trigrams and POS 4grams. Both include the POS of two linked words, wi and wj . The third component in the trigrams is the POS of each word wk located between wi and wj , i < k < j. The two additional components that make up the 4-grams are subsets of the POS of words located to the immediate left and right of wi and wj . • TAG -C ONSISTENCY: For each word, the TAG variables representing the possible POS values are connected to those representing the values of other morphological attributes, yielding O(nm2 ) binary factors. They fire when Tpos,i,v1 and Ta,i,v2 are both true. T"
P11-1089,H05-1060,1,0.188147,"Missing"
P11-1089,D08-1016,1,0.756257,"8000 sentences from the Ancient Greek Dependency Treebank (Bamman et al., 2009), 5800 from the Hungarian Szeged Dependency Treebank (Vincze et al., 2010), and a subset of 3100 from the Prague Dependency Treebank (B¨ohmov´a et al., 2003). 890 Training We define each factor in (1) as a log-linear function: Fk (A) = exp X θh fh (A, W, k) (2) h Given an assignment A and words W , fh is an indicator function describing the presence or absence of the feature, and θh is the corresponding set of weights learned using stochastic gradient ascent, with the gradients inferred by loopy belief propagation (Smith and Eisner, 2008). The variance of the Gaussian prior is set to 1. The other two parameters in the training process, the number of belief propagation iterations and the number of training rounds, were tuned on the development set. 5.3 Decoding The output of the joint model is the assignment to the TAG and L INK variables. Loopy belief propagation (BP) was used to calculate the posterior probabilities of these variables. For TAG, we emit the tag with the highest posterior probability as computed by sum-product BP. We produced head attachments by first calculating the posteriors of the L INK variables with BP an"
P11-1089,N03-1033,0,0.0806641,"Missing"
P11-1089,P06-3009,0,0.112221,"Missing"
P11-1089,D07-1022,0,\N,Missing
P11-1089,W06-2920,0,\N,Missing
P11-1089,P01-1035,0,\N,Missing
P11-1089,vincze-etal-2010-hungarian,0,\N,Missing
P11-1089,D07-1096,0,\N,Missing
P12-2049,P11-1092,0,0.15172,"troduction Learner corpora have been playing an increasingly important role in both Second Language Acquisition and Foreign Language Teaching research (Granger, 2004; Nesi et al., 2004). These corpora contain texts written by non-native speakers of the language (Granger et al., 2009); many also annotate text segments where there are errors, and the corresponding error categories (Nagata et al., 2011). In addition, some learner corpora contain pairs of sentences: a sentence written by a learner of English as a second language (ESL), paired with its correct version produced by a native speaker (Dahlmeier and Ng, 2011). These datasets are intended to support the training of automatic text correction systems (Dale and Kilgarriff, 2011). Less attention has been paid to how a language learner produces a text. Writing is often an iterative and interactive process, with cycles of textual revision, guided by comments from language teachers. # drafts 988 410 2310 705 1754 466 118 1532 651 31 2165 1278 912 13320 Table 1: Draft essays are collected from courses in various disciplines at City University of Hong Kong. These drafts include lab reports, data analysis, argumentative essays, and article summaries. There a"
P12-2049,W11-2838,0,0.0602013,"Foreign Language Teaching research (Granger, 2004; Nesi et al., 2004). These corpora contain texts written by non-native speakers of the language (Granger et al., 2009); many also annotate text segments where there are errors, and the corresponding error categories (Nagata et al., 2011). In addition, some learner corpora contain pairs of sentences: a sentence written by a learner of English as a second language (ESL), paired with its correct version produced by a native speaker (Dahlmeier and Ng, 2011). These datasets are intended to support the training of automatic text correction systems (Dale and Kilgarriff, 2011). Less attention has been paid to how a language learner produces a text. Writing is often an iterative and interactive process, with cycles of textual revision, guided by comments from language teachers. # drafts 988 410 2310 705 1754 466 118 1532 651 31 2165 1278 912 13320 Table 1: Draft essays are collected from courses in various disciplines at City University of Hong Kong. These drafts include lab reports, data analysis, argumentative essays, and article summaries. There are 3760 distinct essays, most of which consist of two to four successive drafts. Each draft has on average 44.2 senten"
P12-2049,ide-etal-2000-xces,0,0.125546,"Missing"
P12-2049,I11-1017,0,0.0428649,"gescale empirical study, however, on the effectiveness of feedback at the paragraph or discourse levels. 2.2 chanical, word-level errors, or also substantially reorganize paragraph or essay structures — has hardly been investigated. One reason for this gap in the literature is the lack of corpus data: none of the existing learner corpora (Izumi et al., 2004; Granger et al., 2009; Nagata et al., 2011; Dahlmeier and Ng, 2011) contains drafts written by non-native speakers that led to the “final version”. Recently, two corpora with text revision information have been compiled (Xue and Hwa, 2010; Mizumoto et al., 2011), but neither contain feedback from language teachers. Our corpus will allow researchers to not only examine the revision process, but also investigate any correlation with the amount and type of feedback. 3 Corpus Description We first introduce the context in which our data was collected (§3.1), then describe the kinds of comments in the drafts (§3.2). We then outline the conversion process of the corpus into XML format (§3.3), followed by an evaluation (§3.4) and an analysis (§3.5). 3.1 Revision Process While text editing in general has been analyzed (Mahlow and Piotrowski, 2008), the nature"
P12-2049,P11-1121,0,0.158551,"upport research on textual revision by language learners, and how it is influenced by feedback. This corpus has been converted into an XML format conforming to the standards of the Text Encoding Initiative (TEI). 1 Introduction Learner corpora have been playing an increasingly important role in both Second Language Acquisition and Foreign Language Teaching research (Granger, 2004; Nesi et al., 2004). These corpora contain texts written by non-native speakers of the language (Granger et al., 2009); many also annotate text segments where there are errors, and the corresponding error categories (Nagata et al., 2011). In addition, some learner corpora contain pairs of sentences: a sentence written by a learner of English as a second language (ESL), paired with its correct version produced by a native speaker (Dahlmeier and Ng, 2011). These datasets are intended to support the training of automatic text correction systems (Dale and Kilgarriff, 2011). Less attention has been paid to how a language learner produces a text. Writing is often an iterative and interactive process, with cycles of textual revision, guided by comments from language teachers. # drafts 988 410 2310 705 1754 466 118 1532 651 31 2165 1"
P12-2049,A97-1004,0,0.152043,"Missing"
P12-2049,C10-2157,0,0.113901,"ere has been no largescale empirical study, however, on the effectiveness of feedback at the paragraph or discourse levels. 2.2 chanical, word-level errors, or also substantially reorganize paragraph or essay structures — has hardly been investigated. One reason for this gap in the literature is the lack of corpus data: none of the existing learner corpora (Izumi et al., 2004; Granger et al., 2009; Nagata et al., 2011; Dahlmeier and Ng, 2011) contains drafts written by non-native speakers that led to the “final version”. Recently, two corpora with text revision information have been compiled (Xue and Hwa, 2010; Mizumoto et al., 2011), but neither contain feedback from language teachers. Our corpus will allow researchers to not only examine the revision process, but also investigate any correlation with the amount and type of feedback. 3 Corpus Description We first introduce the context in which our data was collected (§3.1), then describe the kinds of comments in the drafts (§3.2). We then outline the conversion process of the corpus into XML format (§3.3), followed by an evaluation (§3.4) and an analysis (§3.5). 3.1 Revision Process While text editing in general has been analyzed (Mahlow and Piotr"
P15-2099,W14-1701,0,0.0994576,"ing both a subject and a finite verb; a typical example is a prepositional phrase, such as “Up through the ranks.” Subordinate clause. These fragments consist of a stand-alone subordinate clause; the clause typically begins with a relative pronoun or a subordinating conjunction, such as “While they take pains to hide their assets.” Introduction It is challenging to detect and correct sentencelevel grammatical errors because it involves automatic syntactic analysis on noisy, learner sentences. Indeed, none of the teams achieved any recall for comma splices in the most recent CoNLL shared task (Ng et al., 2014). Sentence fragments fared hardly better: of the thirteen teams, two scored a recall of 0.25 for correction and another scored 0.2; the rest did not achieve any recall. Although parser performance degrades on learner text (Foster, 2007), parsers can still be useful for identifying grammatical errors if they produce consistent patterns that indicate these errors. We show that parse tree patterns, automatically derived from training data, significantly improve system performance on detecting sentence fragments. The rest of the paper is organized as follows. The next section defines the types of"
P15-2099,O09-4002,0,0.0294818,"y commercial systems, such as the Criterion Online Writing Service (Burstein et al., 2004), Grammarly2 , and WhiteSmoke3 , give feedback about sentence fragments. To the best of our knowledge, these systems do not explicitly consider parse tree patterns. The grammar checker embedded in Microsoft Word also gives feedback about sentence fragments, and will serve as one of our baselines. Aside from the CoNLL-2014 shared task (see Section 1), the only other reported evaluation on detecting or correcting sentence fragments has been performed on Microsoft ESL Assistant and the NTNU Grammar Checker (Chen, 2009). Neither tool detected any of the sentence fragments in the test set. 4 The first baseline model was trained on the word trigrams of the sentences, the second model on part-of-speech unigrams, and the third on part-ofspeech trigrams. All of these features can be obtained without syntactic parsing. To reduce the number of features, we filtered out the word trigrams that occur less than twenty times and the POS trigrams that occur less than a hundred times in the training data. Parse Models. Our approach uses parse tree patterns as features. Although any arbitrary subtree structure can potentia"
P15-2099,W13-1703,0,0.0812567,"Missing"
P15-2099,P11-2038,0,0.0244599,"types of sentence fragments treated in this paper. Section 3 reviews related work. Section 4 describes the features used in our model. Section 5 discusses the datasets and section 6 analyzes the experiment results. Our best model significantly outperforms baselines that do not consider syntactic information and a widely used grammar checker. 3 Related Work Using parse tree patterns to judge the grammaticality of a sentence is not new. Wong and Dras (2011) exploited probabilistic context-free grammar (PCFG) rules as features for native language identification. In addition to production rules, Post (2011) incorporated parse fragment features computed from derivations of tree substitution grammars. Heilman et al. (2014) used the parse scores and syntactic features to classify the comprehensibility of learner text, though they made no attempt to correct the errors. In current grammatical error correction systems, parser output is used mainly to locate 1 Our evaluation data distinguishes between imperatives and fragments. Our automatic classifier, however, makes no such attempt because it would require analysis of the context and significant real-world knowledge. 599 Proceedings of the 53rd Annua"
P15-2099,W09-2112,0,0.0336992,"cases. We left all errors in the sentences in place so as to reflect our models’ performance on authentic learner data. While/IN Peter/NNP was/VBD a/DT good/JJ boy/NN FRAG SBAR While Peter was a good boy Peter/NNP was/VBD a/DT good/JJ boy/NN S NP VP Peter was a good boy Figure 1: The POS-tagged words and parse trees of the fragment “While Peter was a good boy.” and the well-formed sentence “Peter was a good boy.”. 5 5.1 Evaluation Data Data Training Data We automatically produced training data from the New York Times portion of the AQUAINT Corpus of English News Text (Graff, 2002). Similar to Foster and Andersen (2009), we artificially generate fragments that correspond to the four categories (Section 2) by removing different components from well-formed English sentences. For the “no subject” category, the NP immediately under the topmost S was removed. For the “no finite verb” category, we removed the finite verb in the VP immediately under the topmost S. For the “no subject and finite verb” category, we removed both the NP and the finite verb in the VP immediately under the topmost S. For the “subordinate clause” category, we looked for any SBAR in the sentence that is preceded by a comma and consists of"
P15-2099,P10-2065,0,0.0176506,"utput is used mainly to locate 1 Our evaluation data distinguishes between imperatives and fragments. Our automatic classifier, however, makes no such attempt because it would require analysis of the context and significant real-world knowledge. 599 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 599–603, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics relevant information involved in long-distance grammatical constructions (Tetreault et al., 2010; Yoshimoto et al., 2013; Zhang and Wang, 2014). To the best of our knowledge, the only previous work that used distinctive parse patterns to detect specific grammatical errors was concerned with comma splices. Lee et al. (2014) manually identified distinctive production rules which, when used as features in a CRF, significantly improved the precision and recall in locating comma splices in learner text. Our method will similarly leverage parse tree patterns, but with the goal of detecting sentence fragment errors. More importantly, our approach is fully automatic, and can thus potentially be"
P15-2099,N03-1033,0,0.0216975,"and the finite verb in the VP immediately under the topmost S. For the “subordinate clause” category, we looked for any SBAR in the sentence that is preceded by a comma and consists of an IN child followed by an S. The words under the SBAR are extracted as the fragment. Using this method, we created a total of 60,000 fragments, with 15,000 sentences in each category. Together with the original sentences, our training data consists of 120,000 sentences, half of which are fragments. 6 Results We obtained the POS tags and parse trees of the sentences in our datasets with the Stanford POS tagger (Toutanova et al., 2003) and the Stanford parser (Manning et al., 2014). We used the logistic regression implementation in scikit-learn (Pedregosa et al., 2011) for the maximum entropy models in our experiments. In addition to the three baseline models described in Section 4.1, we computed a fourth baseline using the grammar 601 System Word Trigrams POS Tags POS Trigrams MS Word Parse Parse + Tag checker in Microsoft Word 2013 by configuring the checker to capture “Fragments and Run-ons” and “Fragment - stylistic suggestions”. 6.1 Fragment detection We first evaluated the systems’ ability to detect fragments. The fra"
P15-2099,P14-2029,0,0.0255845,"e features used in our model. Section 5 discusses the datasets and section 6 analyzes the experiment results. Our best model significantly outperforms baselines that do not consider syntactic information and a widely used grammar checker. 3 Related Work Using parse tree patterns to judge the grammaticality of a sentence is not new. Wong and Dras (2011) exploited probabilistic context-free grammar (PCFG) rules as features for native language identification. In addition to production rules, Post (2011) incorporated parse fragment features computed from derivations of tree substitution grammars. Heilman et al. (2014) used the parse scores and syntactic features to classify the comprehensibility of learner text, though they made no attempt to correct the errors. In current grammatical error correction systems, parser output is used mainly to locate 1 Our evaluation data distinguishes between imperatives and fragments. Our automatic classifier, however, makes no such attempt because it would require analysis of the context and significant real-world knowledge. 599 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Lan"
P15-2099,D11-1148,0,0.0251444,"ning data, significantly improve system performance on detecting sentence fragments. The rest of the paper is organized as follows. The next section defines the types of sentence fragments treated in this paper. Section 3 reviews related work. Section 4 describes the features used in our model. Section 5 discusses the datasets and section 6 analyzes the experiment results. Our best model significantly outperforms baselines that do not consider syntactic information and a widely used grammar checker. 3 Related Work Using parse tree patterns to judge the grammaticality of a sentence is not new. Wong and Dras (2011) exploited probabilistic context-free grammar (PCFG) rules as features for native language identification. In addition to production rules, Post (2011) incorporated parse fragment features computed from derivations of tree substitution grammars. Heilman et al. (2014) used the parse scores and syntactic features to classify the comprehensibility of learner text, though they made no attempt to correct the errors. In current grammatical error correction systems, parser output is used mainly to locate 1 Our evaluation data distinguishes between imperatives and fragments. Our automatic classifier,"
P15-2099,Y11-1026,0,0.0309316,"e syntactic constituents of the sentence, and so often reveal differences between well-formed sentences and fragments. Consider the sentence “While Peter was a good boy.”, shown in the parse tree in Figure 1. The child of the root of the tree is SBAR. When the subordinating conjunction “while” is removed to yield a well-formed sentence, the children nodes change accordingly into the expected NP and VP. In contrast, the POS tags, used in the baseline models, tend to remain the same. We use the label of the root and the trigrams of its children nodes as features, similar to Sj¨obergh (2005) and Lin et al. (2011). We also extend our patterns to grandchildren in some cases. When analyzing an ill-formed sentence, the parser can sometimes group words into constituents to which they do not belong, such as forming a VP that does not contain a verb. For example, the phrase “up the hill” was analyzed as a VP in the fragment “A new challenger up the hill” when in fact the sentence is missing a verb. To take into account such misanalyses, we also include the POS tag of the first child of all NP, VP, PP, ADVP, and ADJP as features. The first child is chosen because it often exposes the parsing error, as is the"
P15-2099,P14-5010,0,0.00391475,"he topmost S. For the “subordinate clause” category, we looked for any SBAR in the sentence that is preceded by a comma and consists of an IN child followed by an S. The words under the SBAR are extracted as the fragment. Using this method, we created a total of 60,000 fragments, with 15,000 sentences in each category. Together with the original sentences, our training data consists of 120,000 sentences, half of which are fragments. 6 Results We obtained the POS tags and parse trees of the sentences in our datasets with the Stanford POS tagger (Toutanova et al., 2003) and the Stanford parser (Manning et al., 2014). We used the logistic regression implementation in scikit-learn (Pedregosa et al., 2011) for the maximum entropy models in our experiments. In addition to the three baseline models described in Section 4.1, we computed a fourth baseline using the grammar 601 System Word Trigrams POS Tags POS Trigrams MS Word Parse Parse + Tag checker in Microsoft Word 2013 by configuring the checker to capture “Fragments and Run-ons” and “Fragment - stylistic suggestions”. 6.1 Fragment detection We first evaluated the systems’ ability to detect fragments. The fragment categories are disregarded in this evalua"
P15-2099,D14-1033,0,0.0131545,"data distinguishes between imperatives and fragments. Our automatic classifier, however, makes no such attempt because it would require analysis of the context and significant real-world knowledge. 599 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 599–603, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics relevant information involved in long-distance grammatical constructions (Tetreault et al., 2010; Yoshimoto et al., 2013; Zhang and Wang, 2014). To the best of our knowledge, the only previous work that used distinctive parse patterns to detect specific grammatical errors was concerned with comma splices. Lee et al. (2014) manually identified distinctive production rules which, when used as features in a CRF, significantly improved the precision and recall in locating comma splices in learner text. Our method will similarly leverage parse tree patterns, but with the goal of detecting sentence fragment errors. More importantly, our approach is fully automatic, and can thus potentially be broadly applied on other syntax-related learner"
P15-2099,W13-3604,0,\N,Missing
P15-2099,Y14-1063,1,\N,Missing
P16-1093,P03-2026,0,0.0817879,"showed that they resulted in items that can better predict the user’s English proficiency level. Less attention has been paid to items for closedclass POS, such as articles, conjunctions and prepositions, which learners also often find difficult (Dahlmeier et al., 2013). For these POS, the standard algorithms based on semantic relatedness for open-class POS are not applicable. Lee and Seneff (2007) reported the only previous study on using learner corpora to generate items for a closed-class POS. They harvested the most frequent preposition errors in a corpus of Japanese learners of English (Izumi et al., 2003), but performed an empirical evaluation with native Chinese speakers on a narrow domain. We expand on this study in several dimensions. First, carrier sentences, selected from the general domain rather than a specific one, will be analyzed in terms of their difficulty level. Second, distractor quality will be evaluated not only by learners but also by experts, who give scores based on their plausibility; in contrast to most previous studies, their quality will be compared with the human gold standard. Thirdly, the effect of mismatched L1 will also be measured. corpora; a second leverages error"
P16-1093,P08-1021,1,0.801757,"rror correction — ensuring that the distractor yields an incorrect sentence — with the additional requirement on the plausibility of the distractor. Most approaches in automatic grammar correction can be classified as one of three types, according to the kind of statistics on which the system is trained. Some systems are trained on examples of correct usage (Tetreault and Chodorow, 2008; Felice and Pulman, 2009). Others are trained on examples of pairs of correct and incorrect usage, either retrieved from error-annotated learner corpora (Han et al., 2010; Dahlmeier et al., 2013) or simulated (Lee and Seneff, 2008; Foster and Andersen, 2009). More recently, a system has been trained on revision statistics from Wikipedia (Cahill et al., 2013). We build on all three paradigms, using standard English corPrevious Work Distractor generation Most research effort on automatic generation of fill-in-the-blank items has focused on vocabulary learning. In these items, the key is typically from an open-class part-of-speech (POS), e.g., nouns, verbs, or adjectives. To ensure that the distractor results in an incorrect sentence, the distractor must rarely, or never, collocate with other words in the carrier sentence"
P16-1093,W09-3010,1,0.879108,"Missing"
P16-1093,H05-1103,0,0.162977,"eech (POS), e.g., nouns, verbs, or adjectives. To ensure that the distractor results in an incorrect sentence, the distractor must rarely, or never, collocate with other words in the carrier sentence (Liu et al., 2005). To ensure the plausibility of the distractor, most approaches require it to be semantically close to the key, as determined by a thesaurus (Sumita et al., 2005; Smith et al., 2010), an ontology (Karamanis et al., 2006), rules handcrafted by experts (Chen et al., 2006), or contextsensitive inference rules (Zesch and Melamud, 2014); or to have similar word frequency (Shei, 2001; Brown et al., 2005). Sakaguchi et al. (2013) 985 prep Co-occurrence method (“Co-occur”) ... kicked the chair with ... ... kicked the can with ... ... with the goal of ... Learner error method (“Error”) ... kicked it &lt;error&gt;in&lt;/error&gt; the goal. ... kick the ball &lt;error&gt;in&lt;/error&gt; the other team’s goal. Learner revision method (“Revision”) ... kick the ball to into his own goal. ... kick the ball to towards his own goal. pobj ... kick the ball into the opponent’s goal VP head prep obj Figure 1: Parse tree for the carrier sentence in Table 1. Distractors are generated on the basis of the prepositional object (“obj”"
P16-1093,N13-1055,0,0.110386,"of the distractor. Most approaches in automatic grammar correction can be classified as one of three types, according to the kind of statistics on which the system is trained. Some systems are trained on examples of correct usage (Tetreault and Chodorow, 2008; Felice and Pulman, 2009). Others are trained on examples of pairs of correct and incorrect usage, either retrieved from error-annotated learner corpora (Han et al., 2010; Dahlmeier et al., 2013) or simulated (Lee and Seneff, 2008; Foster and Andersen, 2009). More recently, a system has been trained on revision statistics from Wikipedia (Cahill et al., 2013). We build on all three paradigms, using standard English corPrevious Work Distractor generation Most research effort on automatic generation of fill-in-the-blank items has focused on vocabulary learning. In these items, the key is typically from an open-class part-of-speech (POS), e.g., nouns, verbs, or adjectives. To ensure that the distractor results in an incorrect sentence, the distractor must rarely, or never, collocate with other words in the carrier sentence (Liu et al., 2005). To ensure the plausibility of the distractor, most approaches require it to be semantically close to the key,"
P16-1093,P06-4001,0,0.568481,"of fill-in-the-blank items has focused on vocabulary learning. In these items, the key is typically from an open-class part-of-speech (POS), e.g., nouns, verbs, or adjectives. To ensure that the distractor results in an incorrect sentence, the distractor must rarely, or never, collocate with other words in the carrier sentence (Liu et al., 2005). To ensure the plausibility of the distractor, most approaches require it to be semantically close to the key, as determined by a thesaurus (Sumita et al., 2005; Smith et al., 2010), an ontology (Karamanis et al., 2006), rules handcrafted by experts (Chen et al., 2006), or contextsensitive inference rules (Zesch and Melamud, 2014); or to have similar word frequency (Shei, 2001; Brown et al., 2005). Sakaguchi et al. (2013) 985 prep Co-occurrence method (“Co-occur”) ... kicked the chair with ... ... kicked the can with ... ... with the goal of ... Learner error method (“Error”) ... kicked it &lt;error&gt;in&lt;/error&gt; the goal. ... kick the ball &lt;error&gt;in&lt;/error&gt; the other team’s goal. Learner revision method (“Revision”) ... kick the ball to into his own goal. ... kick the ball to towards his own goal. pobj ... kick the ball into the opponent’s goal VP head prep obj"
P16-1093,W05-0201,0,0.661423,"Foster and Andersen, 2009). More recently, a system has been trained on revision statistics from Wikipedia (Cahill et al., 2013). We build on all three paradigms, using standard English corPrevious Work Distractor generation Most research effort on automatic generation of fill-in-the-blank items has focused on vocabulary learning. In these items, the key is typically from an open-class part-of-speech (POS), e.g., nouns, verbs, or adjectives. To ensure that the distractor results in an incorrect sentence, the distractor must rarely, or never, collocate with other words in the carrier sentence (Liu et al., 2005). To ensure the plausibility of the distractor, most approaches require it to be semantically close to the key, as determined by a thesaurus (Sumita et al., 2005; Smith et al., 2010), an ontology (Karamanis et al., 2006), rules handcrafted by experts (Chen et al., 2006), or contextsensitive inference rules (Zesch and Melamud, 2014); or to have similar word frequency (Shei, 2001; Brown et al., 2005). Sakaguchi et al. (2013) 985 prep Co-occurrence method (“Co-occur”) ... kicked the chair with ... ... kicked the can with ... ... with the goal of ... Learner error method (“Error”) ... kicked it &lt;e"
P16-1093,W13-1703,0,0.321936,"d fill-in-theblank item, where “into” is the key, and the other three choices are distractors. 984 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 984–993, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics applied machine learning methods to select verb distractors, and showed that they resulted in items that can better predict the user’s English proficiency level. Less attention has been paid to items for closedclass POS, such as articles, conjunctions and prepositions, which learners also often find difficult (Dahlmeier et al., 2013). For these POS, the standard algorithms based on semantic relatedness for open-class POS are not applicable. Lee and Seneff (2007) reported the only previous study on using learner corpora to generate items for a closed-class POS. They harvested the most frequent preposition errors in a corpus of Japanese learners of English (Izumi et al., 2003), but performed an empirical evaluation with native Chinese speakers on a narrow domain. We expand on this study in several dimensions. First, carrier sentences, selected from the general domain rather than a specific one, will be analyzed in terms of"
P16-1093,P14-5010,0,0.00414943,"ces. 1 We do not consider errors where a preposition should be inserted or deleted. 986 3.3 Learner revision method In this section, we first describe our datasets (Section 4.1) and the procedure for item generation (Section 4.2). We then give details on the expert evaluation (Section 4.3) and the learner evaluation (Section 4.4). any of these lists, with frequency counts derived from the Google Web Trillion Word Corpus.3 In order to retrieve the prepositional object and the NP/VP head (cf. Section 3), we parsed the Wikicorpus, as well as the corpora mentioned below, with the Stanford parser (Manning et al., 2014). Co-occurrence method (“Co-occur”). The statistics for the Co-occurrence method were also based on the English portion of Wikicorpus. Learner Revision method (“Revision”). We used an 8-million-word corpus of essay drafts written by Chinese learners of English (Lee et al., 2015). This corpus contains over 4,000 essays, with an average of 2.7 drafts per essay. The sentences and words between successive drafts have been automatically aligned. Learner Error method (“Error”). In addition to the corpus of essay drafts mentioned above, we used two other error-annotated learner corpora. The NUS Corpu"
P16-1093,reese-etal-2010-wikicorpus,0,0.423413,"Missing"
P16-1093,W09-2112,0,0.0220946,"ring that the distractor yields an incorrect sentence — with the additional requirement on the plausibility of the distractor. Most approaches in automatic grammar correction can be classified as one of three types, according to the kind of statistics on which the system is trained. Some systems are trained on examples of correct usage (Tetreault and Chodorow, 2008; Felice and Pulman, 2009). Others are trained on examples of pairs of correct and incorrect usage, either retrieved from error-annotated learner corpora (Han et al., 2010; Dahlmeier et al., 2013) or simulated (Lee and Seneff, 2008; Foster and Andersen, 2009). More recently, a system has been trained on revision statistics from Wikipedia (Cahill et al., 2013). We build on all three paradigms, using standard English corPrevious Work Distractor generation Most research effort on automatic generation of fill-in-the-blank items has focused on vocabulary learning. In these items, the key is typically from an open-class part-of-speech (POS), e.g., nouns, verbs, or adjectives. To ensure that the distractor results in an incorrect sentence, the distractor must rarely, or never, collocate with other words in the carrier sentence (Liu et al., 2005). To ensu"
P16-1093,P13-2043,0,0.342035,"ns, verbs, or adjectives. To ensure that the distractor results in an incorrect sentence, the distractor must rarely, or never, collocate with other words in the carrier sentence (Liu et al., 2005). To ensure the plausibility of the distractor, most approaches require it to be semantically close to the key, as determined by a thesaurus (Sumita et al., 2005; Smith et al., 2010), an ontology (Karamanis et al., 2006), rules handcrafted by experts (Chen et al., 2006), or contextsensitive inference rules (Zesch and Melamud, 2014); or to have similar word frequency (Shei, 2001; Brown et al., 2005). Sakaguchi et al. (2013) 985 prep Co-occurrence method (“Co-occur”) ... kicked the chair with ... ... kicked the can with ... ... with the goal of ... Learner error method (“Error”) ... kicked it &lt;error&gt;in&lt;/error&gt; the goal. ... kick the ball &lt;error&gt;in&lt;/error&gt; the other team’s goal. Learner revision method (“Revision”) ... kick the ball to into his own goal. ... kick the ball to towards his own goal. pobj ... kick the ball into the opponent’s goal VP head prep obj Figure 1: Parse tree for the carrier sentence in Table 1. Distractors are generated on the basis of the prepositional object (“obj”) and the NP/VP head to w"
P16-1093,W05-0210,0,0.592276,"ms, using standard English corPrevious Work Distractor generation Most research effort on automatic generation of fill-in-the-blank items has focused on vocabulary learning. In these items, the key is typically from an open-class part-of-speech (POS), e.g., nouns, verbs, or adjectives. To ensure that the distractor results in an incorrect sentence, the distractor must rarely, or never, collocate with other words in the carrier sentence (Liu et al., 2005). To ensure the plausibility of the distractor, most approaches require it to be semantically close to the key, as determined by a thesaurus (Sumita et al., 2005; Smith et al., 2010), an ontology (Karamanis et al., 2006), rules handcrafted by experts (Chen et al., 2006), or contextsensitive inference rules (Zesch and Melamud, 2014); or to have similar word frequency (Shei, 2001; Brown et al., 2005). Sakaguchi et al. (2013) 985 prep Co-occurrence method (“Co-occur”) ... kicked the chair with ... ... kicked the can with ... ... with the goal of ... Learner error method (“Error”) ... kicked it &lt;error&gt;in&lt;/error&gt; the goal. ... kick the ball &lt;error&gt;in&lt;/error&gt; the other team’s goal. Learner revision method (“Revision”) ... kick the ball to into his own goal."
P16-1093,C08-1109,0,0.489423,"2.2 Learner error correction There has been much recent research on automatic correction of grammatical errors. Correction of preposition usage errors, in particular, has received much attention. Our task can be viewed as the inverse of error correction — ensuring that the distractor yields an incorrect sentence — with the additional requirement on the plausibility of the distractor. Most approaches in automatic grammar correction can be classified as one of three types, according to the kind of statistics on which the system is trained. Some systems are trained on examples of correct usage (Tetreault and Chodorow, 2008; Felice and Pulman, 2009). Others are trained on examples of pairs of correct and incorrect usage, either retrieved from error-annotated learner corpora (Han et al., 2010; Dahlmeier et al., 2013) or simulated (Lee and Seneff, 2008; Foster and Andersen, 2009). More recently, a system has been trained on revision statistics from Wikipedia (Cahill et al., 2013). We build on all three paradigms, using standard English corPrevious Work Distractor generation Most research effort on automatic generation of fill-in-the-blank items has focused on vocabulary learning. In these items, the key is typical"
P16-1093,W14-1817,0,0.238435,"rning. In these items, the key is typically from an open-class part-of-speech (POS), e.g., nouns, verbs, or adjectives. To ensure that the distractor results in an incorrect sentence, the distractor must rarely, or never, collocate with other words in the carrier sentence (Liu et al., 2005). To ensure the plausibility of the distractor, most approaches require it to be semantically close to the key, as determined by a thesaurus (Sumita et al., 2005; Smith et al., 2010), an ontology (Karamanis et al., 2006), rules handcrafted by experts (Chen et al., 2006), or contextsensitive inference rules (Zesch and Melamud, 2014); or to have similar word frequency (Shei, 2001; Brown et al., 2005). Sakaguchi et al. (2013) 985 prep Co-occurrence method (“Co-occur”) ... kicked the chair with ... ... kicked the can with ... ... with the goal of ... Learner error method (“Error”) ... kicked it &lt;error&gt;in&lt;/error&gt; the goal. ... kick the ball &lt;error&gt;in&lt;/error&gt; the other team’s goal. Learner revision method (“Revision”) ... kick the ball to into his own goal. ... kick the ball to towards his own goal. pobj ... kick the ball into the opponent’s goal VP head prep obj Figure 1: Parse tree for the carrier sentence in Table 1. Distr"
P16-1093,W06-1416,0,\N,Missing
P16-4020,W08-0911,0,0.0508147,"Missing"
P16-4020,P06-4001,0,0.0341382,"o the user to select the page. On the other, the selected text does not necessarily suit the user in terms of its language quality, level of difficulty and the desired grammatical constructions. A number of other systems use text corpora to create grammar exercises. The KillerFiller tool in the VISL project, for example, generates slot-fill items from texts drawn from corpora (Bick, 2005). Similar to the WERTi system, an item takes the original word as its only key, and does not account for the possibility of multiple correct answers. Other systems attempt to generate distractors for the key. Chen et al. (2006) manually designed patterns for this purpose. Smith et al. (2010) utilized a theusaurus, while Zesch and Melamud (2014) developed context-sensitive rules. Unlike our approach, they did not adapt to the learner’s behavior. While some of these systems serve to provide draft FIB items for teachers to post-edit (Skory and Eskenazi, 2010), most remain research prototypes. A closely related research topic for this paper is automatic correction of grammatical errors (Ng et al., 2014). While the goal of distractor generation is to identify words that yield incorrect sentences, it is not merely the inv"
P16-4020,W13-1703,0,0.206064,"each sentence as the key, and then automatically generates distractors. It personalizes item selection for the user in two ways. First, it logs items to which the user previously gave incorrect answers, and offers similar items in a future session as review. Second, it progresses from easier to harder sentences, to minimize any hindrance on preposition learning that might be posed by difficult vocabulary. 1 Introduction Many learners of English find it challenging to master the use of prepositions. Preposition usage is a frequent error category in various learner corpora (Izumi et al., 2003; Dahlmeier et al., 2013; Lee et al., 2015); indeed, entire exercise books have been devoted to training learners on preposition usage (Watcyn-Jones and Allsop, 2000; Yates, 2010). To address this area of difficulty, we present a system that automatically generates fillin-the-blank (FIB) preposition items with multiple choices. Also known as gap-fill or cloze items, FIB items are a common form of exercise in computerassisted language learning (CALL) applications. Table 1 shows an example item designed for teaching English preposition usage. It contains a sentence, “The objective is to kick the ball into the opponent’"
P16-4020,D08-1020,0,0.0184253,"Missing"
P16-4020,reese-etal-2010-wikicorpus,0,0.0359269,"Missing"
P16-4020,W10-1007,0,0.0313388,"ates slot-fill items from texts drawn from corpora (Bick, 2005). Similar to the WERTi system, an item takes the original word as its only key, and does not account for the possibility of multiple correct answers. Other systems attempt to generate distractors for the key. Chen et al. (2006) manually designed patterns for this purpose. Smith et al. (2010) utilized a theusaurus, while Zesch and Melamud (2014) developed context-sensitive rules. Unlike our approach, they did not adapt to the learner’s behavior. While some of these systems serve to provide draft FIB items for teachers to post-edit (Skory and Eskenazi, 2010), most remain research prototypes. A closely related research topic for this paper is automatic correction of grammatical errors (Ng et al., 2014). While the goal of distractor generation is to identify words that yield incorrect sentences, it is not merely the inverse of the error correction task. An important element of the distractor generation task is to ensure that distractor appears plausible to the user. In contrast to the considerable effort in developing tools for detecting and correcting preposition errors (Tetreault and Chodorow, 2008; Felice and Pulman, 2009), there is only one pre"
P16-4020,C08-1109,0,0.0343613,"provide draft FIB items for teachers to post-edit (Skory and Eskenazi, 2010), most remain research prototypes. A closely related research topic for this paper is automatic correction of grammatical errors (Ng et al., 2014). While the goal of distractor generation is to identify words that yield incorrect sentences, it is not merely the inverse of the error correction task. An important element of the distractor generation task is to ensure that distractor appears plausible to the user. In contrast to the considerable effort in developing tools for detecting and correcting preposition errors (Tetreault and Chodorow, 2008; Felice and Pulman, 2009), there is only one previous study on preposition distractor generation (Lee and Seneff, 2007). Our system builds on this study by incorporating novel algorithms for distractor generation and personalization features. 3 Item creation The system considers all English sentences in the Wikicorpus (Reese et al., 2010) that have fewer than 20 words as carrier sentence candidates. In each candidate sentence, the system scans for prepositions, and extracts two features from the linguistic context of each preposition: • The prepositional object. In Figure 1, for example, the"
P16-4020,W14-1817,0,0.102764,"language quality, level of difficulty and the desired grammatical constructions. A number of other systems use text corpora to create grammar exercises. The KillerFiller tool in the VISL project, for example, generates slot-fill items from texts drawn from corpora (Bick, 2005). Similar to the WERTi system, an item takes the original word as its only key, and does not account for the possibility of multiple correct answers. Other systems attempt to generate distractors for the key. Chen et al. (2006) manually designed patterns for this purpose. Smith et al. (2010) utilized a theusaurus, while Zesch and Melamud (2014) developed context-sensitive rules. Unlike our approach, they did not adapt to the learner’s behavior. While some of these systems serve to provide draft FIB items for teachers to post-edit (Skory and Eskenazi, 2010), most remain research prototypes. A closely related research topic for this paper is automatic correction of grammatical errors (Ng et al., 2014). While the goal of distractor generation is to identify words that yield incorrect sentences, it is not merely the inverse of the error correction task. An important element of the distractor generation task is to ensure that distractor"
P16-4020,P16-1093,1,0.811589,"revision method Finally, our system exploits the revision behavior of learners in their English writing. This method requires draft versions of the same text written by a learner. It retrieves all learner sentences in a draft that contains a PP with the given prepositional object, and attached to the given NP/VP head. It then selects as distractor the preposition that is most often edited in a later draft. As shown in Table 2, this method generates the distractor “to” for the carrier sentence in Figure 1, since it is most often edited in the given linguistic context. The reader is referred to Lee et al. (2016) for details. To derive statistics for this method, our system also used the aforementioned corpus of essay drafts. • The head of the noun phrase or verb phrase (NP/VP head) to which the prepositional phrase (PP) is attached. In Figure 1, the PP “into the opponents’ goal” is attached to the VP head “kick”; the PP “on Monday” is attached to the NP head “meeting”. In order to retrieve the preposition, the prepositional object, and the NP/VP head (cf. Section 3), we parsed the Wikicorpus, as well as the corpora mentioned below, with the Stanford parser (Manning et al., 2014). The system passes th"
P16-4020,P14-5010,0,0.00335007,"The reader is referred to Lee et al. (2016) for details. To derive statistics for this method, our system also used the aforementioned corpus of essay drafts. • The head of the noun phrase or verb phrase (NP/VP head) to which the prepositional phrase (PP) is attached. In Figure 1, the PP “into the opponents’ goal” is attached to the VP head “kick”; the PP “on Monday” is attached to the NP head “meeting”. In order to retrieve the preposition, the prepositional object, and the NP/VP head (cf. Section 3), we parsed the Wikicorpus, as well as the corpora mentioned below, with the Stanford parser (Manning et al., 2014). The system passes the two features above to the following methods to attempt to generate distractors. If more than one key is possible, it prefers the one for which all three methods can generate a distractor. 3.1 Learner error method 4 Item selection Learners benefit most from items that are neither too easy nor too difficult. Following principles from adaptive testing (Bejar et al., 2003), the system tracks the user’s performance in order to select the most suitable items. It does so by considering the vocabulary level of the carrier sentence (Section 4.1) and the user’s previous mistakes"
P16-4020,W10-1002,0,0.168537,"r Intelligent Applications of Language Studies Department of Linguistics and Translation City University of Hong Kong {jsylee, mengqluo}@cityu.edu.hk Abstract correct answer), and the other three are distractors. These choices enable the CALL application to provide immediate and objective feedback to the learner. Traditional exercise books no longer meet all the needs of today’s learners. The pedagogical benefits of using authentic textual material have been well documented (Larimer and Schleicher, 1999; Erbaggio et al., 2012). One recent approach turns text on web pages into slot-fill items (Meurers et al., 2010). By offering the learner the freedom to choose his or her own preferred text, this approach motivates the learner to complete the exercises. Our system automatically constructs FIB preposition items from sentences in Wikipedia, a corpus that contains authentic language. As more users own mobile devices, mobile applications are now among the most efficient ways to provide on-demand language learning services. Although user attention on mobile devices can be brief and sporadic, each FIB item can be completed within a short time, and therefore our system offers an educational option for users to"
P16-4020,W14-1701,0,\N,Missing
P16-4020,P03-2026,0,\N,Missing
U19-1021,P06-4001,0,0.060848,"ems for prepositions, distractors based on learner errors were indeed found to be more challenging than those selected according to word co-occurrence (Lee et al., 2016). Distractor candidates have been mined from learner corpora and further selected with a discriminative model (Sakaguchi et al., 2013). Semantic similarity. The distractor should be semantically close to the target word. Similarity can be quantified by semantic distance in WordNet (Pino et al., 2008; Chen et al., 2015), thesauri (Sumita et al., 2005; Smith et al., 2010), ontologies (Karamanis et al., 2006), hand-crafted rules (Chen et al., 2006), and word embeddings (Jiang and Lee, 2017; Susanti et al., 2018). Other approaches have also explored synonym of synonyms (Knoop and Wilske, 2013); and words that are semantically similar to the target word in some sense, but not in the particular sense in the carrier sentence (Zesch and Melamud, 2014). A study on gap-fill items for learning Chinese as a foreign language compared the quality of distractors generated by a number of criteria, including spelling, word co-occurrence and semantic similarity (Jiang and Lee, 2017). Experimental results show that a semantic similarity measure, based"
U19-1021,W15-4406,0,0.015473,"and Lee, 2017). Learner error. Typical or frequent learner mistakes can be effective as distractors. When generating gap-fill items for prepositions, distractors based on learner errors were indeed found to be more challenging than those selected according to word co-occurrence (Lee et al., 2016). Distractor candidates have been mined from learner corpora and further selected with a discriminative model (Sakaguchi et al., 2013). Semantic similarity. The distractor should be semantically close to the target word. Similarity can be quantified by semantic distance in WordNet (Pino et al., 2008; Chen et al., 2015), thesauri (Sumita et al., 2005; Smith et al., 2010), ontologies (Karamanis et al., 2006), hand-crafted rules (Chen et al., 2006), and word embeddings (Jiang and Lee, 2017; Susanti et al., 2018). Other approaches have also explored synonym of synonyms (Knoop and Wilske, 2013); and words that are semantically similar to the target word in some sense, but not in the particular sense in the carrier sentence (Zesch and Melamud, 2014). A study on gap-fill items for learning Chinese as a foreign language compared the quality of distractors generated by a number of criteria, including spelling, word"
U19-1021,N19-1423,0,0.131128,"distractors at different levels of plausibility. We therefore propose to investigate the correlation between the estimated ranking of distractors and human judgment on plausibility. This research direction has indeed been taken up in item generation in the natural sciences (Liang et al., 2018; Gao et al., 2019). However, to the best of our knowledge, it has not yet been attempted for gap-fill items for language learning. Language models provide a natural framework for this task by predicting how likely a word appears in a gap within the sentence. This paper is the first attempt to apply BERT (Devlin et al., 2019), a state-of-the-art model trained on the masked language modeling objective, on the task of distractor ranking. Experimental results show that BERT outperforms semantic similarity measures, in terms of both correlation to human judgment and classification accuracy of distractor plausibility. The rest of the paper is organized as follows. In Section 2, we review related research areas. In Section 3, we outline our approach for distractor generation. In Section 4, we report experimental results on ranking distractors for gap-fill items for learning Chinese as a foreign language. Finally, we con"
U19-1021,W17-5015,1,0.247518,"ically and semantically homogenous (Pho et al., 2014). To reduce the manual effort and time needed for selecting distractors, there has been much interest He as class representative for two years. (a) served (b) acted (c) brought [target word] [distractor] [distractor] Table 1: A multiple-choice gap-fill item consists of a carrier sentence with a blank, and choices for filling the blank. In this example, the choices include two distractors and the target word (correct answer). in developing algorithms for automatic distractor generation. Existing algorithms typically take a two-step approach (Jiang and Lee, 2017; Susanti et al., 2018). The first step generates distractor candidates, typically in a list ranked according to measures of semantic similarity and collocation strength. The second step removes candidates that are acceptable answers, using n-gram and collocation frequency or other criteria. Evaluations on distractor generation tend to be limited to the highest-ranked distractors, for example the top-ranked or top three candidates only (Jiang and Lee, 2017; Susanti et al., 2018). Many practical scenarios, however, require not only the most challenging distractors, but distractors across the sp"
U19-1021,W06-1416,0,0.0452303,"e as distractors. When generating gap-fill items for prepositions, distractors based on learner errors were indeed found to be more challenging than those selected according to word co-occurrence (Lee et al., 2016). Distractor candidates have been mined from learner corpora and further selected with a discriminative model (Sakaguchi et al., 2013). Semantic similarity. The distractor should be semantically close to the target word. Similarity can be quantified by semantic distance in WordNet (Pino et al., 2008; Chen et al., 2015), thesauri (Sumita et al., 2005; Smith et al., 2010), ontologies (Karamanis et al., 2006), hand-crafted rules (Chen et al., 2006), and word embeddings (Jiang and Lee, 2017; Susanti et al., 2018). Other approaches have also explored synonym of synonyms (Knoop and Wilske, 2013); and words that are semantically similar to the target word in some sense, but not in the particular sense in the carrier sentence (Zesch and Melamud, 2014). A study on gap-fill items for learning Chinese as a foreign language compared the quality of distractors generated by a number of criteria, including spelling, word co-occurrence and semantic similarity (Jiang and Lee, 2017). Experimental results show th"
U19-1021,P16-1093,1,0.794448,"have been used, including bigram counts (Susanti et al., 2018) and, more generally, n-grams in a context window centered on the distractor (Liu et al., 2005); dependency relations (Sakaguchi et al., 2013); grammatical relations in a Word Sketch (Smith et al., 2010); as well as pointwise mutual information (PMI) (Jiang and Lee, 2017). Learner error. Typical or frequent learner mistakes can be effective as distractors. When generating gap-fill items for prepositions, distractors based on learner errors were indeed found to be more challenging than those selected according to word co-occurrence (Lee et al., 2016). Distractor candidates have been mined from learner corpora and further selected with a discriminative model (Sakaguchi et al., 2013). Semantic similarity. The distractor should be semantically close to the target word. Similarity can be quantified by semantic distance in WordNet (Pino et al., 2008; Chen et al., 2015), thesauri (Sumita et al., 2005; Smith et al., 2010), ontologies (Karamanis et al., 2006), hand-crafted rules (Chen et al., 2006), and word embeddings (Jiang and Lee, 2017; Susanti et al., 2018). Other approaches have also explored synonym of synonyms (Knoop and Wilske, 2013); an"
U19-1021,P18-2023,0,0.0174085,"ceptable answers. Our research focus is on the first step, to which we introduce a re-ranking process with a neural language model. 3.1 Baseline Our baseline uses semantic similarity measures, which have been reported in previous research to yield the best performance in identifying plausible distractor candidates (Section 2.1). Word embeddings have been shown to be effective in measuring word similarity and relatedness in a large range of NLP tasks, including distractor generation (Jiang and Lee, 2017). We used word embeddings trained by Skipgram with negative sampling on Baidu Encyclopedia (Li et al., 2018). Specifically, we calculated cosine similarity between the word embeddings of the distractor candidate and the target word, and obtained candidates with the highest scores. 3.2 Re-ranking The appropriateness of a distractor may depend not only on the target word but also on the context of the carrier sentence. Consider the word served as the target word. In the context of food being served at a restaurant, the word brought may be a plausible distractor since it is semantically close to the target word. However, in the context of serving in a position, the word acted would be a more plausible"
U19-1021,W18-0533,0,0.740562,"t items, for example, it can be useful to generate items at various difficulty levels for comprehensive assessment. In a CALL system, it can be effective to personalize distractor difficulty according to the user’s language proficiency. It is informative, then, to evaluate distractor algorithms on their ability to generate distractors at different levels of plausibility. We therefore propose to investigate the correlation between the estimated ranking of distractors and human judgment on plausibility. This research direction has indeed been taken up in item generation in the natural sciences (Liang et al., 2018; Gao et al., 2019). However, to the best of our knowledge, it has not yet been attempted for gap-fill items for language learning. Language models provide a natural framework for this task by predicting how likely a word appears in a gap within the sentence. This paper is the first attempt to apply BERT (Devlin et al., 2019), a state-of-the-art model trained on the masked language modeling objective, on the task of distractor ranking. Experimental results show that BERT outperforms semantic similarity measures, in terms of both correlation to human judgment and classification accuracy of dist"
U19-1021,W05-0201,0,0.0760628,"wers, leaving only the distractors that are “reliable”, i.e., those that yield an incorrect sentence. Section 2.1 reviews existing approaches for the Candidate Generation task, which is the research focus of this paper. Section 2.2 then surveys related tasks in computer-assisted learning that have adopted evaluation on candidate ranking. 2.1 according to their co-occurrence frequencies with other words in the sentence. Various definitions of co-occurrence have been used, including bigram counts (Susanti et al., 2018) and, more generally, n-grams in a context window centered on the distractor (Liu et al., 2005); dependency relations (Sakaguchi et al., 2013); grammatical relations in a Word Sketch (Smith et al., 2010); as well as pointwise mutual information (PMI) (Jiang and Lee, 2017). Learner error. Typical or frequent learner mistakes can be effective as distractors. When generating gap-fill items for prepositions, distractors based on learner errors were indeed found to be more challenging than those selected according to word co-occurrence (Lee et al., 2016). Distractor candidates have been mined from learner corpora and further selected with a discriminative model (Sakaguchi et al., 2013). Sema"
U19-1021,pho-etal-2014-multiple,0,0.0230463,"Missing"
U19-1021,P13-2043,0,0.0198426,"re “reliable”, i.e., those that yield an incorrect sentence. Section 2.1 reviews existing approaches for the Candidate Generation task, which is the research focus of this paper. Section 2.2 then surveys related tasks in computer-assisted learning that have adopted evaluation on candidate ranking. 2.1 according to their co-occurrence frequencies with other words in the sentence. Various definitions of co-occurrence have been used, including bigram counts (Susanti et al., 2018) and, more generally, n-grams in a context window centered on the distractor (Liu et al., 2005); dependency relations (Sakaguchi et al., 2013); grammatical relations in a Word Sketch (Smith et al., 2010); as well as pointwise mutual information (PMI) (Jiang and Lee, 2017). Learner error. Typical or frequent learner mistakes can be effective as distractors. When generating gap-fill items for prepositions, distractors based on learner errors were indeed found to be more challenging than those selected according to word co-occurrence (Lee et al., 2016). Distractor candidates have been mined from learner corpora and further selected with a discriminative model (Sakaguchi et al., 2013). Semantic similarity. The distractor should be seman"
U19-1021,W05-0210,0,0.683149,". Typical or frequent learner mistakes can be effective as distractors. When generating gap-fill items for prepositions, distractors based on learner errors were indeed found to be more challenging than those selected according to word co-occurrence (Lee et al., 2016). Distractor candidates have been mined from learner corpora and further selected with a discriminative model (Sakaguchi et al., 2013). Semantic similarity. The distractor should be semantically close to the target word. Similarity can be quantified by semantic distance in WordNet (Pino et al., 2008; Chen et al., 2015), thesauri (Sumita et al., 2005; Smith et al., 2010), ontologies (Karamanis et al., 2006), hand-crafted rules (Chen et al., 2006), and word embeddings (Jiang and Lee, 2017; Susanti et al., 2018). Other approaches have also explored synonym of synonyms (Knoop and Wilske, 2013); and words that are semantically similar to the target word in some sense, but not in the particular sense in the carrier sentence (Zesch and Melamud, 2014). A study on gap-fill items for learning Chinese as a foreign language compared the quality of distractors generated by a number of criteria, including spelling, word co-occurrence and semantic simi"
U19-1021,W14-1817,0,0.0185028,"2013). Semantic similarity. The distractor should be semantically close to the target word. Similarity can be quantified by semantic distance in WordNet (Pino et al., 2008; Chen et al., 2015), thesauri (Sumita et al., 2005; Smith et al., 2010), ontologies (Karamanis et al., 2006), hand-crafted rules (Chen et al., 2006), and word embeddings (Jiang and Lee, 2017; Susanti et al., 2018). Other approaches have also explored synonym of synonyms (Knoop and Wilske, 2013); and words that are semantically similar to the target word in some sense, but not in the particular sense in the carrier sentence (Zesch and Melamud, 2014). A study on gap-fill items for learning Chinese as a foreign language compared the quality of distractors generated by a number of criteria, including spelling, word co-occurrence and semantic similarity (Jiang and Lee, 2017). Experimental results show that a semantic similarity measure, based on the word2vec model (Mikolov et al., 2013), yields distractors that are significantly more plausible than those generated by spelling similarity, and by word co-occurrence strength as estimated by PMI. Candidate Generation In most approaches, a distractor needs to have the same part-of-speech (POS) as"
W08-2117,P08-1083,0,0.0543828,"Missing"
W08-2117,P99-1037,0,0.0941122,"Missing"
W08-2117,N07-1020,0,0.0271133,"Missing"
W08-2117,J01-2001,0,0.0952282,"Missing"
W08-2117,C73-2026,0,0.747875,"understand the meaning of the word, or even look it up in a dictionary. For Classics scholars, these myriad forms also pose formidable challenges. In order to search for occurrences of a word in a corpus, all of its forms must be enumerated, since words do not frequently appear in their root forms. This procedure becomes extremely labor-intensive for small words that overlap with other common words (Crane, 1991). Automatic morphological analysis of ancient Greek would be useful for both educational and research purposes. In fact, one of the first analyzers was developed as a pedagogical tool (Packard, 1973). Today, a widely used analyzer is embedded in the Perseus Digital Library (Crane, 1996), an internet resource utilized by both students and researchers. This paper presents an analyzer of ancient Greek that infers the root form of a word. It introduces two innovations. First, it utilizes a nearestneighbor framework that requires no hand-crafted rules, and provides analogies to facilitate learning. 2 The root is also called the “base” or “lexical look-up” form, since it is the form conventionally used in dictionary entries. For verbs in ancient Greek, the root form is the first person singular"
W08-2117,P00-1027,0,0.0477929,"Missing"
W09-3010,N04-2006,1,0.842388,"non-native writing is difficult because of the possibility of multiple correct answers and the variability in human agreement. This paper seeks to improve the best practice of such evaluation by analyzing the frequency of multiple correct answers and identifying factors that influence agreement levels in judging the usage of articles and noun number. 1 Introduction In recent years, systems have been developed with the long-term goal of detecting, in the writing of non-native speakers, usage errors involving articles, prepositions and noun number (Knight and Chander, 1994; Minnen et al., 2000; Lee, 2004; Han et al., 2005; Peng and Araki, 2005; Brockett et al., 2006; Turner and Charniak, 2007). These systems should, ideally, be evaluated on a corpus of learners’ writing, annotated with acceptable corrections. However, since such corpora are expensive to compile, many researchers have instead resorted to measuring the accuracy of predicting what a native writer originally wrote in well-formed text. This type of evaluation effectively makes the assumption that there is one correct form of native usage per context, which may not always be the case. Two studies have already challenged this “singl"
W09-3010,W00-0708,0,0.300286,"Missing"
W09-3010,H94-1048,0,0.0215294,"ld be annotated. Using this annotation scheme, however, raises two questions that have not yet been thoroughly researched: (1) what is the human agreement level on such annotation? (2) what factors might influence the agreement level? In this paper, we consider two factors: the context of a word, and the variability of its usage. In the two studies cited above, the human judges were shown only the target sentence and did not take into account any constraint on the choice of word that might be imposed by the larger context. For PP attachment, human performance improves when given more context (Ratnaparkhi et al., 1994). For other linguistic phenomena, such as article/number selection for nouns, a larger context window of at least several sentences may be required, even though some automatic methods for exploiting context have not been shown to boost performance (Han et al., 2005). The second factor, variability of usage, may be 60 Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 60–63, c Suntec, Singapore, 6-7 August 2009. 2009 ACL and AFNLP null Three years ago John Small, a sheep farmer in the Mendip Hills, read an editorial in his local newspaper which claimed that foxes ne"
W09-3010,N07-2045,0,0.134007,"Missing"
W09-3010,W08-1205,1,\N,Missing
W09-3010,P06-1032,0,\N,Missing
W12-1011,J93-2004,0,0.0387049,"the string 沙門 sha men ‘Buddhist monk’. As a transliteration from Sanskrit, it bears no semantic relation with its constituent characters 沙 sha ‘sand’ and 門 men ‘door’. The two characters therefore form one word. From the point of view of corpus development, word segmentation has two consequences. First, it defines the smallest unit for POS analysis. It would be meaningless to analyze the POS of the individual characters as, Introduction There has been much effort in enriching text corpora with linguistic information, such as parts-of-speech (Francis and Kučera, 1982) and syntactic structures (Marcus et al., 1993). The past decade has seen the development of Chinese corpora, mostly for Modern Chinese (McEnery & Xiao, 2004; Xue et al., 2005), but also a few for pre-modern, or “classical”, Chinese (Wei et al. 97; Huang et al. 2006; Hu & McLaughlin 2007). One common design issue for any corpus of Chinese, whether modern or classical, is word segmentation. Yet, no segmentation standard has emerged in the computational linguistics research community. Hence, two adjacent characters X1X2 may be considered a single word in one corpus, but treated as two distinct words X1 and 1 This phenomenon can be compared w"
W12-1011,mcenery-xiao-2004-lancaster,0,0.0364552,"its constituent characters 沙 sha ‘sand’ and 門 men ‘door’. The two characters therefore form one word. From the point of view of corpus development, word segmentation has two consequences. First, it defines the smallest unit for POS analysis. It would be meaningless to analyze the POS of the individual characters as, Introduction There has been much effort in enriching text corpora with linguistic information, such as parts-of-speech (Francis and Kučera, 1982) and syntactic structures (Marcus et al., 1993). The past decade has seen the development of Chinese corpora, mostly for Modern Chinese (McEnery & Xiao, 2004; Xue et al., 2005), but also a few for pre-modern, or “classical”, Chinese (Wei et al. 97; Huang et al. 2006; Hu & McLaughlin 2007). One common design issue for any corpus of Chinese, whether modern or classical, is word segmentation. Yet, no segmentation standard has emerged in the computational linguistics research community. Hence, two adjacent characters X1X2 may be considered a single word in one corpus, but treated as two distinct words X1 and 1 This phenomenon can be compared with what is often known as multiword expressions (Sag et al., 2002) in other languages. 75 Proceedings of the"
W12-1011,W08-0336,0,0.0352798,"Missing"
W12-1011,Y96-1018,0,0.106622,"In general, a larger tagset provides more precise information, but may result in lower inter-annotator agreement, and hence reduced reliability. Classical Chinese does not have inflectional morphology; this makes POS tags even more informative, but also makes inter-annotator agreement more challenging. As with other languages, the POS tagset is tailored to fit one’s research objective, as reflected in the wideranging levels of granularity in different corpora, from 21 tags in (Huang et al., 2006), 26 in the Peking University corpus (Yu et al., 2002), 46 in the Academia Sinica Balanced Corpus (Chen et al., 1996), to 111 in the Sheffield Corpus of Chinese (Hu et al., 2005). Our tagset is based on that of the Penn Chinese Treebank, which lies towards the lower end of this spectrum, with 33 tags. 3.3 Multi-level Tagging In principle, any text span may be annotated at an arbitrary number of levels using, for example, stand-off annotation. In practice, most effort has concentrated on identifying named entities, such as (Doddington et al., 2004). While our corpus does specify word boundaries of multi-character proper nouns, it tackles all other forms of compounds in general (section 4.2). Turning to the Ch"
W12-1011,doddington-etal-2004-automatic,0,0.0230578,"granularity in different corpora, from 21 tags in (Huang et al., 2006), 26 in the Peking University corpus (Yu et al., 2002), 46 in the Academia Sinica Balanced Corpus (Chen et al., 1996), to 111 in the Sheffield Corpus of Chinese (Hu et al., 2005). Our tagset is based on that of the Penn Chinese Treebank, which lies towards the lower end of this spectrum, with 33 tags. 3.3 Multi-level Tagging In principle, any text span may be annotated at an arbitrary number of levels using, for example, stand-off annotation. In practice, most effort has concentrated on identifying named entities, such as (Doddington et al., 2004). While our corpus does specify word boundaries of multi-character proper nouns, it tackles all other forms of compounds in general (section 4.2). Turning to the Chinese language in particular, we are by no means the first to point out inconsistencies in word segmentation and POS 4 E.g., the morpheme 本 ben is bound to the character 人 ren ‘person’ in the word 本人 ben ren ‘oneself’ 77 tags among different corpora. Annotators of the Penn Chinese Treebank, among others, also recognized this issue (Xia, 2000). As a remedy, a two-level annotation method is used on a number of grammatical construction"
W12-1011,J96-3004,0,0.0762012,"Missing"
W12-1011,W03-1719,0,0.0729931,"Missing"
W12-1011,W03-1726,0,\N,Missing
W13-3409,W02-0108,0,0.0423457,"ompiled them into a dependency treebank, and made it available for search and visualization on a web-based interface. Then, in a research assignment, students tackled questions on Chinese poetry with this 2 Previous Work Many current systems support the use of linguistic corpora for teaching and learning. One of many examples, the Visual Interactive Syntax Learning (VISL) system allows students to search, view, construct and label parse trees (Bick, 2005). The GATE system similarly facilitates corpus annotation, but it can also perform a variety of NLP tasks including POS tagging and parsing (Bontcheva et al., 2002). These systems facilitate pedagogical use of treebanks in two main ways. First, students visualize parse trees and search for linguistic structures on existing treebanks. These functions 56 Proceedings of the Fourth Workshop on Teaching Natural Language Processing, pages 56–60, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics support empirical and quantitative analysis of linguistic phenomena. Second, students also use their editing environment to create new dependency annotations on text, as exercises in learning a new language. The resulting treebank can th"
W13-3409,W09-2307,0,0.0197933,"Missing"
W13-3409,W12-1011,1,0.85471,"Missing"
W13-3409,N12-1020,1,0.8612,"Missing"
W13-3409,C12-2061,1,0.794276,"Missing"
W15-5949,2014.amta-wptp.2,0,0.0779852,"Missing"
W15-5949,eisele-chen-2010-multiun,0,0.0246079,"tigate correlations between translation quality and other variables, namely, different domains and translator abilities. 3 3.1 Experimental setup Domains We chose two contrasting domains on which to conduct our experiments. The first is a resourcerich domain with many commercial MT systems trained with similar bilingual data; the second is resource-poor with limited samples of bilingual sentences, and would be considered out-of-domain for most commercial MT systems. • “Multi UN” domain. This domain consists of a corpus of United Nations documents published between January 2000 and March 2010 (Eisele and Chen, 2010). Among the largest parallel corpora available for Chinese and English (Tian et. al, 2014), the corpus has a total of 9.6 million aligned sentences. • “Literary” domain. This domain is derived from the first 51 chapters of the Chinese classic novel, Romance of the Three Kingdoms, and an English translation by C.H. Brewitt-Taylor.2 These chapters contained 1563 paragraph alignments and 249390 characters. 3.2 For each domain, we compared two translation assistance types: • Translation options. The user first selects translation options for phrases in the source sentence, and then further edits t"
W15-5949,2014.amta-wptp.5,0,0.0193079,"ently work better either with options or with MT post-editing, regardless of domain and of their translation ability. 1 Introduction State-of-the-art computer-assisted translation (CAT) systems offer many types of assistance to the human translator. Most studies have focused on investigating whether such assistance — including translation memory, Machine Translation (MT) output, and word and phrase translation options — results in higher productivity and better quality when compared with unassisted translation (Plitt & Masselot, 2010; Zhechev, 2012; Green et al., 2013; Aranberri et al., 2014; Gaspari et al., 2014). Less attention, however, has been devoted to comparing the relative merits of these assistance types. This paper presents a direct comparison between two common types, namely, selection from translation options, and post-editing of MT output, with Goo* 2 Previous work As publicly available MT systems continue to improve, human translators increasingly adopt postediting of MT output to boost their productivity. Naturally, the quality of the output is a major factor that determines its benefits. It has been shown that better MT systems generally yield greater productivity gains (Koehn and Germ"
W15-5949,2009.mtsummit-btm.7,0,0.0298031,"ces extracted from the corpus of UN documents (Eisele and Chen, 2010); the table for the Literary domain, with those from the aforementioned bilingual data from the Romance of the Three Kingdoms. While the UN corpus provides sentence alignments,3 we aligned the Chinese and English texts in the Literary domain using Microsoft’s “Bilingual Sentence Aligner” tool, 4 followed by manual review to exclude false matches. We then used Giza++ 2 1 Some other studies have focused on comparing post-editing from MT output and from sentences that are exact or fuzzy matches with entries in the phrase table (Guerberof, 2009; Laubli et al., 2013). 344 Translation assistance types http://ctext.org/sanguo-yanyi Sentences with over 40 Chinese characters were excluded. 4 The aligner implements the algorithm described by Moore (2002). See http://research.microsoft.com/enus/downloads/aafd5dcf-4dcc-49b2-8a22-f7055113e656/ 3 • 3.3 (Och and Ney, 2003) to extract alignments between Chinese and English and used these to construct our phrase table. MT output post-editing. The user postedits the translation produced by a fully automatic MT system; in our case, the Google Translate system (translate.google.com). An alternative"
W15-5949,P09-4005,0,0.193117,"ching data, the higher quality of the MT output outweighs the reduction in effort brought by the options. The number of clicks per 100 words is notably higher for the Literary domain than the Multi UN. This can be explained by modern Chinese having longer average word length than literary Chinese. Furthermore, UN documents tend to contain repeated technical terms and phrases that are identified and matched as single phrases when they reoccur. One factor that could potentially influence our results is the degree to which the translator selects the option in the “optimal” manner. As observed by Koehn (2009b), when the translator saw an option that was suitable, he or she might have simply typed it in, rather than using the clicking mechanism to insert. To investigate whether this was a common phenomenon, we needed to compare actual user selection of options to the hypothetically “ideal” selections, given the final translation. To do this, we calculated the optimal set of selections that would have resulted in the closest string to that user’s final translation, as measured by TER. On the Literary domain, the selections chosen by sev347 eral translators had the same TER as the optimal selections"
W15-5949,N10-1078,0,0.0583159,"Missing"
W15-5949,W14-0307,0,0.0643936,"ing time (Tatsumi and Roturier, 2010; Koponen, 2012; Koponen et al., 2012). Therefore, we will hold temporal effort as constant, and instead measure technical effort (Krings, 2001), by explicitly measuring the amount of editing and the number of clicks needed for selecting options. We investigate which assistance type requires less effort, across two domains that differ in terms of MT output quality. Translation quality. We measure whether either of these assistance types results in better human translations. Past studies have suggested that individual work style is an important factor (e.g., Koehn and Germann, 2014); this study provides further evidence by analyzing a number of other possible factors, including the effect of different domains and translator abilities. Past research has shown that various types of computer assistance can reduce translation effort and improve translation quality over manual translation. This paper directly compares two common assistance types – selection from lists of translation options, and postediting of machine translation (MT) output produced by Google Translate – across two significantly different subject domains for Chinese-to-English translation. In terms of transl"
W15-5949,W12-3123,0,0.017624,"Effort: Options versus Post-editing Donald Sturgeon* Fairbank Center for Chinese Studies Harvard University djs@dsturgeon.net John S. Y. Lee Department of Linguistics and Translation City University of Hong Kong jsylee@cityu.edu.hk Abstract gle Translate as the MT system. We analyze these two assistance types along two dimensions: Translation effort. Translation effort can be measured in terms of time or the amount of editing. Previous research has found between-translator variance of the number of post-editing operations to be lower than that of post-editing time (Tatsumi and Roturier, 2010; Koponen, 2012; Koponen et al., 2012). Therefore, we will hold temporal effort as constant, and instead measure technical effort (Krings, 2001), by explicitly measuring the amount of editing and the number of clicks needed for selecting options. We investigate which assistance type requires less effort, across two domains that differ in terms of MT output quality. Translation quality. We measure whether either of these assistance types results in better human translations. Past studies have suggested that individual work style is an important factor (e.g., Koehn and Germann, 2014); this study provides furth"
W15-5949,2012.amta-wptp.2,0,0.0196582,"versus Post-editing Donald Sturgeon* Fairbank Center for Chinese Studies Harvard University djs@dsturgeon.net John S. Y. Lee Department of Linguistics and Translation City University of Hong Kong jsylee@cityu.edu.hk Abstract gle Translate as the MT system. We analyze these two assistance types along two dimensions: Translation effort. Translation effort can be measured in terms of time or the amount of editing. Previous research has found between-translator variance of the number of post-editing operations to be lower than that of post-editing time (Tatsumi and Roturier, 2010; Koponen, 2012; Koponen et al., 2012). Therefore, we will hold temporal effort as constant, and instead measure technical effort (Krings, 2001), by explicitly measuring the amount of editing and the number of clicks needed for selecting options. We investigate which assistance type requires less effort, across two domains that differ in terms of MT output quality. Translation quality. We measure whether either of these assistance types results in better human translations. Past studies have suggested that individual work style is an important factor (e.g., Koehn and Germann, 2014); this study provides further evidence by analyzin"
W15-5949,2013.mtsummit-wptp.1,0,0.101804,"Missing"
W15-5949,langlais-etal-2000-evaluation,0,0.176605,"Missing"
W15-5949,2013.mtsummit-wptp.10,0,0.0198174,"m the corpus of UN documents (Eisele and Chen, 2010); the table for the Literary domain, with those from the aforementioned bilingual data from the Romance of the Three Kingdoms. While the UN corpus provides sentence alignments,3 we aligned the Chinese and English texts in the Literary domain using Microsoft’s “Bilingual Sentence Aligner” tool, 4 followed by manual review to exclude false matches. We then used Giza++ 2 1 Some other studies have focused on comparing post-editing from MT output and from sentences that are exact or fuzzy matches with entries in the phrase table (Guerberof, 2009; Laubli et al., 2013). 344 Translation assistance types http://ctext.org/sanguo-yanyi Sentences with over 40 Chinese characters were excluded. 4 The aligner implements the algorithm described by Moore (2002). See http://research.microsoft.com/enus/downloads/aafd5dcf-4dcc-49b2-8a22-f7055113e656/ 3 • 3.3 (Och and Ney, 2003) to extract alignments between Chinese and English and used these to construct our phrase table. MT output post-editing. The user postedits the translation produced by a fully automatic MT system; in our case, the Google Translate system (translate.google.com). An alternative approach would have b"
W15-5949,moore-2002-fast,0,0.185407,"Missing"
W15-5949,J03-1002,0,0.0116173,"osoft’s “Bilingual Sentence Aligner” tool, 4 followed by manual review to exclude false matches. We then used Giza++ 2 1 Some other studies have focused on comparing post-editing from MT output and from sentences that are exact or fuzzy matches with entries in the phrase table (Guerberof, 2009; Laubli et al., 2013). 344 Translation assistance types http://ctext.org/sanguo-yanyi Sentences with over 40 Chinese characters were excluded. 4 The aligner implements the algorithm described by Moore (2002). See http://research.microsoft.com/enus/downloads/aafd5dcf-4dcc-49b2-8a22-f7055113e656/ 3 • 3.3 (Och and Ney, 2003) to extract alignments between Chinese and English and used these to construct our phrase table. MT output post-editing. The user postedits the translation produced by a fully automatic MT system; in our case, the Google Translate system (translate.google.com). An alternative approach would have been to train statistical machine translation models using the respective bilingual datasets described in the previous section. We trained such a model for the Multi UN domain using Moses MT (Koehn et al., 2007); the resultant model achieved a TER of 56.9 on the test set (see Section 3.4) when measured"
W15-5949,2006.amta-papers.25,0,0.149262,"Missing"
W15-5949,C12-2116,0,0.03081,"have been to train statistical machine translation models using the respective bilingual datasets described in the previous section. We trained such a model for the Multi UN domain using Moses MT (Koehn et al., 2007); the resultant model achieved a TER of 56.9 on the test set (see Section 3.4) when measured against the reference translation, which was on a par with the TER of 58.3 achieved by Google Translate. For the Literary domain, however, it would be impractical to train such a model, given the limited amount of data available. A possible mitigation is to attempt domain adaptation (e.g., Song et al., 2012), but this method would introduce a possibly confounding variable. Instead, for more straightforward comparison, we made use of Google Translate, a generalpurpose and widely used MT system, on both domains. Interface Figure 1 shows the interface used in our experiments for displaying translation options. The user is presented with a list of translation options for phrases in the source sentence, in decreasing order of the frequency with which they occur in the training data.5 Such matches may be strings of any length, and are chosen using forward maximal matching against the phrase table. For"
W15-5949,2009.eamt-1.5,0,0.0845947,"Missing"
W15-5949,2010.jec-1.6,0,0.0952629,"Missing"
W15-5949,temnikova-2010-cognitive,0,0.0416265,"Missing"
W15-5949,tian-etal-2014-um,0,0.0341448,"Missing"
W15-5949,2013.mtsummit-posters.7,0,0.0419733,"Missing"
W15-5949,2012.amta-wptp.10,0,0.0393095,"ve role than assistance type: a translator tends to consistently work better either with options or with MT post-editing, regardless of domain and of their translation ability. 1 Introduction State-of-the-art computer-assisted translation (CAT) systems offer many types of assistance to the human translator. Most studies have focused on investigating whether such assistance — including translation memory, Machine Translation (MT) output, and word and phrase translation options — results in higher productivity and better quality when compared with unassisted translation (Plitt & Masselot, 2010; Zhechev, 2012; Green et al., 2013; Aranberri et al., 2014; Gaspari et al., 2014). Less attention, however, has been devoted to comparing the relative merits of these assistance types. This paper presents a direct comparison between two common types, namely, selection from translation options, and post-editing of MT output, with Goo* 2 Previous work As publicly available MT systems continue to improve, human translators increasingly adopt postediting of MT output to boost their productivity. Naturally, the quality of the output is a major factor that determines its benefits. It has been shown that better MT"
W16-5403,W09-2307,0,0.017177,"icle starts out with a brief overview of existing dependency annotation schemes for Mandarin Chinese and how they compare overall to the UD scheme. We describe a few of the Mandarin POS tag choices of our scheme in section 3. Section 4 is devoted to the important features of our dependency annotation scheme and the sub-types of dependency relations we introduce. 2 Dependency schemes for Mandarin Chinese Two widely used dependency schemes for Mandarin Chinese are Stanford Dependencies (SD) for Chinese (hereafter Stanford Chinese), developed by Huihsen Tseng and Pi-Chuan Chang (see Chang, 2009; Chang, Tseng, Jurafsky, & Manning, 2009), and the Chinese Dependency Treebank (CDT) developed by the Harbin Institute of Technology Research Center for Social Computing and Information Retrieval (see Che, Li, & Liu, 2012; HIT-SCIR, 2010; Zhang, Zhang, Che, & Liu, 2012). Stanford Chinese adopts its part-of-speech (POS) tagset directly from the Chinese Treebank (CTB) currently maintained at Brandeis University (Xue et al., 2013), also previously known as the Penn Chinese Treebank (hereafter Penn Chinese). We have adapted the first version of Universal Dependencies (UD) for Mandarin Chinese (hereafter Mandarin UD) with reference to th"
W16-5403,W16-1715,1,0.839443,"released annotated data in 40 languages (Nivre et al., 2016b)—the proposed structure has been unique (no separate semantic or surface-syntactic annotation1). The scheme has triggered some debate on the syntactic foundation of some choices that have been made (Osborne, 2015), in particular because UD does not rely on one theoretical framework and some of the proposed goals are necessarily contradictory: syntactic correctness, applicability of the schemes for NLP tools and purposes, and above all universality (similarity of structures across languages) cannot all be fulfilled at the same time (Gerdes & Kahane, 2016). Although no separate explicit annotation scheme exists for most UD treebanks, universality seems to outweigh other considerations. This paper describes similar choices in our adoption of UD for Mandarin Chinese, but we will try to be explicit about the advantages and disadvantages of the choices we made. The gaps and problems we describe show more generally that syntactic category and function sets that were originally created for Indo-European languages need important changes and careful balancing of criteria to foster typologically different languages, so that the distinctions become truly"
W16-5403,I11-1174,1,0.802653,"with regard to the morphosyntactic properties of Mandarin. Through these discussions we identified possible gaps in the current UD design, especially with regards to verbal markers and sentence particles that lie beyond the purview of adverbial modifiers and discourse markers. We also identified two common structures in Mandarin, the realis and irrealis descriptive clauses, that may have eluded analysis and explicit treatment in other Chinese treebank schemes. We are in the process of applying our proposed annotation scheme to the Mandarin Chinese text in a Cantonese-Mandarin Parallel Corpus (Lee, 2011) of over 8000 lines of text. We plan to develop Universal Dependencies for Cantonese as well, to enable comparative studies on the grammars of the two Chinese languages. Once the treebanks for Mandarin and Cantonese are finalized, we hope to release them as part of the UD project, to be made publicly available through its website (http://universaldependencies.org). 27 Acknowledgements This work was supported by a grant from the PROCORE-France/Hong Kong Joint Research Scheme sponsored by the Research Grants Council and the Consulate General of France in Hong Kong (Reference No.: F-CityU107/15 a"
W16-5403,de-marneffe-etal-2006-generating,0,0.0816565,"Missing"
W16-5403,de-marneffe-etal-2014-universal,0,0.0539108,"Missing"
W16-5403,L16-1262,0,0.0653141,"Missing"
W16-5403,W15-2128,0,0.0129953,"et al., 2016a) constitutes an important homogenization effort to synthesize ideas and experiences from different dependency treebanks in different languages, with the aim of facilitating multilingual research on syntax and parsing by proposing a unified annotation scheme for all languages. Up to the current version of UD (1.3)— which has released annotated data in 40 languages (Nivre et al., 2016b)—the proposed structure has been unique (no separate semantic or surface-syntactic annotation1). The scheme has triggered some debate on the syntactic foundation of some choices that have been made (Osborne, 2015), in particular because UD does not rely on one theoretical framework and some of the proposed goals are necessarily contradictory: syntactic correctness, applicability of the schemes for NLP tools and purposes, and above all universality (similarity of structures across languages) cannot all be fulfilled at the same time (Gerdes & Kahane, 2016). Although no separate explicit annotation scheme exists for most UD treebanks, universality seems to outweigh other considerations. This paper describes similar choices in our adoption of UD for Mandarin Chinese, but we will try to be explicit about th"
W16-5403,L16-1261,0,0.0234844,"the particle 的 de as an adjectival marker in this case (such as in (1b)). The advantage of shuffling this subclass of verbs is that we are able to separate the intransitive stative uses of the predicate verbs from other verbs, since UD does not allow one to create subcategorical POS tags. Additionally, the modifier and predicate uses of predicate verbs are easily differentiated from each other simply by looking at their head in the dependency representation. Since the decision to tag predicate adjectives as ADJ is also supported in other languages such as in the Japanese implementation of UD (Tanaka et al., 2016), we consider our categorization to be more advantageous for cross-linguistic comparison. 3.2 Localizers This class of words is known in Chinese linguistic literature as 方位詞 fāngwèicí. They come after a noun and primarily indicate spatial information in relation to the noun (with grammaticized uses for temporal and other abstract concepts of location), and are often additionally paired with the preposition 在 zài. Examples include 上 shàng ‘above’, 中 zhōng ‘middle’, 外 wài ‘outside’, 前 qián ‘front’, 旁 páng ‘side’, among others. Both Penn Chinese and CDT give localizers a unique tag—‘LC’ for ‘loca"
W17-0408,W13-1723,0,0.0182211,"xt in the Universal Dependencies (UD) 2.1 Lemma SALLE allows an exception to “literal annotation” when dealing with lexical violations. When there is a spelling error (e.g., “*ballence”), the annotator puts the intended, or corrected form of the word (“balance”) as lemma. For real-word spelling errors, the distinction between a word selection error and spelling error can be blurred. SALLE requires 67 Proceedings of the NoDaLiDa 2017 Workshop on Universal Dependencies (UDW 2017), pages 67–71, Gothenburg, Sweden, 22 May 2017. a spelling error to be “reasonable orthographic or phonetic changes” (Ragheb and Dickinson, 2013). For a sentence such as “... *loss its ballence”, the lemma of the word “loss” would be considered to be “lose”. The lemma forms the basis for further analysis in POS and dependencies. To identify spelling errors, TLE follows the decision in the underlying error-annotated corpus (Nicholls, 2003). Further, when a word is mistakenly segmented into two (e.g., “*be cause”), it uses the UD relation goeswith to connect them. 2.2 “most common usage” of the original word, if different from the POS tag, is indicated in the TYPO field of the metadata; there, “disappoint” is marked as a verb. 2.3 Depend"
W17-0408,W13-1703,0,0.0574168,"elines for standard English; SALLE is based on the POS tagset in the SUSANNE Corpus (Sampson, 1995) and dependency relations in CHILDES (Sagae et al., 2010). Both treebanks adopt the principle of “literal annotation”, i.e., to annotate according to a literal reading of the sentence, and to avoid considering its “intended” meaning or target hypothesis. A learner corpus consists of texts written by nonnative speakers. Recent years have seen a rising number of learner corpora, many of which are error-tagged to support analysis of grammatical mistakes made by learners (Yannakoudakis et al., 2011; Dahlmeier et al., 2013; Lee et al., 2016b). In order to derive overuse and underuse statistics on syntactic structures, some corpus have also been part-of-speech (POS) tagged (Díaz-Negrillo et al., 2010; Reznicek et al., 2013), and syntactically analyzed (Ragheb and Dickinson, 2014; Berzak et al., 2016). These corpora are valuable as training data for robust parsing of learner texts (Geertzen et al., 2013; Rehbein et al., 2012; Napoles et al., 2016), and can also benefit a variety of downstream tasks, including grammatical error correction, learner proficiency identification, and language learning exercise generati"
W17-0408,W15-0614,0,0.0607955,"agged (Díaz-Negrillo et al., 2010; Reznicek et al., 2013), and syntactically analyzed (Ragheb and Dickinson, 2014; Berzak et al., 2016). These corpora are valuable as training data for robust parsing of learner texts (Geertzen et al., 2013; Rehbein et al., 2012; Napoles et al., 2016), and can also benefit a variety of downstream tasks, including grammatical error correction, learner proficiency identification, and language learning exercise generation. While most annotation efforts have focused on learner English, a number of large learner Chinese corpora have also been compiled (Zhang, 2009; Wang et al., 2015; Lee et al., 2016a). However, POS analysis in these corpora has been limited to the erroneous words, and there has not yet been any attempt to annotate syntactic structures. This study presents the first attempt to annotate Chinese learner text in the Universal Dependencies (UD) 2.1 Lemma SALLE allows an exception to “literal annotation” when dealing with lexical violations. When there is a spelling error (e.g., “*ballence”), the annotator puts the intended, or corrected form of the word (“balance”) as lemma. For real-word spelling errors, the distinction between a word selection error and sp"
W17-0408,P11-1019,0,0.0336103,"ndarin Chinese to take interlanguage characteristics into account. We applied the scheme to a set of 100 sentences written by learners of Chinese as a foreign language, and we report inter-annotator agreement on syntactic annotation. 2 Previous work 1 Introduction Two major treebanks for learner language — the Treebank of Learner English (TLE) (Berzak et al., 2016) and the project on Syntactically Annotating Learner Language of English (SALLE) (Ragheb and Dickinson, 2014) — contain English texts written by non-native speakers. TLE annotates a subset of sentences from the Cambridge FCE corpus (Yannakoudakis et al., 2011), while SALLE has been applied on essays written by university students. They both adapt annotation guidelines for standard English: TLE is based on the UD guidelines for standard English; SALLE is based on the POS tagset in the SUSANNE Corpus (Sampson, 1995) and dependency relations in CHILDES (Sagae et al., 2010). Both treebanks adopt the principle of “literal annotation”, i.e., to annotate according to a literal reading of the sentence, and to avoid considering its “intended” meaning or target hypothesis. A learner corpus consists of texts written by nonnative speakers. Recent years have se"
W17-0408,W16-5403,1,0.438637,"lù ‘green’, normally an adjective, serves as a causative verb in this sentence. It is therefore tagged as “adjective used as a verb”. TLE also supplies similar information for spelling and word formation errors, but in a different format. Consider the phrase “a *disappoint unknown actor”. On the one hand, the POS tag reflects the “intended” usage, and so “disappoint” is tagged as an adjective on the basis of its target hypothesis “disappointing”. On the other hand, the 1 English 3 Proposed annotation scheme Our proposed scheme for learner Chinese is based on a UD scheme for Mandarin Chinese (Leung et al., 2016). We adapt this scheme in terms of word segmentation (Section 3.1), POS tagging (Section 3.2) and dependency annotation (Section 3.3). We follow SALLE and TLE in adhering to the principle of “literal annotation”, with some exceptions to be discussed below. 3.1 Word segmentation There are no word boundaries in written Chinese; the first step of analysis is thus to perform word segmentation. “Literal annotation” demands an analysis “as if the sentence were as syntactically well-formed as it can be, possibly ignoring meaning” (Ragheb and Dickinson, 2014). As a rule of thumb, we avoid segmentation"
W17-0408,P16-1173,0,0.414446,"se two tags should agree; in learner text, however, there may be conflicts between the morphological evidence and the distributional evidence. Consider the word “see” in the sentence “*I have see the movie.” The spelling of “see” provides morphological evidence to interpret it as base form (VV0). However, its word position, following the auxiliary “have”, points towards a past participle (VVN). It is thus assigned the morphological tag VV0 and the distributional tag VVN. These two kinds of POS tags are similarly incorporated into a constituent treebank of learner English (Nagata et al., 2011; Nagata and Sakaguchi, 2016). They are also implicitly encoded in a POS tagset designed for Classical Chinese poems (Wang, 2003). This tagset includes, for example, “adjective used as verb”, which can be understood as a morphological tag for adjective doubling as a distributional tag for verb. Consider the sentence 春風又綠江南岸 chūnfēng yòu lù jiāngnán àn “Spring wind again greens Yangtze’s southern shore”1 . The word lù ‘green’, normally an adjective, serves as a causative verb in this sentence. It is therefore tagged as “adjective used as a verb”. TLE also supplies similar information for spelling and word formation errors,"
W17-0408,P11-1121,0,0.0736803,"-formed sentence, these two tags should agree; in learner text, however, there may be conflicts between the morphological evidence and the distributional evidence. Consider the word “see” in the sentence “*I have see the movie.” The spelling of “see” provides morphological evidence to interpret it as base form (VV0). However, its word position, following the auxiliary “have”, points towards a past participle (VVN). It is thus assigned the morphological tag VV0 and the distributional tag VVN. These two kinds of POS tags are similarly incorporated into a constituent treebank of learner English (Nagata et al., 2011; Nagata and Sakaguchi, 2016). They are also implicitly encoded in a POS tagset designed for Classical Chinese poems (Wang, 2003). This tagset includes, for example, “adjective used as verb”, which can be understood as a morphological tag for adjective doubling as a distributional tag for verb. Consider the sentence 春風又綠江南岸 chūnfēng yòu lù jiāngnán àn “Spring wind again greens Yangtze’s southern shore”1 . The word lù ‘green’, normally an adjective, serves as a causative verb in this sentence. It is therefore tagged as “adjective used as a verb”. TLE also supplies similar information for spelli"
W17-0408,W16-0501,0,0.0130127,"have seen a rising number of learner corpora, many of which are error-tagged to support analysis of grammatical mistakes made by learners (Yannakoudakis et al., 2011; Dahlmeier et al., 2013; Lee et al., 2016b). In order to derive overuse and underuse statistics on syntactic structures, some corpus have also been part-of-speech (POS) tagged (Díaz-Negrillo et al., 2010; Reznicek et al., 2013), and syntactically analyzed (Ragheb and Dickinson, 2014; Berzak et al., 2016). These corpora are valuable as training data for robust parsing of learner texts (Geertzen et al., 2013; Rehbein et al., 2012; Napoles et al., 2016), and can also benefit a variety of downstream tasks, including grammatical error correction, learner proficiency identification, and language learning exercise generation. While most annotation efforts have focused on learner English, a number of large learner Chinese corpora have also been compiled (Zhang, 2009; Wang et al., 2015; Lee et al., 2016a). However, POS analysis in these corpora has been limited to the erroneous words, and there has not yet been any attempt to annotate syntactic structures. This study presents the first attempt to annotate Chinese learner text in the Universal Depe"
W17-5015,P16-1093,1,0.845672,"ntically close, which can be quantified with semantic distance in WordNet (Lin et al., 2007; Pino et al., 2008; Chen et al., 2015; Susanti et al., 2015), thesauri (Sumita et al., 2005; Smith et al., 2010), ontologies (Karamanis et al., 2006; Ding and Gu, 2010), or handcrafted rules (Chen et al., 2006). Another approach generates distractors that are semantically similar to the target word in some sense, but not in the particular sense in the carrier sentence (Zesch and Melamud, 2014). Others directly extract frequent mistakes in learner corpora to serve as distractors (Sakaguchi et al., 2013; Lee et al., 2016). Error-annotated Chinese learner corpora are still not large enough, however, to support broadcoverage distractor generation. A second, often competing objective is to ensure that the distractor, however plausible, is not an acceptable answer. Most approaches require that the distractor never, or only rarely, collocate with other words in the carrier sentence. Some define collocation as n-grams in a context window centered on the distractor (Liu et al., 2005). Others also consider words elsewhere in the carrier sentence, for example those present in the Word Sketch of the distractor (Smith et"
W17-5015,P03-1056,0,0.0586384,"t of our knowledge, there is not yet any reported attempt to generate distractors for learning Chinese vocabulary. The only previous work on Chinese distractor generation was designed for testing knowledge in the aviation domain, and leveraged a domain-specific ontology (Ding and Wiki Corpus We extracted 14 million sentences from Chinese Wikipedia for calculating word frequency, similarity and co-occurrence statistics in the Candidate Generation step. We then performed word segmentation, POS tagging and dependency analysis on a subset of 5.5 million sentences with the Stanford Chinese parser (Levy and Manning, 2003) for use in the Candidate Filtering step. 4 Approach We follow a two-step process where the first step, Candidate Generation, optimizes distractor plausibility; and the second step, Candidate Filtering, aims to filter out distractor candidates that are acceptable answers. 4.1 Candidate Generation We implemented the following criteria for generating a ranked list of distractor candidates: Baseline (Baseline) The baseline re-implements the criteria proposed by Coniam (1997): the distractor must have the same POS and the similar difficulty level as the target word. We extract all words in the Wik"
W17-5015,H05-1103,0,0.135718,"Missing"
W17-5015,P06-4001,0,0.778426,"ust attempt a trade-off between two objectives. One objective is plausibility. Most approaches require the distractor and the target word to have the same part-of-speech (POS) and similar level of difficulty, often approximated by word frequency (Coniam, 1997; Shei, 2001; Brown et al., 2005). They must also be semantically close, which can be quantified with semantic distance in WordNet (Lin et al., 2007; Pino et al., 2008; Chen et al., 2015; Susanti et al., 2015), thesauri (Sumita et al., 2005; Smith et al., 2010), ontologies (Karamanis et al., 2006; Ding and Gu, 2010), or handcrafted rules (Chen et al., 2006). Another approach generates distractors that are semantically similar to the target word in some sense, but not in the particular sense in the carrier sentence (Zesch and Melamud, 2014). Others directly extract frequent mistakes in learner corpora to serve as distractors (Sakaguchi et al., 2013; Lee et al., 2016). Error-annotated Chinese learner corpora are still not large enough, however, to support broadcoverage distractor generation. A second, often competing objective is to ensure that the distractor, however plausible, is not an acceptable answer. Most approaches require that the distrac"
W17-5015,W15-4406,0,0.254805,"distractor shares a common character with the target word. 3 Data To facilitate our study, we compiled two datasets: Previous work An algorithm for generating distractors must attempt a trade-off between two objectives. One objective is plausibility. Most approaches require the distractor and the target word to have the same part-of-speech (POS) and similar level of difficulty, often approximated by word frequency (Coniam, 1997; Shei, 2001; Brown et al., 2005). They must also be semantically close, which can be quantified with semantic distance in WordNet (Lin et al., 2007; Pino et al., 2008; Chen et al., 2015; Susanti et al., 2015), thesauri (Sumita et al., 2005; Smith et al., 2010), ontologies (Karamanis et al., 2006; Ding and Gu, 2010), or handcrafted rules (Chen et al., 2006). Another approach generates distractors that are semantically similar to the target word in some sense, but not in the particular sense in the carrier sentence (Zesch and Melamud, 2014). Others directly extract frequent mistakes in learner corpora to serve as distractors (Sakaguchi et al., 2013; Lee et al., 2016). Error-annotated Chinese learner corpora are still not large enough, however, to support broadcoverage distract"
W17-5015,W05-0201,0,0.584179,"Zesch and Melamud, 2014). Others directly extract frequent mistakes in learner corpora to serve as distractors (Sakaguchi et al., 2013; Lee et al., 2016). Error-annotated Chinese learner corpora are still not large enough, however, to support broadcoverage distractor generation. A second, often competing objective is to ensure that the distractor, however plausible, is not an acceptable answer. Most approaches require that the distractor never, or only rarely, collocate with other words in the carrier sentence. Some define collocation as n-grams in a context window centered on the distractor (Liu et al., 2005). Others also consider words elsewhere in the carrier sentence, for example those present in the Word Sketch of the distractor (Smith et al., 2010) or those that are grammatically related to the distractor in dependencies (Sakaguchi et al., 2013). Still others restrict potential distractors to antonyms of the target word, words with the same hypernym, and synonym of synonyms in WordNet (Knoop and Wilske, 2013). To the best of our knowledge, there is not yet any reported attempt to generate distractors for learning Chinese vocabulary. The only previous work on Chinese distractor generation was"
W17-5015,pho-etal-2014-multiple,0,0.329271,"tor should belong to the same word class and same difficult level, and have approximately the same length, as the target word (Heaton, 1989); it should collocate strongly with a word in the sentence (Hoshino, 2013); and it should be semantically related with the target word, ideally a 1 原因 yuanyin ‘reason’ 頻道 pindao ‘channel’ 條約 tiaoyue ‘agreement’ 函數 hanshu ‘function’ 因素 yinsu ‘factor’ ← Target word ↓ Distractors Human Baseline +Spell +Co-occur +Similar “false synonym” (Goodrich, 1977). An empirical study confirmed that distractors indeed tend to be syntactically and semantically homogenous (Pho et al., 2014). To automate the time-consuming process of selecting distractors, there has been much interest in developing algorithms that, given a carrier sentence and a target word, can find appropriate distractors. To-date, most research effort on distractor generation for language learning has focused on English. This paper presents the first attempt to automatically generate distractors in fill-in-the-blank items for learners of Chinese as a foreign language. In Section 2, we review related research areas. In Section 3, we present our datasets. In Section 4, we outline our criteria for distractor gene"
W17-5015,P13-2043,0,0.247555,". They must also be semantically close, which can be quantified with semantic distance in WordNet (Lin et al., 2007; Pino et al., 2008; Chen et al., 2015; Susanti et al., 2015), thesauri (Sumita et al., 2005; Smith et al., 2010), ontologies (Karamanis et al., 2006; Ding and Gu, 2010), or handcrafted rules (Chen et al., 2006). Another approach generates distractors that are semantically similar to the target word in some sense, but not in the particular sense in the carrier sentence (Zesch and Melamud, 2014). Others directly extract frequent mistakes in learner corpora to serve as distractors (Sakaguchi et al., 2013; Lee et al., 2016). Error-annotated Chinese learner corpora are still not large enough, however, to support broadcoverage distractor generation. A second, often competing objective is to ensure that the distractor, however plausible, is not an acceptable answer. Most approaches require that the distractor never, or only rarely, collocate with other words in the carrier sentence. Some define collocation as n-grams in a context window centered on the distractor (Liu et al., 2005). Others also consider words elsewhere in the carrier sentence, for example those present in the Word Sketch of the d"
W17-5015,W06-1416,0,0.682078,"datasets: Previous work An algorithm for generating distractors must attempt a trade-off between two objectives. One objective is plausibility. Most approaches require the distractor and the target word to have the same part-of-speech (POS) and similar level of difficulty, often approximated by word frequency (Coniam, 1997; Shei, 2001; Brown et al., 2005). They must also be semantically close, which can be quantified with semantic distance in WordNet (Lin et al., 2007; Pino et al., 2008; Chen et al., 2015; Susanti et al., 2015), thesauri (Sumita et al., 2005; Smith et al., 2010), ontologies (Karamanis et al., 2006; Ding and Gu, 2010), or handcrafted rules (Chen et al., 2006). Another approach generates distractors that are semantically similar to the target word in some sense, but not in the particular sense in the carrier sentence (Zesch and Melamud, 2014). Others directly extract frequent mistakes in learner corpora to serve as distractors (Sakaguchi et al., 2013; Lee et al., 2016). Error-annotated Chinese learner corpora are still not large enough, however, to support broadcoverage distractor generation. A second, often competing objective is to ensure that the distractor, however plausible, is not"
W17-5015,W05-0210,0,0.757098,"word. 3 Data To facilitate our study, we compiled two datasets: Previous work An algorithm for generating distractors must attempt a trade-off between two objectives. One objective is plausibility. Most approaches require the distractor and the target word to have the same part-of-speech (POS) and similar level of difficulty, often approximated by word frequency (Coniam, 1997; Shei, 2001; Brown et al., 2005). They must also be semantically close, which can be quantified with semantic distance in WordNet (Lin et al., 2007; Pino et al., 2008; Chen et al., 2015; Susanti et al., 2015), thesauri (Sumita et al., 2005; Smith et al., 2010), ontologies (Karamanis et al., 2006; Ding and Gu, 2010), or handcrafted rules (Chen et al., 2006). Another approach generates distractors that are semantically similar to the target word in some sense, but not in the particular sense in the carrier sentence (Zesch and Melamud, 2014). Others directly extract frequent mistakes in learner corpora to serve as distractors (Sakaguchi et al., 2013; Lee et al., 2016). Error-annotated Chinese learner corpora are still not large enough, however, to support broadcoverage distractor generation. A second, often competing objective is"
W17-5015,W14-1817,0,0.449479,"similar level of difficulty, often approximated by word frequency (Coniam, 1997; Shei, 2001; Brown et al., 2005). They must also be semantically close, which can be quantified with semantic distance in WordNet (Lin et al., 2007; Pino et al., 2008; Chen et al., 2015; Susanti et al., 2015), thesauri (Sumita et al., 2005; Smith et al., 2010), ontologies (Karamanis et al., 2006; Ding and Gu, 2010), or handcrafted rules (Chen et al., 2006). Another approach generates distractors that are semantically similar to the target word in some sense, but not in the particular sense in the carrier sentence (Zesch and Melamud, 2014). Others directly extract frequent mistakes in learner corpora to serve as distractors (Sakaguchi et al., 2013; Lee et al., 2016). Error-annotated Chinese learner corpora are still not large enough, however, to support broadcoverage distractor generation. A second, often competing objective is to ensure that the distractor, however plausible, is not an acceptable answer. Most approaches require that the distractor never, or only rarely, collocate with other words in the carrier sentence. Some define collocation as n-grams in a context window centered on the distractor (Liu et al., 2005). Other"
W17-5903,P06-4001,0,0.0960878,"Missing"
W17-5903,W10-1002,0,0.084044,"Missing"
W17-5903,O13-3001,0,0.0344223,"Missing"
W17-5903,W14-1821,0,0.055646,"Missing"
W17-5903,C10-1062,0,0.032403,"Missing"
W17-5903,D08-1020,0,0.085642,"Missing"
W17-5903,P13-2043,0,0.0370225,"Missing"
W17-5903,P05-1065,0,0.147334,"Missing"
W17-5903,P03-1056,0,0.013415,"d by the system as heuristics. We first describe baseline features (Section 3.1) inspired by Volodina et al. (2012), and then investigate word cooccurrence and lexical similarity statistics (Section 3.2). To tune the heuristics, we compiled two datasets. The “Textbook Set” consists of 299 carrier sentences, drawn from fill-in-the-blank questions in three Chinese textbooks (Liu, 2004, 2010; Wang, 2007)1 . The “Wiki Set” contains 9.2 million sentences, harvested from Chinese Wikipedia. We performed word segmentation, part-of-speech tagging and syntactic parsing with the Stanford Chinese parser (Levy and Manning, 2003). 3 Features for carrier sentence selection Example sentence selection A dictionary entry of a word often includes an example sentence. Various criteria for selecting an example sentence for dictionaries have been proposed (Kilgarriff et al., 2008; Didakowski et al., 2012). As far as the sentence is concerned, it must be authentic, complete, well-formed, selfcontained, and not too complex. As for the target word, it should not be used as a proper noun, or in a metaphoric or abstract sense; further, it should co-occur often with, and be semantically related to, one or more words in the rest of"
W17-5903,W05-0201,0,0.115915,"Missing"
W17-5903,W05-0210,0,0.125484,"Missing"
W17-5903,W14-1817,0,0.0379341,"Missing"
W17-6306,I05-6009,0,0.0287824,"rs but does not provide more fine-grained distinctions as in Jiang (2009). Learner corpora and L1-L2 parallel treebanks A major function of a learner corpus is to facilitate retrieval of sentences with specific errors. We first discuss the limitations of the use of error tags (Section 2.1), and then propose tree search in an L1-L2 parallel treebank as an alternative approach (Section 2.2). 2.1 Error tags Errors in a learner sentence are commonly marked with error tags. Each tag labels a problematic text span with an error category, and often also provides a corrected version of the text span (Izumi et al., 2005; Zhang, 2009; Yannakoudakis et al., 2011; Dahlmeier et al., 2013). For example, the Cambridge Learner Corpus uses XML tags to mark error categories (Nicholls, 2003), and supplements the original text with a vertical bar and the target hypothesis: 2.1.2 Corpus interoperability Since existing error tagsets vary widely in granularity, it is difficult to combine information from multiple learner corpora. To cite but a few examples, NUCLE (Dahlmeier et al., 2013) uses a tagset with 27 error categories; the NICT Japanese Learner English Corpus has 46 tags (Izumi et al., 2005); while different combi"
W17-6306,W17-0408,1,0.910619,"ese as a foreign language. We report precision and recall in retrieving a range of word-order error categories from L1-L2 tree pairs annotated in the Universal Dependency framework. 1 nsubj POS tag: PRON L2: 我 wo L1: 我 wo ‘I’ POS tag: PRON obl:tmod VERB NOUN 起床 qichuang 七點 qidian 七點 起床 qidian qichuang ‘7 o’clock’ ‘wake up’ NOUN VERB obl:tmod nsubj root Figure 1: An example L1-L2 tree pair, including word alignments between the learner sentence (“L2”) and its target hypothesis (“L1”), and the parse trees of the two sentences, annotated in Universal Dependencies for Chinese (Leung et al., 2016; Lee et al., 2017). Introduction A parallel treebank consists of multiple treebanks with alignments at the sentence level, and often also at the phrase and word levels. Growing interest in parallel treebanks have yielded treebanks of many language combinations (Čmejrek et al., 2004; Megyesi et al., 2010; Sulger et al., 2013; Volk et al., 2017). So far, there has been no reported attempt to build an L1-L2 parallel treebank — i.e., parse trees of sentences written by non-native speakers (henceforth, “L2 sentences”), aligned to parse trees of their target hypotheses (henceforth, “L1 sentences”). Figure 1 shows an"
W17-6306,W16-5403,1,0.600634,"by learners of Chinese as a foreign language. We report precision and recall in retrieving a range of word-order error categories from L1-L2 tree pairs annotated in the Universal Dependency framework. 1 nsubj POS tag: PRON L2: 我 wo L1: 我 wo ‘I’ POS tag: PRON obl:tmod VERB NOUN 起床 qichuang 七點 qidian 七點 起床 qidian qichuang ‘7 o’clock’ ‘wake up’ NOUN VERB obl:tmod nsubj root Figure 1: An example L1-L2 tree pair, including word alignments between the learner sentence (“L2”) and its target hypothesis (“L1”), and the parse trees of the two sentences, annotated in Universal Dependencies for Chinese (Leung et al., 2016; Lee et al., 2017). Introduction A parallel treebank consists of multiple treebanks with alignments at the sentence level, and often also at the phrase and word levels. Growing interest in parallel treebanks have yielded treebanks of many language combinations (Čmejrek et al., 2004; Megyesi et al., 2010; Sulger et al., 2013; Volk et al., 2017). So far, there has been no reported attempt to build an L1-L2 parallel treebank — i.e., parse trees of sentences written by non-native speakers (henceforth, “L2 sentences”), aligned to parse trees of their target hypotheses (henceforth, “L1 sentences”)."
W17-6306,megyesi-etal-2010-english,0,0.025133,"起床 qidian qichuang ‘7 o’clock’ ‘wake up’ NOUN VERB obl:tmod nsubj root Figure 1: An example L1-L2 tree pair, including word alignments between the learner sentence (“L2”) and its target hypothesis (“L1”), and the parse trees of the two sentences, annotated in Universal Dependencies for Chinese (Leung et al., 2016; Lee et al., 2017). Introduction A parallel treebank consists of multiple treebanks with alignments at the sentence level, and often also at the phrase and word levels. Growing interest in parallel treebanks have yielded treebanks of many language combinations (Čmejrek et al., 2004; Megyesi et al., 2010; Sulger et al., 2013; Volk et al., 2017). So far, there has been no reported attempt to build an L1-L2 parallel treebank — i.e., parse trees of sentences written by non-native speakers (henceforth, “L2 sentences”), aligned to parse trees of their target hypotheses (henceforth, “L1 sentences”). Figure 1 shows an example parse tree pair in such a treebank. The pair consists of the parse tree of a Chinese sentence written by a learner, and the parse tree of its corrected version, or “target hypothesis”. Although a number of L2 treebanks have been built, they either do not provide explicit target"
W17-6306,W13-1703,0,0.060399,"Jiang (2009). Learner corpora and L1-L2 parallel treebanks A major function of a learner corpus is to facilitate retrieval of sentences with specific errors. We first discuss the limitations of the use of error tags (Section 2.1), and then propose tree search in an L1-L2 parallel treebank as an alternative approach (Section 2.2). 2.1 Error tags Errors in a learner sentence are commonly marked with error tags. Each tag labels a problematic text span with an error category, and often also provides a corrected version of the text span (Izumi et al., 2005; Zhang, 2009; Yannakoudakis et al., 2011; Dahlmeier et al., 2013). For example, the Cambridge Learner Corpus uses XML tags to mark error categories (Nicholls, 2003), and supplements the original text with a vertical bar and the target hypothesis: 2.1.2 Corpus interoperability Since existing error tagsets vary widely in granularity, it is difficult to combine information from multiple learner corpora. To cite but a few examples, NUCLE (Dahlmeier et al., 2013) uses a tagset with 27 error categories; the NICT Japanese Learner English Corpus has 46 tags (Izumi et al., 2005); while different combinations in the Cambridge Learner Corpus tagset can recognize up to"
W17-6306,P16-1173,0,0.117194,". So far, there has been no reported attempt to build an L1-L2 parallel treebank — i.e., parse trees of sentences written by non-native speakers (henceforth, “L2 sentences”), aligned to parse trees of their target hypotheses (henceforth, “L1 sentences”). Figure 1 shows an example parse tree pair in such a treebank. The pair consists of the parse tree of a Chinese sentence written by a learner, and the parse tree of its corrected version, or “target hypothesis”. Although a number of L2 treebanks have been built, they either do not provide explicit target hypotheses (Ragheb and Dickinson, 2014; Nagata and Sakaguchi, 2016), or have not yet provided parse trees for the target hypotheses (Berzak et al., 2016). Parallel L1-L2 treebanks can be expected to serve a number of research agendas. First, they would support quantitative studies in Contrastive Interlanguage Analysis (CIA) (Granger, 2015) and Error Analysis (EA). For CIA, they would enable comparisons between native and interlanguages not only on the lexical level but also on the syntactic level. For EA, parallel parse trees would give more fine-grained characterization of the syntactic environment in which learner errors occur, which can inform the design o"
W17-6306,C16-1079,0,0.0123696,"air of parse tree patterns with alignments (Table 1), can be viewed as a dynamically defined error category. The idea of leveraging linguistic annotations to search for learner errors is not new. As noted by Reznicek et al. (2013), when both learner sentences and target hypotheses are POS-tagged and word-aligned, a search query with constraints on POS and word positions can effectively express an error category. This approach is becoming more widely applicable, as more learner corpora are enriched with POS annotation (Lüdeling et al., 2008; Díaz-Negrillo et al., 2010) and enhanced alignments (Felice et al., 2016). Many learner errors, however, cannot be adequately specified with POS alone. Take subjectverb agreement as an example. It does not suffice to search for two aligned verbs with different tags (e.g., VB and VBZ), since the change in conjugation may be a result of other errors (e.g., noun number). The tree query in Table 1(a) provides a more precise and transparent definition of the error. It requires the aligned verbs in both the L1 and L2 sentences to have a singular noun (NN) as subject. Hence, it specifically targets the subjectverb agreement error where the learner mistakes the root form o"
W17-6306,P16-1208,0,0.0174048,"ment in which learner errors occur, which can inform the design of language teaching peda44 Proceedings of the 15th International Conference on Parsing Technologies, pages 44–49, c Pisa, Italy; September 20–22, 2017. 2017 Association for Computational Linguistics gogy. Further, just as parallel treebanks can help train machine translation (MT) systems (Čmejrek et al., 2004; Sennrich, 2015), L1-L2 treebanks can supply sentence pairs to train systems for automatic grammatical error correction (GEC). Indeed, some GEC systems obtained state-of-the-art results by casting the task as an MT problem (Rozovskaya and Roth, 2016; Junczys-Dowmunt and Grundkiewicz, 2014). In this opinion paper, we focus on demonstrating how L1-L2 parallel treebanks can benefit learner language analysis. In the next section, we argue that these treebanks can better facilitate re-use and interoperability among learner corpora, because they provide a more precise and flexible encoding of learner errors. As a proof of concept, Section 3 presents a case study on identifying different word-order errors in Chinese L1-L2 parallel trees. Finally, Section 4 concludes. 2 difficult, or perhaps impossible, to develop a robust and general-purpose er"
W17-6306,Q15-1013,0,0.0266499,"y would enable comparisons between native and interlanguages not only on the lexical level but also on the syntactic level. For EA, parallel parse trees would give more fine-grained characterization of the syntactic environment in which learner errors occur, which can inform the design of language teaching peda44 Proceedings of the 15th International Conference on Parsing Technologies, pages 44–49, c Pisa, Italy; September 20–22, 2017. 2017 Association for Computational Linguistics gogy. Further, just as parallel treebanks can help train machine translation (MT) systems (Čmejrek et al., 2004; Sennrich, 2015), L1-L2 treebanks can supply sentence pairs to train systems for automatic grammatical error correction (GEC). Indeed, some GEC systems obtained state-of-the-art results by casting the task as an MT problem (Rozovskaya and Roth, 2016; Junczys-Dowmunt and Grundkiewicz, 2014). In this opinion paper, we focus on demonstrating how L1-L2 parallel treebanks can benefit learner language analysis. In the next section, we argue that these treebanks can better facilitate re-use and interoperability among learner corpora, because they provide a more precise and flexible encoding of learner errors. As a p"
W17-6306,cmejrek-etal-2004-prague,0,0.264021,"qichuang 七點 qidian 七點 起床 qidian qichuang ‘7 o’clock’ ‘wake up’ NOUN VERB obl:tmod nsubj root Figure 1: An example L1-L2 tree pair, including word alignments between the learner sentence (“L2”) and its target hypothesis (“L1”), and the parse trees of the two sentences, annotated in Universal Dependencies for Chinese (Leung et al., 2016; Lee et al., 2017). Introduction A parallel treebank consists of multiple treebanks with alignments at the sentence level, and often also at the phrase and word levels. Growing interest in parallel treebanks have yielded treebanks of many language combinations (Čmejrek et al., 2004; Megyesi et al., 2010; Sulger et al., 2013; Volk et al., 2017). So far, there has been no reported attempt to build an L1-L2 parallel treebank — i.e., parse trees of sentences written by non-native speakers (henceforth, “L2 sentences”), aligned to parse trees of their target hypotheses (henceforth, “L1 sentences”). Figure 1 shows an example parse tree pair in such a treebank. The pair consists of the parse tree of a Chinese sentence written by a learner, and the parse tree of its corrected version, or “target hypothesis”. Although a number of L2 treebanks have been built, they either do not p"
W17-6306,P11-1019,0,0.0232671,"-grained distinctions as in Jiang (2009). Learner corpora and L1-L2 parallel treebanks A major function of a learner corpus is to facilitate retrieval of sentences with specific errors. We first discuss the limitations of the use of error tags (Section 2.1), and then propose tree search in an L1-L2 parallel treebank as an alternative approach (Section 2.2). 2.1 Error tags Errors in a learner sentence are commonly marked with error tags. Each tag labels a problematic text span with an error category, and often also provides a corrected version of the text span (Izumi et al., 2005; Zhang, 2009; Yannakoudakis et al., 2011; Dahlmeier et al., 2013). For example, the Cambridge Learner Corpus uses XML tags to mark error categories (Nicholls, 2003), and supplements the original text with a vertical bar and the target hypothesis: 2.1.2 Corpus interoperability Since existing error tagsets vary widely in granularity, it is difficult to combine information from multiple learner corpora. To cite but a few examples, NUCLE (Dahlmeier et al., 2013) uses a tagset with 27 error categories; the NICT Japanese Learner English Corpus has 46 tags (Izumi et al., 2005); while different combinations in the Cambridge Learner Corpus t"
W17-6307,P14-1041,0,0.0130537,"undergone task-based evaluations, such as reading comprehension (Angrosh et al., 2014), most have adopted holistic assessment, which commonly includes human ratings on the grammaticality, fluency, meaning preservation, and simplicity of the ˇ system output (Stajner et al., 2016). These ratings are indeed helpful in indicating the overall quality of a system; however, the need for human intervention restricts the scale of the evaluations, and makes direct comparisons difficult. Other systems have been evaluated with automatic metrics, such as BLEU and readability metrics (Aluisio et al., 2010; Narayan and Gardent, 2014), which overcome the limitations of human ratings, but do not reveal what aspects of the simplification process caused the most difficulties. The contribution of this paper is two-fold. First, it presents the first publicly available dataset that facilitates detailed, automatic evaluation on syntactic simplification. Second, we report the results of a decision tree approach that incorporates parse features, giving a detailed analysis on its performance for various constructs. Introduction The task of text simplification aims to rewrite a sentence so as to reduce its complexity, while preservin"
W17-6307,W14-5601,0,0.0520191,"Missing"
W17-6307,W03-1602,0,0.0856486,"o-date on this task, using a dataset of sentences that exhibit simplification based on coordination, subordination, punctuation/parataxis, adjectival clauses, participial phrases, and appositive phrases. We train a decision tree with features derived from text span length, POS tags and dependency relations, and show that it significantly outperforms a parser-only baseline. S1 S2 The man, carrying numerous books, entered the room. The man entered the room. He was carrying numerous books. Table 1: Example input (S) and output sentences (S1 , S2 ) for the task of syntactic simplification. 2002a; Inui et al., 2003; Belder and Moens, 2010; Bott et al., 2012; Siddharthan and Angrosh, 2014; Saggion et al., 2015). While some systems have undergone task-based evaluations, such as reading comprehension (Angrosh et al., 2014), most have adopted holistic assessment, which commonly includes human ratings on the grammaticality, fluency, meaning preservation, and simplicity of the ˇ system output (Stajner et al., 2016). These ratings are indeed helpful in indicating the overall quality of a system; however, the need for human intervention restricts the scale of the evaluations, and makes direct comparisons diffic"
W17-6307,P13-1151,0,0.0479255,"d relative clauses, as well as conversion of passive voice to active voice. Siddharthan and Angorsh (2014) used 26 hand-crafted rules for apposition, relative clauses and combinations of the two; as well as 85 rules handle subordination and 3 Data Many evaluation datasets are available for lexical simplification (Paetzold and Specia, 2016), but there is not yet any that enables automatic evaluation on syntactic simplification systems. We created an annotated dataset for this purpose, based on the 167,689 pairs of “normal” and simplified sentences from Wikipedia and Simple Wikipedia aligned by Kauchak (2013). While a majority of these pairs are one-to-one alignments, 23,715 of them are one-to-two alignments.2 These aligned sentences, in their raw form, can serve as triplets of S, S1 and S2 (Table 1). However, as pointed out by Xu et al. (2015), Wikipedia and Simple Wikipedia contain rather noisy data; indeed, upon manual inspection, not all triplets are good examples for syntactic simplification. These fall into two cases. In the first case, significant content from S is deleted and appear neither in S1 nor S2 ; these triplets provide examples of content deletion rather than splitting. In the sec"
W17-6307,P14-5010,0,0.0117346,"s books.</S2&gt; Table 3: Each triplet in our corpus contains a complex sentence (S) and the two shorter sentences (S1 , S2 ) into which it was re-written. 3.1 4 Test set An annotator marked up 1,070 triplets of (S, S1 , S2 ) in the format shown in Table 3, with the following items:3 Baseline system. We manually developed tree patterns, in the form of dependency relations and POS tags, to identify text spans that should be removed from a complex sentence (Table 4). These patterns are designed to yield high recall but lower precision. The system parses the input sentence with the Stanford parser (Manning et al., 2014), and then performs breadth-first search on the dependency tree for these patterns, returning the first match it finds. This algorithm removes at most one text span from each complex sentence; this assumption is consistent with the material in our dataset, which was derived from one-to-two sentence alignments. Proposed system. Even if one of the constructs in Table 2 is present in a complex sentence, it may not be appropriate or worthwhile to remove it. To refine the tree patterns developed for the baseline system, we trained a decision tree with the scikitlearn package. For each word in the s"
W17-6307,L16-1491,0,0.022724,"and if so, generate an independent sentence from it. For example, Alu´ısio et al. (2008) used a set of transformation rules to treat 22 syntactic constructs. Siddharthan (2011) used 63 rules, which handle coordination of both verb phrases and full clauses, subordination, apposition and relative clauses, as well as conversion of passive voice to active voice. Siddharthan and Angorsh (2014) used 26 hand-crafted rules for apposition, relative clauses and combinations of the two; as well as 85 rules handle subordination and 3 Data Many evaluation datasets are available for lexical simplification (Paetzold and Specia, 2016), but there is not yet any that enables automatic evaluation on syntactic simplification systems. We created an annotated dataset for this purpose, based on the 167,689 pairs of “normal” and simplified sentences from Wikipedia and Simple Wikipedia aligned by Kauchak (2013). While a majority of these pairs are one-to-one alignments, 23,715 of them are one-to-two alignments.2 These aligned sentences, in their raw form, can serve as triplets of S, S1 and S2 (Table 1). However, as pointed out by Xu et al. (2015), Wikipedia and Simple Wikipedia contain rather noisy data; indeed, upon manual inspect"
W17-6307,W10-1001,0,0.0153339,"ile some systems have undergone task-based evaluations, such as reading comprehension (Angrosh et al., 2014), most have adopted holistic assessment, which commonly includes human ratings on the grammaticality, fluency, meaning preservation, and simplicity of the ˇ system output (Stajner et al., 2016). These ratings are indeed helpful in indicating the overall quality of a system; however, the need for human intervention restricts the scale of the evaluations, and makes direct comparisons difficult. Other systems have been evaluated with automatic metrics, such as BLEU and readability metrics (Aluisio et al., 2010; Narayan and Gardent, 2014), which overcome the limitations of human ratings, but do not reveal what aspects of the simplification process caused the most difficulties. The contribution of this paper is two-fold. First, it presents the first publicly available dataset that facilitates detailed, automatic evaluation on syntactic simplification. Second, we report the results of a decision tree approach that incorporates parse features, giving a detailed analysis on its performance for various constructs. Introduction The task of text simplification aims to rewrite a sentence so as to reduce its"
W17-6307,C14-1188,0,0.0139268,"e train a decision tree with features derived from text span length, POS tags and dependency relations, and show that it significantly outperforms a parser-only baseline. S1 S2 The man, carrying numerous books, entered the room. The man entered the room. He was carrying numerous books. Table 1: Example input (S) and output sentences (S1 , S2 ) for the task of syntactic simplification. 2002a; Inui et al., 2003; Belder and Moens, 2010; Bott et al., 2012; Siddharthan and Angrosh, 2014; Saggion et al., 2015). While some systems have undergone task-based evaluations, such as reading comprehension (Angrosh et al., 2014), most have adopted holistic assessment, which commonly includes human ratings on the grammaticality, fluency, meaning preservation, and simplicity of the ˇ system output (Stajner et al., 2016). These ratings are indeed helpful in indicating the overall quality of a system; however, the need for human intervention restricts the scale of the evaluations, and makes direct comparisons difficult. Other systems have been evaluated with automatic metrics, such as BLEU and readability metrics (Aluisio et al., 2010; Narayan and Gardent, 2014), which overcome the limitations of human ratings, but do no"
W17-6307,W11-2802,0,0.0692923,"Missing"
W17-6307,W12-2910,0,0.0161923,"tences that exhibit simplification based on coordination, subordination, punctuation/parataxis, adjectival clauses, participial phrases, and appositive phrases. We train a decision tree with features derived from text span length, POS tags and dependency relations, and show that it significantly outperforms a parser-only baseline. S1 S2 The man, carrying numerous books, entered the room. The man entered the room. He was carrying numerous books. Table 1: Example input (S) and output sentences (S1 , S2 ) for the task of syntactic simplification. 2002a; Inui et al., 2003; Belder and Moens, 2010; Bott et al., 2012; Siddharthan and Angrosh, 2014; Saggion et al., 2015). While some systems have undergone task-based evaluations, such as reading comprehension (Angrosh et al., 2014), most have adopted holistic assessment, which commonly includes human ratings on the grammaticality, fluency, meaning preservation, and simplicity of the ˇ system output (Stajner et al., 2016). These ratings are indeed helpful in indicating the overall quality of a system; however, the need for human intervention restricts the scale of the evaluations, and makes direct comparisons difficult. Other systems have been evaluated with"
W17-6307,E14-1076,0,0.246787,"simplification based on coordination, subordination, punctuation/parataxis, adjectival clauses, participial phrases, and appositive phrases. We train a decision tree with features derived from text span length, POS tags and dependency relations, and show that it significantly outperforms a parser-only baseline. S1 S2 The man, carrying numerous books, entered the room. The man entered the room. He was carrying numerous books. Table 1: Example input (S) and output sentences (S1 , S2 ) for the task of syntactic simplification. 2002a; Inui et al., 2003; Belder and Moens, 2010; Bott et al., 2012; Siddharthan and Angrosh, 2014; Saggion et al., 2015). While some systems have undergone task-based evaluations, such as reading comprehension (Angrosh et al., 2014), most have adopted holistic assessment, which commonly includes human ratings on the grammaticality, fluency, meaning preservation, and simplicity of the ˇ system output (Stajner et al., 2016). These ratings are indeed helpful in indicating the overall quality of a system; however, the need for human intervention restricts the scale of the evaluations, and makes direct comparisons difficult. Other systems have been evaluated with automatic metrics, such as BLE"
W17-6307,C96-2183,0,0.55862,"simplification, syntactic simplification, content deletion, and content insertion for clarification. This paper focuses on syntactic simplification and, specifically, on splitting a complex sentence into two simpler sentences.1 Consider the input sentence S in Table 1, a complex sentence containing a participial phrase, “carrying numerous books”. After removing this phrase from S, the system generates S2 from the phrase by turning the participle “carrying” into the finite form “was carrying”, and by generating the pronoun “he” as the subject. A number of systems can already perform this task (Chandrasekar et al., 1996; Siddharthan, 2 Previous Work The phrase-based and syntax-based machine translation approaches have been used in many text simplification systems (Vickrey and Koller, ∗ The second author completed this work as a Postdoctoral Fellow at City University of Hong Kong. 1 The simplified sentences can in turn be split iteratively. 50 Proceedings of the 15th International Conference on Parsing Technologies, pages 50–55, c Pisa, Italy; September 20–22, 2017. 2017 Association for Computational Linguistics Construct Coordination Adjectival clause Participial phrase Appositive phrase Subordination Punctu"
W17-6307,P16-1119,0,0.0382334,"Missing"
W17-6307,P08-1040,0,0.129751,"Missing"
W17-6307,P12-1107,0,0.10254,"Missing"
W17-6307,Q15-1021,0,0.0249487,"nd 3 Data Many evaluation datasets are available for lexical simplification (Paetzold and Specia, 2016), but there is not yet any that enables automatic evaluation on syntactic simplification systems. We created an annotated dataset for this purpose, based on the 167,689 pairs of “normal” and simplified sentences from Wikipedia and Simple Wikipedia aligned by Kauchak (2013). While a majority of these pairs are one-to-one alignments, 23,715 of them are one-to-two alignments.2 These aligned sentences, in their raw form, can serve as triplets of S, S1 and S2 (Table 1). However, as pointed out by Xu et al. (2015), Wikipedia and Simple Wikipedia contain rather noisy data; indeed, upon manual inspection, not all triplets are good examples for syntactic simplification. These fall into two cases. In the first case, significant content from S is deleted and appear neither in S1 nor S2 ; these triplets provide examples of content deletion rather than splitting. In the second case, S2 (or S1 ) consists mostly of new content. In some instances, S1 (or S2 ) is so similar to S that no real splitting occurs. In others, the new content put into doubt whether the splitting of S was motivated by syntactic complexit"
W17-6307,C10-1152,0,0.130716,"Missing"
W17-6530,W17-6508,1,0.871,"Missing"
W17-6530,W13-3711,1,0.841577,"language with this feature that is described in UD. The UD v1 guide has been completed during the ongoing annotation experience and then adapted to v2. The Mandarin-specific part of the UD documentation is currently one of the most complete language specific annotation descriptions2 The similarity of Cantonese and Mandarin makes it reasonable to conceive the Cantonese annotation guide on the basis of the Mandarin guide, with modifications wherever necessary. The development of this guide is work in progress. The whole semi-automatic annotation process is done in the Arborator annotation tool (Gerdes 2013), which allows blind and open annotation by multiple users as well as integrated parser bootstrapping possibilities 3.2 Outline UD has been conceived with a double objective: The parallel construction of the treebanks facilitates the developments of parsers and other NLP tools. And, more importantly for the present study, it allows studies in empirical comparative syntax. There are some caveats to this claim, some of which we will discuss later. But any comparative measure on the current UD treebanks will always measure either structural differences, genre differences of the underlying corpus,"
W17-6530,W16-1715,1,0.923775,"rparts can still be modified by verbal articles and have thus to be tagged and annotated differently (see section 2.4). The UD annotation scheme handles prepositions as case-markers, and thus as depenFigure 1: Analyses of two (semantically full) prepositions in UD 2.0 English, the first being a simple and the second a complex preposition dent from their argument, i.e. what is commonly called a prepositional object. This results in UD’s infamous “Turkish” analysis of English prepositions (Chris Manning, 2016, personal communication). Figure 1 shows the situation for English (example taken from Gerdes & Kahane 2016, updated to UD 2.0). The following pair of sentence segments illustrates this point for Chinese. The 1st person singular pronoun in the Mandarin tree 我 ‘wǒ’ is an obl:dobj that has a case-marker. In the Cantonese equivalent, what has been analyzed as a (verbal) preposition in Mandarin is now a coverb, which takes its argument as a regular direct object. Mandarin (sentence 0-7): Jiù then bùyào do.not bǎ OM wǒ 1SG dàng treat shì COP nǐ de nāishū 2SG REL youngest.uncle.on.paternal.side ‘Don’t treat me as your uncle.’ Cantonese: Dong treat ngóh 1SG haih COP léih ge lāaisuk 2SG REL youngest.uncle."
W17-6530,I11-1174,1,0.871392,"the existence of expletives in Cantonese (annotated with the relation name expl), which are completely absent from Mandarin. An example is 佢 kéuih ‘3SG’. The pronoun is part of a grammatical construction and actually does not refer to anything or anyone, the condition for qualifying as expletive.1 大家 飲勝 佢！ Daaihgā jámsing kéuih everyone cheers KEUHI ‘Everyone! Cheers (to it)!’ 我 不如 死 咗 佢 好過 啦! Ngóh bātyùh séi jó kéuih hóugwo lā 1SG had.better die PERF KEUIH better SFP ‘It would be better for me to die.’ 3 Treebank construction Our corpus is based on television programs broadcast in Hong Kong (Lee, 2011). The Cantonese text is thus semi-planned spoken text. Cantonese TV dramas are widely distributed in southern China and beyond and mostly have Mandarin subtitles. The annotation is still ongoing and the texts that still await annotation are taken from movies that are distributed on Youtube, which will ultimately allow transforming this part of the treebank into a completely free language resource since the creators agreed to the distribution of the language data. The spo1 For further details and examples see http://universaldependencies.org/yue/dep/expl.html 268 ken Cantonese was transcribed w"
W17-6530,de-marneffe-etal-2014-universal,0,0.0673479,"Missing"
W17-6530,L16-1262,0,0.0999057,"Missing"
W17-6530,W15-2128,0,0.012411,"al 31 1002 1345 26 204 226 11 443 619 9 40 40 3 193 286 2 25 33 1 12 17 1 80 125 1 76 125 0 27 45 0 15 24 0 1 3 0 1 1 0 1 3 0 2 3 0 2 4 -1 34 73 -1 40 75 -1 90 171 -1 0 2 -1 26 52 -1 17 33 -1 47 88 -1 38 76 -1 0 3 -1 53 99 -1 83 154 -1 84 161 -1 69 128 -2 91 184 -2 99 204 -2 393 726 -3 20 56 -3 362 707 -3 64 140 -4 62 148 -5 58 147 -6 56 145 -7 541 1087 -7 0 18 -14 80 245 Table 3: complete dependency relation frequencies ordered by specificity page. And although prepositions in English are considered by any syntactic analysis that we are aware of to be “crucially” different from case markers (Osborne 2015), UD decided to treat them just like Turkish case markers, leading to greater similarity between Turkish and English and at the same time to the structurally very different trees for simple and complex prepositions (Figure 1) A good syntactic annotation scheme would allow for slight structural differences to be reflected by slight differences in the annotation, for example in the case of Cantonese coverbs by a different categorization of the coverb, once as a verb and once as a preposition, but with identical dependency structures in both treebanks. The “Turkish” analysis of prepositions, on t"
W18-1809,W09-2307,0,0.0216363,"e use of modal verbs and predicative adjectives; the expression of epistemicity and comparative construction; the word order in double object constructions; and the system of sentence-final particles, which is significantly more complicated in Cantonese. Further, in a quantitative comparison between Mandarin and Cantonese, Wong et al. (2017) showed that Mandarin adverbs are replaced by Cantonese auxiliaries in a number of cases. Similar to Scherrer (2011), we express syntactic transformations as tree pairs. Rather than constituent trees, however, we used the Stanford dependencies for Chinese (Chang et al., 2009), and also annotated their register level. The system incorporates 10 such transformations, the most frequent of which are shown in Table 3. Proceedings of AMTA 2018, vol. 1: MT Research Track Boston, March 17 - 21, 2018 |Page 92 Construction Adverb position Cantonese &lt;verb> …٣ discourse:sp(&lt;verb>, ٣) low &lt;adjective> ڍ//ዩ/መᙰ advmod(&lt;adjective>, ڍ//ዩ/መᙰ) both  &lt;verb>  advmod(&lt;verb>,) advmod(&lt;verb>,) Ditransitive both &lt;verb> &lt;nound> &lt;nouni> construction obj(&lt;verb>, &lt;nound>), obj(&lt;verb>, &lt;nouni>) Comparative low &lt;adjective> መ &lt;noun> construction prep(&lt;adjective>, መ) pobj(መ, &lt;noun>) Ad"
W18-1809,1997.mtsummit-papers.13,0,0.314395,"Missing"
W18-1809,W09-4610,0,0.0533648,"Missing"
W18-1809,P07-2045,0,0.00685615,"Missing"
W18-1809,W15-2517,0,0.0660819,"Missing"
W18-1809,I11-1174,1,0.816732,"e parser to perform word segmentation, partof-speech (POS) tagging and dependency parsing (Levy and Manning, 2003). Second, it uses forward maximal matching to look up Mandarin-to-Cantonese lexical mappings (Section 3.1), conditioned on POS information and register requirement (Section 3.2). Finally, it applies syntactic transformations on the output, with word re-ordering when warranted (Section 3.3). 3.1. Lexical mappings The lexical mapping contains pairs of equivalent Mandarin and Cantonese words, taken from a parallel corpus of transcribed Cantonese speech and Mandarin Chinese subtitles (Lee, 2011). The speech was transcribed from television programmes broadcast in Hong Kong within the last decade by Television Broadcasts Limited. The Cantonese and Mandarin text were manually word-segmented and aligned. The TV programmes span a variety of genres, including news programmes, current-affairs shows, drama series and talk shows. These programs not only include vocabulary from widely different domains, but also contain Cantonese spoken in different registers. The most formal language is used in news, and the most colloquial in drama series and talk shows. We harvested all word alignments from"
W18-1809,P03-1056,0,0.0142239,"by linguists (Zeng, 1993; ƿuyáng, 1993; etc.). It is thus less costly to exploit existing resources such as word lists and syntactic transformations, than to collect bilingual sentence pairs to overcome data sparseness. Hence, we adopt a knowledge-based approach for Mandarin-toCantonese translation, similar to that of an MT system for translating standard German into Swiss German dialects (Scherrer and Rambow, 2010; Scherrer, 2011). Our approach consists of three steps. First, it uses the Stanford Chinese parser to perform word segmentation, partof-speech (POS) tagging and dependency parsing (Levy and Manning, 2003). Second, it uses forward maximal matching to look up Mandarin-to-Cantonese lexical mappings (Section 3.1), conditioned on POS information and register requirement (Section 3.2). Finally, it applies syntactic transformations on the output, with word re-ordering when warranted (Section 3.3). 3.1. Lexical mappings The lexical mapping contains pairs of equivalent Mandarin and Cantonese words, taken from a parallel corpus of transcribed Cantonese speech and Mandarin Chinese subtitles (Lee, 2011). The speech was transcribed from television programmes broadcast in Hong Kong within the last decade by"
W18-1809,prokopidis-etal-2008-condensing,0,0.0612693,"Missing"
W18-1809,W11-2604,0,0.170282,"d the data sparseness issue, since both low- and high-register training data would be needed. Despite the relative paucity of parallel data, Cantonese has been extensively studied by linguists (Zeng, 1993; ƿuyáng, 1993; etc.). It is thus less costly to exploit existing resources such as word lists and syntactic transformations, than to collect bilingual sentence pairs to overcome data sparseness. Hence, we adopt a knowledge-based approach for Mandarin-toCantonese translation, similar to that of an MT system for translating standard German into Swiss German dialects (Scherrer and Rambow, 2010; Scherrer, 2011). Our approach consists of three steps. First, it uses the Stanford Chinese parser to perform word segmentation, partof-speech (POS) tagging and dependency parsing (Levy and Manning, 2003). Second, it uses forward maximal matching to look up Mandarin-to-Cantonese lexical mappings (Section 3.1), conditioned on POS information and register requirement (Section 3.2). Finally, it applies syntactic transformations on the output, with word re-ordering when warranted (Section 3.3). 3.1. Lexical mappings The lexical mapping contains pairs of equivalent Mandarin and Cantonese words, taken from a parall"
W18-1809,2006.amta-papers.25,0,0.0799228,"6 as the median sentence length. We first report results with automatic evaluation measures (Section 4.1), and then discuss results from a human evaluation on both translation quality and register appropriateness (Section 4.2). 4.1. Automatic evaluation As shown in Table 4, we evaluated two translation models. The “High Register” model excludes lexical mappings and syntactic transformations that are labelled as ‘low’, while the “Low Register” model excludes mappings and transformations that are labelled as ‘high’ (Section 3.2). We evaluated translation quality using the translation edit rate (Snover et al., 2006) (TER). TER is similar to the Word Error Rate, but also allows “shift” as an edit in addition to insertion, deletion, and substitution. On both datasets, both the “High Register” and “Low Register” models outperform the “no change” baseline, i.e., output the Mandarin source sentence as target sentence. The TER of this baseline is lower for the “High Register” dataset, confirming that high-register Cantonese more closely resembles Mandarin. 1 Retrieved from http://webcast.legco.gov.hk/public/zh-hk . Proceedings of AMTA 2018, vol. 1: MT Research Track Boston, March 17 - 21, 2018 |Page 93 On the"
W18-1809,2015.mtsummit-papers.17,0,0.0461489,"Missing"
W18-1809,2007.mtsummit-papers.66,0,0.0939128,"Missing"
W18-1809,W17-6530,1,0.82651,"se target sentence. 3.3. Syntactic transformations In a comparative analysis of Cantonese and Mandarin, ƿuyáng (1993: 274) noted that although their “grammatical structure is similar in most major respects, the differences are not insignificant”. These differences include the use of modal verbs and predicative adjectives; the expression of epistemicity and comparative construction; the word order in double object constructions; and the system of sentence-final particles, which is significantly more complicated in Cantonese. Further, in a quantitative comparison between Mandarin and Cantonese, Wong et al. (2017) showed that Mandarin adverbs are replaced by Cantonese auxiliaries in a number of cases. Similar to Scherrer (2011), we express syntactic transformations as tree pairs. Rather than constituent trees, however, we used the Stanford dependencies for Chinese (Chang et al., 2009), and also annotated their register level. The system incorporates 10 such transformations, the most frequent of which are shown in Table 3. Proceedings of AMTA 2018, vol. 1: MT Research Track Boston, March 17 - 21, 2018 |Page 92 Construction Adverb position Cantonese &lt;verb> …٣ discourse:sp(&lt;verb>, ٣) low &lt;adjective> ڍ/"
W18-1809,D12-1070,0,0.0243673,"Missing"
W18-1809,P98-2238,0,0.116466,"Missing"
W18-6706,L16-1262,0,0.0845061,"Missing"
W18-6706,W12-2910,0,0.0267242,"l simplification (Specia et al., 2012) aims to replace a word or short phrase with another, while preserving the meaning of the original sentence. At the sentence level, most previous work focused on syntactic simplification, i.e., to reduce the syntactic complexity of a sentence by splitting a complex sentence into two or more simple sentences (Siddharthan, 2002). In terms of the cline of metaphoricity, then, it transforms a “complex clause” into a “cohesive sequence” (Table 1). Typically, the system analyzes the input sentence via a parse tree, applies manually written transformation rules (Bott et al., 2012; Siddharthan and Angrosh, 2014; Saggion et al., 2015), and then performs sentence re-generation. Our system adopts a similar architecture and also takes a complex clause as input, but works in the opposite direction on the cline: it attempts to transform the complex clause into a simplex clause. University students who are non-native speakers of English often experience significant difficulties in studying content subjects in English, due in large part to their problems with academic writing (Evans and Green, 2007). The traditional focus in computer-assisted language learning and natural lang"
W18-6706,E14-1076,0,0.0282569,"pecia et al., 2012) aims to replace a word or short phrase with another, while preserving the meaning of the original sentence. At the sentence level, most previous work focused on syntactic simplification, i.e., to reduce the syntactic complexity of a sentence by splitting a complex sentence into two or more simple sentences (Siddharthan, 2002). In terms of the cline of metaphoricity, then, it transforms a “complex clause” into a “cohesive sequence” (Table 1). Typically, the system analyzes the input sentence via a parse tree, applies manually written transformation rules (Bott et al., 2012; Siddharthan and Angrosh, 2014; Saggion et al., 2015), and then performs sentence re-generation. Our system adopts a similar architecture and also takes a complex clause as input, but works in the opposite direction on the cline: it attempts to transform the complex clause into a simplex clause. University students who are non-native speakers of English often experience significant difficulties in studying content subjects in English, due in large part to their problems with academic writing (Evans and Green, 2007). The traditional focus in computer-assisted language learning and natural language processing has been the de"
W18-6706,C14-1160,0,0.0174002,", ‘ism’, ‘-ity’, ‘-ment’, ‘-ry’, ‘-ship’, ‘-th’, ‘-tude’, ‘-ty’, ‘-ure’. Verb-to-noun (v2n) We constructed our verb-tonoun list based on the NOM entries1 from 1 We excluded the NOMLIKE entries, and those whose 28 3.3 Sentence Generator • Major revision, i.e., different choice of nominalized verb or adjective. The Sentence Generator takes as input the noun phrases produced by the Nominalizer for the main clause and subordinate clause. It then recombines them into a complete sentence based on the semantic relation between them. Since implicit discourse identification remains a challenging task (Braud and Denis, 2014), the system determines the relation by keyword spotting; the keywords “because”, “since”, “so”, and “therefore” for the causal relation; “although”, “despite”, “even though” for the concession relation, “after”, “before” for the temporal relation, etc. In Table 2, the system infers the relation as causal based on the keyword “because”, and links the two noun phrases with the verb “to cause” (“The doctor’s negligence caused her sudden death”). In other contexts, another linking word may be warranted, such as “to result in”, “to lead to”, “to be due/attributable to”, “to be a result of”, “to li"
W18-6706,W13-1703,0,0.0251984,"ration. Our system adopts a similar architecture and also takes a complex clause as input, but works in the opposite direction on the cline: it attempts to transform the complex clause into a simplex clause. University students who are non-native speakers of English often experience significant difficulties in studying content subjects in English, due in large part to their problems with academic writing (Evans and Green, 2007). The traditional focus in computer-assisted language learning and natural language processing has been the development of algorithms for correcting grammatical errors (Dahlmeier et al., 2013) and improving sentence fluency (Sakaguchi et al., 2016). The focus of this paper, in contrast, is to help students improve their writing style in Academic English. Our long-term goal is to help students make use of the full range of options available along what M. A. K. Halliday calls the cline of metaphoricity (Halliday and Matthiessen, 2014). This cline ranges between the clausally complex, lexically simple congruent construal of experience at one end, and the clausally simple, lexically dense metaphorical re-construal at other end. Table 1 shows paraphrases of an example sentence along thi"
W18-6706,N03-1013,0,0.722428,"d: Example: Output obj N* V* N* RB noun verb nounobj adv ↓ ↓ ↓ gen(noun) v2n(verb) nounobj adv2adj(adv) She ↓ her died ↓ death suddenly ↓ sudden gen(noun) adv2adj(adv) v2n(verb) of nounobj (Example: “her sudden death”) the adv2adj(adv) v2n(verb) of noun the adv2adj(adv) v2n(verb) of nounobj by noun Table 3: Nominalization rule, where v2n is the mapping from a verb to a noun; adv2adj is the mapping from an adverb to an adjective; and gen is the mapping from a nominative noun to its genitive form. NOMLEX (Meyers et al., 1998). For verbs not covered by NOMLEX, we retrieved their nouns in CATVAR (Habash and Dorr, 2003). When a verb is mapped to multiple nouns, we choose the one with the highest unigram frequency count in the Google Web 1T Corpus (Brants and Franz, 2006) that ends with a typical noun suffix2 . This procedure yielded 7,879 one-to-one verb-noun mappings. verbs and negated verbs, since their nominalization patterns vary considerably depending on meaning and context. • Identify the adverb (adv), if any, and generate its adjectival form, adv2adj(adv). For example, “suddenly” is transformed into “sudden” in Table 3. • Identify the direct object (nounobj ) and prepositional phrases, if any, and pla"
W18-6706,D15-1162,0,0.0253099,"input sentence with the Syntactic Parser (Section 3.1); (2) transforms the clauses into noun phrases with the Nominalizer (Section 3.2); and (3) links the noun phrases to produce a sentence with the Sentence Generator (Section 3.3). 3 Approach and the linking word from the mark relation. In Table 2, for example, it extracts “She died suddenly” as the main clause, “the doctor was negligent” as the subordinate clause, and “because” as the linking word. Our system is a pipeline with three components (Table 2). 3.1 Syntactic parser 3.2 Given an input sentence, we use the SpaCy dependency parser (Honnibal and Johnson, 2015) to derive its syntactic tree in Universal Dependencies (Nivre et al., 2016). The system determines whether a sentence contains a complex clause by searching for the adverbial clause modifier (advcl) relation. If so, the system extracts the main clause from the head word of the advcl relation, the subordinate clause from its child word, Nominalizer Given a clause with a verb phrase, the Nominalizer matches its tree structure to the pattern shown in Table 3. It then transforms the clause into a noun phrase with the following steps: • Identify the main verb (verb) and generate its nominalized fo"
W18-6706,W98-0604,0,0.595179,"sformed into “death”. We do not treat verbs-to-be, modal 27 Input advmod nsubj POS tag: Word: Example: Output obj N* V* N* RB noun verb nounobj adv ↓ ↓ ↓ gen(noun) v2n(verb) nounobj adv2adj(adv) She ↓ her died ↓ death suddenly ↓ sudden gen(noun) adv2adj(adv) v2n(verb) of nounobj (Example: “her sudden death”) the adv2adj(adv) v2n(verb) of noun the adv2adj(adv) v2n(verb) of nounobj by noun Table 3: Nominalization rule, where v2n is the mapping from a verb to a noun; adv2adj is the mapping from an adverb to an adjective; and gen is the mapping from a nominative noun to its genitive form. NOMLEX (Meyers et al., 1998). For verbs not covered by NOMLEX, we retrieved their nouns in CATVAR (Habash and Dorr, 2003). When a verb is mapped to multiple nouns, we choose the one with the highest unigram frequency count in the Google Web 1T Corpus (Brants and Franz, 2006) that ends with a typical noun suffix2 . This procedure yielded 7,879 one-to-one verb-noun mappings. verbs and negated verbs, since their nominalization patterns vary considerably depending on meaning and context. • Identify the adverb (adv), if any, and generate its adjectival form, adv2adj(adv). For example, “suddenly” is transformed into “sudden” i"
W19-8634,E14-1057,0,0.0640172,"Missing"
W19-8634,N15-3024,0,0.0187478,"ntensively studied for the task of lexical substitution (McCarthy and Navigli, 2009). To find the most appropriate paraphrases, PPDB uses a scoring model that estimates semantic distance between two words with WordNet, and also considers lexical overlap, distributional similarity, as well as cosine similarity of word embeddings (Pavlick et al., 2015). The SALSA system performs similarity ranking through latent semantic analysis, explicit semantic analysis and n-gram scores (Sinha and Mihalcea, 2014). It has been deployed in a reading assistance tool that displays synonyms for difficult words (Azab et al., 2015), but it does not attempt LS. LS systems use simplicity, or word complexity, as the basis for ranking (Section 3.1). In contrast, we will propose an approach that takes into account both word complexity and semantic faithfulness, by building on previous research on ranking by semantic similarity (Section 3.2) and personalized CWI (Section 3.3). 3.1 Simplicity ranking Various statistical models have been trained to rank substitution candidates by “simplicity”, using a wide range of features including the number of syllables, word frequencies, n-gram language model scores, word embeddings, as we"
W19-8634,C18-1019,1,0.497909,"). 2.1 Complex Word Identification (CWI) CWI classifies each word in an input text as either “complex” (i.e., difficult to understand) or “noncomplex” (i.e., not difficult to understand) (Paetzold and Specia, 2016c; Yimam et al., 2017). Complex words become target words for substitution in the rest of the pipeline. Most CWI approaches adopt a generic definition of “complexity”, without catering to variation in proficiency level among users. Some studies have begun to build personalized CWI models to predict whether a word can or cannot be understood by an individual user (Ehara et al., 2014; Lee and Yeung, 2018). For example, the top row in Table 1 shows the predictions for the word “liberal” for three users. Given these predictions, the system would attempt to simplify the word “liberal” in the sentence in Table 2 for Users A and B, but would not do so for User C. linguistics. We propose a novel, two-step Substitution Ranking algorithm for the LS pipeline, by combining lexical substitution (McCarthy and Navigli, 2009)1 and personalized complex word identification (CWI) (Ehara et al., 2014). This algorithm first ranks the substitution candidates according to semantic proximity to the target word; it"
W19-8634,S12-1068,0,0.0747107,"Missing"
W19-8634,C18-1021,0,0.0485623,"Missing"
W19-8634,E99-1042,0,0.580276,"ution Ranking to identify the candidate that is the closest synonym and is non-complex for the user. In experiments on learners of English at different proficiency levels, we show that this approach enhances the semantic faithfulness of the output, at the cost of a relatively small increase in the number of complex words. 1 Introduction A lexical simplification (LS) system aims to make a text easier to understand for users such as language learners (Petersen and Ostendorf, 2007), children (Belder and Moens, 2010; Kajiwara et al., 2013), those with language disabilities (Devlin and Tait, 1998; Carroll et al., 1999; Rello et al., 2013), as well as those without the background needed for understanding the text in a specialized domain (Zeng et al., 2005; Elhadad, 2006). Given an input text, the system substitutes difficult words with simpler words or phrases, in such a way as to satisfy two requirements: Semantic faithfulness The output text should preserve the meaning of the input as much as possible; Word complexity The output text should minimize the number of complex words, i.e., words that the user cannot understand. 258 Proceedings of The 12th International Conference on Natural Language Generation,"
W19-8634,P15-4015,0,0.0184082,"ing it. The rest of the paper is organized as follows. After a presentation of the general LS system architecture in the next section, Section 3 summarizes previous research in Substitution Ranking. Section 4 describes our proposed approach. Section 5 gives details on our datasets. Section 6 defines the evaluation metrics. Section 7 discusses the experimental set-up, and Section 8 presents experimental results. Finally, Section 9 concludes. 3 Previous work Substitution Ranking is “the task of ranking a set of selected substitutions for a target complex word with respect to their simplicity.” (Paetzold and Specia, 2015). As this definition suggests, most 2 Some systems take a separate Substitution Selection step to discard candidates that distort the meaning of the text or affect its grammaticality, and retain those that fit the context. 1 In this paper, the abbreviation LS always refers to “Lexical Simplification”, and not to “Lexical Substitution.” 259 Input sentence: She is a product of a tremendously unorthodox family with liberal views toward sex, marriage, religion and child-rearing. Substitution Ranking algorithm (a) Simplicity ranking (b) Similarity ranking (c) Personalized ranking User n/a n/a User"
W19-8634,C16-2017,0,0.106913,"possible; Word complexity The output text should minimize the number of complex words, i.e., words that the user cannot understand. 258 Proceedings of The 12th International Conference on Natural Language Generation, pages 258–267, c Tokyo, Japan, 28 Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics Word liberal open progressive relaxed User A × √ √ √ User B × √ × √ User C √ √ √ √ 2 Background: LS pipeline The most common LS architecture is a pipeline architecture with three steps: Complex Word Identification, Substitution Generation, and Substitution Ranking (Shardlow, 2014; Paetzold and Specia, 2016c). Table 1: Example CWI model predictions on three users’ vocabulary knowledge. Complex words are marked√with a cross (×), and non-complex ones with a tick ( ). 2.1 Complex Word Identification (CWI) CWI classifies each word in an input text as either “complex” (i.e., difficult to understand) or “noncomplex” (i.e., not difficult to understand) (Paetzold and Specia, 2016c; Yimam et al., 2017). Complex words become target words for substitution in the rest of the pipeline. Most CWI approaches adopt a generic definition of “complexity”, without catering to variation in proficiency level among use"
W19-8634,D14-1143,0,0.411591,"x ones with a tick ( ). 2.1 Complex Word Identification (CWI) CWI classifies each word in an input text as either “complex” (i.e., difficult to understand) or “noncomplex” (i.e., not difficult to understand) (Paetzold and Specia, 2016c; Yimam et al., 2017). Complex words become target words for substitution in the rest of the pipeline. Most CWI approaches adopt a generic definition of “complexity”, without catering to variation in proficiency level among users. Some studies have begun to build personalized CWI models to predict whether a word can or cannot be understood by an individual user (Ehara et al., 2014; Lee and Yeung, 2018). For example, the top row in Table 1 shows the predictions for the word “liberal” for three users. Given these predictions, the system would attempt to simplify the word “liberal” in the sentence in Table 2 for Users A and B, but would not do so for User C. linguistics. We propose a novel, two-step Substitution Ranking algorithm for the LS pipeline, by combining lexical substitution (McCarthy and Navigli, 2009)1 and personalized complex word identification (CWI) (Ehara et al., 2014). This algorithm first ranks the substitution candidates according to semantic proximity t"
W19-8634,L16-1491,0,0.0735377,"possible; Word complexity The output text should minimize the number of complex words, i.e., words that the user cannot understand. 258 Proceedings of The 12th International Conference on Natural Language Generation, pages 258–267, c Tokyo, Japan, 28 Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics Word liberal open progressive relaxed User A × √ √ √ User B × √ × √ User C √ √ √ √ 2 Background: LS pipeline The most common LS architecture is a pipeline architecture with three steps: Complex Word Identification, Substitution Generation, and Substitution Ranking (Shardlow, 2014; Paetzold and Specia, 2016c). Table 1: Example CWI model predictions on three users’ vocabulary knowledge. Complex words are marked√with a cross (×), and non-complex ones with a tick ( ). 2.1 Complex Word Identification (CWI) CWI classifies each word in an input text as either “complex” (i.e., difficult to understand) or “noncomplex” (i.e., not difficult to understand) (Paetzold and Specia, 2016c; Yimam et al., 2017). Complex words become target words for substitution in the rest of the pipeline. Most CWI approaches adopt a generic definition of “complexity”, without catering to variation in proficiency level among use"
W19-8634,C12-1049,0,0.518091,"Missing"
W19-8634,P16-2024,0,0.0652852,"takes into account both word complexity and semantic faithfulness, by building on previous research on ranking by semantic similarity (Section 3.2) and personalized CWI (Section 3.3). 3.1 Simplicity ranking Various statistical models have been trained to rank substitution candidates by “simplicity”, using a wide range of features including the number of syllables, word frequencies, n-gram language model scores, word embeddings, as well as relative frequencies in standard Wikipedia and Simple Wikipedia (Carroll et al., 1999; Ligozat et al., ˘ 2012; Horn et al., 2014; Glava˘s and Stajner, 2015; Pavlick and Callison-Burch, 2016). Among the three candidates in our running example, “open” would be ranked first, and thus chosen to substitute for “liberal” (Table 2a). Researchers have recognized that the “one-sizefits-all” approach does not adequately cater to users at different vocabulary proficiency levels, since they do not share the same notion of “simplicity”. Some have begun to explore adaptation of the ranking through user feedback (Paetzold and Specia, 2016a; Yimam and Biemann, 2018). For example, the Lexi system asks the user whether the original word or a candidate substitution makes the sentence easier to unde"
W19-8634,P15-2070,0,0.0236372,"Missing"
W19-8634,P15-2011,0,0.0519037,"Missing"
W19-8634,P14-2075,0,0.0672087,"In contrast, we will propose an approach that takes into account both word complexity and semantic faithfulness, by building on previous research on ranking by semantic similarity (Section 3.2) and personalized CWI (Section 3.3). 3.1 Simplicity ranking Various statistical models have been trained to rank substitution candidates by “simplicity”, using a wide range of features including the number of syllables, word frequencies, n-gram language model scores, word embeddings, as well as relative frequencies in standard Wikipedia and Simple Wikipedia (Carroll et al., 1999; Ligozat et al., ˘ 2012; Horn et al., 2014; Glava˘s and Stajner, 2015; Pavlick and Callison-Burch, 2016). Among the three candidates in our running example, “open” would be ranked first, and thus chosen to substitute for “liberal” (Table 2a). Researchers have recognized that the “one-sizefits-all” approach does not adequately cater to users at different vocabulary proficiency levels, since they do not share the same notion of “simplicity”. Some have begun to explore adaptation of the ranking through user feedback (Paetzold and Specia, 2016a; Yimam and Biemann, 2018). For example, the Lexi system asks the user whether the original word"
W19-8634,shardlow-2014-open,0,0.0245614,"input as much as possible; Word complexity The output text should minimize the number of complex words, i.e., words that the user cannot understand. 258 Proceedings of The 12th International Conference on Natural Language Generation, pages 258–267, c Tokyo, Japan, 28 Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics Word liberal open progressive relaxed User A × √ √ √ User B × √ × √ User C √ √ √ √ 2 Background: LS pipeline The most common LS architecture is a pipeline architecture with three steps: Complex Word Identification, Substitution Generation, and Substitution Ranking (Shardlow, 2014; Paetzold and Specia, 2016c). Table 1: Example CWI model predictions on three users’ vocabulary knowledge. Complex words are marked√with a cross (×), and non-complex ones with a tick ( ). 2.1 Complex Word Identification (CWI) CWI classifies each word in an input text as either “complex” (i.e., difficult to understand) or “noncomplex” (i.e., not difficult to understand) (Paetzold and Specia, 2016c; Yimam et al., 2017). Complex words become target words for substitution in the rest of the pipeline. Most CWI approaches adopt a generic definition of “complexity”, without catering to variation in"
W19-8634,O13-1007,0,0.03179,"cally closer to the original word. We propose a personalized approach for Substitution Ranking to identify the candidate that is the closest synonym and is non-complex for the user. In experiments on learners of English at different proficiency levels, we show that this approach enhances the semantic faithfulness of the output, at the cost of a relatively small increase in the number of complex words. 1 Introduction A lexical simplification (LS) system aims to make a text easier to understand for users such as language learners (Petersen and Ostendorf, 2007), children (Belder and Moens, 2010; Kajiwara et al., 2013), those with language disabilities (Devlin and Tait, 1998; Carroll et al., 1999; Rello et al., 2013), as well as those without the background needed for understanding the text in a specialized domain (Zeng et al., 2005; Elhadad, 2006). Given an input text, the system substitutes difficult words with simpler words or phrases, in such a way as to satisfy two requirements: Semantic faithfulness The output text should preserve the meaning of the input as much as possible; Word complexity The output text should minimize the number of complex words, i.e., words that the user cannot understand. 258 P"
W19-8634,Q15-1021,0,0.0674672,"r word such as “bad” should be preferred. Human editors follow this principle when composing graded versions of a text: they stick to simple substitutions (“bad”) in basic versions, but allow more difficult words (“offensive”) in more advanced versions, if these words better reflect the original meaning. Current LS approaches do not mimic this strategy. The typical system offers the same substitutions, regardless of individual users’ language proficiency, because it is trained on simple-complex text pairs that do not specify the target reader. With the notable exception of the Newsela corpus (Xu et al., 2015), annotators for LS corpora are not typically given precise guidelines on the vocabulary proficiency of the intended user, making it difficult to make optimal or consistent trade-off between semantic faithfulness and word complexity. A possible solution is to use Newsela-style corpora to train systems to automatically generate multiple simplified versions, optimized for readers at different proficiency levels. The sheer amount of annotation required, however, would limit the granularity of the proficiency levels. This paper instead explores an alternative solution that leverages two existing s"
W19-8634,C18-1028,0,0.014413,"he 15 subjects’ annotations on each word. We then randomly selected 50 words as the training set.5 For each user, we computed the precision and recall of each of the 222 CWI models on this training set, and then selected the model with the highest F-score. Our baselines for CWI filtering are as follows6 : No CWI This baseline predicts all words to be non-complex; in other words, the system always returns the top-ranked candidate as output. Simplicity Ranking (Automatic) We ranked the substitution candidates according to their gold scores in the probabilistic track of the 2018 CWI shared task (Yimam and Biemann, 2018). The score is defined as the proportion of annotators who marked the word as “complex”. The use of CWI gold scores, rather than an actual system output, ensured a strong baseline for comparison against our proposed approach. 7.2 Generic CWI This baseline optimizes its prediction on the learner dataset (Section 5.2); in other words, it predicts a word to be complex if and only if a majority of the 15 users annotated it to be so. This baseline establishes the maximum performance of the generic, or user-independent CWI approach. 8 CWI filtering methods In the second step, the proposed approach u"
W19-8634,yimam-etal-2017-multilingual,0,0.0232619,"2 Background: LS pipeline The most common LS architecture is a pipeline architecture with three steps: Complex Word Identification, Substitution Generation, and Substitution Ranking (Shardlow, 2014; Paetzold and Specia, 2016c). Table 1: Example CWI model predictions on three users’ vocabulary knowledge. Complex words are marked√with a cross (×), and non-complex ones with a tick ( ). 2.1 Complex Word Identification (CWI) CWI classifies each word in an input text as either “complex” (i.e., difficult to understand) or “noncomplex” (i.e., not difficult to understand) (Paetzold and Specia, 2016c; Yimam et al., 2017). Complex words become target words for substitution in the rest of the pipeline. Most CWI approaches adopt a generic definition of “complexity”, without catering to variation in proficiency level among users. Some studies have begun to build personalized CWI models to predict whether a word can or cannot be understood by an individual user (Ehara et al., 2014; Lee and Yeung, 2018). For example, the top row in Table 1 shows the predictions for the word “liberal” for three users. Given these predictions, the system would attempt to simplify the word “liberal” in the sentence in Table 2 for User"
W97-1411,J86-3001,0,\N,Missing
Y12-1022,D10-1100,0,0.337241,"sing techniques in quoted speech attribution. A conversational network can then be similarly built; the edges can characterize, for example, the length of dialogues between the two characters (Elson et al., 2010). 2.2 Social Networks Relations between people, however, are not described only, or even primarily, by conversations, in most other genres. The Automated Content Extraction (ACE) task, which focuses on newswire text, aims to infer all entities mentioned in a text, the relations among them, and the events in which they participate (Doddington et al., 2004). Also using newswire corpora, Agarwal and Rambow (2010) extract social events using features from 210 syntactic parse trees. Emphasizing the cognitive states of the participants, they classify the events into “interactions” or “observations”. In the extraction of social networks from biographies, personal relationships are classified as “positive” or “negative” (van de Camp and van den Bosch, 2011). Our goal is to produce overviews of large corpora of literary texts, and is thus most similar to that of (Elson et al., 2010). Our networks are not, however, limited to conversations, so that quoted speech needs not be assumed to be the main vehicle of"
Y12-1022,W10-1803,0,0.187309,"Missing"
Y12-1022,doddington-etal-2004-automatic,0,0.349283,"each of the W’s in isolation. Named entity recognition systems retrieve lists of personal entities, organizations, geographical names, and the like (Chinchor et al., 1999); temporal resolution systems detect temporal expressions (Mani and Wilson, 2000); discourse parsers can help answer why questions (Marcu, 1998). In more recent work, there has been much effort to synthesize two or more of the W’s, for example, detecting co-occurrences of dates and place names (Smith, 2002); linking time to events (Pustejovsky et al., 2005); connecting people to the events in which they interact with others (Doddington et al., 2004; Agarwal et al., 2010); as well as “nexus points” of groups of people at particular locations (Bingenheimer et al., 2009). This paper contributes another step in this direction, reporting the first attempt to automatically construct social networks from literary texts integrating who, what, and where. The rest of the paper is organized as follows. The next section reviews previous work in the automatic generation of social networks. Section 3 defines the research question. Section 4 describes 209 Copyright 2012 byby John Lee and Chak Yan Yeung Copyright 2012 John Lee and Chak Yan Yeung 26th P"
Y12-1022,de-marneffe-etal-2006-generating,0,0.0677121,"Missing"
Y12-1022,P10-1015,0,0.430701,"Missing"
Y12-1022,P05-1045,0,0.0153533,"Missing"
Y12-1022,P00-1010,0,0.0559449,"ateways to the primary source by helping the reader locate points of interest for closer reading. Manually written overviews tend to be centered on one of the W’s. For example, biographies summarize the “who” in a text; a plot précis explains the “what” of a novel; and a gazetteer gives a list of locations. Most approaches in computational linguistics also focused on each of the W’s in isolation. Named entity recognition systems retrieve lists of personal entities, organizations, geographical names, and the like (Chinchor et al., 1999); temporal resolution systems detect temporal expressions (Mani and Wilson, 2000); discourse parsers can help answer why questions (Marcu, 1998). In more recent work, there has been much effort to synthesize two or more of the W’s, for example, detecting co-occurrences of dates and place names (Smith, 2002); linking time to events (Pustejovsky et al., 2005); connecting people to the events in which they interact with others (Doddington et al., 2004; Agarwal et al., 2010); as well as “nexus points” of groups of people at particular locations (Bingenheimer et al., 2009). This paper contributes another step in this direction, reporting the first attempt to automatically const"
Y12-1022,N03-1033,0,0.0698636,"Missing"
Y12-1022,W11-1708,0,0.0445781,"Missing"
Y12-1022,W11-1902,0,\N,Missing
Y14-1063,W13-1703,0,0.130257,"(Connors and Lunsford, 1988; Lunsford and Lunsford, 2008), non-native speakers appear to be especially prone to producing them, possibly due to interference from syntactic differences in L1 (Tseng and Liou, 2006; Bennui, 2008; Rahimi, 2009). This may be especially true for L1s where comma splices are frequently found and are not considered mistakes, such as in Chinese (Lin, 2002). Comma splices are one of the errors addressed in the 2014 CoNLL Shared Task on Grammatical Error Correction (Ng et al., 2014). They are annotated in many learner corpora, including the NUS Corpus of Learner English (Dahlmeier et al., 2013) and the EFCambridge Open Language Database (Geertzen et al., 2013). This paper addresses the task of detecting comma splices. We report human agreement in detecting these errors and propose a CRF model to automatically detect them. Our best model, which uses features derived from parse trees produced by the Stanford parser (Klein and Manning, 2003), significantly outperforms both a baseline that does not consider Copyright 2014 by John Lee, Chak Yan Yeung, and Martin Chodorow 28th Pacific Asia Conference on Language, Information and Computation pages 551–560 !551 PACLIC 28 syntactic informati"
Y14-1063,W11-2838,0,0.0247348,"ices are detected. WhiteSmoke3 underlines the problematic comma and suggests that it should be replaced with either a full-stop or a semi-colon. The grammar checker embedded in Microsoft Word, perhaps the most widely used system, also gives feedback about comma splices. To the best of our knowledge, the first three do not explicitly consider parse tree patterns; we will evaluate our approach against the !552 2 3 www.grammarly.com www.whitesmoke.com PACLIC 28 fourth. In addition to these four, a number of writing assistance systems have also been built for the two Helping Our Own shared tasks (Dale and Kilgarriff, 2011; Dale et al., 2012) and two CoNLL shared tasks (Ng et al., 2013; Ng et al., 2014). Run-on sentences and comma splices were among the 28 error types introduced in the CoNLL-2014 shared task (Ng et al., 2014). Among teams that tackled individual error types, none addressd run-on sentences and comma splices. Among teams that attempted to correct all error types, many obtained good results for word- and phrase-level errors, but none achieved any recall for run-on errors and comma splices. 3 Approach We cast comma splice detection as a sequence labeling task, using a linear-chain CRF as our model."
Y14-1063,W12-2006,0,0.0177049,"ke3 underlines the problematic comma and suggests that it should be replaced with either a full-stop or a semi-colon. The grammar checker embedded in Microsoft Word, perhaps the most widely used system, also gives feedback about comma splices. To the best of our knowledge, the first three do not explicitly consider parse tree patterns; we will evaluate our approach against the !552 2 3 www.grammarly.com www.whitesmoke.com PACLIC 28 fourth. In addition to these four, a number of writing assistance systems have also been built for the two Helping Our Own shared tasks (Dale and Kilgarriff, 2011; Dale et al., 2012) and two CoNLL shared tasks (Ng et al., 2013; Ng et al., 2014). Run-on sentences and comma splices were among the 28 error types introduced in the CoNLL-2014 shared task (Ng et al., 2014). Among teams that tackled individual error types, none addressd run-on sentences and comma splices. Among teams that attempted to correct all error types, many obtained good results for word- and phrase-level errors, but none achieved any recall for run-on errors and comma splices. 3 Approach We cast comma splice detection as a sequence labeling task, using a linear-chain CRF as our model. Each comma in a sen"
Y14-1063,W08-1105,0,0.0248904,"eft words Left POS Left combo there is an NP followed by a VP in the clause to the right. Accurate extraction of parse features depends on the quality of the parse trees, but non-native errors in the sentence often cause the parser to produce unexpected tree patterns (Foster et al., 2008), hence causing noise in the parse tree features. In general, parsers perform better on shorter sentences. To reduce this kind of interference, therefore, we remove those parts of the sentence that cannot contain comma splices. Unlike the task of sentence compression for summarization (Knight and Marcu, 2000; Filippova and Strube, 2008), we do not need to preserve important words or the meaning of the original sentence. Rather, we aim to preserve the phrases in the sentence that can potentially result in comma splices and strip away the rest so that the parser has the best chance to produce the expected parse patterns. Specifically, using the parse tree of the original sentence, we remove (1) introductory phrases at the beginning of a sentence, which include transition phrases such as “for example”, as well as prepositional phrases and adverbials6 ; (2) clauses that are properly connected to the rest of the sentence by a coo"
Y14-1063,P08-2056,0,0.0779017,"Missing"
Y14-1063,W09-2112,0,0.113928,"e a re6 The list of phrases are taken from http://www.msu.edu/user/jdowell/135/transw.html 7 We used a list of 292 verbs that are the hyponyms of the words “express” and “convey” in WordNet 3.0 (Miller, 1995). Example raining, was VBG, VBD raining VBG, was VBD we, stayed PRP, VBD we PRP, stayed VBD it PRP it PRP was raining we stayed VBD VBG PRP VBD 3 3 - 1 2 yes yes no yes no yes yes Table 1: List of features. Example values for each feature are drawn from the comma of the sentence “It was raining, we stayed home”. alistic estimate of system performance on arbitrary learner text. Similar to (Foster and Andersen, 2009), we artificially introduced comma splices into the text by removing conjunctions and relative pronouns to the right of commas. To ensure that the generated sen!554 PACLIC 28 Feature Pattern Example S Pattern S+NP+VP S Pattern S+S Pattern VP+VP , S [It NP VP S was raining], NP [we] VP [stayed home.] S [The pink shirt is $20], S [black skirt is $18], S [dark pant is $15]. S , S VP It can help salesperson VP [to VP [promote up-sales and cross sales] , VP [provide better services]]. VP , VP Table 2: Parse tree patterns distinctive of comma splices, illustrated with examples. tence is a comma spli"
Y14-1063,P03-1054,0,0.0169385,", such as in Chinese (Lin, 2002). Comma splices are one of the errors addressed in the 2014 CoNLL Shared Task on Grammatical Error Correction (Ng et al., 2014). They are annotated in many learner corpora, including the NUS Corpus of Learner English (Dahlmeier et al., 2013) and the EFCambridge Open Language Database (Geertzen et al., 2013). This paper addresses the task of detecting comma splices. We report human agreement in detecting these errors and propose a CRF model to automatically detect them. Our best model, which uses features derived from parse trees produced by the Stanford parser (Klein and Manning, 2003), significantly outperforms both a baseline that does not consider Copyright 2014 by John Lee, Chak Yan Yeung, and Martin Chodorow 28th Pacific Asia Conference on Language, Information and Computation pages 551–560 !551 PACLIC 28 syntactic information and a widely used commercial grammar checker. Recently, there has been much effort in developing writing assistance systems that can automatically correct errors in text written by non-native speakers. Such systems focus mostly on word or phrase-level errors, such as the misuse of articles (Han et al., 2006), prepositions (Tetreault et al., 2010)"
Y14-1063,P08-1021,1,0.817951,"as a sequence labelling problem (Israel et al., 2012), where each space between words was considered by a CRF model to determine whether a comma should be present. Features such as POS, bi-grams, and distances to the nearest conjunctions were effective and these will form the basis of our baseline model. The comma errors addressed in Israel et al. (2012), however, are distinct from ours. Instead of adding in missing commas or deleting unnecessary ones, our focus is on the improper linking of clauses that manifests as wrongly used commas, which cannot be fixed by simply removing them. Work by Lee and Seneff (2008) on correcting the misuse of verb forms is relevant to detecting comma splice errors that involve participles. They found that verb form errors result in predictable irregularities in parse trees which can be used as cues for error detection. We follow their approach of using parse tree patterns, but will incorporate these patterns in a machine learning framework rather than a rule-based system. We are not aware of any previous work on detecting or restoring missing conjunctions, but this task is implicitly or explicitly performed by four existing systems that give feedback about comma splices"
Y14-1063,P12-2049,1,0.857764,"nts’ writing, but it would not be the case for the sentences created with this method. Therefore, we did not include comma splices introduced by fusing sentences together. Out of 13159 instances of commas, this method yielded 2775 comma splices. 4.2 Test Sets Although run-on sentences and comma splices were among the 28 error types introduced in the CoNLL2014 shared task (Ng et al., 2014), the test set used in the task only contained about 26 such errors, and is therefore too small for our purpose. We evaluated our system on two test sets8 : the learner corpus at City University of Hong Kong (Lee and Webster, 2012) (henceforth, the “CityU Set”) and the EFCambridge Open Language Database (Geertzen et al., 2013) (henceforth, the “Cambridge Set”). CityU Set. The learner corpus at City University of Hong Kong consists of academic writing by university students, most of whom are native speakers of Chinese. Three of the error categories in this corpus are concerned with comma splices — “new sentence”, “conjunction missing” and “missing relative pronoun”. We randomly selected 550 sentences that are marked with one of these three categories: 215 with “new sentences”, 215 with “conjunction missing”, and 120 with"
Y14-1063,P02-1047,0,0.0777088,"errors do involve long-distance grammatical constructions, this paper is the first report of a research effort to address the improper linking of clauses, a sentence-level error. Our ultimate goal, after detecting a comma splice, is to automatically correct it. We will, however, not treat the correction task here because it concerns a host of other issues, such as automatic analysis of style, to choose between splitting the comma splice into two sentences (“It was raining. We stayed home.”) and conjoining them (“It was raining, so we stayed home”), as well as inference of discourse relations (Marcu and Echihabi, 2002), to choose an appropriate conjunction (e.g., use of “so” rather than “because” in the above example). The rest of the paper is organized as follows. After reviewing previous research in related areas (section 2), we describe our approach for comma splice detection (section 3). We then describe our datasets, report on human agreement and experimental results (section 4), followed by our conclusions (section 5). 2 Previous work A comma splice may be the result of a misuse of punctuation (comma instead of full-stop), a misuse of verb form (finite instead of participle), or a missing conjunction."
Y14-1063,J93-2004,0,0.0456555,"Missing"
Y14-1063,W13-3601,0,0.0340027,"ts that it should be replaced with either a full-stop or a semi-colon. The grammar checker embedded in Microsoft Word, perhaps the most widely used system, also gives feedback about comma splices. To the best of our knowledge, the first three do not explicitly consider parse tree patterns; we will evaluate our approach against the !552 2 3 www.grammarly.com www.whitesmoke.com PACLIC 28 fourth. In addition to these four, a number of writing assistance systems have also been built for the two Helping Our Own shared tasks (Dale and Kilgarriff, 2011; Dale et al., 2012) and two CoNLL shared tasks (Ng et al., 2013; Ng et al., 2014). Run-on sentences and comma splices were among the 28 error types introduced in the CoNLL-2014 shared task (Ng et al., 2014). Among teams that tackled individual error types, none addressd run-on sentences and comma splices. Among teams that attempted to correct all error types, many obtained good results for word- and phrase-level errors, but none achieved any recall for run-on errors and comma splices. 3 Approach We cast comma splice detection as a sequence labeling task, using a linear-chain CRF as our model. Each comma in a sentence is to be tagged as T[rue] (it is a com"
Y14-1063,P12-2039,0,0.02193,"tly outperforms both a baseline that does not consider Copyright 2014 by John Lee, Chak Yan Yeung, and Martin Chodorow 28th Pacific Asia Conference on Language, Information and Computation pages 551–560 !551 PACLIC 28 syntactic information and a widely used commercial grammar checker. Recently, there has been much effort in developing writing assistance systems that can automatically correct errors in text written by non-native speakers. Such systems focus mostly on word or phrase-level errors, such as the misuse of articles (Han et al., 2006), prepositions (Tetreault et al., 2010) and verbs (Tajiri et al., 2012). Although these errors do involve long-distance grammatical constructions, this paper is the first report of a research effort to address the improper linking of clauses, a sentence-level error. Our ultimate goal, after detecting a comma splice, is to automatically correct it. We will, however, not treat the correction task here because it concerns a host of other issues, such as automatic analysis of style, to choose between splitting the comma splice into two sentences (“It was raining. We stayed home.”) and conjoining them (“It was raining, so we stayed home”), as well as inference of disc"
Y14-1063,P10-2065,1,0.857976,"lein and Manning, 2003), significantly outperforms both a baseline that does not consider Copyright 2014 by John Lee, Chak Yan Yeung, and Martin Chodorow 28th Pacific Asia Conference on Language, Information and Computation pages 551–560 !551 PACLIC 28 syntactic information and a widely used commercial grammar checker. Recently, there has been much effort in developing writing assistance systems that can automatically correct errors in text written by non-native speakers. Such systems focus mostly on word or phrase-level errors, such as the misuse of articles (Han et al., 2006), prepositions (Tetreault et al., 2010) and verbs (Tajiri et al., 2012). Although these errors do involve long-distance grammatical constructions, this paper is the first report of a research effort to address the improper linking of clauses, a sentence-level error. Our ultimate goal, after detecting a comma splice, is to automatically correct it. We will, however, not treat the correction task here because it concerns a host of other issues, such as automatic analysis of style, to choose between splitting the comma splice into two sentences (“It was raining. We stayed home.”) and conjoining them (“It was raining, so we stayed home"
Y14-1063,W14-1701,0,\N,Missing
Y14-1063,N12-1029,1,\N,Missing
