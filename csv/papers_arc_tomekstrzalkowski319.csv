2020.stoc-1.1,Active Defense Against Social Engineering: The Case for Human Language Technology,2020,-1,-1,17,0,14499,adam dalton,Proceedings for the First International Workshop on Social Threats in Online Conversations: Understanding and Management,0,"We describe a system that supports natural language processing (NLP) components for active defenses against social engineering attacks. We deploy a pipeline of human language technology, including Ask and Framing Detection, Named Entity Recognition, Dialogue Engineering, and Stylometry. The system processes modern message formats through a plug-in architecture to accommodate innovative approaches for message analysis, knowledge representation and dialogue generation. The novelty of the system is that it uses NLP for cyber defense and engages the attacker using bots to elicit evidence to attribute to the attacker and to waste the attacker{'}s time and resources."
2020.stoc-1.2,Adaptation of a Lexical Organization for Social Engineering Detection and Response Generation,2020,1,0,7,0,11651,archna bhatia,Proceedings for the First International Workshop on Social Threats in Online Conversations: Understanding and Management,0,We present a paradigm for extensible lexicon development based on Lexical Conceptual Structure to support social engineering detection and response generation. We leverage the central notions of ask (elicitation of behaviors such as providing access to money) and framing (risk/reward implied by the ask). We demonstrate improvements in ask/framing detection through refinements to our lexical organization and show that response generation qualitatively improves as ask/framing detection performance improves. The paradigm presents a systematic and efficient approach to resource adaptation for improved task-specific performance.
2020.stoc-1.8,Email Threat Detection Using Distinct Neural Network Approaches,2020,-1,-1,6,0,14502,esteban castillo,Proceedings for the First International Workshop on Social Threats in Online Conversations: Understanding and Management,0,"This paper describes different approaches to detect malicious content in email interactions through a combination of machine learning and natural language processing tools. Specifically, several neural network designs are tested on word embedding representations to detect suspicious messages and separate them from non-suspicious, benign email. The proposed approaches are trained and tested on distinct email collections, including datasets constructed from publicly available corpora (such as Enron, APWG, etc.) as well as several smaller, non-public datasets used in recent government evaluations. Experimental results show that back-propagation both with and without recurrent neural layers outperforms current state of the art techniques that include supervised learning algorithms with stylometric elements of texts as features. Our results also demonstrate that word embedding vectors are effective means for capturing certain aspects of text meaning that can be teased out through machine learning in non-linear/complex neural networks, in order to obtain highly accurate detection of malicious emails based on email text alone."
2020.findings-emnlp.247,Learning to Plan and Realize Separately for Open-Ended Dialogue Systems,2020,-1,-1,9,0,6267,sashank santhanam,Findings of the Association for Computational Linguistics: EMNLP 2020,0,"Achieving true human-like ability to conduct a conversation remains an elusive goal for open-ended dialogue systems. We posit this is because extant approaches towards natural language generation (NLG) are typically construed as end-to-end architectures that do not adequately model human generation processes. To investigate, we decouple generation into two separate phases: planning and realization. In the planning phase, we train two planners to generate plans for response utterances. The realization phase uses response plans to produce an appropriate response. Through rigorous evaluations, both automated and human, we demonstrate that decoupling the process into planning and realization performs better than an end-to-end approach."
2020.figlang-1.23,Generating Ethnographic Models from Communities{'} Online Data,2020,-1,-1,1,1,14511,tomek strzalkowski,Proceedings of the Second Workshop on Figurative Language Processing,0,"In this paper we describe computational ethnography study to demonstrate how machine learning techniques can be utilized to exploit bias resident in language data produced by communities with online presence. Specifically, we leverage the use of figurative language (i.e., the choice of metaphors) in online text (e.g., news media, blogs) produced by distinct communities to obtain models of community worldviews that can be shown to be distinctly biased and thus different from other communities{'} models. We automatically construct metaphor-based community models for two distinct scenarios: gun rights and marriage equality. We then conduct a series of experiments to validate the hypothesis that the metaphors found in each community{'}s online language convey the bias in the community{'}s worldview."
L18-1110,Gaining and Losing Influence in Online Conversation,2018,0,0,2,0,29625,arun sharma,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L16-1180,{ANEW}+: Automatic Expansion and Validation of Affective Norms of Words Lexicons in Multiple Languages,2016,0,4,3,1,1544,samira shaikh,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"In this article we describe our method of automatically expanding an existing lexicon of words with affective valence scores. The automatic expansion process was done in English. In addition, we describe our procedure for automatically creating lexicons in languages where such resources may not previously exist. The foreign languages we discuss in this paper are Spanish, Russian and Farsi. We also describe the procedures to systematically validate our newly created resources. The main contributions of this work are: 1) A general method for expansion and creation of lexicons with scores of words on psychological constructs such as valence, arousal or dominance; and 2) a procedure for ensuring validity of the newly constructed resources."
L16-1594,The Validation of {MRCPD} Cross-language Expansions on Imageability Ratings,2016,11,0,3,0,1018,ting liu,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"In this article, we present a method to validate a multi-lingual (English, Spanish, Russian, and Farsi) corpus on imageability ratings automatically expanded from MRCPD (Liu et al., 2014). We employed the corpus (Brysbaert et al., 2014) on concreteness ratings for our English MRCPD+ validation because of lacking human assessed imageability ratings and high correlation between concreteness ratings and imageability ratings (e.g. r = .83). For the same reason, we built a small corpus with human imageability assessment for the other language corpus validation. The results show that the automatically expanded imageability ratings are highly correlated with human assessment in all four languages, which demonstrate our automatic expansion method is valid and robust. We believe these new resources can be of significant interest to the research community, particularly in natural language processing and computational sociolinguistics."
W15-1408,Understanding Cultural Conflicts using Metaphors and Sociolinguistic Measures of Influence,2015,-1,-1,2,1,1544,samira shaikh,Proceedings of the Third Workshop on Metaphor in {NLP},0,None
S15-1009,A New Dataset and Evaluation for Belief/Factuality,2015,15,5,6,0,90,vinodkumar prabhakaran,Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics,0,"The terms xe2x80x9cbeliefxe2x80x9d and xe2x80x9cfactualityxe2x80x9d both refer to the intention of the writer to present the propositional content of an utterance as firmly believed by the writer, not firmly believed, or having some other status. This paper presents an ongoing annotation effort and an associated evaluation."
W14-4725,Discovering Conceptual Metaphors using Source Domain Spaces,2014,24,0,2,1,1544,samira shaikh,Proceedings of the 4th Workshop on Cognitive Aspects of the Lexicon ({C}og{AL}ex),0,"This article makes two contributions towards the use of lexical resources and corpora; specifically making use of them for gaining access to and using word associations. The direct application of our approach is for detecting linguistic and conceptual metaphors automatically in text. We describe our method of building conceptual spaces, that is, defining the vocabulary that characterizes a Source Domain (e.g., Disease) of a conceptual metaphor (e.g., Poverty is a Disease). We also describe how these conceptual spaces are used to group linguistic metaphors into conceptual metaphors. Our method works in multiple languages, including English, Spanish, Russian and Farsi. We provide details of how our method can be evaluated and evaluation results that show satisfactory performance across all languages."
W14-2306,Computing Affect in Metaphors,2014,-1,-1,1,1,14511,tomek strzalkowski,Proceedings of the Second Workshop on Metaphor in {NLP},0,None
liu-etal-2014-automatic-expansion,Automatic Expansion of the {MRC} Psycholinguistic Database Imageability Ratings,2014,15,5,5,0,1018,ting liu,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"Recent studies in metaphor extraction across several languages (Broadwell et al., 2013; Strzalkowski et al., 2013) have shown that word imageability ratings are highly correlated with the presence of metaphors in text. Information about imageability of words can be obtained from the MRC Psycholinguistic Database (MRCPD) for English words and L{\'e}xico Informatizado del Espa{\~n}ol Programa (LEXESP) for Spanish words, which is a collection of human ratings obtained in a series of controlled surveys. Unfortunately, word imageability ratings were collected for only a limited number of words: 9,240 words in English, 6,233 in Spanish; and are unavailable at all in the other two languages studied: Russian and Farsi. The present study describes an automated method for expanding the MRCPD by conferring imageability ratings over the synonyms and hyponyms of existing MRCPD words, as identified in Wordnet. The result is an expanded MRCPD+ database with imagea-bility scores for more than 100,000 words. The appropriateness of this expansion process is assessed by examining the structural coherence of the expanded set and by validating the expanded lexicon against human judgment. Finally, the performance of the metaphor extraction system is shown to improve significantly with the expanded database. This paper describes the process for English MRCPD+ and the resulting lexical resource. The process is analogous for other languages."
shaikh-etal-2014-multi,A Multi-Cultural Repository of Automatically Discovered Linguistic and Conceptual Metaphors,2014,0,2,2,1,1544,samira shaikh,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"In this article, we present details about our ongoing work towards building a repository of Linguistic and Conceptual Metaphors. This resource is being developed as part of our research effort into the large-scale detection of metaphors from unrestricted text. We have stored a large amount of automatically extracted metaphors in American English, Mexican Spanish, Russian and Iranian Farsi in a relational database, along with pertinent metadata associated with these metaphors. A substantial subset of the contents of our repository has been systematically validated via rigorous social science experiments. Using information stored in the repository, we are able to posit certain claims in a cross-cultural context about how peoples in these cultures (America, Mexico, Russia and Iran) view particular concepts related to Governance and Economic Inequality through the use of metaphor. Researchers in the field can use this resource as a reference of typical metaphors used across these cultures. In addition, it can be used to recognize metaphors of the same form or pattern, in other domains of research."
W13-1105,Topical Positioning: A New Method for Predicting Opinion Changes in Conversation,2013,-1,-1,5,0,38379,chingsheng lin,Proceedings of the Workshop on Language Analysis in Social Media,0,None
W13-0909,Robust Extraction of Metaphor from Novel Data,2013,27,23,1,1,14511,tomek strzalkowski,Proceedings of the First Workshop on Metaphor in {NLP},0,"This article describes our novel approach to the automated detection and analysis of metaphors in text. We employ robust, quantitative language processing to implement a system prototype combined with sound social science methods for validation. We show results in 4 different languages and discuss how our methods are a significant step forward from previously established techniques of metaphor identification. We use Topical Structure and Tracking, an Imageability score, and innovative methods to build an effective metaphor identification system that is fully automated and performs well over baseline."
lin-etal-2012-revealing,Revealing Contentious Concepts Across Social Groups,2012,9,0,6,0,38379,chingsheng lin,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"In this paper, a computational model based on concept polarity is proposed to investigate the influence of communications across the diacultural groups. The hypothesis of this work is that there are communities or groups which can be characterized by a network of concepts and the corresponding valuations of those concepts that are agreed upon by the members of the community. We apply an existing research tool, ECO, to generate text representative of each community and create community specific Valuation Concept Networks (VCN). We then compare VCNs across the communities, to attempt to find contentious concepts, which could subsequently be the focus of further exploration as points of contention between the two communities. A prototype, CPAM (Changing Positions, Altering Minds), was implemented as a proof of concept for this approach. The experiment was conducted using blog data from pro-Palestinian and pro-Israeli communities. A potential application of this method and future work are discussed as well."
liu-etal-2012-extending,Extending the {MPC} corpus to {C}hinese and {U}rdu - A Multiparty Multi-Lingual Chat Corpus for Modeling Social Phenomena in Language,2012,9,2,3,0.120209,1018,ting liu,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"In this paper, we report our efforts in building a multi-lingual multi-party online chat corpus in order to develop a firm understanding in a set of social constructs such as agenda control, influence, and leadership as well as to computationally model such constructs in online interactions. These automated models will help capture the dialogue dynamics that are essential for developing, among others, realistic human-machine dialogue systems, including autonomous virtual chat agents. In this paper, we first introduce our experiment design and data collection method in Chinese and Urdu, and then report on the current stage of our data collection. We annotated the collected corpus on four levels: communication links, dialogue acts, local topics, and meso-topics. Results from the analyses of annotated data on different languages indicate some interesting phenomena, which are reported in this paper."
E12-1030,Bootstrapping Events and Relations from Text,2012,16,9,2,0.120209,1018,ting liu,Proceedings of the 13th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"In this paper, we describe a new approach to semi-supervised adaptive learning of event extraction from text. Given a set of examples and an un-annotated text corpus, the BEAR system (Bootstrapping Events And Relations) will automatically learn how to recognize and understand descriptions of complex semantic relationships in text, such as events involving multiple entities and their roles. For example, given a series of descriptions of bombing and shooting incidents (e.g., in newswire) the system will learn to extract, with a high degree of accuracy, other attack-type events mentioned elsewhere in text, irrespective of the form of description. A series of evaluations using the ACE data and event set show a significant performance improvement over our baseline system."
C12-1155,Modeling Leadership and Influence in Multi-party Online Discourse,2012,11,7,1,1,14511,tomek strzalkowski,Proceedings of {COLING} 2012,0,None
P11-1018,Multi-Modal Annotation of Quest Games in Second Life,2011,10,5,3,1,42989,sharon small,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,1,"We describe an annotation tool developed to assist in the creation of multimodal action-communication corpora from on-line massively multi-player games, or MMGs. MMGs typically involve groups of players (5--30) who control their avatars, perform various activities (questing, competing, fighting, etc.) and communicate via chat or speech using assumed screen names. We collected a corpus of 48 group quests in Second Life that jointly involved 206 players who generated over 30,000 messages in quasi-synchronous chat during approximately 140 hours of recorded action. Multiple levels of coordinated annotation of this corpus (dialogue, movements, touch, gaze, wear, etc) are required in order to support development of automated predictors of selected real-life social and demographic characteristics of the players. The annotation tool presented in this paper was developed to enable efficient and accurate annotation of all dimensions simultaneously."
W10-2708,{VCA}: An Experiment with a Multiparty Virtual Chat Agent,2010,15,9,2,1,1544,samira shaikh,Proceedings of the 2010 Workshop on Companionable Dialogue Systems,0,"The purpose of this research was to advance the understanding of the behavior of small groups in online chat rooms. The research was conducted using Internet chat data collected through planned exercises with recruited participants. Analysis of the collected data led to construction of preliminary models of social behavior in online discourse. Some of these models, e.g., how to effectively change the topic of conversation, were subsequently implemented into an automated Virtual Chat Agent (VCA) prototype. VCA has been demonstrated to perform effectively and convincingly in Internet conversation in multiparty chat environments."
shaikh-etal-2010-mpc,{MPC}: A Multi-Party Chat Corpus for Modeling Social Phenomena in Discourse,2010,16,18,2,1,1544,samira shaikh,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"In this paper, we describe our experience with collecting and creating an annotated corpus of multi-party online conversations in a chat-room environment. This effort is part of a larger project to develop computational models of social phenomena such as agenda control, influence, and leadership in on-line interactions. Such models will help capturing the dialogue dynamics that are essential for developing, among others, realistic human-machine dialogue systems, including autonomous virtual chat agents. In this paper we describe data collection method used and the characteristics of the initial dataset of English chat. We have devised a multi-tiered collection process in which the subjects start from simple, free-flowing conversations and progress towards more complex and structured interactions. In this paper, we report on the first two stages of this process, which were recently completed. The third, large-scale collection effort is currently being conducted. All English dialogue has been annotated at four levels: communication links, dialogue acts, local topics and meso-topics. Some details of these annotations will be discussed later in this paper, although a full description is impossible within the scope of this article."
C10-1117,Modeling Socio-Cultural Phenomena in Discourse,2010,48,28,1,1,14511,tomek strzalkowski,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),0,"In this paper, we describe a novel approach to computational modeling and understanding of social and cultural phenomena in multi-party dialogues. We developed a two-tier approach in which we first detect and classify certain social language uses, including topic control, disagreement, and involvement, that serve as first order models from which presence the higher level social constructs such as leadership, may be inferred."
P06-1147,Utilizing Co-Occurrence of Answers in Question Answering,2006,13,8,2,0,19766,min wu,Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,1,"In this paper, we discuss how to utilize the co-occurrence of answers in building and automatic question answering system that answers a series of questions on a specific topic in a batch mode. Experiments show that the answers to the many of the questions in the series usually have a high degree of co-occurrence in relevant document passages. This feature sometimes can't be easily utilized in an automatic QA system which processes questions independently. However it can be utilized in a QA system that processes questions in a batch mode. We have used our pervious TREC QA system as baseline and augmented it with new answer clustering and co-occurrence maximization components to build the batch QA system. The experiment results show that the QA system running under the batch mode get significant performance improvement over our baseline TREC QA system."
W04-2507,{HITIQA}: Scenario Based Question Answering,2004,10,7,2,1,42989,sharon small,Proceedings of the Workshop on Pragmatics of Question Answering at {HLT}-{NAACL} 2004,0,Abstract : In this paper we describe some preliminary results of qualitative evaluation of the answering system HITIQA (High-Quality Interactive Question Answering) which has been developed over the last 2 years as an advanced research tool for information analysts. HITIQA is an interactive open-domain question answering technology designed to allow analysts to pose complex exploratory questions in natural language and obtain relevant information units to prepare their briefing reports in order to satisfy a given scenario. The system uses novel data-driven semantics to conduct a clarification dialogue with the user that explores the scope and the context of the desired answer space. The system has undergone extensive hands-on evaluations by a group of intelligence analysts representing various foreign intelligence services. This evaluation validated the overall approach in HITIQA but also exposed limitations of the current prototype.
P04-1010,Data-Driven Strategies for an Automated Dialogue System,2004,13,32,2,0,51747,hilda hardy,Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics ({ACL}-04),1,"We present a prototype natural-language problem-solving application for a financial services call center, developed as part of the Amities multilingual human-computer dialogue project. Our automated dialogue system, based on empirical evidence from real call-center conversations, features a data-driven approach that allows for mixed system/customer initiative and spontaneous conversation. Preliminary evaluation results indicate efficient dialogues and high user satisfaction, with performance comparable to or better than that of current conversational travel information systems."
N04-4014,{HITIQA}: A Data Driven Approach to Interactive Analytical Question Answering,2004,6,7,2,1,42989,sharon small,Proceedings of {HLT}-{NAACL} 2004: Short Papers,0,In this paper we describe the analytic question answering system HITIQA (High-Quality Interactive Question Answering) which has been developed over the last 2 years as an advanced research tool for information analysts. HITIQA is an interactive open-domain question answering technology designed to allow analysts to pose complex exploratory questions in natural language and obtain relevant information units to prepare their briefing reports. The system uses novel data-driven semantics to conduct a clarification dialogue with the user that explores the scope and the context of the desired answer space. The system has undergone extensive hands-on evaluations by a group of intelligence analysts representing various foreign intelligence services. This evaluation validated the overall approach in HITIQA but also exposed limitations of the current prototype.
wacholder-etal-2004-designing,Designing a Realistic Evaluation of an End-to-end Interactive Question Answering System,2004,3,3,12,0,38348,nina wacholder,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"We report on the development of material for an evaluation exercise designed to assess the overall design and usability of HITIQA, an interactive question-answering system for preparing broad ranging reports on complex issues. The two basic objectives of the evaluation were (1) To perform a realistic assessment of the usefulness and usability of HITIQA as an end-to-end system, from the information seekerxe2x80x99s initial questions to completion of a draft report; and (2) To develop metrics to compare the answers obtained by different analysts and evaluate the quality of the support that HITIQA provides. We used qualitative and quantitative tools to obtain data about analystxe2x80x99s comfort with the HITIQA system, especially its novel features such as the ability to answer complex questions and the interactive dialogue. Because of the impracticality of measuring the quality of HITIQA output with the standard metrics of precision and recall, we developed a new task xe2x80x93cross-evaluation--to indirectly measure the quality of the answers obtained using HITIQA; in this black-box assessment, analysts rate the quality of their own and their colleaguesxe2x80x99 reports."
C04-1189,{HITIQA}: Towards Analytical Question Answering,2004,15,21,2,1,42989,sharon small,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,In this paper we describe the analytic question answering system HITIQA (High-Quality Interactive Question Answering) which has been developed over the last 2 years as an advanced research tool for information analysts. HITIQA is an interactive open-domain question answering technology designed to allow analysts to pose complex exploratory questions in natural language and obtain relevant information units to prepare their briefing reports. The system uses novel data-driven semantics to conduct a clarification dialogue with the user that explores the scope and the context of the desired answer space. The system has undergone extensive hands-on evaluations by a group of intelligence analysts. This evaluation validated the overall approach in HITIQA but also exposed limitations of the current prototype.
W03-1206,{HITIQA}: An Interactive Question Answering System: A Preliminary Report,2003,17,30,4,1,42989,sharon small,Proceedings of the {ACL} 2003 Workshop on Multilingual Summarization and Question Answering,0,"HITIQA is an interactive question answering technology designed to allow intelligence analysts and other users of information systems to pose questions in natural language and obtain relevant answers, or the assistance they require in order to perform their tasks. Our objective in HITIQA is to allow the user to submit exploratory, analytical, non-factual questions, such as What has been Russia's reaction to U.S. bombing of Kosovo? The distinguishing property of such questions is that one cannot generally anticipate what might constitute the answer. While certain types of things may be expected (e.g., diplomatic statements), the answer is heavily conditioned by what information is in fact available on the topic. From a practical viewpoint, analytical questions are often under-specified, thus casting a broad net on a space of possible answers. Therefore, clarification dialogue is often needed to negotiate with the user the exact scope and intent of the question."
W03-0704,Dialogue Management for an Automated Multilingual Call Center,2003,7,10,2,0,51747,hilda hardy,Proceedings of the {HLT}-{NAACL} 2003 Workshop on Research Directions in Dialogue Processing,0,"The AMITIES project (Automated Multilingual Interaction with Information and Services) has been established under joint funding from the European Commission's 5th Framework Program and the U.S. DARPA to develop the next generation of empirically-induced human-computer interaction capabilities in spoken language. One of the central goals of this project is to create a dialogue management system capable of engaging the user in human-like conversation within a specific domain. The domain we selected is telephone-based customer service where the system has access to an appropriate information database to support callers' information needs. Our objective is to automate at least some of the more mundane human functions in customer service call centers, but do so in a manner that is maximally responsive to the customer. This practically eliminates all prompt or menu based voice response systems used at commercial call centers today."
N03-2033,Automatically Predicting Information Quality in News Documents,2003,6,12,3,0,52893,rong tang,Companion Volume of the Proceedings of {HLT}-{NAACL} 2003 - Short Papers,0,We report here empirical results of a series of studies aimed at automatically predicting information quality in news documents. Multiple research methods and data analysis techniques enabled a good level of machine prediction of information quality. Procedures regarding user experiments and statistical analysis are described.
stein-etal-2000-evaluating,Evaluating Summaries for Multiple Documents in an Interactive Environment,2000,15,8,2,0,54483,gees stein,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,"While most people have a clear idea of what a single document summary should look like, this is not immediately obvious for a multidocument summary. There are many new questions to answer concerning the amount of documents to be summarized, the type of documents, the kind of summary that should be generated, the way the summary gets presented to the user, etc. The many approaches possible to multi-document summarization makes evaluation especially difficult. In this paper we will describe an approach to multidocument summarization and report work on an evaluation method for this particular system."
A00-1005,{P}arts{ID}: A Dialogue-Based System for Identifying Parts for Medical Systems,2000,10,3,2,0,53560,amit bagga,Sixth Applied Natural Language Processing Conference,0,This paper describes a system that provides customer service by allowing users to retrieve identification numbers of parts for medical systems using spoken natural language dialogue. The paper also presents an evaluation of the system which shows that the system successfully retrieves the identification numbers of approximately 80% of the parts.
X98-1020,Enhancing Detection through Linguistic Indexing and Topic Expansion,1998,10,1,1,1,14511,tomek strzalkowski,"TIPSTER TEXT PROGRAM PHASE III: Proceedings of a Workshop held at Baltimore, {M}aryland, October 13-15, 1998",0,"Natural language processing techniques may hold a tremendous potential for overcoming the inadequacies of purely quantitative methods of text information retrieval. Under the Tipster contracts in phases I through III, GE group has set out to explore this potential through development and evaluation of new text processing techniques. This work resulted in some significant advances and in a better understanding on how NLP may benefit IR. Tipster research has laid a critical groundwork for future work.In this paper we summarize GE work on document detection in Tipster Phase III. Our summarization research is described in a separate paper appearing in this volume."
X98-1028,A Text-Extraction Based Summarizer,1998,12,5,1,1,14511,tomek strzalkowski,"TIPSTER TEXT PROGRAM PHASE III: Proceedings of a Workshop held at Baltimore, {M}aryland, October 13-15, 1998",0,"We present an automated method of generating human-readable summaries from a variety of text documents including newspaper articles, business reports, government documents, even broadcast news transcripts. Our approach exploits an empirical observation that much of the written text display certain regularities of organization and style, which we call the Discourse Macro Structure (DMS). A summary is therefore created to reflect the components of a given DMS. In order to produce a coherent and readable summary we select continuous, well-formed passages from the source document and assemble them into a mini-document within a DMS template. In this paper we describe an automated summarizer that can generate both short indicative abstracts, useful for quick scanning of a list of documents, as well as longer informative digests that can serve as surrogates for the full text. The summarizer can assist the users of an information retrieval system in assessing the quality of the results returned from a search, preparing reports and memos for their customers, and even building more effective search queries."
P98-2205,Summarization-based Query Expansion in Information Retrieval,1998,14,18,1,1,14511,tomek strzalkowski,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 2",0,"We discuss a semi-interactive approach to information retrieval which consists of two tasks performed in a sequence. First, the system assists the searcher in building a comprehensive statement of information need, using automatically generated topical summaries of sample documents. Second, the detailed statement of information need is automatically processed by a series of natural language processing routines in order to derive an optimal search query for a statistical information retrieval system. In this paper, we investigate the role of automated document summarization in building effective search statements. We also discuss the results of latest evaluation of our system at the annual Text Retrieval Conference (TREC)."
C98-2200,Summarization-based Query Expansion in Information Retrieval,1998,14,18,1,1,14511,tomek strzalkowski,{COLING} 1998 Volume 2: The 17th International Conference on Computational Linguistics,0,"We discuss a semi-interactive approach to information retrieval which consists of two tasks performed in a sequence. First, the system assists the searcher in building a comprehensive statement of information need, using automatically generated topical summaries of sample documents. Second, the detailed statement of information need is automatically processed by a series of natural language processing routines in order to derive an optimal search query for a statistical information retrieval system. In this paper, we investigate the role of automated document summarization in building effective search statements. We also discuss the results of latest evaluation of our system at the annual Text Retrieval Conference (TREC)."
W97-0117,A Natural Language Correction Model for Continuous Speech Recognition,1997,16,8,1,1,14511,tomek strzalkowski,Fifth Workshop on Very Large Corpora,0,None
A97-1044,Building Effective Queries In Natural Language Information Retrieval,1997,12,37,1,1,14511,tomek strzalkowski,Fifth Conference on Applied Natural Language Processing,0,"In this paper we report on our natural language information retrieval (NLIR) project as related to the recently concluded 5th Text Retrieval Conference (TREC-5). The main thrust of this project is to use natural language processing techniques to enhance the effectiveness of full-text document retrieval. One of our goals was to demonstrate that robust if relatively shallow NLP can help to derive a better representation of text documents for statistical search. Recently, we have turned our attention away from text representation issues and more towards query development problems. While our NLIR system still performs extensive natural language processing in order to extract phrasal and other indexing terms, our focus has shifted to the problems of building effective search queries. Specifically, we are interested in query construction that uses words, sentences, and entire passages to expand initial topic specifications in an attempt to cover their various angles, aspects and contexts. Based on our earlier results indicating that NLP is more effective with long, descriptive queries, we allowed for long passages from related documents to be liberally imported into the queries. This method appears to have produced a dramatic improvement in the performance of two different statistical search engines that we tested (Cornell's SMART and NIST's Prise) boosting the average precision by at least 40%. In this paper we discuss both manual and automatic procedures for query expansion within a new stream-based information retrieval model."
X96-1030,Natural Language Information Retrieval: {TIPSTER}-2 Final Report,1996,10,11,1,1,14511,tomek strzalkowski,"TIPSTER TEXT PROGRAM PHASE II: Proceedings of a Workshop held at Vienna, Virginia, May 6-8, 1996",0,"We report on the joint GE/NYU natural language information retrieval project as related to the Tipster Phase 2 research conducted initially at NYU and subsequently at GE R&D Center and NYU. The evaluation results discussed here were obtained in connection with the 3rd and 4th Text Retrieval Conferences (TREC-3 and TREC-4). The main thrust of this project is to use natural language processing techniques to enhance the effectiveness of full-text document retrieval. During the course of the four TREC conferences, we have built a prototype IR system designed around a statistical full-text indexing and search backbone provided by the NIST's Prise engine. The original Prise has been modified to allow handling of multi-word phrases, differential term weighting schemes, automatic query expansion, index partitioning and rank merging, as well as dealing with complex documents. Natural language processing is used to preprocess the documents in order to extract content-carrying terms, discover inter-term dependencies and build a conceptual hierarchy specific to the database domain, and process user's natural language requests into effective search queries.The overall architecture of the system is essentially the same for both years, as our efforts were directed at optimizing the performance of all components. A notable exception is the new massive query expansion module used in routing experiments, which replaces a prototype extension used in the TREC-3 system. On the other hand, it has to be noted that the character and the level of difficulty of TREC queries has changed quite significantly since the last year evaluation. TREC-4 new ad-hoc queries are far shorter, less focused, and they have a flavor of information requests (What is the prognosis of ...) rather than search directives typical for earlier TRECs (The relevant document will contain ...). This makes building of good search queries a more sensitive task than before. We thus decided to introduce only minimum number of changes to our indexing and search processes, and even roll back some of the TREC-3 extensions which dealt with longer and somewhat redundant queries.Overall, our system performed quite well as our position with respect to the best systems improved steadily since the beginning of TREC. We participated in both main evaluation categories: category A ad-hoc and routing, working with approx. 3.3 GBytes of text. We submitted 4 official runs in automatic adhoc, manual ad-hoc, and automatic routing (2), and were ranked 6 or 7 in each category (out of 38 participating teams). It should be noted that the most significant gain in performance seems to have occurred in precision near the top of the ranking, at 5, 10, 15 and 20 documents. Indeed, our unofficial manual runs performed after TREC-4 conference show superior results in these categories, topping by a large margin the best manual scores by any system in the official evaluation.In general, we can note substantial improvement in performance when phrasal terms are used, especially in ad-hoc runs. Looking back at TREC-2 and TREC-3 one may observe that these improvements appear to be tied to the length and specificity of the query: the longer the query, the more improvement from linguistic processes. This can be seen comparing the improvement over baseline for automatic adhoc runs (very short queries), for manual runs (longer queries), and for semi-interactive runs (yet longer queries). In addition, our TREC-3 results (with long and detailed queries) showed 20-25% improvement in precision attributed to NLP, as compared to 10-16% in TREC-4."
X96-1036,Integration of Document Detection and Information Extraction,1996,4,2,2,0,37309,louise guthrie,"TIPSTER TEXT PROGRAM PHASE II: Proceedings of a Workshop held at Vienna, Virginia, May 6-8, 1996",0,"We have conducted a number of experiments to evaluate various modes of building an integrated detection/extraction system. The experiments were performed using SMART system as baseline. The goal was to determine if advanced information extraction methods can improve recall and precision of document detection. We identified the following two modes of integration:I. Extraction to Detection: broad-coverage extraction1. Extraction step: identify concepts for indexing2. Detection step 1: low recall, high initial precision3. Detection step 2: automatic relevance feedback using top N retrieved documents to regain recall.II. Detection to Extraction: query-specific extraction1. Detection step 1: high recall, low precision run2. Extraction step: learn concept(s) from query and retrieved subcollection3. Detection step 2: re-rank the subcollection to increase precisionOur integration effort concentrated on mode I, and the following issues:1. use of shallow but fast NLP for phrase extractions and disambiguation in place of a full syntactic parser2. use existing MUC-6 extraction capabilities to index a retrieval collection3. mixed Boolean/soft match retrieval model4. create a Universal Spotter algorithm for learning arbitrary concepts"
C96-2157,A Self-Learning Universal Concept Spotter,1996,7,28,1,1,14511,tomek strzalkowski,{COLING} 1996 Volume 2: The 16th International Conference on Computational Linguistics,0,"We describe the Universal Spotter, a system for identifying in-text references to entities of an arbitrary, user-specified type, such as people, organizations, equipment, products, materials, etc. Starting with some initial seed examples, and a training text corpus, the system generates rules that will find further concepts of the same type. The initial seed information is provided by the user in the form of a typical lexical context in which the entities to be spotted occur, e.g., the name ends with Co., or to the right of produced or made, and so forth, or by simply supplying examples of the concept itself, e.g., Ford Taurus, gas turbine, Big Mac. In addition, negative examples can be supplied, if known. Given a sufficiently large training corpus, an unsupervised learning process is initiated in which the system will: (1) find instances of the sought-after concept using the seed-context information while maximizing recall and precision; (2) find additional contexts in which these entities occur; and (3) expand the initial seed-context with selected new contexts to find even more entities. Preliminary results of creating spotters for organizations and products are discussed."
H94-1072,Document Representation in Natural Language Text Retrieval,1994,12,5,1,1,14511,tomek strzalkowski,"{H}uman {L}anguage {T}echnology: Proceedings of a Workshop held at {P}lainsboro, {N}ew {J}ersey, {M}arch 8-11, 1994",0,"In information retrieval, the content of a document may be represented as a collection of terms: words, stems, phrases, or other units derived or inferred from the text of the document. These terms are usually weighted to indicate their importance within the document which can then be viewed as a vector in a N-dimensional space. In this paper we demonstrate that a proper term weighting is at least as important as their selection, and that different types of terms (e.g., words, phrases, names), and terms derived by different means (e.g., statistical, linguistic) must be treated differently for a maximum benefit in retrieval. We report some observations made during and after the second Text REtrieval Conference (TREC-2)."
H94-1110,Robust Text Processing and Information Retrieval,1994,16,46,1,1,14511,tomek strzalkowski,"{H}uman {L}anguage {T}echnology: Proceedings of a Workshop held at {P}lainsboro, {N}ew {J}ersey, {M}arch 8-11, 1994",0,"We report on the results of a series of experiments with a prototype text retrieval system which uses relatively advanced natural language processing techniques in order to enhance the effectiveness of statistical document retrieval. In this paper we show that large-scale natural language processing (hundreds of millions of words and more) is not only required for a better retrieval, but it is also doable, given appropriate resources. In particular, we demonstrate that the use of syntactic compounds in the representation of database documents as well as in the user queries, coupled with an appropriate term weighting strategy, can considerably improve the effectiveness of retrospective search. The experiments reported here were conducted on TIPSTER database in connection with the Text REtrieval Conference series (TREC)."
C94-1100,Building a Lexical Domain Map From Text Corpora,1994,18,12,1,1,14511,tomek strzalkowski,{COLING} 1994 Volume 1: The 15th {I}nternational {C}onference on {C}omputational {L}inguistics,0,None
A94-1028,Robust Text Processing in Automated Information Retrieval,1994,16,46,1,1,14511,tomek strzalkowski,Fourth Conference on Applied Natural Language Processing,0,"We report on the results of a series of experiments with a prototype text retrieval system which uses relatively advanced natural language processing techniques in order to enhance the effectiveness of statistical document retrieval. In this paper we show that large-scale natural language processing (hundreds of millions of words and more) is not only required for a better retrieval, but it is also doable, given appropriate resources. In particular, we demonstrate that the use of syntactic compounds in the representation of database documents as well as in the user queries, coupled with an appropriate term weighting strategy, can considerably improve the effectiveness of retrospective search. The experiments reported here were conducted on TIPSTER database in connection with the Text REtrieval Conference series (TREC)."
W93-0302,Robust Text Processing in Automated Information Retrieval,1993,0,4,1,1,14511,tomek strzalkowski,{V}ery {L}arge {C}orpora: Academic and Industrial Perspectives,0,None
H93-1102,Robust Text Processing and Information Retrieval,1993,0,3,1,1,14511,tomek strzalkowski,"{H}uman {L}anguage {T}echnology: Proceedings of a Workshop Held at Plainsboro, New Jersey, March 21-24, 1993",0,"The general objective of this research has been the enhancement of traditional key-word based statistical methods of document retrieval with advanced natural language processing techniques. In the work to date the focus has been on obtaining a better representation of document contents by extracting representative phrases from syntactically preprocessed text and devising suitable weighting schemes for different types of terms. In addition, statistical clustering methods have been developed that generate domain-specific term correlations which can be used to obtain better search queries via expansion."
1993.iwpt-1.23,Evaluation of {TTP} Parser: A Preliminary Report,1993,-1,-1,1,1,14511,tomek strzalkowski,Proceedings of the Third International Workshop on Parsing Technologies,0,"TTP (Tagged Text Parser) is a fast and robust natural language parser specifically designed to process vast quantities of unrestricted text. TTP can analyze written text at the speed of approximately 0.3 sec/sentence, or 73 words per second. An important novel feature of TTP parser is that it is equipped with a skip-and-fit recovery mechanism that allows for fast closing of more difficult sub-constituents after a preset amount of time has elapsed without producing a parse. Although a complete analysis is attempted for each sentence, the parser may occasionally ignore fragments of input to resume {``}normal{''} processing after skipping a few words. These fragments are later analyzed separately and attached as incomplete constituents to the main parse tree. TTP has recently been evaluated against several leading parsers. While no formal numbers were released (a formal evaluation is planned later this year), TTP has performed surprisingly well. The main argument of this paper is that TTP can provide a substantial gain in parsing speed giving up relatively little in terms of the quality of output it produces. This property allows TTP to be used effectively in parsing large volumes of text."
P92-1011,Comparing Two Grammar-Based Generation Algorithms: A Case Study,1992,11,9,2,0,56456,miroslav martinovic,30th Annual Meeting of the Association for Computational Linguistics,1,"In this paper we compare two grammar-based generation algorithms: the Semantic-Head-Driven Generation Algorithm (SHDGA), and the Essential Arguments Algorithm (EAA). Both algorithms have successfully addressed several outstanding problems in grammar-based generation, including dealing with non-monotonic compositionality of representation, left-recursion, deadlock-prone rules, and nondeterminism. We concentrate here on the comparison of selected properties: generality, efficiency, and determinism. We show that EAA's traversals of the analysis tree for a given language construct, include also the one taken on by SHDGA. We also demonstrate specific and common situations in which SHDGA will invariably run into serious inefficiency and nondeterminism, and which EAA will handle in an efficient and deterministic manner. We also point out that only EAA allows to treat the underlying grammar in a truly multi-directional manner."
P92-1014,Information Retrieval Using Robust Natural Language Processing,1992,15,5,1,1,14511,tomek strzalkowski,30th Annual Meeting of the Association for Computational Linguistics,1,"We developed a fully automated Information Retrieval System which uses advanced natural language processing techniques to enhance the effectiveness of traditional key-word based document retrieval. In early experiments with the standard CACM-3204 collection of abstracts, the augmented system has displayed capabilities that made it clearly superior to the purely statistical base system."
H92-1040,Information Retrieval Using Robust Natural Language Processing,1992,15,5,1,1,14511,tomek strzalkowski,"Speech and Natural Language: Proceedings of a Workshop Held at Harriman, New York, {F}ebruary 23-26, 1992",0,"We developed a fully automated Information Retrieval System which uses advanced natural language processing techniques to enhance the effectiveness of traditional key-word based document retrieval. In early experiments with the standard CACM-3204 collection of abstracts, the augmented system has displayed capabilities that made it clearly superior to the purely statistical base system."
C92-1033,{TTP}: A Fast and Robust Parser for Natural Language,1992,11,33,1,1,14511,tomek strzalkowski,{COLING} 1992 Volume 1: The 14th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"In this paper we describe TTP, a fast and robust natural language parser which can analyze written text and generate regularized parse structures for sentences and phrases at the speed of approximately 0.5 sec/sentence, or 44 word per second. The parser is based on a wide coverage grammar for English, developed by the New York University's Linguistic String Project, and it uses the machine-readable version of the Oxford Advanced Learner's Dictionary as a source of its basic vocabulary. The parser operates on stochastically tagged text, and contains a powerful skip-and-fit recovery mechanism that allows it to deal with extra-grammatical input and to operate effectively under a severe time pressure. Empirical experiments, testing parser's speed and accuracy, were performed on several collections: a collection of technical abstracts (CACM-3204), a corpus of news messages (MUC-3), a selection from ACM Computer Library database, and a collection of Wall Street Journal articles, approximately 50 million words in total."
W91-0112,A General Computational Method for Grammar Inversion,1991,-1,-1,1,1,14511,tomek strzalkowski,Reversible Grammar in Natural Language Processing,0,None
H91-1068,Fast Text Processing for Information Retrieval,1991,12,8,1,1,14511,tomek strzalkowski,"Speech and Natural Language: Proceedings of a Workshop Held at Pacific Grove, California, {F}ebruary 19-22, 1991",0,"We describe an advanced text processing system for information retrieval from natural language document collections. We use both syntactic processing as well as statistical term clustering to obtain a representation of documents which would be more accurate than those obtained with more traditional key-word methods. A reliable top-down parser has been developed that allows for fast processing of large amounts of text, and for a precise identification of desired types of phrases for statistical analysis. Two statistical measures are computed: the measure of informational contribution of words in phrases, and the similarity measure between words."
P90-1027,Automated Inversion of Logic Grammars for Generation,1990,13,9,1,1,14511,tomek strzalkowski,28th Annual Meeting of the Association for Computational Linguistics,1,"We describe a system of reversible grammar in which, given a logic-grammar specification of a natural language, two efficient PROLOG programs are derived by an off-line compilation process: a parser and a generator for this language. The centerpiece of the system is the inversion algorithm designed to compute the generator code from the parser's PROLOG code, using the collection of minimal sets of essential arguments (MSEA) for predicates. The system has been implemented to work with Definite Clause Grammars (DCG) and is a part of an English-Japanese machine translation project currently under development at NYU's Courant Institute."
J89-3003,Non-singular Concepts in Natural Language Discourse,1989,15,4,1,1,14511,tomek strzalkowski,Computational Linguistics,0,"We introduce a new approach to representing and manipulating various types of non-singular concepts in natural language discourse. The representation we describe is based on a partially ordered structure of levels in which the objects of the same relative singularity are assigned to the same level. Our choice of the representation has been motivated by the following main concerns: 1. The representation should systematically distinguish between those language terms that are used to refer to objects of different singularity, that is, those classified within different but related levels of the model; 2. The representation should capture certain types of inter-sentential dependencies in discourse, most notably anaphoric-type cohesive links; 3. Finally, the representation should serve as a basis for defining a formal semantics of discourse paragraphs that would allow for capturing the exact truth conditions of sentences involving non-singular terms, and for computing interlevel inferences. In this paper we discuss (1) and (2) only. (3) is currently under investigation and will be the topic of a forthcoming article. We believe that our approach promotes computational feasibility, because we avoid the identification of general terms, like temperature, water, etc., with intensions, that is, functions over possible worlds. In our theory, the concept of non-singularity has a local (often subjective) character."
C86-1086,An Approach to Non-Singular Terms in Discourse,1986,7,4,1,1,14511,tomek strzalkowski,Coling 1986 Volume 1: The 11th International Conference on Computational Linguistics,0,A new Theory of Names and Descriptions that offers a uniform treatment for many types of non-singular concepts found in natural language discourse is presented. We introduce a layered model of the language denotational base (the universe) in which every world object is assigned a layer (level) reflecting its relative singularity with respect in other objects in the universe. We define the notion of relative singularity of world objects as an abstraction class of the layermembership relation
