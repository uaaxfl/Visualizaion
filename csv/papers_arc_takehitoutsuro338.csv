2020.paclic-1.49,Text Mining of Evidence on Infants{'} Developmental Stages for Developmental Order Acquisition from Picture Book Reviews,2020,-1,-1,2,0,15894,miho kasamatsu,"Proceedings of the 34th Pacific Asia Conference on Language, Information and Computation",0,None
2020.lrec-1.791,Integrating Disfluency-based and Prosodic Features with Acoustics in Automatic Fluency Evaluation of Spontaneous Speech,2020,-1,-1,3,0,18180,huaijin deng,Proceedings of the 12th Language Resources and Evaluation Conference,0,"This paper describes an automatic fluency evaluation of spontaneous speech. In the task of automatic fluency evaluation, we integrate diverse features of acoustics, prosody, and disfluency-based ones. Then, we attempt to reveal the contribution of each of those diverse features to the task of automatic fluency evaluation. Although a variety of different disfluencies are observed regularly in spontaneous speech, we focus on two types of phenomena, i.e., filled pauses and word fragments. The experimental results demonstrate that the disfluency-based features derived from word fragments and filled pauses are effective relative to evaluating fluent/disfluent speech, especially when combined with prosodic features, e.g., such as speech rate and pauses/silence. Next, we employed an LSTM based framework in order to integrate the disfluency-based and prosodic features with time sequential acoustic features. The experimental evaluation results of those integrated diverse features indicate that time sequential acoustic features contribute to improving the model with disfluency-based and prosodic features when detecting fluent speech, but not when detecting disfluent speech. Furthermore, when detecting disfluent speech, the model without time sequential acoustic features performs best even without word fragments features, but only with filled pauses and prosodic features."
2020.iwslt-1.17,{U}niversity of {T}sukuba{'}s Machine Translation System for {IWSLT}20 Open Domain Translation Task,2020,-1,-1,4,1,18830,hongyi cui,Proceedings of the 17th International Conference on Spoken Language Translation,0,"In this paper, we introduce University of Tsukuba{'}s submission to the IWSLT20 Open Domain Translation Task. We participate in both ChineseâJapanese and JapaneseâChinese directions. For both directions, our machine translation systems are based on the Transformer architecture. Several techniques are integrated in order to boost the performance of our models: data filtering, large-scale noised training, model ensemble, reranking and postprocessing. Consequently, our efforts achieve 33.0 BLEU scores for ChineseâJapanese translation and 32.3 BLEU scores for JapaneseâChinese translation."
2020.gamnlp-1.12,Automatic Annotation of Werewolf Game Corpus with Players Revealing Oneselves as Seer/Medium and Divination/Medium Results,2020,-1,-1,6,0,18181,youchao lin,Workshop on Games and Natural Language Processing,0,"While playing the communication game {``}Are You a Werewolf{''}, a player always guesses other players{'} roles through discussions, based on his own role and other players{'} crucial utterances. The underlying goal of this paper is to construct an agent that can analyze the participating players{'} utterances and play the werewolf game as if it is a human. For a step of this underlying goal, this paper studies how to accumulate werewolf game log data annotated with identification of players revealing oneselves as seer/medium, the acts of the divination and the medium and declaring the results of the divination and the medium. In this paper, we divide the whole task into four sub tasks and apply CNN/SVM classifiers to each sub task and evaluate their performance."
2020.fever-1.4,Developing a How-to Tip Machine Comprehension Dataset and its Evaluation in Machine Comprehension by {BERT},2020,-1,-1,4,0,19288,tengyang chen,Proceedings of the Third Workshop on Fact Extraction and VERification (FEVER),0,"In the field of factoid question answering (QA), it is known that the state-of-the-art technology has achieved an accuracy comparable to that of humans in a certain benchmark challenge. On the other hand, in the area of non-factoid QA, there is still a limited number of datasets for training QA models, i.e., machine comprehension models. Considering such a situation within the field of the non-factoid QA, this paper aims to develop a dataset for training Japanese how-to tip QA models. This paper applies one of the state-of-the-art machine comprehension models to the Japanese how-to tip QA dataset. The trained how-to tip QA model is also compared with a factoid QA model trained with a Japanese factoid QA dataset. Evaluation results revealed that the how-to tip machine comprehension performance was almost comparative with that of the factoid machine comprehension even with the training data size reduced to around 4{\%} of the factoid machine comprehension. Thus, the how-to tip machine comprehension task requires much less training data compared with the factoid machine comprehension task."
2020.aacl-srw.21,{MRC} Examples Answerable by {BERT} without a Question Are Less Effective in {MRC} Model Training,2020,-1,-1,4,0,12638,hongyu li,Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: Student Research Workshop,0,"Models developed for Machine Reading Comprehension (MRC) are asked to predict an answer from a question and its related context. However, there exist cases that can be correctly answered by an MRC model using BERT, where only the context is provided without including the question. In this paper, these types of examples are referred to as {``}easy to answer{''}, while others are as {``}hard to answer{''}, i.e., unanswerable by an MRC model using BERT without being provided the question. Based on classifying examples as answerable or unanswerable by BERT without the given question, we propose a method based on BERT that splits the training examples from the MRC dataset SQuAD1.1 into those that are {``}easy to answer{''} or {``}hard to answer{''}. Experimental evaluation from a comparison of two models, one trained only with {``}easy to answer{''} examples and the other with {``}hard to answer{''} examples demonstrates that the latter outperforms the former."
W19-7203,A Multi-Hop Attention for {RNN} based Neural Machine Translation,2019,0,0,5,1,18832,shohei iida,Proceedings of The 8th Workshop on Patent and Scientific Literature Translation,0,None
W19-6616,Selecting Informative Context Sentence by Forced Back-Translation,2019,0,0,5,1,23531,ryuichiro kimura,Proceedings of Machine Translation Summit XVII: Research Track,0,None
P19-2030,Attention over Heads: A Multi-Hop Attention for Neural Machine Translation,2019,0,1,5,1,18832,shohei iida,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop,0,"In this paper, we propose a multi-hop attention for the Transformer. It refines the attention for an output symbol by integrating that of each head, and consists of two hops. The first hop attention is the scaled dot-product attention which is the same attention mechanism used in the original Transformer. The second hop attention is a combination of multi-layer perceptron (MLP) attention and head gate, which efficiently increases the complexity of the model by adding dependencies between heads. We demonstrate that the translation accuracy of the proposed multi-hop attention outperforms the baseline Transformer significantly, +0.85 BLEU point for the IWSLT-2017 German-to-English task and +2.58 BLEU point for the WMT-2017 German-to-English task. We also find that the number of parameters required for a multi-hop attention is smaller than that for stacking another self-attention layer and the proposed model converges significantly faster than the original Transformer."
D19-5622,Mixed Multi-Head Self-Attention for Neural Machine Translation,2019,0,2,4,1,18830,hongyi cui,Proceedings of the 3rd Workshop on Neural Generation and Translation,0,"Recently, the Transformer becomes a state-of-the-art architecture in the filed of neural machine translation (NMT). A key point of its high-performance is the multi-head self-attention which is supposed to allow the model to independently attend to information from different representation subspaces. However, there is no explicit mechanism to ensure that different attention heads indeed capture different features, and in practice, redundancy has occurred in multiple heads. In this paper, we argue that using the same global attention in multiple heads limits multi-head self-attention{'}s capacity for learning distinct features. In order to improve the expressiveness of multi-head self-attention, we propose a novel Mixed Multi-Head Self-Attention (MMA) which models not only global and local attention but also forward and backward attention in different attention heads. This enables the model to learn distinct representations explicitly among multiple heads. In our experiments on both WAT17 English-Japanese as well as IWSLT14 German-English translation task, we show that, without increasing the number of parameters, our models yield consistent and significant improvements (0.9 BLEU scores on average) over the strong Transformer baseline."
W18-3721,Measuring Beginner Friendliness of {J}apanese Web Pages explaining Academic Concepts by Integrating Neural Image Feature and Text Features,2018,0,0,4,0,28272,hayato shiokawa,Proceedings of the 5th Workshop on Natural Language Processing Techniques for Educational Applications,0,"Search engine is an important tool of modern academic study, but the results are lack of measurement of beginner friendliness. In order to improve the efficiency of using search engine for academic study, it is necessary to invent a technique of measuring the beginner friendliness of a Web page explaining academic concepts and to build an automatic measurement system. This paper studies how to integrate heterogeneous features such as a neural image feature generated from the image of the Web page by a variant of CNN (convolutional neural network) as well as text features extracted from the body text of the HTML file of the Web page. Integration is performed through the framework of the SVM classifier learning. Evaluation results show that heterogeneous features perform better than each individual type of features."
W17-5709,Patent {NMT} integrated with Large Vocabulary Phrase Translation by {SMT} at {WAT} 2017,2017,0,0,3,1,27595,zi long,Proceedings of the 4th Workshop on {A}sian Translation ({WAT}2017),0,"Neural machine translation (NMT) cannot handle a larger vocabulary because the training complexity and decoding complexity proportionally increase with the number of target words. This problem becomes even more serious when translating patent documents, which contain many technical terms that are observed infrequently. Long et al.(2017) proposed to select phrases that contain out-of-vocabulary words using the statistical approach of branching entropy. The selected phrases are then replaced with tokens during training and post-translated by the phrase translation table of SMT. In this paper, we apply the method proposed by Long et al. (2017) to the WAT 2017 Japanese-Chinese and Japanese-English patent datasets. Evaluation on Japanese-to-Chinese, Chinese-to-Japanese, Japanese-to-English and English-to-Japanese patent sentence translation proved the effectiveness of phrases selected with branching entropy, where the NMT model of Long et al.(2017) achieves a substantial improvement over a baseline NMT model without the technique proposed by Long et al.(2017)."
W16-4602,Translation of Patent Sentences with a Large Vocabulary of Technical Terms Using Neural Machine Translation,2016,0,5,2,1,27595,zi long,Proceedings of the 3rd Workshop on {A}sian Translation ({WAT}2016),0,"Neural machine translation (NMT), a new approach to machine translation, has achieved promising results comparable to those of traditional approaches such as statistical machine translation (SMT). Despite its recent success, NMT cannot handle a larger vocabulary because training complexity and decoding complexity proportionally increase with the number of target words. This problem becomes even more serious when translating patent documents, which contain many technical terms that are observed infrequently. In NMTs, words that are out of vocabulary are represented by a single unknown token. In this paper, we propose a method that enables NMT to translate patent sentences comprising a large vocabulary of technical terms. We train an NMT system on bilingual data wherein technical terms are replaced with technical term tokens; this allows it to translate most of the source sentences except technical terms. Further, we use it as a decoder to translate source sentences with technical term tokens and replace the tokens with technical term translations using SMT. We also use it to rerank the 1,000-best SMT translations on the basis of the average of the SMT score and that of the NMT rescoring of the translated sentences with technical term tokens. Our experiments on Japanese-Chinese patent sentences show that the proposed NMT system achieves a substantial improvement of up to 3.1 BLEU points and 2.3 RIBES points over traditional SMT systems and an improvement of approximately 0.6 BLEU points and 0.8 RIBES points over an equivalent NMT system without our proposed technique."
L16-1303,Analyzing Time Series Changes of Correlation between Market Share and Concerns on Companies measured through Search Engine Suggests,2016,3,1,7,0,35027,takakazu imada,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"This paper proposes how to utilize a search engine in order to predict market shares. We propose to compare rates of concerns of those who search for Web pages among several companies which supply products, given a specific products domain. We measure concerns of those who search for Web pages through search engine suggests. Then, we analyze whether rates of concerns of those who search for Web pages have certain correlation with actual market share. We show that those statistics have certain correlations. We finally propose how to predict the market share of a specific product genre based on the rates of concerns of those who search for Web pages."
Y15-2008,Detecting an Infant{'}s Developmental Reactions in Reviews on Picture Books,2015,-1,-1,3,0,36264,hiroshi uehara,"Proceedings of the 29th Pacific Asia Conference on Language, Information and Computation: Posters",0,None
W15-3408,Evaluating Features for Identifying {J}apanese-{C}hinese Bilingual Synonymous Technical Terms from Patent Families,2015,16,0,2,1,27595,zi long,Proceedings of the Eighth Workshop on Building and Using Comparable Corpora,0,"In the process of translating patent documents, a bilingual lexicon of technical terms is inevitable knowledge source. It is important to develop techniques of acquiring technical term translation equivalent pairs automatically from parallel patent documents. We take an approach of utilizing the phrase table of a state-of-theart phrase-based statistical machine translation model. First, we collect candidates of synonymous translation equivalent pairs from parallel patent sentences. Then, we apply the Support Vector Machines (SVMs) to the task of identifying bilingual synonymous technical terms. This paper especially focuses on the issue of examining the effectiveness of each feature and identifies the minimum number of features that perform as comparatively well as the optimal set of features. Finally, we achieve the performance of over 90% precision with the condition of more than or equal to 25% recall."
2015.mtsummit-wpslt.9,Collecting bilingual technical terms from patent families of character-segmented {C}hinese sentences and morpheme-segmented {J}apanese sentences,2015,-1,-1,2,1,27595,zi long,Proceedings of the 6th Workshop on Patent and Scientific Literature Translation,0,None
I13-1118,Time Series Topic Modeling and Bursty Topic Detection of Correlated News and {T}witter,2013,5,7,3,0,41698,daichi koike,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"News and twitter are sometimes closely correlated, while sometimes each of them has quite independent flow of information, due to the difference of the concerns of their information sources. In order to effectively capture the nature of those two text streams, it is very important to model both their correlation and their difference. This paper first models their correlation by applying a time series topic model to the document stream of the mixture of time series news and twitter. Next, we divide news streams and twitter into distinct two series of document streams, and then we apply our model of bursty topic detection based on the Kleinbergxe2x80x99s burst detection model. This approach successfully models the difference of the two time series topic models of news and twitter as each having independent information source and its own concern."
2013.mtsummit-wpt.2,Compositional translation of technical terms by integrating patent families as a parallel corpus and a comparable corpus,2013,-1,-1,4,0,41873,itsuki toyota,Proceedings of the 5th Workshop on Patent Translation,0,None
Y12-1054,Cross-Lingual Topic Alignment in Time Series {J}apanese / {C}hinese News,2012,9,1,4,0,41984,shuo hu,"Proceedings of the 26th Pacific Asia Conference on Language, Information, and Computation",0,"Among various types of recent information explosion, that in news stream is also a kind of serious problems. This paper studies issues regarding topic modeling of information flow in multilingual news streams. If someone wants to find differences in the topics of Japanese news and Chinese news, it is usually necessary for him/her to carefully watch every article in Japanese and Chinese news streams at every moment. In such a situation, topic models such as LDA (Latent Dirichlet Allocation) and DTM (dynamic topic model) are quite effective in estimating distribution of topics over a document collection such as articles in a news stream. Especially, as a topic model, this paper employs DTM, but not LDA,since it canconsidercorrespondence between topics of consecutive dates. Based on the results of estimating distribution of topics in Japanese / Chinese news streams, this paper proposes how to analyze cross-lingual alignment of topics in time series Japanese / Chinese news streams."
suzuki-etal-2012-detecting,Detecting {J}apanese Compound Functional Expressions using Canonical/Derivational Relation,2012,8,2,4,1,40489,takafumi suzuki,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"The Japanese language has various types of functional expressions. In order to organize Japanese functional expressions with various surface forms, a lexicon of Japanese functional expressions with hierarchical organization was compiled. This paper proposes how to design the framework of identifying more than 16,000 functional expressions in Japanese texts by utilizing hierarchical organization of the lexicon. In our framework, more than 16,000 functional expressions are roughly divided into canonical / derived functional expressions. Each derived functional expression is intended to be identified by referring to the most similar occurrence of its canonical expression. In our framework, contextual occurrence information of much fewer canonical expressions are expanded into the whole forms of derived expressions, to be utilized when identifying those derived expressions. We also empirically show that the proposed method can correctly identify more than 80{\%} of the functional / content usages only with less than 38,000 training instances of manually identified canonical expressions."
Y11-1021,Semi-Automatic Identification of Bilingual Synonymous Technical Terms from Phrase Tables and Parallel Patent Sentences,2011,13,1,2,0,43942,bing liang,"Proceedings of the 25th Pacific Asia Conference on Language, Information and Computation",0,"In the research field of machine translation of patent documents, the issue of ac- quiring technical term translation equivalent pairs automatically from parallel patent docu- ments is one of those most important. We take an approach of utilizing the phrase table of a state-of-the-art phrase-based statistical machine translation model. In this task, we con- sider situations where a technical term is observed in many parallel patent sentences and is translated into many translation equivalents. We apply SVM to the task of identifying synonymous translation equivalent pairs and achieve almost 98% precision and over 40% F- measure. Then, in order to improve recall, we introduce a semi-automatic framework, where we employ the strategy of selecting more than one seeds for each set of candidates bilingual synonymous term pairs. By manually judging whether each pair of two seeds is synonymous or not, we achieve over 95% precision and 50% recall."
2011.mtsummit-wpt.10,Example-based Translation of {J}apanese Functional Expressions utilizing Semantic Equivalence Classes,2011,17,1,4,0,43255,yusuke abe,Proceedings of the 4th Workshop on Patent Translation,0,"This paper studies issues on machine translation of Japanese functional expressions into English. Unlike our previous works, in order to address the issue of resolving various ambiguities of a compound expression, this paper takes the approach of example-based machine translation. In this approach, a patent translation example database is developed given the phrase translation tables trained with parallel patent sentences as well as the training parallel patent sentences themselves. When identifying the most similar translation examples, we integrate semantic equivalence classes of Japanese functional expressions as well as more fine-grained similarity measure of translation examples. In the evaluation, we compare the translation accuracy of the proposed framework with that of Moses, and show that the proposed framework somehow outperforms Moses."
nagasaka-etal-2010-utilizing,Utilizing Semantic Equivalence Classes of {J}apanese Functional Expressions in Translation Rule Acquisition from Parallel Patent Sentences,2010,12,2,6,0,46253,taiji nagasaka,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"In the ``Sandglass'' MT architecture, we identify the class of monosemous Japanese functional expressions and utilize it in the task of translating Japanese functional expressions into English. We employ the semantic equivalence classes of a recently compiled large scale hierarchical lexicon of Japanese functional expressions. We then study whether functional expressions within a class can be translated into a single canonical English expression. Based on the results of identifying monosemous semantic equivalence classes, this paper studies how to extract rules for translating functional expressions in Japanese patent documents into English. In this study, we use about 1.8M Japanese-English parallel sentences automatically extracted from Japanese-English patent families, which are distributed through the Patent Translation Task at the NTCIR-7 Workshop. Then, as a toolkit of a phrase-based SMT (Statistical Machine Translation) model, Moses is applied and Japanese-English translation pairs are obtained in the form of a phrase translation table. Finally, we extract translation pairs of Japanese functional expressions from the phrase translation table. Through this study, we found that most of the semantic equivalence classes judged as monosemous based on manual translation into English have only one translation rules even in the patent domain."
Y09-2031,Towards Conceptual Indexing of the Blogosphere through {W}ikipedia Topic Hierarchy,2009,10,0,4,0,46711,mariko kawaba,"Proceedings of the 23rd Pacific Asia Conference on Language, Information and Computation, Volume 2",0,"This paper studies the issue of conceptually indexing the blogosphere through the whole hierarchy of Wikipedia entries. About 300,000 Wikipedia entries are used for representing a hierarchy of topics. Based on the results of judging whether each blog feed is relevant to a given Wikipedia entry, this paper proposes how to judge whether there exist blog feeds to be linked from the given entry. In our experimental evaluation, we achieved over 90% precision in this task."
Y09-2044,Identifying and Utilizing the Class of Monosemous {J}apanese Functional Expressions in Machine Translation,2009,6,3,3,0.666667,41912,akiko sakamoto,"Proceedings of the 23rd Pacific Asia Conference on Language, Information and Computation, Volume 2",0,"In the xe2x80x9cSandglassxe2x80x9d machine translation architecture, we identify the class of monosemous Japanese functional expressions and utilize it in the task of translating Japanese functional expressions into English. We employ the semantic equivalence classes of a recently compiled large scale hierarchical lexicon of Japanese functional expressions. We then study whether functional expressions within a class can be translated into a single canonical English expression. Next, we introduce two types of ambiguities of functional expressions and identify monosemous functional expressions. In the evaluation of our translation rules for Japanese functional expressions, we directly apply those rules to monosemous functional expressions, and show that the proposed framework outperforms commercial machine translation software products. We further study how to extract rules for translating functional expressions in Japanese patent documents into English. In the result of this study, we show that translation rules manually developed based on the corpus for Japanese language grammar learners is reliable also in the patent domain."
2009.mtsummit-wpt.1,Exploiting Patent Information for the Evaluation of Machine Translation,2009,-1,-1,4,0,37781,atsushi fujii,Proceedings of the Third Workshop on Patent Translation,0,None
2009.mtsummit-wpt.2,Meta-evaluation of Automatic Evaluation Methods for Machine using Patent Translation Data in {NTCIR}-7,2009,-1,-1,7,0,26162,hiroshi echizenya,Proceedings of the Third Workshop on Patent Translation,0,None
fujii-etal-2008-producing,Producing a Test Collection for Patent Machine Translation in the Seventh {NTCIR} Workshop,2008,9,2,4,0,37781,atsushi fujii,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"In aiming at research and development on machine translation, we produced a test collection for Japanese-English machine translation in the seventh NTCIR Workshop. This paper describes details of our test collection. From patent documents published in Japan and the United States, we extracted patent families as a parallel corpus. A patent family is a set of patent documents for the same or related invention and these documents are usually filed to more than one country in different languages. In the parallel corpus, we aligned Japanese sentences with their counterpart English sentences. Our test collection, which includes approximately 2,000,000 sentence pairs, can be used to train and test machine translation systems. Our test collection also includes search topics for cross-lingual patent retrieval and the contribution of machine translation to a patent retrieval task can also be evaluated. Our test collection will be available to the public for research purposes after the NTCIR final meeting."
2008.amta-papers.8,Toward the Evaluation of Machine Translation Using Patent Information,2008,12,6,4,0,37781,atsushi fujii,Proceedings of the 8th Conference of the Association for Machine Translation in the Americas: Research Papers,0,"To aid research and development in machine translation, we have produced a test collection for Japanese/English machine translation. To obtain a parallel corpus, we extracted patent documents for the same or related inventions published in Japan and the United States. Our test collection includes approximately 2000000 sentence pairs in Japanese and English, which were extracted automatically from our parallel corpus. These sentence pairs can be used to train and evaluate machine translation systems. Our test collection also includes search topics for cross-lingual patent retrieval, which can be used to evaluate the contribution of machine translation to retrieving patent documents across languages. This paper describes our test collection, methods for evaluating machine translation, and preliminary experiments."
2008.amta-papers.14,Integrating a Phrase-based {SMT} Model and a Bilingual Lexicon for Semi-Automatic Acquisition of Technical Term Translation Lexicons,2008,12,6,2,0,46255,yohei morishita,Proceedings of the 8th Conference of the Association for Machine Translation in the Americas: Research Papers,0,"This paper presents an attempt at developing a technique of acquiring translation pairs of technical terms with sufficiently high precision from parallel patent documents. The approach taken in the proposed technique is based on integrating the phrase translation table of a state-of-the-art statistical phrase-based machine translation model, and compositional translation generation based on an existing bilingual lexicon for human use. Our evaluation results clearly show that the agreement between the two individual techniques definitely contribute to improving precision of translation candidates. We then apply the Support Vector Machines (SVMs) to the task of automatically validating translation candidates in the phrase translation table. Experimental evaluation results again show that the SVMs based approach to translation candidates validation can contribute to improving the precision of translation candidates in the phrase translation table."
W07-1109,Learning Dependency Relations of {J}apanese Compound Functional Expressions,2007,8,8,1,1,15895,takehito utsuro,Proceedings of the Workshop on A Broader Perspective on Multiword Expressions,0,"This paper proposes an approach of processing Japanese compound functional expressions by identifying them and analyzing their dependency relations through a machine learning technique. First, we formalize the task of identifying Japanese compound functional expressions in a text as a machine learning based chunking problem. Next, against the results of identifying compound functional expressions, we apply the method of dependency analysis based on the cascaded chunking model. The results of experimental evaluation show that, the dependency analysis model achieves improvements when applied after identifying compound functional expressions, compared with the case where it is applied without identifying compound functional expressions."
W06-2404,Chunking {J}apanese Compound Functional Expressions by Machine Learning,2006,8,12,4,0,18299,masatoshi tsuchiya,Proceedings of the Workshop on Multi-word-expressions in a multilingual context,0,"The Japanese language has various types of compound functional expressions, which are very important for recognizing the syntactic structures of Japanese sentences and for understanding their semantic contents. In this paper, we formalize the task of identifying Japanese compound functional expressions in a text as a chunking problem. We apply a machine learning technique to this task, where we employ that of Support Vector Machines (SVMs). We show that the proposed method significantly outperforms existing Japanese text processing tools."
W06-1703,A comparative study on compositional translation estimation using a domain/topic-specific corpus collected from the Web,2006,12,11,5,1,45218,masatsugu tonoike,Proceedings of the 2nd International Workshop on Web as Corpus,0,"This paper studies issues related to the compilation of a bilingual lexicon for technical terms. In the task of estimating bilingual term correspondences of technical terms, it is usually rather difficult to find an existing corpus for the domain of such technical terms. In this paper, we adopt an approach of collecting a corpus for the domain of such technical terms from the Web. As a method of translation estimation for technical terms, we employ a compositional translation estimation technique. This paper focuses on quantitatively comparing variations of the components in the scoring functions of compositional translation estimation. Through experimental evaluation, we show that the domain/topic-specific corpus contributes toward improving the performance of the compositional translation estimation."
W06-1407,Adjective-to-Verb Paraphrasing in {J}apanese Based on Lexical Constraints of Verbs,2006,3,1,4,0,5049,atsushi fujita,Proceedings of the Fourth International Natural Language Generation Conference,0,"This paper describes adjective-to-verb paraphrasing in Japanese. In this paraphrasing, generated verbs require additional suffixes according to their difference in meaning. To determine proper suffixes for a given adjective-verb pair, we have examined the verbal features involved in the theory of Lexical Conceptual Structure."
P06-2046,{J}apanese Idiom Recognition: Drawing a Line between Literal and Idiomatic Meanings,2006,4,17,3,0,26950,chikara hashimoto,Proceedings of the {COLING}/{ACL} 2006 Main Conference Poster Sessions,0,"Recognizing idioms in a sentence is important to sentence understanding. This paper discusses the lexical knowledge of idioms for idiom recognition. The challenges are that idioms can be ambiguous between literal and idiomatic meanings, and that they can be transformed when expressed in a sentence. However, there has been little research on Japanese idiom recognition with its ambiguity and transformations taken into account. We propose a set of lexical knowledge for idiom recognition. We evaluated the knowledge by measuring the performance of an idiom recognizer that exploits the knowledge. As a result, more than 90% of the idioms in a corpus are recognized with 90% accuracy."
E06-1029,Compiling {F}rench-{J}apanese Terminologies from the Web,2006,11,31,5,0,50620,xavier robitaille,11th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"We propose a method for compiling bilingual terminologies of multi-word terms (MWTs) for given translation pairs of seed terms. Traditional methods for bilingual terminology compilation exploit parallel texts, while the more recent ones have focused on comparable corpora. We use bilingual corpora collected from the web and tailor made for the seed terms. For each language, we extract from the corpus a set of MWTs pertaining to the seedxe2x80x99s semantic domain, and use a compositional method to align MWTs from both sets. We increase the coverage of our system by using thesauri and by applying a bootstrap method. Experimental results show high precision and indicate promising prospects for future developments."
I05-2020,Effect of Domain-Specific Corpus in Compositional Translation Estimation for Technical Terms,2005,6,3,5,1,45218,masatsugu tonoike,Companion Volume to the Proceedings of Conference including Posters/Demos and tutorial abstracts,0,"This paper studies issues on compiling a bilingual lexicon for technical terms. In the task of estimating bilingual term correspondences of technical terms, it is usually quite difficult to find an existing corpus for the domain of such technical terms. In this paper, we take an approach of collecting a corpus for the domain of such technical terms from the Web. As a method of translation estimation for technical terms, we propose a compositional translation estimation technique. Through experimental evaluation, we show that the domain/topic specific corpus contributes to improving the performance of the compositional translation estimation."
W04-2012,Answer validation by keyword association,2004,8,12,2,1,45218,masatsugu tonoike,Proceedings of the 3rd workshop on {RO}bust Methods in Analysis of Natural Language Data ({ROMAND} 2004),0,"Answer validation is a component of question answering system, which selects reliable answer from answer candidates extracted by certain methods. In this paper, we propose an approach of answer validation based on the strengths of lexical association between the keywords extracted from a question sentence and each answer candidate. The proposed answer validation process is decomposed into two steps: the first is to extract appropriate keywords from a question sentence using word features and the strength of lexical association, while the second is to estimate the strength of the association between the keywords and an answer candidate based on the hits of search engines. In the result of experimental evaluation, we show that a good proportion (79%) of a multiple-choice quiz Who wants to be a millionaire can be solved by the proposed method."
N04-4004,An Empirical Study on Multiple {LVCSR} Model Combination by Machine Learning,2004,10,7,1,1,15895,takehito utsuro,Proceedings of {HLT}-{NAACL} 2004: Short Papers,0,"This paper proposes to apply machine learning techniques to the task of combining outputs of multiple LVCSR models. The proposed technique has advantages over that by voting schemes such as ROVER, especially when the majority of participating models are not reliable. In this machine learning framework, as features of machine learning, information such as the model IDs which output the hypothesized word are useful for improving the word recognition rate. Experimental results show that the combination results achieve a relative word error reduction of up to 39% against the best performing single model and that of up to 23% against ROVER. We further empirically show that it performs better when LVCSR models to be combined are chosen so as to cover as many correctly recognized words as possible, rather than choosing models in descending order of their word correct rates."
C04-1149,Integrating Cross-Lingually Relevant News Articles and Monolingual Web Documents in Bilingual Lexicon Acquisition,2004,7,7,1,1,15895,takehito utsuro,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"In the framework of bilingual lexicon acquisition from cross-lingually relevant news articles on the Web, it is relatively harder to reliably estimate bilingual term correspondences for low frequency terms. Considering such a situation, this paper proposes to complementarily use much larger monolingual Web documents collected by search engines, as a resource for reliably re-estimating bilingual term correspondences. We experimentally show that, using a sufficient number of monolingual Web documents, it is quite possible to have reliable estimate of bilingual term correspondences for those low frequency terms."
E03-1023,Effect of Cross-Language {IR} in Bilingual Lexicon Acquisition from Comparable Corpora,2003,9,12,1,1,15895,takehito utsuro,10th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"Within the framework of translation knowledge acquisition from WWW news sites, this paper studies issues on the effect of cross-language retrieval of relevant texts in bilingual lexicon acquisition from comparable corpora. We experimentally show that it is quite effective to reduce the candidate bilingual term pairs against which bilingual term correspondences are estimated, in terms of both computational complexity and the performance of precise estimation of bilingual term correspondences."
W02-1036,Combining Outputs of Multiple {J}apanese Named Entity Chunkers by Stacking,2002,12,10,1,1,15895,takehito utsuro,Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing ({EMNLP} 2002),0,"In this paper, we propose a method for learning a classifier which combines outputs of more than one Japanese named entity extractors. The proposed combination method belongs to the family of stacked generalizers, which is in principle a technique of combining outputs of several classifiers at the first stage by learning a second stage classifier to combine those outputs at the first stage. Individual models to be combined are based on maximum entropy models, one of which always considers surrounding contexts of a fixed length, while the other considers those of variable lengths according to the number of constituent morphemes of named entities. As an algorithm for learning the second stage classifier, we employ a decision list learning method. Experimental evaluation shows that the proposed method achieves improvement over the best known results with Japanese named entity extractors based on maximum entropy models."
narita-etal-2002-web,A Web-based {E}nglish Abstract Writing Tool Using a Tagged {E}-{J} Parallel Corpus,2002,8,7,3,0,51932,masumi narita,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"In this paper, we present a Web-based English abstract writing tool, the xe2x80x9cBEAR (Building English Abstracts by Ricoh).xe2x80x9d This English writing tool is aimed at helping Japanese software engineers improve the organization of their writing by enabling them to select a rhetorical template of the target abstract and to build up component sentences while having access to good-quality sample sentences. To provide this kind of language assistance, we constructed an E-J parallel corpus of 539 sample abstracts as the core language resource. After analyzing the rhetorical structure of these sample abstracts, we tagged the corpus with textual and linguistic information. The xe2x80x9cBEARxe2x80x9d is not designed for beginners but for intermediate to advanced EFL learners who very often need to write a research paper or a technical report in English. Software development has not yet been completed, but we have already gathered some user feedback at preliminary user trials. We show that the xe2x80x9cBEARxe2x80x9d has been positively evaluated by our users and thus our tagged E-J parallel corpus of sample abstracts can support our users in the difficult task of working with a foreign language. We also discuss the outlook for further development of the xe2x80x9cBEAR.xe2x80x9d"
utsuro-etal-2002-semi,Semi-automatic compilation of bilingual lexcion entries from cross-lingually relevant news articles on {WWW} news sites,2002,4,11,1,1,15895,takehito utsuro,Proceedings of the 5th Conference of the Association for Machine Translation in the Americas: Technical Papers,0,"For the purpose of overcoming resource scarcity bottleneck in corpus-based translation knowledge acquisition research, this paper takes an approach of semi-automatically acquiring domain specific translation knowledge from the collection of bilingual news articles on WWW news sites. This paper presents results of applying standard co-occurrence frequency based techniques of estimating bilingual term correspondences from parallel corpora to relevant article pairs automatically collected from WWW news sites. The experimental evaluation results are very encouraging and it is proved that many useful bilingual term correspondences can be efficiently discovered with little human intervention from relevant article pairs on WWW news sites."
utsuro-2000-learning,Learning Preference of Dependency between {J}apanese Subordinate Clauses and its Evaluation in Parsing,2000,6,2,1,1,15895,takehito utsuro,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,"Utsuro et al., 2000) proposed statistical method for learning dependency preference of Japanese subordinate clauses, in which scope embedding preference of subordinate clauses is exploited as a useful information source for disambiguating dependencies between subordinate clauses. Following (Utsuro et al., 2000), this paper presents detailed results of evaluating the proposed method by comparing it with several closely related existing techniques and shows that the proposed method outperforms those existing techniques."
utsuro-sassano-2000-minimally,Minimally Supervised {J}apanese Named Entity Recognition: Resources and Evaluation,2000,11,5,1,1,15895,takehito utsuro,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,"Approaches to named entity recognition that rely on hand-crafted rules and/or supervised learning techniques have limitations in terms of their portability into new domains as well as in the robustness over time. For the purpose of overcoming those limitations, this paper evaluates named entity chunking and classi cation techniques in Japanese named entity recognition in the context of minimally supervised learning. This experimental evaluation demonstrates that the minimally supervised learning method proposed here improved the performance of the seed knowledge on named entity chunking and classi cation. We also investigated the correlation between performance of the minimally supervised learning and the sizes of the training resources such as the seed set as well as the unlabeled training data."
itou-etal-2000-ipa,{IPA} {J}apanese Dictation Free Software Project,2000,6,11,7,0,53570,katsunobu itou,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,"LREC2000: the 2nd International Conference on Language Resources and Evaluation, May 31 - June 2, 2000, Athens, Greece."
C00-2102,Named Entity Chunking Techniques in Supervised Learning for {J}apanese Named Entity Recognition,2000,7,25,2,0,31516,manabu sassano,{COLING} 2000 Volume 2: The 18th International Conference on Computational Linguistics,0,"This paper focuses on the issue of named entity chunking in Japanese named entity recognition. We apply the supervised decision list learning method to Japanese named entity recognition. We also investigate and incorporate several named-entity noun phrase chunking techniques and experimentally evaluate and compare their performance. In addition, we propose a method for incorporating richer contextual information as well as patterns of constituent morphemes within a named entity, which have not been considered in previous research, and show that the proposed method outperforms these previous approaches."
A00-2015,Analyzing Dependencies of {J}apanese Subordinate Clauses based on Statistics of Scope Embedding Preference,2000,7,6,1,1,15895,takehito utsuro,1st Meeting of the North {A}merican Chapter of the Association for Computational Linguistics,0,"This paper proposes a statistical method for learning dependency preference of Japanese subordinate clauses, in which scope embedding preference of subordinate clauses is exploited as a useful information source for disambiguating dependencies between subordinate clauses. Estimated dependencies of subordinate clauses successfully increase the precision of an existing statistical dependency analyzer."
P98-2214,General-to-Specific Model Selection for Subcategorization Preference,1998,11,2,1,1,15895,takehito utsuro,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 2",0,"This paper proposes a novel method for learning probability models of subcategorization preference of verbs. We consider the issues of case dependencies and noun class generalization in a uniform way by employing the maximum entropy modeling method. We also propose a new model selection algorithm which starts from the most general model and gradually examines more specific models. In the experimental evaluation, it is shown that both of the case dependencies and specific sense restriction selected by the proposed method contribute to improving the performance in subcategorization preference resolution."
C98-2209,General-to-Specific Model Selection for Subcategorization Preference,1998,11,2,1,1,15895,takehito utsuro,{COLING} 1998 Volume 2: The 17th International Conference on Computational Linguistics,0,"This paper proposes a novel method for learning probability models of subcategorization preference of verbs. We consider the issues of case dependencies and noun class generalization in a uniform way by employing the maximum entropy modeling method. We also propose a new model selection algorithm which starts from the most general model and gradually examines more specific models. In the experimental evaluation, it is shown that both of the case dependencies and specific sense restriction selected by the proposed method contribute to improving the performance in subcategorization preference resolution."
W97-0123,Maximum Entropy Model Learning of Subcategorization Preference,1997,8,8,1,1,15895,takehito utsuro,Fifth Workshop on Very Large Corpora,0,"Abstract This paper proposes a novel method for learning probabilistic models of subcategorization preference of verbs. Especially, we propose to consider the issues of case dependencie~ and noun class generalization in a uniform way. We adopt the maximum entropy model learn~,g method and apply it to the task of model learning of subcategorization preference. Case dependencies and noun class generalization are represented as featura~ in the maximum entropy approach. The feature selection facility of the maximum entropy model learning makes it possible to find optimal case dependencies and optimal noun c ! ~ generalization levels. We describe the results of the experiment on learning probabilistic models of subcategorization preference f~om the EDR Japanese bracketed corpus. We also evaluated the performance of the selected features and their estimated parameters in the subcategorization preference task."
A97-1053,Learning Probabilistic Subcategorization Preference by Identifying Case Dependencies and Optimal Noun Class Generalization Level,1997,13,10,1,1,15895,takehito utsuro,Fifth Conference on Applied Natural Language Processing,0,"This paper proposes a novel method of learning probabilistic subcategorization preference. In the method, for the purpose of coping with the ambiguities of case dependencies and noun class generalization of argument/adjunct nouns, we introduce a data structure which represents a tuple of independent partial subcategorization frames. Each collocation of a verb and argument/adjunct nouns is assumed to be generated from one of the possible tuples of independent partial subcategorization frames. Parameters of subcategorization preference are then estimated so as to maximize the subcategorization preference function for each collocation of a verb and argument/adjunct nouns in the training corpus. We also describe the results of the experiments on learning probabilistic subcategorization preference from the EDR Japanese bracketed corpus, as well as those on evaluating the performance of subcategorization preference."
C96-2163,Sense Classification of Verbal Polysemy based-on Bilingual Class/Class Association,1996,7,3,1,1,15895,takehito utsuro,{COLING} 1996 Volume 2: The 16th International Conference on Computational Linguistics,0,"In the field of statistical analysis of natural langauge data, the measure of word/class association has proved to be quite useful for discovering a meaningful sense cluster in an arbitrary level of the thesaurus. In this paper, we apply its idea to the sense classification of Japanese verbal polysemy in case frame acquisition from Japanese-English parallel corpora. Measures of bilingual class/class association and bilingual class/frame association are introduced and used for discovering sense clusters in the sense distribution of English predicates and Japanese case element nouns. In a small experiment, 93.3% of the discovered clusters are correct in that none of them contains examples of more than one hand-classified senses."
C94-2169,Thesaurus-based Efficient Example Retrieval by Generating Retrieval Queries from Similarities,1994,3,20,1,1,15895,takehito utsuro,{COLING} 1994 Volume 2: The 15th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"In example-based NLP, the problem of computational cost of example retrieval is severe, since the retrieval time increases in proportion to the number of examples in the database. This paper proposes a novel example retrieval method for avoiding full retrieval of examples. The proposed method has the following three features, 1) it generates retrieval queries from similarities, 2) efficient example retrieval through the tree structure of a thesaurus, 3) binary search along subsumption ordering of retrieval queries. Example retrieval time drastically decreases with the method."
C94-2175,"Bilingual Text, Matching using Bilingual Dictionary and Statistics",1994,16,33,1,1,15895,takehito utsuro,{COLING} 1994 Volume 2: The 15th {I}nternational {C}onference on {C}omputational {L}inguistics,0,This paper describes a unified framework for bilingual text matching by combining existing hand-written bilingual dictionaries and statistical techniques. The process of bilingual text matching consists of two major steps: sentence alignment and structural matching of bilingual sentences. Statistical techniques are applied to estimate word correspondences not included in bilingual dictionaries. Estimated word correspondences are useful for improving both sentence alignment and structural matching.
P93-1004,Structural Matching of Parallel Texts,1993,11,96,2,0,991,yuji matsumoto,31st Annual Meeting of the Association for Computational Linguistics,1,"This paper describes a method for finding structural matching between parallel sentences of two languages, (such as Japanese and English). Parallel sentences are analyzed based on unification grammars, and structural matching is performed by making use of a similarity measure of word pairs in the two languages. Syntactic ambiguities are resolved simultaneously in the matching process. The results serve as a useful source for extracting linguistic and lexical knowledge."
C92-2088,Lexical Knowledge Acquisition from Bilingual Corpora,1992,8,20,1,1,15895,takehito utsuro,{COLING} 1992 Volume 2: The 14th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"For practical research in natural language processing, it is indispensable to develop a large scale semantic dictionary for computers. It is especially important to improve the techniques for compiling semantic dictionaries from natural language texts such as those in existing human dictionaries or in large corpora. However, there are at least two difficulties in analyzing existing texts: the problem of syntactic ambiguities and the problem of polysemy. Our approach to solve these difficulties is to make use of translation examples in two distinct languages that have quite different syntactic structures and word meanings. The reason we took this approach is that in many cases both syntactic and semantic ambiguities are resolved by comparing analyzed results from both languages. In this paper, we propose a method for resolving the syntactic ambiguities of translation examples of bilingual corpora and a method for acquiring lexical knowledge, such as case frames of verbs and attribute sets of nouns."
