2021.vardial-1.5,Whit{'}s the Richt Pairt o Speech: {P}o{S} tagging for {S}cots,2021,-1,-1,2,0,633,harm lameris,"Proceedings of the Eighth Workshop on NLP for Similar Languages, Varieties and Dialects",0,"In this paper we explore PoS tagging for the Scots language. Scots is spoken in Scotland and Northern Ireland, and is closely related to English. As no linguistically annotated Scots data were available, we manually PoS tagged a small set that is used for evaluation and training. We use English as a transfer language to examine zero-shot transfer and transfer learning methods. We find that training on a very small amount of Scots data was superior to zero-shot transfer from English. Combining the Scots and English data led to further improvements, with a concatenation method giving the best results. We also compared the use of two different English treebanks and found that a treebank containing web data was superior in the zero-shot setting, while it was outperformed by a treebank containing a mix of genres when combined with Scots data."
2021.unimplicit-1.7,A Mention-Based System for Revision Requirements Detection,2021,-1,-1,3,0,669,ahmed ruby,Proceedings of the 1st Workshop on Understanding Implicit and Underspecified Language,0,"Exploring aspects of sentential meaning that are implicit or underspecified in context is important for sentence understanding. In this paper, we propose a novel architecture based on mentions for revision requirements detection. The goal is to improve understandability, addressing some types of revisions, especially for the Replaced Pronoun type. We show that our mention-based system can predict replaced pronouns well on the mention-level. However, our combined sentence-level system does not improve on the sentence-level BERT baseline. We also present additional contrastive systems, and show results for each type of edit."
2021.semeval-1.15,{U}ppsala {NLP} at {S}em{E}val-2021 Task 2: Multilingual Language Models for Fine-tuning and Feature Extraction in Word-in-Context Disambiguation,2021,-1,-1,3,0,1659,huiling you,Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021),0,"We describe the Uppsala NLP submission to SemEval-2021 Task 2 on multilingual and cross-lingual word-in-context disambiguation. We explore the usefulness of three pre-trained multilingual language models, XLM-RoBERTa (XLMR), Multilingual BERT (mBERT) and multilingual distilled BERT (mDistilBERT). We compare these three models in two setups, fine-tuning and as feature extractors. In the second case we also experiment with using dependency-based information. We find that fine-tuning is better than feature extraction. XLMR performs better than mBERT in the cross-lingual setting both with fine-tuning and feature extraction, whereas these two models give a similar performance in the multilingual setting. mDistilBERT performs poorly with fine-tuning but gives similar results to the other models when used as a feature extractor. We submitted our two best systems, fine-tuned with XLMR and mBERT."
2021.nodalida-main.32,Investigation of Transfer Languages for Parsing {L}atin: Italic Branch vs. {H}ellenic Branch,2021,-1,-1,2,0,2686,antonia karamolegkou,Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa),0,"Choosing a transfer language is a crucial step in transfer learning. In much previous research on dependency parsing, related languages have successfully been used. However, when parsing Latin, it has been suggested that languages such as ancient Greek could be helpful. In this work we parse Latin in a low-resource scenario, with the main goal to investigate if Greek languages are more helpful for parsing Latin than related Italic languages, and show that this is indeed the case. We further investigate the influence of other factors including training set size and content as well as linguistic distances. We find that one explanatory factor seems to be the syntactic similarity between Latin and Ancient Greek. The influence of genres or shared annotation projects seems to have a smaller impact."
2020.tlt-1.6,Cross-Lingual Domain Adaptation for Dependency Parsing,2020,-1,-1,1,1,634,sara stymne,Proceedings of the 19th International Workshop on Treebanks and Linguistic Theories,0,None
2020.nlpbt-1.5,{IESTAC}: {E}nglish-{I}talian Parallel Corpus for End-to-End Speech-to-Text Machine Translation,2020,-1,-1,2,0,16325,giuseppe corte,Proceedings of the First International Workshop on Natural Language Processing Beyond Text,0,"We discuss a set of methods for the creation of IESTAC: a English-Italian speech and text parallel corpus designed for the training of end-to-end speech-to-text machine translation models and publicly released as part of this work. We first mapped English LibriVox audiobooks and their corresponding English Gutenberg Project e-books to Italian e-books with a set of three complementary methods. Then we aligned the English and the Italian texts using both traditional Gale-Church based alignment methods and a recently proposed tool to perform bilingual sentences alignment computing the cosine similarity of multilingual sentence embeddings. Finally, we forced the alignment between the English audiobooks and the English side of our textual parallel corpus with a text-to-speech and dynamic time warping based forced alignment tool. For each step, we provide the reader with a critical discussion based on detailed evaluation and comparison of the results of the different methods."
2020.mwe-1.14,Edition 1.2 of the {PARSEME} Shared Task on Semi-supervised Identification of Verbal Multiword Expressions,2020,-1,-1,17,0,12002,carlos ramisch,Proceedings of the Joint Workshop on Multiword Expressions and Electronic Lexicons,0,"We present edition 1.2 of the PARSEME shared task on identification of verbal multiword expressions (VMWEs). Lessons learned from previous editions indicate that VMWEs have low ambiguity, and that the major challenge lies in identifying test instances never seen in the training data. Therefore, this edition focuses on unseen VMWEs. We have split annotated corpora so that the test corpora contain around 300 unseen VMWEs, and we provide non-annotated raw corpora to be used by complementary discovery methods. We released annotated and raw corpora in 14 languages, and this semi-supervised challenge attracted 7 teams who submitted 9 system results. This paper describes the effort of corpus creation, the task design, and the results obtained by the participating systems, especially their performance on unseen expressions."
2020.lrec-1.103,"{SL}{\\\a}{ND}a: An Annotated Corpus of Narrative and Dialogue in {S}wedish Literary Fiction""",2020,-1,-1,1,1,634,sara stymne,Proceedings of the 12th Language Resources and Evaluation Conference,0,"We describe a new corpus, SL{\""a}NDa, the Swedish Literary corpus of Narrative and Dialogue. It contains Swedish literary fiction, which has been manually annotated for cited materials, with a focus on dialogue. The annotation covers excerpts from eight Swedish novels written between 1879{--}1940, a period of modernization of the Swedish language. SL{\""a}NDa contains annotations for all cited materials that are separate from the main narrative, like quotations and signs. The main focus is on dialogue, for which we annotate speech segments, speech tags, and speakers. In this paper we describe the annotation protocol and procedure and show that we can reach a high inter-annotator agreement. In total, SL{\""a}NDa contains annotations of 44 chapters with over 220K tokens. The annotation identified 4,733 instances of cited material and 1,143 named speaker{--}speech mappings. The corpus is useful for developing computational tools for different types of analysis of literary narrative and speech. We perform a small pilot study where we show how our annotation can help in analyzing language change in Swedish. We find that a number of common function words have their modern version appear earlier in speech than in narrative."
2020.cl-4.3,What Should/Do/Can {LSTM}s Learn When Parsing Auxiliary Verb Constructions?,2020,-1,-1,2,0.815603,372,miryam lhoneux,Computational Linguistics,0,"There is a growing interest in investigating what neural NLP models learn about language. A prominent open question is the question of whether or not it is necessary to model hierarchical structure. We present a linguistic investigation of a neural parser adding insights to this question. We look at transitivity and agreement information of auxiliary verb constructions (AVCs) in comparison to finite main verbs (FMVs). This comparison is motivated by theoretical work in dependency grammar and in particular the work of Tesni{\`e}re (1959), where AVCs and FMVs are both instances of a nucleus, the basic unit of syntax. An AVC is a dissociated nucleus; it consists of at least two words, and an FMV is its non-dissociated counterpart, consisting of exactly one word. We suggest that the representation of AVCs and FMVs should capture similar information. We use diagnostic classifiers to probe agreement and transitivity information in vectors learned by a transition-based neural parser in four typologically different languages. We find that the parser learns different information about AVCs and FMVs if only sequential models (BiLSTMs) are used in the architecture but similar information when a recursive layer is used. We find explanations for why this is the case by looking closely at how information is learned in the network and looking at what happens with different dependency representations of AVCs. We conclude that there may be benefits to using a recursive layer in dependency parsing and that we have not yet found the best way to integrate it in our parsers."
2020.calcs-1.4,Evaluating Word Embeddings for {I}ndonesian{--}{E}nglish Code-Mixed Text Based on Synthetic Data,2020,-1,-1,2,0,22192,arradi rizal,Proceedings of the The 4th Workshop on Computational Approaches to Code Switching,0,"Code-mixed texts are abundant, especially in social media, and poses a problem for NLP tools, which are typically trained on monolingual corpora. In this paper, we explore and evaluate different types of word embeddings for Indonesian{--}English code-mixed text. We propose the use of code-mixed embeddings, i.e. embeddings trained on code-mixed text. Because large corpora of code-mixed text are required to train embeddings, we describe a method for synthesizing a code-mixed corpus, grounded in literature and a survey. Using sentiment analysis as a case study, we show that code-mixed embeddings trained on synthesized data are at least as good as cross-lingual embeddings and better than monolingual embeddings."
W18-6305,Discourse-Related Language Contrasts in {E}nglish-{C}roatian Human and Machine Translation,2018,-1,-1,3,0,27748,margita vsovstaric,Proceedings of the Third Conference on Machine Translation: Research Papers,0,"We present an analysis of a number of coreference phenomena in English-Croatian human and machine translations. The aim is to shed light on the differences in the way these structurally different languages make use of discourse information and provide insights for discourse-aware machine translation system development. The phenomena are automatically identified in parallel data using annotation produced by parsers and word alignment tools, enabling us to pinpoint patterns of interest in both languages. We make the analysis more fine-grained by including three corpora pertaining to three different registers. In a second step, we create a test set with the challenging linguistic constructions and use it to evaluate the performance of three MT systems. We show that both SMT and NMT systems struggle with handling these discourse phenomena, even though NMT tends to perform somewhat better than SMT. By providing an overview of patterns frequently occurring in actual language use, as well as by pointing out the weaknesses of current MT systems that commonly mistranslate them, we hope to contribute to the effort of resolving the issue of discourse phenomena in MT applications."
P18-2098,Parser Training with Heterogeneous Treebanks,2018,15,0,1,1,634,sara stymne,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"How to make the most of multiple heterogeneous treebanks when training a monolingual dependency parser is an open question. We start by investigating previously suggested, but little evaluated, strategies for exploiting multiple treebanks based on concatenating training sets, with or without fine-tuning. We go on to propose a new method based on treebank embeddings. We perform experiments for several languages and show that in many cases fine-tuning and treebank embeddings lead to substantial improvements over single treebanks or concatenation, with average gains of 2.0{--}3.5 LAS points. We argue that treebank embeddings should be preferred due to their conceptual simplicity, flexibility and extensibility."
K18-2011,"82 Treebanks, 34 Models: {U}niversal {D}ependency Parsing with Multi-Treebank Models",2018,13,0,6,0,29069,aaron smith,Proceedings of the {C}o{NLL} 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies,0,"We present the Uppsala system for the CoNLL 2018 Shared Task on universal dependency parsing. Our system is a pipeline consisting of three components: the first performs joint word and sentence segmentation; the second predicts part-of-speech tags and morphological features; the third predicts dependency trees from words and tags. Instead of training a single parsing model for each treebank, we trained models with multiple treebanks for one language or closely related languages, greatly reducing the number of models. On the official test run, we ranked 7th of 27 teams for the LAS and MLAS metrics. Our system obtained the best scores overall for word segmentation, universal POS tagging, and morphological features."
D18-1291,"An Investigation of the Interactions Between Pre-Trained Word Embeddings, Character Models and {POS} Tags in Dependency Parsing",2018,0,9,3,0,29069,aaron smith,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"We provide a comprehensive analysis of the interactions between pre-trained word embeddings, character models and POS tags in a transition-based dependency parser. While previous studies have shown POS information to be less important in the presence of character models, we show that in fact there are complex interactions between all three techniques. In isolation each produces large improvements over a baseline system using randomly initialised word embeddings only, but combining them quickly leads to diminishing returns. We categorise words by frequency, POS tag and language in order to systematically investigate how each of the techniques affects parsing quality. For many word categories, applying any two of the three techniques is almost as good as the full combined system. Character models tend to be more important for low-frequency open-class words, especially in morphologically rich languages, while POS tags can help disambiguate high-frequency function words. We also show that large character embedding sizes help even for languages with small character sets, especially in morphologically rich languages."
W17-6314,Arc-Hybrid Non-Projective Dependency Parsing with a Static-Dynamic Oracle,2017,11,6,2,0.977012,372,miryam lhoneux,Proceedings of the 15th International Conference on Parsing Technologies,0,"In this paper, we extend the arc-hybrid system for transition-based parsing with a swap transition that enables reordering of the words and construction of non-projective trees. Although this extension breaks the arc-decomposability of the transition system, we show how the existing dynamic oracle for this system can be modified and combined with a static oracle only for the swap transition. Experiments on 5 languages show that the new system gives competitive accuracy and is significantly better than a system trained with a purely static oracle."
W17-4801,Findings of the 2017 {D}isco{MT} Shared Task on Cross-lingual Pronoun Prediction,2017,0,3,2,0,4241,sharid loaiciga,Proceedings of the Third Workshop on Discourse in Machine Translation,0,"We describe the design, the setup, and the evaluation results of the DiscoMT 2017 shared task on cross-lingual pronoun prediction. The task asked participants to predict a target-language pronoun given a source-language pronoun in the context of a sentence. We further provided a lemmatized target-language human-authored translation of the source sentence, and automatic word alignments between the source sentence words and the target-language lemmata. The aim of the task was to predict, for each target-language pronoun placeholder, the word that should replace it from a small, closed set of classes, using any type of information that can be extracted from the entire document. We offered four subtasks, each for a different language pair and translation direction: English-to-French, English-to-German, German-to-English, and Spanish-to-English. Five teams participated in the shared task, making submissions for all language pairs. The evaluation results show that most participating teams outperformed two strong n-gram-based language model-based baseline systems by a sizable margin."
W17-4805,A {B}i{LSTM}-based System for Cross-lingual Pronoun Prediction,2017,8,0,1,1,634,sara stymne,Proceedings of the Third Workshop on Discourse in Machine Translation,0,We describe the Uppsala system for the 2017 DiscoMT shared task on cross-lingual pronoun prediction. The system is based on a lower layer of BiLSTMs reading the source and target sentences respectively. Classification is based on the BiLSTM representation of the source and target positions for the pronouns. In addition we enrich our system with dependency representations from an external parser and character representations of the source sentence. We show that these additions perform well for German and Spanish as source languages. Our system is competitive and is in first or second place for all language pairs.
W17-0301,Learning with learner corpora: Using the {TLE} for native language identification,2017,0,0,2,0,23628,allison adams,Proceedings of the joint workshop on {NLP} for Computer Assisted Language Learning and {NLP} for Language Acquisition,0,This study investigates the usefulness of the Treebank of Learner English (TLE) when applied to the task of Native Language Identification (NLI). The TLE is effectively a parallel corpus of Standar ...
W17-0306,Annotating errors in student texts: First experiences and experiments,2017,16,1,1,1,634,sara stymne,Proceedings of the joint workshop on {NLP} for Computer Assisted Language Learning and {NLP} for Language Acquisition,0,We describe the creation of an annotation layer for word-based writing errors for a corpus of student writings. The texts are written in Swedish by students between 9 and 19 years old. Our main pur ...
W17-0230,The Effect of Translationese on Tuning for Statistical Machine Translation,2017,14,7,1,1,634,sara stymne,Proceedings of the 21st Nordic Conference on Computational Linguistics,0,We explore how the translation direction in the tuning set used for statistical machine translation affects the translation results. We explore this issue for three language pairs. While the result ...
K17-3022,"From Raw Text to {U}niversal {D}ependencies - Look, No Tags!",2017,9,2,5,0.977012,372,miryam lhoneux,Proceedings of the {C}o{NLL} 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies,0,"We present the Uppsala submission to the CoNLL 2017 shared task on parsing from raw text to universal dependencies. Our system is a simple pipeline consisting of two components. The first performs joint word and sentence segmentation on raw text; the second predicts dependency trees from raw words. The parser bypasses the need for part-of-speech tagging, but uses word embeddings based on universal tag distributions. We achieved a macro-averaged LAS F1 of 65.11 in the official test run, which improved to 70.49 after bug fixes. We obtained the 2nd best result for sentence segmentation with a score of 89.03."
W16-2326,"Phrase-Based {SMT} for {F}innish with More Data, Better Models and Alternative Alignment and Translation Tools",2016,27,3,5,0,2675,jorg tiedemann,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,"This paper summarises the contributions of the teams at the University of Helsinki, Uppsala University and the University of Turku to the news translation tasks for translating from and to Finnish. Our models address the problem of treating morphology and data coverage in various ways. We introduce a new efficient tool for word alignment and discuss factorisations, gappy language models and reinflection techniques for generating proper Finnish output. The results demonstrate once again that training data is the most effective way to increase translation performance."
W16-2345,Findings of the 2016 {WMT} Shared Task on Cross-lingual Pronoun Prediction,2016,20,11,4,0,5894,liane guillou,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,"We describe the design, the evaluation setup, and the results of the 2016 WMT shared task on cross-lingual pronoun prediction. This is a classification task in which participants are asked to provi ..."
W16-2355,Feature Exploration for Cross-Lingual Pronoun Prediction,2016,12,1,1,1,634,sara stymne,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,"We explore a large number of features for cross-lingual pronoun prediction for translation between English and German/French. We find that features related to German/French are more informative than features related to English, regardless of the translation direction. Our most useful features are local context, dependency head features, and source pronouns. We also find that it is sometimes more successful to employ a 2-step procedure that first makes a binary choice between pronouns and other, then classifies pronouns. For the pronoun/other distinction POS ngrams were very useful."
W16-2390,The {UU} Submission to the Machine Translation Quality Estimation Task,2016,14,2,2,0,33915,oscar sagemo,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,This paper outlines the UU-SVM system for Task 1 of the WMT16 Shared Task in Quality Estimation. Our system uses Support Vector Machine Regression to investigate the impact of a series of features aiming to convey translation quality. We propose novel features measuring reordering and noun translation errors. We show that we can outperform the baseline when we combine it with a subset of our new features.
W15-2501,Pronoun-Focused {MT} and Cross-Lingual Pronoun Prediction: Findings of the 2015 {D}isco{MT} Shared Task on Pronoun Translation,2015,48,33,3,0.486597,670,christian hardmeier,Proceedings of the Second Workshop on Discourse in Machine Translation,0,"We describe the design, the evaluation setup, and the results of the DiscoMT 2015 shared task, which included two subtasks, relevant to both the machine translation (MT) and the discourse communities: (i) pronoun-focused translation, a practical MT task, and (ii) cross-lingual pronoun prediction, a classification task that requires no specific MT expertise and is interesting as a machine learning task in its own right. We focused on the Englishxe2x80x90French language pair, for which MT output is generally of high quality, but has visible issues with pronoun translation due to differences in the pronoun systems of the two languages. Six groups participated in the pronoun-focused translation task and eight groups in the cross-lingual pronoun prediction task."
W14-3312,Anaphora Models and Reordering for Phrase-Based {SMT},2014,28,6,2,0.645017,670,christian hardmeier,Proceedings of the Ninth Workshop on Statistical Machine Translation,0,"We describe the Uppsala University systems for WMT14. We look at the integration of a model for translating pronominal anaphora and a syntactic dependency projection model for Englishxe2x80x90French. Furthermore, we investigate post-ordering and tunable POS distortion models for Englishxe2x80x90 German."
W14-3334,Estimating Word Alignment Quality for {SMT} Reordering Tasks,2014,40,0,1,1,634,sara stymne,Proceedings of the Ninth Workshop on Statistical Machine Translation,0,"Previous studies of the effect of word alignment on translation quality in SMT generally explore link level metrics only and mostly do not show any clear connections between alignment and SMT quality. In this paper, we specifically investigate the impact of word alignment on two pre-reordering tasks in translation, using a wider range of quality indicators than previously done. Experiments on Germanxe2x80x90English translation show that reordering may require alignment models different from those used by the core translation system. Sparse alignments with high precision on the link level, for translation units, and on the subset of crossing links, like intersected HMM models, are preferred. Unlike SMT performance the desired alignment characteristics are similar for small and large training data for the pre-reordering tasks. Moreover, we confirm previous research showing that the fuzzy reordering score is a useful and cheap proxy for performance on SMT reordering tasks."
W13-5634,Statistical Machine Translation with Readability Constraints,2013,26,15,1,1,634,sara stymne,Proceedings of the 19th Nordic Conference of Computational Linguistics ({NODALIDA} 2013),0,"This paper presents experiments with document-level machine translation with readability constraints. We describe the task of producing simplified translations from a given source with the aim to optimize machine translation for specific target users such as language learners. In our approach, we introduce global features that are known to affect readability into a documentlevel SMT decoding framework. We show that the decoder is capable of incorporating those features and that we can influence the readability of the output as measured by common metrics. This study presents the first attempt of jointly performing machine translation and text simplification, which is demonstrated through the case of translating parliamentary texts from English to Swedish."
W13-3308,Feature Weight Optimization for Discourse-Level {SMT},2013,36,8,1,1,634,sara stymne,Proceedings of the Workshop on Discourse in Machine Translation,0,"We present an approach to feature weight optimization for document-level decoding. This is an essential task for enabling future development of discourse-level statistical machine translation, as it allows easy integration of discourse features in the decoding process. We extend the framework of sentence-level feature weight optimization to the document-level. We show experimentally that we can get competitive and relatively stable results when using a standard set of features, and that this framework also allows us to optimize documentlevel features, which can be used to model discourse phenomena."
W13-2229,Tunable Distortion Limits and Corpus Cleaning for {SMT},2013,17,11,1,1,634,sara stymne,Proceedings of the Eighth Workshop on Statistical Machine Translation,0,"We describe the Uppsala University system for WMT13, for English-to-German translation. We use the Docent decoder, a local search decoder that translates at the document level. We add tunable distortion limits, that is, soft constraints on the maximum distortion allowed, to Docent. We also investigate cleaning of the noisy Common Crawl corpus. We show that we can use alignment-based filtering for cleaning with good results. Finally we investigate effects of corpus selection for recasing."
P13-4033,{D}ocent: A Document-Level Decoder for Phrase-Based Statistical Machine Translation,2013,20,30,2,0.645017,670,christian hardmeier,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations,0,"We describe Docent, an open-source decoder for statistical machine translation that breaks with the usual sentence-bysentence paradigm and translates complete documents as units. By taking translation to the document level, our decoder can handle feature models with arbitrary discourse-wide dependencies and constitutes an essential infrastructure component in the quest for discourse-aware SMT models. 1 Motivation"
J13-4009,Generation of Compound Words in Statistical Machine Translation into Compounding Languages,2013,58,9,1,1,634,sara stymne,Computational Linguistics,0,"In this article we investigate statistical machine translation SMT into Germanic languages, with a focus on compound processing. Our main goal is to enable the generation of novel compounds that have not been seen in the training data. We adopt a split-merge strategy, where compounds are split before training the SMT system, and merged after the translation step. This approach reduces sparsity in the training data, but runs the risk of placing translations of compound parts in non-consecutive positions. It also requires a postprocessing step of compound merging, where compounds are reconstructed in the translation output. We present a method for increasing the chances that components that should be merged are translated into contiguous positions and in the right order and show that it can lead to improvements both by direct inspection and in terms of standard translation evaluation metrics. We also propose several new methods for compound merging, based on heuristics and machine learning, which outperform previously suggested algorithms. These methods can produce novel compounds and a translation with at least the same overall quality as the baseline. For all subtasks we show that it is useful to include part-of-speech based information in the translation process, in order to handle compounds."
W12-0704,Clustered Word Classes for Preordering in Statistical Machine Translation,2012,35,8,1,1,634,sara stymne,Proceedings of the Joint Workshop on Unsupervised and Semi-Supervised Learning in {NLP},0,"Clustered word classes have been used in connection with statistical machine translation, for instance for improving word alignments. In this work we investigate if clustered word classes can be used in a preordering strategy, where the source language is reordered prior to training and translation. Part-of-speech tagging has previously been successfully used for learning reordering rules that can be applied before training and translation. We show that we can use word clusters for learning rules, and significantly improve on a baseline with only slightly worse performance than for standard POS-tags on an English--German translation task. We also show the usefulness of the approach for the less-resourced language Haitian Creole, for translation into English, where the suggested approach is significantly better than the baseline."
stymne-etal-2012-eye,Eye Tracking as a Tool for Machine Translation Error Analysis,2012,19,13,1,1,634,sara stymne,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"We present a preliminary study where we use eye tracking as a complement to machine translation (MT) error analysis, the task of identifying and classifying MT errors. We performed a user study where subjects read short texts translated by three MT systems and one human translation, while we gathered eye tracking data. The subjects were also asked comprehension questions about the text, and were asked to estimate the text quality. We found that there are a longer gaze time and a higher number of fixations on MT errors, than on correct parts. There are also differences in the gaze time of different error types, with word order errors having the longest gaze time. We also found correlations between eye tracking data and human estimates of text quality. Overall our study shows that eye tracking can give complementary information to error analysis, such as aiding in ranking error types for seriousness."
stymne-ahrenberg-2012-practice,On the practice of error analysis for machine translation evaluation,2012,15,20,1,1,634,sara stymne,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"Error analysis is a means to assess machine translation output in qualitative terms, which can be used as a basis for the generation of error profiles for different systems. As for other subjective approaches to evaluation it runs the risk of low inter-annotator agreement, but very often in papers applying error analysis to MT, this aspect is not even discussed. In this paper, we report results from a comparative evaluation of two systems where agreement initially was low, and discuss the different ways we used to improve it. We compared the effects of using more or less fine-grained taxonomies, and the possibility to restrict analysis to short sentences only. We report results on inter-annotator agreement before and after measures were taken, on error categories that are most likely to be confused, and on the possibility to establish error profiles also in the absence of a high inter-annotator agreement."
holmqvist-etal-2012-alignment,Alignment-based reordering for {SMT},2012,15,7,2,1,43310,maria holmqvist,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"We present a method for improving word alignment quality for phrase-based statistical machine translation by reordering the source text according to the target word order suggested by an initial word alignment. The reordered text is used to create a second word alignment which can be an improvement of the first alignment, since the word order is more similar. The method requires no other pre-processing such as part-of-speech tagging or parsing. We report improved Bleu scores for English-to-German and English-to-Swedish translation. We also examined the effect on word alignment quality and found that the reordering method increased recall while lowering precision, which partly can explain the improved Bleu scores. A manual evaluation of the translation output was also performed to understand what effect our reordering method has on the translation system. We found that where the system employing reordering differed from the baseline in terms of having more words, or a different word order, this generally led to an improvement in translation quality."
W11-4648,Iterative reordering and word alignment for statistical {MT},2011,17,2,1,1,634,sara stymne,Proceedings of the 18th Nordic Conference of Computational Linguistics ({NODALIDA} 2011),0,"Word alignment is necessary for statistical machine translation (SMT), and reordering as a preprocessing step has been shown to improve SMT for many language pairs. In this initial study we investigate if both word alignment and reordering can be improved by iterating these two steps, since they both depend on each other. Overall no consistent improvements were seen on the translation task, but the reordering rules contain different information in the different iterations, leading us to believe that the iterative strategy can be useful."
W11-2129,Productive Generation of Compound Words in Statistical Machine Translation,2011,24,13,1,1,634,sara stymne,Proceedings of the Sixth Workshop on Statistical Machine Translation,0,"In many languages the use of compound words is very productive. A common practice to reduce sparsity consists in splitting compounds in the training data. When this is done, the system incurs the risk of translating components in non-consecutive positions, or in the wrong order. Furthermore, a post-processing step of compound merging is required to reconstruct compound words in the output. We present a method for increasing the chances that components that should be merged are translated into contiguous positions and in the right order. We also propose new heuristic methods for merging components that outperform all known methods, and a learning-based method that has similar accuracy as the heuristic method, is better at producing novel compounds, and can operate with no background linguistic resources."
W11-2147,"Experiments with word alignment, normalization and clause reordering for {SMT} between {E}nglish and {G}erman",2011,19,3,2,1,43310,maria holmqvist,Proceedings of the Sixth Workshop on Statistical Machine Translation,0,This paper presents the LIU system for the WMT 2011 shared task for translation between German and English. For English--German we attempted to improve the translation tables with a combination of standard statistical word alignments and phrase-based word alignments. For German--English translation we tried to make the German text more similar to the English text by normalizing German morphology and performing rule-based clause reordering of the German text. This resulted in small improvements for both translation directions.
W11-2159,Spell Checking Techniques for Replacement of Unknown Words and Data Cleaning for {H}aitian {C}reole {SMS} Translation,2011,21,10,1,1,634,sara stymne,Proceedings of the Sixth Workshop on Statistical Machine Translation,0,We report results on translation of SMS messages from Haitian Creole to English. We show improvements by applying spell checking techniques to unknown words and creating a lattice with the best known spelling equivalents. We also used a small cleaned corpus to train a cleaning model that we applied to the noisy corpora.
P11-4010,{B}last: A Tool for Error Analysis of Machine Translation Output,2011,15,43,1,1,634,sara stymne,Proceedings of the {ACL}-{HLT} 2011 System Demonstrations,0,"We present Blast, an open source tool for error analysis of machine translation (MT) output. We believe that error analysis, i.e., to identify and classify MT errors, should be an integral part of MT development, since it gives a qualitative view, which is not obtained by standard evaluation methods. Blast can aid MT researchers and users in this process, by providing an easy-to-use graphical user interface. It is designed to be flexible, and can be used with any MT system, language pair, and error typology. The annotation task can be aided by highlighting similarities with a reference translation."
P11-3003,Pre- and Postprocessing for Statistical Machine Translation into Germanic Languages,2011,27,7,1,1,634,sara stymne,Proceedings of the {ACL} 2011 Student Session,0,"In this thesis proposal I present my thesis work, about pre- and postprocessing for statistical machine translation, mainly into Germanic languages. I focus my work on four areas: compounding, definite noun phrases, reordering, and error correction. Initial results are positive within all four areas, and there are promising possibilities for extending these approaches. In addition I also focus on methods for performing thorough error analysis of machine translation output, which can both motivate and evaluate the studies performed."
2011.eamt-1.39,Definite Noun Phrases in Statistical Machine Translation into Scandinavian Languages,2011,19,2,1,1,634,sara stymne,Proceedings of the 15th Annual conference of the European Association for Machine Translation,0,"In this thesis I aim to improve phrase-based statistical machine translation (PBSMT) in a number of ways by the use of text harmonization strategies. PBSMT systems are built by training statistical models on large corpora of human translations. This architecture generally performs well for languages with similar structure. If the languages are different for example with respect to word order or morphological complexity, however, the standard methods do not tend to work well. I address this problem through text harmonization, by making texts more similar before training and applying a PBSMT system. I investigate how text harmonization can be used to improve PBSMT with a focus on four areas: compounding, definiteness, word order, and unknown words. For the first three areas, the focus is on linguistic differences between languages, which I address by applying transformation rules, using either rule-based or machine learning-based techniques, to the source or target data. For the last area, unknown words, I harmonize the translation input to the training data by replacing unknown words with known alternatives. I show that translation into languages with closed compounds can be improved by splitting and merging compounds. I develop new merging algorithms that outperform previously suggested algorithms and show how part-of-speech tags can be used to improve the order of compound parts. Scandinavian definite noun phrases are identified as a problem forPBSMT in translation into Scandinavian languages and I propose a preprocessing approach that addresses this problem and gives large improvements over a baseline. Several previous proposals for how to handle differences in reordering exist; I propose two types of extensions, iterating reordering and word alignment and using automatically induced word classes, which allow these methods to be used for less-resourced languages. Finally I identify several ways of replacing unknown words in the translation input, most notably a spell checking-inspired algorithm, which can be trained using character-based PBSMT techniques. Overall I present several approaches for extending PBSMT by the use of pre- and postprocessing techniques for text harmonization, and show experimentally that these methods work. Text harmonization methods are an efficient way to improve statistical machine translation within the phrase-based approach, without resorting to more complex models."
W10-1727,{V}s and {OOV}s: Two Problems for Translation between {G}erman and {E}nglish,2010,18,8,1,1,634,sara stymne,Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and {M}etrics{MATR},0,"In this paper we report on experiments with three preprocessing strategies for improving translation output in a statistical MT system. In training, two reordering strategies were studied: (i) reorder on the basis of the alignments from Giza, and (ii) reorder by moving all verbs to the end of segments. In translation, out-of-vocabulary words were preprocessed in a knowledge-lite fashion to identify a likely equivalent. All three strategies were implemented for our Englishxe2x86x94German system submitted to the WMT10 shared task. Combining them lead to improvements in both language directions."
stymne-ahrenberg-2010-using,Using a Grammar Checker for Evaluation and Postprocessing of Statistical Machine Translation,2010,23,24,1,1,634,sara stymne,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"One problem in statistical machine translation (SMT) is that the output often is ungrammatical. To address this issue, we have investigated the use of a grammar checker for two purposes in connection with SMT: as an evaluation tool and as a postprocessing tool. To assess the feasibility of the grammar checker on SMT output, we performed an error analysis, which showed that the precision of error identification in general was higher on SMT output than in previous studies on human texts. Using the grammar checker as an evaluation tool gives a complementary picture to standard metrics such as Bleu, which do not account well for grammaticality. We use the grammar checker as a postprocessing tool by automatically applying the error correction suggestions it gives. There are only small overall improvements of the postprocessing on automatic metrics, but the sentences that are affected by the changes are improved, as shown both by automatic metrics and by a human error analysis. These results indicate that grammar checker techniques are a useful complement to SMT."
W09-0421,Improving Alignment for {SMT} by Reordering and Augmenting the Training Corpus,2009,17,16,2,0.952381,43310,maria holmqvist,Proceedings of the Fourth Workshop on Statistical Machine Translation,0,"We describe the LIU systems for English-German and German-English translation in the WMT09 shared task. We focus on two methods to improve the word alignment: (i) by applying Giza in a second phase to a reordered training corpus, where reordering is based on the alignments from the first phase, and (ii) by adding lexical data obtained as high-precision alignments from a different word aligner. These methods were studied in the context of a system that uses compound processing, a morphological sequence model for German, and a part-of-speech sequence model for English. Both methods gave some improvements to translation quality as measured by Bleu and Meteor scores, though not consistently. All systems used both out-of-domain and in-domain data as the mixed corpus had better scores in the baseline configuration."
E09-3008,A Comparison of Merging Strategies for Translation of {G}erman Compounds,2009,20,29,1,1,634,sara stymne,Proceedings of the Student Research Workshop at {EACL} 2009,0,"In this article, compound processing for translation into German in a factored statistical MT system is investigated. Compounds are handled by splitting them prior to training, and merging the parts after translation. I have explored eight merging strategies using different combinations of external knowledge sources, such as word lists, and internal sources that are carried through the translation process, such as symbols or parts-of-speech. I show that for merging to be successful, some internal knowledge source is needed. I also show that an extra sequence model for part-of-speech is useful in order to improve the order of compound parts in the output. The best merging results are achieved by a matching scheme for part-of-speech tags."
W08-0317,Effects of Morphological Analysis in Translation between {G}erman and {E}nglish,2008,7,23,1,1,634,sara stymne,Proceedings of the Third Workshop on Statistical Machine Translation,0,"We describe the LIU systems for German-English and English-German translation submitted to the Shared Task of the Third Workshop of Statistical Machine Translation. The main features of the systems, as compared with the baseline, is the use of morphological pre- and post-processing, and a sequence model for German using morphologically rich parts-of-speech. It is shown that these additions lead to improved translations."
2008.eamt-1.25,Processing of {S}wedish compounds for phrase-based statistical machine translation,2008,18,20,1,1,634,sara stymne,Proceedings of the 12th Annual conference of the European Association for Machine Translation,0,"In this thesis I explore how compound processing can be used to improve phrase-based statistical machine translation (PBSMT) between English and German/Swedish. Both German and Swedish generally use closed compounds, which are written as one word without spaces or other indicators of word boundaries. Compounding is both common and productive, which makes it problematic for PBSMT, mainly due to sparse data problems. The adopted strategy for compound processing is to split compounds into their component parts before training and translation. For translation into Swedish and German the parts are merged after translation. I investigate the effect of different splitting algorithms for translation between English and German, and of different merging algorithms for German. I also apply these methods to a different language pair, English--Swedish. Overall the studies show that compound processing is useful, especially for translation from English into German or Swedish. But there are improvements for translation into English as well, such as a reduction of unknown words. I show that for translation between English and German different splitting algorithms work best for different translation directions. I also design and evaluate a novel merging algorithm based on part-of-speech matching, which outperforms previous methods for compound merging, showing the need for information that is carried through the translation process, rather than only external knowledge sources such as word lists. Most of the methods for compound processing were originally developed for German. I show that these methods can be applied to Swedish as well, with similar results."
W07-0723,Getting to Know {M}oses: Initial Experiments on {G}erman-{E}nglish Factored Translation,2007,5,14,2,0.952381,43310,maria holmqvist,Proceedings of the Second Workshop on Statistical Machine Translation,0,We present results and experiences from our experiments with phrase-based statistical machine translation using Moses. The paper is based on the idea of using an off-the-shelf parser to supply linguistic information to a factored translation model and compare the results of German---English translation to the shared task baseline system based on word form. We report partial results for this model and results for two simplified setups. Our best setup takes advantage of the parser's lemmatization and decompounding. A qualitative analysis of compound translation shows that decompounding improves translation quality.
2006.eamt-1.2,A Bilingual Grammar for Translation of {E}nglish-{S}wedish Verb Frame Divergences,2006,17,3,1,1,634,sara stymne,Proceedings of the 11th Annual conference of the European Association for Machine Translation,0,We describe a bilingual grammar used for translation of verb frame divergences between Swedish and English. The grammar is used both for analysis and generation with Minimal Recursion Semantics as interlingua. Our grammar is based on the delph-in resources for which semantic transfer is proposed for MT. We show that an interlingua strategy based on a bilingual grammar can handle many cases of verb frame divergences minimising the need of transfer.
