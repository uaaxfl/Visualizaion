2020.conll-1.30,Q17-1010,0,0.279833,"ems with unidirectional training. B ERT is pretrained on the BookCorpus (800M words) and English Wikipedia (2500M words). The BookCorpus (Zhu et al., 2015) is a collection of 11,038 books available on the Web, from 16 different genres, taking into account only books with more than 20K words to avoid noise coming from shorter stories. In all experiments below, we use the Huggin377 face version of B ERT1 , specifically the “bert-baseuncased” model that we expressly do not modify. FastText The original FAST T EXT model is based on Wikipedia dumps2 in nine differerent languages including English (Bojanowski et al., 2017). However, in this work, we used the pre-trained FASTT EXT embeddings provided by the official site of FAST T EXT, that we expressly do not modify.3 The embeddings are trained on 600-billion tokens from CommonCrawl4 , resulting in two-million word vectors with subword information. 3.2 Human word association norms Nelson et al. (2004) propose a large dataset of free association, rhyme and word fragment norms, elicited from more than 6000 participants. The participants were asked to write the first word that came to mind when presented a particular stimulus word. More than 750’000 free associati"
2020.conll-1.30,J90-1003,0,0.443986,"or certain kinds of associations. We find, instead, that the property of asymmetric similarity does not appear to conform to the operationalisations we tested. 8 Related work Recent interest in vectorial representations of words derives from the realisation that the meaning of words is much better represented when the rich networks of similarities and dissimilarities among words are taken into account. But the realisation that such notions are central to our word representations, that they can be estimated from a corpus and that such representations can be very technologically apt is not new. Church and Hanks (1990) brought to the attention of the computational linguistic community a notion of word association as the informationtheoretic measure of mutual information estimated on large corpora. It was shown that if word order was taken into account the measure could be asymmetric. Mutual information and other word association measures have been intensely studied to describe multi-word expressions or collocations, a stumbling block for many NLP applications (for a recent survey, see Constant et al. (2017)). Levy and Goldberg (2014) show that word embeddings are closely related to information-theoretic not"
2020.conll-1.30,J17-4005,0,0.0369596,"Missing"
2020.conll-1.30,C16-1175,0,0.0542921,"Missing"
2020.conll-1.30,D19-1635,0,0.0250019,"distributed representation of words, and aim to produce vectors that represent a word, or the substrings that compose a word, with information about its surroundings, so that word vectors that share the same meaning tend to be close. A recent comparison of word embeddings, Word2vec and Glove, to Nelson’s norms indicates that vectorial representations that do not take context into account, unlike B ERT, still are unable to capture the triangle inequality (Nematzadeh et al., 2017). 2019), and have revealed important properties of these spaces, from gender stereotypes and demographic variation (Du et al., 2019; Garimella et al., 2017) to their usefulness in the detection of puns (Sevgili et al., 2017), among many others. 9 Conclusions The work described in this paper starts from the assumption that word associations are the expression of underlying meaning properties of words. It confirms that context-aware word embeddings exhibit some properties of human association norms, despite being a vectorial representation of words in space. Future work needs to clarify the underlying mechanisms that give rise to these properties, extend the study to new languages, leveraging also newer association norms (D"
2020.conll-1.30,D15-1036,0,0.0862872,"Missing"
2020.conll-1.30,S17-2074,0,0.0167791,"or the substrings that compose a word, with information about its surroundings, so that word vectors that share the same meaning tend to be close. A recent comparison of word embeddings, Word2vec and Glove, to Nelson’s norms indicates that vectorial representations that do not take context into account, unlike B ERT, still are unable to capture the triangle inequality (Nematzadeh et al., 2017). 2019), and have revealed important properties of these spaces, from gender stereotypes and demographic variation (Du et al., 2019; Garimella et al., 2017) to their usefulness in the detection of puns (Sevgili et al., 2017), among many others. 9 Conclusions The work described in this paper starts from the assumption that word associations are the expression of underlying meaning properties of words. It confirms that context-aware word embeddings exhibit some properties of human association norms, despite being a vectorial representation of words in space. Future work needs to clarify the underlying mechanisms that give rise to these properties, extend the study to new languages, leveraging also newer association norms (De Deyne et al., 2019). It will also extend the investigation of word associations to other pr"
2020.conll-1.30,W19-2006,0,0.0496699,"Missing"
2020.conll-1.30,E17-1016,0,0.0413393,"Missing"
2021.naacl-main.39,Q17-1010,0,0.0461493,"t of the classifier comes from the CLWE models. We report the average accuracy over ten runs. Language pairs In this paper, we focus on projecting foreign language embeddings into the English space. We choose the eight languages included in MLDoc for both the BLI and CLDC tasks. Within the seven non-English languages, Japanese, Russian and Chinese are languages distant from English and the others are languages similar to English. For the task of BLI, we also investigate Turkish, another language distant from English. Monolingual word embeddings We use the pretrained FastText embedding models (Bojanowski et al., 2017) for our experiments. These embeddings of 300 dimensions are pretrained on Wikipedia dumps and publicly available.6 Following previous works, we use the first 200,000 most 5 6 https://github.com/zhangmozhi/retrofit_clwe https://fasttext.cc/docs/en/pretrained-vectors.html BLI Task - with refinement de es fr it ja ru tr zh PROC RCSLS 73.1 83.6 82.2 77.5 37.9 64.3 63.1 40.0 73.1 83.1 83.1 78.9 39.3 64.6 63.1 43.0 MUSE VecMap 73.7 83.0 82.2 78.5 29.3 62.7 60.5 38.1 73.6 83.7 82.9 78.5 34.7 63.1 61.3 36.4 Ours GRef 74.1 83.7 82.4 78.6 34.1 64.0 61.2 38.2 Ours LRef 66.6 79.3 77.8 70.3 23.7 46.5 39.7"
2021.naacl-main.39,D18-1214,0,0.0354404,"Missing"
2021.naacl-main.39,P19-1070,0,0.0248263,"Missing"
2021.naacl-main.39,P17-1042,0,0.0213616,"ictionary. Early work from mately isomorphic. We assume instead that, Mikolov et al. (2013) uses a seed dictionary of fiveespecially across distant languages, the mapthousand word pairs. Since then, the size of the ping is only piece-wise linear, and propose a multi-adversarial learning method. This novel seed dictionary has been gradually reduced, from method induces the seed cross-lingual dictioseveral-thousand to fifty word pairs (Smith et al., nary through multiple mappings, each induced 2017), reaching a minimal version of only sharing to fit the mapping for one subspace. Our exnumerals (Artetxe et al., 2017). periments on unsupervised bilingual lexicon More recent works on unsupervised learning induction and cross-lingual document classifihave shown that mappings across embedding cation show that this method improves performance over previous single-mapping methods, spaces can also be learned without any bilingual evespecially for distant languages. idence (Barone, 2016; Zhang et al., 2017; Conneau et al., 2018; Hoshen and Wolf, 2018; Alvarez-Melis 1 Introduction and background and Jaakkola, 2018; Artetxe et al., 2018). More conWord embeddings, continuous vectorial represen- cretely, these fully"
2021.naacl-main.39,P15-1119,0,0.029859,"part, cross-lingual word step based on this seed dictionary. embeddings (CLWE) —maps of matching words across languages— have been shown to be useful in The system proposed by Conneau et al. (2018) many important cross-lingual transfer and model- can be considered the first successful unsupervised ing tasks such as machine translation, cross-lingual system for learning CLWE. They first use generadocument classification and zero-shot dependency tive adversarial networks (GANs) to learn a single parsing (Klementiev et al., 2012; Zou et al., 2013; linear mapping to induce the seed dictionary, folGuo et al., 2015; Conneau et al., 2018; Glavaš et al., lowed by the Procrustes Analysis (Schönemann, 2019; Zhang et al., 2020). 1966) to refine the linear mapping based on the In these representations, matching words across induced seed dictionary. While this GAN-based different languages are represented by similar vec- model has competitive or even better performance tors. Following the observation of Mikolov et al. compared to supervised methods on typologically(2013) that the geometric positions of similar words similar language pairs, it often exhibits poor perin two embedding spaces of different language"
2021.naacl-main.39,P18-1073,0,0.221034,"l version of only sharing to fit the mapping for one subspace. Our exnumerals (Artetxe et al., 2017). periments on unsupervised bilingual lexicon More recent works on unsupervised learning induction and cross-lingual document classifihave shown that mappings across embedding cation show that this method improves performance over previous single-mapping methods, spaces can also be learned without any bilingual evespecially for distant languages. idence (Barone, 2016; Zhang et al., 2017; Conneau et al., 2018; Hoshen and Wolf, 2018; Alvarez-Melis 1 Introduction and background and Jaakkola, 2018; Artetxe et al., 2018). More conWord embeddings, continuous vectorial represen- cretely, these fully unsupervised methods usually consist of two main steps (Hartmann et al., 2019): tations of words, have become a fundamental an unsupervised step which aims to induce the seed initial step in many natural language processing (NLP) tasks for many languages. In recent years, dictionary by matching the source and target distributions, and then a pseudo-supervised refinement their cross-lingual counterpart, cross-lingual word step based on this seed dictionary. embeddings (CLWE) —maps of matching words across languages—"
2021.naacl-main.39,W16-1614,0,0.0225084,"d cross-lingual dictioseveral-thousand to fifty word pairs (Smith et al., nary through multiple mappings, each induced 2017), reaching a minimal version of only sharing to fit the mapping for one subspace. Our exnumerals (Artetxe et al., 2017). periments on unsupervised bilingual lexicon More recent works on unsupervised learning induction and cross-lingual document classifihave shown that mappings across embedding cation show that this method improves performance over previous single-mapping methods, spaces can also be learned without any bilingual evespecially for distant languages. idence (Barone, 2016; Zhang et al., 2017; Conneau et al., 2018; Hoshen and Wolf, 2018; Alvarez-Melis 1 Introduction and background and Jaakkola, 2018; Artetxe et al., 2018). More conWord embeddings, continuous vectorial represen- cretely, these fully unsupervised methods usually consist of two main steps (Hartmann et al., 2019): tations of words, have become a fundamental an unsupervised step which aims to induce the seed initial step in many natural language processing (NLP) tasks for many languages. In recent years, dictionary by matching the source and target distributions, and then a pseudo-supervised refinem"
2021.naacl-main.39,D18-1330,0,0.0121284,"by learning a multi-linear mapping instead of only a single-linear mapping. Therefore, we use the GAN-based system MUSE (Conneau et al., 2018)8 as our main unsupervised baseline. Since the unsupervised method proposed by Artetxe et al. (2018)9 is considered a robust CLWE system, we also use it as our second unsupervised baseline (VecMap in the tables). In the setting with refinement, we use the iterative refinement with stochastic dictionary induction for all the unsupervised systems.10 We also include two supervised systems, Procrustes (PROC) (Conneau et al., 2018) and Relaxed CSLS (RCSLS) (Joulin et al., 2018), to better understand 7 The original pretrained Latvian fastText model only consists of 171,000 words. 8 https://github.com/facebookresearch/MUSE 9 https://github.com/artetxem/vecmap 10 We disabled the re-weighting technique since it’s not applicable for L-Ref. However, adding re-weighting to VecMap, MUSE and G-Ref doesn’t change the gaps between them. 469 our method. Both PROC and RCSLS are robust supervised systems for learning CLWE and have been widely used previously (Glavaš et al., 2019; Zhang et al., 2020). We also wanted to include the supervised system proposed by Nakashole (2018), wh"
2021.naacl-main.39,C12-1089,0,0.0431514,"nd target distributions, and then a pseudo-supervised refinement their cross-lingual counterpart, cross-lingual word step based on this seed dictionary. embeddings (CLWE) —maps of matching words across languages— have been shown to be useful in The system proposed by Conneau et al. (2018) many important cross-lingual transfer and model- can be considered the first successful unsupervised ing tasks such as machine translation, cross-lingual system for learning CLWE. They first use generadocument classification and zero-shot dependency tive adversarial networks (GANs) to learn a single parsing (Klementiev et al., 2012; Zou et al., 2013; linear mapping to induce the seed dictionary, folGuo et al., 2015; Conneau et al., 2018; Glavaš et al., lowed by the Procrustes Analysis (Schönemann, 2019; Zhang et al., 2020). 1966) to refine the linear mapping based on the In these representations, matching words across induced seed dictionary. While this GAN-based different languages are represented by similar vec- model has competitive or even better performance tors. Following the observation of Mikolov et al. compared to supervised methods on typologically(2013) that the geometric positions of similar words similar la"
2021.naacl-main.39,D18-1047,0,0.0799538,"Because the isomorphism assumption is not observed in reality, we argue that a 464 1 2 https://github.com/facebookresearch/MUSE https://fasttext.cc/docs/en/pretrained-vectors.html To reduce the influence of infrequent words, we only consider the first fifty-thousand most frequent source words. As we can see in Figure 1, the distribution of accuracies of different subspaces is not uniform or even nearly so. This is true for both language pairs, but particularly for the distant languages, where the general mapping does not work at all in some subspaces. Similar phenomena were also discovered by Nakashole (2018) where source words are grouped into different categories. This lack of uniformity in results corroborates the appropriateness of designing a model that learns different linear mappings for different subspaces instead of only learning a single linear mapping for the entire source space. 3 Multi-adversarial CLWE learning To learn different mappings for different source subspaces, we propose a method for training one GAN for each source subspace. These multidiscriminator GANs encourage the distribution of mapped word embeddings from a specific source subspace to match the distribution of word em"
2021.naacl-main.39,P19-1018,0,0.0221705,"encourage words from a specific 3 source subspace to be trained against words from a We use different language discriminator models Dli for each subspace i, even though their training samples all come matching target subspace, we need to align the two from the same distributions. This leads to more stable training, cross-language subspaces. The second problem presumably because initially these language discriminators are randomly different. we need to solve for our multi-adversarial method 466 to work is how to discover this alignment. Although metrics such as Gromov-Hausdorff distance (GH) (Patra et al., 2019) and Eigenvalue Divergence (EVD) (Dubossarsky et al., 2020) can be used to measure the similarity between two distributions and find the most similar target subspace for a given source subspace, matching between two sub-distributions may amplify any bias generated during the clustering. To avoid this problem, we only run the clustering on the source side. For a given source embedding space denote its subspaces after clus V1s , we 2 i n tering as Vs , Vs , ..., Vs , ..., Vs , where n represent the number of subspaces. To align target words to their matching source subspace, we propose to first"
2021.naacl-main.39,L18-1560,0,0.0224698,"eau et al. (2018) for the task of BLI. This dataset contains high quality dictionaries for more than 150 language pairs. For each language pair, it provides a training dictionary of 5000 words and a test dictionary of 1500 words. This dataset allows us to have a better understanding of the performance of our proposal on many different language pairs. For each language pair, we retrieve the best translations of source words in the test dictionary using CSLS, and we report the accuracy with precision at one (P@1). CLDC setting We use the multilingual classification benchmark (MLDoc) provided by Schwenk and Li (2018) for the task of CLDC. MLDoc contains training and test documents with balanced class priors for eight languages: German (de), English (en), Spanish (es), French (fr), Italian (it), Japanese (ja), Russian (ru) and Chinese (zh). We follow previous works (Glavaš et al., 2019; Zhang et al., 2020) and train a CNN classifier on English using 10,000 documents and test the classifier on the other seven languages.5 Each language contains 4000 test documents. The input of the classifier comes from the CLWE models. We report the average accuracy over ten runs. Language pairs In this paper, we focus on p"
2021.naacl-main.39,P18-1072,0,0.0365931,"Missing"
2021.naacl-main.39,D19-1449,0,0.0334078,"Missing"
2021.naacl-main.39,2020.emnlp-main.257,0,0.0218187,"Missing"
2021.naacl-main.39,D19-1450,1,0.811119,"aining the GANs Training the GANs described in Sections 3.2 and 3.3 can be challenging. Based on previous work and our experience, we employ the following techniques during training. Orthogonalization Previous work shows that enforcing the mapping matrix W to be orthogonal during the training can improve the performance (Smith et al., 2017). In the system of Conneau et al. (2018), they follow the work of Cisse et al. (2017) and approximate setting W to an orthogonal matrix with W ← (1+β)W −β(W W > )W . This orthogonalization usually performs well when setting β to 0.001 (Conneau et al., 2018; Wang et al., 2019). Parameter-free hierarchical clustering A major issue in clustering an embedding space is how Cross-Domain Similarity Local Scaling The to find a clustering that adapts to the space, without trained mapping matrix W can be used for retrievfixed parameters. To avoid having to identify the number of subspaces in advance, we use hierarchi- ing the translation for a given source word ws by cal clustering. Recent work proposes a parameter- searching a target word wt whose embedding vecfree method called First Integer Neighbor Clus- tor vt is close to W vs . But Conneau et al. (2018) tering Hierarc"
2021.naacl-main.39,N15-1104,0,0.026297,"s. This cosine-based criterion has been shown to correlate well with the quality of W (Conneau et al., 2018; Hartmann et al., 2019). language (ws0 1 , ws0 2 , ..., ws0 10000 ). The mutual translation pairs (wsi , wti ) such that wsi = ws0 i constitute the seed dictionary. This guarantees that the induced seed dictionary will be bidirectional. Mapping refinement The refinement step is based on the Procrustes Analysis (Schönemann, 1966). With the seed dictionary, the mapping can be updated using the objective in equation (1), and forced to be orthogonal using singular value decomposition (SVD) (Xing et al., 2015). Later work combines the Procrustes Analysis with stochastic dictionary induction (Artetxe et al., 2018) and greatly improves the performance of the standard refinement (Hartmann et al., 2019). More specifically, in order to prevent local optima, after each iteration some elements of the similarity matrix are randomly dropped, so that the similarity distributions of words change randomly and the new seed dictionary for the next iteration varies. Random restarts Previous work (Vuli´c et al., 2019; Glavaš et al., 2019) shows that using GANs to train the mapping matrix W is not stable. Hartmann"
2021.naacl-main.39,P17-1179,0,0.0189729,"l dictioseveral-thousand to fifty word pairs (Smith et al., nary through multiple mappings, each induced 2017), reaching a minimal version of only sharing to fit the mapping for one subspace. Our exnumerals (Artetxe et al., 2017). periments on unsupervised bilingual lexicon More recent works on unsupervised learning induction and cross-lingual document classifihave shown that mappings across embedding cation show that this method improves performance over previous single-mapping methods, spaces can also be learned without any bilingual evespecially for distant languages. idence (Barone, 2016; Zhang et al., 2017; Conneau et al., 2018; Hoshen and Wolf, 2018; Alvarez-Melis 1 Introduction and background and Jaakkola, 2018; Artetxe et al., 2018). More conWord embeddings, continuous vectorial represen- cretely, these fully unsupervised methods usually consist of two main steps (Hartmann et al., 2019): tations of words, have become a fundamental an unsupervised step which aims to induce the seed initial step in many natural language processing (NLP) tasks for many languages. In recent years, dictionary by matching the source and target distributions, and then a pseudo-supervised refinement their cross-ling"
2021.naacl-main.39,2020.acl-main.201,0,0.23031,"ss languages— have been shown to be useful in The system proposed by Conneau et al. (2018) many important cross-lingual transfer and model- can be considered the first successful unsupervised ing tasks such as machine translation, cross-lingual system for learning CLWE. They first use generadocument classification and zero-shot dependency tive adversarial networks (GANs) to learn a single parsing (Klementiev et al., 2012; Zou et al., 2013; linear mapping to induce the seed dictionary, folGuo et al., 2015; Conneau et al., 2018; Glavaš et al., lowed by the Procrustes Analysis (Schönemann, 2019; Zhang et al., 2020). 1966) to refine the linear mapping based on the In these representations, matching words across induced seed dictionary. While this GAN-based different languages are represented by similar vec- model has competitive or even better performance tors. Following the observation of Mikolov et al. compared to supervised methods on typologically(2013) that the geometric positions of similar words similar language pairs, it often exhibits poor perin two embedding spaces of different languages ap- formance on typologically-distant language pairs, 463 Proceedings of the 2021 Conference of the North Am"
2021.naacl-main.39,P19-1307,0,0.0482446,"Missing"
2021.naacl-main.39,D13-1141,0,0.0315019,"and then a pseudo-supervised refinement their cross-lingual counterpart, cross-lingual word step based on this seed dictionary. embeddings (CLWE) —maps of matching words across languages— have been shown to be useful in The system proposed by Conneau et al. (2018) many important cross-lingual transfer and model- can be considered the first successful unsupervised ing tasks such as machine translation, cross-lingual system for learning CLWE. They first use generadocument classification and zero-shot dependency tive adversarial networks (GANs) to learn a single parsing (Klementiev et al., 2012; Zou et al., 2013; linear mapping to induce the seed dictionary, folGuo et al., 2015; Conneau et al., 2018; Glavaš et al., lowed by the Procrustes Analysis (Schönemann, 2019; Zhang et al., 2020). 1966) to refine the linear mapping based on the In these representations, matching words across induced seed dictionary. While this GAN-based different languages are represented by similar vec- model has competitive or even better performance tors. Following the observation of Mikolov et al. compared to supervised methods on typologically(2013) that the geometric positions of similar words similar language pairs, it o"
C00-2118,J93-2002,0,0.0711494,"Missing"
C00-2118,A97-1052,0,0.125704,"Missing"
C00-2118,P97-1003,0,0.0370437,"e (ripped o ) in the corpus of the causative, and borne out in our corpus analysis. from the intended change of state usage. passive or active use (pass), in a past participle or simple past use (vbn), in a causative or noncausative use (caus), and with an animate subject or not (anim), as described below. The rst three counts (trans, pass, vbn) were performed on the LDC&apos;s 65-million word tagged ACL/DCI corpus (Brown, and Wall Street Journal 1987{1989). The last two counts (caus and anim) were performed on a 29-million word parsed corpus (Wall Street Journal 1988, provided by Michael Collins (Collins, 1997)). The features were counted as follows: trans: The closest noun following a verb was considered a potential object. A verb immediately followed by a potential object was counted as transitive, otherwise as intransitive. pass: A token tagged VBD (the tag for simple past) was counted as active. A token tagged VBN (the tag for past participle) was counted as active if the closest preceding auxiliary was have , and as passive if the closest preceding auxiliary was be . vbn: The counts for VBN/VBD were based on the POS label in the tagged corpus. Each of the above counts was normalized over all oc"
C00-2118,P98-1046,0,0.062086,"n the explicit goal, the text has been semantically annotated (Webster and Marcus, 1989), or external semantic resources have been consulted (Aone and McKee, 1996). We extend these results by showing that thematic information can be induced from corpus counts. The experimental results show that our method is powerful, and suited to the classi cation of lexical items. However, we have not yet addressed the problem of verbs that can have multiple classi cations. We think that many cases of ambiguous classi cation of verb types can be addressed with the notion of intersective sets introduced by (Dang et al., 1998). This is an important concept that proposes that egular&quot; ambiguity in classi cation|i.e., sets of verbs that have the same multi-way classi cations according to (Levin, 1993)|can be captured with a nergrained notion of lexical semantic classes. Extending our work to exploit this idea requires only to de ne the classes appropriately; the basic approach will remain the same. When we turn to consider ambiguity, we must also address the problem that individual instances of verbs may come from di erent classes. In future research we plan to extend our method to the classi cation of ambiguous tok"
C00-2118,C96-1055,0,0.0956638,"accuracy in our classi cation is due to argument structure information, as subcategorization is the same for all verbs, con rming that the content of thematic roles is crucial for classi cation. Secondly, our results further support the assumption that thematic di erences such as these are apparent not only in di erences in subcategorization frames, but also in di erences in their frequencies. We thus join the many recent results that all seem to converge in supporting the view that the relation between lexical syntax and semantics can be usefully exploited (Aone and McKee, 1996; Dorr, 1997; Dorr and Jones, 1996; Lapata and Brew, 1999; Schulte im Walde, 1998; Siegel, 1998), especially in a statistical framework. Finally, we observe that this information is detectable in a corpus and can be learned automatically. Thus we view corpora, especially if annotated with currently available tools, as useful repositories of implicit grammars. Technically, our approach extends existing corpus-based learning techniques to a more complex learning problem, in several dimensions. Our statistical approach, which does not require explicit negative examples, extends approaches that encode Levin&apos;s alternations directly"
C00-2118,W99-0632,0,0.466322,"1 Introduction Detailed information about verbs is critical to a broad range of NLP and IR tasks, yet its manual determination for large numbers of verbs is dicult and resource intensive. Research on the automatic acquisition of verb-based knowledge has succeded in gleaning syntactic properties of verbs such as subcategorization frames from online resources (Brent, 1993; Briscoe and Carroll, 1997; Dorr, 1997; Manning, 1993). Recently, researchers have investigated statistical corpusbased methods for lexical semantic classi cation from syntactic properties of verb usage (Aone and McKee, 1996; Lapata and Brew, 1999; Schulte im Walde, 1998; Stevenson and Merlo, 1999; Stevenson et al., 1999; McCarthy, 2000). Corpus-based approaches to lexical semantic classi cation in particular have drawn on Levin&apos;s hypothesis (Levin, 1993) that verbs can be classi ed according to the diathesis alternations (alternations in the syntactic expressions of arguments) in which they participate|for example, whether a This research was partly sponsored by US NSF grants #9702331 and #9818322, Swiss NSF fellowship 821046569, Information Sciences Council of Rutgers University and IRCS, U. of Pennsylvania. This research was conduct"
C00-2118,merlo-stevenson-2000-establishing,1,0.795113,"Missing"
C00-2118,E99-1007,1,0.833312,"is critical to a broad range of NLP and IR tasks, yet its manual determination for large numbers of verbs is dicult and resource intensive. Research on the automatic acquisition of verb-based knowledge has succeded in gleaning syntactic properties of verbs such as subcategorization frames from online resources (Brent, 1993; Briscoe and Carroll, 1997; Dorr, 1997; Manning, 1993). Recently, researchers have investigated statistical corpusbased methods for lexical semantic classi cation from syntactic properties of verb usage (Aone and McKee, 1996; Lapata and Brew, 1999; Schulte im Walde, 1998; Stevenson and Merlo, 1999; Stevenson et al., 1999; McCarthy, 2000). Corpus-based approaches to lexical semantic classi cation in particular have drawn on Levin&apos;s hypothesis (Levin, 1993) that verbs can be classi ed according to the diathesis alternations (alternations in the syntactic expressions of arguments) in which they participate|for example, whether a This research was partly sponsored by US NSF grants #9702331 and #9818322, Swiss NSF fellowship 821046569, Information Sciences Council of Rutgers University and IRCS, U. of Pennsylvania. This research was conducted while the rst author was at Rutgers University."
C00-2118,W99-0503,1,0.525476,"ge of NLP and IR tasks, yet its manual determination for large numbers of verbs is dicult and resource intensive. Research on the automatic acquisition of verb-based knowledge has succeded in gleaning syntactic properties of verbs such as subcategorization frames from online resources (Brent, 1993; Briscoe and Carroll, 1997; Dorr, 1997; Manning, 1993). Recently, researchers have investigated statistical corpusbased methods for lexical semantic classi cation from syntactic properties of verb usage (Aone and McKee, 1996; Lapata and Brew, 1999; Schulte im Walde, 1998; Stevenson and Merlo, 1999; Stevenson et al., 1999; McCarthy, 2000). Corpus-based approaches to lexical semantic classi cation in particular have drawn on Levin&apos;s hypothesis (Levin, 1993) that verbs can be classi ed according to the diathesis alternations (alternations in the syntactic expressions of arguments) in which they participate|for example, whether a This research was partly sponsored by US NSF grants #9702331 and #9818322, Swiss NSF fellowship 821046569, Information Sciences Council of Rutgers University and IRCS, U. of Pennsylvania. This research was conducted while the rst author was at Rutgers University.  Paola Merlo LATL { Dep"
C00-2118,P89-1022,0,0.0330042,"approach by learning argument structure properties, which, unlike grammatical functions, are not marked morphologically. Others have tackled the problem of lexical semantic classi cation, as we have, but using only subcategorization frequencies as input data (Lapata and Brew, 1999; Schulte im Walde, 1998). By contrast, we explicitly address the de nition of features that can tap directly into thematic role differences that are not re ected in subcategorization distinctions. Finally, when learning of thematic role assignment has been the explicit goal, the text has been semantically annotated (Webster and Marcus, 1989), or external semantic resources have been consulted (Aone and McKee, 1996). We extend these results by showing that thematic information can be induced from corpus counts. The experimental results show that our method is powerful, and suited to the classi cation of lexical items. However, we have not yet addressed the problem of verbs that can have multiple classi cations. We think that many cases of ambiguous classi cation of verb types can be addressed with the notion of intersective sets introduced by (Dang et al., 1998). This is an important concept that proposes that egular&quot; ambiguity"
C00-2118,A00-2034,0,\N,Missing
C00-2118,C98-1046,0,\N,Missing
C00-2118,P93-1032,0,\N,Missing
C02-1146,H01-1035,0,\N,Missing
C02-1146,C00-2108,0,\N,Missing
C02-1146,J94-4003,0,\N,Missing
C02-1146,J95-4004,0,\N,Missing
C02-1146,P98-2247,0,\N,Missing
C02-1146,C98-2242,0,\N,Missing
C02-1146,J01-3003,1,\N,Missing
C02-1146,W99-0632,0,\N,Missing
C02-1146,W01-0705,1,\N,Missing
D19-1450,D16-1250,0,0.12013,"t procedure as Lample et al. (2018). After the core mapping matrix is learned, we use it to translate the ten thousand most frequent source words (s1 , s2 , ..., s10000 ) into the target language. We then take these ten thousand translations (t1 , t2 , ..., t10000 ) and translate them back into the source language. We call these translation (s01 , s02 , ..., s010000 ). We then consider all the word pairs (si , ti ) where si = s0i as our seed dictionary and use this dictionary to update our preliminary mapping matrix using the objective in equation 1. Moreover, following Xing et al. (2015) and Artetxe et al. (2016), we force the refined matrix W ∗ to be orthogonal by using singular value decomposition (SVD): W ∗ = ZU > , U ΣZ > = SV D(V s > V t ) 3.3 (6) Cross-Domain Similarity Local Scaling Previous work (Radovanovi´c et al., 2010; Dinu et al., 2015) has shown that standard nearest neighbour techniques are not effective to retrieve target similar words in high-dimensional spaces. The work of Lample et al. (2018) showed that using cross-domain similarity local scaling (CSLS) to retrieve target similar words is more accurate than standard nearest neighbour techniques. Instead of just considering the simi"
D19-1450,P17-1042,0,0.381239,"monolingual space in an end-to-end machine translation task. However, traditional methods for mapping two monolingual word embeddings require high quality aligned sentences or dictionaries (Faruqui and Dyer, 2014; Ammar et al., 2016b). Reducing the need for parallel data, then, has become the main issue for cross-lingual word embedding mapping. Some recent work aiming at reducing resources has shown competitive crosslingual mappings across similar languages, using a pseudo-dictionary, such as identical character strings between two languages (Smith et al., 2017), or a simple list of numerals (Artetxe et al., 2017). In a more general method, Zhang et al. (2017) have shown that learning mappings across languages via adversarial training (Goodfellow et al., 2014) can avoid using bilingual evidence. This generality comes at the expense of performance. To overcome this limitation, Lample et al. (2018) refine the preliminary mapping matrix trained by generative adversarial networks (GANs) and obtain a model that is again comparable to supervised models for several language pairs. Despite these big improvements, the performance of these refined GAN models depends largely on the quality of the preliminary mapp"
D19-1450,P18-1073,0,0.44959,"ranslations are retrieved by using CSLS. Bold face indicates the best result overall and italics indicate the best result between the two columns without refinement. pus. Each dictionary has a training set of 5000 entries and a test set of 1500 entries. Compared to BLI-1, this dataset is much noisier and the entries are selected from different frequency ranges. However, BLI-2 has been widely used for testing by previous methods (Faruqui and Dyer, 2014; Dinu et al., 2015; Xing et al., 2015; Artetxe et al., 2016; Zhang et al., 2016; Artetxe et al., 2017; Smith et al., 2017; Lample et al., 2018; Artetxe et al., 2018a,b).5 Using BLI-2 allows us to have a direct comparison with the state-of-the-art. Monolingual Word Embeddings The quality of monolingual word embeddings has a considerable impact on cross-lingual embeddings (Lample et al., 2018). Compared to CBOW and Skip-gram embeddings, FastText embeddings (Bojanowski et al., 2017) capture syntactic information better. The ideal situation would be to use FastText embeddings for both the BLI-1 and BLI-2 datasets. However, much previous work uses CBOW embeddings, so we use different monolingual word embeddings for BLI-1 and for BLI-2. For BLI-1, we use FastT"
D19-1450,P14-2131,0,0.043752,"which improves the performance of previous unsupervised adversarial methods for most languages, and especially for typologically distant language pairs. 1 Introduction Distributed representations of words which map each word to a continuous vector have proven useful in capturing important linguistic information. Vectors of words that are semantically or syntactically similar have been shown to be close to each other in the same space (Mikolov et al., 2013a,c; Pennington et al., 2014), making them widely useful in many natural language processing tasks such as machine translation and parsing (Bansal et al., 2014; Mi et al., 2016), both in a single language and across different languages. Mikolov et al. (2013b) first observed that the geometric positions of similar words in different languages were related by a linear relation. Zou et al. (2013) showed that a cross-lingually shared word embedding space is more useful than a monolingual space in an end-to-end machine translation task. However, traditional methods for mapping two monolingual word embeddings require high quality aligned sentences or dictionaries (Faruqui and Dyer, 2014; Ammar et al., 2016b). Reducing the need for parallel data, then, has"
D19-1450,D18-1024,0,0.0528092,") + Evs ∼pvs |c log(1 − Dcl (G(vs ), vc )) 3 Shared Components Both the GAN and the concept-based GAN models use a variety of further methods that have been shown to improve results. 3.1 Orthogonalization Previous studies (Xing et al., 2015; Smith et al., 2017) show that enforcing the mapping matrix W to be orthogonal can improve the performance and 4421 make the adversarial training more stable. In this work, we perform the same update step proposed by Cisse et al. (2017) to approximate setting W to an orthogonal matrix:   W ← (1 + β) W − β W W > W (5) According to Lample et al. (2018) and Chen and Cardie (2018), when setting β to less than 0.01, the orthogonalization usually performs well. 3.2 Post-refinement Previous work has shown that the core crosslingual mapping can be improved by refining it by bootstrapping from a dictionary extracted from the learned mapping itself (Lample et al., 2018). Since this refinement process is not the focus of this work, we perform the same refinement procedure as Lample et al. (2018). After the core mapping matrix is learned, we use it to translate the ten thousand most frequent source words (s1 , s2 , ..., s10000 ) into the target language. We then take these ten"
D19-1450,W17-6508,0,0.0160341,"g dictionary of 5000 words and an evaluation dictionary of 1500 words. This dataset allows us to have a better understanding of the performance of our method on many different language pairs. We choose nine languages for testing and compare our method to our supervised and unsupervised baselines described in section 5: English (en), German (de), Finnish (fi), French (fr), Spanish (es), Italian (it), Russian (ru), Turkish (tr) and Chinese (zh). We classify similar and distant languages based on a combination of structural properties (directional dependency distance, as proposed and measured in Chen and Gerdes (2017)) and lexical properties, as measured by the clustering of current large-scale multilingual sentence embeddings.4 We consider en  de, en  fr, en  es, en  it as similar language pairs and en  fi, en  ru, en  tr, en  zh as distant language pairs. BLI-2 Unlike BLI-1, the dataset of Dinu et al. (2015) and its extensions provided by Artetxe et al. (2017, 2018b) only consists of dictionaries of 4 language pairs trained on a Europarl parallel cor4 See for example the clustering in https://code.fb.com/airesearch/laser-multilingual-sentence-embeddings/ 4423 Supervised Similar language pairs Dis"
D19-1450,N19-1423,0,0.0245121,"Missing"
D19-1450,E14-1049,0,0.535658,"tural language processing tasks such as machine translation and parsing (Bansal et al., 2014; Mi et al., 2016), both in a single language and across different languages. Mikolov et al. (2013b) first observed that the geometric positions of similar words in different languages were related by a linear relation. Zou et al. (2013) showed that a cross-lingually shared word embedding space is more useful than a monolingual space in an end-to-end machine translation task. However, traditional methods for mapping two monolingual word embeddings require high quality aligned sentences or dictionaries (Faruqui and Dyer, 2014; Ammar et al., 2016b). Reducing the need for parallel data, then, has become the main issue for cross-lingual word embedding mapping. Some recent work aiming at reducing resources has shown competitive crosslingual mappings across similar languages, using a pseudo-dictionary, such as identical character strings between two languages (Smith et al., 2017), or a simple list of numerals (Artetxe et al., 2017). In a more general method, Zhang et al. (2017) have shown that learning mappings across languages via adversarial training (Goodfellow et al., 2014) can avoid using bilingual evidence. This"
D19-1450,Q17-1010,0,0.0918733,"s are selected from different frequency ranges. However, BLI-2 has been widely used for testing by previous methods (Faruqui and Dyer, 2014; Dinu et al., 2015; Xing et al., 2015; Artetxe et al., 2016; Zhang et al., 2016; Artetxe et al., 2017; Smith et al., 2017; Lample et al., 2018; Artetxe et al., 2018a,b).5 Using BLI-2 allows us to have a direct comparison with the state-of-the-art. Monolingual Word Embeddings The quality of monolingual word embeddings has a considerable impact on cross-lingual embeddings (Lample et al., 2018). Compared to CBOW and Skip-gram embeddings, FastText embeddings (Bojanowski et al., 2017) capture syntactic information better. The ideal situation would be to use FastText embeddings for both the BLI-1 and BLI-2 datasets. However, much previous work uses CBOW embeddings, so we use different monolingual word embeddings for BLI-1 and for BLI-2. For BLI-1, we use FastText6 to train our monolingual word embedding models with 300 dimensions for each language and default settings. The 5 Most of these methods have been tested by Artetxe et al. (2018b) by using their own implementations. 6 https://github.com/facebookresearch/fastText training corpus comes from a Wikipedia dump.7 For Euro"
D19-1450,P14-1006,0,0.0486385,"r the top 3 translation candidates, both models are able to predict the correct translations well. Interestingly, the word ”battery” is most commonly translated as Chinese ”电 池(dianchi)”, correctly predicted by our model, and the top 3 translation candidates provided by 13 As optimisation of the refinement is not the objective of this paper, we follow the work of Lample et al. (2018) and run only five iterations for post-refinement. 4425 ing (Jiang et al., 2015, 2016; Ammar et al., 2016a). Generally, such spaces can be trained directly from bilingual sentence aligned or document aligned text (Hermann and Blunsom, 2014; Chandar A P et al., 2014; Søgaard et al., 2015; Vuli´c and Moens, 2013). However the performance of directly trained models is limited by their vocabulary size. Figure 3: Accuracy of Chinese-English bilingual lexicon induction task for models trained from different concept numbers. Figure 4: Average error reduction of our method compared to unsupervised adversarial method for bilingual lexicon induction on BLI-1 dataset (Lample et al., 2018). Since the Finnish-English pair is an outlier for the unsupervised method, we report both the average with and without this pair. our model are all rela"
D19-1450,P15-1119,0,0.0405513,"we can see that for these selected English words, our model performs better than the unsupervised GANs model of Lample et al. (2018), but when we consider the top 3 translation candidates, both models are able to predict the correct translations well. Interestingly, the word ”battery” is most commonly translated as Chinese ”电 池(dianchi)”, correctly predicted by our model, and the top 3 translation candidates provided by 13 As optimisation of the refinement is not the objective of this paper, we follow the work of Lample et al. (2018) and run only five iterations for post-refinement. 4425 ing (Jiang et al., 2015, 2016; Ammar et al., 2016a). Generally, such spaces can be trained directly from bilingual sentence aligned or document aligned text (Hermann and Blunsom, 2014; Chandar A P et al., 2014; Søgaard et al., 2015; Vuli´c and Moens, 2013). However the performance of directly trained models is limited by their vocabulary size. Figure 3: Accuracy of Chinese-English bilingual lexicon induction task for models trained from different concept numbers. Figure 4: Average error reduction of our method compared to unsupervised adversarial method for bilingual lexicon induction on BLI-1 dataset (Lample et al."
D19-1450,N18-1202,0,0.0748602,"Missing"
D19-1450,P81-1022,0,0.250062,"Missing"
D19-1450,P07-2045,0,0.00444506,"stText embeddings for both the BLI-1 and BLI-2 datasets. However, much previous work uses CBOW embeddings, so we use different monolingual word embeddings for BLI-1 and for BLI-2. For BLI-1, we use FastText6 to train our monolingual word embedding models with 300 dimensions for each language and default settings. The 5 Most of these methods have been tested by Artetxe et al. (2018b) by using their own implementations. 6 https://github.com/facebookresearch/fastText training corpus comes from a Wikipedia dump.7 For European languages, words are lower-cased and tokenized by the scripts of Moses (Koehn et al., 2007).8 For Chinese, we first use OpenCC9 to convert traditional characters to simplified characters and then use Jieba10 to perform tokenization. For each language, we only keep the words that appear more than five times. For BLI-2, following the work of Artetxe et al. (2018a),11 we use their pretrained CBOW embeddings of 300 dimensions. For English, Italian and German, the models are trained on the WacKy corpus. The Finnish model is trained from Common Crawl and the Spanish model is trained from WMT News Crawl. Concept Aligned Data For concept-aligned articles, we use the Linguatools Wikipedia co"
D19-1450,W13-3512,0,0.0509454,"similarity between a source embedding and its neighbours in the target language, and rs (vt ) represent the mean similarity between a target embedding and its neighbours in the source language. In this work, we use CSLS to build a dictionary for our postrefinement. 4 Training In our weakly-supervised model, the sampling and model selection procedures are important. Sampling Procedure As the statistics show in Table 1, even after filtering, the vocabulary of concept-aligned articles is still large. But it has been shown that the embeddings of frequent words are the most informative and useful (Luong et al., 2013; Lample et al., 2018), so we only keep the one-hundred thousand most frequent words for learning W . For each training step, then, the input word s of our generator is randomly sampled from the vocabulary that is common both to the source monolingual word embedding S and the source Wikipedia concept-aligned articles. After the input source word s is sampled, we sample a concept c according to the frequency of s in each source article of the ensemble of concepts. Then we uniformly sample a target word t from the subvocabulary of the target article of concept c.3 Model Selection It is not as di"
D19-1450,D16-1096,0,0.0530842,"erformance of previous unsupervised adversarial methods for most languages, and especially for typologically distant language pairs. 1 Introduction Distributed representations of words which map each word to a continuous vector have proven useful in capturing important linguistic information. Vectors of words that are semantically or syntactically similar have been shown to be close to each other in the same space (Mikolov et al., 2013a,c; Pennington et al., 2014), making them widely useful in many natural language processing tasks such as machine translation and parsing (Bansal et al., 2014; Mi et al., 2016), both in a single language and across different languages. Mikolov et al. (2013b) first observed that the geometric positions of similar words in different languages were related by a linear relation. Zou et al. (2013) showed that a cross-lingually shared word embedding space is more useful than a monolingual space in an end-to-end machine translation task. However, traditional methods for mapping two monolingual word embeddings require high quality aligned sentences or dictionaries (Faruqui and Dyer, 2014; Ammar et al., 2016b). Reducing the need for parallel data, then, has become the main i"
D19-1450,W16-1614,0,0.0303292,"tead of using a pre-build dictionary for initialization, they sort the value of the word vectors in both the source and the target distribution, treat two vectors that have similar permutations as possible translations and use them as the initialization dictionary. Additionally, their unsupervised framework also includes many optimization augmentations, such as stochastic dictionary induction and symmetric re-weighting, among others. Theoretically, employing GANs for training cross-lingual word embedding is also a promising way to avoid the use of bilingual evidence. As far as we know, Miceli Barone (2016) was the first 4426 MUSE Our Method English to Chinese film 电 影(film), 该片(this film), 本片(this film) 电 影(film), 影片 (film), 本片(this film) debate 辩 论(debate), 争论(dispute), 讨论(discuss) 辩 论(debate), 争论(dispute), 议题(topic) 电池(battery), 锂电池(lithium battery), 火炮(artillery), 炮(cannon), 控制器(controller) 装甲车辆(armored car) English to French f´evrier(february), janvier(january), f´evrier(february), january novembre(november), septembre(september) janvier(january) auteur(author), e´ crivain(writer), auteur(author), e´ crivain(writer), author romancier(novelist) romancier(novelist) universit´e(university), un"
D19-1450,P15-1165,0,0.14992,"Missing"
D19-1450,D13-1168,0,0.0289175,"Missing"
D19-1450,N15-1104,0,0.55293,"ibution pv t or the generated target language distribution, pG(vs ) . The simpler discriminator Dl is useful when the concept-based discriminator is not stable. The objective function of the multi-discriminator model, shown in equation (4), combines all these elements. min max Evt ∼pvt log Dl (vt ) G Dl ,Dcl + Evs ∼pvs log(1 − Dl (G(vs ))) (4) + Evt ∼pvt |c log Dcl (vt , vc ) + Evs ∼pvs |c log(1 − Dcl (G(vs ), vc )) 3 Shared Components Both the GAN and the concept-based GAN models use a variety of further methods that have been shown to improve results. 3.1 Orthogonalization Previous studies (Xing et al., 2015; Smith et al., 2017) show that enforcing the mapping matrix W to be orthogonal can improve the performance and 4421 make the adversarial training more stable. In this work, we perform the same update step proposed by Cisse et al. (2017) to approximate setting W to an orthogonal matrix:   W ← (1 + β) W − β W W > W (5) According to Lample et al. (2018) and Chen and Cardie (2018), when setting β to less than 0.01, the orthogonalization usually performs well. 3.2 Post-refinement Previous work has shown that the core crosslingual mapping can be improved by refining it by bootstrapping from a dic"
D19-1450,D18-1268,0,0.0199835,"a strong CSLS-based refinement to the core mapping matrix trained by GANs. Even in this case, though, without refinement, the core mappings are not as good as hoped for some distant language pairs. More recently, Chen and Cardie (2018) extends the work of Lample et al. (2018) from the bilingual setting to the multi-lingual setting. Instead of training crosslingual word embeddings for only one language pair, their approach allows them to train crosslingual word embeddings for many language pairs at the same time. Another recent piece of work which is similar to Lample et al. (2018) comes from Xu et al. (2018). Their approach can be divided into 2 steps: first, using Wasserstein GAN (Arjovsky et al., 2017) to train a preliminary mapping between two monolingual distributions and then minimizing the Sinkhorn Distance across distributions. Although their method performs better than Lample et al. (2018) in several tasks, the improvement mainly comes from the second step, showing that the problem of how to train a better preliminary mapping has not been resolved. 7 Conclusions and Future Work In this paper, we propose a weakly-supervised adversarial training method for cross-lingual word embedding mappi"
D19-1450,P17-1179,0,0.440758,"ation task. However, traditional methods for mapping two monolingual word embeddings require high quality aligned sentences or dictionaries (Faruqui and Dyer, 2014; Ammar et al., 2016b). Reducing the need for parallel data, then, has become the main issue for cross-lingual word embedding mapping. Some recent work aiming at reducing resources has shown competitive crosslingual mappings across similar languages, using a pseudo-dictionary, such as identical character strings between two languages (Smith et al., 2017), or a simple list of numerals (Artetxe et al., 2017). In a more general method, Zhang et al. (2017) have shown that learning mappings across languages via adversarial training (Goodfellow et al., 2014) can avoid using bilingual evidence. This generality comes at the expense of performance. To overcome this limitation, Lample et al. (2018) refine the preliminary mapping matrix trained by generative adversarial networks (GANs) and obtain a model that is again comparable to supervised models for several language pairs. Despite these big improvements, the performance of these refined GAN models depends largely on the quality of the preliminary mappings. It is probably for this reason that these"
D19-1450,D14-1162,0,0.0926268,"ing across languages is better done at the concept level than at the word level. We propose a concept-based adversarial training method which improves the performance of previous unsupervised adversarial methods for most languages, and especially for typologically distant language pairs. 1 Introduction Distributed representations of words which map each word to a continuous vector have proven useful in capturing important linguistic information. Vectors of words that are semantically or syntactically similar have been shown to be close to each other in the same space (Mikolov et al., 2013a,c; Pennington et al., 2014), making them widely useful in many natural language processing tasks such as machine translation and parsing (Bansal et al., 2014; Mi et al., 2016), both in a single language and across different languages. Mikolov et al. (2013b) first observed that the geometric positions of similar words in different languages were related by a linear relation. Zou et al. (2013) showed that a cross-lingually shared word embedding space is more useful than a monolingual space in an end-to-end machine translation task. However, traditional methods for mapping two monolingual word embeddings require high quali"
D19-1450,N16-1156,0,0.12336,"mple et al. (2018), VecMap is the supervised model of Artetxe et al. (2017). Word translations are retrieved by using CSLS. Bold face indicates the best result overall and italics indicate the best result between the two columns without refinement. pus. Each dictionary has a training set of 5000 entries and a test set of 1500 entries. Compared to BLI-1, this dataset is much noisier and the entries are selected from different frequency ranges. However, BLI-2 has been widely used for testing by previous methods (Faruqui and Dyer, 2014; Dinu et al., 2015; Xing et al., 2015; Artetxe et al., 2016; Zhang et al., 2016; Artetxe et al., 2017; Smith et al., 2017; Lample et al., 2018; Artetxe et al., 2018a,b).5 Using BLI-2 allows us to have a direct comparison with the state-of-the-art. Monolingual Word Embeddings The quality of monolingual word embeddings has a considerable impact on cross-lingual embeddings (Lample et al., 2018). Compared to CBOW and Skip-gram embeddings, FastText embeddings (Bojanowski et al., 2017) capture syntactic information better. The ideal situation would be to use FastText embeddings for both the BLI-1 and BLI-2 datasets. However, much previous work uses CBOW embeddings, so we use d"
D19-1450,D13-1141,0,0.151404,"Missing"
E03-1079,W95-0103,0,0.322303,"nstituents in the sentence. Consider the time-worn example I saw the man with the telescope It is important to determine if the PP with the telescope is to be attached as a sister to the noun the man, restricting its interpretation, or if it is to be attached to the verb, thereby indicating the instrument of the main action described by the sentence. Based on examples of this sort, recent approaches have formalised the problem of disambiguating PP attachments as a binary choice, distinguishing between attachment of a PP to a given verb or to the verb&apos;s direct object (Ratnaparkhi et al., 1994; Collins and Brooks, 1995). This is, however, a simplification of the problem, which does not take the nature of the attachment into account. Precisely, it does not distinguish PP arguments from PP adjuncts. Consider the following example, which contains two PPs, both modifying the verb. Put the block on the table in the morning The first PP is a locative PP required by the subcategorisation frame of the verb put, while in the morning is an optional descriptor of the time at which the action was performed. Though both attached to the verb, the two PPs entertain different relationships with the verb — the first is an ar"
E03-1079,W02-0907,0,0.0934777,"Missing"
E03-1079,H94-1020,0,0.0876452,"ammatical function and semantic tags that involve PP constituents in the PTB The Target Attribute Since we are planning to use a supervised learning method, we need to label each example with a four-valued target attribute (the values are Narg, Nadj, Varg, Vadj). The Penn Treebank annotation does not explicitly make the distinction between arguments and adjuncts. Information about this difference then must be gleaned from the semantic and function tags that have been assigned to the nodes (Bies et al., 1995). Figure 1 illustrates the tags that involve PP constituents. Based on the guidelines (Marcus et al., 1994, 4),(Bies et al., 1995, 12), inspection of the actual annotation in the Tree bank, and discussions in the literature (Quirk et al. 1985, sections 8.27-35, 15.22, 16-48), we mapped PPs into arguments and adjuncts as follows: Adjuncts: All PPs tagged with a semantic tag (DIR, LOC, MNR, PRP, TMP). Arguments: All untagged PPs or PPs tagged with CLR, PUT, DTV, BNF, PRD or LGS. 4.2 The Method The Input Data Each input vector represents an instance of a PP attachment, which could be both noun or verb attached, either as an argument or as an adj unct. 4 Each vector contains 20 training features. They"
E03-1079,W01-0715,1,0.916287,"formulation of the PP attachment problem cannot be considered a first step in the solution of the final 4-way discrimination task. Finally, we note that the improvement is especially due to a better recognition of verbs&apos; arguments, thus providing more accurate information to many NLP tasks and applications. Solving this novel 4-way classification task crucially relies on the ability to distinguish arguments from adjuncts using corpus counts. 2 A Novel Method to Distinguish Arguments from Adjuncts Few attempts have been made to distinguish arguments from adjuncts automatically (Buchholz, 1999; Merlo and Leybold, 2001; Villavicencio, 2002; Aldezabal et al., 2002). The core difficulty in this enterprise is to define the notion of argument precisely. There is a consensus in linguistics that arguments and adjuncts are different both with respect to their function in the sentence and in 252 the way they themselves are interpreted (Jackendoff, 1977; Marantz, 1984; Pollard and Sag, 1987; Grimshaw, 1990). With respect to their function, an argument fills a role in the relation described by its associated head, while an adjunct predicates a separate property of its associate head or phrase. With respect to their i"
E03-1079,W97-0317,1,0.881925,"and head noun sequence. One corpus contains data encoding information for attachment of single PPs in the form of four head words (verb, object noun, preposition and PP-internal noun) for each instance of PP attachments found in the corpus. We also create an auxiliary corpus of sequences of two PPs, where each data item consists of verb, direct object and the two following PPs. This corpus is only used to estimate the feature Iterativity. All the data was newly extracted from the Penn Tree-bank. Our goal was to create a more comprehensive and possibly more accurate corpus than existing ones (Merlo et al., 1997; Collins and Brooks, 1995). To improve coverage, we extracted all cases of PPs following transitive, and intransitive verbs and following nominal phrases. We include also passive sentences and sentences containing a sentential object. To improve accuracy, attention was paid not to extract overlapping data, contrary to counts in previous corpora, where multiple PP sequences were counted more than once, each time as part of a different structural configuration. The Counts The linguistic diagnostics illustrated in the previous section are approximated by corpus counts based on the extracted tupl"
E03-1079,H94-1048,0,0.180614,"roper interpretation of constituents in the sentence. Consider the time-worn example I saw the man with the telescope It is important to determine if the PP with the telescope is to be attached as a sister to the noun the man, restricting its interpretation, or if it is to be attached to the verb, thereby indicating the instrument of the main action described by the sentence. Based on examples of this sort, recent approaches have formalised the problem of disambiguating PP attachments as a binary choice, distinguishing between attachment of a PP to a given verb or to the verb&apos;s direct object (Ratnaparkhi et al., 1994; Collins and Brooks, 1995). This is, however, a simplification of the problem, which does not take the nature of the attachment into account. Precisely, it does not distinguish PP arguments from PP adjuncts. Consider the following example, which contains two PPs, both modifying the verb. Put the block on the table in the morning The first PP is a locative PP required by the subcategorisation frame of the verb put, while in the morning is an optional descriptor of the time at which the action was performed. Though both attached to the verb, the two PPs entertain different relationships with th"
E03-1079,J99-2004,0,0.0483871,"djunct. Analogous examples could be built for attachments to the noun. Is it important to model not just the site but also the nature of the attachment of a PP into the tree structure? We would like to claim that it is. Distinguishing arguments from adjuncts is key to identifying the semantic kernel of a sentence. Extracting the core meaning of a sentence or phrase, in turn, is necessary for automatic acquisition of important lexical knowledge, such as subcategorisation frames and argument structures, which is used in several NLP tasks and applications, such as parsing or machine translation (Srinivas and Joshi, 1999; Don, 1997). Moreover, from a quantitative point of view, arguments and adjuncts have different statistical properties, requiring different statistical techniques. For example, (Hindle and Rooth, 1993) clearly indicate that their lexical association technique performs much better for arguments than for adjuncts, whether the attachment is to the verb or to the noun. Researchers have abstracted away from this distinction, because identifying arguments and adjuncts is a notoriously difficult task, taxing many 251 native speakers&apos; intuitions and requiring complex world knowledge. The usual expect"
E03-1079,W02-2033,0,0.0987048,"achment problem cannot be considered a first step in the solution of the final 4-way discrimination task. Finally, we note that the improvement is especially due to a better recognition of verbs&apos; arguments, thus providing more accurate information to many NLP tasks and applications. Solving this novel 4-way classification task crucially relies on the ability to distinguish arguments from adjuncts using corpus counts. 2 A Novel Method to Distinguish Arguments from Adjuncts Few attempts have been made to distinguish arguments from adjuncts automatically (Buchholz, 1999; Merlo and Leybold, 2001; Villavicencio, 2002; Aldezabal et al., 2002). The core difficulty in this enterprise is to define the notion of argument precisely. There is a consensus in linguistics that arguments and adjuncts are different both with respect to their function in the sentence and in 252 the way they themselves are interpreted (Jackendoff, 1977; Marantz, 1984; Pollard and Sag, 1987; Grimshaw, 1990). With respect to their function, an argument fills a role in the relation described by its associated head, while an adjunct predicates a separate property of its associate head or phrase. With respect to their interpretation, a comp"
E03-1079,J93-1005,0,0.222901,"laim that it is. Distinguishing arguments from adjuncts is key to identifying the semantic kernel of a sentence. Extracting the core meaning of a sentence or phrase, in turn, is necessary for automatic acquisition of important lexical knowledge, such as subcategorisation frames and argument structures, which is used in several NLP tasks and applications, such as parsing or machine translation (Srinivas and Joshi, 1999; Don, 1997). Moreover, from a quantitative point of view, arguments and adjuncts have different statistical properties, requiring different statistical techniques. For example, (Hindle and Rooth, 1993) clearly indicate that their lexical association technique performs much better for arguments than for adjuncts, whether the attachment is to the verb or to the noun. Researchers have abstracted away from this distinction, because identifying arguments and adjuncts is a notoriously difficult task, taxing many 251 native speakers&apos; intuitions and requiring complex world knowledge. The usual expectation has been that this discrimination is not amenable to a corpus-based treatment. In recent work, however, we succeed in distinguishing arguments from adjuncts using evidence extracted from a parsed"
E03-1079,W02-0906,0,\N,Missing
E99-1007,P97-1003,0,0.0747996,"Missing"
E99-1007,P98-1046,0,0.157667,"Missing"
E99-1007,C96-1055,0,0.358533,"ave witnessed a shift in grammar development methodology, from crafting large grammars, to annotation of corpora. Correspondingly, there has been a change from developing rule-based parsers to developing statistical methods for inducing grammatical knowledge from annotated corpus data. The shift has mostly occurred because building wide-coverage grammars is time-consuming, error prone, and difficult. The same can be said for crafting the rich lexical representations that are a central component of linguistic knowledge, and research in automatic lexical acquisition has sought to address this ((Dorr and Jones, 1996; Dorr, 1997), among others). Yet there have been few attempts to learn finegrained lexical classifications from the statistical analysis of distributional data, analogously to the induction of syntactic knowledge (though see, e.g., (Brent, 1993; Klavans and Chodorow, 1992; Resnik, 1992)). In this paper, we propose such an approach for the automatic classification of verbs 45 In exploring these questions, we focus on verb classification for several reasons. Verbs are very important sources of knowledge in many language engineering tasks, and the relationships among verbs appear to play a major"
E99-1007,C92-4177,0,0.0611426,"otated corpus data. The shift has mostly occurred because building wide-coverage grammars is time-consuming, error prone, and difficult. The same can be said for crafting the rich lexical representations that are a central component of linguistic knowledge, and research in automatic lexical acquisition has sought to address this ((Dorr and Jones, 1996; Dorr, 1997), among others). Yet there have been few attempts to learn finegrained lexical classifications from the statistical analysis of distributional data, analogously to the induction of syntactic knowledge (though see, e.g., (Brent, 1993; Klavans and Chodorow, 1992; Resnik, 1992)). In this paper, we propose such an approach for the automatic classification of verbs 45 In exploring these questions, we focus on verb classification for several reasons. Verbs are very important sources of knowledge in many language engineering tasks, and the relationships among verbs appear to play a major role in the organization and use of this knowledge: Knowledge about verb classes is crucial for lexical acquisition in support of language generation and machine translation (Dorr, 1997), and document classification (Klavans and Kan, 1998). Manual classification of large"
E99-1007,J93-2002,0,0.173984,"edge from annotated corpus data. The shift has mostly occurred because building wide-coverage grammars is time-consuming, error prone, and difficult. The same can be said for crafting the rich lexical representations that are a central component of linguistic knowledge, and research in automatic lexical acquisition has sought to address this ((Dorr and Jones, 1996; Dorr, 1997), among others). Yet there have been few attempts to learn finegrained lexical classifications from the statistical analysis of distributional data, analogously to the induction of syntactic knowledge (though see, e.g., (Brent, 1993; Klavans and Chodorow, 1992; Resnik, 1992)). In this paper, we propose such an approach for the automatic classification of verbs 45 In exploring these questions, we focus on verb classification for several reasons. Verbs are very important sources of knowledge in many language engineering tasks, and the relationships among verbs appear to play a major role in the organization and use of this knowledge: Knowledge about verb classes is crucial for lexical acquisition in support of language generation and machine translation (Dorr, 1997), and document classification (Klavans and Kan, 1998). Man"
E99-1007,P93-1024,0,0.234604,"Missing"
E99-1007,P97-1008,0,0.0336484,"Missing"
E99-1007,W97-0301,0,0.0172238,"nsitive, causative form. These features correspond directly to the defining alternations of the three verb classes under study (intransitive/transitive, causative). Additionally, we see that other related features to these usages serve to distinguish the two resolutions of the ambiguity. The main verb form is active and a main verb part-of-speech (labeled as VBD by automatic POS taggers); by contrast, the reduced relative form is passive and a past participle (tagged as VBN). Although these properties are redundant with the intransitive/transitive distinction, recent work in machine learning (Ratnaparkhi, 1997; Ratnaparkhi, 1998) has shown that using overlapping features can be beneficial for learning in a maximum entropy framework, and we want to explore it in this setting to test H 3 above. 2 In the next section, 2These properties are redundant with the intransitive/transitive distinction, as passive implies transitive use, and necessarily entails the use of a past participle. We performed a correlation analysis that Proceedings of EACL &apos;99 we describe how we compile the corpus counts for each of the four properties, in order to approximate the distributional information of these alternations. 3"
E99-1007,P98-2177,0,0.0223226,"form. These features correspond directly to the defining alternations of the three verb classes under study (intransitive/transitive, causative). Additionally, we see that other related features to these usages serve to distinguish the two resolutions of the ambiguity. The main verb form is active and a main verb part-of-speech (labeled as VBD by automatic POS taggers); by contrast, the reduced relative form is passive and a past participle (tagged as VBN). Although these properties are redundant with the intransitive/transitive distinction, recent work in machine learning (Ratnaparkhi, 1997; Ratnaparkhi, 1998) has shown that using overlapping features can be beneficial for learning in a maximum entropy framework, and we want to explore it in this setting to test H 3 above. 2 In the next section, 2These properties are redundant with the intransitive/transitive distinction, as passive implies transitive use, and necessarily entails the use of a past participle. We performed a correlation analysis that Proceedings of EACL &apos;99 we describe how we compile the corpus counts for each of the four properties, in order to approximate the distributional information of these alternations. 3 Frequency Distributi"
E99-1007,P98-2184,0,0.0293719,"Missing"
E99-1007,C98-1046,0,\N,Missing
E99-1007,C98-2179,0,\N,Missing
E99-1007,P98-1112,0,\N,Missing
E99-1007,C98-1108,0,\N,Missing
E99-1007,P92-1053,0,\N,Missing
E99-1007,C98-2172,0,\N,Missing
H05-1078,W05-0602,0,0.0366628,"natural language understanding. In this paper, we present a parser that outputs labels indicating the syntactic or semantic function of a constituent in the tree, such as NP - SBJ or PP - TMP shown in bold face in the tree in Figure 1. These labels indicate that the NP is the subject of the sentence and that the PP conveys temporal information. (Labels in parentheses will be explained later in the paper.) Output annotated with such informative labels underlies all domain-independent question answering (Jijkoun et al., 2004) or shallow semantic interpretation systems (Collins and Miller, 1998; Ge and Mooney, 2005). We test the hypothesis that a current statistical parser can output such richer information without any degradation of the parser’s accuracy on the original parsing task. Briefly, our method consists in augmenting a state-of-the-art statistical parser (Henderson, 2003), whose architecture and properties make it particularly adaptive to new tasks. We achieve state-of-the-art results both for parsing and function labelling. Statistical parsers trained on the Penn Treebank (PTB) (Marcus et al., 1993) produce trees annotated with bare phrase structure labels (Collins, 1999; Charniak, 2000). The"
H05-1078,J02-3001,0,0.264449,"Missing"
H05-1078,gimenez-marquez-2004-svmtool,0,0.11316,"Missing"
H05-1078,N03-1014,0,0.0956564,"e subject of the sentence and that the PP conveys temporal information. (Labels in parentheses will be explained later in the paper.) Output annotated with such informative labels underlies all domain-independent question answering (Jijkoun et al., 2004) or shallow semantic interpretation systems (Collins and Miller, 1998; Ge and Mooney, 2005). We test the hypothesis that a current statistical parser can output such richer information without any degradation of the parser’s accuracy on the original parsing task. Briefly, our method consists in augmenting a state-of-the-art statistical parser (Henderson, 2003), whose architecture and properties make it particularly adaptive to new tasks. We achieve state-of-the-art results both for parsing and function labelling. Statistical parsers trained on the Penn Treebank (PTB) (Marcus et al., 1993) produce trees annotated with bare phrase structure labels (Collins, 1999; Charniak, 2000). The trees of the Penn Treebank, however, are also decorated with function labels. Figure 1 shows the simplified tree representation with function labels for a sample sentence from the Penn Treebank corpus (section 00) The Government’s borrowing authority dropped at midnight"
H05-1078,P04-1040,0,0.22777,"Missing"
H05-1078,C04-1188,0,0.0887254,"Missing"
H05-1078,P03-1054,0,0.0722676,"ve labels CLR, OBJ and OTHER. Constituents were assigned the OBJ label according to the conditions stated in (Collins, 1999).4 Another striking property of the simple baseline function parser is that the SSN tends to project NULL labels more than any other label. Since SSNs decide the label of a non-terminal at projection, this behaviour indicates that the parser does not have enough information at this point in the parse to project the correct function label. We hypothesize that finer-grained labelling will improve parsing performance. This observation is consistent with results reported in (Klein and Manning, 2003), who showed that part-of-speech tags occurring in the Treebank are not fine-grained enough to discriminate between 4 Roughly, an OBJ non-terminal is an NP, SBAR or S whose parent is an S, VP or SBAR. Any such non-terminal must not bear either syntactic or semantic function labels, or the CLR label. In addition, the first child following the head of a PP is marked with the OBJ label. preterminals. For example, the tag TO labels both the preposition to and the infinitival marker. Extending (Klein and Manning, 2003)’s technique to function labelling, we split some part-of-speech tags into tags m"
H05-1078,J93-2004,0,0.0279109,"ng (Jijkoun et al., 2004) or shallow semantic interpretation systems (Collins and Miller, 1998; Ge and Mooney, 2005). We test the hypothesis that a current statistical parser can output such richer information without any degradation of the parser’s accuracy on the original parsing task. Briefly, our method consists in augmenting a state-of-the-art statistical parser (Henderson, 2003), whose architecture and properties make it particularly adaptive to new tasks. We achieve state-of-the-art results both for parsing and function labelling. Statistical parsers trained on the Penn Treebank (PTB) (Marcus et al., 1993) produce trees annotated with bare phrase structure labels (Collins, 1999; Charniak, 2000). The trees of the Penn Treebank, however, are also decorated with function labels. Figure 1 shows the simplified tree representation with function labels for a sample sentence from the Penn Treebank corpus (section 00) The Government’s borrowing authority dropped at midnight Tuesday to 2.8 trillion from 2.87 trillion. Table 1 illustrates the complete list of function labels in the Penn Treebank. Unlike phrase structure labels, func620 Proceedings of Human Language Technology Conference and Conference on"
H05-1078,J05-1004,0,0.193211,"Missing"
H05-1078,A00-2031,0,0.361852,"pipeline architecture divided in several stages (Gildea and Jurafsky, 2002). See also the common task of (CoNLL, 2004 2005; Senseval, 2004). Our approach maintains state-of-the-art results in parsing, while also reaching state-of-the-art results in function labelling, by suitably extending a Simple Synchrony Network (SSN) parser (Henderson, 2003) into a single integrated system. This is an interesting result, as a task combining function labelling and parsing is more complex than simple parsing. While the function of a constituent and its structural position are often correlated, they some1 (Blaheta and Charniak, 2000) talk of function tags. We will instead use the term function label, to indicate function identifiers, as they can decorate any node in the tree. We keep the word tag to indicate only those labels that decorate preterminal nodes in a tree – part-of-speech tags– as is standard use. 621 Method Successfully addressing function parsing requires accurate parsing models and training data. Understanding the causes and the relevance of the observed results requires appropriate evaluation measures. In this section, we describe the methodology that will be used to assess our main hypothesis. 2.1 The Bas"
H05-1078,W04-0800,0,0.412406,"Missing"
H05-1078,A00-1010,0,0.011729,"istical parsing methods are sufficiently general to produce accurate shallow semantic annotation. 1 NP-SBJ VP P PP  P  P  @ PPP  the authority @ PP  PP  @ VBD dropped PP-TMP H  H IN(TMP) NP at NN midnight NP-TMP PP-DIR H  H NNP(TMP) TO(DIR) NP Tuesday to QP P PP  $ 2.8 trillion Figure 1: A sample syntactic structure with function labels. Introduction With recent advances in speech recognition, parsing, and information extraction, some domain-specific interactive systems are now of practical use for tasks such as question-answering, flight booking, or restaurant reservation (Stallard, 2000). One of the challenges ahead lies in moving from hand-crafted programs of limited scope to robust systems independent of a given domain. While this ambitious goal will remain in the future for some time to come, recent efforts to develop language processing systems producing richer semantic outputs will likely be the cornerstone of many successful developments in natural language understanding. In this paper, we present a parser that outputs labels indicating the syntactic or semantic function of a constituent in the tree, such as NP - SBJ or PP - TMP shown in bold face in the tree in Figure"
H05-1078,A00-2018,0,0.384203,"Ge and Mooney, 2005). We test the hypothesis that a current statistical parser can output such richer information without any degradation of the parser’s accuracy on the original parsing task. Briefly, our method consists in augmenting a state-of-the-art statistical parser (Henderson, 2003), whose architecture and properties make it particularly adaptive to new tasks. We achieve state-of-the-art results both for parsing and function labelling. Statistical parsers trained on the Penn Treebank (PTB) (Marcus et al., 1993) produce trees annotated with bare phrase structure labels (Collins, 1999; Charniak, 2000). The trees of the Penn Treebank, however, are also decorated with function labels. Figure 1 shows the simplified tree representation with function labels for a sample sentence from the Penn Treebank corpus (section 00) The Government’s borrowing authority dropped at midnight Tuesday to 2.8 trillion from 2.87 trillion. Table 1 illustrates the complete list of function labels in the Penn Treebank. Unlike phrase structure labels, func620 Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language c Processing (HLT/EMNLP), pages 620–627, Vancouver,"
H05-1078,W98-1105,0,0.0606591,"uccessful developments in natural language understanding. In this paper, we present a parser that outputs labels indicating the syntactic or semantic function of a constituent in the tree, such as NP - SBJ or PP - TMP shown in bold face in the tree in Figure 1. These labels indicate that the NP is the subject of the sentence and that the PP conveys temporal information. (Labels in parentheses will be explained later in the paper.) Output annotated with such informative labels underlies all domain-independent question answering (Jijkoun et al., 2004) or shallow semantic interpretation systems (Collins and Miller, 1998; Ge and Mooney, 2005). We test the hypothesis that a current statistical parser can output such richer information without any degradation of the parser’s accuracy on the original parsing task. Briefly, our method consists in augmenting a state-of-the-art statistical parser (Henderson, 2003), whose architecture and properties make it particularly adaptive to new tasks. We achieve state-of-the-art results both for parsing and function labelling. Statistical parsers trained on the Penn Treebank (PTB) (Marcus et al., 1993) produce trees annotated with bare phrase structure labels (Collins, 1999;"
H05-1078,C00-2137,0,0.0679501,"Missing"
H05-1078,W05-0639,0,\N,Missing
H05-1078,J03-4003,0,\N,Missing
J01-3003,J93-2002,0,0.215201,"predicate-argument structure that relates an action or state to its participants (i.e., who did what to whom). In facing the task of automatic acquisition of knowledge about verbs, two basic questions must be addressed: What information about verbs and their relational properties needs to be learned? What information can in practice be learned through automatic means? In answering these questions, some approaches to lexical acquisition have focused on learning syntactic information about verbs, by automatically extracting subcategorization frames from a corpus or machine-readable dictionary (Brent 1993; Briscoe and Carroll 1997; Dorr 1997; Lapata 1999; Manning 1993; McCarthy and Korhonen 1998). * Linguistics Department; University of Geneva; 2 rue de Candolle; 1211 Geneva 4, Switzerland; merlo@ lettres.unige.ch t Department of Computer Science; University of Toronto; 6 King&apos;s College Road; Toronto, ON M5S 3H5 Canada; suzanne@cs.toronto.edu Q 2001 Association for Computational Linguistics Computational Linguistics Volume 27, Number 3 Table 1 Examples of verbs from the three optionally intransitive classes. Unergative The horse raced past the barn. The jockey raced the horse past the barn. Un"
J01-3003,A97-1052,0,0.666125,"Missing"
J01-3003,W98-1106,0,0.217437,"Science; University of Toronto; 6 King&apos;s College Road; Toronto, ON M5S 3H5 Canada; suzanne@cs.toronto.edu Q 2001 Association for Computational Linguistics Computational Linguistics Volume 27, Number 3 Table 1 Examples of verbs from the three optionally intransitive classes. Unergative The horse raced past the barn. The jockey raced the horse past the barn. Unaccusative The butter melted in the pan. The cook melted the butter in the pan. Object-Drop The boy played. The boy played soccer. Other work has attempted to learn deeper semantic properties such as selectional restrictions (Resnik 1996; Riloff and Schmelzenbach 1998), verbal aspect (Klavans and Chodorow 1992; Siegel 1999), or lexical-semantic verb classes such as those proposed by Levin (1993) (Aone and McKee 1996; McCarthy 2000; Lapata and Brew 1999; Schulte im Walde 2000). In this paper, we focus on argument structure--the thematic roles assigned by a verb to its arguments--as the w a y in which the relational semantics of the verb is represented at the syntactic level. Specifically, our proposal is to automatically classify verbs based on argument structure properties, using statistical corpus-based methods. We address the problem of classification bec"
J01-3003,A92-1011,0,0.0346587,"Missing"
J01-3003,C00-2108,0,0.739729,"Missing"
J01-3003,P99-1015,0,0.124924,"Canada; suzanne@cs.toronto.edu Q 2001 Association for Computational Linguistics Computational Linguistics Volume 27, Number 3 Table 1 Examples of verbs from the three optionally intransitive classes. Unergative The horse raced past the barn. The jockey raced the horse past the barn. Unaccusative The butter melted in the pan. The cook melted the butter in the pan. Object-Drop The boy played. The boy played soccer. Other work has attempted to learn deeper semantic properties such as selectional restrictions (Resnik 1996; Riloff and Schmelzenbach 1998), verbal aspect (Klavans and Chodorow 1992; Siegel 1999), or lexical-semantic verb classes such as those proposed by Levin (1993) (Aone and McKee 1996; McCarthy 2000; Lapata and Brew 1999; Schulte im Walde 2000). In this paper, we focus on argument structure--the thematic roles assigned by a verb to its arguments--as the w a y in which the relational semantics of the verb is represented at the syntactic level. Specifically, our proposal is to automatically classify verbs based on argument structure properties, using statistical corpus-based methods. We address the problem of classification because it provides a means for lexical organization which"
J01-3003,J99-2004,0,0.0429239,"Missing"
J01-3003,J98-3003,0,0.0428045,"Missing"
J01-3003,W99-0503,1,0.792109,"Missing"
J01-3003,P89-1022,0,0.0601827,"nnotated text (e.g., Brent [1993]; Sanfilippo and Poznanski [1992]; Manning [1993]; Collins [1997]). Others have tackled the problem of lexical semantic classification, but using only subcategorization frequencies as input data (Lapata and Brew 1999; Schulte im Walde 2000). Specifically, these researchers have not explicitly addressed the definition of features to tap directly into thematic role differences that are not reflected in subcategorization distinctions. On the other hand, when learning of thematic role assignment has been the explicit goal, the text has been semantically annotated (Webster and Marcus 1989), or external semantic resources have been consulted (Aone and McKee 1996; McCarthy 2000). We extend these results by showing that thematic information can be induced from linguistically-guided counts in a corpus, without the use of thematic role tagging or external resources such as WordNet. Finally, our results converge with the increasing agreement that corpus-based techniques are fruitful in the automatic construction of computational lexicons, providing machine readable dictionaries with complementary, reusable resources, such as frequencies of argument structures. Moreover, these techniq"
J01-3003,W98-1116,1,\N,Missing
J01-3003,merlo-stevenson-2000-establishing,1,\N,Missing
J01-3003,A00-2034,0,\N,Missing
J01-3003,W96-0213,0,\N,Missing
J01-3003,C96-1055,0,\N,Missing
J01-3003,P97-1003,0,\N,Missing
J01-3003,C92-4177,0,\N,Missing
J01-3003,P98-1046,0,\N,Missing
J01-3003,C98-1046,0,\N,Missing
J01-3003,P98-1112,0,\N,Missing
J01-3003,C98-1108,0,\N,Missing
J01-3003,J03-2004,0,\N,Missing
J01-3003,P93-1032,0,\N,Missing
J01-3003,P98-2247,0,\N,Missing
J01-3003,C98-2242,0,\N,Missing
J01-3003,J96-2004,0,\N,Missing
J01-3003,P99-1051,0,\N,Missing
J01-3003,W99-0632,0,\N,Missing
J06-3002,P98-1013,0,0.284214,"Missing"
J06-3002,W95-0103,0,0.20938,"e timeworn example (1) I saw the man with the telescope. It is important to determine if the PP with the telescope is to be attached as a sister to the noun the man, restricting its interpretation, or if it is to be attached to the verb, thereby indicating the instrument of the main action described by the sentence. Based on examples of this sort, recent approaches have formalized the problem of disambiguating PP attachments as a binary choice, distinguishing between attachment of a PP to a given verb or to the verb’s direct object (Hindle and Rooth 1993; Ratnaparkhi, Reynar, and Roukos 1994; Collins and Brooks 1995; Merlo, Crocker, and Berthouzoz 1997; Stetina and Nagao 1997; Ratnaparkhi 1997; Zhao and Lin 2004). This is, however, a simplification of the problem, which does not take the nature of the attachment into account. Precisely, it does not distinguish PP arguments from ∗ Linguistics Department, University of Geneva, 2 rue de Candolle, 1211 Gen`eve 4, Switzerland. † Department of Informatics, University of Sussex, Falmer, Brighton BN1 9QH, UK. Submission received: 28 November 2003; revised submission received: 22 June 2005; accepted for publication: 4 November 2005. © 2006 Association for Computa"
J06-3002,J02-3001,0,0.133097,"m adjuncts is key to identifying the elements that belong to the semantic kernel of a sentence. Extracting the kernel of a sentence or phrase, in turn, is necessary for automatic acquisition of important lexical knowledge, such as subcategorization frames and argument structures, which is used in several natural language processing (NLP) tasks and applications, such as parsing, machine translation, and information extraction (Srinivas and Joshi 1999; Dorr 1997; Phillips and Riloff 2002). This task is fundamentally syntactic in nature and complements the task of assigning thematic role labels (Gildea and Jurafsky 2002; Nielsen and Pradhan 2004; Xue and Palmer 2004; Swier and Stevenson 2005). See also the common task of CoNNL (2004, 2005) and SENSEVAL-3 (2004). Both a distinction of arguments from adjuncts and an appropriate thematic labeling of the complements of a predicate, verb, or noun are necessary, as confirmed by the annotations adopted by current corpora. Framenet makes a distinction between complements and satellites (Baker, Fillmore, and Lowe 1998). The developers of PropBank integrate the difference between arguments and adjuncts directly into the level of specificity of their annotation. They a"
J06-3002,J93-1005,0,0.792751,"r interpretation of constituents in the sentence. Consider the timeworn example (1) I saw the man with the telescope. It is important to determine if the PP with the telescope is to be attached as a sister to the noun the man, restricting its interpretation, or if it is to be attached to the verb, thereby indicating the instrument of the main action described by the sentence. Based on examples of this sort, recent approaches have formalized the problem of disambiguating PP attachments as a binary choice, distinguishing between attachment of a PP to a given verb or to the verb’s direct object (Hindle and Rooth 1993; Ratnaparkhi, Reynar, and Roukos 1994; Collins and Brooks 1995; Merlo, Crocker, and Berthouzoz 1997; Stetina and Nagao 1997; Ratnaparkhi 1997; Zhao and Lin 2004). This is, however, a simplification of the problem, which does not take the nature of the attachment into account. Precisely, it does not distinguish PP arguments from ∗ Linguistics Department, University of Geneva, 2 rue de Candolle, 1211 Gen`eve 4, Switzerland. † Department of Informatics, University of Sussex, Falmer, Brighton BN1 9QH, UK. Submission received: 28 November 2003; revised submission received: 22 June 2005; accepted f"
J06-3002,W02-0907,0,0.0152582,"notion of argumenthood in a more comprehensive formulation of the problem of disambiguating the attachment of PPs. Hindle and Rooth (1993) mention the interaction between the structural and the semantic factors in the disambiguation of a PP, indicating that verb complements are the most difficult. We confirm their finding that noun arguments are more easily identified, whereas verb complements (either arguments or adjuncts) are more difficult. Other pieces of work address the current problem in the larger perspective of distinguishing arguments from adjuncts for subcategorization acquisition (Korhonen 2002a; Aldezabal et al. 2002). The goal of the work of Korhonen (2002a, 2002b) is to develop a semantically driven approach to subcategorization frame hypothesis selection that can be used to improve large-scale subcategorization frame acquisition. The main idea underlying the approach is to leverage the well-known mapping between syntax and semantics, inspired by Levin’s (1993) work. Korhonen uses statistics over semantic classes of verbs to smooth a distribution of subcategorization frames and then applies a simple frequency cutoff to select the most reliable subcategorization frames. Her work i"
J06-3002,H94-1020,0,0.115547,"hod, we need to label each example with a target attribute. Deciding whether an example is an instance of an argument or of an adjunct requires making a distinction that the Penn Treebank annotators did not intend to make. The automatic annotation of this attribute therefore must rely on the existing labels for the PP that have been given by the Penn Treebank annotators, inferring from them information that was not explicitly marked. We discuss here the motivation for our interpretation. The PTB annotators found that consistent annotation of argument status and semantic role was not possible (Marcus et al. 1994). The solution adopted, then, was to structurally distinguish arguments from adjuncts only when the distinction was straightforward and to label only some clearly distinguishable semantic roles. Doubtful cases were left untagged. In the Penn Treebank structural distinctions concerning arguments and adjuncts have been oversimplified: All constituents attached to VP are structurally treated as arguments, whereas all constituents attached to NP are treated as adjuncts. The only exception are the arguments of some deverbal nouns, which are represented as arguments. Information about the distinctio"
J06-3002,J93-2004,0,0.0316867,"Missing"
J06-3002,E03-1079,1,0.93272,"nd Rooth (1993) clearly indicate that their lexical association technique performs much better for arguments than for adjuncts, whether the attachment is to the verb or to the noun. Researchers have abstracted away from this distinction, because identifying arguments and adjuncts is a notoriously difficult task, taxing many native speakers’ intuitions. The usual expectation has been that this discrimination is not amenable to a corpus-based treatment. In recent preliminary work, however, we have succeeded in distinguishing arguments from adjuncts using corpus evidence (Merlo and Leybold 2001; Merlo 2003). Our method develops corpus-based statistical correlates for the diagnostics used in linguistics to decide whether a PP is an argument or an adjunct. A numerical vectorial representation of the notion of argumenthood is provided, which supports automatic classification. In the current article, we expand and improve on this work, by developing new measures and refining the previous ones. We also extend that work to attachment to nouns. This extension enables us to explore in what way the distinction between argument and adjunct is best integrated in the traditional attachment disambiguation pr"
J06-3002,W97-0317,1,0.904035,"Missing"
J06-3002,W01-0715,1,0.818672,"Missing"
J06-3002,J01-3003,1,0.854562,"scale subcategorization frame acquisition. The main idea underlying the approach is to leverage the well-known mapping between syntax and semantics, inspired by Levin’s (1993) work. Korhonen uses statistics over semantic classes of verbs to smooth a distribution of subcategorization frames and then applies a simple frequency cutoff to select the most reliable subcategorization frames. Her work is related to ours in several ways. First, the automatic acquisition task leverages correspondences between syntax and semantics, particularly clear in the organization of the verb lexicon, similarly to Merlo and Stevenson (2001). Some of our current results are also based on this correspondence, as we assume that the notion of argument is a notion at the interface of the syntactic and semantic levels, and participates in both, determining not only the valency of a verb but also its subcategorization frame. Our work confirms the results reported in Korhonen (2002a), which indicate that using word classes improves the extraction of subcategorization frames. Differently from Korhonen, however, we do not allow feedback between levels. In her work, syntactic similarity of verbs’ subcategorization sets based on an external"
J06-3002,J05-1004,0,0.0546878,"Missing"
J06-3002,W02-1017,0,0.0438715,"PP) ≈ P(vcopula ≺ PP) (4) Deverbal Nouns. This diagnostic is based on the observation that PPs following a deverbal noun are likely to be arguments, as the noun shares the argument structure of the verb.2 Proper counting of this feature requires identifying a deverbal noun in the head noun position of a noun phrase. We identify deverbal nouns by inspecting their morphology (Quirk et al. 1985). Specifically, the suffixes that can combine ¨ 2 Doubts have been cast on the validity of this diagnostic (Schutze 1995), based on work in theoretical linguistics (Grimshaw 1990). Argaman and Pearlmutter (2002), however, have shown that the argument structures of verbs and related nouns are highly correlated. Hence, we keep deverbal noun as a valid diagnostic here, although we show later that it is not very effective. 347 Computational Linguistics Volume 32, Number 3 with verb bases to form deverbal nouns are listed and exemplified in Figure 1 on page 348. This diagnostic can be captured by a probability indicator function, which assigns probability 1 of being an argument to PPs following a deverbal noun and 0 otherwise.  deverb(PP) = 1 if deverbal n ≺ PP 0 otherwise (5) In conclusion, the diagnost"
J06-3002,W97-0301,0,0.054113,"if the PP with the telescope is to be attached as a sister to the noun the man, restricting its interpretation, or if it is to be attached to the verb, thereby indicating the instrument of the main action described by the sentence. Based on examples of this sort, recent approaches have formalized the problem of disambiguating PP attachments as a binary choice, distinguishing between attachment of a PP to a given verb or to the verb’s direct object (Hindle and Rooth 1993; Ratnaparkhi, Reynar, and Roukos 1994; Collins and Brooks 1995; Merlo, Crocker, and Berthouzoz 1997; Stetina and Nagao 1997; Ratnaparkhi 1997; Zhao and Lin 2004). This is, however, a simplification of the problem, which does not take the nature of the attachment into account. Precisely, it does not distinguish PP arguments from ∗ Linguistics Department, University of Geneva, 2 rue de Candolle, 1211 Gen`eve 4, Switzerland. † Department of Informatics, University of Sussex, Falmer, Brighton BN1 9QH, UK. Submission received: 28 November 2003; revised submission received: 22 June 2005; accepted for publication: 4 November 2005. © 2006 Association for Computational Linguistics Computational Linguistics Volume 32, Number 3 PP adjuncts. C"
J06-3002,H94-1048,0,0.398113,"Missing"
J06-3002,W98-1106,0,0.0102854,"ew researchers who have attempted to perform the same distinction. 2. Distinguishing Arguments from Adjuncts Solving the four-way classification task described in the introduction crucially relies on the ability to distinguish arguments from adjuncts, using corpus counts. The ability to automatically make this distinction is necessary for the correct automatic acquisition of important lexical knowledge, such as subcategorization frames and argument structures, which is used in parsing, generation, machine translation, and information extraction (Srinivas and Joshi 1999; Stede 1998; Dorr 1997; Riloff and Schmelzenbach 1998). Yet, few attempts have been made to make this distinction automatically. The core difficulty in this enterprise is to define the notion of argument precisely enough that it can be used automatically. There is a consensus in linguistics that arguments and adjuncts are different both with respect to their function in the sentence and in the way they themselves are interpreted (Jackendoff 1977; Marantz 1984; Pollard and Sag 1987; Grimshaw 1990). With respect to their function, an argument fills a role in the relation described by its associated head, whereas an adjunct predicates a separate pro"
J06-3002,J99-2004,0,0.145873,"ment in Prepositional Phrase Attachment Modeling both the site and the nature of the attachment of a PP into the tree structure is important. Distinguishing arguments from adjuncts is key to identifying the elements that belong to the semantic kernel of a sentence. Extracting the kernel of a sentence or phrase, in turn, is necessary for automatic acquisition of important lexical knowledge, such as subcategorization frames and argument structures, which is used in several natural language processing (NLP) tasks and applications, such as parsing, machine translation, and information extraction (Srinivas and Joshi 1999; Dorr 1997; Phillips and Riloff 2002). This task is fundamentally syntactic in nature and complements the task of assigning thematic role labels (Gildea and Jurafsky 2002; Nielsen and Pradhan 2004; Xue and Palmer 2004; Swier and Stevenson 2005). See also the common task of CoNNL (2004, 2005) and SENSEVAL-3 (2004). Both a distinction of arguments from adjuncts and an appropriate thematic labeling of the complements of a predicate, verb, or noun are necessary, as confirmed by the annotations adopted by current corpora. Framenet makes a distinction between complements and satellites (Baker, Fill"
J06-3002,J98-3003,0,0.0297065,"r work to that of the few researchers who have attempted to perform the same distinction. 2. Distinguishing Arguments from Adjuncts Solving the four-way classification task described in the introduction crucially relies on the ability to distinguish arguments from adjuncts, using corpus counts. The ability to automatically make this distinction is necessary for the correct automatic acquisition of important lexical knowledge, such as subcategorization frames and argument structures, which is used in parsing, generation, machine translation, and information extraction (Srinivas and Joshi 1999; Stede 1998; Dorr 1997; Riloff and Schmelzenbach 1998). Yet, few attempts have been made to make this distinction automatically. The core difficulty in this enterprise is to define the notion of argument precisely enough that it can be used automatically. There is a consensus in linguistics that arguments and adjuncts are different both with respect to their function in the sentence and in the way they themselves are interpreted (Jackendoff 1977; Marantz 1984; Pollard and Sag 1987; Grimshaw 1990). With respect to their function, an argument fills a role in the relation described by its associated head, w"
J06-3002,W97-0109,0,0.342735,"important to determine if the PP with the telescope is to be attached as a sister to the noun the man, restricting its interpretation, or if it is to be attached to the verb, thereby indicating the instrument of the main action described by the sentence. Based on examples of this sort, recent approaches have formalized the problem of disambiguating PP attachments as a binary choice, distinguishing between attachment of a PP to a given verb or to the verb’s direct object (Hindle and Rooth 1993; Ratnaparkhi, Reynar, and Roukos 1994; Collins and Brooks 1995; Merlo, Crocker, and Berthouzoz 1997; Stetina and Nagao 1997; Ratnaparkhi 1997; Zhao and Lin 2004). This is, however, a simplification of the problem, which does not take the nature of the attachment into account. Precisely, it does not distinguish PP arguments from ∗ Linguistics Department, University of Geneva, 2 rue de Candolle, 1211 Gen`eve 4, Switzerland. † Department of Informatics, University of Sussex, Falmer, Brighton BN1 9QH, UK. Submission received: 28 November 2003; revised submission received: 22 June 2005; accepted for publication: 4 November 2005. © 2006 Association for Computational Linguistics Computational Linguistics Volume 32, Numbe"
J06-3002,H05-1111,0,0.0283719,"kernel of a sentence. Extracting the kernel of a sentence or phrase, in turn, is necessary for automatic acquisition of important lexical knowledge, such as subcategorization frames and argument structures, which is used in several natural language processing (NLP) tasks and applications, such as parsing, machine translation, and information extraction (Srinivas and Joshi 1999; Dorr 1997; Phillips and Riloff 2002). This task is fundamentally syntactic in nature and complements the task of assigning thematic role labels (Gildea and Jurafsky 2002; Nielsen and Pradhan 2004; Xue and Palmer 2004; Swier and Stevenson 2005). See also the common task of CoNNL (2004, 2005) and SENSEVAL-3 (2004). Both a distinction of arguments from adjuncts and an appropriate thematic labeling of the complements of a predicate, verb, or noun are necessary, as confirmed by the annotations adopted by current corpora. Framenet makes a distinction between complements and satellites (Baker, Fillmore, and Lowe 1998). The developers of PropBank integrate the difference between arguments and adjuncts directly into the level of specificity of their annotation. They adopt labels that are common across verbs for adjuncts. They inherit these"
J06-3002,W04-3212,0,0.110046,"Missing"
J06-3002,C00-2137,0,0.0101097,"nt-adjunct classifier, and the test examples classified as nouns are given to the noun argument-adjunct classifier. Thus, this cascade of classifiers performs the same task as the four-way classifier, but it does so in two passes. Table 11 shows that overall the one-step classification is better than the two-step classification, confirming the intuition that the two labeling problems should be solved at the same time.8 However, if we break down the performance, we see that recall of 8 The difference between the two results is significant (p &lt; .05) according to the randomized test described in Yeh (2000). 367 Computational Linguistics Volume 32, Number 3 Table 11 Percent precision, recall, and F score for the best two-step and one-step four-way classification of PPs, including and not including the preposition of. Two-step + of V-arg V-adj N-arg N-adj Prec Rec F Prec Rec F 37.5 56.2 83.0 71.2 45.6 52.2 83.5 57.5 41.2 54.1 83.2 63.6 42.2 59.6 81.3 69.5 29.3 60.2 91.3 56.2 34.6 59.9 86.0 62.1 Accuracy V-arg V-adj N-arg N-adj Accuracy One-step + of 68.9 72.0 Two-step − of One-step − of Prec Rec F Prec Rec F 41.3 52.8 67.3 60.3 50.0 41.6 70.0 60.3 45.3 46.5 68.6 60.3 42.2 59.6 65.4 69.5 31.4 60.2"
J06-3002,W04-3211,0,\N,Missing
J06-3002,W02-0906,0,\N,Missing
J06-3002,W02-2033,0,\N,Missing
J06-3002,J07-4002,0,\N,Missing
J06-3002,C98-1013,0,\N,Missing
J13-4006,W06-2922,0,0.0164481,"anar graphs belong to two conceptual groups: those that manipulate the graph, either by pre-processing or by post-processing (Hall and Novak 2005; McDonald and Pereira 2006), and those that adapt the algorithm to deal with non-planarity. Among the approaches that, like ours, devise an algorithm to deal with non-planarity, Yngve (1960) proposed a limited manipulation of registers to handle discontinuous constituents, which guaranteed that parsing/generation could be performed with a stack of very limited depth. An approach to non-planar parsing that is more similar to ours has been proposed in Attardi (2006). Attardi’s dependency parsing algorithm adds six new actions that allow this algorithm to parse any type of non-planar tree. Our Swap action is related to Attardi’s actions Left2 and Right2, which create dependency arcs between the second element on the stack and the front of the input queue. In the Attardi algorithm, every attachment to an element below the top of the stack requires the use of one of the new actions, whose frequency is much lower than the normal attachment actions, and therefore harder to learn. This contrasts with the Swap action, which handles reordering with a single acti"
J13-4006,P98-1013,0,0.0854963,"Missing"
J13-4006,P93-1005,0,0.21067,"e use latent variables to model the interaction between syntax and semantics. Latent variables serve as an interface between semantics and syntax, capturing properties of both structures relevant to the prediction of semantics given syntax and, conversely, syntax given semantics. Unlike hand-crafted features, latent variables are induced automatically from data, thereby avoiding a priori hard independence assumptions. Instead, the structure of the latent variable model is used to encode soft biases towards learning the types of features we expect to be useful. We define a history-based model (Black et al. 1993) for joint parsing of semantic and syntactic structures. History-based models map structured representations to sequences of derivation steps, and model the probability of each step conditioned on the entire sequence of previous steps. There are standard shift-reduce algorithms (Nivre, Hall, and Nilsson 2004) for mapping a syntactic dependency graph to a derivation sequence, and similar algorithms can be defined for mapping a semantic dependency graph to a derivation sequence, as discussed subsequently. But defining a joint syntactic–semantic derivation presents a challenge. Namely, given the"
J13-4006,D12-1133,0,0.0438463,"2013 Association for Computational Linguistics Computational Linguistics Volume 39, Number 4 nature of the approach, this extension of the ISBN architecture to weakly synchronized syntacticsemantic derivations is also an exemplification of its applicability to other problems where two independent, but related, representations are being learned. 1. Introduction Success in statistical syntactic parsing based on supervised techniques trained on a large corpus of syntactic trees—both constituency-based (Collins 1999; Charniak 2000; Henderson 2003) and dependency-based (McDonald 2006; Nivre 2006; Bohnet and Nivre 2012; Hatori et al. 2012)—has paved the way to applying statistical approaches to the more ambitious goals of recovering semantic representations, such as the logical form of a sentence (Ge and Mooney 2005; Wong and Mooney 2007; Zettlemoyer and Collins 2007; Ge and Mooney 2009; Kwiatkowski et al. 2011) or learning the propositional argument-structure of its main predicates (Miller et al. 2000; Gildea and Jurafsky 2002; Carreras and M`arquez 2005; M`arquez et al. 2008; Li, Zhou, and Ng 2010). Moving towards a semantic level of representation of language and text has many potential applications in q"
J13-4006,burchardt-etal-2006-salsa,0,0.097902,"participants, or representations of objects involving their properties. The participants and properties in a frame are designated with a set of semantic roles called frame elements. One example is the MOTION DIRECTIONAL frame, and its associated frame elements include the THEME (the moving object), the GOAL (the ultimate destination), the SOURCE, and the PATH. The collection of sentences used to exemplify frames in the English FrameNet has been sampled to produce informative lexicographic examples, but no attempt has been made to produce representative distributions. The German SALSA corpus (Burchardt et al. 2006), however, has been annotated with FrameNet annotation. This extension to exhaustive corpus coverage and a new language has only required a few novel frames, demonstrating the cross-linguistic validity of this annotation scheme. FrameNets for other languages, Spanish and Japanese, are also under construction. Another semantically annotated corpus—the one we use in this work for experiments on English—is called Proposition Bank (PropBank) (Palmer, Gildea, and Kingsbury 2005). PropBank is based on the assumption that the lexicon is not a list of irregularities, but that systematic correlations c"
J13-4006,W05-0620,0,0.103932,"Missing"
J13-4006,A00-2018,0,0.0291314,"mber 2012; accepted for publication: 1 November 2012. doi:10.1162/COLI a 00158 © 2013 Association for Computational Linguistics Computational Linguistics Volume 39, Number 4 nature of the approach, this extension of the ISBN architecture to weakly synchronized syntacticsemantic derivations is also an exemplification of its applicability to other problems where two independent, but related, representations are being learned. 1. Introduction Success in statistical syntactic parsing based on supervised techniques trained on a large corpus of syntactic trees—both constituency-based (Collins 1999; Charniak 2000; Henderson 2003) and dependency-based (McDonald 2006; Nivre 2006; Bohnet and Nivre 2012; Hatori et al. 2012)—has paved the way to applying statistical approaches to the more ambitious goals of recovering semantic representations, such as the logical form of a sentence (Ge and Mooney 2005; Wong and Mooney 2007; Zettlemoyer and Collins 2007; Ge and Mooney 2009; Kwiatkowski et al. 2011) or learning the propositional argument-structure of its main predicates (Miller et al. 2000; Gildea and Jurafsky 2002; Carreras and M`arquez 2005; M`arquez et al. 2008; Li, Zhou, and Ng 2010). Moving towards a se"
J13-4006,W08-2134,0,0.0835302,"tions separately, with a pipeline of state-of-the-art systems, and then reranks the joint representation in a Table 7 Scores of the fully connected model on the final testing sets of the CoNLL-2008 shared task (percentages). WSJ Brown WSJ+Brown Syntactic LAS P 88.4 80.4 87.5 79.9 65.9 78.4 Semantic R F1 75.5 60.8 73.9 77.6 63.3 76.1 P Macro R F1 84.2 73.1 83.0 82.0 70.6 80.7 83.0 71.8 81.8 981 Computational Linguistics Volume 39, Number 4 Table 8 Comparison with other models on the CoNLL-2008 test set (percentages). C O NLL M EASURES M ODEL Johansson and Nugues (2008b) Ciaramita et al. (2008) Che et al. (2008) Zhao and Kit (2008) This article Henderson et al. (2008) Llu´ıs and M`arquez (2008) C ROSSING A RCS Synt LAS Semantic F1 Macro F1 P 89.3 87.4 86.7 87.7 87.5 87.6 85.8 81.6 78.0 78.5 76.7 76.1 73.1 70.3 85.5 82.7 82.7 82.2 81.8 80.5 78.1 67.0 59.9 56.9 58.5 62.1 72.6 53.8 Semantics R F1 44.5 34.2 32.4 36.1 29.4 1.7 19.2 53.5 43.5 41.3 44.6 39.9 3.3 28.3 final step (Johansson and Nugues 2008b). Similarly, Che et al. (2008) also implement a pipeline consisting of state-of-the-art components where the final inference stage is performed using Integer Linear Programming to ensure global coherence o"
J13-4006,W08-2139,0,0.0541097,"Missing"
J13-4006,P05-1033,0,0.175384,"forms a linguistically meaningful chunk in that it includes all the decisions about the arcs on the left side of the associated word, both its parents and its children. Thus, synchronizing the syntactic and semantic subsequences according to their associated word places together subsequences that are likely to be correlated. Note that such pairs of syntactic and semantic subsequences will, in general, have different numbers of steps on each side and these numbers of steps are, in general, unbounded. Therefore, instead of defining atomic synchronized rules as in synchronous grammars (Wu 1997; Chiang 2005), we resort to parametrized models that exploit the internal structure of the paired subsequences. This derivational, joint approach to handling these complex representations leads to a new proposal on how to learn them, which avoids extensive and complex feature engineering, as discussed in the following. 1.2 Joint Learning of Syntax and Semantics Our probabilistic model is learned using Incremental Sigmoid Belief Networks (ISBNs) (Henderson and Titov 2010), a recent development of an early latent variable model 952 Henderson et al. Joint Syntactic and Semantic Parsing for syntactic structure"
J13-4006,W10-1811,0,0.0444172,"Missing"
J13-4006,W08-2138,0,0.114405,"Missing"
J13-4006,cmejrek-etal-2004-prague,0,0.0485572,"Missing"
J13-4006,J81-4005,0,0.781848,"Missing"
J13-4006,P07-1071,0,0.0242847,"Missing"
J13-4006,N04-1035,0,0.0110215,"trivial to develop systems that actually succeed in exploiting this intuitively obvious 988 Henderson et al. Joint Syntactic and Semantic Parsing correlation. Li, Zhou, and Ng’s approach is also different from ours in that they do not attempt to induce common representations useful for both tasks or for many languages, and as such cannot be regarded as multi-task, nor as multilingual, learning. Synchronous grammars provide an elegant way to handle multiple levels of representation. They have received much attention because of their applications in syntaxbased statistical machine translation (Galley et al. 2004; Chiang 2005; Nesson and Shieber 2008) and semantic parsing (Wong and Mooney 2006, 2007). Results indicate that these techniques are among the best both in machine translation and in the database query domain. Our method differs from those techniques that use a synchronous grammar, because we do not rewrite pairs of synchronized non-terminals, but instead synchronize chunks of derivation sequences. This difference is in part motivated by the fact that the strings for our two structures are perfectly aligned (being the same string), so synchronizing on the chunks of derivations associated with"
J13-4006,P11-2051,0,0.0133446,"main predicates (Miller et al. 2000; Gildea and Jurafsky 2002; Carreras and M`arquez 2005; M`arquez et al. 2008; Li, Zhou, and Ng 2010). Moving towards a semantic level of representation of language and text has many potential applications in question answering and information extraction (Surdeanu et al. 2003; Moschitti et al. 2007), and has recently been argued to be useful in machine translation and its evaluation (Wu and Fung 2009; Liu and Gildea 2010; Lo and Wu 2011; Wu et al. 2011), dialogue systems (Basili et al. 2009; Van der Plas, Henderson, and Merlo 2009), automatic data generation (Gao and Vogel 2011; Van der Plas, Merlo, and Henderson 2011) and authorship attribution (Hedegaard and Simonsen 2011), among others. The recovery of the full meaning of text requires structured analyses of both its grammar and its semantics. These two forms of linguistic knowledge are usually thought to be at least partly independent, as demonstrated by speakers’ ability to understand the meaning of ungrammatical text or speech and to assign grammatical categories and structures to unknown words and nonsense sentences. These two levels of representation of language, however, are closely correlated. From a lingu"
J13-4006,P11-2003,1,0.92649,"used to estimate the probability of this chosen parser action, also shown in red. The edges to the state that is used to make this decision are specified by identifying the most recent previous state that shares some property with this state. In Figure 8, these edges are labeled with the property, such as having the same word on the top of the stack (S=S) or the top of the stack being the same as the current leftmost child of the top of the stack (S=LS). The argument for the incremental specification of model structure can be applied to any Bayesian network architecture, not just SBNs (e.g., Garg and Henderson 2011). We focus on ISBNs because, as shown in Section 4.1.5, they are closely related to the empirically successful neural network models of Henderson (2003), and they have achieved very good results on the sub-problem of parsing syntactic dependencies (Titov and Henderson 2007d). 4.1.4 ISBNs for Derivations of Structures. The general form of ISBN models that have been proposed for modeling derivations of structures is illustrated in Figure 9. Figure 9 illustrates a situation where we are given a derivation history preceding the elementary decision dik in decision Di , and we wish to compute a prob"
J13-4006,P09-1069,0,0.00946418,"where two independent, but related, representations are being learned. 1. Introduction Success in statistical syntactic parsing based on supervised techniques trained on a large corpus of syntactic trees—both constituency-based (Collins 1999; Charniak 2000; Henderson 2003) and dependency-based (McDonald 2006; Nivre 2006; Bohnet and Nivre 2012; Hatori et al. 2012)—has paved the way to applying statistical approaches to the more ambitious goals of recovering semantic representations, such as the logical form of a sentence (Ge and Mooney 2005; Wong and Mooney 2007; Zettlemoyer and Collins 2007; Ge and Mooney 2009; Kwiatkowski et al. 2011) or learning the propositional argument-structure of its main predicates (Miller et al. 2000; Gildea and Jurafsky 2002; Carreras and M`arquez 2005; M`arquez et al. 2008; Li, Zhou, and Ng 2010). Moving towards a semantic level of representation of language and text has many potential applications in question answering and information extraction (Surdeanu et al. 2003; Moschitti et al. 2007), and has recently been argued to be useful in machine translation and its evaluation (Wu and Fung 2009; Liu and Gildea 2010; Lo and Wu 2011; Wu et al. 2011), dialogue systems (Basili"
J13-4006,W05-0602,0,0.0203331,"ations is also an exemplification of its applicability to other problems where two independent, but related, representations are being learned. 1. Introduction Success in statistical syntactic parsing based on supervised techniques trained on a large corpus of syntactic trees—both constituency-based (Collins 1999; Charniak 2000; Henderson 2003) and dependency-based (McDonald 2006; Nivre 2006; Bohnet and Nivre 2012; Hatori et al. 2012)—has paved the way to applying statistical approaches to the more ambitious goals of recovering semantic representations, such as the logical form of a sentence (Ge and Mooney 2005; Wong and Mooney 2007; Zettlemoyer and Collins 2007; Ge and Mooney 2009; Kwiatkowski et al. 2011) or learning the propositional argument-structure of its main predicates (Miller et al. 2000; Gildea and Jurafsky 2002; Carreras and M`arquez 2005; M`arquez et al. 2008; Li, Zhou, and Ng 2010). Moving towards a semantic level of representation of language and text has many potential applications in question answering and information extraction (Surdeanu et al. 2003; Moschitti et al. 2007), and has recently been argued to be useful in machine translation and its evaluation (Wu and Fung 2009; Liu an"
J13-4006,W09-1205,1,0.929971,"Missing"
J13-4006,J02-3001,0,0.528478,"supervised techniques trained on a large corpus of syntactic trees—both constituency-based (Collins 1999; Charniak 2000; Henderson 2003) and dependency-based (McDonald 2006; Nivre 2006; Bohnet and Nivre 2012; Hatori et al. 2012)—has paved the way to applying statistical approaches to the more ambitious goals of recovering semantic representations, such as the logical form of a sentence (Ge and Mooney 2005; Wong and Mooney 2007; Zettlemoyer and Collins 2007; Ge and Mooney 2009; Kwiatkowski et al. 2011) or learning the propositional argument-structure of its main predicates (Miller et al. 2000; Gildea and Jurafsky 2002; Carreras and M`arquez 2005; M`arquez et al. 2008; Li, Zhou, and Ng 2010). Moving towards a semantic level of representation of language and text has many potential applications in question answering and information extraction (Surdeanu et al. 2003; Moschitti et al. 2007), and has recently been argued to be useful in machine translation and its evaluation (Wu and Fung 2009; Liu and Gildea 2010; Lo and Wu 2011; Wu et al. 2011), dialogue systems (Basili et al. 2009; Van der Plas, Henderson, and Merlo 2009), automatic data generation (Gao and Vogel 2011; Van der Plas, Merlo, and Henderson 2011)"
J13-4006,P10-1151,0,0.037242,"Missing"
J13-4006,W09-1201,0,0.0282323,"Missing"
J13-4006,W05-1505,0,0.0299227,"rcs, rather than to change the order of the target string. The switching of elements of the semantic structure used in Wong and Mooney (2007) is more similar to the word reordering technique of Hajiˇcov´a et al. (2004) than to our Swap operation, because the reordering occurs before, rather than during, the derivation. The notion of planarity has been widely discussed in many works cited herein, and in the dependency parsing literature. Approaches to dealing with non-planar graphs belong to two conceptual groups: those that manipulate the graph, either by pre-processing or by post-processing (Hall and Novak 2005; McDonald and Pereira 2006), and those that adapt the algorithm to deal with non-planarity. Among the approaches that, like ours, devise an algorithm to deal with non-planarity, Yngve (1960) proposed a limited manipulation of registers to handle discontinuous constituents, which guaranteed that parsing/generation could be performed with a stack of very limited depth. An approach to non-planar parsing that is more similar to ours has been proposed in Attardi (2006). Attardi’s dependency parsing algorithm adds six new actions that allow this algorithm to parse any type of non-planar tree. Our S"
J13-4006,P12-1110,0,0.0280159,"omputational Linguistics Computational Linguistics Volume 39, Number 4 nature of the approach, this extension of the ISBN architecture to weakly synchronized syntacticsemantic derivations is also an exemplification of its applicability to other problems where two independent, but related, representations are being learned. 1. Introduction Success in statistical syntactic parsing based on supervised techniques trained on a large corpus of syntactic trees—both constituency-based (Collins 1999; Charniak 2000; Henderson 2003) and dependency-based (McDonald 2006; Nivre 2006; Bohnet and Nivre 2012; Hatori et al. 2012)—has paved the way to applying statistical approaches to the more ambitious goals of recovering semantic representations, such as the logical form of a sentence (Ge and Mooney 2005; Wong and Mooney 2007; Zettlemoyer and Collins 2007; Ge and Mooney 2009; Kwiatkowski et al. 2011) or learning the propositional argument-structure of its main predicates (Miller et al. 2000; Gildea and Jurafsky 2002; Carreras and M`arquez 2005; M`arquez et al. 2008; Li, Zhou, and Ng 2010). Moving towards a semantic level of representation of language and text has many potential applications in question answering and"
J13-4006,P11-2012,0,0.0126839,"; M`arquez et al. 2008; Li, Zhou, and Ng 2010). Moving towards a semantic level of representation of language and text has many potential applications in question answering and information extraction (Surdeanu et al. 2003; Moschitti et al. 2007), and has recently been argued to be useful in machine translation and its evaluation (Wu and Fung 2009; Liu and Gildea 2010; Lo and Wu 2011; Wu et al. 2011), dialogue systems (Basili et al. 2009; Van der Plas, Henderson, and Merlo 2009), automatic data generation (Gao and Vogel 2011; Van der Plas, Merlo, and Henderson 2011) and authorship attribution (Hedegaard and Simonsen 2011), among others. The recovery of the full meaning of text requires structured analyses of both its grammar and its semantics. These two forms of linguistic knowledge are usually thought to be at least partly independent, as demonstrated by speakers’ ability to understand the meaning of ungrammatical text or speech and to assign grammatical categories and structures to unknown words and nonsense sentences. These two levels of representation of language, however, are closely correlated. From a linguistic point of view, the assumption that syntactic distributions will be predictive of semantic rol"
J13-4006,N03-1014,1,0.346312,"pted for publication: 1 November 2012. doi:10.1162/COLI a 00158 © 2013 Association for Computational Linguistics Computational Linguistics Volume 39, Number 4 nature of the approach, this extension of the ISBN architecture to weakly synchronized syntacticsemantic derivations is also an exemplification of its applicability to other problems where two independent, but related, representations are being learned. 1. Introduction Success in statistical syntactic parsing based on supervised techniques trained on a large corpus of syntactic trees—both constituency-based (Collins 1999; Charniak 2000; Henderson 2003) and dependency-based (McDonald 2006; Nivre 2006; Bohnet and Nivre 2012; Hatori et al. 2012)—has paved the way to applying statistical approaches to the more ambitious goals of recovering semantic representations, such as the logical form of a sentence (Ge and Mooney 2005; Wong and Mooney 2007; Zettlemoyer and Collins 2007; Ge and Mooney 2009; Kwiatkowski et al. 2011) or learning the propositional argument-structure of its main predicates (Miller et al. 2000; Gildea and Jurafsky 2002; Carreras and M`arquez 2005; M`arquez et al. 2008; Li, Zhou, and Ng 2010). Moving towards a semantic level of r"
J13-4006,W08-2122,1,0.960335,"her drop of recall on non-planar dependencies. Applying the same planarization approach to semantic dependency structures is not trivial and would require a novel planarization algorithm, because semantic dependency graphs are highly disconnected structures, and direct application of any planarization algorithm, such as the one proposed in Nivre and Nilsson (2005), is unlikely to be appropriate. For instance, a method that extends the planarization method to semantic predicate-argument structures by exploiting the connectedness of the corresponding syntactic dependency trees has been tried in Henderson et al. (2008). Experimental results reported in Section 6 indicate that the method that we will illustrate in the following paragraphs yields better performance. A different way to tackle non-planarity is to extend the set of parsing actions to a more complex set that can parse any type of non-planarity (Attardi 2006). This approach is discussed in more detail in Section 7. We adopt a conservative version of this approach 3 Note that this planarity definition is stricter than the definition normally used in graph theory where the entire plane is used. Some parsing algorithms require projectivity: this is a"
J13-4006,W07-2416,0,0.0276781,"re needed because nominal long distance dependencies are not captured under the Penn Treebank’s system of empty categories. They are used for all those cases in which the nominal argument is outside the noun phrase. For example, in a support verb construction, such as Mary took dozens of walks, the arcs linking walks to of , of to dozens, and dozens to took are all marked as support. The data we use for English are the output of an automatic process of conversion of the original PTB, PropBank, and NomBank into dependency structures, performed by the algorithm described in Johansson and Nugues (2007). These are the data provided to participants to the CoNLL-2008 and CoNLL-2009 shared tasks (http://ifarm.nl/signll/conll/). An example is shown in Figure 3. This representation encodes both the grammatical functions and the semantic labels that describe the sentence. Argument labels in PropBank and NomBank are assigned to constituents, as shown in Figure 2. After the conversion to dependency the PropBank and NomBank labels Figure 3 An example from the PropBank corpus of verbal predicates and their semantic roles (lower half) paired with syntactic dependencies derived from the Penn Treebank. 9"
J13-4006,D08-1008,0,0.0432658,"tic representations is central for any system that taps into the meaning of text. Standard approaches to automatic semantic role labeling use hand-crafted features of syntactic and semantic representations within linear models trained with supervised learning. For example, Gildea and Jurafsky (2002) formulate the shallow semantic task of semantic role labeling (SRL) as a classification problem, where the semantic role to be assigned to each constituent is inferred on the basis of its co-occurrence counts with syntactic features extracted from parse trees. More recent and accurate SRL methods (Johansson and Nugues 2008a; Punyakanok, Roth, and Yih 2008) use complex sets of lexico-syntactic features and declarative constraints to infer the semantic structure. Whereas supervised learning is more flexible, general, and adaptable than hand-crafted systems, linear models require complex features and the number of these features grows with the complexity of the task. To keep the number of features tractable, model designers impose hard constraints on the possible interactions within the semantic or syntactic structures, such as conditioning on grandparents but not great-great-grandparents. Likewise, hard constrain"
J13-4006,W08-2123,0,0.0264032,"tic representations is central for any system that taps into the meaning of text. Standard approaches to automatic semantic role labeling use hand-crafted features of syntactic and semantic representations within linear models trained with supervised learning. For example, Gildea and Jurafsky (2002) formulate the shallow semantic task of semantic role labeling (SRL) as a classification problem, where the semantic role to be assigned to each constituent is inferred on the basis of its co-occurrence counts with syntactic features extracted from parse trees. More recent and accurate SRL methods (Johansson and Nugues 2008a; Punyakanok, Roth, and Yih 2008) use complex sets of lexico-syntactic features and declarative constraints to infer the semantic structure. Whereas supervised learning is more flexible, general, and adaptable than hand-crafted systems, linear models require complex features and the number of these features grows with the complexity of the task. To keep the number of features tractable, model designers impose hard constraints on the possible interactions within the semantic or syntactic structures, such as conditioning on grandparents but not great-great-grandparents. Likewise, hard constrain"
J13-4006,kawahara-etal-2002-construction,0,0.0253574,"Missing"
J13-4006,D11-1140,0,0.0212437,"nt, but related, representations are being learned. 1. Introduction Success in statistical syntactic parsing based on supervised techniques trained on a large corpus of syntactic trees—both constituency-based (Collins 1999; Charniak 2000; Henderson 2003) and dependency-based (McDonald 2006; Nivre 2006; Bohnet and Nivre 2012; Hatori et al. 2012)—has paved the way to applying statistical approaches to the more ambitious goals of recovering semantic representations, such as the logical form of a sentence (Ge and Mooney 2005; Wong and Mooney 2007; Zettlemoyer and Collins 2007; Ge and Mooney 2009; Kwiatkowski et al. 2011) or learning the propositional argument-structure of its main predicates (Miller et al. 2000; Gildea and Jurafsky 2002; Carreras and M`arquez 2005; M`arquez et al. 2008; Li, Zhou, and Ng 2010). Moving towards a semantic level of representation of language and text has many potential applications in question answering and information extraction (Surdeanu et al. 2003; Moschitti et al. 2007), and has recently been argued to be useful in machine translation and its evaluation (Wu and Fung 2009; Liu and Gildea 2010; Lo and Wu 2011; Wu et al. 2011), dialogue systems (Basili et al. 2009; Van der Plas"
J13-4006,P11-1112,0,0.0168707,"sequence, as discussed subsequently. But defining a joint syntactic–semantic derivation presents a challenge. Namely, given the complex nature of correspondences between the structures, it is not obvious how to synchronize individual semantic– syntactic steps in the derivation. Previous joint statistical models of dependency syntax and SRL have either ignored semantic arcs not corresponding to single syntactic arcs (Thompson, Levy, and Manning 2003; Titov and Klementiev 2011) or resorted to pre-/post-processing strategies that modify semantic or syntactic structures (Llu´ıs and M`arquez 2008; Lang and Lapata 2011; Titov and Klementiev 2012). In a constituency setting, Li, Zhou, and Ng (2010) explore different levels of coupling of syntax and semantics, and find that only explicit interleaving or explicit feature selection yield improvements in performance. Instead of synchronizing individual steps, we (1) decompose both the syntactic derivation and the semantic derivation into subsequences, where each subsequence corresponds to a single word in the sentence, and then (2) synchronize syntactic and semantic subsequences corresponding to the same word with each other. To decide which steps correspond to"
J13-4006,P10-1113,0,0.0686563,"Missing"
J13-4006,D07-1072,0,0.0138803,"Missing"
J13-4006,C10-1081,0,0.00983689,"y 2005; Wong and Mooney 2007; Zettlemoyer and Collins 2007; Ge and Mooney 2009; Kwiatkowski et al. 2011) or learning the propositional argument-structure of its main predicates (Miller et al. 2000; Gildea and Jurafsky 2002; Carreras and M`arquez 2005; M`arquez et al. 2008; Li, Zhou, and Ng 2010). Moving towards a semantic level of representation of language and text has many potential applications in question answering and information extraction (Surdeanu et al. 2003; Moschitti et al. 2007), and has recently been argued to be useful in machine translation and its evaluation (Wu and Fung 2009; Liu and Gildea 2010; Lo and Wu 2011; Wu et al. 2011), dialogue systems (Basili et al. 2009; Van der Plas, Henderson, and Merlo 2009), automatic data generation (Gao and Vogel 2011; Van der Plas, Merlo, and Henderson 2011) and authorship attribution (Hedegaard and Simonsen 2011), among others. The recovery of the full meaning of text requires structured analyses of both its grammar and its semantics. These two forms of linguistic knowledge are usually thought to be at least partly independent, as demonstrated by speakers’ ability to understand the meaning of ungrammatical text or speech and to assign grammatical"
J13-4006,W08-2124,0,0.466359,"Missing"
J13-4006,P11-1023,0,0.0121846,"ey 2007; Zettlemoyer and Collins 2007; Ge and Mooney 2009; Kwiatkowski et al. 2011) or learning the propositional argument-structure of its main predicates (Miller et al. 2000; Gildea and Jurafsky 2002; Carreras and M`arquez 2005; M`arquez et al. 2008; Li, Zhou, and Ng 2010). Moving towards a semantic level of representation of language and text has many potential applications in question answering and information extraction (Surdeanu et al. 2003; Moschitti et al. 2007), and has recently been argued to be useful in machine translation and its evaluation (Wu and Fung 2009; Liu and Gildea 2010; Lo and Wu 2011; Wu et al. 2011), dialogue systems (Basili et al. 2009; Van der Plas, Henderson, and Merlo 2009), automatic data generation (Gao and Vogel 2011; Van der Plas, Merlo, and Henderson 2011) and authorship attribution (Hedegaard and Simonsen 2011), among others. The recovery of the full meaning of text requires structured analyses of both its grammar and its semantics. These two forms of linguistic knowledge are usually thought to be at least partly independent, as demonstrated by speakers’ ability to understand the meaning of ungrammatical text or speech and to assign grammatical categories and s"
J13-4006,J93-2004,0,0.048808,"Missing"
J13-4006,J08-2001,0,0.0213094,"Missing"
J13-4006,P05-1010,0,0.0303616,"Missing"
J13-4006,E06-1011,0,0.0289654,"er of the target string. The switching of elements of the semantic structure used in Wong and Mooney (2007) is more similar to the word reordering technique of Hajiˇcov´a et al. (2004) than to our Swap operation, because the reordering occurs before, rather than during, the derivation. The notion of planarity has been widely discussed in many works cited herein, and in the dependency parsing literature. Approaches to dealing with non-planar graphs belong to two conceptual groups: those that manipulate the graph, either by pre-processing or by post-processing (Hall and Novak 2005; McDonald and Pereira 2006), and those that adapt the algorithm to deal with non-planarity. Among the approaches that, like ours, devise an algorithm to deal with non-planarity, Yngve (1960) proposed a limited manipulation of registers to handle discontinuous constituents, which guaranteed that parsing/generation could be performed with a stack of very limited depth. An approach to non-planar parsing that is more similar to ours has been proposed in Attardi (2006). Attardi’s dependency parsing algorithm adds six new actions that allow this algorithm to parse any type of non-planar tree. Our Swap action is related to Att"
J13-4006,W08-2101,1,0.933675,"Missing"
J13-4006,J01-3003,1,0.121086,"orrelated. From a linguistic point of view, the assumption that syntactic distributions will be predictive of semantic role assignments is based on linking theory (Levin 1986). Linking theory assumes the existence of a ranking of semantic roles that are mapped by default on a ranking of grammatical functions and syntactic positions, and it attempts to predict the mapping of the underlying semantic component of a predicate’s meaning onto the syntactic structure. For example, Agents are always mapped in syntactically higher positions than Themes. Linking theory has been confirmed statistically (Merlo and Stevenson 2001). It is currently common to represent the syntactic and semantic role structures of a sentence in terms of dependencies, as illustrated in Figure 1. The complete graph of both the syntax and the semantics of the sentences is composed of two half graphs, which Figure 1 A semantic dependency graph labeled with semantic roles (lower half) paired with a syntactic dependency tree labeled with grammatical relations. 950 Henderson et al. Joint Syntactic and Semantic Parsing share all their vertices—namely, the words. Internally, these two half graphs exhibit different properties. The syntactic graph"
J13-4006,W04-2705,0,0.0149074,"y express consistent semantic roles across verbs, whereas arguments receiving an AM-X label are supposed to be adjuncts, and the roles they express are consistent across all verbs. A0 and A1 arguments are annotated based on the proto-role theory presented in Dowty (1991) and correspond to proto-agents and proto-patients, respectively. Although PropBank, unlike FrameNet, does not attempt to group different predicates evoking the same prototypical situation, it does distinguish between different senses of polysemous verbs, resulting in multiple framesets for such predicates. NomBank annotation (Meyers et al. 2004) extends the PropBank framework to annotate arguments of nouns. Only the subset of nouns that take arguments are annotated in NomBank and only a subset of the non-argument siblings of nouns are marked as ARG-M. The most notable specificity of NomBank is the use of support chains, marked as SU. Support chains are needed because nominal long distance dependencies are not captured under the Penn Treebank’s system of empty categories. They are used for all those cases in which the nominal argument is outside the noun phrase. For example, in a support verb construction, such as Mary took dozens of"
J13-4006,A00-2030,0,0.0353462,"Missing"
J13-4006,R09-1051,0,0.195166,"Missing"
J13-4006,P07-1098,0,0.0109335,"oaches to the more ambitious goals of recovering semantic representations, such as the logical form of a sentence (Ge and Mooney 2005; Wong and Mooney 2007; Zettlemoyer and Collins 2007; Ge and Mooney 2009; Kwiatkowski et al. 2011) or learning the propositional argument-structure of its main predicates (Miller et al. 2000; Gildea and Jurafsky 2002; Carreras and M`arquez 2005; M`arquez et al. 2008; Li, Zhou, and Ng 2010). Moving towards a semantic level of representation of language and text has many potential applications in question answering and information extraction (Surdeanu et al. 2003; Moschitti et al. 2007), and has recently been argued to be useful in machine translation and its evaluation (Wu and Fung 2009; Liu and Gildea 2010; Lo and Wu 2011; Wu et al. 2011), dialogue systems (Basili et al. 2009; Van der Plas, Henderson, and Merlo 2009), automatic data generation (Gao and Vogel 2011; Van der Plas, Merlo, and Henderson 2011) and authorship attribution (Hedegaard and Simonsen 2011), among others. The recovery of the full meaning of text requires structured analyses of both its grammar and its semantics. These two forms of linguistic knowledge are usually thought to be at least partly independen"
J13-4006,W05-1509,1,0.808134,"ub-problems needed to be solved to find a solution for these primary tasks, then one would expect an improvement from inducing shared representations. Multi-task learning methods have been shown to be beneficial in many domains, including natural language processing (Ando and Zhang 2005a, 2005b; Argyriou, Evgeniou, and Pontil 2006; Collobert and Weston 2008). Their application in the context of syntactic-semantic parsing has been very limited, however. The only other such successful multi-task learning approach we are aware of targets a similar, but more restricted, task of function labeling (Musillo and Merlo 2005). Musillo and Merlo (2005) conclusively show that jointly learning functional and syntactic information can significantly improve syntax. Our joint learning approach is an example of a multi-task learning approach in that the induced representations in the vectors of latent variables can capture hidden sub-problems relevant to predicting both syntactic and semantic structures. The rest of this article will first describe the data that are used in this work and their relevant properties. We then present our probabilistic model of joint syntactic parsing 953 Computational Linguistics Volume 39,"
J13-4006,N06-2026,1,0.911364,"Missing"
J13-4006,P08-2054,1,0.840744,"st, our ISBN latent variable models do not require heuristics to control the complexity of the augmented grammars or to search for predictive latent representations. Furthermore, probabilistic context-free grammars augmented with latent annotations do impose context-free independence assumptions between the latent labels, contrary to our models. Finally, our ISBN models have been successfully applied to both phrase-structure and dependency parsing. State-of-the-art results on unlexicalized dependency parsing have recently been achieved with latent variable probabilistic context-free grammars (Musillo and Merlo 2008; Musillo 2010). These latent variable grammars are compact and interpretable from a linguistic perspective, and they integrate grammar transforms that constrain the flow of latent information, thereby drastically limiting the space of latent annotations. For example, they encode the notion of X-bar projection in their constrained latent variables. 8. Conclusions and Future Work The proposed joint model achieves competitive performance on both syntactic and semantic dependency parsing for several languages. Our experiments also demonstrate the benefit of joint learning of syntax and semantics."
J13-4006,P09-1040,0,0.219321,"Missing"
J13-4006,W06-2933,0,0.0211291,"those used in previous dependency parsing work. No independence assumptions are made in the probability decomposition itself. This allows the probability estimation technique (discussed in Section 4) to make maximal use of its latent variables to learn correlations between the different parser actions, both within and between structures. 3.1 Synchronized Derivations We first specify the syntactic and semantic derivations separately, before specifying how they are synchronized in a joint generative model. The derivations for syntactic dependency trees are based on a shift-reduce style parser (Nivre et al. 2006; Titov and Henderson 2007d). The derivations use a stack and an input queue. There are actions for creating a leftward or rightward arc between the top of the stack and the front of the queue, for popping a word from the stack, and for shifting a word from the queue to the stack. A syntactic configuration of the parser is defined by the current stack, the queue of remaining input words, and the partial labeled dependency structure constructed by previous parser actions. The parser starts with an empty stack and terminates when it 957 Computational Linguistics Volume 39, Number 4 reaches a con"
J13-4006,W09-3811,0,0.0251302,"Missing"
J13-4006,P05-1013,0,0.029498,"Missing"
J13-4006,J05-1004,0,0.311193,"Missing"
J13-4006,P06-1055,0,0.0132839,"rchitecture to design a joint model of syntactic–semantic dependency parsing. In traditional fully supervised parsing models, designing a joint syntactic–semantic parsing model would require extensive feature engineering. These features pick out parts of the corpus annotation that are relevant to predicting other parts of the corpus annotation. If features are missing then predicting the annotation cannot be done accurately, and if there are too many features then the model cannot be learned accurately. Latent variable models, such as ISBNs and Latent PCFGs (Matsuzaki, Miyao, and Tsujii 2005; Petrov et al. 2006), have the advantage that the model can induce new, more predictive, features by composing elementary features, or propagate information to include predictive but non-local features. These latent annotations are induced during learning, allowing the model to both predict them from other parts of the annotation and use them to predict the desired corpus annotation. In ISBNs, we use latent variables to induce features of the parse history D1 , . . . , Di−1 that are used to predict future parser decisions Di , . . . , Dm . The main difference between ISBNs and Latent PCFGs is that ISBNs have vect"
J13-4006,W05-1512,0,0.0551746,"Missing"
J13-4006,J08-2005,0,0.301645,"Missing"
J13-4006,P03-1002,0,0.0145641,"lying statistical approaches to the more ambitious goals of recovering semantic representations, such as the logical form of a sentence (Ge and Mooney 2005; Wong and Mooney 2007; Zettlemoyer and Collins 2007; Ge and Mooney 2009; Kwiatkowski et al. 2011) or learning the propositional argument-structure of its main predicates (Miller et al. 2000; Gildea and Jurafsky 2002; Carreras and M`arquez 2005; M`arquez et al. 2008; Li, Zhou, and Ng 2010). Moving towards a semantic level of representation of language and text has many potential applications in question answering and information extraction (Surdeanu et al. 2003; Moschitti et al. 2007), and has recently been argued to be useful in machine translation and its evaluation (Wu and Fung 2009; Liu and Gildea 2010; Lo and Wu 2011; Wu et al. 2011), dialogue systems (Basili et al. 2009; Van der Plas, Henderson, and Merlo 2009), automatic data generation (Gao and Vogel 2011; Van der Plas, Merlo, and Henderson 2011) and authorship attribution (Hedegaard and Simonsen 2011), among others. The recovery of the full meaning of text requires structured analyses of both its grammar and its semantics. These two forms of linguistic knowledge are usually thought to be at"
J13-4006,W08-2121,0,0.0315325,"Missing"
J13-4006,taule-etal-2008-ancora,0,0.0186112,"Missing"
J13-4006,P07-1080,1,0.904311,"re needed because nominal long distance dependencies are not captured under the Penn Treebank’s system of empty categories. They are used for all those cases in which the nominal argument is outside the noun phrase. For example, in a support verb construction, such as Mary took dozens of walks, the arcs linking walks to of , of to dozens, and dozens to took are all marked as support. The data we use for English are the output of an automatic process of conversion of the original PTB, PropBank, and NomBank into dependency structures, performed by the algorithm described in Johansson and Nugues (2007). These are the data provided to participants to the CoNLL-2008 and CoNLL-2009 shared tasks (http://ifarm.nl/signll/conll/). An example is shown in Figure 3. This representation encodes both the grammatical functions and the semantic labels that describe the sentence. Argument labels in PropBank and NomBank are assigned to constituents, as shown in Figure 2. After the conversion to dependency the PropBank and NomBank labels Figure 3 An example from the PropBank corpus of verbal predicates and their semantic roles (lower half) paired with syntactic dependencies derived from the Penn Treebank. 9"
J13-4006,D07-1099,1,0.919153,"Missing"
J13-4006,W07-2218,1,0.414946,"uences. This derivational, joint approach to handling these complex representations leads to a new proposal on how to learn them, which avoids extensive and complex feature engineering, as discussed in the following. 1.2 Joint Learning of Syntax and Semantics Our probabilistic model is learned using Incremental Sigmoid Belief Networks (ISBNs) (Henderson and Titov 2010), a recent development of an early latent variable model 952 Henderson et al. Joint Syntactic and Semantic Parsing for syntactic structure prediction (Henderson 2003), which has shown very good performance for both constituency (Titov and Henderson 2007a) and dependency parsing (Titov and Henderson 2007d). Instead of hand-crafting features of the previous parsing decisions, as is standard in history-based models, ISBNs estimate the probability of the next parsing actions conditioned on a vector of latent-variable features of the parsing history. These features are induced automatically to maximize the likelihood of the syntactic–semantics graphs given in the training set, and therefore they encode important correlations between syntactic and semantic decisions. This makes joint learning of syntax and semantics a crucial component of our appr"
J13-4006,P11-1145,1,0.410843,"yntactic dependency graph to a derivation sequence, and similar algorithms can be defined for mapping a semantic dependency graph to a derivation sequence, as discussed subsequently. But defining a joint syntactic–semantic derivation presents a challenge. Namely, given the complex nature of correspondences between the structures, it is not obvious how to synchronize individual semantic– syntactic steps in the derivation. Previous joint statistical models of dependency syntax and SRL have either ignored semantic arcs not corresponding to single syntactic arcs (Thompson, Levy, and Manning 2003; Titov and Klementiev 2011) or resorted to pre-/post-processing strategies that modify semantic or syntactic structures (Llu´ıs and M`arquez 2008; Lang and Lapata 2011; Titov and Klementiev 2012). In a constituency setting, Li, Zhou, and Ng (2010) explore different levels of coupling of syntax and semantics, and find that only explicit interleaving or explicit feature selection yield improvements in performance. Instead of synchronizing individual steps, we (1) decompose both the syntactic derivation and the semantic derivation into subsequences, where each subsequence corresponds to a single word in the sentence, and t"
J13-4006,E12-1003,1,0.526699,"subsequently. But defining a joint syntactic–semantic derivation presents a challenge. Namely, given the complex nature of correspondences between the structures, it is not obvious how to synchronize individual semantic– syntactic steps in the derivation. Previous joint statistical models of dependency syntax and SRL have either ignored semantic arcs not corresponding to single syntactic arcs (Thompson, Levy, and Manning 2003; Titov and Klementiev 2011) or resorted to pre-/post-processing strategies that modify semantic or syntactic structures (Llu´ıs and M`arquez 2008; Lang and Lapata 2011; Titov and Klementiev 2012). In a constituency setting, Li, Zhou, and Ng (2010) explore different levels of coupling of syntax and semantics, and find that only explicit interleaving or explicit feature selection yield improvements in performance. Instead of synchronizing individual steps, we (1) decompose both the syntactic derivation and the semantic derivation into subsequences, where each subsequence corresponds to a single word in the sentence, and then (2) synchronize syntactic and semantic subsequences corresponding to the same word with each other. To decide which steps correspond to a given word, we use a simpl"
J13-4006,J08-2002,0,0.369439,"Missing"
J13-4006,D09-1088,0,0.0527065,"Missing"
J13-4006,N09-2032,1,0.888657,"Missing"
J13-4006,P11-2052,1,0.891556,"Missing"
J13-4006,N06-1056,0,0.0444389,"obvious 988 Henderson et al. Joint Syntactic and Semantic Parsing correlation. Li, Zhou, and Ng’s approach is also different from ours in that they do not attempt to induce common representations useful for both tasks or for many languages, and as such cannot be regarded as multi-task, nor as multilingual, learning. Synchronous grammars provide an elegant way to handle multiple levels of representation. They have received much attention because of their applications in syntaxbased statistical machine translation (Galley et al. 2004; Chiang 2005; Nesson and Shieber 2008) and semantic parsing (Wong and Mooney 2006, 2007). Results indicate that these techniques are among the best both in machine translation and in the database query domain. Our method differs from those techniques that use a synchronous grammar, because we do not rewrite pairs of synchronized non-terminals, but instead synchronize chunks of derivation sequences. This difference is in part motivated by the fact that the strings for our two structures are perfectly aligned (being the same string), so synchronizing on the chunks of derivations associated with individual words eliminates any further alignment issues. We have also proposed n"
J13-4006,P07-1121,0,0.099988,"emplification of its applicability to other problems where two independent, but related, representations are being learned. 1. Introduction Success in statistical syntactic parsing based on supervised techniques trained on a large corpus of syntactic trees—both constituency-based (Collins 1999; Charniak 2000; Henderson 2003) and dependency-based (McDonald 2006; Nivre 2006; Bohnet and Nivre 2012; Hatori et al. 2012)—has paved the way to applying statistical approaches to the more ambitious goals of recovering semantic representations, such as the logical form of a sentence (Ge and Mooney 2005; Wong and Mooney 2007; Zettlemoyer and Collins 2007; Ge and Mooney 2009; Kwiatkowski et al. 2011) or learning the propositional argument-structure of its main predicates (Miller et al. 2000; Gildea and Jurafsky 2002; Carreras and M`arquez 2005; M`arquez et al. 2008; Li, Zhou, and Ng 2010). Moving towards a semantic level of representation of language and text has many potential applications in question answering and information extraction (Surdeanu et al. 2003; Moschitti et al. 2007), and has recently been argued to be useful in machine translation and its evaluation (Wu and Fung 2009; Liu and Gildea 2010; Lo and"
J13-4006,J97-3002,0,0.0302763,"bsequence forms a linguistically meaningful chunk in that it includes all the decisions about the arcs on the left side of the associated word, both its parents and its children. Thus, synchronizing the syntactic and semantic subsequences according to their associated word places together subsequences that are likely to be correlated. Note that such pairs of syntactic and semantic subsequences will, in general, have different numbers of steps on each side and these numbers of steps are, in general, unbounded. Therefore, instead of defining atomic synchronized rules as in synchronous grammars (Wu 1997; Chiang 2005), we resort to parametrized models that exploit the internal structure of the paired subsequences. This derivational, joint approach to handling these complex representations leads to a new proposal on how to learn them, which avoids extensive and complex feature engineering, as discussed in the following. 1.2 Joint Learning of Syntax and Semantics Our probabilistic model is learned using Incremental Sigmoid Belief Networks (ISBNs) (Henderson and Titov 2010), a recent development of an early latent variable model 952 Henderson et al. Joint Syntactic and Semantic Parsing for synta"
J13-4006,N09-2004,0,0.0248142,"Missing"
J13-4006,C00-2137,0,0.0196533,"ntic measures in the table). For the CoNLL-2008 scores the predicate sense labeling includes predicate identification, but for the CoNLL-2009 scores predicate identification was given in the task input. The syntactic LAS and the semantic F1 are then averaged with equal weight to produce an overall score called Macro F1 .10 When we evaluate the impact of the Swap action on crossing arcs, we also calculate precision, recall, and F-measure on pairs of crossing arcs.11 In our experiments, the statistical significance levels we report are all computed using a stratified shuffling test (Cohen 1995; Yeh 2000) with 10,000 randomized trials. 6.1 Monolingual Experimental Set-up We start by describing the monolingual English experiments. We train and evaluate our English models on data provided for the CoNLL-2008 shared task on joint learning of syntactic and semantic dependencies. The data is derived by merging a dependency transformation of the Penn Treebank with PropBank and NomBank (Surdeanu et al. 2008). An illustrative example of the kind of labeled structures that we need to parse is given in Figure 3. Training, development, and test data follow the usual partition as sections 02–21, 24, and 23"
J13-4006,D07-1071,0,0.0291049,"pplicability to other problems where two independent, but related, representations are being learned. 1. Introduction Success in statistical syntactic parsing based on supervised techniques trained on a large corpus of syntactic trees—both constituency-based (Collins 1999; Charniak 2000; Henderson 2003) and dependency-based (McDonald 2006; Nivre 2006; Bohnet and Nivre 2012; Hatori et al. 2012)—has paved the way to applying statistical approaches to the more ambitious goals of recovering semantic representations, such as the logical form of a sentence (Ge and Mooney 2005; Wong and Mooney 2007; Zettlemoyer and Collins 2007; Ge and Mooney 2009; Kwiatkowski et al. 2011) or learning the propositional argument-structure of its main predicates (Miller et al. 2000; Gildea and Jurafsky 2002; Carreras and M`arquez 2005; M`arquez et al. 2008; Li, Zhou, and Ng 2010). Moving towards a semantic level of representation of language and text has many potential applications in question answering and information extraction (Surdeanu et al. 2003; Moschitti et al. 2007), and has recently been argued to be useful in machine translation and its evaluation (Wu and Fung 2009; Liu and Gildea 2010; Lo and Wu 2011; Wu et al. 2011), dial"
J13-4006,W08-2127,0,0.19821,"Missing"
J13-4006,W04-2407,0,\N,Missing
J13-4006,J03-4003,0,\N,Missing
J13-4006,C98-1013,0,\N,Missing
J13-4006,P05-1001,0,\N,Missing
J13-4006,D07-1096,0,\N,Missing
J13-4006,W06-2303,1,\N,Missing
J95-4003,P81-1022,0,0.135261,"Missing"
J95-4003,P90-1014,0,0.0688042,"Missing"
J95-4003,C92-2095,0,0.163743,"ICMH, nor are proposals in the spirit of licensing grammars (Abney 1989, Frank 1992), where information is e n c o d e d in each lexical item. Second, the I C M H predicts that long-distance dependencies, represented as chains, are c o m p u t e d in steps. E m p t y categories are licensed in two computational steps: structural licensing b y an appropriate head, and feature instantiation. With respect to feature instantiation in particular, it is predicted that precompiling syntactic features speeds up the parsing process. This is different from functional approaches such as Fong (1991), and Fong and Berwick (1992), in which there is no precompilation. 6 These predictions seem to be s u p p o r t e d (and, consequently, so is the ICMH) b y two main results, which are illustrated below: m . separating X from lexical information yields more compact data structures; I propose a parser that uses two compiled tables: one that encodes structural information, and the other that encodes lexical information. . using syntactic features to c o m p u t e e m p t y categories reduces the search space, complex chains can be c o m p u t e d efficiently. These claims are s u p p o r t e d in the next section, where I d"
J95-4003,P91-1014,0,0.0141122,"oot 1990, and Stabler 1994). For my purposes, note that, if anything, I am dealing with the worst case for the parser. 8 The ICMH is not sufficient to predict a specific parsing architecture, but rather it loosely dictates the organization of the parser. The choice of an LR parser then is the result of the ICMH (with which the parser&apos;s organization must be compatible) and additional independent factors. First, LR parsers have the valid prefix property, namely they recognize that a string is not in the language as soon as possible (other parsing methods have this property as well, for instance Schabes 1991). A parser with this property is incremental, in the sense that it does not perform unnecessary work, and it fails as soon as an error occurs. Second, the stack of an LR parser encodes the notion of c-command implicitly. This is 522 Paola Merlo Modularity and Information Content Classes Input I Chains [ I  / LR Parsing ~constr ) Program Stack o-occurrence Table li I il I, II  I 1/1&apos;1 I&apos;i I I I&apos;I , 1, &quot;1 1 1&apos;iLi, I,I I&apos;1-1 LR Table ,,,. - I I I [ !q/I I I I I II II~&apos;NI I il I I I I I ]~- I i I Ill IP ii kll I I I I iI i R I I I IIILI j I I I I Figure 2 Organization of the Parser: The data str"
K15-1025,W06-2920,0,0.512127,"also formulated as a Principle of End Weight, where phrases are presented in order of increasing weight (Wasow, 2002). Cases of heavy NP-shift (Stallings et al., 1998), dative alternation (Bresnan et al., 2007) and other alternation preferences among verbal dependents are traditionally evoked to argue in favour of the “heaviness” effect. In this work, we study the alternations in the noun-phrase domain, much less investigated in 1 We use the following languages and treebanks: English, Czech, Spanish, Chinese, Catalan, German, Italian (Hajiˇc et al., 2009), Danish, Dutch, Portuguese, Swedish (Buchholz and Marsi, 2006), Latin, Ancient Greek (Haug and Jøhndal, 2008), Hungarian (Csendes et al., 2005), Polish (Woli´nski et al., 2011), Arabic (Zeman et al., 2012), French (McDonald et al., 2013). The extraction is based on the conversion to the universal part-of-speech tags (Petrov et al., 2012). 247 Proceedings of the 19th Conference on Computational Language Learning, pages 247–257, c Beijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics in the verbal domain and at the sentence level, but has not yet been investigated in the more limited nominal domain, where dependencies are usually"
K15-1025,P13-2017,0,0.053458,"Missing"
K15-1025,petrov-etal-2012-universal,0,0.0497722,"nally evoked to argue in favour of the “heaviness” effect. In this work, we study the alternations in the noun-phrase domain, much less investigated in 1 We use the following languages and treebanks: English, Czech, Spanish, Chinese, Catalan, German, Italian (Hajiˇc et al., 2009), Danish, Dutch, Portuguese, Swedish (Buchholz and Marsi, 2006), Latin, Ancient Greek (Haug and Jøhndal, 2008), Hungarian (Csendes et al., 2005), Polish (Woli´nski et al., 2011), Arabic (Zeman et al., 2012), French (McDonald et al., 2013). The extraction is based on the conversion to the universal part-of-speech tags (Petrov et al., 2012). 247 Proceedings of the 19th Conference on Computational Language Learning, pages 247–257, c Beijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics in the verbal domain and at the sentence level, but has not yet been investigated in the more limited nominal domain, where dependencies are usually shorter and might create lighter processing loads that do not need to be minimised. In applying the general principle of DLM to the dependency structure of noun phrases, our goal is to test to what extent the DLM principle predicts the observed adjective-noun word order alter"
K15-1025,P07-1024,0,0.476011,"ifferent corpora. From a typological perspective, the formulation needs to be refined from a preference of end weight to a preference for all elements being closer to the governing head: languages with Verb-Object dominant order tend to put constituents in ‘short before long’ order, while Object-Verb languages, like Japanese or Korean, do the reverse (Hawkins, 1994; Wasow, 2002). A more general explanation for the weight effect has been sought in a general tendency to minimise the length of the dependency between two related words, called Dependency Length Minimisation (DLM, Temperley (2007), Gildea and Temperley (2007)). In this paper, we look at the structural factors, such as DLM, and lexical factors that play a role in adjective-noun word order alternations in Romance languages and the predictions they make on prenominal or postnominal placement of adjectives. We concentrate on a smaller set of languages than those shown in Figure 1 to be able to study finer-grained effects than what can be observed at a very large scale and across many different corpus annotation schemes. We choose Romance languages because they show a good amount of variation in the word order of the noun phrase. The DLM principle can"
K15-1025,P15-2078,1,0.874557,"Missing"
K15-1025,zeman-etal-2012-hamledt,0,0.027957,"Missing"
K17-3024,N03-1014,1,0.73595,"Missing"
K17-3024,W07-2218,1,0.840147,"roductory overview (Zeman et al., 2017). This task makes true cross-linguistic comparison possible thanks to the universal dependency annotation project, which underlies the data used in this shared task. We train exactly the same parsing model on every language, thereby allowing further comparisons. In addition, the feature induction abilities of the recurrent neural network help minimise any remaining cross-lingual differences due to pre-processing or annotation. Introduction The system described in this paper is the grandchild of the first transition-based neural network dependency parser (Titov and Henderson, 2007b), which was the University of Geneva’s entry in the CoNLL 2007 multilingual dependency parsing shared task (Titov and Henderson, 2007a). The system has undergone some developments and modifications, in particular the faster discriminative version introduced by Yazdani and Henderson (2015), but in many respects the design and implementation of this parser is unchanged since 2007. One of our motivations for our submission to this CoNLL 2017 multilingual dependency parsing shared task is to provide a baseline to evaluate to what extent recent advances in neural network models and training do in"
K17-3024,K15-1015,1,0.902265,"comparisons. In addition, the feature induction abilities of the recurrent neural network help minimise any remaining cross-lingual differences due to pre-processing or annotation. Introduction The system described in this paper is the grandchild of the first transition-based neural network dependency parser (Titov and Henderson, 2007b), which was the University of Geneva’s entry in the CoNLL 2007 multilingual dependency parsing shared task (Titov and Henderson, 2007a). The system has undergone some developments and modifications, in particular the faster discriminative version introduced by Yazdani and Henderson (2015), but in many respects the design and implementation of this parser is unchanged since 2007. One of our motivations for our submission to this CoNLL 2017 multilingual dependency parsing shared task is to provide a baseline to evaluate to what extent recent advances in neural network models and training do in fact improve performance over “traditional” recurrent neural networks. We are listed in the table of results as the CLCL (Geneva) entry. As with previous work using the Incremental Neural Network architecture (e.g. Henderson, 2003), the main philosophy of our submis2 Data We use only the p"
K17-3024,P81-1022,0,0.728836,"Missing"
K17-3024,K17-3001,0,0.0586965,"Missing"
K17-3024,L16-1680,0,0.104119,"Missing"
K17-3024,D07-1099,1,0.763831,"roductory overview (Zeman et al., 2017). This task makes true cross-linguistic comparison possible thanks to the universal dependency annotation project, which underlies the data used in this shared task. We train exactly the same parsing model on every language, thereby allowing further comparisons. In addition, the feature induction abilities of the recurrent neural network help minimise any remaining cross-lingual differences due to pre-processing or annotation. Introduction The system described in this paper is the grandchild of the first transition-based neural network dependency parser (Titov and Henderson, 2007b), which was the University of Geneva’s entry in the CoNLL 2007 multilingual dependency parsing shared task (Titov and Henderson, 2007a). The system has undergone some developments and modifications, in particular the faster discriminative version introduced by Yazdani and Henderson (2015), but in many respects the design and implementation of this parser is unchanged since 2007. One of our motivations for our submission to this CoNLL 2017 multilingual dependency parsing shared task is to provide a baseline to evaluate to what extent recent advances in neural network models and training do in"
K18-1038,2017.lilt-15.3,0,0.236963,"x e´ tudiants que l’orateur endort < e´ tudiants> s´erieusement depuis le d´ebut. ’Jules smiles to the students who the speaker is putting seriously to sleep from the beginning.’ Introduction Despite their practical success and impressive performances, neural-network-based and distributed semantics techniques have often been criticized as they remain fundamentally opaque and difficult to interpret. To cast light on what linguistic information is learnt and encoded in these representations, several pieces of work have recently studied core properties of language in syntax (Linzen et al., 2016; Bernardy and Lappin, 2017; Gulordava et al., 2018; Linzen and Leonard, 2018; van Schijndel and Linzen, 2018), semantics (Herbelot and Ganesalingam, 2013; Erk, 2016), morphology (Cotterell and Sch¨utze, 2015). In a similar vein, we study another core, defining property of human languages: the property of long-distance dependencies. Long-distance dependencies are not all equally acceptable. The precise description of the facts involving long-distance dependencies is complex, and is one of the major topics of research in current linguistic theory, with many competing proposals 1 To clarify the perhaps confusing terminolo"
K18-1038,N18-1108,0,0.195458,"endort < e´ tudiants> s´erieusement depuis le d´ebut. ’Jules smiles to the students who the speaker is putting seriously to sleep from the beginning.’ Introduction Despite their practical success and impressive performances, neural-network-based and distributed semantics techniques have often been criticized as they remain fundamentally opaque and difficult to interpret. To cast light on what linguistic information is learnt and encoded in these representations, several pieces of work have recently studied core properties of language in syntax (Linzen et al., 2016; Bernardy and Lappin, 2017; Gulordava et al., 2018; Linzen and Leonard, 2018; van Schijndel and Linzen, 2018), semantics (Herbelot and Ganesalingam, 2013; Erk, 2016), morphology (Cotterell and Sch¨utze, 2015). In a similar vein, we study another core, defining property of human languages: the property of long-distance dependencies. Long-distance dependencies are not all equally acceptable. The precise description of the facts involving long-distance dependencies is complex, and is one of the major topics of research in current linguistic theory, with many competing proposals 1 To clarify the perhaps confusing terminology: the term long-distan"
K18-1038,Q16-1025,1,0.862548,"tual and lexical similarity, they do not however encode the notion of similarity that has been shown in many human experiments to be at work and to be definitional in long-distance dependencies. They do not encode therefore this core notion of intervention similarity. 6 evidence. Gulordava et al. (2018) revisit previous work, and extend the work on long-distance agreement to four languages of different linguistic properties (Italian, English, Hebrew, Russian). They use the technique of developing counterfactual data, typical of theoretical and experimental work and already used for parsing in Gulordava and Merlo (2016) and train the system on nonsensical sentences. Their model makes accurate predictions and compares well with humans, thereby suggesting that the networks learn deeper grammatical competence. On the linguistic and psycholinguistic side, this work contributes to the investigation of the formal encoding of long-distance dependencies, following the theoretical lines laid in the first formulation of intervention theory of long-distance dependencies (Rizzi, 1990), made gradual and more finegrained in subsequent work (Rizzi, 2004), and verified experimentally in both sentence processing and acquisit"
K18-1038,N15-1140,0,0.0575611,"Missing"
K18-1038,P16-1193,0,0.103154,"ular, we focus on lexical restriction, number and animacy in the definition of intervention similarity. Sophisticated definition of lexical proximity in feature spaces, called word embeddings, have been defined recently in computational linguistics. These embeddings are the vectorial representation of the meaning of a word, defined as the usage of a word in its context (Wittgenstein, 1953 [2001]; Harris, 1954; Firth, 1957). Tasks that confirm this interpretation are association, analogy, lexical similarity, entailment (Mikolov et al., 2013a,b; Pennington et al., 2014; Bojanowski et al., 2016; Henderson and Popa, 2016). We can, therefore, investigate whether the similarity spaces defined by word embeddings capture the notion of intervention similarity at work in long-distance dependencies. If they do, this means that they encode this core linguistic notion; if they don’t this means that word embeddings semantic spaces capture association-based similarities based on world knowledge and textual cooccurrence, but not this more syntax-internal notion of intervention similarity. 3 ments derived by intuitive or experimental acceptability judgments. If word embeddings encode the linguistic properties that explain"
K18-1038,P13-2078,0,0.016699,"peaker is putting seriously to sleep from the beginning.’ Introduction Despite their practical success and impressive performances, neural-network-based and distributed semantics techniques have often been criticized as they remain fundamentally opaque and difficult to interpret. To cast light on what linguistic information is learnt and encoded in these representations, several pieces of work have recently studied core properties of language in syntax (Linzen et al., 2016; Bernardy and Lappin, 2017; Gulordava et al., 2018; Linzen and Leonard, 2018; van Schijndel and Linzen, 2018), semantics (Herbelot and Ganesalingam, 2013; Erk, 2016), morphology (Cotterell and Sch¨utze, 2015). In a similar vein, we study another core, defining property of human languages: the property of long-distance dependencies. Long-distance dependencies are not all equally acceptable. The precise description of the facts involving long-distance dependencies is complex, and is one of the major topics of research in current linguistic theory, with many competing proposals 1 To clarify the perhaps confusing terminology: the term long-distance dependencies is a technical term that refers to discontinuous constructions where two elements in th"
K18-1038,Q16-1037,0,0.406454,"(3a) Jules sourit aux e´ tudiants que l’orateur endort < e´ tudiants> s´erieusement depuis le d´ebut. ’Jules smiles to the students who the speaker is putting seriously to sleep from the beginning.’ Introduction Despite their practical success and impressive performances, neural-network-based and distributed semantics techniques have often been criticized as they remain fundamentally opaque and difficult to interpret. To cast light on what linguistic information is learnt and encoded in these representations, several pieces of work have recently studied core properties of language in syntax (Linzen et al., 2016; Bernardy and Lappin, 2017; Gulordava et al., 2018; Linzen and Leonard, 2018; van Schijndel and Linzen, 2018), semantics (Herbelot and Ganesalingam, 2013; Erk, 2016), morphology (Cotterell and Sch¨utze, 2015). In a similar vein, we study another core, defining property of human languages: the property of long-distance dependencies. Long-distance dependencies are not all equally acceptable. The precise description of the facts involving long-distance dependencies is complex, and is one of the major topics of research in current linguistic theory, with many competing proposals 1 To clarify the"
K18-1038,W15-2125,1,0.854834,"ce dependencies are not all equally acceptable. The precise description of the facts involving long-distance dependencies is complex, and is one of the major topics of research in current linguistic theory, with many competing proposals 1 To clarify the perhaps confusing terminology: the term long-distance dependencies is a technical term that refers to discontinuous constructions where two elements in the string receive the same interpretation. Long-distance dependency constructions are wh-questions, relative clauses, right-node raising, among others (Rimell et al., 2009; Nivre et al., 2010; Merlo, 2015). Not all long-distance are actually long, for example subject-oriented relative clauses, and not all long dependencies are long-distance dependencies, for example, long subject-verb agreement as studied in Linzen et al. (2016); Bernardy and Lappin (2017); Gulordava et al. (2018) is usually not considered a long-distance dependency. 2 The unpronounced element(s) in the long-distance relation are indicated by < >. 392 Proceedings of the 22nd Conference on Computational Natural Language Learning (CoNLL 2018), pages 392–401 c Brussels, Belgium, October 31 - November 1, 2018. 2018 Association for"
K18-1038,C10-1094,0,0.065681,"Missing"
K18-1038,D14-1162,0,0.0844455,"course-oriented features, such as +Top.) In particular, we focus on lexical restriction, number and animacy in the definition of intervention similarity. Sophisticated definition of lexical proximity in feature spaces, called word embeddings, have been defined recently in computational linguistics. These embeddings are the vectorial representation of the meaning of a word, defined as the usage of a word in its context (Wittgenstein, 1953 [2001]; Harris, 1954; Firth, 1957). Tasks that confirm this interpretation are association, analogy, lexical similarity, entailment (Mikolov et al., 2013a,b; Pennington et al., 2014; Bojanowski et al., 2016; Henderson and Popa, 2016). We can, therefore, investigate whether the similarity spaces defined by word embeddings capture the notion of intervention similarity at work in long-distance dependencies. If they do, this means that they encode this core linguistic notion; if they don’t this means that word embeddings semantic spaces capture association-based similarities based on world knowledge and textual cooccurrence, but not this more syntax-internal notion of intervention similarity. 3 ments derived by intuitive or experimental acceptability judgments. If word embed"
K18-1038,D09-1085,0,0.0183611,"f long-distance dependencies. Long-distance dependencies are not all equally acceptable. The precise description of the facts involving long-distance dependencies is complex, and is one of the major topics of research in current linguistic theory, with many competing proposals 1 To clarify the perhaps confusing terminology: the term long-distance dependencies is a technical term that refers to discontinuous constructions where two elements in the string receive the same interpretation. Long-distance dependency constructions are wh-questions, relative clauses, right-node raising, among others (Rimell et al., 2009; Nivre et al., 2010; Merlo, 2015). Not all long-distance are actually long, for example subject-oriented relative clauses, and not all long dependencies are long-distance dependencies, for example, long subject-verb agreement as studied in Linzen et al. (2016); Bernardy and Lappin (2017); Gulordava et al. (2018) is usually not considered a long-distance dependency. 2 The unpronounced element(s) in the long-distance relation are indicated by < >. 392 Proceedings of the 22nd Conference on Computational Natural Language Learning (CoNLL 2018), pages 392–401 c Brussels, Belgium, October 31 - Novem"
K19-1011,P17-1042,0,0.0233123,"Skip tries to learn not only target word representations from source words but also source word representations from target words. Vulic and Moens (2016) induces bilingual word embeddings from document-aligned comparable data that have been merged and shuffled producing a pseudo-bilingual document. Some recent work aiming at reducing resources has shown competitive cross-lingual mappings across similar languages, using a pseudodictionary, such as identical character strings between two languages (Smith et al., 2017), or a simple list of numerals, thanks to a self-learning iterative framework (Artetxe et al., 2017). Furthermore, as indicated in section 4, Artetxe et al. (2018) extend their self-learning framework to unsupervised models, and build the state-of-the-art for bilingual lexicon induction. Another weaklysupervised model is proposed by Wang et al. (2019), a weakly-supervised concept-based adversarial method, used in our experiments, as also indicated in section 4. Several computational models of human bilingualism exist, see Li (2013) for an overview. More relatedly to the current work that uses distributional approaches, aspects of the bilingual lexicon have been proposed for word associations"
K19-1011,P18-1073,0,0.320457,"at languages use similar words to express similar concepts (Søgaard et al., 2015). The adversarial training uses concepts, drawn from Wikipedia, rather than words, to learn competitive cross-lingual word embeddings. The alignments are learnt by a generative adversarial networks (GAN) adapted to the cross-lingual mapping objective. Experiments 1 and 2 We test our hypotheses using two different crosslingual word embeddings models. (The numbering of the experiments corresponds to the numbering of the hypotheses.) One model is V ECMAP, a word-level crosslingual word embedding method, developed by Artetxe et al. (2018), which offers different op113 FALSE FRIENDS R EAL TRANSLATIONS arrange arrangiare arrange disporre arrange sistemare arrange organizzare attend attendere attend frequentare attend assistere bald baldo bald calvo bald pelato brave bravo brave coraggioso brave valoroso canteen cantina canteen mensa canteen borraccia T RUE FRIENDS family famiglia fantastic fantastico future futuro general generale generation generazione guide guida historial storica industry industria local locale melody melodia minor minore N ORMAL TRANSLATIONS jam marmellata january gennaio journey viaggio keep tenere kind tip"
K19-1011,P14-2131,0,0.0373031,"ined 7 Related work The related work for the investigation reported here comprises work on the human bilingual lexicon, cross-lingual word embeddings models and computational models of the bilingual lexicon. As the relevant work on the first topic has already been discussed, we concentrate here on the latter two. Vectors of words that are semantically or syntactically similar have been shown to be close to each other in the same space (Mikolov et al., 2013a,c; Pennington et al., 2014), making them widely useful in many natural language processing tasks such as machine translation and parsing (Bansal et al., 2014; Mi et al., 2016), both in a single language and across different languages. Mikolov et al. (2013b) first observed that the geometric positions of similar words in different languages were related by a linear relation. Zou et al. (2013) showed that a cross-lingually shared word embedding space is more useful than a monolingual space in an end-to-end machine translation task. However, traditional methods for mapping two monolingual word embeddings require high quality aligned sentences or dictionaries (Faruqui and Dyer, 2014; Ammar et al., 2016). Reducing the need for parallel data, then, has"
K19-1011,Q17-1010,0,0.0425614,"https://www.statmt.org/europarl/ 7 https://dumps.wikimedia.org/ 8 http://linguatools.org/tools/corpora/wikipediacomparable-corpora/ 9 https://dictionary.cambridge.org/dictionary/englishitalian/ 6 5.1 Data Word embedding data In this set of experiments, we use the same data from the previous two experiments (the dataset described in Dinu 114 Judge 1 Judge 2 FF TF RT NT UN Total FF 93 1 0 0 0 94 TF 3 127 0 0 0 130 RT 0 2 130 8 1 141 NT 1 3 7 133 0 144 UN 0 0 0 0 97 97 Total 97 133 137 141 98 606 et al. (2015)) with the addition of the FastText pretrained word embeddings for English and Italian (Bojanowski et al., 2017).10 These publicly available vectors are obtained by a 5-word window, for 300 resulting dimensions, on CommonCrawl and Wikipedia data using the Skip-gram model. Every word is represented as an n-grams of characters, for n training between 3 and 6. Each n-gram is represented by a vector and the sum of these vectors forms the vector representing the given word. So we have three cross-lingual models: two versions of V ECMAP, one trained on CBOW (wordlevel) and the other on FastText (character-level), and the concept-based adversarial model M 2 VEC, which also uses character-based FastText represe"
K19-1011,Q16-1037,0,0.082381,"Missing"
K19-1011,W15-1521,0,0.0204358,"naries (Faruqui and Dyer, 2014; Ammar et al., 2016). Reducing the need for parallel data, then, has become the main issue for cross-lingual word embedding mapping. Methods that rely on sentencealignments and also document-alignments have been proposed. Hermann and Blunsom (2014) present a method that, given enough data, train bilingual word embeddings from a sentence117 We find that predictions about cross-lingual word embeddings are mostly confirmed, making them promising functional models of at least some aspects of the bilingual lexicon, despite their structural simplicity. aligned corpus. Luong et al. (2015) propose a model, BiSkip, that takes as input a parallel corpus with both sentence and word-level alignment. Unlike other methods, BiSkip tries to learn not only target word representations from source words but also source word representations from target words. Vulic and Moens (2016) induces bilingual word embeddings from document-aligned comparable data that have been merged and shuffled producing a pseudo-bilingual document. Some recent work aiming at reducing resources has shown competitive cross-lingual mappings across similar languages, using a pseudodictionary, such as identical charac"
K19-1011,W18-0106,0,0.0218786,"Furthermore, as indicated in section 4, Artetxe et al. (2018) extend their self-learning framework to unsupervised models, and build the state-of-the-art for bilingual lexicon induction. Another weaklysupervised model is proposed by Wang et al. (2019), a weakly-supervised concept-based adversarial method, used in our experiments, as also indicated in section 4. Several computational models of human bilingualism exist, see Li (2013) for an overview. More relatedly to the current work that uses distributional approaches, aspects of the bilingual lexicon have been proposed for word associations (Matusevych et al., 2018). These associations are different in bilingual and monolingual speakers. For example, cognates, collocations and phonological responses are produced more frequently by nonnative speakers. This work proposes a model of word association in bilinguals, implemented as a semantic network paired with a retrieval mechanism. Computational models of the influence of the native language on second language learning have also been investigated in Matusevych (2016), specifically for argument structure. 8 9 Acknowledgments We thank Haozhou Wang for giving us access to his model M 2 VEC and to the embedding"
K19-1011,E14-1049,0,0.0292927,"tural language processing tasks such as machine translation and parsing (Bansal et al., 2014; Mi et al., 2016), both in a single language and across different languages. Mikolov et al. (2013b) first observed that the geometric positions of similar words in different languages were related by a linear relation. Zou et al. (2013) showed that a cross-lingually shared word embedding space is more useful than a monolingual space in an end-to-end machine translation task. However, traditional methods for mapping two monolingual word embeddings require high quality aligned sentences or dictionaries (Faruqui and Dyer, 2014; Ammar et al., 2016). Reducing the need for parallel data, then, has become the main issue for cross-lingual word embedding mapping. Methods that rely on sentencealignments and also document-alignments have been proposed. Hermann and Blunsom (2014) present a method that, given enough data, train bilingual word embeddings from a sentence117 We find that predictions about cross-lingual word embeddings are mostly confirmed, making them promising functional models of at least some aspects of the bilingual lexicon, despite their structural simplicity. aligned corpus. Luong et al. (2015) propose a"
K19-1011,D16-1096,0,0.0279599,"he related work for the investigation reported here comprises work on the human bilingual lexicon, cross-lingual word embeddings models and computational models of the bilingual lexicon. As the relevant work on the first topic has already been discussed, we concentrate here on the latter two. Vectors of words that are semantically or syntactically similar have been shown to be close to each other in the same space (Mikolov et al., 2013a,c; Pennington et al., 2014), making them widely useful in many natural language processing tasks such as machine translation and parsing (Bansal et al., 2014; Mi et al., 2016), both in a single language and across different languages. Mikolov et al. (2013b) first observed that the geometric positions of similar words in different languages were related by a linear relation. Zou et al. (2013) showed that a cross-lingually shared word embedding space is more useful than a monolingual space in an end-to-end machine translation task. However, traditional methods for mapping two monolingual word embeddings require high quality aligned sentences or dictionaries (Faruqui and Dyer, 2014; Ammar et al., 2016). Reducing the need for parallel data, then, has become the main is"
K19-1011,P14-1006,0,0.0239136,"ar words in different languages were related by a linear relation. Zou et al. (2013) showed that a cross-lingually shared word embedding space is more useful than a monolingual space in an end-to-end machine translation task. However, traditional methods for mapping two monolingual word embeddings require high quality aligned sentences or dictionaries (Faruqui and Dyer, 2014; Ammar et al., 2016). Reducing the need for parallel data, then, has become the main issue for cross-lingual word embedding mapping. Methods that rely on sentencealignments and also document-alignments have been proposed. Hermann and Blunsom (2014) present a method that, given enough data, train bilingual word embeddings from a sentence117 We find that predictions about cross-lingual word embeddings are mostly confirmed, making them promising functional models of at least some aspects of the bilingual lexicon, despite their structural simplicity. aligned corpus. Luong et al. (2015) propose a model, BiSkip, that takes as input a parallel corpus with both sentence and word-level alignment. Unlike other methods, BiSkip tries to learn not only target word representations from source words but also source word representations from target wor"
K19-1011,D14-1162,0,0.0900862,"xicon with precise underlying formal models. Because our predictions are confirmed, they also confirm that the similarity structure defined 7 Related work The related work for the investigation reported here comprises work on the human bilingual lexicon, cross-lingual word embeddings models and computational models of the bilingual lexicon. As the relevant work on the first topic has already been discussed, we concentrate here on the latter two. Vectors of words that are semantically or syntactically similar have been shown to be close to each other in the same space (Mikolov et al., 2013a,c; Pennington et al., 2014), making them widely useful in many natural language processing tasks such as machine translation and parsing (Bansal et al., 2014; Mi et al., 2016), both in a single language and across different languages. Mikolov et al. (2013b) first observed that the geometric positions of similar words in different languages were related by a linear relation. Zou et al. (2013) showed that a cross-lingually shared word embedding space is more useful than a monolingual space in an end-to-end machine translation task. However, traditional methods for mapping two monolingual word embeddings require high quali"
K19-1011,P15-1165,0,0.0287513,"Missing"
K19-1011,D19-1450,1,0.838003,"ture of the lexicon conceived as a multidimensional, integrated multilingual space. In particular, they inform us on the respective importance of formal and meaning properties of words in this cross-lingual similarity space. 4 grado voto Figure 2: Sample of translation pairs and sample of shared translation pairs used in experiments 1 and 2. H YPOTHESIS 6 Normal translation pairs have a higher similarity score than real translations of false friends. sim(w1n , w2n ) &gt; sim(w1 , w2 ) shared translation pairs legno bosco We also test M 2 VEC, a weakly-supervised, concept-based adversarial model (Wang et al., 2019). This method is based on the idea that languages use similar words to express similar concepts (Søgaard et al., 2015). The adversarial training uses concepts, drawn from Wikipedia, rather than words, to learn competitive cross-lingual word embeddings. The alignments are learnt by a generative adversarial networks (GAN) adapted to the cross-lingual mapping objective. Experiments 1 and 2 We test our hypotheses using two different crosslingual word embeddings models. (The numbering of the experiments corresponds to the numbering of the hypotheses.) One model is V ECMAP, a word-level crosslingual"
K19-1011,D13-1141,0,0.0387111,"st topic has already been discussed, we concentrate here on the latter two. Vectors of words that are semantically or syntactically similar have been shown to be close to each other in the same space (Mikolov et al., 2013a,c; Pennington et al., 2014), making them widely useful in many natural language processing tasks such as machine translation and parsing (Bansal et al., 2014; Mi et al., 2016), both in a single language and across different languages. Mikolov et al. (2013b) first observed that the geometric positions of similar words in different languages were related by a linear relation. Zou et al. (2013) showed that a cross-lingually shared word embedding space is more useful than a monolingual space in an end-to-end machine translation task. However, traditional methods for mapping two monolingual word embeddings require high quality aligned sentences or dictionaries (Faruqui and Dyer, 2014; Ammar et al., 2016). Reducing the need for parallel data, then, has become the main issue for cross-lingual word embedding mapping. Methods that rely on sentencealignments and also document-alignments have been proposed. Hermann and Blunsom (2014) present a method that, given enough data, train bilingual"
merlo-stevenson-2000-establishing,W98-1106,0,\N,Missing
merlo-stevenson-2000-establishing,J99-2004,0,\N,Missing
merlo-stevenson-2000-establishing,J98-3003,0,\N,Missing
merlo-stevenson-2000-establishing,C96-1055,0,\N,Missing
merlo-stevenson-2000-establishing,C92-4177,0,\N,Missing
merlo-stevenson-2000-establishing,E99-1007,1,\N,Missing
merlo-stevenson-2000-establishing,C00-2118,1,\N,Missing
merlo-stevenson-2000-establishing,A97-1052,0,\N,Missing
merlo-stevenson-2000-establishing,J93-2002,0,\N,Missing
merlo-stevenson-2000-establishing,P98-1046,0,\N,Missing
merlo-stevenson-2000-establishing,C98-1046,0,\N,Missing
merlo-stevenson-2000-establishing,P98-1112,0,\N,Missing
merlo-stevenson-2000-establishing,C98-1108,0,\N,Missing
merlo-stevenson-2000-establishing,P93-1032,0,\N,Missing
merlo-stevenson-2000-establishing,P98-2247,0,\N,Missing
merlo-stevenson-2000-establishing,C98-2242,0,\N,Missing
merlo-stevenson-2000-establishing,J96-2004,0,\N,Missing
merlo-stevenson-2000-establishing,P99-1015,0,\N,Missing
merlo-stevenson-2000-establishing,P99-1051,0,\N,Missing
merlo-stevenson-2000-establishing,W99-0632,0,\N,Missing
N06-2026,W05-0620,0,0.231741,"ures allow the SSN to generalise in several ways. All the constituents bearing an A0A5 and AA labels will have a common feature. The same will be true for all nodes bearing an AM-X label. Thus, the SSN can generalise across these two types of labels. Finally, all constituents that do not bear any label will now constitute a class, the class of the nodes for which these two features are false. 3 Experiments and Discussion Our extended semantic role SSN parser was trained on sections 2-21 and validated on section 24 from the PropBank. Testing data are section 23 from the CoNLL-2005 shared task (Carreras and Marquez, 2005). We perform two different evaluations on our model trained on PropBank data. We distinguish between two parsing tasks: the PropBank parsing task and the PTB parsing task. To evaluate the former parsing task, we compute the standard Parseval measures of labelled recall and precision of constituents, taking into account not only the 33 original labels, 103 but also the newly introduced PropBank labels. This evaluation gives us an indication of how accurately and exhaustively we can recover this richer set of non-terminal labels. The results, computed on the testing data set from the PropBank, a"
N06-2026,N06-1024,0,0.0460982,"Missing"
N06-2026,J02-3001,0,0.512851,"of syntactic trees (Collins, 1999; Charniak, 2000; Henderson, 2003) have brought the hope that the same approach could be applied to the more ambitious goal of recovering the propositional content and the frame semantics of a sentence. Moving towards a shallow semantic level of representation has immediate applications in question-answering and information extraction. For example, an automatic flight reservation system processing the sentence I want to book a flight from Geneva to New York will need to know that from Geneva indicates the origin of the flight and to New York the destination. (Gildea and Jurafsky, 2002) define this shallow semantic task as a classification problem where the semantic role to be assigned to each constituent is inferred on the basis of probability distributions of syntactic features extracted from parse trees. They use learning features such as phrase type, position, voice, and parse tree path. Consider, for example, a sentence such as The authority dropped at midnight Tuesday to $ 2.80 trillion (taken from section 00 of PropBank (Palmer et al., 2005)). The fact that to $ 2.80 trillion receives a direction semantic label The assumption that syntactic distributions will be predi"
N06-2026,P02-1031,0,0.0398642,"comparable to those achieved by the best semantic role labellers (PropBank column). This indicates that the model is robust, as it has been extended to a richer set of labels successfully, without increase in training data. In fact, the limited availability of data is increased further by the high variability of the argumental labels A0-A5 whose semantics is specific to a given verb or a given verb sense. Methodologically, these initial results on a joint solution to parsing and semantic role labelling provide the first direct test of whether parsing is necessary for semantic role labelling (Gildea and Palmer, 2002; Punyakanok et al., 2005a). Comparing semantic role labelling based on chunked input to the better semantic role labels retrieved based on parsed trees, (Gildea and Palmer, 2002) conclude that parsing is necessary. In an extensive experimental investigation of the different learning stages usually involved in semantic role labelling, (Punyakanok et al., 2005a) find instead that sophisticated chunking can achieve state-of-the-art results. Neither of these pieces of work actually used a parser to do SRL. Their investigation was therefore limited to establishing the usefulness of syntactic featu"
N06-2026,W05-0623,0,0.0532711,"output by state-of-the-art parsers (Collins, 1999; Charniak, 2000), both for training and testing, and return partial trees annotated with semantic role labels. An indirect way of comparing our parser with semantic role labellers suggests itself. 2 We merge the partial trees output by a semantic role labeller with the output of the parser on which it was trained, and compute PropBank parsing performance measures on the resulting parse trees. The third line, PropBank column of Table 1 reports such measures summarised for the five best semantic role labelling systems (Punyakanok et al., 2005b; Haghighi et al., 2005; Pradhan et al., 2005; Marquez et al., 2005; Surdeanu and Turmo, 2005) in the CoNLL 2005 shared task. These systems all use (Charniak, 2000)’s parse trees both for training and testing, as well as various other information sources including sets of n-best parse trees, chunks, or named entities. Thus, the partial trees output by these systems were merged with the parse trees returned by Charniak’s parser (second line, PropBank column).3 These results jointly confirm our initial hypothe1 (Shen and Joshi, 2005) use PropBank labels to extract LTAG spinal trees to train an incremental LTAG parser,"
N06-2026,N03-1014,0,0.485259,"e verb is in the active voice, and that the PP is in a certain tree configuration with the governing verb. All the recent systems proposed for semantic role labelling (SRL) follow this same assumption (CoNLL, 2005). We integrate PropBank semantic role labels to an existing statistical parsing model producing richer output. We show conclusive results on joint learning and inference of syntactic and semantic representations. 1 Introduction Recent successes in statistical syntactic parsing based on supervised techniques trained on a large corpus of syntactic trees (Collins, 1999; Charniak, 2000; Henderson, 2003) have brought the hope that the same approach could be applied to the more ambitious goal of recovering the propositional content and the frame semantics of a sentence. Moving towards a shallow semantic level of representation has immediate applications in question-answering and information extraction. For example, an automatic flight reservation system processing the sentence I want to book a flight from Geneva to New York will need to know that from Geneva indicates the origin of the flight and to New York the destination. (Gildea and Jurafsky, 2002) define this shallow semantic task as a cl"
N06-2026,P03-1054,0,0.0115727,"constituents. For more information on this technique to capture structural domains, see (Musillo and Merlo, 2005) where the technique was applied to function parsing. Given the hidden history representation h(d1 , · · · , di−1 ) of a derivation, a normalized exponential output function is computed by the SSNs to estimate a probability distribution over the possible next derivation moves di . To exploit the intuition that semantic role labels are predictive of syntactic structure, we must provide semantic role information as early as possible to the parser. Extending a technique presented in (Klein and Manning, 2003) and adopted in (Merlo and Musillo, 2005) for function labels with stateof-the-art results, we split some part-of-speech tags into tags marked with AM-X semantic role labels. As a result, 240 new POS tags were introduced to partition the original tag set which consisted of 45 tags. Our augmented model has a total of 613 nonterminals to represent both the PTB and PropBank labels, instead of the 33 of the original SSN parser. The 580 newly introduced labels consist of a standard PTB label followed by one or more PropBank semantic roles, such as PP-AM-TMP or NP-A0-A1. These augmented tags and the"
N06-2026,J93-2004,0,0.0274848,"arameters must be estimated. However, given the current limited availability of annotated treebanks, this more complex task will have to be solved with the same overall amount of data, aggravating the difficulty of estimating the model’s parameters due to sparse data. 2 The Data and the Extended Parser In this section we describe the augmentations to our base parsing models necessary to tackle the joint learning of parse tree and semantic role labels. PropBank encodes propositional information by adding a layer of argument structure annotation to the syntactic structures of the Penn Treebank (Marcus et al., 1993). Verbal predicates in the Penn Treebank (PTB) receive a label REL and their arguments are annotated with abstract semantic role labels A0A5 or AA for those complements of the predicative verb that are considered arguments while those complements of the verb labelled with a semantic functional label in the original PTB receive the com102 posite semantic role label AM-X, where X stands for labels such as LOC, TMP or ADV, for locative, temporal and adverbial modifiers respectively. PropBank uses two levels of granularity in its annotation, at least conceptually. Arguments receiving labels A0-A5"
N06-2026,W05-0628,0,0.181786,"1999; Charniak, 2000), both for training and testing, and return partial trees annotated with semantic role labels. An indirect way of comparing our parser with semantic role labellers suggests itself. 2 We merge the partial trees output by a semantic role labeller with the output of the parser on which it was trained, and compute PropBank parsing performance measures on the resulting parse trees. The third line, PropBank column of Table 1 reports such measures summarised for the five best semantic role labelling systems (Punyakanok et al., 2005b; Haghighi et al., 2005; Pradhan et al., 2005; Marquez et al., 2005; Surdeanu and Turmo, 2005) in the CoNLL 2005 shared task. These systems all use (Charniak, 2000)’s parse trees both for training and testing, as well as various other information sources including sets of n-best parse trees, chunks, or named entities. Thus, the partial trees output by these systems were merged with the parse trees returned by Charniak’s parser (second line, PropBank column).3 These results jointly confirm our initial hypothe1 (Shen and Joshi, 2005) use PropBank labels to extract LTAG spinal trees to train an incremental LTAG parser, but they do not parse PropBank. Their resul"
N06-2026,H05-1078,1,0.846144,"is technique to capture structural domains, see (Musillo and Merlo, 2005) where the technique was applied to function parsing. Given the hidden history representation h(d1 , · · · , di−1 ) of a derivation, a normalized exponential output function is computed by the SSNs to estimate a probability distribution over the possible next derivation moves di . To exploit the intuition that semantic role labels are predictive of syntactic structure, we must provide semantic role information as early as possible to the parser. Extending a technique presented in (Klein and Manning, 2003) and adopted in (Merlo and Musillo, 2005) for function labels with stateof-the-art results, we split some part-of-speech tags into tags marked with AM-X semantic role labels. As a result, 240 new POS tags were introduced to partition the original tag set which consisted of 45 tags. Our augmented model has a total of 613 nonterminals to represent both the PTB and PropBank labels, instead of the 33 of the original SSN parser. The 580 newly introduced labels consist of a standard PTB label followed by one or more PropBank semantic roles, such as PP-AM-TMP or NP-A0-A1. These augmented tags and the new non-terminals are included in the se"
N06-2026,W05-1509,1,0.855572,"ed for the move i − 1 is included in the inputs to the computation of the representation for the next move i, virtually any information about the derivation history could flow from history representation to history representation and be used to estimate the probability of a derivation move. In our experiments, the set D of earlier history representations is modified to yield a model that is sensitive to regularities in structurally defined sequences of nodes bearing semantic role labels, within and across constituents. For more information on this technique to capture structural domains, see (Musillo and Merlo, 2005) where the technique was applied to function parsing. Given the hidden history representation h(d1 , · · · , di−1 ) of a derivation, a normalized exponential output function is computed by the SSNs to estimate a probability distribution over the possible next derivation moves di . To exploit the intuition that semantic role labels are predictive of syntactic structure, we must provide semantic role information as early as possible to the parser. Extending a technique presented in (Klein and Manning, 2003) and adopted in (Merlo and Musillo, 2005) for function labels with stateof-the-art results"
N06-2026,J05-1004,0,0.218355,"Geneva to New York will need to know that from Geneva indicates the origin of the flight and to New York the destination. (Gildea and Jurafsky, 2002) define this shallow semantic task as a classification problem where the semantic role to be assigned to each constituent is inferred on the basis of probability distributions of syntactic features extracted from parse trees. They use learning features such as phrase type, position, voice, and parse tree path. Consider, for example, a sentence such as The authority dropped at midnight Tuesday to $ 2.80 trillion (taken from section 00 of PropBank (Palmer et al., 2005)). The fact that to $ 2.80 trillion receives a direction semantic label The assumption that syntactic distributions will be predictive of semantic role assignments is based on linking theory. Linking theory assumes the existence of a hierarchy of semantic roles which are mapped by default on a hierarchy of syntactic positions. It also shows that regular mappings from the semantic to the syntactic level can be posited even for those verbs whose arguments can take several syntactic positions, such as psychological verbs, locatives, or datives, requiring a more complex theory. (See (Hale and Keys"
N06-2026,W05-0634,0,0.0411385,"-art parsers (Collins, 1999; Charniak, 2000), both for training and testing, and return partial trees annotated with semantic role labels. An indirect way of comparing our parser with semantic role labellers suggests itself. 2 We merge the partial trees output by a semantic role labeller with the output of the parser on which it was trained, and compute PropBank parsing performance measures on the resulting parse trees. The third line, PropBank column of Table 1 reports such measures summarised for the five best semantic role labelling systems (Punyakanok et al., 2005b; Haghighi et al., 2005; Pradhan et al., 2005; Marquez et al., 2005; Surdeanu and Turmo, 2005) in the CoNLL 2005 shared task. These systems all use (Charniak, 2000)’s parse trees both for training and testing, as well as various other information sources including sets of n-best parse trees, chunks, or named entities. Thus, the partial trees output by these systems were merged with the parse trees returned by Charniak’s parser (second line, PropBank column).3 These results jointly confirm our initial hypothe1 (Shen and Joshi, 2005) use PropBank labels to extract LTAG spinal trees to train an incremental LTAG parser, but they do not parse"
N06-2026,W05-0639,0,0.307062,"NLL, 2005) use parse trees output by state-of-the-art parsers (Collins, 1999; Charniak, 2000), both for training and testing, and return partial trees annotated with semantic role labels. An indirect way of comparing our parser with semantic role labellers suggests itself. 2 We merge the partial trees output by a semantic role labeller with the output of the parser on which it was trained, and compute PropBank parsing performance measures on the resulting parse trees. The third line, PropBank column of Table 1 reports such measures summarised for the five best semantic role labelling systems (Punyakanok et al., 2005b; Haghighi et al., 2005; Pradhan et al., 2005; Marquez et al., 2005; Surdeanu and Turmo, 2005) in the CoNLL 2005 shared task. These systems all use (Charniak, 2000)’s parse trees both for training and testing, as well as various other information sources including sets of n-best parse trees, chunks, or named entities. Thus, the partial trees output by these systems were merged with the parse trees returned by Charniak’s parser (second line, PropBank column).3 These results jointly confirm our initial hypothe1 (Shen and Joshi, 2005) use PropBank labels to extract LTAG spinal trees to train an"
N06-2026,W05-0625,0,0.428437,"NLL, 2005) use parse trees output by state-of-the-art parsers (Collins, 1999; Charniak, 2000), both for training and testing, and return partial trees annotated with semantic role labels. An indirect way of comparing our parser with semantic role labellers suggests itself. 2 We merge the partial trees output by a semantic role labeller with the output of the parser on which it was trained, and compute PropBank parsing performance measures on the resulting parse trees. The third line, PropBank column of Table 1 reports such measures summarised for the five best semantic role labelling systems (Punyakanok et al., 2005b; Haghighi et al., 2005; Pradhan et al., 2005; Marquez et al., 2005; Surdeanu and Turmo, 2005) in the CoNLL 2005 shared task. These systems all use (Charniak, 2000)’s parse trees both for training and testing, as well as various other information sources including sets of n-best parse trees, chunks, or named entities. Thus, the partial trees output by these systems were merged with the parse trees returned by Charniak’s parser (second line, PropBank column).3 These results jointly confirm our initial hypothe1 (Shen and Joshi, 2005) use PropBank labels to extract LTAG spinal trees to train an"
N06-2026,W05-0635,0,0.0316489,", both for training and testing, and return partial trees annotated with semantic role labels. An indirect way of comparing our parser with semantic role labellers suggests itself. 2 We merge the partial trees output by a semantic role labeller with the output of the parser on which it was trained, and compute PropBank parsing performance measures on the resulting parse trees. The third line, PropBank column of Table 1 reports such measures summarised for the five best semantic role labelling systems (Punyakanok et al., 2005b; Haghighi et al., 2005; Pradhan et al., 2005; Marquez et al., 2005; Surdeanu and Turmo, 2005) in the CoNLL 2005 shared task. These systems all use (Charniak, 2000)’s parse trees both for training and testing, as well as various other information sources including sets of n-best parse trees, chunks, or named entities. Thus, the partial trees output by these systems were merged with the parse trees returned by Charniak’s parser (second line, PropBank column).3 These results jointly confirm our initial hypothe1 (Shen and Joshi, 2005) use PropBank labels to extract LTAG spinal trees to train an incremental LTAG parser, but they do not parse PropBank. Their results on the PTB are not direc"
N06-2026,A00-2018,0,\N,Missing
N06-2026,J03-4003,0,\N,Missing
N06-2026,H05-1102,0,\N,Missing
N09-2032,N01-1016,0,0.0396628,"NAACL HLT 2009: Short Papers, pages 125–128, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics ous work in domain adaptation by Foster (2007), where similar proportions of ungrammatical and grammatical data are combined to train a parser on ungrammatical written text, and by Weilhammer et al. (2006), who use interpolation between two separately trained models, one on an artificial corpus of user utterances generated by a hand-coded domain-specific grammar and one on available corpora. Whereas much previous work on parsing speech has focused on speech repairs, e.g. Charniak and Johnson (2001), we focus on parsing NSUs. 2 The first component: a model of NSUs To construct a model of NSUs we studied a subset of the data under consideration: TownInfo. This small corpus of transcribed spoken human-machine dialogues in the domain of hotel/restaurant/bar search is gathered using the TownInfo tourist information system (Lemon et al., 2006). The NSUs we find in our data are mainly of the type answers, according to the classification given in Fern´andez (2006). More specifically, we find short answers, plain and repeated affirmative answers, plain and helpful rejections, but also greetings."
N09-2032,N03-1014,1,0.87695,"ith and one without the target component.5 These models were tested on two test sets: a small corpus of 150 transcribed utterances taken from the TownInfo corpus, annotated with gold syntactic and semantic annotation by two of the authors6 : the TownInfo test set. The second test set is used to compare the performance of the parser on WSJ-style sentences and consists of section 23 of the merged Penn Treebank/Propbank corpus. We will refer to this test set as the non-augmented test set. 5.1 The parsing model is the one proposed in Merlo and Musillo (2008), which extends the syntactic parser of Henderson (2003) and Titov and Henderson (2007) with annotations which identify semantic role labels, and has competitive performance. The parser uses a generative history-based probability model for a binarised left-corner derivation. The probabilities of derivation decisions are modelled using the neural network approximation (Henderson, 2003) to a type of dynamic Bayesian Network called an Incremental Sigmoid Belief Network (ISBN) (Titov and Henderson, 2007). The ISBN models the derivation history with a vector of binary latent variables. These latent variables learn to represent features of the parse hist"
N09-2032,E06-2009,1,0.81171,"etween two separately trained models, one on an artificial corpus of user utterances generated by a hand-coded domain-specific grammar and one on available corpora. Whereas much previous work on parsing speech has focused on speech repairs, e.g. Charniak and Johnson (2001), we focus on parsing NSUs. 2 The first component: a model of NSUs To construct a model of NSUs we studied a subset of the data under consideration: TownInfo. This small corpus of transcribed spoken human-machine dialogues in the domain of hotel/restaurant/bar search is gathered using the TownInfo tourist information system (Lemon et al., 2006). The NSUs we find in our data are mainly of the type answers, according to the classification given in Fern´andez (2006). More specifically, we find short answers, plain and repeated affirmative answers, plain and helpful rejections, but also greetings. Current linguistic theory provides several approaches to dealing with NSUs (Merchant, 2004; Progovac et al., 2006; Fern´andez, 2006). Following the linguistic analysis of NSUs as non-sentential small clauses (Progovac et al., 2006) that do not have tense or agreement functional nodes, we make the assumption that they are phrasal projections. T"
N09-2032,J93-2004,0,0.0347848,"Missing"
N09-2032,W08-2101,1,0.855716,"ated NSUs except for VPs and modifiers. 5 augmented model: One with and one without the target component.5 These models were tested on two test sets: a small corpus of 150 transcribed utterances taken from the TownInfo corpus, annotated with gold syntactic and semantic annotation by two of the authors6 : the TownInfo test set. The second test set is used to compare the performance of the parser on WSJ-style sentences and consists of section 23 of the merged Penn Treebank/Propbank corpus. We will refer to this test set as the non-augmented test set. 5.1 The parsing model is the one proposed in Merlo and Musillo (2008), which extends the syntactic parser of Henderson (2003) and Titov and Henderson (2007) with annotations which identify semantic role labels, and has competitive performance. The parser uses a generative history-based probability model for a binarised left-corner derivation. The probabilities of derivation decisions are modelled using the neural network approximation (Henderson, 2003) to a type of dynamic Bayesian Network called an Incremental Sigmoid Belief Network (ISBN) (Titov and Henderson, 2007). The ISBN models the derivation history with a vector of binary latent variables. These latent"
N09-2032,J05-1004,0,0.0274982,"ems. 3 Because NSUs can be interpreted only in context, the same NSU can correspond to several syntactic categories: South for example, can be an noun, an adverb, or an adjective. In case of ambiguity, we divided the score up for the several possible tags. This accounts for the fractional counts. Category NP JJ PP NN VP # Occ. 19.0 12.7 12.0 11.7 11.0 Perc. 15.2 10.1 9.6 9.3 8.8 Category RB DT CD Total frag. Full sents # Occ. 1.7 1.0 1.0 70.0 55.0 Perc. 1.3 0.8 0.8 56.0 44.0 Table 1: Distribution of types of NSUs and full sentences in the TownInfo development set. merged with PropBank labels (Palmer et al., 2005). We included all the sentences from this dataset in our artificial corpus, giving us 39,832 full sentences. In accordance with the target distribution we added 50,699 NSUs extracted from the same dataset. We sampled NSUs according to the distribution given in Table 1. After the extraction we added a root FRAG node to the extracted NSUs4 and we capitalised the first letter of each NSU to form an utterance. There are two additional pre-processing steps. First, for some types of NSUs maximal projections are added. For example, in the subset from the target source we saw many occurrences of nouns"
N09-2032,P07-1080,1,0.883131,"he target component.5 These models were tested on two test sets: a small corpus of 150 transcribed utterances taken from the TownInfo corpus, annotated with gold syntactic and semantic annotation by two of the authors6 : the TownInfo test set. The second test set is used to compare the performance of the parser on WSJ-style sentences and consists of section 23 of the merged Penn Treebank/Propbank corpus. We will refer to this test set as the non-augmented test set. 5.1 The parsing model is the one proposed in Merlo and Musillo (2008), which extends the syntactic parser of Henderson (2003) and Titov and Henderson (2007) with annotations which identify semantic role labels, and has competitive performance. The parser uses a generative history-based probability model for a binarised left-corner derivation. The probabilities of derivation decisions are modelled using the neural network approximation (Henderson, 2003) to a type of dynamic Bayesian Network called an Incremental Sigmoid Belief Network (ISBN) (Titov and Henderson, 2007). The ISBN models the derivation history with a vector of binary latent variables. These latent variables learn to represent features of the parse history which are useful for making"
N09-2032,D07-1096,0,\N,Missing
P02-1027,W98-1106,0,\N,Missing
P02-1027,A00-2034,0,\N,Missing
P02-1027,C02-1146,1,\N,Missing
P02-1027,H01-1035,0,\N,Missing
P02-1027,C00-2108,0,\N,Missing
P02-1027,J94-4003,0,\N,Missing
P02-1027,J95-4004,0,\N,Missing
P02-1027,J00-4004,0,\N,Missing
P02-1027,J01-3003,1,\N,Missing
P08-2054,A00-2018,0,0.0192801,"Introduction Recent research in natural language parsing has extensively investigated probabilistic models of phrase-structure parse trees. As well as being the most commonly used probabilistic models of parse trees, probabilistic context-free grammars (PCFGs) are the best understood. As shown in (Klein and Manning, 2003), the ability of PCFG models to disambiguate phrases crucially depends on the expressiveness of the symbolic backbone they use. Treebank-specific heuristics have commonly been used both to alleviate inadequate independence assumptions stipulated by naive PCFGs (Collins, 1999; Charniak, 2000). Such methods stand in sharp contrast to partially supervised techniques that have recently been proposed to induce hidden grammatical representations that are finer-grained than those that can be read off the parsed sentences in treebanks (Henderson, 2003; Matsuzaki et al., 2005; Prescher, 2005; Petrov et al., 2006). ∗ Part of this work was done when Gabriele Musillo was visiting the MIT Computer Science and Artificial Intelligence Laboratory, funded by a grant from the Swiss NSF (PBGE2117146). Many thanks to Michael Collins and Xavier Carreras for their insightful comments on the work prese"
P08-2054,W05-1504,0,0.0238662,"tested on the commonly used sections from the Penn Treebank. Projective dependency trees, ob6 As observed in (Collins, 1999), an unambiguous verbal head such as prove bearing the VB tag may project a clause with an overt subject as well as a clause without an overt subject, but only the latter is a possible dependent of subject control verbs such as try. Development Data – section 24 FOM: q = 1, h = 1 SOM: q = 1, h = 1 FOM: q = 2, h = 2 FOM: q = 2, h = 4 SOM: q = 2, h = 2 SOM: q = 1, h = 4 per word 75.7 80.5 81.9 84.7 84.3 87.0 per sentence 9.9 16.2 17.4 22.0 21.5 25.8 Test Data – section 23 (Eisner and Smith, 2005) SOM: q = 1, h = 4 (McDonald, 2006) per word 75.6 88.0 91.5 per sentence NA 30.6 36.7 Table 1: Accuracy results on the development and test data set, where q denotes the number of hidden states and h the number of hidden values annotating a PoS tag involved in our first-order (FOM) and second-order (SOM) models. tained using the rules stated in (Yamada and Matsumoto, 2003), were transformed into first-order and second-order structures. CFGs extracted from such structures were then annotated with hidden variables encoding the constraints described in the previous section and trained until conve"
P08-2054,N03-1014,0,0.326079,"Gs) are the best understood. As shown in (Klein and Manning, 2003), the ability of PCFG models to disambiguate phrases crucially depends on the expressiveness of the symbolic backbone they use. Treebank-specific heuristics have commonly been used both to alleviate inadequate independence assumptions stipulated by naive PCFGs (Collins, 1999; Charniak, 2000). Such methods stand in sharp contrast to partially supervised techniques that have recently been proposed to induce hidden grammatical representations that are finer-grained than those that can be read off the parsed sentences in treebanks (Henderson, 2003; Matsuzaki et al., 2005; Prescher, 2005; Petrov et al., 2006). ∗ Part of this work was done when Gabriele Musillo was visiting the MIT Computer Science and Artificial Intelligence Laboratory, funded by a grant from the Swiss NSF (PBGE2117146). Many thanks to Michael Collins and Xavier Carreras for their insightful comments on the work presented here. This paper presents extensions of such grammar induction techniques to dependency grammars. Our extensions rely on transformations of dependency grammars into efficiently parsable contextfree grammars (CFG) annotated with hidden symbols. Because"
P08-2054,P07-1022,0,0.0753551,"is possible for the head index v of 2lv or 2rv to vary from i to j, v has to be tracked by the parser, resulting in an overall O(n4 ) time complexity. In the following, we show how to transform our O(n4 ) CFGs into O(n3 ) grammars by ap2 CFGs resulting from such transformations can further be normalised by removing the -productions from P . 3 Indeed, if 1lv or 0v derives wi,j , then v = i; if 1rv derives wi,j , then v = j; if wi,j is derived from Lpv , then v = j + 1; and if wi,j is derived from Rqv , then v = i − 1. plying transformations, closely related to those in (McAllester, 1999) and (Johnson, 2007), that eliminate the 2lv and 2rv symbols. We only detail the elimination of the symbols 2rv . The elimination of the 2lv symbols can be derived symmetrically. By construction, a 2rv symbol is the right successor of a non-terminal Rpu . Consequently, 2rv can only occur in a derivation such as α Rpu β ` α Rqu 2rv β ` α Rqu 1rv R1v β. To substitute for the problematic 2rv non-terminal in the above derivation, we derive the form Rqu 1rv R1v from Rpu /R1v R1v where Rpu /R1v is a new nonterminal whose right-hand side is Rqu 1rv . We thus transform the above derivation into the derivation α Rpu β ` α"
P08-2054,P03-1054,0,0.0315965,"Missing"
P08-2054,P05-1010,0,0.571587,"understood. As shown in (Klein and Manning, 2003), the ability of PCFG models to disambiguate phrases crucially depends on the expressiveness of the symbolic backbone they use. Treebank-specific heuristics have commonly been used both to alleviate inadequate independence assumptions stipulated by naive PCFGs (Collins, 1999; Charniak, 2000). Such methods stand in sharp contrast to partially supervised techniques that have recently been proposed to induce hidden grammatical representations that are finer-grained than those that can be read off the parsed sentences in treebanks (Henderson, 2003; Matsuzaki et al., 2005; Prescher, 2005; Petrov et al., 2006). ∗ Part of this work was done when Gabriele Musillo was visiting the MIT Computer Science and Artificial Intelligence Laboratory, funded by a grant from the Swiss NSF (PBGE2117146). Many thanks to Michael Collins and Xavier Carreras for their insightful comments on the work presented here. This paper presents extensions of such grammar induction techniques to dependency grammars. Our extensions rely on transformations of dependency grammars into efficiently parsable contextfree grammars (CFG) annotated with hidden symbols. Because dependency grammars are"
P08-2054,P92-1017,0,0.173204,"telligence Laboratory, funded by a grant from the Swiss NSF (PBGE2117146). Many thanks to Michael Collins and Xavier Carreras for their insightful comments on the work presented here. This paper presents extensions of such grammar induction techniques to dependency grammars. Our extensions rely on transformations of dependency grammars into efficiently parsable contextfree grammars (CFG) annotated with hidden symbols. Because dependency grammars are reduced to CFGs, any learning algorithm developed for PCFGs can be applied to them. Specifically, we use the Inside-Outside algorithm defined in (Pereira and Schabes, 1992) to learn transformed dependency grammars annotated with hidden symbols. What distinguishes our work from most previous work on dependency parsing is that our models are not lexicalised. Our models are instead decorated with hidden symbols that are designed to capture both lexical and structural information relevant to accurate dependency parsing without having to rely on any explicit supervision. 2 Transforms of Dependency Grammars Contrary to phrase-structure grammars that stipulate the existence of phrasal nodes, dependency grammars assume that syntactic structures are connected acyclic gra"
P08-2054,P06-1055,0,0.570805,"ng, 2003), the ability of PCFG models to disambiguate phrases crucially depends on the expressiveness of the symbolic backbone they use. Treebank-specific heuristics have commonly been used both to alleviate inadequate independence assumptions stipulated by naive PCFGs (Collins, 1999; Charniak, 2000). Such methods stand in sharp contrast to partially supervised techniques that have recently been proposed to induce hidden grammatical representations that are finer-grained than those that can be read off the parsed sentences in treebanks (Henderson, 2003; Matsuzaki et al., 2005; Prescher, 2005; Petrov et al., 2006). ∗ Part of this work was done when Gabriele Musillo was visiting the MIT Computer Science and Artificial Intelligence Laboratory, funded by a grant from the Swiss NSF (PBGE2117146). Many thanks to Michael Collins and Xavier Carreras for their insightful comments on the work presented here. This paper presents extensions of such grammar induction techniques to dependency grammars. Our extensions rely on transformations of dependency grammars into efficiently parsable contextfree grammars (CFG) annotated with hidden symbols. Because dependency grammars are reduced to CFGs, any learning algorith"
P08-2054,W05-1512,0,0.435744,"(Klein and Manning, 2003), the ability of PCFG models to disambiguate phrases crucially depends on the expressiveness of the symbolic backbone they use. Treebank-specific heuristics have commonly been used both to alleviate inadequate independence assumptions stipulated by naive PCFGs (Collins, 1999; Charniak, 2000). Such methods stand in sharp contrast to partially supervised techniques that have recently been proposed to induce hidden grammatical representations that are finer-grained than those that can be read off the parsed sentences in treebanks (Henderson, 2003; Matsuzaki et al., 2005; Prescher, 2005; Petrov et al., 2006). ∗ Part of this work was done when Gabriele Musillo was visiting the MIT Computer Science and Artificial Intelligence Laboratory, funded by a grant from the Swiss NSF (PBGE2117146). Many thanks to Michael Collins and Xavier Carreras for their insightful comments on the work presented here. This paper presents extensions of such grammar induction techniques to dependency grammars. Our extensions rely on transformations of dependency grammars into efficiently parsable contextfree grammars (CFG) annotated with hidden symbols. Because dependency grammars are reduced to CFGs,"
P08-2054,W03-3023,0,0.041097,"ta – section 24 FOM: q = 1, h = 1 SOM: q = 1, h = 1 FOM: q = 2, h = 2 FOM: q = 2, h = 4 SOM: q = 2, h = 2 SOM: q = 1, h = 4 per word 75.7 80.5 81.9 84.7 84.3 87.0 per sentence 9.9 16.2 17.4 22.0 21.5 25.8 Test Data – section 23 (Eisner and Smith, 2005) SOM: q = 1, h = 4 (McDonald, 2006) per word 75.6 88.0 91.5 per sentence NA 30.6 36.7 Table 1: Accuracy results on the development and test data set, where q denotes the number of hidden states and h the number of hidden values annotating a PoS tag involved in our first-order (FOM) and second-order (SOM) models. tained using the rules stated in (Yamada and Matsumoto, 2003), were transformed into first-order and second-order structures. CFGs extracted from such structures were then annotated with hidden variables encoding the constraints described in the previous section and trained until convergence by means of the Inside-Outside algorithm defined in (Pereira and Schabes, 1992) and applied in (Matsuzaki et al., 2005). To efficiently decode our hidden variable models, we pruned the search space as in (Petrov et al., 2006). To evaluate the performance of our models, we report two of the standard measures: the per word and per sentence accuracy (McDonald, 2006). F"
P08-2054,J03-4003,0,\N,Missing
P09-1033,W08-2121,0,0.0685227,"Missing"
P09-1033,P98-1013,0,0.0647586,"a set of semantic roles that can apply to any argument of any verb, to provide an unambiguous identifier of the grammatical roles of the participants in the event described by the sentence (Dowty, 1991). Starting from the first proposals (Gruber, 1965; Fillmore, 1968; Jackendoff, 1972), several approaches have been put forth, ranging from a combination of very few roles to lists of very fine-grained specificity. (See Levin and Rappaport Hovav (2005) for an exhaustive review). In NLP, several proposals have been put forth in recent years and adopted in the annotation of large samples of text (Baker et al., 1998; Palmer et al., 2005; Kipper, 2005; Loper et al., 2007). The annotated PropBank corpus, and therefore implicitly its role labels inventory, has been largely adopted in NLP because of its exhaustiveness and because it is coupled with syntactic annotation, properties that make it very attractive for the automatic learning of these roles and their further applications to NLP tasks. However, the labelling choices made by PropBank have recently come under scrutiny (Zapirain et al., 2008; Loper et al., 2007; Yi et al., 2007). The annotation of PropBank labels has been conceived in a two-tiered fash"
P09-1033,J08-2002,0,0.0434203,"Missing"
P09-1033,N07-1069,0,0.522263,"in a two-tiered fashion. A first tier assigns abstract labels such as ARG0 or ARG1, while a separate annotation records the secondtier, verb-sense specific meaning of these labels. Labels ARG0 or ARG1 are assigned to the most prominent argument in the sentence (ARG1 for unaccusative verbs and ARG0 for all other verbs). The other labels are assigned in the order of prominence. So, while the same high-level labels are used across verbs, they could have different meanings for different verb senses. Researchers have usually concentrated on the high-level annotation, but as indicated in Yi et al. (2007), there is reason to think that these labels do not generalise across verbs, nor to unseen verbs or to novel verb Semantic role labels are the representation of the grammatically relevant aspects of a sentence meaning. Capturing the nature and the number of semantic roles in a sentence is therefore fundamental to correctly describing the interface between grammar and meaning. In this paper, we compare two annotation schemes, PropBank and VerbNet, in a task-independent, general way, analysing how well they fare in capturing the linguistic generalisations that are known to hold for semantic role"
P09-1033,P08-1063,0,0.183745,"Missing"
P09-1033,J93-2004,0,0.0461431,"Missing"
P09-1033,W08-2101,1,0.902183,"Missing"
P09-1033,J01-3003,1,0.863967,"refore fundamental to correctly describe the interface between grammar and meaning, and it is of paramount importance for all natural language processing (NLP) applications that attempt to extract meaning representations from analysed text, such as questionanswering systems or even machine translation. 288 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 288–296, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP ing. Because the well-attested strong correlation between syntactic structure and semantic role labels (Levin and Rappaport Hovav, 2005; Merlo and Stevenson, 2001) could intervene as a confounding factor in this analysis, we expressly limit our investigation to data analyses and statistical measures that do not exploit syntactic properties or parsing techniques. The conclusions reached this way are not task-specific and are therefore widely applicable. To preview, based on results in section 3, we conclude that PropBank is easier to learn, but VerbNet is more informative in general, it generalises better to new role instances and its labels are more strongly correlated to specific verbs. In section 4, we show that VerbNet labels provide finergrained spe"
P09-1033,J05-1004,0,0.867541,"oles that can apply to any argument of any verb, to provide an unambiguous identifier of the grammatical roles of the participants in the event described by the sentence (Dowty, 1991). Starting from the first proposals (Gruber, 1965; Fillmore, 1968; Jackendoff, 1972), several approaches have been put forth, ranging from a combination of very few roles to lists of very fine-grained specificity. (See Levin and Rappaport Hovav (2005) for an exhaustive review). In NLP, several proposals have been put forth in recent years and adopted in the annotation of large samples of text (Baker et al., 1998; Palmer et al., 2005; Kipper, 2005; Loper et al., 2007). The annotated PropBank corpus, and therefore implicitly its role labels inventory, has been largely adopted in NLP because of its exhaustiveness and because it is coupled with syntactic annotation, properties that make it very attractive for the automatic learning of these roles and their further applications to NLP tasks. However, the labelling choices made by PropBank have recently come under scrutiny (Zapirain et al., 2008; Loper et al., 2007; Yi et al., 2007). The annotation of PropBank labels has been conceived in a two-tiered fashion. A first tier ass"
P09-1033,C98-1013,0,\N,Missing
P11-2052,2009.jeptalnrecital-long.4,0,0.0459134,"Missing"
P11-2052,J94-4004,0,0.462313,"es sentences with missing predicate labels based on PoS-information in the French sentence. 2.1 Learning joint syntactic-semantic structures We know from previous work that there is a strong correlation between syntax and semantics (Merlo and van der Plas, 2009), and that this correlation has been successfully applied for the unsupervised induction of semantic roles (Lang and Lapata, 2010). However, previous work in machine translation leads us to believe that transferring the correlations between syntax and semantics across languages would be problematic due to argumentstructure divergences (Dorr, 1994). For example, the English verb like and the French verb plaire do not share correlations between syntax and semantics. The verb like takes an A0 subject and an A1 direct object, whereas the verb plaire licences an A1 subject and an A0 indirect object. We therefore transfer semantic roles crosslingually based only on lexical alignments and add syntactic information after transfer. In Figure 1, we see that cross-lingual transfer takes place at the semantic level, a level that is more abstract and known to port relatively well across languages, while the correlations with syntax, that are known"
P11-2052,2007.tmi-papers.10,0,0.0368477,"points, respectively, lower than the upper bound from manual annotations. 1 Introduction As data-driven techniques tackle more and more complex natural language processing tasks, it becomes increasingly unfeasible to use complete, accurate, hand-annotated data on a large scale for training models in all languages. One approach to addressing this problem is to develop methods that automatically generate annotated data by transferring annotations in parallel corpora from languages for which this information is available to languages for which these data are not available (Yarowsky et al., 2001; Fung et al., 2007; Pad´o and Lapata, 2009). Previous work on the cross-lingual transfer of semantic annotations (Pad´o, 2007; Basili et al., 2009) In this paper, we generate high-quality broadcoverage semantic annotations using an automatic approach that does not rely on a semantic ontology for the target language. Furthermore, to our knowledge, we report the first results on using joint syntactic-semantic learning to improve the quality of the semantic annotations from automatic crosslingual transfer. Results on correlations between syntax and semantics found in previous work (Merlo and van der Plas, 2009; La"
P11-2052,W08-2122,1,0.893548,"Missing"
P11-2052,P02-1050,0,0.0491108,"Missing"
P11-2052,P06-2057,0,0.0855077,"Missing"
P11-2052,N10-1137,0,0.306482,"07; Pad´o and Lapata, 2009). Previous work on the cross-lingual transfer of semantic annotations (Pad´o, 2007; Basili et al., 2009) In this paper, we generate high-quality broadcoverage semantic annotations using an automatic approach that does not rely on a semantic ontology for the target language. Furthermore, to our knowledge, we report the first results on using joint syntactic-semantic learning to improve the quality of the semantic annotations from automatic crosslingual transfer. Results on correlations between syntax and semantics found in previous work (Merlo and van der Plas, 2009; Lang and Lapata, 2010) have led us to make use of the available syntactic annotations on the target language. We use the semantic annotations resulting from cross-lingual transfer combined with syntactic annotations to train a joint syntactic-semantic parser for the target language, which, in turn, re-annotates the corpus (See Figure 1). We show that the semantic annotations produced by this parser are of higher quality than the data on which it was trained. Given our goal of producing broad-coverage annotations in a setting based on an aligned corpus, our choices of formal representation and of labelling scheme di"
P11-2052,J93-2004,0,0.0416764,"c roles across languages (Pad´o, 2007), we select only those parallel sentences in Europarl that are direct translations from English to French, or vice versa. In the end, we have a word-aligned parallel corpus of 276-thousand sentence pairs. Syntactic annotation is available for French. The French Treebank (Abeill´e et al., 2003) is a treebank of 21,564 sentences annotated with constituency annotation. We use the automatic dependency conversion of the French Treebank into dependency format provided to us by Candito and Crabb´e and described in Candito et al. (2009). The Penn Treebank corpus (Marcus et al., 1993) merged with PropBank labels (Palmer et al., 2005) and NomBank labels (Meyers, 2007) is used to train the syntactic-semantic parser described in Subsection 3.1 to annotate the English part of the parallel corpus. 3.3 Test sets For testing, we used the hand-annotated data described in (van der Plas et al., 2010). One-thousand French sentences are extracted randomly from our parallel corpus without any constraints on the semantic parallelism of the sentences, unlike much previous work. We randomly split those 1000 sentences into test and development set containing 500 sentences each. 4 Results W"
P11-2052,P09-1033,1,0.722461,"Missing"
P11-2052,W07-1513,0,0.10955,"Missing"
P11-2052,J03-1002,0,0.00759072,"Missing"
P11-2052,2007.jeptalnrecital-long.25,0,0.0744954,"Missing"
P11-2052,J05-1004,0,0.697041,"only those parallel sentences in Europarl that are direct translations from English to French, or vice versa. In the end, we have a word-aligned parallel corpus of 276-thousand sentence pairs. Syntactic annotation is available for French. The French Treebank (Abeill´e et al., 2003) is a treebank of 21,564 sentences annotated with constituency annotation. We use the automatic dependency conversion of the French Treebank into dependency format provided to us by Candito and Crabb´e and described in Candito et al. (2009). The Penn Treebank corpus (Marcus et al., 1993) merged with PropBank labels (Palmer et al., 2005) and NomBank labels (Meyers, 2007) is used to train the syntactic-semantic parser described in Subsection 3.1 to annotate the English part of the parallel corpus. 3.3 Test sets For testing, we used the hand-annotated data described in (van der Plas et al., 2010). One-thousand French sentences are extracted randomly from our parallel corpus without any constraints on the semantic parallelism of the sentences, unlike much previous work. We randomly split those 1000 sentences into test and development set containing 500 sentences each. 4 Results We evaluate our methods for automatic annotation ge"
P11-2052,W07-2218,1,0.874923,"Missing"
P11-2052,W10-1814,1,0.905206,"Missing"
P11-2052,2009.eamt-1.30,0,0.0516121,"Missing"
P11-2052,N09-2004,0,0.156095,"Missing"
P11-2052,H01-1035,0,0.610552,"Missing"
P15-2078,W06-2920,0,0.353304,"Missing"
P15-2078,K15-1025,1,0.825029,"the external dependency to the noun, since the noun phrase can be entirely predicted based on its left corner. The RightNP factor is significant in the fitted model (βRN P = −0.77, p &lt; 0.001).4 The presence of a noun dependent on the right of the noun favours a prenominal placement, as predicted by DLM (1d). This is a result which, to our knowledge, was not previously observed in the literature, and that clearly answers our initial question, confirming that DLM also applies to very short spans. A much more detailed study of the lexical and structural properties of this effect is developed in (Gulordava and Merlo, 2015). 4 A log-likelihood test of the model including RightAP, LeftAP and RightNP factors compared to the model including only RightAP and LeftAP factors yields χ2 = 107 and p &lt; .001. Douglas Bates, Martin Maechler, Ben Bolker, and Steven Walker, 2014. lme4: Linear mixed-effects models using Eigen and S4. R package version 1.17. 4 Conclusion In this paper, we have developed a model of dependency length minimisation in the noun phrase and shown subtle interactions among its subcomponents. We show that most of DLM predictions are confirmed, and that DLM also apply to short spans. The fact that DLM ef"
P15-2078,W09-1201,0,\N,Missing
P15-2078,petrov-etal-2012-universal,0,\N,Missing
P92-1040,P85-1010,0,0.0796732,"Missing"
P92-1040,C86-1010,0,0.0623982,"Missing"
P92-1040,C86-1050,0,0.0803683,"Missing"
Q16-1025,D11-1037,0,0.0479963,"Missing"
Q16-1025,W06-2920,0,0.835344,"artificially-generated treebanks that are minimal permutations of actual treebanks with respect to two word order properties: word order variation and dependency lengths. Based on these artificial data on twelve languages, we show that longer dependencies and higher word order variability degrade parsing performance. Our method also extends to minimal pairs of individual sentences, leading to a finer-grained understanding of parsing errors. 1 Introduction Fair comparative performance evaluation across languages and their treebanks is one of the difficulties for work on multi-lingual parsing (Buchholz and Marsi, 2006; Nivre et al., 2007; Seddah et al., 2011). The differences in parsing performance can be the result of disparate properties of treebanks (such as their size or average sentence length), choices in annotation schemes, and the linguistic properties of languages. Despite recent attempts to create and apply cross-linguistic and cross-framework evaluation procedures (Tsarfaty et al., 2011; Seddah et al., 2013), there is no commonly used method of analysis of parsing performance which accounts for different linguistic and extra-linguistic factors of treebanks and teases them apart. When investigati"
Q16-1025,de-marneffe-etal-2014-universal,0,0.0602028,"Missing"
Q16-1025,W15-2112,0,0.206608,"e across languages is word order freedom, the ability languages have to express the same or similar meaning in the same context with a free choice of different word orders. The extent of word order freedom in a sentence is reflected in the entropy of word order, given the words and the syntactic structure of the sentence, H(order|words, tree). One approximation of word order entropy is the entropy of the direction of dependencies in a tree345 bank. This measure has been proposed in several recent works to quantitatively describe the typology of word order freedom in many languages (Liu, 2010; Futrell et al., 2015b). Arc direction entropy can be used, for instance, to capture the difference between adjective-noun word order properties in Germanic and Romance languages. In English, this word order is fixed, as adjectives appear almost exclusively prenominally; the adjective-noun arc direction entropy will therefore be close to 0. In Italian, by contrast, the same adjective can both precede and follow nouns; the adjective-noun arc direction entropy will be greater than 0. We calculate the overall entropy of arc directions in a treebank conditioned on the relation type defined by the dependency label Rel"
Q16-1025,W15-2115,1,0.931071,"dependencies. A low degree of DLM is associated with flexibility of word order and in particular with high non-projectivity, i.e., the presence of crossing arcs in a tree, a feature that has been treated in dependency parsing using local word order permutations (Hajiˇcov´a et al., 2004; Nivre, 2009; Titov et al., 2009; Henderson et al., 2013). To estimate the degree of DLM in a language, we follow previous work which analysed the dependency lengths in a treebank with respect to their random and minimal potential alternatives (Temperley, 2007; Gildea and Temperley, 2010; Futrell et al., 2015a; Gulordava and Merlo, 2015). We calculate the overall ratio of DLM in a treebank as shown in equation 2. (2) DLM Ratio = Σs DLs OptDLs /Σs 2 |s| |s|2 For each sentence s and its dependency tree t, we compute the overall dependency length of P the original sentence DL(s) = arc∈t DL(arc) and its minimal projective dependency length OptDL(s) = DL(s0 ), where s0 is obtained by reordering the words in the sentence s using the algorithm described in the next section (following Gildea and Temperley (2010)). To average these values across all sentences, we normalise them by |s|2 , since it has been observed empirically that the"
Q16-1025,P15-2078,1,0.899567,"Missing"
Q16-1025,J13-4006,1,0.85064,"Gibson, 1998; Demberg and Keller, 2008; Tily, 2010; Gulordava and 3 The length of a dependency, DL(arc) below, is the number of words in the span covered by the dependency arc. Merlo, 2015; Gulordava et al., 2015). Languages differ, however, in the degree to which they minimise dependencies. A low degree of DLM is associated with flexibility of word order and in particular with high non-projectivity, i.e., the presence of crossing arcs in a tree, a feature that has been treated in dependency parsing using local word order permutations (Hajiˇcov´a et al., 2004; Nivre, 2009; Titov et al., 2009; Henderson et al., 2013). To estimate the degree of DLM in a language, we follow previous work which analysed the dependency lengths in a treebank with respect to their random and minimal potential alternatives (Temperley, 2007; Gildea and Temperley, 2010; Futrell et al., 2015a; Gulordava and Merlo, 2015). We calculate the overall ratio of DLM in a treebank as shown in equation 2. (2) DLM Ratio = Σs DLs OptDLs /Σs 2 |s| |s|2 For each sentence s and its dependency tree t, we compute the overall dependency length of P the original sentence DL(s) = arc∈t DL(arc) and its minimal projective dependency length OptDL(s) = DL"
Q16-1025,J11-1007,0,0.464149,"ard for parsing, as rich morphology increases the percentage of new words in the test set (Nivre et al., 2007; Tsarfaty et al., 2010). These languages however also often exhibit very flexible word order. It has not so far been investigated how much rich morphology contributes to parsing difficulty compared to the difficulty introduced by word order variation in such languages. The length of the dependencies in the tree has also been shown to affect performance: almost all types of dependency parsers, in different measure, show degraded performance for longer sentences and longer dependencies (McDonald and Nivre, 2011).1 We use arc direction entropy and DLM ratio, respectively, as the measures of these two word order properties because they are formally defined in the previous literature and can be quantified on a dependency treebank in any language. To preview our results, in a set of pairwise comparisons between original and permuted treebanks, we confirm the influence of word order variability and dependency length on parsing performance, at the large scale provided by fourteen different treebanks across twelve different languages.2 Our results suggest, in addition, that word order entropy applies a stro"
Q16-1025,W06-2932,0,0.0870986,"values of the original treebanks, the DLM ratio and Entropy values of the artificial treebanks are much more narrowly distributed: 1.17±0.02 (mean ± SD) compared to 1.19±0.07 for DLM ratio and 0.59 ± 0.03 compared to 0.27 ± 0.17 for Entropy. Notice also that, on average, the treebanks in the LB/RB permuted set have both lower entropy and lower DLM ratio than the original treebanks. The treebanks in the OptDL set have lower DLM ratio, but also higher entropy than the original treebanks. 3.5 Parsing setup To evaluate the impact of word order properties on parsing performance, we use MSTParser (McDonald et al., 2006) and MaltParser (Nivre et al., 2006) — two widely used representatives of two main deLanguage Polish Italian Finnish Spanish French English Bulgarian Vulgate (La) Dutch NewTest (AG) German Cicero (La) Persian Herodotus (AG) Abbr. Size Av. sentence length pl 29k 6.8 it 57k 12.1 fi 46k 5.7 es 63k 15.1 fr 72k 14.5 en 62k 9.5 30k 8.5 bg la.V 63k 8.8 nl 38k 8.4 grc.NT 69k 10.5 de 65k 11.5 la.C 35k 11.6 fa 35k 9.4 grc.H 59k 14.4 Mean (± st. deviation) Original treebanks DLM ratio Entropy 1.13 0.34 1.13 0.18 1.13 0.34 1.15 0.15 1.15 0.11 1.17 0.09 1.17 0.20 1.17 0.43 1.17 0.26 1.19 0.38 1.24 0.21 1.2"
Q16-1025,D11-1006,0,0.0710159,"Missing"
Q16-1025,W15-2125,1,0.883649,"proposed in the literature on dependency length minimisation (DLM). This measure allows comparisons across treebanks with sentences of different size and across dependency trees of different topology. Experimental and theoretical language research has yielded a large and diverse body of evidence showing that languages, synchronically and diachronically, tend to minimise the length of their dependencies (Hawkins, 1994; Gibson, 1998; Demberg and Keller, 2008; Tily, 2010; Gulordava and 3 The length of a dependency, DL(arc) below, is the number of words in the span covered by the dependency arc. Merlo, 2015; Gulordava et al., 2015). Languages differ, however, in the degree to which they minimise dependencies. A low degree of DLM is associated with flexibility of word order and in particular with high non-projectivity, i.e., the presence of crossing arcs in a tree, a feature that has been treated in dependency parsing using local word order permutations (Hajiˇcov´a et al., 2004; Nivre, 2009; Titov et al., 2009; Henderson et al., 2013). To estimate the degree of DLM in a language, we follow previous work which analysed the dependency lengths in a treebank with respect to their random and minimal p"
Q16-1025,P12-1066,0,0.0282647,"uld be taken into account (Section 4.2). However, we are not aware of previous work which proposes a measure for this property and describes it typologically on a large scale. Finally, our method, which consists in creating artificial treebanks, can prove useful beyond parsing evaluation. For instance, our data could enrich the training data for tasks such as de-lexicalized parser transfer (McDonald et al., 2011). Word order properties play an important role in computing similarity between languages and finding the source language leading to the best parser performance in the target language (Naseem et al., 2012; Rosa and Zabokrtsky, 2015). A possibly large artificially permuted treebank with word order properties similar to the target language could then be a better training match than a small treebank of an existing target natural language. 6 Related work Much previous work has been dedicated to the evaluation of parsing performance, also in a multilingual setting. The shared tasks in multilingual dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007) and parsing of morphologically-rich languages (Tsarfaty et al., 2010; Seddah et al., 2013) collected a large set of parsing performance re"
Q16-1025,P81-1022,0,0.757024,"Missing"
Q16-1025,C10-1094,0,0.136761,"Missing"
Q16-1025,P09-1040,0,0.0197346,"eir dependencies (Hawkins, 1994; Gibson, 1998; Demberg and Keller, 2008; Tily, 2010; Gulordava and 3 The length of a dependency, DL(arc) below, is the number of words in the span covered by the dependency arc. Merlo, 2015; Gulordava et al., 2015). Languages differ, however, in the degree to which they minimise dependencies. A low degree of DLM is associated with flexibility of word order and in particular with high non-projectivity, i.e., the presence of crossing arcs in a tree, a feature that has been treated in dependency parsing using local word order permutations (Hajiˇcov´a et al., 2004; Nivre, 2009; Titov et al., 2009; Henderson et al., 2013). To estimate the degree of DLM in a language, we follow previous work which analysed the dependency lengths in a treebank with respect to their random and minimal potential alternatives (Temperley, 2007; Gildea and Temperley, 2010; Futrell et al., 2015a; Gulordava and Merlo, 2015). We calculate the overall ratio of DLM in a treebank as shown in equation 2. (2) DLM Ratio = Σs DLs OptDLs /Σs 2 |s| |s|2 For each sentence s and its dependency tree t, we compute the overall dependency length of P the original sentence DL(s) = arc∈t DL(arc) and its minim"
Q16-1025,N13-1031,0,0.019406,"on left-to-right processing of words and the fully right-branching or fully left-branching orders can yield different results. 3.3 Dependency Treebanks We use a sample of fourteen dependency treebanks for twelve languages. The treebanks for Bulgarian, English, Finnish, French, German, Italian and Spanish come from the Universal Dependency Project and are annotated with the same annotation scheme (Agi´c et al., 2015). We use the treebank for Dutch from the CONLL 2006 shared task (Buchholz and Marsi, 2006). The Polish treebank is described in Woli´nski et al. (2011) and the Persian treebank in Rasooli et al. (2013). In addition, we use two Latin and two Ancient Greek dependency annotated texts (Haug and Jøhndal, 2008) because these languages are well-known for having very free word order.6 The quantitative properties of these treebanks are presented in Table 1 (second and third column). This set of treebanks includes those treebanks which had at least 3,000 sentences in their training set after eliminating sentences not fit for permutation (with punctuation tokens or multiple roots). This excluded from our analysis some otherwise typologi6 The Latin corpora comprise works of Cicero (circa 40 BC) and Vul"
Q16-1025,D08-1093,0,0.148638,"Missing"
Q16-1025,D09-1085,0,0.0934117,"Missing"
Q16-1025,P15-2040,0,0.0278689,"ount (Section 4.2). However, we are not aware of previous work which proposes a measure for this property and describes it typologically on a large scale. Finally, our method, which consists in creating artificial treebanks, can prove useful beyond parsing evaluation. For instance, our data could enrich the training data for tasks such as de-lexicalized parser transfer (McDonald et al., 2011). Word order properties play an important role in computing similarity between languages and finding the source language leading to the best parser performance in the target language (Naseem et al., 2012; Rosa and Zabokrtsky, 2015). A possibly large artificially permuted treebank with word order properties similar to the target language could then be a better training match than a small treebank of an existing target natural language. 6 Related work Much previous work has been dedicated to the evaluation of parsing performance, also in a multilingual setting. The shared tasks in multilingual dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007) and parsing of morphologically-rich languages (Tsarfaty et al., 2010; Seddah et al., 2013) collected a large set of parsing performance results. Some steps towards co"
Q16-1025,N10-1091,0,0.0131129,"at the MaltParser does not perform well on treebanks with high word order variability between the children attached to the same head (see Section 4.2). When two parsing systems are known to have different strengths and weaknesses they can be successfully 9 Overall, the variance in the LB/RB performances on Spanish is relatively high and the mean difference (computed across UAS scores for sentences) is not statistically significant (t-test: p > 0.5) – a result we would expect if errors cannot be imputed to clear structural factors. 352 combined in an ensemble model for more robust performance (Surdeanu and Manning, 2010). A contribution of the parsing performance analyses in a multilingual setting is the identification of difficult properties of treebanks. For Cicero and Herodotus texts, for example, our method reveals that their word order properties are important reasons for the very low parsing performances. This result confirms intuition, but it could not be firmly concluded without factoring out confounds such as the size of the training set or the dissimilarity between the training and test sets, which could also be reasons for low parsing performance. For German, our analysis gives more unexpected resu"
Q16-1025,W07-2218,0,0.0121407,"ontrast, the dependency tree structure of a permuted sentence in T 0 is the same as in the original sentence in T . For each treebank in our sample of languages and a type of permutation, we conduct two parsing evaluations: TT rain → TT est and TT0 rain → TT0 est . The training-test data split for T and T 0 is always the same, that is TT0 rain = P ermuted(TT rain ) and TT0 est = P ermuted(TT est ). The parsing performance is measured as Unlabeled and Labeled Attachment Scores (UAS and LAS), the proportion of correctly attached arcs in the unlabelled or labelled tree, respectively. 2 1 But see Titov and Henderson (2007) for an exception and comparison to Malt. 344 Methodology Polish, Italian, Finnish, Spanish, French, English, Bulgarian, Latin (Vulgate, Cicero), Dutch, Ancient Greek (New Testament, Herodotus), German and Persian. Given the training-testing setup, the differences in unlabelled attachment scores UAS(TT est ) − UAS(TT0 est ) can be directly attributed to the differences in word order properties o between T and T 0 , abstracting away from other treebank properties h. More formally, we assume that UAS(T ) = 0 f (oT , hT ) and UAS(T 0 ) = f (oT , hT ). Except for 0 word order properties oT and oT"
Q16-1025,W10-1401,0,0.118922,"Missing"
Q16-1025,D11-1036,0,0.0222791,"ing to a finer-grained understanding of parsing errors. 1 Introduction Fair comparative performance evaluation across languages and their treebanks is one of the difficulties for work on multi-lingual parsing (Buchholz and Marsi, 2006; Nivre et al., 2007; Seddah et al., 2011). The differences in parsing performance can be the result of disparate properties of treebanks (such as their size or average sentence length), choices in annotation schemes, and the linguistic properties of languages. Despite recent attempts to create and apply cross-linguistic and cross-framework evaluation procedures (Tsarfaty et al., 2011; Seddah et al., 2013), there is no commonly used method of analysis of parsing performance which accounts for different linguistic and extra-linguistic factors of treebanks and teases them apart. When investigating possible causal factors for observed phenomena, one powerful method, if available, consists in intervening on the postulated causes to observe possible changes in the observed effects. In other words, if A causes B, then changing A or properties of A should result in an observable change in B. This interventionist approach to the study of causality creates counterfactual data and a"
Q16-1025,nivre-etal-2006-maltparser,0,\N,Missing
Q16-1025,E12-2012,0,\N,Missing
Q16-1025,D07-1096,0,\N,Missing
Q16-1025,L16-1262,0,\N,Missing
W01-0715,W98-1106,0,\N,Missing
W01-0715,J99-2004,0,\N,Missing
W01-0715,J98-3003,0,\N,Missing
W01-0715,W97-0317,1,\N,Missing
W01-0715,J93-2004,0,\N,Missing
W01-0715,J93-1005,0,\N,Missing
W01-0715,H94-1048,0,\N,Missing
W01-0715,W95-0103,0,\N,Missing
W05-1509,P98-1013,0,0.00984977,"NP-TMP PP-DIR H H  IN NP NNP TO NP at NN Tuesday to QP midnight P  PP $ 2.80 trillion Figure 1: A sample syntactic structure with function labels. 1 Introduction Natural language processing methods producing shallow semantic output are starting to emerge as the next step towards successful developments in natural language understanding. Incremental, robust parsing systems will be the core enabling technology for interactive, speech-based question answering and dialogue systems. In recent years, corpora annotated with semantic and function labels have seen the light (Palmer et al., 2005; Baker et al., 1998) and semantic role labelling has taken centre-stage as a challenging new task. State-of-the-art statistical parsers have not yet responded to this challenge. State-of-the-art statistical parsers trained on the Penn Treebank (PTB) (Marcus et al., 1993) produce trees annotated with bare phrase structure labels (Collins, 1999; Charniak, 2000). The trees of the Penn Treebank, however, are also decorated with function labels, labels that indicate the grammatical and semantic relationship of phrases to each other in the sentence. Figure 1 shows the simplified tree representation with function labels"
W05-1509,A00-2031,0,0.280381,"(Collins, 1999; Charniak, 2000). The trees of the Penn Treebank, however, are also decorated with function labels, labels that indicate the grammatical and semantic relationship of phrases to each other in the sentence. Figure 1 shows the simplified tree representation with function labels for a sample sentence from the PTB corpus (section 00) The Government’s borrowing authority dropped at midnight Tuesday to 2.80 trillion from 2.87 trillion. Unlike phrase structure labels, function labels are contextdependent and encode a shallow level of phrasal and lexical semantics, as observed first in (Blaheta and Charniak, 2000). For example, while the authority in Figure 1 will always be a Noun Phrase, it could be a subject, as in the example, or an object, as in the sentence They questioned his authority, depending on its position in the sentence. To some extent, function labels overlap with semantic role labels as defined in PropBank (Palmer et al., 2005). Table 1 83 Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 83–92, c Vancouver, October 2005. 2005 Association for Computational Linguistics Syntactic Labels DTV dative LGS logical subject PRD predicate PUT compl of put SBJ s"
W05-1509,A00-2018,0,0.936661,"Incremental, robust parsing systems will be the core enabling technology for interactive, speech-based question answering and dialogue systems. In recent years, corpora annotated with semantic and function labels have seen the light (Palmer et al., 2005; Baker et al., 1998) and semantic role labelling has taken centre-stage as a challenging new task. State-of-the-art statistical parsers have not yet responded to this challenge. State-of-the-art statistical parsers trained on the Penn Treebank (PTB) (Marcus et al., 1993) produce trees annotated with bare phrase structure labels (Collins, 1999; Charniak, 2000). The trees of the Penn Treebank, however, are also decorated with function labels, labels that indicate the grammatical and semantic relationship of phrases to each other in the sentence. Figure 1 shows the simplified tree representation with function labels for a sample sentence from the PTB corpus (section 00) The Government’s borrowing authority dropped at midnight Tuesday to 2.80 trillion from 2.87 trillion. Unlike phrase structure labels, function labels are contextdependent and encode a shallow level of phrasal and lexical semantics, as observed first in (Blaheta and Charniak, 2000). Fo"
W05-1509,W98-1105,0,0.0114269,"yakanok et al., 2005), showing that parsing is beneficial. shift On the other hand, function labelling while parsing opens the way to interactive applications that are not possible in a two-stage architecture. Because the parser produces richer output incrementally at the same time as parsing, it can be integrated in speechbased applications, as well as be used for language models. Conversely, output annotated with more informative labels, such as function or semantic labels, underlies all domain-independent question answering (Jijkoun et al., 2004) or shallow semantic interpretation systems (Collins and Miller, 1998; Ge and Mooney, 2005). 2 The Basic Architecture To achieve the complex task of assigning function labels while parsing, we use a family of statistical parsers, the Simple Synchrony Network (SSN) parsers (Henderson, 2003), which do not make any explicit independence assumptions, and are therefore likely to adapt without much modification to the current problem. This architecture has shown stateof-the-art performance. SSN parsers comprise two components, one which estimates the parameters of a stochastic model for syntactic trees, and one which searches for the most probable syntactic tree give"
W05-1509,W05-0602,0,0.0340018,"wing that parsing is beneficial. shift On the other hand, function labelling while parsing opens the way to interactive applications that are not possible in a two-stage architecture. Because the parser produces richer output incrementally at the same time as parsing, it can be integrated in speechbased applications, as well as be used for language models. Conversely, output annotated with more informative labels, such as function or semantic labels, underlies all domain-independent question answering (Jijkoun et al., 2004) or shallow semantic interpretation systems (Collins and Miller, 1998; Ge and Mooney, 2005). 2 The Basic Architecture To achieve the complex task of assigning function labels while parsing, we use a family of statistical parsers, the Simple Synchrony Network (SSN) parsers (Henderson, 2003), which do not make any explicit independence assumptions, and are therefore likely to adapt without much modification to the current problem. This architecture has shown stateof-the-art performance. SSN parsers comprise two components, one which estimates the parameters of a stochastic model for syntactic trees, and one which searches for the most probable syntactic tree given the parameter estima"
W05-1509,H05-1078,1,0.366551,"ome of which of semantic nature, such as CLR, which will be more accurately learnt separately. The NULL label was split into the mutually exclusive labels CLR, OBJ and OTHER. Constituents were assigned the OBJ label according to the conditions stated in (Collins, 1999). Roughly, an OBJ non-terminal is an NP, SBAR or S whose parent is an S, VP or SBAR. Any such non-terminal must not bear either syntactic or semantic function labels, or the CLR label. In addition, the first child following the head of a PP is marked with the OBJ label. (For more detail on this lexical semantics projection, see (Merlo and Musillo, 2005).) We report the effects of these augmentations on parsing results in the experiments described below. 88 S HH  H  P PP  HH VP NP-SBJ P  PP  @ the authority  @ PPP  PP  @ VBD PP-TMP H  H dropped IN-TMP NP at NN NP-TMP PP-DIR H  H NNP-TMP TO-DIR NP Tuesday midnight to QP P   PP $ 2.80 trillion Figure 3: A sample syntactic structure with function labels lowered onto the preterminals. 4 Experiments and Discussion To assess the relevance of our fine-grained tags and history representations for functional labelling, we compare two augmented models to two baseline models witho"
W05-1509,J02-3001,0,0.114413,"n SSN parsing models. It performs the gradient descent with a maximum likelihood objective function and weight decay regularization (Bishop, 1995). 86 tion moves: the best-first search strategy is applied to the five best alternative decisions only. 3 Learning Lexical Projection and Locality Domains of Function Labels Recent approaches to functional or semantic labels are based on two-stage architectures. The first stage selects the elements to be labelled, while the second determines the labels to be assigned to the selected elements. While some of these models are based on full parse trees (Gildea and Jurafsky, 2002; Blaheta, 2004), other methods have been proposed that eschew the need for a full parse (CoNLL, 2004; CoNLL, 2005). Because of the way the problem has been formulated, – as a pipeline of parsing feeding into labelling – specific investigations of the interaction of lexical projections with the relevant structural parsing notions during function labelling has not been studied. The starting point of our augmentation of SSN models is the observation that the distribution of function labels can be better characterised structurally than sequentially. Function labels, similarly to semantic roles, r"
W05-1509,E03-1079,1,0.856824,"Missing"
W05-1509,P02-1031,0,0.0675212,"ge. For example, some nominal temporal modifiers occupy an object position without being objects, like Tuesday in the tree above. Moreover, given current limited availability of annotated tree banks, this more complex task will have to be solved with the same overall amount of data, aggravating the difficulty of estimating the model’s parameters due to sparse data. Solving this more complex problem successfully, then, indicates that the models used are robust. Our results also provide some new insights into the discussion about the necessity of parsing for function or semantic role labelling (Gildea and Palmer, 2002; Punyakanok et al., 2005), showing that parsing is beneficial. shift On the other hand, function labelling while parsing opens the way to interactive applications that are not possible in a two-stage architecture. Because the parser produces richer output incrementally at the same time as parsing, it can be integrated in speechbased applications, as well as be used for language models. Conversely, output annotated with more informative labels, such as function or semantic labels, underlies all domain-independent question answering (Jijkoun et al., 2004) or shallow semantic interpretation syst"
W05-1509,gimenez-marquez-2004-svmtool,0,0.0763055,"Missing"
W05-1509,P98-1087,0,0.388614,"probability of a finite (but unbounded) sequence of derivation moves. To bound the number of parameters, standard history-based models partition the set of wellformed sequences of transitions into equivalence classes. While such a partition makes the problem of searching for the most probable parse polynomial, it introduces hard independence assumptions: a derivation move only depends on the equivalence class to which its history belongs. SSN parsers, on the other hand, do not state any explicit independence assumptions: they use a neural network architecture, called Simple Synchrony Network (Henderson and Lane, 1998), to induce a finite history representation of an unbounded sequence of moves. The history representation of a parse history d1 , . . . , di−1 , which we denote h(d1 , . . . , di−1 ), is assigned to the constituent that is on the top of the stack before the ith move. The representation h(d1 , . . . , di−1 ) is computed from a set f of features of the derivation move d i−1 and from a finite set D of recent history representations h(d1 , . . . , dj ), where j &lt; i − 1. Because the history representation computed for the move i−1 is included in the inputs to the computation of the representation f"
W05-1509,J05-1004,0,0.618335,"dropped PP-TMP H  H NP-TMP PP-DIR H H  IN NP NNP TO NP at NN Tuesday to QP midnight P  PP $ 2.80 trillion Figure 1: A sample syntactic structure with function labels. 1 Introduction Natural language processing methods producing shallow semantic output are starting to emerge as the next step towards successful developments in natural language understanding. Incremental, robust parsing systems will be the core enabling technology for interactive, speech-based question answering and dialogue systems. In recent years, corpora annotated with semantic and function labels have seen the light (Palmer et al., 2005; Baker et al., 1998) and semantic role labelling has taken centre-stage as a challenging new task. State-of-the-art statistical parsers have not yet responded to this challenge. State-of-the-art statistical parsers trained on the Penn Treebank (PTB) (Marcus et al., 1993) produce trees annotated with bare phrase structure labels (Collins, 1999; Charniak, 2000). The trees of the Penn Treebank, however, are also decorated with function labels, labels that indicate the grammatical and semantic relationship of phrases to each other in the sentence. Figure 1 shows the simplified tree representation"
W05-1509,N03-1014,0,0.207698,"t form natural classes. Like previous work, constituents that do not bear any function label will receive a NULL label. Strictly speaking, this label corresponds to two NULL labels: the SYN - NULL and the SEM - NULL. A node bearing the SYN - NULL label is a node that does not bear any other syntactic label. Analogously, the SEM - NULL label completes the set of semantic labels. Note that both the SYN - NULL label and the SEM - NULL are necessary, since both a syntactic and a semantic label can label a given constituent. We present work to test the hypothesis that a current statistical parser (Henderson, 2003) can output richer information robustly, that is without any degradation of the parser’s accuracy on the original parsing task, by explicitly modelling function labels as the locus where the lexical semantics of the elements in the sentence and syntactic locality domains interact. Briefly, our method consists in augmenting the parser with features and biases that capture both lexical semantics projections and structural regularities underlying the distribution of sequences of function labels in a sentence. We achieve state-of-the-art results both in parsing and function labelling. This result"
W05-1509,C04-1188,0,0.0572403,"Missing"
W05-1509,P03-1054,0,0.00773339,"ng a function label such as φ1 and φ2 are not directly input to their respective parents. In our extended model, information conveyed by φ1 and φ2 directly flows to their respective parents. So the distance between the nodes φ1 and φ2 , which stand in a c-command relation, is shortened and kept constant. As well as being subject to locality constraints, functional labels are projected by the lexical semantics of the words in the sentence. We introduce this bottom-up lexical information by fine-grained modelling of function tags in two ways. On the one hand, extending a technique presented in (Klein and Manning, 2003), we split some part-of-speech tags into tags marked with semantic function labels. The labels attached to a non-terminal which appeared to cause the most trouble to the parser in a separate experiment (DIR, LOC, MNR, PRP or TMP) were propagated down to the pre-terminal tag of its head. To affect only labels that are projections of lexical semantics properties, the propagation takes into account the distance of the projection from the lexical head to the label, and distances greater than two are not included. Figure 3 illustrates the result of the tag splitting operation. On the other hand, we"
W05-1509,W04-0832,0,0.12057,"and its relation to parsing. An improvement in parsing performance by better modelling of function labels indicates that this complex problem is better solved as a single integrated task and that current two-step architectures might be missing on successful ways to improve both the parsing and the labelling task. In particular, recent models of semantic role labelling separate input indicators of the correlation between the structural position in the tree and the semantic label, such as path, from those indicators that encode constraints on the sequence, such as the previously assigned role (Kwon et al., 2004). In this way, they can never encode directly the constraining power of a certain role in a given structural position onto a following node in its structural position. In our augmented model, we attempt to capture these constraints by directly modelling syntactic domains. Our results confirm the findings in (Palmer et al., 2005). They take a critical look at some commonly used features in the semantic role labelling task, such as the path feature. They suggest that the path feature is not very effective because it is sparse. Its sparseness is due to the occurrence of intermediate nodes that ar"
W05-1509,J93-2004,0,0.026727,"to emerge as the next step towards successful developments in natural language understanding. Incremental, robust parsing systems will be the core enabling technology for interactive, speech-based question answering and dialogue systems. In recent years, corpora annotated with semantic and function labels have seen the light (Palmer et al., 2005; Baker et al., 1998) and semantic role labelling has taken centre-stage as a challenging new task. State-of-the-art statistical parsers have not yet responded to this challenge. State-of-the-art statistical parsers trained on the Penn Treebank (PTB) (Marcus et al., 1993) produce trees annotated with bare phrase structure labels (Collins, 1999; Charniak, 2000). The trees of the Penn Treebank, however, are also decorated with function labels, labels that indicate the grammatical and semantic relationship of phrases to each other in the sentence. Figure 1 shows the simplified tree representation with function labels for a sample sentence from the PTB corpus (section 00) The Government’s borrowing authority dropped at midnight Tuesday to 2.80 trillion from 2.87 trillion. Unlike phrase structure labels, function labels are contextdependent and encode a shallow lev"
W05-1509,W05-0639,0,0.0592285,"inal temporal modifiers occupy an object position without being objects, like Tuesday in the tree above. Moreover, given current limited availability of annotated tree banks, this more complex task will have to be solved with the same overall amount of data, aggravating the difficulty of estimating the model’s parameters due to sparse data. Solving this more complex problem successfully, then, indicates that the models used are robust. Our results also provide some new insights into the discussion about the necessity of parsing for function or semantic role labelling (Gildea and Palmer, 2002; Punyakanok et al., 2005), showing that parsing is beneficial. shift On the other hand, function labelling while parsing opens the way to interactive applications that are not possible in a two-stage architecture. Because the parser produces richer output incrementally at the same time as parsing, it can be integrated in speechbased applications, as well as be used for language models. Conversely, output annotated with more informative labels, such as function or semantic labels, underlies all domain-independent question answering (Jijkoun et al., 2004) or shallow semantic interpretation systems (Collins and Miller, 1"
W05-1509,W04-0800,0,0.130647,"Missing"
W05-1509,W04-0857,0,0.0143246,"cus only on c-commanding nodes bearing function labels, thus abstracting away from those nodes that smear the pertinent relations. (Yi and Palmer, 2005) share the motivation of our work, although they apply it to a different task. Like the current work, they observe that the distributions of semantic labels could potentially interact with the distributions of syntactic labels and redefine the boundaries of constituents, thus yielding trees that reflect generalisations over both these sources of information. Our results also confirm the importance of lexical information, the lesson drawn from (Thompson et al., 2004), who find that correctly modelling sequence information is not sufficient. Lexical information is very important, as it reflects the lexical semantics of the constituents. Both factors, syntactic domains and lexical information, are needed to significantly improve parsing. 5 Conclusions In this paper, we have explored a new way to improve parsing results in a current statistical parser while at the same time enriching its output. We achieve significant improvements in parsing and function labelling by modelling directly the specific nature of function labels, as both expressions of the lexica"
W05-1509,W04-3212,0,0.0274022,"Missing"
W05-1509,C00-2137,0,0.0845892,"Missing"
W05-1509,W04-3211,0,\N,Missing
W05-1509,J03-4003,0,\N,Missing
W05-1509,C98-1013,0,\N,Missing
W05-1509,J06-3002,1,\N,Missing
W05-1509,C98-1084,0,\N,Missing
W06-2303,P98-1013,0,0.106373,"Missing"
W06-2303,W05-0620,0,0.213749,"Missing"
W06-2303,H05-1078,1,0.889312,"efore that step. In addition to history representations, the inputs to h(d1 , . . . , di−1 ) include handcrafted features of the derivation history that are meant to be relevant to the move to be chosen at step i. For each of the experiments reported here, the set D that is input to the computation of the history representation of the derivation moves d1 , . . . , di−1 includes the most recent history representation of the following nodes: topi , the node on top of the pushdown stack before the ith move; 3 Learning Semantic Role Labels Previous work on learning function labels during parsing (Merlo and Musillo, 2005; Musillo and Merlo, 2005) assumed that function labels represent the interface between lexical semantics and syntax. We extend this hypothesis to the semantic role labels assigned in PropBank, as they are an exhaustive extension of function labels, which have been reorganised in a coherent inventory of labels and assigned exhaustively to all sentences in the PTB. Because PropBank is built on the PTB, it inherits in part its notion of function labels which is directly integrated into the AM-X role labels. A0-A5 or AA labels correspond to many of the unlabelled elements in the PTB and also to t"
W06-2303,W05-1509,1,0.940833,"ion to history representations, the inputs to h(d1 , . . . , di−1 ) include handcrafted features of the derivation history that are meant to be relevant to the move to be chosen at step i. For each of the experiments reported here, the set D that is input to the computation of the history representation of the derivation moves d1 , . . . , di−1 includes the most recent history representation of the following nodes: topi , the node on top of the pushdown stack before the ith move; 3 Learning Semantic Role Labels Previous work on learning function labels during parsing (Merlo and Musillo, 2005; Musillo and Merlo, 2005) assumed that function labels represent the interface between lexical semantics and syntax. We extend this hypothesis to the semantic role labels assigned in PropBank, as they are an exhaustive extension of function labels, which have been reorganised in a coherent inventory of labels and assigned exhaustively to all sentences in the PTB. Because PropBank is built on the PTB, it inherits in part its notion of function labels which is directly integrated into the AM-X role labels. A0-A5 or AA labels correspond to many of the unlabelled elements in the PTB and also to those elements that PTB ann"
W06-2303,J02-3001,0,0.390729,"ses two levels of granularity in its annotation, at least conceptually. Arguments receiving labels A0-A5 or AA do not express consistent semantic roles and are specific to a verb, while arguments receiving an AM-X label are supposed to be adjuncts and the respective roles they express are consistent across all verbs.1 Recent approaches to learning semantic role labels are based on two-stage architectures. The first stage selects the elements to be labelled, while the second determines the labels to be assigned to the selected elements. While some of these models are based on full parse trees (Gildea and Jurafsky, 2002; Gildea and Palmer, 2002), other methods have been proposed that eschew the need for a full In this paper, we extend an existing statistical parsing model to produce richer output parse trees, annotated with PropBank semantic role labels. Our results show that the model can be robustly extended to produce more complex output parse trees without any loss in performance and suggest that joint inference of syntactic and semantic representations is a viable alternative to approaches based on a pipeline of local processing steps. 1 Paola Merlo Department of Linguistics University of Geneva 2 Rue d"
W06-2303,P02-1031,0,0.0252978,"ty in its annotation, at least conceptually. Arguments receiving labels A0-A5 or AA do not express consistent semantic roles and are specific to a verb, while arguments receiving an AM-X label are supposed to be adjuncts and the respective roles they express are consistent across all verbs.1 Recent approaches to learning semantic role labels are based on two-stage architectures. The first stage selects the elements to be labelled, while the second determines the labels to be assigned to the selected elements. While some of these models are based on full parse trees (Gildea and Jurafsky, 2002; Gildea and Palmer, 2002), other methods have been proposed that eschew the need for a full In this paper, we extend an existing statistical parsing model to produce richer output parse trees, annotated with PropBank semantic role labels. Our results show that the model can be robustly extended to produce more complex output parse trees without any loss in performance and suggest that joint inference of syntactic and semantic representations is a viable alternative to approaches based on a pipeline of local processing steps. 1 Paola Merlo Department of Linguistics University of Geneva 2 Rue de Candolle 1211 Geneva 4 S"
W06-2303,J05-1004,0,0.66068,"ring and information extraction. For example, an automatic flight reservation system processing the sentence I want to book a flight from Geneva to Trento will need to know that from Geneva denotes the origin of the flight and to Trento denotes its destination. Knowing that these two phrases are prepositional phrases, the information provided by a syntactic parser, is only moderately useful. The growing interest in learning deeper information is to a large extent supported and due to the recent development of semantically annotated 1 There are thirteen semantic role labels for modifiers. See (Palmer et al., 2005) for a detailed discussion of PropBank semantic roles labels. 11 S H HH  HH  HH     NP-A1 PP  PP  HH H VP H X  XX    H   H XXX  P    HH XXX PP     XXX   HH the government’s borrowing authority  XX   H XXX   H VBD-REL PP-AM-TMP dropped PP-A4 NP-AM-TMP HH IN NP NNP TO at NN Tuesday to midnight PP-A3 HH HH NP IN QP from P  PP $ 2.80 trillion NP QP P  PP $ 2.87 trillion Figure 1: A sample syntactic structure from the PropBank with semantic role annotations. ditional parameters must be estimated. However, given the current limited availability"
W06-2303,gimenez-marquez-2004-svmtool,0,0.0857295,"Missing"
W06-2303,W05-0634,0,0.157389,"tic labels could potentially interact with the distributions of syntactic labels and redefine the boundaries of constituents, thus yielding trees that reflect generalisations over both these sources of information. To our knowledge, no results have yet been published on parsing the PropBank. Accordingly, it is not possible to draw a straigthforward quantitative Discussion These results clearly indicate that our model can perform the PTB parsing task at levels of per3 Such pairs consists of a tag and a word token. No attempt at collecting word types was made. 16 F (Haghighi et al., 2005) 83.4 (Pradhan et al., 2005) 83.3 (Punyakanok et al., 2005) 83.1 (Marquez et al., 2005) 83.1 (Surdeanu and Turmo, 2005) 82.7 PropBank SSN 81.6 R 83.1 83.0 82.8 82.8 82.5 81.3 P 83.7 83.5 83.3 83.3 83.0 81.9 Table 2: Percentage F-measure (F), recall (R), and precision (P) of our Propbank SSN parser and stateof-the-art semantic role labelling systems on the PropBank parsing task (1267 sentences from PropBank validating data sets; Propbank data sets are available at http://www.lsi.upc.edu/ srlconll/st05/st05.html). comparison between our PropBank SSN parser and other PropBank parsers. However, state-ofthe-art semantic role"
W06-2303,W05-0623,0,0.195238,"at the distributions of semantic labels could potentially interact with the distributions of syntactic labels and redefine the boundaries of constituents, thus yielding trees that reflect generalisations over both these sources of information. To our knowledge, no results have yet been published on parsing the PropBank. Accordingly, it is not possible to draw a straigthforward quantitative Discussion These results clearly indicate that our model can perform the PTB parsing task at levels of per3 Such pairs consists of a tag and a word token. No attempt at collecting word types was made. 16 F (Haghighi et al., 2005) 83.4 (Pradhan et al., 2005) 83.3 (Punyakanok et al., 2005) 83.1 (Marquez et al., 2005) 83.1 (Surdeanu and Turmo, 2005) 82.7 PropBank SSN 81.6 R 83.1 83.0 82.8 82.8 82.5 81.3 P 83.7 83.5 83.3 83.3 83.0 81.9 Table 2: Percentage F-measure (F), recall (R), and precision (P) of our Propbank SSN parser and stateof-the-art semantic role labelling systems on the PropBank parsing task (1267 sentences from PropBank validating data sets; Propbank data sets are available at http://www.lsi.upc.edu/ srlconll/st05/st05.html). comparison between our PropBank SSN parser and other PropBank parsers. However, st"
W06-2303,W05-0625,0,0.329325,"interact with the distributions of syntactic labels and redefine the boundaries of constituents, thus yielding trees that reflect generalisations over both these sources of information. To our knowledge, no results have yet been published on parsing the PropBank. Accordingly, it is not possible to draw a straigthforward quantitative Discussion These results clearly indicate that our model can perform the PTB parsing task at levels of per3 Such pairs consists of a tag and a word token. No attempt at collecting word types was made. 16 F (Haghighi et al., 2005) 83.4 (Pradhan et al., 2005) 83.3 (Punyakanok et al., 2005) 83.1 (Marquez et al., 2005) 83.1 (Surdeanu and Turmo, 2005) 82.7 PropBank SSN 81.6 R 83.1 83.0 82.8 82.8 82.5 81.3 P 83.7 83.5 83.3 83.3 83.0 81.9 Table 2: Percentage F-measure (F), recall (R), and precision (P) of our Propbank SSN parser and stateof-the-art semantic role labelling systems on the PropBank parsing task (1267 sentences from PropBank validating data sets; Propbank data sets are available at http://www.lsi.upc.edu/ srlconll/st05/st05.html). comparison between our PropBank SSN parser and other PropBank parsers. However, state-ofthe-art semantic role labelling systems (CoNLL, 2005)"
W06-2303,P98-1087,0,0.0157418,"n such a model are derivation moves. The set of well-formed sequences of derivation moves in this parser is defined by a Predictive LR pushdown automaton (Nederhof, 1994), which implements a form of left-corner parsing strategy. The derivation moves include: projecting a constituent with a specified label, attaching one constituent to another, and shifting a tag-word pair onto the pushdown stack. Unlike standard history-based models, SSN parsers do not state any explicit independence assumptions between derivation steps. They use a neural network architecture, called Simple Synchrony Network (Henderson and Lane, 1998), to induce a finite history representation of an unbounded sequence of moves. The history representation of a parse history d1 , . . . , di−1 , which we denote h(d1 , . . . , di−1 ), is assigned to the constituent that is on the top of the stack before the ith move. The representation h(d1 , . . . , di−1 ) is computed from a set f of features of the derivation move di−1 and from a finite set D of recent history representations h(d1 , . . . , dj ), where j < i − 1. Because the history representation computed for the move i − 1 is included in the inputs to the computation of the representation"
W06-2303,W05-0635,0,0.165517,"define the boundaries of constituents, thus yielding trees that reflect generalisations over both these sources of information. To our knowledge, no results have yet been published on parsing the PropBank. Accordingly, it is not possible to draw a straigthforward quantitative Discussion These results clearly indicate that our model can perform the PTB parsing task at levels of per3 Such pairs consists of a tag and a word token. No attempt at collecting word types was made. 16 F (Haghighi et al., 2005) 83.4 (Pradhan et al., 2005) 83.3 (Punyakanok et al., 2005) 83.1 (Marquez et al., 2005) 83.1 (Surdeanu and Turmo, 2005) 82.7 PropBank SSN 81.6 R 83.1 83.0 82.8 82.8 82.5 81.3 P 83.7 83.5 83.3 83.3 83.0 81.9 Table 2: Percentage F-measure (F), recall (R), and precision (P) of our Propbank SSN parser and stateof-the-art semantic role labelling systems on the PropBank parsing task (1267 sentences from PropBank validating data sets; Propbank data sets are available at http://www.lsi.upc.edu/ srlconll/st05/st05.html). comparison between our PropBank SSN parser and other PropBank parsers. However, state-ofthe-art semantic role labelling systems (CoNLL, 2005) use parse trees output by state-of-the-art parsers (Collins"
W06-2303,I05-1068,0,0.0167977,"onstraints that govern syntactic dependencies, such as argument structure or subcategorization. We attempt to capture such constraints by modelling the c-command relation. Recall that the c-command relation relates two nodes in a tree, even if they are not close to each other, provided that the first node dominating one node also dominate the other. This notion of c-command captures both linear and hierarchical constraints and defines the domain in which semantic role labelling applies. While PTB function labels appear to overlap to a large extent with PropBank semantic rolel labels, work by (Ye and Baldwin, 2005) on semantic labelling prepositional phrases, however, indicates that the function labels in the Penn Treebank are assigned more sporadically and heterogeneously than in PropBank. Apparently only the “easy” cases have been tagged functionally, because assigning these function tags was not the main goal of the annotation. PropBank instead was annotated exhaustively, taking all cases into account, annotating multiple roles, coreferences and discontinuous constituents. It is therefore not void of interest to test our hypothesis that, like function labels, semantic role labels are the interface be"
W06-2303,N03-1014,0,0.256097,"Our results show that the model can be robustly extended to produce more complex output parse trees without any loss in performance and suggest that joint inference of syntactic and semantic representations is a viable alternative to approaches based on a pipeline of local processing steps. 1 Paola Merlo Department of Linguistics University of Geneva 2 Rue de Candolle 1211 Geneva 4 Switzerland merlo@lettres.unige.ch Introduction Recent successes in statistical syntactic parsing based on supervised learning techniques trained on a large corpus of syntactic trees (Collins, 1999; Charniak, 2000; Henderson, 2003) have brought forth the hope that the same approaches could be applied to the more ambitious goal of recovering the propositional content and the frame semantics of a sentence. Moving towards a shallow semantic level of representation is a first initial step towards the distant goal of natural language understanding and has immediate applications in question-answering and information extraction. For example, an automatic flight reservation system processing the sentence I want to book a flight from Geneva to Trento will need to know that from Geneva denotes the origin of the flight and to Tren"
W06-2303,P03-1054,0,0.00817156,"into account, annotating multiple roles, coreferences and discontinuous constituents. It is therefore not void of interest to test our hypothesis that, like function labels, semantic role labels are the interface between syntax and semantics, and they need to be recovered by applying constraints that model both higher level nodes and lower level ones. We assume that semantic roles are very often projected by the lexical semantics of the words in the sentence. We introduce this bottom-up lexical information by fine-grained modelling of semantic role labels. Extending a technique presented in (Klein and Manning, 2003) and adopted in (Merlo and Musillo, 2005; Musillo and Merlo, 2005) for function labels, we split some part-of-speech tags into tags marked with semantic role labels. The semantic role labels attached to a non-terminal directly projected by a preterminal and belonging to a few selected categories (DIR, EXT, LOC, MNR, PNC , CAUS and TMP ) were propagated down to the pre-terminal part-of-speech tag of its head. To affect only labels that are projections of lexical semantics properties, the propagation takes into account the distance of the projection from the lexical head to the label, and distan"
W06-2303,W04-0832,0,0.0189025,"he-art statistical parsing, by extensions that take the nature of the richer labels to be recovered into account. They also suggest that the relationship between syntactic PTB parsing and semantic PropBank parsing is strict enough that an integrated approach to the problem of semantic role labelling is beneficial. In particular, recent models of semantic role labelling separate input indicators of the correlation between the structural position in the tree and the semantic label, such as path, from those indicators that encode constraints on the sequence, such as the previously assigned role (Kwon et al., 2004). In this way, they can never encode directly the constraining power of a certain role in a given structural position onto a following node in its structural position. In our augmented model, we attempt to capture these constraints by directly modelling syntactic domains defined by the notion of c-command. Our results also confirm the findings in (Palmer et al., 2005). They take a critical look at some commonly used features in the semantic role labelling task, such as the path feature. They suggest that the path feature is not very effective because it is sparse. Its sparseness is due to the"
W06-2303,J93-2004,0,0.027332,"Missing"
W06-2303,W05-0628,0,0.293497,"s of syntactic labels and redefine the boundaries of constituents, thus yielding trees that reflect generalisations over both these sources of information. To our knowledge, no results have yet been published on parsing the PropBank. Accordingly, it is not possible to draw a straigthforward quantitative Discussion These results clearly indicate that our model can perform the PTB parsing task at levels of per3 Such pairs consists of a tag and a word token. No attempt at collecting word types was made. 16 F (Haghighi et al., 2005) 83.4 (Pradhan et al., 2005) 83.3 (Punyakanok et al., 2005) 83.1 (Marquez et al., 2005) 83.1 (Surdeanu and Turmo, 2005) 82.7 PropBank SSN 81.6 R 83.1 83.0 82.8 82.8 82.5 81.3 P 83.7 83.5 83.3 83.3 83.0 81.9 Table 2: Percentage F-measure (F), recall (R), and precision (P) of our Propbank SSN parser and stateof-the-art semantic role labelling systems on the PropBank parsing task (1267 sentences from PropBank validating data sets; Propbank data sets are available at http://www.lsi.upc.edu/ srlconll/st05/st05.html). comparison between our PropBank SSN parser and other PropBank parsers. However, state-ofthe-art semantic role labelling systems (CoNLL, 2005) use parse trees output by s"
W06-2303,A00-2018,0,\N,Missing
W06-2303,J03-4003,0,\N,Missing
W06-2303,N06-1024,0,\N,Missing
W06-2303,H05-1102,0,\N,Missing
W08-2101,W05-0624,0,0.0481756,"Missing"
W08-2101,W05-0627,0,0.058664,"Missing"
W08-2101,J93-2004,0,0.0307963,"and limited to the few aspects of interest here. For more detail, explanations and experiments see (Titov and Henderson, 2007). A Bayesian network is a directed acyclic graph that illustrates the statistical dependencies between the random variables describing a set of events (Jensen, 2001). Dynamic networks are Bayesian networks applied to unboundedly long sequences. They are an appropriate model for sequences of derivation steps in The Data Our experiments on joint syntactic and semantic parsing use data that is produced automatically by merging the Penn Treebank (PTB) with PropBank (PRBK) (Marcus et al., 1993; Palmer et al., 2005), as shown in Figure 1. PropBank encodes propositional information by adding a layer of argument structure annotation to the syntactic structures of the Penn Treebank.1 Verbal predicates in the Penn Treebank (PTB) receive a label REL and their arguments are annotated with abstract semantic role labels, such as A0, A1, or AA for those complements of the predicative verb that are considered arguments. Those complements of the verb la1 We use PRBK data as they appear in the CONLL 2005 shared task. 2 t−c S In order to extend this model to learn decisions concerning a joint sy"
W08-2101,A00-2030,0,0.268784,"Missing"
W08-2101,W05-0630,0,0.0292329,"Missing"
W08-2101,W05-1509,1,0.88982,"n and across constituents. These extensions enlarge the locality domain over which dependencies between predicates bearing the REL label, arguments bearing an A0-A5 label, and adjuncts bearing an AM-X role can be specified, and capture both linear and hierarchical constraints between predicates, arguments and adjuncts. Enlarging the locality domain this way ensures for instance that the derivation of the role DIR in Figure 1 is not independent of the derivations of the roles TMP, REL (the predicate) and A0. Second, this version of the Bayesian network tags its sentences internally. Following (Musillo and Merlo, 2005), we split some part-of-speech tags into tags marked with semantic role labels. The semantic role labels attached to a non-terminal directly projected by a preterminal and belonging to a few selected categories (DIR, EXT, LOC, MNR, PRP , CAUS or TMP) are propagated down to the pre-terminal part-of-speech tag of its head.3 This third extension biases the parser to learn the relationship between lexical items, semantic roles and the constituents in which they occur. This technique is illustrated by the bold labels in Figure 1. We compare this augmented model to a simple baseline parser, that doe"
W08-2101,N06-2026,1,0.75884,"ndependent latent variables, referred to as Si . These state vectors encode the probability distributions of features of the history of parsing steps (the features are indicated by sti in Figure 2). As can be seen from the picture, the pattern of inter-connectivity allows previous non-adjacent states to influence future states. Not all states in the history are relevant, however. The interconnectivity is defined dynamically based on the topological structure and the labels of the tree that is being developed. This inter-connectivity depends on a notion of structural locality (Henderson, 2003; Musillo and Merlo, 2006).2 2 stack configuration of the left-corner parser and the derivation tree built so far. The nodes in the partially built tree and stack configuration that are selected to determine the relevant states are the following: top, the node on top of the pushdown stack before the current derivation move; the left-corner ancestor of top (that is, the second top-most node on the parser stack); the leftmost child of top; and the most recent child of top, if any. 3 Exploratory data analysis indicates that these tags are the most useful to disambiguate parsing decisions. Specifically, the conditioning st"
W08-2101,W05-0631,0,0.0523656,"Missing"
W08-2101,J05-1004,0,0.315669,"w aspects of interest here. For more detail, explanations and experiments see (Titov and Henderson, 2007). A Bayesian network is a directed acyclic graph that illustrates the statistical dependencies between the random variables describing a set of events (Jensen, 2001). Dynamic networks are Bayesian networks applied to unboundedly long sequences. They are an appropriate model for sequences of derivation steps in The Data Our experiments on joint syntactic and semantic parsing use data that is produced automatically by merging the Penn Treebank (PTB) with PropBank (PRBK) (Marcus et al., 1993; Palmer et al., 2005), as shown in Figure 1. PropBank encodes propositional information by adding a layer of argument structure annotation to the syntactic structures of the Penn Treebank.1 Verbal predicates in the Penn Treebank (PTB) receive a label REL and their arguments are annotated with abstract semantic role labels, such as A0, A1, or AA for those complements of the predicative verb that are considered arguments. Those complements of the verb la1 We use PRBK data as they appear in the CONLL 2005 shared task. 2 t−c S In order to extend this model to learn decisions concerning a joint syntactic-semantic repre"
W08-2101,W05-0625,0,0.213302,"Missing"
W08-2101,W05-0635,0,0.0361993,"Missing"
W08-2101,P07-1080,0,0.367124,"re 1: A sample syntactic structure with semantic role labels. belled with a semantic functional label in the original PTB receive the composite semantic role label AM-X, where X stands for labels such as LOC, TMP or ADV, for locative, temporal and adverbial modifiers respectively. A tree structure with PropBank labels is shown in Figure 1. (The bold labels are not relevant for the moment and they will be explained later.) 3 The Syntactic and Semantic Parser Architecture To achieve the complex task of joint syntactic and semantic parsing, we extend a current state-of-theart statistical parser (Titov and Henderson, 2007) to learn semantic role annotation as well as syntactic structure. The parser uses a form of left-corner parsing strategy to map parse trees to sequences of derivation steps. We choose this parser because it exhibits the best performance for a single generative parser, and does not impose hard independence assumptions. It is therefore promising for extensions to new tasks. Following (Titov and Henderson, 2007), we describe the original parsing architecture and our modifications to it as a Dynamic Bayesian network. Our description is brief and limited to the few aspects of interest here. For mo"
W08-2101,P07-1121,0,0.105506,"Missing"
W08-2101,D07-1071,0,0.0985103,"Missing"
W08-2101,W05-0602,0,0.135556,"Missing"
W08-2101,J02-3001,0,0.591874,"Missing"
W08-2101,W05-0623,0,0.0573615,"Missing"
W08-2101,N03-1014,0,0.0762803,"tate vectors of independent latent variables, referred to as Si . These state vectors encode the probability distributions of features of the history of parsing steps (the features are indicated by sti in Figure 2). As can be seen from the picture, the pattern of inter-connectivity allows previous non-adjacent states to influence future states. Not all states in the history are relevant, however. The interconnectivity is defined dynamically based on the topological structure and the labels of the tree that is being developed. This inter-connectivity depends on a notion of structural locality (Henderson, 2003; Musillo and Merlo, 2006).2 2 stack configuration of the left-corner parser and the derivation tree built so far. The nodes in the partially built tree and stack configuration that are selected to determine the relevant states are the following: top, the node on top of the pushdown stack before the current derivation move; the left-corner ancestor of top (that is, the second top-most node on the parser stack); the leftmost child of top; and the most recent child of top, if any. 3 Exploratory data analysis indicates that these tags are the most useful to disambiguate parsing decisions. Specifi"
W08-2101,W08-2122,1,0.891083,"Missing"
W08-2101,W05-0639,0,\N,Missing
W08-2101,J03-4003,0,\N,Missing
W08-2101,W06-2303,1,\N,Missing
W08-2122,J93-2004,0,0.0363444,"s statistical parsing and tagging, have recently paved the way to statistical learning techniques for levels of semantic representation, such as recovering the logical form of a sentence for information extraction and question-answering applications (e.g. (Wong and Mooney, 2007)) or jointly learning the syntactic structure of the sentence and the propositional argument-structure of its main predicates (Musillo and Merlo, 2006; Merlo and Musillo, 2008). In this vein, the CoNLL 2008 shared task sets the challenge of learning jointly both syntactic dependencies (extracted from the Penn Treebank (Marcus et al., 1993) ) and semantic dependencies (extracted both from PropBank (Palmer et al., 2005) ∗ c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 0 Authors in alphabetical order. and NomBank (Meyers et al., 2004) under a unified representation. We propose a solution that uses a generative history-based model to predict the most likely derivation of a synchronous dependency parser for both syntactic and semantic dependencies. Our probabilistic model is based on Incremental Sigmoid"
W08-2122,W08-2101,1,0.889757,"Missing"
W08-2122,W04-2705,0,0.277552,"Missing"
W08-2122,N06-2026,1,0.878058,"Missing"
W08-2122,W06-2933,0,0.320816,"e Probability Model Our probability model is a joint generative model of syntactic and semantic dependencies. The two dependency structures are specified as the sequence of actions for a synchronous parser, which requires each dependency structure to be projec178 CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 178–182 Manchester, August 2008 tivised separately. 2.1 Synchronous derivations The derivations for syntactic dependency trees are the same as specified in (Titov and Henderson, 2007b), which are based on the shift-reduce style parser of (Nivre et al., 2006). The derivations use a stack and an input queue. There are actions for creating a leftward or rightward arc between the top of the stack and the front of the queue, for popping a word from the stack, and for shifting a word from the queue to the stack. The derivations for semantic dependency graphs use virtually the same set of actions, but impose fewer constraints on when they can be applied, due to the fact that a word in a semantic dependency graph can have more than one parent. An additional action predicates was introduced to label a predicate with sense s. Let Td be a syntactic dependen"
W08-2122,J05-1004,0,0.0727012,"arning techniques for levels of semantic representation, such as recovering the logical form of a sentence for information extraction and question-answering applications (e.g. (Wong and Mooney, 2007)) or jointly learning the syntactic structure of the sentence and the propositional argument-structure of its main predicates (Musillo and Merlo, 2006; Merlo and Musillo, 2008). In this vein, the CoNLL 2008 shared task sets the challenge of learning jointly both syntactic dependencies (extracted from the Penn Treebank (Marcus et al., 1993) ) and semantic dependencies (extracted both from PropBank (Palmer et al., 2005) ∗ c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 0 Authors in alphabetical order. and NomBank (Meyers et al., 2004) under a unified representation. We propose a solution that uses a generative history-based model to predict the most likely derivation of a synchronous dependency parser for both syntactic and semantic dependencies. Our probabilistic model is based on Incremental Sigmoid Belief Networks (ISBNs), a recently proposed latent variable model for syntactic"
W08-2122,W08-2121,0,0.159555,"Missing"
W08-2122,P07-1080,1,0.599944,"ke 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 0 Authors in alphabetical order. and NomBank (Meyers et al., 2004) under a unified representation. We propose a solution that uses a generative history-based model to predict the most likely derivation of a synchronous dependency parser for both syntactic and semantic dependencies. Our probabilistic model is based on Incremental Sigmoid Belief Networks (ISBNs), a recently proposed latent variable model for syntactic structure prediction, which has shown very good behaviour for both constituency (Titov and Henderson, 2007a) and dependency parsing (Titov and Henderson, 2007b). The ability of ISBNs to induce their features automatically enables us to extend this architecture to learning a synchronous parse of syntax and semantics without modification of the main architecture. By solving the problem with synchronous parsing, a probabilistic model is learnt which maximises the joint probability of the syntactic and semantic dependencies and thereby guarantees that the output structure is globally coherent, while at the same time building the two structures separately. This extension of the ISBN architecture is the"
W08-2122,W07-2218,1,0.911479,"ke 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 0 Authors in alphabetical order. and NomBank (Meyers et al., 2004) under a unified representation. We propose a solution that uses a generative history-based model to predict the most likely derivation of a synchronous dependency parser for both syntactic and semantic dependencies. Our probabilistic model is based on Incremental Sigmoid Belief Networks (ISBNs), a recently proposed latent variable model for syntactic structure prediction, which has shown very good behaviour for both constituency (Titov and Henderson, 2007a) and dependency parsing (Titov and Henderson, 2007b). The ability of ISBNs to induce their features automatically enables us to extend this architecture to learning a synchronous parse of syntax and semantics without modification of the main architecture. By solving the problem with synchronous parsing, a probabilistic model is learnt which maximises the joint probability of the syntactic and semantic dependencies and thereby guarantees that the output structure is globally coherent, while at the same time building the two structures separately. This extension of the ISBN architecture is the"
W08-2122,P07-1121,0,0.0403536,"Missing"
W08-2122,W06-2303,1,\N,Missing
W09-1205,burchardt-etal-2006-salsa,0,0.0412684,"Missing"
W09-1205,W08-2122,1,0.777374,"g. Titov et al. (2009) found that only using the Swap action as a last resort is the best strategy for English (compared to using it preemptively to address future crossing arcs) and we use the same strategy here for all languages. Syntactic graphs do not use a Swap action. We adopt the HEAD method of Nivre and Nilsson (2005) to de-projectivise syntactic dependencies outside of parsing.1 3 Features and New Developments The synchronous derivations described above are modelled with a type of Bayesian Network called an Incremental Sigmoid Belief Network (ISBN) (Titov and Henderson, 2007a). As in Henderson et al. (2008), the ISBN model distinguishes two types of latent states: syntactic states, when syntactic decisions are considered, and semantic states, when semantic decision are considered. Latent states are vectors of binary latent variables, which are conditioned on variables from previous states via a pattern of connecting edges determined by the previous decisions. These latent-to-latent connections are used to engineer soft biases which reflect the relevant domains of locality in the structure being built. For these we used the set of connections proposed in Titov et al. (2009), which includes latent"
W09-1205,kawahara-etal-2002-construction,0,0.0146671,"ment score. All development effort took about two personmonths, mostly by someone who had no previous experience with the system. Most of this time was spent on the above differences in the task definition between the 2008 and 2009 shared tasks. 4 Results and Discussion We participated in the joint task of the closed challenge, as described in Hajiˇc et al. (2009). The datasets used in this challenge are described in Taul´e et al. (2008) (Catalan and Spanish), Palmer and Xue (2009) (Chinese), Hajiˇc et al. (2006) (Czech), Surdeanu et al. (2008) (English), Burchardt et al. (2006) (German), and Kawahara et al. (2002) (Japanese). Rank Average Catalan Chinese Czech 3 82.14 82.66 76.15 83.21 1 @85.77 @87.86 76.11 @80.38 3 78.42 77.44 76.05 86.02 macro F1 syntactic acc semantic F1 English German Japanese Spanish 86.03 79.59 84.91 82.43 88.79 87.29 92.34 @87.64 83.24 71.78 77.23 77.19 Table 2: The three main scores for our system. Rank is within task. Rank macro F1 syn Acc sem F1 3 2 3 Ave 75.93 78.01 73.63 Cze-ood Eng-ood Ger-ood @80.70 75.76 71.32 @76.41 80.84 76.77 84.99 70.65 65.25 Table 3: Results on out-of-domain for our system. Rank is within task. The official results on the testing set are shown in ta"
W09-1205,P08-1069,0,0.0241132,"Missing"
W09-1205,P05-1013,0,0.0314403,"Missing"
W09-1205,W06-2933,0,0.024019,"Missing"
W09-1205,W08-2121,0,0.154607,"Missing"
W09-1205,taule-etal-2008-ancora,0,0.083404,"Missing"
W09-1205,P07-1080,1,0.811623,"een previously imple1 The statistics in Table 1 suggest that, for some languages, swapping might be beneficial for syntax as well. 39 mented. For the former modifications, the system was adapted to allow the use of the PFEAT and FILLPRED fields in the data, which both resulted in improved accuracy for all the languages. The PFEAT data field (automatically predicted morphological features) was introduced in the system in two ways, as an atomic feature bundle that is predicted when predicting the word, and split into its elementary components when conditioning on a previous word, as was done in Titov and Henderson (2007b). Because the testing data included a specification of which words were annotated as predicates (the FILLPRED data field), we constrained the parser’s output so as to be consistent with this specification. For rare predicates, if the predicate was not in the parser’s lexicon (extracted from the training set), then a sense was taken from the list of senses reported in the Lexicon and Frame Set resources available for the closed challenge. If this information was not available, then a default sense was constructed based on the automatically predicted lemma (PLEMMA) of the predicate. We also ma"
W09-1205,D07-1099,1,0.816134,"een previously imple1 The statistics in Table 1 suggest that, for some languages, swapping might be beneficial for syntax as well. 39 mented. For the former modifications, the system was adapted to allow the use of the PFEAT and FILLPRED fields in the data, which both resulted in improved accuracy for all the languages. The PFEAT data field (automatically predicted morphological features) was introduced in the system in two ways, as an atomic feature bundle that is predicted when predicting the word, and split into its elementary components when conditioning on a previous word, as was done in Titov and Henderson (2007b). Because the testing data included a specification of which words were annotated as predicates (the FILLPRED data field), we constrained the parser’s output so as to be consistent with this specification. For rare predicates, if the predicate was not in the parser’s lexicon (extracted from the training set), then a sense was taken from the list of senses reported in the Lexicon and Frame Set resources available for the closed challenge. If this information was not available, then a default sense was constructed based on the automatically predicted lemma (PLEMMA) of the predicate. We also ma"
W09-1205,D07-1096,0,\N,Missing
W10-1814,C08-1085,0,0.0912828,"Missing"
W10-1814,J05-1004,0,0.329392,"to be useful in many applications and for many text genres. The belief that this layer of meaning which is preserved across languages can be formally represented and automatically calculated underlies methods that use parallel corpora for the automatic generation of semantic annotations through crosslingual transfer (Pad´o, 2007; Basili et al., 2009). A methodology similar in spirit — re-use of the existing resources in a different language — has also been applied in developing manually annotated resources. Monachesi et al. (2007) annotate Dutch sentences using the PropBank annotation scheme (Palmer et al., 2005), while Burchardt et al. (2009) use the FrameNet framework (Fillmore et al., 2003) to annotate a German corpus. Instead of building special lexicons containing the specific semantic information needed for the annotation for each language separately, which is a complex and time-consuming endeavour in itself, these approaches rely on the lexicons already developed for English. In this paper, we hypothesize that the level of abstraction that is necessary to develop a semantic lexicon/ontology for a single language 113 Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 113–1"
W10-1814,W09-3007,0,0.0583874,"Missing"
W10-1814,W07-2218,0,0.175555,"Missing"
W10-1814,burchardt-etal-2006-salsa,0,0.0862519,"Missing"
W10-1814,2009.jeptalnrecital-long.4,0,0.311795,"Missing"
W10-1814,2005.mtsummit-papers.11,0,0.0341546,"succeeding or working out Annotation Procedure Annotators have access to PropBank frame files and guidelines adapted for the current task. The frame files provide verb-specific descriptions of all possible semantic roles and illustrate these roles with examples as shown for the verb paid in (1) and the verb senses of pay in Table 1. Annotators need to look up each verb in the frame files to be able to label it with the right verb sense and to be able to allocate the arguments consistently. 2.3 Corpus We selected the French sentences for the manual annotation from the parallel Europarl corpus (Koehn, 2005). Because translation shifts are known to pose problems for the automatic crosslingual transfer of semantic roles (Pad´o, 2007) and for machine translation (Ozdowska and Way, (1) [A0 The Latin American nation] has [REL−P AY.01 paid] [A1 very little] [A3 on its debt] [AM −T M P since early last year]. 114 2009), and these are more likely to appear in indirect translations, we decided to select only those parallel sentences, for which we can infer from the labels used in Europarl that they are direct translations from English to French, or vice versa. We selected 1040 sentences for annotation (4"
W10-1814,P09-1033,1,0.825514,"Missing"
W10-1814,W07-1513,0,0.356023,"e lost in translation. Yet, translation preserves enough meaning across language pairs to be useful in many applications and for many text genres. The belief that this layer of meaning which is preserved across languages can be formally represented and automatically calculated underlies methods that use parallel corpora for the automatic generation of semantic annotations through crosslingual transfer (Pad´o, 2007; Basili et al., 2009). A methodology similar in spirit — re-use of the existing resources in a different language — has also been applied in developing manually annotated resources. Monachesi et al. (2007) annotate Dutch sentences using the PropBank annotation scheme (Palmer et al., 2005), while Burchardt et al. (2009) use the FrameNet framework (Fillmore et al., 2003) to annotate a German corpus. Instead of building special lexicons containing the specific semantic information needed for the annotation for each language separately, which is a complex and time-consuming endeavour in itself, these approaches rely on the lexicons already developed for English. In this paper, we hypothesize that the level of abstraction that is necessary to develop a semantic lexicon/ontology for a single language"
W10-1814,2009.eamt-1.14,0,0.0299566,"Missing"
W10-2108,W04-0401,0,0.38028,"ine-grained analysis of the statistical behaviour of these constructions provides linguistically-informed performance and error analyses that can be used to improve aligners. 2 verbs is necessarily more complicated, since they are not direct translations of each other and cannot be expected to be aligned in other contexts.1 However, the difference between the two types of light verb constructions is not clear cut. They are better seen as two ends of a continuum of verb usages with different degrees of verbs’ lightness and different degrees of compositionality of the meaning of constructions. (Stevenson et al., 2004; Butt and Geuder, 2001; Grimshaw and Mester, 1988). Even though several English verbs have been identified as having light usages (e.g. take, make, have, give, pay), there has been little research on the influence that the properties of the heading light verb can have on the degree of semantic compositionality of the construction. The purpose of the present research is to examine the German translation equivalents of the range of different English light verb constructions occurring in a parallel corpus and study the differential performance of a standard aligner on this language pair for thes"
W10-2108,P05-1066,0,0.0425891,"Missing"
W10-2108,2005.mtsummit-papers.11,0,0.0174375,"m the word-aligned parallel corpus. The constructions are represented as ordered pairs of words, where the first word is the verb that heads the construction and the second is the noun that heads the verb’s complement. Only the constructions where the complement is the direct object were included in the analysis.2 Materials and Methods A random sample of instances of each of the defined types of construction was extracted from a large word-aligned parallel corpus and manually examined. 3.1.1 Corpus The instances of the phrases were taken from the English-German portion of the Europarl corpus (Koehn, 2005) that contains the proceedings of the sessions held in 1999, irrespective of the source language and of the direction of translation. Before sampling, the corpus was word-aligned using GIZA++ (Och and Ney, 2003). Alignments were performed in both directions, with German as the target language and with English as the target language. 3.1.2 Sampling phrase instances 3.1.4 Data collection The following data were collected for each occurrence of the English word pairs. The word or words in the German sentence that are actual translation of the English words were identified. If either the English o"
W10-2108,J03-1002,0,0.102893,"a.Merlo@unige.ch Abstract language (usually English) (Pad´o, 2007; Basili et al., 2009). Such resources can be used for training systems for automatic parsing for different languages. Recently, parallel multilingual corpora have also been used to improve performance in mono-lingual tasks (Snyder et al., 2009). For most of these applications, the aligned sentences in the parallel corpora need to be analysed into smaller units (phrases and words), which, in turn, need to be aligned. Although crucial for successful use of parallel corpora, word (and phrase) alignment is still a challenging task (Och and Ney, 2003; Collins et al., 2005; Pad´o, 2007). Our research concentrates on one type of construction that needs a special treatment in the task of aligning corpora and projecting linguistic annotation from one language to another, namely light verb constructions. These constructions, usually identified as paraphrases of verbs (e.g. have a laugh means laugh, give a talk means talk), are frequent, cross-lingually productive forms, where simple-minded parallelism often breaks down. Their meaning is partially uncompositional, formed in a conventional way, which means that they cannot be analysed as regular"
W10-2108,N09-1010,0,0.0262067,"Missing"
W14-0706,J04-1003,0,0.0174041,"auser (Adam in (1a)). The causative alternation has been attested in almost all languages (Schafer, 2009), but it is realised with considerable cross-linguistic variation in the sets of alternating verbs and in the grammatical encoding (Alexiadou et al., 2006; Alexiadou, 2010). Since the causative alternation involves most verbs, identifying the properties of verbs which allow them to alternate is important for developing representations of the meaning of verbs in general. Analysing the structural components of the meaning of verbs proves important for tasks such as word sense disambiguation (Lapata and Brew, 2004), semantic role labelling (M`arquez et al., 2008), cross-linguistic transfer of semantic annotation (Pad´o and Lapata, 2009; Fung et al., 2007; van der Plas et al., 2011). The knowledge about the likelihood of external causation might be helpful in the task of detecting implicit arguments of verbs and, especially deverbal nouns (Gerber and Chai, 2012; Roth and Frank, 2012). Knowing, for example, that a verb expresses an externally caused event increases the probability of an implicit causer if an explicit causer is not detected in a particular instance of the verb. Our study should Introductio"
W14-0706,J08-2001,0,0.0197786,"Missing"
W14-0706,bouma-etal-2010-towards,0,0.0138605,"e word-aligned with the alternating English verbs listed in the literature are regarded as German equivalents. By extracting cross-linguistic equivalents automatically from a parallel corpus, we avoid manual translation into German of the lists of English verbs discussed in the literature. In this way, we eliminate the judgements which would be involved in the process of translation. The corpus is syntactically parsed (using the MaltParser (Nivre et al., 2007)) and word-aligned (using GIZA++ (Och and Ney, 2003)). For both the syntactic parses and word alignments, we reuse the data provided by Bouma et al. (2010). We extract only the instances of verbs where both the object (if there is one) and the subject are realised in the same clause, excluding the instances involving syntactic movements and coreference. Transitive instances are considered causative realisations, intransitive anticausative. We count passive instances separately because they are formally transitive, but they usually do not express the causer. German equivalents of English alternating verbs are extracted in two steps. First, all verbs occurring as transitive, intransitive, and passive were extracted from the German sentences that a"
W14-0706,J01-3003,1,0.662273,"groups of verbs was not confirmed, the corpus data were still found to support the distinction between the two groups. Examining 50 randomly selected instances of transitive uses of each of the studied verbs, McKoon and Macfarland (2000) find that, when used in a transitive clause, internally caused change-of-state verbs tend to occur with a limited set of subjects, while externally caused verbs can occur with a wider range of subjects. This difference is statistically significant. The relation between frequencies of certain uses and the lexical semantics of English verbs has been explored by Merlo and Stevenson (2001) in the context of automatic verb classification. Merlo and Stevenson (2001) show that information collected from instances of verbs in a corpus can be used to distinguish between three different classes which all include verbs that alternate between transitive and intransitive use. The classes in question are manner of motion verbs (10), which alternate only in a limited number of languages, externally caused change of state verbs (11), alternating across languages, and performance/creation verbs, which are not lexical causatives (12). 7 Conclusion and Future Work The experiments presented in"
W14-0706,2007.tmi-papers.10,0,0.0315622,"ss-linguistic variation in the sets of alternating verbs and in the grammatical encoding (Alexiadou et al., 2006; Alexiadou, 2010). Since the causative alternation involves most verbs, identifying the properties of verbs which allow them to alternate is important for developing representations of the meaning of verbs in general. Analysing the structural components of the meaning of verbs proves important for tasks such as word sense disambiguation (Lapata and Brew, 2004), semantic role labelling (M`arquez et al., 2008), cross-linguistic transfer of semantic annotation (Pad´o and Lapata, 2009; Fung et al., 2007; van der Plas et al., 2011). The knowledge about the likelihood of external causation might be helpful in the task of detecting implicit arguments of verbs and, especially deverbal nouns (Gerber and Chai, 2012; Roth and Frank, 2012). Knowing, for example, that a verb expresses an externally caused event increases the probability of an implicit causer if an explicit causer is not detected in a particular instance of the verb. Our study should Introduction Ubiquitously present in human thinking, causality is encoded in language in various ways. Computational approaches to causality are mostly c"
W14-0706,J12-4003,0,0.0192184,"rties of verbs which allow them to alternate is important for developing representations of the meaning of verbs in general. Analysing the structural components of the meaning of verbs proves important for tasks such as word sense disambiguation (Lapata and Brew, 2004), semantic role labelling (M`arquez et al., 2008), cross-linguistic transfer of semantic annotation (Pad´o and Lapata, 2009; Fung et al., 2007; van der Plas et al., 2011). The knowledge about the likelihood of external causation might be helpful in the task of detecting implicit arguments of verbs and, especially deverbal nouns (Gerber and Chai, 2012; Roth and Frank, 2012). Knowing, for example, that a verb expresses an externally caused event increases the probability of an implicit causer if an explicit causer is not detected in a particular instance of the verb. Our study should Introduction Ubiquitously present in human thinking, causality is encoded in language in various ways. Computational approaches to causality are mostly concerned with automatic extraction of causal schemata (Michotte, 1963; Tversky and Kahneman, 1982; Gilovich et al., 1985) from spontaneously produced texts based on linguistic encoding. A key to success in this"
W14-0706,J03-1002,0,0.00347439,"monolingual and bilingual input data are extracted from the parallel corpus. All German verbs which are word-aligned with the alternating English verbs listed in the literature are regarded as German equivalents. By extracting cross-linguistic equivalents automatically from a parallel corpus, we avoid manual translation into German of the lists of English verbs discussed in the literature. In this way, we eliminate the judgements which would be involved in the process of translation. The corpus is syntactically parsed (using the MaltParser (Nivre et al., 2007)) and word-aligned (using GIZA++ (Och and Ney, 2003)). For both the syntactic parses and word alignments, we reuse the data provided by Bouma et al. (2010). We extract only the instances of verbs where both the object (if there is one) and the subject are realised in the same clause, excluding the instances involving syntactic movements and coreference. Transitive instances are considered causative realisations, intransitive anticausative. We count passive instances separately because they are formally transitive, but they usually do not express the causer. German equivalents of English alternating verbs are extracted in two steps. First, all v"
W14-0706,J05-1004,0,0.00676105,"rivaz, 2012). 40 Proceedings of the EACL 2014 Workshop on Computational Approaches to Causality in Language, pages 40–47, c Gothenburg, Sweden, April 26, 2014. 2014 Association for Computational Linguistics which does not mark the alternation morphologically (note that the two versions of English verbs in (1-3) are morphologically identical), other languages encode the alternation in different ways, as shown in (3). contribute to the development of formal and extensive representations of grammatically relevant semantic properties of verbs, such as Verb Net (Kipper Schuler, 2005) and PropBank (Palmer et al., 2005). 2 External Causation and the Grammar of Language Mongolian The distinction between external and internal causation in events described by verbs is introduced by Levin and Rappaport Hovav (1994) to account for the fact that the alternation is blocked in some verbs such as bloom in (2). In Levin and Rappaport Hovav’s account, verbs which describe externally caused events alternate (1), while verbs which describe internally caused events do not (2). (3) Russian Japanese Causative xajl-uul-ax ’melt’ rasplavit ’melt’ atum-eru ’gather’ Anticausative xajl-ax ’melt’ rasplavit-sja ’melt’ atum-aru ’ga"
W14-0706,2005.mtsummit-papers.11,0,0.0145272,"ch we estimate the likelihood are the 354 verbs that participate in the causative alternation in English, as listed by Levin (1993), and the 26 verbs listed as alternating in a typological study (Haspelmath, 1993). We estimate the parameters of the model by implementing the expectation-maximisation algorithm. The algorithm is initialised by assigning different arbitrary values to the parameters of the model. The classification reported in the paper is obtained after 100 iterations. We train the classifier using the data automatically extracted from an English-German parallel corpus (Europarl (Koehn, 2005)). Both monolingual and bilingual input data are extracted from the parallel corpus. All German verbs which are word-aligned with the alternating English verbs listed in the literature are regarded as German equivalents. By extracting cross-linguistic equivalents automatically from a parallel corpus, we avoid manual translation into German of the lists of English verbs discussed in the literature. In this way, we eliminate the judgements which would be involved in the process of translation. The corpus is syntactically parsed (using the MaltParser (Nivre et al., 2007)) and word-aligned (using"
W14-0706,S12-1030,0,0.0129666,"low them to alternate is important for developing representations of the meaning of verbs in general. Analysing the structural components of the meaning of verbs proves important for tasks such as word sense disambiguation (Lapata and Brew, 2004), semantic role labelling (M`arquez et al., 2008), cross-linguistic transfer of semantic annotation (Pad´o and Lapata, 2009; Fung et al., 2007; van der Plas et al., 2011). The knowledge about the likelihood of external causation might be helpful in the task of detecting implicit arguments of verbs and, especially deverbal nouns (Gerber and Chai, 2012; Roth and Frank, 2012). Knowing, for example, that a verb expresses an externally caused event increases the probability of an implicit causer if an explicit causer is not detected in a particular instance of the verb. Our study should Introduction Ubiquitously present in human thinking, causality is encoded in language in various ways. Computational approaches to causality are mostly concerned with automatic extraction of causal schemata (Michotte, 1963; Tversky and Kahneman, 1982; Gilovich et al., 1985) from spontaneously produced texts based on linguistic encoding. A key to success in this endeavour is understan"
W14-0706,P11-2052,1,0.906471,"Missing"
W15-2115,P11-1089,0,0.0426888,"Missing"
W15-2115,J11-1007,0,0.0924263,"ill be easier to parse than a tree with a large overall dependency length, and that a projective tree will be easier than a non-projective tree. Given our corpus, which is annotated with the same annotation scheme for all texts, we have an opportunity to test this hypothesis on texts that constitute truly controlled minimal pairs for such analysis. The parsing results we report here are obtained using the Mate parser (Bohnet, 2010). Graphbased parsers like Mate do not have architectural constraints on handling non-projective trees and have been shown to be robust at parsing long dependencies (McDonald and Nivre, 2011). Given the high percentage of non-projective arcs and the number of long dependencies in the Latin and Ancient Greek corpora, we expect a graphbased parser to perform better than other types of dependency parsers. On a random trainingtesting split for all our, Mate parser shows the best performance among several of the dependency parsers we tested, including the transitionbased Malt parser (Nivre et al., 2006). We test several training and testing configurations. Since it is not clear how to evaluate a parser to compare texts with different rates of word order freedom, we used two different s"
W15-2115,W07-2216,0,0.0706948,"2006). 5 Word order flexibility and parsing performance The previous section confirms through a globally optimised measure, what is already visible in the diachronic evolution of the adjacency measure in Table 2: older Latin and Ancient Greek texts exhibit longer dependencies and freer word order than later texts. It is often claimed that parsing freer-order languages is harder. Specifically, parsers learn locally contained structures better and have more problems recovering long distance dependencies (Nivre et al., 2010). Handling non-projective dependencies is another long-standing problem (McDonald and Satta, 2007). We investigate the source of these difficulties, by correlating parsing performance on our texts from different time periods to our free word order measures. It is straight-forward to hypothesise that a tree with a small overall dependency length will be easier to parse than a tree with a large overall dependency length, and that a projective tree will be easier than a non-projective tree. Given our corpus, which is annotated with the same annotation scheme for all texts, we have an opportunity to test this hypothesis on texts that constitute truly controlled minimal pairs for such analysis."
W15-2115,P81-1022,0,0.734831,"Missing"
W15-2115,W15-2112,0,0.301045,"ses, they also vary in how fixed and uniform the orders are. We speak of fixed-order languages and free word order languages. Free word order has been associated in the linguistic literature with other properties, such as richness of morphology, for example. In natural language processing, it is often claimed that parsing freer word order languages is more difficult, for instance, than parsing English, whose word order is quite fixed. Quantitative measures of word order freedom and investigations of it on a sufficiently large scale to draw firm conclusions, however, are not common (Liu, 2010; Futrell et al., 2015b). To be able to study word order flexibility quantitatively and computationally, we need a syntactic representation that is appropriate for both fixed and flexible 1 Regarding the diachronic change in word order freedom, Tily (2010) found that in the change from Old to Middle and Modern English, the verb-headed clause changed considerably in word order and dependency length, from verb-final to verb initial, while the domain of the noun phrase did not. 121 Proceedings of the Third International Conference on Dependency Linguistics (Depling 2015), pages 121–130, Uppsala, Sweden, August 24–26 2"
W15-2115,C10-1094,0,0.138459,"Missing"
W15-2115,N09-1038,0,0.0221651,"gure 2. Note that two sentences with the same unordered tree structure will have the same optimal dependency lengths.4 If such sentences have different actual dependency lengths, this must then be directly attributed to the differences in their word order. We can generalise this observation to the structural descriptions of languages that For each language, we tested whether the pairwise differences between DL − OptDL trends are significant by fitting the linear regressions log(DL − OptDL + 1) ∼ log(Sent) for two texts 3 We do not impose any constraints on the random permutation of words. See Park and Levy (2009) for an empirical study of different randomisation strategies for the estimation of minimal dependency length with projectivity constraints. 4 Also, two sentences with the same number of words will have the same random dependency lengths (on average). 5 Since the optimal and random dependency length values depend (non-linearly) on the sentence length n, it is customary to analyse them as functions DL(n) (and E[DL(n)]) and not as global averages over all sentences in a treebank (Ferreri-Cancho and Liu, 2014). 126 empirically a theoretical observation of Ferrer-iCancho (2006). 5 Word order flexi"
W15-2115,P07-1024,0,0.355667,". Their actual dependency lengths, on the contrary, are more variable. If we define the DLM score as the difference between the optimal and the actual dependency length, DL − OptDL, we observe a diachronic pattern aligned with the non-projectivity trends from the previous section. The patterns are shown in Figures 4 and 5, where for the sake of readability, we have plotted DL − OptDL against the sentence length in log-log space. culate the new random dependency length preserving the original unordered tree structure.3 The optimal dependency length is calculated using the algorithm proposed by Gildea and Temperley (2007). Given an unordered dependency tree spanning over a sentence, the algorithm outputs the ordering of words which gives the minimal overall dependency length. Roughly, the algorithm implements the DLM tendencies widely observed in natural languages: if a head has several children, these are placed on both sides of the head; shorter children are closer to the head than longer ones; the order of the output is fully projective. Gildea and Temperley (2007) prove the optimality of the algorithm. For instance, the optimal ordering of the tree in Figure 1 would yield the dependency length of 6, as can"
W15-2115,P15-2078,1,0.391753,"Missing"
W15-2115,zeman-etal-2012-hamledt,0,0.0603245,"Missing"
W15-2115,nivre-etal-2006-maltparser,0,\N,Missing
W15-2125,J07-4004,0,0.028252,"nt constructions need to be mentioned, because they have special chracteristics that had to be taken into account: coordination, right node raising and small clauses. A dependency may be found directly, as a single arc, or by coordination. Regarding coordina5 Results and Discussion Automatic and manual results (percent recall) are shown in Table 1, where we compare our results to the relevant ones of those reported in previous evaluations (Rimell et al., 2009; Nivre et al., 2010; Nguyen et al., 2012).8 These papers compare several statistical parsers. Some parsers like Nguyen, the C&C parser (Clark and Curran, 2007) and Enju (Miyao and Tsujii, 2005) are based on rich grammatical formalisms, and others others are representative of statistical dependency parsers (MST, MALT, (McDonald, 2006; Nivre et al., 2006)). (1993)’s semantic propositions of alternating verbs. PropBank propositions have been shown to be closely related to grammatical functions (Merlo and van der Plas, 2009). So we can assume that grammatical functions can also be inferred from PropBank relations in most cases. 8 All these evaluations, like ours, can report only recall, because of the nature of the output of the parsers, which do not ex"
W15-2125,de-marneffe-etal-2014-universal,0,0.108399,"Missing"
W15-2125,W08-2101,1,0.820339,"dependencies and thereby guarantees that the output structure is globally coherent, while at the same time building the two structures separately. The probabilistic estimation is based on Incremental Sigmoid Belief Networks (ISBNs). The use of latent variables allows ISBNs to induce their feaParsing Two-level Representations Developing models to learn these two-level analyses of syntax and argument structure raises several interesting questions regarding the design of the interface between the syntactic and the argument structure representations and how to learn these complex representations (Merlo and Musillo, 2008; Surdeanu et al., 2008).4 A model that can parse these two leveldependencies is proposed in Henderson et al. (2013) and we adopt it here without modifications. We choose this model for our evaluation of 3 These representations are the same, in practice, as the encoding used in some recent shared tasks (CoNLL 2008 and CoNLL 2009 (Surdeanu et al., 2008; Hajiˇc et al., 2009)) for syntactic-semantic dependencies. 4 Joint syntactic-semantic dependency parsing was the theme of two CoNLL shared tasks. CoNLL 2008 explored syntactic-semantic parsing for English, CoNLL 2009 extended the task to several"
W15-2125,W09-1205,1,0.824769,"all LDDs, enriched with substantive semantic role labels, according to the PropBank labelling scheme.3 They could also be constructed from other resources, for example by augmenting the current Universal dependency annotation scheme with extra semantic annotations (de Marneffe et al., 2014). 3 long-distance dependencies as the best performing among those approaches that have attempted to model jointly the relationship between argument structure and surface syntax (Llu´ıs and M`arquez, 2008; Surdeanu et al., 2008) and developments of this model have shown good performance on several languages (Gesmundo et al., 2009), without any language-specific tailoring. These results suggest that this model can capture abstract linguistic regularities in a single parsing architecture.5 We describe this model here very briefly. For more detail on the parser and the model, we refer the interested reader to Henderson et al. (2013) and references therein. The crucial intuitions behind the two-level approach is that the parsing mechanism must correlate the two half-graphs, but allow them to be constructed separately as they have very different properties. The derivations for both syntactic dependency trees are based on a"
W15-2125,P09-1033,1,0.883868,"Missing"
W15-2125,P05-1011,0,0.0298882,"ned, because they have special chracteristics that had to be taken into account: coordination, right node raising and small clauses. A dependency may be found directly, as a single arc, or by coordination. Regarding coordina5 Results and Discussion Automatic and manual results (percent recall) are shown in Table 1, where we compare our results to the relevant ones of those reported in previous evaluations (Rimell et al., 2009; Nivre et al., 2010; Nguyen et al., 2012).8 These papers compare several statistical parsers. Some parsers like Nguyen, the C&C parser (Clark and Curran, 2007) and Enju (Miyao and Tsujii, 2005) are based on rich grammatical formalisms, and others others are representative of statistical dependency parsers (MST, MALT, (McDonald, 2006; Nivre et al., 2006)). (1993)’s semantic propositions of alternating verbs. PropBank propositions have been shown to be closely related to grammatical functions (Merlo and van der Plas, 2009). So we can assume that grammatical functions can also be inferred from PropBank relations in most cases. 8 All these evaluations, like ours, can report only recall, because of the nature of the output of the parsers, which do not explicitly label a dependency with a"
W15-2125,C12-1130,0,0.0272831,"Missing"
W15-2125,nivre-etal-2006-maltparser,0,0.367601,"ts suggest that this model can capture abstract linguistic regularities in a single parsing architecture.5 We describe this model here very briefly. For more detail on the parser and the model, we refer the interested reader to Henderson et al. (2013) and references therein. The crucial intuitions behind the two-level approach is that the parsing mechanism must correlate the two half-graphs, but allow them to be constructed separately as they have very different properties. The derivations for both syntactic dependency trees are based on a standard transition-based, shift-reduce style parser (Nivre et al., 2006). The derivations for argument structure dependency graphs use virtually the same set of actions, but are augmented with a Swap action, that swaps the two words at the top of the stack. The Swap action is inspired by the planarisation algorithm described in Hajicova et al.(2004), where non-planar trees are transformed into planar ones by recursively rearranging their sub-trees to find a linear order of the words for which the tree is planar. The probability model to determine which action to pursue is a joint generative model of syntactic and argument structure dependencies. The two dependency"
W15-2125,C10-1094,0,0.608153,"Missing"
W15-2125,C08-1085,0,0.0330344,"Missing"
W15-2125,J13-4006,1,0.951099,"LP applications hinges on the assumption that what needs recovering is the lexical semantics content. For example, it is likely that for information extraction, it is more useful to know which are the manner, temporal and location arguments than to know an underspecified adverbial modifier label. In the rest of the paper, then, we will first contrast the one-level representation of long-distance dependencies to a two-level representation, where grammatical functions and argument structure are both explicitly represented. We will then briefly recall a recently proposed two-level parsing model (Henderson et al., 2013), and then present the main contribution of the paper: the evaluation of parsing models that parse these twolevel syntactic-semantic dependencies on longdistance dependencies. We also compare the results to other statistical dependency parsers, investigate the usefulness and informativeness of the extracted information, discuss and conclude. 2 W hati did William hiti with his arrow? (2) Relative clauses This is the applei that William hiti with his arrow. Figure 1: LDDs and their coindexed antecedenttrace representation. pobj det rcmod amod ... prep nsubj xcomp prt pobj for the many actions 3"
W15-2125,J05-1004,0,0.126088,"one-to-one to arcs in the syntactic graph, indicating that a rather flexible framework is needed to capture the correlations between graphs. The challenge, then, arises in developing models of these two-level representations. These models must find an effective way of communicating the necessary information between the syntax and the argument structure representation. From the practical point of view of existing resources, one version of these representations results from the merging of widely used and carefully annotated linguistic resources, PennTreebank (Marcus et al., 1993) and PropBank (Palmer et al., 2005). They are PennTreebank-derived dependency representations that have been stripped of long-distance dependencies, and merged with PropBank encoding of argument structures. But PropBank encodings are often based on the traceenriched PennTreeBank representations as a starting point. Hence, these representations encode all LDDs, enriched with substantive semantic role labels, according to the PropBank labelling scheme.3 They could also be constructed from other resources, for example by augmenting the current Universal dependency annotation scheme with extra semantic annotations (de Marneffe et a"
W15-2125,P02-1018,0,0.0672327,"obj Figure 2: LDDs represented as a syntactic dependency tree labeled with grammatical relations. Recall that the LDD encoded in the arcs under the sentence are the LDD that must be recovered. They are shown for expository purpose and they are not usually part of the syntactic tree. parse tree, either as co-indexed “traces”, such as in the Penn Treebank, as illustrated in Figure 1, or as arcs as in a dependency representation. In practice, current statistical parsers do not encode LDD directly, as illustrated in Figure 2, and leave it to post-processing procedures to recover the LDD relation (Johnson, 2002; Nivre et al., 2010). These approaches exploit the very strong constraints that govern long-distance relations syntactically, and ignore the full or partial recovery of the semantic roles entirely. Consider, for example, the representations for subject embeddings (first tree) and object reduced relatives (second tree) in Figure 2. This figure illustrates the Stanford dependency representation that was used in Rimmel et al. (2009), and Nivre et al. (2010), indicating below the sentence the long distance dependency that needs to be recovered, but that is not in the representation. The first tre"
W15-2125,D09-1085,0,0.0651088,"is trained on the data derived by merging a dependency transformation of the Penn Treebank with Propbank and Nombank (Surdeanu et al., 2008). An illustrative example of the kind of labelled structures that we need to parse was given in Figure 3. Training and development data follow the usual partition as sections 02-21, 24 of the Penn Treebank. More details and references on the data, and the conversion of the Penn Treebank format to dependencies are given in Surdeanu et al. (2008). Figure 4: Sentences exemplifying the different constructions involving LDDs, used in the test set developed by Rimell et al. (2009). tures automatically. 4 Experiments In this section we assess how well the two-level parser performs on constructions involving longdistance dependencies. In so doing, we verify that these two-level models of syntactic and argument structure representations can be learnt even in difficult cases, while also producing an output that is richer than what statistical parsers usually produce. To confirm this statement, we expect to see that the syntactic dependency parsing performance is not degraded, compared to more standard statistical parsing architectures on long-distance dependencies, while a"
W15-2125,W08-2124,0,0.0297594,"Missing"
W15-2125,W08-2121,0,0.0747072,"Missing"
W15-2125,J93-2004,0,0.0516823,"he semantic graph do not correspond one-to-one to arcs in the syntactic graph, indicating that a rather flexible framework is needed to capture the correlations between graphs. The challenge, then, arises in developing models of these two-level representations. These models must find an effective way of communicating the necessary information between the syntax and the argument structure representation. From the practical point of view of existing resources, one version of these representations results from the merging of widely used and carefully annotated linguistic resources, PennTreebank (Marcus et al., 1993) and PropBank (Palmer et al., 2005). They are PennTreebank-derived dependency representations that have been stripped of long-distance dependencies, and merged with PropBank encoding of argument structures. But PropBank encodings are often based on the traceenriched PennTreeBank representations as a starting point. Hence, these representations encode all LDDs, enriched with substantive semantic role labels, according to the PropBank labelling scheme.3 They could also be constructed from other resources, for example by augmenting the current Universal dependency annotation scheme with extra sem"
W15-2125,W09-1201,0,\N,Missing
W16-4505,P14-1023,0,0.0270579,"hat “study” and “studies” are very similar, while “study” and “play” are not very similar. • Vector of “study” is [0.1049, -0.1103, ..., 0.0752] • Vector of “studies” is [0.0035, -0.0799, ..., 0.1178] • Vector of “play” is [-0.0250, 0.0531 ..., 0.0759] • Similarity score of “study” and “studies”: 0.534 1 https://code.google.com/p/word2vec/ 34 • Similarity score of “study” and “play”: 0.058 Word2Vec provides two embedding algorithms, Skip-Gram and Continuous Bag-of-Words (CBOW). The study of Levy et al. (2015) and Mikolov et al. (2013) show that Skip-Gram better represents word similarity, but Baroni et al. (2014) show the opposite. In our study, we will use both of them, and try to find the better one for our modifications of BLEU and WER. Our Python program uses the Gensim package2 for implementing the trained word embeddings. The code of our modified measures is provided on the Github page3 . 2.1 Modification for BLEU metric The original BLEU score is calculated with the modified n-gram precision Pn and the brevity penalty BP , as shown in (1). BLEU = BP · exp( N X wn logPn ) (1) n=1 where wn is a positive weight which is used to adjust the proportions of different n-grams. In the baseline of Papine"
W16-4505,P15-2025,0,0.0183192,"on. Banchs et al. (2015) use Latent Semantic Indexing to project sentences as bag-of-words into a low-dimensional continuous space to measure the adequacy on an hypothesis. A monolingual continuous space has been used to capture the similarity between hypothesis and reference and a cross-language continuous space has been used to calculate the similarity between source sentence and hypothesis. With the same idea, Vela and Tan (2015) proposed a Bayesian Ridge Regressor which use document-level embeddings as features and METEOR score as target to predict the adequacy of hypothesis. The study of Chen and Guo (2015) uses vector representation more directly. In their study, each sentence has been transformed into a vector (they tried 3 kinds of vector representation: one-hot, word embedding and recursive auto-encoder representations). The evaluation score is calculated by the distance between the hypothesis vector and the reference vector, with a length penalty. More recently, Servan et al. (2016) combine word embeddings and DBnary (S´erasset, 2015), a multilingual lexical resource, to enrich METEOR. In this paper, we also incorporate word embeddings in our similarity score to improve machine translation"
W16-4505,Q15-1016,0,0.0398412,"ut-Of-Vocabulary words are skipped when computing the similarity score. For example, word vectors show that “study” and “studies” are very similar, while “study” and “play” are not very similar. • Vector of “study” is [0.1049, -0.1103, ..., 0.0752] • Vector of “studies” is [0.0035, -0.0799, ..., 0.1178] • Vector of “play” is [-0.0250, 0.0531 ..., 0.0759] • Similarity score of “study” and “studies”: 0.534 1 https://code.google.com/p/word2vec/ 34 • Similarity score of “study” and “play”: 0.058 Word2Vec provides two embedding algorithms, Skip-Gram and Continuous Bag-of-Words (CBOW). The study of Levy et al. (2015) and Mikolov et al. (2013) show that Skip-Gram better represents word similarity, but Baroni et al. (2014) show the opposite. In our study, we will use both of them, and try to find the better one for our modifications of BLEU and WER. Our Python program uses the Gensim package2 for implementing the trained word embeddings. The code of our modified measures is provided on the Github page3 . 2.1 Modification for BLEU metric The original BLEU score is calculated with the modified n-gram precision Pn and the brevity penalty BP , as shown in (1). BLEU = BP · exp( N X wn logPn ) (1) n=1 where wn is"
W16-4505,W13-2202,0,0.0186197,"n” and “study” and words that differ only by morphological markers, such as “study” and “studies” are considered different words although they have a similar meaning. The traditional solution for improving their performance is to use more references. However, multiple references are rare and expensive. Moreover, these n-gram-based evaluations have been shown to be biased in favour of statistical methods, largely because they do not allow grammatically-costrained lexical freedom. In recent years, many proposals have been put forth and new metrics have appeared and shown their good performance (Machacek and Bojar, 2013; Machacek and Bojar, 2014; Stanojevi´c et al., 2015). However, improving the performance of existing metrics does not require developing a whole new metric. Proposals that modify existing metrics and show competitive results have also been proposed. One of the common solutions to improve traditional metrics consists in changing strict string matching to fuzzy matching at the surface level. For example, LeBLEU (Virpioja and Gr¨onroos, 2015) — a variant of standard BLEU, also called “Letter-edit-BLEU” or “Levenshtein-BLEU” — takes into account letteredit distance — Levenshtein distance includin"
W16-4505,W14-3336,0,0.0807679,"hat differ only by morphological markers, such as “study” and “studies” are considered different words although they have a similar meaning. The traditional solution for improving their performance is to use more references. However, multiple references are rare and expensive. Moreover, these n-gram-based evaluations have been shown to be biased in favour of statistical methods, largely because they do not allow grammatically-costrained lexical freedom. In recent years, many proposals have been put forth and new metrics have appeared and shown their good performance (Machacek and Bojar, 2013; Machacek and Bojar, 2014; Stanojevi´c et al., 2015). However, improving the performance of existing metrics does not require developing a whole new metric. Proposals that modify existing metrics and show competitive results have also been proposed. One of the common solutions to improve traditional metrics consists in changing strict string matching to fuzzy matching at the surface level. For example, LeBLEU (Virpioja and Gr¨onroos, 2015) — a variant of standard BLEU, also called “Letter-edit-BLEU” or “Levenshtein-BLEU” — takes into account letteredit distance — Levenshtein distance including the spaces between the w"
W16-4505,P02-1040,0,0.104756,"WER have been widely used, but these metrics have poor correlations with human judgements because they badly represent word similarity and impose strict identity matching. In this paper, we propose some modifications to the traditional measures based on word embeddings for these two metrics. The evaluation results show that our modifications significantly improve their correlation with human judgements. 1 Introduction One of the challenges for Machine Translation (MT) research is how to evaluate the quality of translations automatically and correctly. Earlier word-based metrics such as BLEU (Papineni et al., 2002), WER and TER (Snover et al., 2006) have been widely used in machine translation, but these metrics have poor correlations with human judgements, especially at the sentence level. One reason is that they just allow strict string matchings between hypothesis and references. For example, the semantically related words “learn” and “study” and words that differ only by morphological markers, such as “study” and “studies” are considered different words although they have a similar meaning. The traditional solution for improving their performance is to use more references. However, multiple referenc"
W16-4505,C16-1110,0,0.0332397,"Missing"
W16-4505,2006.amta-papers.25,0,0.0699118,"metrics have poor correlations with human judgements because they badly represent word similarity and impose strict identity matching. In this paper, we propose some modifications to the traditional measures based on word embeddings for these two metrics. The evaluation results show that our modifications significantly improve their correlation with human judgements. 1 Introduction One of the challenges for Machine Translation (MT) research is how to evaluate the quality of translations automatically and correctly. Earlier word-based metrics such as BLEU (Papineni et al., 2002), WER and TER (Snover et al., 2006) have been widely used in machine translation, but these metrics have poor correlations with human judgements, especially at the sentence level. One reason is that they just allow strict string matchings between hypothesis and references. For example, the semantically related words “learn” and “study” and words that differ only by morphological markers, such as “study” and “studies” are considered different words although they have a similar meaning. The traditional solution for improving their performance is to use more references. However, multiple references are rare and expensive. Moreover"
W16-4505,W15-3031,0,0.027878,"Missing"
W16-4505,W15-3051,0,0.0194725,"e and document level, which allows them to compute the similarity between two sequence of words. Recently, this kind of vector representation has been widely integrated in MT evaluation. Banchs et al. (2015) use Latent Semantic Indexing to project sentences as bag-of-words into a low-dimensional continuous space to measure the adequacy on an hypothesis. A monolingual continuous space has been used to capture the similarity between hypothesis and reference and a cross-language continuous space has been used to calculate the similarity between source sentence and hypothesis. With the same idea, Vela and Tan (2015) proposed a Bayesian Ridge Regressor which use document-level embeddings as features and METEOR score as target to predict the adequacy of hypothesis. The study of Chen and Guo (2015) uses vector representation more directly. In their study, each sentence has been transformed into a vector (they tried 3 kinds of vector representation: one-hot, word embedding and recursive auto-encoder representations). The evaluation score is calculated by the distance between the hypothesis vector and the reference vector, with a length penalty. More recently, Servan et al. (2016) combine word embeddings and"
W16-4505,W15-3052,0,0.0235107,"Missing"
W16-4505,W16-2342,0,0.0128412,"require developing a whole new metric. Proposals that modify existing metrics and show competitive results have also been proposed. One of the common solutions to improve traditional metrics consists in changing strict string matching to fuzzy matching at the surface level. For example, LeBLEU (Virpioja and Gr¨onroos, 2015) — a variant of standard BLEU, also called “Letter-edit-BLEU” or “Levenshtein-BLEU” — takes into account letteredit distance — Levenshtein distance including the spaces between the words — between hypothesis and references instead of strict n-gram matchings. More recently, Weiyue et al. (2016) have proposed a character-level TER (CharacTER) which calculates the character-level edit distance, while still performing the shift edits at the word level. The evaluation results show that this kind of modifications have a good effect on string-level similar words, but that they don’t work well on words that are semantically similar, but are orthografically different strings. To capture semantic similarity, one established way is to apply additional linguistic knowledge, such as synonym dictionaries. For example, TER-Plus (Snover et al., 2009) use WordNet (Fellbaum, 1998) to compute synonym"
W19-4817,C18-1012,0,0.161431,"k on English has found mixed results (Chowdhury and Zamparelli, 2018). In this paper, we extend previous work on longdistance dependencies to tease apart the potential grounds for the different outcomes by making previous work more comparable. There are several differences between the pieces of work on long-distance dependencies mentioned above. First, the work that does not find a correspondence between the two sources of information being compared (Merlo and Ackermann, 2018) imposes a much stricter test of correspondence — total correlation— than the general effect reported in Wilcox et al. (2018). Secondly, the pieces of work vary in task: it is possible that word embedThe recent wide-spread and strong interest in RNNs has spurred detailed investigations of the distributed representations they generate and specifically if they exhibit properties similar to those characterising human languages. Results are at present inconclusive. In this paper, we extend previous work on long-distance dependencies in three ways. We manipulate word embeddings to translate them in a space that is attuned to the linguistic properties under study. We extend the work to sentence embeddings and to new langu"
W19-7906,W15-2103,0,0.0275895,"in English. 4 Materials and methods Our hypotheses above follow a common schema that requires calculating the observed counts of a feature in the corpus and compare it to the counts we would expect if intervention was not at play. The annotated corpora we use are the universal dependency treebanks for Italian and English. 4.1 The corpus We extract our counts from the Italian and English Universal Dependencies (UD) treebanks (Nivre, 2015) version 2.0 (http://universaldependencies.org/).3 The data comes from five different treebanks: English ParTut (Bosco and Sanguinetti, 2014), English LinEs (Ahrenberg, 2015), English UD (Bies et al., 2012), Italian ParTut (Bosco and Sanguinetti, 2014), Italian UD (Bosco et al., 2013). They comprise a variety of text genres. For English: blogs, social media, reviews, fiction, nonfiction, spoken legal, news, wiki For Italian : legal, news, wiki. In what follows, the analysis of the phenomenon will not be presented according to the different corpora, but all the different treebanks for each languages will be merged and thus we will refer to Italian and English data without the specification of the treebanks. The reason for this merge is the observation that differen"
W19-7906,W13-2308,0,0.0123165,"he observed counts of a feature in the corpus and compare it to the counts we would expect if intervention was not at play. The annotated corpora we use are the universal dependency treebanks for Italian and English. 4.1 The corpus We extract our counts from the Italian and English Universal Dependencies (UD) treebanks (Nivre, 2015) version 2.0 (http://universaldependencies.org/).3 The data comes from five different treebanks: English ParTut (Bosco and Sanguinetti, 2014), English LinEs (Ahrenberg, 2015), English UD (Bies et al., 2012), Italian ParTut (Bosco and Sanguinetti, 2014), Italian UD (Bosco et al., 2013). They comprise a variety of text genres. For English: blogs, social media, reviews, fiction, nonfiction, spoken legal, news, wiki For Italian : legal, news, wiki. In what follows, the analysis of the phenomenon will not be presented according to the different corpora, but all the different treebanks for each languages will be merged and thus we will refer to Italian and English data without the specification of the treebanks. The reason for this merge is the observation that different corpora show very large fluctuations in distributions of grammatical constructions, in general, and specifica"
W19-7906,C18-1012,0,0.0197841,"in computational method, and like other approaches, show for the moment, mixed conclusions. While some experiments have shown that Recursive Neural Networks can learn the main descriptive properties of long-distance dependencies in English, for example the fact that they obey a uniqueness constraint (only one gap per filler) and also that they obey island constraints (Wilcox et al., 2018), work attempting to replicate finer-grained human judgments for French have failed to show a correlation with human behaviour (Merlo and Ackermann, 2018), while other work on English has found mixed results (Chowdhury and Zamparelli, 2018). Lack of correlation with human grammaticality judgments has also been found in wh-islands and object relative clauses for both French and English (Merlo, 2019). More work will be needed to establish the exact boundaries of quantitative properties in long-distance dependencies across several languages. 6 Conclusions and future work The contributions of this treebank study are many-fold. First, we formulate quantitative predictions about object-oriented relative clauses based on intervention theory. These predictions aim to identify which features come into play in defining the notion of inter"
W19-7906,K18-1038,1,0.826354,"rich current debate on the exact nature of structural dependencies and locality in computational method, and like other approaches, show for the moment, mixed conclusions. While some experiments have shown that Recursive Neural Networks can learn the main descriptive properties of long-distance dependencies in English, for example the fact that they obey a uniqueness constraint (only one gap per filler) and also that they obey island constraints (Wilcox et al., 2018), work attempting to replicate finer-grained human judgments for French have failed to show a correlation with human behaviour (Merlo and Ackermann, 2018), while other work on English has found mixed results (Chowdhury and Zamparelli, 2018). Lack of correlation with human grammaticality judgments has also been found in wh-islands and object relative clauses for both French and English (Merlo, 2019). More work will be needed to establish the exact boundaries of quantitative properties in long-distance dependencies across several languages. 6 Conclusions and future work The contributions of this treebank study are many-fold. First, we formulate quantitative predictions about object-oriented relative clauses based on intervention theory. These pre"
W19-7906,W19-4817,1,0.808762,"riptive properties of long-distance dependencies in English, for example the fact that they obey a uniqueness constraint (only one gap per filler) and also that they obey island constraints (Wilcox et al., 2018), work attempting to replicate finer-grained human judgments for French have failed to show a correlation with human behaviour (Merlo and Ackermann, 2018), while other work on English has found mixed results (Chowdhury and Zamparelli, 2018). Lack of correlation with human grammaticality judgments has also been found in wh-islands and object relative clauses for both French and English (Merlo, 2019). More work will be needed to establish the exact boundaries of quantitative properties in long-distance dependencies across several languages. 6 Conclusions and future work The contributions of this treebank study are many-fold. First, we formulate quantitative predictions about object-oriented relative clauses based on intervention theory. These predictions aim to identify which features come into play in defining the notion of intervener, and with what strength. Our results corroborate some previous findings concerning morphosyntactic features and animacy, but not all, opening the door to f"
W19-7906,W18-5423,0,0.0164456,"es relevant to intervention to lexical semantic aspects of the actants in grammatical long-distance dependencies. These corpus results also join the rich current debate on the exact nature of structural dependencies and locality in computational method, and like other approaches, show for the moment, mixed conclusions. While some experiments have shown that Recursive Neural Networks can learn the main descriptive properties of long-distance dependencies in English, for example the fact that they obey a uniqueness constraint (only one gap per filler) and also that they obey island constraints (Wilcox et al., 2018), work attempting to replicate finer-grained human judgments for French have failed to show a correlation with human behaviour (Merlo and Ackermann, 2018), while other work on English has found mixed results (Chowdhury and Zamparelli, 2018). Lack of correlation with human grammaticality judgments has also been found in wh-islands and object relative clauses for both French and English (Merlo, 2019). More work will be needed to establish the exact boundaries of quantitative properties in long-distance dependencies across several languages. 6 Conclusions and future work The contributions of this"
W97-0317,J82-3004,0,0.221105,"which sets them aside from programming languages, and which is at the root of the difficulty of the parsing enterprise, pervading languages at all levels: lexical, morphological, syntactic, semantic and pragmatic. Unless clever techniques are developed to deal with ambiguity, the number of possible parses for an average sentence (20 words) is simply intractable. In the case Of prepositional phrases, the expansion of the number of possible analysis is the Catalan number series, thus the number of possible analyses grows with a function that is exponential in the number of Prepositional Phrase (Church and Patil, 1982). One of the most interesting topics of debate at the moment, is the use of frequency information for automatic syntactic disambiguation. As argued in many pieces of work in the AI tradition (Marcus, 1980; Crain and Steedman, 1985; Altmann and Steedman, 1988; Hirst, 1987), the exact solution of the disambiguation problem requires complex reasoning and high level syntactic and semantic knowledge. However, current work in partof-speech tagging has succeeded in showing that it is possible to carve one particular subproblem and solve it by a p p r o x i m a t i o n - - using statistical techniques"
W97-0317,J93-1005,0,0.887578,"Missing"
W97-0317,J87-3005,0,0.0418755,"e begin with an overview of techniques which have been used for PP attachment disambiguation, and then consider how one of the most successful of these, the backed-off estimation technique, can be applied to the general problem of multiple PP attachment. 2 Existing Models of Attachment Attempts to resolve the problem of PP attachment in computational linguistics are numerous, but the problem is hard and success rate typically depends on the domain of application. Historically, the shift from attempts to resolve the problem completely, by using heuristics developed using typical AI techniques (Jensen and Binot, 1987; Marcus, 1980; Crain and Steedman, 1985; Altmann and Steedman, 1988) has left the place for attempts to solve the problem by less expensive means, even if only approximately. As shown by many psycholinguistic and practical 149 studies (Ford et al., 1982; Taraban and McClelland, 1988; Whittemore et al., 1990), lexical information is one of the main cues to PP attachment disambiguation. In one of the earliest attempts to resolve the problem of P P attachment ambiguity using lexical measures, Hindle and Pmoth (1993) show that a measure of mutual information limited to lexical association can cor"
W97-0317,E91-1004,0,0.0656411,"Missing"
W97-0317,C94-2195,0,\N,Missing
W98-1116,P97-1003,0,0.153094,"etain structural knowledge, such as phrase structween a main clause and reduced relative conture, subcategorization and long distance destruction. We measure the probability distripendencies. So they are equally capable of butions of several linguistic features (transitivmodelling the fine lexical idiosyncrasies and tile ity. tense, voice) over a sample of optionally inmore general syntactic regularities. transitive verbs. In agreement with recent reGiven an annotated training corpus, such suits on parsing with lexicalised probabilistic methods learn its distributions (the lexical cogrammars (Collins, 1997; Srinivas, 1997), we occurrences), which requires being given the find that statistics over lexical, as opposed to correct space of events in the m o d e l - - t h a t is, structural, features best correspond to human the g r a m m a r - - a c c u r a t e l y enough that they can intuitive .judgments and to experimental findparse new instances of the same c o r p u s . The ings. These results are enlightening to invessuccess of such models suggests that a statistitigate novel uses of corpora, by assessing the cal model nmst have access to tile appropriate portability of statistics across task"
W98-1116,C94-2149,0,0.0329316,"d abow,, correspond to a different elementary tree. im:luding the unergative and unaccusative distinction, encoded by different labels referring to theinatic roles. Current LTAG part-of-speech taggers, called supertaggets (Joshi and Srinivas, 1994; Srinivas, 1997) assign a set of elementary trees to each word, in effect chunking the text. The counts performed in the study reported here would have required simply counting the occurrences of the labels assigned to the words in the text by such a supertagger. Refinements in this direction of the annotation of the grammar used by the XTAG system (Doran et al., 1994) are actually tinder way. We also can see, from the raw frequencies obtained, that when collecting counts about syntactic phenomena, corpora must be in the order of hundreds of millions of words for the statistics to be reliable. 4 Conclusions Our main result in this paper is that statistics over lezical features best correspond to independently established human intuitive judgments. We have argued that, methodologically, this result casts light on the relationship between different data collection methods, and shows that some apparently contradictory results can be reconciled by defining prob"
W98-1116,P95-1034,0,0.0241127,"Missing"
W98-1116,W97-0301,0,0.0226157,"Missing"
W98-1116,P98-2177,0,0.0240221,"dings is coherent and in accordance with current dew~lopments in statistical parsing and grammati,:al theory in two important respects. First, the discrepancy between the frequencies of each of the lexical features and the frequencies of the m:tual construction suggests that the frequency of a construction is a composition fimction of (at least some of) its lexical features, even if such t};atures are not-independent. Models that can handle non-independent lexical features have given very good results both for part-of-speech and structural disambiguation (Ratnaparkhi, 1996; Ratnaparkhi, 1997; Ratnaparkhi, 1998). Second, we observe that the lexical and sublexical features we counted are not sufficient to identify all the relevant linguistic classes: statistical tests fail to differentiate between unaccusatives and object-drop verbs. In order to distinguish between these two classes of verbs one needs to look at some of the surrounding context. This result is expected. Performance measures of statistical parsers show that statistics based on one word give poor results, but that statistics on bigrams have much better per139 formance (Charniak, 1997). 3 General Discussion 3.1 Relationship b e t w e e n"
W98-1116,P98-2184,0,0.0189322,"ethod to collect data: tbr example: frequencybased preferences are not used by hmnans; the wi'ong frequencies had been ,:omltcd: experimental results are not representative of natural linguistic behaviour: or corpora are not representative of natural linguistic behaviom'. The findings in this study show a way of reconciling results obtained by different data collection methods: if we count at the level of lexical and sublexical features, we find that differences in native speakers' preferences do correspond to significant differences in distributions. Similar conclusions are being reached in (Roland and Jurafsky, 1998), who compare different corpora. 3.2 Classification Properties of Lexical Features and C o n s e q u e n c e s Looking at the frequencies of the Iexical features in Table 2, we can observe that P12T, PASS and TRANS have counts that can be used to directly predict the difficulty of the 1212.construction. This observation can be used beneficially in a task different fl'om parsing, for instance in a generation system. Some current methods have a generate and filter approach (Knight and Hatzivassiloglou, 1995): all constructic,ns are generated and then filtered based on a sl~atistical model. If th"
W98-1116,E91-1006,0,0.0238761,"Missing"
W98-1116,C98-2179,0,\N,Missing
