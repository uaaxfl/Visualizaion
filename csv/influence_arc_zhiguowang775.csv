2020.acl-main.413,P19-1620,0,0.0523292,", XLNet (Yang et al., 2019) and RoBERTa (Liu et al., 2019). Fine-tuning these language models, however, requires largescale data for fine-tuning. Creating a dataset for every new domain is extremely costly and practically infeasible. The ability to apply QA models on outof-domain data in an efficient manner is thus very 1 2 Equal contribution Work done during internship at the AWS AI Labs desirable. This problem may be approached with domain adaptation or transfer learning techniques (Chung et al., 2018) as well as data augmentation (Yang et al., 2017; Dhingra et al., 2018; Wang et al., 2018; Alberti et al., 2019). However, here we expand upon the recently introduced task of unsupervised question answering (Lewis et al., 2019) to examine the extent to which synthetic training data alone can be used to train a QA model. In particular, we focus on the machine reading comprehension setting in which the context is a given paragraph, and the QA model can only access this paragraph to answer a question. Furthermore, we work on extractive QA, where the answer is assumed to be a contiguous sub-string of the context. A training instance for supervised reading comprehension consists of three components: a questi"
2020.acl-main.413,N18-2092,0,0.304428,"models such as BERT (Devlin et al., 2019), XLNet (Yang et al., 2019) and RoBERTa (Liu et al., 2019). Fine-tuning these language models, however, requires largescale data for fine-tuning. Creating a dataset for every new domain is extremely costly and practically infeasible. The ability to apply QA models on outof-domain data in an efficient manner is thus very 1 2 Equal contribution Work done during internship at the AWS AI Labs desirable. This problem may be approached with domain adaptation or transfer learning techniques (Chung et al., 2018) as well as data augmentation (Yang et al., 2017; Dhingra et al., 2018; Wang et al., 2018; Alberti et al., 2019). However, here we expand upon the recently introduced task of unsupervised question answering (Lewis et al., 2019) to examine the extent to which synthetic training data alone can be used to train a QA model. In particular, we focus on the machine reading comprehension setting in which the context is a given paragraph, and the QA model can only access this paragraph to answer a question. Furthermore, we work on extractive QA, where the answer is assumed to be a contiguous sub-string of the context. A training instance for supervised reading comprehens"
2020.acl-main.413,Q19-1026,0,0.0532274,"named entity, achieving stateof-the-art performance on SQuAD for unsupervised QA. 1 Figure 1: Question Generation Pipeline: the original context sentence containing a given answer is used as a query to retrieve a related sentence containing matching entities, which is input into our question-style converter to create QA training data. Introduction Question Answering aims to answer a question based on a given knowledge source. Recent advances have driven the performance of QA systems to above or near-human performance on QA datasets such as SQuAD (Rajpurkar et al., 2016) and Natural Questions (Kwiatkowski et al., 2019) thanks to pretrained language models such as BERT (Devlin et al., 2019), XLNet (Yang et al., 2019) and RoBERTa (Liu et al., 2019). Fine-tuning these language models, however, requires largescale data for fine-tuning. Creating a dataset for every new domain is extremely costly and practically infeasible. The ability to apply QA models on outof-domain data in an efficient manner is thus very 1 2 Equal contribution Work done during internship at the AWS AI Labs desirable. This problem may be approached with domain adaptation or transfer learning techniques (Chung et al., 2018) as well as data au"
2020.acl-main.413,P19-1484,0,0.13018,"escale data for fine-tuning. Creating a dataset for every new domain is extremely costly and practically infeasible. The ability to apply QA models on outof-domain data in an efficient manner is thus very 1 2 Equal contribution Work done during internship at the AWS AI Labs desirable. This problem may be approached with domain adaptation or transfer learning techniques (Chung et al., 2018) as well as data augmentation (Yang et al., 2017; Dhingra et al., 2018; Wang et al., 2018; Alberti et al., 2019). However, here we expand upon the recently introduced task of unsupervised question answering (Lewis et al., 2019) to examine the extent to which synthetic training data alone can be used to train a QA model. In particular, we focus on the machine reading comprehension setting in which the context is a given paragraph, and the QA model can only access this paragraph to answer a question. Furthermore, we work on extractive QA, where the answer is assumed to be a contiguous sub-string of the context. A training instance for supervised reading comprehension consists of three components: a question, a context, and an answer. For a given dataset domain, a collection of documents can usually be easily obtained,"
2020.acl-main.413,2021.ccl-1.108,0,0.155927,"Missing"
2020.acl-main.413,D16-1264,0,0.0663435,"nerating questions for (context, answer) pairs but shows little improvement over applying a much simpler question generator which drops, permutates and masks words. We improve upon this paper by proposing a simple, intuitive, retrieval and template-based question generation approach, illustrated in Figure 1. The idea is to retrieve a sentence from the corpus similar to the current context, and then generate a question based on that sentence. Having created a question for all (context, answer) pairs, we then fine-tune a pretrained BERT model on this data and evaluate on the SQuAD v1.1 dataset (Rajpurkar et al., 2016). Our contributions are as follows: we introduce a retrieval, template-based framework which achieves state-of-the-art results on SQuAD for unsupervised models, particularly when the answer is a named entity. We perform ablation studies to determine the effect of components in template question generation. We are releasing our synthetic training data and code.1 2 Unsupervised QA Approach We focus on creating high-quality, non-trivial questions which will allow the model to learn to extract the proper answer from a context-question pair. Sentence Retrieval: A standard cloze question can be obta"
2020.acl-main.413,C18-1073,0,0.163828,"evlin et al., 2019), XLNet (Yang et al., 2019) and RoBERTa (Liu et al., 2019). Fine-tuning these language models, however, requires largescale data for fine-tuning. Creating a dataset for every new domain is extremely costly and practically infeasible. The ability to apply QA models on outof-domain data in an efficient manner is thus very 1 2 Equal contribution Work done during internship at the AWS AI Labs desirable. This problem may be approached with domain adaptation or transfer learning techniques (Chung et al., 2018) as well as data augmentation (Yang et al., 2017; Dhingra et al., 2018; Wang et al., 2018; Alberti et al., 2019). However, here we expand upon the recently introduced task of unsupervised question answering (Lewis et al., 2019) to examine the extent to which synthetic training data alone can be used to train a QA model. In particular, we focus on the machine reading comprehension setting in which the context is a given paragraph, and the QA model can only access this paragraph to answer a question. Furthermore, we work on extractive QA, where the answer is assumed to be a contiguous sub-string of the context. A training instance for supervised reading comprehension consists of thr"
2020.acl-main.413,P17-1096,0,0.0188485,"retrained language models such as BERT (Devlin et al., 2019), XLNet (Yang et al., 2019) and RoBERTa (Liu et al., 2019). Fine-tuning these language models, however, requires largescale data for fine-tuning. Creating a dataset for every new domain is extremely costly and practically infeasible. The ability to apply QA models on outof-domain data in an efficient manner is thus very 1 2 Equal contribution Work done during internship at the AWS AI Labs desirable. This problem may be approached with domain adaptation or transfer learning techniques (Chung et al., 2018) as well as data augmentation (Yang et al., 2017; Dhingra et al., 2018; Wang et al., 2018; Alberti et al., 2019). However, here we expand upon the recently introduced task of unsupervised question answering (Lewis et al., 2019) to examine the extent to which synthetic training data alone can be used to train a QA model. In particular, we focus on the machine reading comprehension setting in which the context is a given paragraph, and the QA model can only access this paragraph to answer a question. Furthermore, we work on extractive QA, where the answer is assumed to be a contiguous sub-string of the context. A training instance for supervi"
2020.acl-main.413,N18-1143,0,\N,Missing
2020.acl-main.413,N19-1423,0,\N,Missing
2020.emnlp-main.439,P19-1620,0,0.0985011,"nding research goal (Mitkov and Ha, 2003; Rus et al., 2010). Although many past works have proposed different strategies for question generation, they have limited or no success in improving the downstream QA task (Du et al., 2017; Sun et al., 2018; Song et al., 2018; Klein and Nabi, 2019; Wang et al., 2020; Ma et al., 2020; Chen et al., 2020; Tuan et al., 2019). Some recent approaches for synthetic QA data generation based on large pretrained language models (LM) have started to demonstrate success in improving the downstream Reading Comprehension (RC) task with automatically generated data (Alberti et al., 2019; Puri et al., 2020). However, these approaches typically consist of multi-stage systems that use three modules: span/answer detector, question generator and question filtering. ∗ *equal contribution. † Siamak Shakeri is currently with Google. The work was done when he was at AWS AI. Given an input passage, the span detector is responsible for extracting spans that will serve as answers for which questions will be generated. This module normally combines a pretrained QA model with handcrafted heuristics. The question generator is a large LM fine-tuned for the task of conditional generation of"
2020.emnlp-main.439,P17-1123,0,0.275737,"or a separate filtering model. Our generator is trained by finetuning a pretrained LM using maximum likelihood estimation. The experimental results indicate significant improvements in the domain adaptation of QA models outperforming current state-of-the-art methods. 1 Introduction Improving question answering (QA) systems through automatically generated synthetic data is a long standing research goal (Mitkov and Ha, 2003; Rus et al., 2010). Although many past works have proposed different strategies for question generation, they have limited or no success in improving the downstream QA task (Du et al., 2017; Sun et al., 2018; Song et al., 2018; Klein and Nabi, 2019; Wang et al., 2020; Ma et al., 2020; Chen et al., 2020; Tuan et al., 2019). Some recent approaches for synthetic QA data generation based on large pretrained language models (LM) have started to demonstrate success in improving the downstream Reading Comprehension (RC) task with automatically generated data (Alberti et al., 2019; Puri et al., 2020). However, these approaches typically consist of multi-stage systems that use three modules: span/answer detector, question generator and question filtering. ∗ *equal contribution. † Siamak"
2020.emnlp-main.439,D17-1090,0,0.0452821,"for QAGen and QAGen2S because question quality would have a dominant 5448 3 Related Work Question generation (QG) has been extensively studied from the early heuristic-based methods (Mitkov and Ha, 2003; Rus et al., 2010) to the recent neural-base approaches. However, most work (Du et al., 2017; Sun et al., 2018; Zhao et al., 2018; Kumar et al., 2019; Wang et al., 2020; Ma et al., 2020; Tuan et al., 2019; Chen et al., 2020) only takes QG as a stand-alone task, and evaluates the quality of generated questions with either automatic metrics such as BLEU, or human evaluation. Tang et al. (2017), Duan et al. (2017) and Sachan and Xing (2018) verified that generated questions can improve the downstream answer sentence selection tasks. Song et al. (2018) and Klein and Nabi (2019) leveraged QG to augment the training set for machine reading comprehend tasks. However, they only got improvement when only a small amount of human labeled data is available. Recently, with the help of large pre-trained language models, Alberti et al. (2019) and Puri et al. (2020) have been able to improve the performance of RC models using generated questions. However, they need two extra BERT models to identify high-quality ans"
2020.emnlp-main.439,D19-5801,0,0.0761039,"the language model on the target domains. 4 4.1 Experimental Setup and Results Datasets We used SQuAD 1.1 dataset (Rajpurkar et al., 2016) to train the generative models as well as in-domain supervised data for the downstream RC task in this work. We used the default train and dev splits, which contain 87,599 and 10,570 (q, a) pairs, respectively. Similar to (Nishida et al., 2019), we selected the following four datasets as target domains: Natural Questions (Kwiatkowski et al., 2019), which consist of Google search questions and the annotated answers from Wikipedia. We used MRQA Shared Task (Fisch et al., 2019) preprocessed training and dev sets, which consist of 104,071 and 12,836 (q, a) pairs, respectively. The training set passages were used as the unlabeled target domain corpus, while the evaluations were performed on the dev set. NewsQA (Hermann et al., 2015), which consists of question and answer pairs from CNN news articles. We used the dev set from the MRQA Shared Task, which removes unanswerable questions and those without annotator agreement. We prefer this version as we focus only on the generation of answerable questions. The dev set consists of 4,212 (q, a) pairs. Passages from CNN/Dail"
2020.emnlp-main.439,N18-1058,0,0.0167852,"because question quality would have a dominant 5448 3 Related Work Question generation (QG) has been extensively studied from the early heuristic-based methods (Mitkov and Ha, 2003; Rus et al., 2010) to the recent neural-base approaches. However, most work (Du et al., 2017; Sun et al., 2018; Zhao et al., 2018; Kumar et al., 2019; Wang et al., 2020; Ma et al., 2020; Tuan et al., 2019; Chen et al., 2020) only takes QG as a stand-alone task, and evaluates the quality of generated questions with either automatic metrics such as BLEU, or human evaluation. Tang et al. (2017), Duan et al. (2017) and Sachan and Xing (2018) verified that generated questions can improve the downstream answer sentence selection tasks. Song et al. (2018) and Klein and Nabi (2019) leveraged QG to augment the training set for machine reading comprehend tasks. However, they only got improvement when only a small amount of human labeled data is available. Recently, with the help of large pre-trained language models, Alberti et al. (2019) and Puri et al. (2020) have been able to improve the performance of RC models using generated questions. However, they need two extra BERT models to identify high-quality answer spans, and filter out l"
2020.emnlp-main.439,P18-1156,0,0.0454568,"er pairs from CNN news articles. We used the dev set from the MRQA Shared Task, which removes unanswerable questions and those without annotator agreement. We prefer this version as we focus only on the generation of answerable questions. The dev set consists of 4,212 (q, a) pairs. Passages from CNN/Daily Mail corpus of Hermann et al. (2015) are used as unlabeled target domain corpus. BioASQ (Tsatsaronis et al., 2015): we employed MRQA shared task version of BioASQ, which consists of a dev set with 1,504 samples. We collected PubMed abstracts to use as target domain unlabeled passages. DuoRC (Saha et al., 2018) contains questionanswer pairs from movie plots which are extracted from both Wikipedia and IMDB. ParaphraseRC task of DuoRC dataset was used in our evaluations, consisting of 13,111 pairs. We crawled IMDB movie plots to use as the unlabeled target domain corpus. 4.2 Experimental Setup We used Pytorch (Paszke et al., 2019) and Transformers (Wolf et al., 2019) to develop the models and perform experiments. Generative models are trained on SQuAD 1.1 for 5 epochs, and the best model is selected based on the cross entropy loss on the SQuAD dev set. AdamW (Loshchilov and Hutter, 2017) optimizer wit"
2020.emnlp-main.439,N18-2090,1,0.925526,"generator is trained by finetuning a pretrained LM using maximum likelihood estimation. The experimental results indicate significant improvements in the domain adaptation of QA models outperforming current state-of-the-art methods. 1 Introduction Improving question answering (QA) systems through automatically generated synthetic data is a long standing research goal (Mitkov and Ha, 2003; Rus et al., 2010). Although many past works have proposed different strategies for question generation, they have limited or no success in improving the downstream QA task (Du et al., 2017; Sun et al., 2018; Song et al., 2018; Klein and Nabi, 2019; Wang et al., 2020; Ma et al., 2020; Chen et al., 2020; Tuan et al., 2019). Some recent approaches for synthetic QA data generation based on large pretrained language models (LM) have started to demonstrate success in improving the downstream Reading Comprehension (RC) task with automatically generated data (Alberti et al., 2019; Puri et al., 2020). However, these approaches typically consist of multi-stage systems that use three modules: span/answer detector, question generator and question filtering. ∗ *equal contribution. † Siamak Shakeri is currently with Google. The"
2020.emnlp-main.439,D18-1424,0,0.0268579,"ncluded in Appendix B.3. We speculate this is due to average pooling encouraging longer question-answers, which could be of lower quality than shorter question-answer pairs. Where Nq and Na indicate the lengths of generated question and answer, respectively. We use answer-only scores for QAGen and QAGen2S because question quality would have a dominant 5448 3 Related Work Question generation (QG) has been extensively studied from the early heuristic-based methods (Mitkov and Ha, 2003; Rus et al., 2010) to the recent neural-base approaches. However, most work (Du et al., 2017; Sun et al., 2018; Zhao et al., 2018; Kumar et al., 2019; Wang et al., 2020; Ma et al., 2020; Tuan et al., 2019; Chen et al., 2020) only takes QG as a stand-alone task, and evaluates the quality of generated questions with either automatic metrics such as BLEU, or human evaluation. Tang et al. (2017), Duan et al. (2017) and Sachan and Xing (2018) verified that generated questions can improve the downstream answer sentence selection tasks. Song et al. (2018) and Klein and Nabi (2019) leveraged QG to augment the training set for machine reading comprehend tasks. However, they only got improvement when only a small amount of human"
2020.emnlp-main.439,D18-1427,0,0.233325,"tering model. Our generator is trained by finetuning a pretrained LM using maximum likelihood estimation. The experimental results indicate significant improvements in the domain adaptation of QA models outperforming current state-of-the-art methods. 1 Introduction Improving question answering (QA) systems through automatically generated synthetic data is a long standing research goal (Mitkov and Ha, 2003; Rus et al., 2010). Although many past works have proposed different strategies for question generation, they have limited or no success in improving the downstream QA task (Du et al., 2017; Sun et al., 2018; Song et al., 2018; Klein and Nabi, 2019; Wang et al., 2020; Ma et al., 2020; Chen et al., 2020; Tuan et al., 2019). Some recent approaches for synthetic QA data generation based on large pretrained language models (LM) have started to demonstrate success in improving the downstream Reading Comprehension (RC) task with automatically generated data (Alberti et al., 2019; Puri et al., 2020). However, these approaches typically consist of multi-stage systems that use three modules: span/answer detector, question generator and question filtering. ∗ *equal contribution. † Siamak Shakeri is current"
2021.acl-long.253,P19-1620,0,0.0606793,"Missing"
2021.acl-long.253,P17-1171,0,0.153306,"our R EFUEL as well as several baseline models. We release source code for our models and experiments at https://github. com/amzn/refuel-open-domain-qa. 1 Figure 1: An example from the A MBIG QA (Min et al., 2020) dataset. The Prompt Question is gathered from Google search queries and has three interpretations upon reading Wikipedia. Disambiguated QA Pairs are the full set of acceptable answers, paired with the disambiguated rewriting of the prompt question. Introduction Open-domain Question Answering (QA) is the task of answering questions using a collection of passages with diverse topics (Chen et al., 2017; Guu et al., 2020; Karpukhin et al., 2020). Open-domain questions are highly likely to be ambiguous because people may not have the knowledge of relevant topics when formulating them. For example, in Figure 1, the prompt question “What’s the most ∗ Work done during an internship at AWS AI. points scored in an NBA game?” is ambiguous because the score in this question could be interpreted as the combined score in a game (Q1 A1 ), score from a single team (Q2 A2 ), or score from an individual player (Q3 A3 ). Therefore, a system needs to adaptively predict a single answer, or a set of equally p"
2021.acl-long.253,N19-1423,0,0.0246729,"n the first prediction pass, which we further refine using a conditional-probability-based filtering approach (Sec. 2.3). 2.1 Passage Retrieval & Reranking We use Dense Passage Retriever (DPR) (Karpukhin et al., 2020) for retrieval. First, we split all Wikipedia pages into 100-token passages, resulting in 24M passages in total. Then DPR maps all passages into d-dimensional vectors, computes the representation of the prompt question, and retrieves N passages whose vectors are closest to the question vector (we use N=1000). After retrieving N passages for the prompt question, we fine-tune BERT (Devlin et al., 2019) to rerank these passages. Taking the concatenation of the prompt question and each passage as input, the reranker allows a token-level cross-attention between the prompt question and passages. The relevance score is then derived by taking the [CLS] vector of the input sequence into a linear layer. After reranking, the QA pair generation model takes the top K passages as inputs (we use K=100). 2.2 Single Pass QA Pair Generation The single pass QA pair generation step includes an Answer Prediction module and a Question Disambiguation module. Firstly, taking the reranked passages and the prompt"
2021.acl-long.253,P17-1147,0,0.0259921,"tors to search for, navigate and read multiple Wikipedia pages to find as many interpretations as possible. As a result, each question is annotated with either a single answer or multiple disambiguated QA pairs, depending on how many interpretations can be found. The train, development, and test (not public) dataset sizes are 10036, 2002, 2004, respectively 1 . On average, there are 2.1 distinct answers per question in A MBIG QA. To test the generalization ability of R EFUEL on any possibly ambiguous questions, we additionally evaluate it on two open-domain QA datasets: NQ- OPEN and TriviaQA (Joshi et al., 2017). Implementation Details are in Appendix A. We release source code for our models and experiments at https: //github.com/amzn/refuel-open-domain-qa. Evaluation Metrics. Let (q1 , a1 ), ..., (qm , am ) be m QA pair predictions, (ˆ q1 , a ˆ1 ), ..., (ˆ qn , a ˆn ) be n gold QA pairs, each predicted QA pair (qi , ai ) is evaluated in order by a correctness score towards all gold QA pairs: ci = 1(ai =ˆ aj )f (qi , qˆj ), where f (qi , qˆj ) is a similarity function for questions. (ˆ qj , a ˆj ) will not be further used to evaluate 1 Leaderboard: https://nlp.cs.washington. edu/ambigqa/leaderboard.h"
2021.acl-long.253,2020.emnlp-main.550,0,0.121133,"ine models. We release source code for our models and experiments at https://github. com/amzn/refuel-open-domain-qa. 1 Figure 1: An example from the A MBIG QA (Min et al., 2020) dataset. The Prompt Question is gathered from Google search queries and has three interpretations upon reading Wikipedia. Disambiguated QA Pairs are the full set of acceptable answers, paired with the disambiguated rewriting of the prompt question. Introduction Open-domain Question Answering (QA) is the task of answering questions using a collection of passages with diverse topics (Chen et al., 2017; Guu et al., 2020; Karpukhin et al., 2020). Open-domain questions are highly likely to be ambiguous because people may not have the knowledge of relevant topics when formulating them. For example, in Figure 1, the prompt question “What’s the most ∗ Work done during an internship at AWS AI. points scored in an NBA game?” is ambiguous because the score in this question could be interpreted as the combined score in a game (Q1 A1 ), score from a single team (Q2 A2 ), or score from an individual player (Q3 A3 ). Therefore, a system needs to adaptively predict a single answer, or a set of equally plausible answers when the question has mult"
2021.acl-long.253,P19-1612,0,0.0714291,"EFUEL (w/o RTP) R EFUEL (w/o RTP) Table 2: Dev. set results of A MBIG QA as a function of the number of retrieval/reranking (N) and QA input (K) passages. #QAs: the average number of predicted QA pairs per prompt question. *: our replicated results. NQ- OPEN Model TriviaQA EM Oracle EM EM Oracle EM ORQA (supervised) HardEM (supervised) DPR (supervised) RAG (supervised) 33.3 28.1 41.5 44.5 - 45.0 50.9 57.9 56.8 - R EFUEL w/o RTP (NFT) R EFUEL (NFT) 35.4 37.3 45.2 48.9 48.2 49.8 52.9 54.3 Table 3: Results on NQ- OPEN and TriviaQA test set. RTP: Round-Trip Prediction. NFT: No Fine-Tuning. ORQA (Lee et al., 2019), HardEM (Min et al., 2019), RAG (Lewis et al., 2020b). Experimental Results Main Results. Performance on the dev. and hidden test set of A MBIG QA is shown in Table 1. Even without having round-trip prediction, R E FUEL (w/o RTP) outperforms S PAN S EQ G EN on both the answer prediction subtask and question disambiguation subtask by a large margin. Moreover, the round-trip prediction indeed further improves the performance by finding more and better QA pairs, going from 1.55 to 1.72 pairs per prompt question on the dev. set. A comprehensive analysis on the round-trip prediction is discussed i"
2021.acl-long.253,2020.acl-main.703,0,0.332659,"tion is ambiguous or not. If multiple answers are predicted, the second subtask, Question Disambiguation, requires generating a disambiguated question for each of the plausible answers. They propose S PAN S EQ G EN, which first retrieves and reranks 3263 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3263–3276 August 1–6, 2021. ©2021 Association for Computational Linguistics passages using the prompt question, and then adopts a BART pre-trained sequence-to-sequence model (Lewis et al., 2020a) to generate all plausible answers, conditioned on the concatenation of the prompt question and top 8 passages. For the question disambiguation subtask, based on BART, they first pre-train a question generation model on NQ- OPEN (Kwiatkowski et al., 2019), a large-scale open-domain QA dataset, to generate the question given the answer and top 8 passages. Then they fine-tune it as a question disambiguation model to generate the disambiguated question conditioned on the prompt question, answer, and passages. There are three main drawbacks to S PAN S E Q G EN . Firstly, a complete coverage of a"
2021.acl-long.253,D19-1284,0,0.0226563,"Missing"
2021.acl-long.253,W19-4805,1,0.890201,"Missing"
2021.acl-long.253,N19-4013,0,0.0371484,"Missing"
2021.acl-long.253,2020.emnlp-main.466,0,0.147729,"s, and then verify and filter out the incorrect questionanswer pairs to arrive at the final disambiguated output. Our model, named R EFUEL, achieves a new state-of-the-art performance on the A MBIG QA dataset, and shows competitive performance on NQ- OPEN and TriviaQA. The proposed round-trip prediction is a model-agnostic general approach for answering ambiguous open-domain questions, which improves our R EFUEL as well as several baseline models. We release source code for our models and experiments at https://github. com/amzn/refuel-open-domain-qa. 1 Figure 1: An example from the A MBIG QA (Min et al., 2020) dataset. The Prompt Question is gathered from Google search queries and has three interpretations upon reading Wikipedia. Disambiguated QA Pairs are the full set of acceptable answers, paired with the disambiguated rewriting of the prompt question. Introduction Open-domain Question Answering (QA) is the task of answering questions using a collection of passages with diverse topics (Chen et al., 2017; Guu et al., 2020; Karpukhin et al., 2020). Open-domain questions are highly likely to be ambiguous because people may not have the knowledge of relevant topics when formulating them. For example,"
2021.acl-long.253,P02-1040,0,0.109528,"d QA pairs as it is used for (qi , ai ). The overall correctness is calculated by F1 between predictions and references, Pm Pm 2Pf Rf ci i=1 ci Pf = , Rf = i=1 , F1f = . m n Pf + Rf All examples are evaluated for the answer prediction subtask, in which f function always yields 1. This metric is denoted as F1ans (all). For the subset of examples with multiple gold QA pairs, both answer prediction subtask and question disambiguation subtask are evaluated. The answer prediction metric only computed on this subset is denoted as F1ans (multi). To evaluate question disambiguation performance, BLEU (Papineni et al., 2002) and EDIT-F1 is used for the function f , denoted as F1BLEU and F1EDIT-F1 , respectively. EDIT-F1 compute the F1 score of added and deleted unigrams from the prompt question to the predicted disambiguated question towards references. 4.2 N K #QAs F1ans F1EDIT-F1 100 100 100 100 1000 ≈8 ≈8 8 100 100 1.17 1.14 1.42 1.54 1.55 39.7 41.7 44.7 45.4 48.4 7.2 7.1 10.0 10.7 11.2 Model S PAN S EQ G EN S PAN S EQ G EN* R EFUEL (w/o RTP) R EFUEL (w/o RTP) R EFUEL (w/o RTP) Table 2: Dev. set results of A MBIG QA as a function of the number of retrieval/reranking (N) and QA input (K) passages. #QAs: the ave"
2021.acl-long.253,2020.emnlp-main.468,0,0.0395981,"Missing"
2021.acl-long.253,D16-1264,0,0.134347,"Missing"
2021.acl-long.253,2020.emnlp-main.439,1,0.845795,"Missing"
2021.acl-long.315,P17-1171,0,0.0259057,"e is available during retrieval stage (e.g. their fusion-retriever is pretrained using hyperlinks between tables and paragraphs), whereas we don’t use any link information between tables and passages. Moreover, Chen et al. (2020b) proposed a closed-domain hybrid QA dataset where each table is linked to on average 44 passages. Different from ours, their purpose is to study multi-hop reasoning over both forms of information, and each question is still given the associated table. 3 Related Work Open Domain Question Answering ODQA has been extensively studied recently including extractive models (Chen et al., 2017; Clark and Gardner, 2018; Wang et al., 2019; Min et al., 2019; Yang et al., 2019) that predict spans from evidence passages, and generative models (Raffel et al., 2020; Method In this section, we describe our method for hybrid open-domain question answering. It mainly consists of three components: (1) a retrieval system; (2) a joint reranker and (3) a dual Seq2Seq model that uses fusion-in-decoder (Izacard and Grave, 2020) to generate direct answer or SQL query. 4079 Figure 1: The pipeline of our proposed hybrid model. The candidates are retrieved from knowledge source such as Wikipedia inclu"
2021.acl-long.315,2020.findings-emnlp.91,0,0.140003,"Missing"
2021.acl-long.315,P18-1078,0,0.0211319,"ng retrieval stage (e.g. their fusion-retriever is pretrained using hyperlinks between tables and paragraphs), whereas we don’t use any link information between tables and passages. Moreover, Chen et al. (2020b) proposed a closed-domain hybrid QA dataset where each table is linked to on average 44 passages. Different from ours, their purpose is to study multi-hop reasoning over both forms of information, and each question is still given the associated table. 3 Related Work Open Domain Question Answering ODQA has been extensively studied recently including extractive models (Chen et al., 2017; Clark and Gardner, 2018; Wang et al., 2019; Min et al., 2019; Yang et al., 2019) that predict spans from evidence passages, and generative models (Raffel et al., 2020; Method In this section, we describe our method for hybrid open-domain question answering. It mainly consists of three components: (1) a retrieval system; (2) a joint reranker and (3) a dual Seq2Seq model that uses fusion-in-decoder (Izacard and Grave, 2020) to generate direct answer or SQL query. 4079 Figure 1: The pipeline of our proposed hybrid model. The candidates are retrieved from knowledge source such as Wikipedia including both paragraphs and"
2021.acl-long.315,N19-1423,0,0.0365709,". 3.2 Joint Reranking The purpose of our reranking model is to produce a score si of how relevant a candidate (either an unstructured passage or table) is to a question. Specifically, the reranker input is the concatenation of question, a retrieved candidate-content, and its corresponding title if available2 , separated by special tokens shown in Figure 1. The candidate content can be either the unstructured 2 Wikipedia passages have page titles, and tables have table titles. text or flattened table. We use BERTbase model in this paper. Following Nogueira and Cho (2019), we finetune the BERT (Devlin et al., 2019) model using the following loss: X X L= log(si ) log(1 si ). (1) i2Ipos i2Ineg The Ipos is sampled from all relevant BM25 candidates, and the set Ineg is sampled from all non-relevant BM25 candidates. Different from Nogueira and Cho (2019), during training, for each question, we sample 64 candidates including one positive candidate and 63 negative candidates, that is, |Ipos |= 1 and |Ineg |= 63. If none of the 200 candidates is relevant, we skip the question. During inference, we use the hybrid reranker to assign a score to each of the 200 candidates, and choose the top 50 candidates as the in"
2021.acl-long.315,P19-1444,0,0.110496,"reasoning, as well as offering full explainability. In practice, an ideal ODQA model should be able to retrieve evidence from both unstructured textual and structured tabular information sources, as some questions are better answered by tabular evidence from databases. For example, the current state-of-the-art ODQA models struggle on questions that involve aggregation operations such as counting or averaging. One line of research on accessing databases, although not open domain, is translating natural language questions into SQL queries (Zhong et al., 2017; Xu et al., 2017; Yu et al., 2018c; Guo et al., 2019; Wang et al., 2018a, 2020; Yu et al., 2018a; Guo and Gao, 2019; Choi et al., 2020). These methods all rely on knowing the associated table for each question in advance, and hence are not trivially applicable to the open-domain setting, where the relevant evidence might come from millions of tables. In this paper, we provide a solution to the aforementioned problem by empowering the current generative ODQA models with the Text2SQL ability. More specifically, we propose a dual readerparser (D U R E PA) framework that can take both textual and tabular data as input, and generate either direct an"
2021.acl-long.315,2020.acl-main.398,0,0.0322855,"which only contains 29 tables and lacks annotation in their training set. Recently, with datasets like WikiSQL (Zhong et al., 2017), Spider (Yu et al., 2018c) and CoSQL (Yu et al., 2019) being introduced, many works have shown promising progress on these dataset (Yu et al., 2018b; He et al., 2019; Hwang et al., 2019; Min et al., 2019; Wang et al., 2020; Choi et al., 2020; Guo et al., 2019; Lyu et al., 2020; Zhang et al., 2019; Zhong et al., 2020; Shi et al., 2020). Another line of work proposes to reason over tables without generating logical forms (Neelakantan et al., 2015; Lu et al., 2016; Herzig et al., 2020; Yin et al., 2020). However, they are all closed-domain and each question is given the associated table. Hybrid QA Chen et al. (2020a) also proposed an open-domain QA problem with textual and tabular evidence. Unlike our problem, they generate an answer directly from the tabular evidence instead of generating an SQL query. In addition, they assume some contextual information about table is available during retrieval stage (e.g. their fusion-retriever is pretrained using hyperlinks between tables and paragraphs), whereas we don’t use any link information between tables and passages. Moreover,"
2021.acl-long.315,2020.emnlp-main.550,0,0.0848972,"Missing"
2021.acl-long.315,Q19-1026,0,0.157349,"s and the 11th International Joint Conference on Natural Language Processing, pages 4078–4088 August 1–6, 2021. ©2021 Association for Computational Linguistics the question, and finally we use a fusion-in-decoder model (Izacard and Grave, 2020) for our readerparser, which takes all the reranked candidates in addition to the question to generate direct answers or SQL queries. To evaluate the effectiveness of our D U R E PA, we construct a hybrid dataset that combines SQuAD (Rajpurkar et al., 2016) and WikiSQL (Zhong et al., 2017) questions. We also conduct experiments on NaturalQuestions (NQ) (Kwiatkowski et al., 2019) and OTT-QA (Chen et al., 2020a) to evaluate DuRePa performance. As textual and tabular open-domain knowledge, we used textual and tabular data from Wikipedia via Wikidumps (from Dec. 21, 2016) and Wikitables (Bhagavatula et al., 2015). We study the model performance on different kinds of questions, where some of them only need one supporting evidence type while others need both textual and tabular evidence. On all question types, D U R E PA performs significantly better than baseline models that were trained on a single evidence type. We also demonstrate that D U R E PA can generate humaninte"
2021.acl-long.315,W16-0105,0,0.0296435,"d SENLIDB dataset which only contains 29 tables and lacks annotation in their training set. Recently, with datasets like WikiSQL (Zhong et al., 2017), Spider (Yu et al., 2018c) and CoSQL (Yu et al., 2019) being introduced, many works have shown promising progress on these dataset (Yu et al., 2018b; He et al., 2019; Hwang et al., 2019; Min et al., 2019; Wang et al., 2020; Choi et al., 2020; Guo et al., 2019; Lyu et al., 2020; Zhang et al., 2019; Zhong et al., 2020; Shi et al., 2020). Another line of work proposes to reason over tables without generating logical forms (Neelakantan et al., 2015; Lu et al., 2016; Herzig et al., 2020; Yin et al., 2020). However, they are all closed-domain and each question is given the associated table. Hybrid QA Chen et al. (2020a) also proposed an open-domain QA problem with textual and tabular evidence. Unlike our problem, they generate an answer directly from the tabular evidence instead of generating an SQL query. In addition, they assume some contextual information about table is available during retrieval stage (e.g. their fusion-retriever is pretrained using hyperlinks between tables and paragraphs), whereas we don’t use any link information between tables and"
2021.acl-long.315,D19-1284,0,0.055593,"rectly generate the answers. Wang et al. (2018b,c); Nogueira and Cho (2019) proposed to rerank the retrieved passages to get higher top-n recall. Table Parsing Text2SQL is a task to translate natural questions to executable SQL queries. Brad et al. (2017) proposed SENLIDB dataset which only contains 29 tables and lacks annotation in their training set. Recently, with datasets like WikiSQL (Zhong et al., 2017), Spider (Yu et al., 2018c) and CoSQL (Yu et al., 2019) being introduced, many works have shown promising progress on these dataset (Yu et al., 2018b; He et al., 2019; Hwang et al., 2019; Min et al., 2019; Wang et al., 2020; Choi et al., 2020; Guo et al., 2019; Lyu et al., 2020; Zhang et al., 2019; Zhong et al., 2020; Shi et al., 2020). Another line of work proposes to reason over tables without generating logical forms (Neelakantan et al., 2015; Lu et al., 2016; Herzig et al., 2020; Yin et al., 2020). However, they are all closed-domain and each question is given the associated table. Hybrid QA Chen et al. (2020a) also proposed an open-domain QA problem with textual and tabular evidence. Unlike our problem, they generate an answer directly from the tabular evidence instead of generating an SQ"
2021.acl-long.315,2020.emnlp-main.466,0,0.0494121,"QA datasets, the hybrid methods consistently outperforms the baseline models that only take homogeneous input by a large margin. Specifically we achieve state-of-theart performance on OpenSQuAD dataset using a T5-base model. In a detailed analysis, we demonstrate that the being able to generate structural SQL queries can always bring gains, especially for those questions that requires complex reasoning. 1 Introduction Open-domain question answering (ODQA) is a task to answer factoid questions without a prespecified domain. Recently, generative models (Roberts et al., 2020; Lewis et al., 2020; Min et al., 2020; Izacard and Grave, 2020) have achieved the state-of-the-art performance on many ODQA tasks. These approaches all share the common pipeline where the first stage is retrieving evidence from the free-form text in Wikipedia. However, a large amount of world’s knowledge is not stored as plain text but in structured databases, and need to be accessed using query languages such as SQL. Furthermore, query languages can answer questions that require complex reasoning, as well as offering full explainability. In practice, an ideal ODQA model should be able to retrieve evidence from both unstructured"
2021.acl-long.315,D16-1264,0,0.309104,"AlexanderYogurt/Hybrid-Open-QA 4078 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4078–4088 August 1–6, 2021. ©2021 Association for Computational Linguistics the question, and finally we use a fusion-in-decoder model (Izacard and Grave, 2020) for our readerparser, which takes all the reranked candidates in addition to the question to generate direct answers or SQL queries. To evaluate the effectiveness of our D U R E PA, we construct a hybrid dataset that combines SQuAD (Rajpurkar et al., 2016) and WikiSQL (Zhong et al., 2017) questions. We also conduct experiments on NaturalQuestions (NQ) (Kwiatkowski et al., 2019) and OTT-QA (Chen et al., 2020a) to evaluate DuRePa performance. As textual and tabular open-domain knowledge, we used textual and tabular data from Wikipedia via Wikidumps (from Dec. 21, 2016) and Wikitables (Bhagavatula et al., 2015). We study the model performance on different kinds of questions, where some of them only need one supporting evidence type while others need both textual and tabular evidence. On all question types, D U R E PA performs significantly better"
2021.acl-long.315,2020.emnlp-main.437,0,0.0258695,"Missing"
2021.acl-long.315,2020.acl-main.677,0,0.0265935,"e answers. Wang et al. (2018b,c); Nogueira and Cho (2019) proposed to rerank the retrieved passages to get higher top-n recall. Table Parsing Text2SQL is a task to translate natural questions to executable SQL queries. Brad et al. (2017) proposed SENLIDB dataset which only contains 29 tables and lacks annotation in their training set. Recently, with datasets like WikiSQL (Zhong et al., 2017), Spider (Yu et al., 2018c) and CoSQL (Yu et al., 2019) being introduced, many works have shown promising progress on these dataset (Yu et al., 2018b; He et al., 2019; Hwang et al., 2019; Min et al., 2019; Wang et al., 2020; Choi et al., 2020; Guo et al., 2019; Lyu et al., 2020; Zhang et al., 2019; Zhong et al., 2020; Shi et al., 2020). Another line of work proposes to reason over tables without generating logical forms (Neelakantan et al., 2015; Lu et al., 2016; Herzig et al., 2020; Yin et al., 2020). However, they are all closed-domain and each question is given the associated table. Hybrid QA Chen et al. (2020a) also proposed an open-domain QA problem with textual and tabular evidence. Unlike our problem, they generate an answer directly from the tabular evidence instead of generating an SQL query. In additio"
2021.acl-long.315,D19-1599,1,0.892273,"Missing"
2021.acl-long.315,N19-4013,0,0.0351031,"Missing"
2021.acl-long.315,2020.acl-main.745,0,0.0660488,"29 tables and lacks annotation in their training set. Recently, with datasets like WikiSQL (Zhong et al., 2017), Spider (Yu et al., 2018c) and CoSQL (Yu et al., 2019) being introduced, many works have shown promising progress on these dataset (Yu et al., 2018b; He et al., 2019; Hwang et al., 2019; Min et al., 2019; Wang et al., 2020; Choi et al., 2020; Guo et al., 2019; Lyu et al., 2020; Zhang et al., 2019; Zhong et al., 2020; Shi et al., 2020). Another line of work proposes to reason over tables without generating logical forms (Neelakantan et al., 2015; Lu et al., 2016; Herzig et al., 2020; Yin et al., 2020). However, they are all closed-domain and each question is given the associated table. Hybrid QA Chen et al. (2020a) also proposed an open-domain QA problem with textual and tabular evidence. Unlike our problem, they generate an answer directly from the tabular evidence instead of generating an SQL query. In addition, they assume some contextual information about table is available during retrieval stage (e.g. their fusion-retriever is pretrained using hyperlinks between tables and paragraphs), whereas we don’t use any link information between tables and passages. Moreover, Chen et al. (2020b)"
2021.acl-long.315,N18-2093,0,0.144196,"at require complex reasoning, as well as offering full explainability. In practice, an ideal ODQA model should be able to retrieve evidence from both unstructured textual and structured tabular information sources, as some questions are better answered by tabular evidence from databases. For example, the current state-of-the-art ODQA models struggle on questions that involve aggregation operations such as counting or averaging. One line of research on accessing databases, although not open domain, is translating natural language questions into SQL queries (Zhong et al., 2017; Xu et al., 2017; Yu et al., 2018c; Guo et al., 2019; Wang et al., 2018a, 2020; Yu et al., 2018a; Guo and Gao, 2019; Choi et al., 2020). These methods all rely on knowing the associated table for each question in advance, and hence are not trivially applicable to the open-domain setting, where the relevant evidence might come from millions of tables. In this paper, we provide a solution to the aforementioned problem by empowering the current generative ODQA models with the Text2SQL ability. More specifically, we propose a dual readerparser (D U R E PA) framework that can take both textual and tabular data as input, and genera"
2021.acl-long.315,D18-1193,0,0.122604,"at require complex reasoning, as well as offering full explainability. In practice, an ideal ODQA model should be able to retrieve evidence from both unstructured textual and structured tabular information sources, as some questions are better answered by tabular evidence from databases. For example, the current state-of-the-art ODQA models struggle on questions that involve aggregation operations such as counting or averaging. One line of research on accessing databases, although not open domain, is translating natural language questions into SQL queries (Zhong et al., 2017; Xu et al., 2017; Yu et al., 2018c; Guo et al., 2019; Wang et al., 2018a, 2020; Yu et al., 2018a; Guo and Gao, 2019; Choi et al., 2020). These methods all rely on knowing the associated table for each question in advance, and hence are not trivially applicable to the open-domain setting, where the relevant evidence might come from millions of tables. In this paper, we provide a solution to the aforementioned problem by empowering the current generative ODQA models with the Text2SQL ability. More specifically, we propose a dual readerparser (D U R E PA) framework that can take both textual and tabular data as input, and genera"
2021.acl-long.315,D19-1204,0,0.0281337,"ns that require complex reasoning in the ODQA setting. 2 Roberts et al., 2020; Min et al., 2020; Lewis et al., 2020; Izacard and Grave, 2020) that directly generate the answers. Wang et al. (2018b,c); Nogueira and Cho (2019) proposed to rerank the retrieved passages to get higher top-n recall. Table Parsing Text2SQL is a task to translate natural questions to executable SQL queries. Brad et al. (2017) proposed SENLIDB dataset which only contains 29 tables and lacks annotation in their training set. Recently, with datasets like WikiSQL (Zhong et al., 2017), Spider (Yu et al., 2018c) and CoSQL (Yu et al., 2019) being introduced, many works have shown promising progress on these dataset (Yu et al., 2018b; He et al., 2019; Hwang et al., 2019; Min et al., 2019; Wang et al., 2020; Choi et al., 2020; Guo et al., 2019; Lyu et al., 2020; Zhang et al., 2019; Zhong et al., 2020; Shi et al., 2020). Another line of work proposes to reason over tables without generating logical forms (Neelakantan et al., 2015; Lu et al., 2016; Herzig et al., 2020; Yin et al., 2020). However, they are all closed-domain and each question is given the associated table. Hybrid QA Chen et al. (2020a) also proposed an open-domain QA"
2021.acl-long.315,D18-1425,0,0.0537764,"Missing"
2021.acl-long.315,D19-1537,0,0.0277089,"Missing"
2021.acl-long.315,2020.emnlp-main.558,0,0.027544,"sages to get higher top-n recall. Table Parsing Text2SQL is a task to translate natural questions to executable SQL queries. Brad et al. (2017) proposed SENLIDB dataset which only contains 29 tables and lacks annotation in their training set. Recently, with datasets like WikiSQL (Zhong et al., 2017), Spider (Yu et al., 2018c) and CoSQL (Yu et al., 2019) being introduced, many works have shown promising progress on these dataset (Yu et al., 2018b; He et al., 2019; Hwang et al., 2019; Min et al., 2019; Wang et al., 2020; Choi et al., 2020; Guo et al., 2019; Lyu et al., 2020; Zhang et al., 2019; Zhong et al., 2020; Shi et al., 2020). Another line of work proposes to reason over tables without generating logical forms (Neelakantan et al., 2015; Lu et al., 2016; Herzig et al., 2020; Yin et al., 2020). However, they are all closed-domain and each question is given the associated table. Hybrid QA Chen et al. (2020a) also proposed an open-domain QA problem with textual and tabular evidence. Unlike our problem, they generate an answer directly from the tabular evidence instead of generating an SQL query. In addition, they assume some contextual information about table is available during retrieval stage (e.g"
2021.acl-long.536,N19-4009,0,0.0292469,"e - each summary sentence usually corresponds to an existing sentence in the input document. Evaluation metrics: We use the ROUGE (Lin, 2004) to measure general summarizaiton quality. For factual consistency, we use the QAGS protocol (see Appendix for more details) as well as the FactCC model (Kry´sci´nski et al., 2019) downloaded directly from the official website.4 In contrast to QAGS, FactCC is a BERT-based classification model that makes a binary prediction if the given claim sentence is factually consistent or not with the given input document. Implementation details: We use the Fairseq (Ott et al., 2019) implementation of BART-large (Lewis et al., 2019) for the summarization model as it is shown to achieve the state-of-the-art ROUGE scores for this task. We fine-tune the BART-large model with the standard learning rate of 3 × 10−5 4 https://github.com/salesforce/factCC Figure 3: Correlation between Q UALS and QAGS on XSUM (left) and CNNDM (right). The average QAGS tend to increase with the increase in Q UALS. on XSUM and CNNDM respectively to establish the MLE baselines. We then initialize C ON S EQ with the MLE baseline models. In C ON S EQ we use a learning rate of 3 × 10−6 . For evaluation"
2021.acl-long.536,2020.emnlp-main.439,1,0.828378,"Missing"
2021.acl-long.536,W17-2623,0,0.0355424,"Missing"
2021.acl-long.536,2020.acl-main.450,0,0.134957,"rnational Joint Conference on Natural Language Processing, pages 6881–6894 August 1–6, 2021. ©2021 Association for Computational Linguistics Figure 2: QAGen model: for an input text (p), it generates a question (q) followed by an answer (a). Figure 1: Comparison between QAGS (top) and Q UALS (bottom) protocols. Q UALS uses only one QAGen model instead of the AE, QG and QA models used in QAGS. factualness. Our main contributions lie in both areas. First, we propose an efficient automatic evaluation metric for factual consistency that is a simplification of the recently published QAGS protocol (Wang et al., 2020). Evaluating QAGS is computationally expensive and ill-suited for being part of the model training process. Our proposed protocol achieves a 55x speedup while correlating closely with QAGS1 . Second, we propose a new contrastive learning method that uses factualness as a training objective. We demonstrate through experiments that our method improves the factual consistency of summarization models measured by both automatic metrics such as QAGS as well as human evaluation. 2 An Efficient Metric for Factual Consistency In order to improve factual consistency of summarization models, we must have"
2021.acl-long.536,P18-2124,0,0.113053,"Missing"
2021.acl-long.536,D16-1264,0,0.104627,"Missing"
2021.eacl-main.235,N19-1423,0,0.0139778,"opose a set of simple metrics to quantify factual consistency at the entitylevel. We analyze the factual quality of summaries produced by the state-of-the-art BART model (Lewis et al., 2019) on three news datasets. We then propose several techniques including data filtering, multi-task learning and joint sequence generation to improve performance on these metrics. We leave the relation level consistency to future work. 2 Related work Large transformer-based neural architectures combined with pre-training have set new records across many natural language processing tasks (Vaswani et al., 2017; Devlin et al., 2019; 2727 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 2727–2733 April 19 - 23, 2021. ©2021 Association for Computational Linguistics Radford et al., 2019). In particular, the BART model (Lewis et al., 2019) has shown superior performance in many text generation tasks including abstractive summarization. In contrast to encoder-only pre-training such as in BERT (Devlin et al., 2019) or decoder-only pre-training such as in GPT-2 (Radford et al., 2019), BART is an encoder-decoder transformer-based neural translation model jointly"
2021.eacl-main.235,W18-2706,0,0.0232806,"uthors in (Kry´sci´nski et al., 2019) proposed to train a neural network model to classify if a summary is factually consistent with a given source document, similar to a natural language inference task. In the dialogue generation setting, authors in (Li et al., 2019) proposed using unlikelihood to surpress logically inconsistent responses. Our work is complementary to such existing approaches as we focus on simple entity-level metrics to quantify and improve factual consistency. Our goal of improving entity-level metrics of summaries is also related to controllable abstractive summarization (Fan et al., 2018), where a list of named-entities that a user wants to see in the summary can be passed as input to influence the generated summary. In contrast, our goal is to predict which entities are summary-worthy while generating the summary that contains them. In this view we are trying to solve a more challenging problem. 3 Entity-level factual consistency metrics We propose three new metrics that rely on off-theshelf tools to perform Named-Entity Recognition (NER). 1 We use N (t) and N (h) to denote the number of named-entities in the target (gold summary) and hypothesis (generated summary), respectiv"
2021.eacl-main.235,W19-8665,0,0.0630315,"Missing"
2021.eacl-main.235,N18-1065,0,0.0448655,"Missing"
2021.eacl-main.235,D19-1051,0,0.0373461,"Missing"
2021.eacl-main.235,2020.acl-main.703,0,0.032094,"Missing"
2021.eacl-main.235,K16-1028,1,0.883013,"Missing"
2021.eacl-main.235,D18-1206,0,0.045514,"Missing"
2021.eacl-main.235,N19-4009,0,0.0147707,"rthy named-entities, followed by a special token, and then the summary. We call this approach JAENS (Join sAlient ENtity and Summary generation). Similar to the multitask learning approach discussed earlier, JAENS encourages the model to jointly learn to identify the summary-worthy named-entities while learning to generate summaries. Since the decoder generates the salient named-entities first, the summaries that JAENS generate can further attend to these salient named-entities through decoder self-attention. 6 Experiment results We use the pre-trained BART-large model in the Fairseq library (Ott et al., 2019) to fine-tune on the 3 summarization datasets.3 The appendix contains additional details of experimental setup. In Table 3, we show the effect of the entitybased data filtering. For each dataset, we train two separate models: using the training data before and after entity-based data filtering as shown in Table 2. We evaluate both models on the “clean” test set after entity-based data 3 Our code is available at https://github.com/ amazon-research/fact-check-summarization 2729 train Newsroom val test train CNNDM val test train XSUM val test original 922,500 (1.58) 100,968 (1.60) 100,933 (1.59)"
2021.eacl-main.235,P19-1363,0,0.0279786,"the BART model (Lewis et al., 2019) has shown superior performance in many text generation tasks including abstractive summarization. In contrast to encoder-only pre-training such as in BERT (Devlin et al., 2019) or decoder-only pre-training such as in GPT-2 (Radford et al., 2019), BART is an encoder-decoder transformer-based neural translation model jointly pre-trained to reconstruct corrupted input sequences of text. Several authors have pointed out the problem of factual inconsistency in abstractive summarization models (Kryscinski et al., 2019; Kry´sci´nski et al., 2019; Cao et al., 2018; Welleck et al., 2019). The authors in (Kry´sci´nski et al., 2019) proposed to train a neural network model to classify if a summary is factually consistent with a given source document, similar to a natural language inference task. In the dialogue generation setting, authors in (Li et al., 2019) proposed using unlikelihood to surpress logically inconsistent responses. Our work is complementary to such existing approaches as we focus on simple entity-level metrics to quantify and improve factual consistency. Our goal of improving entity-level metrics of summaries is also related to controllable abstractive summariz"
2021.eacl-main.235,2020.emnlp-demos.6,0,0.0351401,"Missing"
2021.eacl-main.26,C16-1236,0,0.015436,"ge-scale KBs, significant advancements have been made over the years. One main research direction views KBQA as a semantic matching task (Bordes et al., 2014; Dong et al., 2015; Dai et al., 2016; Hao et al., 2017; Mohammed et al., 2018; Yu et al., 2018; Wu et al., 2019; Chen et al., 2019a; Petrochuk and Zettlemoyer, 2018), and finds a relation-chain within KBs that is most similar to the question in a common semantic space, where the relation-chain can be 1-hop, 2-hop or multi-hop (Chen et al., 2019b). Another research direction formulates KBQA as a semantic parsing task (Berant et al., 2013; Bao et al., 2016; Luo et al., 2018), and tackles questions that involve complex reasoning, such as ordinal (e.g. What is the second largest fulfillment center of Amazon?), and aggregation (e.g. How many fulfillment centers does Amazon have?). Most recently, some studies proposed to derive answers from both KBs and free-text corpus to deal with the low-coverage issue of KBs (Xu et al., 2016; Sun et al., 2018; Xiong et al., 2019; Sun et al., 2019). In this paper, we follow the first research direction since the relationchain type of questions counts the vast majority of real-life questions (Berant et al., 2013;"
2021.eacl-main.26,D13-1160,0,0.365824,"e availability of large-scale KBs, significant advancements have been made over the years. One main research direction views KBQA as a semantic matching task (Bordes et al., 2014; Dong et al., 2015; Dai et al., 2016; Hao et al., 2017; Mohammed et al., 2018; Yu et al., 2018; Wu et al., 2019; Chen et al., 2019a; Petrochuk and Zettlemoyer, 2018), and finds a relation-chain within KBs that is most similar to the question in a common semantic space, where the relation-chain can be 1-hop, 2-hop or multi-hop (Chen et al., 2019b). Another research direction formulates KBQA as a semantic parsing task (Berant et al., 2013; Bao et al., 2016; Luo et al., 2018), and tackles questions that involve complex reasoning, such as ordinal (e.g. What is the second largest fulfillment center of Amazon?), and aggregation (e.g. How many fulfillment centers does Amazon have?). Most recently, some studies proposed to derive answers from both KBs and free-text corpus to deal with the low-coverage issue of KBs (Xu et al., 2016; Sun et al., 2018; Xiong et al., 2019; Sun et al., 2019). In this paper, we follow the first research direction since the relationchain type of questions counts the vast majority of real-life questions (Be"
2021.eacl-main.26,D14-1067,0,0.447343,"ultitask learning, the unified model obtains further improvements with only 1/3 of the original parameters. Our final model achieves competitive results on the SimpleQuestions dataset and superior performance on the FreebaseQA dataset. 1 Introduction Answering natural language questions by searching over large-scale knowledge bases (KBQA) is highly demanded by real-life applications, such as Google Assistant, Siri, and Alexa. Owing to the availability of large-scale KBs, significant advancements have been made over the years. One main research direction views KBQA as a semantic matching task (Bordes et al., 2014; Dong et al., 2015; Dai et al., 2016; Hao et al., 2017; Mohammed et al., 2018; Yu et al., 2018; Wu et al., 2019; Chen et al., 2019a; Petrochuk and Zettlemoyer, 2018), and finds a relation-chain within KBs that is most similar to the question in a common semantic space, where the relation-chain can be 1-hop, 2-hop or multi-hop (Chen et al., 2019b). Another research direction formulates KBQA as a semantic parsing task (Berant et al., 2013; Bao et al., 2016; Luo et al., 2018), and tackles questions that involve complex reasoning, such as ordinal (e.g. What is the second largest fulfillment cente"
2021.eacl-main.26,P13-1042,0,0.0198489,"objective for the selected mini-batch. 6 Experiments We evaluate the effectiveness of our model on standard benchmarks in this section. We first conduct experiments on each sub-task with a separate BERT model in Section 6.2, 6.3 and 6.4, then evaluate the influence of sharing a BERT encoder for all three models in Section 6.5. Finally, we benchmark our method on full Freebase in Section 6.6. 6.1 Datasets and Basic Settings We evaluate our proposed model on two large-scale benchmarks: SimpleQuestions and FreebaseQA. Other existing datasets, such as WebQuestions (Berant et al., 2013), Free917 (Cai and Yates, 2013) and WebQSP (Yih et al., 2016), are not considered, because they only contain few thousands of questions which is even less than the number of relation types in Freebase. SimpleQuestions: The SimpleQuestions dataset (Bordes et al., 2015) is so far the largest 351 KBQA dataset. It consists of 108,442 English questions written by human annotators, and all questions can be answered by 1-hop relation chains in Freebase. Each question is annotated with a gold-standard subject-relation-object triple from Freebase. We follow the official train/dev/test split. To fairly compare with previous work, we"
2021.eacl-main.26,P17-1171,0,0.0234016,"vious methods usually narrow down the search space based on some heuristic rules. For example, Yih et al. (2015) and Wu et al. (2019) used keyword search to collect all nodes that have one alias exactly matching the topic entity, and Yin et al. (2016) collected all nodes that have at least one word overlapping with the topic entity. Once a smaller set of candidates is selected, complicated neural networks can be utilized to compute the similarity between a candidate node and the topic entity in the question context. Inspired from the recent success of question answering over free-text corpus (Chen et al., 2017; Wang et al., 2018, 2019), we propose a retrieveand-rerank method to solve the entity linking task in two steps. In the first retrieval step, we create an inverted index for all entity nodes, where each node is represented with all tokens from its aliases and description. Then, we use the topic entity t as a query to retrieve top-K candidate nodes from the index with the TF-IDF algorithm2 . The similar method is also used by Vakulenko et al. (2019) and Nedelchev et al. (2020). This information retrieval (IR) method is better than previous work in the following ways. First, our method can find"
2021.eacl-main.26,N19-1299,0,0.0277876,"Missing"
2021.eacl-main.26,N19-1031,0,0.0622289,"competitive results on the SimpleQuestions dataset and superior performance on the FreebaseQA dataset. 1 Introduction Answering natural language questions by searching over large-scale knowledge bases (KBQA) is highly demanded by real-life applications, such as Google Assistant, Siri, and Alexa. Owing to the availability of large-scale KBs, significant advancements have been made over the years. One main research direction views KBQA as a semantic matching task (Bordes et al., 2014; Dong et al., 2015; Dai et al., 2016; Hao et al., 2017; Mohammed et al., 2018; Yu et al., 2018; Wu et al., 2019; Chen et al., 2019a; Petrochuk and Zettlemoyer, 2018), and finds a relation-chain within KBs that is most similar to the question in a common semantic space, where the relation-chain can be 1-hop, 2-hop or multi-hop (Chen et al., 2019b). Another research direction formulates KBQA as a semantic parsing task (Berant et al., 2013; Bao et al., 2016; Luo et al., 2018), and tackles questions that involve complex reasoning, such as ordinal (e.g. What is the second largest fulfillment center of Amazon?), and aggregation (e.g. How many fulfillment centers does Amazon have?). Most recently, some studies proposed to deriv"
2021.eacl-main.26,P16-1076,0,0.123955,"ains further improvements with only 1/3 of the original parameters. Our final model achieves competitive results on the SimpleQuestions dataset and superior performance on the FreebaseQA dataset. 1 Introduction Answering natural language questions by searching over large-scale knowledge bases (KBQA) is highly demanded by real-life applications, such as Google Assistant, Siri, and Alexa. Owing to the availability of large-scale KBs, significant advancements have been made over the years. One main research direction views KBQA as a semantic matching task (Bordes et al., 2014; Dong et al., 2015; Dai et al., 2016; Hao et al., 2017; Mohammed et al., 2018; Yu et al., 2018; Wu et al., 2019; Chen et al., 2019a; Petrochuk and Zettlemoyer, 2018), and finds a relation-chain within KBs that is most similar to the question in a common semantic space, where the relation-chain can be 1-hop, 2-hop or multi-hop (Chen et al., 2019b). Another research direction formulates KBQA as a semantic parsing task (Berant et al., 2013; Bao et al., 2016; Luo et al., 2018), and tackles questions that involve complex reasoning, such as ordinal (e.g. What is the second largest fulfillment center of Amazon?), and aggregation (e.g."
2021.eacl-main.26,N19-1423,0,0.189986,"cy (TF-IDF) is a ranking function usually used together with an inverted index to estimate the relevance of documents to a given search query (Sch¨utze et al., 2008). P r(a|Q, K) = P r(t, e, r|Q, K) = Pt (t|Q, K)Pl (e|t, Q, K) Pr (r|e, t, Q, K) (2) 4 where Pt (t|Q, K) is the model for topic entity detection, Pl (e|t, Q, K) models the entity linking process, and Pr (r|e, t, Q, K) is the component for relation detection stage. We will discuss how to parameterize these components in Section 4. 3 Background We briefly introduce some background required by the following sections. BERT: BERT model (Devlin et al., 2019) follows the multi-head self-attention architecture (Vaswani et al., 2017), and is pre-trained with a masked language modeling objective on a largescale text corpus. It has achieved state-of-the-art performance on a bunch of textual tasks. Specifically, for semantic matching tasks, BERT simply concatenates two textual sequences together, and encodes the new sequence with multiple selfattention layers. Then, the output vector of the first token is fed into a linear layer to compute the similarity score between the two input textual sequences. Freebase: We take Freebase (Bollacker et al., 2008)"
2021.eacl-main.26,P15-1026,0,0.340705,"e unified model obtains further improvements with only 1/3 of the original parameters. Our final model achieves competitive results on the SimpleQuestions dataset and superior performance on the FreebaseQA dataset. 1 Introduction Answering natural language questions by searching over large-scale knowledge bases (KBQA) is highly demanded by real-life applications, such as Google Assistant, Siri, and Alexa. Owing to the availability of large-scale KBs, significant advancements have been made over the years. One main research direction views KBQA as a semantic matching task (Bordes et al., 2014; Dong et al., 2015; Dai et al., 2016; Hao et al., 2017; Mohammed et al., 2018; Yu et al., 2018; Wu et al., 2019; Chen et al., 2019a; Petrochuk and Zettlemoyer, 2018), and finds a relation-chain within KBs that is most similar to the question in a common semantic space, where the relation-chain can be 1-hop, 2-hop or multi-hop (Chen et al., 2019b). Another research direction formulates KBQA as a semantic parsing task (Berant et al., 2013; Bao et al., 2016; Luo et al., 2018), and tackles questions that involve complex reasoning, such as ordinal (e.g. What is the second largest fulfillment center of Amazon?), and"
2021.eacl-main.26,C18-1277,0,0.026732,"Missing"
2021.eacl-main.26,P17-1021,0,0.0175203,"vements with only 1/3 of the original parameters. Our final model achieves competitive results on the SimpleQuestions dataset and superior performance on the FreebaseQA dataset. 1 Introduction Answering natural language questions by searching over large-scale knowledge bases (KBQA) is highly demanded by real-life applications, such as Google Assistant, Siri, and Alexa. Owing to the availability of large-scale KBs, significant advancements have been made over the years. One main research direction views KBQA as a semantic matching task (Bordes et al., 2014; Dong et al., 2015; Dai et al., 2016; Hao et al., 2017; Mohammed et al., 2018; Yu et al., 2018; Wu et al., 2019; Chen et al., 2019a; Petrochuk and Zettlemoyer, 2018), and finds a relation-chain within KBs that is most similar to the question in a common semantic space, where the relation-chain can be 1-hop, 2-hop or multi-hop (Chen et al., 2019b). Another research direction formulates KBQA as a semantic parsing task (Berant et al., 2013; Bao et al., 2016; Luo et al., 2018), and tackles questions that involve complex reasoning, such as ordinal (e.g. What is the second largest fulfillment center of Amazon?), and aggregation (e.g. How many fulfillme"
2021.eacl-main.26,D16-1166,0,0.0199712,", we describe how to parameterize Pt , Pl and Pr in Equation (2). 4.1 Topic Entity Detection Model Pt The goal of a topic entity detection model Pt (t|Q, K) is to identify a topic entity t that the question Q is asking about, where t is usually a substring of Q. Previous approaches for this task can be categorized into two types: (1) rule-based and (2) sequence labeling. The rule-based approaches take all entity names and their alias from a KB as a gazetteer, and n-grams of the question that exactly match with an entry in the gazetteer are taken as topic entities (Yih et al., 2015; Yao, 2015; He and Golub, 2016; Yu et al., 2017). The advantage of this method is that no machine learning models need to be involved. However, the drawbacks include: (1) topic entities need to have the exact same surface strings as they occur in KB, and (2) memory-efficient data structures need to be designed to load the massive gazetteer into memory (Yao, 2015). Other approaches leverage a sequence labeling model to tag consecutive tokens in the question Q as topic entities (Dai et al., 2016; Bordes et al., 2015; Mohammed et al., 2018; Wu et al., 2019). This approach is able to predict more precise topic entities, thus p"
2021.eacl-main.26,N19-1028,0,0.107931,"nd tackles questions that involve complex reasoning, such as ordinal (e.g. What is the second largest fulfillment center of Amazon?), and aggregation (e.g. How many fulfillment centers does Amazon have?). Most recently, some studies proposed to derive answers from both KBs and free-text corpus to deal with the low-coverage issue of KBs (Xu et al., 2016; Sun et al., 2018; Xiong et al., 2019; Sun et al., 2019). In this paper, we follow the first research direction since the relationchain type of questions counts the vast majority of real-life questions (Berant et al., 2013; Bordes et al., 2015; Jiang et al., 2019). Previous semantic matching methods for KBQA usually decompose the task into sequential subtasks consisting of topic entity detection, entity linking, and relation detection. For example in Figure 1, given the question “Who wrote the book Beau Geste?”, a KBQA system first identifies the topic entity “Beau Geste” from the question, then the topic entity is linked to an entity node (m.04wxy8) from a list of candidate nodes, and finally the relation book.written work.author is selected as the relation-chain leading to the final answer. Previous methods usually worked on a subset of KB in order t"
2021.eacl-main.26,P17-1147,0,0.0423839,"ritten by human annotators, and all questions can be answered by 1-hop relation chains in Freebase. Each question is annotated with a gold-standard subject-relation-object triple from Freebase. We follow the official train/dev/test split. To fairly compare with previous work, we leverage the released FB2M subset of Freebase as the back-end KB for this dataset. FB2M includes 2M entities and 5k relation types between these entities. FreebaseQA: FreebaseQA dataset (Jiang et al., 2019) is a large-scale dataset with 28K unique opendomain factoid questions which are collected from triviaQA dataset (Joshi et al., 2017) and other trivia websites. Each question can be answered by a 1hop or 2-hop relation-chain from Freebase. All questions have been matched to subject-predicateobject triples in Freebase, and verified by human annotators. Comparing with other KBQA datasets, FreebaseQA provides more linguistically sophisticated questions, because all questions are created independently from Freebase. FreebaseQA also released a new subset of Freebase, which includes 16M unique entities, and 182M triples. We follow the official train/dev/test split, and take the Freebase subset as the back-end KB for this dataset."
2021.eacl-main.26,D18-1242,0,0.0181055,"ificant advancements have been made over the years. One main research direction views KBQA as a semantic matching task (Bordes et al., 2014; Dong et al., 2015; Dai et al., 2016; Hao et al., 2017; Mohammed et al., 2018; Yu et al., 2018; Wu et al., 2019; Chen et al., 2019a; Petrochuk and Zettlemoyer, 2018), and finds a relation-chain within KBs that is most similar to the question in a common semantic space, where the relation-chain can be 1-hop, 2-hop or multi-hop (Chen et al., 2019b). Another research direction formulates KBQA as a semantic parsing task (Berant et al., 2013; Bao et al., 2016; Luo et al., 2018), and tackles questions that involve complex reasoning, such as ordinal (e.g. What is the second largest fulfillment center of Amazon?), and aggregation (e.g. How many fulfillment centers does Amazon have?). Most recently, some studies proposed to derive answers from both KBs and free-text corpus to deal with the low-coverage issue of KBs (Xu et al., 2016; Sun et al., 2018; Xiong et al., 2019; Sun et al., 2019). In this paper, we follow the first research direction since the relationchain type of questions counts the vast majority of real-life questions (Berant et al., 2013; Bordes et al., 201"
2021.eacl-main.26,N18-2047,0,0.196746,"1/3 of the original parameters. Our final model achieves competitive results on the SimpleQuestions dataset and superior performance on the FreebaseQA dataset. 1 Introduction Answering natural language questions by searching over large-scale knowledge bases (KBQA) is highly demanded by real-life applications, such as Google Assistant, Siri, and Alexa. Owing to the availability of large-scale KBs, significant advancements have been made over the years. One main research direction views KBQA as a semantic matching task (Bordes et al., 2014; Dong et al., 2015; Dai et al., 2016; Hao et al., 2017; Mohammed et al., 2018; Yu et al., 2018; Wu et al., 2019; Chen et al., 2019a; Petrochuk and Zettlemoyer, 2018), and finds a relation-chain within KBs that is most similar to the question in a common semantic space, where the relation-chain can be 1-hop, 2-hop or multi-hop (Chen et al., 2019b). Another research direction formulates KBQA as a semantic parsing task (Berant et al., 2013; Bao et al., 2016; Luo et al., 2018), and tackles questions that involve complex reasoning, such as ordinal (e.g. What is the second largest fulfillment center of Amazon?), and aggregation (e.g. How many fulfillment centers does Amazon"
2021.eacl-main.26,D18-1051,0,0.0117971,"on the SimpleQuestions dataset and superior performance on the FreebaseQA dataset. 1 Introduction Answering natural language questions by searching over large-scale knowledge bases (KBQA) is highly demanded by real-life applications, such as Google Assistant, Siri, and Alexa. Owing to the availability of large-scale KBs, significant advancements have been made over the years. One main research direction views KBQA as a semantic matching task (Bordes et al., 2014; Dong et al., 2015; Dai et al., 2016; Hao et al., 2017; Mohammed et al., 2018; Yu et al., 2018; Wu et al., 2019; Chen et al., 2019a; Petrochuk and Zettlemoyer, 2018), and finds a relation-chain within KBs that is most similar to the question in a common semantic space, where the relation-chain can be 1-hop, 2-hop or multi-hop (Chen et al., 2019b). Another research direction formulates KBQA as a semantic parsing task (Berant et al., 2013; Bao et al., 2016; Luo et al., 2018), and tackles questions that involve complex reasoning, such as ordinal (e.g. What is the second largest fulfillment center of Amazon?), and aggregation (e.g. How many fulfillment centers does Amazon have?). Most recently, some studies proposed to derive answers from both KBs and free-te"
2021.eacl-main.26,D16-1264,0,0.0461778,"llows the same architecture for the named en4 https://www.elastic.co/products/elasticsearch Models SimpleQ. EM F1 FreebaseQA EM F1 BIO Start/End Pt 94.9 96.4 97.3 97.8 65.1 74.3 75.2 81.5 Multi-task Pt 96.0 97.7 70.6 79.3 Table 1: Results for topic entity detection. tity recognition (NER) task in Devlin et al. (2019), where we use BIO schema to annotate each question token. Since the sequence labeling method may predict multiple spans to be topic entities, we choose the span with the maximum average token score as the final prediction. We employ the metrics exact match (EM) and F1 proposed in Rajpurkar et al. (2016) to evaluate the identified topic entities. Experimental results are shown in Table 1. We can see that our Start/End prediction model works better than the BIO sequence labeling baseline. Specifically, in FreebaseQA dataset, since the questions are longer and more complicated, our Start/End model outperforms the BIO sequence labeling model by a large margin. 6.3 Entity Linking Experiments We retrieve a list of candidate nodes for each question as follows. For questions in the training sets, we use the ground-truth topic entity as the query to retrieve top-100 candidate nodes. For questions in"
2021.eacl-main.26,D19-1242,0,0.015577,"e the relation-chain can be 1-hop, 2-hop or multi-hop (Chen et al., 2019b). Another research direction formulates KBQA as a semantic parsing task (Berant et al., 2013; Bao et al., 2016; Luo et al., 2018), and tackles questions that involve complex reasoning, such as ordinal (e.g. What is the second largest fulfillment center of Amazon?), and aggregation (e.g. How many fulfillment centers does Amazon have?). Most recently, some studies proposed to derive answers from both KBs and free-text corpus to deal with the low-coverage issue of KBs (Xu et al., 2016; Sun et al., 2018; Xiong et al., 2019; Sun et al., 2019). In this paper, we follow the first research direction since the relationchain type of questions counts the vast majority of real-life questions (Berant et al., 2013; Bordes et al., 2015; Jiang et al., 2019). Previous semantic matching methods for KBQA usually decompose the task into sequential subtasks consisting of topic entity detection, entity linking, and relation detection. For example in Figure 1, given the question “Who wrote the book Beau Geste?”, a KBQA system first identifies the topic entity “Beau Geste” from the question, then the topic entity is linked to an entity node (m.04wxy"
2021.eacl-main.26,D18-1455,0,0.0195831,"stion in a common semantic space, where the relation-chain can be 1-hop, 2-hop or multi-hop (Chen et al., 2019b). Another research direction formulates KBQA as a semantic parsing task (Berant et al., 2013; Bao et al., 2016; Luo et al., 2018), and tackles questions that involve complex reasoning, such as ordinal (e.g. What is the second largest fulfillment center of Amazon?), and aggregation (e.g. How many fulfillment centers does Amazon have?). Most recently, some studies proposed to derive answers from both KBs and free-text corpus to deal with the low-coverage issue of KBs (Xu et al., 2016; Sun et al., 2018; Xiong et al., 2019; Sun et al., 2019). In this paper, we follow the first research direction since the relationchain type of questions counts the vast majority of real-life questions (Berant et al., 2013; Bordes et al., 2015; Jiang et al., 2019). Previous semantic matching methods for KBQA usually decompose the task into sequential subtasks consisting of topic entity detection, entity linking, and relation detection. For example in Figure 1, given the question “Who wrote the book Beau Geste?”, a KBQA system first identifies the topic entity “Beau Geste” from the question, then the topic enti"
2021.eacl-main.26,D19-1599,1,0.867436,"Missing"
2021.eacl-main.26,P19-1417,0,0.0159766,"semantic space, where the relation-chain can be 1-hop, 2-hop or multi-hop (Chen et al., 2019b). Another research direction formulates KBQA as a semantic parsing task (Berant et al., 2013; Bao et al., 2016; Luo et al., 2018), and tackles questions that involve complex reasoning, such as ordinal (e.g. What is the second largest fulfillment center of Amazon?), and aggregation (e.g. How many fulfillment centers does Amazon have?). Most recently, some studies proposed to derive answers from both KBs and free-text corpus to deal with the low-coverage issue of KBs (Xu et al., 2016; Sun et al., 2018; Xiong et al., 2019; Sun et al., 2019). In this paper, we follow the first research direction since the relationchain type of questions counts the vast majority of real-life questions (Berant et al., 2013; Bordes et al., 2015; Jiang et al., 2019). Previous semantic matching methods for KBQA usually decompose the task into sequential subtasks consisting of topic entity detection, entity linking, and relation detection. For example in Figure 1, given the question “Who wrote the book Beau Geste?”, a KBQA system first identifies the topic entity “Beau Geste” from the question, then the topic entity is linked to an e"
2021.eacl-main.26,P16-1220,0,0.0137009,"imilar to the question in a common semantic space, where the relation-chain can be 1-hop, 2-hop or multi-hop (Chen et al., 2019b). Another research direction formulates KBQA as a semantic parsing task (Berant et al., 2013; Bao et al., 2016; Luo et al., 2018), and tackles questions that involve complex reasoning, such as ordinal (e.g. What is the second largest fulfillment center of Amazon?), and aggregation (e.g. How many fulfillment centers does Amazon have?). Most recently, some studies proposed to derive answers from both KBs and free-text corpus to deal with the low-coverage issue of KBs (Xu et al., 2016; Sun et al., 2018; Xiong et al., 2019; Sun et al., 2019). In this paper, we follow the first research direction since the relationchain type of questions counts the vast majority of real-life questions (Berant et al., 2013; Bordes et al., 2015; Jiang et al., 2019). Previous semantic matching methods for KBQA usually decompose the task into sequential subtasks consisting of topic entity detection, entity linking, and relation detection. For example in Figure 1, given the question “Who wrote the book Beau Geste?”, a KBQA system first identifies the topic entity “Beau Geste” from the question, t"
2021.eacl-main.26,N15-3014,0,0.0236683,"his section, we describe how to parameterize Pt , Pl and Pr in Equation (2). 4.1 Topic Entity Detection Model Pt The goal of a topic entity detection model Pt (t|Q, K) is to identify a topic entity t that the question Q is asking about, where t is usually a substring of Q. Previous approaches for this task can be categorized into two types: (1) rule-based and (2) sequence labeling. The rule-based approaches take all entity names and their alias from a KB as a gazetteer, and n-grams of the question that exactly match with an entry in the gazetteer are taken as topic entities (Yih et al., 2015; Yao, 2015; He and Golub, 2016; Yu et al., 2017). The advantage of this method is that no machine learning models need to be involved. However, the drawbacks include: (1) topic entities need to have the exact same surface strings as they occur in KB, and (2) memory-efficient data structures need to be designed to load the massive gazetteer into memory (Yao, 2015). Other approaches leverage a sequence labeling model to tag consecutive tokens in the question Q as topic entities (Dai et al., 2016; Bordes et al., 2015; Mohammed et al., 2018; Wu et al., 2019). This approach is able to predict more precise to"
2021.eacl-main.26,P15-1128,0,0.515219,"king for KBQA In this section, we describe how to parameterize Pt , Pl and Pr in Equation (2). 4.1 Topic Entity Detection Model Pt The goal of a topic entity detection model Pt (t|Q, K) is to identify a topic entity t that the question Q is asking about, where t is usually a substring of Q. Previous approaches for this task can be categorized into two types: (1) rule-based and (2) sequence labeling. The rule-based approaches take all entity names and their alias from a KB as a gazetteer, and n-grams of the question that exactly match with an entry in the gazetteer are taken as topic entities (Yih et al., 2015; Yao, 2015; He and Golub, 2016; Yu et al., 2017). The advantage of this method is that no machine learning models need to be involved. However, the drawbacks include: (1) topic entities need to have the exact same surface strings as they occur in KB, and (2) memory-efficient data structures need to be designed to load the massive gazetteer into memory (Yao, 2015). Other approaches leverage a sequence labeling model to tag consecutive tokens in the question Q as topic entities (Dai et al., 2016; Bordes et al., 2015; Mohammed et al., 2018; Wu et al., 2019). This approach is able to predict more"
2021.eacl-main.26,P16-2033,0,0.0140787,"batch. 6 Experiments We evaluate the effectiveness of our model on standard benchmarks in this section. We first conduct experiments on each sub-task with a separate BERT model in Section 6.2, 6.3 and 6.4, then evaluate the influence of sharing a BERT encoder for all three models in Section 6.5. Finally, we benchmark our method on full Freebase in Section 6.6. 6.1 Datasets and Basic Settings We evaluate our proposed model on two large-scale benchmarks: SimpleQuestions and FreebaseQA. Other existing datasets, such as WebQuestions (Berant et al., 2013), Free917 (Cai and Yates, 2013) and WebQSP (Yih et al., 2016), are not considered, because they only contain few thousands of questions which is even less than the number of relation types in Freebase. SimpleQuestions: The SimpleQuestions dataset (Bordes et al., 2015) is so far the largest 351 KBQA dataset. It consists of 108,442 English questions written by human annotators, and all questions can be answered by 1-hop relation chains in Freebase. Each question is annotated with a gold-standard subject-relation-object triple from Freebase. We follow the official train/dev/test split. To fairly compare with previous work, we leverage the released FB2M sub"
2021.eacl-main.26,C16-1164,1,0.879983,"Missing"
2021.eacl-main.26,P17-1053,1,0.921241,"parameterize Pt , Pl and Pr in Equation (2). 4.1 Topic Entity Detection Model Pt The goal of a topic entity detection model Pt (t|Q, K) is to identify a topic entity t that the question Q is asking about, where t is usually a substring of Q. Previous approaches for this task can be categorized into two types: (1) rule-based and (2) sequence labeling. The rule-based approaches take all entity names and their alias from a KB as a gazetteer, and n-grams of the question that exactly match with an entry in the gazetteer are taken as topic entities (Yih et al., 2015; Yao, 2015; He and Golub, 2016; Yu et al., 2017). The advantage of this method is that no machine learning models need to be involved. However, the drawbacks include: (1) topic entities need to have the exact same surface strings as they occur in KB, and (2) memory-efficient data structures need to be designed to load the massive gazetteer into memory (Yao, 2015). Other approaches leverage a sequence labeling model to tag consecutive tokens in the question Q as topic entities (Dai et al., 2016; Bordes et al., 2015; Mohammed et al., 2018; Wu et al., 2019). This approach is able to predict more precise topic entities, thus prunes some unimpor"
C10-2148,P98-1115,0,0.0510352,"earchers believe that the forms of a constituent and its subconstituents are determined more by the constituent’s head than any other of its lexical items (Charniak, 1997), so they annotate nonterminal symbols with the head words information. Both of the two PCFG based approaches have improved the basic PCFG based parsers significantly. However, neither of them has been guided by enough linguistic priori knowledge. Their parsing procedures are too mechanical. Because of this, the efficiency is always worse, and much more artificial ambiguities, which are different from linguistic ambiguities (Krotov et al., 1998; Johnson, 1998), are generated. We believe parsing procedure guided by more linguistic priori knowledge will help to overcome the drawbacks in some extent. From our intuition, dependency structure, another type of syntactic structure with much linguistic knowledge, will be a good candidate to guide phrase parsing procedure. In this paper we present a novel approach to using dependency structure to guide phrase parsing. This novel approach has its virtues from multiple angles. First, dependency structure offers a good compromise between the conflicting demands of analysis depth, which makes it"
C10-2148,P03-1054,0,0.13932,"w years, several high-precision phrase parsers have been presented, and most of them are employing probabilistic context-free grammar (PCFG). As we all know, the basic PCFG has the problems that the independence assumption is too strong and lacks of lexical conditioning (Jurafsky and Martin, 2007). Although researchers have proposed various models and inference algorithms aiming to solve these problems, the performance of existing phrase parsers is still remained to further improve. Most of the existing approaches can be classified into two categories: unlexicalized PCFG based (Johnson, 1998; Klein and Manning, 2003; Levy and Manning, 2003; Matsuzaki et al., 2005; Petrov et al., 2006) and lexicalized PCFG based (Collins, 1999a; Charniak, 1997; Bikel, 2004; Charniak and Johnson, 2005). Unlexicalized PCFG based approach attempts to weaken the independence assumption by annotating non-terminal symbols with labels of ancestor, siblings and even the latent annotations encoded by local information. In lexicalized PCFG based approach, researchers believe that the forms of a constituent and its subconstituents are determined more by the constituent’s head than any other of its lexical items (Charniak, 1997), so"
C10-2148,W00-1201,0,0.0607234,"Missing"
C10-2148,C02-1126,0,0.0645014,"Missing"
C10-2148,W07-2416,0,0.0635481,", the dependency structures are often projective trees. In this paper, we only consider English parsing based on Penn Treebank (PTB) and Chinese parsing based on Penn Chinese Treebank (PCTB), so we just research the consistency between phrase structure and projective dependency structure through PTB/PCTB. Information carried by the two structures isn’t equal. Phrase structure is more flexible, carries more information, and even contains all the information of dependency structure. So the task to convert a phrase structure to dependency structure is more straight, e.g. Nivre and Scholz (2004), Johansson and Nugues (2007). However, the reverse procedure is much more difficult, because dependency structure lacks the syntactic symbols, which are indispensable in phrase structure. Although the two structures are completely different, they have consistency in some deep level. In this paper we analyze the consistency from a practical perspective in order to do phrase parsing with the help of dependency structure. Having investigated the two kinds of trees with dependency structure and phrase structure, we find a consistency 1 that each subtree in dependency structure must correspond to a sub-tree in phrase structur"
C10-2148,N06-1020,0,0.0907076,"Missing"
C10-2148,P03-1056,0,0.123951,"cision phrase parsers have been presented, and most of them are employing probabilistic context-free grammar (PCFG). As we all know, the basic PCFG has the problems that the independence assumption is too strong and lacks of lexical conditioning (Jurafsky and Martin, 2007). Although researchers have proposed various models and inference algorithms aiming to solve these problems, the performance of existing phrase parsers is still remained to further improve. Most of the existing approaches can be classified into two categories: unlexicalized PCFG based (Johnson, 1998; Klein and Manning, 2003; Levy and Manning, 2003; Matsuzaki et al., 2005; Petrov et al., 2006) and lexicalized PCFG based (Collins, 1999a; Charniak, 1997; Bikel, 2004; Charniak and Johnson, 2005). Unlexicalized PCFG based approach attempts to weaken the independence assumption by annotating non-terminal symbols with labels of ancestor, siblings and even the latent annotations encoded by local information. In lexicalized PCFG based approach, researchers believe that the forms of a constituent and its subconstituents are determined more by the constituent’s head than any other of its lexical items (Charniak, 1997), so they annotate nontermina"
C10-2148,E06-1011,0,0.0711487,"Missing"
C10-2148,H01-1014,0,0.859493,"and Johnson, 2005) just bring a little linguistic priori knowledge (head word information) into learning phase. In inference phase, both of the unlexicalized PCFG based approach and lexicalized PCFG based approach are using the pure searching algorithms, which try to parse a sentence monotonously, either from left to right or from right to left. From these states, we can find that manners of current parsers are too mechanical. Because of this, the efficiency of phrase parsers is always worse, and much more artificial ambiguities are generated. There have been some work (Collins et al., 1999b; Xia and Palmer, 2001) about converting dependency structures to phrase structures. Collins et al. (1999b) proposed an algorithm to convert the Czech dependency Treebank into a phrase structure Treebank and do dependency parsing through Collins (1999a)’s model. Results showed the accuracy of dependency parsing for Czech was improved largely. Xia and Palmer (2001) proposed a more generalized algorithm according to X-bar theory and Collins et al. (1999b), and they did some experiments on Penn Treebank. The results showed their algorithm produced phrase structures that were very close to the ones in Penn Treebank. How"
C10-2148,W03-3023,0,0.451785,"Missing"
C10-2148,N07-1051,0,0.060902,"sing procedure with the help of dependency structure, and make the parsing procedure flexibly. Matsuzaki et al. (2005) defined a generative model called PCFG with latent annotations (PCFG-LA). Using EM-algorithm each nonterminal symbols was annotated with a latent variable, and a fine-grained model can be got. In order to get a more compact PCFG-LA model, Petrov et al. (2006) presented a split-and-merge method which can get PCFG-LA model hierarchically, and their final result outperformed state-of-the-art phrase parsers. To make the parsing process of hierarchical PCFGLA model more efficient, Petrov and Klein (2007) presented a coarse-to-fine inference algorithm. In Section 4 of this paper, we try to combine the hierarchical PCFG-LA model in learning phase and coarse-to-fine method in inference phase into our parser in order to get an accurate and efficient parser. 1293 join Vinken will (1) board as the 29 director a Nov (3) nonexecutive (2) (a) Dependency structure S NP VP MD VP NNP will VB NP PP NP Vinken DT (1) NN NP IN NNP CD Nov 29 join the board as DT a JJ NN director (3) nonexecutive (b) Phrase structure (2) Figure 1. The consistency between phrase structure and dependency structure 3 Our framewor"
C10-2148,P06-1055,0,0.651111,"ost of them are employing probabilistic context-free grammar (PCFG). As we all know, the basic PCFG has the problems that the independence assumption is too strong and lacks of lexical conditioning (Jurafsky and Martin, 2007). Although researchers have proposed various models and inference algorithms aiming to solve these problems, the performance of existing phrase parsers is still remained to further improve. Most of the existing approaches can be classified into two categories: unlexicalized PCFG based (Johnson, 1998; Klein and Manning, 2003; Levy and Manning, 2003; Matsuzaki et al., 2005; Petrov et al., 2006) and lexicalized PCFG based (Collins, 1999a; Charniak, 1997; Bikel, 2004; Charniak and Johnson, 2005). Unlexicalized PCFG based approach attempts to weaken the independence assumption by annotating non-terminal symbols with labels of ancestor, siblings and even the latent annotations encoded by local information. In lexicalized PCFG based approach, researchers believe that the forms of a constituent and its subconstituents are determined more by the constituent’s head than any other of its lexical items (Charniak, 1997), so they annotate nonterminal symbols with the head words information. Bot"
C10-2148,P05-1010,0,0.352437,"ve been presented, and most of them are employing probabilistic context-free grammar (PCFG). As we all know, the basic PCFG has the problems that the independence assumption is too strong and lacks of lexical conditioning (Jurafsky and Martin, 2007). Although researchers have proposed various models and inference algorithms aiming to solve these problems, the performance of existing phrase parsers is still remained to further improve. Most of the existing approaches can be classified into two categories: unlexicalized PCFG based (Johnson, 1998; Klein and Manning, 2003; Levy and Manning, 2003; Matsuzaki et al., 2005; Petrov et al., 2006) and lexicalized PCFG based (Collins, 1999a; Charniak, 1997; Bikel, 2004; Charniak and Johnson, 2005). Unlexicalized PCFG based approach attempts to weaken the independence assumption by annotating non-terminal symbols with labels of ancestor, siblings and even the latent annotations encoded by local information. In lexicalized PCFG based approach, researchers believe that the forms of a constituent and its subconstituents are determined more by the constituent’s head than any other of its lexical items (Charniak, 1997), so they annotate nonterminal symbols with the head"
C10-2148,I05-2002,1,0.807382,"hm according to X-bar theory and Collins et al. (1999b), and they did some experiments on Penn Treebank. The results showed their algorithm produced phrase structures that were very close to the ones in Penn Treebank. However, we have to point out that they only computed the unlabeled performance but lost all the exact syntactic symbols. Different from treetransformed PCFG based approach and lexicalized PCFG based approach, both of Collins et al. (1999b) and Xia and Palmer (2001) attempted to build some heuristic rules through linguistic theory, but didn’t try to learn anything from Treebank. Li and Zong (2005) presented a hierarchical parsing algorithm for long complex Chinese sentences with the help of punctuations. They first divided a long sentence into short ones according to punctuation marks, then parsed the short ones into sub-trees individually, and at last combined all the sub-trees into a whole tree. Experimental results showed the parsing time was reduced largely, and performance was improved too. Although the procedure of their parser is more close to human beings’ manner, it appears a little shallow just using the punctuation marks. In this paper our motivations are to bring more lingu"
C10-2148,D09-1087,0,0.066032,"Missing"
C10-2148,P08-1067,0,0.137056,"Missing"
C10-2148,J98-4004,0,0.386337,"ver the past few years, several high-precision phrase parsers have been presented, and most of them are employing probabilistic context-free grammar (PCFG). As we all know, the basic PCFG has the problems that the independence assumption is too strong and lacks of lexical conditioning (Jurafsky and Martin, 2007). Although researchers have proposed various models and inference algorithms aiming to solve these problems, the performance of existing phrase parsers is still remained to further improve. Most of the existing approaches can be classified into two categories: unlexicalized PCFG based (Johnson, 1998; Klein and Manning, 2003; Levy and Manning, 2003; Matsuzaki et al., 2005; Petrov et al., 2006) and lexicalized PCFG based (Collins, 1999a; Charniak, 1997; Bikel, 2004; Charniak and Johnson, 2005). Unlexicalized PCFG based approach attempts to weaken the independence assumption by annotating non-terminal symbols with labels of ancestor, siblings and even the latent annotations encoded by local information. In lexicalized PCFG based approach, researchers believe that the forms of a constituent and its subconstituents are determined more by the constituent’s head than any other of its lexical it"
C10-2148,P99-1065,0,0.622724,"Bikel, 2004; Charniak and Johnson, 2005) just bring a little linguistic priori knowledge (head word information) into learning phase. In inference phase, both of the unlexicalized PCFG based approach and lexicalized PCFG based approach are using the pure searching algorithms, which try to parse a sentence monotonously, either from left to right or from right to left. From these states, we can find that manners of current parsers are too mechanical. Because of this, the efficiency of phrase parsers is always worse, and much more artificial ambiguities are generated. There have been some work (Collins et al., 1999b; Xia and Palmer, 2001) about converting dependency structures to phrase structures. Collins et al. (1999b) proposed an algorithm to convert the Czech dependency Treebank into a phrase structure Treebank and do dependency parsing through Collins (1999a)’s model. Results showed the accuracy of dependency parsing for Czech was improved largely. Xia and Palmer (2001) proposed a more generalized algorithm according to X-bar theory and Collins et al. (1999b), and they did some experiments on Penn Treebank. The results showed their algorithm produced phrase structures that were very close to the on"
C10-2148,A00-2018,0,\N,Missing
C10-2148,J04-4004,0,\N,Missing
C10-2148,W08-2102,0,\N,Missing
C10-2148,J03-4003,0,\N,Missing
C10-2148,C98-1111,0,\N,Missing
C10-2148,P05-1022,0,\N,Missing
C10-2148,C04-1010,0,\N,Missing
C10-2148,P08-1006,0,\N,Missing
C16-1041,P05-1022,0,0.0354764,"h a constraint-based parser, which uses the partial constituent tree as external constraints. Evaluated on Section 22 of the WSJ Treebank, the technique achieves the state-of-the-art conversion Fscore 95.6. When applied to English Universal Dependency treebank and German CoNLL2006 treebank, the converted treebanks added to the human-annotated constituent parser training corpus improve parsing F-scores significantly for both languages. 1 Introduction State-of-the-art parsers require human annotation of a training corpus in a specific representation, e.g. constituent structure in Penn Treebank (Charniak and Johnson, 2005; Petrov and Klein, 2007) or dependency relations in a dependency treebank (Yamada and Matsumoto, 2003; McDonald et al., 2005) . Creation of human-annotated treebanks, however, is knowledge and labor intensive and it is desired that one can improve parsing performance by leveraging treebanks annotated in representations of a wide variety. While there have been quite a few papers on automatic conversion from dependency to constituent trees and vice versa (Wang et al., 1994; Collins et al., 1999; Forst, 2003; de Marneffe et al., 2006; Johansson and Nugues, 2007; Xia et al., 2008; Hall and Nivre,"
C16-1041,P99-1065,0,0.198048,"of a training corpus in a specific representation, e.g. constituent structure in Penn Treebank (Charniak and Johnson, 2005; Petrov and Klein, 2007) or dependency relations in a dependency treebank (Yamada and Matsumoto, 2003; McDonald et al., 2005) . Creation of human-annotated treebanks, however, is knowledge and labor intensive and it is desired that one can improve parsing performance by leveraging treebanks annotated in representations of a wide variety. While there have been quite a few papers on automatic conversion from dependency to constituent trees and vice versa (Wang et al., 1994; Collins et al., 1999; Forst, 2003; de Marneffe et al., 2006; Johansson and Nugues, 2007; Xia et al., 2008; Hall and Nivre, 2008; Rambow, 2010; Wang and Zong, 2010; Zhang et al., 2013; Simk´o et al., 2014; Kong et al., 2015) , very few papers address the issue of whether or not the converted treebank actually improves the performance of the target parser when added to the human-annotated gold treebanks for parser training. In addition, much of the work on dependency to constituency conversion relies on dependency trees automatically derived from the Penn Treebank (Marcus et al., 1993) via head rules and assumes th"
C16-1041,de-marneffe-etal-2006-generating,0,0.0484527,"Missing"
C16-1041,P15-1147,0,0.0295448,"Missing"
C16-1041,W03-2404,0,0.0639179,"n a specific representation, e.g. constituent structure in Penn Treebank (Charniak and Johnson, 2005; Petrov and Klein, 2007) or dependency relations in a dependency treebank (Yamada and Matsumoto, 2003; McDonald et al., 2005) . Creation of human-annotated treebanks, however, is knowledge and labor intensive and it is desired that one can improve parsing performance by leveraging treebanks annotated in representations of a wide variety. While there have been quite a few papers on automatic conversion from dependency to constituent trees and vice versa (Wang et al., 1994; Collins et al., 1999; Forst, 2003; de Marneffe et al., 2006; Johansson and Nugues, 2007; Xia et al., 2008; Hall and Nivre, 2008; Rambow, 2010; Wang and Zong, 2010; Zhang et al., 2013; Simk´o et al., 2014; Kong et al., 2015) , very few papers address the issue of whether or not the converted treebank actually improves the performance of the target parser when added to the human-annotated gold treebanks for parser training. In addition, much of the work on dependency to constituency conversion relies on dependency trees automatically derived from the Penn Treebank (Marcus et al., 1993) via head rules and assumes that the head-m"
C16-1041,W08-1007,0,0.187404,"d Johnson, 2005; Petrov and Klein, 2007) or dependency relations in a dependency treebank (Yamada and Matsumoto, 2003; McDonald et al., 2005) . Creation of human-annotated treebanks, however, is knowledge and labor intensive and it is desired that one can improve parsing performance by leveraging treebanks annotated in representations of a wide variety. While there have been quite a few papers on automatic conversion from dependency to constituent trees and vice versa (Wang et al., 1994; Collins et al., 1999; Forst, 2003; de Marneffe et al., 2006; Johansson and Nugues, 2007; Xia et al., 2008; Hall and Nivre, 2008; Rambow, 2010; Wang and Zong, 2010; Zhang et al., 2013; Simk´o et al., 2014; Kong et al., 2015) , very few papers address the issue of whether or not the converted treebank actually improves the performance of the target parser when added to the human-annotated gold treebanks for parser training. In addition, much of the work on dependency to constituency conversion relies on dependency trees automatically derived from the Penn Treebank (Marcus et al., 1993) via head rules and assumes that the head-modifier definitions are consistent between the constituent and dependency trees (Xia et al., 2"
C16-1041,W07-2416,0,0.0429624,"ituent structure in Penn Treebank (Charniak and Johnson, 2005; Petrov and Klein, 2007) or dependency relations in a dependency treebank (Yamada and Matsumoto, 2003; McDonald et al., 2005) . Creation of human-annotated treebanks, however, is knowledge and labor intensive and it is desired that one can improve parsing performance by leveraging treebanks annotated in representations of a wide variety. While there have been quite a few papers on automatic conversion from dependency to constituent trees and vice versa (Wang et al., 1994; Collins et al., 1999; Forst, 2003; de Marneffe et al., 2006; Johansson and Nugues, 2007; Xia et al., 2008; Hall and Nivre, 2008; Rambow, 2010; Wang and Zong, 2010; Zhang et al., 2013; Simk´o et al., 2014; Kong et al., 2015) , very few papers address the issue of whether or not the converted treebank actually improves the performance of the target parser when added to the human-annotated gold treebanks for parser training. In addition, much of the work on dependency to constituency conversion relies on dependency trees automatically derived from the Penn Treebank (Marcus et al., 1993) via head rules and assumes that the head-modifier definitions are consistent between the constit"
C16-1041,N15-1080,0,0.109642,"and Matsumoto, 2003; McDonald et al., 2005) . Creation of human-annotated treebanks, however, is knowledge and labor intensive and it is desired that one can improve parsing performance by leveraging treebanks annotated in representations of a wide variety. While there have been quite a few papers on automatic conversion from dependency to constituent trees and vice versa (Wang et al., 1994; Collins et al., 1999; Forst, 2003; de Marneffe et al., 2006; Johansson and Nugues, 2007; Xia et al., 2008; Hall and Nivre, 2008; Rambow, 2010; Wang and Zong, 2010; Zhang et al., 2013; Simk´o et al., 2014; Kong et al., 2015) , very few papers address the issue of whether or not the converted treebank actually improves the performance of the target parser when added to the human-annotated gold treebanks for parser training. In addition, much of the work on dependency to constituency conversion relies on dependency trees automatically derived from the Penn Treebank (Marcus et al., 1993) via head rules and assumes that the head-modifier definitions are consistent between the constituent and dependency trees (Xia et al., 2008). However, such techniques cannot easily generalize to dependencies that diverge from the Pe"
C16-1041,H05-1066,0,0.0385758,"reebank, the technique achieves the state-of-the-art conversion Fscore 95.6. When applied to English Universal Dependency treebank and German CoNLL2006 treebank, the converted treebanks added to the human-annotated constituent parser training corpus improve parsing F-scores significantly for both languages. 1 Introduction State-of-the-art parsers require human annotation of a training corpus in a specific representation, e.g. constituent structure in Penn Treebank (Charniak and Johnson, 2005; Petrov and Klein, 2007) or dependency relations in a dependency treebank (Yamada and Matsumoto, 2003; McDonald et al., 2005) . Creation of human-annotated treebanks, however, is knowledge and labor intensive and it is desired that one can improve parsing performance by leveraging treebanks annotated in representations of a wide variety. While there have been quite a few papers on automatic conversion from dependency to constituent trees and vice versa (Wang et al., 1994; Collins et al., 1999; Forst, 2003; de Marneffe et al., 2006; Johansson and Nugues, 2007; Xia et al., 2008; Hall and Nivre, 2008; Rambow, 2010; Wang and Zong, 2010; Zhang et al., 2013; Simk´o et al., 2014; Kong et al., 2015) , very few papers addres"
C16-1041,P09-1006,0,0.247291,"rees. It applies to any DTrees that make a reasonable linguistic assumption on head-modifier relations regardless of languages and dependency types. This simplicity sets the current proposal apart from all of the previous proposals that rely on linguistic rules, as in (Xia et al., 2008), statistical model utilizing manually acquired head rules and the phrase labels of the target constituent treebank, as in (Kong et al., 2015), or a scoring function that computes the similarity between the source DTree and the nbest parsing output of the DTree sentences by the target constituent parser, as in (Niu et al., 2009). 1 https://github.com/slavpetrov/berkeleyparser 422 input: DTree (labeled or unlabeled) with n input words output: Unlabeled CTree with gold POStags and partial constituent brackets Step 1: Identify the dependency span Di of each word wi if the word wi does not have any dependent then Di is length 1, containing only wi itself; else Di subsumes all of its dependents recursively; Step 2: Convert a dependency span Di to a constituent Ci Vertex of Ci dominates the immediate dependents of the head word and the head word itself. Step 3: Remove all constituent brackets containing only one word. Figu"
C16-1041,N07-1051,0,0.164283,"which uses the partial constituent tree as external constraints. Evaluated on Section 22 of the WSJ Treebank, the technique achieves the state-of-the-art conversion Fscore 95.6. When applied to English Universal Dependency treebank and German CoNLL2006 treebank, the converted treebanks added to the human-annotated constituent parser training corpus improve parsing F-scores significantly for both languages. 1 Introduction State-of-the-art parsers require human annotation of a training corpus in a specific representation, e.g. constituent structure in Penn Treebank (Charniak and Johnson, 2005; Petrov and Klein, 2007) or dependency relations in a dependency treebank (Yamada and Matsumoto, 2003; McDonald et al., 2005) . Creation of human-annotated treebanks, however, is knowledge and labor intensive and it is desired that one can improve parsing performance by leveraging treebanks annotated in representations of a wide variety. While there have been quite a few papers on automatic conversion from dependency to constituent trees and vice versa (Wang et al., 1994; Collins et al., 1999; Forst, 2003; de Marneffe et al., 2006; Johansson and Nugues, 2007; Xia et al., 2008; Hall and Nivre, 2008; Rambow, 2010; Wang"
C16-1041,N10-1049,0,0.0145034,"v and Klein, 2007) or dependency relations in a dependency treebank (Yamada and Matsumoto, 2003; McDonald et al., 2005) . Creation of human-annotated treebanks, however, is knowledge and labor intensive and it is desired that one can improve parsing performance by leveraging treebanks annotated in representations of a wide variety. While there have been quite a few papers on automatic conversion from dependency to constituent trees and vice versa (Wang et al., 1994; Collins et al., 1999; Forst, 2003; de Marneffe et al., 2006; Johansson and Nugues, 2007; Xia et al., 2008; Hall and Nivre, 2008; Rambow, 2010; Wang and Zong, 2010; Zhang et al., 2013; Simk´o et al., 2014; Kong et al., 2015) , very few papers address the issue of whether or not the converted treebank actually improves the performance of the target parser when added to the human-annotated gold treebanks for parser training. In addition, much of the work on dependency to constituency conversion relies on dependency trees automatically derived from the Penn Treebank (Marcus et al., 1993) via head rules and assumes that the head-modifier definitions are consistent between the constituent and dependency trees (Xia et al., 2008). However,"
C16-1041,W97-0301,0,0.0391366,"n the prepositional phrase covered by in environmental push than the one derived from CoNLL or Stanford DTrees. Similarity between a given DTree representation and the Penn Treebank CTree is reflected on the conversion accuracy reported in Section 4. 3 Constraint-based Maximum Entropy Parsing To derive the fully specified labeled CTree from a partial CTree, we parse the input sentence with a constraint-based constituent parser that utilizes the gold POStags and partial brackets as model external constraints. We implement the constraint-based parsing algorithm on the maximum entropy parser of (Ratnaparkhi, 1997; Ratnaparkhi, 1999), which works robustly regardless of the grammar coverage of the baseline parsing model and therefore well-suited for constraint-based parsing of partial CTrees derived from out-of-domain as well as in-domain DTrees. 3.1 Baseline Maximum Entropy Parser The baseline MaxEnt parser takes one of the four actions to parse an input sentence: tag , chunk , extend and reduce. Four models corresponding to each action are built separately during training. The model score in (1) is integrated into the parser scoring function (2). In (1) and (2), ai is an action from tag, chunk, extend"
C16-1041,C14-1132,0,0.0465874,"Missing"
C16-1041,C10-2148,1,0.92437,"007) or dependency relations in a dependency treebank (Yamada and Matsumoto, 2003; McDonald et al., 2005) . Creation of human-annotated treebanks, however, is knowledge and labor intensive and it is desired that one can improve parsing performance by leveraging treebanks annotated in representations of a wide variety. While there have been quite a few papers on automatic conversion from dependency to constituent trees and vice versa (Wang et al., 1994; Collins et al., 1999; Forst, 2003; de Marneffe et al., 2006; Johansson and Nugues, 2007; Xia et al., 2008; Hall and Nivre, 2008; Rambow, 2010; Wang and Zong, 2010; Zhang et al., 2013; Simk´o et al., 2014; Kong et al., 2015) , very few papers address the issue of whether or not the converted treebank actually improves the performance of the target parser when added to the human-annotated gold treebanks for parser training. In addition, much of the work on dependency to constituency conversion relies on dependency trees automatically derived from the Penn Treebank (Marcus et al., 1993) via head rules and assumes that the head-modifier definitions are consistent between the constituent and dependency trees (Xia et al., 2008). However, such techniques cann"
C16-1041,P94-1034,0,0.4321,"e human annotation of a training corpus in a specific representation, e.g. constituent structure in Penn Treebank (Charniak and Johnson, 2005; Petrov and Klein, 2007) or dependency relations in a dependency treebank (Yamada and Matsumoto, 2003; McDonald et al., 2005) . Creation of human-annotated treebanks, however, is knowledge and labor intensive and it is desired that one can improve parsing performance by leveraging treebanks annotated in representations of a wide variety. While there have been quite a few papers on automatic conversion from dependency to constituent trees and vice versa (Wang et al., 1994; Collins et al., 1999; Forst, 2003; de Marneffe et al., 2006; Johansson and Nugues, 2007; Xia et al., 2008; Hall and Nivre, 2008; Rambow, 2010; Wang and Zong, 2010; Zhang et al., 2013; Simk´o et al., 2014; Kong et al., 2015) , very few papers address the issue of whether or not the converted treebank actually improves the performance of the target parser when added to the human-annotated gold treebanks for parser training. In addition, much of the work on dependency to constituency conversion relies on dependency trees automatically derived from the Penn Treebank (Marcus et al., 1993) via hea"
C16-1041,W03-3023,0,0.126057,"d on Section 22 of the WSJ Treebank, the technique achieves the state-of-the-art conversion Fscore 95.6. When applied to English Universal Dependency treebank and German CoNLL2006 treebank, the converted treebanks added to the human-annotated constituent parser training corpus improve parsing F-scores significantly for both languages. 1 Introduction State-of-the-art parsers require human annotation of a training corpus in a specific representation, e.g. constituent structure in Penn Treebank (Charniak and Johnson, 2005; Petrov and Klein, 2007) or dependency relations in a dependency treebank (Yamada and Matsumoto, 2003; McDonald et al., 2005) . Creation of human-annotated treebanks, however, is knowledge and labor intensive and it is desired that one can improve parsing performance by leveraging treebanks annotated in representations of a wide variety. While there have been quite a few papers on automatic conversion from dependency to constituent trees and vice versa (Wang et al., 1994; Collins et al., 1999; Forst, 2003; de Marneffe et al., 2006; Johansson and Nugues, 2007; Xia et al., 2008; Hall and Nivre, 2008; Rambow, 2010; Wang and Zong, 2010; Zhang et al., 2013; Simk´o et al., 2014; Kong et al., 2015)"
C16-1041,P13-1029,0,0.0226919,"lations in a dependency treebank (Yamada and Matsumoto, 2003; McDonald et al., 2005) . Creation of human-annotated treebanks, however, is knowledge and labor intensive and it is desired that one can improve parsing performance by leveraging treebanks annotated in representations of a wide variety. While there have been quite a few papers on automatic conversion from dependency to constituent trees and vice versa (Wang et al., 1994; Collins et al., 1999; Forst, 2003; de Marneffe et al., 2006; Johansson and Nugues, 2007; Xia et al., 2008; Hall and Nivre, 2008; Rambow, 2010; Wang and Zong, 2010; Zhang et al., 2013; Simk´o et al., 2014; Kong et al., 2015) , very few papers address the issue of whether or not the converted treebank actually improves the performance of the target parser when added to the human-annotated gold treebanks for parser training. In addition, much of the work on dependency to constituency conversion relies on dependency trees automatically derived from the Penn Treebank (Marcus et al., 1993) via head rules and assumes that the head-modifier definitions are consistent between the constituent and dependency trees (Xia et al., 2008). However, such techniques cannot easily generalize"
C16-1127,C04-1051,0,0.134959,"ed by mean average precision (MAP) and mean reciprocal rank (MRR). We experiment on two datasets: QASent and WikiQA. The statistics of the two datasets can be found in Yang et al. (2015), where QASent (Wang et al., 2007) was created from the TREC QA track, and WikiQA (Yang et al., 2015) is constructed from real queries of Bing and Wikipedia. The paraphrase identification task is to detect whether two sentences are paraphrases based on the similarity between them. The metrics include the accuracy and the positive class F1 score. We experiment on the Microsoft Research Paraphrase corpus (MSRP) (Dolan et al., 2004), which includes 2753 true and 1323 false instances in the training set, and 1147 true and 578 false instances in the test set. We build a development set by randomly selecting 100 true and 100 false instances from the training set. In all experiments, we set the size of word vector dimension as d =300, and pre-train the vectors with the word2vec toolkit (Mikolov et al., 2013) on the English Gigaword (LDC2011T07). 4.2 Model Properties There are several alternative options in our model, e.g., the semantic matching functions, the decomposition operations, and the filter types. The choice of thes"
C16-1127,N13-1092,0,0.0709807,"Missing"
C16-1127,D15-1181,0,0.468852,"estimated over the composed feature vectors. Experimental results show that our model gets the state-of-the-art performance on the answer sentence selection task, and achieves a comparable result on the paraphrase identification task. 1 Introduction Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences. It plays an important role for a variety of tasks in both NLP and IR communities. For example, in paraphrase identification task, sentence similarity is used to determine whether two sentences are paraphrases or not (Yin and Sch¨utze, 2015; He et al., 2015). For question answering and information retrieval tasks, sentence similarities between query-answer pairs are used for assessing the relevance and ranking all the candidate answers (Severyn and Moschitti, 2015; Wang and Ittycheriah, 2015). However, sentence similarity learning has following challenges: 1. There is a lexical gap between semantically equivalent sentences. Take the E1 and E2 in Table 1 for example, they have the similar meaning but with different lexicons. 2. Semantic similarity should be measured at different levels of granularity (word-level, phrase-level and syntax-level). E."
C16-1127,N10-1145,0,0.0802761,", researchers have been working on sentence similarity algorithms for a long time. To bridge the lexical gap (challenge 1), some word similarity metrics were proposed to match different but semantically related words. Examples include knowledge-based metrics (Resnik, 1995) and corpus-based metrics (Jiang and Conrath, 1997; Yin and Sch¨utze, 2015; He et al., 2015). To measure sentence similarity from various granularities (challenge 2), researchers have explored features extracted from n-grams, continuous phrases, discontinuous phrases, and parse trees (Yin and Sch¨utze, 2015; He et al., 2015; Heilman and Smith, 2010). The third challenge did not get much 1340 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 1340–1349, Osaka, Japan, December 11-17 2016. E1 E2 E3 E4 E5 The research is [irrelevant] to sockeye. The study is [not related] to salmon. The research is relevant to salmon. The study is relevant to sockeye, hinstead of cohoi. The study is relevant to sockeye, hrather than flounderi. Table 1: Examples for sentence similarity learning, where sockeye means “red salmon”, and coho means “silver salmon”. “coho” and “sockeye” are in the sal"
C16-1127,D13-1090,0,0.0701327,"e 3 shows that our model is more effective than the other models. MSRP dataset. Table 4 summarized the results from our model and several state-of-the-art models. Yin and Sch¨utze (2015) employed a CNN model to learn sentence representations on multiple level of 1346 Models Acc F1 Yin and Sch¨utze (2015) (without pretraining) 72.5 81.4 Yin and Sch¨utze (2015) (with pretraining) 78.4 84.6 He et al. (2015) (without POS embeddings) 77.8 N/A He et al. (2015) (without Para. embeddings) 77.3 N/A He et al. (2015) (POS and Para. embeddings) 78.6 84.7 Yin et al. (2015) (with sparse features) 78.9 84.8 Ji and Eisenstein (2013) 80.4 86.0 This work 78.4 84.7 Table 4: Experimental results for paraphrase identification on MSRP corpus. granularity and modeled interaction features at each level for a pair of sentences. They obtained their best performance by pretraining the model on a language modeling task (the 3rd row of Table 4). However, their model heavily depends on the pretraining strategy. Without pretraining, they got a much worse performance (the second row of Table 4). He et al. (2015) proposed a similar model to Yin and Sch¨utze (2015). Similarly, they also used a CNN model to extract features at multiple lev"
C16-1127,O97-1002,0,0.267216,"Whereas the meaning of E4 is quite different from E3 , which emphasizes “The study is about red (a special kind of) salmon”, because both “sockeye” and “coho” are in the salmon family. How we can extract and utilize those information becomes another challenge. In order to handle the above challenges, researchers have been working on sentence similarity algorithms for a long time. To bridge the lexical gap (challenge 1), some word similarity metrics were proposed to match different but semantically related words. Examples include knowledge-based metrics (Resnik, 1995) and corpus-based metrics (Jiang and Conrath, 1997; Yin and Sch¨utze, 2015; He et al., 2015). To measure sentence similarity from various granularities (challenge 2), researchers have explored features extracted from n-grams, continuous phrases, discontinuous phrases, and parse trees (Yin and Sch¨utze, 2015; He et al., 2015; Heilman and Smith, 2010). The third challenge did not get much 1340 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 1340–1349, Osaka, Japan, December 11-17 2016. E1 E2 E3 E4 E5 The research is [irrelevant] to sockeye. The study is [not related] to salmon."
C16-1127,D14-1181,0,0.0108218,"lel component and a perpendicular component. Then, the parallel component is viewed as the similar component s+ i , and perpendicular component is − taken as the dissimilar component si . Eq. (9) gives the concrete definitions. si · sˆi sˆi sˆi · sˆi + s− i = si − si s+ i = 3.3 parallel perpendicular (9) Composition Functions The aim of composition function fcomp in Eq. (3) is to extract features from both the similar component matrix and the dissimilar component matrix. We also want to acquire similarities and dissimilarities of various granularity during the composition phase. Inspired from Kim (2014), we utilize a two-channel convolutional neural networks (CNN) and design filters based on various order of n-grams, e.g., unigram, bigram and trigram. The CNN model involves two sequential operations: convolution and max-pooling. For the convolution operation, we define a list of filters {wo }. The shape of each filter is d × h, where d is the dimension of word vectors and h is the window size. Each filter is applied to two patches (a window size h of vectors) from both similar and dissimilar channels, and generates a feature. Eq. (10) expresses this process. − + + bo ) + wo ∗ S[i:i+h] co,i ="
C16-1127,D15-1166,0,0.0266684,"our model obtained a comparable performance (the last row of Table 4) without using any sparse features, extra annotated resources and specific training strategies. However, the best performance so far on this dataset is obtained by Ji and Eisenstein (2013). In their model, they just utilized several hand-crafted features in a Support Vector Machine (SVM) model. Therefore, the deep learning methods still have a long way to go for this task. 5 Related Work The semantic matching functions in subsection 3.1 are inspired from the attention-based neural machine translation (Bahdanau et al., 2014; Luong et al., 2015). However, most of the previous work using the attention mechanism in only LSTM models. Whereas our model introduces the attention mechanism into the CNN model. A similar work is the attention-based CNN model proposed by Yin et al. (2015). They first build an attention matrix for a sentence pair, and then directly take the attention matrix as a new channel of the CNN model. Differently, our model uses the attention matrix (or similarity matrix) to decompose the original sentence matrix into a similar component matrix and a dissimilar component matrix, and then feeds these two matrixes into a t"
C16-1127,W06-1603,0,0.042638,"rnational Conference on Computational Linguistics: Technical Papers, pages 1340–1349, Osaka, Japan, December 11-17 2016. E1 E2 E3 E4 E5 The research is [irrelevant] to sockeye. The study is [not related] to salmon. The research is relevant to salmon. The study is relevant to sockeye, hinstead of cohoi. The study is relevant to sockeye, hrather than flounderi. Table 1: Examples for sentence similarity learning, where sockeye means “red salmon”, and coho means “silver salmon”. “coho” and “sockeye” are in the salmon family, while “flounder” is not. attention in the past, the only related work of Qiu et al. (2006) explored the dissimilarity between sentences in a pair for paraphrase identification task, but they require human annotations in order to train a classifier, and their performance is still below the state of the art. In this paper, we propose a novel model to tackle all these challenges jointly by decomposing and composing lexical semantics over sentences. Given a sentence pair, the model represents each word as a low-dimensional vector (challenge 1), and calculates a semantic matching vector for each word based on all words in the other sentence (challenge 2). Then based on the semantic matc"
C16-1127,D07-1003,0,0.27962,"atical expressions with Theano (Bastien et al., 2012) and use Adam (Kingma and Ba, 2014) for optimization. 1344 4 Experiment 4.1 Experimental Setting We evaluate our model on two tasks: answer sentence selection and paraphrase identification. The answer sentence selection task is to rank a list of candidate answers based on their similarities to a question sentence, and the performance is measured by mean average precision (MAP) and mean reciprocal rank (MRR). We experiment on two datasets: QASent and WikiQA. The statistics of the two datasets can be found in Yang et al. (2015), where QASent (Wang et al., 2007) was created from the TREC QA track, and WikiQA (Yang et al., 2015) is constructed from real queries of Bing and Wikipedia. The paraphrase identification task is to detect whether two sentences are paraphrases based on the similarity between them. The metrics include the accuracy and the positive class F1 score. We experiment on the Microsoft Research Paraphrase corpus (MSRP) (Dolan et al., 2004), which includes 2753 true and 1323 false instances in the training set, and 1147 true and 578 false instances in the test set. We build a development set by randomly selecting 100 true and 100 false i"
C16-1127,D15-1237,0,0.627078,"gn Li = 0. We implement the mathematical expressions with Theano (Bastien et al., 2012) and use Adam (Kingma and Ba, 2014) for optimization. 1344 4 Experiment 4.1 Experimental Setting We evaluate our model on two tasks: answer sentence selection and paraphrase identification. The answer sentence selection task is to rank a list of candidate answers based on their similarities to a question sentence, and the performance is measured by mean average precision (MAP) and mean reciprocal rank (MRR). We experiment on two datasets: QASent and WikiQA. The statistics of the two datasets can be found in Yang et al. (2015), where QASent (Wang et al., 2007) was created from the TREC QA track, and WikiQA (Yang et al., 2015) is constructed from real queries of Bing and Wikipedia. The paraphrase identification task is to detect whether two sentences are paraphrases based on the similarity between them. The metrics include the accuracy and the positive class F1 score. We experiment on the Microsoft Research Paraphrase corpus (MSRP) (Dolan et al., 2004), which includes 2753 true and 1323 false instances in the training set, and 1147 true and 578 false instances in the test set. We build a development set by randomly"
C16-1127,N15-1091,0,0.0263024,"Missing"
C16-1127,Q16-1019,0,\N,Missing
D16-1096,W14-4012,0,0.192539,"Missing"
D16-1096,D13-1052,1,0.906077,"Missing"
D16-1096,N16-1102,0,0.0381397,"xj = zt,j ◦ ct−1,xj + (1 − zt,j ) ◦ c˜t,xj , where, zt is the update gate, rt is the reset gate, c˜t is the new memory content, and ct is the final memory. The matrix W zy , W zα , U z , W ry , W rα , U r , W y , W α and U are shared across different position j. ◦ is a pointwise operation. t=1 i=1 r c˜t,xj = tanh(W yt + W α αt,j + rt,j ◦ U ct−1,xj ) 3.1.2 Objectives t=1 ) l m X X −λ ( ||cj,xi ||) , i=1 j=axi (7) where axi is the maximum index on the target sentence xi can be aligned to. 4 Related Work There are several parallel and independent related work (Tu et al., 2016; Feng et al., 2016; Cohn et al., 2016). Tu et al. (2016) is the most relevant one. In their paper, they also employ a GRU to model the coverage vector. One main difference is that our model introduces a specific coverage embedding vector for each source word, in contrast, their work initializes the word coverage vector with a scalar with a uniform distribution. Another difference lays in the fertility part, Tu et al. (2016) add an accumulate operation and a fertility function to simulate the process of one-to-many alignments. In our approach, we add fertility information directly to coverage embeddings, as each source word has its"
D16-1096,P05-1066,0,0.0470867,"the hybrid system. We test four different settings for our coverage embedding models: • UGRU : updating with a GRU; • USub : updating as a subtraction; • UGRU + USub : combination of two methods (do not share coverage embedding vectors); • +Obj.: UGRU + USub plus an additional objective in Equation 61 . UGRU improves the translation quality by 1.3 points on average over LVNMT. And UGRU + USub achieves the best average score of 13.14, which is about 2.6 points better than LVNMT. All the improvements of our coverage embedding models over LVNMT are statistically significant with the signtest of Collins et al. (2005). We believe that we need to explore more hyper-parameters of +Obj. in order to get even better results over UGRU + USub . 1 We use two λs for UGRU and USub separately, and we test λGRU = 1 × 10−4 and λSub = 1 × 10−2 in our experiments. single system Ours Tree-to-string LVNMT UGRU USub UGRU +USub +Obj. MT08 MT06 BP 0.95 0.96 0.92 0.91 0.92 0.93 B LEU T- B B P 34.93 9.45 0.94 34.53 12.25 0.93 35.59 10.71 0.89 35.90 10.29 0.88 36.60 9.36 0.89 36.80 9.78 0.90 News B LEU 31.12 28.86 30.18 30.49 31.86 31.83 T- B 12.90 17.40 15.33 15.23 13.69 14.20 BP 0.90 0.97 0.97 0.96 0.95 0.95 Web B LEU 23.45 26"
D16-1096,N13-1073,0,0.0138262,"T08 web. For all NMT systems, the full vocabulary sizes for thr two training sets are 300k and 500k respectively. The coverage embedding vector size is 100. In the training procedure, we use AdaDelta (Zeiler, 2012) to update model parameters with a mini-batch size 80. Following Mi et al. (2016b), the output vocabulary for each mini-batch or sentence is a sub-set of the full vocabulary. For each source sentence, the sentence-level target vocabularies are union of top 2k most frequent target words and the top 10 candidates of the word-to-word/phrase translation tables learned from ‘fast align’ (Dyer et al., 2013). The maximum length of a source phrase is 4. In the training time, we add the reference in order to make the translation reachable. Following Jean et al. (2015), We dump the align958 ments, attentions, for each sentence, and replace UNKs with the word-to-word translation model or the aligned source word. Our traditional SMT system is a hybrid syntaxbased tree-to-string model (Zhao and Al-onaizan, 2008), a simplified version of Liu et al. (2009) and Cmejrek et al. (2013). We parse the Chinese side with Berkeley parser, and align the bilingual sentences with GIZA++. Then we extract Hiero and tr"
D16-1096,D11-1125,0,0.00854951,"o-string model (Zhao and Al-onaizan, 2008), a simplified version of Liu et al. (2009) and Cmejrek et al. (2013). We parse the Chinese side with Berkeley parser, and align the bilingual sentences with GIZA++. Then we extract Hiero and tree-to-string rules on the training set. Our two 5gram language models are trained on the English side of the parallel corpus, and on monolingual corpora (around 10 billion words from Gigaword (LDC2011T07)), respectively. As suggestion by Zhang (2016), NMT systems can achieve better results with the help of those monolingual corpora. We tune our system with PRO (Hopkins and May, 2011) to minimize (T ER- B LEU)/2 on the development set. 5.2 Translation Results Table 1 shows the results of all systems on 5 million training set. The traditional syntax-based system achieves 9.45, 12.90, and 17.72 on MT06, MT08 News, and MT08 Web sets respectively, and 13.36 on average in terms of (T ER- B LEU)/2. The largevocabulary NMT (LVNMT), our baseline, achieves an average (T ER- B LEU)/2 score of 15.74, which is about 2 points worse than the hybrid system. We test four different settings for our coverage embedding models: • UGRU : updating with a GRU; • USub : updating as a subtraction;"
D16-1096,P15-1001,0,0.289679,"ding explicit coverage embedding models to alleviate issues of repeating and dropping translations in NMT. For each source word, our model starts with a full coverage embedding vector to track the coverage status, and then keeps updating it with neural networks as the translation goes. Experiments on the large-scale Chinese-to-English task show that our enhanced model improves the translation quality significantly on various test sets over the strong large vocabulary NMT system. 1 Introduction Neural machine translation (NMT) has gained popularity in recent years (e.g. (Bahdanau et al., 2014; Jean et al., 2015; Luong et al., 2015; Mi et al., 2016b; Li et al., 2016)), especially for the attentionbased models of Bahdanau et al. (2014). The attention at each time step shows which source word the model should focus on to predict the next target word. However, the attention in each step only looks at the previous hidden state and the previous target word, there is no history or coverage information typically for each source word. As a result, this kind of model suffers from issues of repeating or dropping translations. The traditional statistical machine translation (SMT) systems (e.g. (Koehn, 2004)) ad"
D16-1096,koen-2004-pharaoh,0,0.0467355,"ean et al., 2015; Luong et al., 2015; Mi et al., 2016b; Li et al., 2016)), especially for the attentionbased models of Bahdanau et al. (2014). The attention at each time step shows which source word the model should focus on to predict the next target word. However, the attention in each step only looks at the previous hidden state and the previous target word, there is no history or coverage information typically for each source word. As a result, this kind of model suffers from issues of repeating or dropping translations. The traditional statistical machine translation (SMT) systems (e.g. (Koehn, 2004)) address the above issues by employing a source side “coverage vector” for each sentence to indicate explicitly which words have been translated, which parts have not yet. A coverage vector starts with all zeros, meaning no word has been translated. If a source word at position j got translated, the coverage vector sets position j as 1, and they won’t use this source ∗ Work done while at IBM. To contact Abe, aittycheriah@google.com. word in future translation. This mechanism avoids the repeating or dropping translation problems. However, it is not easy to adapt the “coverage vector” to NMT di"
D16-1096,P09-1065,1,0.845573,"ies are union of top 2k most frequent target words and the top 10 candidates of the word-to-word/phrase translation tables learned from ‘fast align’ (Dyer et al., 2013). The maximum length of a source phrase is 4. In the training time, we add the reference in order to make the translation reachable. Following Jean et al. (2015), We dump the align958 ments, attentions, for each sentence, and replace UNKs with the word-to-word translation model or the aligned source word. Our traditional SMT system is a hybrid syntaxbased tree-to-string model (Zhao and Al-onaizan, 2008), a simplified version of Liu et al. (2009) and Cmejrek et al. (2013). We parse the Chinese side with Berkeley parser, and align the bilingual sentences with GIZA++. Then we extract Hiero and tree-to-string rules on the training set. Our two 5gram language models are trained on the English side of the parallel corpus, and on monolingual corpora (around 10 billion words from Gigaword (LDC2011T07)), respectively. As suggestion by Zhang (2016), NMT systems can achieve better results with the help of those monolingual corpora. We tune our system with PRO (Hopkins and May, 2011) to minimize (T ER- B LEU)/2 on the development set. 5.2 Transl"
D16-1096,D15-1166,0,0.0333544,"age embedding models to alleviate issues of repeating and dropping translations in NMT. For each source word, our model starts with a full coverage embedding vector to track the coverage status, and then keeps updating it with neural networks as the translation goes. Experiments on the large-scale Chinese-to-English task show that our enhanced model improves the translation quality significantly on various test sets over the strong large vocabulary NMT system. 1 Introduction Neural machine translation (NMT) has gained popularity in recent years (e.g. (Bahdanau et al., 2014; Jean et al., 2015; Luong et al., 2015; Mi et al., 2016b; Li et al., 2016)), especially for the attentionbased models of Bahdanau et al. (2014). The attention at each time step shows which source word the model should focus on to predict the next target word. However, the attention in each step only looks at the previous hidden state and the previous target word, there is no history or coverage information typically for each source word. As a result, this kind of model suffers from issues of repeating or dropping translations. The traditional statistical machine translation (SMT) systems (e.g. (Koehn, 2004)) address the above issu"
D16-1096,D16-1249,1,0.64441,"to alleviate issues of repeating and dropping translations in NMT. For each source word, our model starts with a full coverage embedding vector to track the coverage status, and then keeps updating it with neural networks as the translation goes. Experiments on the large-scale Chinese-to-English task show that our enhanced model improves the translation quality significantly on various test sets over the strong large vocabulary NMT system. 1 Introduction Neural machine translation (NMT) has gained popularity in recent years (e.g. (Bahdanau et al., 2014; Jean et al., 2015; Luong et al., 2015; Mi et al., 2016b; Li et al., 2016)), especially for the attentionbased models of Bahdanau et al. (2014). The attention at each time step shows which source word the model should focus on to predict the next target word. However, the attention in each step only looks at the previous hidden state and the previous target word, there is no history or coverage information typically for each source word. As a result, this kind of model suffers from issues of repeating or dropping translations. The traditional statistical machine translation (SMT) systems (e.g. (Koehn, 2004)) address the above issues by employing a"
D16-1096,P16-2021,1,0.389163,"Missing"
D16-1096,D16-1160,0,0.0181148,"with the word-to-word translation model or the aligned source word. Our traditional SMT system is a hybrid syntaxbased tree-to-string model (Zhao and Al-onaizan, 2008), a simplified version of Liu et al. (2009) and Cmejrek et al. (2013). We parse the Chinese side with Berkeley parser, and align the bilingual sentences with GIZA++. Then we extract Hiero and tree-to-string rules on the training set. Our two 5gram language models are trained on the English side of the parallel corpus, and on monolingual corpora (around 10 billion words from Gigaword (LDC2011T07)), respectively. As suggestion by Zhang (2016), NMT systems can achieve better results with the help of those monolingual corpora. We tune our system with PRO (Hopkins and May, 2011) to minimize (T ER- B LEU)/2 on the development set. 5.2 Translation Results Table 1 shows the results of all systems on 5 million training set. The traditional syntax-based system achieves 9.45, 12.90, and 17.72 on MT06, MT08 News, and MT08 Web sets respectively, and 13.36 on average in terms of (T ER- B LEU)/2. The largevocabulary NMT (LVNMT), our baseline, achieves an average (T ER- B LEU)/2 score of 15.74, which is about 2 points worse than the hybrid syst"
D16-1096,D08-1060,0,0.0338928,"source sentence, the sentence-level target vocabularies are union of top 2k most frequent target words and the top 10 candidates of the word-to-word/phrase translation tables learned from ‘fast align’ (Dyer et al., 2013). The maximum length of a source phrase is 4. In the training time, we add the reference in order to make the translation reachable. Following Jean et al. (2015), We dump the align958 ments, attentions, for each sentence, and replace UNKs with the word-to-word translation model or the aligned source word. Our traditional SMT system is a hybrid syntaxbased tree-to-string model (Zhao and Al-onaizan, 2008), a simplified version of Liu et al. (2009) and Cmejrek et al. (2013). We parse the Chinese side with Berkeley parser, and align the bilingual sentences with GIZA++. Then we extract Hiero and tree-to-string rules on the training set. Our two 5gram language models are trained on the English side of the parallel corpus, and on monolingual corpora (around 10 billion words from Gigaword (LDC2011T07)), respectively. As suggestion by Zhang (2016), NMT systems can achieve better results with the help of those monolingual corpora. We tune our system with PRO (Hopkins and May, 2011) to minimize (T ER-"
D16-1096,P05-1033,0,\N,Missing
D16-1224,D15-1198,0,0.142364,"ations, such as statistical machine translation (SMT) (Jones et al., 2012). The task of AMR-to-text generation is to generate grammatical text containing the same semantic meaning as a given AMR graph. This task is important yet also challenging since each AMR graph usually has multiple corresponding sentences, and syntactic structure and function words are abstracted away when transforming a sentence into AMR (Banarescu et al., 2013). There has been work dealing with text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015). On the other hand, relatively little work has been done on AMR-to-text generation. One recent exception is Flanigan et al. (2016), who first generate a spanning tree for the input AMR graph, and then apply a tree transducer to generate the sentence. Here, we directly generate the sentence from an input AMR by treating AMR-to-text generation as a variant of the traveling salesman problem (TSP). Given an AMR as input, our method first cuts the graph into several rooted and connected fragments (sub-graphs), and then finds the translation for each fragment, before finally generating the sentence"
D16-1224,W13-2322,0,0.526909,"ing for a given AMR graph. We attack the task by first partitioning the AMR graph into smaller fragments, and then generating the translation for each fragment, before finally deciding the order by solving an asymmetric generalized traveling salesman problem (AGTSP). A Maximum Entropy classifier is trained to estimate the traveling costs, and a TSP solver is used to find the optimized solution. The final model reports a BLEU score of 22.44 on the SemEval-2016 Task8 dataset. 1 ARG0 go-01 ARG0 boy Figure 1: AMR graph for “The boy wants to go”. Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism encoding the meaning of a sentence as a rooted, directed graph. Shown in Figure 1, the nodes of an AMR graph (e.g. “boy”, “go-01” and “want01”) represent concepts, and the edges (e.g. “ARG0” and “ARG1”) represent relations between concepts. AMR jointly encodes a set of different semantic phenomena, which makes it useful in applications like question answering and semantics-based machine translation. AMR has served as an intermediate representation for various text-to-text NLP applications, such as statistical machine translation (SMT) (Jones et al., 2012). The task of"
D16-1224,C10-1012,0,0.0287772,"input AMR graph into a tree before linearization, we apply synchronous rules consisting of AMR graph fragments and text to directly transfer a AMR graph into a sentence. In addition to AMR parsing and generation, there has also been work using AMR as a semantic representation in machine translation (Jones et al., 2012). Our work also belongs to the task of text generation (Reiter and Dale, 1997). There has been work on generating natural language text from a bag of words (Wan et al., 2009; Zhang and Clark, 2015), surface syntactic trees (Zhang, 2013; Song et al., 2014), deep semantic graphs (Bohnet et al., 2010) and logical forms (White, 2004; White and Rajkumar, 2009). We are among the first to investigate generation from AMR, which is a different type of semantic representation. 5 Conclusion In conclusion, we showed that a TSP solver with a few real-valued features can be useful for AMR-totext generation. Our method is based on a set of graph to string rules, yet significantly better than a PBMT-based baseline. This shows that our rule induction algorithm is effective and that the TSP solver finds better solutions than beam search. Acknowledgments We are grateful for the help of Jeffrey Flanigan, L"
D16-1224,P13-2131,0,0.0614093,"tence for the whole AMR by ordering the translations. To cut the AMR and translate each fragment, we match the input AMR with rules, each consisting of a rooted, connected AMR fragment and a corresponding translation. These rules serve in a similar way to rules in SMT models. We learn the rules by a modified version of the sampling algorithm of Peng 2084 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2084–2089, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics et al. (2015), and use the rule matching algorithm of Cai and Knight (2013). For decoding the fragments and synthesizing the output, we define a cut to be a subset of matched rules without overlap that covers the AMR, and an ordered cut to be a cut with the rules being ordered. To generate a sentence for the whole AMR, we search for an ordered cut, and concatenate translations of all rules in the cut. TSP is used to traverse different cuts and determine the best order. Intuitively, our method is similar to phrase-based SMT, which first cuts the input sentence into phrases, then obtains the translation for each source phrase, before finally generating the target sente"
D16-1224,P14-1134,0,0.339027,"machine translation. AMR has served as an intermediate representation for various text-to-text NLP applications, such as statistical machine translation (SMT) (Jones et al., 2012). The task of AMR-to-text generation is to generate grammatical text containing the same semantic meaning as a given AMR graph. This task is important yet also challenging since each AMR graph usually has multiple corresponding sentences, and syntactic structure and function words are abstracted away when transforming a sentence into AMR (Banarescu et al., 2013). There has been work dealing with text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015). On the other hand, relatively little work has been done on AMR-to-text generation. One recent exception is Flanigan et al. (2016), who first generate a spanning tree for the input AMR graph, and then apply a tree transducer to generate the sentence. Here, we directly generate the sentence from an input AMR by treating AMR-to-text generation as a variant of the traveling salesman problem (TSP). Given an AMR as input, our method first cuts the graph into several rooted and connected fragment"
D16-1224,N16-1087,0,0.350788,"ammatical text containing the same semantic meaning as a given AMR graph. This task is important yet also challenging since each AMR graph usually has multiple corresponding sentences, and syntactic structure and function words are abstracted away when transforming a sentence into AMR (Banarescu et al., 2013). There has been work dealing with text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015). On the other hand, relatively little work has been done on AMR-to-text generation. One recent exception is Flanigan et al. (2016), who first generate a spanning tree for the input AMR graph, and then apply a tree transducer to generate the sentence. Here, we directly generate the sentence from an input AMR by treating AMR-to-text generation as a variant of the traveling salesman problem (TSP). Given an AMR as input, our method first cuts the graph into several rooted and connected fragments (sub-graphs), and then finds the translation for each fragment, before finally generating the sentence for the whole AMR by ordering the translations. To cut the AMR and translate each fragment, we match the input AMR with rules, eac"
D16-1224,C12-1083,0,0.362052,"on (AMR) (Banarescu et al., 2013) is a semantic formalism encoding the meaning of a sentence as a rooted, directed graph. Shown in Figure 1, the nodes of an AMR graph (e.g. “boy”, “go-01” and “want01”) represent concepts, and the edges (e.g. “ARG0” and “ARG1”) represent relations between concepts. AMR jointly encodes a set of different semantic phenomena, which makes it useful in applications like question answering and semantics-based machine translation. AMR has served as an intermediate representation for various text-to-text NLP applications, such as statistical machine translation (SMT) (Jones et al., 2012). The task of AMR-to-text generation is to generate grammatical text containing the same semantic meaning as a given AMR graph. This task is important yet also challenging since each AMR graph usually has multiple corresponding sentences, and syntactic structure and function words are abstracted away when transforming a sentence into AMR (Banarescu et al., 2013). There has been work dealing with text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015). On the other hand, relatively little work has been do"
D16-1224,N03-1017,0,0.0278261,"hose AMR fragments share common concepts. Otherwise the traveling cost is evaluated by a maximum entropy model, which will be discussed in detail in Section 2.4. 2.3 Rule Acquisition We extract rules from a corpus of (sentence, AMR) pairs using the method of Peng et al. (2015). Given 2086 an aligned (sentence, AMR) pair, a phrase-fragment pair is a pair ([i, j], f ), where [i, j] is a span of the sentence and f represents a connected and rooted AMR fragment. A fragment decomposition forest consists of all possible phrase-fragment pairs that satisfy the alignment agreement for phrase-based MT (Koehn et al., 2003). The rules that we use for generation are the result of applying an MCMC procedure to learn a set of likely phrase-fragment pairs from the forests containing all possible pairs. One difference from the work of Peng et al. (2015) is that, while they require the string side to be tight (does not include unaligned words on both sides), we expand the tight phrases to incorporate unaligned words on both sides. The intuition is that they do text-to-AMR parsing, which often involves discarding function words, while our task is AMR-to-text generation, and we need to be able to fill in these unaligned"
D16-1224,P02-1040,0,0.0987956,"have lower path length than others. 3 3.1 Dev 13.13 13.15 17.68 17.19 21.12 23.00 Test 16.94 14.93 18.09 17.75 22.44 23.00 Table 1: Main results. training instances, 1368 dev instances and 1371 test instances. Each instance consists of an AMR graph and a sentence representing the same meaning. Rules are extracted from the training data, and hyperparameters are tuned on the dev set. For tuning and testing, we filter out sentences that have more than 30 words, resulting in 1103 dev instances and 1055 test instances. We train a 4-gram language model (LM) with gigaword (LDC2011T07), and use BLEU (Papineni et al., 2002) as the evaluation metric. To solve the AGTSP, we use Or-tool3 . Our graph-to-string rules are reminiscent of phrase-to-string rules in phrase-based MT (PBMT). We compare our system to a baseline (PBMT) that first linearizes the input AMR graph by breadth first traversal, and then adopts the PBMT system from Moses4 to translate the linearized AMR into a sentence. To traverse the children of an AMR concept, we use the original order in the text file. The MT system is trained with the default setting on the same dataset and LM. We also compare with JAMRgen5 (Flanigan et al., 2016), which is trai"
D16-1224,K15-1004,1,0.941008,"intermediate representation for various text-to-text NLP applications, such as statistical machine translation (SMT) (Jones et al., 2012). The task of AMR-to-text generation is to generate grammatical text containing the same semantic meaning as a given AMR graph. This task is important yet also challenging since each AMR graph usually has multiple corresponding sentences, and syntactic structure and function words are abstracted away when transforming a sentence into AMR (Banarescu et al., 2013). There has been work dealing with text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015). On the other hand, relatively little work has been done on AMR-to-text generation. One recent exception is Flanigan et al. (2016), who first generate a spanning tree for the input AMR graph, and then apply a tree transducer to generate the sentence. Here, we directly generate the sentence from an input AMR by treating AMR-to-text generation as a variant of the traveling salesman problem (TSP). Given an AMR as input, our method first cuts the graph into several rooted and connected fragments (sub-graphs), and then finds the tra"
D16-1224,D15-1136,0,0.160292,"-to-text NLP applications, such as statistical machine translation (SMT) (Jones et al., 2012). The task of AMR-to-text generation is to generate grammatical text containing the same semantic meaning as a given AMR graph. This task is important yet also challenging since each AMR graph usually has multiple corresponding sentences, and syntactic structure and function words are abstracted away when transforming a sentence into AMR (Banarescu et al., 2013). There has been work dealing with text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015). On the other hand, relatively little work has been done on AMR-to-text generation. One recent exception is Flanigan et al. (2016), who first generate a spanning tree for the input AMR graph, and then apply a tree transducer to generate the sentence. Here, we directly generate the sentence from an input AMR by treating AMR-to-text generation as a variant of the traveling salesman problem (TSP). Given an AMR as input, our method first cuts the graph into several rooted and connected fragments (sub-graphs), and then finds the translation for each fragment, before finally ge"
D16-1224,N15-3006,0,0.10334,"sentation for various text-to-text NLP applications, such as statistical machine translation (SMT) (Jones et al., 2012). The task of AMR-to-text generation is to generate grammatical text containing the same semantic meaning as a given AMR graph. This task is important yet also challenging since each AMR graph usually has multiple corresponding sentences, and syntactic structure and function words are abstracted away when transforming a sentence into AMR (Banarescu et al., 2013). There has been work dealing with text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015). On the other hand, relatively little work has been done on AMR-to-text generation. One recent exception is Flanigan et al. (2016), who first generate a spanning tree for the input AMR graph, and then apply a tree transducer to generate the sentence. Here, we directly generate the sentence from an input AMR by treating AMR-to-text generation as a variant of the traveling salesman problem (TSP). Given an AMR as input, our method first cuts the graph into several rooted and connected fragments (sub-graphs), and then finds the translation for each fragment"
D16-1224,E09-1097,0,0.0297715,"(2016) and our work here study sentence generation from a given AMR graph. Different from Flanigan et al. (2016) who map a input AMR graph into a tree before linearization, we apply synchronous rules consisting of AMR graph fragments and text to directly transfer a AMR graph into a sentence. In addition to AMR parsing and generation, there has also been work using AMR as a semantic representation in machine translation (Jones et al., 2012). Our work also belongs to the task of text generation (Reiter and Dale, 1997). There has been work on generating natural language text from a bag of words (Wan et al., 2009; Zhang and Clark, 2015), surface syntactic trees (Zhang, 2013; Song et al., 2014), deep semantic graphs (Bohnet et al., 2010) and logical forms (White, 2004; White and Rajkumar, 2009). We are among the first to investigate generation from AMR, which is a different type of semantic representation. 5 Conclusion In conclusion, we showed that a TSP solver with a few real-valued features can be useful for AMR-totext generation. Our method is based on a set of graph to string rules, yet significantly better than a PBMT-based baseline. This shows that our rule induction algorithm is effective and th"
D16-1224,N15-1040,0,0.0534278,"Missing"
D16-1224,D09-1043,0,0.0313619,"apply synchronous rules consisting of AMR graph fragments and text to directly transfer a AMR graph into a sentence. In addition to AMR parsing and generation, there has also been work using AMR as a semantic representation in machine translation (Jones et al., 2012). Our work also belongs to the task of text generation (Reiter and Dale, 1997). There has been work on generating natural language text from a bag of words (Wan et al., 2009; Zhang and Clark, 2015), surface syntactic trees (Zhang, 2013; Song et al., 2014), deep semantic graphs (Bohnet et al., 2010) and logical forms (White, 2004; White and Rajkumar, 2009). We are among the first to investigate generation from AMR, which is a different type of semantic representation. 5 Conclusion In conclusion, we showed that a TSP solver with a few real-valued features can be useful for AMR-totext generation. Our method is based on a set of graph to string rules, yet significantly better than a PBMT-based baseline. This shows that our rule induction algorithm is effective and that the TSP solver finds better solutions than beam search. Acknowledgments We are grateful for the help of Jeffrey Flanigan, Lin Zhao, and Yifan He. This work was funded by NSF IIS-144"
D16-1224,P09-1038,0,0.143191,"4 System PBMT OnlyConceptRule OnlyInducedRule OnlyBigramLM All JAMR-gen Traveling cost Considering an AGTSP graph whose nodes are clustered into m groups, we define the traveling cost for a tour T in Equation 1: cost(ns , ne ) = − m X log p(“yes”|nTi , nTi+1 ) (1) i=0 where nT0 = ns , nTm+1 = ne and each nTi (i ∈ [1 . . . m]) belongs to a group that is different from all others. Here p(“yes”|nj , ni ) represents a learned score for a move from nj to ni . The choices before nTi are independent from choosing nTi+1 given nTi because of the Markovian property of the TSP problem. Previous methods (Zaslavskiy et al., 2009) evaluate traveling costs p(nTi+1 |nTi ) by using a language model. Inevitably some rules may only cover one translation word, making only bigram language models naturally applicable. Zaslavskiy et al. (2009) introduces a method for incorporating a trigram language model. However, as a result, the number of nodes in the AGTSP graph grows exponentially. To tackle the problem, we treat it as a local binary (“yes” or “no”) classification problem whether we should move to nj from ni . We train a maximum entropy model, where p(“yes”|ni , nj ) is defined as: p(“yes”|ni , nj ) = k hX i 1 exp λi fi (“"
D16-1224,J15-3005,1,0.852028,"k here study sentence generation from a given AMR graph. Different from Flanigan et al. (2016) who map a input AMR graph into a tree before linearization, we apply synchronous rules consisting of AMR graph fragments and text to directly transfer a AMR graph into a sentence. In addition to AMR parsing and generation, there has also been work using AMR as a semantic representation in machine translation (Jones et al., 2012). Our work also belongs to the task of text generation (Reiter and Dale, 1997). There has been work on generating natural language text from a bag of words (Wan et al., 2009; Zhang and Clark, 2015), surface syntactic trees (Zhang, 2013; Song et al., 2014), deep semantic graphs (Bohnet et al., 2010) and logical forms (White, 2004; White and Rajkumar, 2009). We are among the first to investigate generation from AMR, which is a different type of semantic representation. 5 Conclusion In conclusion, we showed that a TSP solver with a few real-valued features can be useful for AMR-totext generation. Our method is based on a set of graph to string rules, yet significantly better than a PBMT-based baseline. This shows that our rule induction algorithm is effective and that the TSP solver finds"
D16-1249,mauser-etal-2008-automatic,0,0.014603,"tic merge of both directions of GIZA++), • MaxEnt (trained on 67k hand-aligned sentences). 1 The metric used for optimization in this work is (T ERB LEU)/2 to prevent the system from using sentence length alone to impact B LEU or T ER. Typical SMT systems use target word count as a feature and it has been observed that B LEU can be optimized by tweaking the weighting of the target word count with no improvement in human assessments of translation quality. Conversely, in order to optimize T ER shorter sentences can be produced. Optimizing the combination of metrics alleviates this effect (Arne Mauser and Ney, 2008). 5.3 Alignment Results Table 2 shows the alignment F1 scores on the alignment test set (447 hand aligned sentences). The MaxEnt model is trained on 67k hand-aligned sentences, and achieves an F1 score of 75.96. For NMT systems, we dump the alignment matrixes and convert them into alignments with following steps. For each target word, we sort the alphas and add the max probability link if it is higher than 0.2. If we only tune the alignment component (A in line 3), we improve the alignment F1 score from 45.76 to 47.87. 2287 system MaxEnt Cov LVNMT (Mi et al., 2016b) A A→J Zh → En A→T A→T→J J G"
D16-1249,D13-1052,1,0.917255,"Missing"
D16-1249,P05-1066,0,0.018233,"es 3 to 5). We have to conduct joint optimization J in order to get a comparable or better result (lines 3, 5 and 6) over the baseline system. Second, when we change the training alignment seeds (Zh → En, GDFA, and MaxEnt) NMT model does not yield significant different results (lines 6 to 8). Third, the smoothed transformation (J + Gau.) gives some improvements over the simple transformation (the last two lines), and achieves the best result (1.2 better than LVNMT, and 0.3 better than Tree-to-string). In terms of B LEU scores, we conduct the statistical significance tests with the signtest of Collins et al. (2005), the results show that the improvements of our J + Gau. over LVNMT are significant on three test sets (p < 0.01). At last, the brevity penalty (BP) consistently gets better after we add the alignment cost to NMT objective. Our alignment objective adjusts the translation length to be more in line with the human references accordingly. pre. 74.86 51.11 50.88 53.18 50.29 53.71 54.29 53.88 44.42 48.90 rec. 77.10 41.42 45.19 49.37 44.90 49.33 48.02 48.25 55.25 55.38 F1 75.96 45.76 47.87 51.21 47.44 51.43 50.97 50.91 49.25 51.94 Table 2: Alignment F1 scores of different models. And we further boost"
D16-1249,N13-1073,0,0.0546246,"7.04 15.61 16.72 16.21 15.80 T- B 13.36 14.24 13.87 20.40 13.97 13.36 13.96 13.24 13.04 Table 1: Single system results in terms of (T ER-B LEU)/2 (T- B, the lower the better) on 5 million Chinese to English training set. B P denotes the brevity penalty. NMT results are on a large vocabulary (300k) and with UNK replaced. The second column shows different alignments (Zh → En (one direction), GDFA (“grow-diag-final-and”), and MaxEnt (Ittycheriah and Roukos, 2005). A, T, and J mean optimize alignment only, translation only, and jointly. Gau. denotes the smoothed transformation. from ‘fast align’ (Dyer et al., 2013). The maximum length of a source phrase is 4. In the training time, we add the reference in order to make the translation reachable. May, 2011) to minimize (T ER- B LEU)/2 1 on the development set. The Cov. LVNMT system is a re-implementation of the enhanced NMT system of Mi et al. (2016a), which employs a coverage embedding model and achieves better performance over large vocabulary NMT Jean et al. (2015). The coverage embedding dimension of each source word is 100. Table 1 shows the translation results of all systems. The syntax-based statistical machine translation model achieves an average"
D16-1249,D11-1125,0,0.0180526,"Missing"
D16-1249,H05-1012,0,0.275361,"s the n-th sentence pair (xn , y∗ n ) in the training set, N is the total number of pairs. 3 Alignment Component The attentions, αt,1 ...αt,l , in each step t play an important role in NMT. However, the accuracy is still far behind the traditional MaxEnt alignment model in terms of alignment F1 score (Mi et al., 2016b; Tu et al., 2016). Thus, in this section, we explicitly add an alignment distance to the objective function in Eq. 5. The “truth” alignments for each sentence pair can be from human annotated data, unsupervised or supervised alignments (e.g. GIZA++ (Och and Ney, 2000) or MaxEnt (Ittycheriah and Roukos, 2005)). Given an alignment matrix A for a sentence pair (x, y) in Figure 2 (a), where we have an end-ofsource-sentence token heosi = xl , and we align all the unaligned target words (y3∗ in this example) to ∗ (end-of-target-sentence) to heosi, also we force ym be aligned to xl with probability one. Then we conduct two transformations to get the probability distribution matrices ((b) and (c) in Figure 2). 2284 Simple Transformation The first transformation simply normalizes each row. Figure 2 (b) shows the result matrix A∗ . The last column in red dashed lines shows the alignments of the special end"
D16-1249,P15-1001,0,0.149248,"ing the alignments of training sentence pairs. We simply compute the distance between the machine attentions and the “true” alignments, and minimize this cost in the training procedure. Our experiments on large-scale Chinese-to-English task show that our model improves both translation and alignment qualities significantly over the large-vocabulary neural machine translation system, and even beats a state-of-the-art traditional syntax-based system. 1 Neural Machine Translation Introduction Neural machine translation (NMT) has gained popularity in recent two years (e.g. (Bahdanau et al., 2014; Jean et al., 2015; Luong et al., 2015; Mi et al., 2016b; Li et al., 2016), especially for the attentionbased models of Bahdanau et al. (2014). The attention model plays a crucial role in NMT, as it shows which source word(s) the model should focus on in order to predict the next target word. However, the attention or alignment quality of NMT is still very low (Mi et al., 2016a; Tu et al., 2016). In this paper, we alleviate the above issue by utilizing the alignments (human annotated data or machine alignments) of the training set. Given the alignments of all the training sentence pairs, we add an alignment dis"
D16-1249,N06-1014,0,0.0159153,"ignment only, then optimize translation). Thus, we divide the network in Figure 1 into alignment A and translation T parts: • A: all networks before the hidden state st , ∗ ). • T: the network g(st , yt−1 If we only optimize A, we keep the parameters in T unchanged. We can also optimize them jointly J. In our experiments, we test different optimization strategies. 4 By contrast, our approach directly uses and optimizes NMT parameters using the “supervised” alignments. Related Work In order to improve the attention or alignment accuracy, Cheng et al. (2016) adapted the agreementbased learning (Liang et al., 2006; Liang et al., 2008), and introduced a combined objective that takes into account both translation directions (source-to-target and target-to-source) and an agreement term between the two alignment directions. 2285 Data Preparation We run our experiments on Chinese to English task. The training corpus consists of approximately 5 million sentences available within the DARPA BOLT Chinese-English task. The corpus includes a mix of newswire, broadcast news, and webblog. We do not include HK Law, HK Hansard and UN data. The Chinese text is segmented with a segmenter trained on CTB data using condi"
D16-1249,P09-1065,1,0.821065,"verage (T ER-B LEU)/2 of 14.24, which is about 0.9 points worse than Tree-to-string SMT system. Please note that all systems are single systems. It is highly possible that ensemble of NMT systems with different random seeds can lead to better results over SMT. We test three different alignments: Following Jean et al. (2015), we dump the alignments, attentions, for each sentence, and replace UNKs with the word-to-word translation model or the aligned source word. Our SMT system is a hybrid syntax-based tree-tostring model (Zhao and Al-onaizan, 2008), a simplified version of the joint decoding (Liu et al., 2009; Cmejrek et al., 2013). We parse the Chinese side with Berkeley parser, and align the bilingual sentences with GIZA++ and MaxEnt. and extract Hiero and tree-to-string rules on the training set. Our two 5-gram language models are trained on the English side of the parallel corpus, and on monolingual corpora (around 10 billion words from Gigaword (LDC2011T07), respectively.As suggested by Zhang (2016), NMT systems can achieve better results with the help of those monolingual corpora. In this paper, our NMT systems only use the bilingual data. We tune our system with PRO (Hopkins and 2286 5.2 Tr"
D16-1249,D15-1166,0,0.0873705,"of training sentence pairs. We simply compute the distance between the machine attentions and the “true” alignments, and minimize this cost in the training procedure. Our experiments on large-scale Chinese-to-English task show that our model improves both translation and alignment qualities significantly over the large-vocabulary neural machine translation system, and even beats a state-of-the-art traditional syntax-based system. 1 Neural Machine Translation Introduction Neural machine translation (NMT) has gained popularity in recent two years (e.g. (Bahdanau et al., 2014; Jean et al., 2015; Luong et al., 2015; Mi et al., 2016b; Li et al., 2016), especially for the attentionbased models of Bahdanau et al. (2014). The attention model plays a crucial role in NMT, as it shows which source word(s) the model should focus on in order to predict the next target word. However, the attention or alignment quality of NMT is still very low (Mi et al., 2016a; Tu et al., 2016). In this paper, we alleviate the above issue by utilizing the alignments (human annotated data or machine alignments) of the training set. Given the alignments of all the training sentence pairs, we add an alignment distance cost to the ob"
D16-1249,D16-1096,1,0.667593,"pairs. We simply compute the distance between the machine attentions and the “true” alignments, and minimize this cost in the training procedure. Our experiments on large-scale Chinese-to-English task show that our model improves both translation and alignment qualities significantly over the large-vocabulary neural machine translation system, and even beats a state-of-the-art traditional syntax-based system. 1 Neural Machine Translation Introduction Neural machine translation (NMT) has gained popularity in recent two years (e.g. (Bahdanau et al., 2014; Jean et al., 2015; Luong et al., 2015; Mi et al., 2016b; Li et al., 2016), especially for the attentionbased models of Bahdanau et al. (2014). The attention model plays a crucial role in NMT, as it shows which source word(s) the model should focus on in order to predict the next target word. However, the attention or alignment quality of NMT is still very low (Mi et al., 2016a; Tu et al., 2016). In this paper, we alleviate the above issue by utilizing the alignments (human annotated data or machine alignments) of the training set. Given the alignments of all the training sentence pairs, we add an alignment distance cost to the objective function."
D16-1249,P16-5005,0,0.0391706,"tem, and even beats a state-of-the-art traditional syntax-based system. 1 Neural Machine Translation Introduction Neural machine translation (NMT) has gained popularity in recent two years (e.g. (Bahdanau et al., 2014; Jean et al., 2015; Luong et al., 2015; Mi et al., 2016b; Li et al., 2016), especially for the attentionbased models of Bahdanau et al. (2014). The attention model plays a crucial role in NMT, as it shows which source word(s) the model should focus on in order to predict the next target word. However, the attention or alignment quality of NMT is still very low (Mi et al., 2016a; Tu et al., 2016). In this paper, we alleviate the above issue by utilizing the alignments (human annotated data or machine alignments) of the training set. Given the alignments of all the training sentence pairs, we add an alignment distance cost to the objective function. Thus, we not only maximize the log translation probabilities, but also minimize the alignment distance cost. Large-scale experiments over Chineseto-English on various test sets show that our best method for a single system improves the translation quality significantly over the large vocabulary NMT system (Section 5) and beats the state-of-"
D16-1249,D16-1160,0,0.0343645,"word-to-word translation model or the aligned source word. Our SMT system is a hybrid syntax-based tree-tostring model (Zhao and Al-onaizan, 2008), a simplified version of the joint decoding (Liu et al., 2009; Cmejrek et al., 2013). We parse the Chinese side with Berkeley parser, and align the bilingual sentences with GIZA++ and MaxEnt. and extract Hiero and tree-to-string rules on the training set. Our two 5-gram language models are trained on the English side of the parallel corpus, and on monolingual corpora (around 10 billion words from Gigaword (LDC2011T07), respectively.As suggested by Zhang (2016), NMT systems can achieve better results with the help of those monolingual corpora. In this paper, our NMT systems only use the bilingual data. We tune our system with PRO (Hopkins and 2286 5.2 Translation Results • Zh → En (one direction of GIZA++), • GDFA (the “grow-diag-final-and” heuristic merge of both directions of GIZA++), • MaxEnt (trained on 67k hand-aligned sentences). 1 The metric used for optimization in this work is (T ERB LEU)/2 to prevent the system from using sentence length alone to impact B LEU or T ER. Typical SMT systems use target word count as a feature and it has been o"
D16-1249,D08-1060,0,0.0535876,"LEU)/2 of 13.36 on three test sets. The Cov. LVNMT system achieves an average (T ER-B LEU)/2 of 14.24, which is about 0.9 points worse than Tree-to-string SMT system. Please note that all systems are single systems. It is highly possible that ensemble of NMT systems with different random seeds can lead to better results over SMT. We test three different alignments: Following Jean et al. (2015), we dump the alignments, attentions, for each sentence, and replace UNKs with the word-to-word translation model or the aligned source word. Our SMT system is a hybrid syntax-based tree-tostring model (Zhao and Al-onaizan, 2008), a simplified version of the joint decoding (Liu et al., 2009; Cmejrek et al., 2013). We parse the Chinese side with Berkeley parser, and align the bilingual sentences with GIZA++ and MaxEnt. and extract Hiero and tree-to-string rules on the training set. Our two 5-gram language models are trained on the English side of the parallel corpus, and on monolingual corpora (around 10 billion words from Gigaword (LDC2011T07), respectively.As suggested by Zhang (2016), NMT systems can achieve better results with the help of those monolingual corpora. In this paper, our NMT systems only use the biling"
D16-1249,P16-2021,1,\N,Missing
D16-1249,P00-1056,0,\N,Missing
D18-1110,P11-1060,0,0.107614,"Missing"
D18-1110,P14-5010,0,0.00409275,", and tuned on the development set for ATIS. The learning rate is set to 0.001. The decoder has 1 layer, and its hidden state size is 300. The dropout strategy (Srivastava et al., 2014) with the ratio of 0.5 is applied at the decoder layer to avoid overfitting. We is initialized using GloVe word vectors from Pennington et al. (2014) and the dimension of word embedding is 300. For the graph encoder, the hop size K is set to 10, the non-linearity function σ is implemented as ReLU (Glorot et al., 2011), the parameters of the aggregators are randomly initialized. We use the Stanford CoreNLP tool (Manning et al., 2014) to generate the dependency and constituent trees. Results and Discussion. Table 1 summarizes the results of our model and existing semantic parsers on three datasets. Our model achieves competitive performance on Jobs640, ATIS and Geo880. Our work is the first to use both multiple trees and the word sequence for semantic parsing, and it outperforms the Seq2Seq model reported in Dong and Lapata (2016), which only uses limited syntactic information. Comparison with Baseline. To better demonstrate that our work is an effective way to utilize both multiple trees and the word sequence for semantic"
D18-1110,D14-1162,0,0.0832142,"standard train/development/test split as previous works, and the logical form accuracy as our evaluation metric. The model is trained using the Adam optimizer (Kingma and Ba, 2014), with mini-batch size 30. Our hyper-parameters are cross-validated on the training set for Jobs640 and Geo880, and tuned on the development set for ATIS. The learning rate is set to 0.001. The decoder has 1 layer, and its hidden state size is 300. The dropout strategy (Srivastava et al., 2014) with the ratio of 0.5 is applied at the decoder layer to avoid overfitting. We is initialized using GloVe word vectors from Pennington et al. (2014) and the dimension of word embedding is 300. For the graph encoder, the hop size K is set to 10, the non-linearity function σ is implemented as ReLU (Glorot et al., 2011), the parameters of the aggregators are randomly initialized. We use the Stanford CoreNLP tool (Manning et al., 2014) to generate the dependency and constituent trees. Results and Discussion. Table 1 summarizes the results of our model and existing semantic parsers on three datasets. Our model achieves competitive performance on Jobs640, ATIS and Geo880. Our work is the first to use both multiple trees and the word sequence fo"
D18-1110,P16-1004,0,0.418197,"form. Experimental results on benchmark datasets show that our model is comparable to the state-of-the-art on Jobs640, ATIS, and Geo880. Experimental results on adversarial examples demonstrate the robustness of the model is also improved by encoding more syntactic information. 1 Introduction The task of semantic parsing is to translate text to its formal meaning representations, such as logical forms or structured queries. Recent neural semantic parsers approach this problem by learning soft alignments between natural language and logical forms from (text, logic) pairs (Jia and Liang, 2016; Dong and Lapata, 2016; Krishnamurthy et al., 2017). All these parsers follow the conventional encoder-decoder architecture that first encodes the text into a distributional representation and then decodes it to a logical form. These parsers may differ in the choice of the decoders, such as sequence or tree decoders, but they utilize the same encoder which is essentially a sequential Long Short-Term Memory network (SeqLSTM). This encoder only extracts word order features while neglecting useful syntactic information, such as dependency parse and constituency parse. However, the syntactic features capture important"
D18-1110,J08-2005,0,0.0433929,"der + Dep + Cons Accori 84.8 83.5 82.9 84.0 85.2 84.9 86.0 Accpara 78.7 80.1 77.3 80.7 82.3 79.9 83.5 Related Work Existing works of generating text representation has evolved into two main streams. The first one is based on the word order, that is, either generating general purpose and domain independent embeddings of word sequences (Wu et al., 2018a; Arora et al., 2017), or building Bi-directional LSTMs over the text (Zhang et al., 2018). These methods neglect other syntactic information, which, however, has been proved to be useful in shallow semantic parsing, e.g., semantic role labeling (Punyakanok et al., 2008). To address this, recent works attempt to incorporate these syntactic information into the text representation. For example, Xu et al. (2016) builds separated neural networks for different types of syntactic annotation. Gormley et al. (2015); Wu et al. (2018b) decompose a graph to simpler sub-graphs and embed these subgraphs independently. Our approach, compared to the above methods, provided a unified solution to arbitrary combinations of syntactic graphs. In parallel to syntactic features, other works leverage additional information such as dialogue and paraphrasing for semantic parsing (Su"
D18-1110,P17-1105,0,0.045337,"Missing"
D18-1110,D15-1205,1,0.917794,"Missing"
D18-1110,P18-1124,0,0.0396135,"s, recent works attempt to incorporate these syntactic information into the text representation. For example, Xu et al. (2016) builds separated neural networks for different types of syntactic annotation. Gormley et al. (2015); Wu et al. (2018b) decompose a graph to simpler sub-graphs and embed these subgraphs independently. Our approach, compared to the above methods, provided a unified solution to arbitrary combinations of syntactic graphs. In parallel to syntactic features, other works leverage additional information such as dialogue and paraphrasing for semantic parsing (Su and Yan, 2017; Gur et al., 2018). 6 Diff. -6.1 -3.4 -5.6 -3.3 -2.9 -5.0 -2.5 Conclusions Existing neural semantic parsers mainly leverage word order features while neglecting other valuable syntactic information. To address this, we propose to build a syntactic graph which represents three types of syntactic information, and further apply a novel graph-to-sequence model to map the syntactic graph to a logical form. Experimental results show that the robustness of our model is improved due to the incorporating more aspects of syntactic information, and our model outperforms previous semantic parsing systems. Table 3: Evaluati"
D18-1110,D17-1009,0,0.058631,"Missing"
D18-1110,P16-1002,0,0.0205339,"and decode a logical form. Experimental results on benchmark datasets show that our model is comparable to the state-of-the-art on Jobs640, ATIS, and Geo880. Experimental results on adversarial examples demonstrate the robustness of the model is also improved by encoding more syntactic information. 1 Introduction The task of semantic parsing is to translate text to its formal meaning representations, such as logical forms or structured queries. Recent neural semantic parsers approach this problem by learning soft alignments between natural language and logical forms from (text, logic) pairs (Jia and Liang, 2016; Dong and Lapata, 2016; Krishnamurthy et al., 2017). All these parsers follow the conventional encoder-decoder architecture that first encodes the text into a distributional representation and then decodes it to a logical form. These parsers may differ in the choice of the decoders, such as sequence or tree decoders, but they utilize the same encoder which is essentially a sequential Long Short-Term Memory network (SeqLSTM). This encoder only extracts word order features while neglecting useful syntactic information, such as dependency parse and constituency parse. However, the syntactic feat"
D18-1110,D17-1127,0,0.0233625,"Missing"
D18-1110,D17-1160,0,0.0118697,"ults on benchmark datasets show that our model is comparable to the state-of-the-art on Jobs640, ATIS, and Geo880. Experimental results on adversarial examples demonstrate the robustness of the model is also improved by encoding more syntactic information. 1 Introduction The task of semantic parsing is to translate text to its formal meaning representations, such as logical forms or structured queries. Recent neural semantic parsers approach this problem by learning soft alignments between natural language and logical forms from (text, logic) pairs (Jia and Liang, 2016; Dong and Lapata, 2016; Krishnamurthy et al., 2017). All these parsers follow the conventional encoder-decoder architecture that first encodes the text into a distributional representation and then decodes it to a logical form. These parsers may differ in the choice of the decoders, such as sequence or tree decoders, but they utilize the same encoder which is essentially a sequential Long Short-Term Memory network (SeqLSTM). This encoder only extracts word order features while neglecting useful syntactic information, such as dependency parse and constituency parse. However, the syntactic features capture important structural information of the"
D18-1110,P15-1150,0,0.0594178,"d sequence for semantic parsing, we compare with an addiExperiments We evaluate our model on three datasets: Jobs640, a set of 640 queries to a database of job listings; 920 could correctly predict these logical forms while the model that only uses word order features may fail. tional straightforward baseline method (referred as BASELINE in Table 1). To deal with the graph input, the BASELINE decomposes the graph embedding to two steps and applies different types of encoders sequentially: (1) a SeqLSTM to extract word order features, which results in word embeddings, Wseq ; (2) two TreeLSTMs (Tai et al., 2015) to extract the dependency tree and constituency features while taking Wseq as initial word embeddings. The resulted word embeddings and nonterminal node embeddings (from TreeLSTMs) are then fed into a sequence decoder. Complicated Query & Predicted Logical Forms Jobs Q: what are the jobs for programmer that has salary 50000 that uses c++ and not related with AI Pred: answer(J,(job(J),-((area(J,R),const(R,’ai’))), language(J,L),const(L,’c++’), title(J,P), const(P,’Programmer’),salary greater than(J, 50000,year)))). Geo Q: which is the density of the state that the largest river in the united s"
D18-1110,D13-1161,0,0.0445237,"Missing"
D18-1110,D14-1135,0,0.0276615,"Missing"
D18-1110,D11-1140,0,0.0608927,"Missing"
D18-1110,D18-1482,1,0.874088,"Missing"
D18-1110,P16-1220,1,0.856177,"epresentation has evolved into two main streams. The first one is based on the word order, that is, either generating general purpose and domain independent embeddings of word sequences (Wu et al., 2018a; Arora et al., 2017), or building Bi-directional LSTMs over the text (Zhang et al., 2018). These methods neglect other syntactic information, which, however, has been proved to be useful in shallow semantic parsing, e.g., semantic role labeling (Punyakanok et al., 2008). To address this, recent works attempt to incorporate these syntactic information into the text representation. For example, Xu et al. (2016) builds separated neural networks for different types of syntactic annotation. Gormley et al. (2015); Wu et al. (2018b) decompose a graph to simpler sub-graphs and embed these subgraphs independently. Our approach, compared to the above methods, provided a unified solution to arbitrary combinations of syntactic graphs. In parallel to syntactic features, other works leverage additional information such as dialogue and paraphrasing for semantic parsing (Su and Yan, 2017; Gur et al., 2018). 6 Diff. -6.1 -3.4 -5.6 -3.3 -2.9 -5.0 -2.5 Conclusions Existing neural semantic parsers mainly leverage wor"
D18-1110,D07-1071,0,0.124847,"Missing"
D18-1110,P18-1030,0,0.101787,"t experiment, we still train the model on Devori and evaluate it on the newly created dataset. Feature Word Order Dep Cons Dep + Cons Word Order + Dep Word Order + Cons Word Order + Dep + Cons Accori 84.8 83.5 82.9 84.0 85.2 84.9 86.0 Accpara 78.7 80.1 77.3 80.7 82.3 79.9 83.5 Related Work Existing works of generating text representation has evolved into two main streams. The first one is based on the word order, that is, either generating general purpose and domain independent embeddings of word sequences (Wu et al., 2018a; Arora et al., 2017), or building Bi-directional LSTMs over the text (Zhang et al., 2018). These methods neglect other syntactic information, which, however, has been proved to be useful in shallow semantic parsing, e.g., semantic role labeling (Punyakanok et al., 2008). To address this, recent works attempt to incorporate these syntactic information into the text representation. For example, Xu et al. (2016) builds separated neural networks for different types of syntactic annotation. Gormley et al. (2015); Wu et al. (2018b) decompose a graph to simpler sub-graphs and embed these subgraphs independently. Our approach, compared to the above methods, provided a unified solution to"
D18-1110,N15-1162,0,0.0287305,"Missing"
D18-1112,P16-1154,0,0.0711736,"ble 1: Results on the WikiSQL (above) and Stackoverflow tions where S and G denotes Seq2Seq and Graph2Seq models, respectively. (below). map SELECT to which, WHERE to where, &gt; to more than. This method translates the SQL query of Figure 1 to which company where assets more than val0 and sales more than val0 and industry less than or equal to val1 and profits equals val2 . Seq2Seq. We choose two Seq2Seq models as our baselines. The first one is the attentionbased Seq2Seq model proposed by Bahdanau et al. (2014), and the second one additionally introduces the copy mechanism in the decoder side (Gu et al., 2016). To evaluate these models, we employ a template to convert the SQL query into a sequence: “SELECT + &lt;aggregation function&gt; + &lt;Split Results and Discussion Table 1 summarizes the results of our models and baselines. Although the template-based method achieves decent BLEU scores, its grammaticality score is substantially worse than other baselines. We can see that on both two datasets, our Graph2Seq models perform significantly better than the Seq2Seq and Tree2Seq baselines. One possible reason is that in our graph encoder, the node embedding retains the information of neighbor nodes within K h"
D18-1112,P16-1195,0,0.301055,"e since it helps non-expert users to understand the esoteric SQL queries that are used to retrieve the answers through the questionanswering process (Simitsis and Ioannidis, 2009) using varous text embeddings techniques (Kim, 2014; Arora et al., 2017; Wu et al., 2018a). Earlier attempts for SQL-to-text task are rulebased and template-based (Koutrika et al., 2010; Ngonga Ngomo et al., 2013). Despite requiring intensive human efforts to design temples or rules, these approaches still tend to generate rigid and stylized language that lacks the natural text of the human language. To address this, Iyer et al. (2016) proposes a sequence-to-sequence (Seq2Seq) network to model the SQL query and natural language jointly. However, since the SQL is designed ∗ Work done when the author was at IBM Research. 931 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 931–936 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics On the decoder side, we develop an RNN-based decoder which takes the graph vector representation as the initial hidden state to generate the sequences while employing an attention mechanism over all node emb"
D18-1112,P02-1040,0,0.109045,"nference phase, we use the beam search algorithm with beam size = 5. 4 Experiments We evaluate our model on two datasets, WikiSQL (Zhong et al., 2017) and Stackoverflow (Iyer et al., 2016). WikiSQL consists of a corpus of 87,726 hand-annotated SQL query and natural language question pairs. These SQL queries are further split into training (61,297 examples), development (9,145 examples) and test sets (17,284 examples). StackOverflow consists of 32,337 SQL query and natural language question pairs, and we use the same train/development/test split as (Iyer et al., 2016). We use the BLEU-4 score (Papineni et al., 2002) as our automatic evaluation metric and also perform a human study. For human evaluation, we randomly sampled 1,000 predicted results and asked three native English speakers to rate each interpretation against both the correctness conforming to the input SQL and grammaticality on a scale between 1 and 5. We compare some variants of our model against the template, Seq2Seq, and Tree2Seq baselines. Graph2Seq-PGE. This method uses the Pooling method for generating Graph Embedding. Graph2Seq-NGE. This method uses the Node based Graph Embedding. Template. We implement a template-based method which f"
D18-1112,D14-1162,0,0.0903546,"Missing"
D18-1112,P18-1150,1,0.769483,"Missing"
D18-1112,D18-1482,1,0.913451,"he Seq2Seq and Tree2Seq baselines, achieving the state-of-the-art performance. Interpretation: which company has both the market value and assets higher than val0, ranking in top val2 and revenue of val3 Figure 1: An example of SQL query and its interpretation. to express graph-structured query intent, the sequence encoder may need an elaborate design to fully capture the global structure information. Intuitively, varous graph encoding techniques base on deep neural network (Kipf and Welling, 2016; Hamilton et al., 2017; Song et al., 2018) or based on Graph Kernels (Vishwanathan et al., 2010; Wu et al., 2018b), whose goal is to learn the node-level or graph-level representations for a given graph, are more proper to tackle this problem. In this paper, we first introduce a strategy to represent the SQL query as a directed graph (see §2) and further make full use of a novel graphto-sequence (Graph2Seq) model (Xu et al., 2018) that encodes this graph-structured SQL query, and then decodes its interpretation (see §3). On the encoder side, we extend the graph encoding work of Hamilton et al. (2017) by encoding the edge direction information into the node embedding. Our encoder learns the representatio"
D18-1246,D17-1209,0,0.168044,"ia the dependency PREP OF NN path “exon-19 ! gene ! EGFR” is lost from the original subgraph. Second, using LSTMs on both DAGs, information of only ancestors and descendants can be incorporated for each word. Sibling information, which may also be important, is not included. A potential solution to the problems above is to model a graph as a whole, learning its representation without breaking it into two DAGs. Due to the existence of cycles, naive extension of tree LSTMs cannot serve this goal. Recently, graph convolutional networks (GCN) (Kipf and Welling, 2017; Marcheggiani and Titov, 2017; Bastings et al., 2017) and graph recurrent networks (GRN) (Song et al., 2018; Zhang et al., 2018) have been proposed for representing graph structures for NLP tasks. Such methods encode a given graph by hierarchically learning representations of neighboring nodes in the graphs via their connecting edges. While GCNs use CNN for information exchange, GRNs take gated recurrent steps to this end. For fair comparison with DAG LSTMs, we build a graph LSTM by extending Song et al. (2018), which strictly follow the configurations of Peng et al. (2017) such as the source of features and hyper parameter settings. In particul"
D18-1246,P10-1160,0,0.346512,"to be useful for detecting explicit facts, such as cause-effect (Hendrickx et al., 2009), and predicting the effectiveness of a medicine on a cancer caused by mutation of a certain gene in the biomedical domain (Quirk and Poon, 2017; Peng et al., 2017). While most existing work extracts relations within a sentence (Zelenko et al., 2003; Palmer et al., 2005; Zhao and Grishman, 2005; Jiang and Zhai, 2007; Plank and Moschitti, 2013; Li and Ji, 2014; Gormley et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2017), the task of cross-sentence relation extraction has received increasing attention (Gerber and Chai, 2010; Yoshikawa et al., 2011). Recently, Peng ⇤ Equal contribution et al. (2017) extend cross-sentence relation extraction by further detecting relations among several entity mentions (n-ary relation). Table 1 shows an example, which conveys the fact that cancers caused by the 858E mutation on EGFR gene can respond to the gefitinib medicine. The three entity mentions form a ternary relation yet appear in distinct sentences. Peng et al. (2017) proposed a graph-structured LSTM for n-ary relation extraction. As shown in Figure 1 (a), graphs are constructed from input sentences with dependency edges,"
D18-1246,N18-1082,0,0.0254715,"Missing"
D18-1246,D15-1205,0,0.0337129,"a central task in natural language processing, relation extraction has been investigated on news, web text and biomedical domains. It has been shown to be useful for detecting explicit facts, such as cause-effect (Hendrickx et al., 2009), and predicting the effectiveness of a medicine on a cancer caused by mutation of a certain gene in the biomedical domain (Quirk and Poon, 2017; Peng et al., 2017). While most existing work extracts relations within a sentence (Zelenko et al., 2003; Palmer et al., 2005; Zhao and Grishman, 2005; Jiang and Zhai, 2007; Plank and Moschitti, 2013; Li and Ji, 2014; Gormley et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2017), the task of cross-sentence relation extraction has received increasing attention (Gerber and Chai, 2010; Yoshikawa et al., 2011). Recently, Peng ⇤ Equal contribution et al. (2017) extend cross-sentence relation extraction by further detecting relations among several entity mentions (n-ary relation). Table 1 shows an example, which conveys the fact that cancers caused by the 858E mutation on EGFR gene can respond to the gefitinib medicine. The three entity mentions form a ternary relation yet appear in distinct sentences. Peng et al. (2017) proposed"
D18-1246,W09-2415,0,0.154489,"Missing"
D18-1246,N07-1015,0,0.0610435,"tion in EGFR gene respond to gefitinib treatment. Introduction As a central task in natural language processing, relation extraction has been investigated on news, web text and biomedical domains. It has been shown to be useful for detecting explicit facts, such as cause-effect (Hendrickx et al., 2009), and predicting the effectiveness of a medicine on a cancer caused by mutation of a certain gene in the biomedical domain (Quirk and Poon, 2017; Peng et al., 2017). While most existing work extracts relations within a sentence (Zelenko et al., 2003; Palmer et al., 2005; Zhao and Grishman, 2005; Jiang and Zhai, 2007; Plank and Moschitti, 2013; Li and Ji, 2014; Gormley et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2017), the task of cross-sentence relation extraction has received increasing attention (Gerber and Chai, 2010; Yoshikawa et al., 2011). Recently, Peng ⇤ Equal contribution et al. (2017) extend cross-sentence relation extraction by further detecting relations among several entity mentions (n-ary relation). Table 1 shows an example, which conveys the fact that cancers caused by the 858E mutation on EGFR gene can respond to the gefitinib medicine. The three entity mentions form a ternary rela"
D18-1246,P14-1038,0,0.0304269,"Introduction As a central task in natural language processing, relation extraction has been investigated on news, web text and biomedical domains. It has been shown to be useful for detecting explicit facts, such as cause-effect (Hendrickx et al., 2009), and predicting the effectiveness of a medicine on a cancer caused by mutation of a certain gene in the biomedical domain (Quirk and Poon, 2017; Peng et al., 2017). While most existing work extracts relations within a sentence (Zelenko et al., 2003; Palmer et al., 2005; Zhao and Grishman, 2005; Jiang and Zhai, 2007; Plank and Moschitti, 2013; Li and Ji, 2014; Gormley et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2017), the task of cross-sentence relation extraction has received increasing attention (Gerber and Chai, 2010; Yoshikawa et al., 2011). Recently, Peng ⇤ Equal contribution et al. (2017) extend cross-sentence relation extraction by further detecting relations among several entity mentions (n-ary relation). Table 1 shows an example, which conveys the fact that cancers caused by the 858E mutation on EGFR gene can respond to the gefitinib medicine. The three entity mentions form a ternary relation yet appear in distinct sentences. Peng"
D18-1246,P14-5010,0,0.00328524,", or a multi-class classification problem of detecting which relation holds for the entity mentions. Take Table 1 as an example. The binary classification task is to determine whether gefitinib would have an effect on this type of cancer, given a cancer patient with 858E mutation on gene EGFR. The multi-class classification task is to detect the exact drug effect: response, resistance, sensitivity, etc. 3 Baseline: Bi-directional DAG LSTM Peng et al. (2017) formulate the task as a graphstructured problem in order to adopt rich dependency and discourse features. In particular, Stanford parser (Manning et al., 2014) is used to assign syntactic structure to input sentences, and heads of two consecutive sentences are connected to represent discourse information, resulting in a graph structure. For each input graph G = (V, E), the nodes V are words within input sentences, and each edge e 2 E connects two words that either have a relation or are adjacent to each other. Each edge is denoted as a triple (i, j, l), where i and j are the indices of the source and target words, respectively, and the edge label l indicates either a dependency or discourse relation (such as “nsubj”) or a relative position (such as"
D18-1246,D17-1159,0,0.24825,"between “exon-19” and “EGFR” via the dependency PREP OF NN path “exon-19 ! gene ! EGFR” is lost from the original subgraph. Second, using LSTMs on both DAGs, information of only ancestors and descendants can be incorporated for each word. Sibling information, which may also be important, is not included. A potential solution to the problems above is to model a graph as a whole, learning its representation without breaking it into two DAGs. Due to the existence of cycles, naive extension of tree LSTMs cannot serve this goal. Recently, graph convolutional networks (GCN) (Kipf and Welling, 2017; Marcheggiani and Titov, 2017; Bastings et al., 2017) and graph recurrent networks (GRN) (Song et al., 2018; Zhang et al., 2018) have been proposed for representing graph structures for NLP tasks. Such methods encode a given graph by hierarchically learning representations of neighboring nodes in the graphs via their connecting edges. While GCNs use CNN for information exchange, GRNs take gated recurrent steps to this end. For fair comparison with DAG LSTMs, we build a graph LSTM by extending Song et al. (2018), which strictly follow the configurations of Peng et al. (2017) such as the source of features and hyper paramet"
D18-1246,P05-1061,0,0.240902,"comparisons with the binary relation extraction results. However, the performance gaps between GS GLSTM and Bidir DAG LSTM dramatically increase, showing the superiority of GS GLSTM over Bidir DAG LSTM in utilizing context information. 7 B INARY 50.7 71.7* Table 6: Average test accuracies for multi-class relation extraction with all instances (“Cross”). GS GLSTM 85 80 T ERNARY 51.7 71.1* Related Work N -ary relation extraction N -ary relation extractions can be traced back to MUC-7 (Chinchor, 1998), which focuses on entity-attribution relations. It has also been studied in biomedical domain (McDonald et al., 2005), but only the instances within a single sentence are considered. Previous work on cross-sentence relation extraction relies on either explicit co-reference annotation (Gerber and Chai, 2010; Yoshikawa et al., 2011), or the assumption that the whole document refers to a single coherent event (Wick et al., 2006; Swampillai and Stevenson, 2011). Both simplify the problem and reduce the need for learning better contextual representation of entity mentions. A notable exception is Quirk and Poon (2017), who adopt distant supervision and integrated contextual evidence of diverse types without relyin"
D18-1246,P16-1105,0,0.546468,"ral language processing, relation extraction has been investigated on news, web text and biomedical domains. It has been shown to be useful for detecting explicit facts, such as cause-effect (Hendrickx et al., 2009), and predicting the effectiveness of a medicine on a cancer caused by mutation of a certain gene in the biomedical domain (Quirk and Poon, 2017; Peng et al., 2017). While most existing work extracts relations within a sentence (Zelenko et al., 2003; Palmer et al., 2005; Zhao and Grishman, 2005; Jiang and Zhai, 2007; Plank and Moschitti, 2013; Li and Ji, 2014; Gormley et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2017), the task of cross-sentence relation extraction has received increasing attention (Gerber and Chai, 2010; Yoshikawa et al., 2011). Recently, Peng ⇤ Equal contribution et al. (2017) extend cross-sentence relation extraction by further detecting relations among several entity mentions (n-ary relation). Table 1 shows an example, which conveys the fact that cancers caused by the 858E mutation on EGFR gene can respond to the gefitinib medicine. The three entity mentions form a ternary relation yet appear in distinct sentences. Peng et al. (2017) proposed a graph-structured LST"
D18-1246,P15-1150,0,0.174145,"Missing"
D18-1246,W06-1671,0,0.0844934,"extraction with all instances (“Cross”). GS GLSTM 85 80 T ERNARY 51.7 71.1* Related Work N -ary relation extraction N -ary relation extractions can be traced back to MUC-7 (Chinchor, 1998), which focuses on entity-attribution relations. It has also been studied in biomedical domain (McDonald et al., 2005), but only the instances within a single sentence are considered. Previous work on cross-sentence relation extraction relies on either explicit co-reference annotation (Gerber and Chai, 2010; Yoshikawa et al., 2011), or the assumption that the whole document refers to a single coherent event (Wick et al., 2006; Swampillai and Stevenson, 2011). Both simplify the problem and reduce the need for learning better contextual representation of entity mentions. A notable exception is Quirk and Poon (2017), who adopt distant supervision and integrated contextual evidence of diverse types without relying on these assumptions. However, they only study binary relations. We follow Peng et al. (2017) by studying ternary cross-sentence relations. Graph encoder Liang et al. (2016) build a graph LSTM model for semantic object parsing, which aims to segment objects within an image into more fine-grained, semanticall"
D18-1246,J05-1004,1,0.140836,"An example showing that tumors with L858E mutation in EGFR gene respond to gefitinib treatment. Introduction As a central task in natural language processing, relation extraction has been investigated on news, web text and biomedical domains. It has been shown to be useful for detecting explicit facts, such as cause-effect (Hendrickx et al., 2009), and predicting the effectiveness of a medicine on a cancer caused by mutation of a certain gene in the biomedical domain (Quirk and Poon, 2017; Peng et al., 2017). While most existing work extracts relations within a sentence (Zelenko et al., 2003; Palmer et al., 2005; Zhao and Grishman, 2005; Jiang and Zhai, 2007; Plank and Moschitti, 2013; Li and Ji, 2014; Gormley et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2017), the task of cross-sentence relation extraction has received increasing attention (Gerber and Chai, 2010; Yoshikawa et al., 2011). Recently, Peng ⇤ Equal contribution et al. (2017) extend cross-sentence relation extraction by further detecting relations among several entity mentions (n-ary relation). Table 1 shows an example, which conveys the fact that cancers caused by the 858E mutation on EGFR gene can respond to the gefitinib medicine"
D18-1246,D18-1110,1,0.848544,"ng node states sequentially: for each input graph, a start node and a node sequence are chosen, which determines the order of recurrent state updates. In contrast, our graph LSTM do not need ordering of graph nodes, and is highly parallelizable. 2233 Graph convolutional networks (GCNs) and very recently graph recurrent networks (GRNs) have been used to model graph structures in NLP tasks, such as semantic role labeling (Marcheggiani and Titov, 2017), machine translation (Bastings et al., 2017), text generation (Song et al., 2018), text representation (Zhang et al., 2018) and semantic parsing (Xu et al., 2018b,a). In particular, Zhang et al. (2018) use GRN to represent raw sentences by building a graph structure of neighboring words and a sentence-level node, showing that the encoder outperforms BiLSTMs and Transformer (Vaswani et al., 2017) on classification and sequence labeling tasks; Song et al. (2018) build a GRN for encoding AMR graphs, showing that the representation is superior compared to BiLSTM on serialized AMR. Our work is in line with their work in the investigation of GRN on NLP. To our knowledge, we are the first to use GRN for representing dependency and discourse structures. Under"
D18-1246,Q17-1008,0,0.11233,"parallelization. On a standard benchmark, our model shows the best result in the literature. 1 Table 1: An example showing that tumors with L858E mutation in EGFR gene respond to gefitinib treatment. Introduction As a central task in natural language processing, relation extraction has been investigated on news, web text and biomedical domains. It has been shown to be useful for detecting explicit facts, such as cause-effect (Hendrickx et al., 2009), and predicting the effectiveness of a medicine on a cancer caused by mutation of a certain gene in the biomedical domain (Quirk and Poon, 2017; Peng et al., 2017). While most existing work extracts relations within a sentence (Zelenko et al., 2003; Palmer et al., 2005; Zhao and Grishman, 2005; Jiang and Zhai, 2007; Plank and Moschitti, 2013; Li and Ji, 2014; Gormley et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2017), the task of cross-sentence relation extraction has received increasing attention (Gerber and Chai, 2010; Yoshikawa et al., 2011). Recently, Peng ⇤ Equal contribution et al. (2017) extend cross-sentence relation extraction by further detecting relations among several entity mentions (n-ary relation). Table 1 shows an example, which co"
D18-1246,D14-1162,0,0.0965546,"or nonresponse”, “sensitivity”, “response”, “resistance” and “None”. We follow Peng et al. (2017) and binarize multi-class labels by grouping all relation classes as “Yes” and treat “None” as “No”. 6.2 Settings Following Peng et al. (2017), five-fold crossvalidation is used for evaluating the models,3 and the final test accuracy is calculated by averaging the test accuracies over all five folds. For each fold, we randomly separate 200 instances from the training set for development. The batch size is set as 8 for all experiments. Word embeddings are initialized with the 100-dimensional GloVe (Pennington et al., 2014) vectors, pretrained on 6 billion words from Wikipedia and web text. The edge label embeddings are 3-dimensional and randomly 2 The dataset is available at http://hanover.azurewebsites.net. 3 The released data has been separated into 5 portions, and we follow the exact split. 2230 Model Quirk and Poon (2017) Peng et al. (2017) - EMBED Peng et al. (2017) - FULL + multi-task Bidir DAG LSTM GS GLSTM Figure 3: Dev accuracies against transition steps for the graph state LSTM model. initialized. Pretrained word embeddings are not updated during training. The dimension of hidden vectors in LSTM units"
D18-1246,P13-1147,0,0.0141763,"ond to gefitinib treatment. Introduction As a central task in natural language processing, relation extraction has been investigated on news, web text and biomedical domains. It has been shown to be useful for detecting explicit facts, such as cause-effect (Hendrickx et al., 2009), and predicting the effectiveness of a medicine on a cancer caused by mutation of a certain gene in the biomedical domain (Quirk and Poon, 2017; Peng et al., 2017). While most existing work extracts relations within a sentence (Zelenko et al., 2003; Palmer et al., 2005; Zhao and Grishman, 2005; Jiang and Zhai, 2007; Plank and Moschitti, 2013; Li and Ji, 2014; Gormley et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2017), the task of cross-sentence relation extraction has received increasing attention (Gerber and Chai, 2010; Yoshikawa et al., 2011). Recently, Peng ⇤ Equal contribution et al. (2017) extend cross-sentence relation extraction by further detecting relations among several entity mentions (n-ary relation). Table 1 shows an example, which conveys the fact that cancers caused by the 858E mutation on EGFR gene can respond to the gefitinib medicine. The three entity mentions form a ternary relation yet appear in distinct"
D18-1246,E17-1110,0,0.142755,"ation by allowing more parallelization. On a standard benchmark, our model shows the best result in the literature. 1 Table 1: An example showing that tumors with L858E mutation in EGFR gene respond to gefitinib treatment. Introduction As a central task in natural language processing, relation extraction has been investigated on news, web text and biomedical domains. It has been shown to be useful for detecting explicit facts, such as cause-effect (Hendrickx et al., 2009), and predicting the effectiveness of a medicine on a cancer caused by mutation of a certain gene in the biomedical domain (Quirk and Poon, 2017; Peng et al., 2017). While most existing work extracts relations within a sentence (Zelenko et al., 2003; Palmer et al., 2005; Zhao and Grishman, 2005; Jiang and Zhai, 2007; Plank and Moschitti, 2013; Li and Ji, 2014; Gormley et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2017), the task of cross-sentence relation extraction has received increasing attention (Gerber and Chai, 2010; Yoshikawa et al., 2011). Recently, Peng ⇤ Equal contribution et al. (2017) extend cross-sentence relation extraction by further detecting relations among several entity mentions (n-ary relation). Table 1 shows"
D18-1246,P18-1150,1,0.937565,"s lost from the original subgraph. Second, using LSTMs on both DAGs, information of only ancestors and descendants can be incorporated for each word. Sibling information, which may also be important, is not included. A potential solution to the problems above is to model a graph as a whole, learning its representation without breaking it into two DAGs. Due to the existence of cycles, naive extension of tree LSTMs cannot serve this goal. Recently, graph convolutional networks (GCN) (Kipf and Welling, 2017; Marcheggiani and Titov, 2017; Bastings et al., 2017) and graph recurrent networks (GRN) (Song et al., 2018; Zhang et al., 2018) have been proposed for representing graph structures for NLP tasks. Such methods encode a given graph by hierarchically learning representations of neighboring nodes in the graphs via their connecting edges. While GCNs use CNN for information exchange, GRNs take gated recurrent steps to this end. For fair comparison with DAG LSTMs, we build a graph LSTM by extending Song et al. (2018), which strictly follow the configurations of Peng et al. (2017) such as the source of features and hyper parameter settings. In particular, the full input graph is modeled as a single state,"
D18-1246,D17-1182,1,0.86292,", relation extraction has been investigated on news, web text and biomedical domains. It has been shown to be useful for detecting explicit facts, such as cause-effect (Hendrickx et al., 2009), and predicting the effectiveness of a medicine on a cancer caused by mutation of a certain gene in the biomedical domain (Quirk and Poon, 2017; Peng et al., 2017). While most existing work extracts relations within a sentence (Zelenko et al., 2003; Palmer et al., 2005; Zhao and Grishman, 2005; Jiang and Zhai, 2007; Plank and Moschitti, 2013; Li and Ji, 2014; Gormley et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2017), the task of cross-sentence relation extraction has received increasing attention (Gerber and Chai, 2010; Yoshikawa et al., 2011). Recently, Peng ⇤ Equal contribution et al. (2017) extend cross-sentence relation extraction by further detecting relations among several entity mentions (n-ary relation). Table 1 shows an example, which conveys the fact that cancers caused by the 858E mutation on EGFR gene can respond to the gefitinib medicine. The three entity mentions form a ternary relation yet appear in distinct sentences. Peng et al. (2017) proposed a graph-structured LSTM for n-ary relation"
D18-1246,P18-1030,1,0.910776,"ginal subgraph. Second, using LSTMs on both DAGs, information of only ancestors and descendants can be incorporated for each word. Sibling information, which may also be important, is not included. A potential solution to the problems above is to model a graph as a whole, learning its representation without breaking it into two DAGs. Due to the existence of cycles, naive extension of tree LSTMs cannot serve this goal. Recently, graph convolutional networks (GCN) (Kipf and Welling, 2017; Marcheggiani and Titov, 2017; Bastings et al., 2017) and graph recurrent networks (GRN) (Song et al., 2018; Zhang et al., 2018) have been proposed for representing graph structures for NLP tasks. Such methods encode a given graph by hierarchically learning representations of neighboring nodes in the graphs via their connecting edges. While GCNs use CNN for information exchange, GRNs take gated recurrent steps to this end. For fair comparison with DAG LSTMs, we build a graph LSTM by extending Song et al. (2018), which strictly follow the configurations of Peng et al. (2017) such as the source of features and hyper parameter settings. In particular, the full input graph is modeled as a single state, with words in the gr"
D18-1246,P05-1052,0,0.0482608,"at tumors with L858E mutation in EGFR gene respond to gefitinib treatment. Introduction As a central task in natural language processing, relation extraction has been investigated on news, web text and biomedical domains. It has been shown to be useful for detecting explicit facts, such as cause-effect (Hendrickx et al., 2009), and predicting the effectiveness of a medicine on a cancer caused by mutation of a certain gene in the biomedical domain (Quirk and Poon, 2017; Peng et al., 2017). While most existing work extracts relations within a sentence (Zelenko et al., 2003; Palmer et al., 2005; Zhao and Grishman, 2005; Jiang and Zhai, 2007; Plank and Moschitti, 2013; Li and Ji, 2014; Gormley et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2017), the task of cross-sentence relation extraction has received increasing attention (Gerber and Chai, 2010; Yoshikawa et al., 2011). Recently, Peng ⇤ Equal contribution et al. (2017) extend cross-sentence relation extraction by further detecting relations among several entity mentions (n-ary relation). Table 1 shows an example, which conveys the fact that cancers caused by the 858E mutation on EGFR gene can respond to the gefitinib medicine. The three entity mentio"
D18-1246,R11-1004,0,0.0888552,"instances (“Cross”). GS GLSTM 85 80 T ERNARY 51.7 71.1* Related Work N -ary relation extraction N -ary relation extractions can be traced back to MUC-7 (Chinchor, 1998), which focuses on entity-attribution relations. It has also been studied in biomedical domain (McDonald et al., 2005), but only the instances within a single sentence are considered. Previous work on cross-sentence relation extraction relies on either explicit co-reference annotation (Gerber and Chai, 2010; Yoshikawa et al., 2011), or the assumption that the whole document refers to a single coherent event (Wick et al., 2006; Swampillai and Stevenson, 2011). Both simplify the problem and reduce the need for learning better contextual representation of entity mentions. A notable exception is Quirk and Poon (2017), who adopt distant supervision and integrated contextual evidence of diverse types without relying on these assumptions. However, they only study binary relations. We follow Peng et al. (2017) by studying ternary cross-sentence relations. Graph encoder Liang et al. (2016) build a graph LSTM model for semantic object parsing, which aims to segment objects within an image into more fine-grained, semantically meaningful parts. The nodes of"
D19-1020,D17-1209,0,0.142049,"endency forests is relatively rarely studied, with one exception being Tu et al. (2010), who use dependency forests to enhance long-range word-to-word dependencies for statistical machine translation. To our knowledge, we are the first to study the usefulness of dependency forests for relation extraction under a strong neural framework. Graph neural network Graph neural networks (GNNs) have been successful in encoding dependency trees for downstream tasks, such as semantic role labeling (Marcheggiani and Titov, 2017), semantic parsing (Xu et al., 2018), machine translation (Song et al., 2019; Bastings et al., 2017), relation extraction (Song et al., 2018b) and sentence ordering (Yin et al., 2019). In particular, Song et al. (2018b) showed that GNNs are more effective than DAG networks (Peng et al., 2017) for modeling syntactic trees in relation extraction, which cause loss of important structural information. We are the first to exploit GNNs for encoding search spaces in the form of dependency forests. 3 Task Formally, the input to our task is a sentence s = w1 , w2 , . . . , wN , where N is the number of words in the sentence and wi represents the i-th input word. s is annotated with boundary informati"
D19-1020,P18-1026,0,0.0260887,"nce. First, a dependency parser is used to label the syntactic structure of the input. Here our baseline system takes the standard approach, using the 1-best parser output tree DT as features. In contrast, our proposed model uses the most confident parser forest DF as features. Given DT or DF , the second step is to encode both s and DT /DF using a neural network, before making a prediction. We make use of the same graph neural network encoder structure to represent dependency syntax information for both the baseline and our model. In particular, a graph recurrent neural network architecture (Beck et al., 2018; Song et al., 2018a; Zhang et al., 2018a) is used, which has been shown effective in encoding graph structures (Song et al., 2019), giving competitive results with alternative graph networks such as graph convolutional neural networks (Marcheggiani and Titov, 2017; Bastings et al., 2017). 4 Baseline: D EP T REE As shown in Figure 2, our baseline model stacks a bidirectional LSTM layer to encode an input sentence w1 , . . . , wN with a graph recurrent network 209 In order to capture non-local interactions between words, the GRN layer adopts a message passing framework that performs iterative i"
D19-1020,H05-1091,0,0.867277,"the corresponding author nmod comp comp amod comp ... observed ... interaction of orexin receptor antagonist almorexant (a) nmod comp comp nmod comp amod comp ... observed ... interaction of orexin receptor antagonist almorexant (b) Figure 1: (a) 1-best dependency tree and (b) dependency forest for a medical-domain sentence, where edge label “comp” represents “compound”. Associated mentions are in different colors. Some irrelevant words and edges are omitted for simplicity. Previous work has shown that dependency syntax is important for guiding relation extraction (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Liu et al., 2015; Gormley et al., 2015; Xu et al., 2015a,b; Miwa and Bansal, 2016; Zhang et al., 2018b), especially in biological and medical domains (Quirk and Poon, 2017; Peng et al., 2017; Song et al., 2018b). Compared with sequential surface-level structures, such as POS tags, dependency trees help to model word-toword relations more easily by drawing direct connections between distant words that are syntactically correlated. Take the phrase “effect on the medicine” for example; “effect” and “medicine” are directly connected in a dependency tree, regardless of how many modifiers are adde"
D19-1020,W11-2905,0,0.0727527,"Missing"
D19-1020,W11-0216,0,0.0751446,"Missing"
D19-1020,C96-1058,0,0.0517257,"w recall given an imperfect parser. We investigate two algorithms to generate high-quality forests by judging “quality” from different perspectives: one focusing on arcs, and the other focusing on trees. E DGEWISE This algorithm focuses on the local relation of each individual edge and uses parser probabilities as confidence scores to assess edge qualities. Starting from the whole parser search space, it keeps all the edges with scores greater than a threshold . The time complexity is O(N 2 ), where N represents the sentence length.1 KB EST E ISNER This algorithm extends the Eisner algorithm (Eisner, 1996) with cube pruning (Huang and Chiang, 2005) for finding K highest-scored tree structures. The Eisner algorithm is a standard method for decoding 1-best trees for graph-based dependency parsing. Based on bottom-up dynamic programming, it stores the 1-best subtree for each span and takes O(N 3 ) time complexity for decoding a sentence of N words. KB EST E ISNER keeps a sorted list of K-best hypotheses for each span. Cube pruning (Huang and Chiang, 2005) is adopted to generate the Kbest list for each larger span from the K-best lists of its sub-spans. After the bottom-up decoding, we merge the fi"
D19-1020,D15-1205,1,0.779495,"Missing"
D19-1020,P19-1024,0,0.154261,"Missing"
D19-1020,W09-2415,0,0.0207045,"Missing"
D19-1020,W05-1506,0,0.102938,"r. We investigate two algorithms to generate high-quality forests by judging “quality” from different perspectives: one focusing on arcs, and the other focusing on trees. E DGEWISE This algorithm focuses on the local relation of each individual edge and uses parser probabilities as confidence scores to assess edge qualities. Starting from the whole parser search space, it keeps all the edges with scores greater than a threshold . The time complexity is O(N 2 ), where N represents the sentence length.1 KB EST E ISNER This algorithm extends the Eisner algorithm (Eisner, 1996) with cube pruning (Huang and Chiang, 2005) for finding K highest-scored tree structures. The Eisner algorithm is a standard method for decoding 1-best trees for graph-based dependency parsing. Based on bottom-up dynamic programming, it stores the 1-best subtree for each span and takes O(N 3 ) time complexity for decoding a sentence of N words. KB EST E ISNER keeps a sorted list of K-best hypotheses for each span. Cube pruning (Huang and Chiang, 2005) is adopted to generate the Kbest list for each larger span from the K-best lists of its sub-spans. After the bottom-up decoding, we merge the final K-bests by combining identical dependen"
D19-1020,D15-1137,0,0.031279,"2019) show that our method outperforms a strong baseline that uses 1-best dependency trees as features, giving the state-of-the-art accuracies in the literature. To our knowledge, we are the first to study dependency forests for medical information extraction, showing their advantages over 1-best tree structures. Our code is available at http://github.com/freesunshine/ dep-forest-re. 2 Related work Syntactic forests There have been previous studies leveraging constituent forests for machine translation (Mi et al., 2008; Ma et al., 2018; Zaremoodi and Haffari, 2018), sentiment analysis (Le and Zuidema, 2015) and text generation (Lu and Ng, 2011). However, the usefulness of dependency forests is relatively rarely studied, with one exception being Tu et al. (2010), who use dependency forests to enhance long-range word-to-word dependencies for statistical machine translation. To our knowledge, we are the first to study the usefulness of dependency forests for relation extraction under a strong neural framework. Graph neural network Graph neural networks (GNNs) have been successful in encoding dependency trees for downstream tasks, such as semantic role labeling (Marcheggiani and Titov, 2017), semant"
D19-1020,I05-1006,0,0.184,"Missing"
D19-1020,Q17-1029,1,0.895317,"Missing"
D19-1020,P15-2047,0,0.103796,"Missing"
D19-1020,D11-1149,0,0.0191059,"ms a strong baseline that uses 1-best dependency trees as features, giving the state-of-the-art accuracies in the literature. To our knowledge, we are the first to study dependency forests for medical information extraction, showing their advantages over 1-best tree structures. Our code is available at http://github.com/freesunshine/ dep-forest-re. 2 Related work Syntactic forests There have been previous studies leveraging constituent forests for machine translation (Mi et al., 2008; Ma et al., 2018; Zaremoodi and Haffari, 2018), sentiment analysis (Le and Zuidema, 2015) and text generation (Lu and Ng, 2011). However, the usefulness of dependency forests is relatively rarely studied, with one exception being Tu et al. (2010), who use dependency forests to enhance long-range word-to-word dependencies for statistical machine translation. To our knowledge, we are the first to study the usefulness of dependency forests for relation extraction under a strong neural framework. Graph neural network Graph neural networks (GNNs) have been successful in encoding dependency trees for downstream tasks, such as semantic role labeling (Marcheggiani and Titov, 2017), semantic parsing (Xu et al., 2018), machine"
D19-1020,P18-1249,0,0.0333378,"Missing"
D19-1020,D17-1159,0,0.500712,"iment analysis (Le and Zuidema, 2015) and text generation (Lu and Ng, 2011). However, the usefulness of dependency forests is relatively rarely studied, with one exception being Tu et al. (2010), who use dependency forests to enhance long-range word-to-word dependencies for statistical machine translation. To our knowledge, we are the first to study the usefulness of dependency forests for relation extraction under a strong neural framework. Graph neural network Graph neural networks (GNNs) have been successful in encoding dependency trees for downstream tasks, such as semantic role labeling (Marcheggiani and Titov, 2017), semantic parsing (Xu et al., 2018), machine translation (Song et al., 2019; Bastings et al., 2017), relation extraction (Song et al., 2018b) and sentence ordering (Yin et al., 2019). In particular, Song et al. (2018b) showed that GNNs are more effective than DAG networks (Peng et al., 2017) for modeling syntactic trees in relation extraction, which cause loss of important structural information. We are the first to exploit GNNs for encoding search spaces in the form of dependency forests. 3 Task Formally, the input to our task is a sentence s = w1 , w2 , . . . , wN , where N is the number of"
D19-1020,J93-2004,0,0.064101,"able 1: Statistics on forests generated with various (upper half) and K (lower half) on the development set. • D EP T REE: Our baseline using 1-best dependency trees, as shown in Section 4. 7.4 • E DGEWISE PS and E DGEWISE: Our models using the forests generated by our E DGEWISE algorithm with or without parser scores. • KB EST E ISNER PS and KB EST E ISNER: Our model using the forests generated by our KB EST E ISNER algorithm with or without parser scores, respectively. 7.3 Settings We take a state-of-the-art deep biaffine parser (Dozat and Manning, 2017), trained on the Penn Treebank (PTB) (Marcus and Marcinkiewicz, 1993) converted to Universal Dependency, to obtain 1-best trees and full search spaces for generating forests. Using standard PTB data split (02–21 for training, 22 for development and 23 for testing), it gives UAS and LAS scores of 95.7 and 94.6, respectively. For the other hyper-parameters, word embeddings are initialized with the 200-dimensional BioASQ vectors5 , pretrained on 10M abstracts of biomedical articles, and are fixed during training. The dimension of hidden vectors in Bi-LSTM is set to 200, and the number of message passing steps T is set to 2 based on Zhang et al. (2018b). We use Ada"
D19-1020,P08-2026,0,0.0861827,"Missing"
D19-1020,W16-3009,0,0.0314234,"Missing"
D19-1020,C10-1123,0,0.0261185,"ature. To our knowledge, we are the first to study dependency forests for medical information extraction, showing their advantages over 1-best tree structures. Our code is available at http://github.com/freesunshine/ dep-forest-re. 2 Related work Syntactic forests There have been previous studies leveraging constituent forests for machine translation (Mi et al., 2008; Ma et al., 2018; Zaremoodi and Haffari, 2018), sentiment analysis (Le and Zuidema, 2015) and text generation (Lu and Ng, 2011). However, the usefulness of dependency forests is relatively rarely studied, with one exception being Tu et al. (2010), who use dependency forests to enhance long-range word-to-word dependencies for statistical machine translation. To our knowledge, we are the first to study the usefulness of dependency forests for relation extraction under a strong neural framework. Graph neural network Graph neural networks (GNNs) have been successful in encoding dependency trees for downstream tasks, such as semantic role labeling (Marcheggiani and Titov, 2017), semantic parsing (Xu et al., 2018), machine translation (Song et al., 2019; Bastings et al., 2017), relation extraction (Song et al., 2018b) and sentence ordering"
D19-1020,P08-1023,0,0.0537829,"t al., 2017) and a recent dataset focused on phenotype-gene relations (PGR) (Sousa et al., 2019) show that our method outperforms a strong baseline that uses 1-best dependency trees as features, giving the state-of-the-art accuracies in the literature. To our knowledge, we are the first to study dependency forests for medical information extraction, showing their advantages over 1-best tree structures. Our code is available at http://github.com/freesunshine/ dep-forest-re. 2 Related work Syntactic forests There have been previous studies leveraging constituent forests for machine translation (Mi et al., 2008; Ma et al., 2018; Zaremoodi and Haffari, 2018), sentiment analysis (Le and Zuidema, 2015) and text generation (Lu and Ng, 2011). However, the usefulness of dependency forests is relatively rarely studied, with one exception being Tu et al. (2010), who use dependency forests to enhance long-range word-to-word dependencies for statistical machine translation. To our knowledge, we are the first to study the usefulness of dependency forests for relation extraction under a strong neural framework. Graph neural network Graph neural networks (GNNs) have been successful in encoding dependency trees f"
D19-1020,N18-1080,0,0.246962,"present an edge for simplicity, and p✏ is the parser probability for edge ✏. The edge probabilities are not adjusted during end-task training. 6 Training Relation loss Given a set of training instances, each containing a sentence s with two target mentions ⇠ and ⇣, and a dependency structure D (tree or forest), we train our models with a crossentropy loss between the gold-standard relations r and model distribution: lR = log p(r|s, ⇠, ⇣, D; ✓), (13) where ✓ represents the model parameters. Using additional NER loss For training on BioCreative VI CPR, we follow previous work (Liu et al., 2017; Verga et al., 2018) to take NER loss as additional supervision, though the mention boundaries are known during testing. lN ER = N 1 X log p(tn |s, D; ✓), N (14) n=1 where tn is the gold NE tag of wn with the “BIO” scheme. Both losses are conditionally independent given the deep features produced by our Experiments We conduct experiments on two medical benchmarks to test the usefulness of dependency forest. 7.1 Data BioCreative VI CPR (Krallinger et al., 2017) This task2 focuses on the relations between chemical compounds (such as drugs) and proteins (such as genes). The full corpus contains 1020, 612 and 800 ext"
D19-1020,P16-1105,0,0.0839479,"Missing"
D19-1020,Q17-1008,0,0.357248,"on. To our knowledge, we are the first to study the usefulness of dependency forests for relation extraction under a strong neural framework. Graph neural network Graph neural networks (GNNs) have been successful in encoding dependency trees for downstream tasks, such as semantic role labeling (Marcheggiani and Titov, 2017), semantic parsing (Xu et al., 2018), machine translation (Song et al., 2019; Bastings et al., 2017), relation extraction (Song et al., 2018b) and sentence ordering (Yin et al., 2019). In particular, Song et al. (2018b) showed that GNNs are more effective than DAG networks (Peng et al., 2017) for modeling syntactic trees in relation extraction, which cause loss of important structural information. We are the first to exploit GNNs for encoding search spaces in the form of dependency forests. 3 Task Formally, the input to our task is a sentence s = w1 , w2 , . . . , wN , where N is the number of words in the sentence and wi represents the i-th input word. s is annotated with boundary information (⇠1 : ⇠2 and ⇣1 : ⇣2 ) of target entity mentions (⇠ and ⇣). We focus on the classic binary relation extraction setting (Quirk and Poon, 2017), where the number of associated mentions is two."
D19-1020,D15-1062,0,0.231787,"Missing"
D19-1020,E17-1110,0,0.0869635,"wed that GNNs are more effective than DAG networks (Peng et al., 2017) for modeling syntactic trees in relation extraction, which cause loss of important structural information. We are the first to exploit GNNs for encoding search spaces in the form of dependency forests. 3 Task Formally, the input to our task is a sentence s = w1 , w2 , . . . , wN , where N is the number of words in the sentence and wi represents the i-th input word. s is annotated with boundary information (⇠1 : ⇠2 and ⇣1 : ⇣2 ) of target entity mentions (⇠ and ⇣). We focus on the classic binary relation extraction setting (Quirk and Poon, 2017), where the number of associated mentions is two. The output is a relation from a predefined relation set R = (r1 , . . . , rM , None), where “None” means that no relation holds for the entities. Two steps are taken for predicting the correct relation given an input sentence. First, a dependency parser is used to label the syntactic structure of the input. Here our baseline system takes the standard approach, using the 1-best parser output tree DT as features. In contrast, our proposed model uses the most confident parser forest DF as features. Given DT or DF , the second step is to encode bot"
D19-1020,W08-0504,0,0.0520198,"Missing"
D19-1020,C10-2133,0,0.0320316,"from parsing noise. Results on two biomedical benchmarks show that our method outperforms the standard tree-based methods, giving the state-of-the-art results in the literature. 1 Introduction The sheer amount of medical articles and their rapid growth prevent researchers from receiving comprehensive literature knowledge by direct reading. This can hamper both medical research and clinical diagnosis. NLP techniques have been used for automating the knowledge extraction process from the medical literature (Friedman et al., 2001; Yu and Agichtein, 2003; Hirschman et al., 2005; Xu et al., 2010; Sondhi et al., 2010; Abacha and Zweigenbaum, 2011). Along this line of work, a long-standing task is relation extraction, which mines factual knowledge from free text by labeling relations between entity mentions. As shown in Figure 1, the sub-clause “previously observed cytochrome P450 3A4 ( CYP3A4 ) interaction of the dual orexin receptor antagonist almorexant” contains two entities, namely “orexin receptor” and “almorexant”. There is an “adversary” relation between these two entities, denoted as“CPR:6”. ⇤ Yue Zhang is the corresponding author nmod comp comp amod comp ... observed ... interaction of orexin rec"
D19-1020,Q19-1002,1,0.928194,"e usefulness of dependency forests is relatively rarely studied, with one exception being Tu et al. (2010), who use dependency forests to enhance long-range word-to-word dependencies for statistical machine translation. To our knowledge, we are the first to study the usefulness of dependency forests for relation extraction under a strong neural framework. Graph neural network Graph neural networks (GNNs) have been successful in encoding dependency trees for downstream tasks, such as semantic role labeling (Marcheggiani and Titov, 2017), semantic parsing (Xu et al., 2018), machine translation (Song et al., 2019; Bastings et al., 2017), relation extraction (Song et al., 2018b) and sentence ordering (Yin et al., 2019). In particular, Song et al. (2018b) showed that GNNs are more effective than DAG networks (Peng et al., 2017) for modeling syntactic trees in relation extraction, which cause loss of important structural information. We are the first to exploit GNNs for encoding search spaces in the form of dependency forests. 3 Task Formally, the input to our task is a sentence s = w1 , w2 , . . . , wN , where N is the number of words in the sentence and wi represents the i-th input word. s is annotated"
D19-1020,P18-1150,1,0.94662,"with one exception being Tu et al. (2010), who use dependency forests to enhance long-range word-to-word dependencies for statistical machine translation. To our knowledge, we are the first to study the usefulness of dependency forests for relation extraction under a strong neural framework. Graph neural network Graph neural networks (GNNs) have been successful in encoding dependency trees for downstream tasks, such as semantic role labeling (Marcheggiani and Titov, 2017), semantic parsing (Xu et al., 2018), machine translation (Song et al., 2019; Bastings et al., 2017), relation extraction (Song et al., 2018b) and sentence ordering (Yin et al., 2019). In particular, Song et al. (2018b) showed that GNNs are more effective than DAG networks (Peng et al., 2017) for modeling syntactic trees in relation extraction, which cause loss of important structural information. We are the first to exploit GNNs for encoding search spaces in the form of dependency forests. 3 Task Formally, the input to our task is a sentence s = w1 , w2 , . . . , wN , where N is the number of words in the sentence and wi represents the i-th input word. s is annotated with boundary information (⇠1 : ⇠2 and ⇣1 : ⇣2 ) of target enti"
D19-1020,D18-1110,1,0.810915,"neration (Lu and Ng, 2011). However, the usefulness of dependency forests is relatively rarely studied, with one exception being Tu et al. (2010), who use dependency forests to enhance long-range word-to-word dependencies for statistical machine translation. To our knowledge, we are the first to study the usefulness of dependency forests for relation extraction under a strong neural framework. Graph neural network Graph neural networks (GNNs) have been successful in encoding dependency trees for downstream tasks, such as semantic role labeling (Marcheggiani and Titov, 2017), semantic parsing (Xu et al., 2018), machine translation (Song et al., 2019; Bastings et al., 2017), relation extraction (Song et al., 2018b) and sentence ordering (Yin et al., 2019). In particular, Song et al. (2018b) showed that GNNs are more effective than DAG networks (Peng et al., 2017) for modeling syntactic trees in relation extraction, which cause loss of important structural information. We are the first to exploit GNNs for encoding search spaces in the form of dependency forests. 3 Task Formally, the input to our task is a sentence s = w1 , w2 , . . . , wN , where N is the number of words in the sentence and wi repres"
D19-1020,D15-1206,0,0.201939,"Missing"
D19-1020,C18-1120,0,0.0200441,"focused on phenotype-gene relations (PGR) (Sousa et al., 2019) show that our method outperforms a strong baseline that uses 1-best dependency trees as features, giving the state-of-the-art accuracies in the literature. To our knowledge, we are the first to study dependency forests for medical information extraction, showing their advantages over 1-best tree structures. Our code is available at http://github.com/freesunshine/ dep-forest-re. 2 Related work Syntactic forests There have been previous studies leveraging constituent forests for machine translation (Mi et al., 2008; Ma et al., 2018; Zaremoodi and Haffari, 2018), sentiment analysis (Le and Zuidema, 2015) and text generation (Lu and Ng, 2011). However, the usefulness of dependency forests is relatively rarely studied, with one exception being Tu et al. (2010), who use dependency forests to enhance long-range word-to-word dependencies for statistical machine translation. To our knowledge, we are the first to study the usefulness of dependency forests for relation extraction under a strong neural framework. Graph neural network Graph neural networks (GNNs) have been successful in encoding dependency trees for downstream tasks, such as semantic role labe"
D19-1020,D18-1246,1,0.942065,"with one exception being Tu et al. (2010), who use dependency forests to enhance long-range word-to-word dependencies for statistical machine translation. To our knowledge, we are the first to study the usefulness of dependency forests for relation extraction under a strong neural framework. Graph neural network Graph neural networks (GNNs) have been successful in encoding dependency trees for downstream tasks, such as semantic role labeling (Marcheggiani and Titov, 2017), semantic parsing (Xu et al., 2018), machine translation (Song et al., 2019; Bastings et al., 2017), relation extraction (Song et al., 2018b) and sentence ordering (Yin et al., 2019). In particular, Song et al. (2018b) showed that GNNs are more effective than DAG networks (Peng et al., 2017) for modeling syntactic trees in relation extraction, which cause loss of important structural information. We are the first to exploit GNNs for encoding search spaces in the form of dependency forests. 3 Task Formally, the input to our task is a sentence s = w1 , w2 , . . . , wN , where N is the number of words in the sentence and wi represents the i-th input word. s is annotated with boundary information (⇠1 : ⇠2 and ⇣1 : ⇣2 ) of target enti"
D19-1020,P18-1030,1,0.867698,"to label the syntactic structure of the input. Here our baseline system takes the standard approach, using the 1-best parser output tree DT as features. In contrast, our proposed model uses the most confident parser forest DF as features. Given DT or DF , the second step is to encode both s and DT /DF using a neural network, before making a prediction. We make use of the same graph neural network encoder structure to represent dependency syntax information for both the baseline and our model. In particular, a graph recurrent neural network architecture (Beck et al., 2018; Song et al., 2018a; Zhang et al., 2018a) is used, which has been shown effective in encoding graph structures (Song et al., 2019), giving competitive results with alternative graph networks such as graph convolutional neural networks (Marcheggiani and Titov, 2017; Bastings et al., 2017). 4 Baseline: D EP T REE As shown in Figure 2, our baseline model stacks a bidirectional LSTM layer to encode an input sentence w1 , . . . , wN with a graph recurrent network 209 In order to capture non-local interactions between words, the GRN layer adopts a message passing framework that performs iterative information exchange between directly con"
D19-1020,N19-1152,0,0.0439742,"Missing"
D19-1020,D18-1244,0,0.376206,"to label the syntactic structure of the input. Here our baseline system takes the standard approach, using the 1-best parser output tree DT as features. In contrast, our proposed model uses the most confident parser forest DF as features. Given DT or DF , the second step is to encode both s and DT /DF using a neural network, before making a prediction. We make use of the same graph neural network encoder structure to represent dependency syntax information for both the baseline and our model. In particular, a graph recurrent neural network architecture (Beck et al., 2018; Song et al., 2018a; Zhang et al., 2018a) is used, which has been shown effective in encoding graph structures (Song et al., 2019), giving competitive results with alternative graph networks such as graph convolutional neural networks (Marcheggiani and Titov, 2017; Bastings et al., 2017). 4 Baseline: D EP T REE As shown in Figure 2, our baseline model stacks a bidirectional LSTM layer to encode an input sentence w1 , . . . , wN with a graph recurrent network 209 In order to capture non-local interactions between words, the GRN layer adopts a message passing framework that performs iterative information exchange between directly con"
D19-1599,P17-1171,0,0.0590093,"ntation of the first token [CLS]. We also apply sof tmax over all passage scores corresponding to the same question, and train to maximize the log-likelihood of passages containing the correct answers. Denote the passage score as P r(Pi |Q, P ), then the score of an answer span from passage Pi will be P r(Pi |Q, P )Ps (as |Q, P )Pe (ae |Q, P ). 3 Experiments Datasets: We experiment on four open-domain QA datasets. (1) OpenSQuAD: question-answer pairs are from SQuAD 1.1 (Rajpurkar et al., 2016), but a QA model will find answers from the entire Wikipedia rather than the given context. Following Chen et al. (2017), we use the 2016-1221 English Wikipedia dump. 5,000 QA pairs are randomly selected from the original training set as our validation set, and the remaining QA pairs are taken as our new training set. The original development set is used as our test set. (2) TriviaQA: TriviaQA unfiltered version (Joshi et al., 2017) are used. Following Pang et al. (2019), we randomly hold out 5,000 QA pairs from the original training set as our validation set, and take the remaining pairs as our new training set. The original development set is used as our test set. (3) Quasar-T (Dhingra et al., 2017) and (4) S"
D19-1599,P18-1078,0,0.255981,"training, passages corresponding to the same question are taken as independent training instances. During inference, the BERT-RC model is applied to each passage individually to predict an answer span, and then the highest scoring span is selected as the final answer. Although this method achieves significant improvements on several datasets, there are still several unaddressed issues. First, viewing passages of the same question as independent training instances may result in incomparable answer scores across passages. Thus, globally normalizing scores over all passages of the same question (Clark and Gardner, 2018) may be helpful. Second, previous work defines passages as articles, paragraphs, or sentences. However, the question of proper granularity of passages is still underexplored. Third, passage ranker for selecting high-quality passages has been shown to be very useful in previous open-domain QA systems (Wang et al., 2018a; Lin et al., 2018; Pang et al., 2019). However, we do not know whether it is still required for BERT. Fourth, most effective QA and RC models highly rely on explicit inter-sentence matching between questions and passages (Wang and Jiang, 2017; Wang et al., 2016; Seo et al., 2017"
D19-1599,P17-1147,0,0.0600789,")Ps (as |Q, P )Pe (ae |Q, P ). 3 Experiments Datasets: We experiment on four open-domain QA datasets. (1) OpenSQuAD: question-answer pairs are from SQuAD 1.1 (Rajpurkar et al., 2016), but a QA model will find answers from the entire Wikipedia rather than the given context. Following Chen et al. (2017), we use the 2016-1221 English Wikipedia dump. 5,000 QA pairs are randomly selected from the original training set as our validation set, and the remaining QA pairs are taken as our new training set. The original development set is used as our test set. (2) TriviaQA: TriviaQA unfiltered version (Joshi et al., 2017) are used. Following Pang et al. (2019), we randomly hold out 5,000 QA pairs from the original training set as our validation set, and take the remaining pairs as our new training set. The original development set is used as our test set. (3) Quasar-T (Dhingra et al., 2017) and (4) SearchQA (Dunn et al., 2017) are leveraged with the official split. Basic Settings: If not specified, the pre-trained BERT-base model with default hyper-parameters is leveraged. ElasticSearch with BM25 algorithm is employed as our retriever for OpenSQuAD. Passages for other datasets are from the corresponding releas"
D19-1599,P18-1161,0,0.287338,"Missing"
D19-1599,P17-1018,0,0.034762,"1 28.7 - 37.5 36.6 - BERT (Large) (Nogueira et al., 2018) BERT serini (Yang et al., 2019) BERT-RC (Ours) 49.7 56.8 63.7 69.1 68.7 61.0 66.9 38.6 45.4 46.1 52.5 Multi-Passage BERT (Base) Multi-Passage BERT (Large) 51.3 51.1 59.0 59.1 65.2 65.1 70.6 70.7 62.0 63.7 67.5 69.2 51.2 53.0 59.0 60.9 Table 2: Comparison with state-of-the-art models, where the first group are models without using BERT, the second group are BERT-based models, and the last group are our multi-passage BERT models. sages with questions, aka inter-sentence matching (Wang and Jiang, 2017; Wang et al., 2016; Seo et al., 2017; Wang et al., 2017; Song et al., 2017). However, BERT model simply concatenates a passage with a question, and differentiates them by separating them with a delimiter token [SEP], and assigning different segment ids for them. Here, we aim to check whether explicit inter-sentence matching still matters for BERT. We employ a shared BERT model to encode a passage and a question individually, and a weighted sum of all BERT layers is used as the final tokenlevel representation for the question or passage, where weights for all BERT layers are trainable parameters. Then the passage and question representations are in"
D19-1599,N19-4013,0,0.23695,"Missing"
D19-1599,D16-1264,0,0.139503,"lti-passage BERT except that at the output layer it only predicts a single score for each passage based on the vector representation of the first token [CLS]. We also apply sof tmax over all passage scores corresponding to the same question, and train to maximize the log-likelihood of passages containing the correct answers. Denote the passage score as P r(Pi |Q, P ), then the score of an answer span from passage Pi will be P r(Pi |Q, P )Ps (as |Q, P )Pe (ae |Q, P ). 3 Experiments Datasets: We experiment on four open-domain QA datasets. (1) OpenSQuAD: question-answer pairs are from SQuAD 1.1 (Rajpurkar et al., 2016), but a QA model will find answers from the entire Wikipedia rather than the given context. Following Chen et al. (2017), we use the 2016-1221 English Wikipedia dump. 5,000 QA pairs are randomly selected from the original training set as our validation set, and the remaining QA pairs are taken as our new training set. The original development set is used as our test set. (2) TriviaQA: TriviaQA unfiltered version (Joshi et al., 2017) are used. Following Pang et al. (2019), we randomly hold out 5,000 QA pairs from the original training set as our validation set, and take the remaining pairs as o"
D19-6109,P13-2119,0,0.0614421,"Missing"
D19-6109,P18-1031,0,0.0347485,"a base network is trained with the source data, and then the first n layers of the 76 Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo), pages 76–83 c Hong Kong, China, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 to the complex attention mechanisms and large parameter size, it is hard to train BERT for domain adaptation using the domain-adversarial approach. Our initial experiments demonstrated the unsteadiness of this approach when applied to BERT. Unsupervised language model (LM) finetuning method (Howard and Ruder, 2018) consisting of general-domain LM pre-training and target task LM fine-tuning is effective using a AWDLSTM language model on many text classification tasks such as sentimental analysis, question classification and topic classification. However, due to the unique objective of BERT language model pre-training (masked LM and next sentence prediction) which requires multi-sentences natural language paragraphs, unsupervised fine-tuning of BERT LM does not apply to many sentence-pair classification datasets. In this work, we propose a novel domain adaptation framework, in which the idea of domainadve"
D19-6109,P07-1034,0,0.166658,"n classifier is trained via a gradient reversal layer that multiplies the gradient by a certain negative constant during the backpropagation. As the training progresses, the approach promotes the emergence of a representation that is discriminative for the main learning task and indiscriminate with respect to the shift between the domains. However, such type of models are usually hard to train since the optimization problem involves a minimization with respect to some parameters, as well as a maximization with respect to the others. Very early approaches in NLP utilized instance re-weighting (Jiang and Zhai, 2007) and target data co-training (Chen et al., 2011) to achieve domain adaptation. Recently, Denoising Autoencoders (Glorot et al., 2011), domain discrepancy regularization and domain adversarial training (Shah et al., 2019; Shen et al., 2017) have been employed to learn a domain invariant representation for neural network models. Many domain adaptation studies have focused on tasks such as sentiment analysis (Glorot et al., 2011; Shen et al., 2017) , Part-Of-Speech (POS) tagging (Ruder et al., 2017a) and paraphrase detection (Shah et al., 2019), and tested on neural network models such as multila"
D19-6109,D11-1033,0,0.202275,"Missing"
D19-6109,D16-1264,0,0.038806,"ents, in order to determine the optimal number of data points selected from the source domain, we set aside a small target domain dataset for validation. Starting from only a hundred examples, we double the training data size every time we observe a significant change in transfer performance evaluated on the validation set. 4.1 matched (in-domain) section. Similar as in SNLI, we convert the three-label classification task into a binary classification task. QNLI The Question-answering Natural Language Inference (QNLI) is a dataset converted from the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016). Although its name contains “natural language inference”, the text domain and task type of QNLI are fundamentally different from those of SNLI and MNLI. The original SQuAD dataset consists of questionparagraph pairs, where one of the sentences in the paragraph (drawn from Wikipedia) contains the answer to the corresponding question (written by an annotator). GLUE converts the task into sentence pair classification by forming a pair between each question and each sentence in the corresponding context and filtering out pairs with low lexical overlap between the question and the context sentence"
D19-6109,D15-1075,0,0.0265892,"much smaller variance in evaluation metrics compared with smaller datasets. We used the pre-processed datasets from GLUE natural language understanding benchmark (Wang et al., 2018). A summary of the dataset statistics and the details of the experiment setup are presented in Table 1. Task Category Natural Language Inference Natural Language Inference Answer Sentence Selection Paraphrase Detection Dataset SNLI MNLI QNLI Quora Train Size 510,711 392,702 108,436 363,847 Dev Size 9,831 9,815 5,732 40,430 Table 1: Summary of the datasets SNLI The Stanford Natural Language Inference (SNLI) Corpus (Bowman et al., 2015) is a collection of 570k human-written English sentence pairs supporting the task of natural language inference. Given a premise sentence and a hypothesis sentence, the task is to predict whether the premise entails the hypothesis (entailment), contradicts the hypothesis (contradiction), or neither (neutral). In order to make the label set the same across all the datasets, we convert the original three-label classification task into a binary classification task with “entailment” as the positive label, and “contradiction” and “neutral” as negative. MNLI The Multi-Genre Natural Language Inferenc"
D19-6109,D17-1038,0,0.0887119,"Missing"
D19-6109,P16-1013,0,0.067069,"Missing"
D19-6109,W18-5446,0,0.0330477,"mples are pairs of “related questions” which, although pertaining to similar topics, are not truly semantically equivalent. Due to community nature, the ground-truth labels contain some amount of noise. Datasets We tested our framework on four large public datasets across three task categories: natural language inference (SNLI and MNLI), answer sentence selection (QNLI) and paraphrase detection (Quora). Large datasets usually have a much smaller variance in evaluation metrics compared with smaller datasets. We used the pre-processed datasets from GLUE natural language understanding benchmark (Wang et al., 2018). A summary of the dataset statistics and the details of the experiment setup are presented in Table 1. Task Category Natural Language Inference Natural Language Inference Answer Sentence Selection Paraphrase Detection Dataset SNLI MNLI QNLI Quora Train Size 510,711 392,702 108,436 363,847 Dev Size 9,831 9,815 5,732 40,430 Table 1: Summary of the datasets SNLI The Stanford Natural Language Inference (SNLI) Corpus (Bowman et al., 2015) is a collection of 570k human-written English sentence pairs supporting the task of natural language inference. Given a premise sentence and a hypothesis sentenc"
I11-1140,J04-4004,0,0.012281,"ture parsing is probabilistic context-free grammar (PCFG). However, as demonstrated in Klein and Manning (2003a), PCFG estimated straightforwardly from Treebank does not perform well. The reason is that the basic PCFG has certain recognized drawbacks: its independence assumption is too strong, and it lacks of lexical conditioning (Jurafsky and Martin, 2008). To address these drawbacks, several variants of PCFG-based models have been proposed (Klein and Manning, 2003a; Matsuzaki et al., 2005; Petrov et al., 2006; Petrov and Klein, 2007). Lexicalized PCFG (LPCFG) (Collins, 1999; Charniak, 2000; Bikel, 2004) is a representative work that tries to ameliorate the deficiency of lexical conditioning. In LPCFG, non-terminals are annotated with lexical heads and the probabilities of CFG rules are estimated conditioned upon these lexical heads. Thus LPCFG becomes sensitive to lexical heads, and its performance is improved. However, the information provided by lexical heads is limited. To obtain higher parsing performance, we must seek additional information. We believe that dependency trees are good candidates because they encode grammatical relations between words and provide much more lexical conditio"
I11-1140,D08-1092,0,0.0516678,"Missing"
I11-1140,A00-2018,0,0.290074,"onstituent structure parsing is probabilistic context-free grammar (PCFG). However, as demonstrated in Klein and Manning (2003a), PCFG estimated straightforwardly from Treebank does not perform well. The reason is that the basic PCFG has certain recognized drawbacks: its independence assumption is too strong, and it lacks of lexical conditioning (Jurafsky and Martin, 2008). To address these drawbacks, several variants of PCFG-based models have been proposed (Klein and Manning, 2003a; Matsuzaki et al., 2005; Petrov et al., 2006; Petrov and Klein, 2007). Lexicalized PCFG (LPCFG) (Collins, 1999; Charniak, 2000; Bikel, 2004) is a representative work that tries to ameliorate the deficiency of lexical conditioning. In LPCFG, non-terminals are annotated with lexical heads and the probabilities of CFG rules are estimated conditioned upon these lexical heads. Thus LPCFG becomes sensitive to lexical heads, and its performance is improved. However, the information provided by lexical heads is limited. To obtain higher parsing performance, we must seek additional information. We believe that dependency trees are good candidates because they encode grammatical relations between words and provide much more le"
I11-1140,P05-1022,0,0.282414,"neficial for constituent tree evaluation. The remainder of this paper is organized as follows: Section 2 briefly reviews related work and proposes our ideas. Section 3 describes our parsing approach. Section 4 describes our parse reranking algorithms based on higher-order lexical dependencies. In Section 5, we describe our training algorithms. We discuss and analyze our experiments in Section 6. Finally, we conclude and mention future work in Section 7. 2 Related Work and Our Ideas Over the past few years, two kinds of parse reranking methods have been proposed. The first is N-best reranking (Charniak and Johnson, 2005; Collins and Koo, 2005). In this method, an existing generative parser is used to enumerate N-best parse trees for an input sentence, and then a reranking model is used to rescore the N-best lists with the help of various sorts of features. However, the N-best reranking method suffers from the limited scope of the N-best list in that potentially good alternatives may have been ruled out. The second method, called the forest reranking model, was proposed by Huang (2008). In Huang’s method, a forest, instead of an N-best list, is generated first. Then a beam search algorithm is used to generate"
I11-1140,W02-1001,0,0.0107667,"Missing"
I11-1140,P96-1025,0,0.474364,"red into sets of lexical dependency parts for evaluation. The order of a lexical dependency part can be defined according to the number of dependency arcs it contains. For example, in Figure 1, dependency is first-order, sibling and grandchild are secondorder and grand-sibling and tri-sibling are thirdorder. During the past few years, higher-order 1 lexical dependencies have been successfully used for dependency parsing (McDonald et al., 2005; McDonald and Pereira, 2006; Koo and Collins, 2010). But for constituent tree evaluation, only first-order (bigram) lexical dependencies have been used (Collins, 1996; Klein and Manning, 2003a; Collins and Koo, 2005). However, firstorder lexical dependency parts are quite limited and thus lose much of the contextual information within the dependency tree. To improve parsing performance, we propose to evaluate constituent trees with higher-order lexical dependencies. h h m dependency g h s g m sibling s grand-sibling m m h grandchild h t s m tri-sibling Figure 1. Lexical dependency types. The lowercase letters h, m, s, g are words in a sentence. In this paper, we propose a method for evaluating constituent trees using higher-order lexical dependencies withi"
I11-1140,J05-1003,0,0.274005,"for evaluation. The order of a lexical dependency part can be defined according to the number of dependency arcs it contains. For example, in Figure 1, dependency is first-order, sibling and grandchild are secondorder and grand-sibling and tri-sibling are thirdorder. During the past few years, higher-order 1 lexical dependencies have been successfully used for dependency parsing (McDonald et al., 2005; McDonald and Pereira, 2006; Koo and Collins, 2010). But for constituent tree evaluation, only first-order (bigram) lexical dependencies have been used (Collins, 1996; Klein and Manning, 2003a; Collins and Koo, 2005). However, firstorder lexical dependency parts are quite limited and thus lose much of the contextual information within the dependency tree. To improve parsing performance, we propose to evaluate constituent trees with higher-order lexical dependencies. h h m dependency g h s g m sibling s grand-sibling m m h grandchild h t s m tri-sibling Figure 1. Lexical dependency types. The lowercase letters h, m, s, g are words in a sentence. In this paper, we propose a method for evaluating constituent trees using higher-order lexical dependencies within a parse reranking framework. We evaluate our met"
I11-1140,P04-1015,0,0.0835963,"Missing"
I11-1140,N09-2064,0,0.0134145,"(a) gold-standard NN 2: α (0) ← 0 ; v ← 0; i ← 0 3: for n in 1…N do 4: for t in 1…T do NN 高 科技 项目 (b) generated by LPCFG 5: NP0,3 NP1,3 ADJP0,1 JJ 高 (high) NP1,2 NP2,3 NN 科技 (technology) (c) packed forest NN 项目 (project) Figure 4. Constituent trees and forest A forest is a compact representation of many parse trees. Figure 4(c) is a sample forest which is the compact representation of the constituent trees shown in Figures 4(a) and 4(b). To obtain forests, Huang (2008) tried to modify the Charniak parser to output forest directly. Inspired by parser combination methods (Sagae and Lavie, 2006; Fossum and Knight, 2009), we have designed a simple method of building forests starting from N-best lists. First, we convert each parse tree in an N-best list into context-free productions and label each constituent in each production with its span and syntactic category. Then these converted context-free productions are used to build the forest. For example, in Figure 4, given two candidates (Figure 4(a) and Figure 4(b)), we first convert them into context-free productions, e.g. NP0,3ADJP0,1 NP1,3, NP0,3  NP0,2 NP2,3 and so on. Then we combine these productions into the forest shown in Figure 4(c). The recombined"
I11-1140,W08-1007,0,0.0602922,"atives may have been ruled out. The second method, called the forest reranking model, was proposed by Huang (2008). In Huang’s method, a forest, instead of an N-best list, is generated first. Then a beam search algorithm is used to generate N-best sub-trees for each node in bottom-up order and the best-first sub-tree of the root node is chosen as the final parse tree. In recent years, there have been many attempts to use dependency trees for constituent parsing. All these approaches can be classified into three types. The first type is dependency-driven constituent parsing (Hall et al., 2007; Hall and Nivre, 2008). Given an input sentence, this approach first parses it into a labeled dependency tree (with complex arc labels, which makes it possible to recover the constituent tree) and then transforms the dependency tree into a constituent tree. The second approach is dependency-constrained constituent parsing (Xia and Palmer, 2001; Xia et al., 2008; Wang and Zhang, 2010; Wang and Zong, 2010). In this approach, dependency trees, once generated, are used to constrain the search space of a constituent parser. The third approach is dependency-based constituent parsing (Collins, 1996; Klein and Manning, 200"
I11-1140,W07-2444,0,0.110232,"ntially good alternatives may have been ruled out. The second method, called the forest reranking model, was proposed by Huang (2008). In Huang’s method, a forest, instead of an N-best list, is generated first. Then a beam search algorithm is used to generate N-best sub-trees for each node in bottom-up order and the best-first sub-tree of the root node is chosen as the final parse tree. In recent years, there have been many attempts to use dependency trees for constituent parsing. All these approaches can be classified into three types. The first type is dependency-driven constituent parsing (Hall et al., 2007; Hall and Nivre, 2008). Given an input sentence, this approach first parses it into a labeled dependency tree (with complex arc labels, which makes it possible to recover the constituent tree) and then transforms the dependency tree into a constituent tree. The second approach is dependency-constrained constituent parsing (Xia and Palmer, 2001; Xia et al., 2008; Wang and Zhang, 2010; Wang and Zong, 2010). In this approach, dependency trees, once generated, are used to constrain the search space of a constituent parser. The third approach is dependency-based constituent parsing (Collins, 1996;"
I11-1140,P08-1067,0,0.746438,"ver the past few years, two kinds of parse reranking methods have been proposed. The first is N-best reranking (Charniak and Johnson, 2005; Collins and Koo, 2005). In this method, an existing generative parser is used to enumerate N-best parse trees for an input sentence, and then a reranking model is used to rescore the N-best lists with the help of various sorts of features. However, the N-best reranking method suffers from the limited scope of the N-best list in that potentially good alternatives may have been ruled out. The second method, called the forest reranking model, was proposed by Huang (2008). In Huang’s method, a forest, instead of an N-best list, is generated first. Then a beam search algorithm is used to generate N-best sub-trees for each node in bottom-up order and the best-first sub-tree of the root node is chosen as the final parse tree. In recent years, there have been many attempts to use dependency trees for constituent parsing. All these approaches can be classified into three types. The first type is dependency-driven constituent parsing (Hall et al., 2007; Hall and Nivre, 2008). Given an input sentence, this approach first parses it into a labeled dependency tree (with"
I11-1140,W05-1506,0,0.0292677,"orest. For example, in Figure 4, given two candidates (Figure 4(a) and Figure 4(b)), we first convert them into context-free productions, e.g. NP0,3ADJP0,1 NP1,3, NP0,3  NP0,2 NP2,3 and so on. Then we combine these productions into the forest shown in Figure 4(c). The recombined forest probably contains some parse trees that are not included in the N-best list, as will be shown in sub-section 6.1. Our algorithm for forest reranking is similar to Algorithm 1. The only difference is that there may be more than one hyperedge for each node in a forest. So we make use of a beam search algorithm (Huang and Chiang, 2005) and store Nbest sub-trees for each internal node. Finally, we choose the best-first sub-tree of the root node as the result. 5 α (i +1) ← update α (i ) according to ( xt , ct ) 6: v ← v + α (i +1) 7: i←i + 1 8: α ← v/(N*T) 9: return α NP0,2 Training Algorithm The training task is to tune the parameter weights α in Eq. (1) using the training examples as evidence. We employ the online-learning algorithm shown in Algorithm 2 because it has been proven  initial weights  N iterations  T training instances  averaging weights to be effective and efficient in many studies (Collins, 2002; Collins"
I11-1140,D09-1087,0,0.0656089,"Missing"
I11-1140,P03-1054,0,0.242069,"f lexical dependency parts for evaluation. The order of a lexical dependency part can be defined according to the number of dependency arcs it contains. For example, in Figure 1, dependency is first-order, sibling and grandchild are secondorder and grand-sibling and tri-sibling are thirdorder. During the past few years, higher-order 1 lexical dependencies have been successfully used for dependency parsing (McDonald et al., 2005; McDonald and Pereira, 2006; Koo and Collins, 2010). But for constituent tree evaluation, only first-order (bigram) lexical dependencies have been used (Collins, 1996; Klein and Manning, 2003a; Collins and Koo, 2005). However, firstorder lexical dependency parts are quite limited and thus lose much of the contextual information within the dependency tree. To improve parsing performance, we propose to evaluate constituent trees with higher-order lexical dependencies. h h m dependency g h s g m sibling s grand-sibling m m h grandchild h t s m tri-sibling Figure 1. Lexical dependency types. The lowercase letters h, m, s, g are words in a sentence. In this paper, we propose a method for evaluating constituent trees using higher-order lexical dependencies within a parse reranking frame"
I11-1140,H01-1014,0,0.0393784,"e root node is chosen as the final parse tree. In recent years, there have been many attempts to use dependency trees for constituent parsing. All these approaches can be classified into three types. The first type is dependency-driven constituent parsing (Hall et al., 2007; Hall and Nivre, 2008). Given an input sentence, this approach first parses it into a labeled dependency tree (with complex arc labels, which makes it possible to recover the constituent tree) and then transforms the dependency tree into a constituent tree. The second approach is dependency-constrained constituent parsing (Xia and Palmer, 2001; Xia et al., 2008; Wang and Zhang, 2010; Wang and Zong, 2010). In this approach, dependency trees, once generated, are used to constrain the search space of a constituent parser. The third approach is dependency-based constituent parsing (Collins, 1996; Klein and Manning, 2003b). In this approach, the constituent tree is evaluated with the help of its corresponding lexical dependencies. All three existing approaches have certain limitations. In the first approach, the dependencydriven constituent parser is not constrained by the Treebank grammar, so a constituent tree transformed from its cor"
I11-1140,W03-3023,0,0.118149,"rocedure EvalSubTree ( CP ) (b) Lexicalized constituent tree (a) Constituent tree fragment B:A:E B:A:D B:A:C w0 w1 F:D:H F:D:G w2 w3 w4 w5 (c) Labeled dependency tree  Assume the constituent is P → N1  N n 8: Find the head-child N h for P 9: WP ← WN h Figure 2. Representation of constituent tree with labeled dependency tree with associated dependency trees. Our method includes the following two steps: Step 1: Lexicalize the constituent tree, i.e. annotate each node in the constituent tree with its head-word. First, find the head-child of each nonterminal node using a head percolation table (Yamada and Matsumoto, 2003). For example, in Figure 2(a), node B is identified as the head-child of rule A → B C D E. Then the head-words propagate up through the leaf nodes and each parent receives its head-word from its head-child. For example, in Figure 2(b), w 0 is propagated up from node B to A. According to this procedure, we can get the lexicalized constituent tree (shown in Figure 2(b)) for the constituent fragment shown in Figure 2(a). Step 2: Transform the lexicalized tree into a labeled dependency tree. First, let the head-word of each non-head-child depend on the head-word of the head-child for each rule. Fo"
I11-1140,D09-1161,0,0.0854744,"Missing"
I11-1140,zhang-etal-2004-interpreting,0,0.086996,"Missing"
I11-1140,P10-1001,0,0.0335465,"ons between words and provide much more lexical conditioning than lexical heads for PCFG. Dependency trees are usually factored into sets of lexical dependency parts for evaluation. The order of a lexical dependency part can be defined according to the number of dependency arcs it contains. For example, in Figure 1, dependency is first-order, sibling and grandchild are secondorder and grand-sibling and tri-sibling are thirdorder. During the past few years, higher-order 1 lexical dependencies have been successfully used for dependency parsing (McDonald et al., 2005; McDonald and Pereira, 2006; Koo and Collins, 2010). But for constituent tree evaluation, only first-order (bigram) lexical dependencies have been used (Collins, 1996; Klein and Manning, 2003a; Collins and Koo, 2005). However, firstorder lexical dependency parts are quite limited and thus lose much of the contextual information within the dependency tree. To improve parsing performance, we propose to evaluate constituent trees with higher-order lexical dependencies. h h m dependency g h s g m sibling s grand-sibling m m h grandchild h t s m tri-sibling Figure 1. Lexical dependency types. The lowercase letters h, m, s, g are words in a sentence"
I11-1140,E06-1011,0,0.0724329,"ey encode grammatical relations between words and provide much more lexical conditioning than lexical heads for PCFG. Dependency trees are usually factored into sets of lexical dependency parts for evaluation. The order of a lexical dependency part can be defined according to the number of dependency arcs it contains. For example, in Figure 1, dependency is first-order, sibling and grandchild are secondorder and grand-sibling and tri-sibling are thirdorder. During the past few years, higher-order 1 lexical dependencies have been successfully used for dependency parsing (McDonald et al., 2005; McDonald and Pereira, 2006; Koo and Collins, 2010). But for constituent tree evaluation, only first-order (bigram) lexical dependencies have been used (Collins, 1996; Klein and Manning, 2003a; Collins and Koo, 2005). However, firstorder lexical dependency parts are quite limited and thus lose much of the contextual information within the dependency tree. To improve parsing performance, we propose to evaluate constituent trees with higher-order lexical dependencies. h h m dependency g h s g m sibling s grand-sibling m m h grandchild h t s m tri-sibling Figure 1. Lexical dependency types. The lowercase letters h, m, s, g"
I11-1140,P09-1006,0,0.0371333,"Missing"
I11-1140,P06-1055,0,0.167591,"een significantly improved as well. 1 Introduction The most commonly used grammar for constituent structure parsing is probabilistic context-free grammar (PCFG). However, as demonstrated in Klein and Manning (2003a), PCFG estimated straightforwardly from Treebank does not perform well. The reason is that the basic PCFG has certain recognized drawbacks: its independence assumption is too strong, and it lacks of lexical conditioning (Jurafsky and Martin, 2008). To address these drawbacks, several variants of PCFG-based models have been proposed (Klein and Manning, 2003a; Matsuzaki et al., 2005; Petrov et al., 2006; Petrov and Klein, 2007). Lexicalized PCFG (LPCFG) (Collins, 1999; Charniak, 2000; Bikel, 2004) is a representative work that tries to ameliorate the deficiency of lexical conditioning. In LPCFG, non-terminals are annotated with lexical heads and the probabilities of CFG rules are estimated conditioned upon these lexical heads. Thus LPCFG becomes sensitive to lexical heads, and its performance is improved. However, the information provided by lexical heads is limited. To obtain higher parsing performance, we must seek additional information. We believe that dependency trees are good candidate"
I11-1140,N07-1051,0,0.032417,"roved as well. 1 Introduction The most commonly used grammar for constituent structure parsing is probabilistic context-free grammar (PCFG). However, as demonstrated in Klein and Manning (2003a), PCFG estimated straightforwardly from Treebank does not perform well. The reason is that the basic PCFG has certain recognized drawbacks: its independence assumption is too strong, and it lacks of lexical conditioning (Jurafsky and Martin, 2008). To address these drawbacks, several variants of PCFG-based models have been proposed (Klein and Manning, 2003a; Matsuzaki et al., 2005; Petrov et al., 2006; Petrov and Klein, 2007). Lexicalized PCFG (LPCFG) (Collins, 1999; Charniak, 2000; Bikel, 2004) is a representative work that tries to ameliorate the deficiency of lexical conditioning. In LPCFG, non-terminals are annotated with lexical heads and the probabilities of CFG rules are estimated conditioned upon these lexical heads. Thus LPCFG becomes sensitive to lexical heads, and its performance is improved. However, the information provided by lexical heads is limited. To obtain higher parsing performance, we must seek additional information. We believe that dependency trees are good candidates because they encode gra"
I11-1140,N06-2033,0,0.024041,"T NP NP NN JJ 高 科技 项目 (a) gold-standard NN 2: α (0) ← 0 ; v ← 0; i ← 0 3: for n in 1…N do 4: for t in 1…T do NN 高 科技 项目 (b) generated by LPCFG 5: NP0,3 NP1,3 ADJP0,1 JJ 高 (high) NP1,2 NP2,3 NN 科技 (technology) (c) packed forest NN 项目 (project) Figure 4. Constituent trees and forest A forest is a compact representation of many parse trees. Figure 4(c) is a sample forest which is the compact representation of the constituent trees shown in Figures 4(a) and 4(b). To obtain forests, Huang (2008) tried to modify the Charniak parser to output forest directly. Inspired by parser combination methods (Sagae and Lavie, 2006; Fossum and Knight, 2009), we have designed a simple method of building forests starting from N-best lists. First, we convert each parse tree in an N-best list into context-free productions and label each constituent in each production with its span and syntactic category. Then these converted context-free productions are used to build the forest. For example, in Figure 4, given two candidates (Figure 4(a) and Figure 4(b)), we first convert them into context-free productions, e.g. NP0,3ADJP0,1 NP1,3, NP0,3  NP0,2 NP2,3 and so on. Then we combine these productions into the forest shown in Fi"
I11-1140,wang-zhang-2010-hybrid,0,0.0182648,"tree. In recent years, there have been many attempts to use dependency trees for constituent parsing. All these approaches can be classified into three types. The first type is dependency-driven constituent parsing (Hall et al., 2007; Hall and Nivre, 2008). Given an input sentence, this approach first parses it into a labeled dependency tree (with complex arc labels, which makes it possible to recover the constituent tree) and then transforms the dependency tree into a constituent tree. The second approach is dependency-constrained constituent parsing (Xia and Palmer, 2001; Xia et al., 2008; Wang and Zhang, 2010; Wang and Zong, 2010). In this approach, dependency trees, once generated, are used to constrain the search space of a constituent parser. The third approach is dependency-based constituent parsing (Collins, 1996; Klein and Manning, 2003b). In this approach, the constituent tree is evaluated with the help of its corresponding lexical dependencies. All three existing approaches have certain limitations. In the first approach, the dependencydriven constituent parser is not constrained by the Treebank grammar, so a constituent tree transformed from its corresponding dependency tree may contain c"
I11-1140,C10-2148,1,0.855811,", there have been many attempts to use dependency trees for constituent parsing. All these approaches can be classified into three types. The first type is dependency-driven constituent parsing (Hall et al., 2007; Hall and Nivre, 2008). Given an input sentence, this approach first parses it into a labeled dependency tree (with complex arc labels, which makes it possible to recover the constituent tree) and then transforms the dependency tree into a constituent tree. The second approach is dependency-constrained constituent parsing (Xia and Palmer, 2001; Xia et al., 2008; Wang and Zhang, 2010; Wang and Zong, 2010). In this approach, dependency trees, once generated, are used to constrain the search space of a constituent parser. The third approach is dependency-based constituent parsing (Collins, 1996; Klein and Manning, 2003b). In this approach, the constituent tree is evaluated with the help of its corresponding lexical dependencies. All three existing approaches have certain limitations. In the first approach, the dependencydriven constituent parser is not constrained by the Treebank grammar, so a constituent tree transformed from its corresponding dependency tree may contain context-free production"
I11-1140,J03-4003,0,\N,Missing
K16-1004,W15-1509,0,0.0303111,"ns: apple {a, b, c} and orange {d, e, f} with a fruit type intention, or what-question {a, d}, when-question {b, e}, and yes/no-question cluster {c, f} with a question type intension. To address the lexical sparity issue, one direction is to enrich text representations by extracting features and relations from Wikipedia (Banerjee et al., 2007) or an ontology (Fodeh et al., 2011). But this approach requires the annotated knowledge, which is also language dependent. So the other direction, which directly encode texts into distributed vectors with neural networks (Hinton and Salakhutdinov, 2006; Xu et al., 2015), becomes more interesting. To tackle the second problem, semi-supervised approaches (e.g. (Bilenko et al., 2004; Davidson and Basu, 2007; Bair, 2013)) have gained significant popularity in the past decades. Our question is can we have a unified model to integrate neural networks into the semi-supervised framework? In this paper, we propose a unified framework for the short text clustering task. We employ a Introduction Text clustering is a fundamental problem in text mining and information retrieval. Its task is to group similar texts together such that texts within a cluster are more similar"
K16-1004,D14-1181,0,0.00666017,"Representation Learning for Short Texts We represent each word with a dense vector w, so that a short text s is first represented as a matrix S = [w1 , ..., w|s |], which is a concatenation of all vectors of w in s, |s |is the length of s. Then we design two different types of neural networks to ingest the word vector sequence S: the convolutional neural networks (CNN) and the long shortterm memory (LSTM). More formally, we define the presentation function as x = f (s), where x represents the vector of the text s. We test two encoding functions (CNN and LSTM) in our experiments. Inspired from Kim (2014), our CNN model views the sequence of word vectors as a matrix, and applies two sequential operations: convolution and max-pooling. Then, a fully connected layer is 32 mean of the hidden states over the entire sentence is taken as the final representation vector. 3.2 3 The classical k-means algorithm only uses unlabeled data, and solves the clustering problem under the unsupervised learning framework. As already mentioned, the clustering results may not be consistent to our intention. In order to acquire useful clustering results, some supervised information should be introduced into the learn"
K16-1004,C02-1150,0,0.12767,"the weight of sn for µk . The second term is acquired from labeled data, and wnk is the weight of a labeled instance sn for µk . The update parameter step minimizes Jsemi with respect to f (·) by keeping {rnk } and {µk } fixed, which has no counterpart in the k-means algorithm. The main goal is to update parameters for the text representation model. We take Jsemi as the loss function, and train neural networks with the Adam algorithm (Kingma and Ba, 2014). 4 4.1 Experiment Experimental Setting We evaluate our method on four short text datasets. (1) question type is the TREC question dataset (Li and Roth, 2002), where all the questions are classified into 6 categories: abbreviation, description, entity, human, location and numeric. (2) ag news dataset contains short texts extracted from the AG’s news corpus, where all the texts are classified into 4 categories: World, Sports, Business, and Sci/Tech (Zhang and LeCun, 2015). (3) dbpedia is the DBpedia ontology dataset, which is constructed by picking 14 non-overlapping classes from DBpedia 2014 (Lehmann et al., 2014). (4) yahoo answer is the 10 topics classification dataset extracted from Yahoo! Answers Comprehensive Questions and Answers version 1.0"
N18-2090,P16-1014,0,0.046067,"wer, which encodes all words and the word order. Attentive-matching synthesizes a vector by computing a weighted sum of all answer states against the passage state, then compares the vector with the passage state. It also considers all words in the answer but without word order. Finally, max-attentive-matching only considers the most relevant answer state to the passage state. Pvocab = softmax(V1 [st ; ct ] + b1 ), where V1 and b1 are model parameters, and the number of rows in V1 is the size of the vocabulary. Since many passage words also appear in the question, we adopt the copy mechanism (Gulcehre et al., 2016; Gu et al., 2016), which integrates the attention over input words into the final vocabulary distribution. The probability distribution is defined as the interpolation: Pf inal = gt Pvocab + (1 − gt )Pattn , where gt is the switch for controlling generating a word from the vocabulary or directly copying it from the passage. Pvocab is the vocabulary probability distribution as defined above, and Pattn is calculated based on the current attention distribution by merging probabilities of duplicated words. Finally, gt is defined as: Multi-perspective matching These strategies require a function f"
N18-2090,D16-1264,0,0.134063,"ng et al., 2017; Wang et al., 2017a; Yuan et al., 2017). These methods can neglect rich potential ∗ We investigate explicit interaction between the target answer and the passage, so that contextual information can be better considered by the encoder. In particular, matching is used between the target answer and the passage for collecting relevant contextual information. We adopt the multiperspective context matching (MPCM) algorithm (Wang et al., 2017b), which takes two texts as input before producing a vector of numbers, representing similarity under different perspectives. Results on SQuAD (Rajpurkar et al., 2016) show that our model gives better BLEU scores than the state of the art. Furthermore, the questions generated by our model help to improve a strong extractive QA system. Our code is available at https://github.com/freesunshine0316/MPQG. Work done during an internship at IBM. 569 Proceedings of NAACL-HLT 2018, pages 569–574 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics 2 Baseline: sequence-to-sequence Our baseline is a sequence-to-sequence model (Bahdanau et al., 2015) with the copy mechanism (Gulcehre et al., 2016; Gu et al., 2016). It uses an LSTM"
N18-2090,N10-1086,0,0.479507,"born. This can be easily determined by leveraging the contextual information of “10 july 1856 – 7 january 1943”, while it is relatively hard when only the answer position information is adopted. Introduction The task of natural question generation (NQG) is to generate a fluent and relevant question given a passage and a target answer. Recently NQG has received increasing attention from both the industrial and academic communities because of its values for improving QA systems by automatically increasing the training data. It can also be used for educational purposes such as language learning (Heilman and Smith, 2010). One example is shown in Table 1, where a question “when was nikola tesla born ?” is generated given a passage and a fact “1856”. Existing work for NQG uses a sequence-to-sequence model (Sutskever et al., 2014), which takes a passage as input for generating a question. They either entirely ignore the target answer (Du et al., 2017), or directly hard-code answer positions (Zhou et al., 2017; Yang et al., 2017; Subramanian et al., 2017; Tang et al., 2017; Wang et al., 2017a; Yuan et al., 2017). These methods can neglect rich potential ∗ We investigate explicit interaction between the target ans"
N18-2090,D17-1090,0,0.106253,"Missing"
N18-2090,P17-1123,0,0.132216,"tly NQG has received increasing attention from both the industrial and academic communities because of its values for improving QA systems by automatically increasing the training data. It can also be used for educational purposes such as language learning (Heilman and Smith, 2010). One example is shown in Table 1, where a question “when was nikola tesla born ?” is generated given a passage and a fact “1856”. Existing work for NQG uses a sequence-to-sequence model (Sutskever et al., 2014), which takes a passage as input for generating a question. They either entirely ignore the target answer (Du et al., 2017), or directly hard-code answer positions (Zhou et al., 2017; Yang et al., 2017; Subramanian et al., 2017; Tang et al., 2017; Wang et al., 2017a; Yuan et al., 2017). These methods can neglect rich potential ∗ We investigate explicit interaction between the target answer and the passage, so that contextual information can be better considered by the encoder. In particular, matching is used between the target answer and the passage for collecting relevant contextual information. We adopt the multiperspective context matching (MPCM) algorithm (Wang et al., 2017b), which takes two texts as input be"
N18-2090,P16-1154,0,0.116662,"words and the word order. Attentive-matching synthesizes a vector by computing a weighted sum of all answer states against the passage state, then compares the vector with the passage state. It also considers all words in the answer but without word order. Finally, max-attentive-matching only considers the most relevant answer state to the passage state. Pvocab = softmax(V1 [st ; ct ] + b1 ), where V1 and b1 are model parameters, and the number of rows in V1 is the size of the vocabulary. Since many passage words also appear in the question, we adopt the copy mechanism (Gulcehre et al., 2016; Gu et al., 2016), which integrates the attention over input words into the final vocabulary distribution. The probability distribution is defined as the interpolation: Pf inal = gt Pvocab + (1 − gt )Pattn , where gt is the switch for controlling generating a word from the vocabulary or directly copying it from the passage. Pvocab is the vocabulary probability distribution as defined above, and Pattn is calculated based on the current attention distribution by merging probabilities of duplicated words. Finally, gt is defined as: Multi-perspective matching These strategies require a function fm to match two vec"
N18-2090,P17-1096,0,0.0263172,"ic communities because of its values for improving QA systems by automatically increasing the training data. It can also be used for educational purposes such as language learning (Heilman and Smith, 2010). One example is shown in Table 1, where a question “when was nikola tesla born ?” is generated given a passage and a fact “1856”. Existing work for NQG uses a sequence-to-sequence model (Sutskever et al., 2014), which takes a passage as input for generating a question. They either entirely ignore the target answer (Du et al., 2017), or directly hard-code answer positions (Zhou et al., 2017; Yang et al., 2017; Subramanian et al., 2017; Tang et al., 2017; Wang et al., 2017a; Yuan et al., 2017). These methods can neglect rich potential ∗ We investigate explicit interaction between the target answer and the passage, so that contextual information can be better considered by the encoder. In particular, matching is used between the target answer and the passage for collecting relevant contextual information. We adopt the multiperspective context matching (MPCM) algorithm (Wang et al., 2017b), which takes two texts as input before producing a vector of numbers, representing similarity under different pe"
N18-2090,W17-2603,0,0.0998808,"asing the training data. It can also be used for educational purposes such as language learning (Heilman and Smith, 2010). One example is shown in Table 1, where a question “when was nikola tesla born ?” is generated given a passage and a fact “1856”. Existing work for NQG uses a sequence-to-sequence model (Sutskever et al., 2014), which takes a passage as input for generating a question. They either entirely ignore the target answer (Du et al., 2017), or directly hard-code answer positions (Zhou et al., 2017; Yang et al., 2017; Subramanian et al., 2017; Tang et al., 2017; Wang et al., 2017a; Yuan et al., 2017). These methods can neglect rich potential ∗ We investigate explicit interaction between the target answer and the passage, so that contextual information can be better considered by the encoder. In particular, matching is used between the target answer and the passage for collecting relevant contextual information. We adopt the multiperspective context matching (MPCM) algorithm (Wang et al., 2017b), which takes two texts as input before producing a vector of numbers, representing similarity under different perspectives. Results on SQuAD (Rajpurkar et al., 2016) show that our model gives bette"
N19-1301,C16-1236,0,0.0294861,"fashion, making the models relatively less interpretable. Conceptually similar to our STOP strategy, Shen et al. [2017] propose the termination gate mechanism based on a random variable generated from the internal state for reading comprehension. In contrast, our model attempts to learn a general STOP key embedding based on the incrementally updated query representations, which can be learned from question-answer pairs only and lead to more explicit reasoning interpretations over structured KBs. This make our model potentially suit more real scenarios. Our work is also related to [Jain, 2016; Bao et al., 2016], which are designed to support reasoning for multi-relation questions by exploring the relation path and certain KB schema, e.g., CVT nodes, in the Freebase. The former also considers previously-addressed keys during query updating, but ignores the value representations. Thus, it still requires predefined rules and threshold to artificially add intermediate value representations to update the query. The latter also relies on a set of predefined rules to perform reasoning over Freebase. In contrast, our model incorporates both the key and value representations into the query representations,"
N19-1301,P14-1133,0,0.0160715,"in practice. Existing works address these using either the information retrieval (IR) based solutions or the semantic paring (SP) based approaches. The IRbased models [Yao and Van Durme, 2014; Yao, 2015; Bast and Haussmann, 2015; Bordes et al., 2015; Dong et al., 2015; Jain, 2016; Lai et al., 2019] tackle the KB-QA task by developing various ranking models towards the candidate answers, which implicitly meet the reasoning requirements during the candidate-searching step or in designing the ranking functions. In contrast, the SP-based approaches [Berant et al., 2013; Kwiatkowski et al., 2013; Berant and Liang, 2014; Reddy et al., 2014; Yih et al., 2015; Xu et al., 2016] explicitly represent the meaning of questions as logical forms or structured queries that naturally support reasoning over structured KBs. More recently, memory based reasoning solutions [Weston et al., 2014; Miller et al., 2016] are proposed to tackle the task. [Weston et al., 2014] proposed the Memory Neural Networks (MemNNs), which enable the neural network models read/write on an external memory component, and are further extended into an End-to-End fashion [Sukhbaatar et al., 2015a]. [Miller et al., 2938 2016] further proposed the K"
N19-1301,D13-1160,0,0.133507,"trong annotations, which is expensive to obtain in practice. Existing works address these using either the information retrieval (IR) based solutions or the semantic paring (SP) based approaches. The IRbased models [Yao and Van Durme, 2014; Yao, 2015; Bast and Haussmann, 2015; Bordes et al., 2015; Dong et al., 2015; Jain, 2016; Lai et al., 2019] tackle the KB-QA task by developing various ranking models towards the candidate answers, which implicitly meet the reasoning requirements during the candidate-searching step or in designing the ranking functions. In contrast, the SP-based approaches [Berant et al., 2013; Kwiatkowski et al., 2013; Berant and Liang, 2014; Reddy et al., 2014; Yih et al., 2015; Xu et al., 2016] explicitly represent the meaning of questions as logical forms or structured queries that naturally support reasoning over structured KBs. More recently, memory based reasoning solutions [Weston et al., 2014; Miller et al., 2016] are proposed to tackle the task. [Weston et al., 2014] proposed the Memory Neural Networks (MemNNs), which enable the neural network models read/write on an external memory component, and are further extended into an End-to-End fashion [Sukhbaatar et al., 2015a]."
N19-1301,P15-1026,0,0.0224465,"res the ability to properly analyze and represent the natural language questions against knowledge bases, especially for those involving multiple entities and relations, which we also call as reasoning over the KBs; (2) training such interpretable question understanding models requires considerable strong annotations, which is expensive to obtain in practice. Existing works address these using either the information retrieval (IR) based solutions or the semantic paring (SP) based approaches. The IRbased models [Yao and Van Durme, 2014; Yao, 2015; Bast and Haussmann, 2015; Bordes et al., 2015; Dong et al., 2015; Jain, 2016; Lai et al., 2019] tackle the KB-QA task by developing various ranking models towards the candidate answers, which implicitly meet the reasoning requirements during the candidate-searching step or in designing the ranking functions. In contrast, the SP-based approaches [Berant et al., 2013; Kwiatkowski et al., 2013; Berant and Liang, 2014; Reddy et al., 2014; Yih et al., 2015; Xu et al., 2016] explicitly represent the meaning of questions as logical forms or structured queries that naturally support reasoning over structured KBs. More recently, memory based reasoning solutions [We"
N19-1301,N16-2016,0,0.26833,"properly analyze and represent the natural language questions against knowledge bases, especially for those involving multiple entities and relations, which we also call as reasoning over the KBs; (2) training such interpretable question understanding models requires considerable strong annotations, which is expensive to obtain in practice. Existing works address these using either the information retrieval (IR) based solutions or the semantic paring (SP) based approaches. The IRbased models [Yao and Van Durme, 2014; Yao, 2015; Bast and Haussmann, 2015; Bordes et al., 2015; Dong et al., 2015; Jain, 2016; Lai et al., 2019] tackle the KB-QA task by developing various ranking models towards the candidate answers, which implicitly meet the reasoning requirements during the candidate-searching step or in designing the ranking functions. In contrast, the SP-based approaches [Berant et al., 2013; Kwiatkowski et al., 2013; Berant and Liang, 2014; Reddy et al., 2014; Yih et al., 2015; Xu et al., 2016] explicitly represent the meaning of questions as logical forms or structured queries that naturally support reasoning over structured KBs. More recently, memory based reasoning solutions [Weston et al.,"
N19-1301,D13-1161,0,0.0368492,"ich is expensive to obtain in practice. Existing works address these using either the information retrieval (IR) based solutions or the semantic paring (SP) based approaches. The IRbased models [Yao and Van Durme, 2014; Yao, 2015; Bast and Haussmann, 2015; Bordes et al., 2015; Dong et al., 2015; Jain, 2016; Lai et al., 2019] tackle the KB-QA task by developing various ranking models towards the candidate answers, which implicitly meet the reasoning requirements during the candidate-searching step or in designing the ranking functions. In contrast, the SP-based approaches [Berant et al., 2013; Kwiatkowski et al., 2013; Berant and Liang, 2014; Reddy et al., 2014; Yih et al., 2015; Xu et al., 2016] explicitly represent the meaning of questions as logical forms or structured queries that naturally support reasoning over structured KBs. More recently, memory based reasoning solutions [Weston et al., 2014; Miller et al., 2016] are proposed to tackle the task. [Weston et al., 2014] proposed the Memory Neural Networks (MemNNs), which enable the neural network models read/write on an external memory component, and are further extended into an End-to-End fashion [Sukhbaatar et al., 2015a]. [Miller et al., 2938 2016"
N19-1301,D16-1147,0,0.0729346,"t al., 2019] tackle the KB-QA task by developing various ranking models towards the candidate answers, which implicitly meet the reasoning requirements during the candidate-searching step or in designing the ranking functions. In contrast, the SP-based approaches [Berant et al., 2013; Kwiatkowski et al., 2013; Berant and Liang, 2014; Reddy et al., 2014; Yih et al., 2015; Xu et al., 2016] explicitly represent the meaning of questions as logical forms or structured queries that naturally support reasoning over structured KBs. More recently, memory based reasoning solutions [Weston et al., 2014; Miller et al., 2016] are proposed to tackle the task. [Weston et al., 2014] proposed the Memory Neural Networks (MemNNs), which enable the neural network models read/write on an external memory component, and are further extended into an End-to-End fashion [Sukhbaatar et al., 2015a]. [Miller et al., 2938 2016] further proposed the Key-Value Memory Network, which generalizes the MemNN by storing facts in a key-value structured memory. Both of the two models could perform shallow reasoning over the memory, since they can find answers by consecutively making predictions over multiple memory slots. Compared to the f"
N19-1301,Q14-1030,0,0.0494423,"Missing"
N19-1301,P10-1040,0,0.218523,"Missing"
N19-1301,P14-1090,0,0.125604,"Missing"
N19-1301,N15-3014,0,0.0209391,"allenges in the open domain KB-QA task: (1) it often requires the ability to properly analyze and represent the natural language questions against knowledge bases, especially for those involving multiple entities and relations, which we also call as reasoning over the KBs; (2) training such interpretable question understanding models requires considerable strong annotations, which is expensive to obtain in practice. Existing works address these using either the information retrieval (IR) based solutions or the semantic paring (SP) based approaches. The IRbased models [Yao and Van Durme, 2014; Yao, 2015; Bast and Haussmann, 2015; Bordes et al., 2015; Dong et al., 2015; Jain, 2016; Lai et al., 2019] tackle the KB-QA task by developing various ranking models towards the candidate answers, which implicitly meet the reasoning requirements during the candidate-searching step or in designing the ranking functions. In contrast, the SP-based approaches [Berant et al., 2013; Kwiatkowski et al., 2013; Berant and Liang, 2014; Reddy et al., 2014; Yih et al., 2015; Xu et al., 2016] explicitly represent the meaning of questions as logical forms or structured queries that naturally support reasoning over s"
N19-1301,P15-1128,0,0.127047,"sing either the information retrieval (IR) based solutions or the semantic paring (SP) based approaches. The IRbased models [Yao and Van Durme, 2014; Yao, 2015; Bast and Haussmann, 2015; Bordes et al., 2015; Dong et al., 2015; Jain, 2016; Lai et al., 2019] tackle the KB-QA task by developing various ranking models towards the candidate answers, which implicitly meet the reasoning requirements during the candidate-searching step or in designing the ranking functions. In contrast, the SP-based approaches [Berant et al., 2013; Kwiatkowski et al., 2013; Berant and Liang, 2014; Reddy et al., 2014; Yih et al., 2015; Xu et al., 2016] explicitly represent the meaning of questions as logical forms or structured queries that naturally support reasoning over structured KBs. More recently, memory based reasoning solutions [Weston et al., 2014; Miller et al., 2016] are proposed to tackle the task. [Weston et al., 2014] proposed the Memory Neural Networks (MemNNs), which enable the neural network models read/write on an external memory component, and are further extended into an End-to-End fashion [Sukhbaatar et al., 2015a]. [Miller et al., 2938 2016] further proposed the Key-Value Memory Network, which general"
P13-2110,D12-1046,0,0.577613,"orithm adjusts the Lagrange multiplier values based on the differences between ( ) and ( ) (line 8). A crucial point is that the argmax problems in line 3 and line 4 can be solved efficiently using the original decoding algorithms, because the Lagrange multiplier can be regarded as adjustments for lexical rule probabilities and word probabilities. 4 Experiments We conduct experiments on the Chinese Treebank Version 5.0 and use the standard data split 625 (Petrov and Klein, 2007). The traditional evaluation metrics for POS tagging and parsing are not suitable for the joint task. Following with Qian and Liu (2012), we redefine precision and recall by computing the span of a constituent based on character offsets rather than word offsets. 4.1 from our word segmentation system as input and “Lattice-based Parser” represents the system taking the compacted word lattice as input. We find the lattice-based parser gets better performance than the pipeline system among all three subtasks. Performance of the Basic Sub-systems We train the word segmentation system with 100 iterations of the Maximum Entropy model using the OpenNLP toolkit. Table 1 shows the performance. It shows that our word segmentation system"
P13-2110,D10-1001,0,0.0231916,"timization problem by optimizing the dual problem. First, we introduce a vector of Lagrange multipliers ( , , ) for each equality constraint. Then, the Lagrangian is formulated as: ( , , )= + ( )+ , , ( ) ( , , )( ( , , ) − ( , , )) ( )− , , (, , ) (, , ) (, , ) (, , ) , , Then, the dual objective is Combined Optimization Between The Lattice-based POS Tagger and The Lattice-based Parser , ( )+ and , ( ) = max ( , , ) = max ( )+ max ( )− , , , , , (, , ) (, , ) + (, , ) (, , ) The dual problem is to find min ( ). We use the subgradient method (Boyd et al., 2003) to minimize the dual. Following Rush et al. (2010), we define the subgradient of ( ) as: ( , , ) = ( , , ) − ( , , ) for all ( , , ) Then, adjust ( , , ) as follows: ( , , ) = ( , , ) − ( ( , , ) − ( , , )) where &gt;0 is a step size. Algorithm 1: Combined Optimization 1: Set ( ) ( , , )=0, for all ( , , ) 2: For k=1 to K ( ) + ∑ , , ( )( , , ) ( , , ) 3: ( ) ← argmax ( ) ( ) − ∑ , , ( )( , , ) ( , , ) 4: ← argmax ( )( ( ) ( , , ) for all ( , , ) 5: If , , )= 6: Return ( ( ) , ( ) ) 7: Else ( ) (, , )= 8: ( )( , , ) − ( ( ) ( , , ) − ( ) ( , , )) Algorithm 1 presents the subgradient method to solve the dual problem. The algorithm initializes the"
P13-2110,P11-1139,0,0.0605215,"Missing"
P13-2110,A00-2018,0,0.323933,"Missing"
P13-2110,O03-4002,1,0.833333,"Missing"
P13-2110,W02-1001,0,0.0115278,"one edge. We also assign a probability to each edge, which is calculated by multiplying the tagging probabilities of each character in the word. The goal of the lattice-based POS tagger is to predict a tagged word sequence for an input word lattice : = argmax ∈ ( ) ∙ ( ) where ( ) represents the set of all possible tagged word sequences derived from the word lattice . ( ) is used to map onto a global feature vector, and is the corresponding weight vector. We use the same non-local feature templates used in Jiang et al. (2008) and a similar decoding algorithm. We use the perceptron algorithm (Collins, 2002) for parameter estimation. Goldberg and Elhadad (2011) proposed a lattice-based parser for Heberw based on the PCFG-LA model (Matsuzaki et al., 2005). We adopted their approach, but found the unweighted word lattice their parser takes as input to be ineffective for our Chinese experiments. Instead, we use a weighted lattice as input and weigh each edge in the lattice with the word probability. In our model, each syntactic category is split into multiple subcategories [ ] by labeling a latent annotation . Then, a parse tree 624 is refined into [ ], where X is the latent annotation vector for al"
P13-2110,D10-1082,0,0.157435,"Missing"
P13-2110,P11-2124,0,0.0248862,"o each edge, which is calculated by multiplying the tagging probabilities of each character in the word. The goal of the lattice-based POS tagger is to predict a tagged word sequence for an input word lattice : = argmax ∈ ( ) ∙ ( ) where ( ) represents the set of all possible tagged word sequences derived from the word lattice . ( ) is used to map onto a global feature vector, and is the corresponding weight vector. We use the same non-local feature templates used in Jiang et al. (2008) and a similar decoding algorithm. We use the perceptron algorithm (Collins, 2002) for parameter estimation. Goldberg and Elhadad (2011) proposed a lattice-based parser for Heberw based on the PCFG-LA model (Matsuzaki et al., 2005). We adopted their approach, but found the unweighted word lattice their parser takes as input to be ineffective for our Chinese experiments. Instead, we use a weighted lattice as input and weigh each edge in the lattice with the word probability. In our model, each syntactic category is split into multiple subcategories [ ] by labeling a latent annotation . Then, a parse tree 624 is refined into [ ], where X is the latent annotation vector for all non-terminals in . The probability of [ ] is calcula"
P13-2110,C08-1049,0,0.135876,"compact the N-best lists into a word lattice by collapsing all the identical words into one edge. We also assign a probability to each edge, which is calculated by multiplying the tagging probabilities of each character in the word. The goal of the lattice-based POS tagger is to predict a tagged word sequence for an input word lattice : = argmax ∈ ( ) ∙ ( ) where ( ) represents the set of all possible tagged word sequences derived from the word lattice . ( ) is used to map onto a global feature vector, and is the corresponding weight vector. We use the same non-local feature templates used in Jiang et al. (2008) and a similar decoding algorithm. We use the perceptron algorithm (Collins, 2002) for parameter estimation. Goldberg and Elhadad (2011) proposed a lattice-based parser for Heberw based on the PCFG-LA model (Matsuzaki et al., 2005). We adopted their approach, but found the unweighted word lattice their parser takes as input to be ineffective for our Chinese experiments. Instead, we use a weighted lattice as input and weigh each edge in the lattice with the word probability. In our model, each syntactic category is split into multiple subcategories [ ] by labeling a latent annotation . Then, a"
P13-2110,P09-1058,0,0.0593855,"Missing"
P13-2110,P05-1010,0,0.0149397,"word. The goal of the lattice-based POS tagger is to predict a tagged word sequence for an input word lattice : = argmax ∈ ( ) ∙ ( ) where ( ) represents the set of all possible tagged word sequences derived from the word lattice . ( ) is used to map onto a global feature vector, and is the corresponding weight vector. We use the same non-local feature templates used in Jiang et al. (2008) and a similar decoding algorithm. We use the perceptron algorithm (Collins, 2002) for parameter estimation. Goldberg and Elhadad (2011) proposed a lattice-based parser for Heberw based on the PCFG-LA model (Matsuzaki et al., 2005). We adopted their approach, but found the unweighted word lattice their parser takes as input to be ineffective for our Chinese experiments. Instead, we use a weighted lattice as input and weigh each edge in the lattice with the word probability. In our model, each syntactic category is split into multiple subcategories [ ] by labeling a latent annotation . Then, a parse tree 624 is refined into [ ], where X is the latent annotation vector for all non-terminals in . The probability of [ ] is calculated as: ( [ ]) = ( [ ] → [ ] [ ]) × × ( ) ( [ ]→ By grouping the terms that depend on we rewrit"
P13-2110,P06-1055,0,0.0363155,"S tagger with 20 iterations of the average perceptron algorithm. Table 2 presents the joint word segmentation and POS tagging performance and shows that our lattice-based POS tagger obtains results that are comparable with state-of-the-art systems. (Kruengkrai et al., 2009) (Zhang and Clark, 2010) (Qian and Liu, 2012) (Sun, 2011) Lattice-based POS tagger P 93.28 93.1 93.64 R 94.07 93.96 93.87 F 93.67 93.67 93.53 94.02 93.75 Table 2: POS tagging evaluation. We implement the lattice-based parser by modifying the Berkeley Parser, and train it with 5 iterations of the split-merge-smooth strategy (Petrov et al., 2006). Table 3 shows the performance, where the “Pipeline Parser” represents the system taking one-best segmentation result Pipeline Parser Lattice-based Parser P 96.97 92.01 80.86 97.73 93.24 81.83 Seg. POS Parse Seg. POS Parse R 98.06 93.04 81.47 97.66 93.18 81.71 F 97.52 92.52 81.17 97.70 93.21 81.77 Table 3: Parsing evaluation. 4.2 Performance of the Framework For the lattice-based framework, we set the maximum iteration in Algorithm 1 as K = 20. The step size is tuned on the development set and empirically set to be 0.8. Table 4 shows the parsing performance on the test set. It shows that the"
P13-2110,N07-1051,0,0.253157,"based parser are used to process the lattice from two different viewpoints: sequential POS tagging and hierarchical tree building. A strategy is designed to exploit the complementary strengths of the tagger and parser, and encourage them to predict agreed structures. Experimental results on Chinese Treebank show that our lattice-based framework significantly improves the accuracy of the three sub-tasks. 1 Introduction Previous work on syntactic parsing generally assumes a processing pipeline where an input sentence is first tokenized, POS-tagged and then parsed (Collins, 1999; Charniak, 2000; Petrov and Klein, 2007). This approach works well for languages like English where automatic tokenization and POS tagging can be performed with high accuracy without the guidance of the highlevel syntactic structure. Such an approach, however, is not optimal for languages like Chinese where there are no natural delimiters for word boundaries, and word segmentation (or tokenization) is a non-trivial research problem by itself. Errors in word segmentation would propagate to later processing stages such as POS tagging and syntactic parsing. More importantly, Chinese is a language that lacks the morphological clues that"
P13-2110,J03-4003,0,\N,Missing
P13-2110,I11-1035,0,\N,Missing
P13-2110,P12-1026,0,\N,Missing
P14-1069,I11-1136,0,0.102266,"into P0 16: beami+1 ← k best states of P0 17: return the best state in beam2n−1 With such an action, POS tagging becomes a natural part of transition-based parsing. However, some feature templates in Table 1 become unavailable, because POS tags for the look-ahead words are not specified yet under the joint framework. For example, for the template q0 wt , the POS tag of the first word q0 in the queue β is required, but it is not specified yet at the present state. To overcome the lack of look-ahead POS tags, we borrow the concept of delayed features originally developed for dependency parsing (Hatori et al., 2011). Features that require look-ahead POS tags are defined as delayed features. In these features, look-ahead POS tags are taken as variables. During parsing, delayed features are extracted and passed from one state to the next state. When a sh-x action is performed, the look-ahead POS tag of some delayed features is specified, therefore these delayed features can be transformed into normal features (by replacing variable with the newly specified POS tag). The remaining delayed features will be transformed similarly when their look-ahead POS tags are specified during the following parsing steps."
P14-1069,D09-1087,0,0.0517862,"Missing"
P14-1069,P08-1067,0,0.0187858,"les. For transition-based parsing, Hatori et al. (2011) proposed to integrate POS tagging with dependency parsing. Our joint approach can be seen as an adaption of Hatori et al. (2011)’s approach for constituent parsing. Zhang et al. (2013) proposed a transition-based constituent parser to process an input sentence from the character level. However, manual annotation of the word-internal structures need to be added to the original Treebank in order to train such a parser. Non-local features have been successfully used for constituent parsing (Charniak and Johnson, 2005; Collins and Koo, 2005; Huang, 2008). However, almost all of the previous work use nonlocal features at the parse reranking stage. The reason is that the single-stage chart-based parser cannot use non-local structural features. In contrast, the transition-based parser can use arbitrarily complex structural features. Therefore, we can concisely utilize non-local features in a singleTable 7 presents the statistics of frequent POS tagging error patterns. We can see that JointParsing system disambiguates {VV, NN} and {DEC, DEG} better than Pipeline system, but cannot deal with the NN→JJ pattern very well. StateAlign system got bette"
P14-1069,P08-1068,0,0.0229619,"eighbours NGramTree Heads Wproj Word Feature Extension One advantage of transition-based constituent parsing is that it is capable of incorporating arbitrarily complex structural features from the already constructed subtrees in σ and unprocessed words in β. However, all the feature templates given in Table 1 are just some simple structural features. To further improve the performance of our transition-based constituent parser, we consider two group of complex structural features: non-local features (Charniak and Johnson, 2005; Collins and Koo, 2005) and semi-supervised word cluster features (Koo et al., 2008). Table 2 lists all the non-local features we want to use. These features have been proved very helpful for constituent parsing (Charniak and Johnson, 2005; Collins and Koo, 2005). But almost all previous work considered non-local features only in parse reranking frameworks. Instead, we attempt to extract non-local features from newly constructed subtrees during the decoding process as they become incrementally available and score newly generated parser states with them. One difficulty is that the subtrees built by our baseline parser are binary trees (only the complete parse tree is debinariz"
P14-1069,P13-2018,0,0.0358112,"Missing"
P14-1069,P05-1022,0,0.130415,"y identified by its path from the root, and represented as a bit-string. By using various length of prefixes of the bit-string, we can produce word clusters of different granularities (Miller et al., 2004). Inspired from Koo et al. (2008), we employ two types of word clusters: (1) taking 4 bit-string prefixes of word clusters as replacements of POS tags, and (2) taking 8 bit-string prefixes as replacements of words. Using these two types of clusters, we construct semi-supervised word cluster features by mimicking the template structure of the original baseline features in Table 1. beams. 3.3 (Charniak and Johnson, 2005) CoPar HeadTree CoLenPar RightBranch Heavy Neighbours NGramTree Heads Wproj Word Feature Extension One advantage of transition-based constituent parsing is that it is capable of incorporating arbitrarily complex structural features from the already constructed subtrees in σ and unprocessed words in β. However, all the feature templates given in Table 1 are just some simple structural features. To further improve the performance of our transition-based constituent parser, we consider two group of complex structural features: non-local features (Charniak and Johnson, 2005; Collins and Koo, 2005)"
P14-1069,N04-1043,0,0.0427075,"Missing"
P14-1069,A00-2018,0,0.215926,"produced by the Pipeline system. The JointParsing system reduced errors of all types produced by the Pipeline system except for the coordination error type (Coord). The StateAlign system corrected a lot of the NP-internal errors (NP Int.). The Nonlocal system and the Cluster system produced similar numbers of errors for all error types. The Nonlocal&Cluster system produced the Best numbers for all the error types. NPinternal errors are still the most frequent error type in our parsing systems. Related Work Joint POS tagging with parsing is not a new idea. In PCFG-based parsing (Collins, 1999; Charniak, 2000; Petrov et al., 2006), POS tagging is considered as a natural step of parsing by employing lexical rules. For transition-based parsing, Hatori et al. (2011) proposed to integrate POS tagging with dependency parsing. Our joint approach can be seen as an adaption of Hatori et al. (2011)’s approach for constituent parsing. Zhang et al. (2013) proposed a transition-based constituent parser to process an input sentence from the character level. However, manual annotation of the word-internal structures need to be added to the original Treebank in order to train such a parser. Non-local features ha"
P14-1069,N07-1051,0,0.0632134,"Missing"
P14-1069,J05-1003,0,0.0243004,"iak and Johnson, 2005) CoPar HeadTree CoLenPar RightBranch Heavy Neighbours NGramTree Heads Wproj Word Feature Extension One advantage of transition-based constituent parsing is that it is capable of incorporating arbitrarily complex structural features from the already constructed subtrees in σ and unprocessed words in β. However, all the feature templates given in Table 1 are just some simple structural features. To further improve the performance of our transition-based constituent parser, we consider two group of complex structural features: non-local features (Charniak and Johnson, 2005; Collins and Koo, 2005) and semi-supervised word cluster features (Koo et al., 2008). Table 2 lists all the non-local features we want to use. These features have been proved very helpful for constituent parsing (Charniak and Johnson, 2005; Collins and Koo, 2005). But almost all previous work considered non-local features only in parse reranking frameworks. Instead, we attempt to extract non-local features from newly constructed subtrees during the decoding process as they become incrementally available and score newly generated parser states with them. One difficulty is that the subtrees built by our baseline parse"
P14-1069,P06-1055,0,0.0298089,"Pipeline system. The JointParsing system reduced errors of all types produced by the Pipeline system except for the coordination error type (Coord). The StateAlign system corrected a lot of the NP-internal errors (NP Int.). The Nonlocal system and the Cluster system produced similar numbers of errors for all error types. The Nonlocal&Cluster system produced the Best numbers for all the error types. NPinternal errors are still the most frequent error type in our parsing systems. Related Work Joint POS tagging with parsing is not a new idea. In PCFG-based parsing (Collins, 1999; Charniak, 2000; Petrov et al., 2006), POS tagging is considered as a natural step of parsing by employing lexical rules. For transition-based parsing, Hatori et al. (2011) proposed to integrate POS tagging with dependency parsing. Our joint approach can be seen as an adaption of Hatori et al. (2011)’s approach for constituent parsing. Zhang et al. (2013) proposed a transition-based constituent parser to process an input sentence from the character level. However, manual annotation of the word-internal structures need to be added to the original Treebank in order to train such a parser. Non-local features have been successfully u"
P14-1069,P04-1015,0,0.216455,"Missing"
P14-1069,W05-1513,0,0.605273,"roduction Constituent parsing is one of the most fundamental tasks in Natural Language Processing (NLP). It seeks to uncover the underlying recursive phrase structure of sentences. Most of the state-of-theart parsers are based on the PCFG paradigm and chart-based decoding algorithms (Collins, 1999; Charniak, 2000; Petrov et al., 2006). Chart-based parsers perform exhaustive search with dynamic programming, which contributes to their high accuracy, but they also suffer from higher runtime complexity and can only exploit simple local structural information. Transition-based constituent parsing (Sagae and Lavie, 2005; Wang et al., 2006; Zhang and Clark, 2009) is an attractive alternative. It utilizes a se733 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 733–742, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics determination, the accuracy of POS tagging improves, and this will in turn improve parsing accuracy. Second, we propose a novel state alignment strategy to align candidate parses with different action sizes during beam-search decoding. With this strategy, parser states and their unary extensions are put into"
P14-1069,I11-1140,1,0.908945,"Missing"
P14-1069,P06-1054,0,0.126292,"arsing is one of the most fundamental tasks in Natural Language Processing (NLP). It seeks to uncover the underlying recursive phrase structure of sentences. Most of the state-of-theart parsers are based on the PCFG paradigm and chart-based decoding algorithms (Collins, 1999; Charniak, 2000; Petrov et al., 2006). Chart-based parsers perform exhaustive search with dynamic programming, which contributes to their high accuracy, but they also suffer from higher runtime complexity and can only exploit simple local structural information. Transition-based constituent parsing (Sagae and Lavie, 2005; Wang et al., 2006; Zhang and Clark, 2009) is an attractive alternative. It utilizes a se733 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 733–742, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics determination, the accuracy of POS tagging improves, and this will in turn improve parsing accuracy. Second, we propose a novel state alignment strategy to align candidate parses with different action sizes during beam-search decoding. With this strategy, parser states and their unary extensions are put into the same beam, ther"
P14-1069,W09-3825,0,0.839428,"e most fundamental tasks in Natural Language Processing (NLP). It seeks to uncover the underlying recursive phrase structure of sentences. Most of the state-of-theart parsers are based on the PCFG paradigm and chart-based decoding algorithms (Collins, 1999; Charniak, 2000; Petrov et al., 2006). Chart-based parsers perform exhaustive search with dynamic programming, which contributes to their high accuracy, but they also suffer from higher runtime complexity and can only exploit simple local structural information. Transition-based constituent parsing (Sagae and Lavie, 2005; Wang et al., 2006; Zhang and Clark, 2009) is an attractive alternative. It utilizes a se733 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 733–742, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics determination, the accuracy of POS tagging improves, and this will in turn improve parsing accuracy. Second, we propose a novel state alignment strategy to align candidate parses with different action sizes during beam-search decoding. With this strategy, parser states and their unary extensions are put into the same beam, therefore the parsing model"
P14-1069,P13-1013,0,0.0123913,". The Nonlocal&Cluster system produced the Best numbers for all the error types. NPinternal errors are still the most frequent error type in our parsing systems. Related Work Joint POS tagging with parsing is not a new idea. In PCFG-based parsing (Collins, 1999; Charniak, 2000; Petrov et al., 2006), POS tagging is considered as a natural step of parsing by employing lexical rules. For transition-based parsing, Hatori et al. (2011) proposed to integrate POS tagging with dependency parsing. Our joint approach can be seen as an adaption of Hatori et al. (2011)’s approach for constituent parsing. Zhang et al. (2013) proposed a transition-based constituent parser to process an input sentence from the character level. However, manual annotation of the word-internal structures need to be added to the original Treebank in order to train such a parser. Non-local features have been successfully used for constituent parsing (Charniak and Johnson, 2005; Collins and Koo, 2005; Huang, 2008). However, almost all of the previous work use nonlocal features at the parse reranking stage. The reason is that the single-stage chart-based parser cannot use non-local structural features. In contrast, the transition-based pa"
P14-1069,P13-1043,0,0.477547,"taken as variables. During parsing, delayed features are extracted and passed from one state to the next state. When a sh-x action is performed, the look-ahead POS tag of some delayed features is specified, therefore these delayed features can be transformed into normal features (by replacing variable with the newly specified POS tag). The remaining delayed features will be transformed similarly when their look-ahead POS tags are specified during the following parsing steps. 3.2 We propose a novel method to align states during the parsing process instead of just aligning terminal states like Zhu et al. (2013). We classify all the actions into two groups according to whether they consume items in σ or β. sh-x, rl-x, and rr-x belong to consuming actions, and ru-x belongs to non-consuming action. Algorithm 2 gives the details of our method. It is based on the beam search decoding algorithm described in Algorithm 1. Different from Algorithm 1, Algorithm 2 is guaranteed to perform 2n − 1 parsing steps for an input sentence containing n words (line 2), and divides each parsing step into two parsing phases. In the first phase (line 4-9), each of the k states in beami is extended by consuming actions. In"
P14-1069,J03-4003,0,\N,Missing
P15-1110,C14-1076,0,0.0522234,"Missing"
P15-1110,P05-1022,0,0.0395646,"Missing"
P15-1110,A00-2018,0,0.0909756,"entences that can be used in a variety of natural language applications such as machine translation, information extraction and question answering (Jurafsky and Martin, 2008). One of the major challenges for this task is that constituent parsers require an inference algorithm of high computational complexity in order Nianwen Xue Brandeis University 415 South St Waltham, MA, USA xuen@brandeis.edu to search over their large structural space, which makes it very hard to efficiently train discriminative models. So, for a long time, the task was mainly solved with generative models (Collins, 1999; Charniak, 2000; Petrov et al., 2006). In the last few years, however, with the use of effective parsing strategies, approximate inference algorithms, and more efficient training methods, discriminative models began to surpass the generative models (Carreras et al., 2008; Zhu et al., 2013; Wang and Xue, 2014). Just like other NLP tasks, the performance of discriminative constituent parsing crucially relies on feature engineering. If the feature set is too small, it might underfit the model and leads to low performance. On the other hand, too many features may result in an overfitting problem. Usually, an eff"
P15-1110,D14-1082,0,0.145268,"Missing"
P15-1110,E03-1002,0,0.330686,"but it is also tedious and time-consuming. Even after this painstaking process, it is still hard to say whether the selected feature set is complete or optimal to obtain the best possible results. A more desirable alternative is to learn features automatically with machine learning algorithms. Lei et al. (2014) proposed to learn features by representing the cross-products of some primitive units with low-rank tensors for dependency parsing. However, to achieve competitive performance, they had to combine the learned features with the traditional hand-crafted features. For constituent parsing, Henderson (2003) employed a recurrent neural network to induce features from an unbounded parsing history. However, the final performance was below the state of the art. In this work, we design a much simpler neural network to automatically induce features from just the local context for constituent parsing. Con1138 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1138–1147, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics DT NN VP MD That debt would VB be V"
P15-1110,P04-1013,0,0.0531726,"Missing"
P15-1110,D10-1002,0,0.0703841,"Missing"
P15-1110,P08-1067,0,0.0590737,"Missing"
P15-1110,P14-1130,0,0.0385871,"the other hand, too many features may result in an overfitting problem. Usually, an effective set of features have to be designed manually and selected through repeated experiments (Sagae and Lavie, 2005; Wang et al., 2006; Zhang and Clark, 2009). Not only does this procedure require a lot of expertise, but it is also tedious and time-consuming. Even after this painstaking process, it is still hard to say whether the selected feature set is complete or optimal to obtain the best possible results. A more desirable alternative is to learn features automatically with machine learning algorithms. Lei et al. (2014) proposed to learn features by representing the cross-products of some primitive units with low-rank tensors for dependency parsing. However, to achieve competitive performance, they had to combine the learned features with the traditional hand-crafted features. For constituent parsing, Henderson (2003) employed a recurrent neural network to induce features from an unbounded parsing history. However, the final performance was below the state of the art. In this work, we design a much simpler neural network to automatically induce features from just the local context for constituent parsing. Co"
P15-1110,J93-2004,0,0.0509943,"ining has demonstrated its effectiveness as a way of initializing neural network models (Erhan et al., 2010). Since our model requires many run-time primitive units (POS tags and constituent labels), we employ an in-house shift-reduce parser to parse a large amount of unlabeled sentences, and pre-train the model with the automatically parsed data. Third, we utilize the Dropout strategy to address the overfitting probExperimental Setting We conducted experiments on the Penn Chinese Treebank (CTB) version 5.1 (Xue et al., 2005) and the Wall Street Journal (WSJ) portion of Penn English Treebank (Marcus et al., 1993). To fairly compare with other work, we follow the standard data division. For Chinese, we allocated Articles 001-270 and 400-1151 as the training set, Articles 301-325 as the development set, and Articles 271300 as the testing set. For English, we use sections 2-21 for training, section 22 for developing and section 23 for testing. We also utilized some unlabeled corpora and used the word2vec2 toolkit to train word embeddings. For Chinese, we used the unlabeled Chinese Gigaword (LDC2003T09) and performed Chinese word segmentation using our in-house segmenter. For English, we randomly selected"
P15-1110,N06-1020,0,0.0318095,"Missing"
P15-1110,C10-1093,0,0.0603562,"Missing"
P15-1110,N07-1051,0,0.0498802,"t conversation (bc), weblogs (wb) and discussion forums (df). Since all of the mz domain data is already included in our training set, we only selected sample sentences from the other five domains as the test sets 5 , and made sure these test sets had no overlap with our treebank training, development and test sets. Note that we did not use any data from these five domains for training or development. The models are still the ones described in the previous subsection. The results are presented in Table 7. Although our “Supervised” model got slightly worse performance than the Berkeley Parser (Petrov and Klein, 2007), as shown in Table 5, it outperformed the Berkeley Parser on the cross-domain data sets. This suggests that the learned features can better adapt to cross-domain situations. Compared with the Berkeley Parser, on average our “Pretrain-Finetune” model is 3.4 percentage points better in terms of parsing accuracy, and 3.2 percentage points better in terms of POS tagging accuracy. We also presented the performance of our pre-trained model (“Only-Pretrain”). We found the “Only-Pretrain” model performs poorly on this cross-domain data sets. But even pretraining based on this less than competitive mo"
P15-1110,P06-1055,0,0.0172632,"n be used in a variety of natural language applications such as machine translation, information extraction and question answering (Jurafsky and Martin, 2008). One of the major challenges for this task is that constituent parsers require an inference algorithm of high computational complexity in order Nianwen Xue Brandeis University 415 South St Waltham, MA, USA xuen@brandeis.edu to search over their large structural space, which makes it very hard to efficiently train discriminative models. So, for a long time, the task was mainly solved with generative models (Collins, 1999; Charniak, 2000; Petrov et al., 2006). In the last few years, however, with the use of effective parsing strategies, approximate inference algorithms, and more efficient training methods, discriminative models began to surpass the generative models (Carreras et al., 2008; Zhu et al., 2013; Wang and Xue, 2014). Just like other NLP tasks, the performance of discriminative constituent parsing crucially relies on feature engineering. If the feature set is too small, it might underfit the model and leads to low performance. On the other hand, too many features may result in an overfitting problem. Usually, an effective set of features"
P15-1110,W96-0213,0,0.123723,"l we cannot say for sure that this is the optimal subset of features for the parsing task. To cope with this problem, we propose to simultaneously optimize feature representation and parsing accuracy via a neural network model. Figure 2 illustrates the architecture of our model. Our model consists of input, projection, hidden and output layers. First, in the input layer, all primitive units (shown in Table 1(a)) are imported to the network. We also import the suffixes and prefixes of the first word in the queue, because these units have been shown to be very effective for predicting POS tags (Ratnaparkhi, 1996). Then, in the projection layer, each primitive unit is projected into a vector. Specifically, word-type units are represented as word embeddings, and other units are transformed into one-hot representations. The p0 w, p0 t,p0 c, p1 w, p1 t,p1 c, p2 w, p2 t,p2 c, p3 w, p3 t,p3 c p0l w, p0l c, p0r w, p0r c,p0u w, p0u c, p1l w, p1l c, p1r w, p1r c,p1u w, p1u c q0 w, q1 w, q2 w, q3 w trigrams p0 tc, p0 wc, p1 tc, p1 wc, p2 tc p2 wc, p3 tc, p3 wc, q0 wt, q1 wt q2 wt, q3 wt, p0l wc, p0r wc p0u wc, p1l wc, p1r wc, p1u wc p0 wp1 w, p0 wp1 c, p0 cp1 w, p0 cp1 c p0 wq0 w, p0 wq0 t, p0 cq0 w, p0 cq0 t q"
P15-1110,W05-1513,0,0.0851226,"egies, approximate inference algorithms, and more efficient training methods, discriminative models began to surpass the generative models (Carreras et al., 2008; Zhu et al., 2013; Wang and Xue, 2014). Just like other NLP tasks, the performance of discriminative constituent parsing crucially relies on feature engineering. If the feature set is too small, it might underfit the model and leads to low performance. On the other hand, too many features may result in an overfitting problem. Usually, an effective set of features have to be designed manually and selected through repeated experiments (Sagae and Lavie, 2005; Wang et al., 2006; Zhang and Clark, 2009). Not only does this procedure require a lot of expertise, but it is also tedious and time-consuming. Even after this painstaking process, it is still hard to say whether the selected feature set is complete or optimal to obtain the best possible results. A more desirable alternative is to learn features automatically with machine learning algorithms. Lei et al. (2014) proposed to learn features by representing the cross-products of some primitive units with low-rank tensors for dependency parsing. However, to achieve competitive performance, they had"
P15-1110,W09-3825,0,0.392801,"nd more efficient training methods, discriminative models began to surpass the generative models (Carreras et al., 2008; Zhu et al., 2013; Wang and Xue, 2014). Just like other NLP tasks, the performance of discriminative constituent parsing crucially relies on feature engineering. If the feature set is too small, it might underfit the model and leads to low performance. On the other hand, too many features may result in an overfitting problem. Usually, an effective set of features have to be designed manually and selected through repeated experiments (Sagae and Lavie, 2005; Wang et al., 2006; Zhang and Clark, 2009). Not only does this procedure require a lot of expertise, but it is also tedious and time-consuming. Even after this painstaking process, it is still hard to say whether the selected feature set is complete or optimal to obtain the best possible results. A more desirable alternative is to learn features automatically with machine learning algorithms. Lei et al. (2014) proposed to learn features by representing the cross-products of some primitive units with low-rank tensors for dependency parsing. However, to achieve competitive performance, they had to combine the learned features with the t"
P15-1110,P13-1043,0,0.4563,"ithm of high computational complexity in order Nianwen Xue Brandeis University 415 South St Waltham, MA, USA xuen@brandeis.edu to search over their large structural space, which makes it very hard to efficiently train discriminative models. So, for a long time, the task was mainly solved with generative models (Collins, 1999; Charniak, 2000; Petrov et al., 2006). In the last few years, however, with the use of effective parsing strategies, approximate inference algorithms, and more efficient training methods, discriminative models began to surpass the generative models (Carreras et al., 2008; Zhu et al., 2013; Wang and Xue, 2014). Just like other NLP tasks, the performance of discriminative constituent parsing crucially relies on feature engineering. If the feature set is too small, it might underfit the model and leads to low performance. On the other hand, too many features may result in an overfitting problem. Usually, an effective set of features have to be designed manually and selected through repeated experiments (Sagae and Lavie, 2005; Wang et al., 2006; Zhang and Clark, 2009). Not only does this procedure require a lot of expertise, but it is also tedious and time-consuming. Even after th"
P15-1110,P13-1045,0,0.111934,"Missing"
P15-1110,P14-1069,1,0.936891,"tational complexity in order Nianwen Xue Brandeis University 415 South St Waltham, MA, USA xuen@brandeis.edu to search over their large structural space, which makes it very hard to efficiently train discriminative models. So, for a long time, the task was mainly solved with generative models (Collins, 1999; Charniak, 2000; Petrov et al., 2006). In the last few years, however, with the use of effective parsing strategies, approximate inference algorithms, and more efficient training methods, discriminative models began to surpass the generative models (Carreras et al., 2008; Zhu et al., 2013; Wang and Xue, 2014). Just like other NLP tasks, the performance of discriminative constituent parsing crucially relies on feature engineering. If the feature set is too small, it might underfit the model and leads to low performance. On the other hand, too many features may result in an overfitting problem. Usually, an effective set of features have to be designed manually and selected through repeated experiments (Sagae and Lavie, 2005; Wang et al., 2006; Zhang and Clark, 2009). Not only does this procedure require a lot of expertise, but it is also tedious and time-consuming. Even after this painstaking proces"
P15-1110,I11-1140,1,0.901748,"Missing"
P15-1110,P06-1054,0,0.0708317,"rence algorithms, and more efficient training methods, discriminative models began to surpass the generative models (Carreras et al., 2008; Zhu et al., 2013; Wang and Xue, 2014). Just like other NLP tasks, the performance of discriminative constituent parsing crucially relies on feature engineering. If the feature set is too small, it might underfit the model and leads to low performance. On the other hand, too many features may result in an overfitting problem. Usually, an effective set of features have to be designed manually and selected through repeated experiments (Sagae and Lavie, 2005; Wang et al., 2006; Zhang and Clark, 2009). Not only does this procedure require a lot of expertise, but it is also tedious and time-consuming. Even after this painstaking process, it is still hard to say whether the selected feature set is complete or optimal to obtain the best possible results. A more desirable alternative is to learn features automatically with machine learning algorithms. Lei et al. (2014) proposed to learn features by representing the cross-products of some primitive units with low-rank tensors for dependency parsing. However, to achieve competitive performance, they had to combine the lea"
P15-1110,W08-2102,0,\N,Missing
P15-1110,J03-4003,0,\N,Missing
P16-2021,P07-2045,0,0.00793318,"Missing"
P16-2021,W14-3309,0,0.0574079,"Missing"
P16-2021,N13-1073,0,0.0275834,"common words for VxT . Then we check various top n (10, 20, and 50) candidates for the word-to-word dictionary for VxD . train dev. sentence mini-batch sentence Jean (2015) 30k 30k 30k Ours 2080 6153 2067 has 6003 sentences in total. Our test set has 3003 sentences from WMT news-test 2014. We evaluate the translation quality using the case-sensitive BLEU-4 metric (Papineni et al., 2002) with the multi-bleu.perl script. Same as Jean et al. (2015), our full vocabulary size is 500k, we use AdaDelta (Zeiler, 2012), and mini-batch size is 80. Given the training set, we first run the ‘fast align’ (Dyer et al., 2013) in one direction, and use the translation table as our word-to-word dictionary. Then we run the reverse direction and apply ‘grow-diag-final-and’ heuristics to get the alignment. The phrase table is extracted with a standard algorithm in Moses (Koehn et al., 2007). In the decoding procedure, our method is very similar to the ‘candidate list’ of Jean et al. (2015), except that we also use bilingual phrases and we only include top 2k most frequent target words. Following Jean et al. (2015), we dump the alignments for each sentence, and replace UNKs with the word-to-word dictionary or the source"
P16-2021,P02-1040,0,0.102533,"D ∪ VxT 10 20 50 92.7 94.2 96.2 91.7 92.7 94.3 Table 1: The average reference coverage ratios (in word-level) on the training and development sets. We use fixed top 10 candidates for each phrase when generating VxP , and top 2k most common words for VxT . Then we check various top n (10, 20, and 50) candidates for the word-to-word dictionary for VxD . train dev. sentence mini-batch sentence Jean (2015) 30k 30k 30k Ours 2080 6153 2067 has 6003 sentences in total. Our test set has 3003 sentences from WMT news-test 2014. We evaluate the translation quality using the case-sensitive BLEU-4 metric (Papineni et al., 2002) with the multi-bleu.perl script. Same as Jean et al. (2015), our full vocabulary size is 500k, we use AdaDelta (Zeiler, 2012), and mini-batch size is 80. Given the training set, we first run the ‘fast align’ (Dyer et al., 2013) in one direction, and use the translation table as our word-to-word dictionary. Then we run the reverse direction and apply ‘grow-diag-final-and’ heuristics to get the alignment. The phrase table is extracted with a standard algorithm in Moses (Koehn et al., 2007). In the decoding procedure, our method is very similar to the ‘candidate list’ of Jean et al. (2015), exce"
P16-2021,P15-1001,0,0.797784,"ts sentencelevel or batch-level vocabulary. Thus, we reduce both the computing time and the memory usage. Our method simply takes into account the translation options of each word or phrase in the source sentence, and picks a very small target vocabulary for each sentence based on a wordto-word translation model or a bilingual phrase library learned from a traditional machine translation model. Experimental results on the large-scale English-toFrench task show that our method achieves better translation performance by 1 BLEU point over the large vocabulary neural machine translation system of Jean et al. (2015). 1 Introduction Neural machine translation (NMT) (Bahdanau et al., 2014) has gained popularity in recent two years. But it can only handle a small vocabulary size due to the computational complexity. In order to capture rich language phenomena and have a better word coverage, neural machine translation models have to use a large vocabulary. Jean et al. (2015) alleviated the large vocabulary issue by proposing an approach that partitions the training corpus and defines a subset of the full target vocabulary for each partition. Thus, they only use a subset vocabulary for each partition in the t"
P16-2021,D14-1179,0,\N,Missing
P17-2002,2006.amta-papers.8,0,0.0358799,"istinguish different non-terminal instances. Figure 2 shows an example derivation process for the sentence “the boy wants to go” given the rule set in Table 1. Given the start symbol S, which is first replaced with X1 , rule (c) is applied to generate “X2 to go” and its AMR counterpart. Then rule (b) is used to generate “X3 wants” and its AMR counterpart from X2 . Finally, rule (a) is used to generate “the boy” and its AMR counterpart from X3 . Our graph-to-string rules are inspired by synchronous grammars for machine translation (Wu, 1997; Yamada and Knight, 2002; Gildea, 2003; Chiang, 2005; Huang et al., 2006; Liu et al., 2006; Shen et al., 2008; Xie et al., 2011; Meng et al., 2013). Synchronous Node Replacement Grammar Grammar Definition A synchronous node replacement grammar (NRG) is a rewriting formalism: G = hN, Σ, ∆, P, Si, where N is a finite set of nonterminals, Σ and ∆ are finite sets of terminal symbols for the source and target sides, respectively. S ∈ N is the start symbol, and P is a finite set of productions. Each instance of P takes the form Xi → (hF, Ei, ∼), where Xi ∈ N is a nonterminal node, F is a rooted, connected AMR fragment with edge labels over Σ and node labels over N ∪ Σ,"
P17-2002,D15-1198,0,0.0124762,"s. Encoding many semantic phenomena into a graph structure, AMR is useful for NLP tasks such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015). AMR-to-text generation is challenging as function words and syntactic structures are abstracted away, making an AMR graph correspond to multiple realizations. Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016; Peng et al., 2017), there has been little work on AMR-to-text generation (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016). 7 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 7–13 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2002 ARG1 (c) AMR: #S# want-01 (root) #X2# #X1# go-01 String: {#S#} {#X1#} #X3# {#X2# to go} ARG1 (a) ARG0 ARG0 want-01 ARG1 (b) go-01 ARG0 {#X3#"
P17-2002,C12-1083,0,0.0608432,"Missing"
P17-2002,W13-2322,0,0.227385,"nto a sentence as a traveling salesman problem, using local features and a language model to rank candidate sentences. However, their method does not learn hierarchical structural correspondences between AMR graphs and strings. We propose to leverage the advantages of hierarchical rules without suffering from graph-to-tree errors by directly learning graph-to-string rules. As shown in Figure 1, we learn a synchronous node replacement grammar (NRG) from a corpus of aligned AMR and sentence pairs. At test time, we apply a graph transducer to collapse input Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism encoding the meaning of a sentence as a rooted, directed graph. AMR uses a graph to represent meaning, where nodes (such as “boy”, “want-01”) represent concepts, and edges (such as “ARG0”, “ARG1”) represent relations between concepts. Encoding many semantic phenomena into a graph structure, AMR is useful for NLP tasks such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015). AMR-to-text generation is challenging as function words and syn"
P17-2002,N03-1017,0,0.0322761,"rule list R (Lines 5 and 9), which will be further normalized to obtain the final induced rule set. 2.3 t where g denotes the input AMR, fi (·, ·) and wi represent a feature and the corresponding weight, respectively. The feature set that we adopt includes phrase-to-graph and graph-to-phrase translation probabilities and their corresponding lexicalized translation probabilities (section 3.1), language model score, word count, rule count, reordering model score (section 3.2) and moving distance (section 3.3). The language model score, word count and phrase count features are adopted from SMT (Koehn et al., 2003; Chiang, 2005). We perform bottom-up search to transduce input AMRs to surface strings. Each hypothesis contains the current AMR graph, translations of collapsed subgraphs, the feature vector and the current model score. Beam search is adopted, where hypotheses with the same number of collapsed edges and nodes are put into the same beam. Concept Rules and Glue Rules In addition to induced rules, we adopt concept rules (Song et al., 2016) and graph glue rules to ensure existence of derivations. For a concept rule, F is a single node in the input AMR graph, and E is a morphological string of th"
P17-2002,P05-1033,0,0.0721117,"ubscripts to distinguish different non-terminal instances. Figure 2 shows an example derivation process for the sentence “the boy wants to go” given the rule set in Table 1. Given the start symbol S, which is first replaced with X1 , rule (c) is applied to generate “X2 to go” and its AMR counterpart. Then rule (b) is used to generate “X3 wants” and its AMR counterpart from X2 . Finally, rule (a) is used to generate “the boy” and its AMR counterpart from X3 . Our graph-to-string rules are inspired by synchronous grammars for machine translation (Wu, 1997; Yamada and Knight, 2002; Gildea, 2003; Chiang, 2005; Huang et al., 2006; Liu et al., 2006; Shen et al., 2008; Xie et al., 2011; Meng et al., 2013). Synchronous Node Replacement Grammar Grammar Definition A synchronous node replacement grammar (NRG) is a rewriting formalism: G = hN, Σ, ∆, P, Si, where N is a finite set of nonterminals, Σ and ∆ are finite sets of terminal symbols for the source and target sides, respectively. S ∈ N is the start symbol, and P is a finite set of productions. Each instance of P takes the form Xi → (hF, Ei, ∼), where Xi ∈ N is a nonterminal node, F is a rooted, connected AMR fragment with edge labels over Σ and node"
P17-2002,W15-4502,0,0.0386307,"to collapse input Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism encoding the meaning of a sentence as a rooted, directed graph. AMR uses a graph to represent meaning, where nodes (such as “boy”, “want-01”) represent concepts, and edges (such as “ARG0”, “ARG1”) represent relations between concepts. Encoding many semantic phenomena into a graph structure, AMR is useful for NLP tasks such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015). AMR-to-text generation is challenging as function words and syntactic structures are abstracted away, making an AMR graph correspond to multiple realizations. Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016; Peng et al., 2017), there has been little work on AMR-to-text generation (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016). 7 Proceedings of the 55th Annual Meeting of the"
P17-2002,P06-1077,0,0.016854,"non-terminal instances. Figure 2 shows an example derivation process for the sentence “the boy wants to go” given the rule set in Table 1. Given the start symbol S, which is first replaced with X1 , rule (c) is applied to generate “X2 to go” and its AMR counterpart. Then rule (b) is used to generate “X3 wants” and its AMR counterpart from X2 . Finally, rule (a) is used to generate “the boy” and its AMR counterpart from X3 . Our graph-to-string rules are inspired by synchronous grammars for machine translation (Wu, 1997; Yamada and Knight, 2002; Gildea, 2003; Chiang, 2005; Huang et al., 2006; Liu et al., 2006; Shen et al., 2008; Xie et al., 2011; Meng et al., 2013). Synchronous Node Replacement Grammar Grammar Definition A synchronous node replacement grammar (NRG) is a rewriting formalism: G = hN, Σ, ∆, P, Si, where N is a finite set of nonterminals, Σ and ∆ are finite sets of terminal symbols for the source and target sides, respectively. S ∈ N is the start symbol, and P is a finite set of productions. Each instance of P takes the form Xi → (hF, Ei, ∼), where Xi ∈ N is a nonterminal node, F is a rooted, connected AMR fragment with edge labels over Σ and node labels over N ∪ Σ, E is a correspondi"
P17-2002,D13-1108,1,0.889096,"Missing"
P17-2002,N16-1087,0,0.26532,"versity of Technology and Design Abstract This paper addresses the task of AMR-totext generation by leveraging synchronous node replacement grammar. During training, graph-to-string rules are learned using a heuristic extraction algorithm. At test time, a graph transducer is applied to collapse input AMRs and generate output sentences. Evaluated on a standard benchmark, our method gives the state-of-the-art result. 1 #X3# #X3# ARG1 #X2# go-01 #X2# ARG0 ARG0 #X1# ARG0 boy want-01 ARG0 want-01 ARG0 ARG1 #X1# go-01 ARG1 go-01 the boy wants to go Figure 1: Graph-to-string derivation. Introduction Flanigan et al. (2016) transform a given AMR graph into a spanning tree, before translating it to a sentence using a tree-to-string transducer. Their method leverages existing machine translation techniques, capturing hierarchical correspondences between the spanning tree and the surface string. However, it suffers from error propagation since the output is constrained given a spanning tree due to the projective correspondence between them. Information loss in the graph-to-tree transformation step cannot be recovered. Song et al. (2016) directly generate sentences using graphfragment-to-string rules. They cast the"
P17-2002,P14-1134,0,0.0267127,"oy”, “want-01”) represent concepts, and edges (such as “ARG0”, “ARG1”) represent relations between concepts. Encoding many semantic phenomena into a graph structure, AMR is useful for NLP tasks such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015). AMR-to-text generation is challenging as function words and syntactic structures are abstracted away, making an AMR graph correspond to multiple realizations. Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016; Peng et al., 2017), there has been little work on AMR-to-text generation (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016). 7 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 7–13 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2002 ARG1 (c) AMR: #S# want-01 (root) #X2#"
P17-2002,P03-1021,0,0.0915657,"want-01 (root) #X2# #X1# go-01 String: {#S#} {#X1#} #X3# {#X2# to go} ARG1 (a) ARG0 ARG0 want-01 ARG1 (b) go-01 ARG0 {#X3# wants to go} ARG0 boy go-01 ARG0 {the boy wants to go} Figure 2: Example deduction procedure ID. (a) (b) (c) (d) F (b / boy) (w / want-01 :ARG0 (X / #X#)) (X / #X# :ARG1 (g / go-01 :ARG0 X)) (w / want-01 :ARG0 (b / boy)) E the boy 1 #X# wants 2 3 #X# to go 4 5 6 the boy wants 7 8 Table 1: Example rule set 9 10 11 AMR graphs and generate output strings according to the learned grammar. Our system makes use of a log-linear model with real-valued features, tuned using MERT (Och, 2003), and beam search decoding. It gives a BLEU score of 25.62 on LDC2015E86, which is the state-of-the-art on this dataset. 2 2.1 12 13 Data: training corpus C Result: rule instances R R ← []; for (Sent, AM R, ∼) in C do Rcur ← F RAGMENT E XTRACT(Sent,AM R,∼); for ri in Rcur do R.APPEND(ri ) ; for rj in Rcur /{ri } do if ri .C ONTAINS(rj ) then rij ← ri .COLLAPSE(rj ); R.APPEND(rij ) ; end end end end Algorithm 1: Rule extraction (2005), we use only one nonterminal X in addition to S, and use subscripts to distinguish different non-terminal instances. Figure 2 shows an example derivation process"
P17-2002,P03-1011,1,0.658507,"o S, and use subscripts to distinguish different non-terminal instances. Figure 2 shows an example derivation process for the sentence “the boy wants to go” given the rule set in Table 1. Given the start symbol S, which is first replaced with X1 , rule (c) is applied to generate “X2 to go” and its AMR counterpart. Then rule (b) is used to generate “X3 wants” and its AMR counterpart from X2 . Finally, rule (a) is used to generate “the boy” and its AMR counterpart from X3 . Our graph-to-string rules are inspired by synchronous grammars for machine translation (Wu, 1997; Yamada and Knight, 2002; Gildea, 2003; Chiang, 2005; Huang et al., 2006; Liu et al., 2006; Shen et al., 2008; Xie et al., 2011; Meng et al., 2013). Synchronous Node Replacement Grammar Grammar Definition A synchronous node replacement grammar (NRG) is a rewriting formalism: G = hN, Σ, ∆, P, Si, where N is a finite set of nonterminals, Σ and ∆ are finite sets of terminal symbols for the source and target sides, respectively. S ∈ N is the start symbol, and P is a finite set of productions. Each instance of P takes the form Xi → (hF, Ei, ∼), where Xi ∈ N is a nonterminal node, F is a rooted, connected AMR fragment with edge labels o"
P17-2002,P02-1040,0,0.101213,"connected by edge l. 3.3 Dev 21.12 23.00 25.24 16.75 23.99 23.48 25.09 Experiments Setup We use LDC2015E86 as our experimental dataset, which contains 16833 training, 1368 dev and 1371 test instances. Each instance contains a sentence, an AMR graph and the alignment generated by a heuristic aligner. Rules are extracted from the training data, and model parameters are tuned on the dev set. For tuning and testing, we filter out sentences with more than 30 words, resulting in 1103 dev instances and 1055 test instances. We train a 4-gram language model (LM) on gigaword (LDC2011T07), and use BLEU (Papineni et al., 2002) as the evaluation metric. MERT is used (Och, 2003) to tune model parameters on k-best outputs on the devset, where k is set 50. We investigate the effectiveness of rules and features by ablation tests: “NoInducedRule” does not adopt induced rules, “NoConceptRule” does not adopt concept rules, “NoMovingDistance” does not adopt the moving distance feature, and “NoReorderModel” disables the reordering model. Given an AMR graph, if NoConceptRule cannot produce a legal derivation, we concatenate 4.3 Grammar analysis We have shown the effectiveness of our synchronous node replacement grammar (SNRG)"
P17-2002,P16-1001,0,0.0154203,"aph structure, AMR is useful for NLP tasks such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015). AMR-to-text generation is challenging as function words and syntactic structures are abstracted away, making an AMR graph correspond to multiple realizations. Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016; Peng et al., 2017), there has been little work on AMR-to-text generation (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016). 7 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 7–13 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2002 ARG1 (c) AMR: #S# want-01 (root) #X2# #X1# go-01 String: {#S#} {#X1#} #X3# {#X2# to go} ARG1 (a) ARG0 ARG0 want-01 ARG1 (b) go-01 ARG0 {#X3# wants to go} ARG0 boy go-01 ARG0 {the boy want"
P17-2002,K15-1004,1,0.839012,"ges (such as “ARG0”, “ARG1”) represent relations between concepts. Encoding many semantic phenomena into a graph structure, AMR is useful for NLP tasks such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015). AMR-to-text generation is challenging as function words and syntactic structures are abstracted away, making an AMR graph correspond to multiple realizations. Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016; Peng et al., 2017), there has been little work on AMR-to-text generation (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016). 7 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 7–13 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2002 ARG1 (c) AMR: #S# want-01 (root) #X2# #X1# go-01 String: {#S#} {#X1#} #X3#"
P17-2002,P15-1143,0,0.0161628,"antic phenomena into a graph structure, AMR is useful for NLP tasks such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015). AMR-to-text generation is challenging as function words and syntactic structures are abstracted away, making an AMR graph correspond to multiple realizations. Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016; Peng et al., 2017), there has been little work on AMR-to-text generation (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016). 7 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 7–13 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2002 ARG1 (c) AMR: #S# want-01 (root) #X2# #X1# go-01 String: {#S#} {#X1#} #X3# {#X2# to go} ARG1 (a) ARG0 ARG0 want-01 ARG1 (b) go-01 ARG0 {#X3# wants to go} ARG0 boy go"
P17-2002,E17-1035,1,0.853477,"s such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015). AMR-to-text generation is challenging as function words and syntactic structures are abstracted away, making an AMR graph correspond to multiple realizations. Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016; Peng et al., 2017), there has been little work on AMR-to-text generation (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016). 7 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 7–13 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2002 ARG1 (c) AMR: #S# want-01 (root) #X2# #X1# go-01 String: {#S#} {#X1#} #X3# {#X2# to go} ARG1 (a) ARG0 ARG0 want-01 ARG1 (b) go-01 ARG0 {#X3# wants to go} ARG0 boy go-01 ARG0 {the boy wants to go} Figure 2: Example deduction pr"
P17-2002,W16-6603,0,0.200621,"), summarization (Takase et al., 2016) and event detection (Li et al., 2015). AMR-to-text generation is challenging as function words and syntactic structures are abstracted away, making an AMR graph correspond to multiple realizations. Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016; Peng et al., 2017), there has been little work on AMR-to-text generation (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016). 7 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 7–13 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2002 ARG1 (c) AMR: #S# want-01 (root) #X2# #X1# go-01 String: {#S#} {#X1#} #X3# {#X2# to go} ARG1 (a) ARG0 ARG0 want-01 ARG1 (b) go-01 ARG0 {#X3# wants to go} ARG0 boy go-01 ARG0 {the boy wants to go} Figure 2: Example deduction procedure ID. (a) (b) (c) (d) F (b / boy) (w / want-01 :ARG0 (X / #X#)) (X / #X# :ARG1 (g / go-01 :ARG0 X)) (w / want-01 :ARG0"
P17-2002,D16-1065,0,0.0438864,"useful for NLP tasks such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015). AMR-to-text generation is challenging as function words and syntactic structures are abstracted away, making an AMR graph correspond to multiple realizations. Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016; Peng et al., 2017), there has been little work on AMR-to-text generation (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016). 7 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 7–13 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2002 ARG1 (c) AMR: #S# want-01 (root) #X2# #X1# go-01 String: {#S#} {#X1#} #X3# {#X2# to go} ARG1 (a) ARG0 ARG0 want-01 ARG1 (b) go-01 ARG0 {#X3# wants to go} ARG0 boy go-01 ARG0 {the boy wants to go} Figure 2:"
P17-2002,D15-1136,0,0.0142351,"ons between concepts. Encoding many semantic phenomena into a graph structure, AMR is useful for NLP tasks such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015). AMR-to-text generation is challenging as function words and syntactic structures are abstracted away, making an AMR graph correspond to multiple realizations. Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016; Peng et al., 2017), there has been little work on AMR-to-text generation (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016). 7 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 7–13 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2002 ARG1 (c) AMR: #S# want-01 (root) #X2# #X1# go-01 String: {#S#} {#X1#} #X3# {#X2# to go} ARG1 (a) ARG0 ARG0 want-01 ARG1"
P17-2002,P08-1066,0,0.00990465,"ances. Figure 2 shows an example derivation process for the sentence “the boy wants to go” given the rule set in Table 1. Given the start symbol S, which is first replaced with X1 , rule (c) is applied to generate “X2 to go” and its AMR counterpart. Then rule (b) is used to generate “X3 wants” and its AMR counterpart from X2 . Finally, rule (a) is used to generate “the boy” and its AMR counterpart from X3 . Our graph-to-string rules are inspired by synchronous grammars for machine translation (Wu, 1997; Yamada and Knight, 2002; Gildea, 2003; Chiang, 2005; Huang et al., 2006; Liu et al., 2006; Shen et al., 2008; Xie et al., 2011; Meng et al., 2013). Synchronous Node Replacement Grammar Grammar Definition A synchronous node replacement grammar (NRG) is a rewriting formalism: G = hN, Σ, ∆, P, Si, where N is a finite set of nonterminals, Σ and ∆ are finite sets of terminal symbols for the source and target sides, respectively. S ∈ N is the start symbol, and P is a finite set of productions. Each instance of P takes the form Xi → (hF, Ei, ∼), where Xi ∈ N is a nonterminal node, F is a rooted, connected AMR fragment with edge labels over Σ and node labels over N ∪ Σ, E is a corresponding target string ov"
P17-2002,D16-1224,1,0.913448,"o-01 the boy wants to go Figure 1: Graph-to-string derivation. Introduction Flanigan et al. (2016) transform a given AMR graph into a spanning tree, before translating it to a sentence using a tree-to-string transducer. Their method leverages existing machine translation techniques, capturing hierarchical correspondences between the spanning tree and the surface string. However, it suffers from error propagation since the output is constrained given a spanning tree due to the projective correspondence between them. Information loss in the graph-to-tree transformation step cannot be recovered. Song et al. (2016) directly generate sentences using graphfragment-to-string rules. They cast the task of finding a sequence of disjoint rules to transduce an AMR graph into a sentence as a traveling salesman problem, using local features and a language model to rank candidate sentences. However, their method does not learn hierarchical structural correspondences between AMR graphs and strings. We propose to leverage the advantages of hierarchical rules without suffering from graph-to-tree errors by directly learning graph-to-string rules. As shown in Figure 1, we learn a synchronous node replacement grammar (N"
P17-2002,D16-1112,0,0.0090019,"At test time, we apply a graph transducer to collapse input Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism encoding the meaning of a sentence as a rooted, directed graph. AMR uses a graph to represent meaning, where nodes (such as “boy”, “want-01”) represent concepts, and edges (such as “ARG0”, “ARG1”) represent relations between concepts. Encoding many semantic phenomena into a graph structure, AMR is useful for NLP tasks such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015). AMR-to-text generation is challenging as function words and syntactic structures are abstracted away, making an AMR graph correspond to multiple realizations. Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016; Peng et al., 2017), there has been little work on AMR-to-text generation (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016). 7 Proceedi"
P17-2002,W15-3504,0,0.363273,"Missing"
P17-2002,N15-3006,0,0.0208597,", “ARG1”) represent relations between concepts. Encoding many semantic phenomena into a graph structure, AMR is useful for NLP tasks such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015). AMR-to-text generation is challenging as function words and syntactic structures are abstracted away, making an AMR graph correspond to multiple realizations. Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016; Peng et al., 2017), there has been little work on AMR-to-text generation (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016). 7 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 7–13 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2002 ARG1 (c) AMR: #S# want-01 (root) #X2# #X1# go-01 String: {#S#} {#X1#} #X3# {#X2# to go} ARG1 (a) ARG0"
P17-2002,N15-1040,0,0.0145351,"nt concepts, and edges (such as “ARG0”, “ARG1”) represent relations between concepts. Encoding many semantic phenomena into a graph structure, AMR is useful for NLP tasks such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015). AMR-to-text generation is challenging as function words and syntactic structures are abstracted away, making an AMR graph correspond to multiple realizations. Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016; Peng et al., 2017), there has been little work on AMR-to-text generation (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016). 7 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 7–13 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2002 ARG1 (c) AMR: #S# want-01 (root) #X2# #X1# go-01 String:"
P17-2002,J97-3002,0,0.0460913,"nly one nonterminal X in addition to S, and use subscripts to distinguish different non-terminal instances. Figure 2 shows an example derivation process for the sentence “the boy wants to go” given the rule set in Table 1. Given the start symbol S, which is first replaced with X1 , rule (c) is applied to generate “X2 to go” and its AMR counterpart. Then rule (b) is used to generate “X3 wants” and its AMR counterpart from X2 . Finally, rule (a) is used to generate “the boy” and its AMR counterpart from X3 . Our graph-to-string rules are inspired by synchronous grammars for machine translation (Wu, 1997; Yamada and Knight, 2002; Gildea, 2003; Chiang, 2005; Huang et al., 2006; Liu et al., 2006; Shen et al., 2008; Xie et al., 2011; Meng et al., 2013). Synchronous Node Replacement Grammar Grammar Definition A synchronous node replacement grammar (NRG) is a rewriting formalism: G = hN, Σ, ∆, P, Si, where N is a finite set of nonterminals, Σ and ∆ are finite sets of terminal symbols for the source and target sides, respectively. S ∈ N is the start symbol, and P is a finite set of productions. Each instance of P takes the form Xi → (hF, Ei, ∼), where Xi ∈ N is a nonterminal node, F is a rooted, co"
P17-2002,D11-1020,0,0.0167829,"ws an example derivation process for the sentence “the boy wants to go” given the rule set in Table 1. Given the start symbol S, which is first replaced with X1 , rule (c) is applied to generate “X2 to go” and its AMR counterpart. Then rule (b) is used to generate “X3 wants” and its AMR counterpart from X2 . Finally, rule (a) is used to generate “the boy” and its AMR counterpart from X3 . Our graph-to-string rules are inspired by synchronous grammars for machine translation (Wu, 1997; Yamada and Knight, 2002; Gildea, 2003; Chiang, 2005; Huang et al., 2006; Liu et al., 2006; Shen et al., 2008; Xie et al., 2011; Meng et al., 2013). Synchronous Node Replacement Grammar Grammar Definition A synchronous node replacement grammar (NRG) is a rewriting formalism: G = hN, Σ, ∆, P, Si, where N is a finite set of nonterminals, Σ and ∆ are finite sets of terminal symbols for the source and target sides, respectively. S ∈ N is the start symbol, and P is a finite set of productions. Each instance of P takes the form Xi → (hF, Ei, ∼), where Xi ∈ N is a nonterminal node, F is a rooted, connected AMR fragment with edge labels over Σ and node labels over N ∪ Σ, E is a corresponding target string over N ∪ ∆ and ∼ den"
P17-2002,P02-1039,0,\N,Missing
P17-2002,P13-2131,0,\N,Missing
P18-1150,W13-2322,0,0.343839,"le to model non-local semantic information, a sequence LSTM can lose information from the AMR graph structure, and thus faces challenges with large graphs, which result in long sequences. We introduce a neural graph-to-sequence model, using a novel LSTM structure for directly encoding graph-level semantics. On a standard benchmark, our model shows superior results to existing methods in the literature. 1 :ARG0 :ARG1 :ARG2 person :name name genius :op1 ""Ryan"" Figure 1: An example of AMR graph meaning “Ryan’s description of himself: a genius.” Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism that encodes the meaning of a sentence as a rooted, directed graph. Figure 1 shows an AMR graph in which the nodes (such as “describe-01” and “person”) represent the concepts, and edges (such as “:ARG0” and “:name”) represent the relations between concepts they connect. AMR has been proven helpful on other NLP tasks, such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015). The task of AMR-to-text generation is to produce a text with the"
P18-1150,D17-1209,0,0.0587751,"to provide research and institutes in the center . G2S: the agreement provides the staff of research centers and funding . G2S+CP: the agreement provides the staff of the research center and the funding . Table 3: Example system outputs. graphs by breadth-first traversal, and then use a phrase-based machine translation system2 to generate results by translating linearized sequences. Prior work using graph neural networks for NLP include the use graph convolutional networks (GCN) (Kipf and Welling, 2017) for semantic role labeling (Marcheggiani and Titov, 2017) and neural machine translation (Bastings et al., 2017). Both GCN and the graph LSTM update node states by exchanging information between neighboring nodes within each iteration. However, our graph state LSTM adopts gated operations for making updates, while GCN uses a linear transformation. Intuitively, the former has better learning power than the later. Another major difference is that our graph state LSTM keeps a cell vector for each node to remember all history. The contrast 1623 2 http://www.statmt.org/moses/ between our model with GCN is reminiscent of the contrast between RNN and CNN. We leave empirical comparison of their effectiveness to"
P18-1150,S16-1186,0,0.439676,"al, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015). The task of AMR-to-text generation is to produce a text with the same meaning as a given input AMR graph. The task is challenging as word tenses and function words are abstracted away when constructing AMR graphs from texts. The translation from AMR nodes to text phrases can be far from literal. For example, shown in Figure 1, “Ryan” is represented as “(p / person :name (n / name :op1 “Ryan”))”, and “description of” is represented as “(d / describe-01 :ARG1 )”. While initial work used statistical approaches (Flanigan et al., 2016b; Pourdamghani et al., 2016; Song et al., 2017; Lampouras and Vlachos, 2017; Mille et al., 2017; Gruzitis et al., 2017), recent research has demonstrated the success of deep learning, and in particular the sequence-to-sequence model (Sutskever et al., 2014), which has achieved the state-of-the-art results on AMR-to-text generation (Konstas et al., 2017). One limitation of sequence-to-sequence models, however, is that they require serialization of input AMR graphs, which adds to the challenge of representing graph structure information, especially when the graph is large. In particular, closel"
P18-1150,N16-1087,0,0.285942,"al, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015). The task of AMR-to-text generation is to produce a text with the same meaning as a given input AMR graph. The task is challenging as word tenses and function words are abstracted away when constructing AMR graphs from texts. The translation from AMR nodes to text phrases can be far from literal. For example, shown in Figure 1, “Ryan” is represented as “(p / person :name (n / name :op1 “Ryan”))”, and “description of” is represented as “(d / describe-01 :ARG1 )”. While initial work used statistical approaches (Flanigan et al., 2016b; Pourdamghani et al., 2016; Song et al., 2017; Lampouras and Vlachos, 2017; Mille et al., 2017; Gruzitis et al., 2017), recent research has demonstrated the success of deep learning, and in particular the sequence-to-sequence model (Sutskever et al., 2014), which has achieved the state-of-the-art results on AMR-to-text generation (Konstas et al., 2017). One limitation of sequence-to-sequence models, however, is that they require serialization of input AMR graphs, which adds to the challenge of representing graph structure information, especially when the graph is large. In particular, closel"
P18-1150,S17-2159,0,0.0529915,"n is to produce a text with the same meaning as a given input AMR graph. The task is challenging as word tenses and function words are abstracted away when constructing AMR graphs from texts. The translation from AMR nodes to text phrases can be far from literal. For example, shown in Figure 1, “Ryan” is represented as “(p / person :name (n / name :op1 “Ryan”))”, and “description of” is represented as “(d / describe-01 :ARG1 )”. While initial work used statistical approaches (Flanigan et al., 2016b; Pourdamghani et al., 2016; Song et al., 2017; Lampouras and Vlachos, 2017; Mille et al., 2017; Gruzitis et al., 2017), recent research has demonstrated the success of deep learning, and in particular the sequence-to-sequence model (Sutskever et al., 2014), which has achieved the state-of-the-art results on AMR-to-text generation (Konstas et al., 2017). One limitation of sequence-to-sequence models, however, is that they require serialization of input AMR graphs, which adds to the challenge of representing graph structure information, especially when the graph is large. In particular, closely-related nodes, such as parents, children and siblings can be far away after serialization. It can be difficult for a l"
P18-1150,P16-1154,0,0.593108,"tional Linguistics (Long Papers), pages 1616–1626 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics To capture non-local information, the encoder performs graph state transition by information exchange between connected nodes, with a graph state consisting of all node states. Multiple recurrent transition steps are taken so that information can propagate non-locally, and LSTM (Hochreiter and Schmidhuber, 1997) is used to avoid gradient diminishing and bursting in the recurrent process. The decoder is an attention-based LSTM model with a copy mechanism (Gu et al., 2016; Gulcehre et al., 2016), which helps copy sparse tokens (such as numbers and named entities) from the input. Trained on a standard dataset (LDC2015E86), our model surpasses a strong sequence-tosequence baseline by 2.3 BLEU points, demonstrating the advantage of graph-to-sequence models for AMR-to-text generation compared to sequence-to-sequence models. Our final model achieves a BLEU score of 23.3 on the test set, which is 1.3 points higher than the existing state of the art (Konstas et al., 2017) trained on the same dataset. When using gigaword sentences as additional training data, our mode"
P18-1150,P16-1014,0,0.452163,"s (Long Papers), pages 1616–1626 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics To capture non-local information, the encoder performs graph state transition by information exchange between connected nodes, with a graph state consisting of all node states. Multiple recurrent transition steps are taken so that information can propagate non-locally, and LSTM (Hochreiter and Schmidhuber, 1997) is used to avoid gradient diminishing and bursting in the recurrent process. The decoder is an attention-based LSTM model with a copy mechanism (Gu et al., 2016; Gulcehre et al., 2016), which helps copy sparse tokens (such as numbers and named entities) from the input. Trained on a standard dataset (LDC2015E86), our model surpasses a strong sequence-tosequence baseline by 2.3 BLEU points, demonstrating the advantage of graph-to-sequence models for AMR-to-text generation compared to sequence-to-sequence models. Our final model achieves a BLEU score of 23.3 on the test set, which is 1.3 points higher than the existing state of the art (Konstas et al., 2017) trained on the same dataset. When using gigaword sentences as additional training data, our model is consistently better"
P18-1150,C12-1083,0,0.109094,"Missing"
P18-1150,N03-1017,0,0.00861135,"We perform a similar experiment for the Seq2seq+copy baseline by only executing singledirectional LSTM for the encoder. We observe BLEU scores of 11.8 and 12.7 using only forward or backward LSTM, respectively. This is consistent with our graph model in that execution using only one direction leads to a huge performance drop. The contrast is also reminiscent of using the normal input versus the reversed input in neural machine translation (Sutskever et al., 2014). 5.5 model trained with the anonymized data. PBMT (Pourdamghani et al., 2016) adopts a phrase-based model for machine translation (Koehn et al., 2003) on the input of linearized AMR graph, SNRG (Song et al., 2017) uses synchronous node replacement grammar for parsing the AMR graph while generating the text, and Tree2Str (Flanigan et al., 2016b) converts AMR graphs into trees by splitting the re-entrances before using a tree transducer to generate the results. Graph2seq+charLSTM+copy achieves a BLEU score of 23.3, which is 1.3 points better than MSeq2seq+Anon trained on the same AMR corpus. In addition, our model without character LSTM is still 0.7 BLEU points higher than MSeq2seq+Anon. Note that MSeq2seq+Anon relies on anonymization, which"
P18-1150,P17-1014,0,0.218578,"s can be far from literal. For example, shown in Figure 1, “Ryan” is represented as “(p / person :name (n / name :op1 “Ryan”))”, and “description of” is represented as “(d / describe-01 :ARG1 )”. While initial work used statistical approaches (Flanigan et al., 2016b; Pourdamghani et al., 2016; Song et al., 2017; Lampouras and Vlachos, 2017; Mille et al., 2017; Gruzitis et al., 2017), recent research has demonstrated the success of deep learning, and in particular the sequence-to-sequence model (Sutskever et al., 2014), which has achieved the state-of-the-art results on AMR-to-text generation (Konstas et al., 2017). One limitation of sequence-to-sequence models, however, is that they require serialization of input AMR graphs, which adds to the challenge of representing graph structure information, especially when the graph is large. In particular, closely-related nodes, such as parents, children and siblings can be far away after serialization. It can be difficult for a linear recurrent neural network to automatically induce their original connections from bracketed string forms. To address this issue, we introduce a novel graph-to-sequence model, where a graph-state LSTM is used to encode AMR structure"
P18-1150,S17-2096,0,0.214613,"Missing"
P18-1150,W15-4502,0,0.0367213,"nius.” Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism that encodes the meaning of a sentence as a rooted, directed graph. Figure 1 shows an AMR graph in which the nodes (such as “describe-01” and “person”) represent the concepts, and edges (such as “:ARG0” and “:name”) represent the relations between concepts they connect. AMR has been proven helpful on other NLP tasks, such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015). The task of AMR-to-text generation is to produce a text with the same meaning as a given input AMR graph. The task is challenging as word tenses and function words are abstracted away when constructing AMR graphs from texts. The translation from AMR nodes to text phrases can be far from literal. For example, shown in Figure 1, “Ryan” is represented as “(p / person :name (n / name :op1 “Ryan”))”, and “description of” is represented as “(d / describe-01 :ARG1 )”. While initial work used statistical approaches (Flanigan et al., 2016b; Pourdamghani et al., 2016; Song et al., 2017; Lampouras and"
P18-1150,D17-1159,0,0.0610451,"rovide staff and funding for the research center . S2S: agreed to provide research and institutes in the center . G2S: the agreement provides the staff of research centers and funding . G2S+CP: the agreement provides the staff of the research center and the funding . Table 3: Example system outputs. graphs by breadth-first traversal, and then use a phrase-based machine translation system2 to generate results by translating linearized sequences. Prior work using graph neural networks for NLP include the use graph convolutional networks (GCN) (Kipf and Welling, 2017) for semantic role labeling (Marcheggiani and Titov, 2017) and neural machine translation (Bastings et al., 2017). Both GCN and the graph LSTM update node states by exchanging information between neighboring nodes within each iteration. However, our graph state LSTM adopts gated operations for making updates, while GCN uses a linear transformation. Intuitively, the former has better learning power than the later. Another major difference is that our graph state LSTM keeps a cell vector for each node to remember all history. The contrast 1623 2 http://www.statmt.org/moses/ between our model with GCN is reminiscent of the contrast between RNN and CNN."
P18-1150,P05-1012,0,0.0220157,"y incoming or outgoing edges are used. From the results, we can see that there is a huge drop when state transition is performed only with incoming or outgoing edges. Using edges of one direction, the node states only contain information of ancestors or descendants. On the other hand, node states contain information of ancestors, descendants, and siblings if edges of both directions are used. From the results, we can conclude that not only the ancestors and descendants, but also the siblings are important for modeling the AMR graphs. This is similar to observations on syntactic parsing tasks (McDonald et al., 2005), where sibling features are adopted. We perform a similar experiment for the Seq2seq+copy baseline by only executing singledirectional LSTM for the encoder. We observe BLEU scores of 11.8 and 12.7 using only forward or backward LSTM, respectively. This is consistent with our graph model in that execution using only one direction leads to a huge performance drop. The contrast is also reminiscent of using the normal input versus the reversed input in neural machine translation (Sutskever et al., 2014). 5.5 model trained with the anonymized data. PBMT (Pourdamghani et al., 2016) adopts a phrase-"
P18-1150,S17-2158,0,0.0361727,"MR-to-text generation is to produce a text with the same meaning as a given input AMR graph. The task is challenging as word tenses and function words are abstracted away when constructing AMR graphs from texts. The translation from AMR nodes to text phrases can be far from literal. For example, shown in Figure 1, “Ryan” is represented as “(p / person :name (n / name :op1 “Ryan”))”, and “description of” is represented as “(d / describe-01 :ARG1 )”. While initial work used statistical approaches (Flanigan et al., 2016b; Pourdamghani et al., 2016; Song et al., 2017; Lampouras and Vlachos, 2017; Mille et al., 2017; Gruzitis et al., 2017), recent research has demonstrated the success of deep learning, and in particular the sequence-to-sequence model (Sutskever et al., 2014), which has achieved the state-of-the-art results on AMR-to-text generation (Konstas et al., 2017). One limitation of sequence-to-sequence models, however, is that they require serialization of input AMR graphs, which adds to the challenge of representing graph structure information, especially when the graph is large. In particular, closely-related nodes, such as parents, children and siblings can be far away after serialization. It"
P18-1150,P02-1040,0,0.103679,"onstas et al. (2017) only works on the anonymized data. For training on both sampled data and LDC2015E86, we also follow the method of Konstas et al. (2017), which is fine-tuning the model on the AMR corpus after every epoch of pretraining on the gigaword data. 5.2 Settings We extract a vocabulary from the training set, which is shared by both the encoder and the decoder. The word embeddings are initialized from Glove pretrained word embeddings (Pennington et al., 2014) on Common Crawl, and are not updated during training. Following existing work, we evaluate the results with the BLEU metric (Papineni et al., 2002). For model hyperparameters, we set the graph state transition number as 9 according to development experiments. Each node takes information from at most 10 neighbors. The hidden vector sizes for both encoder and decoder are set to 300 (They are set to 600 for experiments using largescale automatic data). Both character embeddings and hidden layer sizes for character LSTMs are set 100, and at most 20 characters are taken for each graph node or linearized token. Data We use a standard AMR corpus (LDC2015E86) as our experimental dataset, which contains 16,833 instances for training, 1368 for dev"
P18-1150,Q17-1008,0,0.0361108,"te LSTM adopts gated operations for making updates, while GCN uses a linear transformation. Intuitively, the former has better learning power than the later. Another major difference is that our graph state LSTM keeps a cell vector for each node to remember all history. The contrast 1623 2 http://www.statmt.org/moses/ between our model with GCN is reminiscent of the contrast between RNN and CNN. We leave empirical comparison of their effectiveness to future work. In this work our main goal is to show that graph LSTM encoding of AMR is superior compared with sequence LSTM. Closest to our work, Peng et al. (2017) modeled syntactic and discourse structures using DAG LSTM, which can be viewed as extensions to tree LSTMs (Tai et al., 2015). The state update follows the sentence order for each node, and has sequential nature. Our state update is in parallel. In addition, Peng et al. (2017) split input graphs into separate DAGs before their method can be used. To our knowledge, we are the first to apply an LSTM structure to encode AMR graphs. The recurrent information exchange mechanism in our state transition process is remotely related to the idea of loopy belief propagation (LBP) (Murphy et al., 1999)."
P18-1150,D14-1162,0,0.0936009,"19.9 20.6 20.4 22.2 22.1 22.8 Time 35.4s 37.4s 39.7s 11.2s 11.1s 9.2s 16.3s Table 1: D EV BLEU scores and decoding times. AMRs, as the AMR parser of Konstas et al. (2017) only works on the anonymized data. For training on both sampled data and LDC2015E86, we also follow the method of Konstas et al. (2017), which is fine-tuning the model on the AMR corpus after every epoch of pretraining on the gigaword data. 5.2 Settings We extract a vocabulary from the training set, which is shared by both the encoder and the decoder. The word embeddings are initialized from Glove pretrained word embeddings (Pennington et al., 2014) on Common Crawl, and are not updated during training. Following existing work, we evaluate the results with the BLEU metric (Papineni et al., 2002). For model hyperparameters, we set the graph state transition number as 9 according to development experiments. Each node takes information from at most 10 neighbors. The hidden vector sizes for both encoder and decoder are set to 300 (They are set to 600 for experiments using largescale automatic data). Both character embeddings and hidden layer sizes for character LSTMs are set 100, and at most 20 characters are taken for each graph node or line"
P18-1150,W16-6603,0,0.403474,"(Takase et al., 2016) and event detection (Li et al., 2015). The task of AMR-to-text generation is to produce a text with the same meaning as a given input AMR graph. The task is challenging as word tenses and function words are abstracted away when constructing AMR graphs from texts. The translation from AMR nodes to text phrases can be far from literal. For example, shown in Figure 1, “Ryan” is represented as “(p / person :name (n / name :op1 “Ryan”))”, and “description of” is represented as “(d / describe-01 :ARG1 )”. While initial work used statistical approaches (Flanigan et al., 2016b; Pourdamghani et al., 2016; Song et al., 2017; Lampouras and Vlachos, 2017; Mille et al., 2017; Gruzitis et al., 2017), recent research has demonstrated the success of deep learning, and in particular the sequence-to-sequence model (Sutskever et al., 2014), which has achieved the state-of-the-art results on AMR-to-text generation (Konstas et al., 2017). One limitation of sequence-to-sequence models, however, is that they require serialization of input AMR graphs, which adds to the challenge of representing graph structure information, especially when the graph is large. In particular, closely-related nodes, such as par"
P18-1150,P17-1099,0,0.0409756,"tely related to the idea of loopy belief propagation (LBP) (Murphy et al., 1999). However, there are two major differences. First, messages between LSTM states are gated neural node values, rather than probabilities in LBP. Second, while the goal of LBP is to estimate marginal probabilities, the goal of information exchange between graph states in our LSTM is to find neural representation features, which are directly optimized by a task objective. In addition to NMT (Gulcehre et al., 2016), the copy mechanism has been shown effective on tasks such as dialogue (Gu et al., 2016), summarization (See et al., 2017) and question generation (Song et al., 2018). We investigate the copy mechanism on AMR-to-text generation. 7 Conclusion We introduced a novel graph-to-sequence model for AMR-to-text generation. Compared to sequence-to-sequence models, which require linearization of AMR before decoding, a graph LSTM is leveraged to directly model full AMR structure. Allowing high parallelization, the graph encoder is more efficient than the sequence encoder. In our experiments, the graph model outperforms a strong sequence-to-sequence model, achieving the best performance. Acknowledgments We thank the anonymize"
P18-1150,P17-2002,1,0.902838,"event detection (Li et al., 2015). The task of AMR-to-text generation is to produce a text with the same meaning as a given input AMR graph. The task is challenging as word tenses and function words are abstracted away when constructing AMR graphs from texts. The translation from AMR nodes to text phrases can be far from literal. For example, shown in Figure 1, “Ryan” is represented as “(p / person :name (n / name :op1 “Ryan”))”, and “description of” is represented as “(d / describe-01 :ARG1 )”. While initial work used statistical approaches (Flanigan et al., 2016b; Pourdamghani et al., 2016; Song et al., 2017; Lampouras and Vlachos, 2017; Mille et al., 2017; Gruzitis et al., 2017), recent research has demonstrated the success of deep learning, and in particular the sequence-to-sequence model (Sutskever et al., 2014), which has achieved the state-of-the-art results on AMR-to-text generation (Konstas et al., 2017). One limitation of sequence-to-sequence models, however, is that they require serialization of input AMR graphs, which adds to the challenge of representing graph structure information, especially when the graph is large. In particular, closely-related nodes, such as parents, children and"
P18-1150,N18-2090,1,0.825928,"opagation (LBP) (Murphy et al., 1999). However, there are two major differences. First, messages between LSTM states are gated neural node values, rather than probabilities in LBP. Second, while the goal of LBP is to estimate marginal probabilities, the goal of information exchange between graph states in our LSTM is to find neural representation features, which are directly optimized by a task objective. In addition to NMT (Gulcehre et al., 2016), the copy mechanism has been shown effective on tasks such as dialogue (Gu et al., 2016), summarization (See et al., 2017) and question generation (Song et al., 2018). We investigate the copy mechanism on AMR-to-text generation. 7 Conclusion We introduced a novel graph-to-sequence model for AMR-to-text generation. Compared to sequence-to-sequence models, which require linearization of AMR before decoding, a graph LSTM is leveraged to directly model full AMR structure. Allowing high parallelization, the graph encoder is more efficient than the sequence encoder. In our experiments, the graph model outperforms a strong sequence-to-sequence model, achieving the best performance. Acknowledgments We thank the anonymized reviewers for the insightful comments, and"
P18-1150,P15-1150,0,0.208715,"Missing"
P18-1150,D16-1112,0,0.0316661,"aning “Ryan’s description of himself: a genius.” Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism that encodes the meaning of a sentence as a rooted, directed graph. Figure 1 shows an AMR graph in which the nodes (such as “describe-01” and “person”) represent the concepts, and edges (such as “:ARG0” and “:name”) represent the relations between concepts they connect. AMR has been proven helpful on other NLP tasks, such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015). The task of AMR-to-text generation is to produce a text with the same meaning as a given input AMR graph. The task is challenging as word tenses and function words are abstracted away when constructing AMR graphs from texts. The translation from AMR nodes to text phrases can be far from literal. For example, shown in Figure 1, “Ryan” is represented as “(p / person :name (n / name :op1 “Ryan”))”, and “description of” is represented as “(d / describe-01 :ARG1 )”. While initial work used statistical approaches (Flanigan et al., 2016b; Pourdamghani et al., 2"
P18-1150,W15-3504,0,0.0515809,"Missing"
P18-1150,P16-1008,0,0.0232013,". ; aN ] (4) where N is the number of input tokens. The decoder yields an output sequence w1 , w2 , . . . , wM by calculating a sequence of hidden states s1 , s2 . . . , sM recurrently. While generating the t-th word, the decoder considers five factors: (1) the attention memory A; (2) the previous hidden state of the LSTM model st 1 ; (3) the embedding of the current input (previously generated word) et ; (4) the previous context vector µt 1 , which is calculated with attention from A; and (5) the previous coverage vector γt 1 , which is the accumulation of all attention distributions so far (Tu et al., 2016). When t = 1, we initialize µ0 and γ0 as zero vectors, set e1 to the embedding of the start token “<s>”, and s0 as the average of all encoder states. For each time-step t, the decoder feeds the concatenation of the embedding of the current input et and the previous context vector µt 1 into the 1617 In order to capture non-local interaction between nodes, we allow information exchange between nodes through a sequence of state transitions, leading to a sequence of states g0 , g1 , . . . , gt , . . . , where gt = {hjt }|vj ∈V . The initial state g0 consists of a set of initial node states hj0 = h"
P19-1304,D18-1246,1,0.897995,"ic entity graphs of Lebron James from two KGs, namely G1 and G2 . Then, we propose a graph matching model to estimate the probability that G1 and G2 are describing the same entity. In particular, the matching model includes the following four layers: Input Representation Layer The goal of this layer is to learn embeddings for entities that occurred in topic entity graphs by using a GCN (henceforth GCN 1 ) (Xu et al., 2018a). Recently, GCN has been successfully applied in many NLP tasks, such as semantic parsing (Xu et al., 2018b), text representation (Zhang et al., 2018), relation extraction (Song et al., 2018) and text generation (Xu et al., 2018c). We use the following embedding generation of entity v as an example to explain the GCN algorithm: (1) We first employ a word-based LSTM to transform v’s entity name to its initial feature vector av ; (2) We categorize the neighbors of v into incoming neighbors N` (v) and outgoing neighbors Na (v) according to the edge direction. (3) We leverage an aggregator to aggregate the incoming representations of v’s incoming neighbors k {hk−1 u` , ∀u ∈ N` (v)} into a single vector, hN` (v) , where k is the iteration index. This aggregator 1 Lebron James is transl"
P19-1304,D18-1032,0,0.0812748,"ultilingual knowledge graphs (KGs), such as DBpedia (Auer et al., 2007) and Yago (Suchanek et al., 2007), represent human knowledge in the structured format and have been successfully used in many natural language processing applications. These KGs encode rich monolingual knowledge but lack the cross-lingual links to bridge the language gap. Therefore, the cross-lingual KG alignment task, which automatically matches entities in a multilingual KG, is proposed to address this problem. Most recently, several entity matching based approaches (Hao et al., 2016; Chen et al., 2016; Sun et al., 2017; Wang et al., 2018) have been proposed for this task. Generally, these approaches first project entities of each KG into lowdimensional vector spaces by encoding monolingual KG facts, and then learn a similarity score function to match entities based on their vector representations. However, since some entities in different languages may have different KG To address these drawbacks, we propose a topic entity graph to represent the KG context information of an entity. Unlike previous methods that utilize entity embeddings to match entities, we formulate this task as a graph matching problem between the topic enti"
P19-1304,Q17-1010,0,0.0352098,"8 37.29 74.49 83.45 91.56 84.71 92.35 EN-FR @1 @10 14.61 37.25 21.26 50.60 32.97 65.91 36.77 73.06 81.03 90.79 84.15 91.76 66.91 67.93 67.92 64.01 65.28 65.21 72.63 73.97 73.52 69.76 71.29 70.18 87.62 89.38 88.96 87.65 88.18 88.01 77.52 78.48 78.36 78.12 79.64 79.48 85.09 87.15 86.87 83.48 84.63 84.29 94.19 95.24 94.28 93.66 94.75 94.37 Table 1: Evaluation results on the datasets. non-linearity function σ is ReLU (Glorot et al., 2011) and the parameters of aggregators are randomly initialized. Since KGs are represented in different languages, we first retrieve monolingual fastText embeddings (Bojanowski et al., 2017) for each language, and apply the method proposed in Conneau et al. (2017) to align these word embeddings into a same vector space, namely, crosslingual word embeddings. We use these embeddings to initialize word representations in the first layer of GCN1 . Results and Discussion. Following previous works, we used Hits@1 and Hits@10 to evaluate our model, where Hits@k measures the proportion of correctly aligned entities ranked in the top k. We implemented a baseline (referred as BASELINE in Table 1) that selects k closest G2 entities to a given G1 entity in the cross-lingual embedding space,"
P19-1304,D18-1223,1,0.789993,"Missing"
P19-1304,D18-1110,1,0.821566,"will discuss in §4. 3 Graph Matching Model Figure 2 gives an overview of our method for aligning Lebron James in the English and Chinese knowledge graph1 . Specifically, we fist retrieve topic entity graphs of Lebron James from two KGs, namely G1 and G2 . Then, we propose a graph matching model to estimate the probability that G1 and G2 are describing the same entity. In particular, the matching model includes the following four layers: Input Representation Layer The goal of this layer is to learn embeddings for entities that occurred in topic entity graphs by using a GCN (henceforth GCN 1 ) (Xu et al., 2018a). Recently, GCN has been successfully applied in many NLP tasks, such as semantic parsing (Xu et al., 2018b), text representation (Zhang et al., 2018), relation extraction (Song et al., 2018) and text generation (Xu et al., 2018c). We use the following embedding generation of entity v as an example to explain the GCN algorithm: (1) We first employ a word-based LSTM to transform v’s entity name to its initial feature vector av ; (2) We categorize the neighbors of v into incoming neighbors N` (v) and outgoing neighbors Na (v) according to the edge direction. (3) We leverage an aggregator to ag"
P19-1304,D18-1112,1,0.597172,"will discuss in §4. 3 Graph Matching Model Figure 2 gives an overview of our method for aligning Lebron James in the English and Chinese knowledge graph1 . Specifically, we fist retrieve topic entity graphs of Lebron James from two KGs, namely G1 and G2 . Then, we propose a graph matching model to estimate the probability that G1 and G2 are describing the same entity. In particular, the matching model includes the following four layers: Input Representation Layer The goal of this layer is to learn embeddings for entities that occurred in topic entity graphs by using a GCN (henceforth GCN 1 ) (Xu et al., 2018a). Recently, GCN has been successfully applied in many NLP tasks, such as semantic parsing (Xu et al., 2018b), text representation (Zhang et al., 2018), relation extraction (Song et al., 2018) and text generation (Xu et al., 2018c). We use the following embedding generation of entity v as an example to explain the GCN algorithm: (1) We first employ a word-based LSTM to transform v’s entity name to its initial feature vector av ; (2) We categorize the neighbors of v into incoming neighbors N` (v) and outgoing neighbors Na (v) according to the edge direction. (3) We leverage an aggregator to ag"
P19-1304,P18-1030,0,0.0133111,"raph1 . Specifically, we fist retrieve topic entity graphs of Lebron James from two KGs, namely G1 and G2 . Then, we propose a graph matching model to estimate the probability that G1 and G2 are describing the same entity. In particular, the matching model includes the following four layers: Input Representation Layer The goal of this layer is to learn embeddings for entities that occurred in topic entity graphs by using a GCN (henceforth GCN 1 ) (Xu et al., 2018a). Recently, GCN has been successfully applied in many NLP tasks, such as semantic parsing (Xu et al., 2018b), text representation (Zhang et al., 2018), relation extraction (Song et al., 2018) and text generation (Xu et al., 2018c). We use the following embedding generation of entity v as an example to explain the GCN algorithm: (1) We first employ a word-based LSTM to transform v’s entity name to its initial feature vector av ; (2) We categorize the neighbors of v into incoming neighbors N` (v) and outgoing neighbors Na (v) according to the edge direction. (3) We leverage an aggregator to aggregate the incoming representations of v’s incoming neighbors k {hk−1 u` , ∀u ∈ N` (v)} into a single vector, hN` (v) , where k is the iteration index."
Q13-1024,P11-1131,0,0.0366234,"Missing"
Q13-1024,J93-2003,0,0.086468,"Missing"
Q13-1024,P03-1012,0,0.590582,"nt technique, which allows alignments to violate syntactic constraints by incurring a cost in probability. Pauls et al. (2010) gave a new instance of the ITG formalism, in which one side of the synchronous derivation is constrained by the syntactic tree. Fox (2002) measured syntactic cohesion in gold standard alignments and showed syntactic cohesion is generally maintained between English and French. She also compared three variant syntactic representations (phrase tree, verb phrase flattening tree and dependency tree), and found the dependency tree produced the highest degree of cohesion. So Cherry and Lin (2003; 2006a) used dependency cohesion as a hard constraint to restrict the alignment space, where all potential alignments violating cohesion constraint are ruled out directly. Although the alignment quality is improved, they ignored situations where a small set of correct alignments can violate cohesion. To address this limitation, Cherry and Lin (2006b) proposed a soft constraint approach, which took dependency cohesion as a feature of a discriminative model, and verified that the soft constraint works better than the hard constraint. However, the training procedure is very timeconsuming, and th"
Q13-1024,E06-1019,0,0.0898349,"ive to create. A more practical way to improve large-scale word alignment quality is to introduce syntactic knowledge into a generative model and train the model in an unsupervised manner (Wu, 1997; Yamada and Knight, 2001; Lopez and Resnik, 2005; DeNero and Klein, 2007; Pauls et al., 2010). In this paper, we take dependency cohesion (Fox, 2002) into account, which assumes phrases dominated by disjoint dependency subtrees tend not to overlap after translation. Instead of treating dependency cohesion as a hard constraint (Lin and Cherry, 2003) or using it as a feature in discriminative models (Cherry and Lin, 2006b), we treat dependency cohesion as a distortion constraint, and integrate it into a modified HMM word alignment model to softly influence the probabilities of alignment candidates. We also propose an approximate EM algorithm and an explicit Gibbs sampling algorithm to train the model in an unsupervised manner. Experiments on a large-scale Chinese-English translation task demonstrate that our model achieves improvements in both word alignment quality and machine translation quality. The remainder of this paper is organized as follows: Section 2 introduces dependency cohesion 291 Transactions o"
Q13-1024,P06-2014,0,0.741722,"ive to create. A more practical way to improve large-scale word alignment quality is to introduce syntactic knowledge into a generative model and train the model in an unsupervised manner (Wu, 1997; Yamada and Knight, 2001; Lopez and Resnik, 2005; DeNero and Klein, 2007; Pauls et al., 2010). In this paper, we take dependency cohesion (Fox, 2002) into account, which assumes phrases dominated by disjoint dependency subtrees tend not to overlap after translation. Instead of treating dependency cohesion as a hard constraint (Lin and Cherry, 2003) or using it as a feature in discriminative models (Cherry and Lin, 2006b), we treat dependency cohesion as a distortion constraint, and integrate it into a modified HMM word alignment model to softly influence the probabilities of alignment candidates. We also propose an approximate EM algorithm and an explicit Gibbs sampling algorithm to train the model in an unsupervised manner. Experiments on a large-scale Chinese-English translation task demonstrate that our model achieves improvements in both word alignment quality and machine translation quality. The remainder of this paper is organized as follows: Section 2 introduces dependency cohesion 291 Transactions o"
Q13-1024,P07-1003,0,0.479084,"linguistically motivated features have become increasingly popular (Moore, 2005; Taskar et al., 2005; Riesa and Marcu, 2010; Saers et al., 2010; Riesa et al., 2011). These models are usually trained with manually annotated parallel data. However, when moving to a new language pair, large amount of hand-aligned data are usually unavailable and expensive to create. A more practical way to improve large-scale word alignment quality is to introduce syntactic knowledge into a generative model and train the model in an unsupervised manner (Wu, 1997; Yamada and Knight, 2001; Lopez and Resnik, 2005; DeNero and Klein, 2007; Pauls et al., 2010). In this paper, we take dependency cohesion (Fox, 2002) into account, which assumes phrases dominated by disjoint dependency subtrees tend not to overlap after translation. Instead of treating dependency cohesion as a hard constraint (Lin and Cherry, 2003) or using it as a feature in discriminative models (Cherry and Lin, 2006b), we treat dependency cohesion as a distortion constraint, and integrate it into a modified HMM word alignment model to softly influence the probabilities of alignment candidates. We also propose an approximate EM algorithm and an explicit Gibbs sa"
Q13-1024,P11-1042,0,0.0741043,"Missing"
Q13-1024,W02-1039,0,0.601548,"et al., 2005; Riesa and Marcu, 2010; Saers et al., 2010; Riesa et al., 2011). These models are usually trained with manually annotated parallel data. However, when moving to a new language pair, large amount of hand-aligned data are usually unavailable and expensive to create. A more practical way to improve large-scale word alignment quality is to introduce syntactic knowledge into a generative model and train the model in an unsupervised manner (Wu, 1997; Yamada and Knight, 2001; Lopez and Resnik, 2005; DeNero and Klein, 2007; Pauls et al., 2010). In this paper, we take dependency cohesion (Fox, 2002) into account, which assumes phrases dominated by disjoint dependency subtrees tend not to overlap after translation. Instead of treating dependency cohesion as a hard constraint (Lin and Cherry, 2003) or using it as a feature in discriminative models (Cherry and Lin, 2006b), we treat dependency cohesion as a distortion constraint, and integrate it into a modified HMM word alignment model to softly influence the probabilities of alignment candidates. We also propose an approximate EM algorithm and an explicit Gibbs sampling algorithm to train the model in an unsupervised manner. Experiments on"
Q13-1024,N04-1035,0,0.175101,"algorithm to estimate model parameters in an unsupervised manner. Experiments on large-scale Chinese-English translation tasks demonstrate that our model achieves improvements in both alignment quality and translation quality. 1 Introduction Word alignment is the task of identifying word correspondences between parallel sentence pairs. Word alignment has become a vital component of statistical machine translation (SMT) systems, since it is required by almost all state-of-the-art SMT systems for the purpose of extracting phrase tables or even syntactic transformation rules (Koehn et al., 2007; Galley et al., 2004). During the past two decades, generative word alignment models such as the IBM Models (Brown et al., 1993) and the HMM model (Vogel et al., 1996) have been widely used, primarily because they are trained on bilingual sentences in an unsupervised manner and the implementation is freely available in the GIZA++ toolkit (Och and Ney, 2003). However, the word alignment quality of generative models is still far from satisfactory for SMT systems. In recent years, discriminative alignment models incorporating linguistically motivated features have become increasingly popular (Moore, 2005; Taskar et a"
Q13-1024,D08-1036,0,0.028431,"Missing"
Q13-1024,P03-1011,0,0.0747953,"sion systems improve word alignment than IBM4, they fail to outperform the IBM4 system on machine translation. The BLEU score of our Soft-Cohesion-EM system is better than the IBM4 system when using the FBIS training set, but 298 6 Related Work There have been many proposals of integrating syntactic knowledge into generative alignment models. Wu (1997) proposed the inversion transduction grammar (ITG) to model word alignment as synchronous parsing for a sentence pair. Yamada and Knight (2001) represented translation as a sequence of re-ordering operations over child nodes of a syntactic tree. Gildea (2003) introduced a “loosely” tree-based alignment technique, which allows alignments to violate syntactic constraints by incurring a cost in probability. Pauls et al. (2010) gave a new instance of the ITG formalism, in which one side of the synchronous derivation is constrained by the syntactic tree. Fox (2002) measured syntactic cohesion in gold standard alignments and showed syntactic cohesion is generally maintained between English and French. She also compared three variant syntactic representations (phrase tree, verb phrase flattening tree and dependency tree), and found the dependency tree pr"
Q13-1024,D11-1137,0,0.0676682,"Missing"
Q13-1024,P10-1017,0,0.228077,"he past two decades, generative word alignment models such as the IBM Models (Brown et al., 1993) and the HMM model (Vogel et al., 1996) have been widely used, primarily because they are trained on bilingual sentences in an unsupervised manner and the implementation is freely available in the GIZA++ toolkit (Och and Ney, 2003). However, the word alignment quality of generative models is still far from satisfactory for SMT systems. In recent years, discriminative alignment models incorporating linguistically motivated features have become increasingly popular (Moore, 2005; Taskar et al., 2005; Riesa and Marcu, 2010; Saers et al., 2010; Riesa et al., 2011). These models are usually trained with manually annotated parallel data. However, when moving to a new language pair, large amount of hand-aligned data are usually unavailable and expensive to create. A more practical way to improve large-scale word alignment quality is to introduce syntactic knowledge into a generative model and train the model in an unsupervised manner (Wu, 1997; Yamada and Knight, 2001; Lopez and Resnik, 2005; DeNero and Klein, 2007; Pauls et al., 2010). In this paper, we take dependency cohesion (Fox, 2002) into account, which assu"
Q13-1024,N07-1018,0,0.0184611,"ignment with Eq. (2) among neighbor alignments of the initial point, and taking ??? as the higher-order node: then make the best alignment as the initial point for ??? (??? |?[1,?] ) = ∏?? ,?∈?? ?? (??? ,? |?? , ??? , ??? ) the next iteration. The algorithm iterates until no ? ? ? update could be made. (6) 4.2 Gibbs Sampling Algorithm where ???,? ∈ {??ℎ?????, ????????} is the modifiermodifier cohesion relationship between ??? and Gibbs sampling is another effective algorithm for unsupervised learning problems. As is described in one of its sibling ?? , ?? is the corresponding the literatures (Johnson et al., 2007; Gao and probability, ??? and ??? are the aligned words for Johnson, 2008), there are two types of Gibbs ? samplers: explicit and collapsed. An explicit sampler represents and samples the model parameters in addition to the word alignments, while in a collapsed sampler the parameters are integrated out and only alignments are sampled. Mermer and Saraçlar (2011) proposed a collapsed sampler for IBM Model 1. However, their sampler updates parameters constantly and thus cannot run efficiently on large-scale tasks. Instead, we take 4 Parameter Estimation advantage of explicit Gibbs sampling to ma"
Q13-1024,D11-1046,0,0.0565794,"ent models such as the IBM Models (Brown et al., 1993) and the HMM model (Vogel et al., 1996) have been widely used, primarily because they are trained on bilingual sentences in an unsupervised manner and the implementation is freely available in the GIZA++ toolkit (Och and Ney, 2003). However, the word alignment quality of generative models is still far from satisfactory for SMT systems. In recent years, discriminative alignment models incorporating linguistically motivated features have become increasingly popular (Moore, 2005; Taskar et al., 2005; Riesa and Marcu, 2010; Saers et al., 2010; Riesa et al., 2011). These models are usually trained with manually annotated parallel data. However, when moving to a new language pair, large amount of hand-aligned data are usually unavailable and expensive to create. A more practical way to improve large-scale word alignment quality is to introduce syntactic knowledge into a generative model and train the model in an unsupervised manner (Wu, 1997; Yamada and Knight, 2001; Lopez and Resnik, 2005; DeNero and Klein, 2007; Pauls et al., 2010). In this paper, we take dependency cohesion (Fox, 2002) into account, which assumes phrases dominated by disjoint depende"
Q13-1024,W04-3250,0,0.0824144,"ent tasks. IBM4 IBM4-L0 IBM4-Prior Agree-HMM Tree-Distance Hard-Cohesion Soft-Cohesion-EM Soft-Cohesion-Gibbs gold-standard forward HCP MCP 60.53 63.94 60.57 62.53 66.48 74.65 75.52 66.61 81.37 74.69 98.70 97.43 85.21 81.96 88.74 85.55 88.43 95.82 reverse HCP MCP 56.15 64.80 66.49 65.68 67.19 72.32 73.88 66.07 78.00 71.73 98.25 97.84 82.96 81.36 87.81 84.83 81.53 91.62 worse when using the LARGE training set. Our Soft-Cohesion-Gibbs system produces the best BLEU score when using both training sets. We also performed a statistical significance test using bootstrap resampling with 1000 samples (Koehn, 2004; Zhang et al., 2004). Experimental results show the Soft-Cohesion-Gibbs system is significantly better (p&lt;0.05) than the IBM4 system. The IBM4-Prior system slightly outperforms IBM4, but it’s not significant. FBIS LARGE IBM4 30.7 33.1 IBM4-L0 30.4 32.3 IBM4-Prior 30.9 33.2 Agree-HMM 27.2 30.1 Tree-Distance 28.2 N/A Hard-Cohesion 30.4 32.2 Soft-Cohesion-EM 30.9 33.1 Soft-Cohesion-Gibbs 31.6* 33.9* Table 7: BLEU scores, where * indicates significantly better than IBM4 (p&lt;0.05). Table 5: HCPs and MCPs on the development set. IBM4 IBM4-L0 IBM4-Prior Agree-HMM Hard-Cohesion Soft-Cohesion-EM Soft-C"
Q13-1024,P12-2060,0,0.154069,"Missing"
Q13-1024,P07-2045,0,0.030301,"nd a Gibbs sampling algorithm to estimate model parameters in an unsupervised manner. Experiments on large-scale Chinese-English translation tasks demonstrate that our model achieves improvements in both alignment quality and translation quality. 1 Introduction Word alignment is the task of identifying word correspondences between parallel sentence pairs. Word alignment has become a vital component of statistical machine translation (SMT) systems, since it is required by almost all state-of-the-art SMT systems for the purpose of extracting phrase tables or even syntactic transformation rules (Koehn et al., 2007; Galley et al., 2004). During the past two decades, generative word alignment models such as the IBM Models (Brown et al., 1993) and the HMM model (Vogel et al., 1996) have been widely used, primarily because they are trained on bilingual sentences in an unsupervised manner and the implementation is freely available in the GIZA++ toolkit (Och and Ney, 2003). However, the word alignment quality of generative models is still far from satisfactory for SMT systems. In recent years, discriminative alignment models incorporating linguistically motivated features have become increasingly popular (Mo"
Q13-1024,N10-1050,0,0.0147816,"nerative word alignment models such as the IBM Models (Brown et al., 1993) and the HMM model (Vogel et al., 1996) have been widely used, primarily because they are trained on bilingual sentences in an unsupervised manner and the implementation is freely available in the GIZA++ toolkit (Och and Ney, 2003). However, the word alignment quality of generative models is still far from satisfactory for SMT systems. In recent years, discriminative alignment models incorporating linguistically motivated features have become increasingly popular (Moore, 2005; Taskar et al., 2005; Riesa and Marcu, 2010; Saers et al., 2010; Riesa et al., 2011). These models are usually trained with manually annotated parallel data. However, when moving to a new language pair, large amount of hand-aligned data are usually unavailable and expensive to create. A more practical way to improve large-scale word alignment quality is to introduce syntactic knowledge into a generative model and train the model in an unsupervised manner (Wu, 1997; Yamada and Knight, 2001; Lopez and Resnik, 2005; DeNero and Klein, 2007; Pauls et al., 2010). In this paper, we take dependency cohesion (Fox, 2002) into account, which assumes phrases dominate"
Q13-1024,N06-1014,0,0.218471,"se and English. 3 A Generative Word Alignment Model with Dependency Cohesion Constraint The most influential generative word alignment models are the IBM Models 1-5 and the HMM model (Brown et al., 1993; Vogel et al., 1996; Och and Ney, 2003). These models can be classified into sequence-based models (IBM Models 1, 2 and HMM) and fertility-based models (IBM Models 3, 4 and 5). The sequence-based model is easier to implement, and recent experiments have shown that appropriately modified sequence-based model can produce comparable performance with fertility-based models (Lopez and Resnik, 2005; Liang et al., 2006; DeNero and Klein, 2007; Zhao and Gildea, 2010; Bansal et al., 2011). So we built a generative word alignment model with dependency cohesion constraint based on the sequence-based model. 3.1 The Sequence-based Alignment Model According to Brown et al. (1993) and Och and Ney (2003), the sequence-based model is built as a noisy channel model, where the source sentence ?1? and the alignment ?1? are generated conditioning on the target sentence ?1? . The model assumes each source word is assigned to exactly one target word, and defines an asymmetric alignment for the sentence pair as ?1? = ?1 , ?"
Q13-1024,N03-2017,0,0.123944,"air, large amount of hand-aligned data are usually unavailable and expensive to create. A more practical way to improve large-scale word alignment quality is to introduce syntactic knowledge into a generative model and train the model in an unsupervised manner (Wu, 1997; Yamada and Knight, 2001; Lopez and Resnik, 2005; DeNero and Klein, 2007; Pauls et al., 2010). In this paper, we take dependency cohesion (Fox, 2002) into account, which assumes phrases dominated by disjoint dependency subtrees tend not to overlap after translation. Instead of treating dependency cohesion as a hard constraint (Lin and Cherry, 2003) or using it as a feature in discriminative models (Cherry and Lin, 2006b), we treat dependency cohesion as a distortion constraint, and integrate it into a modified HMM word alignment model to softly influence the probabilities of alignment candidates. We also propose an approximate EM algorithm and an explicit Gibbs sampling algorithm to train the model in an unsupervised manner. Experiments on a large-scale Chinese-English translation task demonstrate that our model achieves improvements in both word alignment quality and machine translation quality. The remainder of this paper is organized"
Q13-1024,W05-0812,0,0.81294,"ent models incorporating linguistically motivated features have become increasingly popular (Moore, 2005; Taskar et al., 2005; Riesa and Marcu, 2010; Saers et al., 2010; Riesa et al., 2011). These models are usually trained with manually annotated parallel data. However, when moving to a new language pair, large amount of hand-aligned data are usually unavailable and expensive to create. A more practical way to improve large-scale word alignment quality is to introduce syntactic knowledge into a generative model and train the model in an unsupervised manner (Wu, 1997; Yamada and Knight, 2001; Lopez and Resnik, 2005; DeNero and Klein, 2007; Pauls et al., 2010). In this paper, we take dependency cohesion (Fox, 2002) into account, which assumes phrases dominated by disjoint dependency subtrees tend not to overlap after translation. Instead of treating dependency cohesion as a hard constraint (Lin and Cherry, 2003) or using it as a feature in discriminative models (Cherry and Lin, 2006b), we treat dependency cohesion as a distortion constraint, and integrate it into a modified HMM word alignment model to softly influence the probabilities of alignment candidates. We also propose an approximate EM algorithm"
Q13-1024,P11-2032,0,0.245202,"??, ????????} is the modifiermodifier cohesion relationship between ??? and Gibbs sampling is another effective algorithm for unsupervised learning problems. As is described in one of its sibling ?? , ?? is the corresponding the literatures (Johnson et al., 2007; Gao and probability, ??? and ??? are the aligned words for Johnson, 2008), there are two types of Gibbs ? samplers: explicit and collapsed. An explicit sampler represents and samples the model parameters in addition to the word alignments, while in a collapsed sampler the parameters are integrated out and only alignments are sampled. Mermer and Saraçlar (2011) proposed a collapsed sampler for IBM Model 1. However, their sampler updates parameters constantly and thus cannot run efficiently on large-scale tasks. Instead, we take 4 Parameter Estimation advantage of explicit Gibbs sampling to make a To align sentence pairs with the model in Eq. (2), highly parallelizable sampler. Our Gibbs sampler we have to estimate some parameters: ?? , ??? , ?ℎ is similar to the MCMC algorithm in Zhao and and ?? . The traditional approach for sequence- Gildea (2010), but we assume Dirichlet priors based models uses Expectation Maximization (EM) when sampling model p"
Q13-1024,H05-1011,0,0.0668312,"07; Galley et al., 2004). During the past two decades, generative word alignment models such as the IBM Models (Brown et al., 1993) and the HMM model (Vogel et al., 1996) have been widely used, primarily because they are trained on bilingual sentences in an unsupervised manner and the implementation is freely available in the GIZA++ toolkit (Och and Ney, 2003). However, the word alignment quality of generative models is still far from satisfactory for SMT systems. In recent years, discriminative alignment models incorporating linguistically motivated features have become increasingly popular (Moore, 2005; Taskar et al., 2005; Riesa and Marcu, 2010; Saers et al., 2010; Riesa et al., 2011). These models are usually trained with manually annotated parallel data. However, when moving to a new language pair, large amount of hand-aligned data are usually unavailable and expensive to create. A more practical way to improve large-scale word alignment quality is to introduce syntactic knowledge into a generative model and train the model in an unsupervised manner (Wu, 1997; Yamada and Knight, 2001; Lopez and Resnik, 2005; DeNero and Klein, 2007; Pauls et al., 2010). In this paper, we take dependency c"
Q13-1024,W99-0604,0,0.146059,"ble alignments, which is required Our sampler performs a sequence of consecutive in the E-step of EM algorithm. Therefore, we iterations. Each iteration consists of two sampling propose an approximate EM algorithm and a Gibbs steps. The first step samples the aligned position sampling algorithm for parameter estimation. for each dependency node according to the BUTorder. Concretely, when sampling the aligned ??? and ?? . Both ?ℎ and ?? in Eq. (5) and Eq. (6) are conditioned on three words, which would make them very sparse. To cope with this problem, we use the word clustering toolkit, mkcls (Och et al., 1999), to cluster all words into 50 classes, and replace the three words with their classes. 295 (?+1) position ??? for node ??? on iteration ?+1, the aligned positions for ?[1,?−1] are fixed on the new sampling results ?(?+1) [1,?−1] on iteration ? +1, and the aligned positions for ?[?+1,?] are fixed on the old sampling results ?(?) [?+1,?] on iteration ? . Therefore, we sample the aligned position ??(?+1) as follows: ? (?+1) ??? (?+1) (?) ~ ? (??? |?[1,?−1] , ?[?+1,?] , ?1? , ?1? ) = ?? = where ? ? (?+1) ?[1,?−1] ?? |?1? ) ? (?1? , ? ? ∑?? ? ? ?? |?1? ) ∈{0,1,…,?} ? (?1 , ? ? ∪ ??? ∪ (?) ?[?+1"
Q13-1024,J03-1002,0,0.226771,"rd alignment has become a vital component of statistical machine translation (SMT) systems, since it is required by almost all state-of-the-art SMT systems for the purpose of extracting phrase tables or even syntactic transformation rules (Koehn et al., 2007; Galley et al., 2004). During the past two decades, generative word alignment models such as the IBM Models (Brown et al., 1993) and the HMM model (Vogel et al., 1996) have been widely used, primarily because they are trained on bilingual sentences in an unsupervised manner and the implementation is freely available in the GIZA++ toolkit (Och and Ney, 2003). However, the word alignment quality of generative models is still far from satisfactory for SMT systems. In recent years, discriminative alignment models incorporating linguistically motivated features have become increasingly popular (Moore, 2005; Taskar et al., 2005; Riesa and Marcu, 2010; Saers et al., 2010; Riesa et al., 2011). These models are usually trained with manually annotated parallel data. However, when moving to a new language pair, large amount of hand-aligned data are usually unavailable and expensive to create. A more practical way to improve large-scale word alignment quali"
Q13-1024,P02-1040,0,0.0916915,"on We then evaluate the effect of word alignment on machine translation quality using the phrase-based translation system Moses (Koehn et al., 2007). We take NIST MT03 test data as the development set, NIST MT05 test data as the testing set. We train a 5-gram language model with the Xinhua portion of English Gigaword corpus and the English side of the training set using the SRILM Toolkit (Stolcke, 2002). We train machine translation models using GDFA alignments of each system. BLEU scores on NIST MT05 are listed in Table 7, where BLEU scores are calculated using lowercased and tokenized data (Papineni et al., 2002). Although the IBM4-L0, Agree-HMM, Tree-Distance and Hard-Cohesion systems improve word alignment than IBM4, they fail to outperform the IBM4 system on machine translation. The BLEU score of our Soft-Cohesion-EM system is better than the IBM4 system when using the FBIS training set, but 298 6 Related Work There have been many proposals of integrating syntactic knowledge into generative alignment models. Wu (1997) proposed the inversion transduction grammar (ITG) to model word alignment as synchronous parsing for a sentence pair. Yamada and Knight (2001) represented translation as a sequence of"
Q13-1024,N10-1014,0,0.555753,"d features have become increasingly popular (Moore, 2005; Taskar et al., 2005; Riesa and Marcu, 2010; Saers et al., 2010; Riesa et al., 2011). These models are usually trained with manually annotated parallel data. However, when moving to a new language pair, large amount of hand-aligned data are usually unavailable and expensive to create. A more practical way to improve large-scale word alignment quality is to introduce syntactic knowledge into a generative model and train the model in an unsupervised manner (Wu, 1997; Yamada and Knight, 2001; Lopez and Resnik, 2005; DeNero and Klein, 2007; Pauls et al., 2010). In this paper, we take dependency cohesion (Fox, 2002) into account, which assumes phrases dominated by disjoint dependency subtrees tend not to overlap after translation. Instead of treating dependency cohesion as a hard constraint (Lin and Cherry, 2003) or using it as a feature in discriminative models (Cherry and Lin, 2006b), we treat dependency cohesion as a distortion constraint, and integrate it into a modified HMM word alignment model to softly influence the probabilities of alignment candidates. We also propose an approximate EM algorithm and an explicit Gibbs sampling algorithm to t"
Q13-1024,P06-1055,0,0.00759307,"hether our model is adaptable for large-scale task. For word alignment quality evaluation, we take the handaligned data sets from SSMT20072, which contains 2 http://nlp.ict.ac.cn/guidelines/guidelines-2007SSMT(English).doc 296 505 sentence pairs in the testing set and 502 sentence pairs in the development set. Following Och and Ney (2003), we evaluate word alignment quality with the alignment error rate (AER), where lower AER is better. Because our model takes dependency trees as input, we parse both sides of the two training sets, the development set and the testing set with Berkeley parser (Petrov et al., 2006), and then convert the generated phrase trees into dependency trees according to Wang and Zong (2010; 2011). Our model is an asymmetric model, so we perform word alignment in both forward (ChineseEnglish) and reverse (EnglishChinese) directions. Train Set FBIS Source Corpus FBIS newswire data LARGE LDC2000T50, LDC2003E14, LDC2003E07, LDC2004T07, LDC2005T06, LDC2002L27, LDC2005T10, LDC2005T34 # Words Ch: 7.1M En: 9.1M Ch: 27.6M En: 31.8M Table 2: The size and the source corpus of the two training sets. 5.1 Effectiveness of Cohesion Constraints In Eq. (3), the distortion probability ?? is deco"
Q13-1024,H05-1010,0,0.0658408,"al., 2004). During the past two decades, generative word alignment models such as the IBM Models (Brown et al., 1993) and the HMM model (Vogel et al., 1996) have been widely used, primarily because they are trained on bilingual sentences in an unsupervised manner and the implementation is freely available in the GIZA++ toolkit (Och and Ney, 2003). However, the word alignment quality of generative models is still far from satisfactory for SMT systems. In recent years, discriminative alignment models incorporating linguistically motivated features have become increasingly popular (Moore, 2005; Taskar et al., 2005; Riesa and Marcu, 2010; Saers et al., 2010; Riesa et al., 2011). These models are usually trained with manually annotated parallel data. However, when moving to a new language pair, large amount of hand-aligned data are usually unavailable and expensive to create. A more practical way to improve large-scale word alignment quality is to introduce syntactic knowledge into a generative model and train the model in an unsupervised manner (Wu, 1997; Yamada and Knight, 2001; Lopez and Resnik, 2005; DeNero and Klein, 2007; Pauls et al., 2010). In this paper, we take dependency cohesion (Fox, 2002) i"
Q13-1024,P12-1033,0,0.090783,"Missing"
Q13-1024,C96-2141,0,0.899422,"model achieves improvements in both alignment quality and translation quality. 1 Introduction Word alignment is the task of identifying word correspondences between parallel sentence pairs. Word alignment has become a vital component of statistical machine translation (SMT) systems, since it is required by almost all state-of-the-art SMT systems for the purpose of extracting phrase tables or even syntactic transformation rules (Koehn et al., 2007; Galley et al., 2004). During the past two decades, generative word alignment models such as the IBM Models (Brown et al., 1993) and the HMM model (Vogel et al., 1996) have been widely used, primarily because they are trained on bilingual sentences in an unsupervised manner and the implementation is freely available in the GIZA++ toolkit (Och and Ney, 2003). However, the word alignment quality of generative models is still far from satisfactory for SMT systems. In recent years, discriminative alignment models incorporating linguistically motivated features have become increasingly popular (Moore, 2005; Taskar et al., 2005; Riesa and Marcu, 2010; Saers et al., 2010; Riesa et al., 2011). These models are usually trained with manually annotated parallel data."
Q13-1024,J97-3002,0,0.672161,"recent years, discriminative alignment models incorporating linguistically motivated features have become increasingly popular (Moore, 2005; Taskar et al., 2005; Riesa and Marcu, 2010; Saers et al., 2010; Riesa et al., 2011). These models are usually trained with manually annotated parallel data. However, when moving to a new language pair, large amount of hand-aligned data are usually unavailable and expensive to create. A more practical way to improve large-scale word alignment quality is to introduce syntactic knowledge into a generative model and train the model in an unsupervised manner (Wu, 1997; Yamada and Knight, 2001; Lopez and Resnik, 2005; DeNero and Klein, 2007; Pauls et al., 2010). In this paper, we take dependency cohesion (Fox, 2002) into account, which assumes phrases dominated by disjoint dependency subtrees tend not to overlap after translation. Instead of treating dependency cohesion as a hard constraint (Lin and Cherry, 2003) or using it as a feature in discriminative models (Cherry and Lin, 2006b), we treat dependency cohesion as a distortion constraint, and integrate it into a modified HMM word alignment model to softly influence the probabilities of alignment candida"
Q13-1024,P01-1067,0,0.813018,"rs, discriminative alignment models incorporating linguistically motivated features have become increasingly popular (Moore, 2005; Taskar et al., 2005; Riesa and Marcu, 2010; Saers et al., 2010; Riesa et al., 2011). These models are usually trained with manually annotated parallel data. However, when moving to a new language pair, large amount of hand-aligned data are usually unavailable and expensive to create. A more practical way to improve large-scale word alignment quality is to introduce syntactic knowledge into a generative model and train the model in an unsupervised manner (Wu, 1997; Yamada and Knight, 2001; Lopez and Resnik, 2005; DeNero and Klein, 2007; Pauls et al., 2010). In this paper, we take dependency cohesion (Fox, 2002) into account, which assumes phrases dominated by disjoint dependency subtrees tend not to overlap after translation. Instead of treating dependency cohesion as a hard constraint (Lin and Cherry, 2003) or using it as a feature in discriminative models (Cherry and Lin, 2006b), we treat dependency cohesion as a distortion constraint, and integrate it into a modified HMM word alignment model to softly influence the probabilities of alignment candidates. We also propose an a"
Q13-1024,zhang-etal-2004-interpreting,0,0.014757,"M4 IBM4-L0 IBM4-Prior Agree-HMM Tree-Distance Hard-Cohesion Soft-Cohesion-EM Soft-Cohesion-Gibbs gold-standard forward HCP MCP 60.53 63.94 60.57 62.53 66.48 74.65 75.52 66.61 81.37 74.69 98.70 97.43 85.21 81.96 88.74 85.55 88.43 95.82 reverse HCP MCP 56.15 64.80 66.49 65.68 67.19 72.32 73.88 66.07 78.00 71.73 98.25 97.84 82.96 81.36 87.81 84.83 81.53 91.62 worse when using the LARGE training set. Our Soft-Cohesion-Gibbs system produces the best BLEU score when using both training sets. We also performed a statistical significance test using bootstrap resampling with 1000 samples (Koehn, 2004; Zhang et al., 2004). Experimental results show the Soft-Cohesion-Gibbs system is significantly better (p&lt;0.05) than the IBM4 system. The IBM4-Prior system slightly outperforms IBM4, but it’s not significant. FBIS LARGE IBM4 30.7 33.1 IBM4-L0 30.4 32.3 IBM4-Prior 30.9 33.2 Agree-HMM 27.2 30.1 Tree-Distance 28.2 N/A Hard-Cohesion 30.4 32.2 Soft-Cohesion-EM 30.9 33.1 Soft-Cohesion-Gibbs 31.6* 33.9* Table 7: BLEU scores, where * indicates significantly better than IBM4 (p&lt;0.05). Table 5: HCPs and MCPs on the development set. IBM4 IBM4-L0 IBM4-Prior Agree-HMM Hard-Cohesion Soft-Cohesion-EM Soft-Cohesion-Gibbs forward"
Q13-1024,D10-1058,0,0.206812,"t Model with Dependency Cohesion Constraint The most influential generative word alignment models are the IBM Models 1-5 and the HMM model (Brown et al., 1993; Vogel et al., 1996; Och and Ney, 2003). These models can be classified into sequence-based models (IBM Models 1, 2 and HMM) and fertility-based models (IBM Models 3, 4 and 5). The sequence-based model is easier to implement, and recent experiments have shown that appropriately modified sequence-based model can produce comparable performance with fertility-based models (Lopez and Resnik, 2005; Liang et al., 2006; DeNero and Klein, 2007; Zhao and Gildea, 2010; Bansal et al., 2011). So we built a generative word alignment model with dependency cohesion constraint based on the sequence-based model. 3.1 The Sequence-based Alignment Model According to Brown et al. (1993) and Och and Ney (2003), the sequence-based model is built as a noisy channel model, where the source sentence ?1? and the alignment ?1? are generated conditioning on the target sentence ?1? . The model assumes each source word is assigned to exactly one target word, and defines an asymmetric alignment for the sentence pair as ?1? = ?1 , ?2 , … , ?? , … , ?? , where each ?? ∈ [0, ?] is"
Q13-1024,I11-1140,1,\N,Missing
Q13-1024,C10-2148,1,\N,Missing
Q19-1002,P17-2021,0,0.0274704,"ledge can significantly improve a strong attention-based sequenceto-sequence neural translation model. 1 Introduction It is intuitive that semantic representations ought to be relevant to machine translation, given that the task is to produce a target language sentence with the same meaning as the source language input. Semantic representations formed the core of the earliest symbolic machine translation systems, and have been applied to statistical but non-neural systems as well. Leveraging syntax for neural machine translation (NMT) has been an active research topic (Stahlberg et al., 2016; Aharoni and Goldberg, 2017; Li et al., 2017; Chen et al., 2017; Bastings et al., 2017; Wu et al., 2017; Chen et al., 2018). On the other hand, exploring semantics for NMT has so far received relatively little attention. Recently, Marcheggiani et al. (2018) exploited semantic role labeling (SRL) for NMT, showing that the predicate–argument information 1 That is, flipping arguments corresponding to different roles. 19 Transactions of the Association for Computational Linguistics, vol. 7, pp. 19–31, 2019. Action Editor: Philipp Koehn. Submission batch: 6/2018; Revision batch: 10/2018; Published 3/2019. c 2019 Association"
Q19-1002,D15-1198,0,0.0207633,"MRs capture more relations, such as the relation between John and wife (represented by the subgraph within dotted lines). In addition, AMRs directly capture entity relations and abstract away inflections and function words. As a result, they can serve as a source of knowledge for machine translation that is orthogonal to the textual input. Furthermore, structural information from AMR graphs can help reduce data sparsity when training data is not sufficient for large-scale training. Recent advances in AMR parsing keep pushing the boundary of state-of-the-art performance (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015; Flanigan et al., 2016; Buys and Blunsom, 2017; Konstas et al., 2017; Wang and Xue, 2017; Lyu and Titov, 2018; It is intuitive that semantic representations can be useful for machine translation, mainly because they can help in enforcing meaning preservation and handling data sparsity (many sentences correspond to one meaning) of machine translation models. On the other hand, little work has been done on leveraging semantics for neural machine translation (NMT). In this work, we study the usefulness of AMR (abstract meaning representation) on NMT. Experim"
Q19-1002,J12-2006,0,0.0211864,"en training data are not sufficient. To our knowledge, we are the first to investigate AMR for NMT. Our code and parallel data (training/dev/test) with automatically parsed AMRs are available at https://github.com/freesunshine0316/semantic-nmt. 2 Figure 1: (a) A sentence with semantic roles annotations; (b) the corresponding AMR graph of that sentence. Related Work Most previous work on exploring semantics for statistical machine translation (SMT) studies the usefulness of predicate–argument structure from semantic role labeling (Wong and Mooney, 2006; Wu and Fung, 2009; Liu and Gildea, 2010; Baker et al., 2012). Jones et al. (2012) first convert Prolog expressions into graphical meaning representations, leveraging synchronous hyperedge replacement grammar to parse the input graphs while generating the outputs. Their graphical meaning representation is different from AMR under a strict definition, and their experimental data are limited to 880 sentences. We are the first to investigate AMR on a largescale machine translation task. Recently, Marcheggiani et al. (2018) investigated SRL on NMT. The predicate–argument structures are encoded via graph convolutional network (GCN) layers (Kipf and Welling,"
Q19-1002,W13-2322,0,0.484662,"ow@gmail.com 4 jssu@xmu.edu.cn Abstract from SRL can improve the performance of an attention-based sequence-to-sequence model by alleviating the ‘‘argument switching’’ problem,1 one frequent and severe issue faced by NMT systems (Isabelle et al., 2017). Figure 1(a) shows one example of semantic role information, which only captures the relations between a predicate (gave) and its arguments (John, wife, and present). Other important information, such as the relation between John and wife, cannot be incorporated. In this paper, we explore the usefulness of abstract meaning representation (AMR) (Banarescu et al., 2013) as a semantic representation for NMT. AMR is a semantic formalism that encodes the meaning of a sentence as a rooted, directed graph. Figure 1(b) shows an AMR graph, in which the nodes (such as give-01 and John) represent the concepts and edges (such as :ARG0 and :ARG1) represent the relations between concepts they connect. Comparing with semantic roles, AMRs capture more relations, such as the relation between John and wife (represented by the subgraph within dotted lines). In addition, AMRs directly capture entity relations and abstract away inflections and function words. As a result, they"
Q19-1002,S16-1186,0,0.244245,"ohn and wife (represented by the subgraph within dotted lines). In addition, AMRs directly capture entity relations and abstract away inflections and function words. As a result, they can serve as a source of knowledge for machine translation that is orthogonal to the textual input. Furthermore, structural information from AMR graphs can help reduce data sparsity when training data is not sufficient for large-scale training. Recent advances in AMR parsing keep pushing the boundary of state-of-the-art performance (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015; Flanigan et al., 2016; Buys and Blunsom, 2017; Konstas et al., 2017; Wang and Xue, 2017; Lyu and Titov, 2018; It is intuitive that semantic representations can be useful for machine translation, mainly because they can help in enforcing meaning preservation and handling data sparsity (many sentences correspond to one meaning) of machine translation models. On the other hand, little work has been done on leveraging semantics for neural machine translation (NMT). In this work, we study the usefulness of AMR (abstract meaning representation) on NMT. Experiments on a standard English-toGerman dataset show that incorpo"
Q19-1002,D17-1209,0,0.0409234,"eto-sequence neural translation model. 1 Introduction It is intuitive that semantic representations ought to be relevant to machine translation, given that the task is to produce a target language sentence with the same meaning as the source language input. Semantic representations formed the core of the earliest symbolic machine translation systems, and have been applied to statistical but non-neural systems as well. Leveraging syntax for neural machine translation (NMT) has been an active research topic (Stahlberg et al., 2016; Aharoni and Goldberg, 2017; Li et al., 2017; Chen et al., 2017; Bastings et al., 2017; Wu et al., 2017; Chen et al., 2018). On the other hand, exploring semantics for NMT has so far received relatively little attention. Recently, Marcheggiani et al. (2018) exploited semantic role labeling (SRL) for NMT, showing that the predicate–argument information 1 That is, flipping arguments corresponding to different roles. 19 Transactions of the Association for Computational Linguistics, vol. 7, pp. 19–31, 2019. Action Editor: Philipp Koehn. Submission batch: 6/2018; Revision batch: 10/2018; Published 3/2019. c 2019 Association for Computational Linguistics. Distributed under a CC-BY 4."
Q19-1002,P14-1134,0,0.0836165,"with semantic roles, AMRs capture more relations, such as the relation between John and wife (represented by the subgraph within dotted lines). In addition, AMRs directly capture entity relations and abstract away inflections and function words. As a result, they can serve as a source of knowledge for machine translation that is orthogonal to the textual input. Furthermore, structural information from AMR graphs can help reduce data sparsity when training data is not sufficient for large-scale training. Recent advances in AMR parsing keep pushing the boundary of state-of-the-art performance (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015; Flanigan et al., 2016; Buys and Blunsom, 2017; Konstas et al., 2017; Wang and Xue, 2017; Lyu and Titov, 2018; It is intuitive that semantic representations can be useful for machine translation, mainly because they can help in enforcing meaning preservation and handling data sparsity (many sentences correspond to one meaning) of machine translation models. On the other hand, little work has been done on leveraging semantics for neural machine translation (NMT). In this work, we study the usefulness of AMR (abstract meaning representat"
Q19-1002,P18-1026,0,0.190312,"se layer: − ← − → s 0 = W 1 [ h 0 ; h N ] + b1 , and Transformer (Vaswani et al., 2017) on classification and sequence labeling tasks; Song et al. (2018) build a GRN for encoding AMR graphs for text generation, showing that the representation is superior compared to BiLSTM on serialized AMR. We extend Song et al. (2018) by investigating the usefulness of AMR for neural machine translation. To our knowledge, we are the first to use GRN for machine translation. In addition to GRNs and GCNs, there have been other graph neural networks, such as graph gated neural network (GGNN) (Li et al., 2015b; Beck et al., 2018). Because our main concern is to empirically investigate the effectiveness of AMR for NMT, we leave it to future work to compare GCN, GGNN, and GRN for our task. 3 where W 1 and b1 are model parameters. For each decoding step m, the decoder feeds the concatenation of the embedding of the current input eym and the previous context vector ζ m−1 into the LSTM model to update its hidden state: Baseline: Attention-Based BiLSTM sm = LSTM(sm−1 , [eym ; ζ m−1 ]). We take the attention-based sequence-to-sequence model of Bahdanau et al. (2015) as the baseline, but use LSTM cells (Hochreiter and Schmidh"
Q19-1002,P18-1170,0,0.0223019,"2018; Zhang et al., 2018) for modeling AMRs rather than GCN, which is formally consistent with the RNN sentence encoder. Since there is no one-to-one correspondence between AMR nodes and source words, we adopt a doubly attentive LSTM decoder, which is another major difference from Marcheggiani et al. (2018). GRNs have recently been used to model graph structures in NLP tasks. In particular, Zhang et al. (2018) use a GRN model to represent raw sentences by building a graph structure of neighboring words and a sentence-level node, showing that the encoder outperforms BiLSTMs Peng et al., 2018; Groschwitz et al., 2018; Guo and Lu, 2018), and have made it possible for automatically generated AMRs to benefit downstream tasks, such as question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016), and event detection (Li et al., 2015a). However, to our knowledge, no existing work has exploited AMR for enhancing NMT. We fill in this gap, taking an attention-based sequence-to-sequence system as our baseline, which is similar to Bahdanau et al. (2015). To leverage knowledge within an AMR graph, we adopt a graph recurrent network (GRN) (Song et al., 2018; Zhang et al., 2018) as the AMR encoder. I"
Q19-1002,P17-1112,0,0.0195771,"ed by the subgraph within dotted lines). In addition, AMRs directly capture entity relations and abstract away inflections and function words. As a result, they can serve as a source of knowledge for machine translation that is orthogonal to the textual input. Furthermore, structural information from AMR graphs can help reduce data sparsity when training data is not sufficient for large-scale training. Recent advances in AMR parsing keep pushing the boundary of state-of-the-art performance (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015; Flanigan et al., 2016; Buys and Blunsom, 2017; Konstas et al., 2017; Wang and Xue, 2017; Lyu and Titov, 2018; It is intuitive that semantic representations can be useful for machine translation, mainly because they can help in enforcing meaning preservation and handling data sparsity (many sentences correspond to one meaning) of machine translation models. On the other hand, little work has been done on leveraging semantics for neural machine translation (NMT). In this work, we study the usefulness of AMR (abstract meaning representation) on NMT. Experiments on a standard English-toGerman dataset show that incorporating AMR as additional"
Q19-1002,D18-1198,0,0.0365651,") for modeling AMRs rather than GCN, which is formally consistent with the RNN sentence encoder. Since there is no one-to-one correspondence between AMR nodes and source words, we adopt a doubly attentive LSTM decoder, which is another major difference from Marcheggiani et al. (2018). GRNs have recently been used to model graph structures in NLP tasks. In particular, Zhang et al. (2018) use a GRN model to represent raw sentences by building a graph structure of neighboring words and a sentence-level node, showing that the encoder outperforms BiLSTMs Peng et al., 2018; Groschwitz et al., 2018; Guo and Lu, 2018), and have made it possible for automatically generated AMRs to benefit downstream tasks, such as question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016), and event detection (Li et al., 2015a). However, to our knowledge, no existing work has exploited AMR for enhancing NMT. We fill in this gap, taking an attention-based sequence-to-sequence system as our baseline, which is similar to Bahdanau et al. (2015). To leverage knowledge within an AMR graph, we adopt a graph recurrent network (GRN) (Song et al., 2018; Zhang et al., 2018) as the AMR encoder. In particular, a ful"
Q19-1002,P17-1177,0,0.0237591,"ntion-based sequenceto-sequence neural translation model. 1 Introduction It is intuitive that semantic representations ought to be relevant to machine translation, given that the task is to produce a target language sentence with the same meaning as the source language input. Semantic representations formed the core of the earliest symbolic machine translation systems, and have been applied to statistical but non-neural systems as well. Leveraging syntax for neural machine translation (NMT) has been an active research topic (Stahlberg et al., 2016; Aharoni and Goldberg, 2017; Li et al., 2017; Chen et al., 2017; Bastings et al., 2017; Wu et al., 2017; Chen et al., 2018). On the other hand, exploring semantics for NMT has so far received relatively little attention. Recently, Marcheggiani et al. (2018) exploited semantic role labeling (SRL) for NMT, showing that the predicate–argument information 1 That is, flipping arguments corresponding to different roles. 19 Transactions of the Association for Computational Linguistics, vol. 7, pp. 19–31, 2019. Action Editor: Philipp Koehn. Submission batch: 6/2018; Revision batch: 10/2018; Published 3/2019. c 2019 Association for Computational Linguistics. Distr"
Q19-1002,P82-1020,0,0.739452,"Missing"
Q19-1002,D17-1263,0,0.0258882,"feng Song,1 Daniel Gildea,1 Yue Zhang,2 Zhiguo Wang,3 and Jinsong Su4 1 Department of Computer Science, University of Rochester, Rochester, NY 14627 2 School of Engineering, Westlake University, China 3 IBM T.J. Watson Research Center, Yorktown Heights, NY 10598 4 Xiamen University, Xiamen, China 1 {lsong10,gildea}@cs.rochester.edu 2 yue.zhang@wias.org.cn 3 zgw.tomorrow@gmail.com 4 jssu@xmu.edu.cn Abstract from SRL can improve the performance of an attention-based sequence-to-sequence model by alleviating the ‘‘argument switching’’ problem,1 one frequent and severe issue faced by NMT systems (Isabelle et al., 2017). Figure 1(a) shows one example of semantic role information, which only captures the relations between a predicate (gave) and its arguments (John, wife, and present). Other important information, such as the relation between John and wife, cannot be incorporated. In this paper, we explore the usefulness of abstract meaning representation (AMR) (Banarescu et al., 2013) as a semantic representation for NMT. AMR is a semantic formalism that encodes the meaning of a sentence as a rooted, directed graph. Figure 1(b) shows an AMR graph, in which the nodes (such as give-01 and John) represent the co"
Q19-1002,D14-1179,0,0.0368049,"Missing"
Q19-1002,C12-1083,0,0.0622695,"not sufficient. To our knowledge, we are the first to investigate AMR for NMT. Our code and parallel data (training/dev/test) with automatically parsed AMRs are available at https://github.com/freesunshine0316/semantic-nmt. 2 Figure 1: (a) A sentence with semantic roles annotations; (b) the corresponding AMR graph of that sentence. Related Work Most previous work on exploring semantics for statistical machine translation (SMT) studies the usefulness of predicate–argument structure from semantic role labeling (Wong and Mooney, 2006; Wu and Fung, 2009; Liu and Gildea, 2010; Baker et al., 2012). Jones et al. (2012) first convert Prolog expressions into graphical meaning representations, leveraging synchronous hyperedge replacement grammar to parse the input graphs while generating the outputs. Their graphical meaning representation is different from AMR under a strict definition, and their experimental data are limited to 880 sentences. We are the first to investigate AMR on a largescale machine translation task. Recently, Marcheggiani et al. (2018) investigated SRL on NMT. The predicate–argument structures are encoded via graph convolutional network (GCN) layers (Kipf and Welling, 2017), which are laid"
Q19-1002,W14-3348,0,0.0650882,"Missing"
Q19-1002,P14-5010,0,0.00592251,"Missing"
Q19-1002,P17-4012,0,0.114376,"Missing"
Q19-1002,N18-2078,0,0.264809,"s to produce a target language sentence with the same meaning as the source language input. Semantic representations formed the core of the earliest symbolic machine translation systems, and have been applied to statistical but non-neural systems as well. Leveraging syntax for neural machine translation (NMT) has been an active research topic (Stahlberg et al., 2016; Aharoni and Goldberg, 2017; Li et al., 2017; Chen et al., 2017; Bastings et al., 2017; Wu et al., 2017; Chen et al., 2018). On the other hand, exploring semantics for NMT has so far received relatively little attention. Recently, Marcheggiani et al. (2018) exploited semantic role labeling (SRL) for NMT, showing that the predicate–argument information 1 That is, flipping arguments corresponding to different roles. 19 Transactions of the Association for Computational Linguistics, vol. 7, pp. 19–31, 2019. Action Editor: Philipp Koehn. Submission batch: 6/2018; Revision batch: 10/2018; Published 3/2019. c 2019 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. cantly improves a strong attention-based sequenceto-sequence baseline (25.5 vs 23.7 BLEU). When trained with small-scale (226K) data, the improvement increases"
Q19-1002,W04-3250,0,0.44934,"Missing"
Q19-1002,P17-1014,0,0.345035,"n dotted lines). In addition, AMRs directly capture entity relations and abstract away inflections and function words. As a result, they can serve as a source of knowledge for machine translation that is orthogonal to the textual input. Furthermore, structural information from AMR graphs can help reduce data sparsity when training data is not sufficient for large-scale training. Recent advances in AMR parsing keep pushing the boundary of state-of-the-art performance (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015; Flanigan et al., 2016; Buys and Blunsom, 2017; Konstas et al., 2017; Wang and Xue, 2017; Lyu and Titov, 2018; It is intuitive that semantic representations can be useful for machine translation, mainly because they can help in enforcing meaning preservation and handling data sparsity (many sentences correspond to one meaning) of machine translation models. On the other hand, little work has been done on leveraging semantics for neural machine translation (NMT). In this work, we study the usefulness of AMR (abstract meaning representation) on NMT. Experiments on a standard English-toGerman dataset show that incorporating AMR as additional knowledge can signifi"
Q19-1002,P02-1040,0,0.103866,"Missing"
Q19-1002,P17-1064,0,0.0168608,"ove a strong attention-based sequenceto-sequence neural translation model. 1 Introduction It is intuitive that semantic representations ought to be relevant to machine translation, given that the task is to produce a target language sentence with the same meaning as the source language input. Semantic representations formed the core of the earliest symbolic machine translation systems, and have been applied to statistical but non-neural systems as well. Leveraging syntax for neural machine translation (NMT) has been an active research topic (Stahlberg et al., 2016; Aharoni and Goldberg, 2017; Li et al., 2017; Chen et al., 2017; Bastings et al., 2017; Wu et al., 2017; Chen et al., 2018). On the other hand, exploring semantics for NMT has so far received relatively little attention. Recently, Marcheggiani et al. (2018) exploited semantic role labeling (SRL) for NMT, showing that the predicate–argument information 1 That is, flipping arguments corresponding to different roles. 19 Transactions of the Association for Computational Linguistics, vol. 7, pp. 19–31, 2019. Action Editor: Philipp Koehn. Submission batch: 6/2018; Revision batch: 10/2018; Published 3/2019. c 2019 Association for Computational"
Q19-1002,K15-1004,1,0.859168,"relation between John and wife (represented by the subgraph within dotted lines). In addition, AMRs directly capture entity relations and abstract away inflections and function words. As a result, they can serve as a source of knowledge for machine translation that is orthogonal to the textual input. Furthermore, structural information from AMR graphs can help reduce data sparsity when training data is not sufficient for large-scale training. Recent advances in AMR parsing keep pushing the boundary of state-of-the-art performance (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015; Flanigan et al., 2016; Buys and Blunsom, 2017; Konstas et al., 2017; Wang and Xue, 2017; Lyu and Titov, 2018; It is intuitive that semantic representations can be useful for machine translation, mainly because they can help in enforcing meaning preservation and handling data sparsity (many sentences correspond to one meaning) of machine translation models. On the other hand, little work has been done on leveraging semantics for neural machine translation (NMT). In this work, we study the usefulness of AMR (abstract meaning representation) on NMT. Experiments on a standard English-toGerman da"
Q19-1002,W15-4502,0,0.34139,", which is another major difference from Marcheggiani et al. (2018). GRNs have recently been used to model graph structures in NLP tasks. In particular, Zhang et al. (2018) use a GRN model to represent raw sentences by building a graph structure of neighboring words and a sentence-level node, showing that the encoder outperforms BiLSTMs Peng et al., 2018; Groschwitz et al., 2018; Guo and Lu, 2018), and have made it possible for automatically generated AMRs to benefit downstream tasks, such as question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016), and event detection (Li et al., 2015a). However, to our knowledge, no existing work has exploited AMR for enhancing NMT. We fill in this gap, taking an attention-based sequence-to-sequence system as our baseline, which is similar to Bahdanau et al. (2015). To leverage knowledge within an AMR graph, we adopt a graph recurrent network (GRN) (Song et al., 2018; Zhang et al., 2018) as the AMR encoder. In particular, a full AMR graph is considered as a single state, with nodes in the graph being its substates. State transitions are performed on the graph recurrently, allowing substates to exchange information through edges. At each r"
Q19-1002,P18-1171,1,0.850218,"a GRN (Song et al., 2018; Zhang et al., 2018) for modeling AMRs rather than GCN, which is formally consistent with the RNN sentence encoder. Since there is no one-to-one correspondence between AMR nodes and source words, we adopt a doubly attentive LSTM decoder, which is another major difference from Marcheggiani et al. (2018). GRNs have recently been used to model graph structures in NLP tasks. In particular, Zhang et al. (2018) use a GRN model to represent raw sentences by building a graph structure of neighboring words and a sentence-level node, showing that the encoder outperforms BiLSTMs Peng et al., 2018; Groschwitz et al., 2018; Guo and Lu, 2018), and have made it possible for automatically generated AMRs to benefit downstream tasks, such as question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016), and event detection (Li et al., 2015a). However, to our knowledge, no existing work has exploited AMR for enhancing NMT. We fill in this gap, taking an attention-based sequence-to-sequence system as our baseline, which is similar to Bahdanau et al. (2015). To leverage knowledge within an AMR graph, we adopt a graph recurrent network (GRN) (Song et al., 2018; Zhang et al., 20"
Q19-1002,C10-1081,1,0.785947,"viate data sparsity when training data are not sufficient. To our knowledge, we are the first to investigate AMR for NMT. Our code and parallel data (training/dev/test) with automatically parsed AMRs are available at https://github.com/freesunshine0316/semantic-nmt. 2 Figure 1: (a) A sentence with semantic roles annotations; (b) the corresponding AMR graph of that sentence. Related Work Most previous work on exploring semantics for statistical machine translation (SMT) studies the usefulness of predicate–argument structure from semantic role labeling (Wong and Mooney, 2006; Wu and Fung, 2009; Liu and Gildea, 2010; Baker et al., 2012). Jones et al. (2012) first convert Prolog expressions into graphical meaning representations, leveraging synchronous hyperedge replacement grammar to parse the input graphs while generating the outputs. Their graphical meaning representation is different from AMR under a strict definition, and their experimental data are limited to 880 sentences. We are the first to investigate AMR on a largescale machine translation task. Recently, Marcheggiani et al. (2018) investigated SRL on NMT. The predicate–argument structures are encoded via graph convolutional network (GCN) layer"
Q19-1002,D15-1136,0,0.0408605,"ations, such as the relation between John and wife (represented by the subgraph within dotted lines). In addition, AMRs directly capture entity relations and abstract away inflections and function words. As a result, they can serve as a source of knowledge for machine translation that is orthogonal to the textual input. Furthermore, structural information from AMR graphs can help reduce data sparsity when training data is not sufficient for large-scale training. Recent advances in AMR parsing keep pushing the boundary of state-of-the-art performance (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015; Flanigan et al., 2016; Buys and Blunsom, 2017; Konstas et al., 2017; Wang and Xue, 2017; Lyu and Titov, 2018; It is intuitive that semantic representations can be useful for machine translation, mainly because they can help in enforcing meaning preservation and handling data sparsity (many sentences correspond to one meaning) of machine translation models. On the other hand, little work has been done on leveraging semantics for neural machine translation (NMT). In this work, we study the usefulness of AMR (abstract meaning representation) on NMT. Experiments on a standard"
Q19-1002,P18-1037,0,0.181496,"y capture entity relations and abstract away inflections and function words. As a result, they can serve as a source of knowledge for machine translation that is orthogonal to the textual input. Furthermore, structural information from AMR graphs can help reduce data sparsity when training data is not sufficient for large-scale training. Recent advances in AMR parsing keep pushing the boundary of state-of-the-art performance (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015; Flanigan et al., 2016; Buys and Blunsom, 2017; Konstas et al., 2017; Wang and Xue, 2017; Lyu and Titov, 2018; It is intuitive that semantic representations can be useful for machine translation, mainly because they can help in enforcing meaning preservation and handling data sparsity (many sentences correspond to one meaning) of machine translation models. On the other hand, little work has been done on leveraging semantics for neural machine translation (NMT). In this work, we study the usefulness of AMR (abstract meaning representation) on NMT. Experiments on a standard English-toGerman dataset show that incorporating AMR as additional knowledge can significantly improve a strong attention-based s"
Q19-1002,P16-1162,0,0.0975416,"ns around 4.5 million sentence pairs for training. In addition, we use a subset of the full dataset (News Commentary v11 [NC-v11], containing around 243,000 sentence pairs) for development and additional experiments. For all experiments, we use newstest2013 and newstest2016 as the development and test sets, respectively. To preprocess the data, the tokenizer from Moses4 is used to tokenize both the English and German sides. The training sentence pairs where either side is longer than 50 words are filtered out after tokenization. To deal with rare and compound words, byte-pair encoding (BPE)5 (Sennrich et al., 2016) is applied to both sides. In particular, 8,000 and 16,000 BPE merges are used on the News Commentary v11 subset and the full training set, respectively. On the other hand, JAMR6 (Flanigan et al., 2016) is adopted to parse the English sentences into AMRs before BPE is applied. The statistics of the training data and vocabularies after preprocessing are shown in Tables 1 and 2, respectively. For the experiments with the full training set, we used the top 40K where el and ei are the embeddings of edge label l and source node vi , and W 4 and b4 are model parameters. 4.2 Training Incorporating AM"
Q19-1002,2006.amta-papers.25,0,0.0803469,"Missing"
Q19-1002,D17-1129,0,0.0841465,"dition, AMRs directly capture entity relations and abstract away inflections and function words. As a result, they can serve as a source of knowledge for machine translation that is orthogonal to the textual input. Furthermore, structural information from AMR graphs can help reduce data sparsity when training data is not sufficient for large-scale training. Recent advances in AMR parsing keep pushing the boundary of state-of-the-art performance (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015; Flanigan et al., 2016; Buys and Blunsom, 2017; Konstas et al., 2017; Wang and Xue, 2017; Lyu and Titov, 2018; It is intuitive that semantic representations can be useful for machine translation, mainly because they can help in enforcing meaning preservation and handling data sparsity (many sentences correspond to one meaning) of machine translation models. On the other hand, little work has been done on leveraging semantics for neural machine translation (NMT). In this work, we study the usefulness of AMR (abstract meaning representation) on NMT. Experiments on a standard English-toGerman dataset show that incorporating AMR as additional knowledge can significantly improve a str"
Q19-1002,N06-1056,0,0.151113,"Missing"
Q19-1002,P18-1150,1,0.928432,"ir graphical meaning representation is different from AMR under a strict definition, and their experimental data are limited to 880 sentences. We are the first to investigate AMR on a largescale machine translation task. Recently, Marcheggiani et al. (2018) investigated SRL on NMT. The predicate–argument structures are encoded via graph convolutional network (GCN) layers (Kipf and Welling, 2017), which are laid on top of regular BiRNN or CNN layers. Our work is in line with exploring semantic information, but different in exploiting AMR rather than SRL for NMT. In addition, we leverage a GRN (Song et al., 2018; Zhang et al., 2018) for modeling AMRs rather than GCN, which is formally consistent with the RNN sentence encoder. Since there is no one-to-one correspondence between AMR nodes and source words, we adopt a doubly attentive LSTM decoder, which is another major difference from Marcheggiani et al. (2018). GRNs have recently been used to model graph structures in NLP tasks. In particular, Zhang et al. (2018) use a GRN model to represent raw sentences by building a graph structure of neighboring words and a sentence-level node, showing that the encoder outperforms BiLSTMs Peng et al., 2018; Grosc"
Q19-1002,N09-2004,0,0.0443451,"n from AMR can alleviate data sparsity when training data are not sufficient. To our knowledge, we are the first to investigate AMR for NMT. Our code and parallel data (training/dev/test) with automatically parsed AMRs are available at https://github.com/freesunshine0316/semantic-nmt. 2 Figure 1: (a) A sentence with semantic roles annotations; (b) the corresponding AMR graph of that sentence. Related Work Most previous work on exploring semantics for statistical machine translation (SMT) studies the usefulness of predicate–argument structure from semantic role labeling (Wong and Mooney, 2006; Wu and Fung, 2009; Liu and Gildea, 2010; Baker et al., 2012). Jones et al. (2012) first convert Prolog expressions into graphical meaning representations, leveraging synchronous hyperedge replacement grammar to parse the input graphs while generating the outputs. Their graphical meaning representation is different from AMR under a strict definition, and their experimental data are limited to 880 sentences. We are the first to investigate AMR on a largescale machine translation task. Recently, Marcheggiani et al. (2018) investigated SRL on NMT. The predicate–argument structures are encoded via graph convolution"
Q19-1002,P16-2049,0,0.0266319,"g AMR as additional knowledge can significantly improve a strong attention-based sequenceto-sequence neural translation model. 1 Introduction It is intuitive that semantic representations ought to be relevant to machine translation, given that the task is to produce a target language sentence with the same meaning as the source language input. Semantic representations formed the core of the earliest symbolic machine translation systems, and have been applied to statistical but non-neural systems as well. Leveraging syntax for neural machine translation (NMT) has been an active research topic (Stahlberg et al., 2016; Aharoni and Goldberg, 2017; Li et al., 2017; Chen et al., 2017; Bastings et al., 2017; Wu et al., 2017; Chen et al., 2018). On the other hand, exploring semantics for NMT has so far received relatively little attention. Recently, Marcheggiani et al. (2018) exploited semantic role labeling (SRL) for NMT, showing that the predicate–argument information 1 That is, flipping arguments corresponding to different roles. 19 Transactions of the Association for Computational Linguistics, vol. 7, pp. 19–31, 2019. Action Editor: Philipp Koehn. Submission batch: 6/2018; Revision batch: 10/2018; Published"
Q19-1002,D16-1112,0,0.0991011,"Missing"
Q19-1002,P18-1030,1,0.718086,"g representation is different from AMR under a strict definition, and their experimental data are limited to 880 sentences. We are the first to investigate AMR on a largescale machine translation task. Recently, Marcheggiani et al. (2018) investigated SRL on NMT. The predicate–argument structures are encoded via graph convolutional network (GCN) layers (Kipf and Welling, 2017), which are laid on top of regular BiRNN or CNN layers. Our work is in line with exploring semantic information, but different in exploiting AMR rather than SRL for NMT. In addition, we leverage a GRN (Song et al., 2018; Zhang et al., 2018) for modeling AMRs rather than GCN, which is formally consistent with the RNN sentence encoder. Since there is no one-to-one correspondence between AMR nodes and source words, we adopt a doubly attentive LSTM decoder, which is another major difference from Marcheggiani et al. (2018). GRNs have recently been used to model graph structures in NLP tasks. In particular, Zhang et al. (2018) use a GRN model to represent raw sentences by building a graph structure of neighboring words and a sentence-level node, showing that the encoder outperforms BiLSTMs Peng et al., 2018; Groschwitz et al., 2018; G"
S16-2009,N09-1004,0,0.0251889,"ense Embedding Learning for Word Sense Induction 1 Linfeng Song1 , Zhiguo Wang2 , Haitao Mi2 and Daniel Gildea1 Department of Computer Science, University of Rochester, Rochester, NY 14627 2 IBM T.J. Watson Research Center, Yorktown Heights, NY 10598 Abstract are utilized in WSD tasks. WSI has been successfully applied to many NLP tasks such as machine translation (Xiong and Zhang, 2014), information retrieval (Navigli and Crisafulli, 2010) and novel sense detection (Lau et al., 2012). However, existing methods usually represent each instance with discrete hand-crafted features (Bordag, 2006; Chen et al., 2009; Van de Cruys and Apidianaki, 2011; Purandare and Pedersen, 2004), which are designed manually and require linguistic knowledge. Most previous methods require learning a specific model for each polysemous word, which limits their usability for downstream applications and loses the chance to jointly learn senses for multiple words. There is a great advance in recent distributed semantics, such as word embedding (Mikolov et al., 2013; Pennington et al., 2014) and sense embedding (Reisinger and Mooney, 2010; Huang et al., 2012; Jauhar et al., 2015; Rothe and Sch¨utze, 2015; Chen et al., 2014; Ti"
S16-2009,D14-1110,0,0.0217553,"06; Chen et al., 2009; Van de Cruys and Apidianaki, 2011; Purandare and Pedersen, 2004), which are designed manually and require linguistic knowledge. Most previous methods require learning a specific model for each polysemous word, which limits their usability for downstream applications and loses the chance to jointly learn senses for multiple words. There is a great advance in recent distributed semantics, such as word embedding (Mikolov et al., 2013; Pennington et al., 2014) and sense embedding (Reisinger and Mooney, 2010; Huang et al., 2012; Jauhar et al., 2015; Rothe and Sch¨utze, 2015; Chen et al., 2014; Tian et al., 2014). Comparing with word embedding, sense embedding methods learn distributed representations for senses of a polysemous word, which is similar to the sense centroid of WSI tasks. In this work, we point out that the WSI task and the sense embedding task are highly interrelated, and propose to jointly learn sense centroids (embeddings) of all polysemous words for the WSI task. Concretely, our method induces several sense centroids (embedding) for each polysemous word in training stage. In testing stage, our method represents each instance as a contextual vector, and induces its"
S16-2009,C14-1123,0,0.0578541,"w, and vc is the representation vector of the instance. Methodology Word Sense Induction WSI is generally considered as an unsupervised clustering task under the distributional hypothesis (Harris, 1954) that the word meaning is reflected by the set of contexts in which it appears. Existing WSI methods can be roughly divided into featurebased or Bayesian. Feature-based methods first represent each instance as a context vector, then utilize a clustering algorithm on the context vectors to induce all the senses. Bayesian methods (Brody and Lapata, 2009; Yao and Van Durme, 2011; Lau et al., 2012; Goyal and Hovy, 2014; Wang et al., 2015), on the other hand, discover senses based on topic models. They adopt either the LDA (Blei et al., 2003) or HDP (Teh et al., 2006) model by viewing each target word as a corpus and the contexts as pseudo-documents, where a context includes all words within a window centred by the target word. For sense induction, they first extract pseudo-documents for the target word, then train topic model, finally pick the most probable topic for each test pseudo-document as the sense. All of the existing WSI methods have two important factors: 1) how to group similar instances (cluster"
S16-2009,N15-1070,0,0.0201635,"with discrete hand-crafted features (Bordag, 2006; Chen et al., 2009; Van de Cruys and Apidianaki, 2011; Purandare and Pedersen, 2004), which are designed manually and require linguistic knowledge. Most previous methods require learning a specific model for each polysemous word, which limits their usability for downstream applications and loses the chance to jointly learn senses for multiple words. There is a great advance in recent distributed semantics, such as word embedding (Mikolov et al., 2013; Pennington et al., 2014) and sense embedding (Reisinger and Mooney, 2010; Huang et al., 2012; Jauhar et al., 2015; Rothe and Sch¨utze, 2015; Chen et al., 2014; Tian et al., 2014). Comparing with word embedding, sense embedding methods learn distributed representations for senses of a polysemous word, which is similar to the sense centroid of WSI tasks. In this work, we point out that the WSI task and the sense embedding task are highly interrelated, and propose to jointly learn sense centroids (embeddings) of all polysemous words for the WSI task. Concretely, our method induces several sense centroids (embedding) for each polysemous word in training stage. In testing stage, our method represents each ins"
S16-2009,P14-1023,0,0.236732,"rd sense disambiguation (WSD) assumes there exists an already-known sense inventory, and the sense of a word type is disambiguated according to the sense inventory. Therefore, clustering methods are generally applied in WSI tasks, while classification methods 85 Proceedings of the Fifth Joint Conference on Lexical and Computational Semantics (*SEM 2016), pages 85–90, Berlin, Germany, August 11-12, 2016. ods separately train a specific VSM for each word. No methods have shown distributional vectors can keep knowledge for multiple words while showing competitive performance. tributional models (Baroni et al., 2014), and (2) a general model for the whole vocabulary is jointly trained to induce sense centroids under the mutlitask learning framework (Caruana, 1997). Evaluated on SemEval-2010 WSI dataset, our method outperforms all participants and most of the recent state-of-the-art methods. 2 2.1 2.2 Sense Embedding for WSI As mentioned in Section 1, sense embedding methods learn a distributed representation for each sense of a polysemous word. There are two key factors for sense embedding learning: (1) how to decide the number of senses for each polysemous word and (2) how to learn an embedding represent"
S16-2009,S10-1079,0,0.0264613,"raditional methods for WSI tasks, the advantages of our method include: 1) WSI models for all the polysemous words are trained jointly under the multi-task learning framework; 2) distributed sense embeddings are taken as the knowledge representations which are trained discriminatively, and usually have better performance than traditional count-based distributional models (Baroni et al., 2014). To verify the two statements, we carefully designed comparative experiments described in the next section. 3 3.1 3.2 Comparing on SemEval-2010 We compare our methods with the following systems: (1) UoY (Korkontzelos and Manandhar, 2010) which is the best system in the SemEval2010 WSI competition; (2) NMFlib (Van de Cruys and Apidianaki, 2011) which adopts non-negative matrix factorization to factor a matrix and then conducts word sense clustering on the test set; (3) NB (Choe and Charniak, 2013) which adopts naive Bayes with the generative story that a context is generated by picking a sense and then all context words given the sense; and (4) Spectral (Goyal and Hovy, 2014) which applies spectral clustering on a set of distributional context vectors. Experimental results are shown in Table 1. Let us see the results on superv"
S16-2009,E06-1018,0,0.0790095,"Missing"
S16-2009,W15-1504,0,0.0378227,"Missing"
S16-2009,E09-1013,0,0.0267862,"where µ(wt , k) is the vector for the k-th sense centroid of word w, and vc is the representation vector of the instance. Methodology Word Sense Induction WSI is generally considered as an unsupervised clustering task under the distributional hypothesis (Harris, 1954) that the word meaning is reflected by the set of contexts in which it appears. Existing WSI methods can be roughly divided into featurebased or Bayesian. Feature-based methods first represent each instance as a context vector, then utilize a clustering algorithm on the context vectors to induce all the senses. Bayesian methods (Brody and Lapata, 2009; Yao and Van Durme, 2011; Lau et al., 2012; Goyal and Hovy, 2014; Wang et al., 2015), on the other hand, discover senses based on topic models. They adopt either the LDA (Blei et al., 2003) or HDP (Teh et al., 2006) model by viewing each target word as a corpus and the contexts as pseudo-documents, where a context includes all words within a window centred by the target word. For sense induction, they first extract pseudo-documents for the target word, then train topic model, finally pick the most probable topic for each test pseudo-document as the sense. All of the existing WSI methods have"
S16-2009,E12-1060,0,0.0146655,"centroid of word w, and vc is the representation vector of the instance. Methodology Word Sense Induction WSI is generally considered as an unsupervised clustering task under the distributional hypothesis (Harris, 1954) that the word meaning is reflected by the set of contexts in which it appears. Existing WSI methods can be roughly divided into featurebased or Bayesian. Feature-based methods first represent each instance as a context vector, then utilize a clustering algorithm on the context vectors to induce all the senses. Bayesian methods (Brody and Lapata, 2009; Yao and Van Durme, 2011; Lau et al., 2012; Goyal and Hovy, 2014; Wang et al., 2015), on the other hand, discover senses based on topic models. They adopt either the LDA (Blei et al., 2003) or HDP (Teh et al., 2006) model by viewing each target word as a corpus and the contexts as pseudo-documents, where a context includes all words within a window centred by the target word. For sense induction, they first extract pseudo-documents for the target word, then train topic model, finally pick the most probable topic for each test pseudo-document as the sense. All of the existing WSI methods have two important factors: 1) how to group simi"
S16-2009,N10-1013,0,0.196952,"-PPMI on both SR and VM. CRP-PPMI has higher PF mainly because it induces fewer number of senses. The above results prove that using sense embeddings have better performance than using count-based distributional models. Besides, SE-WSI-fix is significantly better than WE-Kmeans on every metric. As WEKmeans and SE-WSI-fix learn sense centroids in the same vectors space, while the latter performs joint learning. Therefore, the joint learning is better than learning separately. As for the unsupervised evaluations, SE-WSIfix achieves a good V-Measure score (VM) with a few induced senses. Pedersen (2010) points out that bad models can increase VM by increasing the number of clusters, but doing this will harm performance on both Paired F-score (PF) and SR. Even though UoY, NMFlib and NB show better VM, they (especially UoY) induced more senses than SE-WSI-fix. SE-WSI-fix has higher PF than all others, and higher SR than UoY and NMFlib . 4 Related Work K˚ageb¨ack et al. (2015) proposed two methods to utilize distributed representations for the WSI task. The first method learned centroid vectors by clustering all pre-computed context vectors of each target word. The other method simply adopted M"
S16-2009,D15-1200,0,0.170573,"lity distribution among all the senses for each instance, which can be seen as soft clustering algorithms. As for knowledge representation, existing WSI methods use the vector space model (VSM) to represent each context. In feature-based models, each instance is represented as a vector of values, where a value can be the count of a feature or the co-occurrence between two words. In Bayesian methods, the vectors are represented as co-occurrences between documents and senses or between senses and words. Overall existing methst = arg max sim(µ(wt , k), vc ) k=1,..,K (1) Another group of methods (Li and Jurafsky, 2015) employs non-parametric algorithms to dynamically decide the number of senses for each word, and each instance is assigned to a sense following a probability distribution in Equation 2, where St is the set of already generated senses for wt , and γ is a constant probability for generating a new sense for wt . ( p(k|µ(wt , k), vc ) ∀ k ∈ St st ∼ γ for new sense (2) From the above discussions, we can obviously notice that WSI task and sense embedding task are inter-related. The two factors in sense embedding learning can be aligned to the two factors of WSI task. Concretely, deciding the number"
S16-2009,P15-1173,0,0.0663194,"Missing"
S16-2009,S10-1011,0,0.223826,"Missing"
S16-2009,D10-1012,0,0.0843825,"Missing"
S16-2009,C14-1016,0,0.0236297,"09; Van de Cruys and Apidianaki, 2011; Purandare and Pedersen, 2004), which are designed manually and require linguistic knowledge. Most previous methods require learning a specific model for each polysemous word, which limits their usability for downstream applications and loses the chance to jointly learn senses for multiple words. There is a great advance in recent distributed semantics, such as word embedding (Mikolov et al., 2013; Pennington et al., 2014) and sense embedding (Reisinger and Mooney, 2010; Huang et al., 2012; Jauhar et al., 2015; Rothe and Sch¨utze, 2015; Chen et al., 2014; Tian et al., 2014). Comparing with word embedding, sense embedding methods learn distributed representations for senses of a polysemous word, which is similar to the sense centroid of WSI tasks. In this work, we point out that the WSI task and the sense embedding task are highly interrelated, and propose to jointly learn sense centroids (embeddings) of all polysemous words for the WSI task. Concretely, our method induces several sense centroids (embedding) for each polysemous word in training stage. In testing stage, our method represents each instance as a contextual vector, and induces its sense by finding th"
S16-2009,D14-1113,0,0.547899,"itask learning framework (Caruana, 1997). Evaluated on SemEval-2010 WSI dataset, our method outperforms all participants and most of the recent state-of-the-art methods. 2 2.1 2.2 Sense Embedding for WSI As mentioned in Section 1, sense embedding methods learn a distributed representation for each sense of a polysemous word. There are two key factors for sense embedding learning: (1) how to decide the number of senses for each polysemous word and (2) how to learn an embedding representation for each sense. To decide the number of senses in factor (1), one group of methods (Huang et al., 2012; Neelakantan et al., 2014) set a fixed number K of senses for each word, and each instance is assigned to the most probable sense according to Equation 1, where µ(wt , k) is the vector for the k-th sense centroid of word w, and vc is the representation vector of the instance. Methodology Word Sense Induction WSI is generally considered as an unsupervised clustering task under the distributional hypothesis (Harris, 1954) that the word meaning is reflected by the set of contexts in which it appears. Existing WSI methods can be roughly divided into featurebased or Bayesian. Feature-based methods first represent each insta"
S16-2009,P11-1148,0,0.044133,"Missing"
S16-2009,S10-1081,0,0.0243244,"than CRP-PPMI on both SR and VM. CRP-PPMI has higher PF mainly because it induces fewer number of senses. The above results prove that using sense embeddings have better performance than using count-based distributional models. Besides, SE-WSI-fix is significantly better than WE-Kmeans on every metric. As WEKmeans and SE-WSI-fix learn sense centroids in the same vectors space, while the latter performs joint learning. Therefore, the joint learning is better than learning separately. As for the unsupervised evaluations, SE-WSIfix achieves a good V-Measure score (VM) with a few induced senses. Pedersen (2010) points out that bad models can increase VM by increasing the number of clusters, but doing this will harm performance on both Paired F-score (PF) and SR. Even though UoY, NMFlib and NB show better VM, they (especially UoY) induced more senses than SE-WSI-fix. SE-WSI-fix has higher PF than all others, and higher SR than UoY and NMFlib . 4 Related Work K˚ageb¨ack et al. (2015) proposed two methods to utilize distributed representations for the WSI task. The first method learned centroid vectors by clustering all pre-computed context vectors of each target word. The other method simply adopted M"
S16-2009,Q15-1005,0,0.0201342,"sentation vector of the instance. Methodology Word Sense Induction WSI is generally considered as an unsupervised clustering task under the distributional hypothesis (Harris, 1954) that the word meaning is reflected by the set of contexts in which it appears. Existing WSI methods can be roughly divided into featurebased or Bayesian. Feature-based methods first represent each instance as a context vector, then utilize a clustering algorithm on the context vectors to induce all the senses. Bayesian methods (Brody and Lapata, 2009; Yao and Van Durme, 2011; Lau et al., 2012; Goyal and Hovy, 2014; Wang et al., 2015), on the other hand, discover senses based on topic models. They adopt either the LDA (Blei et al., 2003) or HDP (Teh et al., 2006) model by viewing each target word as a corpus and the contexts as pseudo-documents, where a context includes all words within a window centred by the target word. For sense induction, they first extract pseudo-documents for the target word, then train topic model, finally pick the most probable topic for each test pseudo-document as the sense. All of the existing WSI methods have two important factors: 1) how to group similar instances (clustering algorithm) and 2"
S16-2009,D14-1162,0,0.0881563,"sense detection (Lau et al., 2012). However, existing methods usually represent each instance with discrete hand-crafted features (Bordag, 2006; Chen et al., 2009; Van de Cruys and Apidianaki, 2011; Purandare and Pedersen, 2004), which are designed manually and require linguistic knowledge. Most previous methods require learning a specific model for each polysemous word, which limits their usability for downstream applications and loses the chance to jointly learn senses for multiple words. There is a great advance in recent distributed semantics, such as word embedding (Mikolov et al., 2013; Pennington et al., 2014) and sense embedding (Reisinger and Mooney, 2010; Huang et al., 2012; Jauhar et al., 2015; Rothe and Sch¨utze, 2015; Chen et al., 2014; Tian et al., 2014). Comparing with word embedding, sense embedding methods learn distributed representations for senses of a polysemous word, which is similar to the sense centroid of WSI tasks. In this work, we point out that the WSI task and the sense embedding task are highly interrelated, and propose to jointly learn sense centroids (embeddings) of all polysemous words for the WSI task. Concretely, our method induces several sense centroids (embedding) for"
S16-2009,P14-1137,0,0.0424369,"Missing"
S16-2009,W04-2406,0,0.0636251,"feng Song1 , Zhiguo Wang2 , Haitao Mi2 and Daniel Gildea1 Department of Computer Science, University of Rochester, Rochester, NY 14627 2 IBM T.J. Watson Research Center, Yorktown Heights, NY 10598 Abstract are utilized in WSD tasks. WSI has been successfully applied to many NLP tasks such as machine translation (Xiong and Zhang, 2014), information retrieval (Navigli and Crisafulli, 2010) and novel sense detection (Lau et al., 2012). However, existing methods usually represent each instance with discrete hand-crafted features (Bordag, 2006; Chen et al., 2009; Van de Cruys and Apidianaki, 2011; Purandare and Pedersen, 2004), which are designed manually and require linguistic knowledge. Most previous methods require learning a specific model for each polysemous word, which limits their usability for downstream applications and loses the chance to jointly learn senses for multiple words. There is a great advance in recent distributed semantics, such as word embedding (Mikolov et al., 2013; Pennington et al., 2014) and sense embedding (Reisinger and Mooney, 2010; Huang et al., 2012; Jauhar et al., 2015; Rothe and Sch¨utze, 2015; Chen et al., 2014; Tian et al., 2014). Comparing with word embedding, sense embedding m"
S16-2009,W11-1102,0,0.0490206,"Missing"
S16-2009,P12-1092,0,\N,Missing
S16-2009,D13-1148,0,\N,Missing
W10-4149,A00-2018,0,0.370444,"Missing"
W10-4149,P05-1022,0,0.13125,"Missing"
W10-4149,J03-4003,0,0.160504,"Missing"
W10-4149,D09-1087,0,0.0134932,"ors in automatically conversion trees are unavoidable and they would limit the accuracy of the self-trained model. So we have to take some measures to weight the gold target Treebank and the automatically conversion trees. McClosky et al. (2006) and Niu et al. (2009) take the strategy that duplicates the gold Treebank data many times. However, this strategy isn’t suitable for PCFG-LA parser 1 (Matsuzaki et al., 2005; Petrov et al., 2006), because PCFG-LA employs an EM algorithm in training stage, so duplicating gold Treebank would increase the training time tremendously. Instead, according to Huang and Harper (2009), we weight the posterior probabilities computed for the gold and automatically converted trees to balance their importance. Let count ( A o E |t ) be the count of rule A o E in a parse tree t . Tt and Ts ot are the sets of target Treebank and automatically converted trees from source Treebank respectively. The posterior probability of rule A o E (with weighting parameter D ) can be expressed as: 1 We will use BerkeleyParser as our baseline parser, which is a PCFG-LA based parser. Feature templates The label of the current constituent; The label of the left most child, the middle child and the"
W10-4149,J93-2004,0,0.034718,"Missing"
W10-4149,N06-1020,0,0.0328367,"rted into every tag in target Treebank with various probabilities. So there is a converting matrix representing the converting probabilities, and we can calculate the converting matrix through source Treebank and N-best conversion trees. 3.3 Corpus weighting technique In line 12 of Algorithm 1, we train a new parser with target Treebank and conversion trees. However, the errors in automatically conversion trees are unavoidable and they would limit the accuracy of the self-trained model. So we have to take some measures to weight the gold target Treebank and the automatically conversion trees. McClosky et al. (2006) and Niu et al. (2009) take the strategy that duplicates the gold Treebank data many times. However, this strategy isn’t suitable for PCFG-LA parser 1 (Matsuzaki et al., 2005; Petrov et al., 2006), because PCFG-LA employs an EM algorithm in training stage, so duplicating gold Treebank would increase the training time tremendously. Instead, according to Huang and Harper (2009), we weight the posterior probabilities computed for the gold and automatically converted trees to balance their importance. Let count ( A o E |t ) be the count of rule A o E in a parse tree t . Tt and Ts ot are the sets o"
W10-4149,P09-1006,0,0.0721924,"ing set. Experimental result shows their algorithm is effective. Collins et al. (1999) performed statistical constituency parsing of Czech on a Treebank that was converted from the Prague Dependency Treebank under the guidance of conversion rules and heuristic rules, and the final performance was also improved. Xia and Palmer (2001) proposed three methods to convert dependency trees into phrase structure trees with some hand-written heuristic rules. For acquisition of better conversion rules, Xia et al. (2008) proposed a method to automatically extract conversion rules from a target Treebank. Niu et al. (2009) tried to exploit heterogeneous Treebanks for parsing. They proposed a grammar formalism conversion algorithm to convert dependency formalism Treebank into phrase structure formalism, and did phrase structure parsing with the conversion trees. Their experiments are done in Chinese parsing, and the final performance is improved indeed. In summary, from the existing work we are confident that the strategies of self-training and Treebank conversion are effective to improve the performance of parser. 3 3.1 Our Strategy Parsing Algorithm Although self-training and Treebank Conversion are effective"
W10-4149,P06-1055,0,0.0279084,"e Treebank and N-best conversion trees. 3.3 Corpus weighting technique In line 12 of Algorithm 1, we train a new parser with target Treebank and conversion trees. However, the errors in automatically conversion trees are unavoidable and they would limit the accuracy of the self-trained model. So we have to take some measures to weight the gold target Treebank and the automatically conversion trees. McClosky et al. (2006) and Niu et al. (2009) take the strategy that duplicates the gold Treebank data many times. However, this strategy isn’t suitable for PCFG-LA parser 1 (Matsuzaki et al., 2005; Petrov et al., 2006), because PCFG-LA employs an EM algorithm in training stage, so duplicating gold Treebank would increase the training time tremendously. Instead, according to Huang and Harper (2009), we weight the posterior probabilities computed for the gold and automatically converted trees to balance their importance. Let count ( A o E |t ) be the count of rule A o E in a parse tree t . Tt and Ts ot are the sets of target Treebank and automatically converted trees from source Treebank respectively. The posterior probability of rule A o E (with weighting parameter D ) can be expressed as: 1 We will use Berk"
W10-4149,P94-1034,0,0.0930985,"train and development set Tdev (line 3). And we train an Algorithm 1 2:  initialize 3: {Ttrain , Tdev } m Split (Tt ) 5:  Iter iterations 6: for i m 1… Iter do i Ts ot m I 8: for k m 1… N do 10: 11: ParseList k m Nbest ( Parseri 1 , sk ) pˆ k i s ot T In line 10 of Algorithm 1, we select the highest quality parse pˆ k from ParseList k according to function Score( ps , ps ot ) , where ps denotes a tree in source Treebank and ps ot denotes a conversion tree with target Treebank grammar formalism for ps . Score( ps , ps ot ) compares taken ps as a reference. According to the idea proposed in Wang et al. (1994), we use the number of aligned constituents in the source and target trees to construct Score( ps , ps ot ) . We 4: Parser0 m Train(Ttrain , Tdev ) 9: Parse selection ps ot with ps and computes a score for ps ot 1: Input: Tt and Ts 7: 3.2 arg max p ParseList Score( ps , k , p j ) j k m pˆ k i 12: Parseri m Train (Ttrain , Tdev , Ts ot ) 13: return ParserIter initial parser with Ttrain and Tdev in line 4. From line 6 to line 12, we train parsers with SSPTC strategy Iter times iteratively. Let Tsiot be the automatically converted Treebank from source Treebank to target Treebank grammar formalis"
W10-4149,H01-1014,0,0.0715911,"Missing"
W10-4149,P04-1013,0,\N,Missing
W10-4149,P05-1010,0,\N,Missing
W10-4149,P99-1065,0,\N,Missing
W19-4805,D16-1011,0,0.0782085,"Missing"
W19-4805,N18-1100,0,0.0839713,"their attention weights. However, previous work has several limitations. Lin et al. (2017), for example, take single words as basic units, while meaningful information is usually carried by multi-word phrases. For instance, useful symptoms in Table 1, such as “bleeding after nasogastric tube insertion”, are larger than a single word. Another issue of Lin et al. (2017) is that their attention model is applied on the representation vectors produced by an LSTM. Each LSTM output contains more than just the information of that position, thus the real range for the highlighted position is unclear. Mullenbach et al. (2018) defines all 4-grams of the input text as basic units and uses a convolutional layer to learn their representations, which still suffers from fixed-length highlighting. Thus the explainability of the model is limited. Lei et al. (2016) introduce a regularizer over the selected (single-word) positions to encourage the model to extract larger phrases. However, their method can not tell how much a selected unit contributes to the model’s decision through a weight value. In this paper, we study what the meaningful units to highlight are. We define multi-granular ngrams as basic units, so that all"
W19-4805,D14-1162,0,0.0826462,"coder, all ngrams with the same order can be computed in parallel, and the model needs at most 7 iterative steps along the depth dimension for representing a given text of arbitrary length. 4 ACC #Param. 2.6 4.6 1.4 64.8 64.5 66.2 848,228 147,928 168,228 tokens, and one label out of five categories indicating which disease this document is about. We randomly split the dataset into train/dev/test sets by 8:1:1 for each category, and end up with 11,216/1,442/1,444 instances for each set. Hyperparameters We use the 300-dimensional GloVe word vectors pre-trained from the 840B Common Crawl corpus (Pennington et al., 2014), and set the hidden size as 100 for node embeddings. We apply dropout to every layer with a dropout ratio 0.2, and set the batch size as 50. We minimize the cross-entropy of the training set with the ADAM optimizer (Kingma and Ba, 2014), and set the learning rate is to 0.001. During training, the pre-trained word embeddings are not updated. (5) (6) Eval Time 57.0 92.1 30.3 Table 2: Efficiency evaluation. (4) c=i u+f h +f h Train Time 4.1 Properties of the multi-granular encoder Influence of the n-gram order: For CNN and our LeftForest encoder, we vary the order of ngrams from 1 to 9, and plot"
W19-4805,P15-1150,0,0.13561,"Missing"
