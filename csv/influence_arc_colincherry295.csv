2008.amta-papers.4,P98-1035,0,0.157867,"Missing"
2008.amta-papers.4,P95-1031,0,0.0921724,"Missing"
2008.amta-papers.4,P05-1063,0,0.0304499,"Missing"
2008.amta-papers.4,N04-1035,0,0.00945573,"Missing"
2008.amta-papers.4,P06-1029,0,0.0611099,"Missing"
2008.amta-papers.4,H05-1049,0,0.0381745,"Missing"
2008.amta-papers.4,P03-1054,0,0.034085,"Missing"
2008.amta-papers.4,P04-1061,0,0.0609874,"Missing"
2008.amta-papers.4,N03-1017,0,0.00846665,"Missing"
2008.amta-papers.4,2004.tmi-1.8,0,0.0192825,"Missing"
2008.amta-papers.4,N04-1021,0,0.0695552,"P approximation to the language model P (x) = z P (x, z). This form of language model, with its syntactic hidden structure, might be able to outperform n-gram methods by capturing long-distance dependencies. Unfortunately, even when tested in domain, scores from generative Treebank parsers fail to consistently rank real sentences above pseudo-negatives, as we show in §5.2. There have been other indications that parse scores do not always behave as useful language models. While investigating the poor performance of parser score as a re-ranking feature for statistical machine translation (SMT), Och et al. (2004) discovered that their Treebank parser had a tendency to rank MT outputs above human reference translations. This is attributed to the fact that MT outputs use fewer unseen words, which causes the parser’s terminal productions to dominate any syntactic distinctions. Attempts to normalize for word choice did not improve the performance of parser probability as a translation feature. Sentence length is another factor that could produce spurious differences in parser probability; however, SMT systems already include a length term in their discriminatively weighted linear models, so this does not"
2008.amta-papers.4,2003.mtsummit-papers.6,0,0.458548,"Missing"
2008.amta-papers.4,D08-1076,0,0.0258286,"Missing"
2008.amta-papers.4,P07-1010,0,0.112831,"ences, as sampled sentences include both syntactic disfluencies and semantic nonsense, such as: We construct a discriminative, syntactic language model (LM) by using a latent support vector machine (SVM) to train an unlexicalized parser to judge sentences. That is, the parser is optimized so that correct sentences receive high-scoring trees, while incorrect sentences do not. Because of this alternative objective, the parser can be trained with only a part-of-speech dictionary and binary-labeled sentences. We follow the paradigm of discriminative language modeling with pseudonegative examples (Okanohara and Tsujii, 2007), and demonstrate significant improvements in distinguishing real sentences from pseudo-negatives. We also investigate the related task of separating machine-translation (MT) outputs from reference translations, again showing large improvements. Finally, we test our LM in MT reranking, and investigate the language-modeling parser in the context of unsupervised parsing. 1 • Basically, we are a fighter jet. • The shortage of topsoil moisture in a personal basis, unlike his book, and extended last summer’s semiconductor’s partner in a big banks. Discriminating between sampled and human sentences"
2008.amta-papers.4,J01-2004,0,0.172013,"Missing"
2008.amta-papers.4,W04-3201,0,0.065068,"Missing"
2008.amta-papers.4,C98-1035,0,\N,Missing
2008.amta-papers.4,W06-3114,0,\N,Missing
2008.amta-papers.4,P01-1017,0,\N,Missing
2016.amta-researchers.8,D11-1033,0,0.260007,"high-quality data and picking data that ensure the SMT system is well-adapted to a given domain, have often been achieved separately. For instance, the papers (Munteanu and Marcu, 2005; Khadivi and Ney, 2005; Okita et al., 2009; Jiang et al., 2010; Denkowski et al., 2012) focus on reducing the noise in the data. They use different scoring functions, such as language model perplexity, word alignment score, or IBM model 1 score, to score each sentence pair, top scored sentence pairs are selected. While the papers (Zhao et al., 2004; L¨u et al., 2007; Yasuda et al., 2008; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) focus on domain adaptation. They all select monolingual or bilingual data that 1 Within each sentence pair, the target-language sentence is a good translation of the source sentence 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S are similar to the in-domain data according to some criterion. These state-of-the-art adaptive data selection approaches (Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) search for bilingual parallel sentences using the difference in language model perplexity betwee"
2016.amta-researchers.8,W15-3003,0,0.0599708,"ensure the SMT system is well-adapted to a given domain, have often been achieved separately. For instance, the papers (Munteanu and Marcu, 2005; Khadivi and Ney, 2005; Okita et al., 2009; Jiang et al., 2010; Denkowski et al., 2012) focus on reducing the noise in the data. They use different scoring functions, such as language model perplexity, word alignment score, or IBM model 1 score, to score each sentence pair, top scored sentence pairs are selected. While the papers (Zhao et al., 2004; L¨u et al., 2007; Yasuda et al., 2008; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) focus on domain adaptation. They all select monolingual or bilingual data that 1 Within each sentence pair, the target-language sentence is a good translation of the source sentence 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S are similar to the in-domain data according to some criterion. These state-of-the-art adaptive data selection approaches (Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) search for bilingual parallel sentences using the difference in language model perplexity between two language models trained on in-domai"
2016.amta-researchers.8,K16-1031,1,0.892142,"15) search for bilingual parallel sentences using the difference in language model perplexity between two language models trained on in-domain and out-domain data, respectively. Furthermore, (Duh et al., 2013) extends these approaches from n-gram models to recurrent neural network language models (Mikolov et al., 2010). While some previous work considers achieving the two goals simultaneously, such as (Mansour et al., 2011) which uses IBM model 1 and a language model to do data selection, (Durrani et al., 2015) uses a neural network joint model to select the in-domain data. In a recent paper (Chen and Huang, 2016), we describe one type of neural network for carrying out data selection: a semi-supervised Convolutional Neural Network (SSCNN) that is trained on the in-domain set to score one side of each sentence in a general-domain bilingual corpus (either the source side or the target side) for its suitability as training material for an SMT system. The highest-scoring sentence pairs are chosen to train the SMT system. Experiments described in that paper, covering three different types of test domain and four language directions, show that this SSCNN method yields signiﬁcantly higher BLEU scores for the"
2016.amta-researchers.8,W12-3131,0,0.0978263,"an English-to-French WMT task when Bi-SSCNNs were used. 1 Introduction When building a statistical machine translation (SMT) system, it is important to choose bilingual training data that are of high quality 1 and that are typical of the domain in which the SMT system will operate. In previous work, these two goals of data selection, i.e., picking high-quality data and picking data that ensure the SMT system is well-adapted to a given domain, have often been achieved separately. For instance, the papers (Munteanu and Marcu, 2005; Khadivi and Ney, 2005; Okita et al., 2009; Jiang et al., 2010; Denkowski et al., 2012) focus on reducing the noise in the data. They use different scoring functions, such as language model perplexity, word alignment score, or IBM model 1 score, to score each sentence pair, top scored sentence pairs are selected. While the papers (Zhao et al., 2004; L¨u et al., 2007; Yasuda et al., 2008; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) focus on domain adaptation. They all select monolingual or bilingual data that 1 Within each sentence pair, the target-language sentence is a good translation of the source sentence 3URFHHGLQJVRI$07$YRO"
2016.amta-researchers.8,P14-1129,0,0.0302692,"a subset of data to be used for training an SMT system from a bilingual corpus, the user must specify the number N of sentence pairs to be chosen. The N sentence pairs with the highest global scores S(s, t) will be selected. This method is symmetrical - the roles of the source-language and target-language sides of the corpus are the same - and bilingual, because the IBM model 1 measures the degree to which each target sentence t is a good translation of its partner s, and vice versa. 2.2 Data Selection with Neural Net Joint Model (NNJM) The Neural Network Joint Model (NNJM), as described in (Devlin et al., 2014), is a joint language and translation model based on a feedforward neural net (NN). It incorporats a wide span of contextual information from the source sentence, in addition to the traditional n-gram information from preceding target-language words. Speciﬁcally, when scoring a target word wi , the NNJM inputs not only the n − 1 preceding words wi−n+1 , ..., wi−1 , but also 2m + 1 source words: the source word si most closely aligned with wi along with the m source words si−m , ..., si−1 to the left of si and the m source words si+1 , ..., si+m to the right of si . The NNJMs used in our experi"
2016.amta-researchers.8,P13-2119,0,0.133955,"picking data that ensure the SMT system is well-adapted to a given domain, have often been achieved separately. For instance, the papers (Munteanu and Marcu, 2005; Khadivi and Ney, 2005; Okita et al., 2009; Jiang et al., 2010; Denkowski et al., 2012) focus on reducing the noise in the data. They use different scoring functions, such as language model perplexity, word alignment score, or IBM model 1 score, to score each sentence pair, top scored sentence pairs are selected. While the papers (Zhao et al., 2004; L¨u et al., 2007; Yasuda et al., 2008; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) focus on domain adaptation. They all select monolingual or bilingual data that 1 Within each sentence pair, the target-language sentence is a good translation of the source sentence 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S are similar to the in-domain data according to some criterion. These state-of-the-art adaptive data selection approaches (Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) search for bilingual parallel sentences using the difference in language model perplexity between two language mod"
2016.amta-researchers.8,2015.mtsummit-papers.10,0,0.686015,"of-the-art adaptive data selection approaches (Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) search for bilingual parallel sentences using the difference in language model perplexity between two language models trained on in-domain and out-domain data, respectively. Furthermore, (Duh et al., 2013) extends these approaches from n-gram models to recurrent neural network language models (Mikolov et al., 2010). While some previous work considers achieving the two goals simultaneously, such as (Mansour et al., 2011) which uses IBM model 1 and a language model to do data selection, (Durrani et al., 2015) uses a neural network joint model to select the in-domain data. In a recent paper (Chen and Huang, 2016), we describe one type of neural network for carrying out data selection: a semi-supervised Convolutional Neural Network (SSCNN) that is trained on the in-domain set to score one side of each sentence in a general-domain bilingual corpus (either the source side or the target side) for its suitability as training material for an SMT system. The highest-scoring sentence pairs are chosen to train the SMT system. Experiments described in that paper, covering three different types of test domain"
2016.amta-researchers.8,2012.amta-papers.7,1,0.926825,"Missing"
2016.amta-researchers.8,2010.eamt-1.26,0,0.0690526,"ent over 3 points on an English-to-French WMT task when Bi-SSCNNs were used. 1 Introduction When building a statistical machine translation (SMT) system, it is important to choose bilingual training data that are of high quality 1 and that are typical of the domain in which the SMT system will operate. In previous work, these two goals of data selection, i.e., picking high-quality data and picking data that ensure the SMT system is well-adapted to a given domain, have often been achieved separately. For instance, the papers (Munteanu and Marcu, 2005; Khadivi and Ney, 2005; Okita et al., 2009; Jiang et al., 2010; Denkowski et al., 2012) focus on reducing the noise in the data. They use different scoring functions, such as language model perplexity, word alignment score, or IBM model 1 score, to score each sentence pair, top scored sentence pairs are selected. While the papers (Zhao et al., 2004; L¨u et al., 2007; Yasuda et al., 2008; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) focus on domain adaptation. They all select monolingual or bilingual data that 1 Within each sentence pair, the target-language sentence is a good translation of the source sentence 3URF"
2016.amta-researchers.8,N15-1011,0,0.0239437,"de up of equal contributions from these four models. Since this metric contains information about the translation relationship between each source sentence and its target counterpart, and since the ways in which the source and target languages are used are mirror images of each other, the NNJM data selection method is both bilingual and symmetrical. 2.3 Data Selection with Semi-Supervised CNN As described in more detail in (Chen and Huang, 2016), we were inspired by the success of convolutional neural networks (CNNs) applied to image and text classiﬁcation (Krizhevsky et al., 2012; Kim, 2014; Johnson and Zhang, 2015a,b) to use CNNs to classify training sentences in either the source language or the target language as in-domain or out-of-domain. Convolutional neural networks (CNNs) (LeCun and Bengio, 1998) are feed-forward neural networks that exploit the internal structure of data through convolutional and pooling layers; each computation unit of the convolutional layer processes a small region of the input data. When CNNs are applied to text input, the convolution layers process small regions of a document, i.e., a sequence of words. CNNs are now used in many text classiﬁcation tasks (Kalchbrenner et al"
2016.amta-researchers.8,P14-1062,0,0.00850774,"on and Zhang, 2015a,b) to use CNNs to classify training sentences in either the source language or the target language as in-domain or out-of-domain. Convolutional neural networks (CNNs) (LeCun and Bengio, 1998) are feed-forward neural networks that exploit the internal structure of data through convolutional and pooling layers; each computation unit of the convolutional layer processes a small region of the input data. When CNNs are applied to text input, the convolution layers process small regions of a document, i.e., a sequence of words. CNNs are now used in many text classiﬁcation tasks (Kalchbrenner et al., 2014; Johnson and Zhang, 2015b; Wang et al., 2015). Chen and Huang (2016) use CNNs to classify sentence pairs to in-domain and out-of-domain sentence pairs. In many of these studies, the ﬁrst layer of the network converts words to word embeddings using table lookup; the embeddings are sometimes pre-trained on an unnlabeled data. The embeddings remain ﬁxed during subsequent model training. A CNN trained with small number of labled data and pre-trained word embeddings on large unlabeled data is termed “semi-supervised”. Because we were interested in data selection scenarios where only small amounts"
2016.amta-researchers.8,D14-1181,0,0.00555345,"pair is made up of equal contributions from these four models. Since this metric contains information about the translation relationship between each source sentence and its target counterpart, and since the ways in which the source and target languages are used are mirror images of each other, the NNJM data selection method is both bilingual and symmetrical. 2.3 Data Selection with Semi-Supervised CNN As described in more detail in (Chen and Huang, 2016), we were inspired by the success of convolutional neural networks (CNNs) applied to image and text classiﬁcation (Krizhevsky et al., 2012; Kim, 2014; Johnson and Zhang, 2015a,b) to use CNNs to classify training sentences in either the source language or the target language as in-domain or out-of-domain. Convolutional neural networks (CNNs) (LeCun and Bengio, 1998) are feed-forward neural networks that exploit the internal structure of data through convolutional and pooling layers; each computation unit of the convolutional layer processes a small region of the input data. When CNNs are applied to text input, the convolution layers process small regions of a document, i.e., a sequence of words. CNNs are now used in many text classiﬁcation"
2016.amta-researchers.8,W04-3250,0,0.208233,"f-the-art data selection method for domain adaptation (Axelrod et al., 2011). The “sum LM” variant uses the sum of the source and target LM scores for a sentence pair. 2. SSCNN: Data selection by semi-supervised CNN based on monolingual tokens (Section 2.3) 3. IBM-LM: Data selection by both IBM and language models (Section 2.1) 4. NNJM: Data selection by neural network joint models (Section 2.2) 5. Bi-SSCNN: Data selection by bitoken based semi-supervised CNN (Section 2.4) 3.3 Experimental results We evaluated the system using the BLEU (Papineni et al., 2002) score on the test set. Following (Koehn, 2004), we apply the bootstrap resampling test to do signiﬁcance testing. Table 2 summarizes the results for each task. The number of selected sentence pairs for each language pair (1.8 million pairs for Chinese-to-English, and 1.4 million pairs for Arabic-to-English) was decided on the basis of tests on held-out data using the IBM-LM method. That is, 1.8 million was the value of N that maximized the BLEU score of the ﬁnal SMT system when IBM-LM was used to select N sentence pairs as training data for Chinese-to-English, and 1.4 had the same property for Arabic-to-English. In the table, the bilingua"
2016.amta-researchers.8,D07-1036,0,0.0723693,"Missing"
2016.amta-researchers.8,2011.iwslt-papers.5,0,0.0415047,"_S are similar to the in-domain data according to some criterion. These state-of-the-art adaptive data selection approaches (Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) search for bilingual parallel sentences using the difference in language model perplexity between two language models trained on in-domain and out-domain data, respectively. Furthermore, (Duh et al., 2013) extends these approaches from n-gram models to recurrent neural network language models (Mikolov et al., 2010). While some previous work considers achieving the two goals simultaneously, such as (Mansour et al., 2011) which uses IBM model 1 and a language model to do data selection, (Durrani et al., 2015) uses a neural network joint model to select the in-domain data. In a recent paper (Chen and Huang, 2016), we describe one type of neural network for carrying out data selection: a semi-supervised Convolutional Neural Network (SSCNN) that is trained on the in-domain set to score one side of each sentence in a general-domain bilingual corpus (either the source side or the target side) for its suitability as training material for an SMT system. The highest-scoring sentence pairs are chosen to train the SMT s"
2016.amta-researchers.8,P10-2041,0,0.263994,"election, i.e., picking high-quality data and picking data that ensure the SMT system is well-adapted to a given domain, have often been achieved separately. For instance, the papers (Munteanu and Marcu, 2005; Khadivi and Ney, 2005; Okita et al., 2009; Jiang et al., 2010; Denkowski et al., 2012) focus on reducing the noise in the data. They use different scoring functions, such as language model perplexity, word alignment score, or IBM model 1 score, to score each sentence pair, top scored sentence pairs are selected. While the papers (Zhao et al., 2004; L¨u et al., 2007; Yasuda et al., 2008; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) focus on domain adaptation. They all select monolingual or bilingual data that 1 Within each sentence pair, the target-language sentence is a good translation of the source sentence 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S are similar to the in-domain data according to some criterion. These state-of-the-art adaptive data selection approaches (Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) search for bilingual parallel sentences using the difference in language m"
2016.amta-researchers.8,J05-4003,0,0.19056,"pairs, can beneﬁt NMT much more than SMT.We observed a BLEU improvement over 3 points on an English-to-French WMT task when Bi-SSCNNs were used. 1 Introduction When building a statistical machine translation (SMT) system, it is important to choose bilingual training data that are of high quality 1 and that are typical of the domain in which the SMT system will operate. In previous work, these two goals of data selection, i.e., picking high-quality data and picking data that ensure the SMT system is well-adapted to a given domain, have often been achieved separately. For instance, the papers (Munteanu and Marcu, 2005; Khadivi and Ney, 2005; Okita et al., 2009; Jiang et al., 2010; Denkowski et al., 2012) focus on reducing the noise in the data. They use different scoring functions, such as language model perplexity, word alignment score, or IBM model 1 score, to score each sentence pair, top scored sentence pairs are selected. While the papers (Zhao et al., 2004; L¨u et al., 2007; Yasuda et al., 2008; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) focus on domain adaptation. They all select monolingual or bilingual data that 1 Within each sentence pair, the target-lang"
2016.amta-researchers.8,W11-2124,0,0.186448,"nce. Essentially, it scores the extent to which both the source and target sentence are in-domain, but does not in any way penalize bad translations. We say that such a method is “symmetric”: it incorporates equal amounts of information from the source and the target language, but it is not “bilingual”: it does not incorporate information about the quality of translations. The main motivation for this paper is to explore CNN-based data selection techniques that are bilingual. It is based on semi-supervised CNNs that use bitokens as units instead of source or target words (Marino et al., 2006; Niehues et al., 2011). For the bitoken semi-supervised CNN, we should use the abbreviation “Bi-SSCNN”. We also experiment with the bilingual method that combines IBM model 1 and language model (LM) scores and neural network joint model. In this paper, we carried out experiments reported on two language pairs: Chinese-toEnglish and Arabic-to-English. We ﬁx the number of training sentences to be chosen for the data selection techniques so that they can be fairly compared, and measure the BLEU score on test data from the resulting MT systems. It turns out that three techniques have roughly the same performance in ter"
2016.amta-researchers.8,P02-1040,0,0.0972421,"ence as the criterion. This is considered to be a state-of-the-art data selection method for domain adaptation (Axelrod et al., 2011). The “sum LM” variant uses the sum of the source and target LM scores for a sentence pair. 2. SSCNN: Data selection by semi-supervised CNN based on monolingual tokens (Section 2.3) 3. IBM-LM: Data selection by both IBM and language models (Section 2.1) 4. NNJM: Data selection by neural network joint models (Section 2.2) 5. Bi-SSCNN: Data selection by bitoken based semi-supervised CNN (Section 2.4) 3.3 Experimental results We evaluated the system using the BLEU (Papineni et al., 2002) score on the test set. Following (Koehn, 2004), we apply the bootstrap resampling test to do signiﬁcance testing. Table 2 summarizes the results for each task. The number of selected sentence pairs for each language pair (1.8 million pairs for Chinese-to-English, and 1.4 million pairs for Arabic-to-English) was decided on the basis of tests on held-out data using the IBM-LM method. That is, 1.8 million was the value of N that maximized the BLEU score of the ﬁnal SMT system when IBM-LM was used to select N sentence pairs as training data for Chinese-to-English, and 1.4 had the same property fo"
2016.amta-researchers.8,W16-2323,0,0.0611686,"Missing"
2016.amta-researchers.8,2014.amta-researchers.3,1,0.757845,"h SSCNNs that take as input the bitokens of (Marino et al., 2006; Niehues et al., 2011). 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S Figure 2: Bitoken sequence. The paper (Niehues et al., 2011) describes a “bilingual language model” (biLM): the idea that SMT systems would beneﬁt from wider contextual information from the source sentence. BiLMs provide this context by aligning each target word in the training data with source words to create bitokens. An n-gram bitoken LM for the sequence of target words is then trained. Figure 2 (taken from (Stewart et al., 2014)) shows how a bitoken sequence is obtained from a word-aligned sentence pair for the English to French language pair. Unaligned target words (e.g., French word “d´’’ in the example) are aligned with NULL. Unaligned source words (e.g., “very”) are dropped. A source word aligned with more than one target word (e.g., “we”) aligned with two instances of “nous” is duplicated: each target word aligned with it receives a copy of that source word. The word embeddings for bitokens are learned directly by word2vec, treating each bitoken as a word. For instance, in the French sentence shown in Figure 2,"
2016.amta-researchers.8,P15-2058,0,0.0114968,"ng sentences in either the source language or the target language as in-domain or out-of-domain. Convolutional neural networks (CNNs) (LeCun and Bengio, 1998) are feed-forward neural networks that exploit the internal structure of data through convolutional and pooling layers; each computation unit of the convolutional layer processes a small region of the input data. When CNNs are applied to text input, the convolution layers process small regions of a document, i.e., a sequence of words. CNNs are now used in many text classiﬁcation tasks (Kalchbrenner et al., 2014; Johnson and Zhang, 2015b; Wang et al., 2015). Chen and Huang (2016) use CNNs to classify sentence pairs to in-domain and out-of-domain sentence pairs. In many of these studies, the ﬁrst layer of the network converts words to word embeddings using table lookup; the embeddings are sometimes pre-trained on an unnlabeled data. The embeddings remain ﬁxed during subsequent model training. A CNN trained with small number of labled data and pre-trained word embeddings on large unlabeled data is termed “semi-supervised”. Because we were interested in data selection scenarios where only small amounts of in-domain data are available, we chose to u"
2016.amta-researchers.8,I08-2088,0,0.0573278,"e two goals of data selection, i.e., picking high-quality data and picking data that ensure the SMT system is well-adapted to a given domain, have often been achieved separately. For instance, the papers (Munteanu and Marcu, 2005; Khadivi and Ney, 2005; Okita et al., 2009; Jiang et al., 2010; Denkowski et al., 2012) focus on reducing the noise in the data. They use different scoring functions, such as language model perplexity, word alignment score, or IBM model 1 score, to score each sentence pair, top scored sentence pairs are selected. While the papers (Zhao et al., 2004; L¨u et al., 2007; Yasuda et al., 2008; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) focus on domain adaptation. They all select monolingual or bilingual data that 1 Within each sentence pair, the target-language sentence is a good translation of the source sentence 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S are similar to the in-domain data according to some criterion. These state-of-the-art adaptive data selection approaches (Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) search for bilingual parallel sentences using the d"
2016.amta-researchers.8,C04-1059,0,0.0497328,"will operate. In previous work, these two goals of data selection, i.e., picking high-quality data and picking data that ensure the SMT system is well-adapted to a given domain, have often been achieved separately. For instance, the papers (Munteanu and Marcu, 2005; Khadivi and Ney, 2005; Okita et al., 2009; Jiang et al., 2010; Denkowski et al., 2012) focus on reducing the noise in the data. They use different scoring functions, such as language model perplexity, word alignment score, or IBM model 1 score, to score each sentence pair, top scored sentence pairs are selected. While the papers (Zhao et al., 2004; L¨u et al., 2007; Yasuda et al., 2008; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) focus on domain adaptation. They all select monolingual or bilingual data that 1 Within each sentence pair, the target-language sentence is a good translation of the source sentence 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S are similar to the in-domain data according to some criterion. These state-of-the-art adaptive data selection approaches (Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) search for b"
2020.emnlp-main.465,N19-1423,0,0.0337366,"n tasks, such as machine translation. Given a trained CMLM, however, it is not clear what the best inference strategy is. We formulate masked inference as a factorization of conditional probabilities of partial sequences, show that this does not harm performance, and investigate a number of simple heuristics motivated by this perspective. We identify a thresholding strategy that has advantages over the standard “mask-predict” algorithm, and provide analyses of its behavior on machine translation tasks. 1 Introduction The widely successful masked language modeling paradigm popularized by BERT (Devlin et al., 2019) has recently been adapted to conditional masked language model (CMLM) training for semiautoregressive sequence generation (Ghazvininejad et al., 2019), where model predictions are conditioned on the complete input sequence and the observed (non-masked) portion of the output sequence. The CMLM’s simplicity and its clear links to the very active field of linguistic representation learning are advantages over its semiautoregressive competitors, such as iterative refinement of token sequences (Lee et al., 2018), refinement of non-linguistic intermediate representations (Kaiser et al., 2018; Shu e"
2020.emnlp-main.465,D19-1633,0,0.358732,"ference as a factorization of conditional probabilities of partial sequences, show that this does not harm performance, and investigate a number of simple heuristics motivated by this perspective. We identify a thresholding strategy that has advantages over the standard “mask-predict” algorithm, and provide analyses of its behavior on machine translation tasks. 1 Introduction The widely successful masked language modeling paradigm popularized by BERT (Devlin et al., 2019) has recently been adapted to conditional masked language model (CMLM) training for semiautoregressive sequence generation (Ghazvininejad et al., 2019), where model predictions are conditioned on the complete input sequence and the observed (non-masked) portion of the output sequence. The CMLM’s simplicity and its clear links to the very active field of linguistic representation learning are advantages over its semiautoregressive competitors, such as iterative refinement of token sequences (Lee et al., 2018), refinement of non-linguistic intermediate representations (Kaiser et al., 2018; Shu et al., 2020) and learning to predict parallel edit operations (Stern et al., 2019; Gu et al., 2019). It is not obvious how to best perform inference wi"
2020.emnlp-main.465,D17-1036,1,0.810285,"rations T is less than the sentence length N it is semi-autoregressive; and when N = T it is fully autoregressive. Due to the use of a uniform distribution over reference contexts, training is agnostic to these different regimes. In general, we seek to minimize T without trading off too much quality. The challenge in doing so is to identify the subset of predictions that are most likely to provide suitable conditioning context for future iterations (Mansimov et al., 2019). Structural or linguistic dependencies in the output may also play an important role for resolving linguistic ambiguities (Martins and Kreutzer, 2017). M (t) {1,2,3} {} {1,2,3} {2,3} {3} {} {1,2,3} {2} {} t 0 1 0 1 2 3 0 1 2 • update-all: update tokens and scores at all positions, no constraint on new mask2 • update-masked: update tokens at masked positions only, no constraint on new mask3 • update-masked-sub: update tokens at masked positions only, new mask must be a subset of the current one In this paper we focus on the update-masked-sub strategy. It is empirically competitive (Section 4.1), and interesting because it corresponds to a valid probabilistic factorization of the target sequence, governed by a latent variable M = M (0) . . ."
2020.emnlp-main.465,W18-6319,0,0.0312515,"Missing"
2020.emnlp-main.465,D18-1149,0,0.233344,"uction The widely successful masked language modeling paradigm popularized by BERT (Devlin et al., 2019) has recently been adapted to conditional masked language model (CMLM) training for semiautoregressive sequence generation (Ghazvininejad et al., 2019), where model predictions are conditioned on the complete input sequence and the observed (non-masked) portion of the output sequence. The CMLM’s simplicity and its clear links to the very active field of linguistic representation learning are advantages over its semiautoregressive competitors, such as iterative refinement of token sequences (Lee et al., 2018), refinement of non-linguistic intermediate representations (Kaiser et al., 2018; Shu et al., 2020) and learning to predict parallel edit operations (Stern et al., 2019; Gu et al., 2019). It is not obvious how to best perform inference with the CMLM. Starting from a partially-observed output sequence, the optimal choice to complete it within a single step would be to generate the most likely token at each unobserved (masked) position independently. However, it is less clear how to progress from an initial, completely masked sequence to a final hypothesis semi-autoregressively over a number of"
2020.iwslt-1.27,P19-1126,1,0.860606,"Missing"
2020.iwslt-1.27,P18-1008,1,0.805687,"tandard WMT14 English-to-French (EnFr; 36.3M sentences) and WMT15 German-to-English (DeEn; 4.5M sentences) data. For EnFr, we use newstest 2012+2013 for development, and newstest 2014 for test. For DeEn, we validate on newstest 2013 and report results on newstest 2015. We use BPE (Sennrich et al., 2016) on the training data to construct a 32K-type vocabulary that is shared between the source and target languages. 5.1 Models Our streaming and re-translation models are implemented in Lingvo (Shen et al., 2019), sharing architecture and hyper-parameters wherever possible. Our RNMT+ architecture (Chen et al., 2018) consists of a 6 layer LSTM encoder and an 8 layer LSTM decoder with additive attention (Bahdanau et al., 2014). Both encoder and decoder LSTMs have 512 hidden units, apply per-gate layer normalization (Ba et al., 2016), and use residual skip connections after the second layer. The models are regularized using a dropout of 0.2 and label smoothing of 0.1 (Szegedy et al., 2016). Models are optimized using 32-way data parallelism with Google Cloud’s TPUv3, using Adam (Kingma and Ba, 2015) with the learning rate schedule described in Chen et al. (2018) and a batch size of 4,096 sentence-pairs. Che"
2020.iwslt-1.27,N18-2079,0,0.609248,"anslation, the goal is to translate an incoming stream of source words with as low latency as possible. A typical application is speech translation, where we often assume the eventual output modality to also be speech. In a speech-to-speech scenario, target words must be appended to existing output with no possibility for revision. The corresponding translation task, which we refer to as streaming translation, has received considerable recent attention, generating custom approaches designed to maximize quality and minimize latency (Cho and Esipova, 2016; Gu et al., ∗ Equal contributions 2017; Dalvi et al., 2018; Ma et al., 2019a). However, for applications where the output modality is text, such as live captioning, the prohibition against revising output is overly stringent. The ability to revise previous partial translations makes simply re-translating each successive source prefix a viable strategy. Compared to streaming models, re-translation has the advantage of low latency, since it always attempts a translation of the complete source prefix, and high final-translation quality, since it is not restricted to preserving previous output. It has the disadvantages of higher computational cost, and a"
2020.iwslt-1.27,E17-1099,0,0.428399,"ion techniques that have not previously been studied together. (2) We provide the first empirical comparison of re-translation and streaming models, demonstrating that re-translation operating in a very low-revision regime can match or beat the quality-latency trade-offs of streaming models. (3) We test a 0-revision configuration of re-translation, and show that it is surprisingly competitive, due to the effectiveness of data augmentation with prefix pairs. 2 Related Work Cho and Esipova (2016) propose the first streaming techniques for NMT, using heuristic agents based on model scores, while Gu et al. (2017) extend their work with agents learned using reinforcement learning. Ma et al. (2019a) recently broke new ground by integrating their read-write agent directly into NMT training. Similar to Dalvi et al. (2018), they employ a simple agent that first reads k source to220 Proceedings of the 17th International Conference on Spoken Language Translation (IWSLT), pages 220–227 c July 9-10, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 kens, and then proceeds to alternate between writes and reads until the source sentence has finished. This agent is easily integr"
2020.iwslt-1.27,P02-1040,0,0.108727,"veral latency metrics content-aware, including average proportion (Cho and Esipova, 2016), consecutive wait (Gu et al., 2017), average lagging (Ma et al., 2019a), and differentiable average lagging (Arivazhagan et al., 2019b). We opt for differentiable average lagging (DAL) because of its interpretability and because it sidesteps some problems with average lagging (Cherry and Foster, 2019). It can be thought of as the average number of source tokens a system lags behind a perfectly simultaneous translator:  J  1 X 0 j−1 DAL = gj − J γ j=1 Translation quality is measured by calculating BLEU (Papineni et al., 2002) on the final output of each PTL; that is, standard corpus-level BLEU on complete translations. Specifically, we report tokenized, cased BLEU calculated by an internal tool. We make no attempt to directly measure the quality where γ = J/I accounts for the source and target having different lengths, and g 0 adjusts g to incorporate a minimal time cost of γ1 for each token:  gj  0  j=1 gj = 0 max gj , gj−1 + γ1 j &gt; 1 221 Source 1: Neue 2: Arzneimittel 3: k¨onnten 4: Lungen5: und 6: Eierstockkrebs 7: verlangsamen Content Delay Output New New Medicines New Medicines New drugs New drugs New drug"
2020.iwslt-1.27,P16-1162,0,0.0399594,"a randomly-selected fraction of their original lengths, 1/3 in this example. No effort is made to ensure that the two halves of the prefix pair are semantically equivalent. avoid confusion with Ma et al. (2019a)’s wait-k training, we refer to wait-k used for re-translation as wait-k inference.2 5 Experiments We use standard WMT14 English-to-French (EnFr; 36.3M sentences) and WMT15 German-to-English (DeEn; 4.5M sentences) data. For EnFr, we use newstest 2012+2013 for development, and newstest 2014 for test. For DeEn, we validate on newstest 2013 and report results on newstest 2015. We use BPE (Sennrich et al., 2016) on the training data to construct a 32K-type vocabulary that is shared between the source and target languages. 5.1 Models Our streaming and re-translation models are implemented in Lingvo (Shen et al., 2019), sharing architecture and hyper-parameters wherever possible. Our RNMT+ architecture (Chen et al., 2018) consists of a 6 layer LSTM encoder and an 8 layer LSTM decoder with additive attention (Bahdanau et al., 2014). Both encoder and decoder LSTMs have 512 hidden units, apply per-gate layer normalization (Ba et al., 2016), and use residual skip connections after the second layer. The mod"
2020.iwslt-1.27,D19-1137,0,0.734803,"reaming baselines above; and a more powerful Bidi+Beam system using bidirectional encoding and beam search of size 20, designed to test the impact of an improved base model. Training data is augmented through the proportional prefix training method unless stated otherwise (§ 4.1). Beam-search bias β is varied in the range 0.0 to 1.0 in increments of 0.2. When wait-k inference is enabled, k is varied in 1, 2, 4, 6, 8, 10, 15, 20, 30. Note that we do not need to re-train to test different values of β or k. 2 When wait-k truncation is combined with beam search, its behavior is similar to that of Zheng et al. (2019b): sequences are scored accounting for “future” tokens that will not be shown to the user. 223 5.2 Translation with few revisions Biased search and wait-k inference used together can reduce re-translation’s revisions, as measured by normalized erasure (NE in § 3.3), to negligible levels (Arivazhagan et al., 2019a). But how does retranslation compare to competing approaches? To answer this, we compare the quality-latency tradeoffs achieved by re-translation in a low-revision regime to those of our streaming baselines. First, we need a clear definition of low-revision re-translation. By manual"
2020.iwslt-1.27,D19-1144,0,0.309083,"reaming baselines above; and a more powerful Bidi+Beam system using bidirectional encoding and beam search of size 20, designed to test the impact of an improved base model. Training data is augmented through the proportional prefix training method unless stated otherwise (§ 4.1). Beam-search bias β is varied in the range 0.0 to 1.0 in increments of 0.2. When wait-k inference is enabled, k is varied in 1, 2, 4, 6, 8, 10, 15, 20, 30. Note that we do not need to re-train to test different values of β or k. 2 When wait-k truncation is combined with beam search, its behavior is similar to that of Zheng et al. (2019b): sequences are scored accounting for “future” tokens that will not be shown to the user. 223 5.2 Translation with few revisions Biased search and wait-k inference used together can reduce re-translation’s revisions, as measured by normalized erasure (NE in § 3.3), to negligible levels (Arivazhagan et al., 2019a). But how does retranslation compare to competing approaches? To answer this, we compare the quality-latency tradeoffs achieved by re-translation in a low-revision regime to those of our streaming baselines. First, we need a clear definition of low-revision re-translation. By manual"
2020.wmt-1.140,W05-0909,0,0.366937,"human evaluations for large scale comparisons and system development. The recent progress in MT has raised concerns about whether automated evaluation methodologies reliably reflect human ratings in high accuracy ranges. In particular, it has been observed that the best systems according to humans might fare less well with automated metrics (Barrault et al., 2019). Most metrics such as B LEU (Papineni et al., 2002) and TER (Snover et al., 2006) measure overlap between a system output and a human reference translation. More refined ways to compute such overlap have consequently been proposed (Banerjee and Lavie, 2005; Lo, 2019; Zhang et al., 2019). Orthogonal to the work of building improved metrics, Freitag et al. (2020) hypothesized that human references are also an important factor in the reliability of automated evaluations. In particular, they observed that standard references exhibit simple, monotonic language due to human ‘translationese‘ effects. These standard references might favor systems which excel at reproducing these effects, independent of the underlying translation quality. They showed that better correlation between human and automated evaluations could be obtained when replacing standar"
2020.wmt-1.140,P16-2013,0,0.0508388,"Missing"
2020.wmt-1.140,W19-5204,1,0.929349,"is work showed some specific cases where Translation Error Rate (TER) was superior to B LEU. Our work is also related to the bias that the human translation process introduces in the references, including source language artifacts— Translationese (Koppel and Ordan, 2011)—as well as source-independent artifacts—Translation Universals (Mauranen and Kujam¨aki, 2004). The professional translation community studies both systematic biases inherent to translated texts (Baker, 1993; Selinker, 1972), as well as biases resulting specifically from interference from the source text (Toury, 1995). For MT, Freitag et al. (2019) point at Translationese as a source of mismatch between BLEU and human evaluation, raising concerns that overlap-based metrics might reward hypotheses with translationese language more than hypotheses using more natural language. The impact of Translationese on human evaluation of MT has recently received attention as well (Toral et al., 2018; Zhang and Toral, 2019; Graham et al., 2019). More generally, the question of bias to a specific reference has also been raised, in the case of monolingual manual evaluation (Fomicheva and Specia, 2016; Ma et al., 2017). Different from the impact of Tran"
2020.wmt-1.140,2020.emnlp-main.5,1,0.864243,"ncerns about whether automated evaluation methodologies reliably reflect human ratings in high accuracy ranges. In particular, it has been observed that the best systems according to humans might fare less well with automated metrics (Barrault et al., 2019). Most metrics such as B LEU (Papineni et al., 2002) and TER (Snover et al., 2006) measure overlap between a system output and a human reference translation. More refined ways to compute such overlap have consequently been proposed (Banerjee and Lavie, 2005; Lo, 2019; Zhang et al., 2019). Orthogonal to the work of building improved metrics, Freitag et al. (2020) hypothesized that human references are also an important factor in the reliability of automated evaluations. In particular, they observed that standard references exhibit simple, monotonic language due to human ‘translationese‘ effects. These standard references might favor systems which excel at reproducing these effects, independent of the underlying translation quality. They showed that better correlation between human and automated evaluations could be obtained when replacing standard references with paraphrased references, even when still using surface overlap metrics such as BLEU (Papin"
2020.wmt-1.140,P17-1012,1,0.826853,"e-of-the-art English-German NMT components, we show that tuning to paraphrased references produces a system that is significantly better according to human judgment, but 5 BLEU points worse when tested on standard references. Our work confirms the finding that paraphrased references yield metric scores that correlate better with human judgment, and demonstrates for the first time that using these scores for system development can lead to significant improvements. 1 Introduction Machine Translation (MT) has shown impressive progress in recent years. Neural architectures (Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017) have greatly contributed to this improvement, especially for languages with abundant training data (Bojar et al., 2016, 2018; Barrault et al., 2019). This progress creates novel challenges for the evaluation of machine translation, both for human (Toral, 2020; L¨aubli et al., 2020) and automated evaluation protocols (Lo, 2019; Zhang et al., 2019). Both types of evaluation play an important role in machine translation (Koehn, 2010). While human evaluations provide a gold standard evaluation, they involve a fair amount of careful and hence expensive work by human assessor"
2020.wmt-1.140,W09-0421,0,0.0403691,"monolingual manual evaluation (Fomicheva and Specia, 2016; Ma et al., 2017). Different from the impact of Translationese on evaluation, the impact of Translationese in the training data has also been studied (Kurokawa et al., 2009; Lembersky et al., 2012a; Bogoychev and Sennrich, 2019; Riley et al., 2020). 1184 Finally, our work is also related to studies measuring the importance of the test data quality, looking specifically at the test set translation direction. For SMT evaluation, Lembersky et al. (2012b) and Stymne (2017) explored how the translation direction affects translation results. Holmqvist et al. (2009) noted that the original language of the test sentences influences the BLEU score of translations. They showed that the BLEU scores for targetoriginal sentences are on average higher than sentences that have their original source in a different language. Recently, a similar study was conducted for neural MT (Bogoychev and Sennrich, 2019). 3 Experimental Setup We first describe data and models, then present our human evaluation protocol. 3.1 Data We ran all experiments on the WMT 2019 English→German news translation task (Barrault et al., 2019). The task provides ∼38M parallel sentences. As Ger"
2020.wmt-1.140,J10-4005,0,0.0167351,". 1 Introduction Machine Translation (MT) has shown impressive progress in recent years. Neural architectures (Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017) have greatly contributed to this improvement, especially for languages with abundant training data (Bojar et al., 2016, 2018; Barrault et al., 2019). This progress creates novel challenges for the evaluation of machine translation, both for human (Toral, 2020; L¨aubli et al., 2020) and automated evaluation protocols (Lo, 2019; Zhang et al., 2019). Both types of evaluation play an important role in machine translation (Koehn, 2010). While human evaluations provide a gold standard evaluation, they involve a fair amount of careful and hence expensive work by human assessors. Cost therefore limits the scale of their application. On the other hand, automated evaluations are much less expensive. They typically only involve human labor when collecting human reference translations and can hence be run at scale to compare a wide range of systems or validate design decisions. The value of automatic evaluations therefore resides in their capacity to be used as a proxy for human evaluations for large scale comparisons and system d"
2020.wmt-1.140,P11-1132,0,0.0201247,"uch as possible‘ – is necessary for evaluation. Our work can be seen as replacing the regular BLEU metric with a new paraphrase BLEU metric for system tuning. Different alternative automatic evaluation metric have also been considered for system tuning (He and Way, 2010; Servan and Schwenk, 2011) with Minimum Error Rate Training, MERT (Och, 2003). This work showed some specific cases where Translation Error Rate (TER) was superior to B LEU. Our work is also related to the bias that the human translation process introduces in the references, including source language artifacts— Translationese (Koppel and Ordan, 2011)—as well as source-independent artifacts—Translation Universals (Mauranen and Kujam¨aki, 2004). The professional translation community studies both systematic biases inherent to translated texts (Baker, 1993; Selinker, 1972), as well as biases resulting specifically from interference from the source text (Toury, 1995). For MT, Freitag et al. (2019) point at Translationese as a source of mismatch between BLEU and human evaluation, raising concerns that overlap-based metrics might reward hypotheses with translationese language more than hypotheses using more natural language. The impact of Trans"
2020.wmt-1.140,2009.mtsummit-papers.9,0,0.0764009,"s that overlap-based metrics might reward hypotheses with translationese language more than hypotheses using more natural language. The impact of Translationese on human evaluation of MT has recently received attention as well (Toral et al., 2018; Zhang and Toral, 2019; Graham et al., 2019). More generally, the question of bias to a specific reference has also been raised, in the case of monolingual manual evaluation (Fomicheva and Specia, 2016; Ma et al., 2017). Different from the impact of Translationese on evaluation, the impact of Translationese in the training data has also been studied (Kurokawa et al., 2009; Lembersky et al., 2012a; Bogoychev and Sennrich, 2019; Riley et al., 2020). 1184 Finally, our work is also related to studies measuring the importance of the test data quality, looking specifically at the test set translation direction. For SMT evaluation, Lembersky et al. (2012b) and Stymne (2017) explored how the translation direction affects translation results. Holmqvist et al. (2009) noted that the original language of the test sentences influences the BLEU score of translations. They showed that the BLEU scores for targetoriginal sentences are on average higher than sentences that have"
2020.wmt-1.140,E12-1026,0,0.0255666,"trics might reward hypotheses with translationese language more than hypotheses using more natural language. The impact of Translationese on human evaluation of MT has recently received attention as well (Toral et al., 2018; Zhang and Toral, 2019; Graham et al., 2019). More generally, the question of bias to a specific reference has also been raised, in the case of monolingual manual evaluation (Fomicheva and Specia, 2016; Ma et al., 2017). Different from the impact of Translationese on evaluation, the impact of Translationese in the training data has also been studied (Kurokawa et al., 2009; Lembersky et al., 2012a; Bogoychev and Sennrich, 2019; Riley et al., 2020). 1184 Finally, our work is also related to studies measuring the importance of the test data quality, looking specifically at the test set translation direction. For SMT evaluation, Lembersky et al. (2012b) and Stymne (2017) explored how the translation direction affects translation results. Holmqvist et al. (2009) noted that the original language of the test sentences influences the BLEU score of translations. They showed that the BLEU scores for targetoriginal sentences are on average higher than sentences that have their original source i"
2020.wmt-1.140,J12-4004,0,0.0211266,"trics might reward hypotheses with translationese language more than hypotheses using more natural language. The impact of Translationese on human evaluation of MT has recently received attention as well (Toral et al., 2018; Zhang and Toral, 2019; Graham et al., 2019). More generally, the question of bias to a specific reference has also been raised, in the case of monolingual manual evaluation (Fomicheva and Specia, 2016; Ma et al., 2017). Different from the impact of Translationese on evaluation, the impact of Translationese in the training data has also been studied (Kurokawa et al., 2009; Lembersky et al., 2012a; Bogoychev and Sennrich, 2019; Riley et al., 2020). 1184 Finally, our work is also related to studies measuring the importance of the test data quality, looking specifically at the test set translation direction. For SMT evaluation, Lembersky et al. (2012b) and Stymne (2017) explored how the translation direction affects translation results. Holmqvist et al. (2009) noted that the original language of the test sentences influences the BLEU score of translations. They showed that the BLEU scores for targetoriginal sentences are on average higher than sentences that have their original source i"
2020.wmt-1.140,W19-5358,0,0.166306,"for the first time that using these scores for system development can lead to significant improvements. 1 Introduction Machine Translation (MT) has shown impressive progress in recent years. Neural architectures (Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017) have greatly contributed to this improvement, especially for languages with abundant training data (Bojar et al., 2016, 2018; Barrault et al., 2019). This progress creates novel challenges for the evaluation of machine translation, both for human (Toral, 2020; L¨aubli et al., 2020) and automated evaluation protocols (Lo, 2019; Zhang et al., 2019). Both types of evaluation play an important role in machine translation (Koehn, 2010). While human evaluations provide a gold standard evaluation, they involve a fair amount of careful and hence expensive work by human assessors. Cost therefore limits the scale of their application. On the other hand, automated evaluations are much less expensive. They typically only involve human labor when collecting human reference translations and can hence be run at scale to compare a wide range of systems or validate design decisions. The value of automatic evaluations therefore res"
2020.wmt-1.140,D17-1262,0,0.0207085,"e text (Toury, 1995). For MT, Freitag et al. (2019) point at Translationese as a source of mismatch between BLEU and human evaluation, raising concerns that overlap-based metrics might reward hypotheses with translationese language more than hypotheses using more natural language. The impact of Translationese on human evaluation of MT has recently received attention as well (Toral et al., 2018; Zhang and Toral, 2019; Graham et al., 2019). More generally, the question of bias to a specific reference has also been raised, in the case of monolingual manual evaluation (Fomicheva and Specia, 2016; Ma et al., 2017). Different from the impact of Translationese on evaluation, the impact of Translationese in the training data has also been studied (Kurokawa et al., 2009; Lembersky et al., 2012a; Bogoychev and Sennrich, 2019; Riley et al., 2020). 1184 Finally, our work is also related to studies measuring the importance of the test data quality, looking specifically at the test set translation direction. For SMT evaluation, Lembersky et al. (2012b) and Stymne (2017) explored how the translation direction affects translation results. Holmqvist et al. (2009) noted that the original language of the test senten"
2020.wmt-1.140,W07-0716,0,0.0811537,"Missing"
2020.wmt-1.140,E17-1083,0,0.0225836,"uning, especially for Statistical Machine Translation (SMT). Madnani et al. (2007) introduced an automatic paraphrasing technique based on English-to-English translation of full sentences using a statistical MT system, and showed that this permitted reliable system tuning using half as much data. Similar automatic paraphrasing has also been used to augment training data, e.g. (Marton et al., 2009), but relying on standard references for evaluation. In contrast to human paraphrases, the quality of current machine generated paraphrases degrades significantly as overlap with the input decreases (Mallinson et al., 2017; Roy and Grangier, 2019). This makes their use difficult for evaluation since (Freitag et al., 2020) suggests that substantial paraphrasing – ‘paraphrase as much as possible‘ – is necessary for evaluation. Our work can be seen as replacing the regular BLEU metric with a new paraphrase BLEU metric for system tuning. Different alternative automatic evaluation metric have also been considered for system tuning (He and Way, 2010; Servan and Schwenk, 2011) with Minimum Error Rate Training, MERT (Och, 2003). This work showed some specific cases where Translation Error Rate (TER) was superior to B L"
2020.wmt-1.140,D09-1040,0,0.0472392,"recently been shown to be useful for system evaluation (Freitag et al., 2020). Our work considers applying the same methodology for system tuning. There is some earlier work relying on automated paraphrases for system tuning, especially for Statistical Machine Translation (SMT). Madnani et al. (2007) introduced an automatic paraphrasing technique based on English-to-English translation of full sentences using a statistical MT system, and showed that this permitted reliable system tuning using half as much data. Similar automatic paraphrasing has also been used to augment training data, e.g. (Marton et al., 2009), but relying on standard references for evaluation. In contrast to human paraphrases, the quality of current machine generated paraphrases degrades significantly as overlap with the input decreases (Mallinson et al., 2017; Roy and Grangier, 2019). This makes their use difficult for evaluation since (Freitag et al., 2020) suggests that substantial paraphrasing – ‘paraphrase as much as possible‘ – is necessary for evaluation. Our work can be seen as replacing the regular BLEU metric with a new paraphrase BLEU metric for system tuning. Different alternative automatic evaluation metric have also"
2020.wmt-1.140,W19-5333,0,0.176462,"efits: it can help identify choices which improve BLEU on standard references but have limited impact on final human evaluations; or those that result in better translations for the human reader, but worse in terms of standard reference BLEU. Conversely, it might turn out that paraphrased references are not robust enough to support system development due to the presence of ‘metric honeypots’: settings that produce poor translations, but which are nevertheless assigned high BLEU scores. To address these points, we revisit the major design choices of the best English→German system from WMT2019 (Ng et al., 2019) step-by-step, and measure their impact on standard reference BLEU as well as on paraphrased BLEU. This allows us to measure the extent to which steps such as data cleaning, back-translation, fine-tuning, ensemble decoding and reranking benefit standard reference BLEU more than paraphrase BLEU. Revisiting these development choices with the two metrics results in two systems with quite different behaviors. We conduct a human evaluation for adequacy and fluency to assess the overall impact of designing a system using paraphrased BLEU. Our main findings show that optimizing for paraphrased BLEU i"
2020.wmt-1.140,P03-1021,0,0.240098,"enerated paraphrases degrades significantly as overlap with the input decreases (Mallinson et al., 2017; Roy and Grangier, 2019). This makes their use difficult for evaluation since (Freitag et al., 2020) suggests that substantial paraphrasing – ‘paraphrase as much as possible‘ – is necessary for evaluation. Our work can be seen as replacing the regular BLEU metric with a new paraphrase BLEU metric for system tuning. Different alternative automatic evaluation metric have also been considered for system tuning (He and Way, 2010; Servan and Schwenk, 2011) with Minimum Error Rate Training, MERT (Och, 2003). This work showed some specific cases where Translation Error Rate (TER) was superior to B LEU. Our work is also related to the bias that the human translation process introduces in the references, including source language artifacts— Translationese (Koppel and Ordan, 2011)—as well as source-independent artifacts—Translation Universals (Mauranen and Kujam¨aki, 2004). The professional translation community studies both systematic biases inherent to translated texts (Baker, 1993; Selinker, 1972), as well as biases resulting specifically from interference from the source text (Toury, 1995). For"
2020.wmt-1.140,P02-1040,0,0.112525,"ranslations and can hence be run at scale to compare a wide range of systems or validate design decisions. The value of automatic evaluations therefore resides in their capacity to be used as a proxy for human evaluations for large scale comparisons and system development. The recent progress in MT has raised concerns about whether automated evaluation methodologies reliably reflect human ratings in high accuracy ranges. In particular, it has been observed that the best systems according to humans might fare less well with automated metrics (Barrault et al., 2019). Most metrics such as B LEU (Papineni et al., 2002) and TER (Snover et al., 2006) measure overlap between a system output and a human reference translation. More refined ways to compute such overlap have consequently been proposed (Banerjee and Lavie, 2005; Lo, 2019; Zhang et al., 2019). Orthogonal to the work of building improved metrics, Freitag et al. (2020) hypothesized that human references are also an important factor in the reliability of automated evaluations. In particular, they observed that standard references exhibit simple, monotonic language due to human ‘translationese‘ effects. These standard references might favor systems whic"
2020.wmt-1.140,W18-6319,0,0.0224486,"erence, newstest2018.orig-en.p, as part of our work. 3.2 Models For our translation models, we adopt the transformer implementation from Lingvo (Shen et al., 2019), using the transformer-big model size (Vaswani et al., 2017). We use a vocabulary of 32k subword units and exponentially moving averaging of checkpoints (EMA decay) with the weight decrease parameter set to α = 0.999 (Buduma and Locascio, 2017). We used a batch size of around 32k sentences in all our experiments. We report B LEU (Papineni et al., 2002) in addition to human evaluation. All B LEU scores are calculated with sacreBLEU (Post, 2018)1 . 3.3 Human Evaluation To collect human rankings, we ran side-by-side evaluation for overall quality and fluency. We hired 20 linguists and divided them equally between the two evaluations. Each evaluation included 1,000 items with each item being rated exactly once. We acquired only a single rating per sentence from the professional linguists as we found that they were 1 BLEU+case.mixed+lang.ende+numrefs.1+smooth.exp+ SET+tok.13a+version.1.4.12 SET ∈{wmt18, wmt19, wmt19/google/ar, wmt19/google/arp, wmt19/google/wmtp} 1185 more reliable than crowd workers (Toral, 2020). We evaluated the orig"
2020.wmt-1.140,W05-0908,0,0.169955,"Missing"
2020.wmt-1.140,2020.acl-main.691,1,0.731231,"age more than hypotheses using more natural language. The impact of Translationese on human evaluation of MT has recently received attention as well (Toral et al., 2018; Zhang and Toral, 2019; Graham et al., 2019). More generally, the question of bias to a specific reference has also been raised, in the case of monolingual manual evaluation (Fomicheva and Specia, 2016; Ma et al., 2017). Different from the impact of Translationese on evaluation, the impact of Translationese in the training data has also been studied (Kurokawa et al., 2009; Lembersky et al., 2012a; Bogoychev and Sennrich, 2019; Riley et al., 2020). 1184 Finally, our work is also related to studies measuring the importance of the test data quality, looking specifically at the test set translation direction. For SMT evaluation, Lembersky et al. (2012b) and Stymne (2017) explored how the translation direction affects translation results. Holmqvist et al. (2009) noted that the original language of the test sentences influences the BLEU score of translations. They showed that the BLEU scores for targetoriginal sentences are on average higher than sentences that have their original source in a different language. Recently, a similar study wa"
2020.wmt-1.140,P19-1605,1,0.835756,"atistical Machine Translation (SMT). Madnani et al. (2007) introduced an automatic paraphrasing technique based on English-to-English translation of full sentences using a statistical MT system, and showed that this permitted reliable system tuning using half as much data. Similar automatic paraphrasing has also been used to augment training data, e.g. (Marton et al., 2009), but relying on standard references for evaluation. In contrast to human paraphrases, the quality of current machine generated paraphrases degrades significantly as overlap with the input decreases (Mallinson et al., 2017; Roy and Grangier, 2019). This makes their use difficult for evaluation since (Freitag et al., 2020) suggests that substantial paraphrasing – ‘paraphrase as much as possible‘ – is necessary for evaluation. Our work can be seen as replacing the regular BLEU metric with a new paraphrase BLEU metric for system tuning. Different alternative automatic evaluation metric have also been considered for system tuning (He and Way, 2010; Servan and Schwenk, 2011) with Minimum Error Rate Training, MERT (Och, 2003). This work showed some specific cases where Translation Error Rate (TER) was superior to B LEU. Our work is also rela"
2020.wmt-1.140,W17-0230,0,0.0152883,"the question of bias to a specific reference has also been raised, in the case of monolingual manual evaluation (Fomicheva and Specia, 2016; Ma et al., 2017). Different from the impact of Translationese on evaluation, the impact of Translationese in the training data has also been studied (Kurokawa et al., 2009; Lembersky et al., 2012a; Bogoychev and Sennrich, 2019; Riley et al., 2020). 1184 Finally, our work is also related to studies measuring the importance of the test data quality, looking specifically at the test set translation direction. For SMT evaluation, Lembersky et al. (2012b) and Stymne (2017) explored how the translation direction affects translation results. Holmqvist et al. (2009) noted that the original language of the test sentences influences the BLEU score of translations. They showed that the BLEU scores for targetoriginal sentences are on average higher than sentences that have their original source in a different language. Recently, a similar study was conducted for neural MT (Bogoychev and Sennrich, 2019). 3 Experimental Setup We first describe data and models, then present our human evaluation protocol. 3.1 Data We ran all experiments on the WMT 2019 English→German news"
2020.wmt-1.140,2020.eamt-1.20,0,0.31806,"tric scores that correlate better with human judgment, and demonstrates for the first time that using these scores for system development can lead to significant improvements. 1 Introduction Machine Translation (MT) has shown impressive progress in recent years. Neural architectures (Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017) have greatly contributed to this improvement, especially for languages with abundant training data (Bojar et al., 2016, 2018; Barrault et al., 2019). This progress creates novel challenges for the evaluation of machine translation, both for human (Toral, 2020; L¨aubli et al., 2020) and automated evaluation protocols (Lo, 2019; Zhang et al., 2019). Both types of evaluation play an important role in machine translation (Koehn, 2010). While human evaluations provide a gold standard evaluation, they involve a fair amount of careful and hence expensive work by human assessors. Cost therefore limits the scale of their application. On the other hand, automated evaluations are much less expensive. They typically only involve human labor when collecting human reference translations and can hence be run at scale to compare a wide range of systems or validat"
2020.wmt-1.140,W18-6312,0,0.0284513,"Kujam¨aki, 2004). The professional translation community studies both systematic biases inherent to translated texts (Baker, 1993; Selinker, 1972), as well as biases resulting specifically from interference from the source text (Toury, 1995). For MT, Freitag et al. (2019) point at Translationese as a source of mismatch between BLEU and human evaluation, raising concerns that overlap-based metrics might reward hypotheses with translationese language more than hypotheses using more natural language. The impact of Translationese on human evaluation of MT has recently received attention as well (Toral et al., 2018; Zhang and Toral, 2019; Graham et al., 2019). More generally, the question of bias to a specific reference has also been raised, in the case of monolingual manual evaluation (Fomicheva and Specia, 2016; Ma et al., 2017). Different from the impact of Translationese on evaluation, the impact of Translationese in the training data has also been studied (Kurokawa et al., 2009; Lembersky et al., 2012a; Bogoychev and Sennrich, 2019; Riley et al., 2020). 1184 Finally, our work is also related to studies measuring the importance of the test data quality, looking specifically at the test set translati"
2020.wmt-1.140,W18-6314,0,0.156971,"Missing"
2020.wmt-1.140,D19-1571,0,0.0203297,"n and orig-de subsets. The orig-en.p sets use paraphrased references instead of standard references. Our experiments compared newstest2018.joint and newstest2018.orig-en.p for system tuning. The standard newstest2018 and newstest2019 sets are newstest2018.joint and newstest2019.orig-en, respectively. only for standard reference B LEU. Similar to the WMT 2019 winning submission, we include the ensemble approach in our system that is optimized on the joint B LEU scores. However, we do not include it in our system optimized on B LEU P. 4.3 Reranking Finally, we extend the noisy-channel approach (Yee et al., 2019) which consists of re-ranking the top-50 beam search output of either the ensemble model (when tuned for B LEU) or the fine-tuned model (when tuned for B LEU P). Instead of using 4 features—forward probability, backward probability, language model and word penalty—we use 11 forward probabilities, 10 backward probabilities and 2 language model scores. Different to (Ng et al., 2019), we did not pick the re-ranking weights through random search, but used MERT (Och, 2003) for efficient tuning. The 11 different forward translation scores come from different English→German NMT models that are replic"
2020.wmt-1.140,W19-5208,0,0.0744881,"he professional translation community studies both systematic biases inherent to translated texts (Baker, 1993; Selinker, 1972), as well as biases resulting specifically from interference from the source text (Toury, 1995). For MT, Freitag et al. (2019) point at Translationese as a source of mismatch between BLEU and human evaluation, raising concerns that overlap-based metrics might reward hypotheses with translationese language more than hypotheses using more natural language. The impact of Translationese on human evaluation of MT has recently received attention as well (Toral et al., 2018; Zhang and Toral, 2019; Graham et al., 2019). More generally, the question of bias to a specific reference has also been raised, in the case of monolingual manual evaluation (Fomicheva and Specia, 2016; Ma et al., 2017). Different from the impact of Translationese on evaluation, the impact of Translationese in the training data has also been studied (Kurokawa et al., 2009; Lembersky et al., 2012a; Bogoychev and Sennrich, 2019; Riley et al., 2020). 1184 Finally, our work is also related to studies measuring the importance of the test data quality, looking specifically at the test set translation direction. For SMT e"
2020.wmt-1.140,2006.amta-papers.25,0,0.212798,"at scale to compare a wide range of systems or validate design decisions. The value of automatic evaluations therefore resides in their capacity to be used as a proxy for human evaluations for large scale comparisons and system development. The recent progress in MT has raised concerns about whether automated evaluation methodologies reliably reflect human ratings in high accuracy ranges. In particular, it has been observed that the best systems according to humans might fare less well with automated metrics (Barrault et al., 2019). Most metrics such as B LEU (Papineni et al., 2002) and TER (Snover et al., 2006) measure overlap between a system output and a human reference translation. More refined ways to compute such overlap have consequently been proposed (Banerjee and Lavie, 2005; Lo, 2019; Zhang et al., 2019). Orthogonal to the work of building improved metrics, Freitag et al. (2020) hypothesized that human references are also an important factor in the reliability of automated evaluations. In particular, they observed that standard references exhibit simple, monotonic language due to human ‘translationese‘ effects. These standard references might favor systems which excel at reproducing these e"
2021.iwslt-1.28,2020.iwslt-1.27,1,0.827508,"s human tokens onto system segments, and then human-transcript-to-translation length ratios are used to align the German tokens to both. We have greyed out punctuation and lowercased to show the actual English text used in training. corresponding segment boundaries for the goldstandard translations when training for Segment Robustness. We could perform a statistical word alignment between the human transcription and its translation to determine word-level interlingual semantic correspondence, but in similar situations such as prefix training for simultaneous translation (Niehues et al., 2018; Arivazhagan et al., 2020), this has not resulted in improvements over a simple proportional length-based heuristic. Therefore, we use human-transcript-to-translation length ratios (in tokens) to segment the gold translations so that their new segment lengths match the projected human source segment lengths. Finally, we train on (projected-human-source, projected-goldtranslation) pairs. This is similar to how artificial target sentences were constructed by Li et al. (2021), but in our case, the boundaries are determined by automatic punctuation on ASR output, rather than from introducing boundary errors at random. Tabl"
2021.iwslt-1.28,P18-1163,0,0.0666711,"Missing"
2021.iwslt-1.28,2020.iwslt-1.26,0,0.0398622,"n other training approaches. The new projection approach achieves gains of over 1 BLEU point over a baseline that is exposed to the human transcripts and segmentations, and these gains hold for both IWSLT data and YouTube data. 1 Introduction Speech translation is an important field that becomes more relevant with every improvement to its component technologies of automatic speech recognition (ASR) and machine translation (MT). It enables exciting applications like live machine interpretation (Cho and Esipova, 2016; Ma et al., 2019) and automatic foreign-language subtitling for video content (Karakanta et al., 2020). However, translation of speech presents unique challenges compared to text translation. Traditional text translation systems are often trained with clean, well-structured text consisting of (source language, target language) sentence pairs gathered from text documents. This works well for translating written text, but for cascaded systems composed of speech → automatic transcription → automatic translation, errors from ASR and automatic punctuation are amplified as they pass through the translation ∗ equal contribution system. Such systems suffer from three issues: 1) spoken language structu"
2021.iwslt-1.28,D18-2012,0,0.067765,"Missing"
2021.iwslt-1.28,P19-1291,0,0.0406992,"Missing"
2021.iwslt-1.28,2005.iwslt-1.19,0,0.127552,"ment Robustness (Section 3.3), while segment and token errors together result in System Robustness (Section 3.4); that is, MT that is robust to the complete long-form transcription pipeline. We will show in the following sections how we can project system segments onto the source and target text; we call this an inverted projection. 3.1 Levenshtein Projection A key component to all of the approaches in Table 1 is an alignment between the system (ASR) transcription and a human transcription of the same long-form audio. Inspired by common practice in evaluation for long-form speech translation (Matusov et al., 2005), we employ a token-level, caseinsensitive Levenshtein alignment of the two transcripts. The Levenshtein alignment is monotonic, parameter-free, and its dynamic programming algorithm is fast enough to be easily applied to very long sequences. We show an example alignment in Table 2. By tracking the alignment of tokens immediately before segment boundaries (always end-of-sentence periods in our example), we can project segment boundaries from one transcription to another, which allows us to produce the various entries in Table 1, as we describe in more detail in the following subsections. 3.2 T"
2021.iwslt-1.28,2013.iwslt-papers.14,0,0.0277626,"eir approach when building the target sides of our inverted projections. 3 Methods Our approach to producing MT systems that are robust to automatic transcription errors is to introduce errors from our ASR system into our MT training data. Throughout the discussion of our methods, we make use of both human (manual) and system (automated) transcriptions of the source audio. When discussing the target-side of our training data, we use instead the term “gold” to indicate a trusted reference translation. Throughout our experiments, the gold standard is a human translation of the human transcript (Post et al., 2013; Sperber et al., 2017), though it could just as easily, and much less expensively, be a machine translation of the human transcript (Cheng et al., 2019). We divide transcription errors into two categories: token and segment errors. A token error is any word that is transcribed incorrectly by ASR, such as a homophone substitution or the omission of a mumbled word. Meanwhile, segment errors are introduced by failing to correctly break the recognized text into sentence-like segments. A human transcription is expected to have error-free tokens and segments. Table 1 presents a baseline and three w"
2021.iwslt-1.28,Q19-1020,0,0.0463403,"Missing"
2021.iwslt-1.28,W19-6601,0,0.0177209,"→ automatic transcription → automatic translation, errors from ASR and automatic punctuation are amplified as they pass through the translation ∗ equal contribution system. Such systems suffer from three issues: 1) spoken language structure is different from written language structure and can include aspects like disfluencies and partial sentences, 2) ASR systems are not perfect and introduce errors in the stage from speech to source transcript, and 3) mistakes from automatic punctuation systems can lead to unnatural sentence segments and boundaries (Makhija et al., 2019; Nguyen et al., 2019; Wang et al., 2019). These problems can lead to poor translations and pose unique challenges for MT that are not readily addressed by current methods. In this work, we set out to make MT robust to the second and third issues in particular. We have developed an approach to train translation models that are robust to transcription errors and punctuation errors, by introducing errors from actual ASR and automatic punctuation systems into the source side of our MT training data. This is similar in spirit to the method of Li et al. (2021), which introduces artificial sentence boundary errors into the training bitext."
2021.naacl-main.91,2020.wmt-1.140,1,0.761763,"ons. This has several disadvantages. First, high-quality reference translations are expensive to create. This means that in practice, evaluation is usually carried out with relatively small, carefully curated test corpora. The need for careful preparation limits the number of domains for which an MT system can be conveniently assessed, and small test-set sizes can make it difficult to draw robust conclusions (Card et al., 2020). Second, enshrining ground truth in a small number of references (usually just one) is inherently problematic, since valid translations can vary along many dimensions; Freitag et al. (2020b) demonstrate that different (correct) references for the same test set can result in different system rankings according to the same reference-based metric. Finally, scoring the similarity between an MT hypothesis and a reference translation involves recognizing the extent to which they are mutual paraphrases. When gross discrepancies exist, this is a relatively easy problem 1 for which surface-level metrics can provide a reliExcept Gujarati, which was absent from their training able signal, but capturing the subtle errors typical corpus. 1158 Proceedings of the 2021 Conference of the North"
2021.naacl-main.91,2020.emnlp-main.5,1,0.816965,"ons. This has several disadvantages. First, high-quality reference translations are expensive to create. This means that in practice, evaluation is usually carried out with relatively small, carefully curated test corpora. The need for careful preparation limits the number of domains for which an MT system can be conveniently assessed, and small test-set sizes can make it difficult to draw robust conclusions (Card et al., 2020). Second, enshrining ground truth in a small number of references (usually just one) is inherently problematic, since valid translations can vary along many dimensions; Freitag et al. (2020b) demonstrate that different (correct) references for the same test set can result in different system rankings according to the same reference-based metric. Finally, scoring the similarity between an MT hypothesis and a reference translation involves recognizing the extent to which they are mutual paraphrases. When gross discrepancies exist, this is a relatively easy problem 1 for which surface-level metrics can provide a reliExcept Gujarati, which was absent from their training able signal, but capturing the subtle errors typical corpus. 1158 Proceedings of the 2021 Conference of the North"
2021.naacl-main.91,W19-5302,0,0.158498,"ly robust and likely to offer reasonable performance across a broad spectrum of domains and different system qualities. 1 Introduction of high-quality MT is more difficult, and it is not clear whether it is substantially easier than scoring the similarity between texts in different languages. These problems can be avoided by looking only at the source text when assessing MT output. There is evidence that this is the best practice for human evaluation (Toral, 2020). Moreover, it has recently been investigated for automatic metrics as well (Yankovskaya et al., 2019; Lo, 2019; Zhao et al., 2020; Ma et al., 2019). Such reference-free metrics are flexible and scalable, but since they are essentially performing the same task as an MT model, they raise a circularity concern: if we can reliably score MT output, why wouldn’t we use the scoring model to produce better output? One answer to this is practical: the scoring model might be too large to deploy, or it might not easily support efficient inference (Yu et al., 2016). A more interesting answer is that a scoring model could be set up to provide a signal that is complementary to the systems under evaluation. That is, it might be capable of correctly ran"
2021.naacl-main.91,2020.acl-main.448,0,0.0431198,"roportional to a unigram estimate q α (˜ x|x). 1160 for 18 language pairs. For each language pair, we compute a metric score for each system, then use correlation with the provided human scores to assess the quality of our metric.3 Following Ma et al. (2019) we measure correlation using Pearson’s coefficient, and use Williams’ test (Williams, 1959) to compute the significance of correlation differences, with a p-value &lt; 0.05. Ma et al. (2019) note that correlation scores are unrealistically high for many language pairs, and suggest using only the best k systems for small values of k. However, Mathur et al. (2020) show that this results in noisy and unreliable estimates. We adopt their suggestion to instead remove outlier systems whose scores have large deviations from the median according to the formula: ˜ |h − h| &gt; 2.5, ˜ 1.483 × medianh (|h − h|) ˜ is where h is a system-level human score, and h the median score across all systems for a given language pair. To summarize a metric’s performance across a set of language pairs, we report the weighted average of its Pearson correlations across languages. We first apply the Fisher Z-transformation to normalize raw language-specific correlations, then weig"
2021.naacl-main.91,2020.acl-main.64,0,0.021386,"ularizing token-level probabilities (Monte-Carlo dropout, subword sampling) and for combining them into system-level scores (summary statistics over tokens, confidence thresholds over sentences). Finally, we analyze the results of our best model, measuring how its performance depends on various factors: language pair and human-judgment methodology, output quality, proximity to the systems under evaluation, and size of the test set. 2 Related Work Reference-free evaluation is widely used for many NLP tasks such as grammatical error correction (Napoles et al., 2016), dialog (Sinha et al., 2020; Mehri and Eskenazi, 2020) and text generation (Ethayarajh and Sadigh, 2020). There has been recent interest in reference-free evaluation for MT, which was a joint track between the WMT 2019 metrics task (Ma et al., 2019) and quality estimation task (Fonseca et al., 2019). Reference-free metrics competed head-to-head with standard metrics, and generally did worse. However, the results from the best reference-free systems, UNI+ (Yankovskaya et al., 2019) and YiSi-2 (Lo, 2019) were surprisingly close to the standard metric scores on the language pairs for which they were evaluated. UNI+ computes word-level embeddings for"
2021.naacl-main.91,D16-1228,0,0.0239105,"iginal architecture); and different methods for regularizing token-level probabilities (Monte-Carlo dropout, subword sampling) and for combining them into system-level scores (summary statistics over tokens, confidence thresholds over sentences). Finally, we analyze the results of our best model, measuring how its performance depends on various factors: language pair and human-judgment methodology, output quality, proximity to the systems under evaluation, and size of the test set. 2 Related Work Reference-free evaluation is widely used for many NLP tasks such as grammatical error correction (Napoles et al., 2016), dialog (Sinha et al., 2020; Mehri and Eskenazi, 2020) and text generation (Ethayarajh and Sadigh, 2020). There has been recent interest in reference-free evaluation for MT, which was a joint track between the WMT 2019 metrics task (Ma et al., 2019) and quality estimation task (Fonseca et al., 2019). Reference-free metrics competed head-to-head with standard metrics, and generally did worse. However, the results from the best reference-free systems, UNI+ (Yankovskaya et al., 2019) and YiSi-2 (Lo, 2019) were surprisingly close to the standard metric scores on the language pairs for which they"
2021.naacl-main.91,P02-1040,0,0.113179,"ent paraphrase recognition when used in zero-shot mode to compare MT output with reference sentences in the same language. On the WMT 2019 metrics task, their method (Prism) beat or tied all previous reference-based metrics on all languages.1 Although it was not the main focus of their work, Prism achieved a new state-of-the-art as a referencefree metric, simply scoring target given source text using an MT model, in a post-competition comparison to the 2019 “Quality Estimation as a metric” shared task (Ma et al., 2019). Traditional automatic metrics for machine translation (MT), such as BLEU (Papineni et al., 2002), score MT output by comparing it to one or more reference translations. This has several disadvantages. First, high-quality reference translations are expensive to create. This means that in practice, evaluation is usually carried out with relatively small, carefully curated test corpora. The need for careful preparation limits the number of domains for which an MT system can be conveniently assessed, and small test-set sizes can make it difficult to draw robust conclusions (Card et al., 2020). Second, enshrining ground truth in a small number of references (usually just one) is inherently pr"
2021.naacl-main.91,2020.acl-main.252,0,0.0422351,"Missing"
2021.naacl-main.91,2020.acl-main.220,0,0.0201059,"rent methods for regularizing token-level probabilities (Monte-Carlo dropout, subword sampling) and for combining them into system-level scores (summary statistics over tokens, confidence thresholds over sentences). Finally, we analyze the results of our best model, measuring how its performance depends on various factors: language pair and human-judgment methodology, output quality, proximity to the systems under evaluation, and size of the test set. 2 Related Work Reference-free evaluation is widely used for many NLP tasks such as grammatical error correction (Napoles et al., 2016), dialog (Sinha et al., 2020; Mehri and Eskenazi, 2020) and text generation (Ethayarajh and Sadigh, 2020). There has been recent interest in reference-free evaluation for MT, which was a joint track between the WMT 2019 metrics task (Ma et al., 2019) and quality estimation task (Fonseca et al., 2019). Reference-free metrics competed head-to-head with standard metrics, and generally did worse. However, the results from the best reference-free systems, UNI+ (Yankovskaya et al., 2019) and YiSi-2 (Lo, 2019) were surprisingly close to the standard metric scores on the language pairs for which they were evaluated. UNI+ compute"
2021.naacl-main.91,2020.emnlp-main.8,0,0.10668,"’t we use the scoring model to produce better output? One answer to this is practical: the scoring model might be too large to deploy, or it might not easily support efficient inference (Yu et al., 2016). A more interesting answer is that a scoring model could be set up to provide a signal that is complementary to the systems under evaluation. That is, it might be capable of correctly ranking competing MT hypotheses even when its own preferred hypothesis is worse on average than those of the systems it is evaluating. In our experiments we find that this can indeed be the case. In recent work, Thompson and Post (2020) showed that a single multilingual MT model trained on 39 languages can achieve excellent paraphrase recognition when used in zero-shot mode to compare MT output with reference sentences in the same language. On the WMT 2019 metrics task, their method (Prism) beat or tied all previous reference-based metrics on all languages.1 Although it was not the main focus of their work, Prism achieved a new state-of-the-art as a referencefree metric, simply scoring target given source text using an MT model, in a post-competition comparison to the 2019 “Quality Estimation as a metric” shared task (Ma et"
2021.naacl-main.91,W19-5410,0,0.0480859,"Missing"
2021.naacl-main.91,2020.acl-main.756,0,0.0282354,"y applied at the sentence systems under evaluation and find no evidence that level, and it can make use of powerful “glass-box” this is a source of bias. Despite using no references, features which capture the internals of an MT sysour model achieves approximate parity with BLEU tem. In contrast, reference-free evaluation is most both in system-level correlation with human judg- naturally applied at the system (test-set) level, and ment, and when used for pairwise comparisons. ideally should make no assumptions about the sys1159 tems under evaluation. The second task is parallelcorpus mining (Zhang et al., 2020; Yang et al., 2019), which aims to identify valid translations at various levels of granularity. Its scoring aspect is similar to reference-free evaluation, but it is applied to a different input distribution, attempting to identify human-generated translation pairs rather than scoring MT outputs for a given human-generated source text. 3 Methods We P aim to generate a quality score s(X, Y ) = x,y s(x, y) for source and target texts X, Y which consist of segment (nominally, sentence) pairs x, y. We assume no document or ordering information among segments, and do not directly evaluate scores"
2021.naacl-main.91,2020.acl-main.151,0,0.0381675,"Missing"
2021.naacl-main.91,D19-1053,0,0.0224356,"ce and MT output sentences using pre-trained multilingual BERT and LASER (Artetxe and Schwenk, 2019) models, then feeds averaged vectors to a neural classifier trained to predict human scores from previous MT metrics tasks. YiSi-2 is similar, except that it works in an unsupervised fashion, computing similarities between mBERT embeddings for aligned source and target words, and returning an F-measure statistic. In more recent work, Zhao et al. (2020) adopt a similar approach based on mBERT, aligning representations from multilingual embedding spaces before computing distances with MoverScore (Zhao et al., 2019), and adding a GPT-based target-side language model. We demonstrate improvements over the original The current state-of-the-art in reference-free evaluPrism metric due to model capacity and different methods for combining probabilities; surprisingly, ation for MT is represented by the Prism approach (Thompson and Post, 2020) which we extend here. we find little gain from adjusting the domain or languages in the original multilingual corpus (alIt is worth distinguishing reference-free evaluathough we show that a competition-grade English- tion from two related tasks that share formal simiGerman"
2021.naacl-main.91,2020.eamt-1.20,0,0.0185314,"t by scaling it up we can match the performance of BLEU. We analyze various potential weaknesses of the approach, and find that it is surprisingly robust and likely to offer reasonable performance across a broad spectrum of domains and different system qualities. 1 Introduction of high-quality MT is more difficult, and it is not clear whether it is substantially easier than scoring the similarity between texts in different languages. These problems can be avoided by looking only at the source text when assessing MT output. There is evidence that this is the best practice for human evaluation (Toral, 2020). Moreover, it has recently been investigated for automatic metrics as well (Yankovskaya et al., 2019; Lo, 2019; Zhao et al., 2020; Ma et al., 2019). Such reference-free metrics are flexible and scalable, but since they are essentially performing the same task as an MT model, they raise a circularity concern: if we can reliably score MT output, why wouldn’t we use the scoring model to produce better output? One answer to this is practical: the scoring model might be too large to deploy, or it might not easily support efficient inference (Yu et al., 2016). A more interesting answer is that a sc"
C10-1007,W08-2102,0,0.0261055,"oarse model prunes the search space for later, more expensive models (Charniak et al., 2006; Petrov and Klein, 2007). This approach assumes a common forest or chart representation, shared by all granularities, where one can efficiently track the pruning decisions of the coarse models. One could imagine applying such a solution to dependency parsing, but the exact implementation of the coarse pass would vary according to the choice in search algorithm. Our filters are much more modular: they apply to both 1st -order spanning tree parsing and 2nd -order projective parsing, with no modification. Carreras et al. (2008) use coarse-to-fine pruning with dependency parsing, but in that case, a graphbased dependency parser provides the coarse pass, with the fine pass being a far-more-expensive treeadjoining grammar. Our filters could become a 0th pass, further increasing the efficiency of their approach. X 1 l(w, ¯ yi , x ¯i ) ¯ 2 + C1 min ||w|| w ¯ 2 i:yi =1 X l(w, ¯ yi , x ¯i ) +C2 (1) i:yi =−1 where l() is the learning method’s loss function, x ¯i and yi are the features and label for the ith 4 Arc Filters We propose arc filtering as a preprocessing step for dependency parsing. An arc filter removes im3 Learn"
C10-1007,D07-1101,0,0.137063,"ngth for each triple starts at the highest value seen in the training data. Thresholds are then decreased in a greedy fashion, with each step producing the smallest possible reduction in reachable training arcs. We employ this algorithm as a baseline in our experiments. To our knowledge, vine parsing [h,m]∈t The weights w ¯ are typically learned using an online method, such as an averaged perceptron (Collins, 2002) or MIRA (Crammer and Singer, 2003). 2nd -order searches, which consider two siblings at a time, are available with no increase in asymptotic complexity (McDonald and Pereira, 2006; Carreras, 2007). The complexity of graph-based parsing is bounded by two processes: parsing (carrying out the argmax) and arc scoring (calculating w ¯ · f¯(h, m, s)). For a sentence with n words, projective parsing takes O(n3 ) time, while the spanning tree algorithm is O(n2 ). Both parsers require scores for arcs connecting each possible [h, m] 1 To calibrate this speed, consider that the publicly available 1st -order MST parser processes 16 sentences per second on modern hardware. This includes I/O costs in addition to the costs of arc scoring and parsing. 54 has not previously been tested with a state-of-"
C10-1007,N06-1040,0,0.049413,"Missing"
C10-1007,N06-1022,0,0.10005,"+1. A similar process generates training examples for the other filters. Since our goal is to only filter very implausible arcs, we bias the classifier to high precision, increasing the cost for misclassifying a true arc during learning.3 Class-specific costs are command-line parameters for many learning packages. One can interpret the learning objective as minimizing regularized, weighted loss: Coarse-to-fine Parsing Another common method employed to speed up exhaustive parsers is a coarse-to-fine approach, where a cheap, coarse model prunes the search space for later, more expensive models (Charniak et al., 2006; Petrov and Klein, 2007). This approach assumes a common forest or chart representation, shared by all granularities, where one can efficiently track the pruning decisions of the coarse models. One could imagine applying such a solution to dependency parsing, but the exact implementation of the coarse pass would vary according to the choice in search algorithm. Our filters are much more modular: they apply to both 1st -order spanning tree parsing and 2nd -order projective parsing, with no modification. Carreras et al. (2008) use coarse-to-fine pruning with dependency parsing, but in that case"
C10-1007,N07-1051,0,0.0266617,"enerates training examples for the other filters. Since our goal is to only filter very implausible arcs, we bias the classifier to high precision, increasing the cost for misclassifying a true arc during learning.3 Class-specific costs are command-line parameters for many learning packages. One can interpret the learning objective as minimizing regularized, weighted loss: Coarse-to-fine Parsing Another common method employed to speed up exhaustive parsers is a coarse-to-fine approach, where a cheap, coarse model prunes the search space for later, more expensive models (Charniak et al., 2006; Petrov and Klein, 2007). This approach assumes a common forest or chart representation, shared by all granularities, where one can efficiently track the pruning decisions of the coarse models. One could imagine applying such a solution to dependency parsing, but the exact implementation of the coarse pass would vary according to the choice in search algorithm. Our filters are much more modular: they apply to both 1st -order spanning tree parsing and 2nd -order projective parsing, with no modification. Carreras et al. (2008) use coarse-to-fine pruning with dependency parsing, but in that case, a graphbased dependency"
C10-1007,W02-1001,0,0.0557475,"art-of-speech (PoS) tags being linked and the direction of the arc, resulting in a separate threshold for each [tag(h), tag(m), dir(h, m)] triple. They sketch an algorithm where the thresholded length for each triple starts at the highest value seen in the training data. Thresholds are then decreased in a greedy fashion, with each step producing the smallest possible reduction in reachable training arcs. We employ this algorithm as a baseline in our experiments. To our knowledge, vine parsing [h,m]∈t The weights w ¯ are typically learned using an online method, such as an averaged perceptron (Collins, 2002) or MIRA (Crammer and Singer, 2003). 2nd -order searches, which consider two siblings at a time, are available with no increase in asymptotic complexity (McDonald and Pereira, 2006; Carreras, 2007). The complexity of graph-based parsing is bounded by two processes: parsing (carrying out the argmax) and arc scoring (calculating w ¯ · f¯(h, m, s)). For a sentence with n words, projective parsing takes O(n3 ) time, while the spanning tree algorithm is O(n2 ). Both parsers require scores for arcs connecting each possible [h, m] 1 To calibrate this speed, consider that the publicly available 1st -o"
C10-1007,D09-1001,0,0.0795809,"Missing"
C10-1007,C08-1094,0,0.350206,".2 plausible head-modifier arcs from the complete dependency graph (which initially includes all head-modifier arcs). We use three stages of filters that operate in sequence on progressively sparser graphs: 1) rule-based, 2) linear: a single pass through the n nodes in a sentence (O(n) complexity), and 3) quadratic: a scoring of all remaining arcs (O(n2 )). The less intensive filters are used first, saving time by leaving fewer arcs to be processed by the more intensive systems. Implementations of our rule-based, linear, and quadratic filters are publicly available at: CFG Cell Classification Roark and Hollingshead (2008) speed up another exhaustive parsing algorithm, the CKY parser for CFGs, by classifying each word in the sentence according to whether it can open (or close) a multi-word constituent. With a high-precision tagger that errs on the side of permitting constituents, they show a significant improvement in speed with no reduction in accuracy. It is difficult to port their idea directly to dependency parsing without committing to a particular search algorithm,2 and thereby sacrificing some of the graph-based formalism’s modularity. However, some of our linear filters (see Section 4.3) were inspired b"
C10-1007,W09-3831,0,0.310481,"Missing"
C10-1007,W06-2929,0,0.14634,"t decomposes under an exhaustive search (McDonald et al., 2005). The most natural decomposition scores individual arcs, represented as head-modifier pairs [h, m]. This enables search by either minimum spanning tree (West, 2001) or by Eisner’s (1996) projective parser. This paper focuses on the projective case, though our techniques transfer to spanning tree parsing. With a linear scoring function, the parser solves: X w ¯ · f¯(h, m, s) parse(s) = argmaxt∈s 3 3.1 Related Work Vine Parsing Filtering dependency arcs has been explored primarily in the form of vine parsing (Eisner and Smith, 2005; Dreyer et al., 2006). Vine parsing establishes that, since most dependencies are short, one can parse quickly by placing a hard constraint on arc length. As this coarse filter quickly degrades the best achievable performance, Eisner and Smith (2005) also consider conditioning the constraint on the part-of-speech (PoS) tags being linked and the direction of the arc, resulting in a separate threshold for each [tag(h), tag(m), dir(h, m)] triple. They sketch an algorithm where the thresholded length for each triple starts at the highest value seen in the training data. Thresholds are then decreased in a greedy fashio"
C10-1007,N03-1033,0,0.128234,"Missing"
C10-1007,W05-1504,0,0.283883,"o a scoring function that decomposes under an exhaustive search (McDonald et al., 2005). The most natural decomposition scores individual arcs, represented as head-modifier pairs [h, m]. This enables search by either minimum spanning tree (West, 2001) or by Eisner’s (1996) projective parser. This paper focuses on the projective case, though our techniques transfer to spanning tree parsing. With a linear scoring function, the parser solves: X w ¯ · f¯(h, m, s) parse(s) = argmaxt∈s 3 3.1 Related Work Vine Parsing Filtering dependency arcs has been explored primarily in the form of vine parsing (Eisner and Smith, 2005; Dreyer et al., 2006). Vine parsing establishes that, since most dependencies are short, one can parse quickly by placing a hard constraint on arc length. As this coarse filter quickly degrades the best achievable performance, Eisner and Smith (2005) also consider conditioning the constraint on the part-of-speech (PoS) tags being linked and the direction of the arc, resulting in a separate threshold for each [tag(h), tag(m), dir(h, m)] triple. They sketch an algorithm where the thresholded length for each triple starts at the highest value seen in the training data. Thresholds are then decrea"
C10-1007,C96-1058,0,0.356992,"Missing"
C10-1007,W06-2904,1,0.897232,"Missing"
C10-1007,D07-1003,0,0.0600961,"Missing"
C10-1007,P09-1087,0,0.0404163,"g as a light preprocessing step, using only a portion of the resources that might be used in the final scoring function. Features Quadratic filtering uses both binary and realvalued features (Table 3). Real-valued features promote a smaller feature space. For example, one value can encode distance rather than separate features for different distances. We also generalize the “between-tag features” used in McDonald et al. (2005) to be the count of each tag between the head and modifier. The count may be more informative than tag presence alone, particularly for high-precision filters. We follow Galley and Manning (2009) in using only between-tags within a fixed range of the head or modifier, so that the extraction for each pair is O(1) and the overall feature extraction is O(n2 ). Using only a subset of the between-tags as features has been shown to improve speed but impair parser performance (Galley and Manning, 2009). By filtering quickly first, then scoring all remaining arcs with a cubic scoring function in the parser, we hope to get the best of both worlds. Combining linear decisions We originally optimized the C1 and C2 parameter separately for each linear decision function. However, we found we could"
C10-1007,P08-1061,0,0.0281066,"Missing"
C10-1007,P07-1022,0,0.288832,"n3 ) if the features include words in s between h and m (Galley and Manning, 2009). Arc scoring also has a significant constant term: the number of features extracted for an [h, m] pair. Our in-house graphbased parser collects on average 62 features for each potential arc, a number larger than the length of most sentences. With the cluster-based features suggested by Koo et al. (2008), this could easily grow by a factor of 3 or 4. The high cost of arc scoring, coupled with the parsing stage’s low grammar constant, means that graph-based parsers spend much of their time scoring potential arcs. Johnson (2007) reports that when arc scores have been precomputed, the dynamic programming component of his 1st -order parser can process an amazing 3,580 sentences per second.1 Beyond reducing the number of features, the easiest way to reduce the computational burden of arc scoring is to score only plausible arcs. money funds Figure 1: An example dependency parse. 2 Dependency Parsing A dependency tree represents the syntactic structure of a sentence as a directed graph (Figure 1), with a node for each word, and arcs indicating head-modifier pairs (Me´lˇcuk, 1987). Though dependencies can be extracted from"
C10-1007,W03-3023,0,0.354985,"Missing"
C10-1007,P08-1068,0,0.115209,"f the 23rd International Conference on Computational Linguistics (Coling 2010), pages 53–61, Beijing, August 2010 Investors continue to pour cash into pair in s; therefore, the cost of arc scoring is also O(n2 ), and may become O(n3 ) if the features include words in s between h and m (Galley and Manning, 2009). Arc scoring also has a significant constant term: the number of features extracted for an [h, m] pair. Our in-house graphbased parser collects on average 62 features for each potential arc, a number larger than the length of most sentences. With the cluster-based features suggested by Koo et al. (2008), this could easily grow by a factor of 3 or 4. The high cost of arc scoring, coupled with the parsing stage’s low grammar constant, means that graph-based parsers spend much of their time scoring potential arcs. Johnson (2007) reports that when arc scores have been precomputed, the dynamic programming component of his 1st -order parser can process an amazing 3,580 sentences per second.1 Beyond reducing the number of features, the easiest way to reduce the computational burden of arc scoring is to score only plausible arcs. money funds Figure 1: An example dependency parse. 2 Dependency Parsin"
C10-1007,D07-1013,0,0.0526525,"cs. money funds Figure 1: An example dependency parse. 2 Dependency Parsing A dependency tree represents the syntactic structure of a sentence as a directed graph (Figure 1), with a node for each word, and arcs indicating head-modifier pairs (Me´lˇcuk, 1987). Though dependencies can be extracted from many formalisms, there is a growing interest in predicting dependency trees directly. To that end, there are two dominant approaches: graph-based methods, characterized by arc features in an exhaustive search, and transition-based methods, characterized by operational features in a greedy search (McDonald and Nivre, 2007). We focus on graph-based parsing, as its exhaustive search has the most to gain from our filters. Graph-based dependency parsing finds the highest-scoring tree according to a scoring function that decomposes under an exhaustive search (McDonald et al., 2005). The most natural decomposition scores individual arcs, represented as head-modifier pairs [h, m]. This enables search by either minimum spanning tree (West, 2001) or by Eisner’s (1996) projective parser. This paper focuses on the projective case, though our techniques transfer to spanning tree parsing. With a linear scoring function, the"
C10-1007,E06-1011,0,0.0947049,"thm where the thresholded length for each triple starts at the highest value seen in the training data. Thresholds are then decreased in a greedy fashion, with each step producing the smallest possible reduction in reachable training arcs. We employ this algorithm as a baseline in our experiments. To our knowledge, vine parsing [h,m]∈t The weights w ¯ are typically learned using an online method, such as an averaged perceptron (Collins, 2002) or MIRA (Crammer and Singer, 2003). 2nd -order searches, which consider two siblings at a time, are available with no increase in asymptotic complexity (McDonald and Pereira, 2006; Carreras, 2007). The complexity of graph-based parsing is bounded by two processes: parsing (carrying out the argmax) and arc scoring (calculating w ¯ · f¯(h, m, s)). For a sentence with n words, projective parsing takes O(n3 ) time, while the spanning tree algorithm is O(n2 ). Both parsers require scores for arcs connecting each possible [h, m] 1 To calibrate this speed, consider that the publicly available 1st -order MST parser processes 16 sentences per second on modern hardware. This includes I/O costs in addition to the costs of arc scoring and parsing. 54 has not previously been tested"
C10-1007,P05-1012,0,0.792021,"ough dependencies can be extracted from many formalisms, there is a growing interest in predicting dependency trees directly. To that end, there are two dominant approaches: graph-based methods, characterized by arc features in an exhaustive search, and transition-based methods, characterized by operational features in a greedy search (McDonald and Nivre, 2007). We focus on graph-based parsing, as its exhaustive search has the most to gain from our filters. Graph-based dependency parsing finds the highest-scoring tree according to a scoring function that decomposes under an exhaustive search (McDonald et al., 2005). The most natural decomposition scores individual arcs, represented as head-modifier pairs [h, m]. This enables search by either minimum spanning tree (West, 2001) or by Eisner’s (1996) projective parser. This paper focuses on the projective case, though our techniques transfer to spanning tree parsing. With a linear scoring function, the parser solves: X w ¯ · f¯(h, m, s) parse(s) = argmaxt∈s 3 3.1 Related Work Vine Parsing Filtering dependency arcs has been explored primarily in the form of vine parsing (Eisner and Smith, 2005; Dreyer et al., 2006). Vine parsing establishes that, since most"
C10-2177,J97-1003,0,0.389377,"2 0.3 0.4 Word error rate WindowDiff under different WERs B−ALN HG−ALN AUDIO 0.4 WindowDiff utilizes the hierarchical structures of slides and global distribution of words, i.e., the HG-ALN model, reduces both Pk and WindowDiff scores over the baseline model, B-ALN. As discussed earlier, the baseline is a re-implementation of standard dynamic time warping based only on a pre-order walk of the slides, while the HG-ALN model incorporates also hierarchical bullet constraints and global word distribution. Table 1 also presents the performance of a typical topic segmentation algorithm, TextTiling (Hearst, 1997). Note that similar to (Malioutov et al., 2007), we force the number of predicted topic segments to be the target number, i.e., in our task, the number of bullets. The results show that both the Pk and WindowDiff scores of TextTiling are significantly higher than those of the alignment algorithms. Our manual analysis suggests that many segments are as short as several utterances and the difference between two consecutive segments is too subtle to be captured by a lexical cohesionbased method such as TextTiling. For comparison, We also present the results of uniform segmentation (UNI), which si"
C10-2177,W06-1644,0,0.33355,"Missing"
C10-2177,J02-4006,0,0.125056,"w the tree structure in advance and therefore we know that the starting position of the next sibling bullet is the ending boundary for the current bullet. 4 Our approaches Our task is to find the correspondence between slide bullets and a speech sequence or its transcripts. Research on finding correspondences between parallel texts pervades natural language processing. For example, aligning bilingual sentence pairs is an essential step in training machine translation models. In text summarization, the correspondence between human-written summaries and their original texts has been identified (Jing, 2002), too. In speech recognition, forced alignment is applied to align speech and transcripts. In this paper, we keep the general framework of alignment in solving our problem. Our solution, however, should be flexible to consider multiple constraints such as those conveyed in hierarchical bullet structures and global word distribution. Accordingly, the model proposed in this paper depends on two orthogonal strategies to ensure efficiency and richness of the model. First of all, we formulate all our solutions within a classic dynamic programming framework to enforce computational efficiency (secti"
C10-2177,P07-1064,0,0.360242,"gnores the hierarchical structure of bullets within slides. We also explore the impact of speech recognition errors on this task. Furthermore, we study the feasibility of directly aligning a structure to raw speech, as opposed to a transcript. 2 Related work Topic/slide boundary detection The previous work most directly related to ours is research that attempts to find flat structures of spoken documents, such as topic and slide boundaries. For example, the work of (Chen and Heng, 2003; Ruddarraju, 2006; Zhu et al., 2008) aims to find slide boundaries in the corresponding lecture transcripts. Malioutov et al. (2007) developed an approach to detecting topic boundaries of lecture recordings by finding repeated acoustic patterns. None of this work, however, has involved hierarchical structures that exist at different levels of a document. In addition, researchers have also analyzed other multimedia channels, e.g., video (Liu et al., 2002; Wang et al., 2003; Fan et al., 2006), to detect slide transitions. Such approaches, however, are unlikely to find semantic structures that are more detailed than slide transitions, e.g., the bullet hierarchical structures that we are interested in. Building tables-of-conte"
C10-2177,A00-2025,0,0.595888,"rarchical and global features and the improvement is consistent on transcripts with different WERs. Directly imposing such hierarchical structures onto raw speech without using transcripts yields competitive results. The efficiency and convenience of reading spoken documents are affected by at least two facts. First, the quality of transcripts can impair browsing efficiency, e.g., as shown in (Stark et al., 2000; Munteanu et al., 2006), though if the goal is only to browse salient excerpts, recognition errors on the extracts can be reduced by considering the confidence scores assigned by ASR (Zechner and Waibel, 2000; Hori and Furui, 2003). 1 Introduction Though speech has long served as a basic method of human communication, revisiting and browsing speech content had never been a possibility before human can record their own voice. Recent technological advances in recording, compressing, and distributing such archives have led to the consistently increasing availability of spoken content. Along with this availability comes a demand for better ways to browse such archives, which is inherently more difficult than browsing text. In relying on human beings’ ability to browse text, a solution is therefore to"
C10-2177,J02-1002,0,0.448335,"Missing"
C10-2177,P07-1069,0,\N,Missing
C12-1177,P05-1074,0,0.0667073,"Missing"
C12-1177,N03-1003,0,0.0552004,"Missing"
C12-1177,D08-1021,0,0.0816547,"Missing"
C12-1177,C08-1013,0,0.0540476,"Missing"
C12-1177,C96-2183,0,0.047339,"Missing"
C12-1177,P11-1020,0,0.164662,"Missing"
C12-1177,P09-1053,0,0.0276086,"Missing"
C12-1177,C04-1051,1,0.720944,"Missing"
C12-1177,C04-1088,0,0.0176159,"Missing"
C12-1177,W07-1401,1,0.326763,"Missing"
C12-1177,D10-1051,0,0.0161918,"Missing"
C12-1177,P07-2045,0,0.0606319,"Missing"
C12-1177,N10-1017,0,0.0920108,"Missing"
C12-1177,D10-1090,0,0.0393625,"Missing"
C12-1177,J10-3003,0,0.0626954,"Missing"
C12-1177,moore-2002-fast,0,0.0606931,"Missing"
C12-1177,J03-1002,0,0.0531454,"Missing"
C12-1177,P02-1040,0,0.110879,"Missing"
C12-1177,W04-3219,0,0.0160521,"Missing"
C12-1177,P10-2008,0,0.0191379,"Missing"
C12-1177,W03-1609,0,0.0609861,"Missing"
C12-1177,D08-1027,0,0.0349755,"Missing"
C12-1177,N10-1056,0,0.0714576,"Missing"
C12-1177,P03-1021,0,\N,Missing
D09-1111,P07-1083,0,0.0306609,"aka (2005) harvest transliteration candidates from comparable bilingual corpora (conference abstracts in English and Japanese), and use distributional as well as phonetic similarity to choose among them. Sherif and Kondrak (2007a) also bootstrap a learned edit distance for Arabic named entities, with candidate pairs drawn from sentence or document-aligned parallel text. Klementiev and Roth (2006) bootstrap an SVM classifier trained to detect true transliteration-pairs. They draw candidates from comparable news text, using date information to provide further clues as to aligned named entities. Bergsma and Kondrak (2007) extend the classification approach with features derived from a character alignment. They train from bilingual dictionaries and word-aligned parallel text, selecting negative examples to target false-friends. The work of Hermjakob et al. (2008) is particularly relevant to this paper, as they incorporate a similarity-based transliteration system into an Arabic-to-English SMT engine. They employ a hand-crafted cross-lingual similarity metric, and use capitalized n-grams from the Google n-gram corpus as candidates. With such a huge candidate list, a cross-lingual indexing scheme is designed for"
D09-1111,C04-1086,0,0.188899,"candidates, but many incorporate a target lexicon, favoring target words that occur in the lexicon. This approach is also known as transliteration generation. The majority of transliteration generation approaches are based on the noisy channel model, where a target t is generated according to P (t|s) ∝ P (s|t)P (t). This approach is typified by finite-state transliteration, where the various stages of the channel model are represented by finite state transducers and automata. Early systems employed a complex channel, passing through multiple phonetic representations (Knight and Graehl, 1998; Bilac and Tanaka, 2004), but later versions replaced characters directly (AlOnaizan and Knight, 2002). Sherif and Kondrak (2007b) extend this approach with substring operations in the style of phrasal SMT, and show that doing so improves both accuracy as well as space and time efficiency. Note that it is possible to incorporate a target lexicon by making P (t) a word unigram model with a character-based back-off. Li et al. (2004) present an alternative to the noisy channel with their joint n-gram model, which calculates P (s, t). This formulation allows operations to be conditioned on both source and target context."
D09-1111,P00-1037,0,0.0579974,"dard derivations, as opposed to a maxtransliteration decoder trained directly on sourcetarget pairs. By building the entire system on the derivation level, we side-step issues that can occur when perceptron training with hidden derivations (Liang et al., 2006), but we also introduce the need to transform our training source-target pairs into training derivations. Training derivations can be learned unsupervised from source-target pairs using character alignment techniques. Previously, this has been done using an EM-learned edit distance (Ristad and Yianilos, 1998), or generalizations thereof (Brill and Moore, 2000; Jiampojamarn et al., 2007). We opt for an alternative alignment technique, similar to the word-aligner described by Zhang et al. (2008). This approach employs variational EM with sparse priors, along with hard length limits, to reduce the length of substrings operated upon. By doing so, we hope to learn only non-compositional transliteration units. Our aligner produces only monotonic alignments, and does not allow either the source or target side of an operation to be empty. The same restrictions are imposed during decoding. In this way, each alignment found by variational EM is also an unam"
D09-1111,P05-1063,0,0.067867,"Missing"
D09-1111,W02-1001,0,0.010154,"sduction. Jiampojamarn et al. (2008) describe a discriminative letter-to-phoneme substring transducer, while Dreyer et al. (2008) describe a discriminative character transducer with a latent derivation structure for morphological transformations. Both models are extremely effective, but both rely exclusively on indicator features; they do not explore the use of knowledge-rich generative models. Our indicator system uses an extended version of the Jiampojamarn et al. (2008) feature set. 3 Methods We adopt a discriminative substring decoder for our transliteration task. A structured perceptron (Collins, 2002) learns weights for our transliteration features, which are drawn from two broad classes: indicator and hybrid generative features. 3.1 Structured perceptron The decoder’s discriminative parameters are learned with structured perceptron training. Let a derivation d describe a substring operation sequence that transliterates a source word into a target word. Given an input training corpus of such derivations D = d1 . . . dn , a vector feature function on derivations F~ (d), and an initial weight vector w, ~ the perceptron performs two steps for each training example di ∈ D:  ´ • Decode: d¯ = a"
D09-1111,D08-1113,0,0.0399033,"Missing"
D09-1111,D07-1025,0,0.231485,"erating on characters rather than words. Unlike out-of-the-box phrasal SMT solutions, our generative hybrid benefits from a target a lexicon. As we will show, this is the difference between a weak baseline and a strong competitor. We demonstrate that despite recent successes in discriminative character transduction using indicator features (Jiampojamarn et al., 2008; Dreyer et al., 2008), our generative hybrid performs surprisingly well, producing our highest transliteration accuracies. Researchers frequently compare against a phrasal SMT baseline when evaluating a new transduction technique (Freitag and Khadivi, 2007; Dreyer et al., 2008); however, we are careful to vary only the features in our comparison. Confounding variables, such as alignment, decoder and training method, are held constant. We also include a human evaluation of transliteration-augmented SMT output. Though human evaluations are too expensive to allow a comparison between transliteration systems, we are able to show that adding our transliterations to a production-level SMT engine results in a substantial improvement in translation quality. 2.1 Similarity-based transliteration In similarity-based transliteration, a characterbased, cros"
D09-1111,P08-1045,0,0.0591737,"Missing"
D09-1111,N07-1047,0,0.165958,"posed to a maxtransliteration decoder trained directly on sourcetarget pairs. By building the entire system on the derivation level, we side-step issues that can occur when perceptron training with hidden derivations (Liang et al., 2006), but we also introduce the need to transform our training source-target pairs into training derivations. Training derivations can be learned unsupervised from source-target pairs using character alignment techniques. Previously, this has been done using an EM-learned edit distance (Ristad and Yianilos, 1998), or generalizations thereof (Brill and Moore, 2000; Jiampojamarn et al., 2007). We opt for an alternative alignment technique, similar to the word-aligner described by Zhang et al. (2008). This approach employs variational EM with sparse priors, along with hard length limits, to reduce the length of substrings operated upon. By doing so, we hope to learn only non-compositional transliteration units. Our aligner produces only monotonic alignments, and does not allow either the source or target side of an operation to be empty. The same restrictions are imposed during decoding. In this way, each alignment found by variational EM is also an unambiguous derivation. We align"
D09-1111,P08-1103,1,0.937656,"orm in SMT, where translation scores are provided by a discriminative combination of generative models (Och, 2003). Substring-based transliteration with a generative hybrid model is very similar to existing solutions for phrasal SMT (Koehn et al., 2003), operating on characters rather than words. Unlike out-of-the-box phrasal SMT solutions, our generative hybrid benefits from a target a lexicon. As we will show, this is the difference between a weak baseline and a strong competitor. We demonstrate that despite recent successes in discriminative character transduction using indicator features (Jiampojamarn et al., 2008; Dreyer et al., 2008), our generative hybrid performs surprisingly well, producing our highest transliteration accuracies. Researchers frequently compare against a phrasal SMT baseline when evaluating a new transduction technique (Freitag and Khadivi, 2007; Dreyer et al., 2008); however, we are careful to vary only the features in our comparison. Confounding variables, such as alignment, decoder and training method, are held constant. We also include a human evaluation of transliteration-augmented SMT output. Though human evaluations are too expensive to allow a comparison between translitera"
D09-1111,W06-1668,0,0.0410998,"Missing"
D09-1111,N06-1011,0,0.0276798,"nsliteration discovery. Brill et al. (2001) describe a katakana-toEnglish approach with an EM-learned edit distance, which bootstraps from a small number of examples to learn transliteration pairs from query logs. Bilac and Tanaka (2005) harvest transliteration candidates from comparable bilingual corpora (conference abstracts in English and Japanese), and use distributional as well as phonetic similarity to choose among them. Sherif and Kondrak (2007a) also bootstrap a learned edit distance for Arabic named entities, with candidate pairs drawn from sentence or document-aligned parallel text. Klementiev and Roth (2006) bootstrap an SVM classifier trained to detect true transliteration-pairs. They draw candidates from comparable news text, using date information to provide further clues as to aligned named entities. Bergsma and Kondrak (2007) extend the classification approach with features derived from a character alignment. They train from bilingual dictionaries and word-aligned parallel text, selecting negative examples to target false-friends. The work of Hermjakob et al. (2008) is particularly relevant to this paper, as they incorporate a similarity-based transliteration system into an Arabic-to-English"
D09-1111,W06-1672,0,0.0130199,"is approach with substring operations in the style of phrasal SMT, and show that doing so improves both accuracy as well as space and time efficiency. Note that it is possible to incorporate a target lexicon by making P (t) a word unigram model with a character-based back-off. Li et al. (2004) present an alternative to the noisy channel with their joint n-gram model, which calculates P (s, t). This formulation allows operations to be conditioned on both source and target context. However, the inclusion of a candidate list is more difficult in this setting, as P (t) is not given its own model. Zelenko and Aone (2006) investigate a purely discriminative, alignment-free approach to transliteration generation. The target word is constructed one character at a time, with each new character triggering a suite of features, including indicators for near-by source and target characters, as well a generative target language model. Freitag and Khadivi (2007) propose a discriminative, latent edit distance for transliteration. In this case, training data need not be aligned in advance, but a latent alignment is produced during decoding. Again, the target word is constructed one character at a time, using edit operati"
D09-1111,N04-1033,0,0.0426406,"rently. The lexicon encoding is the most dramatic difference, with the indicators using a small number of frequency bins, and the generative unigram model providing a single, real-valued feature that is proportional to frequency. In the case of their emission features, the two systems actually encode different information. Both have access to the same training derivations, but the indicator system provides source context through n-gram indicators, while the generative system does so using composed operations. 3.4 Decoder Our decoder builds upon machine translation’s monotone phrasal decoding (Zens and Ney, 2004), or equivalently, the sequence tagging algorithm used in semi-Markov CRFs (Sarawagi and Cohen, 2004). This dynamic programming (DP) decoder extends the Viterbi algorithm for HMMs by operating on one or more source characters (a substring) at each step. A DP block stores the best scoring solution for a particular prefix. Each block is subdivided into cells, which maintain the context necessary to calculate target-side features. We employ a beam, keeping only the 40 highestscoring cells for each block, which speeds up inference at the expense of optimality. We found that the beam had no major e"
D09-1111,P08-1012,0,0.0241749,"erivation level, we side-step issues that can occur when perceptron training with hidden derivations (Liang et al., 2006), but we also introduce the need to transform our training source-target pairs into training derivations. Training derivations can be learned unsupervised from source-target pairs using character alignment techniques. Previously, this has been done using an EM-learned edit distance (Ristad and Yianilos, 1998), or generalizations thereof (Brill and Moore, 2000; Jiampojamarn et al., 2007). We opt for an alternative alignment technique, similar to the word-aligner described by Zhang et al. (2008). This approach employs variational EM with sparse priors, along with hard length limits, to reduce the length of substrings operated upon. By doing so, we hope to learn only non-compositional transliteration units. Our aligner produces only monotonic alignments, and does not allow either the source or target side of an operation to be empty. The same restrictions are imposed during decoding. In this way, each alignment found by variational EM is also an unambiguous derivation. We align our training corpus with a maximum substring length of three characters. The same derivations are used to tr"
D09-1111,N03-1017,0,0.00669814,"cantly, our framework allows us to test two competing styles of features: • sparse indicators, designed to capture the same channel and language modeling data collected by previous generative models, and • components of existing generative models, used as real-valued features in a discriminatively weighted, generative hybrid. Note that generative hybrids are the norm in SMT, where translation scores are provided by a discriminative combination of generative models (Och, 2003). Substring-based transliteration with a generative hybrid model is very similar to existing solutions for phrasal SMT (Koehn et al., 2003), operating on characters rather than words. Unlike out-of-the-box phrasal SMT solutions, our generative hybrid benefits from a target a lexicon. As we will show, this is the difference between a weak baseline and a strong competitor. We demonstrate that despite recent successes in discriminative character transduction using indicator features (Jiampojamarn et al., 2008; Dreyer et al., 2008), our generative hybrid performs surprisingly well, producing our highest transliteration accuracies. Researchers frequently compare against a phrasal SMT baseline when evaluating a new transduction techniq"
D09-1111,P04-1021,0,0.187861,"model are represented by finite state transducers and automata. Early systems employed a complex channel, passing through multiple phonetic representations (Knight and Graehl, 1998; Bilac and Tanaka, 2004), but later versions replaced characters directly (AlOnaizan and Knight, 2002). Sherif and Kondrak (2007b) extend this approach with substring operations in the style of phrasal SMT, and show that doing so improves both accuracy as well as space and time efficiency. Note that it is possible to incorporate a target lexicon by making P (t) a word unigram model with a character-based back-off. Li et al. (2004) present an alternative to the noisy channel with their joint n-gram model, which calculates P (s, t). This formulation allows operations to be conditioned on both source and target context. However, the inclusion of a candidate list is more difficult in this setting, as P (t) is not given its own model. Zelenko and Aone (2006) investigate a purely discriminative, alignment-free approach to transliteration generation. The target word is constructed one character at a time, with each new character triggering a suite of features, including indicators for near-by source and target characters, as"
D09-1111,P06-1096,0,0.0565694,"Missing"
D09-1111,2007.mtsummit-papers.43,0,0.0179996,"t list. 5.1 Baselines We compare our systems against a reimplementation of Sherif and Kondrak’s (2007b) noisy-channel substring decoder. This uses the same PE , PT and PL models as our hybrid generative system, but employs a two-pass decoding scheme to find the max transliteration according to Equation 1. It represents a purely generative solution using otherwise identical architecture. 1071 6 http://www.microsofttranslator.com Since our hybrid generative system implements a model that is very similar to those used in phrasal SMT, we also compare against a state-of-the-art phrasal SMT system (Moore and Quirk, 2007). This system is trained by applying the standard SMT pipeline to our Wikipedia title pairs, treating characters as words, using a 7-gram characterlevel language model, and disabling re-ordering. Unfortunately, the decoder’s architecture does not allow the use of a word-level unigram model, reducing the usefulness of this baseline. Instead, we include the target lexicon as a second characterlevel language model. This baseline indicates the level of performance one can expect by applying phrasal SMT straight out of the box. Comparing the two baselines qualitatively, both use a combination of ge"
D09-1111,P03-1021,0,0.0209826,"in discriminative character transduction, allowing our decoder to benefit from a list of known target words. Perhaps more significantly, our framework allows us to test two competing styles of features: • sparse indicators, designed to capture the same channel and language modeling data collected by previous generative models, and • components of existing generative models, used as real-valued features in a discriminatively weighted, generative hybrid. Note that generative hybrids are the norm in SMT, where translation scores are provided by a discriminative combination of generative models (Och, 2003). Substring-based transliteration with a generative hybrid model is very similar to existing solutions for phrasal SMT (Koehn et al., 2003), operating on characters rather than words. Unlike out-of-the-box phrasal SMT solutions, our generative hybrid benefits from a target a lexicon. As we will show, this is the difference between a weak baseline and a strong competitor. We demonstrate that despite recent successes in discriminative character transduction using indicator features (Jiampojamarn et al., 2008; Dreyer et al., 2008), our generative hybrid performs surprisingly well, producing our h"
D09-1111,P05-1034,1,0.862944,"Missing"
D09-1111,P07-1109,0,0.229186,"a source word s, its transliteration is the target word t most similar to s, where t is drawn from some pool of candidates. This approach may also be referred to as transliteration discovery. Brill et al. (2001) describe a katakana-toEnglish approach with an EM-learned edit distance, which bootstraps from a small number of examples to learn transliteration pairs from query logs. Bilac and Tanaka (2005) harvest transliteration candidates from comparable bilingual corpora (conference abstracts in English and Japanese), and use distributional as well as phonetic similarity to choose among them. Sherif and Kondrak (2007a) also bootstrap a learned edit distance for Arabic named entities, with candidate pairs drawn from sentence or document-aligned parallel text. Klementiev and Roth (2006) bootstrap an SVM classifier trained to detect true transliteration-pairs. They draw candidates from comparable news text, using date information to provide further clues as to aligned named entities. Bergsma and Kondrak (2007) extend the classification approach with features derived from a character alignment. They train from bilingual dictionaries and word-aligned parallel text, selecting negative examples to target false-f"
D09-1111,P07-1119,0,0.720238,"a source word s, its transliteration is the target word t most similar to s, where t is drawn from some pool of candidates. This approach may also be referred to as transliteration discovery. Brill et al. (2001) describe a katakana-toEnglish approach with an EM-learned edit distance, which bootstraps from a small number of examples to learn transliteration pairs from query logs. Bilac and Tanaka (2005) harvest transliteration candidates from comparable bilingual corpora (conference abstracts in English and Japanese), and use distributional as well as phonetic similarity to choose among them. Sherif and Kondrak (2007a) also bootstrap a learned edit distance for Arabic named entities, with candidate pairs drawn from sentence or document-aligned parallel text. Klementiev and Roth (2006) bootstrap an SVM classifier trained to detect true transliteration-pairs. They draw candidates from comparable news text, using date information to provide further clues as to aligned named entities. Bergsma and Kondrak (2007) extend the classification approach with features derived from a character alignment. They train from bilingual dictionaries and word-aligned parallel text, selecting negative examples to target false-f"
D09-1111,W02-0505,0,\N,Missing
D09-1111,J98-4003,0,\N,Missing
D09-1111,D08-1076,0,\N,Missing
D11-1054,P05-1018,0,0.0133253,"nerating more natural utterances in goal-directed dialogue systems (Ratnaparkhi, 2000; Rambow et al., 2001). Currently, most dialogue systems rely on either canned responses or templates for generation, which can result in utterances which sound very unnatural in context (Chambers and Allen, 2004). Recent work has investigated the use of SMT in translating internal dialogue state into natural language (Langner et al., 2010). In addition to dialogue state, we believe it may be beneficial to consider the user’s utterance when generating responses in order to generate locally coherent discourse (Barzilay and Lapata, 2005). Data-driven generation based on users’ utterances might also be a useful way to fill in knowledge gaps in the system (Galley et al., 2001; Knight and Hatzivassiloglou, 1995). Statistical machine translation has been applied to a sm¨org˚asbord of NLP problems, including question answering (Echihabi and Marcu, 2003), semantic parsing and generation (Wong and Mooney, 2006; Wong and Mooney, 2007), summarization (Daum´e III and Marcu, 2009), generating bid-phrases in online advertising (Ravi et al., 2010), spelling correction (Sun et al., 2010), paraphrase (Dolan et al., 2004; Quirk et al., 2004)"
D11-1054,W10-0733,0,0.0758945,"Missing"
D11-1054,J90-2002,0,0.0683599,"ether in the training data. Because there is a wide range of acceptable responses to any status, these identical pairs have the strongest associations in the data, and therefore dominate the phrase table. In order to discourage lexically similar translations, we filter out all phrase-pairs where one phrase is a substring of the other, and introduce a novel feature to penalize lexical similarity: φlex (s, t) = J(s, t) Where J(s, t) is the Jaccard similarity between the set of words in s and t. 4.2 Challenge: Word Alignment Alignment is more difficult in conversational data than bilingual data (Brown et al., 1990), or textual entailment data (Brockett, 2006; MacCartney et al., 2008). In conversational data, there are some cases in which there is a decomposable alignment between . . . . . . . .         . . . . . . . . .         . . . . . . . . .         . question . . . . . . . . . . . . . . . .  easier please . if anyones still awake lets play a game. name 3 kevin costner movies that dont suck . Figure 2: Example from the data where word alignment is difficult (requires alignment between large phrases in the status and response). words, as seen in figure 1, and some difficult c"
D11-1054,W09-0401,0,0.0484964,"Missing"
D11-1054,W04-2302,0,0.0128491,"plate rules for generating responses. In contrast, we focus on the simpler task of generating an appropriate response to a single utterance. We leverage large amounts of conversational training data to scale to our Social Media domain, where conversations can be on just about any topic. Additionally, there has been work on generating more natural utterances in goal-directed dialogue systems (Ratnaparkhi, 2000; Rambow et al., 2001). Currently, most dialogue systems rely on either canned responses or templates for generation, which can result in utterances which sound very unnatural in context (Chambers and Allen, 2004). Recent work has investigated the use of SMT in translating internal dialogue state into natural language (Langner et al., 2010). In addition to dialogue state, we believe it may be beneficial to consider the user’s utterance when generating responses in order to generate locally coherent discourse (Barzilay and Lapata, 2005). Data-driven generation based on users’ utterances might also be a useful way to fill in knowledge gaps in the system (Galley et al., 2001; Knight and Hatzivassiloglou, 1995). Statistical machine translation has been applied to a sm¨org˚asbord of NLP problems, including"
D11-1054,C04-1051,0,0.0105841,"t discourse (Barzilay and Lapata, 2005). Data-driven generation based on users’ utterances might also be a useful way to fill in knowledge gaps in the system (Galley et al., 2001; Knight and Hatzivassiloglou, 1995). Statistical machine translation has been applied to a sm¨org˚asbord of NLP problems, including question answering (Echihabi and Marcu, 2003), semantic parsing and generation (Wong and Mooney, 2006; Wong and Mooney, 2007), summarization (Daum´e III and Marcu, 2009), generating bid-phrases in online advertising (Ravi et al., 2010), spelling correction (Sun et al., 2010), paraphrase (Dolan et al., 2004; Quirk et al., 2004) and query expansion (Riezler et al., 2007). Most relevant to our efforts is the work by Soricut and Marcu (2006), who applied the IBM word alignment models to a discourse ordering task, exploiting the same intuition investigated 3 Data For learning response-generation models, we use a corpus of roughly 1.3 million conversations scraped from Twitter (Ritter et al., 2010; DanescuNiculescu-Mizil et al., 2011). Twitter conversations don’t occur in real-time as in IRC; rather as in email, users typically take turns responding to each other. Twitter’s 140 character limit, howev"
D11-1054,P03-1003,0,0.00509224,"vestigated the use of SMT in translating internal dialogue state into natural language (Langner et al., 2010). In addition to dialogue state, we believe it may be beneficial to consider the user’s utterance when generating responses in order to generate locally coherent discourse (Barzilay and Lapata, 2005). Data-driven generation based on users’ utterances might also be a useful way to fill in knowledge gaps in the system (Galley et al., 2001; Knight and Hatzivassiloglou, 1995). Statistical machine translation has been applied to a sm¨org˚asbord of NLP problems, including question answering (Echihabi and Marcu, 2003), semantic parsing and generation (Wong and Mooney, 2006; Wong and Mooney, 2007), summarization (Daum´e III and Marcu, 2009), generating bid-phrases in online advertising (Ravi et al., 2010), spelling correction (Sun et al., 2010), paraphrase (Dolan et al., 2004; Quirk et al., 2004) and query expansion (Riezler et al., 2007). Most relevant to our efforts is the work by Soricut and Marcu (2006), who applied the IBM word alignment models to a discourse ordering task, exploiting the same intuition investigated 3 Data For learning response-generation models, we use a corpus of roughly 1.3 million"
D11-1054,P08-1095,0,0.00653504,"ering task, exploiting the same intuition investigated 3 Data For learning response-generation models, we use a corpus of roughly 1.3 million conversations scraped from Twitter (Ritter et al., 2010; DanescuNiculescu-Mizil et al., 2011). Twitter conversations don’t occur in real-time as in IRC; rather as in email, users typically take turns responding to each other. Twitter’s 140 character limit, however, keeps conversations chat-like. In addition, the Twitter API maintains a reference from each reply to the post it responds to, so unlike IRC, there is no need for conversation disentanglement (Elsner and Charniak, 2008; Wang and Oard, 2009). The first message of a conversation is typically unique, not directed at any particular user but instead broadcast to the author’s followers (a status message). For the purposes of this paper, we limit the data set to only the first two utterances from each conversation. As a result of this constraint, any system trained with this data will be specialized for responding to Twitter status posts. 4 Response Generation as Translation When applied to conversations, SMT models the probability of a response r given the input statuspost s using a log-linear combination of feat"
D11-1054,W03-2506,0,0.0113476,"s posts. Note that we make no mention of context, intent or dialogue state; our goal is to generate any response that fits the provided stimulus; however, we do so without employing rules or templates, with the hope of creating a system that is both flexible and extensible when operating in an open domain. Success in open domain response generation could be immediately useful to social media platforms, providing a list of suggested responses to a target status, or providing conversation-aware autocomplete for responses in progress. These features are especially important on hand-held devices (Hasselgren et al., 2003). Response generation should also be beneficial in building “chatterbots” (Weizenbaum, 1966) for entertainment purposes or companionship (Wilks, 2006). However, we are most excited by the future potential of data-driven response generation when used inside larger dialogue systems, where direct consideration of the user’s utterance could be combined with dialogue state (Wong and Mooney, 2007; Langner et al., 2010) to generate locally coherent, purposeful dialogue. In this work, we investigate statistical machine translation as an approach for response generation. We are motivated by the followi"
D11-1054,D07-1103,0,0.0466526,"pared to a dataset of aligned French-English sentence pairs (the WMT 08 news commentary data) where the average number of intersection alignments is 14. Direct Phrase Pair Extraction C(s, t) C(¬s, t) C(t) C(s, ¬t) C(¬s, ¬t) N − C(t) C(s) N − C(s) N Figure 3: Contingency table for phrase pair (s,t). Fisher’s Exact Test estimates the probability of seeing this event, or one more extreme assuming s and t are independent. ing whether its phrases form a valid mapping. We consider all possible phrase-pairs in the training data,1 then use Fisher’s Exact Test to filter out pairs with low correlation (Johnson et al., 2007). Given a source and target phrase s and t, we consider the contingency table illustrated in figure 3, which includes co-occurrence counts for s and t, the number of sentence-pairs containing s, but not t and vice versa, in addition to the number of pairs containing neither s nor t. Fisher’s Exact Test provides us with an estimate of the probability of observing this table, or one more extreme, assuming s and t are independent; in other words it gives us a measure of how strongly associated they are. In contrast to statistical tests such as χ2 , or the G2 Log Likelihood Ratio, Fisher’s Exact T"
D11-1054,P95-1034,0,0.012965,"ponses or templates for generation, which can result in utterances which sound very unnatural in context (Chambers and Allen, 2004). Recent work has investigated the use of SMT in translating internal dialogue state into natural language (Langner et al., 2010). In addition to dialogue state, we believe it may be beneficial to consider the user’s utterance when generating responses in order to generate locally coherent discourse (Barzilay and Lapata, 2005). Data-driven generation based on users’ utterances might also be a useful way to fill in knowledge gaps in the system (Galley et al., 2001; Knight and Hatzivassiloglou, 1995). Statistical machine translation has been applied to a sm¨org˚asbord of NLP problems, including question answering (Echihabi and Marcu, 2003), semantic parsing and generation (Wong and Mooney, 2006; Wong and Mooney, 2007), summarization (Daum´e III and Marcu, 2009), generating bid-phrases in online advertising (Ravi et al., 2010), spelling correction (Sun et al., 2010), paraphrase (Dolan et al., 2004; Quirk et al., 2004) and query expansion (Riezler et al., 2007). Most relevant to our efforts is the work by Soricut and Marcu (2006), who applied the IBM word alignment models to a discourse ord"
D11-1054,D08-1084,0,0.0102901,"Missing"
D11-1054,W04-3243,0,0.0261509,"to statistical tests such as χ2 , or the G2 Log Likelihood Ratio, Fisher’s Exact Test produces accurate p-values even when the expected counts are small (as is extremely common in our case). In Fisher’s Exact Test, the hypergeometric probability distribution is used to compute the exact probability of a particular joint frequency assuming a model of independence: C(s)!C(¬s)!C(t)!C(¬t)! N !C(s, t)!C(¬s, t)!C(s, ¬t)!C(¬s, ¬t)! The statistic is computed by summing the probability for the joint frequency in Table 3, and every more extreme joint frequency consistent with the marginal frequencies. Moore (2004) illustrates several tricks which make this computation feasible in practice. We found that this approach generates phrasetable entries which appear quite reasonable upon manual inspection. The top 20 phrase-pairs (after filtering out identical source/target phrases, substrings, Because word alignment in status/response pairs is 1 We define a possible phrase-pair as any pair of phrases a difficult problem, instead of relying on local align- found in a sentence-pair from our training corpus, where both ments for extracting phrase pairs, we exploit infor- phrases consist of 4 tokens or fewer. Th"
D11-1054,J03-1002,0,0.0503241,". . . . . . . . . . . .  easier please . if anyones still awake lets play a game. name 3 kevin costner movies that dont suck . Figure 2: Example from the data where word alignment is difficult (requires alignment between large phrases in the status and response). words, as seen in figure 1, and some difficult cases where alignment between large phrases is required, for example figure 2. These difficult sentence pairs confuse the IBM word alignment models which have no way to distinguish between the easy and hard cases. We aligned words in our parallel data using the widely used tool GIZA++ (Och and Ney, 2003); however, the standard growing heuristic resulted in very noisy alignments. Precision could be improved considerably by using the intersection of GIZA++ trained in two directions (s → r, and r → s), but the alignment also became extremely sparse. The average number of alignments-per status/response pair in our data was only 1.7, as compared to a dataset of aligned French-English sentence pairs (the WMT 08 news commentary data) where the average number of intersection alignments is 14. Direct Phrase Pair Extraction C(s, t) C(¬s, t) C(t) C(s, ¬t) C(¬s, ¬t) N − C(t) C(s) N − C(s) N Figure 3: Con"
D11-1054,P03-1021,0,0.0439363,"arry cream you ? morning norris movie miss you too i ’m happy birthday good luck it was i miss flu love you too are you ? i did michael i ’m good mj We do not use any form of SMT reordering model, as the position of the phrase in the response does not seem to be very correlated with the corresponding position in the status. Instead we let the language model drive reordering. We used the default feature weights provided by Moses.4 Because automatic evaluation of response generation is an open problem, we avoided the use of discriminative training algorithms such as Minimum Error-Rate Training (Och, 2003). 5 One straightforward data-driven approach to response generation is nearest neighbour, or information retrieval. This general approach has been applied previously by several authors (Isbell et al., 2000; Swanson and Gordon, 2008; Jafarpour and Burges, 2010), and is used as a point of comparison in our experiments. Given a novel status s and a training corpus of status/response pairs, two retrieval strategies can be used to return a best response r0 : Table 1: Top 20 Phrase Pairs ranked by the Fisher Exact Test statistic. Slight variations (substrings or symmetric pairs) were removed to show"
D11-1054,P02-1040,0,0.11072,"ul in this conversational setting. Finally, as an additional baseline, we compared M T-C HAT’s output to random responses selected from those observed 2 or more times in the training data. One might argue that short, common responses are very general, and that a reply like “lol” could be considered a good response to almost any status. However, the human evaluation shows a clear preference for M T-C HAT’s output: raters favour responses that are tailored to the stimulus. 6.3 Automatic Evaluation The field of SMT has benefited greatly from the existence of an automatic evaluation metric, BLEU (Papineni et al., 2002), which grades an output candidate according to n-gram matches to one or more reference outputs. To evaluate whether BLEU is an appropriate automatic evaluation measure for response generation, we attempted to measure its agreement with the human judgments. We calculate BLEU using a single reference derived from our parallel corpus. We show the smoothed BLEU 1-4 scores for each system on each dataset evaluated in Table 4. Although these scores are extremely low, the overall BLEU scores agree with overall annotator judgments in all cases except when comparing M T-C HAT and I R -R ESPONSE. It wo"
D11-1054,W04-3219,1,0.21248,"Missing"
D11-1054,H01-1055,0,0.0615326,"which attempt to engage users, typically leading the topic of conversation. They usually limit interactions to a specific scenario (e.g. a Rogerian psychotherapist), and use a set of template rules for generating responses. In contrast, we focus on the simpler task of generating an appropriate response to a single utterance. We leverage large amounts of conversational training data to scale to our Social Media domain, where conversations can be on just about any topic. Additionally, there has been work on generating more natural utterances in goal-directed dialogue systems (Ratnaparkhi, 2000; Rambow et al., 2001). Currently, most dialogue systems rely on either canned responses or templates for generation, which can result in utterances which sound very unnatural in context (Chambers and Allen, 2004). Recent work has investigated the use of SMT in translating internal dialogue state into natural language (Langner et al., 2010). In addition to dialogue state, we believe it may be beneficial to consider the user’s utterance when generating responses in order to generate locally coherent discourse (Barzilay and Lapata, 2005). Data-driven generation based on users’ utterances might also be a useful way to"
D11-1054,A00-2026,0,0.0223351,"ikh et al., 2010), which attempt to engage users, typically leading the topic of conversation. They usually limit interactions to a specific scenario (e.g. a Rogerian psychotherapist), and use a set of template rules for generating responses. In contrast, we focus on the simpler task of generating an appropriate response to a single utterance. We leverage large amounts of conversational training data to scale to our Social Media domain, where conversations can be on just about any topic. Additionally, there has been work on generating more natural utterances in goal-directed dialogue systems (Ratnaparkhi, 2000; Rambow et al., 2001). Currently, most dialogue systems rely on either canned responses or templates for generation, which can result in utterances which sound very unnatural in context (Chambers and Allen, 2004). Recent work has investigated the use of SMT in translating internal dialogue state into natural language (Langner et al., 2010). In addition to dialogue state, we believe it may be beneficial to consider the user’s utterance when generating responses in order to generate locally coherent discourse (Barzilay and Lapata, 2005). Data-driven generation based on users’ utterances might a"
D11-1054,P07-1059,0,0.00589638,"on based on users’ utterances might also be a useful way to fill in knowledge gaps in the system (Galley et al., 2001; Knight and Hatzivassiloglou, 1995). Statistical machine translation has been applied to a sm¨org˚asbord of NLP problems, including question answering (Echihabi and Marcu, 2003), semantic parsing and generation (Wong and Mooney, 2006; Wong and Mooney, 2007), summarization (Daum´e III and Marcu, 2009), generating bid-phrases in online advertising (Ravi et al., 2010), spelling correction (Sun et al., 2010), paraphrase (Dolan et al., 2004; Quirk et al., 2004) and query expansion (Riezler et al., 2007). Most relevant to our efforts is the work by Soricut and Marcu (2006), who applied the IBM word alignment models to a discourse ordering task, exploiting the same intuition investigated 3 Data For learning response-generation models, we use a corpus of roughly 1.3 million conversations scraped from Twitter (Ritter et al., 2010; DanescuNiculescu-Mizil et al., 2011). Twitter conversations don’t occur in real-time as in IRC; rather as in email, users typically take turns responding to each other. Twitter’s 140 character limit, however, keeps conversations chat-like. In addition, the Twitter API"
D11-1054,N10-1020,1,0.362331,"ong and Mooney, 2006; Wong and Mooney, 2007), summarization (Daum´e III and Marcu, 2009), generating bid-phrases in online advertising (Ravi et al., 2010), spelling correction (Sun et al., 2010), paraphrase (Dolan et al., 2004; Quirk et al., 2004) and query expansion (Riezler et al., 2007). Most relevant to our efforts is the work by Soricut and Marcu (2006), who applied the IBM word alignment models to a discourse ordering task, exploiting the same intuition investigated 3 Data For learning response-generation models, we use a corpus of roughly 1.3 million conversations scraped from Twitter (Ritter et al., 2010; DanescuNiculescu-Mizil et al., 2011). Twitter conversations don’t occur in real-time as in IRC; rather as in email, users typically take turns responding to each other. Twitter’s 140 character limit, however, keeps conversations chat-like. In addition, the Twitter API maintains a reference from each reply to the post it responds to, so unlike IRC, there is no need for conversation disentanglement (Elsner and Charniak, 2008; Wang and Oard, 2009). The first message of a conversation is typically unique, not directed at any particular user but instead broadcast to the author’s followers (a stat"
D11-1054,W10-2708,0,0.0332719,"Missing"
D11-1054,D08-1027,0,0.0254844,"Missing"
D11-1054,P06-2103,0,0.00920762,"knowledge gaps in the system (Galley et al., 2001; Knight and Hatzivassiloglou, 1995). Statistical machine translation has been applied to a sm¨org˚asbord of NLP problems, including question answering (Echihabi and Marcu, 2003), semantic parsing and generation (Wong and Mooney, 2006; Wong and Mooney, 2007), summarization (Daum´e III and Marcu, 2009), generating bid-phrases in online advertising (Ravi et al., 2010), spelling correction (Sun et al., 2010), paraphrase (Dolan et al., 2004; Quirk et al., 2004) and query expansion (Riezler et al., 2007). Most relevant to our efforts is the work by Soricut and Marcu (2006), who applied the IBM word alignment models to a discourse ordering task, exploiting the same intuition investigated 3 Data For learning response-generation models, we use a corpus of roughly 1.3 million conversations scraped from Twitter (Ritter et al., 2010; DanescuNiculescu-Mizil et al., 2011). Twitter conversations don’t occur in real-time as in IRC; rather as in email, users typically take turns responding to each other. Twitter’s 140 character limit, however, keeps conversations chat-like. In addition, the Twitter API maintains a reference from each reply to the post it responds to, so u"
D11-1054,P10-1028,0,0.0462903,"der to generate locally coherent discourse (Barzilay and Lapata, 2005). Data-driven generation based on users’ utterances might also be a useful way to fill in knowledge gaps in the system (Galley et al., 2001; Knight and Hatzivassiloglou, 1995). Statistical machine translation has been applied to a sm¨org˚asbord of NLP problems, including question answering (Echihabi and Marcu, 2003), semantic parsing and generation (Wong and Mooney, 2006; Wong and Mooney, 2007), summarization (Daum´e III and Marcu, 2009), generating bid-phrases in online advertising (Ravi et al., 2010), spelling correction (Sun et al., 2010), paraphrase (Dolan et al., 2004; Quirk et al., 2004) and query expansion (Riezler et al., 2007). Most relevant to our efforts is the work by Soricut and Marcu (2006), who applied the IBM word alignment models to a discourse ordering task, exploiting the same intuition investigated 3 Data For learning response-generation models, we use a corpus of roughly 1.3 million conversations scraped from Twitter (Ritter et al., 2010; DanescuNiculescu-Mizil et al., 2011). Twitter conversations don’t occur in real-time as in IRC; rather as in email, users typically take turns responding to each other. Twit"
D11-1054,N09-1023,0,0.0165476,"same intuition investigated 3 Data For learning response-generation models, we use a corpus of roughly 1.3 million conversations scraped from Twitter (Ritter et al., 2010; DanescuNiculescu-Mizil et al., 2011). Twitter conversations don’t occur in real-time as in IRC; rather as in email, users typically take turns responding to each other. Twitter’s 140 character limit, however, keeps conversations chat-like. In addition, the Twitter API maintains a reference from each reply to the post it responds to, so unlike IRC, there is no need for conversation disentanglement (Elsner and Charniak, 2008; Wang and Oard, 2009). The first message of a conversation is typically unique, not directed at any particular user but instead broadcast to the author’s followers (a status message). For the purposes of this paper, we limit the data set to only the first two utterances from each conversation. As a result of this constraint, any system trained with this data will be specialized for responding to Twitter status posts. 4 Response Generation as Translation When applied to conversations, SMT models the probability of a response r given the input statuspost s using a log-linear combination of feature functions. Most pr"
D11-1054,N06-1056,0,0.00410199,"tate into natural language (Langner et al., 2010). In addition to dialogue state, we believe it may be beneficial to consider the user’s utterance when generating responses in order to generate locally coherent discourse (Barzilay and Lapata, 2005). Data-driven generation based on users’ utterances might also be a useful way to fill in knowledge gaps in the system (Galley et al., 2001; Knight and Hatzivassiloglou, 1995). Statistical machine translation has been applied to a sm¨org˚asbord of NLP problems, including question answering (Echihabi and Marcu, 2003), semantic parsing and generation (Wong and Mooney, 2006; Wong and Mooney, 2007), summarization (Daum´e III and Marcu, 2009), generating bid-phrases in online advertising (Ravi et al., 2010), spelling correction (Sun et al., 2010), paraphrase (Dolan et al., 2004; Quirk et al., 2004) and query expansion (Riezler et al., 2007). Most relevant to our efforts is the work by Soricut and Marcu (2006), who applied the IBM word alignment models to a discourse ordering task, exploiting the same intuition investigated 3 Data For learning response-generation models, we use a corpus of roughly 1.3 million conversations scraped from Twitter (Ritter et al., 2010;"
D11-1054,N07-1022,0,0.0183228,"latforms, providing a list of suggested responses to a target status, or providing conversation-aware autocomplete for responses in progress. These features are especially important on hand-held devices (Hasselgren et al., 2003). Response generation should also be beneficial in building “chatterbots” (Weizenbaum, 1966) for entertainment purposes or companionship (Wilks, 2006). However, we are most excited by the future potential of data-driven response generation when used inside larger dialogue systems, where direct consideration of the user’s utterance could be combined with dialogue state (Wong and Mooney, 2007; Langner et al., 2010) to generate locally coherent, purposeful dialogue. In this work, we investigate statistical machine translation as an approach for response generation. We are motivated by the following observation: In naturally occurring discourse, there is often a strong structural relationship between adjacent utterances (Hobbs, 1985). For example, consider the stimulusresponse pair from the data: Stimulus: I’m slowly making this soup ...... and it smells gorgeous! Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 583–593, c Edinburgh, Scot"
D11-1054,P07-2045,0,\N,Missing
D11-1054,J08-1001,0,\N,Missing
D11-1054,J08-4004,0,\N,Missing
D11-1054,J05-4004,0,\N,Missing
D11-1054,D08-1076,0,\N,Missing
D13-1201,J93-2003,0,0.0480096,"g to compute w0 in the first place is a bit of a disadvantage compared to standard MERT, the need for good initializer is hardly surprising in the context of non-convex optimization. Other non-convex problems in machine learning, such as deep neural networks (DNN) and word alignment models, commonly require such initializers in order to obtain decent performance. In the case of DNN, extensive research is devoted to the problem of finding good initializers.10 In the case of word alignment, it is common practice to initialize search in non-convex optimization problems—such as IBM Model 3 and 4 (Brown et al., 1993)—with solutions of simpler models—such as IBM Model 1. 7 Related work the weights in the context of MERT, (Cer et al., 2008) achieves a related effect. Cer et al.’s goal is to achieve a more regular or smooth objective function, while ours is to obtain a more regular set of parameters. The two approaches may be complementary. More recently, new research has explored direction finding using a smooth surrogate loss function (Flanigan et al., 2013). Although this method is successful in helping MERT find better directions, it also exacerbates the tendency of MERT to overfit.11 As an indirect way"
D13-1201,W08-0304,0,0.721887,"ularized objective function along the line. Finally, we address the issue of searching in a high-dimensional space by using the gradient of expected BLEU (Smith and Eisner, 2006) to find better search directions for our line searches. This direction finder addresses one of the serious concerns raised by Hopkins and May (2011): MERT widely failed to reach the optimum of a synthetic linear objective function. In replicating Hopkins and May’s experiments, we confirm that existing search algorithms for MERT—including coordinate ascent, Powell’s algorithm (Powell, 1964), and random direction sets (Cer et al., 2008)—perform poorly in this experimental condition. However, when using our gradient-based direction finder, MERT has no problem finding the true optimum even in a 1000-dimensional space. Our results suggest that the combination of a regularized objective function and a gradient-informed line search algorithm enables MERT to scale well with a large number of features. Experiments with up to 3600 features show that these extensions of MERT yield results comparable to PRO (Hopkins and May, 2011), a parameter tuning method known to be effective with large feature sets. 2 Unregularized MERT Prior to i"
D13-1201,N12-1047,1,0.910733,"Microsoft Research National Research Council Kristina Toutanova Microsoft Research mgalley@microsoft.com chrisq@microsoft.com kristout@microsoft.com colin.cherry@nrc-cnrc.gc.ca Abstract Secondly, it offers a globally optimal line search. Unfortunately, there are several potential difficulties in scaling MERT to larger numbers of features, due to its non-convex loss function and its lack of regularization. These challenges have prompted some researchers to move away from MERT, in favor of linearly decomposable approximations of the evaluation metric (Chiang et al., 2009; Hopkins and May, 2011; Cherry and Foster, 2012), which correspond to easier optimization problems and which naturally incorporate regularization. In particular, recent work (Chiang et al., 2009) has shown that adding thousands or tens of thousands of features can improve MT quality when weights are optimized using a margin-based approximation. On simulated datasets, Hopkins and May (2011) found that conventional MERT struggles to find reasonable parameter vectors, where a smooth loss function based on Pairwise Ranking Optimization (PRO) performs much better; on real data, this PRO method appears at least as good as MERT on small feature se"
D13-1201,N13-1003,1,0.916487,"g to w∗ has a BLEU score of 1, and so that the translation with lowest model score for the sentence gets a BLEU of zero. This normalization has no impact on search, but makes results more interpretable. For our translation experiments, we use multistack phrase-based decoding (Koehn et al., 2007). We report results for two feature sets: non-linear features induced using Gradient Boosting Machines (Toutanova and Ahn, 2013) and sparse lexicalized 7 The objective function remains piecewise constant, and the plateau containing w∗ maps to the optimal value of the function. 1954 reordering features (Cherry, 2013). We exploit these feature sets (GBM and SparseHRM, respectively) in two distinct experimental conditions, which we detail in the two next paragraphs. Both GBM and SparseHRM augment baseline features similar to Moses’: relative frequency and lexicalized phrase translation scores for both translation directions; one or two language model features, depending on the language pair; distortion penalty; word and phrase count; six lexicalized reordering features. For both experimental conditions, phrase tables have maximum phrase length of 7 words on either side. In reference to Table 1, we used the"
D13-1201,D08-1024,0,0.392627,"Missing"
D13-1201,N09-1025,0,0.103202,"Microsoft Research Chris Quirk Colin Cherry Microsoft Research National Research Council Kristina Toutanova Microsoft Research mgalley@microsoft.com chrisq@microsoft.com kristout@microsoft.com colin.cherry@nrc-cnrc.gc.ca Abstract Secondly, it offers a globally optimal line search. Unfortunately, there are several potential difficulties in scaling MERT to larger numbers of features, due to its non-convex loss function and its lack of regularization. These challenges have prompted some researchers to move away from MERT, in favor of linearly decomposable approximations of the evaluation metric (Chiang et al., 2009; Hopkins and May, 2011; Cherry and Foster, 2012), which correspond to easier optimization problems and which naturally incorporate regularization. In particular, recent work (Chiang et al., 2009) has shown that adding thousands or tens of thousands of features can improve MT quality when weights are optimized using a margin-based approximation. On simulated datasets, Hopkins and May (2011) found that conventional MERT struggles to find reasonable parameter vectors, where a smooth loss function based on Pairwise Ranking Optimization (PRO) performs much better; on real data, this PRO method app"
D13-1201,J07-2003,0,0.09406,"tion can drive the regularization term down to zero by scaling down w. As special treatments for `2 , we evaluate three linear transforms of the weight vector, where the vector w of the regularization term ||w||22 /2σ 2 is replaced with either: 1. an affine transform: w − w0 2. a vector with only (D − 1) free parameters, e.g., 0 ) (1, w20 , · · · , wD 3. an `1 renormalization: w/||w||1 In (1), regularization is biased towards w0 , a weight vector previously optimized using a competitive yet much smaller feature set, such as core features of a phrase-based (Koehn et al., 2007) or hierarchical (Chiang, 2007) system. The requirement that this feature set be small is to prevent overfitting. Otherwise, any regularization toward an overfit parameter vector w0 would defeat the purpose of introducing a regularization term in the first place.3 In (2), the transformation is motivated by the observation that the D-parameter linear model of Equation 2 only needs (D − 1) degrees of freedom. Fixing one of the components of w to any non-zero constant and allowing the others to vary, the new linear model retains the same modeling power, but the (D − 1) free parameters are no longer scale invariant, i.e., scali"
D13-1201,N13-1025,0,0.320973,"Missing"
D13-1201,D11-1004,1,0.672028,"search towards the greatest increase of expected BLEU score. While our best results are comparable to PRO and not significantly better, we think that this paper provides a deeper understanding of why standard MERT can fail when handling an increasingly larger number of features. Furthermore, this paper complements the analysis by Hopkins and May (2011) of the differences between MERT and optimization with a surrogate loss function. MERT and its extensions have been the target of extensive research (Och, 2003; Macherey et al., 2008; Cer et al., 2008; Moore and Quirk, 2008; Kumar et al., 2009; Galley and Quirk, 2011). More recent work has focused on replacing MERT with a linearly decomposable approximations of the evaluation metric (Smith and Eisner, 2006; Liang et al., 2006; Watanabe et al., 2007; Chiang et al., 2008; Hopkins and May, 2011; Rosti et al., 2011; Gimpel and Smith, 2012; Cherry and Foster, 2012), which generally involve a surrogate loss function incorporating a regularization term such as the `2 -norm. While we are not aware of any previous work adding a penalty on We thank the anonymous reviewers for their helpful comments and suggestions. 10 For example, (Larochelle et al., 2009) presents"
D13-1201,N12-1023,0,0.787448,"one of the components of w to any non-zero constant and allowing the others to vary, the new linear model retains the same modeling power, but the (D − 1) free parameters are no longer scale invariant, i.e., scaling the (D − 1)-dimensional vector now has an effect on linear model predictions. In (3), the weight vector is normalized as to have an `1 -norm equal to 1. In contrast, the `0 norm is scale insensitive, thus not affected by this problem. 3.1 Exact line search with regularization Optimizing with a regularized error surface requires a change in the line search algorithm presented in 3 (Gimpel and Smith, 2012, footnote 6) briefly mentions the use of such a regularizer with its ramp loss objective function. 1951 Section 2, but the other aspects of MERT remain the same, and we can still use global search algorithms such as coordinate ascent, Powell, and random directions exactly the same way as with unregularized MERT. Line search with a regularization term is still as efficient as in (Och, 2003), and it is still guaranteed to find the optimum of the (now regularized) objective function along the line. Considering again a given point wt and a given direction dt at line search iteration t, finding th"
D13-1201,P12-1031,0,0.190707,"Missing"
D13-1201,D11-1125,0,0.578272,"ris Quirk Colin Cherry Microsoft Research National Research Council Kristina Toutanova Microsoft Research mgalley@microsoft.com chrisq@microsoft.com kristout@microsoft.com colin.cherry@nrc-cnrc.gc.ca Abstract Secondly, it offers a globally optimal line search. Unfortunately, there are several potential difficulties in scaling MERT to larger numbers of features, due to its non-convex loss function and its lack of regularization. These challenges have prompted some researchers to move away from MERT, in favor of linearly decomposable approximations of the evaluation metric (Chiang et al., 2009; Hopkins and May, 2011; Cherry and Foster, 2012), which correspond to easier optimization problems and which naturally incorporate regularization. In particular, recent work (Chiang et al., 2009) has shown that adding thousands or tens of thousands of features can improve MT quality when weights are optimized using a margin-based approximation. On simulated datasets, Hopkins and May (2011) found that conventional MERT struggles to find reasonable parameter vectors, where a smooth loss function based on Pairwise Ranking Optimization (PRO) performs much better; on real data, this PRO method appears at least as good a"
D13-1201,P07-2045,0,0.0817011,"constant intervals of the corpus-level error function, and by selecting the one that has the lowest error count (or, correspondingly, highest BLEU score). Assuming the optimum is found in the interval [γk−1 , γk ], we define γopt = (γk−1 + γk )/2 and change the parameters using the update wt+1 = wt + γopt · dt . Finally, this method is turned into a global Ddimensional search using algorithms that repeatedly use the aforementioned exact line search algorithm. Och (2003) first advocated the use of Powell’s method (Powell, 1964; Press et al., 2007). Pharaoh (Koehn, 2004) and subsequently Moses (Koehn et al., 2007) instead use coordinate ascent, and more recent work often uses random search directions (Cer et al., 2008; Macherey et al., 2008). In Section 4, we will present a novel direction finder for maximum-BLEU optimization, which uses the gradient of expected BLEU to find directions where the BLEU score is most likely to increase. 3 Regularization for MERT Because MERT is prone to overfitting when a large number of parameters must be optimized, we study the addition of a regularization term to the objective function. One conventional approach is to regularize the objective function with a penalty ba"
D13-1201,koen-2004-pharaoh,0,0.526758,"omputed by enumerating all piecewise constant intervals of the corpus-level error function, and by selecting the one that has the lowest error count (or, correspondingly, highest BLEU score). Assuming the optimum is found in the interval [γk−1 , γk ], we define γopt = (γk−1 + γk )/2 and change the parameters using the update wt+1 = wt + γopt · dt . Finally, this method is turned into a global Ddimensional search using algorithms that repeatedly use the aforementioned exact line search algorithm. Och (2003) first advocated the use of Powell’s method (Powell, 1964; Press et al., 2007). Pharaoh (Koehn, 2004) and subsequently Moses (Koehn et al., 2007) instead use coordinate ascent, and more recent work often uses random search directions (Cer et al., 2008; Macherey et al., 2008). In Section 4, we will present a novel direction finder for maximum-BLEU optimization, which uses the gradient of expected BLEU to find directions where the BLEU score is most likely to increase. 3 Regularization for MERT Because MERT is prone to overfitting when a large number of parameters must be optimized, we study the addition of a regularization term to the objective function. One conventional approach is to regular"
D13-1201,P09-1019,0,0.704285,"rches. Experiments with up to 3600 features show that these extensions of MERT yield results comparable to PRO, a learner often used with large feature sets. 1 Introduction Minimum Error Rate Training emerged a decade ago (Och, 2003) as a superior training method for small numbers of linear model parameters of machine translation systems, improving over prior work using maximum likelihood criteria (Och and Ney, 2002). This technique quickly rose to prominence, becoming standard in many research and commercial MT systems. Variants operating over lattices (Macherey et al., 2008) or hypergraphs (Kumar et al., 2009) were subsequently developed, with the benefit of reducing the approximation error from n-best lists. The primary advantages of MERT are twofold. It directly optimizes the evaluation metric under consideration (e.g., BLEU) instead of some surrogate loss. In this paper, we seek to preserve the advantages of MERT while addressing its shortcomings in terms of regularization and search. The idea of adding a regularization term to the MERT objective function can be perplexing at first, because the most common regularizers, such as `1 and `2 , are not directly applicable to MERT. Indeed, these regul"
D13-1201,P06-1096,0,0.129495,"Missing"
D13-1201,C04-1072,0,0.0497267,"our optimizer featuring both regularization and the gradient-based direction finder. All variants of MERT are initialized with a single starting point, which is either uniform weight or w0 . Instead of providing MERT with additional random starting points as in Moses, we use random walks as in (Moore and Quirk, 2008) to attempt to move out of local optima.8 Since PRO and our optimizer have hyperparameters, we use a held-out set (Dev) for adjusting them. For PRO, we adjust three parameters: a regularization penalty for `2 , the parameter α in the add-α smoothed sentence-level version of BLEU (Lin and Och, 2004), and a parameter for scaling the corpus-level length of the references. The latter scaling parameter is discussed in (He and 8 In the case of the gradient-based direction finder, we also use the following strategy whenever optimization converges to a (possibly local) optimum. We run one round of coordinate ascent, and continue with the gradient direction finder as soon as the optimum improves. If the none of the coordinate directions helped, we stop the search. Method MERT MERT MERT PRO `2 MERT (v1: ||w − w0 ||) `2 MERT (v2: D − 1 dimensions) `2 MERT (v3: `1 -renormalized) `0 MERT Starting pt"
D13-1201,D08-1076,0,0.707037,"ed BLEU to orient MERT’s exact line searches. Experiments with up to 3600 features show that these extensions of MERT yield results comparable to PRO, a learner often used with large feature sets. 1 Introduction Minimum Error Rate Training emerged a decade ago (Och, 2003) as a superior training method for small numbers of linear model parameters of machine translation systems, improving over prior work using maximum likelihood criteria (Och and Ney, 2002). This technique quickly rose to prominence, becoming standard in many research and commercial MT systems. Variants operating over lattices (Macherey et al., 2008) or hypergraphs (Kumar et al., 2009) were subsequently developed, with the benefit of reducing the approximation error from n-best lists. The primary advantages of MERT are twofold. It directly optimizes the evaluation metric under consideration (e.g., BLEU) instead of some surrogate loss. In this paper, we seek to preserve the advantages of MERT while addressing its shortcomings in terms of regularization and search. The idea of adding a regularization term to the MERT objective function can be perplexing at first, because the most common regularizers, such as `1 and `2 , are not directly app"
D13-1201,C08-1074,1,0.906037,"Finally, Figure 3 shows our rate of convergence compared to coordinate ascent. Our experimental results with the GBM feature set data are shown in Table 2. Each table is divided into three sections corresponding respectively to MERT (Och, 2003) with Koehn-style coordinate ascent (Koehn, 2004), PRO, and our optimizer featuring both regularization and the gradient-based direction finder. All variants of MERT are initialized with a single starting point, which is either uniform weight or w0 . Instead of providing MERT with additional random starting points as in Moses, we use random walks as in (Moore and Quirk, 2008) to attempt to move out of local optima.8 Since PRO and our optimizer have hyperparameters, we use a held-out set (Dev) for adjusting them. For PRO, we adjust three parameters: a regularization penalty for `2 , the parameter α in the add-α smoothed sentence-level version of BLEU (Lin and Och, 2004), and a parameter for scaling the corpus-level length of the references. The latter scaling parameter is discussed in (He and 8 In the case of the gradient-based direction finder, we also use the following strategy whenever optimization converges to a (possibly local) optimum. We run one round of coo"
D13-1201,C12-1121,0,0.0629013,"on the Tune set. For PRO and regularized MERT, we optimized with different hyperparameters (regularization weight, etc.), and retained for each experimental condition the model that worked best on Dev. The table shows the performance of these retained models. 52.6 52.4 BLEU 52.2 52 51.8 51.6 51.4 51.2 1e-05 expected BLEU gradient coordinate ascent 0.0001 0.001 0.01 0.1 regularization weight 1 10 Figure 4: BLEU score on the Finnish Dev set (GBM) with different values for the 1/2σ 2 regularization weight. To enable comparable results, the other hyperparameter (length) is kept fixed. Deng, 2012; Nakov et al., 2012) and addresses the problem that systems tuned with PRO tend to produce sentences that are too short. On the other hand, regularized MERT only requires one hyperparameter to tune: a regularization penalty for `2 or `0 . However, since PRO optimizes translation length on the Dev dataset and MERT does so using the Tune set, a comparison of the two systems would yield a discrepancy in length that would be undesirable. Therefore, we add another hyperparameter to regularized MERT to tune length in the same manner using the Dev set. Table 2 offers several findings. First, unregularized MERT can achie"
D13-1201,P02-1038,0,0.192281,"integrating them during search. To improve search in large parameter spaces, we also present a new direction finding algorithm that uses the gradient of expected BLEU to orient MERT’s exact line searches. Experiments with up to 3600 features show that these extensions of MERT yield results comparable to PRO, a learner often used with large feature sets. 1 Introduction Minimum Error Rate Training emerged a decade ago (Och, 2003) as a superior training method for small numbers of linear model parameters of machine translation systems, improving over prior work using maximum likelihood criteria (Och and Ney, 2002). This technique quickly rose to prominence, becoming standard in many research and commercial MT systems. Variants operating over lattices (Macherey et al., 2008) or hypergraphs (Kumar et al., 2009) were subsequently developed, with the benefit of reducing the approximation error from n-best lists. The primary advantages of MERT are twofold. It directly optimizes the evaluation metric under consideration (e.g., BLEU) instead of some surrogate loss. In this paper, we seek to preserve the advantages of MERT while addressing its shortcomings in terms of regularization and search. The idea of add"
D13-1201,P03-1021,0,0.145765,"s `2 are inapplicable to MERT due to the scale invariance of its objective function, we turn to two regularizers—`0 and a modification of `2 — and present methods for efficiently integrating them during search. To improve search in large parameter spaces, we also present a new direction finding algorithm that uses the gradient of expected BLEU to orient MERT’s exact line searches. Experiments with up to 3600 features show that these extensions of MERT yield results comparable to PRO, a learner often used with large feature sets. 1 Introduction Minimum Error Rate Training emerged a decade ago (Och, 2003) as a superior training method for small numbers of linear model parameters of machine translation systems, improving over prior work using maximum likelihood criteria (Och and Ney, 2002). This technique quickly rose to prominence, becoming standard in many research and commercial MT systems. Variants operating over lattices (Macherey et al., 2008) or hypergraphs (Kumar et al., 2009) were subsequently developed, with the benefit of reducing the approximation error from n-best lists. The primary advantages of MERT are twofold. It directly optimizes the evaluation metric under consideration (e.g"
D13-1201,2001.mtsummit-papers.68,0,0.129733,"umber of parameters must be optimized, we study the addition of a regularization term to the objective function. One conventional approach is to regularize the objective function with a penalty based on the qP 2 Euclidean norm ||w||2 = i wi , also known as `2 regularization. In the case of MERT, this yields the following objective function:2 ˆ = arg min w w X S s=1 ||w||22 E(rs , ˆ e(fs ; w)) + 2σ 2  (4) 1 This assumes that the sufficient statistics of the metric under consideration are additively decomposable by sentence, which is the case with most popular evaluation metrics such as BLEU (Papineni et al., 2001). 2 The `2 regularizer is often used in conjunction with loglikelihood objectives. The regularization term of Equation 4 could similarly be added to the log of an objective—e.g., log(BLEU) instead of BLEU—but we found that the distinction doesn’t have much of an impact in practice. 1950 1.4 1.2 1 0.8 0.6 0.4 0.2 0 -0.2 -0.4 -0.3 -0.2 -0.1 MERT Max at 0.225 × × 0 0.1 0.2 0.3 0.4 1.4 MERT − `2 1.2 Max at -0.018 × 1 −`2 0.8 0.6 × 0.4 0.2 0 -0.2 -0.4 -0.3 -0.2 -0.1 0 0.1 0.2 0.3 0.4 × 1.4 MERT − `0 1.2 Max at 0 × 1 `0 0.8 0.6 0.4 0.2 0 -0.2 -0.4 -0.3 -0.2 -0.1 0 0.1 0.2 0.3 γ, the step size in the"
D13-1201,W11-2119,0,0.0513815,"larger number of features. Furthermore, this paper complements the analysis by Hopkins and May (2011) of the differences between MERT and optimization with a surrogate loss function. MERT and its extensions have been the target of extensive research (Och, 2003; Macherey et al., 2008; Cer et al., 2008; Moore and Quirk, 2008; Kumar et al., 2009; Galley and Quirk, 2011). More recent work has focused on replacing MERT with a linearly decomposable approximations of the evaluation metric (Smith and Eisner, 2006; Liang et al., 2006; Watanabe et al., 2007; Chiang et al., 2008; Hopkins and May, 2011; Rosti et al., 2011; Gimpel and Smith, 2012; Cherry and Foster, 2012), which generally involve a surrogate loss function incorporating a regularization term such as the `2 -norm. While we are not aware of any previous work adding a penalty on We thank the anonymous reviewers for their helpful comments and suggestions. 10 For example, (Larochelle et al., 2009) presents a pre-trained DNN that outperforms a shallow network, but the performance of the DNN becomes much worse relative to the shallow network once pre-training is turned off. 11 Indeed, in their Table 3, a comparison between HILS and HOLS suggests tuning"
D13-1201,P06-2101,0,0.503336,"l Language Processing, pages 1948–1959, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics regularization, where we apply `2 regularization to scale-senstive linear transforms of the original linear model. In addition, we introduce efficient methods of incorporating regularization in Och (2003)’s exact line searches. For all of these regularizers, our methods let us find the true optimum of the regularized objective function along the line. Finally, we address the issue of searching in a high-dimensional space by using the gradient of expected BLEU (Smith and Eisner, 2006) to find better search directions for our line searches. This direction finder addresses one of the serious concerns raised by Hopkins and May (2011): MERT widely failed to reach the optimum of a synthetic linear objective function. In replicating Hopkins and May’s experiments, we confirm that existing search algorithms for MERT—including coordinate ascent, Powell’s algorithm (Powell, 1964), and random direction sets (Cer et al., 2008)—perform poorly in this experimental condition. However, when using our gradient-based direction finder, MERT has no problem finding the true optimum even in a 1"
D13-1201,P13-2072,1,0.846348,"ector hs,m . By this linear construction, w∗ is guaranteed to be a global optimum.7 The pseudo-BLEU score is normalized for each M -best list, so that the translation with highest model score according to w∗ has a BLEU score of 1, and so that the translation with lowest model score for the sentence gets a BLEU of zero. This normalization has no impact on search, but makes results more interpretable. For our translation experiments, we use multistack phrase-based decoding (Koehn et al., 2007). We report results for two feature sets: non-linear features induced using Gradient Boosting Machines (Toutanova and Ahn, 2013) and sparse lexicalized 7 The objective function remains piecewise constant, and the plateau containing w∗ maps to the optimal value of the function. 1954 reordering features (Cherry, 2013). We exploit these feature sets (GBM and SparseHRM, respectively) in two distinct experimental conditions, which we detail in the two next paragraphs. Both GBM and SparseHRM augment baseline features similar to Moses’: relative frequency and lexicalized phrase translation scores for both translation directions; one or two language model features, depending on the language pair; distortion penalty; word and p"
D13-1201,D07-1080,0,0.35769,"tanding of why standard MERT can fail when handling an increasingly larger number of features. Furthermore, this paper complements the analysis by Hopkins and May (2011) of the differences between MERT and optimization with a surrogate loss function. MERT and its extensions have been the target of extensive research (Och, 2003; Macherey et al., 2008; Cer et al., 2008; Moore and Quirk, 2008; Kumar et al., 2009; Galley and Quirk, 2011). More recent work has focused on replacing MERT with a linearly decomposable approximations of the evaluation metric (Smith and Eisner, 2006; Liang et al., 2006; Watanabe et al., 2007; Chiang et al., 2008; Hopkins and May, 2011; Rosti et al., 2011; Gimpel and Smith, 2012; Cherry and Foster, 2012), which generally involve a surrogate loss function incorporating a regularization term such as the `2 -norm. While we are not aware of any previous work adding a penalty on We thank the anonymous reviewers for their helpful comments and suggestions. 10 For example, (Larochelle et al., 2009) presents a pre-trained DNN that outperforms a shallow network, but the performance of the DNN becomes much worse relative to the shallow network once pre-training is turned off. 11 Indeed, in t"
D13-1201,P02-1040,0,\N,Missing
D17-1263,D16-1025,0,0.041234,"winning 4 of these and tying for first or second in the other 5, according to the official human ranking. Since then, controlled comparisons have used BLEU to show that NMT outperforms strong PBMT systems on 30 translation directions from the United Nations Parallel Corpus (Junczys-Dowmunt et al., 2016a), and on the IWSLT English-Arabic tasks (Durrani et al., 2016). These evaluations indicate that NMT performs better on average than previous technologies, but they do not help us understand what aspects of the translation have improved. Some groups have conducted more detailed error analyses. Bentivogli et al. (2016) carried out a number of experiments on IWSLT 2015 EnglishGerman evaluation data, where they compare machine outputs to professional post-edits in order to automatically detect a number of error categories. Compared to PBMT, NMT required less postediting effort overall, with substantial improvements in lexical, morphological and word order errors. NMT consistently out-performed PBMT, but its performance degraded faster as sentence length increased. Later, Toral and S´anchez-Cartagena (2017) conducted a similar study, examining the outputs of competition-grade systems for the 9 WMT 2016 directi"
D17-1263,N13-1003,1,0.850294,"model of Galley and Manning (2008). We trained an NNJM model (Devlin et al., 2014) on the HMM-aligned training corpus, with input and output vocabulary sizes of 64k and 32k. Words not in the vocabulary were mapped to one of 100 mkcls classes. We trained for 60 epochs of 20k × 128 minibatches, yielding a final dev-set perplexity of 6.88. Our set of log-linear features consisted of forward and backward Kneser-Ney smoothed phrase probabilities and HMM lexical probabilities (4 features); hierarchical reordering probabilities (6); the NNJM probability (1); a set of sparse features as described by Cherry (2013) (10,386); wordcount and distortion penalties (2); and 5-gram language models trained on the French half of the training corpus and the French monolingual corpus (2). Tuning was carried out using batch lattice MIRA (Cherry and Foster, 2012). Decoding used the cube-pruning algorithm of Huang and Chiang (2007), with a distortion limit of 7. We include two phrase-based systems in our comparison: PBMT-1 has data conditions that exactly match those of the NMT system, in that it does not use the language model trained on the French monolingual corpus, while PBMT-2 uses both language models. Neural s"
D17-1263,N12-1047,1,0.308919,"mkcls classes. We trained for 60 epochs of 20k × 128 minibatches, yielding a final dev-set perplexity of 6.88. Our set of log-linear features consisted of forward and backward Kneser-Ney smoothed phrase probabilities and HMM lexical probabilities (4 features); hierarchical reordering probabilities (6); the NNJM probability (1); a set of sparse features as described by Cherry (2013) (10,386); wordcount and distortion penalties (2); and 5-gram language models trained on the French half of the training corpus and the French monolingual corpus (2). Tuning was carried out using batch lattice MIRA (Cherry and Foster, 2012). Decoding used the cube-pruning algorithm of Huang and Chiang (2007), with a distortion limit of 7. We include two phrase-based systems in our comparison: PBMT-1 has data conditions that exactly match those of the NMT system, in that it does not use the language model trained on the French monolingual corpus, while PBMT-2 uses both language models. Neural systems To build our NMT system, we used the Nematus toolkit,5 which implements a single-layer neural sequence-to-sequence architecture with attention (Bahdanau et al., 2015) and gated recurrent units (Cho et al., 2014). We used 512-dimensio"
D17-1263,D14-1179,0,0.0082884,"Missing"
D17-1263,P14-1129,0,0.0261041,"k 4.3 Table 1: Corpus statistics. The WMT12/13 eval sets are used for dev, and the WMT14 eval set is used for test. and corpus organization, but mapping characters to lowercase. Table 1 gives corpus statistics. 4.2 Phrase-based systems To ensure a competitive PBMT baseline, we performed phrase extraction using both IBM4 and HMM alignments with a phrase-length limit of 7; after frequency pruning, the resulting phrase table contained 516M entries. For each extracted phrase pair, we collected statistics for the hierarchical reordering model of Galley and Manning (2008). We trained an NNJM model (Devlin et al., 2014) on the HMM-aligned training corpus, with input and output vocabulary sizes of 64k and 32k. Words not in the vocabulary were mapped to one of 100 mkcls classes. We trained for 60 epochs of 20k × 128 minibatches, yielding a final dev-set perplexity of 6.88. Our set of log-linear features consisted of forward and backward Kneser-Ney smoothed phrase probabilities and HMM lexical probabilities (4 features); hierarchical reordering probabilities (6); the NNJM probability (1); a set of sparse features as described by Cherry (2013) (10,386); wordcount and distortion penalties (2); and 5-gram language"
D17-1263,J94-4004,0,0.130952,"uation is robustness to sparse data. To control for this, when crafting source and reference sentences, we chose words that occurred at least 100 times in our training corpus (section 4.1).1 2487 1 With two exceptions: spilt (58 occurrences), which is The challenging aspect of the test set we are presenting stems from the fact that the source English sentences have been chosen so that their closest French equivalent will be structurally divergent from the source in some crucial way. Translational divergences have been extensively studied in the past—see for example (Vinay and Darbelnet, 1958; Dorr, 1994). We expect the level of difficulty of an MT test set to correlate well with its density in divergence phenomena, which we classify into three main types: morpho-syntactic, lexico-syntactic and purely syntactic divergences. 3.1 Morpho-syntactic divergences In some languages, word morphology (e.g. inflections) carries more grammatical information than in others. When translating a word towards the richer language, there is a need to recover additional grammatically-relevant information from the context of the target language word. Note that we only include in our set cases where the relevant in"
D17-1263,D08-1089,0,0.0139265,"Missing"
D17-1263,P07-1019,0,0.0213604,"ding a final dev-set perplexity of 6.88. Our set of log-linear features consisted of forward and backward Kneser-Ney smoothed phrase probabilities and HMM lexical probabilities (4 features); hierarchical reordering probabilities (6); the NNJM probability (1); a set of sparse features as described by Cherry (2013) (10,386); wordcount and distortion penalties (2); and 5-gram language models trained on the French half of the training corpus and the French monolingual corpus (2). Tuning was carried out using batch lattice MIRA (Cherry and Foster, 2012). Decoding used the cube-pruning algorithm of Huang and Chiang (2007), with a distortion limit of 7. We include two phrase-based systems in our comparison: PBMT-1 has data conditions that exactly match those of the NMT system, in that it does not use the language model trained on the French monolingual corpus, while PBMT-2 uses both language models. Neural systems To build our NMT system, we used the Nematus toolkit,5 which implements a single-layer neural sequence-to-sequence architecture with attention (Bahdanau et al., 2015) and gated recurrent units (Cho et al., 2014). We used 512-dimensional word embeddings with source and target vocabulary sizes of 90k, a"
D17-1263,W16-2316,0,0.0796396,"s received little attention thus far. 2 Related Work A number of recent papers have evaluated NMT using broad performance metrics. The WMT 2016 News Translation Task (Bojar et al., 2016) evaluated submitted systems according to both BLEU and human judgments. NMT systems were submitted to 9 of the 12 translation directions, winning 4 of these and tying for first or second in the other 5, according to the official human ranking. Since then, controlled comparisons have used BLEU to show that NMT outperforms strong PBMT systems on 30 translation directions from the United Nations Parallel Corpus (Junczys-Dowmunt et al., 2016a), and on the IWSLT English-Arabic tasks (Durrani et al., 2016). These evaluations indicate that NMT performs better on average than previous technologies, but they do not help us understand what aspects of the translation have improved. Some groups have conducted more detailed error analyses. Bentivogli et al. (2016) carried out a number of experiments on IWSLT 2015 EnglishGerman evaluation data, where they compare machine outputs to professional post-edits in order to automatically detect a number of error categories. Compared to PBMT, NMT required less postediting effort overall, with subs"
D17-1263,D13-1176,0,0.0157253,"ror analysis. A challenge set consists of a small set of sentences, each hand-designed to probe a system’s capacity to bridge a particular structural divergence between languages. To exemplify this approach, we present an English–French challenge set, and use it to analyze phrase-based and neural systems. The resulting analysis provides not only a more fine-grained picture of the strengths of neural systems, but also insight into which linguistic phenomena remain out of reach. 1 Figure 1: Example challenge set question. Introduction The advent of neural techniques in machine translation (MT) (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014) has led to profound improvements in MT quality. For “easy” language pairs such as English/French or English/Spanish in particular, neural (NMT) systems are much closer to human performance than previous statistical techniques (Wu et al., 2016). This puts pressure on automatic evaluation metrics such as BLEU (Papineni et al., 2002), which exploit surface-matching heuristics that are relatively insensitive to subtle differences. As NMT continues to improve, these metrics will inevitably lose their effectiveness. Another challenge posed by NMT systems i"
D17-1263,C90-2037,0,0.88258,"upted version. Using this technique, they are able to determine that a recently-proposed character-based model improves generalization on unseen words, but at the cost of introducing new grammatical errors. Our approach differs from these studies in a number of ways. First, whereas others have analyzed sentences drawn from an existing bitext, we conduct our study on sentences that are manually constructed to exhibit canonical examples of specific linguistic phenomena. We focus on phenomena that we expect to be more difficult than average, resulting in a particularly challenging MT test suite (King and Falkedal, 1990). These sentences are designed to dive deep into linguistic phenomena of interest, and to provide a much finer-grained analysis of the strengths and weaknesses of existing technologies, including NMT systems. However, this strategy also necessitates that we work on fewer sentences. We leverage the small size of our challenge set to manually evaluate whether the system’s actual output correctly handles our phenomena of interest. Manual evaluation side-steps some of the pitfalls that can come with Sennrich (2016)’s contrastive pairs, as a ranking of two contrastive sentences may not necessarily"
D17-1263,P02-1040,0,0.113514,"trengths of neural systems, but also insight into which linguistic phenomena remain out of reach. 1 Figure 1: Example challenge set question. Introduction The advent of neural techniques in machine translation (MT) (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014) has led to profound improvements in MT quality. For “easy” language pairs such as English/French or English/Spanish in particular, neural (NMT) systems are much closer to human performance than previous statistical techniques (Wu et al., 2016). This puts pressure on automatic evaluation metrics such as BLEU (Papineni et al., 2002), which exploit surface-matching heuristics that are relatively insensitive to subtle differences. As NMT continues to improve, these metrics will inevitably lose their effectiveness. Another challenge posed by NMT systems is their opacity: while it was usually clear which phenomena were ill-handled ∗ Work performed while at NRC. by previous statistical systems—and why—these questions are more difficult to answer for NMT. We propose a new evaluation methodology centered around a challenge set of difficult examples that are designed using expert linguistic knowledge to probe an MT system’s capa"
D17-1263,P16-1162,0,0.00842166,"em, in that it does not use the language model trained on the French monolingual corpus, while PBMT-2 uses both language models. Neural systems To build our NMT system, we used the Nematus toolkit,5 which implements a single-layer neural sequence-to-sequence architecture with attention (Bahdanau et al., 2015) and gated recurrent units (Cho et al., 2014). We used 512-dimensional word embeddings with source and target vocabulary sizes of 90k, and 1024-dimensional state vectors. The model contains 172M parameters. We preprocessed the data using a BPE model learned from source and target corpora (Sennrich et al., 2016). Sentences longer than 50 words were discarded. Training used the Adadelta algorithm (Zeiler, 2012), with a minibatch size of 100 and gradients clipped to 1.0. It ran for 5 epochs, writing a checkpoint model every 30k minibatches. Following Junczys-Dowmunt et al. (2016b), we averaged the parameters from the last 8 checkpoints. To decode, we used the AmuNMT decoder (Junczys-Dowmunt et al., 2016a) with a beam size of 4. While our primary results will focus on the above PBMT and NMT systems, where we can describe replicable configurations, we have also evaluated Google’s production system,6 whic"
D17-1263,E17-1100,0,0.106584,"Missing"
D17-1263,W16-2301,0,\N,Missing
D18-1461,P18-1008,1,0.92754,"t approaches, interest in character-level processing fell off, but has recently been reignited with the work of Lee et al. (2017). They propose a specialized character-level encoder, connected to an unmodified character-level RNN decoder. They address the modeling and efficiency challenges of long character sequences using a convolutional layer, max-pooling over time, and highway layers. We agree with their conclusion that character-level translation is effective, but revisit the question 3.1 Methods Baseline Sequence-to-Sequence Model We adopt a simplified version of the LSTM architecture of Chen et al. (2018) that achieves state-ofthe-art performance on the competitive WMT14 English-French and English-German benchmarks. This incorporates bidirectional LSTM (BiLSTM) layers in the encoder, concatenating the output from forward and backward directions before feeding the next layer. Output from the top encoder layer is projected down to the decoder dimension and used in an additive attention mechanism computed over the bottom decoder layer. The decoder consists of unidirectional layers, all of which use the encoder context vectors computed from attention weights over the bottom layer. For both encoder"
D18-1461,P16-2058,0,0.12014,"Missing"
D18-1461,W17-2627,0,0.0277599,"en an NMT system must handle multiple source and target languages, as in multilingual translation or zero-shot approaches (Johnson et al., 2017). Translating characters instead of word fragments avoids these problems, and gives the system access to all available information about source and target sequences. However, it presents significant modeling and computational challenges. Longer sequences incur linear per-layer cost and quadratic attention cost, and require information to be retained over longer temporal spans. Finer temporal granularity also creates the potential for attention jitter (Gulcehre et al., 2017). Perhaps most significantly, since the meaning of a word is not a compositional function of its characters, the system must learn to memorize many character sequences, a different task from the (mostly) compositional operations it performs at higher levels of linguistic abstraction. In this paper, we show that a standard LSTM sequence-to-sequence model works very well for characters, and given sufficient depth, consistently outperforms identical models operating over word fragments. This result suggests that a productive line of research on character-level models is to seek architectures that"
D18-1461,P18-1007,0,0.0707514,"ing (BPE; Sennrich et al., 2016). Although these are effective, they involve hyperparameters ∗ *Equal contributions that should ideally be tuned for each language pair and corpus, an expensive step that is frequently omitted. Even when properly tuned, the representation of the corpus generated by pipelined external processing is likely to be sub-optimal. For instance, it is easy to find examples of word fragmentations, such as fling → fl + ing, that are linguistically implausible. NMT systems are generally robust to such infelicities—and can be made more robust through subword regularization (Kudo, 2018)—but their effect on performance has not been carefully studied. The problem of finding optimal segmentations becomes more complex when an NMT system must handle multiple source and target languages, as in multilingual translation or zero-shot approaches (Johnson et al., 2017). Translating characters instead of word fragments avoids these problems, and gives the system access to all available information about source and target sequences. However, it presents significant modeling and computational challenges. Longer sequences incur linear per-layer cost and quadratic attention cost, and requir"
D18-1461,Q17-1026,0,0.353472,"of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4295–4305 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics tationally cheaper. One approach to this problem is temporal compression: reducing the number of state vectors required to represent input or output sequences. We evaluate various approaches for performing temporal compression, both according to a fixed schedule; and, more ambitiously, learning compression decisions with a Hierarchical Multiscale architecture (Chung et al., 2017). Following recent work by Lee et al. (2017), we focus on compressing the encoder. Our contributions are as follows: • The first large-scale empirical investigation of the translation quality of standard LSTM sequence-to-sequence architectures operating at the character level, demonstrating improvements in translation quality over word fragments, and quantifying the effect of corpus size and model capacity. • A comparison of techniques to compress character sequences, assessing their ability to trade translation quality for increased speed. • A first attempt to learn how to compress the source sequence during NMT training by using the H"
D18-1461,P16-1100,0,0.0391557,"m their pooling solution for reducing sequence length, along with similar ideas from the speech community (Chan et al., 2016), when devising fixed-schedule reduction strategies in Section 3.3. One of our primary contributions is an extensive invesigation of the efficacy of a typical LSTM-based NMT system when operating at the character-level. The vast majority of existing studies compare a specialized character-level architecture to a distinct word-level one. To the best of our knowledge, only a small number of papers have explored running NMT unmodified on character sequences; these include: Luong and Manning (2016) on WMT’15 English-Czech, Wu et al. (2016) on WMT’14 English-German, and Bradbury et al. (2016) on IWSLT German-English. All report scores that either trail behind or reach parity with word-level models. Only Wu et al. (2016) compare to word fragment models, which they show to outperform characters by a sizeable margin. We revisit the question of character- versus fragment-level NMT here, and reach quite different conclusions. 3 Related Work Early work on modeling characters in NMT focused on solving the out-of-vocabulary and softmax bottleneck problems associated with wordlevel models (Ling e"
D18-1461,W18-6319,0,0.0292093,"Missing"
D18-1461,E17-2060,0,0.0437512,"d as being identical or of roughly the same quality. The remaining 53 exhibited a large variety of differences. Table 4 summarizes the errors that were most easily characterized. BPE and character sys6 Recall that we use batches containing 16,384 tokens— corresponding to a fixed memory budget—for both character and BPE models. Thus character models are slowed not only by having longer sentences, but also by parallelizing across fewer sentences in each batch. 7 The annotating author does not speak German. 8 Our annotator also looked specifically for agreement and negation errors, as studied by Sennrich (2017) for English-toGerman character-level NMT. However, neither system exhibited these error types with sufficient frequency to draw meaningful conclusions. 4300 Figure 1: Test BLEU for character and BPE translation as architectures scale from 1 BiLSTM encoder layer and 2 LSTM decoder layers (1×2+2) to our standard 6×2+8. The y-axis spans 6 BLEU points for each language pair. Error Type Lexical Choice Compounds Proper Names Morphological Other lexical Dropped Content Figure 2: BLEU versus training corpus size in millions of sentence pairs, for the EnFr language-pair. Figure 3: Training time per se"
D18-1461,P16-1162,0,0.805951,"ation time tradeoffs they offer. Within this framework, we also perform the first evaluation for NMT of conditional computation over time, in which the model learns which timesteps can be skipped, rather than having them be dictated by a fixed schedule specified before training begins. 1 Introduction Neural Machine Translation (NMT) has largely replaced the complex pipeline of Phrase-Based MT with a single model that is trained end-to-end. However, NMT systems still typically rely on preand post-processing operations such as tokenization and word fragmentation through byte-pair encoding (BPE; Sennrich et al., 2016). Although these are effective, they involve hyperparameters ∗ *Equal contributions that should ideally be tuned for each language pair and corpus, an expensive step that is frequently omitted. Even when properly tuned, the representation of the corpus generated by pipelined external processing is likely to be sub-optimal. For instance, it is easy to find examples of word fragmentations, such as fling → fl + ing, that are linguistically implausible. NMT systems are generally robust to such infelicities—and can be made more robust through subword regularization (Kudo, 2018)—but their effect on"
D18-1461,P16-1008,0,0.0579145,"Missing"
E06-1019,J93-2003,0,0.048577,"only difference between (Yamada and Knight, 2001) and ITGs; the probability models are also very different. By using a fixed dependency tree inside an ITG, we can revisit the question of whether using a fixed tree is harmful, but in a controlled environment. 2 Alignment Spaces Let an alignment be the entire structure that connects a sentence pair, and let a link be the individual word-to-word connections that make up an alignment. An alignment space determines the set of all possible alignments that can ex145 ist for a given sentence pair. Alignment spaces can emerge from generative stories (Brown et al., 1993), from syntactic notions (Wu, 1997), or they can be imposed to create competition between links (Melamed, 2000). They can generally be described in terms of how links interact. For the sake of describing the size of alignment spaces, we will assume that both sentences have n tokens. The largest alignment space for a sentence 2 pair has 2n possible alignments. This describes the case where each of the n2 potential links can be either on or off with no restrictions. 2.1 Permutation Space A straight-forward way to limit the space of possible alignments is to enforce a one-to-one constraint (Melam"
E06-1019,W02-1039,0,0.0749289,"ted onto the bottom sentence. Zens and Ney (2003) explore the re-orderings allowed by ITGs, and provide a formulation for the number of structures that can be built for a sentence pair of size n. ITGs explore almost all of permutation space when n is small, but their coverage of permutation space falls off quickly for n > 5 (Wu, 1997). 2.3 Dependency Space Dependency space defines the set of all alignments that maintain phrasal cohesion with respect to a dependency tree provided for the English sentence. The space is constrained so that the phrases in the dependency tree always move together. Fox (2002) introduced the notion of headmodifier and modifier-modifier crossings. These occur when a phrase’s image in the Foreign sentence overlaps with the image of its head, or one of its siblings. An alignment with no crossings maintains phrasal cohesion. Figure 2 shows a headmodifier crossing: the image c of a head 2 overlaps with the image (b, d) of 2’s modifier, (3, 4). Lin  A → [AA] |hAAi |e/f  (1)   Figure 2: A phrasal cohesion violation. 1 This is a simplification that ignores null links. The actual number of possible alignments lies between n! and (n + 1)n . and Cherry (2003) used the not"
E06-1019,H91-1026,0,0.0487314,"the ITG and dependency spaces. HD-ITG: The D-ITG method with an added head constraint, as described in Section 3.3. 4.3 Learned objective function The link score flink is usually imperfect, because it is learned from data. Appropriately defined alignment spaces may rule out bad links even if they are assigned high flink values, based on other links in the alignment. We define the following simple link score to test the guidance provided by different alignment spaces: flink (a, E, F ) = φ2 (ei , fj ) − C|i − j| (6) Here, a = (i, j) is a link and φ2 (ei , fj ) returns the φ2 correlation metric (Gale and Church, 1991) 150 Table 1: Results with the learned link score. Method Prec Rec F AER Greedy 78.1 81.4 79.5 20.47 Beam 79.1 82.7 80.7 19.32 Match 79.3 82.7 80.8 19.24 ITG 81.8 83.7 82.6 17.36 Dep 88.8 84.0 86.6 13.40 D-ITG 88.8 84.2 86.7 13.32 HD-ITG 89.2 84.0 86.9 13.15 the only method we have available to search dependency space is also a beam search. The error rates for the three dependency-based methods are similar; no one method provides much more guidance than the other. Enforcing head constraints produces only a small improvement over the D-ITG. Assuming our beam search is approximating a complete s"
E06-1019,N03-2017,1,0.893893,"st advantage of syntactic alignment. We Dekang Lin Google Inc. 1600 Amphitheatre Parkway Mountain View, CA, USA, 94043 lindek@google.com fix an alignment scoring model that works equally well on flat strings as on parse trees, but we vary the space of alignments evaluated with that model. These spaces become smaller as more linguistic guidance is added. We measure the benefits and detriments of these constrained searches. Several of the spaces we investigate draw guidance from a dependency tree for one of the sentences. We will refer to the parsed language as English and the other as Foreign. Lin and Cherry (2003) have shown that adding a dependency-based cohesion constraint to an alignment search can improve alignment quality. Unfortunately, the usefulness of their beam search solution is limited: potential alignments are constructed explicitly, which prevents a perfect search of alignment space and the use of algorithms like EM. However, the cohesion constraint is based on a tree, which should make it amenable to dynamic programming solutions. To enable such techniques, we bring the cohesion constraint inside the ITG framework (Wu, 1997). Zhang and Gildea (2004) compared Yamada and Knight’s (2001) tr"
E06-1019,C94-1079,1,0.699647,"uidance provided by a space, or its capacity to stop an aligner from selecting bad alignments. We also test expressiveness, or how often a space allows an aligner to select the best alignment. In all cases, we report our results in terms of alignment quality, using the standard word alignment error metrics: precision, recall, F-measure and alignment error rate (Och and Ney, 2003). Our test set is the 500 manually aligned sentence pairs created by Franz Och and Hermann Ney (2003). These English-French pairs are drawn from the Canadian Hansards. English dependency trees are supplied by Minipar (Lin, 1994). 4.1 Objective Function In our experiments, we hold all variables constant except for the alignment space being searched, and in the case of imperfect searches, the search method. In particular, all of the methods we test will use the same objective function to select the “best” alignment from their space. Let A be an alignment for an English, Foreign sentence pair, (E, F ). A is represented as a set of links, where each link is a pair of English and Foreign positions, (i, j), that are connected by the alignment. The score of a proposed alignment is: falign (A, E, F ) = X flink (a, E, F ) (5)"
E06-1019,J00-2004,0,0.197774,"g a fixed dependency tree inside an ITG, we can revisit the question of whether using a fixed tree is harmful, but in a controlled environment. 2 Alignment Spaces Let an alignment be the entire structure that connects a sentence pair, and let a link be the individual word-to-word connections that make up an alignment. An alignment space determines the set of all possible alignments that can ex145 ist for a given sentence pair. Alignment spaces can emerge from generative stories (Brown et al., 1993), from syntactic notions (Wu, 1997), or they can be imposed to create competition between links (Melamed, 2000). They can generally be described in terms of how links interact. For the sake of describing the size of alignment spaces, we will assume that both sentences have n tokens. The largest alignment space for a sentence 2 pair has 2n possible alignments. This describes the case where each of the n2 potential links can be either on or off with no restrictions. 2.1 Permutation Space A straight-forward way to limit the space of possible alignments is to enforce a one-to-one constraint (Melamed, 2000). Under such a constraint, each token in the sentence pair can participate in at most one link. Each t"
E06-1019,N03-1021,0,0.0467551,"6 fewer correct links than the D-ITG, each corresponding to a single missed link in a different sentence pair. These misses occur in cases where two modifiers switch position with respect to their head during translation. Surprisingly, there are regularly occurring, systematic constructs that violate the head constraints. An example of such a construct is when an English noun has both adjective and noun modifiers. Cases like “Canadian Wheat Board” are translated as, “Board Canadian of Wheat”, switching the modifiers’ relative positions. These switches correspond to discontinuous constituents (Melamed, 2003) in general bitext parsing. The D-ITG can handle discontinuities by freely grouping constituents to create continuity, but the HD-ITG, with its fixed head and modifiers, cannot. Given that the HD-ITG provides only slightly more guidance than the DITG, we recommend that this type of head information be included only as a soft constraint. 5 Conclusion We have presented two new alignment spaces based on a dependency tree provided for one of the sentences in a sentence pair. We have given grammars to conduct a perfect search of these spaces using an ITG parser. The grammars derive exactly one stru"
E06-1019,J03-1002,0,0.0215508,"Figure 6: Structures allowed by the head constraint. outer modifier Mo between H and the inner modifier Mi . 4 Experiments and Results We compare the alignment spaces described in this paper under two criteria. First we test the guidance provided by a space, or its capacity to stop an aligner from selecting bad alignments. We also test expressiveness, or how often a space allows an aligner to select the best alignment. In all cases, we report our results in terms of alignment quality, using the standard word alignment error metrics: precision, recall, F-measure and alignment error rate (Och and Ney, 2003). Our test set is the 500 manually aligned sentence pairs created by Franz Och and Hermann Ney (2003). These English-French pairs are drawn from the Canadian Hansards. English dependency trees are supplied by Minipar (Lin, 1994). 4.1 Objective Function In our experiments, we hold all variables constant except for the alignment space being searched, and in the case of imperfect searches, the search method. In particular, all of the methods we test will use the same objective function to select the “best” alignment from their space. Let A be an alignment for an English, Foreign sentence pair, (E"
E06-1019,H05-1010,0,0.254959,"(Melamed, 2000). Under such a constraint, each token in the sentence pair can participate in at most one link. Each token in the English sentence picks a token from the Foreign sentence to link to, which is then removed from competition. This allows for n! possible alignments1 , a substan2 tial reduction from 2n . Note that n! is also the number of possible permutations of the n tokens in either one of the two sentences. Permutation space enforces the one-to-one constraint, but allows any reordering of tokens as they are translated. Permutation space methods include weighted maximum matching (Taskar et al., 2005), and approximations to maximum matching like competitive linking (Melamed, 2000). The IBM models (Brown et al., 1993) search a version of permutation space with a one-to-many constraint. 2.2 ITG Space Inversion Transduction Grammars, or ITGs (Wu, 1997) provide an efficient formalism to synchronously parse bitext. This produces a parse tree that decomposes both sentences and also implies a word alignment. ITGs are transduction grammars because their terminal symbols can produce tokens in both the English and Foreign sentences. Inversions occur when the order of constituents is reversed in one"
E06-1019,J97-3002,0,0.895076,"arsed language as English and the other as Foreign. Lin and Cherry (2003) have shown that adding a dependency-based cohesion constraint to an alignment search can improve alignment quality. Unfortunately, the usefulness of their beam search solution is limited: potential alignments are constructed explicitly, which prevents a perfect search of alignment space and the use of algorithms like EM. However, the cohesion constraint is based on a tree, which should make it amenable to dynamic programming solutions. To enable such techniques, we bring the cohesion constraint inside the ITG framework (Wu, 1997). Zhang and Gildea (2004) compared Yamada and Knight’s (2001) tree-to-string alignment model to ITGs. They concluded that methods like ITGs, which create a tree during alignment, perform better than methods with a fixed tree established before alignment begins. However, the use of a fixed tree is not the only difference between (Yamada and Knight, 2001) and ITGs; the probability models are also very different. By using a fixed dependency tree inside an ITG, we can revisit the question of whether using a fixed tree is harmful, but in a controlled environment. 2 Alignment Spaces Let an alignment"
E06-1019,P01-1067,0,0.675498,"s word-level correspondences between parallel sentences. The task originally emerged as an intermediate result of training the IBM translation models (Brown et al., 1993). These models use minimal linguistic intuitions; they essentially treat sentences as flat strings. They remain the dominant method for word alignment (Och and Ney, 2003). There have been several proposals to introduce syntax into word alignment. Some work within the framework of synchronous grammars (Wu, 1997; Melamed, 2003), while others create a generative story that includes a parse tree provided for one of the sentences (Yamada and Knight, 2001). There are three primary reasons to add syntax to word alignment. First, one can incorporate syntactic features, such as grammar productions, into the models that guide the alignment search. Second, movement can be modeled more naturally; when a three-word noun phrase moves during translation, it can be modeled as one movement operation instead of three. Finally, one can restrict the type of movement that is considered, shrinking the number of alignments that are attempted. We investigate this last advantage of syntactic alignment. We Dekang Lin Google Inc. 1600 Amphitheatre Parkway Mountain"
E06-1019,P03-1019,0,0.0391902,"ars in the Foreign sentence. Used as a word aligner, an ITG parser searches a subspace of permutation space: the ITG requires that any movement that occurs during translation be explained by a binary tree with inversions. Alignments that allow no phrases to be formed in bitext are not attempted. This results in two forbidden alignment structures, shown in Figure 1, called “inside-out” transpositions in (Wu, 1997). Note that no pair of contiguous tokens in the top              Figure 1: Forbidden alignments in ITG sentence remain contiguous when projected onto the bottom sentence. Zens and Ney (2003) explore the re-orderings allowed by ITGs, and provide a formulation for the number of structures that can be built for a sentence pair of size n. ITGs explore almost all of permutation space when n is small, but their coverage of permutation space falls off quickly for n > 5 (Wu, 1997). 2.3 Dependency Space Dependency space defines the set of all alignments that maintain phrasal cohesion with respect to a dependency tree provided for the English sentence. The space is constrained so that the phrases in the dependency tree always move together. Fox (2002) introduced the notion of headmodifier"
E06-1019,C04-1060,0,0.788156,"age as English and the other as Foreign. Lin and Cherry (2003) have shown that adding a dependency-based cohesion constraint to an alignment search can improve alignment quality. Unfortunately, the usefulness of their beam search solution is limited: potential alignments are constructed explicitly, which prevents a perfect search of alignment space and the use of algorithms like EM. However, the cohesion constraint is based on a tree, which should make it amenable to dynamic programming solutions. To enable such techniques, we bring the cohesion constraint inside the ITG framework (Wu, 1997). Zhang and Gildea (2004) compared Yamada and Knight’s (2001) tree-to-string alignment model to ITGs. They concluded that methods like ITGs, which create a tree during alignment, perform better than methods with a fixed tree established before alignment begins. However, the use of a fixed tree is not the only difference between (Yamada and Knight, 2001) and ITGs; the probability models are also very different. By using a fixed dependency tree inside an ITG, we can revisit the question of whether using a fixed tree is harmful, but in a controlled environment. 2 Alignment Spaces Let an alignment be the entire structure"
I11-1057,J02-1002,0,0.0308484,"es on transcripts for each slide bullet are compared against the corresponding gold-standard boundaries to calculate offsets measured in number of words, counted after stopwords having been removed, which are then averaged over all boundaries to evaluate model performance. Though one may consider that different bullets may be of different importance, in this paper we do not use any heuristics to judge this and we treat all bullets equally in our evaluation. Note that topic segmentation research often uses metrics such as Pk and WindowDiff (Malioutov and Barzilay, 2006; Beeferman et al., 1999; Pevsner and Hearst, 2002). Our problem here, as an alignment problem, has an exact 1-to-1 correspondence between a gold and automatic boundary, in which we can directly measure the exact offset of each boundary. 2006), which denotes the normalized partition cost of the segment from utterance uj+1 to uk , inclusively. For complexity, since the cohesion model is O(M N 2 ), linearly combining it would not increase the time complexities of the corresponding polynomial alignment models, which are at least O(M N 2 ) by themselves. 7 Experiment Set-up Corpus Our experiment uses a corpus of four 50-minute university lectures"
I11-1057,J97-1003,0,0.0482495,"ed the HieBase model in the remainder of this paper. One major benefit of the deterministic hierarchical alignment models is their time complexity: still quadratic, same as the sequential alignment model discussed above, though models like HieCut can achieve a very competitive perfor6 The Topic-segmentation Model Up to now, we have discussed a variety of alignment models with different model capabilities and time complexities, which, however, consider only similarities between bullets and utterances. Cohesion in text or speech, by itself, often evidenced by the change of lexical distribution (Hearst, 1997), can also indicate topic or subtopic transitions, even among subtle subtopics (Malioutov and Barzilay, 2006). In our problem here, when a lecturer discusses a bullet, the words used are likely to be different from those used in another bullet, suggesting that the spoken documents themselves, when ignoring the alignment model above for the time being, could potentially indicate the semantic boundaries that we are interested in here. Particularly, the cohesion conveyed by the repetition of the words that appear in transcripts but not in slides could be additionally helpful; this is very likely"
I11-1057,W06-1644,0,0.018891,"ame as in (Malioutov et al., 2007), for which we split each lecture into M chunks, the number of bullets. Finally, we obtained a M-by-N bullet-utterance similarity matrix and a N-by-N utterance-utterance matrix to optimize the alignment model and topic-segmentation 8 Experimental Results Alignment Models Table 1 presents the experimental results obtained on the automatic transcripts generated by the ASR models discussed above, with WERs of 0.43 and 0.48, respectively, which are typical for lectures and conference presentations in realistic and less controlled situations (Leeuwis et al., 2003; Hsu and Glass, 2006; Munteanu et al., 2007). The results show that among the four quadratic models, i.e., the first four models in the table, HieCut achieves the best performance. The results also suggest that the improvement of HieCut over SeqBase comes from two aspects. First, the normalized-cut objective used in the graph-partitioning based model seems to outperform that used in the baseline, indicated by the better performance of SeqCut over SeqBase, since both take as input the same, sequentialized bullet sequence and the corresponding transcribed utterances. The DTW-based objective used in SeqBase correspo"
I11-1057,J02-4006,0,0.0196473,"nicity between transcripts and slide trees, which violates some basic properties of the problem that we will discuss. More recently, the work of (Zhu, 2011) proposes a graphpartitioning based model (revisited in Section 4) and shows that the model outperforms a bulletsequentializing model. 2 Related Work Alignment of parallel texts In general, research on finding correspondences between parallel texts pervades both spoken and written language processing, e.g., in training statistical machine translation models, identifying relationship between human-written summaries and their original texts (Jing, 2002), force-aligning speech and transcripts in ASR, and grounding text with database facts (Snyder and Barzilay, 2007; Chen and Mooney, 2008; Liang et al., 2009). Our problem here, however, is distinguished in several major aspects, which need to be considered in our modeling. First, it involves segmentation—alignment is conducted together with the decision of the corresponding segment boundaries on transcripts; in other words, we are not finally concerned with the specific utterances that a bullet is aligned to, but the region of utterances. In such a sense, graph partitioning seems intuitively t"
I11-1057,P09-1011,0,0.036298,", 2011) proposes a graphpartitioning based model (revisited in Section 4) and shows that the model outperforms a bulletsequentializing model. 2 Related Work Alignment of parallel texts In general, research on finding correspondences between parallel texts pervades both spoken and written language processing, e.g., in training statistical machine translation models, identifying relationship between human-written summaries and their original texts (Jing, 2002), force-aligning speech and transcripts in ASR, and grounding text with database facts (Snyder and Barzilay, 2007; Chen and Mooney, 2008; Liang et al., 2009). Our problem here, however, is distinguished in several major aspects, which need to be considered in our modeling. First, it involves segmentation—alignment is conducted together with the decision of the corresponding segment boundaries on transcripts; in other words, we are not finally concerned with the specific utterances that a bullet is aligned to, but the region of utterances. In such a sense, graph partitioning seems intuitively to be more relevant than models optimizing a full-alignment score. Second, unlike a string-to-string alignment task, the problem involves hierarchical tree st"
I11-1057,P06-1004,0,0.239262,"ing of the semantic tree-to-string alignment task. First of all, a basic question is associated with different ways of exploiting the semantic trees when performing alignment, which, as will be studied comprehensively in this paper, results in models of different modeling capabilities and time complexities. Second, all the models discussed above consider only similarities between bullets and transcribed utterances, while similarities among utterances, which directly underline a cohesion model, are generally ignored. We will show in this paper that the stateof-the-art topic-segmentation model (Malioutov and Barzilay, 2006) can be inherently incorporated into the graph-partitioning-based alignment models. Third, the different alignment objectives, e.g., that of the graph-partitioning models versus that of basic DTW-based models, are entangled together with different ways of exploiting the bullet tree structures in (Zhu, 2011). In this paper, we discuss two more quadratic-time models to bridge the gap. Specifically, this paper studies nine different models, with the aim to provide a comprehensive 510 utterance uj and ends at the kth, inclusively. Constrained by the tree structure, the transcript region correspond"
I11-1057,P07-1064,0,0.101654,"elivering, and even automatic transcription were possible. Navigating audio documents is often inherently much more difficult than browsing text. An obvious solution, in relying on human beings’ ability of reading text, is to conduct a speech-to-text conversion through ASR, which in turn raises a new set of problems to be considered. First, the convenience and efficiency of reading transcripts Semantic Structures of Spoken Documents Much previous work, similar to its written-text counterpart, has attempted to find certain flat structures of spoken documents such as topic and slide boundaries (Malioutov et al., 2007; Zhu et al., 2008), which, however, involve no hierarchical structures of a spoken document, thought as will be shown in this paper, topic-segmentation models can be considered in our alignment task. Research has also resorted to other multimedia channels, e.g., video (Fan et al., 2006), to detect slide 509 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 509–517, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP transitions. This type of approaches, however, are unlikely to recover semantic structures more detailed than slide boundaries. und"
I11-1057,N10-1006,0,0.0128394,"ent Models Xiaodan Zhu & Colin Cherry Institute for Information Technology National Research Council Canada Gerald Penn Department of Computer Science University of Toronto {Xiaodan.Zhu,Colin.Cherry}@nrc-cnrc.gc.ca gpenn@cs.toronto.edu Abstract are affected by errors produced in transcription channels, though if the goal is only to browse the most salient parts, recognition errors in excerpts can be reduced by considering ASR confidence (Xie and Liu, 2010; Hori and Furui, 2003; Zechner and Waibel, 2000) and the quality of excerpts can be improved from various perspectives (Zhang et al., 2010; Xie and Liu, 2010; Zhu et al., 2009; Murray, 2008; Zhu and Penn, 2006; Maskey and Hirschberg, 2005). Even if transcription quality were not a problem, browsing lengthy transcripts is not straightforward, since, as mentioned above, indicative browsing structures are barely manually created for and aligned with spoken documents. Ideally, such semantic structures should be inferred directly from the spoken documents themselves, but this is known to be difficult even for written texts, which are often more linguistically well-formed and less noisy than automatically transcribed text. This paper studies a less ambi"
I11-1057,A00-2025,0,0.0218271,"Missing"
I11-1057,P09-1062,1,0.850006,"Zhu & Colin Cherry Institute for Information Technology National Research Council Canada Gerald Penn Department of Computer Science University of Toronto {Xiaodan.Zhu,Colin.Cherry}@nrc-cnrc.gc.ca gpenn@cs.toronto.edu Abstract are affected by errors produced in transcription channels, though if the goal is only to browse the most salient parts, recognition errors in excerpts can be reduced by considering ASR confidence (Xie and Liu, 2010; Hori and Furui, 2003; Zechner and Waibel, 2000) and the quality of excerpts can be improved from various perspectives (Zhang et al., 2010; Xie and Liu, 2010; Zhu et al., 2009; Murray, 2008; Zhu and Penn, 2006; Maskey and Hirschberg, 2005). Even if transcription quality were not a problem, browsing lengthy transcripts is not straightforward, since, as mentioned above, indicative browsing structures are barely manually created for and aligned with spoken documents. Ideally, such semantic structures should be inferred directly from the spoken documents themselves, but this is known to be difficult even for written texts, which are often more linguistically well-formed and less noisy than automatically transcribed text. This paper studies a less ambitious problem: we"
I11-1057,C10-2177,1,0.418791,"Thailand, November 8 – 13, 2011. 2011 AFNLP transitions. This type of approaches, however, are unlikely to recover semantic structures more detailed than slide boundaries. understanding of the questions discussed above. In the remainder of the paper, we will first review the related work (Section 2) and more formally describe our problem (Section 3). Then we revisit the graph-partitioning alignment model (Section 4), before present all the alignment models we will study (Section 5). We describe our experiment setup in Section 7 and results in Section 8, and draw our conclusions in Section 9. Zhu et al. (2010) investigate the problem of aligning electronic slides with lecture transcripts by first sequentializing bullet trees on slides with a pre-order walk before conducting alignment, through which the problem is reduced to a string-to-string alignment problem and conventional methods such as DTW (dynamic time warping) based alignment can then be directly applicable. A pre-order walk of bullet tree on slides is actually a natural choice, since speakers of presentations often follow such an order to develop their talks, i.e., they discuss a parent bullet first and then each of its children in sequen"
I11-1057,P07-1069,0,\N,Missing
J10-4010,J93-2003,0,0.0327719,"Missing"
J10-4010,N04-1021,0,0.0633801,"Missing"
J10-4010,C96-2141,0,0.350233,"Missing"
L16-1623,W11-1701,0,0.0742288,"Missing"
L16-1623,W14-2107,0,0.042192,"Missing"
L16-1623,W12-3810,0,0.0175543,"understand how stance can be detected from tweets that do not explicitly mention the target of interest. Stance detection can be thought of as a textual inference or textual entailment task, where the goal is to determine whether the favorability of the target is entailed by the tweet. Thus the dataset can be used for developing textual inference engines and open domain reasoning. 6. Related Work Past work on stance detection includes that by Somasundaran and Wiebe (2010), Anand et al. (2011), Faulkner (2014), Rajadesingan and Liu (2014), Djemili et al. (2014), ˇ Boltuzic and Snajder (2014), Conrad et al. (2012), Hasan and Ng (2013a), Djemili et al. (2014), Sridhar et al. (2014), and Sobhani et al. (2015). In one of the few works on stance detection in tweets, Rajadesingan and Liu (2014) determine stance at user-level based on the assumption that if several users retweet one pair of tweets about a controversial topic, it is likely that they support the same side of a debate. Djemili et al. (2014) use a set of rules based on the syntax and discourse structure of the tweet to identify tweets that contain ideological stance. However, none of these works attempts to determine stance from a single tweet."
L16-1623,P13-2142,0,0.00626643,"can be detected from tweets that do not explicitly mention the target of interest. Stance detection can be thought of as a textual inference or textual entailment task, where the goal is to determine whether the favorability of the target is entailed by the tweet. Thus the dataset can be used for developing textual inference engines and open domain reasoning. 6. Related Work Past work on stance detection includes that by Somasundaran and Wiebe (2010), Anand et al. (2011), Faulkner (2014), Rajadesingan and Liu (2014), Djemili et al. (2014), ˇ Boltuzic and Snajder (2014), Conrad et al. (2012), Hasan and Ng (2013a), Djemili et al. (2014), Sridhar et al. (2014), and Sobhani et al. (2015). In one of the few works on stance detection in tweets, Rajadesingan and Liu (2014) determine stance at user-level based on the assumption that if several users retweet one pair of tweets about a controversial topic, it is likely that they support the same side of a debate. Djemili et al. (2014) use a set of rules based on the syntax and discourse structure of the tweet to identify tweets that contain ideological stance. However, none of these works attempts to determine stance from a single tweet. There is a vast amou"
L16-1623,I13-1191,0,0.0256128,"can be detected from tweets that do not explicitly mention the target of interest. Stance detection can be thought of as a textual inference or textual entailment task, where the goal is to determine whether the favorability of the target is entailed by the tweet. Thus the dataset can be used for developing textual inference engines and open domain reasoning. 6. Related Work Past work on stance detection includes that by Somasundaran and Wiebe (2010), Anand et al. (2011), Faulkner (2014), Rajadesingan and Liu (2014), Djemili et al. (2014), ˇ Boltuzic and Snajder (2014), Conrad et al. (2012), Hasan and Ng (2013a), Djemili et al. (2014), Sridhar et al. (2014), and Sobhani et al. (2015). In one of the few works on stance detection in tweets, Rajadesingan and Liu (2014) determine stance at user-level based on the assumption that if several users retweet one pair of tweets about a controversial topic, it is likely that they support the same side of a debate. Djemili et al. (2014) use a set of rules based on the syntax and discourse structure of the tweet to identify tweets that contain ideological stance. However, none of these works attempts to determine stance from a single tweet. There is a vast amou"
L16-1623,S14-2076,1,0.86144,"ce from a single tweet. There is a vast amount of work in sentiment analysis of tweets, and we refer the reader to surveys (Pang and Lee, 2008; Liu and Zhang, 2012; Mohammad, 2015) and proceedings of recent shared task competitions (Wilson et al., 2013; Mohammad et al., 2013; Rosenthal et al., 2015). Closely-related is the area of aspect based sentiment analysis (ABSA), where the goal is to determine sentiment towards aspects of a product such as speed of processor and screen resolution of a cell phone. We refer the reader to SemEval proceedings for related work on ABSA (Pontiki et al., 2015; Kiritchenko et al., 2014; Pontiki et al., 2014). 7. Summary We presented a new dataset of 4,870 tweet–target pairs annotated for stance of the tweeter towards the target. This dataset, which we refer to as the Stance Dataset, has instances corresponding to six pre-chosen targets of interest: ‘Atheism’, ‘Climate Change is a Real Concern’, ‘Feminist Movement’, ‘Hillary Clinton’, ‘Legalization of Abortion’, and ‘Donald Trump’. The annotations were performed by crowdsourcing. Several techniques were employed to encourage high-quality annotations and to identify and discard poor annotations. We analyzed the dataset to sho"
L16-1623,S13-2053,1,0.0870714,"tion that if several users retweet one pair of tweets about a controversial topic, it is likely that they support the same side of a debate. Djemili et al. (2014) use a set of rules based on the syntax and discourse structure of the tweet to identify tweets that contain ideological stance. However, none of these works attempts to determine stance from a single tweet. There is a vast amount of work in sentiment analysis of tweets, and we refer the reader to surveys (Pang and Lee, 2008; Liu and Zhang, 2012; Mohammad, 2015) and proceedings of recent shared task competitions (Wilson et al., 2013; Mohammad et al., 2013; Rosenthal et al., 2015). Closely-related is the area of aspect based sentiment analysis (ABSA), where the goal is to determine sentiment towards aspects of a product such as speed of processor and screen resolution of a cell phone. We refer the reader to SemEval proceedings for related work on ABSA (Pontiki et al., 2015; Kiritchenko et al., 2014; Pontiki et al., 2014). 7. Summary We presented a new dataset of 4,870 tweet–target pairs annotated for stance of the tweeter towards the target. This dataset, which we refer to as the Stance Dataset, has instances corresponding to six pre-chosen tar"
L16-1623,S16-1003,1,0.853828,"at one can be in favor of Jeb Bush and yet also be in favor of Donald Trump. However, the goal in stance detection, is to determine which is more probable: that the author is in favor of, against, or neutral towards the target. In this case, most annotators will agree that the tweeter is likely against Donald Trump. To aid further analysis, the tweets in the Stance Dataset are also annotated for whether target of interest is the target of opinion in the tweet. Partitions of the Stance Dataset were used to create training and test sets for the SemEval-2016 Task 6: Detecting Stance from Tweets (Mohammad et al., 2016a).1 Mohammad et al. (2016b) subsequently annotated the Stance Dataset for sentiment and quantitatively explored the relationship between stance and sentiment. The rest of the paper is structured as follows. In Section 2, we describe how we created the Stance Dataset. Section 3 presents a detailed analysis of the stance annotations. Section 4 presents an online interactive visualization of the Stance Dataset. Section 5 discusses how the dataset can be (and is being) used by the research community. Finally we present concluding remarks in Section 6. All of the data created as part of this proje"
L16-1623,C10-2100,0,0.0609499,"Missing"
L16-1623,S14-2004,0,0.174273,"Missing"
L16-1623,S15-2082,0,0.0208171,"mpts to determine stance from a single tweet. There is a vast amount of work in sentiment analysis of tweets, and we refer the reader to surveys (Pang and Lee, 2008; Liu and Zhang, 2012; Mohammad, 2015) and proceedings of recent shared task competitions (Wilson et al., 2013; Mohammad et al., 2013; Rosenthal et al., 2015). Closely-related is the area of aspect based sentiment analysis (ABSA), where the goal is to determine sentiment towards aspects of a product such as speed of processor and screen resolution of a cell phone. We refer the reader to SemEval proceedings for related work on ABSA (Pontiki et al., 2015; Kiritchenko et al., 2014; Pontiki et al., 2014). 7. Summary We presented a new dataset of 4,870 tweet–target pairs annotated for stance of the tweeter towards the target. This dataset, which we refer to as the Stance Dataset, has instances corresponding to six pre-chosen targets of interest: ‘Atheism’, ‘Climate Change is a Real Concern’, ‘Feminist Movement’, ‘Hillary Clinton’, ‘Legalization of Abortion’, and ‘Donald Trump’. The annotations were performed by crowdsourcing. Several techniques were employed to encourage high-quality annotations and to identify and discard poor annotations. We a"
L16-1623,S15-2078,1,0.0922389,"ers retweet one pair of tweets about a controversial topic, it is likely that they support the same side of a debate. Djemili et al. (2014) use a set of rules based on the syntax and discourse structure of the tweet to identify tweets that contain ideological stance. However, none of these works attempts to determine stance from a single tweet. There is a vast amount of work in sentiment analysis of tweets, and we refer the reader to surveys (Pang and Lee, 2008; Liu and Zhang, 2012; Mohammad, 2015) and proceedings of recent shared task competitions (Wilson et al., 2013; Mohammad et al., 2013; Rosenthal et al., 2015). Closely-related is the area of aspect based sentiment analysis (ABSA), where the goal is to determine sentiment towards aspects of a product such as speed of processor and screen resolution of a cell phone. We refer the reader to SemEval proceedings for related work on ABSA (Pontiki et al., 2015; Kiritchenko et al., 2014; Pontiki et al., 2014). 7. Summary We presented a new dataset of 4,870 tweet–target pairs annotated for stance of the tweeter towards the target. This dataset, which we refer to as the Stance Dataset, has instances corresponding to six pre-chosen targets of interest: ‘Atheis"
L16-1623,W15-0509,1,0.0660808,"f interest. Stance detection can be thought of as a textual inference or textual entailment task, where the goal is to determine whether the favorability of the target is entailed by the tweet. Thus the dataset can be used for developing textual inference engines and open domain reasoning. 6. Related Work Past work on stance detection includes that by Somasundaran and Wiebe (2010), Anand et al. (2011), Faulkner (2014), Rajadesingan and Liu (2014), Djemili et al. (2014), ˇ Boltuzic and Snajder (2014), Conrad et al. (2012), Hasan and Ng (2013a), Djemili et al. (2014), Sridhar et al. (2014), and Sobhani et al. (2015). In one of the few works on stance detection in tweets, Rajadesingan and Liu (2014) determine stance at user-level based on the assumption that if several users retweet one pair of tweets about a controversial topic, it is likely that they support the same side of a debate. Djemili et al. (2014) use a set of rules based on the syntax and discourse structure of the tweet to identify tweets that contain ideological stance. However, none of these works attempts to determine stance from a single tweet. There is a vast amount of work in sentiment analysis of tweets, and we refer the reader to surv"
L16-1623,P09-1026,0,0.0238553,"Missing"
L16-1623,W10-0214,0,0.0476215,"Missing"
L16-1623,W14-2715,0,0.0421924,"icitly mention the target of interest. Stance detection can be thought of as a textual inference or textual entailment task, where the goal is to determine whether the favorability of the target is entailed by the tweet. Thus the dataset can be used for developing textual inference engines and open domain reasoning. 6. Related Work Past work on stance detection includes that by Somasundaran and Wiebe (2010), Anand et al. (2011), Faulkner (2014), Rajadesingan and Liu (2014), Djemili et al. (2014), ˇ Boltuzic and Snajder (2014), Conrad et al. (2012), Hasan and Ng (2013a), Djemili et al. (2014), Sridhar et al. (2014), and Sobhani et al. (2015). In one of the few works on stance detection in tweets, Rajadesingan and Liu (2014) determine stance at user-level based on the assumption that if several users retweet one pair of tweets about a controversial topic, it is likely that they support the same side of a debate. Djemili et al. (2014) use a set of rules based on the syntax and discourse structure of the tweet to identify tweets that contain ideological stance. However, none of these works attempts to determine stance from a single tweet. There is a vast amount of work in sentiment analysis of tweets, and"
L16-1623,W06-1639,0,0.0852829,"Missing"
L16-1623,N12-1072,0,0.0225157,"Missing"
L16-1623,S13-2052,0,0.0239726,"l based on the assumption that if several users retweet one pair of tweets about a controversial topic, it is likely that they support the same side of a debate. Djemili et al. (2014) use a set of rules based on the syntax and discourse structure of the tweet to identify tweets that contain ideological stance. However, none of these works attempts to determine stance from a single tweet. There is a vast amount of work in sentiment analysis of tweets, and we refer the reader to surveys (Pang and Lee, 2008; Liu and Zhang, 2012; Mohammad, 2015) and proceedings of recent shared task competitions (Wilson et al., 2013; Mohammad et al., 2013; Rosenthal et al., 2015). Closely-related is the area of aspect based sentiment analysis (ABSA), where the goal is to determine sentiment towards aspects of a product such as speed of processor and screen resolution of a cell phone. We refer the reader to SemEval proceedings for related work on ABSA (Pontiki et al., 2015; Kiritchenko et al., 2014; Pontiki et al., 2014). 7. Summary We presented a new dataset of 4,870 tweet–target pairs annotated for stance of the tweeter towards the target. This dataset, which we refer to as the Stance Dataset, has instances correspondin"
N03-2017,J00-1004,0,0.0301127,"Missing"
N03-2017,P03-1012,1,0.835917,"rithm uses a best-first search (with fixed beam width and agenda size) to find an alignment that maximizes P (A|E, F ). A state in this search space is a partial alignment. A transition is defined as the addition of a single link to the current state. The algorithm computes P (A|E, F ) based on statistics obtained from a word-aligned corpus. We construct the initial corpus with a system that is similar to the φ2 method. The algorithm then re-aligns the corpus and trains again for three iterations. We will refer to this as the P (A|E, F ) method. The details of this algorithm are described in (Cherry and Lin, 2003). We trained our alignment programs with the same 50K pairs of sentences as (Och and Ney, 2000) and tested it on the same 500 manually aligned sentences. Both the training and testing sentences are from the Hansard corpus. We parsed the training and testing corpora with Minipar.1 We adopted the evaluation methodology in (Och and Ney, 2000), which defines three metrics: precision, recall and alignment error rate (AER). Table 1 shows the results of our experiments. The first four rows correspond to the methods described above. As a reference point, we also provide the results reported in (Och an"
N03-2017,dorr-etal-2002-duster,0,0.0312102,"Missing"
N03-2017,W02-1039,0,0.557949,"alignment can be represented as a binary relation A in [1, l] × [1, m]. A pair (i, j) is in A if ei and fj are a translation (or part of a translation) of each other. We call such pairs links. In Figure 2, the links in the alignment are represented by dashed lines. comp det subj obj subj det pre aux det The reboot causes the host to discover all the devices 1 1 2 Suite à after to 2 3 3 4 4 5 6 5 6 7 7 8 9 10 8 9 10 11 la réinitialisation , l&apos; hôte repère tous les périphériques the reboot the host locate all the peripherals Figure 2: An example pair of aligned sentence The cohesion constraint (Fox, 2002) uses the dependency tree TE (Mel’ˇcuk, 1987) of the English sentence to restrict possible link combinations. Let TE (ei ) be the subtree of TE rooted at ei . The phrase span of ei , spanP (ei , TE , A), is the image of the English phrase headed by ei in F given a (partial) alignment A. More precisely, spanP (ei , TE , A) = [k1 , k2 ], where k1 = min{j|(u, j) ∈ A, eu ∈ TE (ei )} k2 = max{j|(u, j) ∈ A, eu ∈ TE (ei )} The head span is the image of ei itself. We define spanH (ei , TE , A) = [k1 , k2 ], where k1 = min{j|(i, j) ∈ A} k2 = max{j|(i, j) ∈ A} In Figure 2, the phrase span of the node di"
N03-2017,H91-1026,0,0.407521,"algorithms take as input an English-French sentence pair and the dependency tree of the English sentence. Both algorithms build an alignment by adding one link at a time. We implement two versions of each algorithm: one with the cohesion constraint and one without. We will describe the versions without cohesion constraint below. For the versions with cohesion constraint, it is understood that each new link must also pass the test described in Section 2. The first algorithm is similar to Competitive Linking (Melamed, 1997). We use a sentence-aligned corpus to compute the φ2 correlation metric (Gale and Church, 1991) between all English-French word pairs. For a given sentence pair, we begin with an empty alignment. We then add links in the order of their φ2 scores so that each word participates in at most one link. We will refer to this as the φ2 method. The second algorithm uses a best-first search (with fixed beam width and agenda size) to find an alignment that maximizes P (A|E, F ). A state in this search space is a partial alignment. A transition is defined as the addition of a single link to the current state. The algorithm computes P (A|E, F ) based on statistics obtained from a word-aligned corpus"
N03-2017,P97-1063,0,0.0280054,"lity of the cohesion constraint, we incorporated it into two alignment algorithms. The algorithms take as input an English-French sentence pair and the dependency tree of the English sentence. Both algorithms build an alignment by adding one link at a time. We implement two versions of each algorithm: one with the cohesion constraint and one without. We will describe the versions without cohesion constraint below. For the versions with cohesion constraint, it is understood that each new link must also pass the test described in Section 2. The first algorithm is similar to Competitive Linking (Melamed, 1997). We use a sentence-aligned corpus to compute the φ2 correlation metric (Gale and Church, 1991) between all English-French word pairs. For a given sentence pair, we begin with an empty alignment. We then add links in the order of their φ2 scores so that each word participates in at most one link. We will refer to this as the φ2 method. The second algorithm uses a best-first search (with fixed beam width and agenda size) to find an alignment that maximizes P (A|E, F ). A state in this search space is a partial alignment. A transition is defined as the addition of a single link to the current st"
N03-2017,P00-1056,0,0.0693053,"aximizes P (A|E, F ). A state in this search space is a partial alignment. A transition is defined as the addition of a single link to the current state. The algorithm computes P (A|E, F ) based on statistics obtained from a word-aligned corpus. We construct the initial corpus with a system that is similar to the φ2 method. The algorithm then re-aligns the corpus and trains again for three iterations. We will refer to this as the P (A|E, F ) method. The details of this algorithm are described in (Cherry and Lin, 2003). We trained our alignment programs with the same 50K pairs of sentences as (Och and Ney, 2000) and tested it on the same 500 manually aligned sentences. Both the training and testing sentences are from the Hansard corpus. We parsed the training and testing corpora with Minipar.1 We adopted the evaluation methodology in (Och and Ney, 2000), which defines three metrics: precision, recall and alignment error rate (AER). Table 1 shows the results of our experiments. The first four rows correspond to the methods described above. As a reference point, we also provide the results reported in (Och and Ney, 2000). They implemented IBM Model 4 by bootstrapping from an HMM model. The rows F→E 1 a"
N03-2017,J97-3002,0,0.458896,"Missing"
N03-2017,P01-1067,0,0.297671,"Missing"
N09-1024,W06-1655,0,0.0176511,"overlapping contextual features previously have been used in directed generative models (in the form of Markov models) for unsupervised morphological segmentation (Creutz and Lagus, 2007) or word segmentation (Goldwater et al., 2007). In terms of feature sets, our model is most closely related to the constituent-context model proposed by Klein and Manning (2001) for grammar induction. If we exclude the priors, our model can also be seen as a semi-Markov conditional random field (CRF) model (Sarawagi and Cohen, 2004). Semi-Markov CRFs previously have been used for supervised word segmentation (Andrew, 2006), but not for unsupervised morphological segmentation. Unsupervised learning with log-linear models has received little attention in the past. Two notable exceptions are Smith & Eisner (2005) for POS tagging, and Poon & Domingos (2008) for coreference resolution. Learning with log-linear models requires computing the normalization constant (a.k.a. the partition function) Z. This is already challenging in supervised learning. In unsupervised learning, the difficulty is further compounded by the absence of supervised labels. Smith & Eisner (2005) proposed contrastive estimation, which uses a sma"
N09-1024,N07-1020,0,0.25384,"soft Research. NLP applications, including machine translation, speech recognition and question answering. Past approaches include rule-based morphological analyzers (Buckwalter, 2004) and supervised learning (Habash and Rambow, 2005). While successful, these require deep language expertise and a long and laborious process in system building or labeling. Unsupervised approaches are attractive due to the the availability of large quantities of unlabeled text, and unsupervised morphological segmentation has been extensively studied for a number of languages (Brent et al., 1995; Goldsmith, 2001; Dasgupta and Ng, 2007; Creutz and Lagus, 2007). The lack of supervised labels makes it even more important to leverage rich features and global dependencies. However, existing systems use directed generative models (Creutz and Lagus, 2007; Snyder and Barzilay, 2008b), making it difficult to extend them with arbitrary overlapping dependencies that are potentially helpful to segmentation. In this paper, we present the first log-linear model for unsupervised morphological segmentation. Our model incorporates simple priors inspired by the minimum description length (MDL) principle, as well as overlapping features such"
N09-1024,P07-1116,0,0.0818885,"resentations of the lexicon and corpus (Brent et al., 1995; Goldsmith, 2001; Creutz and Lagus, 2007). Other approaches use statistics on morpheme context, such as conditional entropy between adjacent n-grams, to identify morpheme candidates (Harris, 1955; Keshava and Pitler, 2006). In this paper, we incorporate both intuitions into a simple yet powerful model, and show that each contributes significantly to performance. Unsupervised morphological segmentation systems also differ from the engineering perspective. Some adopt a pipeline approach (Schone and Jurafsky, 2001; Dasgupta and Ng, 2007; Demberg, 2007), which works by first extracting candidate affixes and stems, and then segmenting the words based on the candidates. Others model segmentation using a joint probabilistic distribution (Goldwater et al., 2006; Creutz and Lagus, 2007; Snyder and 210 Barzilay, 2008b); they learn the model parameters from unlabeled data and produce the most probable segmentation as the final output. The latter approach is arguably more appealing from the modeling standpoint and avoids error propagation along the pipeline. However, most existing systems use directed generative models; Creutz & Lagus (2007) used an"
N09-1024,J01-2001,0,0.879538,"ternship at Microsoft Research. NLP applications, including machine translation, speech recognition and question answering. Past approaches include rule-based morphological analyzers (Buckwalter, 2004) and supervised learning (Habash and Rambow, 2005). While successful, these require deep language expertise and a long and laborious process in system building or labeling. Unsupervised approaches are attractive due to the the availability of large quantities of unlabeled text, and unsupervised morphological segmentation has been extensively studied for a number of languages (Brent et al., 1995; Goldsmith, 2001; Dasgupta and Ng, 2007; Creutz and Lagus, 2007). The lack of supervised labels makes it even more important to leverage rich features and global dependencies. However, existing systems use directed generative models (Creutz and Lagus, 2007; Snyder and Barzilay, 2008b), making it difficult to extend them with arbitrary overlapping dependencies that are potentially helpful to segmentation. In this paper, we present the first log-linear model for unsupervised morphological segmentation. Our model incorporates simple priors inspired by the minimum description length (MDL) principle, as well as ov"
N09-1024,P05-1071,0,0.0464249,"when computing the expected counts, we initialize the sampler with the most probable segmentation output by annealing. 7 Experiments We evaluated our system on two datasets. Our main evaluation is on a multi-lingual dataset constructed by Snyder & Barzilay (2008a; 2008b). It consists of 6192 short parallel phrases in Hebrew, Arabic, Aramaic (a dialect of Arabic), and English. The parallel phrases were extracted from the Hebrew Bible and its translations via word alignment and postprocessing. For Arabic, the gold segmentation was obtained using a highly accurate Arabic morphological analyzer (Habash and Rambow, 2005); for Hebrew, from a Bible edition distributed by Westminster Hebrew Institute (Groves and Lowery, 2006). There is no gold segmentation for English and Aramaic. Like Snyder & Barzilay, we evaluate on the Arabic and Hebrew portions only; unlike their approach, our system does not use any bilingual information. We refer to this dataset as S&B . We also report our results on the Arabic Penn Treebank (ATB), which provides gold segmentations for an 214 Arabic corpus with about 120,000 Arabic words. As in previous work, we report recall, precision, and F1 over segmentation points. We used 500 phrase"
N09-1024,D08-1068,1,0.212887,", 2007). In terms of feature sets, our model is most closely related to the constituent-context model proposed by Klein and Manning (2001) for grammar induction. If we exclude the priors, our model can also be seen as a semi-Markov conditional random field (CRF) model (Sarawagi and Cohen, 2004). Semi-Markov CRFs previously have been used for supervised word segmentation (Andrew, 2006), but not for unsupervised morphological segmentation. Unsupervised learning with log-linear models has received little attention in the past. Two notable exceptions are Smith & Eisner (2005) for POS tagging, and Poon & Domingos (2008) for coreference resolution. Learning with log-linear models requires computing the normalization constant (a.k.a. the partition function) Z. This is already challenging in supervised learning. In unsupervised learning, the difficulty is further compounded by the absence of supervised labels. Smith & Eisner (2005) proposed contrastive estimation, which uses a small neighborhood to compute Z. The neighborhood is carefully designed so that it not only makes computation easier but also offers sufficient contrastive information to aid unsupervised learning. Poon & Domingos (2008), on the other han"
N09-1024,N01-1024,0,0.522611,"z and Lagus, 2007). Our system can be readily applied to supervised and semi-supervised learning. Using a fraction of the labeled data, it already outperforms Snyder & Barzilay’s supervised results (2008a), which further demonstrates the benefit of using a log-linear model. 2 Related Work There is a large body of work on the unsupervised learning of morphology. In addition to morphological segmentation, there has been work on unsupervised morpheme analysis, where one needs to determine features of word forms (Kurimo et al., 2007) or identify words with the same lemma by modeling stem changes (Schone and Jurafsky, 2001; Goldsmith, 2001). However, we focus our review specifically on morphological segmentation. In the absence of labels, unsupervised learning must incorporate a strong learning bias that reflects prior knowledge about the task. In morphological segmentation, an often-used bias is the minimum description length (MDL) principle, which favors compact representations of the lexicon and corpus (Brent et al., 1995; Goldsmith, 2001; Creutz and Lagus, 2007). Other approaches use statistics on morpheme context, such as conditional entropy between adjacent n-grams, to identify morpheme candidates (Harris"
N09-1024,P05-1044,0,0.807103,"007) or word segmentation (Goldwater et al., 2007). In terms of feature sets, our model is most closely related to the constituent-context model proposed by Klein and Manning (2001) for grammar induction. If we exclude the priors, our model can also be seen as a semi-Markov conditional random field (CRF) model (Sarawagi and Cohen, 2004). Semi-Markov CRFs previously have been used for supervised word segmentation (Andrew, 2006), but not for unsupervised morphological segmentation. Unsupervised learning with log-linear models has received little attention in the past. Two notable exceptions are Smith & Eisner (2005) for POS tagging, and Poon & Domingos (2008) for coreference resolution. Learning with log-linear models requires computing the normalization constant (a.k.a. the partition function) Z. This is already challenging in supervised learning. In unsupervised learning, the difficulty is further compounded by the absence of supervised labels. Smith & Eisner (2005) proposed contrastive estimation, which uses a small neighborhood to compute Z. The neighborhood is carefully designed so that it not only makes computation easier but also offers sufficient contrastive information to aid unsupervised learni"
N09-1024,P08-1084,0,0.766837,"e successful, these require deep language expertise and a long and laborious process in system building or labeling. Unsupervised approaches are attractive due to the the availability of large quantities of unlabeled text, and unsupervised morphological segmentation has been extensively studied for a number of languages (Brent et al., 1995; Goldsmith, 2001; Dasgupta and Ng, 2007; Creutz and Lagus, 2007). The lack of supervised labels makes it even more important to leverage rich features and global dependencies. However, existing systems use directed generative models (Creutz and Lagus, 2007; Snyder and Barzilay, 2008b), making it difficult to extend them with arbitrary overlapping dependencies that are potentially helpful to segmentation. In this paper, we present the first log-linear model for unsupervised morphological segmentation. Our model incorporates simple priors inspired by the minimum description length (MDL) principle, as well as overlapping features such as morphemes and their contexts (e.g., in Arabic, the string Al is likely a morpheme, as is any string between Al and a word boundary). We develop efficient learning and inference algorithms using a novel combination of two ideas from previous"
N09-1024,D08-1109,0,\N,Missing
N09-1035,P08-1065,1,0.750599,"Missing"
N09-1035,W05-0615,0,0.0660334,"based methods. M¨uller (2001) presents a hybrid of a categorical and data-driven approach. First, she manually constructs a context-free grammar of possible syllables. This grammar is then made probabilistic using counts obtained from training data. M¨uller (2006) attempts to make her method language-independent. Rather than hand-crafting her context-free grammar, she automatically generates all possible onsets, nuclei, and codas, based on the phonemes existing in the language. The results are somewhat lower than in (M¨uller, 2001), but the approach can be more easily ported across languages. Goldwater and Johnson (2005) also explore using EM to learn the structure of English and German phonemes in an unsupervised setting, following M¨uller in modeling syllable structure with PCFGs. They initialize their parameters using a deterministic 310 parser implementing the sonority principle and estimate the parameters for their maximum likelihood approach using EM. Marchand et al. (2007) apply their Syllabification by Analogy (SbA) technique, originally developed for orthographic forms, to the pronunciation domain. For each input word, SbA finds the most similar substrings in a lexicon of syllabified phoneme strings"
N09-1035,P00-1029,0,0.0648972,"Missing"
N09-1035,P01-1053,0,0.0592242,"Missing"
N09-1035,W02-0608,0,0.0416667,"Missing"
N09-1035,W06-3202,0,0.0247676,"Missing"
N09-2001,P08-1009,1,0.824989,"the source phrase to be translated does not immediately follow the previously translated phrase. This is penalized with a discriminatively-trained distortion penalty. In order to calculate the current translation score, each state can be represented by a triple: • A coverage vector HC indicates which source words have already been translated. 1 As cohesion concerns only movement in the source, we can completely ignore the language model context, making state effectively an (f¯, HC ) tuple. To enforce cohesion during the state expansion process, cohesive phrasal decoding has been proposed in (Cherry, 2008; Yamamoto et al., 2008). The cohesionenhanced decoder enforces the following constraint: once the decoder begins translating any part of a source subtree, it must cover all the words under that subtree before it can translate anything outside of it. This notion can be applied to any projective tree structure, but we use dependency trees, which have been shown to demonstrate greater cross-lingual cohesion than other structures (Fox, 2002). We use a tree data structure to store the dependency tree. Each node in the tree contains surface word form, word position, parent position, dependency type"
N09-2001,W02-1039,0,0.180787,"ntext, making state effectively an (f¯, HC ) tuple. To enforce cohesion during the state expansion process, cohesive phrasal decoding has been proposed in (Cherry, 2008; Yamamoto et al., 2008). The cohesionenhanced decoder enforces the following constraint: once the decoder begins translating any part of a source subtree, it must cover all the words under that subtree before it can translate anything outside of it. This notion can be applied to any projective tree structure, but we use dependency trees, which have been shown to demonstrate greater cross-lingual cohesion than other structures (Fox, 2002). We use a tree data structure to store the dependency tree. Each node in the tree contains surface word form, word position, parent position, dependency type and POS tag. We use T to stand for our dependency tree, and T (n) to stand for the subtree rooted at node n. Each subtree T (n) covers a span of contiguous source words; for subspan f¯ covered by T (n), we say f¯ ∈ T (n). Cohesion is checked as we extend a state (f¯h , HC h ) with the translation of f¯h+1 , creating a new state (f¯h+1 , HC h+1 ). Algorithm 1 presents the cohesion check described by Cherry (2008). Line 2 selects focal poi"
N09-2001,P07-2045,0,0.00803084,"have not been covered. For example, we want to translate the English sentence “the presidential election of the united states begins tomorrow” to French with the dependency structure as in Figure 1. We consider f¯h = “the united states”, f¯h+1 = “begins”. The coverage bit vector HC h+1 is “0 0 0 0 1 1 1 1 0”. Algorithm 5 will return true for Interruption, 4 for ICount (“the”; “presidential”; “election”; “of”), 0 for V erbCount and 1 for N ounCount (“election”). 3 Experiments We built baseline systems using GIZA++ (Och and Ney, 2003), Moses’ phrase extraction with grow-diag-finalend heuristic (Koehn et al., 2007), a standard phrasebased decoder (Vogel, 2003), the SRI LM toolkit (Stolcke, 2002), the suffix-array language model (Zhang and Vogel, 2005), a distance-based word reordering model 3 Algorithm 5 Rich Interruption Constraints (Coh5) Input: Source tree T , previous phrase f¯h , current phrase f¯h+1 , coverage vector HC 1: Interruption ← F alse 2: ICount, V erbCount, N ounCount ← 0 3: F ← the left and right-most tokens of f¯h 4: for each of f ∈ F do Climb the dependency tree from f until you reach 5: the highest node n such that f¯h+1 ∈ / T (n). 6: if n exists then 7: for each of e ∈ T (n) and HCh"
N09-2001,de-marneffe-etal-2006-generating,0,0.012338,"+ 1 12: else if POS of e is “NN” then 13: N ounCount ← N ounCount + 1 14: end if 15: end for 16: end if 17: end for 18: Return Interruption, ICount, V erbCount, N ounCount with a window of 3, and the maximum number of target phrases restricted to 10. Results are reported using lowercase BLEU (Papineni et al., 2002). All model weights were trained on development sets via minimum-error rate training (MERT) (Och, 2003) with 200 unique n-best lists and optimizing toward BLEU. We used the MALT parser (Nivre et al., 2006) to obtain source English dependency trees and the Stanford parser for Arabic (Marneffe et al., 2006). In order to decide whether the translation output of one MT engine is significantly better than another one, we used the bootstrap method (Zhang et al., 2004) with 1000 samples (p &lt; 0.05). We perform experiments on English→Iraqi and English→Spanish. Detailed corpus statistics are shown in Table 1. Table 2 shows results in lowercase BLEU and bold type is used to indicate highest scores. An italic text indicates the score is statistically significant better than the baseline. sentence pairs unique sent. pairs avg. sentence length # words vocabulary English→Iraqi English Iraqi 654,556 510,314 8"
N09-2001,nivre-etal-2006-maltparser,0,0.0395762,"erruption ← T rue ICount = ICount + 1 9: 10: if POS of e is “VB” then 11: V erbCount ← V erbCount + 1 12: else if POS of e is “NN” then 13: N ounCount ← N ounCount + 1 14: end if 15: end for 16: end if 17: end for 18: Return Interruption, ICount, V erbCount, N ounCount with a window of 3, and the maximum number of target phrases restricted to 10. Results are reported using lowercase BLEU (Papineni et al., 2002). All model weights were trained on development sets via minimum-error rate training (MERT) (Och, 2003) with 200 unique n-best lists and optimizing toward BLEU. We used the MALT parser (Nivre et al., 2006) to obtain source English dependency trees and the Stanford parser for Arabic (Marneffe et al., 2006). In order to decide whether the translation output of one MT engine is significantly better than another one, we used the bootstrap method (Zhang et al., 2004) with 1000 samples (p &lt; 0.05). We perform experiments on English→Iraqi and English→Spanish. Detailed corpus statistics are shown in Table 1. Table 2 shows results in lowercase BLEU and bold type is used to indicate highest scores. An italic text indicates the score is statistically significant better than the baseline. sentence pairs uni"
N09-2001,J03-1002,0,0.00592023,"ation will be penalized more in terms of the number of verb and noun words that have not been covered. For example, we want to translate the English sentence “the presidential election of the united states begins tomorrow” to French with the dependency structure as in Figure 1. We consider f¯h = “the united states”, f¯h+1 = “begins”. The coverage bit vector HC h+1 is “0 0 0 0 1 1 1 1 0”. Algorithm 5 will return true for Interruption, 4 for ICount (“the”; “presidential”; “election”; “of”), 0 for V erbCount and 1 for N ounCount (“election”). 3 Experiments We built baseline systems using GIZA++ (Och and Ney, 2003), Moses’ phrase extraction with grow-diag-finalend heuristic (Koehn et al., 2007), a standard phrasebased decoder (Vogel, 2003), the SRI LM toolkit (Stolcke, 2002), the suffix-array language model (Zhang and Vogel, 2005), a distance-based word reordering model 3 Algorithm 5 Rich Interruption Constraints (Coh5) Input: Source tree T , previous phrase f¯h , current phrase f¯h+1 , coverage vector HC 1: Interruption ← F alse 2: ICount, V erbCount, N ounCount ← 0 3: F ← the left and right-most tokens of f¯h 4: for each of f ∈ F do Climb the dependency tree from f until you reach 5: the highest node"
N09-2001,P03-1021,0,0.0622502,"res context needed by the target language model. Introduction Phrase-based machine translation is driven by a phrasal translation model, which relates phrases (contiguous segments of words) in the source to phrases in the target. This translation model can be derived from a wordaligned bitext. Translation candidates are scored according to a linear model combining several informative feature functions. Crucially, this model incorporates translation model scores and n-gram language model scores. The component features are weighted to minimize a translation error criterion on a development set (Och, 2003). Decoding the source sentence takes the form of a beam search through the translation space, with intermediate states corresponding to partial translations. The decoding process advances by extending a state with the translation of a source phrase, until each source word has been translated exactly once. Re-ordering occurs when the source phrase to be translated does not immediately follow the previously translated phrase. This is penalized with a discriminatively-trained distortion penalty. In order to calculate the current translation score, each state can be represented by a triple: • A co"
N09-2001,P02-1040,0,0.0778749,"tokens of f¯h 4: for each of f ∈ F do Climb the dependency tree from f until you reach 5: the highest node n such that f¯h+1 ∈ / T (n). 6: if n exists then 7: for each of e ∈ T (n) and HCh+1 (e) = 0 do 8: Interruption ← T rue ICount = ICount + 1 9: 10: if POS of e is “VB” then 11: V erbCount ← V erbCount + 1 12: else if POS of e is “NN” then 13: N ounCount ← N ounCount + 1 14: end if 15: end for 16: end if 17: end for 18: Return Interruption, ICount, V erbCount, N ounCount with a window of 3, and the maximum number of target phrases restricted to 10. Results are reported using lowercase BLEU (Papineni et al., 2002). All model weights were trained on development sets via minimum-error rate training (MERT) (Och, 2003) with 200 unique n-best lists and optimizing toward BLEU. We used the MALT parser (Nivre et al., 2006) to obtain source English dependency trees and the Stanford parser for Arabic (Marneffe et al., 2006). In order to decide whether the translation output of one MT engine is significantly better than another one, we used the bootstrap method (Zhang et al., 2004) with 1000 samples (p &lt; 0.05). We perform experiments on English→Iraqi and English→Spanish. Detailed corpus statistics are shown in Ta"
N09-2001,W08-0401,0,0.104581,"rase to be translated does not immediately follow the previously translated phrase. This is penalized with a discriminatively-trained distortion penalty. In order to calculate the current translation score, each state can be represented by a triple: • A coverage vector HC indicates which source words have already been translated. 1 As cohesion concerns only movement in the source, we can completely ignore the language model context, making state effectively an (f¯, HC ) tuple. To enforce cohesion during the state expansion process, cohesive phrasal decoding has been proposed in (Cherry, 2008; Yamamoto et al., 2008). The cohesionenhanced decoder enforces the following constraint: once the decoder begins translating any part of a source subtree, it must cover all the words under that subtree before it can translate anything outside of it. This notion can be applied to any projective tree structure, but we use dependency trees, which have been shown to demonstrate greater cross-lingual cohesion than other structures (Fox, 2002). We use a tree data structure to store the dependency tree. Each node in the tree contains surface word form, word position, parent position, dependency type and POS tag. We use T t"
N09-2001,2005.eamt-1.39,1,0.877337,"orrow” to French with the dependency structure as in Figure 1. We consider f¯h = “the united states”, f¯h+1 = “begins”. The coverage bit vector HC h+1 is “0 0 0 0 1 1 1 1 0”. Algorithm 5 will return true for Interruption, 4 for ICount (“the”; “presidential”; “election”; “of”), 0 for V erbCount and 1 for N ounCount (“election”). 3 Experiments We built baseline systems using GIZA++ (Och and Ney, 2003), Moses’ phrase extraction with grow-diag-finalend heuristic (Koehn et al., 2007), a standard phrasebased decoder (Vogel, 2003), the SRI LM toolkit (Stolcke, 2002), the suffix-array language model (Zhang and Vogel, 2005), a distance-based word reordering model 3 Algorithm 5 Rich Interruption Constraints (Coh5) Input: Source tree T , previous phrase f¯h , current phrase f¯h+1 , coverage vector HC 1: Interruption ← F alse 2: ICount, V erbCount, N ounCount ← 0 3: F ← the left and right-most tokens of f¯h 4: for each of f ∈ F do Climb the dependency tree from f until you reach 5: the highest node n such that f¯h+1 ∈ / T (n). 6: if n exists then 7: for each of e ∈ T (n) and HCh+1 (e) = 0 do 8: Interruption ← T rue ICount = ICount + 1 9: 10: if POS of e is “VB” then 11: V erbCount ← V erbCount + 1 12: else if POS o"
N09-2001,zhang-etal-2004-interpreting,1,0.825322,"ounCount with a window of 3, and the maximum number of target phrases restricted to 10. Results are reported using lowercase BLEU (Papineni et al., 2002). All model weights were trained on development sets via minimum-error rate training (MERT) (Och, 2003) with 200 unique n-best lists and optimizing toward BLEU. We used the MALT parser (Nivre et al., 2006) to obtain source English dependency trees and the Stanford parser for Arabic (Marneffe et al., 2006). In order to decide whether the translation output of one MT engine is significantly better than another one, we used the bootstrap method (Zhang et al., 2004) with 1000 samples (p &lt; 0.05). We perform experiments on English→Iraqi and English→Spanish. Detailed corpus statistics are shown in Table 1. Table 2 shows results in lowercase BLEU and bold type is used to indicate highest scores. An italic text indicates the score is statistically significant better than the baseline. sentence pairs unique sent. pairs avg. sentence length # words vocabulary English→Iraqi English Iraqi 654,556 510,314 8.4 5.9 5.5 M 3.8 M 34 K 109 K English→Spanish English Spanish 1,310,127 1,287,016 27.4 28.6 35.8 M 37.4 M 117 K 173 K Table 1: Corpus statistics Our English-Ira"
N09-2001,N04-1035,0,\N,Missing
N09-2001,N04-4026,0,\N,Missing
N09-2001,C04-1030,0,\N,Missing
N09-2001,C04-1073,0,\N,Missing
N09-2001,D08-1089,0,\N,Missing
N09-2001,P06-1067,0,\N,Missing
N09-2001,W06-1608,0,\N,Missing
N09-2001,P05-1033,0,\N,Missing
N09-2001,P05-1034,1,\N,Missing
N09-2001,J97-3002,0,\N,Missing
N09-2001,P05-1066,0,\N,Missing
N09-2001,D07-1077,0,\N,Missing
N09-2001,2005.iwslt-1.8,0,\N,Missing
N09-2001,W08-0509,1,\N,Missing
N09-2001,N06-1004,0,\N,Missing
N10-1020,N04-1015,0,0.0371749,"Twitter through mobile devices. Posts are often highly ungrammatical, and filled with spelling errors. In order to illustrate the spelling variation found on Twitter, we ran the Jcluster word clustering algorithm (Goodman, 2001) on our cor4 http://twitter.com/public_timeline vides the 20 most recent posts on Twitter procoming comming enough enought enuff enuf be4 b4 befor before yuhr yur your yor ur youur yhur msgs messages couldnt culdnt cldnt cannae cudnt couldent about bou abt abour abut bowt Ck Ck 174 πk θk W0 s0,j w0,j Our base model structure is inspired by the content model proposed by Barzilay and Lee (2004) for multi-document summarization. Their sentencelevel HMM discovers the sequence of topics used to describe a particular type of news event, such as earthquakes. A news story is modeled by first generating a sequence of hidden topics according to a Markov model, with each topic generating an observed sentence according to a topic-specific language model. These models capture the sequential structure of news stories, and can be used for summarization tasks such as sentence extraction and ordering. Our goals are not so different: we wish to discover the sequential dialogue structure of conversa"
N10-1020,W04-3240,0,0.126967,"Missing"
N10-1020,W09-3951,0,0.364465,"sations are carried out by replying to specific posts. The Twitter API provides a link from each reply to the post it is responding to, allowing 2 The Crook et al. model should be able to be combined with the models we present here. 3 Will be available at http://www.cs.washington. edu/homes/aritter/twitter_chat/ 173 10 12 14 8 6 log frequency 4 2 0 There is surprisingly little work in unsupervised dialogue act tagging. Woszczyna and Waibel (1994) propose an unsupervised Hidden Markov Model (HMM) for dialogue structure in a meeting scheduling domain, but model dialogue state at the word level. Crook et al. (2009) use Dirichlet process mixture models to cluster utterances into a flexible number of acts in a travel-planning domain, but do not examine the sequential structure of dialogue.2 In contrast to previous work, we address the problem of discovering dialogue acts in an informal, open-topic domain, where an unsupervised learner may be distracted by strong topic clusters. We also train and test our models in a new medium: Twitter. Rather than test against existing dialogue inventories, we evaluate using qualitative visualizations and a novel conversation ordering task, to ensure our models have the"
N10-1020,P06-1039,0,0.0465747,"Missing"
N10-1020,P08-1095,0,0.00778227,"edium. 172 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 172–180, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics 2 Data To enable the study of large-data solutions to dialogue modeling, we have collected a corpus of 1.3 million conversations drawn from the microblogging service, Twitter. 3 To our knowledge, this is the largest corpus of naturally occurring chat data that has been available for study thus far. Similar datasets include the NUS SMS corpus (How and Kan, 2005), several IRC chat corpora (Elsner and Charniak, 2008; Forsyth and Martell, 2007), and blog datasets (Yano et al., 2009; Gamon et al., 2008), which can display conversational structure in the blog comments. As it characterizes itself as a micro-blog, it should not be surprising that structurally, Twitter conversations lie somewhere between chat and blogs. Like blogs, conversations on Twitter occur in a public environment, where they can be collected for research purposes. However, Twitter posts are restricted to be no longer than 140 characters, which keeps interactions chat-like. Like e-mail and unlike IRC, Twitter conversations are carried out"
N10-1020,P07-1094,0,0.0183454,"we first sample its act, and then sample a source for each word in the post. The hidden act and source variables are sampled according to the following transition distributions: 6 This figure omits hyperparameters as well as act transition and emission multinomials to reduce clutter. Dirichlet priors are placed over all multinomials. Ptrans (ai |a−i , s, w) ∝ P (ai |a−i ) Wi Y P (wi,j |a, s, w−(i,j) ) j=1 Ptrans (si,j |a, s−(i,j) , w) ∝ P (si,j |s−(i,j) )P (wi,j |a, s, w−(i,j) ) These probabilities can be computed analogously to the calculations used in the collapsed sampler for a bigram HMM (Goldwater and Griffiths, 2007), and those used for LDA (Griffiths and Steyvers, 2004). Note that our model contains five hyperparameters. Rather than attempt to set them using an expensive grid search, we treat the concentration parameters as additional hidden variables and sample each in turn, conditioned on the current assignment to all other variables. Because these variables are continuous, we apply slice sampling (Neal, 2003). Slice sampling is a general technique for drawing samples from a distribution by sampling uniformly from the area under its density function. 3.3 Estimating Likelihood on Held-Out Data In Sectio"
N10-1020,N09-1041,0,0.075662,"om nom! Table 2: Example of a topical cluster discovered by the EM Conversation Model. similar to previous HMMs for supervised dialogue act recognition (Stolcke et al., 2000), but our model is trained unsupervised. 3.2 generated word clusters, but we found that these approaches degrade model performance. Another approach to filtering out topic information leaves the data intact, but modifies the model to account for topic. To that end, we adopt a Latent Dirichlet Allocation, or LDA, framework (Blei et al., 2003) similar to approaches used recently in summarization (Daum´e III and Marcu, 2006; Haghighi and Vanderwende, 2009). The goal of this extended model is to separate content words from dialogue indicators. Each word in a conversation is generated from one of three sources: • The current post’s dialogue act Conversation + Topic model Our conversations are not restricted to any particular topic: Twitter users can and will talk about anything. Therefore, there is no guarantee that our model, charged with discovering clusters of posts that aid in the prediction of the next cluster, will necessarily discover dialogue acts. The sequence model could instead partition entire conversations into topics, such as food,"
N10-1020,D09-1130,0,0.095308,"ire not only new annotations, but new annotation guidelines and new dialogue acts. This issue becomes more pressing as the Internet continues to expand the number of ways in which we communicate, bringing us e-mail, newsgroups, IRC, forums, blogs, Facebook, Twitter, and whatever is on the horizon. Previous work has taken a variety of approaches to dialogue act tagging in new media. Cohen et al. (2004) develop an inventory of dialogue acts specific to e-mail in an office domain. They design their inventory by inspecting a large corpus of e-mail, and refine it during the manual tagging process. Jeong et al. (2009) use semi-supervised learning to transfer dialogue acts from labeled speech corpora to the Internet media of forums and e-mail. They manually restructure the source act inventories in an attempt to create coarse, domain-independent acts. Each approach relies on a human designer to inject knowledge into the system through the inventory of available acts. As an alternative solution for new media, we propose a series of unsupervised conversation models, where the discovery of acts amounts to clustering utterances with similar conversational roles. This avoids manual construction of an act invento"
N10-1020,N06-1047,0,0.00439271,"mount of data, available only because Twitter blurs the line between chatting and publishing, highlights the need to be able to adapt quickly to a new medium. 1 Introduction Automatic detection of dialogue structure is an important first step toward deep understanding of human conversations. Dialogue acts1 provide an initial level of structure by annotating utterances with shallow discourse roles such as “statement”, “question” and “answer”. These acts are useful in many applications, including conversational agents (Wilks, 2006), dialogue systems (Allen et al., 2007), dialogue summarization (Murray et al., 2006), and flirtation detection (Ranganath et al., 2009). Dialogue act tagging has traditionally followed an annotate-train-test paradigm, which begins with the ∗ 1 This work was conducted at Microsoft Research. Also called “speech acts” design of annotation guidelines, followed by the collection and labeling of corpora (Jurafsky et al., 1997; Dhillon et al., 2004). Only then can one train a tagger to automatically recognize dialogue acts (Stolcke et al., 2000). This paradigm has been quite successful, but the labeling process is both slow and expensive, limiting the amount of data available for tr"
N10-1020,D09-1035,0,0.00897471,"urs the line between chatting and publishing, highlights the need to be able to adapt quickly to a new medium. 1 Introduction Automatic detection of dialogue structure is an important first step toward deep understanding of human conversations. Dialogue acts1 provide an initial level of structure by annotating utterances with shallow discourse roles such as “statement”, “question” and “answer”. These acts are useful in many applications, including conversational agents (Wilks, 2006), dialogue systems (Allen et al., 2007), dialogue summarization (Murray et al., 2006), and flirtation detection (Ranganath et al., 2009). Dialogue act tagging has traditionally followed an annotate-train-test paradigm, which begins with the ∗ 1 This work was conducted at Microsoft Research. Also called “speech acts” design of annotation guidelines, followed by the collection and labeling of corpora (Jurafsky et al., 1997; Dhillon et al., 2004). Only then can one train a tagger to automatically recognize dialogue acts (Stolcke et al., 2000). This paradigm has been quite successful, but the labeling process is both slow and expensive, limiting the amount of data available for training. The expense is compounded as we consider ne"
N10-1020,J00-3003,0,0.517721,"Missing"
N10-1020,N09-1054,0,0.0160001,"North American Chapter of the ACL, pages 172–180, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics 2 Data To enable the study of large-data solutions to dialogue modeling, we have collected a corpus of 1.3 million conversations drawn from the microblogging service, Twitter. 3 To our knowledge, this is the largest corpus of naturally occurring chat data that has been available for study thus far. Similar datasets include the NUS SMS corpus (How and Kan, 2005), several IRC chat corpora (Elsner and Charniak, 2008; Forsyth and Martell, 2007), and blog datasets (Yano et al., 2009; Gamon et al., 2008), which can display conversational structure in the blog comments. As it characterizes itself as a micro-blog, it should not be surprising that structurally, Twitter conversations lie somewhere between chat and blogs. Like blogs, conversations on Twitter occur in a public environment, where they can be collected for research purposes. However, Twitter posts are restricted to be no longer than 140 characters, which keeps interactions chat-like. Like e-mail and unlike IRC, Twitter conversations are carried out by replying to specific posts. The Twitter API provides a link fr"
N10-1103,P07-1013,0,0.0271278,"oviding their respective models with different information. To combine the strengths of these two systems, we include joint n-gram features inside a state-of-the-art discriminative sequence model. We evaluate our approach on several letter-to-phoneme and transliteration data sets. Our results indicate an improvement in overall performance with respect to both the joint n-gram approach and traditional feature sets for discriminative models. 1 ‡ Joint n-gram models (Bisani and Ney, 2002; Chen, 2003; Bisani and Ney, 2008) have been widely applied to string transduction problems (Li et al., 2004; Demberg et al., 2007; Jansche and Sproat, 2009). The power of the approach lies in building a language model over the operations used in the conversion from source to target. Crucially, this allows the inclusion of source context in the generative story. Smoothing techniques play an important role in joint n-gram models, greatly affecting their performance. Although joint n-gram models are capable of capturing context information in both source and target, they cannot selectively use only source or target information, nor can they consider arbitrary sequences within their context window, as they are limited by th"
N10-1103,W09-3505,0,0.0423609,"ve models with different information. To combine the strengths of these two systems, we include joint n-gram features inside a state-of-the-art discriminative sequence model. We evaluate our approach on several letter-to-phoneme and transliteration data sets. Our results indicate an improvement in overall performance with respect to both the joint n-gram approach and traditional feature sets for discriminative models. 1 ‡ Joint n-gram models (Bisani and Ney, 2002; Chen, 2003; Bisani and Ney, 2008) have been widely applied to string transduction problems (Li et al., 2004; Demberg et al., 2007; Jansche and Sproat, 2009). The power of the approach lies in building a language model over the operations used in the conversion from source to target. Crucially, this allows the inclusion of source context in the generative story. Smoothing techniques play an important role in joint n-gram models, greatly affecting their performance. Although joint n-gram models are capable of capturing context information in both source and target, they cannot selectively use only source or target information, nor can they consider arbitrary sequences within their context window, as they are limited by their back-off schedule. Intr"
N10-1103,N07-1047,1,0.360301,"ever, since sounds are often represented by multicharacter units, the relationship between the input and output characters is often complex. This prevents the straightforward application of standard tagging techniques, but can be addressed by substring decoders or semi-Markov models. Because the relationship between x and y is hidden, alignments between the input and output characters (or substrings) are often provided in a preprocessing step. These are usually generated in an unsupervised fashion using a variant of the EM algorithm. Our system employs the many-to-many alignment described in (Jiampojamarn et al., 2007). We trained our system on these aligned examples by using the online discriminative training of (Jiampojamarn et al., 2009). At each step, the parameter update is provided by MIRA. 3 Features Jiampojamarn et al. (2009) describe a set of indicator feature templates that include (1) context features (2) transition features and (3) linear-chain features. 698 transition linear-chain xi−c yi ... xi+c yi xi−c xi−c+1 yi ... xi+c−1 xi+c yi ...... xi−c . . . xi+c yi yi−1 yi xi−c yi−1 yi ... xi+c yi−1 yi xi−c xi−c+1 yi−1 yi ... xi+c−1 xi+c yi−1 yi ...... xi−c . . . xi+c , yi−1 yi xi+1−n yi+1−n xi yi .."
N10-1103,P08-1103,1,0.889131,"e begin with a Hidden Markov Model architecture, augmented with substring operations and discriminative training. The primary strength of these systems is their ability to include rich indicator features representing long sequences of source context. We will assume a specific instance of discriminative sequence modeling, D I REC TL (Jiampojamarn et al., 2009), which achieved the best results on several language pairs in the NEWS Machine Transliteration Shared Task (Li et al., 2009). The same system matches or exceeds the performance of the joint n-gram approach on letterto-phoneme conversion (Jiampojamarn et al., 2008). Its features are optimized by an online, margin697 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 697–700, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics based learning algorithm, specifically, the Margin Infused Relaxed Algorithm, MIRA (Crammer and Singer, 2003). In this paper, we propose an approach that combines these two different paradigms by formulating the joint n-gram model as a new set of features in the discriminative model. This leverages an advantage of discriminative training, in that"
N10-1103,W09-3504,1,0.929033,"pronunciation of orthography complicate conversion. Transliteration suffers from the same ambiguities, but the transformation is further complicated Discriminative sequence models have also been shown to perform extremely well on string transduction problems. These begin with a Hidden Markov Model architecture, augmented with substring operations and discriminative training. The primary strength of these systems is their ability to include rich indicator features representing long sequences of source context. We will assume a specific instance of discriminative sequence modeling, D I REC TL (Jiampojamarn et al., 2009), which achieved the best results on several language pairs in the NEWS Machine Transliteration Shared Task (Li et al., 2009). The same system matches or exceeds the performance of the joint n-gram approach on letterto-phoneme conversion (Jiampojamarn et al., 2008). Its features are optimized by an online, margin697 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 697–700, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics based learning algorithm, specifically, the Margin Infused Relaxed Algorithm, MIRA ("
N10-1103,P04-1021,0,0.251494,"ifferent ways, providing their respective models with different information. To combine the strengths of these two systems, we include joint n-gram features inside a state-of-the-art discriminative sequence model. We evaluate our approach on several letter-to-phoneme and transliteration data sets. Our results indicate an improvement in overall performance with respect to both the joint n-gram approach and traditional feature sets for discriminative models. 1 ‡ Joint n-gram models (Bisani and Ney, 2002; Chen, 2003; Bisani and Ney, 2008) have been widely applied to string transduction problems (Li et al., 2004; Demberg et al., 2007; Jansche and Sproat, 2009). The power of the approach lies in building a language model over the operations used in the conversion from source to target. Crucially, this allows the inclusion of source context in the generative story. Smoothing techniques play an important role in joint n-gram models, greatly affecting their performance. Although joint n-gram models are capable of capturing context information in both source and target, they cannot selectively use only source or target information, nor can they consider arbitrary sequences within their context window, as"
N10-1103,W09-3501,0,\N,Missing
N12-1047,W10-1756,0,0.0120603,"ed BLEU, and provide access to richer hypothesis spaces through the use of online training or lattices.5 The direct optimizers have access to a tunable regularization parameter λ, and do not require special purpose code for hope and fear lattice decoding. Batch 4 SVM training with interpolated BLEU outperformed add-1 BLEU in preliminary testing. A comparison of different BLEU approximations under different tuning objectives would be an interesting path for future work. 5 MR approaches that use lattices (Li and Eisner, 2009; Pauls et al., 2009; Rosti et al., 2011) or the complete search space (Arun et al., 2010) exist, but are not tested here. k-best MIRA straddles the two groups, benefiting from pseudo-corpus BLEU and easy implementation, while being restricted to a k-best list. 4 Experimental Design We evaluated the six tuning strategies described in this paper, along with two MERT baselines, on three language pairs French-English (Fr-En), English-French (En-Fr) and Chinese-English (Zh En) , across three different feature-set sizes. Each setting was run five times over randomized variants to improve reliability. To cope with the resulting large number of configurations, we ran all experiments usin"
N12-1047,D08-1024,0,0.835224,"U, which we re-encode into a cost ∆i (e) on derivations, where a high cost indicates that e receives a low BLEU score. Unless otherwise stated, we will assume the use of sentence BLEU with add1 smoothing (Lin and Och, 2004). The learners differ in their definition of ` and ∆, and in how they employ their loss functions to tune their weights. 1 This is true of any evaluation metric that considers only the ranking of hypotheses and not their model scores; ie, it is true of all common MT metrics. 2.1 Margin Infused Relaxed Algorithm First employed in SMT by Watanabe et al. (2007), and refined by Chiang et al. (2008; 2009), the Margin Infused Relaxed Algorithm (MIRA) employs a structured hinge loss: h  i `i (w) ~ = max ∆i (e) + w ~ · ~hi (e) − ~hi (e∗i ) e∈Ei (2) where e∗i is an oracle derivation, and cost is defined as ∆i (e) = BLEUi (e∗i ) − BLEUi (e), so that ∆i (e∗i ) = 0. The loss `i (w) ~ is 0 only if w ~ separates each e ∈ Ei from e∗i by a margin proportional to their BLEU differentials. MIRA is an instance of online learning, repeating the following steps: visit an example i, decode according to w, ~ and update w ~ to reduce `i (w). ~ Each update makes the smallest change to w ~ (subject to a s"
N12-1047,N09-1025,0,0.167068,"Missing"
N12-1047,P11-2031,0,0.556511,"et al.’s (2008) hope-fear MIRA that use k-best or lattice-approximated search spaces, producing a Batch MIRA that outperforms a popular mechanism for parallelizing online learners. We also investigate the direct optimization of hinge loss on k-best lists, through the use of a Structured SVM (Tsochantaridis et al., 2004). We review and organize the existing tuning literature, providing sentence-level loss functions for minimum risk, online and pairwise training. Finally, since randomization plays a different role in each tuner, we also suggest a new method for testing an optimizer’s stability (Clark et al., 2011), which sub-samples the tuning set instead of varying a random seed. 2 Background We begin by establishing some notation. We view our training set as a list of triples [f, R, E]ni=1 , where f is a source-language sentence, R is a set of targetlanguage reference sentences, and E is the set of all reachable hypotheses; that is, each e ∈ Ei is a target-language derivation that can be decoded from fi . The function ~hi (e) describes e’s relationship to its source fi using features that decompose into the decoder. A linear model w ~ scores derivations according to their features, meaning that the d"
N12-1047,N12-1023,0,0.261828,"t term provides regularization, weighted by λ. Throughout this paper, (4) is optimized with respect to a fixed approximation of the decoder’s true search space, represented as a collection of k-best lists. The various methods differ in their definition of loss and in how they optimize their objective. Without the complications added by hope decoding and a time-dependent cost function, unmodified MIRA can be shown to be carrying out dual coordinate descent for an SVM training objective (Martins et al., 2010). However, exactly what objective hopefear MIRA is optimizing remains an open question. Gimpel and Smith (2012) discuss these issues in greater detail, while also providing an interpretable alternative to MIRA. 2.3 Pairwise Ranking Optimization Introduced by Hopkins and May (2011), Pairwise Ranking Optimization (PRO) aims to handle large feature sets inside the traditional MERT architecture. That is, PRO employs a growing approximation of Ei by aggregating the k-best hypotheses from a series of increasingly refined models. This architecture is desirable, as most groups have infrastructure to k-best decode their tuning sets in parallel. For a given approximate E˜i , PRO creates a sample Si of (eg , eb )"
N12-1047,D11-1125,0,0.625231,"ver, MERT has trouble scaling to more than 30 features, which has led to a surge in research on tuning schemes that can handle high-dimensional feature spaces. These methods fall into a number of broad categories. Minimum risk approaches (Och, 2003; Smith and Eisner, 2006) have been quietly capable of handling many features for some time, but have yet to see widespread adoption. Online methods (Liang et al., 2006; Watanabe et al., 2007), are recognized to be effective, but require substantial implementation efforts due to difficulties with parallelization. Pairwise ranking (Shen et al., 2004; Hopkins and May, 2011) recasts tuning as classification, and can be very easy to implement, as it fits nicely into the established MERT infrastructure. The MERT algorithm optimizes linear weights relative to a collection of k-best lists or lattices, which provide an approximation to the true search space. This optimization is wrapped in an outer loop that iterates between optimizing weights and re-decoding with those weights to enhance the approximation. Our primary contribution is to empirically compare eight tuning algorithms and variants, focusing on methods that work within MERT’s established outer loop. This i"
N12-1047,N03-1017,0,0.222749,"09) summarized in table 1.6 The dev and test sets were chosen randomly from among the most recent 5 days of Hansard transcripts. The system for Zh-En was trained on data from the NIST 2009 Chinese MT evaluation, summarized in table 2. The dev set was taken from the NIST 05 evaluation set, augmented with some material reserved from other NIST corpora. The NIST 04, 06, and 08 evaluation sets were used for testing. 4.2 SMT Features For all language pairs, phrases were extracted with a length limit of 7 from separate word alignments performed by IBM2 and HMM models and symmetrized using diag-and (Koehn et al., 2003). Conditional phrase probabilities in both directions were 6 This corpus will be distributed on request. template tgt unal count bin word pair length bin total max 50 11 6724 63 6848 fren 50 11 1298 63 1422 enfr 50 11 1291 63 1415 zhen 31 11 1664 63 1769 Table 3: Sparse feature templates used in Big. estimated from relative frequencies, and from lexical probabilities (Zens and Ney, 2004). Language models were estimated with Kneser-Ney smoothing using SRILM. Six-feature lexicalized distortion models were estimated and applied as in Moses. For each language pair, we defined roughly equivalent sy"
N12-1047,P07-2045,0,0.0854368,"the two groups, benefiting from pseudo-corpus BLEU and easy implementation, while being restricted to a k-best list. 4 Experimental Design We evaluated the six tuning strategies described in this paper, along with two MERT baselines, on three language pairs French-English (Fr-En), English-French (En-Fr) and Chinese-English (Zh En) , across three different feature-set sizes. Each setting was run five times over randomized variants to improve reliability. To cope with the resulting large number of configurations, we ran all experiments using an efficient phrase-based decoder similar to Moses (Koehn et al., 2007). All tuning methods that use an approximate E˜ perform 15 iterations of the outer loop and return the weights that achieve the best development BLEU score. When present, λ was coarsely tuned (trying 3 values differing by magnitudes of 10) in our largefeature Chinese-English setting. • kb-mert : k-best MERT with 20 random restarts. All k-best methods use k = 100. • lb-mert : Lattice MERT (Machery et al., 2008) using unpruned lattices and aggregating only those paths on the line search’s upper envelope. • mira : Online MIRA (§2.1). All MIRA variants use a pseudo-corpus decay γ = 0.999 and C = 0"
N12-1047,D09-1005,0,0.0136064,"arge uniform sample and then selects a subset of pairs with large BLEU differentials. The PRO loss uses a sum over pairs in place of MIRA’s max, which allows PRO to bypass oracle selection, and to optimize with off-the-shelf classifiers. This sum is potentially a weakness, as PRO receives credit for each correctly ordered pair in its sample, and these pairs are not equally relevant to the final BLEU score. 2.4 Minimum Risk Training Minimum risk training (MR) interprets w ~ as a probabilistic model, and optimizes expected BLEU. We focus on expected sentence costs (Och, 2003; Zens et al., 2007; Li and Eisner, 2009), as this risk is simple to optimize and fits nicely into our mathematical framework. Variants that use the expected sufficient statistics of BLEU also exist (Smith and Eisner, 2006; Pauls et al., 2009; Rosti et al., 2011). We again assume a MERT-like tuning architecture. Let ∆i (e) = −BLEUi (e) and let h i P ~hi (e))∆i (e) exp( w ~ · ˜ e∈Ei `i (w) ~ = EPw~ [∆i (e)] = P ~ · ~hi (e0 )) e0 ∈E˜i exp(w This expected cost becomes increasingly small as greater probability mass is placed on derivations with high BLEU scores. This smooth, non-convex objective can be solved to a local minimum using gra"
N12-1047,P06-1096,0,0.181544,"Missing"
N12-1047,C04-1072,0,0.227698,"resist standard mechanisms of regularization that aim to keep ||w|| ~ small. The problems with MERT can be addressed through the use of surrogate loss functions. In this paper, we focus on linear losses that decompose over training examples. Using Ri and Ei , each loss `i (w) ~ th indicates how poorly w ~ performs on the i training example. This requires a sentence-level approximation of BLEU, which we re-encode into a cost ∆i (e) on derivations, where a high cost indicates that e receives a low BLEU score. Unless otherwise stated, we will assume the use of sentence BLEU with add1 smoothing (Lin and Och, 2004). The learners differ in their definition of ` and ∆, and in how they employ their loss functions to tune their weights. 1 This is true of any evaluation metric that considers only the ranking of hypotheses and not their model scores; ie, it is true of all common MT metrics. 2.1 Margin Infused Relaxed Algorithm First employed in SMT by Watanabe et al. (2007), and refined by Chiang et al. (2008; 2009), the Margin Infused Relaxed Algorithm (MIRA) employs a structured hinge loss: h  i `i (w) ~ = max ∆i (e) + w ~ · ~hi (e) − ~hi (e∗i ) e∈Ei (2) where e∗i is an oracle derivation, and cost is defi"
N12-1047,D08-1076,0,0.0782686,"over randomized variants to improve reliability. To cope with the resulting large number of configurations, we ran all experiments using an efficient phrase-based decoder similar to Moses (Koehn et al., 2007). All tuning methods that use an approximate E˜ perform 15 iterations of the outer loop and return the weights that achieve the best development BLEU score. When present, λ was coarsely tuned (trying 3 values differing by magnitudes of 10) in our largefeature Chinese-English setting. • kb-mert : k-best MERT with 20 random restarts. All k-best methods use k = 100. • lb-mert : Lattice MERT (Machery et al., 2008) using unpruned lattices and aggregating only those paths on the line search’s upper envelope. • mira : Online MIRA (§2.1). All MIRA variants use a pseudo-corpus decay γ = 0.999 and C = 0.01. Online parallelization follows McDonald et al. (2010), using 8 shards. We tested 20, 15, 10, 8 and 5 shards during development. • lb-mira : Batch Lattice MIRA (§3.1). • kb-mira : Batch k-best MIRA (§3.1). • pro : PRO (§2.3) follows Hopkins and May (2011), footnote 6, implementing a logistic sigmoid sampler with both the initial and maximum sample size set to 100 pairs. For classification, we employ an in-"
N12-1047,N10-1069,0,0.114766,"gin tuners that explore these trade-offs. 3.1 Batch MIRA Online training makes it possible to learn with the decoder in the loop, forgoing the need to approximate the search space, but it is not necessarily convenient to do so. Online algorithms are notoriously difficult to parallelize, as they assume each example is visited in sequence. Parallelization is important for efficient SMT tuning, as decoding is still relatively expensive. The parallel online updates suggested by Chiang et al. (2008) involve substantial inter-process communication, which may not be easily supported by all clusters. McDonald et al. (2010) suggest a simpler distributed strategy that is amenable to map-reduce-like frameworks, which interleaves online training on shards with weight averaging across shards. This strategy has been adopted by Moses (Hasler et al., 2011), and it is the one we adopt in our MIRA implementation. However, online training using the decoder may not be necessary for good performance. The success of MERT, PRO and MR indicates that their shared search approximation is actually quite reasonable. Therefore, we propose Batch MIRA, which sits exactly where MERT sits in the standard tuning architecture, greatly si"
N12-1047,P02-1038,0,0.548994,"evel loss functions, which motivates several new approaches, including a Structured SVM. We perform empirical comparisons of eight different tuning strategies, including MERT, in a variety of settings. Among other results, we find that a simple and efficient batch version of MIRA performs at least as well as training online, and consistently outperforms other options. 1 Introduction The availability of linear models and discriminative tuning algorithms has been a huge boon to statistical machine translation (SMT), allowing the field to move beyond the constraints of generative noisy channels (Och and Ney, 2002). The ability to optimize these models according to an error metric has become a standard assumption in SMT, due to the wide-spread adoption of Minimum Error Rate Training or MERT (Och, 2003). However, MERT has trouble scaling to more than 30 features, which has led to a surge in research on tuning schemes that can handle high-dimensional feature spaces. These methods fall into a number of broad categories. Minimum risk approaches (Och, 2003; Smith and Eisner, 2006) have been quietly capable of handling many features for some time, but have yet to see widespread adoption. Online methods (Liang"
N12-1047,P03-1021,0,0.826491,"s. Among other results, we find that a simple and efficient batch version of MIRA performs at least as well as training online, and consistently outperforms other options. 1 Introduction The availability of linear models and discriminative tuning algorithms has been a huge boon to statistical machine translation (SMT), allowing the field to move beyond the constraints of generative noisy channels (Och and Ney, 2002). The ability to optimize these models according to an error metric has become a standard assumption in SMT, due to the wide-spread adoption of Minimum Error Rate Training or MERT (Och, 2003). However, MERT has trouble scaling to more than 30 features, which has led to a surge in research on tuning schemes that can handle high-dimensional feature spaces. These methods fall into a number of broad categories. Minimum risk approaches (Och, 2003; Smith and Eisner, 2006) have been quietly capable of handling many features for some time, but have yet to see widespread adoption. Online methods (Liang et al., 2006; Watanabe et al., 2007), are recognized to be effective, but require substantial implementation efforts due to difficulties with parallelization. Pairwise ranking (Shen et al.,"
N12-1047,P02-1040,0,0.114886,"notation. We view our training set as a list of triples [f, R, E]ni=1 , where f is a source-language sentence, R is a set of targetlanguage reference sentences, and E is the set of all reachable hypotheses; that is, each e ∈ Ei is a target-language derivation that can be decoded from fi . The function ~hi (e) describes e’s relationship to its source fi using features that decompose into the decoder. A linear model w ~ scores derivations according to their features, meaning that the decoder solves: ei (w) ~ = arg max w ~ · ~hi (e) (1) e∈Ei Assuming we wish to optimize our decoder’s BLEU score (Papineni et al., 2002), the natural objective of learning would be to find a w ~ such that BLEU([e(w), ~ R]n1 ) is maximal. In most machine learning papers, this would be the point where we would say, “unfortunately, this objective is unfeasible.” But in SMT, we have been happily optimizing exactly this objective for years using MERT. However, it is now acknowledged that the MERT approach is not feasible for more than 30 or so features. This is due to two main factors: 1. MERT’s parameter search slows and becomes less effective as the number of features rises, stopping it from finding good training scores. 2. BLEU"
N12-1047,D09-1147,0,0.137005,"ze with off-the-shelf classifiers. This sum is potentially a weakness, as PRO receives credit for each correctly ordered pair in its sample, and these pairs are not equally relevant to the final BLEU score. 2.4 Minimum Risk Training Minimum risk training (MR) interprets w ~ as a probabilistic model, and optimizes expected BLEU. We focus on expected sentence costs (Och, 2003; Zens et al., 2007; Li and Eisner, 2009), as this risk is simple to optimize and fits nicely into our mathematical framework. Variants that use the expected sufficient statistics of BLEU also exist (Smith and Eisner, 2006; Pauls et al., 2009; Rosti et al., 2011). We again assume a MERT-like tuning architecture. Let ∆i (e) = −BLEUi (e) and let h i P ~hi (e))∆i (e) exp( w ~ · ˜ e∈Ei `i (w) ~ = EPw~ [∆i (e)] = P ~ · ~hi (e0 )) e0 ∈E˜i exp(w This expected cost becomes increasingly small as greater probability mass is placed on derivations with high BLEU scores. This smooth, non-convex objective can be solved to a local minimum using gradient-based optimizers; we have found stochastic gradient descent to be quite effective (Bottou, 2010). Like PRO, MR requires no oracle derivation, and fits nicely into the established MERT architectur"
N12-1047,W11-2119,0,0.0952467,"f classifiers. This sum is potentially a weakness, as PRO receives credit for each correctly ordered pair in its sample, and these pairs are not equally relevant to the final BLEU score. 2.4 Minimum Risk Training Minimum risk training (MR) interprets w ~ as a probabilistic model, and optimizes expected BLEU. We focus on expected sentence costs (Och, 2003; Zens et al., 2007; Li and Eisner, 2009), as this risk is simple to optimize and fits nicely into our mathematical framework. Variants that use the expected sufficient statistics of BLEU also exist (Smith and Eisner, 2006; Pauls et al., 2009; Rosti et al., 2011). We again assume a MERT-like tuning architecture. Let ∆i (e) = −BLEUi (e) and let h i P ~hi (e))∆i (e) exp( w ~ · ˜ e∈Ei `i (w) ~ = EPw~ [∆i (e)] = P ~ · ~hi (e0 )) e0 ∈E˜i exp(w This expected cost becomes increasingly small as greater probability mass is placed on derivations with high BLEU scores. This smooth, non-convex objective can be solved to a local minimum using gradient-based optimizers; we have found stochastic gradient descent to be quite effective (Bottou, 2010). Like PRO, MR requires no oracle derivation, and fits nicely into the established MERT architecture. The expectations n"
N12-1047,N04-1023,0,0.0579498,"T (Och, 2003). However, MERT has trouble scaling to more than 30 features, which has led to a surge in research on tuning schemes that can handle high-dimensional feature spaces. These methods fall into a number of broad categories. Minimum risk approaches (Och, 2003; Smith and Eisner, 2006) have been quietly capable of handling many features for some time, but have yet to see widespread adoption. Online methods (Liang et al., 2006; Watanabe et al., 2007), are recognized to be effective, but require substantial implementation efforts due to difficulties with parallelization. Pairwise ranking (Shen et al., 2004; Hopkins and May, 2011) recasts tuning as classification, and can be very easy to implement, as it fits nicely into the established MERT infrastructure. The MERT algorithm optimizes linear weights relative to a collection of k-best lists or lattices, which provide an approximation to the true search space. This optimization is wrapped in an outer loop that iterates between optimizing weights and re-decoding with those weights to enhance the approximation. Our primary contribution is to empirically compare eight tuning algorithms and variants, focusing on methods that work within MERT’s establ"
N12-1047,P06-2101,0,0.320437,"a huge boon to statistical machine translation (SMT), allowing the field to move beyond the constraints of generative noisy channels (Och and Ney, 2002). The ability to optimize these models according to an error metric has become a standard assumption in SMT, due to the wide-spread adoption of Minimum Error Rate Training or MERT (Och, 2003). However, MERT has trouble scaling to more than 30 features, which has led to a surge in research on tuning schemes that can handle high-dimensional feature spaces. These methods fall into a number of broad categories. Minimum risk approaches (Och, 2003; Smith and Eisner, 2006) have been quietly capable of handling many features for some time, but have yet to see widespread adoption. Online methods (Liang et al., 2006; Watanabe et al., 2007), are recognized to be effective, but require substantial implementation efforts due to difficulties with parallelization. Pairwise ranking (Shen et al., 2004; Hopkins and May, 2011) recasts tuning as classification, and can be very easy to implement, as it fits nicely into the established MERT infrastructure. The MERT algorithm optimizes linear weights relative to a collection of k-best lists or lattices, which provide an approx"
N12-1047,D07-1080,0,0.249835,"o optimize these models according to an error metric has become a standard assumption in SMT, due to the wide-spread adoption of Minimum Error Rate Training or MERT (Och, 2003). However, MERT has trouble scaling to more than 30 features, which has led to a surge in research on tuning schemes that can handle high-dimensional feature spaces. These methods fall into a number of broad categories. Minimum risk approaches (Och, 2003; Smith and Eisner, 2006) have been quietly capable of handling many features for some time, but have yet to see widespread adoption. Online methods (Liang et al., 2006; Watanabe et al., 2007), are recognized to be effective, but require substantial implementation efforts due to difficulties with parallelization. Pairwise ranking (Shen et al., 2004; Hopkins and May, 2011) recasts tuning as classification, and can be very easy to implement, as it fits nicely into the established MERT infrastructure. The MERT algorithm optimizes linear weights relative to a collection of k-best lists or lattices, which provide an approximation to the true search space. This optimization is wrapped in an outer loop that iterates between optimizing weights and re-decoding with those weights to enhance"
N12-1047,P10-1049,0,0.017593,"mprises 4 TM features, one LM, and length and distortion features. For the Chinese system, the LM is a 5-gram trained on the NIST09 Gigaword corpus; for English/French, it is a 4-gram trained on the target half of the parallel Hansard. The Medium set is a more competitive 18-feature system. It adds 4 TM features, one LM, and 6 lexicalized distortion features. For Zh-En, Small’s TM (trained on both train1 and train2 in table 2) is replaced by 2 separate TMs from these sub-corpora; for En/Fr, the extra TM (4 features) comes from a forced-decoding alignment of the training corpus, as proposed by Wuebker et al. (2010). For Zh-En, the extra LM is a 4-gram trained on the target half of the parallel corpus; for En/Fr, it is a 4-gram trained on 5m sentences of similar parliamentary data. The Big set adds sparse Boolean features to Medium, for a maximum of 6,848 features. We used sparse feature templates that are equivalent to the PBMT set described in (Hopkins and May, 2011): tgt unal picks out each of the 50 most frequent target words to appear unaligned in the phrase table; count bin uniquely bins joint phrase pair counts with upper bounds 1,2,4,8,16,32,64,128,1k,10k,∞; word pair fires when each of the 80 mo"
N12-1047,N04-1033,0,0.0275206,"ts were used for testing. 4.2 SMT Features For all language pairs, phrases were extracted with a length limit of 7 from separate word alignments performed by IBM2 and HMM models and symmetrized using diag-and (Koehn et al., 2003). Conditional phrase probabilities in both directions were 6 This corpus will be distributed on request. template tgt unal count bin word pair length bin total max 50 11 6724 63 6848 fren 50 11 1298 63 1422 enfr 50 11 1291 63 1415 zhen 31 11 1664 63 1769 Table 3: Sparse feature templates used in Big. estimated from relative frequencies, and from lexical probabilities (Zens and Ney, 2004). Language models were estimated with Kneser-Ney smoothing using SRILM. Six-feature lexicalized distortion models were estimated and applied as in Moses. For each language pair, we defined roughly equivalent systems (exactly equivalent for En-Fr and FrEn, which are mirror images) for each of three nested feature sets: Small, Medium, and Big. The Small set defines a minimal 7-feature system intended to be within easy reach of all tuning strategies. It comprises 4 TM features, one LM, and length and distortion features. For the Chinese system, the LM is a 5-gram trained on the NIST09 Gigaword co"
N12-1047,D07-1055,0,0.0451808,"which performs a large uniform sample and then selects a subset of pairs with large BLEU differentials. The PRO loss uses a sum over pairs in place of MIRA’s max, which allows PRO to bypass oracle selection, and to optimize with off-the-shelf classifiers. This sum is potentially a weakness, as PRO receives credit for each correctly ordered pair in its sample, and these pairs are not equally relevant to the final BLEU score. 2.4 Minimum Risk Training Minimum risk training (MR) interprets w ~ as a probabilistic model, and optimizes expected BLEU. We focus on expected sentence costs (Och, 2003; Zens et al., 2007; Li and Eisner, 2009), as this risk is simple to optimize and fits nicely into our mathematical framework. Variants that use the expected sufficient statistics of BLEU also exist (Smith and Eisner, 2006; Pauls et al., 2009; Rosti et al., 2011). We again assume a MERT-like tuning architecture. Let ∆i (e) = −BLEUi (e) and let h i P ~hi (e))∆i (e) exp( w ~ · ˜ e∈Ei `i (w) ~ = EPw~ [∆i (e)] = P ~ · ~hi (e0 )) e0 ∈E˜i exp(w This expected cost becomes increasingly small as greater probability mass is placed on derivations with high BLEU scores. This smooth, non-convex objective can be solved to a l"
N12-3006,D09-1111,1,0.814882,"Missing"
N12-3006,J93-2004,0,0.04636,"the NLP group at Microsoft Research. The tools include both traditional linguistic analysis tools such as part-of-speech taggers, constituency and dependency parsers, and more recent developments such as sentiment detection and linguistically valid morphology. As we expand the tools we develop for our own research, the set of tools available in MSR SPLAT will be extended. The toolkit is accessible as a web service, which can be used from a broad set of programming languages. 1 Introduction The availability of annotated data sets that have become community standards, such as the Penn TreeBank (Marcus et al., 1993) and PropBank (Palmer et al., 2005), has enabled many research institutions to build core natural language processing components, including part-of-speech taggers, chunkers, and parsers. There remain many differences in how these components are built, resulting in slight but noticeable variation in the component output. In experimental settings, it has proved sometimes difficult to distinguish between improvements contributed by a specific component feature from improvements due to using a differently-trained linguistic component, such as tokenization. The community recognizes this difficulty,"
N12-3006,J05-1004,0,0.0277178,". The tools include both traditional linguistic analysis tools such as part-of-speech taggers, constituency and dependency parsers, and more recent developments such as sentiment detection and linguistically valid morphology. As we expand the tools we develop for our own research, the set of tools available in MSR SPLAT will be extended. The toolkit is accessible as a web service, which can be used from a broad set of programming languages. 1 Introduction The availability of annotated data sets that have become community standards, such as the Penn TreeBank (Marcus et al., 1993) and PropBank (Palmer et al., 2005), has enabled many research institutions to build core natural language processing components, including part-of-speech taggers, chunkers, and parsers. There remain many differences in how these components are built, resulting in slight but noticeable variation in the component output. In experimental settings, it has proved sometimes difficult to distinguish between improvements contributed by a specific component feature from improvements due to using a differently-trained linguistic component, such as tokenization. The community recognizes this difficulty, and shared task organizers are now"
N12-3006,P06-1055,0,0.0153341,"Missing"
N12-3006,C08-1094,0,0.0225179,"Missing"
N12-3006,J08-2002,1,0.0600596,"abels, like ARG0, ARG1, …, ARG5 for core arguments, and labels like ARGMTMP,ARGM-LOC, etc. for adjunct-like arguments. The meaning of the numbered arguments is verb-specific, with ARG0 typically representing an agent-like role, and ARG1 a patient-like role. This implementation of an SRL system follows the approach described in (Xue and Palmer, 04), and includes two log-linear models for argument identification and classification. A single syntax tree generated by the MSR SPLAT split-merge parser is used as input. Non-overlapping arguments are derived using the dynamic programming algorithm by Toutanova et al. (2008). 3 3.1 Other Language Analysis Functionality Sentence Boundary / Tokenization This analyzer identifies sentence boundaries and breaks the input into tokens. Both are represented as offsets of character ranges. Each token has both a raw form from the string and a normalized form in the PTB specification, e.g., open and close parentheses are replaced by -LRB- and -RRB-, respectively, to remove ambiguity with parentheses indicating syntactic structure. A finite state machine using simple rules and abbreviations detects sentence boundaries with high accuracy, and a set of regular expressions toke"
N12-3006,W04-3212,0,0.0291436,"Missing"
N12-3006,J03-4003,0,\N,Missing
N13-1003,P11-1087,0,0.0115797,"into the decoder. Instead, one simply implements the desired features in the decoder’s feature API and then tunes as normal. The challenge is to design features so that the model can be learned from small tuning sets. The standard approach for sparse feature design in SMT is to lexicalize only on extremely frequent words, such as the top-80 words from each language (Chiang et al., 2009; Hopkins and May, 2011). We take that approach here, but we also use deterministic clusters to represent words from both languages, as provided by mkcls. These clusters mirror parts-of-speech quite effectively (Blunsom and Cohn, 2011), without requiring linguistic resources. They should provide useful generalization for reordering decisions. Inspired by recent successes in semi-supervised learning (Koo et al., 2008; corpus train dev mt08 mt09 sentences 1,490,514 1,663 1,360 1,313 words (ar) 46,403,734 45,243 45,002 40,684 corpus train dev mt06 mt08 words (en) 47,109,486 50,550 51,341 46,813 Table 3: Arabic-English Corpus. For English dev and test sets, word counts are averaged across 4 references. Lin and Wu, 2009), we cluster at two granularities (20 clusters and 50 clusters), and allow the discriminative tuner to determi"
N13-1003,N12-1047,1,0.3156,"rated into a maximum entropy reordering model, as well as sparse decoder features, to see which approach best complements the now-standard relative-frequency lexicalized reordering model. We also view our work as an example of strong sparse features for phrase-based translation. Features from hierarchical and syntax-based translation (Chiang et al., 2009) do not easily transfer to the phrase-based paradigm, and most work that has looked at large feature counts in the context of phrase-based translation has focused on the learning method, and not the features themselves (Hopkins and May, 2011; Cherry and Foster, 2012; Gimpel and Smith, 2012). We show that by targeting reordering, large gains can be made with relatively simple features. 2 Background Phrase-based machine translation constructs its target sentence from left-to-right, with each translation operation selecting a source phrase and appending its translation to the growing target sentence, until 1 Some systems tune for BLEU on much larger sets (Simianer et al., 2012; He and Deng, 2012), but these require exceptional commitments of resources and time. 22 Proceedings of NAACL-HLT 2013, pages 22–31, c Atlanta, Georgia, 9–14 June 2013. 2013 Associati"
N13-1003,W12-3125,1,0.859641,"4. Phrases are extracted with a length limit of 7 from alignments symmetrized using growdiag-final-and (Koehn et al., 2003). Conditional phrase probabilities in both directions are estimated from relative frequencies, and from lexical probabilities (Zens and Ney, 2004). 4-gram language models are estimated from the target side of the bitext with Kneser-Ney smoothing. Relative frequency and maximum entropy RMs are represented with six features, with separate weights for M, S and D in both directions (Koehn et al., 2007). HRM orientations are determined using an unrestricted shiftreduce parser (Cherry et al., 2012). We also employ a standard distortion penalty incorporating the minimum completion cost described by Moore and Quirk (2007). Our multi-stack phrase-based decoder is quite similar to Moses (Koehn et al., 2007). For all systems, parameters are tuned with a batch-lattice variant of hope-fear MIRA (Chiang et al., 2008; Cherry and Foster, 2012). Preliminary experiments suggested that the sparse reordering features have a larger impact when tuned with lattices as opposed to n-best lists. 4.2 We report lower-cased BLEU (Papineni et al., 2002), evaluated using the same English tokenization used in tr"
N13-1003,D08-1024,0,0.364915,"frequent words or wordclusters can improve translation quality, with boosts of up to 1.2 BLEU points in ChineseEnglish and 1.8 in Arabic-English. We compare this solution to a more traditional maximum entropy approach, where a probability model with similar features is trained on wordaligned bitext. We show that sparse decoder features outperform maximum entropy handily, indicating that there are major advantages to optimizing reordering features directly for BLEU with the decoder in the loop. 1 Introduction With the growing adoption of tuning algorithms that can handle thousands of features (Chiang et al., 2008; Hopkins and May, 2011), SMT system designers now face a choice when incorporating new ideas into their translation models. Maximum likelihood models can be estimated from large wordaligned bitexts, creating a small number of highly informative decoder features; or the same ideas can be incorporated into the decoder’s linear model directly. There are trade-offs to each approach. Maximum likelihood models can be estimated from millions of sentences of bitext, but optimize a mismatched objective, predicting events observed in word aligned bitext instead of optimizing translation quality. Sparse"
N13-1003,N09-1025,0,0.27787,"intuition that most lexicalized reordering models do not smooth their orientation distributions intelligently for low-frequency phrase-pairs, we design features that track the first and last words (or clusters) of the phrases in a pair. These features are incorporated into a maximum entropy reordering model, as well as sparse decoder features, to see which approach best complements the now-standard relative-frequency lexicalized reordering model. We also view our work as an example of strong sparse features for phrase-based translation. Features from hierarchical and syntax-based translation (Chiang et al., 2009) do not easily transfer to the phrase-based paradigm, and most work that has looked at large feature counts in the context of phrase-based translation has focused on the learning method, and not the features themselves (Hopkins and May, 2011; Cherry and Foster, 2012; Gimpel and Smith, 2012). We show that by targeting reordering, large gains can be made with relatively simple features. 2 Background Phrase-based machine translation constructs its target sentence from left-to-right, with each translation operation selecting a source phrase and appending its translation to the growing target sente"
N13-1003,P11-2031,0,0.00857079,"Moore and Quirk (2007). Our multi-stack phrase-based decoder is quite similar to Moses (Koehn et al., 2007). For all systems, parameters are tuned with a batch-lattice variant of hope-fear MIRA (Chiang et al., 2008; Cherry and Foster, 2012). Preliminary experiments suggested that the sparse reordering features have a larger impact when tuned with lattices as opposed to n-best lists. 4.2 We report lower-cased BLEU (Papineni et al., 2002), evaluated using the same English tokenization used in training. For our primary results, we perform random replications of parameter tuning, as suggested by Clark et al. (2011). Each replication uses a different random seed to determine the order in which MIRA visits tuning sentences. We test for significance using Clark et al.’s MultEval tool, which uses a stratified approximate randomization test to account for multiple replications. 5 Base: src.first; src.last; tgt.first; tgt.last × Representation {80-words, 50-clusters} × Orientation {M, S, D} Evaluation Results We begin with a comparison of the reordering models described in this paper: the hierarchical reordering model (HRM), the maximum entropy HRM (Maxent HRM) and our sparse reordering features (Sparse HRM)."
N13-1003,W07-0717,0,0.0471104,"has been designed for both translation quality and replicability. We now investigate the impact of our Sparse HRM on a far more complex baseline: our internal system used for MT competitions such as NIST. The Arabic system uses roughly the same bilingual data as our original baseline, but also includes a 5-gram language model learned from the English Gigaword. The Chinese system adds the UN bitext as well as the English Gigaword. Both systems make heavy use of linear mixtures to create refined translation and language models, mixing across sources of corpora, genre and translation direction (Foster and Kuhn, 2007; Goutte et al., 2009). They also mix many different sources of word alignments, with the system adapting across alignment sources using either binary indicators or linear mixtures. Importantly, these systems already incorporate thousands of sparse features as described by Hopkins and May (2011). These provide additional information for each phrase pair through frequency bins, phraselength bins, and indicators for frequent alignment pairs. Both systems include a standard HRM. The result of adding the Sparse HRM to these systems is shown in Table 9. Improvements range from 0.4 to 1.1 BLEU, but"
N13-1003,D08-1089,0,0.511554,"ght-toleft. Most decoders incorporate RMs for both directions; our discussion will generally only cover leftto-right, with the right-to-left case being implicit and symmetrical. As the decoder extends its hypothesis by translating a source phrase, we can assess the implied orientations to determine if the resulting reordering makes sense. This is done using the probability of an orientation given the phrase pair pp = [src, tgt] extending the hypothesis:2 performance is achieved by giving each orientation its own log-linear weight (Koehn et al., 2005). 2.2 Hierarchical Reordering Introduced by Galley and Manning (2008), the hierarchical reordering model (HRM) also tracks statistics over orientations, but attempts to increase the consistency of orientation assignments. To do so, they remove the emphasis on the previously translated phrase (prev ), and instead determine orientation using a compact representation of the full translation history, as represented by a shift-reduce stack. Each source span is shifted onto the stack as it is translated; if the new top is adjacent to the span below it, then a reduction merges the two. Orientations are determined in terms of the top of this stack,3 rather than the pre"
N13-1003,N12-1023,0,0.0069909,"opy reordering model, as well as sparse decoder features, to see which approach best complements the now-standard relative-frequency lexicalized reordering model. We also view our work as an example of strong sparse features for phrase-based translation. Features from hierarchical and syntax-based translation (Chiang et al., 2009) do not easily transfer to the phrase-based paradigm, and most work that has looked at large feature counts in the context of phrase-based translation has focused on the learning method, and not the features themselves (Hopkins and May, 2011; Cherry and Foster, 2012; Gimpel and Smith, 2012). We show that by targeting reordering, large gains can be made with relatively simple features. 2 Background Phrase-based machine translation constructs its target sentence from left-to-right, with each translation operation selecting a source phrase and appending its translation to the growing target sentence, until 1 Some systems tune for BLEU on much larger sets (Simianer et al., 2012; He and Deng, 2012), but these require exceptional commitments of resources and time. 22 Proceedings of NAACL-HLT 2013, pages 22–31, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Ling"
N13-1003,2009.eamt-smart.7,0,0.0236552,"both translation quality and replicability. We now investigate the impact of our Sparse HRM on a far more complex baseline: our internal system used for MT competitions such as NIST. The Arabic system uses roughly the same bilingual data as our original baseline, but also includes a 5-gram language model learned from the English Gigaword. The Chinese system adds the UN bitext as well as the English Gigaword. Both systems make heavy use of linear mixtures to create refined translation and language models, mixing across sources of corpora, genre and translation direction (Foster and Kuhn, 2007; Goutte et al., 2009). They also mix many different sources of word alignments, with the system adapting across alignment sources using either binary indicators or linear mixtures. Importantly, these systems already incorporate thousands of sparse features as described by Hopkins and May (2011). These provide additional information for each phrase pair through frequency bins, phraselength bins, and indicators for frequent alignment pairs. Both systems include a standard HRM. The result of adding the Sparse HRM to these systems is shown in Table 9. Improvements range from 0.4 to 1.1 BLEU, but importantly, all four"
N13-1003,N10-1129,0,0.142807,"ntactic heads and constituent labels to create a rich feature set. They show gains over an HRM baseline, albeit on a small training set. A related approach is to build a reordering model over words, which is evaluated at phrase boundaries at decoding time. Zens and Ney (2006) propose one such model, with jumps between words binned very coarsely according to their direction and distance, testing models that differentiate only left jumps from right, as well as the cross-product of {left, right} × {adjacent, discontinuous}. Their features consider word identity and automaticallyinduced clusters. Green et al. (2010) present a similar approach, with finer-grained distance bins, using word-identity and part-of-speech for features. Yahyaei and Monz (2010) also predict distance bins, but use much more context, opting to look at both sides of a reordering jump; they also experiment with hard constraints based on their model. Tracking word-level reordering simplifies the extraction of complex models from word alignments; however, it is not clear if it is possible to enhance a word reordering model with the stack-based histories used by HRMs. In this work, we construct a phrase orientation maximum entropy model"
N13-1003,P12-1031,0,0.0226142,"at large feature counts in the context of phrase-based translation has focused on the learning method, and not the features themselves (Hopkins and May, 2011; Cherry and Foster, 2012; Gimpel and Smith, 2012). We show that by targeting reordering, large gains can be made with relatively simple features. 2 Background Phrase-based machine translation constructs its target sentence from left-to-right, with each translation operation selecting a source phrase and appending its translation to the growing target sentence, until 1 Some systems tune for BLEU on much larger sets (Simianer et al., 2012; He and Deng, 2012), but these require exceptional commitments of resources and time. 22 Proceedings of NAACL-HLT 2013, pages 22–31, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics all source words have been covered exactly once. The first phrase-based translation systems applied only a distortion penalty to model reordering (Koehn et al., 2003; Och and Ney, 2004). Any deviation from monotone translation is penalized, with a single linear weight determining how quickly the penalty grows. 2.1 Lexicalized Reordering Implemented in a number of phrase-based decoders, the lexicalize"
N13-1003,D11-1125,0,0.511884,"dclusters can improve translation quality, with boosts of up to 1.2 BLEU points in ChineseEnglish and 1.8 in Arabic-English. We compare this solution to a more traditional maximum entropy approach, where a probability model with similar features is trained on wordaligned bitext. We show that sparse decoder features outperform maximum entropy handily, indicating that there are major advantages to optimizing reordering features directly for BLEU with the decoder in the loop. 1 Introduction With the growing adoption of tuning algorithms that can handle thousands of features (Chiang et al., 2008; Hopkins and May, 2011), SMT system designers now face a choice when incorporating new ideas into their translation models. Maximum likelihood models can be estimated from large wordaligned bitexts, creating a small number of highly informative decoder features; or the same ideas can be incorporated into the decoder’s linear model directly. There are trade-offs to each approach. Maximum likelihood models can be estimated from millions of sentences of bitext, but optimize a mismatched objective, predicting events observed in word aligned bitext instead of optimizing translation quality. Sparse decoder features have t"
N13-1003,2012.eamt-1.66,0,0.0334877,"Missing"
N13-1003,N03-1017,0,0.157988,"ts target sentence from left-to-right, with each translation operation selecting a source phrase and appending its translation to the growing target sentence, until 1 Some systems tune for BLEU on much larger sets (Simianer et al., 2012; He and Deng, 2012), but these require exceptional commitments of resources and time. 22 Proceedings of NAACL-HLT 2013, pages 22–31, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics all source words have been covered exactly once. The first phrase-based translation systems applied only a distortion penalty to model reordering (Koehn et al., 2003; Och and Ney, 2004). Any deviation from monotone translation is penalized, with a single linear weight determining how quickly the penalty grows. 2.1 Lexicalized Reordering Implemented in a number of phrase-based decoders, the lexicalized reordering model (RM) uses wordaligned data to determine how each phrase-pair tends to be reordered during translation (Tillmann, 2004; Koehn et al., 2005; Koehn et al., 2007). The core idea in this RM is to divide reordering events into three orientations that can be easily determined both during decoding and from word-aligned data. The orientations can be"
N13-1003,2005.iwslt-1.8,0,0.0271184,"June 2013. 2013 Association for Computational Linguistics all source words have been covered exactly once. The first phrase-based translation systems applied only a distortion penalty to model reordering (Koehn et al., 2003; Och and Ney, 2004). Any deviation from monotone translation is penalized, with a single linear weight determining how quickly the penalty grows. 2.1 Lexicalized Reordering Implemented in a number of phrase-based decoders, the lexicalized reordering model (RM) uses wordaligned data to determine how each phrase-pair tends to be reordered during translation (Tillmann, 2004; Koehn et al., 2005; Koehn et al., 2007). The core idea in this RM is to divide reordering events into three orientations that can be easily determined both during decoding and from word-aligned data. The orientations can be described in terms of the previously translated source phrase (prev) and the next source phrase to be translated (next): • Monotone (M): next immediately follows prev. • Swap (S): prev immediately follows next. • Discontinuous (D): next and prev are not adjacent in the source. Note that prev and next can be defined for constructing a translation from left-to-right or from right-toleft. Most"
N13-1003,P07-2045,0,0.0172142,"ociation for Computational Linguistics all source words have been covered exactly once. The first phrase-based translation systems applied only a distortion penalty to model reordering (Koehn et al., 2003; Och and Ney, 2004). Any deviation from monotone translation is penalized, with a single linear weight determining how quickly the penalty grows. 2.1 Lexicalized Reordering Implemented in a number of phrase-based decoders, the lexicalized reordering model (RM) uses wordaligned data to determine how each phrase-pair tends to be reordered during translation (Tillmann, 2004; Koehn et al., 2005; Koehn et al., 2007). The core idea in this RM is to divide reordering events into three orientations that can be easily determined both during decoding and from word-aligned data. The orientations can be described in terms of the previously translated source phrase (prev) and the next source phrase to be translated (next): • Monotone (M): next immediately follows prev. • Swap (S): prev immediately follows next. • Discontinuous (D): next and prev are not adjacent in the source. Note that prev and next can be defined for constructing a translation from left-to-right or from right-toleft. Most decoders incorporate"
N13-1003,P08-1068,0,0.0209883,"ed from small tuning sets. The standard approach for sparse feature design in SMT is to lexicalize only on extremely frequent words, such as the top-80 words from each language (Chiang et al., 2009; Hopkins and May, 2011). We take that approach here, but we also use deterministic clusters to represent words from both languages, as provided by mkcls. These clusters mirror parts-of-speech quite effectively (Blunsom and Cohn, 2011), without requiring linguistic resources. They should provide useful generalization for reordering decisions. Inspired by recent successes in semi-supervised learning (Koo et al., 2008; corpus train dev mt08 mt09 sentences 1,490,514 1,663 1,360 1,313 words (ar) 46,403,734 45,243 45,002 40,684 corpus train dev mt06 mt08 words (en) 47,109,486 50,550 51,341 46,813 Table 3: Arabic-English Corpus. For English dev and test sets, word counts are averaged across 4 references. Lin and Wu, 2009), we cluster at two granularities (20 clusters and 50 clusters), and allow the discriminative tuner to determine how to best employ the various representations. We add the sparse features in Table 2 to our decoder to help assess reordering decisions. As with the maximum entropy model, orientat"
N13-1003,P09-1116,0,0.0166583,"rds from both languages, as provided by mkcls. These clusters mirror parts-of-speech quite effectively (Blunsom and Cohn, 2011), without requiring linguistic resources. They should provide useful generalization for reordering decisions. Inspired by recent successes in semi-supervised learning (Koo et al., 2008; corpus train dev mt08 mt09 sentences 1,490,514 1,663 1,360 1,313 words (ar) 46,403,734 45,243 45,002 40,684 corpus train dev mt06 mt08 words (en) 47,109,486 50,550 51,341 46,813 Table 3: Arabic-English Corpus. For English dev and test sets, word counts are averaged across 4 references. Lin and Wu, 2009), we cluster at two granularities (20 clusters and 50 clusters), and allow the discriminative tuner to determine how to best employ the various representations. We add the sparse features in Table 2 to our decoder to help assess reordering decisions. As with the maximum entropy model, orientation is appended to each feature. Furthermore, each feature has a different version for each of our three word representations. Like the maximum entropy model, we describe the phrase pair being added to the hypothesis in terms of the first and last words of its phrases. Unlike the maximum entropy model, we"
N13-1003,2007.mtsummit-papers.43,0,0.0607854,"). Conditional phrase probabilities in both directions are estimated from relative frequencies, and from lexical probabilities (Zens and Ney, 2004). 4-gram language models are estimated from the target side of the bitext with Kneser-Ney smoothing. Relative frequency and maximum entropy RMs are represented with six features, with separate weights for M, S and D in both directions (Koehn et al., 2007). HRM orientations are determined using an unrestricted shiftreduce parser (Cherry et al., 2012). We also employ a standard distortion penalty incorporating the minimum completion cost described by Moore and Quirk (2007). Our multi-stack phrase-based decoder is quite similar to Moses (Koehn et al., 2007). For all systems, parameters are tuned with a batch-lattice variant of hope-fear MIRA (Chiang et al., 2008; Cherry and Foster, 2012). Preliminary experiments suggested that the sparse reordering features have a larger impact when tuned with lattices as opposed to n-best lists. 4.2 We report lower-cased BLEU (Papineni et al., 2002), evaluated using the same English tokenization used in training. For our primary results, we perform random replications of parameter tuning, as suggested by Clark et al. (2011). Ea"
N13-1003,2009.mtsummit-papers.10,0,0.612857,"Missing"
N13-1003,J03-1002,0,0.00841955,"Missing"
N13-1003,J04-4002,0,0.35006,"rom left-to-right, with each translation operation selecting a source phrase and appending its translation to the growing target sentence, until 1 Some systems tune for BLEU on much larger sets (Simianer et al., 2012; He and Deng, 2012), but these require exceptional commitments of resources and time. 22 Proceedings of NAACL-HLT 2013, pages 22–31, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics all source words have been covered exactly once. The first phrase-based translation systems applied only a distortion penalty to model reordering (Koehn et al., 2003; Och and Ney, 2004). Any deviation from monotone translation is penalized, with a single linear weight determining how quickly the penalty grows. 2.1 Lexicalized Reordering Implemented in a number of phrase-based decoders, the lexicalized reordering model (RM) uses wordaligned data to determine how each phrase-pair tends to be reordered during translation (Tillmann, 2004; Koehn et al., 2005; Koehn et al., 2007). The core idea in this RM is to divide reordering events into three orientations that can be easily determined both during decoding and from word-aligned data. The orientations can be described in terms o"
N13-1003,E99-1010,0,0.503433,"elative frequency estimates for very frequent phrase pairs, and will smooth intelligently using features for less frequent phrase pairs. All of the features returned by f (o|pp) are derived from the phrase pair pp = [src, tgt], with the goal of describing the phrase pair at a variety of granularities. Our features are described in Table 1, using the following notation: the operators first and last return the first and last words of phrases,6 while the operator clust 50 maps a word onto its corresponding cluster from an automatically-induced, deterministic 50-word clustering provided by mkcls (Och, 1999). Our use of words at the corners of phrases (as opposed to the syntactic head, or the last aligned word) follows Xiong et al. (2006), while our use of word clusters follows Zens and Ney (2006). Each feature has the orientation o appended onto it. To help scale and to encourage smoothing, we only allow features that occur in at least 5 phrase pair X 1 ||w||2 +C ci 2 i P # tokens. Furthermore, to deal with the huge number 5 Preliminary experiments indicated that the model is robust to the choice of C; we use C = 0.1 throughout. 6 first = last for a single-word phrase 25 of extracted phrase pair"
N13-1003,P02-1040,0,0.104645,"ns are determined using an unrestricted shiftreduce parser (Cherry et al., 2012). We also employ a standard distortion penalty incorporating the minimum completion cost described by Moore and Quirk (2007). Our multi-stack phrase-based decoder is quite similar to Moses (Koehn et al., 2007). For all systems, parameters are tuned with a batch-lattice variant of hope-fear MIRA (Chiang et al., 2008; Cherry and Foster, 2012). Preliminary experiments suggested that the sparse reordering features have a larger impact when tuned with lattices as opposed to n-best lists. 4.2 We report lower-cased BLEU (Papineni et al., 2002), evaluated using the same English tokenization used in training. For our primary results, we perform random replications of parameter tuning, as suggested by Clark et al. (2011). Each replication uses a different random seed to determine the order in which MIRA visits tuning sentences. We test for significance using Clark et al.’s MultEval tool, which uses a stratified approximate randomization test to account for multiple replications. 5 Base: src.first; src.last; tgt.first; tgt.last × Representation {80-words, 50-clusters} × Orientation {M, S, D} Evaluation Results We begin with a compariso"
N13-1003,P12-1002,0,0.0119838,"t work that has looked at large feature counts in the context of phrase-based translation has focused on the learning method, and not the features themselves (Hopkins and May, 2011; Cherry and Foster, 2012; Gimpel and Smith, 2012). We show that by targeting reordering, large gains can be made with relatively simple features. 2 Background Phrase-based machine translation constructs its target sentence from left-to-right, with each translation operation selecting a source phrase and appending its translation to the growing target sentence, until 1 Some systems tune for BLEU on much larger sets (Simianer et al., 2012; He and Deng, 2012), but these require exceptional commitments of resources and time. 22 Proceedings of NAACL-HLT 2013, pages 22–31, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics all source words have been covered exactly once. The first phrase-based translation systems applied only a distortion penalty to model reordering (Koehn et al., 2003; Och and Ney, 2004). Any deviation from monotone translation is penalized, with a single linear weight determining how quickly the penalty grows. 2.1 Lexicalized Reordering Implemented in a number of phrase-based deco"
N13-1003,N04-4026,0,0.146478,"a, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics all source words have been covered exactly once. The first phrase-based translation systems applied only a distortion penalty to model reordering (Koehn et al., 2003; Och and Ney, 2004). Any deviation from monotone translation is penalized, with a single linear weight determining how quickly the penalty grows. 2.1 Lexicalized Reordering Implemented in a number of phrase-based decoders, the lexicalized reordering model (RM) uses wordaligned data to determine how each phrase-pair tends to be reordered during translation (Tillmann, 2004; Koehn et al., 2005; Koehn et al., 2007). The core idea in this RM is to divide reordering events into three orientations that can be easily determined both during decoding and from word-aligned data. The orientations can be described in terms of the previously translated source phrase (prev) and the next source phrase to be translated (next): • Monotone (M): next immediately follows prev. • Swap (S): prev immediately follows next. • Discontinuous (D): next and prev are not adjacent in the source. Note that prev and next can be defined for constructing a translation from left-to-right or from"
N13-1003,P06-1066,0,0.284409,"es are based on a single observation. Because of these heavy tails, there have been several attempts to use maximum entropy to create more flexible distributions. One straight-forward way to do so is to continue predicting orientations on phrases, but to use maxi2 pp corresponds to the phrase pair translating next for the left-to-right model, and prev for right-to-left. 3 In the case of the right-to-left model, an approximation of the top of the stack is used instead. cnt(o, pp) P (o|pp) ≈ P o cnt(o, pp) (1) 23 mum entropy to consider features of the phrase pair. This is the approach taken by Xiong et al. (2006); their maximum entropy model chooses between M and S orientations, which are the only two options available in their chart-based ITG decoder. Nguyen et al. (2009) build a similar model for a phrase-based HRM, using syntactic heads and constituent labels to create a rich feature set. They show gains over an HRM baseline, albeit on a small training set. A related approach is to build a reordering model over words, which is evaluated at phrase boundaries at decoding time. Zens and Ney (2006) propose one such model, with jumps between words binned very coarsely according to their direction and di"
N13-1003,2010.iwslt-papers.19,0,0.137905,". A related approach is to build a reordering model over words, which is evaluated at phrase boundaries at decoding time. Zens and Ney (2006) propose one such model, with jumps between words binned very coarsely according to their direction and distance, testing models that differentiate only left jumps from right, as well as the cross-product of {left, right} × {adjacent, discontinuous}. Their features consider word identity and automaticallyinduced clusters. Green et al. (2010) present a similar approach, with finer-grained distance bins, using word-identity and part-of-speech for features. Yahyaei and Monz (2010) also predict distance bins, but use much more context, opting to look at both sides of a reordering jump; they also experiment with hard constraints based on their model. Tracking word-level reordering simplifies the extraction of complex models from word alignments; however, it is not clear if it is possible to enhance a word reordering model with the stack-based histories used by HRMs. In this work, we construct a phrase orientation maximum entropy model. 3 P (o|pp) = cnt(o, pp) + αs Ps (o|src) + αt Pt (o|tgt) P o cnt(o, pp) + αs + αt P Ps (o|src) = cnt(o, src, tgt) + αg Pg (o) o,tgt cnt(o,"
N13-1003,N04-1033,0,0.0453237,"from the NIST mt05 evaluation set, augmented with some material reserved from our NIST training corpora in order to better cover newsgroup and weblog domains. Its test sets are mt06 and mt08. 4.1 Baseline System For both language pairs, word alignment is performed by GIZA++ (Och and Ney, 2003), with 5 iterations of Model 1, HMM, Model 3 and Model 4. Phrases are extracted with a length limit of 7 from alignments symmetrized using growdiag-final-and (Koehn et al., 2003). Conditional phrase probabilities in both directions are estimated from relative frequencies, and from lexical probabilities (Zens and Ney, 2004). 4-gram language models are estimated from the target side of the bitext with Kneser-Ney smoothing. Relative frequency and maximum entropy RMs are represented with six features, with separate weights for M, S and D in both directions (Koehn et al., 2007). HRM orientations are determined using an unrestricted shiftreduce parser (Cherry et al., 2012). We also employ a standard distortion penalty incorporating the minimum completion cost described by Moore and Quirk (2007). Our multi-stack phrase-based decoder is quite similar to Moses (Koehn et al., 2007). For all systems, parameters are tuned"
N13-1003,W06-3108,0,0.144918,"P o cnt(o, pp) (1) 23 mum entropy to consider features of the phrase pair. This is the approach taken by Xiong et al. (2006); their maximum entropy model chooses between M and S orientations, which are the only two options available in their chart-based ITG decoder. Nguyen et al. (2009) build a similar model for a phrase-based HRM, using syntactic heads and constituent labels to create a rich feature set. They show gains over an HRM baseline, albeit on a small training set. A related approach is to build a reordering model over words, which is evaluated at phrase boundaries at decoding time. Zens and Ney (2006) propose one such model, with jumps between words binned very coarsely according to their direction and distance, testing models that differentiate only left jumps from right, as well as the cross-product of {left, right} × {adjacent, discontinuous}. Their features consider word identity and automaticallyinduced clusters. Green et al. (2010) present a similar approach, with finer-grained distance bins, using word-identity and part-of-speech for features. Yahyaei and Monz (2010) also predict distance bins, but use much more context, opting to look at both sides of a reordering jump; they also e"
N13-2007,P08-2039,0,0.354486,"ent, rule extraction and decoding. When translating from Arabic into English, the tokenization is a form of preprocessing, and the output translation is readable, space-separated English. However, when translating from English to Arabic, the output will be in a tokenized form, which cannot be compared to the original reference without detokenization. Simply concatenating the tokenized morphemes cannot fully reverse this process, because of character transformations that occurred during tokenization. The techniques that have been proposed for the detokenization task fall into three categories (Badr et al., 2008). The simplest detokenization approach concatenates morphemes based on token markers without any adjustment. Table-based detokenization maps tokenized words into their surface form with a look-up table built by observing the tokenizer’s inProceedings of the NAACL HLT 2013 Student Research Workshop, pages 47–53, c Atlanta, Georgia, 13 June 2013. 2013 Association for Computational Linguistics 2 Arabic Morphology Figure 1: Alignment between tokenized form of ð and its English translation. “wsymnςhm” ÑêªJÒJ put and output on large amounts of text. Rule-based detokenization relies on hand-built ru"
N13-2007,W12-5611,0,0.0357877,"Missing"
N13-2007,P07-2045,0,0.00583703,"T+R) Disambig (T+LM) Disambig (T+R+LM) D IREC TL+ WER 1.710 0.590 0.192 0.122 0.164 0.094 0.087 SER 34.3 14.0 4.9 3.2 4.1 2.4 2.1 BLEU 26.30 28.32 28.54 28.55 28.53 28.54 28.55 Table 3: Word and sentence error rate of detokenization schemes on the Arabic reference text of NIST MT05. BLEU score refers to English-Arabic SMT output. effectively memorize many words. We found these settings using grid search on the development set, NIST MT04. For the SMT experiment, we use GIZA++ for the alignment between English and tokenized Arabic, and perform the translation using Moses phrasebased SMT system (Hoang et al., 2007), with a maximum phrase length of 5. We apply each detokenization scheme on the SMT tokenized Arabic output test set, and evaluate using the BLEU score (Papineni et al., 2002). 5.3 Results Table 3 shows the performance of several detokenization schemes. For evaluation, we use the sentence and word error rates on naturally occurring Arabic text, and BLEU score on tokenized Arabic output of the SMT system. The baseline scheme, which is a simple concatenation of morphemes, introduces errors in over a third of all sentences. The table-based approach outperforms the rule-based approach, indicating"
N13-2007,N07-1047,1,0.86606,"s. The set of pairs is initially aligned on the character level, and the alignment pairs become the operations that are applied during transduction. For detokenization, most operations simply copy over characters, but more complex rules such as l+ Al → ll are learned from the training data as well. The tool that we use to perform the transduction is D IREC TL+, a discriminative, character-level string transducer, which was originally designed for letterto-phoneme conversion (Jiampojamarn et al., 2008). To align the characters in each training example. D IREC TL+ uses an EM-based M2M-A LIGNER (Jiampojamarn et al., 2007). After alignment is complete, MIRA training repeatedly decodes the training set to tune the features that determine when each operation should be applied. The features include both n-gram source context and HMM-style target transitions. D IREC TL+ employs a fully discriminative decoder to learn character transformations and when they should be applied. The decoder resembles a monotone phrase-based SMT decoder, but is built to allow for hundreds of thousands of features. The following example illustrates how string transduction applies to detokenization. The seg Q.K. mented and surface forms"
N13-2007,P08-1103,1,0.848711,"nization as a string transduction task. We train a discriminative transducer on a set of tokenized-detokenized word pairs. The set of pairs is initially aligned on the character level, and the alignment pairs become the operations that are applied during transduction. For detokenization, most operations simply copy over characters, but more complex rules such as l+ Al → ll are learned from the training data as well. The tool that we use to perform the transduction is D IREC TL+, a discriminative, character-level string transducer, which was originally designed for letterto-phoneme conversion (Jiampojamarn et al., 2008). To align the characters in each training example. D IREC TL+ uses an EM-based M2M-A LIGNER (Jiampojamarn et al., 2007). After alignment is complete, MIRA training repeatedly decodes the training set to tune the features that determine when each operation should be applied. The features include both n-gram source context and HMM-style target transitions. D IREC TL+ employs a fully discriminative decoder to learn character transformations and when they should be applied. The decoder resembles a monotone phrase-based SMT decoder, but is built to allow for hundreds of thousands of features. The"
N13-2007,P02-1040,0,0.0867771,"8.55 Table 3: Word and sentence error rate of detokenization schemes on the Arabic reference text of NIST MT05. BLEU score refers to English-Arabic SMT output. effectively memorize many words. We found these settings using grid search on the development set, NIST MT04. For the SMT experiment, we use GIZA++ for the alignment between English and tokenized Arabic, and perform the translation using Moses phrasebased SMT system (Hoang et al., 2007), with a maximum phrase length of 5. We apply each detokenization scheme on the SMT tokenized Arabic output test set, and evaluate using the BLEU score (Papineni et al., 2002). 5.3 Results Table 3 shows the performance of several detokenization schemes. For evaluation, we use the sentence and word error rates on naturally occurring Arabic text, and BLEU score on tokenized Arabic output of the SMT system. The baseline scheme, which is a simple concatenation of morphemes, introduces errors in over a third of all sentences. The table-based approach outperforms the rule-based approach, indicating that there are frequent exceptions to the rules in Table 1 that require memorization. Their combination (T+R) fares better, leveraging the strengths of both approaches. The ad"
N13-2007,P06-1001,0,0.0186785,"fferent forms of Hamzated Alif “ @ @” are usually written without the Hamza “ Z”. Likewise, when the letter Ya ’Y’ ø is present at the end of the word, it is sometimes written in the form of “Alif Maqsura” letter ’ý’ ø. Also, short vowels in Arabic are represented using diacritics, which are usually absent in written text. In order to deal with these ambiguities in SMT, normalization is often performed as a preprocessing step, which usually involves converting different forms of Alif and Ya to a single form. This decreases Arabic’s lexical sparsity and improves SMT performance. 3 Related Work Sadat and Habash (2006) address the issue of lexical sparsity by presenting different preprocessing schemes for Arabic-to-English SMT. The schemes include simple tokenization, orthographic normalization, and decliticization. The combination of these schemes results in improved translation out2 We use Habash-Soudi-Buckwalter transliteration scheme (Habash, 2007) for all Arabic examples. put. This is one of many studies on normalization and tokenization for translation from Arabic, which we will not attempt to review completely here. Badr et al. (2008) show that tokenizing Arabic also has a positive influence on Engli"
N15-1075,P11-1087,0,0.0109198,"res in φ(s, t, yj , x). brn(n, xi ) maps a word xi to the first n bits of its Brown cluster bit sequence. w2v(n, xi ) maps xi to the nth component of its word vector, and [str] = v stands for a realvalued feature with name str and value v. Brown Clusters The Brown clustering algorithm assigns types to a deterministic, hierarchical clustering, which has been trained to optimize the likelihood of a firstorder, class-based language model (Brown et al., 1992). The clusters capture both syntactic and semantic regularities, and have been shown to perform well as unsupervised part-of-speech taggers (Blunsom and Cohn, 2011). The clusters are organized into a binary tree structure; therefore, each cluster can be represented as a bit string that encodes the branching decisions required to reach its leaf from the root. By truncating the bit string at different prefix lengths, one can access different granularities of clusters. Cluster membership can then be used to create indicators similar to the baseline’s word identity features. This results in two feature templates, shown in Table 2.2 This technique has been previously applied to both newswire NER (Miller et al., 2004; Turian et al., 2010; Passos et al., 2014)"
N15-1075,J92-4003,0,0.0507589,"4,8,12} Word vectors, for each i s.t. s ≤ i &lt; t: {[yj , n] = w2v(n, xi )}300 n=1 , {[yj , ers,t (i), n] = w2v(n, xi )}300 n=1 Table 2: Word representation features in φ(s, t, yj , x). brn(n, xi ) maps a word xi to the first n bits of its Brown cluster bit sequence. w2v(n, xi ) maps xi to the nth component of its word vector, and [str] = v stands for a realvalued feature with name str and value v. Brown Clusters The Brown clustering algorithm assigns types to a deterministic, hierarchical clustering, which has been trained to optimize the likelihood of a firstorder, class-based language model (Brown et al., 1992). The clusters capture both syntactic and semantic regularities, and have been shown to perform well as unsupervised part-of-speech taggers (Blunsom and Cohn, 2011). The clusters are organized into a binary tree structure; therefore, each cluster can be represented as a bit string that encodes the branching decisions required to reach its leaf from the root. By truncating the bit string at different prefix lengths, one can access different granularities of clusters. Cluster membership can then be used to create indicators similar to the baseline’s word identity features. This results in two fe"
N15-1075,H05-1091,0,0.0196291,"e-art on two common test sets. Though it is wellknown that word representations are useful for NER, supporting experiments have thus far focused on newswire data. We emphasize the effectiveness of representations on Twitter NER, and demonstrate that their inclusion can improve performance by up to 20 F1. 1 Introduction Named entity recognition (NER) is the task of finding rigid designators as they appear in free text and classifying them into coarse categories such as person or location (Nadeau and Sekine, 2007). NER enables many other information extraction tasks such as relation extraction (Bunescu and Mooney, 2005) and entity linking (Ratinov et al., 2011). There is considerable excitement at the prospect of porting information extraction technology to social media platforms such as Twitter. Social media reacts to world events faster than traditional news sources, and its sub-communities pay close attention to topics that other sources might ignore. An early example of the potential inherent in social information extraction is the Twitter Calendar (Ritter et al., 2012), which detects upcoming events (concerts, elections, video game releases, etc.) based on the anticipatory chatter of Twitter users. Unfo"
N15-1075,W02-1001,0,0.439658,"h continuous and cluster-based word representations on Twitter NER, emphasizing the dramatic improvement that they bring. We also bring the experimental methodology of the domain adaptation community to Twitter NER, testing indomain, out-of-domain and combined training scenarios, and revealing that it is not trivial to benefit from out-of-domain training data. Finally, an error analysis helps us begin to understand which social media challenges are being addressed by our adaptations, and which problems persist. 2 Background Our work builds on a long line of research in discriminative tagging (Collins, 2002), and its application to named entity recognition (McCallum and Li, 2003). Our baseline tagger draws inspiration from Sarawagi and Cohen (2004), who introduce the no735 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 735–745, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics tion of semi-Markov tagging for NER, and from de Bruijn et al. (2011), who apply a similar tagger to clinical information extraction. A number of previous studies have closely examined the use of word representations in NER, whe"
N15-1075,W10-0713,0,0.272137,"Missing"
N15-1075,fromreide-etal-2014-crowdsourcing,0,0.702591,"a boot-strapping scheme for semi-supervised learning. Interestingly, they find no utility in using cluster-based word representations, perhaps because their model directly accounts for a type’s global context with bag-of-word features. Ritter et al. (2011) also examine Twitter NER, developing a semi-supervised technique that uses labeled LDA to project information from Freebase gazetteers onto unlabeled tweets. Plank et al. (2014) suggest a distant-supervision scheme, creating artificial training data by projecting reliable NER tags from web pages onto the tweets that link to those pages. 736 Fromreide et al. (2014) and Plank et al. (2014) point out that NER performance can be overestimated when a system is tested on data extracted from the same pool as its training data. Temporal effects and annotation biases can result in gains that disappear when shifting to another test set. We follow their lead by testing on data that was annotated independently from our training data. 3 Methods Our named entity recognizer is a discriminative, semi-Markov tagger, trained online using largemargin updates. It differs from word-based CRF systems in three ways: its inference algorithm, its tag structure, and its learnin"
N15-1075,N13-1132,0,0.00522225,"eir baseline includes CoNLL and Twitter data, and uses Brown clusters trained on a comparable number of unlabeled tweets. Their strongest system uses distant supervision over linked web-pages to create artificial training data. But we are able to outperform it with our vector representations and importance weights. Note that this comparison is not perfect, as they train on a much larger pool of crowd-sourced, NER-annotated tweets, consisting of 170k tokens compared to our 17k. The size of their training data is balanced by the fact that its annotations were automatically correctly using MACE (Hovy et al., 2013), where ours were corrected manually, making it unclear which group has the advantage. Nonetheless, our results establish a new state-of-the-art for both test sets, and they do so using only 1k annotated tweets. 6 Analysis We inspected 100 tweets from the Rit11 test set, focusing on the output from our primary system, Base+Reps+Weights, and our baseline, Base, both trained on the CoNLL+Fin10 data. We noted cases where the primary system improved upon the baseline, and cases where it failed to achieve the goldstandard, and placed the phenomena we observed into bins. In general, the baseline was"
N15-1075,P09-1116,0,0.072423,"e North American Chapter of the ACL, pages 735–745, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics tion of semi-Markov tagging for NER, and from de Bruijn et al. (2011), who apply a similar tagger to clinical information extraction. A number of previous studies have closely examined the use of word representations in NER, where one leverages unlabeled data to build features that help the tagger generalize across similar words. Miller et al. (2004) introduce this idea and provide the framework to build representation features from word clusters, while Lin and Wu (2009) extend this technique with phrases and sheer masses of unlabeled data. Turian et al. (2010) introduce continuous vectors as alternative word representations, and provide several experiments comparing these with clusters. Recently, Passos et al. (2014) have shown how continuous representations can be tailored to NER with a combination of context- and gazetteer-aware objectives. All of these studies employ representations only in newswire scenarios. Ratinov and Roth (2009) investigate cluster representations in a Web NER task, but the performance of their baseline indicates that it is not nearl"
N15-1075,P11-1037,0,0.136875,"(2009) investigate cluster representations in a Web NER task, but the performance of their baseline indicates that it is not nearly so drastic a domain shift as our Twitter task. 2.1 Adapting to Social Media There has been much recent activity in adapting NLP tools for social media. Ritter et al. (2011) collect training data and adapt tools for a number of tasks, including part-of-speech (POS) tagging, shallow parsing and NER. Owoputi et al. (2013) extends a line of research on building robust POS taggers for Twitter, and share our focus on the utility of word representations in this domain. Liu et al. (2011) carry out the first study to specifically examine NER on Twitter. They use a nearestneighbour word classifier stacked with a CRF, along with a boot-strapping scheme for semi-supervised learning. Interestingly, they find no utility in using cluster-based word representations, perhaps because their model directly accounts for a type’s global context with bag-of-word features. Ritter et al. (2011) also examine Twitter NER, developing a semi-supervised technique that uses labeled LDA to project information from Freebase gazetteers onto unlabeled tweets. Plank et al. (2014) suggest a distant-super"
N15-1075,W03-0430,0,0.0476041,"R, emphasizing the dramatic improvement that they bring. We also bring the experimental methodology of the domain adaptation community to Twitter NER, testing indomain, out-of-domain and combined training scenarios, and revealing that it is not trivial to benefit from out-of-domain training data. Finally, an error analysis helps us begin to understand which social media challenges are being addressed by our adaptations, and which problems persist. 2 Background Our work builds on a long line of research in discriminative tagging (Collins, 2002), and its application to named entity recognition (McCallum and Li, 2003). Our baseline tagger draws inspiration from Sarawagi and Cohen (2004), who introduce the no735 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 735–745, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics tion of semi-Markov tagging for NER, and from de Bruijn et al. (2011), who apply a similar tagger to clinical information extraction. A number of previous studies have closely examined the use of word representations in NER, where one leverages unlabeled data to build features that help the tagger ge"
N15-1075,N04-1043,0,0.733785,"piration from Sarawagi and Cohen (2004), who introduce the no735 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 735–745, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics tion of semi-Markov tagging for NER, and from de Bruijn et al. (2011), who apply a similar tagger to clinical information extraction. A number of previous studies have closely examined the use of word representations in NER, where one leverages unlabeled data to build features that help the tagger generalize across similar words. Miller et al. (2004) introduce this idea and provide the framework to build representation features from word clusters, while Lin and Wu (2009) extend this technique with phrases and sheer masses of unlabeled data. Turian et al. (2010) introduce continuous vectors as alternative word representations, and provide several experiments comparing these with clusters. Recently, Passos et al. (2014) have shown how continuous representations can be tailored to NER with a combination of context- and gazetteer-aware objectives. All of these studies employ representations only in newswire scenarios. Ratinov and Roth (2009)"
N15-1075,N13-1039,0,0.0301307,"Missing"
N15-1075,W14-1609,0,0.0385702,"al information extraction. A number of previous studies have closely examined the use of word representations in NER, where one leverages unlabeled data to build features that help the tagger generalize across similar words. Miller et al. (2004) introduce this idea and provide the framework to build representation features from word clusters, while Lin and Wu (2009) extend this technique with phrases and sheer masses of unlabeled data. Turian et al. (2010) introduce continuous vectors as alternative word representations, and provide several experiments comparing these with clusters. Recently, Passos et al. (2014) have shown how continuous representations can be tailored to NER with a combination of context- and gazetteer-aware objectives. All of these studies employ representations only in newswire scenarios. Ratinov and Roth (2009) investigate cluster representations in a Web NER task, but the performance of their baseline indicates that it is not nearly so drastic a domain shift as our Twitter task. 2.1 Adapting to Social Media There has been much recent activity in adapting NLP tools for social media. Ritter et al. (2011) collect training data and adapt tools for a number of tasks, including part-o"
N15-1075,C14-1168,0,0.462598,"entations in this domain. Liu et al. (2011) carry out the first study to specifically examine NER on Twitter. They use a nearestneighbour word classifier stacked with a CRF, along with a boot-strapping scheme for semi-supervised learning. Interestingly, they find no utility in using cluster-based word representations, perhaps because their model directly accounts for a type’s global context with bag-of-word features. Ritter et al. (2011) also examine Twitter NER, developing a semi-supervised technique that uses labeled LDA to project information from Freebase gazetteers onto unlabeled tweets. Plank et al. (2014) suggest a distant-supervision scheme, creating artificial training data by projecting reliable NER tags from web pages onto the tweets that link to those pages. 736 Fromreide et al. (2014) and Plank et al. (2014) point out that NER performance can be overestimated when a system is tested on data extracted from the same pool as its training data. Temporal effects and annotation biases can result in gains that disappear when shifting to another test set. We follow their lead by testing on data that was annotated independently from our training data. 3 Methods Our named entity recognizer is a di"
N15-1075,W09-1119,0,0.59667,"ds. Miller et al. (2004) introduce this idea and provide the framework to build representation features from word clusters, while Lin and Wu (2009) extend this technique with phrases and sheer masses of unlabeled data. Turian et al. (2010) introduce continuous vectors as alternative word representations, and provide several experiments comparing these with clusters. Recently, Passos et al. (2014) have shown how continuous representations can be tailored to NER with a combination of context- and gazetteer-aware objectives. All of these studies employ representations only in newswire scenarios. Ratinov and Roth (2009) investigate cluster representations in a Web NER task, but the performance of their baseline indicates that it is not nearly so drastic a domain shift as our Twitter task. 2.1 Adapting to Social Media There has been much recent activity in adapting NLP tools for social media. Ritter et al. (2011) collect training data and adapt tools for a number of tasks, including part-of-speech (POS) tagging, shallow parsing and NER. Owoputi et al. (2013) extends a line of research on building robust POS taggers for Twitter, and share our focus on the utility of word representations in this domain. Liu et"
N15-1075,P11-1138,0,0.0179374,"llknown that word representations are useful for NER, supporting experiments have thus far focused on newswire data. We emphasize the effectiveness of representations on Twitter NER, and demonstrate that their inclusion can improve performance by up to 20 F1. 1 Introduction Named entity recognition (NER) is the task of finding rigid designators as they appear in free text and classifying them into coarse categories such as person or location (Nadeau and Sekine, 2007). NER enables many other information extraction tasks such as relation extraction (Bunescu and Mooney, 2005) and entity linking (Ratinov et al., 2011). There is considerable excitement at the prospect of porting information extraction technology to social media platforms such as Twitter. Social media reacts to world events faster than traditional news sources, and its sub-communities pay close attention to topics that other sources might ignore. An early example of the potential inherent in social information extraction is the Twitter Calendar (Ritter et al., 2012), which detects upcoming events (concerts, elections, video game releases, etc.) based on the anticipatory chatter of Twitter users. Unfortunately, processing social media text pr"
N15-1075,W96-0213,0,0.130183,"rs,t (i), sf(n, xi )]}3n=1 , Table 1: Baseline features φ(s, t, yj , x). [str] stands for an indicator feature with the name str; lc() maps a string onto its lowercased form; ss() maps a string onto its word shape (“Apple Inc.” becomes “Aa Aa.”); pf(n, xi ) and sf(n, xi ) are n-character prefixes and suffixes of xi ; and ers,t (i) maps an absolute sentence position i (s ≤ i &lt; t) to a relative entity position drawn from {B, I, L, U }. ambiguity, constrained so that Outside can tag only single-word spans (t = s + 1). Our baseline feature set, shown in Table 1, closely mimics the set proposed by Ratnaparkhi (1996), covering word identity, prefixes, suffixes and surrounding words. It has been augmented with phraseidentity indicators and hierarchical word-level tags. These conjoin the entity class yj with the word’s entity-relative position, backing off to yj alone. Most features look only at a single word xi , which improves efficiency by allowing the tagger to re-use word-level scores across many phrasal tags. There are some standard NER features that we chose not to include. We follow Lin and Wu (2009) in omitting POS tags and gazetteers in order to reduce our dependence on linguistic resources. We ex"
N15-1075,D11-1141,0,0.90021,"ons, and provide several experiments comparing these with clusters. Recently, Passos et al. (2014) have shown how continuous representations can be tailored to NER with a combination of context- and gazetteer-aware objectives. All of these studies employ representations only in newswire scenarios. Ratinov and Roth (2009) investigate cluster representations in a Web NER task, but the performance of their baseline indicates that it is not nearly so drastic a domain shift as our Twitter task. 2.1 Adapting to Social Media There has been much recent activity in adapting NLP tools for social media. Ritter et al. (2011) collect training data and adapt tools for a number of tasks, including part-of-speech (POS) tagging, shallow parsing and NER. Owoputi et al. (2013) extends a line of research on building robust POS taggers for Twitter, and share our focus on the utility of word representations in this domain. Liu et al. (2011) carry out the first study to specifically examine NER on Twitter. They use a nearestneighbour word classifier stacked with a CRF, along with a boot-strapping scheme for semi-supervised learning. Interestingly, they find no utility in using cluster-based word representations, perhaps bec"
N15-1075,P10-1040,0,0.853639,"2015. 2015 Association for Computational Linguistics tion of semi-Markov tagging for NER, and from de Bruijn et al. (2011), who apply a similar tagger to clinical information extraction. A number of previous studies have closely examined the use of word representations in NER, where one leverages unlabeled data to build features that help the tagger generalize across similar words. Miller et al. (2004) introduce this idea and provide the framework to build representation features from word clusters, while Lin and Wu (2009) extend this technique with phrases and sheer masses of unlabeled data. Turian et al. (2010) introduce continuous vectors as alternative word representations, and provide several experiments comparing these with clusters. Recently, Passos et al. (2014) have shown how continuous representations can be tailored to NER with a combination of context- and gazetteer-aware objectives. All of these studies employ representations only in newswire scenarios. Ratinov and Roth (2009) investigate cluster representations in a Web NER task, but the performance of their baseline indicates that it is not nearly so drastic a domain shift as our Twitter task. 2.1 Adapting to Social Media There has been"
N15-1093,E14-1060,0,0.208948,"ing levels of morphological complexity. In each experiment we either match or improve over the state of the art reported in previous work. In addition to providing a detailed comparison of the available inflection prediction systems, we also contribute four new inflection datasets composed of Dutch and French verbs, and Czech verbs and nouns, which are made available for future research. 923 2 Inflection generation Durrett and DeNero (2013) formulate the specific task of supervised generation of inflected forms for a given base-form based on a large number of training inflection tables, while Ahlberg et al. (2014) test their alternative method on the same Wiktionary dataset. In this section, we compare their work to our approach with respect to the following three subtasks: 1. character-wise alignment of the word-forms in an inflection table (Section 2.1), 2. extraction of rules from aligned forms (2.2), 3. matching of rules to new base-forms (2.3). 2.1 Table alignment The first step in supervised paradigm learning is the alignment of related inflected forms in a table. Though technically a multiple-alignment problem, this can also be addressed by aligning each inflected form to a base-form. Durrett &"
N15-1093,P11-1004,0,0.0483563,"depending on their role in a sentence, and adjectives agree with the nouns that they modify. For such languages, many forms will not be attested even in a large corpus. However, different lemmas often exhibit the same inflectional patterns, called paradigms, which are based on phonological, semantic, or morphological criteria. The paradigm of a given lemma can be identified and used to generate unseen forms. Inflection prediction has the potential to improve Statistical Machine Translation (SMT) into morphologically complex languages. In order to address data sparsity in the training bitext, Clifton and Sarkar (2011) and Fraser et al. (2012) reduce diverse inflected forms in the target language into the corresponding base forms, or lemmas. At test time, they predict an abstract inflection tag for each translated lemma, which is then transformed into a proper word-form. Unfortunately, hand-crafted morphological generators such as the ones that they use for this purpose are available only for a small number of languages, and are expensive to create from scratch. The supervised inflection generation models that we investigate in this paper can instead be trained on publicly available inflection tables. The t"
N15-1093,D11-1057,0,0.0194676,"Missing"
N15-1093,N13-1138,0,0.573516,"n publicly available inflection tables. The task of an inflection generator is to produce an inflected form given a base-form (e.g., an infinitive) and desired inflection, which can be specified as an abstract inflectional tag. The generator is trained on a number of inflection tables, such as the one in Figure 1, which enumerate inflection forms for a given lemma. At test time, the generator predicts inflections for previously unseen base-forms. For example, given the input atmen + 1SIA, where the tag stands for “first person singular indicative preterite,” it should output atmete. Recently, Durrett and DeNero (2013) and Ahlberg 922 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 922–931, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics et al. (2014) have proposed to model inflection generation as a two-stage process: an input base-form is first matched with rules corresponding to a paradigm seen during training, which is then used to generate all inflections for that base-form simultaneously. Although their methods are quite different, both systems account for paradigm-wide regularities by creating rules that"
N15-1093,E12-1068,0,0.0159665,"sentence, and adjectives agree with the nouns that they modify. For such languages, many forms will not be attested even in a large corpus. However, different lemmas often exhibit the same inflectional patterns, called paradigms, which are based on phonological, semantic, or morphological criteria. The paradigm of a given lemma can be identified and used to generate unseen forms. Inflection prediction has the potential to improve Statistical Machine Translation (SMT) into morphologically complex languages. In order to address data sparsity in the training bitext, Clifton and Sarkar (2011) and Fraser et al. (2012) reduce diverse inflected forms in the target language into the corresponding base forms, or lemmas. At test time, they predict an abstract inflection tag for each translated lemma, which is then transformed into a proper word-form. Unfortunately, hand-crafted morphological generators such as the ones that they use for this purpose are available only for a small number of languages, and are expensive to create from scratch. The supervised inflection generation models that we investigate in this paper can instead be trained on publicly available inflection tables. The task of an inflection gene"
N15-1093,N07-1047,1,0.80485,"ed on all inflected word-forms, we derive tag-specific models for each type of inflection. Development experiments showed the general model to be slightly more accurate overall, but we use both types of models in our reranker. 3.3 String alignment D IREC TL+ training requires a set of aligned pairs of source and target strings. The alignments account for every input and output character without the use of insertion. Derivations that transform the input substrings into the desired output substrings are then extracted from the alignments. We induce the alignments by adapting the M2M aligner of (Jiampojamarn et al., 2007), which uses Expectation-Maximization to maximize the joint likelihood of its input under a pairwise alignment scheme. Previous work creates alignments based upon entire inflection tables, while ours considers each inflection paired with its base form independently. M2M goes beyond linking single characters by aligning entire substrings instead. In practice, the base-form serves as a pivot for the entire inflection table, leading to consistent multiple alignments. We modify the M2M aligner to differentiate between stems and affixes. The alignments between stem letters rarely require more than"
N15-1093,N10-1103,1,0.404007,"es into empty strings. During development, we experimented with an alternative method, in which affixes are represented by a default allomorph. Allomorphic representations have the potential advantage of reducing the complexity of transductions by the virtue of being similar to the correct form of the affix. However, we found that allomorphic affixes tend to obfuscate differences between distinct inflections, so we decided to employ abstract tags instead. 3.2 String transduction We perform string transduction adapting the tool D IREC TL+, originally designed for grapheme-tophoneme conversion (Jiampojamarn et al., 2010). D IREC TL+ is a feature-rich, discriminative character transducer, which searches for a model-optimal sequence of character transformation rules for its input. The core of the engine is a dynamic programming algorithm capable of transducing many consecutive characters in a single operation, also known as a semi-Markov model. Using a structured version of the MIRA algorithm (McDonald et al., 2005), training attempts to assign weights to each feature so that its linear model separates the gold-standard derivation from all others in its search space. D IREC TL+ uses a number of feature template"
N15-1093,P05-1012,0,0.0152706,"ctions, so we decided to employ abstract tags instead. 3.2 String transduction We perform string transduction adapting the tool D IREC TL+, originally designed for grapheme-tophoneme conversion (Jiampojamarn et al., 2010). D IREC TL+ is a feature-rich, discriminative character transducer, which searches for a model-optimal sequence of character transformation rules for its input. The core of the engine is a dynamic programming algorithm capable of transducing many consecutive characters in a single operation, also known as a semi-Markov model. Using a structured version of the MIRA algorithm (McDonald et al., 2005), training attempts to assign weights to each feature so that its linear model separates the gold-standard derivation from all others in its search space. D IREC TL+ uses a number of feature templates to assess the quality of a rule: source context, target n-gram, and joint n-gram features. Context features conjoin the rule with indicators for all source character n-grams within a fixed window of where the rule is being applied. Target n-grams provide indi925 cators on target character sequences, describing the shape of the target as it is being produced, and may also be conjoined with our sou"
N15-1093,P09-1055,1,0.358775,"he rule with indicators for all source character n-grams within a fixed window of where the rule is being applied. Target n-grams provide indi925 cators on target character sequences, describing the shape of the target as it is being produced, and may also be conjoined with our source context features. Joint n-grams build indicators on rule sequences, combining source and target context, and memorizing frequently-used rule patterns. Durrett & DeNero also use source context features, but we are the first group to account for features that consider rule sequences or target word shape. Following Toutanova and Cherry (2009), we modify the out-of-the-box version of D IREC TL+ by implementing an abstract copy feature that indicates when a rule simply copies its source characters into the target, e.g. p → p. The copy feature has the effect of biasing the transducer towards preserving the base-form within the inflected form. In addition to the general model that is trained on all inflected word-forms, we derive tag-specific models for each type of inflection. Development experiments showed the general model to be slightly more accurate overall, but we use both types of models in our reranker. 3.3 String alignment D"
N15-1093,N04-1033,0,0.0594358,"than those of our predecessors, which makes it easy to get statistical support for these additional features. Finally, since our rules are not bound by paradigm structure, we employ a reranking step to account for intra-paradigm regularities. 3 Discriminative Transduction In this section, we describe the details of our approach, including the affix representation, the string alignment and transduction, and the paradigm reranking. 3.1 Affix representation Our inflection generation engine is a discriminative semi-Markov model, similar to a monotonic phrasebased decoder from machine translation (Zens and Ney, 2004). This system cannot insert characters, except as a part of a phrasal substitution, so when inflecting a base form, we add an abstract affix representation to both provide an insertion site and to indicate the desired inflection. Abstract tags are separated from their lemmas with a single ‘+’ character. Marking the morpheme boundary in such a way allows the transducer to generalize the context of a morpheme boundary. For example, the third person singular indicative present of the verb atmen is represented as atmen+3SIE. We use readable tags throughout this paper, but they are presented to the"
N16-1006,N15-1083,0,0.0128476,"trong feature for statistical machine translation. The NNJM uses both target and source tokens as context for a feedforward neural network language model (LM). Unfortunately, its softmax layer requires a sum over the entire output vocabulary, which slows the calculation of LM probabilities and the maximum likelihood estimation (MLE) of model parameters. Devlin et al. (2014) address this problem at runtime only with a self-normalized MLE objective. Others advocate the use of Noise Contrastive Estimation (NCE) to train NNJMs and similar monolingual LMs (Mnih and Teh, 2012; Vaswani et al., 2013; Baltescu and Blunsom, 2015; Zhang et al., 2015). NCE avoids the sum over the output vocabulary at both train- and run-time by wrapping the NNJM inside a classifier that attempts to separate real data from sampled noise, greatly improving training speed. The training efficiency of NCE is well-documented, and will not be evaluated here. However, the experimental evidence that NCE matches MLE in terms of resulting model quality is all on monolingual language modeling tasks (Mnih and Teh, 2012). Since cross-lingual contexts provide substantially stronger signals than monolingual ones, there is reason to suspect these resul"
N16-1006,N12-1047,1,0.823399,"i-stack phrasebased decoder that is quite similar to Moses (Koehn et al., 2007). Its features include standard phrase table probabilities, KN-smoothed language models including a 6-gram model trained on the English Gigaword and a 4-gram model trained on the target side of the parallel training data, domainadapted phrase tables and language models (Foster and Kuhn, 2007), a hierarchical lexicalized reordering model (Galley and Manning, 2008), and sparse features drawn from Hopkins and May (2011) and Cherry (2013). It is tuned with a batch-lattice variant of hope-fear MIRA (Chiang et al., 2008; Cherry and Foster, 2012). 4 Experiments We test two translation scenarios drawn from the recent BOLT evaluations: Arabic-to-English and Chinese-to-English. The vital statistics for our corpora are given in Table 1. The training set mixes 1 Recommended by Jacob Devlin, personal communication. 43 Lang. Arabic Chinese Train 38.6M 29.2M In-dom 1.8M 1.9M Dev 72K 77K Test1 38K 38K Test2 40K 36K Table 1: Corpus sizes in terms of number of target tokens. Dev and Test sets have 3 references for Arabic and 5 for Chinese. NIST data with BOLT-specific informal genres. The development and test sets are focused specifically on the"
N16-1006,N13-1003,1,0.79197,"This in-domain pass uses a lower initial learning rate of 0.03. Our translation system is a multi-stack phrasebased decoder that is quite similar to Moses (Koehn et al., 2007). Its features include standard phrase table probabilities, KN-smoothed language models including a 6-gram model trained on the English Gigaword and a 4-gram model trained on the target side of the parallel training data, domainadapted phrase tables and language models (Foster and Kuhn, 2007), a hierarchical lexicalized reordering model (Galley and Manning, 2008), and sparse features drawn from Hopkins and May (2011) and Cherry (2013). It is tuned with a batch-lattice variant of hope-fear MIRA (Chiang et al., 2008; Cherry and Foster, 2012). 4 Experiments We test two translation scenarios drawn from the recent BOLT evaluations: Arabic-to-English and Chinese-to-English. The vital statistics for our corpora are given in Table 1. The training set mixes 1 Recommended by Jacob Devlin, personal communication. 43 Lang. Arabic Chinese Train 38.6M 29.2M In-dom 1.8M 1.9M Dev 72K 77K Test1 38K 38K Test2 40K 36K Table 1: Corpus sizes in terms of number of target tokens. Dev and Test sets have 3 references for Arabic and 5 for Chinese."
N16-1006,D08-1024,0,0.0430012,"tion system is a multi-stack phrasebased decoder that is quite similar to Moses (Koehn et al., 2007). Its features include standard phrase table probabilities, KN-smoothed language models including a 6-gram model trained on the English Gigaword and a 4-gram model trained on the target side of the parallel training data, domainadapted phrase tables and language models (Foster and Kuhn, 2007), a hierarchical lexicalized reordering model (Galley and Manning, 2008), and sparse features drawn from Hopkins and May (2011) and Cherry (2013). It is tuned with a batch-lattice variant of hope-fear MIRA (Chiang et al., 2008; Cherry and Foster, 2012). 4 Experiments We test two translation scenarios drawn from the recent BOLT evaluations: Arabic-to-English and Chinese-to-English. The vital statistics for our corpora are given in Table 1. The training set mixes 1 Recommended by Jacob Devlin, personal communication. 43 Lang. Arabic Chinese Train 38.6M 29.2M In-dom 1.8M 1.9M Dev 72K 77K Test1 38K 38K Test2 40K 36K Table 1: Corpus sizes in terms of number of target tokens. Dev and Test sets have 3 references for Arabic and 5 for Chinese. NIST data with BOLT-specific informal genres. The development and test sets are f"
N16-1006,P11-2031,0,0.0346065,"re noise and compare their performance to that of a system with no NNJM. Each NNJM was trained as described in Section 3, varying only the learning objective.2 To measure intrinsic NNJM quality, we report average negative log likelihoods (NLL) and average |log Z|, both calculated on Dev. Lower NLL scores indicate better prediction accuracy, while lower |log Z |values indicate more effective self-normalization. We also provide average BLEU scores and standard deviations for Test1 and Test2, each calculated over 5 random tuning replications. Statistical significance is calculated with MultEval (Clark et al., 2011). Our results are shown in Table 2. By comparing MLE to no NNJM, we can confirm that the NNJM is a very effective translation feature, showing large 2 The only exception was the Arabic NCE-M system, which showed some instability during optimization, leading us to reduce its initial learning rate to 0.2. Method No NNJM MLE NCE-U NCE-T NCE-M NLL – 1.76 1.85 3.87 1.85 Arabic-English |log Z |test1 std test2 – 39.2 0.1 39.9 0.50 41.7 0.1 42.0 0.42 40.9 0.2 41.5 2.36 41.6 0.1 42.4 0.30 41.4 0.1 42.1 std 0.1 0.1 0.1 0.2 0.1 NLL – 2.35 2.54 3.93 2.40 Chinese-English |log Z |test1 std – 31.6 0.2 0.49 3"
N16-1006,P14-1129,0,0.0617662,"Missing"
N16-1006,W07-0717,0,0.0339491,"ete training data, we use that model to initialize a second training run, on a smaller in-domain training set known to better match the test conditions.1 This in-domain pass uses a lower initial learning rate of 0.03. Our translation system is a multi-stack phrasebased decoder that is quite similar to Moses (Koehn et al., 2007). Its features include standard phrase table probabilities, KN-smoothed language models including a 6-gram model trained on the English Gigaword and a 4-gram model trained on the target side of the parallel training data, domainadapted phrase tables and language models (Foster and Kuhn, 2007), a hierarchical lexicalized reordering model (Galley and Manning, 2008), and sparse features drawn from Hopkins and May (2011) and Cherry (2013). It is tuned with a batch-lattice variant of hope-fear MIRA (Chiang et al., 2008; Cherry and Foster, 2012). 4 Experiments We test two translation scenarios drawn from the recent BOLT evaluations: Arabic-to-English and Chinese-to-English. The vital statistics for our corpora are given in Table 1. The training set mixes 1 Recommended by Jacob Devlin, personal communication. 43 Lang. Arabic Chinese Train 38.6M 29.2M In-dom 1.8M 1.9M Dev 72K 77K Test1 38"
N16-1006,D08-1089,0,0.0262687,"run, on a smaller in-domain training set known to better match the test conditions.1 This in-domain pass uses a lower initial learning rate of 0.03. Our translation system is a multi-stack phrasebased decoder that is quite similar to Moses (Koehn et al., 2007). Its features include standard phrase table probabilities, KN-smoothed language models including a 6-gram model trained on the English Gigaword and a 4-gram model trained on the target side of the parallel training data, domainadapted phrase tables and language models (Foster and Kuhn, 2007), a hierarchical lexicalized reordering model (Galley and Manning, 2008), and sparse features drawn from Hopkins and May (2011) and Cherry (2013). It is tuned with a batch-lattice variant of hope-fear MIRA (Chiang et al., 2008; Cherry and Foster, 2012). 4 Experiments We test two translation scenarios drawn from the recent BOLT evaluations: Arabic-to-English and Chinese-to-English. The vital statistics for our corpora are given in Table 1. The training set mixes 1 Recommended by Jacob Devlin, personal communication. 43 Lang. Arabic Chinese Train 38.6M 29.2M In-dom 1.8M 1.9M Dev 72K 77K Test1 38K 38K Test2 40K 36K Table 1: Corpus sizes in terms of number of target t"
N16-1006,N13-1044,0,0.028917,"ese-to-English. The vital statistics for our corpora are given in Table 1. The training set mixes 1 Recommended by Jacob Devlin, personal communication. 43 Lang. Arabic Chinese Train 38.6M 29.2M In-dom 1.8M 1.9M Dev 72K 77K Test1 38K 38K Test2 40K 36K Table 1: Corpus sizes in terms of number of target tokens. Dev and Test sets have 3 references for Arabic and 5 for Chinese. NIST data with BOLT-specific informal genres. The development and test sets are focused specifically on the web-forum genre, as is the in-domain subset of the training data (In-dom). The Arabic was segmented with MADA-ARZ (Habash et al., 2013), while the Chinese was segmented with a lexiconbased approach. All data was word-aligned with IBM-4 in GIZA++ (Och and Ney, 2003), with growdiag-final-and symmetrization (Koehn et al., 2003). 4.1 Comparing Training Objectives Our main experiment is designed to answer two questions: (1) does training NNJMs with NCE impact translation quality? and (2) can any reduction be mitigated through alternate noise distributions? To this end, we train four NNJMs. • MLE: Maximum likelihood training with selfnormalization α = 0.1 • NCE-U: NCE with unigram noise • NCE-T: NCE with translation noise • NCE-M:"
N16-1006,D11-1125,0,0.0309169,"atch the test conditions.1 This in-domain pass uses a lower initial learning rate of 0.03. Our translation system is a multi-stack phrasebased decoder that is quite similar to Moses (Koehn et al., 2007). Its features include standard phrase table probabilities, KN-smoothed language models including a 6-gram model trained on the English Gigaword and a 4-gram model trained on the target side of the parallel training data, domainadapted phrase tables and language models (Foster and Kuhn, 2007), a hierarchical lexicalized reordering model (Galley and Manning, 2008), and sparse features drawn from Hopkins and May (2011) and Cherry (2013). It is tuned with a batch-lattice variant of hope-fear MIRA (Chiang et al., 2008; Cherry and Foster, 2012). 4 Experiments We test two translation scenarios drawn from the recent BOLT evaluations: Arabic-to-English and Chinese-to-English. The vital statistics for our corpora are given in Table 1. The training set mixes 1 Recommended by Jacob Devlin, personal communication. 43 Lang. Arabic Chinese Train 38.6M 29.2M In-dom 1.8M 1.9M Dev 72K 77K Test1 38K 38K Test2 40K 36K Table 1: Corpus sizes in terms of number of target tokens. Dev and Test sets have 3 references for Arabic a"
N16-1006,P15-1001,0,0.0381452,"bed by Devlin et al. (2014), the impact of any single-hidden-layer NNJM is negligible. For training, the main benefit of NCE is that it reduces the cost of the network’s output layer, replacing a term that was linear in the vocabulary size with one that is linear in the sample size. In our experiments, this is a reduction from 32K to 100. The actual benefit from this reduction is highly implementation- and architecture-dependent. It is difficult to get a substantial speedup from NCE using Theano on GPU hardware, as both reward dense matrix operations, and NCE demands sparse vector operations (Jean et al., 2015). Therefore, our decision to implement all methods in a shared codebase, which ensured a fair comparison of model quality, also prevented us from providing a meaningful evaluation of training speed, as the code and architecture were implicitly optimized to favour the most demanding method (MLE). Fortunately, there is ample evidence that NCE can provide large improvements to per-batch training speeds for NNLMs, ranging from a 2× speed-up for 20K-word vocabularies on a GPU (Chen et al., 2015) to more than 10× for 70K-word vocabularies on a CPU (Vaswani et al., 2013). Meanwhile, our experiments s"
N16-1006,N03-1017,0,0.0116035,"2M In-dom 1.8M 1.9M Dev 72K 77K Test1 38K 38K Test2 40K 36K Table 1: Corpus sizes in terms of number of target tokens. Dev and Test sets have 3 references for Arabic and 5 for Chinese. NIST data with BOLT-specific informal genres. The development and test sets are focused specifically on the web-forum genre, as is the in-domain subset of the training data (In-dom). The Arabic was segmented with MADA-ARZ (Habash et al., 2013), while the Chinese was segmented with a lexiconbased approach. All data was word-aligned with IBM-4 in GIZA++ (Och and Ney, 2003), with growdiag-final-and symmetrization (Koehn et al., 2003). 4.1 Comparing Training Objectives Our main experiment is designed to answer two questions: (1) does training NNJMs with NCE impact translation quality? and (2) can any reduction be mitigated through alternate noise distributions? To this end, we train four NNJMs. • MLE: Maximum likelihood training with selfnormalization α = 0.1 • NCE-U: NCE with unigram noise • NCE-T: NCE with translation noise • NCE-M: NCE with mixture noise and compare their performance to that of a system with no NNJM. Each NNJM was trained as described in Section 3, varying only the learning objective.2 To measure intrin"
N16-1006,J03-1002,0,0.00663997,"sonal communication. 43 Lang. Arabic Chinese Train 38.6M 29.2M In-dom 1.8M 1.9M Dev 72K 77K Test1 38K 38K Test2 40K 36K Table 1: Corpus sizes in terms of number of target tokens. Dev and Test sets have 3 references for Arabic and 5 for Chinese. NIST data with BOLT-specific informal genres. The development and test sets are focused specifically on the web-forum genre, as is the in-domain subset of the training data (In-dom). The Arabic was segmented with MADA-ARZ (Habash et al., 2013), while the Chinese was segmented with a lexiconbased approach. All data was word-aligned with IBM-4 in GIZA++ (Och and Ney, 2003), with growdiag-final-and symmetrization (Koehn et al., 2003). 4.1 Comparing Training Objectives Our main experiment is designed to answer two questions: (1) does training NNJMs with NCE impact translation quality? and (2) can any reduction be mitigated through alternate noise distributions? To this end, we train four NNJMs. • MLE: Maximum likelihood training with selfnormalization α = 0.1 • NCE-U: NCE with unigram noise • NCE-T: NCE with translation noise • NCE-M: NCE with mixture noise and compare their performance to that of a system with no NNJM. Each NNJM was trained as described in Secti"
N16-1006,E99-1010,0,0.0692157,"eir training objectives. A GeForce GTX TITAN GPU enables efficient MLE training. Following Devlin et al. (2014), all NNJMs use 3 tokens for target context, a source context window with m = 5, and a 192-node embedding layer. We deviate from their configuration by using a single 512-node hidden layer, motivated by our internal development experiments. All NCE variants use k = 100 noise samples. NNJM training data is pre-processed to limit vocabularies to 16K types for source or target inputs, and 32K types for target outputs. We build 400 deterministic word clusters for each corpus using mkcls (Och, 1999). Any word not among the 16K / 32K most frequent words is replaced with its cluster. We train our models with mini-batch stochastic gradient descent, with a batch size of 128 words, and an initial learning rate of 0.3. We check our training objective on the development set every 20K batches, and if it fails to improve for two consecutive checks, the learning rate is halved. Training stops after 5 consecutive failed checks or after 60 checks. As NCE may take longer to converge than MLE, we occasionally let NCE models train to 90 checks, but this never resulted in improved performance. Finally,"
N16-1006,D13-1140,0,0.169377,"et al., 2014), is a strong feature for statistical machine translation. The NNJM uses both target and source tokens as context for a feedforward neural network language model (LM). Unfortunately, its softmax layer requires a sum over the entire output vocabulary, which slows the calculation of LM probabilities and the maximum likelihood estimation (MLE) of model parameters. Devlin et al. (2014) address this problem at runtime only with a self-normalized MLE objective. Others advocate the use of Noise Contrastive Estimation (NCE) to train NNJMs and similar monolingual LMs (Mnih and Teh, 2012; Vaswani et al., 2013; Baltescu and Blunsom, 2015; Zhang et al., 2015). NCE avoids the sum over the output vocabulary at both train- and run-time by wrapping the NNJM inside a classifier that attempts to separate real data from sampled noise, greatly improving training speed. The training efficiency of NCE is well-documented, and will not be evaluated here. However, the experimental evidence that NCE matches MLE in terms of resulting model quality is all on monolingual language modeling tasks (Mnih and Teh, 2012). Since cross-lingual contexts provide substantially stronger signals than monolingual ones, there is r"
N16-1006,D15-1250,0,0.200275,"l machine translation. The NNJM uses both target and source tokens as context for a feedforward neural network language model (LM). Unfortunately, its softmax layer requires a sum over the entire output vocabulary, which slows the calculation of LM probabilities and the maximum likelihood estimation (MLE) of model parameters. Devlin et al. (2014) address this problem at runtime only with a self-normalized MLE objective. Others advocate the use of Noise Contrastive Estimation (NCE) to train NNJMs and similar monolingual LMs (Mnih and Teh, 2012; Vaswani et al., 2013; Baltescu and Blunsom, 2015; Zhang et al., 2015). NCE avoids the sum over the output vocabulary at both train- and run-time by wrapping the NNJM inside a classifier that attempts to separate real data from sampled noise, greatly improving training speed. The training efficiency of NCE is well-documented, and will not be evaluated here. However, the experimental evidence that NCE matches MLE in terms of resulting model quality is all on monolingual language modeling tasks (Mnih and Teh, 2012). Since cross-lingual contexts provide substantially stronger signals than monolingual ones, there is reason to suspect these results may not carry over"
N16-1006,P07-2045,0,\N,Missing
N16-1140,P08-2039,0,0.0239392,"that the target side of the parallel corpus has been segmented into morphemes with prefixes and suffixes marked.2 This allows us to define a complete word as a maximal morpheme sequence consisting of 0 or more prefixes, followed by at most one stem, and then 0 or more suffixes. We also assume access to a desegmentation function that takes as input a morpheme sequence matching the above definition, and returns the corresponding word as output. Depending on the complexity of the segmentation, desegmentation can be achieved through simple concatenation, a small set of rules, a statistical table (Badr et al., 2008), or a statis1 The ideas presented here could also be applied to hierarchical decoding, which would require generalizing them to account for right context as well as left. 2 Throughout this paper, we use a token-final “+” to denote a prefix, and a token-initial “+” for a suffix. 1176 tical transducer (Salameh et al., 2013). El Kholy and Habash (2012) provide an extensive study on the influence of segmentation and desegmentation on English-to-Arabic SMT. In this work, we adopt the Table+Rules technique of El Kholy and Habash (2012) for English-Arabic SMT. The technique relies on a look-up table"
N16-1140,P11-2031,0,0.0181231,"rget side of the parallel data. We experiment with word penalties based on either morphemes or desegmented words. The decoder uses Moses’ default search parameters, except for the maximum phrase length, which is set to 8, and the translation table limit, which is set to 40. The decoder’s log-linear model is tuned with MERT (Och, 2003) using unsegmented Arabic reference translations. When necessary, we desegment our 100-best-lists before MERT evaluates each hypothesis. We evaluate with BLEU (Papineni et al., 2002) measured on unsegmented Arabic, and test statistical significance with multeval (Clark et al., 2011) over 3 tuning replications. We test four systems that differ in their desegmentation approach. The NoSegm. baseline involves no segmentation. The One-best baseline translates into segmented Arabic and desegments the decoder’s 1-best output. The Lattice system is the lattice-desegmentation approach of Salameh et al. (2014). We implement our in-Decoder desegmentaSystem NoSegm. One-best Lattice Delayed Optimistic WP word morph. morph. morph. word morph. word mt05 33.2 33.8 34.4 34.1 34.1 34.2 34.5 mt08 18.6 19.1 19.7 19.4 19.5 19.6 19.7 mt09 25.6 26.8 27.4 27.0 26.8 27.2 27.2 Table 1: Evaluation"
N16-1140,P07-2045,0,0.0857459,"nd considered throughout the entire search space. We achieve this by augmenting the decoder to desegment hypotheses on the fly, allowing the inclusion of an unsegmented language model and other features. Our results on a large-scale, NIST-data English to Arabic translation task show significant improvements over the 1best desegmentation baseline, and match the performance of the state-of-the-art lattice desegmentation approach of Salameh et al. (2014), while eliminating the complication and cost of its rescoring step. Our approach is implemented as a single stateful feature function in Moses (Koehn et al., 2007), which we will submit back to the community. 2 Method Our approach extends the multi-stack phrase-based decoding paradigm to enable the extraction of wordlevel features inside morpheme-segmented models.1 We assume that the target side of the parallel corpus has been segmented into morphemes with prefixes and suffixes marked.2 This allows us to define a complete word as a maximal morpheme sequence consisting of 0 or more prefixes, followed by at most one stem, and then 0 or more suffixes. We also assume access to a desegmentation function that takes as input a morpheme sequence matching the ab"
N16-1140,D10-1015,0,0.0426598,"Missing"
N16-1140,J03-1002,0,0.016086,"n of NIST 2008 (813 pairs) and NIST 2009 (586 pairs). As there are multiple English reference translations provided for these evaluation sets, we use the first reference as our source text. The Arabic part of the training set is morphologically segmented and tokenized by MADA 3.2 (Habash et al., 2009) using the Penn Arabic Treebank (PATB) segmentation scheme. Variants of Alif and Ya characters are uniformly normalized. We generate a desegmentation table from the Arabic side of the training data by collecting mappings of segmented forms to surface forms. We align the parallel data with GIZA++ (Och et al., 2003), and decode with Moses (Koehn et al., 2007). The decoder’s log-linear model uses a standard feature set, including four phrase table scores, six features from a lexicalized distortion model, along with a phrase penalty and a distance-based distortion penalty. KN-smoothed 5-gram language models are trained on both the segmented and unsegmented views of the target side of the parallel data. We experiment with word penalties based on either morphemes or desegmented words. The decoder uses Moses’ default search parameters, except for the maximum phrase length, which is set to 8, and the translati"
N16-1140,P03-1021,0,0.0256788,"ndard feature set, including four phrase table scores, six features from a lexicalized distortion model, along with a phrase penalty and a distance-based distortion penalty. KN-smoothed 5-gram language models are trained on both the segmented and unsegmented views of the target side of the parallel data. We experiment with word penalties based on either morphemes or desegmented words. The decoder uses Moses’ default search parameters, except for the maximum phrase length, which is set to 8, and the translation table limit, which is set to 40. The decoder’s log-linear model is tuned with MERT (Och, 2003) using unsegmented Arabic reference translations. When necessary, we desegment our 100-best-lists before MERT evaluates each hypothesis. We evaluate with BLEU (Papineni et al., 2002) measured on unsegmented Arabic, and test statistical significance with multeval (Clark et al., 2011) over 3 tuning replications. We test four systems that differ in their desegmentation approach. The NoSegm. baseline involves no segmentation. The One-best baseline translates into segmented Arabic and desegments the decoder’s 1-best output. The Lattice system is the lattice-desegmentation approach of Salameh et al."
N16-1140,W07-0704,0,0.0807303,"Missing"
N16-1140,P02-1040,0,0.0952456,"ty. KN-smoothed 5-gram language models are trained on both the segmented and unsegmented views of the target side of the parallel data. We experiment with word penalties based on either morphemes or desegmented words. The decoder uses Moses’ default search parameters, except for the maximum phrase length, which is set to 8, and the translation table limit, which is set to 40. The decoder’s log-linear model is tuned with MERT (Och, 2003) using unsegmented Arabic reference translations. When necessary, we desegment our 100-best-lists before MERT evaluates each hypothesis. We evaluate with BLEU (Papineni et al., 2002) measured on unsegmented Arabic, and test statistical significance with multeval (Clark et al., 2011) over 3 tuning replications. We test four systems that differ in their desegmentation approach. The NoSegm. baseline involves no segmentation. The One-best baseline translates into segmented Arabic and desegments the decoder’s 1-best output. The Lattice system is the lattice-desegmentation approach of Salameh et al. (2014). We implement our in-Decoder desegmentaSystem NoSegm. One-best Lattice Delayed Optimistic WP word morph. morph. morph. word morph. word mt05 33.2 33.8 34.4 34.1 34.1 34.2 34."
N16-1140,N13-2007,1,0.839267,"function that takes as input a morpheme sequence matching the above definition, and returns the corresponding word as output. Depending on the complexity of the segmentation, desegmentation can be achieved through simple concatenation, a small set of rules, a statistical table (Badr et al., 2008), or a statis1 The ideas presented here could also be applied to hierarchical decoding, which would require generalizing them to account for right context as well as left. 2 Throughout this paper, we use a token-final “+” to denote a prefix, and a token-initial “+” for a suffix. 1176 tical transducer (Salameh et al., 2013). El Kholy and Habash (2012) provide an extensive study on the influence of segmentation and desegmentation on English-to-Arabic SMT. In this work, we adopt the Table+Rules technique of El Kholy and Habash (2012) for English-Arabic SMT. The technique relies on a look-up table that stores mappings of segmented-unsegmented forms, and falls back on manually crafted rules for segmented sequences not found in the table. When a segmented form has multiple desegmentation options available in the table, we select the most frequent option. The output of a phrase-based decoder is built from left to righ"
N16-1140,P14-1010,1,0.668945,"phic normalizations, such as transforming the stem-final t to p. The result is not only a reduction in the number of word types, but also better token-to-token correspondence with the source language. Morphological segmentation is typically performed as a pre-processing step before the training phase, which results in a model that translates the source language into segmented target language. Desegmentation is the process of transforming the segmented output into a readable word sequence, The rescoring approach desegments either an n-best list (Oflazer and Durgar El-Kahlout, 2007) or lattice (Salameh et al., 2014), and then re-ranks with features that consider the desegmented word sequence of each hypothesis. Rescoring features include the score from an unsegmented target language model and contiguity indicators that flag target words that were translated from contiguous source tokens. Rescoring widens the desegmentation pipeline, allowing desegmentation features to reduce the number of translation errors. However, these features are calculated for only a subset of the search space, and the extra rescoring step complicates the training and translation processes. Phrase-table desegmentation (Luong et al"
N16-1140,W15-1011,1,0.711888,"-aware phrase extraction. The extracted phrases are constrained to contain only complete target words, without any dangling affixes. With this restriction in place, the phrase table can be desegmented before decoding begins, allowing the decoder to track features over both the segmented and desegmented target. This ensures that desegmentation features are integrated into the complete search space, and 1175 Proceedings of NAACL-HLT 2016, pages 1175–1180, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics side-steps the complications of rescoring. However, Salameh et al. (2015) show experimentally that these benefits are not worth giving up phrase-pairs with dangling affixes, which are eliminated by wordboundary-aware phrase extraction. We present a method for decoder-integrated desegmentation that combines the strengths of these two approaches. Like a rescoring approach, it places no restrictions on what morpheme sequences can appear in the target side of a phrase pair. Like phrasetable desegmentation, its desegmentation features are integrated directly into decoding and considered throughout the entire search space. We achieve this by augmenting the decoder to des"
N16-1140,D08-1076,0,\N,Missing
N19-1208,W17-3205,1,0.71787,"gy from the reference. This result opens the door to learning more sophisticated curricula that exploit multiple data at2054 Proceedings of NAACL-HLT 2019, pages 2054–2061 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics Figure 1: The agent’s interface with the NMT system. Figure 2: Linearly-decaying -greedy exploration. tributes and work with arbitrary corpora. 2 Related Work s(e, f ) for a translation pair e, f is defined as: Among the very extensive work on handling heterogeneous data in NMT, the closest to ours are techniques that re-weight (Chen et al., 2017) or re-order examples to deal with domain mismatch (van der Wees et al., 2017; Sajjad et al., 2017) or noise (Wang et al., 2018). The idea of a curriculum was popularized by Bengio et al. (2009), who viewed it as a way to improve convergence by presenting heuristicallyidentified easy examples first. Several recent papers (Kocmi and Bojar, 2017; Zhang et al., 2019; Platanios et al., 2019) explore similar ideas for NMT, and verify that this strategy can reduce training time and improve quality. Work on meta-learning a curriculum originated with Tsvetkov et al. (2016), who used Bayesian optimizat"
N19-1208,P18-1008,1,0.893687,"Missing"
N19-1208,P13-2119,0,0.0479278,"earn one automatically, jointly with the NMT system, in the course of a single training run. We show that this approach can beat uniform baselines on Paracrawl and WMT English-to-French datasets by +3.4 and +1.3 BLEU respectively. Additionally, we match the performance of strong filtering baselines and hand-designed, state-of-the-art curricula. 1 Introduction Machine Translation training data is typically heterogeneous: it may vary in characteristics such as domain, translation quality, and degree of difficulty. Many approaches have been proposed to cope with heterogeneity, such as filtering (Duh et al., 2013) or down-weighting (Wang et al., 2017) examples that are likely to be noisy or out of domain. A powerful technique is to control the curriculum—the order in which examples are presented to the system—as is done in fine-tuning (Freitag and Al-Onaizan, 2016), where training occurs first on general data, and then on more valuable in-domain data. Curriculum based approaches generalize data filtering and weighting1 by allowing examples to be visited multiple times 1 Assuming integer weights. or not at all; and they additionally potentially enable steering the training trajectory toward a better glo"
N19-1208,W18-6300,0,0.195937,"Missing"
N19-1208,D17-1147,0,0.171788,"Missing"
N19-1208,kocmi-bojar-2017-curriculum,0,0.0667793,"caying -greedy exploration. tributes and work with arbitrary corpora. 2 Related Work s(e, f ) for a translation pair e, f is defined as: Among the very extensive work on handling heterogeneous data in NMT, the closest to ours are techniques that re-weight (Chen et al., 2017) or re-order examples to deal with domain mismatch (van der Wees et al., 2017; Sajjad et al., 2017) or noise (Wang et al., 2018). The idea of a curriculum was popularized by Bengio et al. (2009), who viewed it as a way to improve convergence by presenting heuristicallyidentified easy examples first. Several recent papers (Kocmi and Bojar, 2017; Zhang et al., 2019; Platanios et al., 2019) explore similar ideas for NMT, and verify that this strategy can reduce training time and improve quality. Work on meta-learning a curriculum originated with Tsvetkov et al. (2016), who used Bayesian optimization to learn a linear model for ranking examples in a word-embedding task. This approach requires a large number of complete training runs, and is thus impractical for NMT. More recent work has explored bandit optimization for scheduling tasks in a multi-task problem (Graves et al., 2017), and reinforcement learning for selecting examples in a"
N19-1208,N18-1113,0,0.154268,"matter of extensive trial and error. Automating this process with meta-learning is thus an attractive proposition. However, it comes with many potential pitfalls such as failing to match a human-designed curriculum, or significantly increasing training time. In this paper we present an initial study on meta-learning an NMT curriculum. Starting from scratch, we attempt to match the performance of a successful non-trivial reference curriculum proposed by Wang et al. (2018), in which training gradually focuses on increasingly cleaner data, as measured by an external scoring function. Inspired by Wu et al. (2018), we use a reinforcement-learning (RL) approach involving a learned agent whose task is to choose a corpus bin, representing a given noise level, at each NMT training step. A challenging aspect of this task is that choosing only the cleanest bin is sub-optimal; the reference curriculum uses all the data in the early stages of training, and only gradually anneals toward the cleanest. Furthermore, we impose the condition that the agent must learn its curriculum in the course of a single NMT training run. We demonstrate that our RL agent can learn a curriculum that works as well as the reference,"
N19-1208,K18-1033,0,0.0304824,"re similar ideas for NMT, and verify that this strategy can reduce training time and improve quality. Work on meta-learning a curriculum originated with Tsvetkov et al. (2016), who used Bayesian optimization to learn a linear model for ranking examples in a word-embedding task. This approach requires a large number of complete training runs, and is thus impractical for NMT. More recent work has explored bandit optimization for scheduling tasks in a multi-task problem (Graves et al., 2017), and reinforcement learning for selecting examples in a co-trained classifier (Wu et al., 2018). Finally, Liu et al. (2018) apply imitation learning to actively select monolingual training sentences for labeling in NMT, and show that the learned strategy can be transferred to a related language pair. 3 Methods The attribute we choose to learn a curriculum over is noise. To determine a per-sentence noise score, we use the contrastive data selection (CDS) method defined in Wang et al. (2018). Given the parameters θn of an NMT model trained on a noisy corpus, and parameters θc of the same model finetuned on a very small trusted corpus, the score s(e, f ) = log pθc (f |e) − log pθn (f |e) (1) Wang et al. (2018) show t"
N19-1208,N19-1189,1,0.732527,"Missing"
N19-1208,N19-1119,0,0.0585249,"ork with arbitrary corpora. 2 Related Work s(e, f ) for a translation pair e, f is defined as: Among the very extensive work on handling heterogeneous data in NMT, the closest to ours are techniques that re-weight (Chen et al., 2017) or re-order examples to deal with domain mismatch (van der Wees et al., 2017; Sajjad et al., 2017) or noise (Wang et al., 2018). The idea of a curriculum was popularized by Bengio et al. (2009), who viewed it as a way to improve convergence by presenting heuristicallyidentified easy examples first. Several recent papers (Kocmi and Bojar, 2017; Zhang et al., 2019; Platanios et al., 2019) explore similar ideas for NMT, and verify that this strategy can reduce training time and improve quality. Work on meta-learning a curriculum originated with Tsvetkov et al. (2016), who used Bayesian optimization to learn a linear model for ranking examples in a word-embedding task. This approach requires a large number of complete training runs, and is thus impractical for NMT. More recent work has explored bandit optimization for scheduling tasks in a multi-task problem (Graves et al., 2017), and reinforcement learning for selecting examples in a co-trained classifier (Wu et al., 2018). Fin"
N19-1208,E17-2045,0,0.0897074,"xploit multiple data at2054 Proceedings of NAACL-HLT 2019, pages 2054–2061 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics Figure 1: The agent’s interface with the NMT system. Figure 2: Linearly-decaying -greedy exploration. tributes and work with arbitrary corpora. 2 Related Work s(e, f ) for a translation pair e, f is defined as: Among the very extensive work on handling heterogeneous data in NMT, the closest to ours are techniques that re-weight (Chen et al., 2017) or re-order examples to deal with domain mismatch (van der Wees et al., 2017; Sajjad et al., 2017) or noise (Wang et al., 2018). The idea of a curriculum was popularized by Bengio et al. (2009), who viewed it as a way to improve convergence by presenting heuristicallyidentified easy examples first. Several recent papers (Kocmi and Bojar, 2017; Zhang et al., 2019; Platanios et al., 2019) explore similar ideas for NMT, and verify that this strategy can reduce training time and improve quality. Work on meta-learning a curriculum originated with Tsvetkov et al. (2016), who used Bayesian optimization to learn a linear model for ranking examples in a word-embedding task. This approach requires a"
N19-1208,P16-1013,0,0.0955844,"s are techniques that re-weight (Chen et al., 2017) or re-order examples to deal with domain mismatch (van der Wees et al., 2017; Sajjad et al., 2017) or noise (Wang et al., 2018). The idea of a curriculum was popularized by Bengio et al. (2009), who viewed it as a way to improve convergence by presenting heuristicallyidentified easy examples first. Several recent papers (Kocmi and Bojar, 2017; Zhang et al., 2019; Platanios et al., 2019) explore similar ideas for NMT, and verify that this strategy can reduce training time and improve quality. Work on meta-learning a curriculum originated with Tsvetkov et al. (2016), who used Bayesian optimization to learn a linear model for ranking examples in a word-embedding task. This approach requires a large number of complete training runs, and is thus impractical for NMT. More recent work has explored bandit optimization for scheduling tasks in a multi-task problem (Graves et al., 2017), and reinforcement learning for selecting examples in a co-trained classifier (Wu et al., 2018). Finally, Liu et al. (2018) apply imitation learning to actively select monolingual training sentences for labeling in NMT, and show that the learned strategy can be transferred to a re"
N19-1208,D17-1155,0,0.0306993,"the NMT system, in the course of a single training run. We show that this approach can beat uniform baselines on Paracrawl and WMT English-to-French datasets by +3.4 and +1.3 BLEU respectively. Additionally, we match the performance of strong filtering baselines and hand-designed, state-of-the-art curricula. 1 Introduction Machine Translation training data is typically heterogeneous: it may vary in characteristics such as domain, translation quality, and degree of difficulty. Many approaches have been proposed to cope with heterogeneity, such as filtering (Duh et al., 2013) or down-weighting (Wang et al., 2017) examples that are likely to be noisy or out of domain. A powerful technique is to control the curriculum—the order in which examples are presented to the system—as is done in fine-tuning (Freitag and Al-Onaizan, 2016), where training occurs first on general data, and then on more valuable in-domain data. Curriculum based approaches generalize data filtering and weighting1 by allowing examples to be visited multiple times 1 Assuming integer weights. or not at all; and they additionally potentially enable steering the training trajectory toward a better global optimum than might be attainable w"
N19-1208,W18-6314,0,0.44856,"a and its attributes. Although powerful heuristics like fine-tuning are helpful, setting hyper-parameters to specify a curriculum is usually a matter of extensive trial and error. Automating this process with meta-learning is thus an attractive proposition. However, it comes with many potential pitfalls such as failing to match a human-designed curriculum, or significantly increasing training time. In this paper we present an initial study on meta-learning an NMT curriculum. Starting from scratch, we attempt to match the performance of a successful non-trivial reference curriculum proposed by Wang et al. (2018), in which training gradually focuses on increasingly cleaner data, as measured by an external scoring function. Inspired by Wu et al. (2018), we use a reinforcement-learning (RL) approach involving a learned agent whose task is to choose a corpus bin, representing a given noise level, at each NMT training step. A challenging aspect of this task is that choosing only the cleanest bin is sub-optimal; the reference curriculum uses all the data in the early stages of training, and only gradually anneals toward the cleanest. Furthermore, we impose the condition that the agent must learn its curric"
P03-1012,J00-1004,0,0.0884703,"e translation methods, which also employ modular features. Maximum entropy can be used to improve IBM-style translation probabilities by using features, such as improvements to P (f |e) in (Berger et al., 1996). By the same token we can use maximum entropy to improve our estimates of P (lk |eik , fjk , Ck ). We are currently investigating maximum entropy as an alternative to our current feature model which assumes conditional independence among features. 6.2 Grammatical Constraints There have been many recent proposals to leverage syntactic data in word alignment. Methods such as (Wu, 1997), (Alshawi et al., 2000) and (Lopez et al., 2002) employ a synchronous parsing procedure to constrain a statistical alignment. The work done in (Yamada and Knight, 2001) measures statistics on operations that transform a parse tree from one language into another. 7 Future Work The alignment algorithm described here is incapable of creating alignments that are not one-to-one. The model we describe, however is not limited in the same manner. The model is currently capable of creating many-to-one alignments so long as the null probabilities of the words added on the “many” side are less than the probabilities of the lin"
P03-1012,J96-1002,0,0.0547202,"Missing"
P03-1012,J93-2003,0,0.195679,"colinc,lindek}@cs.ualberta.ca Abstract Word alignment plays a crucial role in statistical machine translation. Word-aligned corpora have been found to be an excellent source of translation-related knowledge. We present a statistical model for computing the probability of an alignment given a sentence pair. This model allows easy integration of context-specific features. Our experiments show that this model can be an effective tool for improving an existing word alignment. 1 Introduction Word alignments were first introduced as an intermediate result of statistical machine translation systems (Brown et al., 1993). Since their introduction, many researchers have become interested in word alignments as a knowledge source. For example, alignments can be used to learn translation lexicons (Melamed, 1996), transfer rules (Carbonell et al., 2002; Menezes and Richardson, 2001), and classifiers to find safe sentence segmentation points (Berger et al., 1996). In addition to the IBM models, researchers have proposed a number of alternative alignment methods. These methods often involve using a statistic such as φ2 (Gale and Church, 1991) or the log likelihood ratio (Dunning, 1993) to create a score to measure t"
P03-1012,carbonell-etal-2002-automatic,0,0.022999,"ical model for computing the probability of an alignment given a sentence pair. This model allows easy integration of context-specific features. Our experiments show that this model can be an effective tool for improving an existing word alignment. 1 Introduction Word alignments were first introduced as an intermediate result of statistical machine translation systems (Brown et al., 1993). Since their introduction, many researchers have become interested in word alignments as a knowledge source. For example, alignments can be used to learn translation lexicons (Melamed, 1996), transfer rules (Carbonell et al., 2002; Menezes and Richardson, 2001), and classifiers to find safe sentence segmentation points (Berger et al., 1996). In addition to the IBM models, researchers have proposed a number of alternative alignment methods. These methods often involve using a statistic such as φ2 (Gale and Church, 1991) or the log likelihood ratio (Dunning, 1993) to create a score to measure the strength of correlation between source and target words. Such measures can then be used to guide a constrained search to produce word alignments (Melamed, 2000). It has been shown that once a baseline alignment has been created,"
P03-1012,J93-1003,0,0.0605019,"hine translation systems (Brown et al., 1993). Since their introduction, many researchers have become interested in word alignments as a knowledge source. For example, alignments can be used to learn translation lexicons (Melamed, 1996), transfer rules (Carbonell et al., 2002; Menezes and Richardson, 2001), and classifiers to find safe sentence segmentation points (Berger et al., 1996). In addition to the IBM models, researchers have proposed a number of alternative alignment methods. These methods often involve using a statistic such as φ2 (Gale and Church, 1991) or the log likelihood ratio (Dunning, 1993) to create a score to measure the strength of correlation between source and target words. Such measures can then be used to guide a constrained search to produce word alignments (Melamed, 2000). It has been shown that once a baseline alignment has been created, one can improve results by using a refined scoring metric that is based on the alignment. For example Melamed uses competitive linking along with an explicit noise model in (Melamed, 2000) to produce a new scoring metric, which in turn creates better alignments. In this paper, we present a simple, flexible, statistical model that is de"
P03-1012,W02-1039,0,0.0542962,"linguistic intuitions. 3.1 det det pre det the host discovers all the devices Constraints The reader will note that our alignment model as described above has very few factors to prevent undesirable alignments, such as having all French words align to the same English word. To guide the model to correct alignments, we employ two constraints to limit our search for the most probable alignment. The first constraint is the one-to-one constraint (Melamed, 2000): every word (except the null words e0 and f0 ) participates in exactly one link. The second constraint, known as the cohesion constraint (Fox, 2002), uses the dependency tree (Mel’ˇcuk, 1987) of the English sentence to restrict possible link combinations. Given the dependency tree TE , the alignment can induce a dependency tree for F (Hwa et al., 2002). The cohesion constraint requires that this induced dependency tree does not have any crossing dependencies. The details about how the cohesion constraint is implemented are outside the scope of this paper.3 Here we will use a simple example to illustrate the effect of the constraint. Consider the partial alignment in Figure 2. When the system attempts to link of and de, the new link will i"
P03-1012,H91-1026,0,0.764885,"roduced as an intermediate result of statistical machine translation systems (Brown et al., 1993). Since their introduction, many researchers have become interested in word alignments as a knowledge source. For example, alignments can be used to learn translation lexicons (Melamed, 1996), transfer rules (Carbonell et al., 2002; Menezes and Richardson, 2001), and classifiers to find safe sentence segmentation points (Berger et al., 1996). In addition to the IBM models, researchers have proposed a number of alternative alignment methods. These methods often involve using a statistic such as φ2 (Gale and Church, 1991) or the log likelihood ratio (Dunning, 1993) to create a score to measure the strength of correlation between source and target words. Such measures can then be used to guide a constrained search to produce word alignments (Melamed, 2000). It has been shown that once a baseline alignment has been created, one can improve results by using a refined scoring metric that is based on the alignment. For example Melamed uses competitive linking along with an explicit noise model in (Melamed, 2000) to produce a new scoring metric, which in turn creates better alignments. In this paper, we present a si"
P03-1012,P02-1050,0,0.0142299,"alignments, such as having all French words align to the same English word. To guide the model to correct alignments, we employ two constraints to limit our search for the most probable alignment. The first constraint is the one-to-one constraint (Melamed, 2000): every word (except the null words e0 and f0 ) participates in exactly one link. The second constraint, known as the cohesion constraint (Fox, 2002), uses the dependency tree (Mel’ˇcuk, 1987) of the English sentence to restrict possible link combinations. Given the dependency tree TE , the alignment can induce a dependency tree for F (Hwa et al., 2002). The cohesion constraint requires that this induced dependency tree does not have any crossing dependencies. The details about how the cohesion constraint is implemented are outside the scope of this paper.3 Here we will use a simple example to illustrate the effect of the constraint. Consider the partial alignment in Figure 2. When the system attempts to link of and de, the new link will induce the dotted dependency, which crosses a previously induced dependency between service and donn´ees. Therefore, of and de will not be linked. mod pcomp det nn the status of the data service l' état du s"
P03-1012,O97-4004,0,0.0207898,"e status of the data service l' état du service de données Figure 2: An Example of Cohesion Constraint 3.2 obj subj Features In this section we introduce two types of features that we use in our implementation of the probability model described in Section 2. The first feature 1 2 3 4 5 6 1 2 3 4 5 6 l' hôte repère tous les périphériques the host locate all the peripherals Figure 3: Feature Extraction Example type f ta concerns surrounding links. It has been observed that words close to each other in the source language tend to remain close to each other in the translation (Vogel et al., 1996; Ker and Change, 1997). To capture this notion, for any word pair (ei , fj ), if a link l(ei0 , fj 0 ) exists where i − 2 ≤ i0 ≤ i + 2 and j − 2 ≤ j 0 ≤ j + 2, then we say that the feature f ta (i−i0 , j −j 0 , ei0 ) is active for this context. We refer to these as adjacency features. The second feature type f td uses the English parse tree to capture regularities among grammatical relations between languages. For example, when dealing with French and English, the location of the determiner with respect to its governor4 is never swapped during translation, while the location of adjectives is swapped frequently. For"
P03-1012,1996.amta-1.13,0,0.0399712,"knowledge. We present a statistical model for computing the probability of an alignment given a sentence pair. This model allows easy integration of context-specific features. Our experiments show that this model can be an effective tool for improving an existing word alignment. 1 Introduction Word alignments were first introduced as an intermediate result of statistical machine translation systems (Brown et al., 1993). Since their introduction, many researchers have become interested in word alignments as a knowledge source. For example, alignments can be used to learn translation lexicons (Melamed, 1996), transfer rules (Carbonell et al., 2002; Menezes and Richardson, 2001), and classifiers to find safe sentence segmentation points (Berger et al., 1996). In addition to the IBM models, researchers have proposed a number of alternative alignment methods. These methods often involve using a statistic such as φ2 (Gale and Church, 1991) or the log likelihood ratio (Dunning, 1993) to create a score to measure the strength of correlation between source and target words. Such measures can then be used to guide a constrained search to produce word alignments (Melamed, 2000). It has been shown that onc"
P03-1012,J00-2004,0,0.771457,"learn translation lexicons (Melamed, 1996), transfer rules (Carbonell et al., 2002; Menezes and Richardson, 2001), and classifiers to find safe sentence segmentation points (Berger et al., 1996). In addition to the IBM models, researchers have proposed a number of alternative alignment methods. These methods often involve using a statistic such as φ2 (Gale and Church, 1991) or the log likelihood ratio (Dunning, 1993) to create a score to measure the strength of correlation between source and target words. Such measures can then be used to guide a constrained search to produce word alignments (Melamed, 2000). It has been shown that once a baseline alignment has been created, one can improve results by using a refined scoring metric that is based on the alignment. For example Melamed uses competitive linking along with an explicit noise model in (Melamed, 2000) to produce a new scoring metric, which in turn creates better alignments. In this paper, we present a simple, flexible, statistical model that is designed to capture the information present in a baseline alignment. This model allows us to compute the probability of an alignment for a given sentence pair. It also allows for the easy incorpor"
P03-1012,W01-1406,0,0.0252005,"the probability of an alignment given a sentence pair. This model allows easy integration of context-specific features. Our experiments show that this model can be an effective tool for improving an existing word alignment. 1 Introduction Word alignments were first introduced as an intermediate result of statistical machine translation systems (Brown et al., 1993). Since their introduction, many researchers have become interested in word alignments as a knowledge source. For example, alignments can be used to learn translation lexicons (Melamed, 1996), transfer rules (Carbonell et al., 2002; Menezes and Richardson, 2001), and classifiers to find safe sentence segmentation points (Berger et al., 1996). In addition to the IBM models, researchers have proposed a number of alternative alignment methods. These methods often involve using a statistic such as φ2 (Gale and Church, 1991) or the log likelihood ratio (Dunning, 1993) to create a score to measure the strength of correlation between source and target words. Such measures can then be used to guide a constrained search to produce word alignments (Melamed, 2000). It has been shown that once a baseline alignment has been created, one can improve results by usi"
P03-1012,P00-1056,0,0.591811,"ote that any sampling method that concentrates on complete, valid and high probability alignments will accomplish the same task. When collecting the statistics needed to calculate P (A|E, F ) from our initial φ2 alignment, we give each s ∈ S a uniform weight. This is reasonable, as we have no probability estimates at this point. When training from the alignments produced by our model, we normalize P (s|E, F ) so P that s∈S P (s|E, F ) = 1. We then count links and features in S according to these normalized probabilities. 5 Experimental Results We adopted the same evaluation methodology as in (Och and Ney, 2000), which compared alignment outputs with manually aligned sentences. Och and Ney classify manual alignments into two categories: Sure (S) and Possible (P ) (S⊆P ). They defined the following metrics to evaluate an alignment A: recall = |A∩S| |S| precision = alignment error rate (AER) = |A∩P | |P | |A∩S|+|A∩P | |S|+|P | We trained our alignment program with the same 50K pairs of sentences as (Och and Ney, 2000) and tested it on the same 500 manually aligned sentences. Both the training and testing sentences are from the Hansard corpus. We parsed the training Table 3: Evaluation of Features Table"
P03-1012,C96-2141,0,0.404202,"mod pcomp det nn the status of the data service l' état du service de données Figure 2: An Example of Cohesion Constraint 3.2 obj subj Features In this section we introduce two types of features that we use in our implementation of the probability model described in Section 2. The first feature 1 2 3 4 5 6 1 2 3 4 5 6 l' hôte repère tous les périphériques the host locate all the peripherals Figure 3: Feature Extraction Example type f ta concerns surrounding links. It has been observed that words close to each other in the source language tend to remain close to each other in the translation (Vogel et al., 1996; Ker and Change, 1997). To capture this notion, for any word pair (ei , fj ), if a link l(ei0 , fj 0 ) exists where i − 2 ≤ i0 ≤ i + 2 and j − 2 ≤ j 0 ≤ j + 2, then we say that the feature f ta (i−i0 , j −j 0 , ei0 ) is active for this context. We refer to these as adjacency features. The second feature type f td uses the English parse tree to capture regularities among grammatical relations between languages. For example, when dealing with French and English, the location of the determiner with respect to its governor4 is never swapped during translation, while the location of adjectives is"
P03-1012,J97-3002,0,0.62674,"based machine translation methods, which also employ modular features. Maximum entropy can be used to improve IBM-style translation probabilities by using features, such as improvements to P (f |e) in (Berger et al., 1996). By the same token we can use maximum entropy to improve our estimates of P (lk |eik , fjk , Ck ). We are currently investigating maximum entropy as an alternative to our current feature model which assumes conditional independence among features. 6.2 Grammatical Constraints There have been many recent proposals to leverage syntactic data in word alignment. Methods such as (Wu, 1997), (Alshawi et al., 2000) and (Lopez et al., 2002) employ a synchronous parsing procedure to constrain a statistical alignment. The work done in (Yamada and Knight, 2001) measures statistics on operations that transform a parse tree from one language into another. 7 Future Work The alignment algorithm described here is incapable of creating alignments that are not one-to-one. The model we describe, however is not limited in the same manner. The model is currently capable of creating many-to-one alignments so long as the null probabilities of the words added on the “many” side are less than the"
P03-1012,P01-1067,0,0.281272,"features, such as improvements to P (f |e) in (Berger et al., 1996). By the same token we can use maximum entropy to improve our estimates of P (lk |eik , fjk , Ck ). We are currently investigating maximum entropy as an alternative to our current feature model which assumes conditional independence among features. 6.2 Grammatical Constraints There have been many recent proposals to leverage syntactic data in word alignment. Methods such as (Wu, 1997), (Alshawi et al., 2000) and (Lopez et al., 2002) employ a synchronous parsing procedure to constrain a statistical alignment. The work done in (Yamada and Knight, 2001) measures statistics on operations that transform a parse tree from one language into another. 7 Future Work The alignment algorithm described here is incapable of creating alignments that are not one-to-one. The model we describe, however is not limited in the same manner. The model is currently capable of creating many-to-one alignments so long as the null probabilities of the words added on the “many” side are less than the probabilities of the links that would be created. Under the current implementation, the training corpus is one-to-one, which gives our model no opportunity to learn many"
P03-1012,2001.mtsummit-ebmt.4,0,\N,Missing
P05-1034,P03-1012,1,0.660767,"Missing"
P05-1034,J00-1004,0,0.0980277,"Missing"
P05-1034,2004.tmi-1.14,1,0.686027,"Missing"
P05-1034,2003.mtsummit-papers.6,0,0.185854,"Missing"
P05-1034,C04-1090,0,0.373079,"Missing"
P05-1034,W01-1406,1,0.192233,"Missing"
P05-1034,J03-1002,0,0.0553796,"Missing"
P05-1034,P03-1021,0,0.333042,"Missing"
P05-1034,N04-1021,0,0.0987084,"Missing"
P05-1034,P02-1040,0,0.11186,"Missing"
P05-1034,C04-1097,0,0.0277307,"Missing"
P05-1034,2003.mtsummit-papers.53,0,0.0108741,"Missing"
P05-1034,P01-1067,0,0.956293,"Missing"
P05-1034,N04-1014,0,\N,Missing
P05-1034,koen-2004-pharaoh,0,\N,Missing
P05-1034,W99-0604,0,\N,Missing
P05-1034,J99-4005,0,\N,Missing
P05-1034,N04-1023,0,\N,Missing
P05-1034,C04-1030,0,\N,Missing
P05-1034,J93-2003,0,\N,Missing
P05-1034,2001.mtsummit-ebmt.4,1,\N,Missing
P05-1034,C96-2141,0,\N,Missing
P05-1034,P03-1011,0,\N,Missing
P05-1034,W03-0301,0,\N,Missing
P05-1034,P03-2041,0,\N,Missing
P05-1034,N03-1017,0,\N,Missing
P05-1034,P02-1038,0,\N,Missing
P05-1034,P97-1037,0,\N,Missing
P05-1034,J97-3002,0,\N,Missing
P05-1034,W02-1039,0,\N,Missing
P05-1034,P98-2230,0,\N,Missing
P05-1034,C98-2225,0,\N,Missing
P05-1034,N04-1033,0,\N,Missing
P05-1034,P00-1056,0,\N,Missing
P05-1034,W01-1408,0,\N,Missing
P05-1034,P01-1030,0,\N,Missing
P05-1034,P03-1019,0,\N,Missing
P05-1034,W04-1513,0,\N,Missing
P06-2014,P03-1012,1,0.947574,"alignment is said to maintain phrasal cohesion. Fox (2002) measured phrasal cohesion in gold standard alignments by counting crossings. Crossings occur when the projections of two disjoint phrases overlap. For example, Figure 1 shows a head-modifier crossing: the projection of the the tax subtree, impˆot . . . le, is interrupted by the projection of its head, cause. Alignments with no crossings maintain phrasal cohesion. Fox’s experiments show that cohesion is generally maintained for French-English, and that dependency trees produce the highest degree of cohesion among the tested structures. Cherry and Lin (2003) use the phrasal cohesion of a dependency tree as a constraint on a beam search aligner. This constraint produces a significant reduction in alignment error rate. However, as Fox (2002) showed, even in a language pair as close as French-English, there are situations where phrasal cohesion should not be maintained. These include incorrect parses, systematic violations such as not → ne . . . pas, paraphrases, and linguistic exceptions. We aim to create an alignment system that obeys cohesion constraints most of the time, but can violate them when necessary. Unfortunately, Cherry and Lin’s beam s"
P06-2014,E06-1019,1,0.863873,"Missing"
P06-2014,W02-1039,0,0.0448603,"sm, described in (Wu, 1997), is well suited for our purposes. ITGs perform string-to-string alignment, but do so through a parsing algorithm that will allow us to inform the objective function of our dependency tree. Cohesion Constraint Suppose we are given a parse tree for one of the two sentences in our sentence pair. We will refer to the parsed language as English, and the unparsed language as Foreign. Given this information, a reasonable expectation is that English phrases will move together when projected onto Foreign. When this occurs, the alignment is said to maintain phrasal cohesion. Fox (2002) measured phrasal cohesion in gold standard alignments by counting crossings. Crossings occur when the projections of two disjoint phrases overlap. For example, Figure 1 shows a head-modifier crossing: the projection of the the tax subtree, impˆot . . . le, is interrupted by the projection of its head, cause. Alignments with no crossings maintain phrasal cohesion. Fox’s experiments show that cohesion is generally maintained for French-English, and that dependency trees produce the highest degree of cohesion among the tested structures. Cherry and Lin (2003) use the phrasal cohesion of a depend"
P06-2014,H91-1026,0,0.434311,"Missing"
P06-2014,P03-1011,0,0.0426412,"syntax constraints. 2.1 3 Syntax-aware Alignment Search We require an alignment search that can find the globally best alignment under its current objective function, and can account for phrasal cohesion in this objective. IBM Models 1 and 2, HMM (Vogel et al., 1996), and weighted maximum matching alignment all conduct complete searches, but they would not be amenable to monitoring the syntactic interactions of links. The tree-to-string models of (Yamada and Knight, 2001) naturally consider syntax, but special modeling considerations are needed to allow any deviations from the provided tree (Gildea, 2003). The Inversion Transduction Grammar or ITG formalism, described in (Wu, 1997), is well suited for our purposes. ITGs perform string-to-string alignment, but do so through a parsing algorithm that will allow us to inform the objective function of our dependency tree. Cohesion Constraint Suppose we are given a parse tree for one of the two sentences in our sentence pair. We will refer to the parsed language as English, and the unparsed language as Foreign. Given this information, a reasonable expectation is that English phrases will move together when projected onto Foreign. When this occurs, t"
P06-2014,C94-1079,1,0.29748,"quency counts are determined using a sentence-aligned bitext consisting of 50K sentence pairs. Our training set for the discriminative aligners is the first 100 sentence pairs from the FrenchEnglish gold standard provided for the 2003 WPT workshop (Mihalcea and Pedersen, 2003). For evaluation we compare to the remaining 347 gold standard pairs using the alignment evaluation metrics: precision, recall and alignment error rate or AER (Och and Ney, 2003). SVM learning parameters are tuned using the 37-pair development set provided with this data. English dependency trees are provided by Minipar (Lin, 1994). The ψT vector has two new features in addition to those present in the matching system’s ψ. These features can be active only for non-terminal productions, which have the form A → [AA] |hAAi. One feature indicates an inverted production A → hAAi, while the other indicates the use of an invalid span according to a provided English dependency tree, as described in Section 3.2. These are the only features that can be active for nonterminal productions. A terminal production rl that corresponds to a link l is given that link’s features from the match110 Table 1: The effect of hard cohesion const"
P06-2014,P05-1057,0,0.296877,"n Cherry Department of Computing Science University of Alberta Edmonton, AB, Canada, T6G 2E8 colinc@cs.ualberta.ca Abstract to be very different from their IBM counterparts. They model operations that are meaningful at a syntax level, like re-ordering children, but ignore features that have proven useful in IBM models, such as the preference to align words with similar positions, and the HMM preference for links to appear near one another (Vogel et al., 1996). Recently, discriminative learning technology for structured output spaces has enabled several discriminative word alignment solutions (Liu et al., 2005; Moore, 2005; Taskar et al., 2005). Discriminative learning allows easy incorporation of any feature one might have access to during the alignment search. Because the features are handled so easily, discriminative methods use features that are not tied directly to the search: the search and the model become decoupled. In this work, we view synchronous parsing only as a vehicle to expose syntactic features to a discriminative model. This allows us to include the constraints that would usually be imposed by a tree-to-string alignment method as a feature in our model, creating a powerful soft co"
P06-2014,J00-2004,0,0.187379,"itself to a soft cohesion constraint. The imperfect beam search may not be able to find the optimal alignment under a soft constraint. Furthermore, it is not clear what penalty to assign to crossings, or how to learn such a penalty from an iterative training process. The remainder of this paper will develop a complete alignment search that is aware of cohesion violations, and use discriminative learning technology to assign a meaningful penalty to those violations. unrest le malaise Figure 1: A cohesion constraint violation. actly one generator in the source. Methods like competitive linking (Melamed, 2000) and maximum matching (Taskar et al., 2005) use a one-toone constraint, where words in either sentence can participate in at most one link. Throughout this paper we assume a one-to-one constraint in addition to any syntax constraints. 2.1 3 Syntax-aware Alignment Search We require an alignment search that can find the globally best alignment under its current objective function, and can account for phrasal cohesion in this objective. IBM Models 1 and 2, HMM (Vogel et al., 1996), and weighted maximum matching alignment all conduct complete searches, but they would not be amenable to monitoring"
P06-2014,W03-0301,0,0.0160597,"nged, defined in terms of the alignment induced by y. 4. A loss-augmented ITG is used to find the max cost. Productions of the form A → e/f that correspond to links have their scores augmented as in the matching system. 5.1 Experimental setup We conduct our experiments using French-English Hansard data. Our φ2 scores, link probabilities and word frequency counts are determined using a sentence-aligned bitext consisting of 50K sentence pairs. Our training set for the discriminative aligners is the first 100 sentence pairs from the FrenchEnglish gold standard provided for the 2003 WPT workshop (Mihalcea and Pedersen, 2003). For evaluation we compare to the remaining 347 gold standard pairs using the alignment evaluation metrics: precision, recall and alignment error rate or AER (Och and Ney, 2003). SVM learning parameters are tuned using the 37-pair development set provided with this data. English dependency trees are provided by Minipar (Lin, 1994). The ψT vector has two new features in addition to those present in the matching system’s ψ. These features can be active only for non-terminal productions, which have the form A → [AA] |hAAi. One feature indicates an inverted production A → hAAi, while the other in"
P06-2014,H05-1011,0,0.692725,"t of Computing Science University of Alberta Edmonton, AB, Canada, T6G 2E8 colinc@cs.ualberta.ca Abstract to be very different from their IBM counterparts. They model operations that are meaningful at a syntax level, like re-ordering children, but ignore features that have proven useful in IBM models, such as the preference to align words with similar positions, and the HMM preference for links to appear near one another (Vogel et al., 1996). Recently, discriminative learning technology for structured output spaces has enabled several discriminative word alignment solutions (Liu et al., 2005; Moore, 2005; Taskar et al., 2005). Discriminative learning allows easy incorporation of any feature one might have access to during the alignment search. Because the features are handled so easily, discriminative methods use features that are not tied directly to the search: the search and the model become decoupled. In this work, we view synchronous parsing only as a vehicle to expose syntactic features to a discriminative model. This allows us to include the constraints that would usually be imposed by a tree-to-string alignment method as a feature in our model, creating a powerful soft constraint. We"
P06-2014,J03-1002,0,0.282462,"x-margin syntactic aligner with a soft cohesion constraint. The resulting aligner is the first, to our knowledge, to use a discriminative learning method to train an ITG bitext parser. 1 Introduction Given a parallel sentence pair, or bitext, bilingual word alignment finds word-to-word connections across languages. Originally introduced as a byproduct of training statistical translation models in (Brown et al., 1993), word alignment has become the first step in training most statistical translation systems, and alignments are useful to a host of other tasks. The dominant IBM alignment models (Och and Ney, 2003) use minimal linguistic intuitions: sentences are treated as flat strings. These carefully designed generative models are difficult to extend, and have resisted the incorporation of intuitively useful features, such as morphology. There have been many attempts to incorporate syntax into alignment; we will not present a complete list here. Some methods parse two flat strings at once using a bitext grammar (Wu, 1997). Others parse one of the two strings before alignment begins, and align the resulting tree to the remaining string (Yamada and Knight, 2001). The statistical models associated with"
P06-2014,H05-1010,0,0.604001,"g Science University of Alberta Edmonton, AB, Canada, T6G 2E8 colinc@cs.ualberta.ca Abstract to be very different from their IBM counterparts. They model operations that are meaningful at a syntax level, like re-ordering children, but ignore features that have proven useful in IBM models, such as the preference to align words with similar positions, and the HMM preference for links to appear near one another (Vogel et al., 1996). Recently, discriminative learning technology for structured output spaces has enabled several discriminative word alignment solutions (Liu et al., 2005; Moore, 2005; Taskar et al., 2005). Discriminative learning allows easy incorporation of any feature one might have access to during the alignment search. Because the features are handled so easily, discriminative methods use features that are not tied directly to the search: the search and the model become decoupled. In this work, we view synchronous parsing only as a vehicle to expose syntactic features to a discriminative model. This allows us to include the constraints that would usually be imposed by a tree-to-string alignment method as a feature in our model, creating a powerful soft constraint. We add our syntactic feat"
P06-2014,C96-2141,0,0.970649,"nts for Word Alignment through Discriminative Training Dekang Lin Google Inc. 1600 Amphitheatre Parkway Mountain View, CA, USA, 94043 lindek@google.com Colin Cherry Department of Computing Science University of Alberta Edmonton, AB, Canada, T6G 2E8 colinc@cs.ualberta.ca Abstract to be very different from their IBM counterparts. They model operations that are meaningful at a syntax level, like re-ordering children, but ignore features that have proven useful in IBM models, such as the preference to align words with similar positions, and the HMM preference for links to appear near one another (Vogel et al., 1996). Recently, discriminative learning technology for structured output spaces has enabled several discriminative word alignment solutions (Liu et al., 2005; Moore, 2005; Taskar et al., 2005). Discriminative learning allows easy incorporation of any feature one might have access to during the alignment search. Because the features are handled so easily, discriminative methods use features that are not tied directly to the search: the search and the model become decoupled. In this work, we view synchronous parsing only as a vehicle to expose syntactic features to a discriminative model. This allow"
P06-2014,J97-3002,0,0.916039,"alignment has become the first step in training most statistical translation systems, and alignments are useful to a host of other tasks. The dominant IBM alignment models (Och and Ney, 2003) use minimal linguistic intuitions: sentences are treated as flat strings. These carefully designed generative models are difficult to extend, and have resisted the incorporation of intuitively useful features, such as morphology. There have been many attempts to incorporate syntax into alignment; we will not present a complete list here. Some methods parse two flat strings at once using a bitext grammar (Wu, 1997). Others parse one of the two strings before alignment begins, and align the resulting tree to the remaining string (Yamada and Knight, 2001). The statistical models associated with syntactic aligners tend 2 Constrained Alignment Let an alignment be the complete structure that connects two parallel sentences, and a link be one of the word-to-word connections that make up an alignment. All word alignment methods benefit from some set of constraints. These limit the alignment search space and encourage competition between potential links. The IBM models (Brown et al., 1993) benefit from a one-to"
P06-2014,P01-1067,0,0.680579,"ther tasks. The dominant IBM alignment models (Och and Ney, 2003) use minimal linguistic intuitions: sentences are treated as flat strings. These carefully designed generative models are difficult to extend, and have resisted the incorporation of intuitively useful features, such as morphology. There have been many attempts to incorporate syntax into alignment; we will not present a complete list here. Some methods parse two flat strings at once using a bitext grammar (Wu, 1997). Others parse one of the two strings before alignment begins, and align the resulting tree to the remaining string (Yamada and Knight, 2001). The statistical models associated with syntactic aligners tend 2 Constrained Alignment Let an alignment be the complete structure that connects two parallel sentences, and a link be one of the word-to-word connections that make up an alignment. All word alignment methods benefit from some set of constraints. These limit the alignment search space and encourage competition between potential links. The IBM models (Brown et al., 1993) benefit from a one-tomany constraint, where each target word has ex105 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 105–112, c Sydney"
P06-2014,C04-1060,0,0.0228973,"mpared to the matching system. The improved error rate is caused by gains in both precision and recall. This indicates that the invalid span feature is doing more than just ruling out links; perhaps it is de-emphasizing another, less accurate feature’s role. The SD-ITG overrides the cohesion constraint in only 41 of the 347 test sentences, so we can see that it is indeed a soft constraint: it is obeyed nearly all the time, but it can be broken when necessary. The SD-ITG achieves by far the strongest ITG alignment result reported on this French-English set; surpassing the 0.16 AER reported in (Zhang and Gildea, 2004). Training times for this system are quite low; unsupervised statistics can be collected quickly over a large set, while only the 100-sentence training The goal of this experiment is to empirically confirm that the English spans marked invalid by Section 3.2’s dependency-augmented ITG provide useful guidance to an aligner. To do so, we compare an ITG with hard cohesion constraints, an unconstrained ITG, and a weighted maximum matching aligner. All aligners use the same simple objective function. They maximize summed link values v(l), where v(l) is defined as follows for an l = (Ej , Fk ): 2 Pr"
P06-2014,J93-2003,0,\N,Missing
P08-1009,P06-1067,0,0.142503,"Missing"
P08-1009,E06-1032,0,0.016916,"Missing"
P08-1009,P06-2014,1,0.93207,"ually superseded by tree transducers and tree substitution grammars, which allow translation events to span subtree units, providing several advantages, including the ability to selectively produce uncohesive translations (Eisner, 2003; Graehl and Knight, 2004; Quirk et al., 2005). What may have been forgotten during this transition is that there is a reason it was once believed that a cohesive translation model would work: for some language pairs, cohesion explains nearly all translation movement. Fox (2002) showed that cohesion is held in the vast majority of cases for English-French, while Cherry and Lin (2006) have shown it to be a strong feature for word alignment. We attempt to use this strong, but imperfect, characterization of movement to assist a non-syntactic translation method: phrase-based SMT. Phrase-based decoding produces state-of-theart translations with no regard for syntax. We add syntax to this process with a cohesion constraint based on a dependency tree for the source sentence. The constraint allows the decoder to employ arbitrary, non-syntactic phrases, but ensures that those phrases are translated in an order that respects the source tree’s structure. In this way, we target the p"
P08-1009,J07-2003,0,0.049156,"003). This alignment is used to project the spans of subtrees from the source tree onto the target sentence. If a modifier and its head, or two modifiers of the same head, have overlapping spans in the projection, then this indicates a cohesion violation. To check phrasal translations for cohesion violations, we need a way to project the source tree onto the decoder’s output. Fortunately, each phrase used to create the target sentence can be tracked back to its original source phrase, providing an alignment between source and 2 While certainly both syntactic and successful, we consider Hiero (Chiang, 2007) to be a distinct approach, and not an extension to phrasal decoding’s left-to-right beam search. 73 target phrases. Since each source token is used exactly once during translation, we can transform this phrasal alignment into a word-to-phrase alignment, where each source token is linked to a target phrase. We can then project the source subtree spans onto the target phrase sequence. Note that we never consider individual tokens on the target side, as their connection to the source tree is obscured by the phrasal abstraction that occurred during translation. ¯p Let em 1 be the input source sen"
P08-1009,P05-1066,0,0.341873,"and Papineni, 2006; Kuhn et al., 2006). There have been a number of proposals to incorporate syntactic information into phrasal decoding. Early experiments with syntactically-informed phrases (Koehn et al., 2003), and syntactic reranking of K-best lists (Och et al., 2004) produced mostly negative results. The most successful attempts at syntax-enhanced phrasal SMT have directly targeted movement modeling: Zens et al. (2004) modified a phrasal decoder with ITG constraints, while a number of researchers have employed syntax-driven source reordering before decoding begins (Xia and McCord, 2004; Collins et al., 2005; Wang et al., 2007).2 We attempt something between these two approaches: our constraint is derived from a linguistic parse tree, but it is used inside the decoder, not as a preprocessing step. We begin in Section 2 by defining syntactic cohesion so it can be applied to phrasal decoder output. Section 3 describes how to add both hard and soft cohesion constraints to a phrasal decoder. Section 4 provides our results from both automatic and human evaluations. Sections 5 and 6 provide a qualitative discussion of cohesive output and conclude. 2 Cohesive Phrasal Output Previous approaches to measur"
P08-1009,P03-2041,0,0.0377448,"Missing"
P08-1009,W02-1039,0,0.83211,"this assumption in its entirety (Wu, 1997; Yamada and Knight, 2001). These approaches were eventually superseded by tree transducers and tree substitution grammars, which allow translation events to span subtree units, providing several advantages, including the ability to selectively produce uncohesive translations (Eisner, 2003; Graehl and Knight, 2004; Quirk et al., 2005). What may have been forgotten during this transition is that there is a reason it was once believed that a cohesive translation model would work: for some language pairs, cohesion explains nearly all translation movement. Fox (2002) showed that cohesion is held in the vast majority of cases for English-French, while Cherry and Lin (2006) have shown it to be a strong feature for word alignment. We attempt to use this strong, but imperfect, characterization of movement to assist a non-syntactic translation method: phrase-based SMT. Phrase-based decoding produces state-of-theart translations with no regard for syntax. We add syntax to this process with a cohesion constraint based on a dependency tree for the source sentence. The constraint allows the decoder to employ arbitrary, non-syntactic phrases, but ensures that those"
P08-1009,N04-1014,0,0.120917,"Missing"
P08-1009,J99-4005,0,0.253876,"s way, we target the phrasal decoder’s weakness in order modeling, without affecting its strengths. To further increase flexibility, we incorporate cohesion as a decoder feature, creating a soft constraint. The resulting cohesive, phrase-based decoder is shown to produce translations that are preferred over non-cohesive output in both automatic and human evaluations. 1 Introduction Statistical machine translation (SMT) is complicated by the fact that words can move during translation. If one assumes arbitrary movement is possible, that alone is sufficient to show the problem to be NPcomplete (Knight, 1999). Syntactic cohesion1 is the notion that all movement occurring during translation can be explained by permuting children in a parse tree (Fox, 2002). Equivalently, one can say that phrases in the source, defined by subtrees in its parse, remain contiguous after translation. Early ∗ Work conducted while at the University of Alberta. We use the term “syntactic cohesion” throughout this paper to mean what has previously been referred to as “phrasal cohesion”, because the non-linguistic sense of “phrase” has become so common in machine translation literature. 1 Phrase-based decoding (Koehn et al."
P08-1009,N03-1017,0,0.144691,"night, 1999). Syntactic cohesion1 is the notion that all movement occurring during translation can be explained by permuting children in a parse tree (Fox, 2002). Equivalently, one can say that phrases in the source, defined by subtrees in its parse, remain contiguous after translation. Early ∗ Work conducted while at the University of Alberta. We use the term “syntactic cohesion” throughout this paper to mean what has previously been referred to as “phrasal cohesion”, because the non-linguistic sense of “phrase” has become so common in machine translation literature. 1 Phrase-based decoding (Koehn et al., 2003) is a dominant formalism in statistical machine translation. Contiguous segments of the source are translated and placed in the target, which is constructed from left to right. The process iterates within a beam search until each word from the source has been covered by exactly one phrasal translation. Candidate translations are scored by a linear combination of models, weighted according to Minimum Error Rate Training or MERT (Och, 2003). Phrasal SMT draws strength from being able to memorize noncompositional and context-specific translations, as well as local reorderings. Its primary weaknes"
P08-1009,2005.iwslt-1.8,0,0.0379365,"th from being able to memorize noncompositional and context-specific translations, as well as local reorderings. Its primary weakness is in movement modeling; its default distortion model applies a flat penalty to any deviation from source 72 Proceedings of ACL-08: HLT, pages 72–80, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics order, forcing the decoder to rely heavily on its language model. Recently, a number of data-driven distortion models, based on lexical features and relative distance, have been proposed to compensate for this weakness (Tillman, 2004; Koehn et al., 2005; AlOnaizan and Papineni, 2006; Kuhn et al., 2006). There have been a number of proposals to incorporate syntactic information into phrasal decoding. Early experiments with syntactically-informed phrases (Koehn et al., 2003), and syntactic reranking of K-best lists (Och et al., 2004) produced mostly negative results. The most successful attempts at syntax-enhanced phrasal SMT have directly targeted movement modeling: Zens et al. (2004) modified a phrasal decoder with ITG constraints, while a number of researchers have employed syntax-driven source reordering before decoding begins (Xia and McC"
P08-1009,P07-2045,0,0.00748093,"further interruptions becomes difficult to interpret; but the interruption count does provide a useful estimate of the extent to which the translation is faithful to the source tree structure. Initially, we were not certain to what extent this feature would be used by the MERT module, as BLEU is not always sensitive to syntactic improvements. However, trained with our French-English tuning set, the interruption count received the largest absolute feature weight, indicating, at the very least, that the feature is worth scaling to impact decoder. 3.3 Implementation We modify the Moses decoder (Koehn et al., 2007) to translate head-annotated sentences. The decoder stores the flat sentence in the original sentence data structure, and the head-encoded dependency tree in an attached tree data structure. The tree structure caches the source spans corresponding to each of its subtrees. We then implement both a hard check for interruptions to be used before hypotheses are placed on the stack,4 and a soft check that is used to calculate an interruption count feature. 4 A hard cohesion constraint used in conjunction with a traditional distortion limit also requires a second linear-time check to ensure that all"
P08-1009,N06-1004,0,0.0612081,"d context-specific translations, as well as local reorderings. Its primary weakness is in movement modeling; its default distortion model applies a flat penalty to any deviation from source 72 Proceedings of ACL-08: HLT, pages 72–80, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics order, forcing the decoder to rely heavily on its language model. Recently, a number of data-driven distortion models, based on lexical features and relative distance, have been proposed to compensate for this weakness (Tillman, 2004; Koehn et al., 2005; AlOnaizan and Papineni, 2006; Kuhn et al., 2006). There have been a number of proposals to incorporate syntactic information into phrasal decoding. Early experiments with syntactically-informed phrases (Koehn et al., 2003), and syntactic reranking of K-best lists (Och et al., 2004) produced mostly negative results. The most successful attempts at syntax-enhanced phrasal SMT have directly targeted movement modeling: Zens et al. (2004) modified a phrasal decoder with ITG constraints, while a number of researchers have employed syntax-driven source reordering before decoding begins (Xia and McCord, 2004; Collins et al., 2005; Wang et al., 2007"
P08-1009,N03-2017,1,0.885292,"traint is derived from a linguistic parse tree, but it is used inside the decoder, not as a preprocessing step. We begin in Section 2 by defining syntactic cohesion so it can be applied to phrasal decoder output. Section 3 describes how to add both hard and soft cohesion constraints to a phrasal decoder. Section 4 provides our results from both automatic and human evaluations. Sections 5 and 6 provide a qualitative discussion of cohesive output and conclude. 2 Cohesive Phrasal Output Previous approaches to measuring the cohesion of a sentence pair have worked with a word alignment (Fox, 2002; Lin and Cherry, 2003). This alignment is used to project the spans of subtrees from the source tree onto the target sentence. If a modifier and its head, or two modifiers of the same head, have overlapping spans in the projection, then this indicates a cohesion violation. To check phrasal translations for cohesion violations, we need a way to project the source tree onto the decoder’s output. Fortunately, each phrase used to create the target sentence can be tracked back to its original source phrase, providing an alignment between source and 2 While certainly both syntactic and successful, we consider Hiero (Chia"
P08-1009,C94-1079,0,0.00920418,"red task. We decode with Moses, using a stack size of 100, a beam threshold of 0.03 and a distortion limit of 4. Weights for the log-linear model are set using MERT, as implemented by Venugopal and Vogel (2005). Our tuning set is the first 500 sentences of the SMT06 development data. We hold out the remaining 1500 development sentences for development testing (dev-test), and the entirety of the provided 2000-sentence test set for blind testing (test). Since we require source dependency trees, all experiments test English to French translation. English dependency trees are provided by Minipar (Lin, 1994). Our cohesion constraint directly targets sentences for which an unmodified phrasal decoder produces uncohesive output according to the definition in Section 2. Therefore, we present our results not only on each test set in its entirety, but also on the subsets defined by whether or not the baseline naturally produces a cohesive translation. The sizes of the resulting evaluation sets are given in Table 2. Our development tests indicated that the soft and hard cohesion constraints performed somewhat sim77 4.2 Automatic Evaluation We first present our soft cohesion constraint’s effect on BLEU s"
P08-1009,J03-1002,0,0.00436847,"also defines our evaluation subsets. 4 Experiments We have adapted the notion of syntactic cohesion so that it is applicable to phrase-based decoding. This results in a translation process that respects sourceside syntactic boundaries when distorting phrases. In this section we will test the impact of such information on an English to French translation task. 4.1 Experimental Details We test our cohesion-enhanced Moses decoder trained using 688K sentence pairs of Europarl French-English data, provided by the SMT 2006 Shared Task (Koehn and Monz, 2006). Word alignments are provided by GIZA++ (Och and Ney, 2003) with grow-diag-final combination, with infrastructure for alignment combination and phrase extraction provided by the shared task. We decode with Moses, using a stack size of 100, a beam threshold of 0.03 and a distortion limit of 4. Weights for the log-linear model are set using MERT, as implemented by Venugopal and Vogel (2005). Our tuning set is the first 500 sentences of the SMT06 development data. We hold out the remaining 1500 development sentences for development testing (dev-test), and the entirety of the provided 2000-sentence test set for blind testing (test). Since we require sourc"
P08-1009,N04-1021,0,0.0296775,"Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics order, forcing the decoder to rely heavily on its language model. Recently, a number of data-driven distortion models, based on lexical features and relative distance, have been proposed to compensate for this weakness (Tillman, 2004; Koehn et al., 2005; AlOnaizan and Papineni, 2006; Kuhn et al., 2006). There have been a number of proposals to incorporate syntactic information into phrasal decoding. Early experiments with syntactically-informed phrases (Koehn et al., 2003), and syntactic reranking of K-best lists (Och et al., 2004) produced mostly negative results. The most successful attempts at syntax-enhanced phrasal SMT have directly targeted movement modeling: Zens et al. (2004) modified a phrasal decoder with ITG constraints, while a number of researchers have employed syntax-driven source reordering before decoding begins (Xia and McCord, 2004; Collins et al., 2005; Wang et al., 2007).2 We attempt something between these two approaches: our constraint is derived from a linguistic parse tree, but it is used inside the decoder, not as a preprocessing step. We begin in Section 2 by defining syntactic cohesion so it"
P08-1009,P03-1021,0,0.00909106,"as “phrasal cohesion”, because the non-linguistic sense of “phrase” has become so common in machine translation literature. 1 Phrase-based decoding (Koehn et al., 2003) is a dominant formalism in statistical machine translation. Contiguous segments of the source are translated and placed in the target, which is constructed from left to right. The process iterates within a beam search until each word from the source has been covered by exactly one phrasal translation. Candidate translations are scored by a linear combination of models, weighted according to Minimum Error Rate Training or MERT (Och, 2003). Phrasal SMT draws strength from being able to memorize noncompositional and context-specific translations, as well as local reorderings. Its primary weakness is in movement modeling; its default distortion model applies a flat penalty to any deviation from source 72 Proceedings of ACL-08: HLT, pages 72–80, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics order, forcing the decoder to rely heavily on its language model. Recently, a number of data-driven distortion models, based on lexical features and relative distance, have been proposed to compensate for this"
P08-1009,P02-1040,0,0.107203,"cohesion constraint directly targets sentences for which an unmodified phrasal decoder produces uncohesive output according to the definition in Section 2. Therefore, we present our results not only on each test set in its entirety, but also on the subsets defined by whether or not the baseline naturally produces a cohesive translation. The sizes of the resulting evaluation sets are given in Table 2. Our development tests indicated that the soft and hard cohesion constraints performed somewhat sim77 4.2 Automatic Evaluation We first present our soft cohesion constraint’s effect on BLEU score (Papineni et al., 2002) for both our dev-test and test sets. We compare against an unmodified baseline decoder, as well as a decoder enhanced with a lexical reordering model (Tillman, 2004; Koehn et al., 2005). For each phrase pair in our translation table, the lexical reordering model tracks statistics on its reordering behavior as observed in our word-aligned training text. The lexical reordering model provides a good comparison point as a non-syntactic, and potentially orthogonal, improvement to phrase-based movement modeling. We use the implementation provided in Moses, with probabilities conditioned on bilingua"
P08-1009,P05-1034,1,0.859149,"Missing"
P08-1009,N04-4026,0,0.107963,"MT draws strength from being able to memorize noncompositional and context-specific translations, as well as local reorderings. Its primary weakness is in movement modeling; its default distortion model applies a flat penalty to any deviation from source 72 Proceedings of ACL-08: HLT, pages 72–80, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics order, forcing the decoder to rely heavily on its language model. Recently, a number of data-driven distortion models, based on lexical features and relative distance, have been proposed to compensate for this weakness (Tillman, 2004; Koehn et al., 2005; AlOnaizan and Papineni, 2006; Kuhn et al., 2006). There have been a number of proposals to incorporate syntactic information into phrasal decoding. Early experiments with syntactically-informed phrases (Koehn et al., 2003), and syntactic reranking of K-best lists (Och et al., 2004) produced mostly negative results. The most successful attempts at syntax-enhanced phrasal SMT have directly targeted movement modeling: Zens et al. (2004) modified a phrasal decoder with ITG constraints, while a number of researchers have employed syntax-driven source reordering before decoding"
P08-1009,2005.eamt-1.36,0,0.0255016,"an English to French translation task. 4.1 Experimental Details We test our cohesion-enhanced Moses decoder trained using 688K sentence pairs of Europarl French-English data, provided by the SMT 2006 Shared Task (Koehn and Monz, 2006). Word alignments are provided by GIZA++ (Och and Ney, 2003) with grow-diag-final combination, with infrastructure for alignment combination and phrase extraction provided by the shared task. We decode with Moses, using a stack size of 100, a beam threshold of 0.03 and a distortion limit of 4. Weights for the log-linear model are set using MERT, as implemented by Venugopal and Vogel (2005). Our tuning set is the first 500 sentences of the SMT06 development data. We hold out the remaining 1500 development sentences for development testing (dev-test), and the entirety of the provided 2000-sentence test set for blind testing (test). Since we require source dependency trees, all experiments test English to French translation. English dependency trees are provided by Minipar (Lin, 1994). Our cohesion constraint directly targets sentences for which an unmodified phrasal decoder produces uncohesive output according to the definition in Section 2. Therefore, we present our results not"
P08-1009,D07-1077,0,0.190447,"uhn et al., 2006). There have been a number of proposals to incorporate syntactic information into phrasal decoding. Early experiments with syntactically-informed phrases (Koehn et al., 2003), and syntactic reranking of K-best lists (Och et al., 2004) produced mostly negative results. The most successful attempts at syntax-enhanced phrasal SMT have directly targeted movement modeling: Zens et al. (2004) modified a phrasal decoder with ITG constraints, while a number of researchers have employed syntax-driven source reordering before decoding begins (Xia and McCord, 2004; Collins et al., 2005; Wang et al., 2007).2 We attempt something between these two approaches: our constraint is derived from a linguistic parse tree, but it is used inside the decoder, not as a preprocessing step. We begin in Section 2 by defining syntactic cohesion so it can be applied to phrasal decoder output. Section 3 describes how to add both hard and soft cohesion constraints to a phrasal decoder. Section 4 provides our results from both automatic and human evaluations. Sections 5 and 6 provide a qualitative discussion of cohesive output and conclude. 2 Cohesive Phrasal Output Previous approaches to measuring the cohesion of"
P08-1009,J97-3002,0,0.0752541,"Missing"
P08-1009,C04-1073,0,0.0501236,"t al., 2005; AlOnaizan and Papineni, 2006; Kuhn et al., 2006). There have been a number of proposals to incorporate syntactic information into phrasal decoding. Early experiments with syntactically-informed phrases (Koehn et al., 2003), and syntactic reranking of K-best lists (Och et al., 2004) produced mostly negative results. The most successful attempts at syntax-enhanced phrasal SMT have directly targeted movement modeling: Zens et al. (2004) modified a phrasal decoder with ITG constraints, while a number of researchers have employed syntax-driven source reordering before decoding begins (Xia and McCord, 2004; Collins et al., 2005; Wang et al., 2007).2 We attempt something between these two approaches: our constraint is derived from a linguistic parse tree, but it is used inside the decoder, not as a preprocessing step. We begin in Section 2 by defining syntactic cohesion so it can be applied to phrasal decoder output. Section 3 describes how to add both hard and soft cohesion constraints to a phrasal decoder. Section 4 provides our results from both automatic and human evaluations. Sections 5 and 6 provide a qualitative discussion of cohesive output and conclude. 2 Cohesive Phrasal Output Previou"
P08-1009,P01-1067,0,0.418459,"Missing"
P08-1009,C04-1030,0,0.373311,", a number of data-driven distortion models, based on lexical features and relative distance, have been proposed to compensate for this weakness (Tillman, 2004; Koehn et al., 2005; AlOnaizan and Papineni, 2006; Kuhn et al., 2006). There have been a number of proposals to incorporate syntactic information into phrasal decoding. Early experiments with syntactically-informed phrases (Koehn et al., 2003), and syntactic reranking of K-best lists (Och et al., 2004) produced mostly negative results. The most successful attempts at syntax-enhanced phrasal SMT have directly targeted movement modeling: Zens et al. (2004) modified a phrasal decoder with ITG constraints, while a number of researchers have employed syntax-driven source reordering before decoding begins (Xia and McCord, 2004; Collins et al., 2005; Wang et al., 2007).2 We attempt something between these two approaches: our constraint is derived from a linguistic parse tree, but it is used inside the decoder, not as a preprocessing step. We begin in Section 2 by defining syntactic cohesion so it can be applied to phrasal decoder output. Section 3 describes how to add both hard and soft cohesion constraints to a phrasal decoder. Section 4 provides o"
P08-1009,J08-3004,0,\N,Missing
P08-1009,D08-1076,0,\N,Missing
P08-1065,P07-1013,0,0.364566,"Missing"
P08-1065,N07-1047,1,0.73259,"pear in both the training and test sets. This makes the problem much easier with large training sets, where the chance of this sort of overlap becomes high. Therefore, any large data results may be slightly inflated as a prediction of actual out-of-dictionary performance. 6 L2P Performance As we stated from the outset, one of our primary motivations for exploring orthographic syllabification is the improvements it can produce in L2P systems. To explore this, we tested our model in conjunction with a recent L2P system that has been shown to predict phonemes with state-of-the-art word accuracy (Jiampojamarn et al., 2007). Using a model derived from training data, this L2P system first divides a word into letter chunks, each containing one or two letters. A local classifier then predicts a number of likely phonemes for each chunk, with confidence values. A phoneme-sequence Markov model is then used to select the most likely sequence from the phonemes proposed by the local classifier. Syllabification None Numbered NB Break ONC Dictionary English 84.67 85.55 85.59 86.29 Dutch 91.56 92.60 N/A 93.03 German 90.18 90.59 N/A 90.57 Table 3: Word accuracy percentage on the letter-tophoneme task with and without the syl"
P08-1065,P01-1053,0,0.172004,"Missing"
P08-1103,W06-3206,0,0.159283,"Missing"
P08-1103,W98-1224,0,0.596528,"Missing"
P08-1103,W02-1001,0,0.537739,"sequence can propagate forward and throw off later processing. Second, each module is trained independently, and the training methods are not aware of the tasks performed later in the pipeline. For example, optimal parameters for a phoneme prediction module may vary depending on whether or not the module will be used in conjunction with a phoneme sequence model. We propose a joint approach to L2P conversion, grounded in dynamic programming and online discriminative training. We view L2P as a tagging task that can be performed with a discriminative learning method, such as the Perceptron HMM (Collins, 2002). The Perceptron HMM naturally handles phoneme prediction (#1) and sequence modeling (#3) simultaneously, as shown in Figure 1b. Furthermore, unlike a generative HMM, it can incorporate many overlapping source n-gram features to represent context. In order to complete the conversion from a pipeline approach to a joint approach, we fold our input segmentation step into the exact search framework by replacing a separate segmentation module (#2) with a monotone phrasal decoder (Zens and Ney, 2004). At this point all three of our desiderata are incorporated into a single module, 907 as shown in Fi"
P08-1103,P07-1013,0,0.269044,"Missing"
P08-1103,2005.iwslt-1.22,0,0.0906111,"Missing"
P08-1103,N07-1047,1,0.927132,"ks, called letter-to-phoneme alignment, is not always straightforward. For example, consider the word “phoenix” and its corresponding phoneme sequence [f i n I k s], where we encounter cases of two letters generating a single phoneme (ph→f), and a single letter generating two phonemes (x→k s). Fortunately, alignments between letters and phonemes can be discovered reliably with unsupervised generative models. Originally, L2P systems assumed one-to-one alignment (Black et al., 1998; Damper et al., 2005), but recently many-to-many alignment has been shown to perform better (Bisani and Ney, 2002; Jiampojamarn et al., 2007). Given such an alignment, L2P can be viewed either as a sequence of classification problems, or as a sequence modeling problem. In the classification approach, each phoneme is predicted independently using a multi-class classifier such as decision trees (Daelemans and Bosch, 1997; Black et al., 1998) or instance-based learning (Bosch and Daelemans, 1998). These systems predict a phoneme for each input letter, using the letter and its context as features. They leverage the structure of the input but ignore any structure in the output. L2P can also be viewed as a sequence modeling, or tagging p"
P08-1103,N06-1030,0,0.23683,"Missing"
P08-1103,J00-2003,0,0.326833,"for each input letter, using the letter and its context as features. They leverage the structure of the input but ignore any structure in the output. L2P can also be viewed as a sequence modeling, or tagging problem. These approaches model the structure of the output, allowing previously predicted phonemes to inform future decisions. The supervised Hidden Markov Model (HMM) applied by Taylor (2005) achieved poor results, mostly because its maximum-likelihood emission probabilities cannot be informed by the emitted letter’s context. Other approaches, such as those of Bisani and Ney (2002) and Marchand and Damper (2000), have shown that better performance can be achieved by pairing letter substrings with phoneme substrings, allowing context to be captured implicitly by these groupings. Recently, two hybrid methods have attempted to capture the flexible context handling of classification-based methods, while also modeling the sequential nature of the output. The 906 constraint satisfaction inference (CSInf) approach (Bosch and Canisius, 2006) improves the performance of instance-based classification (Bosch and Daelemans, 1998) by predicting for each letter a trigram of phonemes consisting of the previous, cur"
P08-1103,N04-1033,0,0.119611,"s a tagging task that can be performed with a discriminative learning method, such as the Perceptron HMM (Collins, 2002). The Perceptron HMM naturally handles phoneme prediction (#1) and sequence modeling (#3) simultaneously, as shown in Figure 1b. Furthermore, unlike a generative HMM, it can incorporate many overlapping source n-gram features to represent context. In order to complete the conversion from a pipeline approach to a joint approach, we fold our input segmentation step into the exact search framework by replacing a separate segmentation module (#2) with a monotone phrasal decoder (Zens and Ney, 2004). At this point all three of our desiderata are incorporated into a single module, 907 as shown in Figure 1c. Our joint approach to L2P lends itself to several refinements. We address an underfitting problem of the perceptron by replacing it with a more robust Margin Infused Relaxed Algorithm (MIRA), which adds an explicit notion of margin and takes into account the system’s current n-best outputs. In addition, with all of our features collected under a unified framework, we are free to conjoin context features with sequence features to create a powerful linearchain model (Sutton and McCallum,"
P08-1103,P02-1019,0,\N,Missing
P09-1055,P08-1103,1,0.624672,"cess to tags and ask it to predict a single lemma for each word in testing. Our joint model, described in Section 5, is defined in a re-ranking framework, and can choose from among k-best predictions of tag-sets and lemmas generated from the component tagger and lemmatizer models. 4.1 Morphological analyser We employ a discriminative character transducer as a component morphological analyzer. The input to the transducer is an inflected word (the source) and possibly an estimated part-of-speech; the output is the lemma of the word (the target). The transducer is similar to the one described by Jiampojamarn et al. (2008) for letter-to-phoneme conversion, but extended to allow for whole-word features on both the input and the output. The core of our engine is the dynamic programming algorithm for monotone phrasal decoding (Zens and Ney, 2004). The main feature of this algorithm is its capability to transduce many consecutive characters with a single operation; the same algorithm is employed to tag subsequences in semi-Markov CRFs (Sarawagi and Cohen, 2004). We employ three main categories of features: context, transition, and vocabulary (rootlist) features. The first two are described in detail by Jiampojamarn"
P09-1055,P08-1083,0,0.0228182,"sducer which is related to these approaches and draws on previous work in this and related string transduction areas. Our transducer is described in detail in Section 4.1. Another related line of work approaches the disambiguation problem directly, where the task is to predict the correct analysis of word-forms in context (in sentences), and not all possible analyses. In such work it is often assumed that the correct POS tags can be predicted with high accuracy using labeled POS-disambiguated sentences (Erjavec and Dˇzeroski, 2004; Habash and Rambow, 2005). A notable exception is the work of (Adler et al., 2008), which uses unlabeled data and a morphological analyzer to learn a semi-supervised HMM model for disambiguation in context, and also guesses analyses for unknown words using a guesser of likely POS-tags. It is most closely related to our work, but does not attempt to predict all possible analyses, and does not have to tackle a complex string transduction problem for lemmatization since segmentation is mostly sufficient for the focus language of that study (Hebrew). The idea of solving two related tasks jointly to improve performance on both has been successful for other pairs of tasks (e.g.,"
P09-1055,J93-2004,0,0.0308704,"to all words in the graph.4 We 6 Experiments 6.1 Data We use datasets for four languages: English, Bulgarian, Slovene, and Czech. For each of the languages, we need a lexicon with morphological analyses L and unlabeled text. For English we derive the lexicon from CELEX (Baayen et al., 1995), and for the other languages we use the Multext-East resources (Erjavec, 2004). For English we use only open-class words (nouns, verbs, adjectives, and adverbs), and for the other languages we use words of all classes. The unlabeled data for English we use is the union of the Penn Treebank tagged WSJ data (Marcus et al., 1993) and the BLLIP corpus.5 For the rest of the languages we use only the text of George Orwell’s novel 1984, which is provided in morphologically disambiguated form as part of MultextEast (but we don’t use the annotations). Table 2 4 We start the Gibbs sampler by the assignments found by the pipeline method and then use an annealing schedule to find a neighborhood of high-likelihood assignments, before taking about 10 complete samples from the graph to compute expectations. 5 The BLLIP corpus contains approximately 30 million words of automatically parsed WSJ data. We used these corpora as plain"
P09-1055,W04-3220,0,0.0153107,"which uses unlabeled data and a morphological analyzer to learn a semi-supervised HMM model for disambiguation in context, and also guesses analyses for unknown words using a guesser of likely POS-tags. It is most closely related to our work, but does not attempt to predict all possible analyses, and does not have to tackle a complex string transduction problem for lemmatization since segmentation is mostly sufficient for the focus language of that study (Hebrew). The idea of solving two related tasks jointly to improve performance on both has been successful for other pairs of tasks (e.g., (Andrew et al., 2004)). Doing joint inference instead of taking a pipeline approach has also been shown useful for other problems (e.g., (Finkel et al., 2006; Cohen and Smith, 2007)). Related work In work on morphological analysis using machine learning, the task is rarely addressed in the form described above. Some exceptions are the work (Bosch and Daelemans, 1999) which presents a model for segmenting, stemming, and tagging words in Dutch, and requires the prediction of all possible analyses, and (Antal van den Bosch and Soudi, 2007) which similarly requires the prediction of all morpho-syntactically annotated"
P09-1055,P99-1037,0,0.120438,"Missing"
P09-1055,P02-1065,0,0.0249792,"T but there are no guarantees about the coverage of the target lemmas or the number of noise words which may occur in T (see Table 2 for data statistics). Our setting is thus more realistic since it is what one would have in a real application scenario. 3 our work, these approaches do not make use of unlabeled data and make predictions for each word type in isolation. In machine learning work on lemmatization for highly inflective languages, it is most often assumed that a word form and a POS tag are given, and the task is to predict the set of corresponding lemma(s) (Mooney and Califf, 1995; Clark, 2002; Wicentowski, 2002; Erjavec and Dˇzeroski, 2004; Dreyer et al., 2008). In our task setting, we do not assume the availability of gold-standard POS tags. As a component model, we use a lemmatizing string transducer which is related to these approaches and draws on previous work in this and related string transduction areas. Our transducer is described in detail in Section 4.1. Another related line of work approaches the disambiguation problem directly, where the task is to predict the correct analysis of word-forms in context (in sentences), and not all possible analyses. In such work it is of"
P09-1055,N04-1033,0,0.0189474,"from the component tagger and lemmatizer models. 4.1 Morphological analyser We employ a discriminative character transducer as a component morphological analyzer. The input to the transducer is an inflected word (the source) and possibly an estimated part-of-speech; the output is the lemma of the word (the target). The transducer is similar to the one described by Jiampojamarn et al. (2008) for letter-to-phoneme conversion, but extended to allow for whole-word features on both the input and the output. The core of our engine is the dynamic programming algorithm for monotone phrasal decoding (Zens and Ney, 2004). The main feature of this algorithm is its capability to transduce many consecutive characters with a single operation; the same algorithm is employed to tag subsequences in semi-Markov CRFs (Sarawagi and Cohen, 2004). We employ three main categories of features: context, transition, and vocabulary (rootlist) features. The first two are described in detail by Jiampojamarn et al. (2008), while the final is novel to this work. Context features are centered around a transduction operation such as es → e, as employed in gives → give. Context features include an indicator for the operation itself,"
P09-1055,D07-1022,0,0.0151303,"own words using a guesser of likely POS-tags. It is most closely related to our work, but does not attempt to predict all possible analyses, and does not have to tackle a complex string transduction problem for lemmatization since segmentation is mostly sufficient for the focus language of that study (Hebrew). The idea of solving two related tasks jointly to improve performance on both has been successful for other pairs of tasks (e.g., (Andrew et al., 2004)). Doing joint inference instead of taking a pipeline approach has also been shown useful for other problems (e.g., (Finkel et al., 2006; Cohen and Smith, 2007)). Related work In work on morphological analysis using machine learning, the task is rarely addressed in the form described above. Some exceptions are the work (Bosch and Daelemans, 1999) which presents a model for segmenting, stemming, and tagging words in Dutch, and requires the prediction of all possible analyses, and (Antal van den Bosch and Soudi, 2007) which similarly requires the prediction of all morpho-syntactically annotated segmentations of words for Arabic. As opposed to 2 These settings refer to the availability of a set of word forms which are possible lemmas; in the no rootlist"
P09-1055,W02-1001,0,0.134599,"the target context tracked by our dynamic programming chart, we can efficiently track these frequencies during transduction. We incorporate the source part-of-speech tag by appending it to each feature, thus the context feature es → e may become es → e, VBZ. To enable communication between the various parts-ofspeech, a universal set of unannotated features also fires, regardless of the part-of-speech, acting as a back-off model of how words in general behave during stemming. Linear weights are assigned to each of the transducer’s features using an averaged perceptron for structure prediction (Collins, 2002). Note that our features are defined in terms of the operations employed during transduction, therefore to create gold-standard feature vectors, we require not only target outputs, but also derivations to produce those outputs. We employ a deterministic heuristic to create these derivations; given a goldstandard source-target pair, we construct a derivation that uses only trivial copy operations until the first character mismatch. The remainder of the transduction is performed with a single multicharacter replacement. For example, the derivation for living → live would be l → l , i → i , v → v"
P09-1055,P00-1035,0,0.0347005,"r instead of a Naive Bayes model, and second, we use features derived from related words appearing in T. The possible classes predicted by the classifier are as many as the observed tag-sets in L. The sparsity is relieved by adding features for individual tags t which get shared across tag-sets containing t. There are two types of features in the model: (i) word-internal features: word suffixes, capitalization, existence of hyphen, and word prefixes (such features were also used in (Toutanova and Johnson, 2008)), and (ii) features based on related words. These latter features are inspired by (Cucerzan and Yarowsky, 2000) and are defined as follows: for a word w such as telling, there is an indicator feature for every combination of two suffixes α and β, such that there is a prefix p where telling= pα and pβ exists in T. For example, if the word tells is found in T, there would be a feature for the suffixes α=ing,β=s that fires. The suffixes are defined as all character suffixes up to length three which occur with at least 100 words. 489 for word wi , for j = 1 . . . k. Also, let li (t)j denote the top lemmas for word wi given tag t. An assignment of a tag-set and lemmas to a word wi consists of a choice of a"
P09-1055,D08-1113,0,0.0803994,"Missing"
P09-1055,erjavec-2004-multext,0,0.0466541,"tion to the lexicon, we are allowed to make use of unannotated text T in the language. We will predict morphological analyses for words which occur in T. Note that the task is defined on word types and not on words in context. A morphological analysis of a word w consists of a (possibly structured) POS tag t, together with one or several lemmas, which are the possible basic forms of w when it has tag t. As an example, Table 1 illustrates the morphological analyses of several words taken from the CELEX lexical database of English (Baayen et al., 1995) and the Multext-East lexicon of Bulgarian (Erjavec, 2004). The Bulgarian words are transcribed in 1 Tag sets are useful, for example, as a basis of sparsityreducing features for text labeling tasks; lemmatization is useful for information retrieval and machine translation from a morphologically rich to a morphologically poor language, where full analysis may not be important. 486 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 486–494, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP Word Forms tell told tells telling izpravena izpraviha Morphological Analyses verb base (VB), tell verb past tense (VBD"
P09-1055,W06-1673,0,0.0247872,"Missing"
P09-1055,P05-1071,0,0.0282082,"S tags. As a component model, we use a lemmatizing string transducer which is related to these approaches and draws on previous work in this and related string transduction areas. Our transducer is described in detail in Section 4.1. Another related line of work approaches the disambiguation problem directly, where the task is to predict the correct analysis of word-forms in context (in sentences), and not all possible analyses. In such work it is often assumed that the correct POS tags can be predicted with high accuracy using labeled POS-disambiguated sentences (Erjavec and Dˇzeroski, 2004; Habash and Rambow, 2005). A notable exception is the work of (Adler et al., 2008), which uses unlabeled data and a morphological analyzer to learn a semi-supervised HMM model for disambiguation in context, and also guesses analyses for unknown words using a guesser of likely POS-tags. It is most closely related to our work, but does not attempt to predict all possible analyses, and does not have to tackle a complex string transduction problem for lemmatization since segmentation is mostly sufficient for the focus language of that study (Hebrew). The idea of solving two related tasks jointly to improve performance on"
P09-1055,P08-1000,0,\N,Missing
P11-1075,W07-1014,0,0.0238492,"r hand-crafted rules can be built in a semi-automatic way: the initial set of rules adopted from the official coding guidelines were automatically extended with additional synonyms and code dependency rules generated from the training data (Farkas and Szarvas, 2008). Statistical systems trained on only text-derived features (such as n-grams) did not show good performance due to a wide variety of medical language and a relatively small training set (Goldstein et al., 2007). This led to the creation of hybrid systems: symbolic and statistical classifiers used together in an ensemble or cascade (Aronson et al., 2007; Crammer et al., 2007) or a symbolic component providing features for a statistical component (Patrick et al., 2007; Suominen et al., 2008). Strong competition systems had good answers for dealing with negative and speculative contexts, taking advantage of the competition’s limited set of possible code combinations, and handling of low-frequency codes. Our proposed approach is a combination system as well. We combine a symbolic component that matches lexical strings of a document against a medical dictionary to determine possible codes (Lussier et al., 2000; Kevers and Medori, 2010) and a sta"
P11-1075,W02-1001,0,0.0224538,"raining time is less than one minute on modern hardware. 5 Experiments (1) 5.1 (x,y)∈S where   0 0 δ(y, y ) + w · φ(x, y , y) `(w; (x, y)) = max 0 y (2) and where δ(y, y 0 ) = 0 when y = y 0 and 1 otherwise.4 Intuitively, the objective attempts to find a small weight vector w that separates all incorrect tag sequences y 0 from the correct tag sequence y by a margin of 1. λ controls the trade-off between regularization and training hinge-loss. The stochastic gradient descent algorithm used to optimize this objective is shown in Figure 2. It bears many similarities to perceptron HMM training (Collins, 2002), with theoretically-motivated alterations, such as selecting training points at random5 and the explicit inclusion of a learning rate η 4 We did not experiment with structured versions of δ that account for the number of incorrect tags in the label sequence y 0 , as a fixed margin was already working very well. We intend to explore structured costs in future work. 5 Like many implementations, we make n passes through S, shuffling S before each pass, rather than sampling from S with replacement n|S |times. 748 Data For testing purposes, we use the CMC Challenge dataset. The data consists of 97"
P11-1075,W07-1017,0,0.0183002,"an be built in a semi-automatic way: the initial set of rules adopted from the official coding guidelines were automatically extended with additional synonyms and code dependency rules generated from the training data (Farkas and Szarvas, 2008). Statistical systems trained on only text-derived features (such as n-grams) did not show good performance due to a wide variety of medical language and a relatively small training set (Goldstein et al., 2007). This led to the creation of hybrid systems: symbolic and statistical classifiers used together in an ensemble or cascade (Aronson et al., 2007; Crammer et al., 2007) or a symbolic component providing features for a statistical component (Patrick et al., 2007; Suominen et al., 2008). Strong competition systems had good answers for dealing with negative and speculative contexts, taking advantage of the competition’s limited set of possible code combinations, and handling of low-frequency codes. Our proposed approach is a combination system as well. We combine a symbolic component that matches lexical strings of a document against a medical dictionary to determine possible codes (Lussier et al., 2000; Kevers and Medori, 2010) and a statistical component that"
P11-1075,W07-1027,0,0.124129,"esting challenges for natural language processing. Conventionally, most clinical documentation, such as doctor’s notes, discharge summaries and referrals, are written in a free-text form. This narrative form is flexible, allowing healthcare professionals to express any kind of concept or event, but it is not particularly suited for large-scale analysis, search, Traditionally, statistical document coding is viewed as multi-class multi-label document classification, where each clinical free-text document is labelled with one or several codes from a pre-defined, possibly very large set of codes (Patrick et al., 2007; Suominen et al., 2008). One classification model is learned for each code, and then all models are applied in turn to a new document to determine which codes should be assigned to the document. The drawback of this approach is poor predictive performance on low-frequency codes, which are ubiquitous in the clinical domain. This paper presents a novel approach to document coding that simultaneously models code-specific as well as general patterns in the data. This allows 742 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 742–751, c Portland, Oreg"
P11-1075,W07-1013,0,0.0163774,"s assessed in terms of both its local context (item 1) and the presence of other candidate codes for the document (items 2 and 3). 1 Note that dictionary-based trigger detection could be replaced by tagging approaches similar to those used in namedentity-recognition or information extraction. 2.2 ICD-9-CM Coding As a specific application we have chosen the task of assigning ICD-9-CM codes to free-form clinical narratives. We use the dataset collected for the 2007 Medical NLP Challenge organized by the Computational Medicine Center in Cincinnati, Ohio, hereafter refereed to as “CMC Challenge” (Pestian et al., 2007). For this challenge, 1954 radiology reports on outpatient chest x-ray and renal procedures were collected, disambiguated, and anonymized. The reports were annotated with ICD-9-CM codes by three coding companies, and the majority codes were selected as a gold standard. In total, 45 distinct codes were used. For this task, our use of a dictionary to detect lexical triggers is quite reasonable. The medical domain is rich with manually-created and carefullymaintained knowledge resources. In particular, the ICD-9-CM coding guidelines come with an index file that contains hundreds of thousands of t"
P11-1075,D10-1102,0,0.0268207,"Missing"
P11-2035,C10-1007,1,0.885113,"ch tag our data using a perceptron tagger similar to the one described by Collins (2002). The training set is tagged with jack-knifing: the data is split into 10 folds and each fold is tagged by a system trained on the other 9 folds. Development and test sets are tagged using the entire training set. We train our joint filter using an in-house latent SVM framework, which repeatedly calls a multiclass exponentiated gradient SVM (Collins et al., 2008). LSVM training was stopped after 4 iterations, as determined during development.5 For the token-role classifiers, we re-implement the Bergsma and Cherry (2010) feature set, initializing w ¯ with high-precision subclassifiers trained independently for each token-role. Vine and None subclassifiers are initialized with a zero vector. At test time, we extract subclassifiers from the joint weight vector, and use them as parameters in the filtering tools of Bergsma and Cherry (2010).6 Parsing experiments are carried out using the MST parser (McDonald et al., 2005),7 which we have modified to filter arcs before carrying out feature extraction. It is trained using 5-best MIRA (Crammer and Singer, 2003). Following Bergsma and Cherry (2010), we measure intrin"
P11-2035,N10-1066,0,0.0230471,"g that the head the 3 is not the head of any arc, or that the modifier his 6 attaches elsewhere. Role truth values, derived from the gold-standard tree (in grey), are listed adjacent to the boxes, in parentheses. classifier is given credit for eliminating an arc. The identity of the responsible classifier is modeled as a latent variable, which is filled in during training using a latent SVM (LSVM) formulation. Our use of an LSVM to assign credit during joint training differs substantially from previous LSVM applications, which have induced latent linguistic structures (Cherry and Quirk, 2008; Chang et al., 2010) or sentence labels (Yessenalina et al., 2010). In our framework, each classifier learns to focus on the cases where the other classifiers are less confident. Furthermore, the integrated approach directly optimizes for arc-filtering accuracy (rather than token-labeling fidelity). We trade-off filtering precision/recall using two hyperparameters, while the previous approach trained classifiers for eight different tasks resulting in sixteen hyperparameters. Ultimately, the biggest gains in filter quality are achieved when we jointly train the token-role classifiers together with a dynamic thresh"
P11-2035,2008.amta-papers.4,1,0.838663,"as True; i.e., predicting that the head the 3 is not the head of any arc, or that the modifier his 6 attaches elsewhere. Role truth values, derived from the gold-standard tree (in grey), are listed adjacent to the boxes, in parentheses. classifier is given credit for eliminating an arc. The identity of the responsible classifier is modeled as a latent variable, which is filled in during training using a latent SVM (LSVM) formulation. Our use of an LSVM to assign credit during joint training differs substantially from previous LSVM applications, which have induced latent linguistic structures (Cherry and Quirk, 2008; Chang et al., 2010) or sentence labels (Yessenalina et al., 2010). In our framework, each classifier learns to focus on the cases where the other classifiers are less confident. Furthermore, the integrated approach directly optimizes for arc-filtering accuracy (rather than token-labeling fidelity). We trade-off filtering precision/recall using two hyperparameters, while the previous approach trained classifiers for eight different tasks resulting in sixteen hyperparameters. Ultimately, the biggest gains in filter quality are achieved when we jointly train the token-role classifiers together"
P11-2035,W02-1001,0,0.0979336,").3 3 Because tokens and arcs are scored independently and coupled only through score comparison, the impact of Vine a and None a on classification speed should be no greater than doing vine and token-role filtering in sequence. In practice, it is no slower than running token-role filtering on its own. 4 Experiments We extract dependency structures from the Penn Treebank using the head rules of Yamada and Matsumoto (2003).4 We divide the Treebank into train (sections 2–21), development (22) and test (23). We part-of-speech tag our data using a perceptron tagger similar to the one described by Collins (2002). The training set is tagged with jack-knifing: the data is split into 10 folds and each fold is tagged by a system trained on the other 9 folds. Development and test sets are tagged using the entire training set. We train our joint filter using an in-house latent SVM framework, which repeatedly calls a multiclass exponentiated gradient SVM (Collins et al., 2008). LSVM training was stopped after 4 iterations, as determined during development.5 For the token-role classifiers, we re-implement the Bergsma and Cherry (2010) feature set, initializing w ¯ with high-precision subclassifiers trained i"
P11-2035,W06-2929,0,0.249621,"s to dependency parsing: graph-based and transition-based, where graphbased parsing is understood to be slower, but often more accurate (McDonald and Nivre, 2007). In the graph-based setting, a complete search finds the highest-scoring tree under a model that decomposes over one or two arcs at a time. Much of the time for parsing is spent scoring each potential arc in the complete dependency graph (JohnSimilar to Roark and Hollingshead (2008), each role has a corresponding binary classifier. These tokenrole classifiers were shown to be more effective than vine parsing (Eisner and Smith, 2005; Dreyer et al., 2006), a competing filtering scheme that filters arcs based on their length (leveraging the observation that most dependencies are short). In this work, we propose a novel filtering framework that integrates all the information used in token-role classification and vine parsing, but offers a number of advantages. In our previous work, classifier decisions would often overlap: different token-role classifiers would agree to filter the same arc. Based on this observation, we propose a joint training framework where only the most confident 200 Proceedings of the 49th Annual Meeting of the Association"
P11-2035,W05-1504,0,0.689536,"e two dominant approaches to dependency parsing: graph-based and transition-based, where graphbased parsing is understood to be slower, but often more accurate (McDonald and Nivre, 2007). In the graph-based setting, a complete search finds the highest-scoring tree under a model that decomposes over one or two arcs at a time. Much of the time for parsing is spent scoring each potential arc in the complete dependency graph (JohnSimilar to Roark and Hollingshead (2008), each role has a corresponding binary classifier. These tokenrole classifiers were shown to be more effective than vine parsing (Eisner and Smith, 2005; Dreyer et al., 2006), a competing filtering scheme that filters arcs based on their length (leveraging the observation that most dependencies are short). In this work, we propose a novel filtering framework that integrates all the information used in token-role classification and vine parsing, but offers a number of advantages. In our previous work, classifier decisions would often overlap: different token-role classifiers would agree to filter the same arc. Based on this observation, we propose a joint training framework where only the most confident 200 Proceedings of the 49th Annual Meeti"
P11-2035,P10-1110,0,0.0475035,"Missing"
P11-2035,P07-1022,0,0.0599611,"Missing"
P11-2035,D07-1013,0,0.0274342,"• Head-to-left (HtL{1/5/*}): t’s head is to its left within 1, 5 or any number of words • Head-to-right (HtR{1/5/*}): as head-to-left • Root (Root): t is the root node, which eliminates arcs according to projectivity Introduction A dependency tree represents syntactic relationships between words using directed arcs (Me´lˇcuk, 1987). Each token in the sentence is a node in the tree, and each arc connects a head to its modifier. There are two dominant approaches to dependency parsing: graph-based and transition-based, where graphbased parsing is understood to be slower, but often more accurate (McDonald and Nivre, 2007). In the graph-based setting, a complete search finds the highest-scoring tree under a model that decomposes over one or two arcs at a time. Much of the time for parsing is spent scoring each potential arc in the complete dependency graph (JohnSimilar to Roark and Hollingshead (2008), each role has a corresponding binary classifier. These tokenrole classifiers were shown to be more effective than vine parsing (Eisner and Smith, 2005; Dreyer et al., 2006), a competing filtering scheme that filters arcs based on their length (leveraging the observation that most dependencies are short). In this"
P11-2035,P05-1012,0,0.0541965,"ticlass exponentiated gradient SVM (Collins et al., 2008). LSVM training was stopped after 4 iterations, as determined during development.5 For the token-role classifiers, we re-implement the Bergsma and Cherry (2010) feature set, initializing w ¯ with high-precision subclassifiers trained independently for each token-role. Vine and None subclassifiers are initialized with a zero vector. At test time, we extract subclassifiers from the joint weight vector, and use them as parameters in the filtering tools of Bergsma and Cherry (2010).6 Parsing experiments are carried out using the MST parser (McDonald et al., 2005),7 which we have modified to filter arcs before carrying out feature extraction. It is trained using 5-best MIRA (Crammer and Singer, 2003). Following Bergsma and Cherry (2010), we measure intrinsic filter quality with reduction, the proportion of total arcs removed, and coverage, the proportion of true arcs retained. For parsing results, we present dependency accuracy, the percentage of tokens that are assigned the correct head. 4.1 Impact of Joint Training Our technical contribution consists of our proposed joint training scheme for token-role filters, along 4 As implemented at http://w3.msi"
P11-2035,C08-1094,0,0.253919,"ps between words using directed arcs (Me´lˇcuk, 1987). Each token in the sentence is a node in the tree, and each arc connects a head to its modifier. There are two dominant approaches to dependency parsing: graph-based and transition-based, where graphbased parsing is understood to be slower, but often more accurate (McDonald and Nivre, 2007). In the graph-based setting, a complete search finds the highest-scoring tree under a model that decomposes over one or two arcs at a time. Much of the time for parsing is spent scoring each potential arc in the complete dependency graph (JohnSimilar to Roark and Hollingshead (2008), each role has a corresponding binary classifier. These tokenrole classifiers were shown to be more effective than vine parsing (Eisner and Smith, 2005; Dreyer et al., 2006), a competing filtering scheme that filters arcs based on their length (leveraging the observation that most dependencies are short). In this work, we propose a novel filtering framework that integrates all the information used in token-role classification and vine parsing, but offers a number of advantages. In our previous work, classifier decisions would often overlap: different token-role classifiers would agree to filt"
P11-2035,W03-3023,0,0.0185557,"ur interpretation of filtering events: where before they were either active or inactive, events are now assigned scores, which are compared with the threshold to make final filtering decisions (Figure 2).3 3 Because tokens and arcs are scored independently and coupled only through score comparison, the impact of Vine a and None a on classification speed should be no greater than doing vine and token-role filtering in sequence. In practice, it is no slower than running token-role filtering on its own. 4 Experiments We extract dependency structures from the Penn Treebank using the head rules of Yamada and Matsumoto (2003).4 We divide the Treebank into train (sections 2–21), development (22) and test (23). We part-of-speech tag our data using a perceptron tagger similar to the one described by Collins (2002). The training set is tagged with jack-knifing: the data is split into 10 folds and each fold is tagged by a system trained on the other 9 folds. Development and test sets are tagged using the entire training set. We train our joint filter using an in-house latent SVM framework, which repeatedly calls a multiclass exponentiated gradient SVM (Collins et al., 2008). LSVM training was stopped after 4 iterations"
P11-2035,D10-1102,0,0.0175481,"any arc, or that the modifier his 6 attaches elsewhere. Role truth values, derived from the gold-standard tree (in grey), are listed adjacent to the boxes, in parentheses. classifier is given credit for eliminating an arc. The identity of the responsible classifier is modeled as a latent variable, which is filled in during training using a latent SVM (LSVM) formulation. Our use of an LSVM to assign credit during joint training differs substantially from previous LSVM applications, which have induced latent linguistic structures (Cherry and Quirk, 2008; Chang et al., 2010) or sentence labels (Yessenalina et al., 2010). In our framework, each classifier learns to focus on the cases where the other classifiers are less confident. Furthermore, the integrated approach directly optimizes for arc-filtering accuracy (rather than token-labeling fidelity). We trade-off filtering precision/recall using two hyperparameters, while the previous approach trained classifiers for eight different tasks resulting in sixteen hyperparameters. Ultimately, the biggest gains in filter quality are achieved when we jointly train the token-role classifiers together with a dynamic threshold that is based on arc length and shared acr"
P11-2035,C10-2168,0,0.158886,"Missing"
P14-1010,P08-2039,0,0.532592,"gmentation strategies recommended by previous work: the Penn Arabic Treebank scheme for English-Arabic (El Kholy and Habash, 2012a), and an unsupervised scheme for EnglishFinnish (Clifton and Sarkar, 2011). Desegmentation is the process of converting segmented words into their original surface form. For many segmentations, especially unsupervised ones, this amounts to simple concatenation. However, more complex segmentations, such as the Arabic tokenization provided by MADA (Habash et al., 2009), require further orthographic adjustments to reverse normalizations performed during segmentation. Badr et al. (2008) present two Arabic desegmentation schemes: table-based and rule-based. El Kholy and Habash (2012a) provide an extensive study on the influence of segmentation and desegmentation on English-toArabic SMT. They introduce an additional desegmentation technique that augments the table-based approach with an unsegmented language model. Salameh et al. (2013) replace rule-based desegmentation with a discriminatively-trained character transducer. In this work, we adopt the Table+Rules approach of El Kholy and Habash (2012a) for English-Arabic, while concatenation is sufficient for English-Finnish. Wor"
P14-1010,P07-1019,0,0.027739,"nodes that lie on the boundary between words, and for each node on this list, it launches a depth first search Programmatic Desegmentation Lattice desegmentation is a non-local lattice transformation. That is, the morphemes forming a word might span several edges, making desegmentation non-trivial. Luong et al. (2010) address this problem by forcing the decoder’s phrase table to respect word boundaries, guaranteeing that each desegmentable token sequence is local to an edge. 4 5 Or the LM composition can be done dynamically, effectively decoding the lattice with a beam or cube-pruned search (Huang and Chiang, 2007). Sentence-initial suffix morphemes and sentence-final prefix morphemes represent a special case that we omit for the sake of brevity. Lacking stems, they are left segmented. 104 3.4 Algorithm 1 Desegment a lattice hns , N , Ei {Initialize output lattice and work list WL} n0s = ns , N 0 = ∅, E 0 = ∅, WL = [ns ] while n = WL.pop() do {Work on each node only once} if n ∈ N 0 then continue N 0 = N 0 ∪ {n} {Initialize the chain stack C} C=∅ for e ∈ n.out do if [e] is valid then C.push([e]) {Depth-first search for complete chains} while [e1 , . . . , el ] = C.pop() do {Attempt to extend chain} for"
P14-1010,W07-0735,0,0.0238566,"r work, they replace the segmented language model with the unsegmented one, allowing them to tune the linear model parameters by hand. We use both segmented and unsegmented language models, and tune automatically to optimize BLEU. Like us, Luong et al. (2010) tune on unsegmented references,1 and translate with both segmented and unsegmented language models for English-to-Finnish translation. However, they adopt a scheme of word-boundary-aware Morphological Analysis Many languages have access to morphological analyzers, which annotate surface forms with their lemmas and morphological features. Bojar (2007) incorporates such analyses into a factored model, to either include a language model over target morphological tags, or model the generation of morphological features. Other approaches train an SMT system to predict lemmas instead of surface forms, and then inflect the SMT output as a postprocessing step (Minkov et al., 2007; Clifton and Sarkar, 2011; Fraser et al., 2012; El Kholy and Habash, 2012b). Alternatively, one can reparameterize existing phrase tables as exponential models, so that translation probabilities account for source context and morphological features (Jeong et al., 2010; Su"
P14-1010,P08-1067,0,0.0370348,"cceptor over morphemes. We compose this acceptor with a desegmenting transducer, and then with an unsegmented LM acceptor, producing a fully annotated, desegmented lattice. Instead of using a tool kit such as OpenFst (Allauzen et al., 2007), we implement both the desegmenting transducer and the LM acceptor programmatically. This eliminates the need to construct intermediate machines, such as the lattice-specific desegmenter in Figure 1b, and facilitates working with edges annotated with feature vectors as opposed to single weights. Inspired by the use of non-local features in forest decoding (Huang, 2008), we present an algorithm to find chains of edges that correspond to desegmentable token sequences, allowing lattice desegmentation with no phrase-table restrictions. This algorithm can be seen as implicitly constructing a customized desegmenting transducer and composing it with the input lattice on the fly. Before describing the algorithm, we define some notation. An input morpheme lattice is a triple hns , N , Ei, where N is a set of nodes, E is a set of edges, and ns ∈ N is the start node that begins each path through the lattice. Each edge e ∈ E is a 4-tuple hfrom, to, lex , wi, where from"
P14-1010,N12-1047,1,0.838563,"about 40 million Arabic tokens before 6 Allowing the re-ranker to choose between multiple Y s is a natural avenue for future work. 7 We also experimented on log p(X|Y ) as an additional feature, but observed no improvement in translation quality. 105 maximum phrase length is set to 8. For Englishto-Finnish, we follow Clifton and Sarkar (2011) in setting the hypothesis stack size to 100, distortion limit to 6, and maximum phrase length to 20. The decoder’s log-linear model is tuned with MERT (Och, 2003). Re-ranking models are tuned using a batch variant of hope-fear MIRA (Chiang et al., 2008; Cherry and Foster, 2012), using the n-best variant for n-best desegmentation, and the lattice variant for lattice desegmentation. MIRA was selected over MERT because we have an in-house implementation that can tune on lattices very quickly. During development, we confirmed that MERT and MIRA perform similarly, as is expected with fewer than 20 features. Both the decoder’s log-linear model and the re-ranking models are trained on the same development set. Historically, we have not seen improvements from using different tuning sets for decoding and reranking. Lattices are pruned to a density of 50 edges per word before"
P14-1010,D08-1024,0,0.0241362,"training set contains about 40 million Arabic tokens before 6 Allowing the re-ranker to choose between multiple Y s is a natural avenue for future work. 7 We also experimented on log p(X|Y ) as an additional feature, but observed no improvement in translation quality. 105 maximum phrase length is set to 8. For Englishto-Finnish, we follow Clifton and Sarkar (2011) in setting the hypothesis stack size to 100, distortion limit to 6, and maximum phrase length to 20. The decoder’s log-linear model is tuned with MERT (Och, 2003). Re-ranking models are tuned using a batch variant of hope-fear MIRA (Chiang et al., 2008; Cherry and Foster, 2012), using the n-best variant for n-best desegmentation, and the lattice variant for lattice desegmentation. MIRA was selected over MERT because we have an in-house implementation that can tune on lattices very quickly. During development, we confirmed that MERT and MIRA perform similarly, as is expected with fewer than 20 features. Both the decoder’s log-linear model and the re-ranking models are trained on the same development set. Historically, we have not seen improvements from using different tuning sets for decoding and reranking. Lattices are pruned to a density o"
P14-1010,2010.amta-papers.33,0,0.0179071,"features. Bojar (2007) incorporates such analyses into a factored model, to either include a language model over target morphological tags, or model the generation of morphological features. Other approaches train an SMT system to predict lemmas instead of surface forms, and then inflect the SMT output as a postprocessing step (Minkov et al., 2007; Clifton and Sarkar, 2011; Fraser et al., 2012; El Kholy and Habash, 2012b). Alternatively, one can reparameterize existing phrase tables as exponential models, so that translation probabilities account for source context and morphological features (Jeong et al., 2010; Subotin, 2011). Of these approaches, ours is most similar to the translate-then-inflect approach, except we translate and then desegment. In particular, Toutanova et al. (2008) inflect and re-rank n-best lists in a similar manner to how we desegment and re-rank n-best lists or lattices. 2.2 Morphological Segmentation Instead of producing an abstract feature layer, morphological segmentation transforms the target sentence by segmenting relevant morphemes, which are then handled as regular tokens during alignment and translation. This is done to reduce sparsity and to improve correspondence wi"
P14-1010,P11-2031,0,0.033925,"ntation step. Our second baseline is 1-best Deseg, where we train on segmented target text and desegment the decoder’s 1-best output. Starting from the system that produced 1-best Deseg, we then output either 1000-best lists or lattices to create our two experimental systems. The 1000-best Deseg system desegments, augments and re-ranks the decoder’s 1000-best list, while Lattice Deseg does the same in the lattice. We augment n-best lists and lattices using the features described in Section 3.4.8 We evaluate our system using BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). Following Clark et al. (2011), we report average scores over five random tuning replications to account for optimizer instability. For the baselines, this means 5 runs of decoder tuning. For the desegmenting re-rankers, this means 5 runs of reranker tuning, each working on n-best lists or lattices produced by the same (representative) decoder weights. We measure statistical significance using MultEval (Clark et al., 2011), which implements a stratified approximate randomization test to account for multiple tuning replications. segmentation, and 47 million after segmentation. We tune on the NIST 2004 evaluation set (1353 s"
P14-1010,D07-1091,0,0.123681,"Missing"
P14-1010,P11-1004,0,0.0694546,"es is a challenging and interesting task that has received much recent attention. Most techniques approach the problem by transforming the target language in some manner before training the translation model. They differ in what transformations are performed and at what stage they are reversed. The transformation might take the form of a morphological analysis or a morphological segmentation. 100 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 100–110, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics 2.1 Clifton and Sarkar, 2011; El Kholy and Habash, 2012a). Since our focus here is on integrating segmentation into the decoding process, we simply adopt the segmentation strategies recommended by previous work: the Penn Arabic Treebank scheme for English-Arabic (El Kholy and Habash, 2012a), and an unsupervised scheme for EnglishFinnish (Clifton and Sarkar, 2011). Desegmentation is the process of converting segmented words into their original surface form. For many segmentations, especially unsupervised ones, this amounts to simple concatenation. However, more complex segmentations, such as the Arabic tokenization provid"
P14-1010,N03-1017,0,0.00445976,"can be constructed by first encoding our desegmenter as a table that maps morpheme sequences to words. Regardless of whether the original desegmenter was based on concatenation, rules or table-lookup, it can be encoded as a lattice-specific table by applying it to an enumeration of all words found in the lattice. We can then transform that table into a finite state transducer with one path per table entry. Finally, we take the closure of this transducer, so that the resulting machine can transduce any sequence of words. The desegmenting transtil each source word has been covered exactly once (Koehn et al., 2003). The search graph of a phrase-based decoder can be interpreted as a lattice, which can be interpreted as a finite state acceptor over target strings. In its most natural form, such an acceptor emits target phrases on each edge, but it can easily be transformed into a form with one edge per token, as shown in Figure 1a. This is sometimes referred to as a word graph (Ueffing et al., 2002), although in our case the segmented phrase table also produces tokens that correspond to morphemes. Our goal is to desegment the decoder’s output lattice, and in doing so, gain access to a compact, desegmented"
P14-1010,P07-2045,0,0.0638514,"enting an SMT system built over target segments with features that reflect the desegmented target words. In this section, we describe our various strategies for desegmenting the SMT system’s output space, along with the features that we add to take advantage of this desegmented view. 3.1 3.3 Lattice Desegmentation An n-best list reflects a tiny portion of a decoder’s search space, typically fixed at 1000 hypotheses. Lattices2 can represent an exponential number of hypotheses in a compact structure. In this section, we discuss how a lattice from a multi-stack phrasebased decoder such as Moses (Koehn et al., 2007) can be desegmented to enable word-level features. Baselines The two obvious baseline approaches each decode using one view of the target language. The unsegmented approach translates without segmenting the target. This trivially allows for an unsegmented language model and never makes desegmentation errors. However, it suffers from data sparsity and poor token-to-token correspondence with the source language. The one-best desegmentation approach segments the target language at training time and Finite State Analogy A phrase-based decoder produces its output from left to right, with each opera"
P14-1010,2012.eamt-1.6,0,0.0247451,"Missing"
P14-1010,D10-1015,0,0.151014,"concatenation is sufficient for English-Finnish. Work on integration attempts to improve SMT performance for morphologically complex target languages by going beyond simple pre- and postprocessing. Oflazer and Durgar El-Kahlout (2007) desegment 1000-best lists for English-to-Turkish translation to enable scoring with an unsegmented language model. Unlike our work, they replace the segmented language model with the unsegmented one, allowing them to tune the linear model parameters by hand. We use both segmented and unsegmented language models, and tune automatically to optimize BLEU. Like us, Luong et al. (2010) tune on unsegmented references,1 and translate with both segmented and unsegmented language models for English-to-Finnish translation. However, they adopt a scheme of word-boundary-aware Morphological Analysis Many languages have access to morphological analyzers, which annotate surface forms with their lemmas and morphological features. Bojar (2007) incorporates such analyses into a factored model, to either include a language model over target morphological tags, or model the generation of morphological features. Other approaches train an SMT system to predict lemmas instead of surface form"
P14-1010,E12-1068,0,0.0124372,"ish-to-Finnish translation. However, they adopt a scheme of word-boundary-aware Morphological Analysis Many languages have access to morphological analyzers, which annotate surface forms with their lemmas and morphological features. Bojar (2007) incorporates such analyses into a factored model, to either include a language model over target morphological tags, or model the generation of morphological features. Other approaches train an SMT system to predict lemmas instead of surface forms, and then inflect the SMT output as a postprocessing step (Minkov et al., 2007; Clifton and Sarkar, 2011; Fraser et al., 2012; El Kholy and Habash, 2012b). Alternatively, one can reparameterize existing phrase tables as exponential models, so that translation probabilities account for source context and morphological features (Jeong et al., 2010; Subotin, 2011). Of these approaches, ours is most similar to the translate-then-inflect approach, except we translate and then desegment. In particular, Toutanova et al. (2008) inflect and re-rank n-best lists in a similar manner to how we desegment and re-rank n-best lists or lattices. 2.2 Morphological Segmentation Instead of producing an abstract feature layer, morpholog"
P14-1010,J03-1002,0,0.00599847,"ces to surface forms. For Finnish, we adopt the Unsup L-match segmentation technique of Clifton and Sarkar (2011), which uses Morfessor (Creutz and Lagus, 2005) to analyze the 5,000 most frequent Finnish words. The analysis is then applied to the Finnish side of the parallel text, and a list of segmented suffixes is collected. To improve coverage, words are further segmented according to their longest matching suffix from the list. As Morfessor does not perform any orthographic normalizations, it can be desegmented with simple concatenation. 4.2 Systems We align the parallel data with GIZA++ (Och et al., 2003) and decode using Moses (Koehn et al., 2007). The decoder’s log-linear model includes a standard feature set. Four translation model features encode phrase translation probabilities and lexical scores in both directions. Seven distortion features encode a standard distortion penalty as well as a bidirectional lexicalized reordering model. A KN-smoothed 5-gram language model is trained on the target side of the parallel data with SRILM (Stolcke, 2002). Finally, we include word and phrase penalties. The decoder uses the default parameters for English-to-Arabic, except that the 8 Development expe"
P14-1010,P03-1021,0,0.087911,"entence pairs drawn from the NIST 2012 training set, excluding the UN data. This training set contains about 40 million Arabic tokens before 6 Allowing the re-ranker to choose between multiple Y s is a natural avenue for future work. 7 We also experimented on log p(X|Y ) as an additional feature, but observed no improvement in translation quality. 105 maximum phrase length is set to 8. For Englishto-Finnish, we follow Clifton and Sarkar (2011) in setting the hypothesis stack size to 100, distortion limit to 6, and maximum phrase length to 20. The decoder’s log-linear model is tuned with MERT (Och, 2003). Re-ranking models are tuned using a batch variant of hope-fear MIRA (Chiang et al., 2008; Cherry and Foster, 2012), using the n-best variant for n-best desegmentation, and the lattice variant for lattice desegmentation. MIRA was selected over MERT because we have an in-house implementation that can tune on lattices very quickly. During development, we confirmed that MERT and MIRA perform similarly, as is expected with fewer than 20 features. Both the decoder’s log-linear model and the re-ranking models are trained on the same development set. Historically, we have not seen improvements from"
P14-1010,W07-0704,0,0.66386,"Missing"
P14-1010,P02-1040,0,0.0911867,"where we train on unsegmented target text, requiring no desegmentation step. Our second baseline is 1-best Deseg, where we train on segmented target text and desegment the decoder’s 1-best output. Starting from the system that produced 1-best Deseg, we then output either 1000-best lists or lattices to create our two experimental systems. The 1000-best Deseg system desegments, augments and re-ranks the decoder’s 1000-best list, while Lattice Deseg does the same in the lattice. We augment n-best lists and lattices using the features described in Section 3.4.8 We evaluate our system using BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). Following Clark et al. (2011), we report average scores over five random tuning replications to account for optimizer instability. For the baselines, this means 5 runs of decoder tuning. For the desegmenting re-rankers, this means 5 runs of reranker tuning, each working on n-best lists or lattices produced by the same (representative) decoder weights. We measure statistical significance using MultEval (Clark et al., 2011), which implements a stratified approximate randomization test to account for multiple tuning replications. segmentation, and 47 million after"
P14-1010,P11-2001,0,0.0202286,"s with an n-gram LM, every path coming into a node must end with the same sequence of (n−1) tokens. If this property does not hold, then nodes must be split until it does.4 This property is maintained by the decoder’s recombination rules for the segmented LM, but it is not guaranteed for the desegmented LM. Indeed, the expanded word-level context is one of the main benefits of incorporating a word-level LM. Fortunately, LM annotation as well as any necessary lattice modifications can be performed simultaneously by composing the desegmented lattice with a finite state acceptor encoding the LM (Roark et al., 2011). In summary, we are given a segmented lattice, which encodes the decoder’s translation space as an acceptor over morphemes. We compose this acceptor with a desegmenting transducer, and then with an unsegmented LM acceptor, producing a fully annotated, desegmented lattice. Instead of using a tool kit such as OpenFst (Allauzen et al., 2007), we implement both the desegmenting transducer and the LM acceptor programmatically. This eliminates the need to construct intermediate machines, such as the lattice-specific desegmenter in Figure 1b, and facilitates working with edges annotated with feature"
P14-1010,N13-2007,1,0.660172,"es, this amounts to simple concatenation. However, more complex segmentations, such as the Arabic tokenization provided by MADA (Habash et al., 2009), require further orthographic adjustments to reverse normalizations performed during segmentation. Badr et al. (2008) present two Arabic desegmentation schemes: table-based and rule-based. El Kholy and Habash (2012a) provide an extensive study on the influence of segmentation and desegmentation on English-toArabic SMT. They introduce an additional desegmentation technique that augments the table-based approach with an unsegmented language model. Salameh et al. (2013) replace rule-based desegmentation with a discriminatively-trained character transducer. In this work, we adopt the Table+Rules approach of El Kholy and Habash (2012a) for English-Arabic, while concatenation is sufficient for English-Finnish. Work on integration attempts to improve SMT performance for morphologically complex target languages by going beyond simple pre- and postprocessing. Oflazer and Durgar El-Kahlout (2007) desegment 1000-best lists for English-to-Turkish translation to enable scoring with an unsegmented language model. Unlike our work, they replace the segmented language mod"
P14-1010,2006.amta-papers.25,0,0.00976777,"arget text, requiring no desegmentation step. Our second baseline is 1-best Deseg, where we train on segmented target text and desegment the decoder’s 1-best output. Starting from the system that produced 1-best Deseg, we then output either 1000-best lists or lattices to create our two experimental systems. The 1000-best Deseg system desegments, augments and re-ranks the decoder’s 1000-best list, while Lattice Deseg does the same in the lattice. We augment n-best lists and lattices using the features described in Section 3.4.8 We evaluate our system using BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). Following Clark et al. (2011), we report average scores over five random tuning replications to account for optimizer instability. For the baselines, this means 5 runs of decoder tuning. For the desegmenting re-rankers, this means 5 runs of reranker tuning, each working on n-best lists or lattices produced by the same (representative) decoder weights. We measure statistical significance using MultEval (Clark et al., 2011), which implements a stratified approximate randomization test to account for multiple tuning replications. segmentation, and 47 million after segmentation. We tune on the N"
P14-1010,P11-1024,0,0.0132681,"7) incorporates such analyses into a factored model, to either include a language model over target morphological tags, or model the generation of morphological features. Other approaches train an SMT system to predict lemmas instead of surface forms, and then inflect the SMT output as a postprocessing step (Minkov et al., 2007; Clifton and Sarkar, 2011; Fraser et al., 2012; El Kholy and Habash, 2012b). Alternatively, one can reparameterize existing phrase tables as exponential models, so that translation probabilities account for source context and morphological features (Jeong et al., 2010; Subotin, 2011). Of these approaches, ours is most similar to the translate-then-inflect approach, except we translate and then desegment. In particular, Toutanova et al. (2008) inflect and re-rank n-best lists in a similar manner to how we desegment and re-rank n-best lists or lattices. 2.2 Morphological Segmentation Instead of producing an abstract feature layer, morphological segmentation transforms the target sentence by segmenting relevant morphemes, which are then handled as regular tokens during alignment and translation. This is done to reduce sparsity and to improve correspondence with the source la"
P14-1010,P08-1059,0,0.0173145,"logical features. Other approaches train an SMT system to predict lemmas instead of surface forms, and then inflect the SMT output as a postprocessing step (Minkov et al., 2007; Clifton and Sarkar, 2011; Fraser et al., 2012; El Kholy and Habash, 2012b). Alternatively, one can reparameterize existing phrase tables as exponential models, so that translation probabilities account for source context and morphological features (Jeong et al., 2010; Subotin, 2011). Of these approaches, ours is most similar to the translate-then-inflect approach, except we translate and then desegment. In particular, Toutanova et al. (2008) inflect and re-rank n-best lists in a similar manner to how we desegment and re-rank n-best lists or lattices. 2.2 Morphological Segmentation Instead of producing an abstract feature layer, morphological segmentation transforms the target sentence by segmenting relevant morphemes, which are then handled as regular tokens during alignment and translation. This is done to reduce sparsity and to improve correspondence with the source language (usually English). Such a segmentation can be produced as a byproduct of analysis (Oflazer and Durgar El-Kahlout, 2007; Badr et al., 2008; El Kholy and Hab"
P14-1010,W02-1021,0,0.0335071,"path per table entry. Finally, we take the closure of this transducer, so that the resulting machine can transduce any sequence of words. The desegmenting transtil each source word has been covered exactly once (Koehn et al., 2003). The search graph of a phrase-based decoder can be interpreted as a lattice, which can be interpreted as a finite state acceptor over target strings. In its most natural form, such an acceptor emits target phrases on each edge, but it can easily be transformed into a form with one edge per token, as shown in Figure 1a. This is sometimes referred to as a word graph (Ueffing et al., 2002), although in our case the segmented phrase table also produces tokens that correspond to morphemes. Our goal is to desegment the decoder’s output lattice, and in doing so, gain access to a compact, desegmented view of a large portion of the translation search space. This can be accomplished by composing the lattice with a desegmenting transducer that consumes morphemes and outputs desegmented words. This transducer must be able to consume every word in our lattice’s output vocabulary. We define a word using the following regular expression: 3 Throughout this paper, we use “+” to mark morpheme"
P14-1010,P07-1017,0,\N,Missing
P14-1010,D08-1076,0,\N,Missing
P19-1126,D18-1337,0,0.632017,"involves discrete decisions, learning via back-propagation is obstructed. Previous work on simultaneous NMT has thus far side-stepped this problem by making restrictive simplifications, either on the underlying NMT model or on the flexibility of the schedule. Cho and Esipova (2016) apply heuristics measures to estimate and then threshold the confidence of an NMT model trained on full sentences to adapt it at inference time to the streaming scenario. Several others use reinforcement learning (RL) to develop an agent to predict read and write decisions (Satija and Pineau, 2016; Gu et al., 2017; Alinejad et al., 2018). However, due to computational challenges, they pre-train an NMT model on full sentences and then train an agent that sees the fixed NMT model as part of its environment. Dalvi et al. (2018) and Ma et al. (2018) use fixed schedules and train their NMT systems accordingly. In particular, Ma et al. (2018) advocate for a wait-k strategy, wherein the system always waits for exactly k tokens before beginning to translate, and then alternates between reading and writing at a constant pre-specified emission rate. Due to the deterministic nature of their schedule, they can easily train the NMT system"
P19-1126,N12-1048,0,0.480757,"ntly-proposed Average Lagging latency metric (Ma et al., 2018), making it differentiable and calculable in expectation, which allows it to be used as a training objective. 3. We demonstrate favorable trade-offs to those of wait-k strategies at many latency values, and provide evidence that MILk’s advantage extends from its ability to adapt based on source content. 2 Background Much of the earlier work on simultaneous MT took the form of strategies to chunk the source sentence into partial segments that can be translated safely. These segments could be triggered by prosody (Fügen et al., 2007; Bangalore et al., 2012) or lexical cues (Rangarajan Sridhar et al., 2013), or optimized directly for translation quality (Oda et al., 2014). Segmentation decisions are surrogates for the core problem, which is deciding whether enough source content has been read to write the next target word correctly (Grissom II et al., 2014). However, since doing so involves discrete decisions, learning via back-propagation is obstructed. Previous work on simultaneous NMT has thus far side-stepped this problem by making restrictive simplifications, either on the underlying NMT model or on the flexibility of the schedule. Cho and E"
P19-1126,P18-1008,1,0.829926,"6.3M sentences) and WMT15 German-to-English (DeEn; 4.5M sentences) tasks. For EnFr we use a combination of newstest 2012 and newstest 2013 for development and report results on newstest 2014. For DeEn we validate on newstest 2013 and then report results on newstest 2015. Translation quality is measured using detokenized, cased BLEU (Papineni et al., 2002). For each data set, we use BPE (Sennrich et al., 2016) on the training data to construct a 32,000-type vocabulary that is shared between the source and target languages. 5.1 Model Our model closely follows the RNMT+ architecture described by Chen et al. (2018) with modifications to support streaming translation. It consists of a 6 layer LSTM encoder and an 8 layer LSTM decoder with additive attention (Bahdanau et al., 2014). All streaming models including waitk, MoChA and MILk use unidirectional encoders, while offline translation models use a bidirectional encoder. Both encoder and decoder LSTMs have 512 hidden units, per gate layer normalization (Ba et al., 2016), and residual skip connections after the second layer. The models are regularized using dropout with probability 0.2 and label smoothing with an uncertainty of 0.1 (Szegedy et al., 2016)"
P19-1126,N18-2079,0,0.391153,"to the beginning of the source. We show that MILk’s adaptive schedule allows it to arrive at latency-quality trade-offs that are favorable to those of a recently proposed wait-k strategy for many latency values. 1 • The schedule is learned and/or adaptive to the current context, but assumes a fixed MT system trained on complete source sentences, as typified by wait-if-* (Cho and Esipova, 2016) and reinforcement learning approaches (Grissom II et al., 2014; Gu et al., 2017). • The schedule is simple and fixed and can thus be easily integrated into MT training, as typified by wait-k approaches (Dalvi et al., 2018; Ma et al., 2018). Introduction Simultaneous machine translation (MT) addresses the problem of how to begin translating a source sentence before the source speaker has finished speaking. This capability is crucial for live or streaming translation scenarios, such as speech-tospeech translation, where waiting for one speaker to complete their sentence before beginning the translation would introduce an intolerable delay. In these scenarios, the MT engine must balance latency against quality: if it acts before the necessary source content arrives, translation quality degrades; but waiting for t"
P19-1126,D14-1140,0,0.493445,"Missing"
P19-1126,E17-1099,0,0.425863,"notonic attention head to schedule the reading of the source sentence, and a soft attention head that extends from the monotonic head back to the beginning of the source. We show that MILk’s adaptive schedule allows it to arrive at latency-quality trade-offs that are favorable to those of a recently proposed wait-k strategy for many latency values. 1 • The schedule is learned and/or adaptive to the current context, but assumes a fixed MT system trained on complete source sentences, as typified by wait-if-* (Cho and Esipova, 2016) and reinforcement learning approaches (Grissom II et al., 2014; Gu et al., 2017). • The schedule is simple and fixed and can thus be easily integrated into MT training, as typified by wait-k approaches (Dalvi et al., 2018; Ma et al., 2018). Introduction Simultaneous machine translation (MT) addresses the problem of how to begin translating a source sentence before the source speaker has finished speaking. This capability is crucial for live or streaming translation scenarios, such as speech-tospeech translation, where waiting for one speaker to complete their sentence before beginning the translation would introduce an intolerable delay. In these scenarios, the MT engine"
P19-1126,P14-2090,0,0.498658,"ich allows it to be used as a training objective. 3. We demonstrate favorable trade-offs to those of wait-k strategies at many latency values, and provide evidence that MILk’s advantage extends from its ability to adapt based on source content. 2 Background Much of the earlier work on simultaneous MT took the form of strategies to chunk the source sentence into partial segments that can be translated safely. These segments could be triggered by prosody (Fügen et al., 2007; Bangalore et al., 2012) or lexical cues (Rangarajan Sridhar et al., 2013), or optimized directly for translation quality (Oda et al., 2014). Segmentation decisions are surrogates for the core problem, which is deciding whether enough source content has been read to write the next target word correctly (Grissom II et al., 2014). However, since doing so involves discrete decisions, learning via back-propagation is obstructed. Previous work on simultaneous NMT has thus far side-stepped this problem by making restrictive simplifications, either on the underlying NMT model or on the flexibility of the schedule. Cho and Esipova (2016) apply heuristics measures to estimate and then threshold the confidence of an NMT model trained on ful"
P19-1126,P02-1040,0,0.106553,"able 2: DAL’s time-indexed lag DALi = gi0 − when |x |= |y |= 4 for a wait-3 system. i−1 γ DAL, alongside several examples of cases where DAL yields more intuitive results than AL. 5 Experiments We run our experiments on the standard WMT14 English-to-French (EnFr; 36.3M sentences) and WMT15 German-to-English (DeEn; 4.5M sentences) tasks. For EnFr we use a combination of newstest 2012 and newstest 2013 for development and report results on newstest 2014. For DeEn we validate on newstest 2013 and then report results on newstest 2015. Translation quality is measured using detokenized, cased BLEU (Papineni et al., 2002). For each data set, we use BPE (Sennrich et al., 2016) on the training data to construct a 32,000-type vocabulary that is shared between the source and target languages. 5.1 Model Our model closely follows the RNMT+ architecture described by Chen et al. (2018) with modifications to support streaming translation. It consists of a 6 layer LSTM encoder and an 8 layer LSTM decoder with additive attention (Bahdanau et al., 2014). All streaming models including waitk, MoChA and MILk use unidirectional encoders, while offline translation models use a bidirectional encoder. Both encoder and decoder L"
P19-1126,N13-1023,0,0.456647,"al., 2018), making it differentiable and calculable in expectation, which allows it to be used as a training objective. 3. We demonstrate favorable trade-offs to those of wait-k strategies at many latency values, and provide evidence that MILk’s advantage extends from its ability to adapt based on source content. 2 Background Much of the earlier work on simultaneous MT took the form of strategies to chunk the source sentence into partial segments that can be translated safely. These segments could be triggered by prosody (Fügen et al., 2007; Bangalore et al., 2012) or lexical cues (Rangarajan Sridhar et al., 2013), or optimized directly for translation quality (Oda et al., 2014). Segmentation decisions are surrogates for the core problem, which is deciding whether enough source content has been read to write the next target word correctly (Grissom II et al., 2014). However, since doing so involves discrete decisions, learning via back-propagation is obstructed. Previous work on simultaneous NMT has thus far side-stepped this problem by making restrictive simplifications, either on the underlying NMT model or on the flexibility of the schedule. Cho and Esipova (2016) apply heuristics measures to estimat"
P19-1126,P16-1162,0,0.20387,"|y |= 4 for a wait-3 system. i−1 γ DAL, alongside several examples of cases where DAL yields more intuitive results than AL. 5 Experiments We run our experiments on the standard WMT14 English-to-French (EnFr; 36.3M sentences) and WMT15 German-to-English (DeEn; 4.5M sentences) tasks. For EnFr we use a combination of newstest 2012 and newstest 2013 for development and report results on newstest 2014. For DeEn we validate on newstest 2013 and then report results on newstest 2015. Translation quality is measured using detokenized, cased BLEU (Papineni et al., 2002). For each data set, we use BPE (Sennrich et al., 2016) on the training data to construct a 32,000-type vocabulary that is shared between the source and target languages. 5.1 Model Our model closely follows the RNMT+ architecture described by Chen et al. (2018) with modifications to support streaming translation. It consists of a 6 layer LSTM encoder and an 8 layer LSTM decoder with additive attention (Bahdanau et al., 2014). All streaming models including waitk, MoChA and MILk use unidirectional encoders, while offline translation models use a bidirectional encoder. Both encoder and decoder LSTMs have 512 hidden units, per gate layer normalizatio"
S14-2076,J92-4003,0,0.121213,"quency of term w in the corpus, freq (pos) is the total number of tokens in positive reviews, and N is the total number of tokens in the corpus. PMI (w , neg) was calculated in a similar way. Since PMI is known to be a poor estimator of association for low-frequency events, we ignored terms that occurred less than five times in each (positive and negative) groups of reviews. 1 score (w , c) = PMI (w , c) − PMI (w , ¬c) (3) 2.3 Word Clusters Word clusters can provide an alternative representation of text, significantly reducing the sparsity of the token space. Using Brown clustering algorithm (Brown et al., 1992), we generated 1,000 word clusters from the Yelp restaurant reviews corpus. Additionally, we used publicly available http://www.yelp.com/dataset_challenge 438 word clusters generated from 56 million Englishlanguage tweets (Owoputi et al., 2013). 3 Subtask 1: Aspect Term Extraction The objective of this subtask is to detect aspect terms in sentences. We approached this problem using in-house entity-recognition software, very similar to the system used by de Bruijn et al. (2011) to detect medical concepts. First, sentences were tokenized to split away punctuation, and then the token sequence was"
S14-2076,W10-0204,1,0.338337,"ts has a sentiment score of 1.2 whereas the same term in negated contexts has a score of -1.4. We built two lexicons, Yelp Restaurant Sentiment AffLex and Yelp Restaurant Sentiment NegLex, as described in (Kiritchenko et al., 2014). Similarly, we generated in-domain sentiment lexicons from the Amazon laptop reviews corpus. In addition, we employed existing out-ofdomain sentiment lexicons: (1) large-coverage automatic tweet sentiment lexicons, Hashtag Sentiment lexicons and Sentiment140 lexicons (Kiritchenko et al., 2014), and (2) three manually created sentiment lexicons, NRC Emotion Lexicon (Mohammad and Turney, 2010), Bing Liu’s Lexicon (Hu and Liu, 2004), and the MPQA Subjectivity Lexicon (Wilson et al., 2005). Yelp Restaurant Word–Aspect Association Lexicon: The Yelp restaurant reviews corpus was also used to generate a lexicon of terms associated with the aspect categories of food, price, service, ambiance, and anecdotes. Each sentence of the corpus was labeled with zero, one, or more of the five aspect categories by our aspect category classification system (described in Section 5). Then, for each term w and each category c an association score was calculated as follows: Lexicons Sentiment Lexicons: F"
S14-2076,W02-1001,0,0.046524,"i+2 )), and prefixes and suffixes of wi (up to 3 characters in length). There are only two phrase-level emission feature templates: the cased and uncased identity of the entire phrase covered by a tag, which allow the system to memorize complete terms such as, “getting a table” or “fish and chips.” Transition features couple tags with tags. Let the current tag be yj . Its transition feature templates are short n-grams of tag identities: yj ; yj , yj−1 ; and yj , yj−1 , yj−2 . During development, we experimented with the training algorithm, trying both PA and the simpler structured perceptron (Collins, 2002). We also added the lowercased back-off features. In Table 2, we re-test these design decisions on the test set, revealing that lower-cased back-off features made a strong contribution, while PA training was perhaps not as important. Our complete system achieved an F1-score of 80.19 on the restaurant domain and 68.57 on the laptop domain, ranking third among 24 teams in both. System NRC-Canada (All) All − lower-casing All − PA + percep Restaurants P R F1 84.41 76.37 80.19 83.68 75.49 79.37 83.37 76.45 79.76 System NRC-Canada (All) All − lower-casing All − PA + percep Laptops P R F1 78.77 60.70"
S14-2076,S13-2053,1,0.543874,"elp restaurant reviews corpus was also used to generate a lexicon of terms associated with the aspect categories of food, price, service, ambiance, and anecdotes. Each sentence of the corpus was labeled with zero, one, or more of the five aspect categories by our aspect category classification system (described in Section 5). Then, for each term w and each category c an association score was calculated as follows: Lexicons Sentiment Lexicons: From the Yelp restaurant reviews corpus, we automatically created an indomain sentiment lexicon for restaurants. Following Turney and Littman (2003) and Mohammad et al. (2013), we calculated a sentiment score for each term w in the corpus: score (w ) = PMI (w , pos) − PMI (w , neg) (1) where pos denotes positive reviews and neg denotes negative reviews. PMI stands for pointwise mutual information: freq (w , pos) ∗ N PMI (w , pos) = log2 (2) freq (w ) ∗ freq (pos) where freq (w, pos) is the number of times a term w occurs in positive reviews, freq (w) is the total frequency of term w in the corpus, freq (pos) is the total number of tokens in positive reviews, and N is the total number of tokens in the corpus. PMI (w , neg) was calculated in a similar way. Since PMI"
S14-2076,N13-1039,0,0.0130894,"iation for low-frequency events, we ignored terms that occurred less than five times in each (positive and negative) groups of reviews. 1 score (w , c) = PMI (w , c) − PMI (w , ¬c) (3) 2.3 Word Clusters Word clusters can provide an alternative representation of text, significantly reducing the sparsity of the token space. Using Brown clustering algorithm (Brown et al., 1992), we generated 1,000 word clusters from the Yelp restaurant reviews corpus. Additionally, we used publicly available http://www.yelp.com/dataset_challenge 438 word clusters generated from 56 million Englishlanguage tweets (Owoputi et al., 2013). 3 Subtask 1: Aspect Term Extraction The objective of this subtask is to detect aspect terms in sentences. We approached this problem using in-house entity-recognition software, very similar to the system used by de Bruijn et al. (2011) to detect medical concepts. First, sentences were tokenized to split away punctuation, and then the token sequence was tagged using a semi-Markov tagger (Sarawagi and Cohen, 2004). The tagger had two possible tags: O for outside, and T for aspect term, where an aspect term could tag a phrase of up to 5 consecutive tokens. The tagger was trained using the struc"
S14-2076,S14-2004,0,0.53975,"ect category of ‘food’. In Task 4, customer reviews are provided for two domains: restaurants and laptops. A fixed set of five aspect categories is defined for the restaurant domain: food, service, price, ambiance, and anecdotes. Automatic systems are to determine if any of those aspect categories are described in a review. The example sentence above describes the aspect categories of food (positive sentiment) and service (negative sentiment). For the laptop reviews, there is no aspect category detection subtask. Further details of the task and data can be found in the task description paper (Pontiki et al., 2014). Introduction Automatically identifying sentiment expressed in text has a number of applications, including tracking sentiment towards products, movies, politicians, etc.; improving customer relation models; and detecting happiness and well-being. In many applications, it is important to associate sentiment with a particular entity or an aspect of an entity. For example, in reviews, customers might express different sentiment towards various aspects of a product or service they have availed. Consider: The lasagna was great, but the service was a bit slow. The review is for a restaurant, and w"
S14-2076,P07-1033,0,0.0159279,"Missing"
S14-2076,de-marneffe-etal-2006-generating,0,0.0335632,"Missing"
S14-2076,H05-1044,0,0.0642609,"t two lexicons, Yelp Restaurant Sentiment AffLex and Yelp Restaurant Sentiment NegLex, as described in (Kiritchenko et al., 2014). Similarly, we generated in-domain sentiment lexicons from the Amazon laptop reviews corpus. In addition, we employed existing out-ofdomain sentiment lexicons: (1) large-coverage automatic tweet sentiment lexicons, Hashtag Sentiment lexicons and Sentiment140 lexicons (Kiritchenko et al., 2014), and (2) three manually created sentiment lexicons, NRC Emotion Lexicon (Mohammad and Turney, 2010), Bing Liu’s Lexicon (Hu and Liu, 2004), and the MPQA Subjectivity Lexicon (Wilson et al., 2005). Yelp Restaurant Word–Aspect Association Lexicon: The Yelp restaurant reviews corpus was also used to generate a lexicon of terms associated with the aspect categories of food, price, service, ambiance, and anecdotes. Each sentence of the corpus was labeled with zero, one, or more of the five aspect categories by our aspect category classification system (described in Section 5). Then, for each term w and each category c an association score was calculated as follows: Lexicons Sentiment Lexicons: From the Yelp restaurant reviews corpus, we automatically created an indomain sentiment lexicon f"
S14-2076,P11-2008,0,0.0134045,"Missing"
S14-2076,P14-1029,1,0.61803,"n of 124,712 reviews as the Amazon laptop reviews corpus. Both the Yelp and the Amazon reviews have one to five star ratings associated with each review. We treated the one- and two-star reviews as negative reviews, and the four- and five-star reviews as positive reviews. 2.2 A positive sentiment score indicates a greater overall association with positive sentiment, whereas a negative score indicates a greater association with negative sentiment. The magnitude is indicative of the degree of association. Negation words (e.g., not, never) can significantly affect the sentiment of an expression (Zhu et al., 2014). Therefore, when generating the sentiment lexicons we distinguished terms appearing in negated contexts (defined as text spans between a negation word and a punctuation mark) and affirmative (non-negated) contexts. The sentiment scores were then calculated separately for the two types of contexts. For example, the term good in affirmative contexts has a sentiment score of 1.2 whereas the same term in negated contexts has a score of -1.4. We built two lexicons, Yelp Restaurant Sentiment AffLex and Yelp Restaurant Sentiment NegLex, as described in (Kiritchenko et al., 2014). Similarly, we gener"
S16-1003,W11-1701,0,0.686791,"labels, with pkudblab using keyword rules, LitisMind using hashtag rules on external data, and INF-UFRGS using a combination of rules and third-party sentiment classifiers. However, we were pleased to see other teams attempting to generalize the supervised data from Task A in interesting ways, either using rules or multi-stage classifiers to bridge the target gap. We are optimistic that there is much interesting follow-up work yet to come on this task. Further details on the submissions can be found 39 Related Work Past work on stance detection includes that by Somasundaran and Wiebe (2010), Anand et al. (2011), Faulkner (2014), Rajadesingan and Liu ˇ (2014), Djemili et al. (2014), Boltuzic and Snajder (2014), Conrad et al. (2012), Sridhar et al. (2014), Rajadesingan and Liu (2014), and Sobhani et al. (2015). There is a vast amount of work in sentiment analysis of tweets, and we refer the reader to surveys (Pang and Lee, 2008; Liu and Zhang, 2012; Mohammad, 2015) and proceedings of recent shared task competitions (Wilson et al., 2013; Mohammad et al., 2013; Rosenthal et al., 2015). See Pontiki et al. (2014), Pontiki et al. (2015), and Kiritchenko et al. (2014a) for tasks and systems on aspect based"
S16-1003,S16-1063,0,0.18439,"25.52 20.13 29.72 28.43 67.19 51.60 50.04 50.62 40.66 38.87 38.06 34.16 28.86 25.77 29.50 22.66 25.02 19.14 22.80 22.60 4.69 18.35 56.28 44.66 42.32 42.02 34.08 32.70 32.39 26.14 25.73 in the system description papers published in the SemEval-2016 proceedings, including papers by Elfardy and Diab (2016) for CU-GWU, Dias and Becker (2016) for INF-URGS, Patra et al. (2016) for JU NLP, Wojatzki and Zesch (2016) for ltl.uni-due, Zarrella and Marsh (2016) for MITRE, Misra et al. (2016) for nldsucsc, Wei et al. (2016) for pkudblab, Tutek et al. (2016) for TakeLab, Yuki et al. (2016) for Tohoku, and Augenstein et al. (2016) for USFD. 7 Table 5: Results for Task B (the official competition metric Favg ) on different subsets of the test data. The highest score in each column is shown in bold. Table 5 shows results for Task B on subsets of the test set where opinion is expressed towards the target of interest and towards some other entity. Observe that here too results are markedly lower when stance is to be inferred from a tweet expressing opinion about some other entity (and not the target of interest). 6.3 Discussion Some teams did very well detecting tweets in favor of Trump (ltl.uni-due), with most of the othe"
S16-1003,W14-2107,0,0.0579715,"and INF-UFRGS using a combination of rules and third-party sentiment classifiers. However, we were pleased to see other teams attempting to generalize the supervised data from Task A in interesting ways, either using rules or multi-stage classifiers to bridge the target gap. We are optimistic that there is much interesting follow-up work yet to come on this task. Further details on the submissions can be found 39 Related Work Past work on stance detection includes that by Somasundaran and Wiebe (2010), Anand et al. (2011), Faulkner (2014), Rajadesingan and Liu ˇ (2014), Djemili et al. (2014), Boltuzic and Snajder (2014), Conrad et al. (2012), Sridhar et al. (2014), Rajadesingan and Liu (2014), and Sobhani et al. (2015). There is a vast amount of work in sentiment analysis of tweets, and we refer the reader to surveys (Pang and Lee, 2008; Liu and Zhang, 2012; Mohammad, 2015) and proceedings of recent shared task competitions (Wilson et al., 2013; Mohammad et al., 2013; Rosenthal et al., 2015). See Pontiki et al. (2014), Pontiki et al. (2015), and Kiritchenko et al. (2014a) for tasks and systems on aspect based sentiment analysis, where the goal is to determine sentiment towards aspects of a product such as sp"
S16-1003,W12-3810,0,0.00926055,"tion of rules and third-party sentiment classifiers. However, we were pleased to see other teams attempting to generalize the supervised data from Task A in interesting ways, either using rules or multi-stage classifiers to bridge the target gap. We are optimistic that there is much interesting follow-up work yet to come on this task. Further details on the submissions can be found 39 Related Work Past work on stance detection includes that by Somasundaran and Wiebe (2010), Anand et al. (2011), Faulkner (2014), Rajadesingan and Liu ˇ (2014), Djemili et al. (2014), Boltuzic and Snajder (2014), Conrad et al. (2012), Sridhar et al. (2014), Rajadesingan and Liu (2014), and Sobhani et al. (2015). There is a vast amount of work in sentiment analysis of tweets, and we refer the reader to surveys (Pang and Lee, 2008; Liu and Zhang, 2012; Mohammad, 2015) and proceedings of recent shared task competitions (Wilson et al., 2013; Mohammad et al., 2013; Rosenthal et al., 2015). See Pontiki et al. (2014), Pontiki et al. (2015), and Kiritchenko et al. (2014a) for tasks and systems on aspect based sentiment analysis, where the goal is to determine sentiment towards aspects of a product such as speed of processor and s"
S16-1003,S16-1061,0,0.0301153,"lt of 56.28 actually beating the best result for the supervised ‘Climate Change’ task (54.86). Team Baselines Majority class SVM-ngrams-comb Opinion Towards Target Other Participating Teams pkudblab LitisMind INF-UFRGS UWB ECNU USFD Thomson Reuters ltl.uni-due NEUSA All 35.20 31.39 25.52 20.13 29.72 28.43 67.19 51.60 50.04 50.62 40.66 38.87 38.06 34.16 28.86 25.77 29.50 22.66 25.02 19.14 22.80 22.60 4.69 18.35 56.28 44.66 42.32 42.02 34.08 32.70 32.39 26.14 25.73 in the system description papers published in the SemEval-2016 proceedings, including papers by Elfardy and Diab (2016) for CU-GWU, Dias and Becker (2016) for INF-URGS, Patra et al. (2016) for JU NLP, Wojatzki and Zesch (2016) for ltl.uni-due, Zarrella and Marsh (2016) for MITRE, Misra et al. (2016) for nldsucsc, Wei et al. (2016) for pkudblab, Tutek et al. (2016) for TakeLab, Yuki et al. (2016) for Tohoku, and Augenstein et al. (2016) for USFD. 7 Table 5: Results for Task B (the official competition metric Favg ) on different subsets of the test data. The highest score in each column is shown in bold. Table 5 shows results for Task B on subsets of the test set where opinion is expressed towards the target of interest and towards some other ent"
S16-1003,S16-1070,0,0.0161479,"ht have expected, with the best result of 56.28 actually beating the best result for the supervised ‘Climate Change’ task (54.86). Team Baselines Majority class SVM-ngrams-comb Opinion Towards Target Other Participating Teams pkudblab LitisMind INF-UFRGS UWB ECNU USFD Thomson Reuters ltl.uni-due NEUSA All 35.20 31.39 25.52 20.13 29.72 28.43 67.19 51.60 50.04 50.62 40.66 38.87 38.06 34.16 28.86 25.77 29.50 22.66 25.02 19.14 22.80 22.60 4.69 18.35 56.28 44.66 42.32 42.02 34.08 32.70 32.39 26.14 25.73 in the system description papers published in the SemEval-2016 proceedings, including papers by Elfardy and Diab (2016) for CU-GWU, Dias and Becker (2016) for INF-URGS, Patra et al. (2016) for JU NLP, Wojatzki and Zesch (2016) for ltl.uni-due, Zarrella and Marsh (2016) for MITRE, Misra et al. (2016) for nldsucsc, Wei et al. (2016) for pkudblab, Tutek et al. (2016) for TakeLab, Yuki et al. (2016) for Tohoku, and Augenstein et al. (2016) for USFD. 7 Table 5: Results for Task B (the official competition metric Favg ) on different subsets of the test data. The highest score in each column is shown in bold. Table 5 shows results for Task B on subsets of the test set where opinion is expressed towards the target of"
S16-1003,S14-2076,1,0.512441,"s these results. It also shows results on the complete test set (All), for easy reference. Observe that the stance task is markedly more difficult when stance is to be inferred from a tweet expressing opinion about some other entity (and not the target of interest). This is not surprising because it is a more challenging task, and because there has been very little work on this in the past. 5.3 Discussion Most teams used standard text classification features such as n-grams and word embedding vectors, as well as standard sentiment analysis features such as those drawn from sentiment lexicons (Kiritchenko et al., 2014b). Some teams polled Twitter for stancebearing hashtags, creating additional noisy stance data. Three teams tried variants of this strategy: MITRE, DeepStance and nldsucsc. These teams are distributed somewhat evenly throughout the standings, and although MITRE did use extra data in its top-placing entry, pkudblab achieved nearly the same score with only the provided data. Another possible differentiator would be the use of continuous word representations, derived either from extremely large sources such as Google News, directly from Twitter corpora, or as a by-product of training a neural ne"
S16-1003,S16-1068,0,0.0138727,"Towards Target Other Participating Teams pkudblab LitisMind INF-UFRGS UWB ECNU USFD Thomson Reuters ltl.uni-due NEUSA All 35.20 31.39 25.52 20.13 29.72 28.43 67.19 51.60 50.04 50.62 40.66 38.87 38.06 34.16 28.86 25.77 29.50 22.66 25.02 19.14 22.80 22.60 4.69 18.35 56.28 44.66 42.32 42.02 34.08 32.70 32.39 26.14 25.73 in the system description papers published in the SemEval-2016 proceedings, including papers by Elfardy and Diab (2016) for CU-GWU, Dias and Becker (2016) for INF-URGS, Patra et al. (2016) for JU NLP, Wojatzki and Zesch (2016) for ltl.uni-due, Zarrella and Marsh (2016) for MITRE, Misra et al. (2016) for nldsucsc, Wei et al. (2016) for pkudblab, Tutek et al. (2016) for TakeLab, Yuki et al. (2016) for Tohoku, and Augenstein et al. (2016) for USFD. 7 Table 5: Results for Task B (the official competition metric Favg ) on different subsets of the test data. The highest score in each column is shown in bold. Table 5 shows results for Task B on subsets of the test set where opinion is expressed towards the target of interest and towards some other entity. Observe that here too results are markedly lower when stance is to be inferred from a tweet expressing opinion about some other entity (and n"
S16-1003,W10-0204,1,0.110514,"ier. Nine of the nineteen entries used some form of word embedding, including the top three entries, but PKULCWM’s fourth place result shows that it is possible to do well with a more traditional approach that relies instead on Twitter-specific linguistic pre-processing. Along these lines, it is worth noting that both MITRE and pkudblab reflect knowledge-light approaches to the problem, each relying minimally on linguistic processing and external lexicons. Seven of the nineteen submissions made extensive use of publicly-available sentiment and emotion lexicons such as the NRC Emotion Lexicon (Mohammad and Turney, 2010), Hu and Liu Lexicon (Hu and Liu, 2004), MPQA Subjectivity Lexicon (Wilson et al., 2005), and NRC Hashtag Lexicons (Kiritchenko et al., 2014b). Recall that the SVM-ngrams baseline also performed very well, using only word and character ngrams in its classifiers. This helps emphasize the fact that for this young task, the community is still a long way from an established set of best practices. 6 Systems and Results for Task B The sub-sections below discuss baselines and official submissions to Task B. Recall, that the test data for Task B is for the target ‘Donald Trump’, and no training data f"
S16-1003,S13-2053,1,0.777604,"task. Further details on the submissions can be found 39 Related Work Past work on stance detection includes that by Somasundaran and Wiebe (2010), Anand et al. (2011), Faulkner (2014), Rajadesingan and Liu ˇ (2014), Djemili et al. (2014), Boltuzic and Snajder (2014), Conrad et al. (2012), Sridhar et al. (2014), Rajadesingan and Liu (2014), and Sobhani et al. (2015). There is a vast amount of work in sentiment analysis of tweets, and we refer the reader to surveys (Pang and Lee, 2008; Liu and Zhang, 2012; Mohammad, 2015) and proceedings of recent shared task competitions (Wilson et al., 2013; Mohammad et al., 2013; Rosenthal et al., 2015). See Pontiki et al. (2014), Pontiki et al. (2015), and Kiritchenko et al. (2014a) for tasks and systems on aspect based sentiment analysis, where the goal is to determine sentiment towards aspects of a product such as speed of processor and screen resolution of a cell phone. 8 Conclusions and Future Work We described a new shared task on detecting stance towards pre-chosen targets of interest from tweets. We formulated two tasks: a traditional supervised task where labeled training data for the test data targets is made available (Task A) and a more challenging formul"
S16-1003,L16-1623,1,0.884636,"d submissions from 9 teams wherein the highest classification F-score obtained was 56.28. The best performing systems used standard text classification features such as those drawn from n-grams, word vectors, and sentiment lexicons. Some teams drew additional gains from noisy stance-labeled data created using distant supervision techniques. A large number of teams used word embeddings and some used deep neural networks such as RNNs and convolutional neural nets. Nonetheless, for Task A, none of these systems surpassed a baseline SVM classifier that uses word and character n-grams as features (Mohammad et al., 2016b). Further, results are markedly worse for instances where the target of interest is not the target of opinion. More gains can be expected in the future on both tasks, as researchers better understand this new task and data. All of the data, an interactive visualization of the data, and the evaluation scripts are available on the task website as well as the homepage for this Stance project.2 2 Subtleties of Stance Detection In the sub-sections below we discuss some of the nuances of stance detection, including a discussion on neutral stance and the relationship between stance and sentiment. 2"
S16-1003,S16-1071,0,0.0205908,"result for the supervised ‘Climate Change’ task (54.86). Team Baselines Majority class SVM-ngrams-comb Opinion Towards Target Other Participating Teams pkudblab LitisMind INF-UFRGS UWB ECNU USFD Thomson Reuters ltl.uni-due NEUSA All 35.20 31.39 25.52 20.13 29.72 28.43 67.19 51.60 50.04 50.62 40.66 38.87 38.06 34.16 28.86 25.77 29.50 22.66 25.02 19.14 22.80 22.60 4.69 18.35 56.28 44.66 42.32 42.02 34.08 32.70 32.39 26.14 25.73 in the system description papers published in the SemEval-2016 proceedings, including papers by Elfardy and Diab (2016) for CU-GWU, Dias and Becker (2016) for INF-URGS, Patra et al. (2016) for JU NLP, Wojatzki and Zesch (2016) for ltl.uni-due, Zarrella and Marsh (2016) for MITRE, Misra et al. (2016) for nldsucsc, Wei et al. (2016) for pkudblab, Tutek et al. (2016) for TakeLab, Yuki et al. (2016) for Tohoku, and Augenstein et al. (2016) for USFD. 7 Table 5: Results for Task B (the official competition metric Favg ) on different subsets of the test data. The highest score in each column is shown in bold. Table 5 shows results for Task B on subsets of the test set where opinion is expressed towards the target of interest and towards some other entity. Observe that here too results"
S16-1003,S14-2004,0,0.0916108,"d 39 Related Work Past work on stance detection includes that by Somasundaran and Wiebe (2010), Anand et al. (2011), Faulkner (2014), Rajadesingan and Liu ˇ (2014), Djemili et al. (2014), Boltuzic and Snajder (2014), Conrad et al. (2012), Sridhar et al. (2014), Rajadesingan and Liu (2014), and Sobhani et al. (2015). There is a vast amount of work in sentiment analysis of tweets, and we refer the reader to surveys (Pang and Lee, 2008; Liu and Zhang, 2012; Mohammad, 2015) and proceedings of recent shared task competitions (Wilson et al., 2013; Mohammad et al., 2013; Rosenthal et al., 2015). See Pontiki et al. (2014), Pontiki et al. (2015), and Kiritchenko et al. (2014a) for tasks and systems on aspect based sentiment analysis, where the goal is to determine sentiment towards aspects of a product such as speed of processor and screen resolution of a cell phone. 8 Conclusions and Future Work We described a new shared task on detecting stance towards pre-chosen targets of interest from tweets. We formulated two tasks: a traditional supervised task where labeled training data for the test data targets is made available (Task A) and a more challenging formulation where no labeled data pertaining to the test d"
S16-1003,S15-2082,0,0.099003,"Missing"
S16-1003,S15-2078,1,0.755235,"n the submissions can be found 39 Related Work Past work on stance detection includes that by Somasundaran and Wiebe (2010), Anand et al. (2011), Faulkner (2014), Rajadesingan and Liu ˇ (2014), Djemili et al. (2014), Boltuzic and Snajder (2014), Conrad et al. (2012), Sridhar et al. (2014), Rajadesingan and Liu (2014), and Sobhani et al. (2015). There is a vast amount of work in sentiment analysis of tweets, and we refer the reader to surveys (Pang and Lee, 2008; Liu and Zhang, 2012; Mohammad, 2015) and proceedings of recent shared task competitions (Wilson et al., 2013; Mohammad et al., 2013; Rosenthal et al., 2015). See Pontiki et al. (2014), Pontiki et al. (2015), and Kiritchenko et al. (2014a) for tasks and systems on aspect based sentiment analysis, where the goal is to determine sentiment towards aspects of a product such as speed of processor and screen resolution of a cell phone. 8 Conclusions and Future Work We described a new shared task on detecting stance towards pre-chosen targets of interest from tweets. We formulated two tasks: a traditional supervised task where labeled training data for the test data targets is made available (Task A) and a more challenging formulation where no labeled da"
S16-1003,W15-0509,1,0.694399,"to see other teams attempting to generalize the supervised data from Task A in interesting ways, either using rules or multi-stage classifiers to bridge the target gap. We are optimistic that there is much interesting follow-up work yet to come on this task. Further details on the submissions can be found 39 Related Work Past work on stance detection includes that by Somasundaran and Wiebe (2010), Anand et al. (2011), Faulkner (2014), Rajadesingan and Liu ˇ (2014), Djemili et al. (2014), Boltuzic and Snajder (2014), Conrad et al. (2012), Sridhar et al. (2014), Rajadesingan and Liu (2014), and Sobhani et al. (2015). There is a vast amount of work in sentiment analysis of tweets, and we refer the reader to surveys (Pang and Lee, 2008; Liu and Zhang, 2012; Mohammad, 2015) and proceedings of recent shared task competitions (Wilson et al., 2013; Mohammad et al., 2013; Rosenthal et al., 2015). See Pontiki et al. (2014), Pontiki et al. (2015), and Kiritchenko et al. (2014a) for tasks and systems on aspect based sentiment analysis, where the goal is to determine sentiment towards aspects of a product such as speed of processor and screen resolution of a cell phone. 8 Conclusions and Future Work We described a"
S16-1003,W10-0214,0,0.524808,"the approach of producing noisy labels, with pkudblab using keyword rules, LitisMind using hashtag rules on external data, and INF-UFRGS using a combination of rules and third-party sentiment classifiers. However, we were pleased to see other teams attempting to generalize the supervised data from Task A in interesting ways, either using rules or multi-stage classifiers to bridge the target gap. We are optimistic that there is much interesting follow-up work yet to come on this task. Further details on the submissions can be found 39 Related Work Past work on stance detection includes that by Somasundaran and Wiebe (2010), Anand et al. (2011), Faulkner (2014), Rajadesingan and Liu ˇ (2014), Djemili et al. (2014), Boltuzic and Snajder (2014), Conrad et al. (2012), Sridhar et al. (2014), Rajadesingan and Liu (2014), and Sobhani et al. (2015). There is a vast amount of work in sentiment analysis of tweets, and we refer the reader to surveys (Pang and Lee, 2008; Liu and Zhang, 2012; Mohammad, 2015) and proceedings of recent shared task competitions (Wilson et al., 2013; Mohammad et al., 2013; Rosenthal et al., 2015). See Pontiki et al. (2014), Pontiki et al. (2015), and Kiritchenko et al. (2014a) for tasks and sys"
S16-1003,W14-2715,0,0.0663582,"d-party sentiment classifiers. However, we were pleased to see other teams attempting to generalize the supervised data from Task A in interesting ways, either using rules or multi-stage classifiers to bridge the target gap. We are optimistic that there is much interesting follow-up work yet to come on this task. Further details on the submissions can be found 39 Related Work Past work on stance detection includes that by Somasundaran and Wiebe (2010), Anand et al. (2011), Faulkner (2014), Rajadesingan and Liu ˇ (2014), Djemili et al. (2014), Boltuzic and Snajder (2014), Conrad et al. (2012), Sridhar et al. (2014), Rajadesingan and Liu (2014), and Sobhani et al. (2015). There is a vast amount of work in sentiment analysis of tweets, and we refer the reader to surveys (Pang and Lee, 2008; Liu and Zhang, 2012; Mohammad, 2015) and proceedings of recent shared task competitions (Wilson et al., 2013; Mohammad et al., 2013; Rosenthal et al., 2015). See Pontiki et al. (2014), Pontiki et al. (2015), and Kiritchenko et al. (2014a) for tasks and systems on aspect based sentiment analysis, where the goal is to determine sentiment towards aspects of a product such as speed of processor and screen resolution of a c"
S16-1003,S16-1075,0,0.119664,"Missing"
S16-1003,S16-1062,0,0.246544,"Teams pkudblab LitisMind INF-UFRGS UWB ECNU USFD Thomson Reuters ltl.uni-due NEUSA All 35.20 31.39 25.52 20.13 29.72 28.43 67.19 51.60 50.04 50.62 40.66 38.87 38.06 34.16 28.86 25.77 29.50 22.66 25.02 19.14 22.80 22.60 4.69 18.35 56.28 44.66 42.32 42.02 34.08 32.70 32.39 26.14 25.73 in the system description papers published in the SemEval-2016 proceedings, including papers by Elfardy and Diab (2016) for CU-GWU, Dias and Becker (2016) for INF-URGS, Patra et al. (2016) for JU NLP, Wojatzki and Zesch (2016) for ltl.uni-due, Zarrella and Marsh (2016) for MITRE, Misra et al. (2016) for nldsucsc, Wei et al. (2016) for pkudblab, Tutek et al. (2016) for TakeLab, Yuki et al. (2016) for Tohoku, and Augenstein et al. (2016) for USFD. 7 Table 5: Results for Task B (the official competition metric Favg ) on different subsets of the test data. The highest score in each column is shown in bold. Table 5 shows results for Task B on subsets of the test set where opinion is expressed towards the target of interest and towards some other entity. Observe that here too results are markedly lower when stance is to be inferred from a tweet expressing opinion about some other entity (and not the target of interest). 6.3"
S16-1003,H05-1044,0,0.231407,"tries, but PKULCWM’s fourth place result shows that it is possible to do well with a more traditional approach that relies instead on Twitter-specific linguistic pre-processing. Along these lines, it is worth noting that both MITRE and pkudblab reflect knowledge-light approaches to the problem, each relying minimally on linguistic processing and external lexicons. Seven of the nineteen submissions made extensive use of publicly-available sentiment and emotion lexicons such as the NRC Emotion Lexicon (Mohammad and Turney, 2010), Hu and Liu Lexicon (Hu and Liu, 2004), MPQA Subjectivity Lexicon (Wilson et al., 2005), and NRC Hashtag Lexicons (Kiritchenko et al., 2014b). Recall that the SVM-ngrams baseline also performed very well, using only word and character ngrams in its classifiers. This helps emphasize the fact that for this young task, the community is still a long way from an established set of best practices. 6 Systems and Results for Task B The sub-sections below discuss baselines and official submissions to Task B. Recall, that the test data for Task B is for the target ‘Donald Trump’, and no training data for this target was provided. 6.1 Task B Baselines We calculated two baselines listed bel"
S16-1003,S13-2052,0,0.0609198,"Missing"
S16-1003,S16-1069,0,0.0184379,"ate Change’ task (54.86). Team Baselines Majority class SVM-ngrams-comb Opinion Towards Target Other Participating Teams pkudblab LitisMind INF-UFRGS UWB ECNU USFD Thomson Reuters ltl.uni-due NEUSA All 35.20 31.39 25.52 20.13 29.72 28.43 67.19 51.60 50.04 50.62 40.66 38.87 38.06 34.16 28.86 25.77 29.50 22.66 25.02 19.14 22.80 22.60 4.69 18.35 56.28 44.66 42.32 42.02 34.08 32.70 32.39 26.14 25.73 in the system description papers published in the SemEval-2016 proceedings, including papers by Elfardy and Diab (2016) for CU-GWU, Dias and Becker (2016) for INF-URGS, Patra et al. (2016) for JU NLP, Wojatzki and Zesch (2016) for ltl.uni-due, Zarrella and Marsh (2016) for MITRE, Misra et al. (2016) for nldsucsc, Wei et al. (2016) for pkudblab, Tutek et al. (2016) for TakeLab, Yuki et al. (2016) for Tohoku, and Augenstein et al. (2016) for USFD. 7 Table 5: Results for Task B (the official competition metric Favg ) on different subsets of the test data. The highest score in each column is shown in bold. Table 5 shows results for Task B on subsets of the test set where opinion is expressed towards the target of interest and towards some other entity. Observe that here too results are markedly lower when stance is to"
S16-1003,S16-1065,0,0.0288745,"ltl.uni-due NEUSA All 35.20 31.39 25.52 20.13 29.72 28.43 67.19 51.60 50.04 50.62 40.66 38.87 38.06 34.16 28.86 25.77 29.50 22.66 25.02 19.14 22.80 22.60 4.69 18.35 56.28 44.66 42.32 42.02 34.08 32.70 32.39 26.14 25.73 in the system description papers published in the SemEval-2016 proceedings, including papers by Elfardy and Diab (2016) for CU-GWU, Dias and Becker (2016) for INF-URGS, Patra et al. (2016) for JU NLP, Wojatzki and Zesch (2016) for ltl.uni-due, Zarrella and Marsh (2016) for MITRE, Misra et al. (2016) for nldsucsc, Wei et al. (2016) for pkudblab, Tutek et al. (2016) for TakeLab, Yuki et al. (2016) for Tohoku, and Augenstein et al. (2016) for USFD. 7 Table 5: Results for Task B (the official competition metric Favg ) on different subsets of the test data. The highest score in each column is shown in bold. Table 5 shows results for Task B on subsets of the test set where opinion is expressed towards the target of interest and towards some other entity. Observe that here too results are markedly lower when stance is to be inferred from a tweet expressing opinion about some other entity (and not the target of interest). 6.3 Discussion Some teams did very well detecting tweets in favor of T"
S16-1003,S16-1074,0,0.155861,"jority class SVM-ngrams-comb Opinion Towards Target Other Participating Teams pkudblab LitisMind INF-UFRGS UWB ECNU USFD Thomson Reuters ltl.uni-due NEUSA All 35.20 31.39 25.52 20.13 29.72 28.43 67.19 51.60 50.04 50.62 40.66 38.87 38.06 34.16 28.86 25.77 29.50 22.66 25.02 19.14 22.80 22.60 4.69 18.35 56.28 44.66 42.32 42.02 34.08 32.70 32.39 26.14 25.73 in the system description papers published in the SemEval-2016 proceedings, including papers by Elfardy and Diab (2016) for CU-GWU, Dias and Becker (2016) for INF-URGS, Patra et al. (2016) for JU NLP, Wojatzki and Zesch (2016) for ltl.uni-due, Zarrella and Marsh (2016) for MITRE, Misra et al. (2016) for nldsucsc, Wei et al. (2016) for pkudblab, Tutek et al. (2016) for TakeLab, Yuki et al. (2016) for Tohoku, and Augenstein et al. (2016) for USFD. 7 Table 5: Results for Task B (the official competition metric Favg ) on different subsets of the test data. The highest score in each column is shown in bold. Table 5 shows results for Task B on subsets of the test set where opinion is expressed towards the target of interest and towards some other entity. Observe that here too results are markedly lower when stance is to be inferred from a tweet expressing opinion"
W03-0302,J93-2003,0,0.0465703,": An Example of Cohesion Constraint To define this notion more formally, let TE (ei ) be the subtree of TE rooted at ei . The phrase span of ei , spanP(ei , TE , A), is the image of the English phrase headed by ei in F given a (partial) alignment A. More precisely, spanP(ei , TE , A) = [k1 , k2 ], where k1 = min{j|l(u, j) ∈ A, eu ∈ TE (ei )} k2 = max{j|l(u, j) ∈ A, eu ∈ TE (ei )} Probability Model We define the word alignment problem as finding the alignment A that maximizes P (A|E, F ). ProAlign models P (A|E, F ) directly, using a different decomposition of terms than the model used by IBM (Brown et al., 1993). In the IBM models of translation, alignments exist as artifacts of a stochastic process, where the words in the English sentence generate the words in the French sentence. Our model does not assume that one sentence generates the other. Instead it takes both sentences as given, and uses the sentences to determine an alignment. An alignment A consists of t links {l1 , l2 , . . . , lt }, where each lk = l(eik , fjk ) for some ik and jk . We will refer to consecutive subsets of A as lij = {li , li+1 , . . . , lj }. Given this notation, P (A|E, F ) can be decomposed as follows: P (A|E, F ) = P ("
W03-0302,P03-1012,1,0.81084,"cal systems. We will consider only a subset FT k of relevant features of Ck . We will make the Na¨ıve Bayes-style assumption that these features ft ∈ FT k are conditionally independent given either lk or (eik , fjk ). This produces a tractable formulation for P (A|E, F ):   t Y Y P (ft|lk )  P (lk |eik , fjk ) × P (ft|eik , fjk ) obj pre subj det det the host discovers all the devices 1 2 3 4 5 6 1 2 3 4 5 6 l&apos; hôte repère tous les périphériques the host locate all the peripherals Figure 2: Feature Extraction Example More details on the probability model used by ProAlign are available in (Cherry and Lin, 2003). In contrast, the incorrect link (the1 , les) will have only ft d (+3, det), which will work to lower the link probability, since most determiners are located before their governors. 3.1 3.2 k=1 ft∈FT k Features used in the shared task For the purposes of the shared task, we use two feature types. Each type could have any number of instantiations for any number of contexts. Note that each feature type is described in terms of the context surrounding a word pair. The first feature type ft a concerns surrounding links. It has been observed that words close to each other in the source language t"
W03-0302,W02-1039,0,0.413507,"orate linguistic features into our model and linguistic intuitions into our constraints. 2 Constraints The model used for scoring alignments has no mechanism to prevent certain types of undesirable alignments, such as having all French words align to the same English word. To guide the search to correct alignments, we employ two constraints to limit our search for the most probable alignment. The first constraint is the one-to-one constraint (Melamed, 2000): every word (except the null words e0 and f0 ) participates in exactly one link. The second constraint, known as the cohesion constraint (Fox, 2002), uses the dependency tree (Mel’ˇcuk, 1987) of the English sentence to restrict possible link combinations. Given the dependency tree TE and a (partial) alignment A, the cohesion constraint requires that phrasal cohesion is maintained in the French sentence. If two phrases are disjoint in the English sentence, the alignment must not map them to overlapping intervals in the French sentence. This notion of phrasal constraints on alignments need not be restricted to phrases determined from a dependency structure. However, the experiments conducted in (Fox, 2002) indicate that dependency trees dem"
W03-0302,H91-1026,0,0.482989,"Missing"
W03-0302,N03-2017,1,0.842847,"alignment is cohesive with respect to TE if it does not introduce any head-modifier or modifier-modifier overlaps. For example, the alignment A in Figure 1 is not cohesive because spanP (reboot, TE , A) = [4, 4] intersects spanP (discover, TE , A) = [2, 11]. Since both reboot and discover modify causes, this creates a modifiermodifier overlap. One can check for constraint violations inexpensively by incrementally updating the various spans as new links are added to the partial alignment, and checking for overlap after each modification. More details on the cohesion constraint can be found in (Lin and Cherry, 2003). 3 det aux 2002) as crossings. Given a head node eh and its modifier em , a head-modifier overlap occurs when: peripherals Figure 1: An Example of Cohesion Constraint To define this notion more formally, let TE (ei ) be the subtree of TE rooted at ei . The phrase span of ei , spanP(ei , TE , A), is the image of the English phrase headed by ei in F given a (partial) alignment A. More precisely, spanP(ei , TE , A) = [k1 , k2 ], where k1 = min{j|l(u, j) ∈ A, eu ∈ TE (ei )} k2 = max{j|l(u, j) ∈ A, eu ∈ TE (ei )} Probability Model We define the word alignment problem as finding the alignment A tha"
W03-0302,J00-2004,0,0.126241,"of English-French sentence pairs, along with dependency trees for the English sentences. The presence of the English dependency tree allows us to incorporate linguistic features into our model and linguistic intuitions into our constraints. 2 Constraints The model used for scoring alignments has no mechanism to prevent certain types of undesirable alignments, such as having all French words align to the same English word. To guide the search to correct alignments, we employ two constraints to limit our search for the most probable alignment. The first constraint is the one-to-one constraint (Melamed, 2000): every word (except the null words e0 and f0 ) participates in exactly one link. The second constraint, known as the cohesion constraint (Fox, 2002), uses the dependency tree (Mel’ˇcuk, 1987) of the English sentence to restrict possible link combinations. Given the dependency tree TE and a (partial) alignment A, the cohesion constraint requires that phrasal cohesion is maintained in the French sentence. If two phrases are disjoint in the English sentence, the alignment must not map them to overlapping intervals in the French sentence. This notion of phrasal constraints on alignments need not"
W03-0302,C96-2141,0,0.0886709,"only ft d (+3, det), which will work to lower the link probability, since most determiners are located before their governors. 3.1 3.2 k=1 ft∈FT k Features used in the shared task For the purposes of the shared task, we use two feature types. Each type could have any number of instantiations for any number of contexts. Note that each feature type is described in terms of the context surrounding a word pair. The first feature type ft a concerns surrounding links. It has been observed that words close to each other in the source language tend to remain close to each other in the translation (S. Vogel and Tillmann, 1996). To capture this notion, for any word pair (ei , fj ), if a link l(ei0 , fj 0 ) exists within a window of two words (where i − 2 ≤ i0 ≤ i + 2 and j − 2 ≤ j 0 ≤ j + 2), then we say that the feature ft a (i − i0 , j − j 0 , ei0 ) is active for this context. We refer to these as adjacency features. The second feature type ft d uses the English parse tree to capture regularities among grammatical relations between languages. For example, when dealing with French and English, the location of the determiner with respect to its governor is never swapped during translation, while the location of adje"
W05-0612,J93-1005,0,0.209078,"Missing"
W05-0612,N04-1038,0,0.0388071,"er candidates. The above learning approaches require annotated training data for supervised learning. Cardie and Wagstaff developed an unsupervised approach that partitions noun phrases into coreferent groups through clustering (1999). However, the partitions they generate for a particular document are not useful for processing new documents, while our approach learns distributions that can be used on unseen data. There are also approaches to anaphora resolution using unsupervised methods to extract useful information, such as gender and number (Ge et al., 1998), or contextual role-knowledge (Bean and Riloff, 2004). Co-training can also leverage unlabeled data through weakly-supervised reference resolution learning (M¨uller et al., 2002). As an alternative to co-training, Ng and Cardie (2003) use EM to augment a supervised coreference system with unlabeled data. Their feature set is quite different, as it is designed to generalize from the data in a labeled set, while our system models individual words. We suspect that the two approaches can be combined. Our approach is inspired by the use of EM in bilingual word alignment, which finds word-to-word correspondences between a sentence and its translation."
W05-0612,J96-1002,0,0.0120458,"reasonable independence assumptions, our four models may not be combined optimally for our pronoun resolution task, as the models are only approximations of the true distributions they are intended to represent. Following the approach in (Och and Ney, 2002), we can view the right-hand-side of Equation 5 as a special case of: exp λ1 log Pr(p|l) + λ2 log Pr(k|l)+ λ3 log Pr(l) + λ4 log Pr(j) ! (8) 92 where ∀i : λi = 1. Effectively, the log probabilities of our models become feature functions in a log-linear model. When labeled training data is available, we can use the Maximum Entropy principle (Berger et al., 1996) to optimize the λ weights. This provides us with an optional supervised extension to the unsupervised system. Given a small set of data that has the correct candidates indicated, such as the set we used while developing our unsupervised system, we can re-weight the final models provided by EM to maximize the probability of observing the indicated candidates. To this end, we follow the approach of (Och and Ney, 2002) very closely, including their handling of multiple correct answers. We use the limited memory variable metric method as implemented in Malouf’s maximum entropy package (2002) to s"
W05-0612,J93-2003,0,0.00746346,"Missing"
W05-0612,W99-0611,0,0.262101,"Missing"
W05-0612,W98-1119,0,0.785981,"some combination of constraints and preferences to select the antecedent from preceding noun phrase candidates. Constraints filter the candidate list of improbable antecedents, while preferences encourage selection of antecedents that are more recent, frequent, etc. Implementation of constraints and preferences can be based on empirical insight (Lappin and Leass, 1994), or machine learning from a reference88 Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL), c pages 88–95, Ann Arbor, June 2005. 2005 Association for Computational Linguistics annotated corpus (Ge et al., 1998). The majority of pronoun resolution approaches have thus far relied on manual intervention in the resolution process, such as using a manually-parsed corpus, or manually removing difficult non-anaphoric cases; we follow Mitkov et al.’s approach (2002) with a fully-automatic pronoun resolution method. Parsing, noun-phrase identification, and non-anaphoric pronoun removal are all done automatically. Machine-learned, fully-automatic systems are more common in noun phrase coreference resolution, where the method of choice has been decision trees (Soon et al., 2001; Ng and Cardie, 2002). These sys"
W05-0612,W97-0319,0,0.0890578,"Missing"
W05-0612,C96-1021,0,0.2979,"eonastic. We use the syntactic constraints from Binding Theory to eliminate candidates (Haegeman, 1994). For the reflexives himself, herself, itself and themselves, this allows immediate syntactic identification of the antecedent. These cases become unambiguous; only the indicated antecedent is included in C. We improve the quality of our training set by removing known noisy cases before passing the set to EM. For example, we anticipate that sentences with quotation marks will be problematic, as other researchers have observed that quoted text requires special handling for pronoun resolution (Kennedy and Boguraev, 1996). Thus we remove pronouns occurring in the same sentences as quotes from the learning process. Also, we exclude triples where the constraints removed all possible antecedents, or where the pronoun was deemed to be pleonastic. Performing these exclusions is justified for training, but in testing we state results for all pronouns. 3.5 EM initialization Early in the development of this system, we were impressed with the quality of the pronoun model Pr(p|l) learned by EM. However, we found we could construct an even more precise pronoun model for common words by examining unambiguous cases in our"
W05-0612,J94-4002,0,0.539752,"data. This model is shown to perform reliably on its own. We also demonstrate how the models learned through our unsupervised method can be used as features in a supervised pronoun resolution system. 2 Related Work Pronoun resolution typically employs some combination of constraints and preferences to select the antecedent from preceding noun phrase candidates. Constraints filter the candidate list of improbable antecedents, while preferences encourage selection of antecedents that are more recent, frequent, etc. Implementation of constraints and preferences can be based on empirical insight (Lappin and Leass, 1994), or machine learning from a reference88 Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL), c pages 88–95, Ann Arbor, June 2005. 2005 Association for Computational Linguistics annotated corpus (Ge et al., 1998). The majority of pronoun resolution approaches have thus far relied on manual intervention in the resolution process, such as using a manually-parsed corpus, or manually removing difficult non-anaphoric cases; we follow Mitkov et al.’s approach (2002) with a fully-automatic pronoun resolution method. Parsing, noun-phrase identification, and non-anaphor"
W05-0612,C94-1079,0,0.0105797,"s key were used to guide us while designing the probability model, and to fine-tune EM and smoothing parameters. We also use the development key as labeled training data for our supervised extension. The test set consists of 890,000 pronouns drawn from 50,000 documents. The test key consists of 1209 labeled pronouns drawn from 118 documents; 892 are drawn from sentences without quotation marks. All of the results reported in Section 5 are determined using the test key. 4.2 Implementation Details To get the context values and implement the syntactic filters, we parsed our corpora with Minipar (Lin, 1994). Experiments on the development set indicated that EM generally began to overfit after 2 iterations, so we stop EM after the second iteration, using the models from the second M-step for testing. During testing, ties in likelihood are broken by taking the candidate closest to the pronoun. The EM-produced models need to be smoothed, as there will be unseen words and unobserved (p, l) or (k, l) pairs in the test set. This is because problematic cases are omitted from the training set, while all pronouns are included in the key. We handle out-of-vocabulary events by replacing words or context-va"
W05-0612,W02-2018,0,0.0357864,"Missing"
W05-0612,P02-1045,0,0.146722,"Missing"
W05-0612,P02-1014,0,0.117125,"otated corpus (Ge et al., 1998). The majority of pronoun resolution approaches have thus far relied on manual intervention in the resolution process, such as using a manually-parsed corpus, or manually removing difficult non-anaphoric cases; we follow Mitkov et al.’s approach (2002) with a fully-automatic pronoun resolution method. Parsing, noun-phrase identification, and non-anaphoric pronoun removal are all done automatically. Machine-learned, fully-automatic systems are more common in noun phrase coreference resolution, where the method of choice has been decision trees (Soon et al., 2001; Ng and Cardie, 2002). These systems generally handle pronouns as a subset of all noun phrases, but with limited features compared to systems devoted solely to pronouns. Kehler used Maximum Entropy to assign a probability distribution over possible noun phrase coreference relationships (1997). Like his approach, our system does not make hard coreference decisions, but returns a distribution over candidates. The above learning approaches require annotated training data for supervised learning. Cardie and Wagstaff developed an unsupervised approach that partitions noun phrases into coreferent groups through clusteri"
W05-0612,N03-1023,0,0.0215374,"es into coreferent groups through clustering (1999). However, the partitions they generate for a particular document are not useful for processing new documents, while our approach learns distributions that can be used on unseen data. There are also approaches to anaphora resolution using unsupervised methods to extract useful information, such as gender and number (Ge et al., 1998), or contextual role-knowledge (Bean and Riloff, 2004). Co-training can also leverage unlabeled data through weakly-supervised reference resolution learning (M¨uller et al., 2002). As an alternative to co-training, Ng and Cardie (2003) use EM to augment a supervised coreference system with unlabeled data. Their feature set is quite different, as it is designed to generalize from the data in a labeled set, while our system models individual words. We suspect that the two approaches can be combined. Our approach is inspired by the use of EM in bilingual word alignment, which finds word-to-word correspondences between a sentence and its translation. The prominent statistical methods in this field are unsupervised. Our methods are most influenced by IBM’s Model 1 (Brown et al., 1993). 89 3 3.1 Methods Problem formulation We wil"
W05-0612,P02-1038,0,0.03558,"ender/number distribution, while those observed frequently will closely match the observed distribution. During development, we also tried clever initializers for the other three models, including an extensive language model initializer, but none were able to improve over PrU (p|l) alone. 3.6 Supervised extension Even though we have justified Equation 5 with reasonable independence assumptions, our four models may not be combined optimally for our pronoun resolution task, as the models are only approximations of the true distributions they are intended to represent. Following the approach in (Och and Ney, 2002), we can view the right-hand-side of Equation 5 as a special case of: exp λ1 log Pr(p|l) + λ2 log Pr(k|l)+ λ3 log Pr(l) + λ4 log Pr(j) ! (8) 92 where ∀i : λi = 1. Effectively, the log probabilities of our models become feature functions in a log-linear model. When labeled training data is available, we can use the Maximum Entropy principle (Berger et al., 1996) to optimize the λ weights. This provides us with an optional supervised extension to the unsupervised system. Given a small set of data that has the correct candidates indicated, such as the set we used while developing our unsupervised"
W05-0612,J01-4004,0,0.567683,"nal Linguistics annotated corpus (Ge et al., 1998). The majority of pronoun resolution approaches have thus far relied on manual intervention in the resolution process, such as using a manually-parsed corpus, or manually removing difficult non-anaphoric cases; we follow Mitkov et al.’s approach (2002) with a fully-automatic pronoun resolution method. Parsing, noun-phrase identification, and non-anaphoric pronoun removal are all done automatically. Machine-learned, fully-automatic systems are more common in noun phrase coreference resolution, where the method of choice has been decision trees (Soon et al., 2001; Ng and Cardie, 2002). These systems generally handle pronouns as a subset of all noun phrases, but with limited features compared to systems devoted solely to pronouns. Kehler used Maximum Entropy to assign a probability distribution over possible noun phrase coreference relationships (1997). Like his approach, our system does not make hard coreference decisions, but returns a distribution over candidates. The above learning approaches require annotated training data for supervised learning. Cardie and Wagstaff developed an unsupervised approach that partitions noun phrases into coreferent g"
W06-2904,W04-3201,0,\N,Missing
W06-2904,A00-2018,0,\N,Missing
W06-2904,W03-3023,0,\N,Missing
W06-2904,J04-4004,0,\N,Missing
W06-2904,P97-1003,0,\N,Missing
W06-2904,W01-0521,0,\N,Missing
W06-2904,C96-1058,0,\N,Missing
W06-2904,W05-1516,1,\N,Missing
W06-2904,P99-1059,0,\N,Missing
W06-2904,P03-1054,0,\N,Missing
W06-2904,P05-1012,0,\N,Missing
W06-2904,P04-1054,0,\N,Missing
W06-2904,P90-1034,0,\N,Missing
W06-2904,P03-1012,1,\N,Missing
W06-2904,W02-1039,0,\N,Missing
W06-2904,P98-2127,0,\N,Missing
W06-2904,C98-2122,0,\N,Missing
W06-2904,P93-1024,0,\N,Missing
W06-2904,A00-1039,0,\N,Missing
W06-3319,W04-1213,0,0.0878962,"Missing"
W06-3319,W02-1001,0,\N,Missing
W07-0403,E06-1019,1,0.866268,"am only. The work described here uses the binary bracketing ITG, which has a single non-terminal: A → [AA] |hAAi |e/f (2) This grammar admits an efficient bitext parsing algorithm, and holds no language-specific biases. (2) cannot represent all possible permutations of concepts that may occur during translation, because some permutations will require discontinuous constituents (Melamed, 2003). This ITG constraint is characterized by the two forbidden structures shown in Figure 1 (Wu, 1997). Empirical studies suggest that only a small percentage of human translations violate these constraints (Cherry and Lin, 2006). 19 Stochastic ITGs are parameterized like their PCFG counterparts (Wu, 1997); productions A → X are assigned probability Pr(X |A). These parameters can be learned from sentence-aligned bitext using the EM algorithm. The expectation task of counting productions weighted by their probability is handled with dynamic programming, using the inside-outside algorithm extended to bitext (Zhang and Gildea, 2004). 3 ITG as a Phrasal Translation Model This paper introduces a phrasal ITG; in doing so, we combine ITG with the JPTM. ITG parsing algorithms consider every possible two-dimensional span of bi"
W07-0403,P06-1097,0,0.0301103,"GIZA++ alignments are trained with no sentence-length limit, using the full 688K corpus. 5.1 Table 1: Inside-outside run-time comparison. Alignment Experiments The goal of this experiment is to compare the Viterbi alignments from the phrasal ITG to gold standard human alignments. We do this to validate our noncompositional constraint and to select good alignments for use with the surface heuristic. 22 Method GIZA++ Intersect GIZA++ Union GIZA++ GDF Phrasal ITG Phrasal ITG + NCC Prec 96.7 82.5 84.0 50.7 75.4 Rec 53.0 69.0 68.2 80.3 78.0 F-measure 68.5 75.1 75.2 62.2 76.7 Following the lead of (Fraser and Marcu, 2006), we hand-aligned the first 100 sentence pairs of our training set according to the Blinker annotation guidelines (Melamed, 1998). We did not differentiate between sure and possible links. We report precision, recall and balanced F-measure (Och and Ney, 2003). For comparison purposes, we include the results of three types of GIZA++ combination, including the grow-diag-final heuristic (GDF). We tested our phrasal ITG with fixed link pruning, and then added the non-compositional constraint (NCC). During development we determined that performance levels off for both of the ITG models after 3 EM i"
W07-0403,N03-1017,0,0.467844,"gorithmically wellfounded method for phrasal analysis of bitext. Section 2 begins by outlining the phrase extraction system we intend to replace and the two methods we combine to do so: the joint phrasal translation model (JPTM) and inversion transduction grammar (ITG). Section 3 describes our proposed solution, a phrasal ITG. Section 4 describes how to apply our phrasal ITG, both as a translation model and as a phrasal word-aligner. Section 5 tests our system in both these capacities, while Section 6 concludes. 2 2.1 Background Phrase Table Extraction Phrasal decoders require a phrase table (Koehn et al., 2003), which contains bilingual phrase pairs and 17 Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 17–24, c Rochester, New York, April 2007. 2007 Association for Computational Linguistics scores indicating their utility. The surface heuristic is the most popular method for phrase-table construction. It extracts all consistent phrase pairs from word-aligned bitext (Koehn et al., 2003). The word alignment provides bilingual links, indicating translation relationships between words. Consistency is defined so that alignment links are never"
W07-0403,N03-1021,0,0.0187489,"h their non-terminals inside square brackets [. . .], produce their symbols in the given order in both streams. Inverted productions, indicated by angled brackets h. . .i, are output in reverse order in the Foreign stream only. The work described here uses the binary bracketing ITG, which has a single non-terminal: A → [AA] |hAAi |e/f (2) This grammar admits an efficient bitext parsing algorithm, and holds no language-specific biases. (2) cannot represent all possible permutations of concepts that may occur during translation, because some permutations will require discontinuous constituents (Melamed, 2003). This ITG constraint is characterized by the two forbidden structures shown in Figure 1 (Wu, 1997). Empirical studies suggest that only a small percentage of human translations violate these constraints (Cherry and Lin, 2006). 19 Stochastic ITGs are parameterized like their PCFG counterparts (Wu, 1997); productions A → X are assigned probability Pr(X |A). These parameters can be learned from sentence-aligned bitext using the EM algorithm. The expectation task of counting productions weighted by their probability is handled with dynamic programming, using the inside-outside algorithm extended"
W07-0403,J03-1002,0,0.0176673,"defined so that alignment links are never broken by phrase boundaries. For each token w in a consistent phrase pair p¯, all tokens linked to w by the alignment must also be included in p¯. Each consistent phrase pair is counted as occurring once per sentence pair. The scores for the extracted phrase pairs are provided by normalizing these flat counts according to common English or Foreign components, producing the conditional distributions p(f¯|¯ e) and p(¯ e|f¯). The surface heuristic can define consistency according to any word alignment; but most often, the alignment is provided by GIZA++ (Och and Ney, 2003). This alignment system is powered by the IBM translation models (Brown et al., 1993), in which one sentence generates the other. These models produce only one-to-many alignments: each generated token can participate in at most one link. Many-to-many alignments can be created by combining two GIZA++ alignments, one where English generates Foreign and another with those roles reversed (Och and Ney, 2003). Combination approaches begin with the intersection of the two alignments, and add links from the union heuristically. The grow-diag-final (GDF) combination heuristic (Koehn et al., 2003) adds"
W07-0403,P03-1021,0,0.0649402,"e phrase tables. Two are conditionalized phrasal models, each EM trained until performance degrades: • C-JPTM3 as described in (Birch et al., 2006) • Phrasal ITG as described in Section 4.1 Three provide alignments for the surface heuristic: • GIZA++ with grow-diag-final (GDF) • Viterbi Phrasal ITG with and without the noncompositional constraint We use the Pharaoh decoder (Koehn et al., 2003) with the SMT Shared Task baseline system (Koehn and Monz, 2006). Weights for the log-linear model are set using the 500-sentence tuning set provided for the shared task with minimum error rate training (Och, 2003) as implemented by Venugopal and Vogel (2005). Results on the provided 2000sentence development set are reported using the BLEU metric (Papineni et al., 2002). For all methods, we report performance with and without IBM Model 1 features (M1), along with the size of the resulting tables in millions of phrase pairs. The results of all experiments are shown in Table 3. We see that the Phrasal ITG surpasses the CJPTM by more than 2.5 BLEU points. A large component of this improvement is due to the ITG’s use of inside-outside for expectation calculation, though 3 Supplied by personal communication."
W07-0403,P02-1040,0,0.108799,") • Phrasal ITG as described in Section 4.1 Three provide alignments for the surface heuristic: • GIZA++ with grow-diag-final (GDF) • Viterbi Phrasal ITG with and without the noncompositional constraint We use the Pharaoh decoder (Koehn et al., 2003) with the SMT Shared Task baseline system (Koehn and Monz, 2006). Weights for the log-linear model are set using the 500-sentence tuning set provided for the shared task with minimum error rate training (Och, 2003) as implemented by Venugopal and Vogel (2005). Results on the provided 2000sentence development set are reported using the BLEU metric (Papineni et al., 2002). For all methods, we report performance with and without IBM Model 1 features (M1), along with the size of the resulting tables in millions of phrase pairs. The results of all experiments are shown in Table 3. We see that the Phrasal ITG surpasses the CJPTM by more than 2.5 BLEU points. A large component of this improvement is due to the ITG’s use of inside-outside for expectation calculation, though 3 Supplied by personal communication. Run with default parameters, but with maximum phrase length increased to 5. 23 Table 3: Translation Comparison. Method BLEU +M1 Size Conditionalized Phrasal"
W07-0403,2005.eamt-1.36,0,0.0100519,"itionalized phrasal models, each EM trained until performance degrades: • C-JPTM3 as described in (Birch et al., 2006) • Phrasal ITG as described in Section 4.1 Three provide alignments for the surface heuristic: • GIZA++ with grow-diag-final (GDF) • Viterbi Phrasal ITG with and without the noncompositional constraint We use the Pharaoh decoder (Koehn et al., 2003) with the SMT Shared Task baseline system (Koehn and Monz, 2006). Weights for the log-linear model are set using the 500-sentence tuning set provided for the shared task with minimum error rate training (Och, 2003) as implemented by Venugopal and Vogel (2005). Results on the provided 2000sentence development set are reported using the BLEU metric (Papineni et al., 2002). For all methods, we report performance with and without IBM Model 1 features (M1), along with the size of the resulting tables in millions of phrase pairs. The results of all experiments are shown in Table 3. We see that the Phrasal ITG surpasses the CJPTM by more than 2.5 BLEU points. A large component of this improvement is due to the ITG’s use of inside-outside for expectation calculation, though 3 Supplied by personal communication. Run with default parameters, but with maximu"
W07-0403,W05-0835,0,0.061732,"the span under consideration might have been drawn directly from the lexicon. This option can be added to our grammar by altering the definition of a terminal production to include phrases: A → e¯/f¯. This third option is shown in Figure 2 (c). The model implied by this extended grammar is trained using inside-outside and EM. Our approach differs from previous attempts to use ITGs for phrasal bitext analysis. Wu (1997) used a binary bracketing ITG to segment a sentence while simultaneously word-aligning it to its translation, but the model was trained heuristically with a fixed segmentation. Vilar and Vidal (2005) used ITG-like dynamic programming to drive both training and alignment for their recursive translation model, but they employed a conditional model that did not maintain a phrasal lexicon. Instead, they scored phrase pairs using IBM Model 1. Our phrasal ITG is quite similar to the JPTM. Both models are trained with EM, and both employ generative stories that create a sentence and its translation simultaneously. The similarities become more apparent when we consider the canonical-form binary-bracketing ITG (Wu, 1997) shown here: S → A|B|C A → [AB] |[BB] |[CB] | [AC] |[BC] |[CC] B → hAAi |hBAi"
W07-0403,2003.mtsummit-papers.53,0,0.00962983,"n produce the necessary conditional probabilities by conditionalizing the joint table in both directions. We use our p(¯ e/f¯|C) distribution from our stochastic grammar to produce p(¯ e|f¯) and p(f¯|¯ e) values for its phrasal lexicon. 21 Pharaoh also includes lexical weighting parameters that are derived from the alignments used to induce its phrase pairs (Koehn et al., 2003). Using the phrasal ITG as a direct translation model, we do not produce alignments for individual sentence pairs. Instead, we provide a lexical preference with an IBM Model 1 feature pM1 that penalizes unmatched words (Vogel et al., 2003). We include both pM1 (¯ e|f¯) and pM1 (f¯|¯ e). 4.2 Phrasal Word Alignment We can produce a translation model using insideoutside, without ever creating a Viterbi parse. However, we can also examine the maximum likelihood phrasal alignments predicted by the trained model. Despite its strengths derived from using phrases throughout training, the alignments predicted by our phrasal ITG are usually unsatisfying. For example, the fragment pair (order of business, ordre des travaux) is aligned as a phrase pair by our system, linking every English word to every French word. This is frustrating, sin"
W07-0403,J97-3002,0,0.906229,"e statistical analysis used at the word level in a phrasal setting have met with limited success, held back by the sheer size of phrasal alignment space. Hybrid methods that combine well-founded statistical analysis with high-confidence word-level alignments have made some headway (Birch et al., 2006), but suffer from the daunting task of heuristically exploring a still very large alignment space. In the meantime, synchronous parsing methods efficiently process the same bitext phrases while building their bilingual constituents, but continue to be employed primarily for word-to-word analysis (Wu, 1997). In this paper we unify the probability models for phrasal translation with the algorithms for synchronous parsing, harnessing the benefits of both to create a statistically and algorithmically wellfounded method for phrasal analysis of bitext. Section 2 begins by outlining the phrase extraction system we intend to replace and the two methods we combine to do so: the joint phrasal translation model (JPTM) and inversion transduction grammar (ITG). Section 3 describes our proposed solution, a phrasal ITG. Section 4 describes how to apply our phrasal ITG, both as a translation model and as a phr"
W07-0403,C04-1060,0,0.0445884,"constraint is characterized by the two forbidden structures shown in Figure 1 (Wu, 1997). Empirical studies suggest that only a small percentage of human translations violate these constraints (Cherry and Lin, 2006). 19 Stochastic ITGs are parameterized like their PCFG counterparts (Wu, 1997); productions A → X are assigned probability Pr(X |A). These parameters can be learned from sentence-aligned bitext using the EM algorithm. The expectation task of counting productions weighted by their probability is handled with dynamic programming, using the inside-outside algorithm extended to bitext (Zhang and Gildea, 2004). 3 ITG as a Phrasal Translation Model This paper introduces a phrasal ITG; in doing so, we combine ITG with the JPTM. ITG parsing algorithms consider every possible two-dimensional span of bitext, each corresponding to a bilingual phrase pair. Each multi-token span is analyzed in terms of how it could be built from smaller spans using a straight or inverted production, as is illustrated in Figures 2 (a) and (b). To extend ITG to a phrasal setting, we add a third option for span analysis: that the span under consideration might have been drawn directly from the lexicon. This option can be adde"
W07-0403,W06-3123,0,0.784815,"uch as monolingual agreement and short-range movement, taking pressure off of language and distortion models. Despite the success of phrasal decoders, knowledge acquisition for translation generally begins with a word-level analysis of the training text, taking the form of a word alignment. Attempts to apply the same statistical analysis used at the word level in a phrasal setting have met with limited success, held back by the sheer size of phrasal alignment space. Hybrid methods that combine well-founded statistical analysis with high-confidence word-level alignments have made some headway (Birch et al., 2006), but suffer from the daunting task of heuristically exploring a still very large alignment space. In the meantime, synchronous parsing methods efficiently process the same bitext phrases while building their bilingual constituents, but continue to be employed primarily for word-to-word analysis (Wu, 1997). In this paper we unify the probability models for phrasal translation with the algorithms for synchronous parsing, harnessing the benefits of both to create a statistically and algorithmically wellfounded method for phrasal analysis of bitext. Section 2 begins by outlining the phrase extrac"
W07-0403,N06-1033,0,0.0675552,"Missing"
W07-0403,J93-2003,0,\N,Missing
W07-0403,P05-1059,0,\N,Missing
W07-0403,2006.amta-papers.2,0,\N,Missing
W07-0403,W06-3114,0,\N,Missing
W07-0403,D08-1076,0,\N,Missing
W09-3514,W02-1001,0,0.0346424,"1998), distributional similarity (Bilac and Tanaka, 2005), or the 1 Provided by http://www.cjk.org 69 Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 69–71, c Suntec, Singapore, 7 August 2009. 2009 ACL and AFNLP Table 1: Development and test 1-best accuracies, as reported by the official evaluation tool ame → A , ri → J , can → S System / Test set Hindi Dev Hindi Test Katakana Dev Katakana Test Figure 1: Example derivation transforming “American” into “AJ S”. accuracy on the provided development sets. The engine’s features are trained using the structured perceptron (Collins, 2002). Jiampojamarn et al. (2008) show strong improvements in the L2P domain using MIRA in place of the perceptron update; unfortunately, we did not implement a k-best MIRA update due to time constraints. In our implementation, no special consideration was given to the availability of multiple correct answers in the training data; we always pick the first reference transliteration and treat it as the only correct answer. Investigating the use of all correct answers would be an obvious next step to improve the system. 3 With Bug 36.7 41.8 46.0 46.6 Fixed 39.6 46.6 47.1 46.9 The Bug The submitted ver"
W09-3514,P08-1103,1,0.920003,"component, which makes L2P technology a nearideal match. For our submission, we re-implement the L2P approach described by Jiampojamarn et al. (2008) as faithfully as possible, and apply it unmodified to the transliteration shared task for the English-to-Hindi (Kumaran and Kellner, 2007) and English-to-Japanese Katakana1 tests. We interpret the problem of transliterating English named entities into Hindi or Japanese Katakana as a variant of the letter-to-phoneme (L2P) subtask of textto-speech processing. Therefore, we apply a re-implementation of a state-of-the-art, discriminative L2P system (Jiampojamarn et al., 2008) to the problem, without further modification. In doing so, we hope to provide a baseline for the NEWS 2009 Machine Transliteration Shared Task (Li et al., 2009), indicating how much can be achieved without transliteration-specific technology. This paper briefly summarizes the original work and our reimplementation. We also describe a bug in our submitted implementation, and provide updated results on the development and test sets. 1 2 Approach 2.1 Summary of L2P approach The core of the L2P transduction engine is the dynamic programming algorithm for monotone phrasal decoding (Zens and Ney, 2"
W09-3514,N06-1011,0,0.057759,"Missing"
W09-3514,N04-1033,0,0.0290842,"n et al., 2008) to the problem, without further modification. In doing so, we hope to provide a baseline for the NEWS 2009 Machine Transliteration Shared Task (Li et al., 2009), indicating how much can be achieved without transliteration-specific technology. This paper briefly summarizes the original work and our reimplementation. We also describe a bug in our submitted implementation, and provide updated results on the development and test sets. 1 2 Approach 2.1 Summary of L2P approach The core of the L2P transduction engine is the dynamic programming algorithm for monotone phrasal decoding (Zens and Ney, 2004). The main feature of this algorithm is its capability to transduce many consecutive characters with a single operation. This algorithm is used to conduct a search for a max-weight derivation according to a linear model with indicator features. A sample derivation is shown in Figure 1. There are two main categories of features: context and transition features, which follow the first two feature templates described by Jiampojamarn et al. (2008). Context features are centered around a transduction operation. These features include an indicator for the operation itself, which is then conjoined wi"
W09-3514,P08-1012,0,0.0139574,"ors of length up to n = 6 provided optimal development performance. 4 Development Development consisted of performing a parameter grid search over S and T for each language pair’s development set. All combinations of S = 0 . . . 4 and T = 0 . . . 7 were tested for each language pair. Based on these experiments, we selected (for the fixed version), values of S = 2, T = 6 for English-Hindi, and S = 4, T = 3 for EnglishKatakana. Second, we employed an alternate character aligner to create our training derivations. This aligner is similar to recent non-compositional phrasal word-alignment models (Zhang et al., 2008), limited so it can only produce monotone character alignments. The aligner creates substring alignments, without insertion or deletion operators. As such, an aligned transliteration pair also serves as a transliteration derivation. We employed a maximum substring length of 3. 5 The training data was heuristically cleaned after alignment. Any derivation found by the aligner that uses an operation occurring fewer than 3 times throughout the entire training set was eliminated. This reduced training set sizes to 8,511 pairs for English-Hindi and 20,306 pairs for EnglishKatakana. Results The resul"
W09-3514,W09-3501,0,\N,Missing
W09-3514,J98-4003,0,\N,Missing
W12-3125,N12-1047,1,0.88858,"Missing"
W12-3125,D08-1024,0,0.0773411,"Missing"
W12-3125,C10-2033,0,0.317379,"lin Cherry National Research Council colin.cherry@nrc-cnrc.gc.ca Robert C. Moore Google bobmoore@google.com Abstract decoding can be constrained by distortion limits or by mimicking the restrictions of inversion transduction grammars (Wu, 1997; Zens et al., 2004). The addition of a deterministic permutation parser can provide valuable hierarchical information to a phrase-based statistical machine translation (PBSMT) system. Permutation parsers have been used to implement hierarchical re-ordering models (Galley and Manning, 2008) and to enforce inversion transduction grammar (ITG) constraints (Feng et al., 2010). We present a number of theoretical results regarding the use of permutation parsers in PBSMT. In particular, we show that an existing ITG constraint (Zens et al., 2004) does not prevent all non-ITG permutations, and we demonstrate that the hierarchical reordering model can produce analyses during decoding that are inconsistent with analyses made during training. Experimentally, we verify the utility of hierarchical re-ordering, and compare several theoretically-motivated variants in terms of both translation quality and the syntactic complexity of their output. 1 Chris Quirk Microsoft Resear"
W12-3125,D08-1089,0,0.866851,"of hierarchical re-ordering, and compare several theoretically-motivated variants in terms of both translation quality and the syntactic complexity of their output. 1 Chris Quirk Microsoft Research chrisq@microsoft.com Recently, we have begun to see deterministic permutation parsers incorporated into phrase-based decoders. These efficient parsers analyze the sequence of phrases used to produce the target, and assemble them into a hierarchical translation history that can be used to inform re-ordering decisions. Thus far, they have been used to enable a hierarchical re-ordering model, or HRM (Galley and Manning, 2008), as well as an ITG constraint (Feng et al., 2010). We discuss each of these techniques in turn, and then explore the implications of ITG violations on hierarchical re-ordering. Introduction Despite the emergence of a number of syntax-based techniques, phrase-based statistical machine translation remains a competitive and very efficient translation paradigm (Galley and Manning, 2010). However, it lacks the syntactically-informed movement models and constraints that are provided implicitly by working with synchronous grammars. Therefore, re-ordering must be modeled and constrained explicitly. M"
W12-3125,N10-1140,0,0.04572,"to produce the target, and assemble them into a hierarchical translation history that can be used to inform re-ordering decisions. Thus far, they have been used to enable a hierarchical re-ordering model, or HRM (Galley and Manning, 2008), as well as an ITG constraint (Feng et al., 2010). We discuss each of these techniques in turn, and then explore the implications of ITG violations on hierarchical re-ordering. Introduction Despite the emergence of a number of syntax-based techniques, phrase-based statistical machine translation remains a competitive and very efficient translation paradigm (Galley and Manning, 2010). However, it lacks the syntactically-informed movement models and constraints that are provided implicitly by working with synchronous grammars. Therefore, re-ordering must be modeled and constrained explicitly. Movement can be modeled with a distortion penalty or lexicalized re-ordering probabilities (Koehn et al., 2003; Koehn et al., 2007), while We present one experimental and four theoretical contributions. Examining the HRM alone, we present an improved algorithm for extracting HRM statistics, reducing the complexity of Galley and Manning’s solution from O(n4 ) to O(n2 ). Examining ITG c"
W12-3125,J99-4005,0,0.485608,"Missing"
W12-3125,N03-1017,0,0.0695883,"turn, and then explore the implications of ITG violations on hierarchical re-ordering. Introduction Despite the emergence of a number of syntax-based techniques, phrase-based statistical machine translation remains a competitive and very efficient translation paradigm (Galley and Manning, 2010). However, it lacks the syntactically-informed movement models and constraints that are provided implicitly by working with synchronous grammars. Therefore, re-ordering must be modeled and constrained explicitly. Movement can be modeled with a distortion penalty or lexicalized re-ordering probabilities (Koehn et al., 2003; Koehn et al., 2007), while We present one experimental and four theoretical contributions. Examining the HRM alone, we present an improved algorithm for extracting HRM statistics, reducing the complexity of Galley and Manning’s solution from O(n4 ) to O(n2 ). Examining ITG constraints alone, we demonstrate that the three-stack constraint of Feng et al. can be reduced to one augmented stack, and we show that another phrase-based ITG constraint (Zens et al., 2004) actually allows some ITG violations to pass. Finally, we show that in the presence of ITG violations, the original HRM can fail to"
W12-3125,P07-2045,0,0.0669013,"re the implications of ITG violations on hierarchical re-ordering. Introduction Despite the emergence of a number of syntax-based techniques, phrase-based statistical machine translation remains a competitive and very efficient translation paradigm (Galley and Manning, 2010). However, it lacks the syntactically-informed movement models and constraints that are provided implicitly by working with synchronous grammars. Therefore, re-ordering must be modeled and constrained explicitly. Movement can be modeled with a distortion penalty or lexicalized re-ordering probabilities (Koehn et al., 2003; Koehn et al., 2007), while We present one experimental and four theoretical contributions. Examining the HRM alone, we present an improved algorithm for extracting HRM statistics, reducing the complexity of Galley and Manning’s solution from O(n4 ) to O(n2 ). Examining ITG constraints alone, we demonstrate that the three-stack constraint of Feng et al. can be reduced to one augmented stack, and we show that another phrase-based ITG constraint (Zens et al., 2004) actually allows some ITG violations to pass. Finally, we show that in the presence of ITG violations, the original HRM can fail to produce orientations"
W12-3125,2007.mtsummit-papers.43,1,0.945694,"Missing"
W12-3125,2009.mtsummit-papers.10,0,0.327246,"Missing"
W12-3125,J04-4002,0,0.132136,"training data. Galley and Manning (2008) propose an algorithm that begins by run1 This would require a second, right-to-left decoding pass. Galley and Manning (2008) present an under-specified approximation that is consistent with what we present here. 2 202 2 Prev Cov / Approx Top 4 5 6 7 Figure 2: Illustration of the coverage-vector stack approximation, as applied to right-to-left HRM orientation. èS Source Op S S R S S R R S R çM Phrase èM çS Target Figure 3: Relevant corners in HRM extraction. → indicates left-to-right orientation, and ← right-to-left. ning standard phrase extraction (Och and Ney, 2004) without a phrase-length limit, noting the corners of each phrase found. Next, the left-to-right and rightto-left orientation for each phrase of interest (those within the phrase-length limit) can be determined by checking to see if any corners noted in the previous step are adjacent, as shown in Figure 3. 2.1 Efficient Extraction of HRM statistics The time complexity of phrase extraction is bounded by the number of phrases to be extracted, which is determined by the sparsity of the input word alignment. Without a limit on phrase length, a sentence pair with n words in each language can have a"
W12-3125,P02-1040,0,0.0838935,"Missing"
W12-3125,N04-4026,0,0.4744,"ation, including an approximate HRM that requires no permutation parser, and compare them experimentally. The variants perform similarly to the original in terms of BLEU score, but differently in terms of how they permute the source sentence. 200 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 200–209, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics 2 Hierarchical Re-ordering Hierarchical re-ordering models (HRMs) for phrasebased SMT are an extension of lexicalized reordering models (LRMs), so we begin by briefly reviewing the LRM (Tillmann, 2004; Koehn et al., 2007). The goal of an LRM is to characterize how a phrase-pair tends to be placed with respect to the block that immediately precedes it. Both the LRM and the HRM track orientations traveling through the target from left-to-right as well as right-to-left. For the sake of brevity and clarity, we discuss only the left-to-right direction except when stated otherwise. Re-ordering is typically categorized into three orientations, which are determined by examining two sequential blocks [si−1 , ti−1 ] and [si , ti ]: • Monotone Adjacent (M): ti−1 = si • Swap Adjacent (S): ti = si−1 •"
W12-3125,J97-3002,0,0.703132,"Missing"
W12-3125,P03-1019,0,0.285671,"Missing"
W12-3125,C04-1030,0,0.649146,"y mimicking the restrictions of inversion transduction grammars (Wu, 1997; Zens et al., 2004). The addition of a deterministic permutation parser can provide valuable hierarchical information to a phrase-based statistical machine translation (PBSMT) system. Permutation parsers have been used to implement hierarchical re-ordering models (Galley and Manning, 2008) and to enforce inversion transduction grammar (ITG) constraints (Feng et al., 2010). We present a number of theoretical results regarding the use of permutation parsers in PBSMT. In particular, we show that an existing ITG constraint (Zens et al., 2004) does not prevent all non-ITG permutations, and we demonstrate that the hierarchical reordering model can produce analyses during decoding that are inconsistent with analyses made during training. Experimentally, we verify the utility of hierarchical re-ordering, and compare several theoretically-motivated variants in terms of both translation quality and the syntactic complexity of their output. 1 Chris Quirk Microsoft Research chrisq@microsoft.com Recently, we have begun to see deterministic permutation parsers incorporated into phrase-based decoders. These efficient parsers analyze the sequ"
W12-3125,W07-0404,0,0.154947,"Missing"
W12-3125,N06-1033,0,0.0624761,"Missing"
W12-3125,D11-1003,0,\N,Missing
W14-3346,W11-2103,0,0.0229985,"iques for sentence-level BLEU. Three of them are first proposed in this paper, and they correlate better with human judgments on the sentence-level than other smoothing techniques. Moreover, we also compare the performance of using the 7 smoothing techniques in statistical machine translation tuning. 1 Introduction Since its invention, BLEU (Papineni et al., 2002) has been the most widely used metric for both machine translation (MT) evaluation and tuning. Many other metrics correlate better with human judgments of translation quality than BLEU, as shown in recent WMT Evaluation Task reports (Callison-Burch et al., 2011; Callison-Burch et al., 2012). However, BLEU remains the de facto standard evaluation and tuning metric. This is probably due to the following facts: 1. BLEU is language independent (except for word segmentation decisions). 2. BLEU can be computed quickly. This is important when choosing a tuning metric. 3. BLEU seems to be the best tuning metric from a quality point of view - i.e., models trained using BLEU obtain the highest scores from humans and even from other metrics (Cer et al., 2010). One of the main criticisms of BLEU is that it has a poor correlation with human judgments on the sent"
W14-3346,W12-3102,0,0.0550855,"Missing"
W14-3346,2011.mtsummit-papers.30,1,0.719107,"We test on the evaluation sets from NIST 2006 and 2008. For the Arabic-to-English task, we use the evaluation sets from NIST 2006, 2008, and 2009 as our dev set and two test sets, respectively. Table 4 summarizes the training, dev and test sets. Experiments were carried out with an in-house, state-of-the-art phrase-based system. Each corpus was word-aligned using IBM2, HMM, and IBM4 models, and the phrase table was the union of phrase pairs extracted from these separate alignments, with a length limit of 7. The translation model (TM) was smoothed in both directions with Kneser-Ney smoothing (Chen et al., 2011). We use the hierarchical lexicalized reordering model (RM) (Galley and Manning, 2008), with a distortion limit of 7. Other features include lexical weighting in both directions, word count, a distance-based RM, a 4-gram LM trained on the target side of the parallel data, and a 6-gram En3 We use K = 100 in our experiments. 365 http://www.nist.gov/itl/iad/mig/openmt12.cfm 0 1 2 3 4 5 6 7 Tune 27.6 27.6 27.5 27.6 27.6 27.6 27.5 27.6 std 0.1 0.0 0.1 0.1 0.1 0.1 0.1 0.1 MT06 35.6 35.7 35.8 35.8 35.7 35.5 35.7 35.6 std 0.1 0.1 0.1 0.1 0.2 0.1 0.1 0.1 MT08 29.0 29.1 29.1 29.1 29.1 28.9 29.0 29.0 std"
W14-3346,N12-1047,1,0.883272,"l BLEU have been proposed (Lin and Och, 2004; Gao and He, 2013). In this paper, we systematically compare 7 smoothing techniques for sentence-level BLEU. Three of them are first proposed in this paper, and they correlate better with human judgments on the sentence-level than other smoothing techniques on the WMT metrics task. Moreover, we compare the performance of using the 7 smoothing techniques in statistical machine translation tuning on NIST Chinese-to-English and Arabic-to-English tasks. We show that when tuning optimizes the expected sum of these sentence-level metrics (as advocated by Cherry and Foster (2012) and Gao and He (2013) among others), all of these metrics perform similarly in terms of their ability to produce strong BLEU scores on a held-out test set. 2 BLEU and smoothing 2.1 BLEU Suppose we have a translation T and its reference R, BLEU is computed with precision P (N, T, R) and brevity penalty BP(T,R): BLEU(N, T, R) = P (N, T, R) × BP(T, R) (1) where P (N, T, R) is the geometric mean of ngram precisions: P (N, T, R) = N Y ! N1 pn n=1 362 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 362–367, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association fo"
W14-3346,P11-2031,0,0.0127436,"also conducted a set of experiments with a much larger feature set. This system used only GIZA++ for word alignment, increased the distortion limit from 7 to 9, and is trained on a highquality subset of the parallel corpora used earlier. Most importantly, it includes the full set of sparse phrase-pair features used by both Hopkins and May (2011) and Cherry and Foster (2012), which results in nearly 7,000 features. Our evaluation metric is the original IBM BLEU, which performs case-insensitive matching of n-grams up to n = 4. We perform random replications of parameter tuning, as suggested by Clark et al. (2011). Each replication uses a different random seed to determine the order in which SGD visits tuning sentences. We test for significance using the MultEval tool,4 which uses a stratified approximate randomization test to account for multiple replications. We report results averaged across replications as well as standard deviations, which indicate optimizer stability. Results for the small feature set are shown in Tables 5 and 6. All 7 smoothing techniques, as well as the no smoothing baseline, all yield very similar results on both Chinese and Arabic tasks. We did not find any two results to be"
W14-3346,D08-1089,0,0.0170121,"ish task, we use the evaluation sets from NIST 2006, 2008, and 2009 as our dev set and two test sets, respectively. Table 4 summarizes the training, dev and test sets. Experiments were carried out with an in-house, state-of-the-art phrase-based system. Each corpus was word-aligned using IBM2, HMM, and IBM4 models, and the phrase table was the union of phrase pairs extracted from these separate alignments, with a length limit of 7. The translation model (TM) was smoothed in both directions with Kneser-Ney smoothing (Chen et al., 2011). We use the hierarchical lexicalized reordering model (RM) (Galley and Manning, 2008), with a distortion limit of 7. Other features include lexical weighting in both directions, word count, a distance-based RM, a 4-gram LM trained on the target side of the parallel data, and a 6-gram En3 We use K = 100 in our experiments. 365 http://www.nist.gov/itl/iad/mig/openmt12.cfm 0 1 2 3 4 5 6 7 Tune 27.6 27.6 27.5 27.6 27.6 27.6 27.5 27.6 std 0.1 0.0 0.1 0.1 0.1 0.1 0.1 0.1 MT06 35.6 35.7 35.8 35.8 35.7 35.5 35.7 35.6 std 0.1 0.1 0.1 0.1 0.2 0.1 0.1 0.1 MT08 29.0 29.1 29.1 29.1 29.1 28.9 29.0 29.0 std 0.2 0.1 0.1 0.1 0.2 0.2 0.2 0.1 0 1 2 3 4 5 6 7 Table 5: Chinese-to-English Results f"
W14-3346,N13-1048,0,0.202148,"g metric from a quality point of view - i.e., models trained using BLEU obtain the highest scores from humans and even from other metrics (Cer et al., 2010). One of the main criticisms of BLEU is that it has a poor correlation with human judgments on the sentence-level. Because it computes a geometric mean of n-gram precisions, if a higher order n-gram precision (eg. n = 4) of a sentence is 0, then the BLEU score of the entire sentence is 0, no matter how many 1-grams or 2-grams are matched. Therefore, several smoothing techniques for sentence-level BLEU have been proposed (Lin and Och, 2004; Gao and He, 2013). In this paper, we systematically compare 7 smoothing techniques for sentence-level BLEU. Three of them are first proposed in this paper, and they correlate better with human judgments on the sentence-level than other smoothing techniques on the WMT metrics task. Moreover, we compare the performance of using the 7 smoothing techniques in statistical machine translation tuning on NIST Chinese-to-English and Arabic-to-English tasks. We show that when tuning optimizes the expected sum of these sentence-level metrics (as advocated by Cherry and Foster (2012) and Gao and He (2013) among others), a"
W14-3346,D11-1125,0,0.0319222,"1 0.1 MT08 29.0 29.1 29.1 29.1 29.1 28.9 29.0 29.0 std 0.2 0.1 0.1 0.1 0.2 0.2 0.2 0.1 0 1 2 3 4 5 6 7 Table 5: Chinese-to-English Results for the small feature set tuning task. Results are averaged across 5 replications; std is the standard deviation. glish Gigaword LM. We also conducted a set of experiments with a much larger feature set. This system used only GIZA++ for word alignment, increased the distortion limit from 7 to 9, and is trained on a highquality subset of the parallel corpora used earlier. Most importantly, it includes the full set of sparse phrase-pair features used by both Hopkins and May (2011) and Cherry and Foster (2012), which results in nearly 7,000 features. Our evaluation metric is the original IBM BLEU, which performs case-insensitive matching of n-grams up to n = 4. We perform random replications of parameter tuning, as suggested by Clark et al. (2011). Each replication uses a different random seed to determine the order in which SGD visits tuning sentences. We test for significance using the MultEval tool,4 which uses a stratified approximate randomization test to account for multiple replications. We report results averaged across replications as well as standard deviation"
W14-3346,P04-1077,0,0.55547,"o be the best tuning metric from a quality point of view - i.e., models trained using BLEU obtain the highest scores from humans and even from other metrics (Cer et al., 2010). One of the main criticisms of BLEU is that it has a poor correlation with human judgments on the sentence-level. Because it computes a geometric mean of n-gram precisions, if a higher order n-gram precision (eg. n = 4) of a sentence is 0, then the BLEU score of the entire sentence is 0, no matter how many 1-grams or 2-grams are matched. Therefore, several smoothing techniques for sentence-level BLEU have been proposed (Lin and Och, 2004; Gao and He, 2013). In this paper, we systematically compare 7 smoothing techniques for sentence-level BLEU. Three of them are first proposed in this paper, and they correlate better with human judgments on the sentence-level than other smoothing techniques on the WMT metrics task. Moreover, we compare the performance of using the 7 smoothing techniques in statistical machine translation tuning on NIST Chinese-to-English and Arabic-to-English tasks. We show that when tuning optimizes the expected sum of these sentence-level metrics (as advocated by Cherry and Foster (2012) and Gao and He (201"
W14-3346,W13-2202,0,0.06674,"task as evaluation metrics, then they were compared as metrics for tuning SMT systems to maximize the sum of expected sentence-level BLEU scores. 3.1 Evaluation task We first compare the correlations with human judgment for the 7 smoothing techniques on WMT data; the development set (dev) is the WMT 2008 all-to-English data; the test sets are the WMT 2012 and WMT 2013 all-to-English, and English-to-all submissions. The languages “all” (“xx” in Table 1) include French, Spanish, German, Czech and Russian. Table 1 summarizes the dev/test set statistics. Following WMT 2013’s metric task (Mach´acˇ ek and Bojar, 2013), for the segment level, we use Kendall’s rank correlation coefficient τ to measure the correlation with human judgment: #concordant-pairs − #discordant-pairs #concordant-pairs + #discordant-pairs (12) We extract all pairwise comparisons where one system’s translation of a particular segment was judged to be better than the other system’s translation, i.e., we removed all tied human judgments for a particular segment. If two translations for a particular segment are assigned the same BLEU score, then the #concordant-pairs and #discordant-pairs both get a half count. In this way, we can keep th"
W14-3346,P02-1040,0,0.108856,"ic. However, because BLEU computes a geometric mean of n-gram precisions, it often correlates poorly with human judgment on the sentence-level. Therefore, several smoothing techniques have been proposed. This paper systematically compares 7 smoothing techniques for sentence-level BLEU. Three of them are first proposed in this paper, and they correlate better with human judgments on the sentence-level than other smoothing techniques. Moreover, we also compare the performance of using the 7 smoothing techniques in statistical machine translation tuning. 1 Introduction Since its invention, BLEU (Papineni et al., 2002) has been the most widely used metric for both machine translation (MT) evaluation and tuning. Many other metrics correlate better with human judgments of translation quality than BLEU, as shown in recent WMT Evaluation Task reports (Callison-Burch et al., 2011; Callison-Burch et al., 2012). However, BLEU remains the de facto standard evaluation and tuning metric. This is probably due to the following facts: 1. BLEU is language independent (except for word segmentation decisions). 2. BLEU can be computed quickly. This is important when choosing a tuning metric. 3. BLEU seems to be the best tun"
W14-3346,N10-1080,0,\N,Missing
W15-1011,P08-2039,0,0.152648,"aluable language portability, but which generally fall behind supervised methods when labeled data is available. There is a large body of work studying the best form of segmentation when translating from a morphologically complex source language (Sadat and Habash, 2006; Stallard et al., 2012), where the segmentation can be used as a simple preprocessing step, or to create an input lattice (Dyer et al., 2008). Recently, there has been a growing interest in segmentation on the target side (Oflazer and Durgar El-Kahlout, 2007), which introduces a question of how to perform proper desegmentation (Badr et al., 2008). El Kholy and Habash (2012) have conducted a thorough exploration of the various segmentation and desegmentation options for English to Arabic translation, and we follow their work when designing our test bed. Method Unsegmented Desegment before: Alignment model Lexical weights Language model Tuning Flexible boundaries? Never segment Word Word Word Word No Alignment Deseg. Phrase extraction Morph Word Word Word No Phrase Table Deseg. Decoding Morph Morph Word Word No One-best Deseg. Evaluation Morph Morph Morph Morph Yes Lattice Deseg. Evaluation Morph Morph Morph + Word Morph then Word Yes T"
W15-1011,J93-2003,0,0.0425605,"s the Arabic target language before training begins, and the decoder’s output is generated in segmented form. As a post-processing step, the one-best output is desegmented using a mapping table and desegmentation rules. All of the component models used during decoding are based on morphemes instead of words. The segmented models are intended to help alleviate data sparsity and improve token correspondence. Unlike the unsegmented system, this system requires a desegmentation step, which can produce morphologically incorrect words. 3.2 Alignment Desegmentation Our unsupervised alignment models (Brown et al., 1993; Och and Ney, 2003) are sensitive both to poor word-to-word correspondence and to data sparsity issues. They are also at the very start of the SMT pipeline; they impact nearly all other downstream models. Therefore, it would be reasonable to suspect that the primary benefit of segmentation could come from improved word alignment. Alignment desegmentation allows us to test this theory by desegmenting immediately after alignment. More specifically, we segment the target side as pre-processing. After word alignment, we replace the segmented Arabic training data with its unsegmented form. Note th"
W15-1011,N12-1047,1,0.818464,"by converting different forms of Alif and Ya to bare Alif and dotless Ya. In order to generate the desegmentation table, we analyze the MADA segmentations from the Arabic side of the parallel training data to collect mappings from morpheme sequences to surface forms. 4.2 penalties. The decoder uses Moses’ default search parameters, except that the maximum phrase length is set to 8. The decoder’s log-linear model is tuned with MERT (Och, 2003). Following Salameh et al. (2014), the tuning of the re-ranking models for lattice desegmentation is performed using a lattice variant of hope-fear MIRA (Cherry and Foster, 2012); lattices are pruned to a density of 50 edges per word before re-ranking. We evaluate our system using BLEU (Papineni et al., 2002). Decoder Integration Lattice Desegmentation performs best overall, which is not entirely surprising, as it has access to all of the information present in the other systems. Notably, it outperforms Phrase Table Desegmentation; this is the first time to our knowledge that the two have been compared directly. The main disadvantage of Lattice Deseg, which is not present in Alignment and Phrase Table Deseg, is the lack of decoder integration of its unsegmented view o"
W15-1011,P08-1115,0,0.0351294,"y on adaptation to dialects (Habash et al., 2013; Monroe et al., 2014). There are also a host of unsupervised techniques (Creutz and Lagus, 2005; Lee et al., 2011; Sirts and Goldwater, 2013), which provide valuable language portability, but which generally fall behind supervised methods when labeled data is available. There is a large body of work studying the best form of segmentation when translating from a morphologically complex source language (Sadat and Habash, 2006; Stallard et al., 2012), where the segmentation can be used as a simple preprocessing step, or to create an input lattice (Dyer et al., 2008). Recently, there has been a growing interest in segmentation on the target side (Oflazer and Durgar El-Kahlout, 2007), which introduces a question of how to perform proper desegmentation (Badr et al., 2008). El Kholy and Habash (2012) have conducted a thorough exploration of the various segmentation and desegmentation options for English to Arabic translation, and we follow their work when designing our test bed. Method Unsegmented Desegment before: Alignment model Lexical weights Language model Tuning Flexible boundaries? Never segment Word Word Word Word No Alignment Deseg. Phrase extractio"
W15-1011,P12-1016,0,0.0174589,"crease BLEU score, they also reduce the system’s use of morphological affixes to well below that of a human. Finally, we present the first direct comparison between phrase table desegmentation (Luong et al., 2010) and lattice desegmentation (Salameh et al., 2014). 2 Background Our work builds on earlier studies of automatic morphological segmentation and its impact on SMT. There are many ways to segment syntactically relevant affixes from stems. Supervised techniques may either pass through an intermediate morphological analysis (Habash et al., 2009), or directly segment the character stream (Green and DeNero, 2012); recent work on supervised Arabic segmentation focuses primarily on adaptation to dialects (Habash et al., 2013; Monroe et al., 2014). There are also a host of unsupervised techniques (Creutz and Lagus, 2005; Lee et al., 2011; Sirts and Goldwater, 2013), which provide valuable language portability, but which generally fall behind supervised methods when labeled data is available. There is a large body of work studying the best form of segmentation when translating from a morphologically complex source language (Sadat and Habash, 2006; Stallard et al., 2012), where the segmentation can be used"
W15-1011,N13-1044,0,0.0132258,", we present the first direct comparison between phrase table desegmentation (Luong et al., 2010) and lattice desegmentation (Salameh et al., 2014). 2 Background Our work builds on earlier studies of automatic morphological segmentation and its impact on SMT. There are many ways to segment syntactically relevant affixes from stems. Supervised techniques may either pass through an intermediate morphological analysis (Habash et al., 2009), or directly segment the character stream (Green and DeNero, 2012); recent work on supervised Arabic segmentation focuses primarily on adaptation to dialects (Habash et al., 2013; Monroe et al., 2014). There are also a host of unsupervised techniques (Creutz and Lagus, 2005; Lee et al., 2011; Sirts and Goldwater, 2013), which provide valuable language portability, but which generally fall behind supervised methods when labeled data is available. There is a large body of work studying the best form of segmentation when translating from a morphologically complex source language (Sadat and Habash, 2006; Stallard et al., 2012), where the segmentation can be used as a simple preprocessing step, or to create an input lattice (Dyer et al., 2008). Recently, there has been a g"
W15-1011,D07-1091,0,0.0234547,"ethods to combine the scores across tables. In addition, they incorporate both segmented and unsegmented language models, which is a difference that we address in the next section. 68 3.4 Segmented LM Scoring in Desegmented Models Both alignment desegmentation and phrase table desegmentation rely on an unsegmented language model, as they naturally decode directly into a desegmented target language. We experiment with augmenting both of these systems with an extra feature: a segmented language model. For each Arabic target word, we add its segmented form to the phrase table as an extra factor (Koehn and Hoang, 2007). We insert this factor after phrase extraction, so it has no impact on alignment or the calculation of translation model scores. The factor merely gives us access to the segmented morphemes during decoding. The decoder uses this factor to apply a segmented language model during each hypothesis extension. Although the segmented language model spans a shorter context, its scores benefit from the reduced data sparsity that comes from modeling morphemes. In particular, it can unveil whether attaching two hypotheses is grammatical. For example, the unsegmented language model score for the consecut"
W15-1011,P07-2045,0,0.0842681,"Word Yes Table 1: Desegmentation scenarios and their effect on the components of a typical SMT system. 3 Methods When translating into a segmented target language, such as Arabic, the segmentation will need to eventually be reversed for the output to be readable. The key insight driving our experiments is that by varying the point in the SMT pipeline where this reversal occurs, we can alter which models are based on morphemes and which are based on words, and thereby determine which components most benefit from segmentation. We assume a phrase-based SMT architecture similar to that of Moses (Koehn et al., 2007), but most of our observations hold for hierarchical and tree-based models. In all of our approaches, we desegment using a mapping table that counts the segmentations performed on the target side of our training data. The table uses counts of wordsegmentation pairs to map each morpheme sequence back to its most likely unsegmented word form. We back off to manually crafted rules in cases where the segmented form does not exist in the mapping table (El Kholy and Habash, 2012). Table 1 summarizes the effect of the desegmentation point on the components of a typical SMT system, indicating which co"
W15-1011,W11-0301,0,0.0315674,"ntation (Salameh et al., 2014). 2 Background Our work builds on earlier studies of automatic morphological segmentation and its impact on SMT. There are many ways to segment syntactically relevant affixes from stems. Supervised techniques may either pass through an intermediate morphological analysis (Habash et al., 2009), or directly segment the character stream (Green and DeNero, 2012); recent work on supervised Arabic segmentation focuses primarily on adaptation to dialects (Habash et al., 2013; Monroe et al., 2014). There are also a host of unsupervised techniques (Creutz and Lagus, 2005; Lee et al., 2011; Sirts and Goldwater, 2013), which provide valuable language portability, but which generally fall behind supervised methods when labeled data is available. There is a large body of work studying the best form of segmentation when translating from a morphologically complex source language (Sadat and Habash, 2006; Stallard et al., 2012), where the segmentation can be used as a simple preprocessing step, or to create an input lattice (Dyer et al., 2008). Recently, there has been a growing interest in segmentation on the target side (Oflazer and Durgar El-Kahlout, 2007), which introduces a quest"
W15-1011,D10-1015,0,0.372773,"Missing"
W15-1011,P14-2034,0,0.0168156,"t direct comparison between phrase table desegmentation (Luong et al., 2010) and lattice desegmentation (Salameh et al., 2014). 2 Background Our work builds on earlier studies of automatic morphological segmentation and its impact on SMT. There are many ways to segment syntactically relevant affixes from stems. Supervised techniques may either pass through an intermediate morphological analysis (Habash et al., 2009), or directly segment the character stream (Green and DeNero, 2012); recent work on supervised Arabic segmentation focuses primarily on adaptation to dialects (Habash et al., 2013; Monroe et al., 2014). There are also a host of unsupervised techniques (Creutz and Lagus, 2005; Lee et al., 2011; Sirts and Goldwater, 2013), which provide valuable language portability, but which generally fall behind supervised methods when labeled data is available. There is a large body of work studying the best form of segmentation when translating from a morphologically complex source language (Sadat and Habash, 2006; Stallard et al., 2012), where the segmentation can be used as a simple preprocessing step, or to create an input lattice (Dyer et al., 2008). Recently, there has been a growing interest in seg"
W15-1011,J03-1002,0,0.0434558,"language before training begins, and the decoder’s output is generated in segmented form. As a post-processing step, the one-best output is desegmented using a mapping table and desegmentation rules. All of the component models used during decoding are based on morphemes instead of words. The segmented models are intended to help alleviate data sparsity and improve token correspondence. Unlike the unsegmented system, this system requires a desegmentation step, which can produce morphologically incorrect words. 3.2 Alignment Desegmentation Our unsupervised alignment models (Brown et al., 1993; Och and Ney, 2003) are sensitive both to poor word-to-word correspondence and to data sparsity issues. They are also at the very start of the SMT pipeline; they impact nearly all other downstream models. Therefore, it would be reasonable to suspect that the primary benefit of segmentation could come from improved word alignment. Alignment desegmentation allows us to test this theory by desegmenting immediately after alignment. More specifically, we segment the target side as pre-processing. After word alignment, we replace the segmented Arabic training data with its unsegmented form. Note that this desegmentati"
W15-1011,P03-1021,0,0.106912,"Arabic Treebank (PATB) segmentation scheme as recommended by El Kholy and Habash (2012). For both segmented and unsegmented Arabic, we further normalize the script by converting different forms of Alif and Ya to bare Alif and dotless Ya. In order to generate the desegmentation table, we analyze the MADA segmentations from the Arabic side of the parallel training data to collect mappings from morpheme sequences to surface forms. 4.2 penalties. The decoder uses Moses’ default search parameters, except that the maximum phrase length is set to 8. The decoder’s log-linear model is tuned with MERT (Och, 2003). Following Salameh et al. (2014), the tuning of the re-ranking models for lattice desegmentation is performed using a lattice variant of hope-fear MIRA (Cherry and Foster, 2012); lattices are pruned to a density of 50 edges per word before re-ranking. We evaluate our system using BLEU (Papineni et al., 2002). Decoder Integration Lattice Desegmentation performs best overall, which is not entirely surprising, as it has access to all of the information present in the other systems. Notably, it outperforms Phrase Table Desegmentation; this is the first time to our knowledge that the two have been"
W15-1011,W07-0704,0,0.406451,"Missing"
W15-1011,P02-1040,0,0.0922354,"e MADA segmentations from the Arabic side of the parallel training data to collect mappings from morpheme sequences to surface forms. 4.2 penalties. The decoder uses Moses’ default search parameters, except that the maximum phrase length is set to 8. The decoder’s log-linear model is tuned with MERT (Och, 2003). Following Salameh et al. (2014), the tuning of the re-ranking models for lattice desegmentation is performed using a lattice variant of hope-fear MIRA (Cherry and Foster, 2012); lattices are pruned to a density of 50 edges per word before re-ranking. We evaluate our system using BLEU (Papineni et al., 2002). Decoder Integration Lattice Desegmentation performs best overall, which is not entirely surprising, as it has access to all of the information present in the other systems. Notably, it outperforms Phrase Table Desegmentation; this is the first time to our knowledge that the two have been compared directly. The main disadvantage of Lattice Deseg, which is not present in Alignment and Phrase Table Deseg, is the lack of decoder integration of its unsegmented view of the target; instead, it is handled by re-ranking a lattice in post-processing. In fact, the top two systems, Lattice Deseg and 1-B"
W15-1011,P06-1001,0,0.0286995,"h et al., 2009), or directly segment the character stream (Green and DeNero, 2012); recent work on supervised Arabic segmentation focuses primarily on adaptation to dialects (Habash et al., 2013; Monroe et al., 2014). There are also a host of unsupervised techniques (Creutz and Lagus, 2005; Lee et al., 2011; Sirts and Goldwater, 2013), which provide valuable language portability, but which generally fall behind supervised methods when labeled data is available. There is a large body of work studying the best form of segmentation when translating from a morphologically complex source language (Sadat and Habash, 2006; Stallard et al., 2012), where the segmentation can be used as a simple preprocessing step, or to create an input lattice (Dyer et al., 2008). Recently, there has been a growing interest in segmentation on the target side (Oflazer and Durgar El-Kahlout, 2007), which introduces a question of how to perform proper desegmentation (Badr et al., 2008). El Kholy and Habash (2012) have conducted a thorough exploration of the various segmentation and desegmentation options for English to Arabic translation, and we follow their work when designing our test bed. Method Unsegmented Desegment before: Ali"
W15-1011,P14-1010,1,0.912641,"rase table, allowing morphemes to reduce sparsity while words expand context, and eliminating the need for a separate desegmentation step. Their word-boundary-aware morpheme-level phrase extraction technique restricts phrase boundaries so that no target phrase can begin with a suffix or end with a prefix. This allows them to desegment each target phrase independently, enabling the use of both word- and morpheme-level language models during decoding. However, this phrase-table desegmentation approach lacks the expressive power that comes from translating morphemes independently. More recently, Salameh et al. (2014) propose a lattice desegmentation approach, which comes close to combining all the advantages of word and morpheme views. By desegmenting a lattice that compactly represents many translation options, and rescoring it with a word-level language model, they avoid restricting the phrase table. However, by delaying desegmentation until rescoring, the approach loses Luong et al. (2010)’s advantage of full decoder integration. In this paper, we present an experimental study of English-to-Arabic translation that is designed to better understand the impact of various trade-offs when translating into a"
W15-1011,Q13-1021,0,0.0258496,"t al., 2014). 2 Background Our work builds on earlier studies of automatic morphological segmentation and its impact on SMT. There are many ways to segment syntactically relevant affixes from stems. Supervised techniques may either pass through an intermediate morphological analysis (Habash et al., 2009), or directly segment the character stream (Green and DeNero, 2012); recent work on supervised Arabic segmentation focuses primarily on adaptation to dialects (Habash et al., 2013; Monroe et al., 2014). There are also a host of unsupervised techniques (Creutz and Lagus, 2005; Lee et al., 2011; Sirts and Goldwater, 2013), which provide valuable language portability, but which generally fall behind supervised methods when labeled data is available. There is a large body of work studying the best form of segmentation when translating from a morphologically complex source language (Sadat and Habash, 2006; Stallard et al., 2012), where the segmentation can be used as a simple preprocessing step, or to create an input lattice (Dyer et al., 2008). Recently, there has been a growing interest in segmentation on the target side (Oflazer and Durgar El-Kahlout, 2007), which introduces a question of how to perform proper"
W15-1011,P12-2063,0,0.0157824,"ctly segment the character stream (Green and DeNero, 2012); recent work on supervised Arabic segmentation focuses primarily on adaptation to dialects (Habash et al., 2013; Monroe et al., 2014). There are also a host of unsupervised techniques (Creutz and Lagus, 2005; Lee et al., 2011; Sirts and Goldwater, 2013), which provide valuable language portability, but which generally fall behind supervised methods when labeled data is available. There is a large body of work studying the best form of segmentation when translating from a morphologically complex source language (Sadat and Habash, 2006; Stallard et al., 2012), where the segmentation can be used as a simple preprocessing step, or to create an input lattice (Dyer et al., 2008). Recently, there has been a growing interest in segmentation on the target side (Oflazer and Durgar El-Kahlout, 2007), which introduces a question of how to perform proper desegmentation (Badr et al., 2008). El Kholy and Habash (2012) have conducted a thorough exploration of the various segmentation and desegmentation options for English to Arabic translation, and we follow their work when designing our test bed. Method Unsegmented Desegment before: Alignment model Lexical wei"
W15-1011,D08-1076,0,\N,Missing
W15-1518,W13-3520,0,0.0233018,"xity. In this paper, we replicate their syntactic experiments on four languages that are more morphologically complex than English: Dutch, French, German, and Spanish. 2 Replication Experiments In order to to validate our methodology, we first replicate the results of Mikolov et al. (2013b) on English syntactic analogies. 2.1 Training Corpus for Word Vectors The vectors of Mikolov et al. (2013b) were trained on 320M tokens of broadcast news data, as described by Mikolov et al. (2011). Since we have no access to this data, we instead train English vectors on a corpus from the Polyglot project (Al-Rfou et al., 2013), which contains tokenized Wikipedia dumps intended for the training of vector-space models. For comparison with the results of Mikolov et al. (2013b), we limit the data to the first 320M lowercased tokens of the corpus. 129 Proceedings of NAACL-HLT 2015, pages 129–134, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics Mikolov et al. (2013b) obtain their best results with vectors of size 1600 that combine several models, but do not elaborate how this composite model was constructed. Instead, we take as a point of reference their second-best model, which"
W15-1518,N13-1138,0,0.0331844,"gy for the other languages. 3.2 Test Sets In order to make results between multiple languages comparable, we made several changes to the construction of syntactic analogy questions. We follow the methodology of Mikolov et al. (2013b) in limiting analogy questions to the 100 most frequent verbs or nouns. The frequencies are obtained from corpora tagged by T REE TAGGER (Schmid, 1994). We identify inflections using manually constructed inflection tables from several sources. Spanish and German verbal inflections, as well as German nominal inflections, are from a Wiktionary data set introduced by Durrett and DeNero (2013).4 Dutch verbal inflections and English verbal and nominal inflections are from the CELEX database (Baayen et al., 1995). French verbal inflections are from Verbiste, an online French conjugation dictionary.5 Whereas Mikolov et al. create analogies from various inflectional forms, we require the analogies to always include the base dictionary form: the infinitive for verbs, and the nominative singular for nouns. In other words, all analogies are limited to 4 We exclude Finnish because of its high morphological complexity and the small size of the corresponding Polyglot corpus. 5 http://perso.b"
W15-1518,N13-1090,0,0.683358,"treal Road Ottawa, ON, K1A 0R6, Canada Colin.Cherry@nrc-cnrc.gc.ca Abstract We replicate the syntactic experiments of Mikolov et al. (2013b) on English, and expand them to include morphologically complex languages. We learn vector representations for Dutch, French, German, and Spanish with the W ORD 2V EC tool, and investigate to what extent inflectional information is preserved across vectors. We observe that the accuracy of vectors on a set of syntactic analogies is inversely correlated with the morphological complexity of the language. 1 Figure 1: An example of vector offsets. Introduction Mikolov et al. (2013b) demonstrate that vector representations of words obtained from a neural network language model provide a way of capturing both semantic and syntactic regularities in language. They observe that by manipulating vector offsets between pairs of words, it is possible to derive an approximation of vectors representing other words, such as queen ≈ king − man + woman. Similarly, an abstract relationship between the present and past tense may be computed by subtracting the base form eat from the past form ate; the result of composing such an offset with the base form cook may turn out to be similar"
W15-1518,D14-1113,0,0.0871772,"Missing"
W15-4307,W15-4319,0,0.200554,"Missing"
W15-4307,J92-4003,0,0.130869,"ts, as our entity labels cohere naturally. This speeds up tagging dramatically. Semi-Markov tagging also introduces a hyper-parameter P , the maximum entity length in tokens. Training: Our tagger is trained online with large-margin updates, following a structured variant of the passive aggressive (PA) algorithm (Crammer et al., 2006). We regularize the model both with a fixed number of epochs E through the data, and using PA’s regularization Representation Features: We also produce word-level features corresponding to a number of external representations: gazetteer membership, Brown clusters (Brown et al., 1992) and word embeddings. For gazetteers, we first segment the tweet into longest matching gazetteer phrases, resolving overlapping phrases with a greedy left-toright walk through the tweet. Each word then generates a set of features indicating which gazetteers (if any) contain its phrase. For cluster representations, we train Brown clusters on our unannotated corpus, using the implementation by Liang (2005) to build 1,000 clusters over types that occur with a minimum frequency of 10. Following Miller et al. (2004), each word generates indicators for bit prefixes of its binary cluster signature, f"
W15-4307,P14-1146,0,0.0343136,"om domain-specific gazetteers to make the distributed phrase representations more relevant to our NER task. In recent years, two sources of information have been found to be valuable to boost the performance for NER: distributed representation learned from a large corpus and domain-specific lexicons (Turian et al., 2010; Cherry and Guo, 2015). Research has also shown that merging these two forms of information can result in further predictive improvement for an NER system (Passos et al., 2014). A similar strategy for enhancing word embeddings has also been demonstrated for sentiment analysis (Tang et al., 2014). Following this line of research, we aim to tailor (post-process) the unsupervised phrase embeddings, created in Section 3.3, for our NER task, using an auto-encoder. The auto-encoder eliminates the need to have access to the original training data and the vector training model, requiring only the trained distributed vectors. In this sense, it can be considered computationally lighter than the above mentioned information fusion methods.1 Our approach is inspired by Ngiam et al. (2011) and Glorot et al. (2011), where auto-encoders are efficiently deployed to generate improved features for doma"
W15-4307,N15-1075,1,0.602185,"close to the test data, with both dev 2015 and test being drawn from the winter of 2014–2015. We present a postcompetition system that achieves an F1 of 54.2 using the same features and hyper-parameters as our submitted system, except that our tagger is also trained on all provided development data. We close with an analysis of dev 2015’s relation to the test set, and argue that these results may overestimate the impact that a small, in-domain training set can have on NER performance. Our submission to the W-NUT Named Entity Recognition in Twitter task closely follows the approach detailed by Cherry and Guo (2015), who use a discriminative, semi-Markov tagger, augmented with multiple word representations. We enhance this approach with updated gazetteers, and with infused phrase embeddings that have been adapted to better predict the gazetteer membership of each phrase. Our system achieves a typed F1 of 44.7, resulting in a third-place finish, despite training only on the official training set. A post-competition analysis indicates that also training on the provided development data improves our performance to 54.2 F1. 1 Introduction Named entity recognition (NER) is the task of finding rigid designator"
W15-4307,P10-1040,0,0.131176,"cated with new=no. We also added a number of entirely new queries (new=yes). Any baseline lexicon that is not mentioned in Table 1 was left untouched, and remains included in our updated gazetteers. 3.3 3.4 Gazetteer-Infused Phrase Vectors We employ an auto-encoder to leverage knowledge derived from domain-specific gazetteers to make the distributed phrase representations more relevant to our NER task. In recent years, two sources of information have been found to be valuable to boost the performance for NER: distributed representation learned from a large corpus and domain-specific lexicons (Turian et al., 2010; Cherry and Guo, 2015). Research has also shown that merging these two forms of information can result in further predictive improvement for an NER system (Passos et al., 2014). A similar strategy for enhancing word embeddings has also been demonstrated for sentiment analysis (Tang et al., 2014). Following this line of research, we aim to tailor (post-process) the unsupervised phrase embeddings, created in Section 3.3, for our NER task, using an auto-encoder. The auto-encoder eliminates the need to have access to the original training data and the vector training model, requiring only the tra"
W15-4307,N04-1043,0,0.0602527,"nding to a number of external representations: gazetteer membership, Brown clusters (Brown et al., 1992) and word embeddings. For gazetteers, we first segment the tweet into longest matching gazetteer phrases, resolving overlapping phrases with a greedy left-toright walk through the tweet. Each word then generates a set of features indicating which gazetteers (if any) contain its phrase. For cluster representations, we train Brown clusters on our unannotated corpus, using the implementation by Liang (2005) to build 1,000 clusters over types that occur with a minimum frequency of 10. Following Miller et al. (2004), each word generates indicators for bit prefixes of its binary cluster signature, for prefixes of length 2, 4 8 and 12. For word embeddings, we use an in-house Java re-implementation of word2vec (Mikolov et al., 2013a) to build 300dimensional vector representations for all types that occur at least 10 times in our unannotated corpus. Each word then reports a real-valued feature (as opposed to an indicator) for each of the 300 55 Team O Person O O O O Other O O Ducks sign LW Beleskey to 2-‐year extension -‐ San Jose Mercury News h=p://dlvr.it/5RcvP #ANADucks Figure 1: An example of semi-Mark"
W15-4307,W14-1609,0,0.026332,"ur updated gazetteers. 3.3 3.4 Gazetteer-Infused Phrase Vectors We employ an auto-encoder to leverage knowledge derived from domain-specific gazetteers to make the distributed phrase representations more relevant to our NER task. In recent years, two sources of information have been found to be valuable to boost the performance for NER: distributed representation learned from a large corpus and domain-specific lexicons (Turian et al., 2010; Cherry and Guo, 2015). Research has also shown that merging these two forms of information can result in further predictive improvement for an NER system (Passos et al., 2014). A similar strategy for enhancing word embeddings has also been demonstrated for sentiment analysis (Tang et al., 2014). Following this line of research, we aim to tailor (post-process) the unsupervised phrase embeddings, created in Section 3.3, for our NER task, using an auto-encoder. The auto-encoder eliminates the need to have access to the original training data and the vector training model, requiring only the trained distributed vectors. In this sense, it can be considered computationally lighter than the above mentioned information fusion methods.1 Our approach is inspired by Ngiam et"
W15-4307,W96-0213,0,0.228757,"e entity-rich. This corpus of recent tweets has an average tweet length of only 13.8 tokens. Our attempts to use this data to build word representations did not improve NER performance on the 2015 development set, regardless of whether we used the data on its own or in combination with our larger corpus. 3 3.1 Lexical Features: Recall that our semi-Markov model allows for both word and phrase-level features. The vast majority of our features are wordlevel, with the representation for a phrase being the sum of the features of its words. Our wordlevel features closely follow the set proposed by Ratnaparkhi (1996), covering word identity, the identities of surrounding words within a window of 2 tokens, and prefixes and suffixes up to three characters in length. Each word identity feature has three variants, with the first reporting the original word, the second reporting a lowercased version, and the third reporting a summary of the word’s shape (“Mrs.” becomes “Aa.”) All wordlevel features also have a variant that appends the word’s Begin/Inside/Last/Unique position within its entity. Our phrase-level features report phrase identity, with lowercased and word shape variants, along with a bias feature t"
W15-4307,D11-1141,0,0.401298,"ocessed to remove many special Unicode 54 Proceedings of the ACL 2015 Workshop on Noisy User-generated Text, pages 54–60, c Beijing, China, July 31, 2015. 2015 Association for Computational Linguistics term C, which is similar to that of an SVM. We also have the capacity to deploy example-specific C-parameters, allowing us to assign some examples more weight during training, which we use only in post-competition analysis. characters; they closely resemble those that appear in the provided training and development sets. Furthermore, the corpus consists only of tweets in which the NER system of Ritter et al. (2011) detects at least one entity. The automatic NER tags are used only to select tweets for inclusion in the corpus, after which the annotations are discarded. Filtering our tweets in this way has two immediate effects: first, each tweet is very likely to contain an entity mention. Second, the tweets are very long, with an average of 20.4 tokens per Tweet. As the test data is drawn from the winter of 2014–2015, we attempted to augment our corpus with more recent data: 13M unannotated English tweets drawn from Twitter’s public stream, from between April 24 and May 6, 2015. As we had very little rec"
W16-2317,N12-1047,1,0.854573,"ierarchical distortion model and sparse feature model are based on Russian words but are built on the lemmatized alignment. The sparse feature model consists of the standard sparse features proposed in 328 transliteration (Section 3.3), so case for transliterated words was restored via a post-processing script. As a final step, the output was detokenized with rule-based methods. log partition function, weighted with α = 0.1 (Devlin et al., 2014). 2.6 Tuning and decoding The parameters of the log-linear model were tuned by optimizing BLEU on the development set using the batch variant of MIRA (Cherry and Foster, 2012). Decoding uses the cube-pruning algorithm of (Huang and Chiang, 2007) with a 7-word distortion limit. 2.7 3 Our success in using Russian lemmas to improve word alignment and NNJMs to improve the overall system performance has inspired us to further develop new components to leverage these ideas. In this section, we describe the new features integrated with Portage in our submitted system: a neural network lexical translation model (NNLTM), a fallback Russian lemma phrase table, and a semi-supervised transliteration model. Rescoring We rescored 1000-best lists output from the decoder using a r"
W16-2317,N13-1003,1,0.836258,"). Due to resource limits, we have not used the newly released 3 billion sentence CommonCrawl monolingual English corpus. Our submitted system was tuned on the WMT 2014 test set. Both the Russian and English text in the parallel and monolingual corpora in the training/development/test corPortage - the NRC PBMT system The core of the NRC MT system is Portage (Larkin et al., 2010). Portage is a conventional log-linear phrase-based SMT system. We describe the basic features of Portage in this section and the 327 Hopkins and May (2011) and sparse hierarchical distortion model features proposed in Cherry (2013). pora were lower cased and tokenized. 2.2 Translation model We obtain the word alignment by first lemmatizing the Russian side of the parallel training data using Yandex MyStem (Segalovich, 2003). Word alignments are built for the lemmatized Russian using IBM2, HMM and IBM4 models. The Russian is then restored to its fully inflected surface form, and phrase-pairs are extracted for each of our three alignment methods. Counts from all three alignments are then combined into a single phrase table, with a maximum phrase length of 7 tokens. Phrase pairs were filtered so that the top 30 translation"
W16-2317,P14-1129,0,0.161205,"Missing"
W16-2317,E14-4029,0,0.0478782,"Missing"
W16-2317,D11-1125,0,0.0353707,"mentary v11 monolingual corpus and the NewsCrawl 2015 (206 million sentences in total). Due to resource limits, we have not used the newly released 3 billion sentence CommonCrawl monolingual English corpus. Our submitted system was tuned on the WMT 2014 test set. Both the Russian and English text in the parallel and monolingual corpora in the training/development/test corPortage - the NRC PBMT system The core of the NRC MT system is Portage (Larkin et al., 2010). Portage is a conventional log-linear phrase-based SMT system. We describe the basic features of Portage in this section and the 327 Hopkins and May (2011) and sparse hierarchical distortion model features proposed in Cherry (2013). pora were lower cased and tokenized. 2.2 Translation model We obtain the word alignment by first lemmatizing the Russian side of the parallel training data using Yandex MyStem (Segalovich, 2003). Word alignments are built for the lemmatized Russian using IBM2, HMM and IBM4 models. The Russian is then restored to its fully inflected surface form, and phrase-pairs are extracted for each of our three alignment methods. Counts from all three alignments are then combined into a single phrase table, with a maximum phrase l"
W16-2317,P07-1019,0,0.0247693,"ian words but are built on the lemmatized alignment. The sparse feature model consists of the standard sparse features proposed in 328 transliteration (Section 3.3), so case for transliterated words was restored via a post-processing script. As a final step, the output was detokenized with rule-based methods. log partition function, weighted with α = 0.1 (Devlin et al., 2014). 2.6 Tuning and decoding The parameters of the log-linear model were tuned by optimizing BLEU on the development set using the batch variant of MIRA (Cherry and Foster, 2012). Decoding uses the cube-pruning algorithm of (Huang and Chiang, 2007) with a 7-word distortion limit. 2.7 3 Our success in using Russian lemmas to improve word alignment and NNJMs to improve the overall system performance has inspired us to further develop new components to leverage these ideas. In this section, we describe the new features integrated with Portage in our submitted system: a neural network lexical translation model (NNLTM), a fallback Russian lemma phrase table, and a semi-supervised transliteration model. Rescoring We rescored 1000-best lists output from the decoder using a rescoring model (Och et al., 2004; Foster et al., 2009) consisting of 8"
W16-2317,W10-1717,1,0.823355,"ces) and the monolingual English corpus available for the constrained news translation task, which is a combination of the Europarl v7 corpus, the NewsCommentary v11 monolingual corpus and the NewsCrawl 2015 (206 million sentences in total). Due to resource limits, we have not used the newly released 3 billion sentence CommonCrawl monolingual English corpus. Our submitted system was tuned on the WMT 2014 test set. Both the Russian and English text in the parallel and monolingual corpora in the training/development/test corPortage - the NRC PBMT system The core of the NRC MT system is Portage (Larkin et al., 2010). Portage is a conventional log-linear phrase-based SMT system. We describe the basic features of Portage in this section and the 327 Hopkins and May (2011) and sparse hierarchical distortion model features proposed in Cherry (2013). pora were lower cased and tokenized. 2.2 Translation model We obtain the word alignment by first lemmatizing the Russian side of the parallel training data using Yandex MyStem (Segalovich, 2003). Word alignments are built for the lemmatized Russian using IBM2, HMM and IBM4 models. The Russian is then restored to its fully inflected surface form, and phrase-pairs a"
W16-2317,N04-1021,0,0.137306,"the cube-pruning algorithm of (Huang and Chiang, 2007) with a 7-word distortion limit. 2.7 3 Our success in using Russian lemmas to improve word alignment and NNJMs to improve the overall system performance has inspired us to further develop new components to leverage these ideas. In this section, we describe the new features integrated with Portage in our submitted system: a neural network lexical translation model (NNLTM), a fallback Russian lemma phrase table, and a semi-supervised transliteration model. Rescoring We rescored 1000-best lists output from the decoder using a rescoring model (Och et al., 2004; Foster et al., 2009) consisting of 82 features: 27 decoder features and 55 additional rescoring features. The rescoring model was tuned using nbest MIRA. Of the rescoring features, 51 consisted of various IBM features for word- and lemmaaligned IBM1, IBM2, IBM4 and HMM models, as well as various other standard length, n-gram, and n-best features. The final four features used NNJMs for rescoring, two Russian-word NNJM rescoring features and two Russian-lemma ones. Following Devlin et al. (2014), one NNJM feature rescored the 1000best list using a English-to-Russian NNJM, where the roles of th"
W16-2317,E99-1010,0,0.130019,"(Stewart et al., 2014). Each is included as a distinct feature in the decoder’s log-linear model. • A 4-gram LM trained on the target side of all the WMT parallel training corpora. • A 6-gram LM trained on the Gigaword corpus. • A 6-gram LM trained on the WMT monolingual English training corpus. • A 6-gram, 200-word-class coarse LM trained on a concatenation of the target side of all the WMT parallel training corpora and the WMT monolingual English training corpus. • A 6-gram, 800-word-class coarse LM trained on the same corpus as the 200-word-class model. Word classes are built using mkcls (Och, 1999). 2.4 Neural network joint model Distortion and sparse feature models Similar to the translation model, our hierarchical distortion model and sparse feature model are based on Russian words but are built on the lemmatized alignment. The sparse feature model consists of the standard sparse features proposed in 328 transliteration (Section 3.3), so case for transliterated words was restored via a post-processing script. As a final step, the output was detokenized with rule-based methods. log partition function, weighted with α = 0.1 (Devlin et al., 2014). 2.6 Tuning and decoding The parameters o"
W16-2317,P12-1049,0,0.0701105,"Missing"
W16-2317,2014.amta-researchers.3,1,0.793782,"12 units for the single hidden layer. We train our models with mini-batch stochastic gradient descent, with a batch size of 128 words, and an initial learning rate of 0.3. We check our training objective on the development set every 20K batches, and if it fails to improve for two consecutive checks, the learning rate is halved. Training stops after 5 consecutive failed checks or after 90 checks. To enable efficient decoding, our models are self-normalized with a squared penalty on the Language models Our system consists of three n-gram language models (LMs) and two word class language models (Stewart et al., 2014). Each is included as a distinct feature in the decoder’s log-linear model. • A 4-gram LM trained on the target side of all the WMT parallel training corpora. • A 6-gram LM trained on the Gigaword corpus. • A 6-gram LM trained on the WMT monolingual English training corpus. • A 6-gram, 200-word-class coarse LM trained on a concatenation of the target side of all the WMT parallel training corpora and the WMT monolingual English training corpus. • A 6-gram, 800-word-class coarse LM trained on the same corpus as the 200-word-class model. Word classes are built using mkcls (Och, 1999). 2.4 Neural"
W16-2317,D13-1140,0,0.0283969,"xtracted for each of our three alignment methods. Counts from all three alignments are then combined into a single phrase table, with a maximum phrase length of 7 tokens. Phrase pairs were filtered so that the top 30 translations for each source phrase were retained. Our internal development experiments indicated that using lemma alignments improved the translation quality of a baseline phrase-based system by roughly 0.2 BLEU, and also benefited the perplexity of the bilingual neural language models described in Section 2.5 and 3.1. 2.3 2.5 We employ two neural network joint models, or NNJMs (Vaswani et al., 2013; Devlin et al., 2014). The NNJM is a feed-forward neural network language model that assumes access to a source sentence f and an aligned source index ai , which points to the most influential source word for the translation of the target word ei . The NNJM calculates the language modeling proba+m i−1 bility p(ei |ei−n+1 , faaii−m ), which accounts for the n−1 preceding target words, and for 2m+1 words of source context, centered around fai . Following Devlin et al. (2014), we use n = 4 and m = 5, resulting in 3 words of target context and 11 words of source context, effectively a 15-gram lan"
W17-3205,D11-1033,0,0.559108,"or author’s or publication’s style (Chen et al., 2013). Training data varies significantly across domains, and cross-domain translations are unreliable, so performance can often be improved by adapting the MT system to the test domain. Domain adaptation (DA) techniques for SMT systems have been widely studied. Approaches in40 Proceedings of the First Workshop on Neural Machine Translation, pages 40–46, c Vancouver, Canada, August 4, 2017. 2017 Association for Computational Linguistics model to distinguish the translations for different topics. Data selection approaches (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Chen and Huang, 2016) search for data that are similar to the in-domain data according to some criterion, then use the results for training, either alone or in combination with existing data. Data weighting approaches weight each data item according to its proximity to the in-domain data. This can be applied at corpus (Foster and Kuhn, 2007; Sennrich, 2012), sentence (Matsoukas et al., 2009), or phrase level (Foster et al., 2010; Chen et al., 2013). We also study the application of existing SMT domain adaptation techniques to NMT, specifically data selection and corpus weig"
W17-3205,W09-0432,0,0.0213964,"on them, then combine these in a weighted fashion. Specifically, we: 1) train an NMT model on the large combined general-domain corpus; 2) initialize with the previous model, and train several new models on sub-corpora; 3) weight each sub-corpus according to its proximity to the in-domain data (dev set), using target-side language model perSMT adaptation techniques Most SMT domain adaptation (DA) techniques can be classified into one of five categories: selftraining, context-based DA, topic-based DA, data selection, and data weighting. With self-training (Ueffing and Ney, 2007; Schwenk, 2008; Bertoldi and Federico, 2009), an MT system trained on general domain data is used to translate large in-domain monolingual data. The resulting bilingual sentence pairs are then used as additional training data. Sennrich (2016b) has shown that back-translating a large amount of target-language text and using the resulting synthetic parallel text can improve NMT performance significantly. We can expect greater improvement if the monolingual data are in-domain. This method assumes the availability of large amounts of in-domain monolingual data, which is not the adaptation scenario in this paper. Context-based DA includes wo"
W17-3205,P13-1141,0,0.0625807,"data is used to translate large in-domain monolingual data. The resulting bilingual sentence pairs are then used as additional training data. Sennrich (2016b) has shown that back-translating a large amount of target-language text and using the resulting synthetic parallel text can improve NMT performance significantly. We can expect greater improvement if the monolingual data are in-domain. This method assumes the availability of large amounts of in-domain monolingual data, which is not the adaptation scenario in this paper. Context-based DA includes word sense disambiguation for adaptation (Carpuat et al., 2013), which employs local context to distinguish the translations for different domains. The cachebased method (Tiedemann, 2010; Gong et al., 2011) uses local or document-level context. Work on topic-based DA includes (Tam et al., 2007; Eidelman et al., 2012; Hasler et al., 2012; Hewavitharana et al., 2013), and employs a topic 41 through an alignment model αti , which is a feedforward neural network to model the probability that yt is aligned to xi . plexity (Foster and Kuhn, 2007; Sennrich, 2012); and 4) take a weighted average of the parameters in the sub-models to form our final adapted model."
W17-3205,P13-1126,1,0.917929,"Missing"
W17-3205,P17-2061,0,0.160124,"Missing"
W17-3205,P10-2041,0,0.180906,"ional origin, dialect, or author’s or publication’s style (Chen et al., 2013). Training data varies significantly across domains, and cross-domain translations are unreliable, so performance can often be improved by adapting the MT system to the test domain. Domain adaptation (DA) techniques for SMT systems have been widely studied. Approaches in40 Proceedings of the First Workshop on Neural Machine Translation, pages 40–46, c Vancouver, Canada, August 4, 2017. 2017 Association for Computational Linguistics model to distinguish the translations for different topics. Data selection approaches (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Chen and Huang, 2016) search for data that are similar to the in-domain data according to some criterion, then use the results for training, either alone or in combination with existing data. Data weighting approaches weight each data item according to its proximity to the in-domain data. This can be applied at corpus (Foster and Kuhn, 2007; Sennrich, 2012), sentence (Matsoukas et al., 2009), or phrase level (Foster et al., 2010; Chen et al., 2013). We also study the application of existing SMT domain adaptation techniques to NMT, specifically data sel"
W17-3205,P12-2023,0,0.0191037,"synthetic parallel text can improve NMT performance significantly. We can expect greater improvement if the monolingual data are in-domain. This method assumes the availability of large amounts of in-domain monolingual data, which is not the adaptation scenario in this paper. Context-based DA includes word sense disambiguation for adaptation (Carpuat et al., 2013), which employs local context to distinguish the translations for different domains. The cachebased method (Tiedemann, 2010; Gong et al., 2011) uses local or document-level context. Work on topic-based DA includes (Tam et al., 2007; Eidelman et al., 2012; Hasler et al., 2012; Hewavitharana et al., 2013), and employs a topic 41 through an alignment model αti , which is a feedforward neural network to model the probability that yt is aligned to xi . plexity (Foster and Kuhn, 2007; Sennrich, 2012); and 4) take a weighted average of the parameters in the sub-models to form our final adapted model. Sentence-level weighting Our new method for weighting individual sentence pairs uses a classifier to assign weights, and applies them when computing the cost of each mini-batch during NMT training. We defer a detailed description to section 4, after fir"
W17-3205,P02-1040,0,0.115578,"Missing"
W17-3205,W07-0717,1,0.366807,"on Neural Machine Translation, pages 40–46, c Vancouver, Canada, August 4, 2017. 2017 Association for Computational Linguistics model to distinguish the translations for different topics. Data selection approaches (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Chen and Huang, 2016) search for data that are similar to the in-domain data according to some criterion, then use the results for training, either alone or in combination with existing data. Data weighting approaches weight each data item according to its proximity to the in-domain data. This can be applied at corpus (Foster and Kuhn, 2007; Sennrich, 2012), sentence (Matsoukas et al., 2009), or phrase level (Foster et al., 2010; Chen et al., 2013). We also study the application of existing SMT domain adaptation techniques to NMT, specifically data selection and corpus weighting methods. Experiments on Chinese-to-English NIST and English-to-French WMT tasks show that: 1) data selection and corpus weighting methods yield significant improvement over the non-adapted baseline; and 2) the new cost weighting method obtains the biggest improvement. The cost weighting scheme has the additional advantage of being integrated with sequenc"
W17-3205,2008.iwslt-papers.6,0,0.0546254,"NMT sub-models on them, then combine these in a weighted fashion. Specifically, we: 1) train an NMT model on the large combined general-domain corpus; 2) initialize with the previous model, and train several new models on sub-corpora; 3) weight each sub-corpus according to its proximity to the in-domain data (dev set), using target-side language model perSMT adaptation techniques Most SMT domain adaptation (DA) techniques can be classified into one of five categories: selftraining, context-based DA, topic-based DA, data selection, and data weighting. With self-training (Ueffing and Ney, 2007; Schwenk, 2008; Bertoldi and Federico, 2009), an MT system trained on general domain data is used to translate large in-domain monolingual data. The resulting bilingual sentence pairs are then used as additional training data. Sennrich (2016b) has shown that back-translating a large amount of target-language text and using the resulting synthetic parallel text can improve NMT performance significantly. We can expect greater improvement if the monolingual data are in-domain. This method assumes the availability of large amounts of in-domain monolingual data, which is not the adaptation scenario in this paper"
W17-3205,D10-1044,1,0.472945,"Missing"
W17-3205,W16-2323,0,0.0410984,"being integrated with sequence-to-sequence training. 2 Applying SMT adaptation techniques to NMT 2.2 There are several adaptation scenarios for MT, of which the most common is: 1) the training material is heterogeneous, with some parts that are not too far from the test domain; 2) a bilingual development set drawn from the test domain is available. In this paper, we study adaptation techniques for this scenario. 2.1 Application to NMT In this paper, we apply data selection, corpus weighting, and sentence weighting strategies to NMT. Data selection Some previous work (Luong and Manning, 2015; Sennrich et al., 2016b) has shown that the performance of NMT systems is highly sensitive to data size. Therefore, we follow the solution in (Luong and Manning, 2015): we first train an NMT system on all available training data, then further train on the selected in-domain data. We adopt two data selection methods in this paper. The first one is based on bilingual language model cross-entropy difference (Axelrod et al., 2011). For both the source and target language, two language models are trained on in-domain and out-of-domain data respectively; then, a sentence pair is evaluated with the cross-entropy differenc"
W17-3205,P16-1009,0,0.0550095,"being integrated with sequence-to-sequence training. 2 Applying SMT adaptation techniques to NMT 2.2 There are several adaptation scenarios for MT, of which the most common is: 1) the training material is heterogeneous, with some parts that are not too far from the test domain; 2) a bilingual development set drawn from the test domain is available. In this paper, we study adaptation techniques for this scenario. 2.1 Application to NMT In this paper, we apply data selection, corpus weighting, and sentence weighting strategies to NMT. Data selection Some previous work (Luong and Manning, 2015; Sennrich et al., 2016b) has shown that the performance of NMT systems is highly sensitive to data size. Therefore, we follow the solution in (Luong and Manning, 2015): we first train an NMT system on all available training data, then further train on the selected in-domain data. We adopt two data selection methods in this paper. The first one is based on bilingual language model cross-entropy difference (Axelrod et al., 2011). For both the source and target language, two language models are trained on in-domain and out-of-domain data respectively; then, a sentence pair is evaluated with the cross-entropy differenc"
W17-3205,D11-1084,0,0.0213312,"nrich (2016b) has shown that back-translating a large amount of target-language text and using the resulting synthetic parallel text can improve NMT performance significantly. We can expect greater improvement if the monolingual data are in-domain. This method assumes the availability of large amounts of in-domain monolingual data, which is not the adaptation scenario in this paper. Context-based DA includes word sense disambiguation for adaptation (Carpuat et al., 2013), which employs local context to distinguish the translations for different domains. The cachebased method (Tiedemann, 2010; Gong et al., 2011) uses local or document-level context. Work on topic-based DA includes (Tam et al., 2007; Eidelman et al., 2012; Hasler et al., 2012; Hewavitharana et al., 2013), and employs a topic 41 through an alignment model αti , which is a feedforward neural network to model the probability that yt is aligned to xi . plexity (Foster and Kuhn, 2007; Sennrich, 2012); and 4) take a weighted average of the parameters in the sub-models to form our final adapted model. Sentence-level weighting Our new method for weighting individual sentence pairs uses a classifier to assign weights, and applies them when com"
W17-3205,2012.iwslt-papers.17,0,0.0170511,"t can improve NMT performance significantly. We can expect greater improvement if the monolingual data are in-domain. This method assumes the availability of large amounts of in-domain monolingual data, which is not the adaptation scenario in this paper. Context-based DA includes word sense disambiguation for adaptation (Carpuat et al., 2013), which employs local context to distinguish the translations for different domains. The cachebased method (Tiedemann, 2010; Gong et al., 2011) uses local or document-level context. Work on topic-based DA includes (Tam et al., 2007; Eidelman et al., 2012; Hasler et al., 2012; Hewavitharana et al., 2013), and employs a topic 41 through an alignment model αti , which is a feedforward neural network to model the probability that yt is aligned to xi . plexity (Foster and Kuhn, 2007; Sennrich, 2012); and 4) take a weighted average of the parameters in the sub-models to form our final adapted model. Sentence-level weighting Our new method for weighting individual sentence pairs uses a classifier to assign weights, and applies them when computing the cost of each mini-batch during NMT training. We defer a detailed description to section 4, after first presenting the NMT"
W17-3205,E12-1055,0,0.0524815,"slation, pages 40–46, c Vancouver, Canada, August 4, 2017. 2017 Association for Computational Linguistics model to distinguish the translations for different topics. Data selection approaches (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Chen and Huang, 2016) search for data that are similar to the in-domain data according to some criterion, then use the results for training, either alone or in combination with existing data. Data weighting approaches weight each data item according to its proximity to the in-domain data. This can be applied at corpus (Foster and Kuhn, 2007; Sennrich, 2012), sentence (Matsoukas et al., 2009), or phrase level (Foster et al., 2010; Chen et al., 2013). We also study the application of existing SMT domain adaptation techniques to NMT, specifically data selection and corpus weighting methods. Experiments on Chinese-to-English NIST and English-to-French WMT tasks show that: 1) data selection and corpus weighting methods yield significant improvement over the non-adapted baseline; and 2) the new cost weighting method obtains the biggest improvement. The cost weighting scheme has the additional advantage of being integrated with sequence-to-sequence tra"
W17-3205,P13-2122,0,0.0372216,"formance significantly. We can expect greater improvement if the monolingual data are in-domain. This method assumes the availability of large amounts of in-domain monolingual data, which is not the adaptation scenario in this paper. Context-based DA includes word sense disambiguation for adaptation (Carpuat et al., 2013), which employs local context to distinguish the translations for different domains. The cachebased method (Tiedemann, 2010; Gong et al., 2011) uses local or document-level context. Work on topic-based DA includes (Tam et al., 2007; Eidelman et al., 2012; Hasler et al., 2012; Hewavitharana et al., 2013), and employs a topic 41 through an alignment model αti , which is a feedforward neural network to model the probability that yt is aligned to xi . plexity (Foster and Kuhn, 2007; Sennrich, 2012); and 4) take a weighted average of the parameters in the sub-models to form our final adapted model. Sentence-level weighting Our new method for weighting individual sentence pairs uses a classifier to assign weights, and applies them when computing the cost of each mini-batch during NMT training. We defer a detailed description to section 4, after first presenting the NMT approach used in our experim"
W17-3205,P07-1066,0,0.0237287,"sing the resulting synthetic parallel text can improve NMT performance significantly. We can expect greater improvement if the monolingual data are in-domain. This method assumes the availability of large amounts of in-domain monolingual data, which is not the adaptation scenario in this paper. Context-based DA includes word sense disambiguation for adaptation (Carpuat et al., 2013), which employs local context to distinguish the translations for different domains. The cachebased method (Tiedemann, 2010; Gong et al., 2011) uses local or document-level context. Work on topic-based DA includes (Tam et al., 2007; Eidelman et al., 2012; Hasler et al., 2012; Hewavitharana et al., 2013), and employs a topic 41 through an alignment model αti , which is a feedforward neural network to model the probability that yt is aligned to xi . plexity (Foster and Kuhn, 2007; Sennrich, 2012); and 4) take a weighted average of the parameters in the sub-models to form our final adapted model. Sentence-level weighting Our new method for weighting individual sentence pairs uses a classifier to assign weights, and applies them when computing the cost of each mini-batch during NMT training. We defer a detailed description"
W17-3205,W10-2602,0,0.0122925,"raining data. Sennrich (2016b) has shown that back-translating a large amount of target-language text and using the resulting synthetic parallel text can improve NMT performance significantly. We can expect greater improvement if the monolingual data are in-domain. This method assumes the availability of large amounts of in-domain monolingual data, which is not the adaptation scenario in this paper. Context-based DA includes word sense disambiguation for adaptation (Carpuat et al., 2013), which employs local context to distinguish the translations for different domains. The cachebased method (Tiedemann, 2010; Gong et al., 2011) uses local or document-level context. Work on topic-based DA includes (Tam et al., 2007; Eidelman et al., 2012; Hasler et al., 2012; Hewavitharana et al., 2013), and employs a topic 41 through an alignment model αti , which is a feedforward neural network to model the probability that yt is aligned to xi . plexity (Foster and Kuhn, 2007; Sennrich, 2012); and 4) take a weighted average of the parameters in the sub-models to form our final adapted model. Sentence-level weighting Our new method for weighting individual sentence pairs uses a classifier to assign weights, and a"
W17-3205,J07-1003,0,0.0120236,"orpora, we first train NMT sub-models on them, then combine these in a weighted fashion. Specifically, we: 1) train an NMT model on the large combined general-domain corpus; 2) initialize with the previous model, and train several new models on sub-corpora; 3) weight each sub-corpus according to its proximity to the in-domain data (dev set), using target-side language model perSMT adaptation techniques Most SMT domain adaptation (DA) techniques can be classified into one of five categories: selftraining, context-based DA, topic-based DA, data selection, and data weighting. With self-training (Ueffing and Ney, 2007; Schwenk, 2008; Bertoldi and Federico, 2009), an MT system trained on general domain data is used to translate large in-domain monolingual data. The resulting bilingual sentence pairs are then used as additional training data. Sennrich (2016b) has shown that back-translating a large amount of target-language text and using the resulting synthetic parallel text can improve NMT performance significantly. We can expect greater improvement if the monolingual data are in-domain. This method assumes the availability of large amounts of in-domain monolingual data, which is not the adaptation scenari"
W17-3205,W04-3250,0,0.0729876,"Missing"
W17-3205,2015.iwslt-evaluation.11,0,0.218972,"e additional advantage of being integrated with sequence-to-sequence training. 2 Applying SMT adaptation techniques to NMT 2.2 There are several adaptation scenarios for MT, of which the most common is: 1) the training material is heterogeneous, with some parts that are not too far from the test domain; 2) a bilingual development set drawn from the test domain is available. In this paper, we study adaptation techniques for this scenario. 2.1 Application to NMT In this paper, we apply data selection, corpus weighting, and sentence weighting strategies to NMT. Data selection Some previous work (Luong and Manning, 2015; Sennrich et al., 2016b) has shown that the performance of NMT systems is highly sensitive to data size. Therefore, we follow the solution in (Luong and Manning, 2015): we first train an NMT system on all available training data, then further train on the selected in-domain data. We adopt two data selection methods in this paper. The first one is based on bilingual language model cross-entropy difference (Axelrod et al., 2011). For both the source and target language, two language models are trained on in-domain and out-of-domain data respectively; then, a sentence pair is evaluated with the"
W17-3205,D09-1074,0,0.0117488,"couver, Canada, August 4, 2017. 2017 Association for Computational Linguistics model to distinguish the translations for different topics. Data selection approaches (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Chen and Huang, 2016) search for data that are similar to the in-domain data according to some criterion, then use the results for training, either alone or in combination with existing data. Data weighting approaches weight each data item according to its proximity to the in-domain data. This can be applied at corpus (Foster and Kuhn, 2007; Sennrich, 2012), sentence (Matsoukas et al., 2009), or phrase level (Foster et al., 2010; Chen et al., 2013). We also study the application of existing SMT domain adaptation techniques to NMT, specifically data selection and corpus weighting methods. Experiments on Chinese-to-English NIST and English-to-French WMT tasks show that: 1) data selection and corpus weighting methods yield significant improvement over the non-adapted baseline; and 2) the new cost weighting method obtains the biggest improvement. The cost weighting scheme has the additional advantage of being integrated with sequence-to-sequence training. 2 Applying SMT adaptation te"
W17-3205,P13-2119,0,\N,Missing
W17-3205,K16-1031,1,\N,Missing
W17-4732,D11-1033,0,0.0310836,"omain. Finally, we experimented with a cost weighting domain adaptation technique (Chen et al., 2017). This technique trains a domain classifier concurrently with the NMT system, and uses the classifier probabilities to weight training instances according to their similarity to the development set. Since the majority of the 25 million sentence pairs in the training corpus are general domain, we experimented with different data selection and domain adaptation techniques to further train the NMT system with data that are similar to the development set so as to perform better in the news domain. Axelrod et al. (2011) introduced the bilingual language model cross-entropy difference as a similarity function for identifying sentence pairs from general-domain training corpora that are close to the target domain. We built four language models using the input and output sides of the training corpora and the development set respectively to select 3 million sentence pairs from the training corpora that are close to the news domain. However, the development set, which consists of only 1k sentence pairs, is too tiny to be a suitable corpus for building the in-domain language models that will enable the bilingual LM"
W17-4732,W16-2317,1,0.894593,"Missing"
W17-4732,W17-3205,1,0.837791,"cted by bilingual LM cross-entropy difference (xent), c) further trained with synthetic data, d) further trained with cost weighting, e) further trained with in-domain data selected by semi-supervised convolutional neural network classifier (sscnn), f) greedy model averaging and g) optimized against sentence-level BLEU on the intersection of the subsets of data selected by xent and sscnn using MRT. 3.2 Data selection and domain adaptation lion sentence pairs from the training corpora that are close to the news domain. Finally, we experimented with a cost weighting domain adaptation technique (Chen et al., 2017). This technique trains a domain classifier concurrently with the NMT system, and uses the classifier probabilities to weight training instances according to their similarity to the development set. Since the majority of the 25 million sentence pairs in the training corpus are general domain, we experimented with different data selection and domain adaptation techniques to further train the NMT system with data that are similar to the development set so as to perform better in the news domain. Axelrod et al. (2011) introduced the bilingual language model cross-entropy difference as a similarit"
W17-4732,K16-1031,1,0.92652,"-English (out of twenty participants) in WMT 2017 human evaluation. 1 George Foster∗ Work performed while at NRC. 330 Proceedings of the Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 330–337 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics the one described in Section 2.1,1 and then employed the machine-translated Russian and perfect English sentence pairs as additional data to train the Russian-English MT system. To select sentences for back-translation, we used a semi-supervised convolutional neural network classifier (Chen and Huang, 2016). We sampled two million sentences from the English monolingual News Crawl 2015 & 2016 corpora according to their classifier scores, which reflect their similarity to the the English half of our development set. formance (third place in both language pairs) in the preliminary automatic evaluation of WMT 2017. In this paper, we discuss the lessons learned in building large-scale state-of-the-art NMT systems. 2 Russian-English news translation We used all the Russian-English parallel corpora available for the constrained news translation task. They include the CommonCrawl corpus, the NewsComment"
W17-4732,N04-1021,0,0.0967458,"lt is rather disappointing by comparison with the exciting improvement reported in Sennrich et al. (2016a), i.e. 3-4 BLEU. Another disappointing result is that model averaging does not work well with the dropout models. We can see model averaging yields around 1 BLEU gain on non-dropout systems. However, the improvement achieved by model averaging drops to 0-0.1 BLEU on dropout systems. In other experiments not shown here, we also saw no improvement from ensembling the checkpoints of our dropout systems. We rescored 1000-best lists output from the phrase-based decoder using a rescoring model (Och et al., 2004; Foster et al., 2009) consisting of 13 features: 3 NMT models, 2 language models, 5 NNJMs and 3 n-best features. The rescoring model was tuned using n-best MIRA (Cherry and Foster, 2012). The three NMT systems used as rescoring features were: 1) baseline further trained with synthetic data, 2) dropout baseline further trained with synthetic data and with dropout turned off, and 3) the previous model optimized to the development set using minimum risk training. The five NNJM rescoring features include two Russian-word NNJMs and three Russian-lemma ones. Following Devlin et al. (2014), we take"
W17-4732,N12-1047,1,0.741262,"s not work well with the dropout models. We can see model averaging yields around 1 BLEU gain on non-dropout systems. However, the improvement achieved by model averaging drops to 0-0.1 BLEU on dropout systems. In other experiments not shown here, we also saw no improvement from ensembling the checkpoints of our dropout systems. We rescored 1000-best lists output from the phrase-based decoder using a rescoring model (Och et al., 2004; Foster et al., 2009) consisting of 13 features: 3 NMT models, 2 language models, 5 NNJMs and 3 n-best features. The rescoring model was tuned using n-best MIRA (Cherry and Foster, 2012). The three NMT systems used as rescoring features were: 1) baseline further trained with synthetic data, 2) dropout baseline further trained with synthetic data and with dropout turned off, and 3) the previous model optimized to the development set using minimum risk training. The five NNJM rescoring features include two Russian-word NNJMs and three Russian-lemma ones. Following Devlin et al. (2014), we take advantage of the rescoring framework to have our NNJMs view each candidate translation from 332 Figure 1: Russian-English learning curve on development set in cased BLEU of selected model"
W17-4732,P14-1129,0,0.0771307,"Missing"
W17-4732,E17-3017,0,0.0554434,"Missing"
W17-4732,P16-1009,0,0.450161,"inflections, since they play an important role in disambiguating the meaning of sentences. Chinese does not have clear word boundaries. The number of Chinese word types created by automatic word segmentation software is high, while naive character segmentation would result in a skewed Chinese to English sentence length ratio. These characteristics make it difficult for machine translation systems to learn the correct association between words in Chinese and English. Since this was the first time we deployed NMT models in an evaluation, we first tried to replicate the results of previous work (Sennrich et al., 2016a). Our NMT systems are based on Nematus (Sennrich et al., 2017). We used automatic back-translation (Sennrich et al., 2016b) of a subselected monolingual News corpus as additional training data, and all the training data is segmented into subword units using BPE (Sennrich et al., 2016c). We also experimented with pervasive dropout as implemented in Nematus. For Russian-English, our WMT16 PBMT system scored higher than all the NMT systems we built this year. We therefore experimented with using the NMT systems as features for rescoring the 1000-best output from our WMT16 PBMT system. This stra"
W17-4732,W16-2316,0,0.0118976,"and embedding layers to 0.15. For the hidden layers, we set the dropout probability to 0.3. NMT baseline system Our NMT baseline system is developed using Nematus (Sennrich et al., 2017). The dimension of word embeddings is set to 512 and that of the hidden layers is set to 1024. We train the models with rmsprop (Tieleman and Hinton, 2012), reshuffling the training corpus between epochs. We use minibatches of size 100 and validate the model every 8000 minibatches against BLEU on the WMT 15 news translation test set. We perform early stopping on the baseline system. We use AmuNMT C++ decoder (Junczys-Dowmunt et al., 2016a) with a beam size of 4. 2.2 Pervasive dropout 2.4 Minimum risk training Minimum risk training (MRT) (Shen et al., 2016) allows model optimization to arbitrary loss functions, which do not need to be differentiable, thus enabling direct model tuning against automatic MT evaluation metrics. It uses the MT evaluation metric as the loss function and minimizes the expected loss on the training data at sentence-level. We experimented with further model optimization using MRT on the whole training corpus against sentence BLEU at the final stage. 2.5 Synthetic training data Greedy model averaging A"
W17-4732,P16-1162,0,0.103844,"Missing"
W17-4732,P16-1159,0,0.0343062,"ystem is developed using Nematus (Sennrich et al., 2017). The dimension of word embeddings is set to 512 and that of the hidden layers is set to 1024. We train the models with rmsprop (Tieleman and Hinton, 2012), reshuffling the training corpus between epochs. We use minibatches of size 100 and validate the model every 8000 minibatches against BLEU on the WMT 15 news translation test set. We perform early stopping on the baseline system. We use AmuNMT C++ decoder (Junczys-Dowmunt et al., 2016a) with a beam size of 4. 2.2 Pervasive dropout 2.4 Minimum risk training Minimum risk training (MRT) (Shen et al., 2016) allows model optimization to arbitrary loss functions, which do not need to be differentiable, thus enabling direct model tuning against automatic MT evaluation metrics. It uses the MT evaluation metric as the loss function and minimizes the expected loss on the training data at sentence-level. We experimented with further model optimization using MRT on the whole training corpus against sentence BLEU at the final stage. 2.5 Synthetic training data Greedy model averaging A common practice for avoiding overfitting to the training data is ensembling the last few models saved as checkpoints. Rec"
W17-4732,W03-1730,0,0.0742719,"We used all the Chinese-English parallel corpora available for the constrained news translation task. They include the UN corpus, the NewsCommentary v12 corpus and the CWMT corpus. In total, 25 million parallel Chinese-English sentences were used to train the baseline system. We used half of the WMT 17 news translation development set as our development set and the other half as internal test set. The English texts in the training/development/test corpora were tokenized and lowercased while the Chinese texts in the training/development/test corpora were segmented using the ICTCLAS segmenter (Zhang et al., 2003). Then the Chinese and English text were combined to train a BPE model with vocabulary size of 90k. Although in figure 1 we see that none of the NMT systems manage to beat our WMT16 PBMT submission, the more interesting result is that there is more than 1.8 BLEU gain on the development set and 1.1 BLEU gain on the test set by rescoring the PBMT 1000-best list using just one of our NMT systems and no other features, as in line (g). The final rescoring with weighted collections of NMT systems, language model features, NNJM features and n-best features shows 1.8 BLEU improvement over the WMT 16 s"
