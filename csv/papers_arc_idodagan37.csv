2021.starsem-1.8,"Teach the Rules, Provide the Facts: Targeted Relational-knowledge Enhancement for Textual Inference",2021,-1,-1,4,1,952,ohad rozen,Proceedings of *SEM 2021: The Tenth Joint Conference on Lexical and Computational Semantics,0,"We present InferBert, a method to enhance transformer-based inference models with relevant relational knowledge. Our approach facilitates learning generic inference patterns requiring relational knowledge (e.g. inferences related to hypernymy) during training, while injecting on-demand the relevant relational facts (e.g. pangolin is an animal) at test time. We apply InferBERT to the NLI task over a diverse set of inference types (hypernymy, location, color, and country of origin), for which we collected challenge datasets. In this setting, InferBert succeeds to learn general inference patterns, from a relatively small number of training instances, while not hurting performance on the original NLI data and substantially outperforming prior knowledge enhancement models on the challenge data. It further applies its inferences successfully at test time to previously unobserved entities. InferBert is computationally more efficient than most prior methods, in terms of number of parameters, memory consumption and training time."
2021.starsem-1.13,Realistic Evaluation Principles for Cross-document Coreference Resolution,2021,-1,-1,5,1,971,arie cattan,Proceedings of *SEM 2021: The Tenth Joint Conference on Lexical and Computational Semantics,0,"We point out that common evaluation practices for cross-document coreference resolution have been unrealistically permissive in their assumed settings, yielding inflated results. We propose addressing this issue via two evaluation methodology principles. First, as in other tasks, models should be evaluated on predicted mentions rather than on gold mentions. Doing this raises a subtle issue regarding singleton coreference clusters, which we address by decoupling the evaluation of mention detection from that of coreference linking. Second, we argue that models should not exploit the synthetic topic structure of the standard ECB+ dataset, forcing models to confront the lexical ambiguity challenge, as intended by the dataset creators. We demonstrate empirically the drastic impact of our more realistic evaluation principles on a competitive model, yielding a score which is 33 F1 lower compared to evaluating by prior lenient practices."
2021.starsem-1.28,Denoising Word Embeddings by Averaging in a Shared Space,2021,-1,-1,2,1,1010,avi caciularu,Proceedings of *SEM 2021: The Tenth Joint Conference on Lexical and Computational Semantics,0,"We introduce a new approach for smoothing and improving the quality of word embeddings. We consider a method of fusing word embeddings that were trained on the same corpus but with different initializations. We project all the models to a shared vector space using an efficient implementation of the Generalized Procrustes Analysis (GPA) procedure, previously used in multilingual word translation. Our word representation demonstrates consistent improvements over the raw models as well as their simplistic average, on a range of tasks. As the new representations are more stable and reliable, there is a noticeable improvement in rare word evaluations."
2021.naacl-main.54,Extending Multi-Document Summarization Evaluation to the Interactive Setting,2021,-1,-1,6,1,3395,ori shapira,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Allowing users to interact with multi-document summarizers is a promising direction towards improving and customizing summary results. Different ideas for interactive summarization have been proposed in previous work but these solutions are highly divergent and incomparable. In this paper, we develop an end-to-end evaluation framework for interactive summarization, focusing on expansion-based interaction, which considers the accumulating information along a user session. Our framework includes a procedure of collecting real user sessions, as well as evaluation measures relying on summarization standards, but adapted to reflect interaction. All of our solutions and resources are available publicly as a benchmark, allowing comparison of future developments in interactive summarization, and spurring progress in its methodological evaluation. We demonstrate the use of our framework by evaluating and comparing baseline implementations that we developed for this purpose, which will serve as part of our benchmark. Our extensive experimentation and analysis motivate the proposed evaluation framework design and support its viability."
2021.naacl-main.198,{WEC}: Deriving a Large-scale Cross-document Event Coreference dataset from {W}ikipedia,2021,-1,-1,3,1,972,alon eirew,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Cross-document event coreference resolution is a foundational task for NLP applications involving multi-text processing. However, existing corpora for this task are scarce and relatively small, while annotating only modest-size clusters of documents belonging to the same topic. To complement these resources and enhance future research, we present Wikipedia Event Coreference (WEC), an efficient methodology for gathering a large-scale dataset for cross-document event coreference from Wikipedia, where coreference links are not restricted within predefined topics. We apply this methodology to the English Wikipedia and extract our large-scale WEC-Eng dataset. Notably, our dataset creation method is generic and can be applied with relatively little effort to other Wikipedia languages. To set baseline results, we develop an algorithm that adapts components of state-of-the-art models for within-document coreference resolution to the cross-document setting. Our model is suitably efficient and outperforms previously published state-of-the-art results for the task."
2021.findings-emnlp.225,{CDLM}: Cross-Document Language Modeling,2021,-1,-1,6,1,1010,avi caciularu,Findings of the Association for Computational Linguistics: EMNLP 2021,0,"We introduce a new pretraining approach geared for multi-document language modeling, incorporating two key ideas into the masked language modeling self-supervised objective. First, instead of considering documents in isolation, we pretrain over sets of multiple related documents, encouraging the model to learn cross-document relationships. Second, we improve over recent long-range transformers by introducing dynamic global attention that has access to the entire input to predict masked tokens. We release CDLM (Cross-Document Language Model), a new general language model for multi-document setting that can be easily applied to downstream tasks. Our extensive analysis shows that both ideas are essential for the success of CDLM, and work in synergy to set new state-of-the-art results for several multi-text tasks."
2021.findings-acl.453,Cross-document Coreference Resolution over Predicted Mentions,2021,-1,-1,5,1,971,arie cattan,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,0,None
2021.emnlp-main.108,Asking It All: Generating Contextualized Questions for any Semantic Role,2021,-1,-1,6,1,8857,valentina pyatkin,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Asking questions about a situation is an inherent step towards understanding it. To this end, we introduce the task of role question generation, which, given a predicate mention and a passage, requires producing a set of questions asking about all possible semantic roles of the predicate. We develop a two-stage model for this task, which first produces a context-independent question prototype for each role and then revises it to be contextually appropriate for the passage. Unlike most existing approaches to question generation, our approach does not require conditioning on existing answers in the text. Instead, we condition on the type of information to inquire about, regardless of whether the answer appears explicitly in the text, could be inferred from it, or should be sought elsewhere. Our evaluation demonstrates that we generate diverse and well-formed questions for a large, broad-coverage ontology of predicates and roles."
2021.emnlp-main.778,{QA}-Align: Representing Cross-Text Content Overlap by Aligning Question-Answer Propositions,2021,-1,-1,5,0,10184,daniela weiss,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Multi-text applications, such as multi-document summarization, are typically required to model redundancies across related texts. Current methods confronting consolidation struggle to fuse overlapping information. In order to explicitly represent content overlap, we propose to align predicate-argument relations across texts, providing a potential scaffold for information consolidation. We go beyond clustering coreferring mentions, and instead model overlap with respect to redundancy at a propositional level, rather than merely detecting shared referents. Our setting exploits QA-SRL, utilizing question-answer pairs to capture predicate-argument relations, facilitating laymen annotation of cross-text alignments. We employ crowd-workers for constructing a dataset of QA-based alignments, and present a baseline QA alignment model trained over our dataset. Analyses show that our new task is semantically challenging, capturing content overlap beyond lexical similarity and complements cross-document coreference with proposition-level links, offering potential use for downstream tasks."
2021.emnlp-demo.33,i{F}acet{S}um: Coreference-based Interactive Faceted Summarization for Multi-Document Exploration,2021,-1,-1,10,0,10393,eran hirsch,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,0,"We introduce iFá´á´á´á´Sá´á´, a web application for exploring topical document collections. iFá´á´á´á´Sá´á´ integrates interactive summarization together with faceted search, by providing a novel faceted navigation scheme that yields abstractive summaries for the user{'}s selections. This approach offers both a comprehensive overview as well as particular details regard-ing subtopics of choice. The facets are automatically produced based on cross-document coreference pipelines, rendering generic concepts, entities and statements surfacing in the source texts. We analyze the effectiveness of our application through small-scale user studies that suggest the usefulness of our tool."
2021.eacl-main.21,{CD}{\\^{}}2{CR}: Co-reference resolution across documents and domains,2021,-1,-1,4,0,10537,james ravenscroft,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,0,"Cross-document co-reference resolution (CDCR) is the task of identifying and linking mentions to entities and concepts across many text documents. Current state-of-the-art models for this task assume that all documents are of the same type (e.g. news articles) or fall under the same theme. However, it is also desirable to perform CDCR across different domains (type or theme). A particular use case we focus on in this paper is the resolution of entities mentioned across scientific work and newspaper articles that discuss them. Identifying the same entities and corresponding concepts in both scientific articles and news can help scientists understand how their work is represented in mainstream media. We propose a new task and English language dataset for cross-document cross-domain co-reference resolution (CD{\^{}}2CR). The task aims to identify links between entities across heterogeneous document types. We show that in this cross-domain, cross-document setting, existing CDCR models do not perform well and we provide a baseline model that outperforms current state-of-the-art CDCR models on CD{\^{}}2CR. Our data set, annotation tool and guidelines as well as our model for cross-document cross-domain co-reference are all supplied as open access open source resources."
2021.conll-1.25,"Summary-Source Proposition-level Alignment: Task, Datasets and Supervised Baseline",2021,-1,-1,7,0,10186,ori ernst,Proceedings of the 25th Conference on Computational Natural Language Learning,0,"Aligning sentences in a reference summary with their counterparts in source documents was shown as a useful auxiliary summarization task, notably for generating training data for salience detection. Despite its assessed utility, the alignment step was mostly approached with heuristic unsupervised methods, typically ROUGE-based, and was never independently optimized or evaluated. In this paper, we propose establishing summary-source alignment as an explicit task, while introducing two major novelties: (1) applying it at the more accurate proposition span level, and (2) approaching it as a supervised classification task. To that end, we created a novel training dataset for proposition-level alignment, derived automatically from available summarization evaluation data. In addition, we crowdsourced dev and test datasets, enabling model development and proper evaluation. Utilizing these data, we present a supervised proposition alignment baseline model, showing improved alignment-quality over the unsupervised approach."
2020.findings-emnlp.440,Paraphrasing vs Coreferring: Two Sides of the Same Coin,2020,25,0,4,0,19976,yehudit meged,Findings of the Association for Computational Linguistics: EMNLP 2020,0,"We study the potential synergy between two different NLP tasks, both confronting predicate lexical variability: identifying predicate paraphrases, and event coreference resolution. First, we used annotations from an event coreference dataset as distant supervision to re-score heuristically-extracted predicate paraphrases. The new scoring gained more than 18 points in average precision upon their ranking by the original scoring method. Then, we used the same re-ranking features as additional inputs to a state-of-the-art event coreference resolution model, which yielded modest but consistent improvements to the model{'}s performance. The results suggest a promising direction to leverage data and models for each of the tasks to the benefit of the other."
2020.emnlp-main.224,"{QAD}iscourse - {D}iscourse {R}elations as {QA} {P}airs: {R}epresentation, {C}rowdsourcing and {B}aselines",2020,-1,-1,4,1,8857,valentina pyatkin,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"Discourse relations describe how two propositions relate to one another, and identifying them automatically is an integral part of natural language understanding. However, annotating discourse relations typically requires expert annotators. Recently, different semantic aspects of a sentence have been represented and crowd-sourced via question-and-answer (QA) pairs. This paper proposes a novel representation of discourse relations as QA pairs, which in turn allows us to crowd-source wide-coverage data annotated with discourse relations, via an intuitively appealing interface for composing such questions and answers. Based on our proposed representation, we collect a novel and wide-coverage QADiscourse dataset, and present baseline algorithms for predicting QADiscourse relations."
2020.emnlp-main.284,Within-Between Lexical Relation Classification,2020,-1,-1,3,0,8163,oren barkan,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"We propose the novel \textit{Within-Between} Relation model for recognizing lexical-semantic relations between words. Our model integrates relational and distributional signals, forming an effective sub-space representation for each relation. We show that the proposed model is competitive and outperforms other baselines, across various benchmarks."
2020.emnlp-demos.27,{C}o{R}efi: A Crowd Sourcing Suite for Coreference Annotation,2020,-1,-1,3,0,20776,ari bornstein,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,0,"Coreference annotation is an important, yet expensive and time consuming, task, which often involved expert annotators trained on complex decision guidelines. To enable cheaper and more efficient annotation, we present CoRefi, a web-based coreference annotation suite, oriented for crowdsourcing. Beyond the core coreference annotation tool, CoRefi provides guided onboarding for the task as well as a novel algorithm for a reviewing phase. CoRefi is open source and directly embeds into any website, including popular crowdsourcing platforms. CoRefi Demo: aka.ms/corefi Video Tour: aka.ms/corefivideo Github Repo: https://github.com/aribornstein/corefi"
2020.coling-main.274,{QAN}om: Question-Answer driven {SRL} for Nominalizations,2020,-1,-1,8,1,10185,ayal klein,Proceedings of the 28th International Conference on Computational Linguistics,0,"We propose a new semantic scheme for capturing predicate-argument relations for nominalizations, termed QANom. This scheme extends the QA-SRL formalism (He et al., 2015), modeling the relations between nominalizations and their arguments via natural language question-answer pairs. We construct the first QANom dataset using controlled crowdsourcing, analyze its quality and compare it to expertly annotated nominal-SRL annotations, as well as to other QA-driven annotations. In addition, we train a baseline QANom parser for identifying nominalizations and labeling their arguments with question-answer pairs. Finally, we demonstrate the extrinsic utility of our annotations for downstream tasks using both indirect supervision and zero-shot settings."
2020.acl-main.626,Controlled Crowdsourcing for High-Quality {QA}-{SRL} Annotation,2020,12,0,8,0,8858,paul roit,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"Question-answer driven Semantic Role Labeling (QA-SRL) was proposed as an attractive open and natural flavour of SRL, potentially attainable from laymen. Recently, a large-scale crowdsourced QA-SRL corpus and a trained parser were released. Trying to replicate the QA-SRL annotation for new texts, we found that the resulting annotations were lacking in quality, particularly in coverage, making them insufficient for further research and evaluation. In this paper, we present an improved crowdsourcing protocol for complex semantic annotation, involving worker selection and training, and a data consolidation phase. Applying this protocol to QA-SRL yielded high-quality annotation with drastically higher coverage, producing a new gold evaluation dataset. We believe that our annotation protocol and gold standard will facilitate future replicable research of natural semantic annotations."
W19-8635,Revisiting the Binary Linearization Technique for Surface Realization,2019,0,0,3,0.57971,23339,yevgeniy puzikov,Proceedings of the 12th International Conference on Natural Language Generation,0,"End-to-end neural approaches have achieved state-of-the-art performance in many natural language processing (NLP) tasks. Yet, they often lack transparency of the underlying decision-making process, hindering error analysis and certain model improvements. In this work, we revisit the binary linearization approach to surface realization, which exhibits more interpretable behavior, but was falling short in terms of prediction accuracy. We show how enriching the training data to better capture word order constraints almost doubles the performance of the system. We further demonstrate that encoding both local and global prediction contexts yields another considerable performance boost. With the proposed modifications, the system which ranked low in the latest shared task on multilingual surface realization now achieves best results in five out of ten languages, while being on par with the state-of-the-art approaches in others."
W19-8645,Improving Quality and Efficiency in Plan-based Neural Data-to-text Generation,2019,0,1,3,0,5096,amit moryossef,Proceedings of the 12th International Conference on Natural Language Generation,0,"We follow the step-by-step approach to neural data-to-text generation proposed by Moryossef et al (2019), in which the generation process is divided into a text planning stage followed by a plan realization stage. We suggest four extensions to that framework: (1) we introduce a trainable neural planning component that can generate effective plans several orders of magnitude faster than the original planner; (2) we incorporate typing hints that improve the model{'}s ability to deal with unseen relations and entities; (3) we introduce a verification-by-reranking stage that substantially improves the faithfulness of the resulting texts; (4) we incorporate a simple but effective referring expression generation module. These extensions result in a generation process that is faster, more fluent, and more accurate."
W19-2303,"How to Compare Summarizers without Target Length? Pitfalls, Solutions and Re-Examination of the Neural Summarization Literature",2019,0,4,3,0,4439,simeng sun,Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation,0,"We show that plain ROUGE F1 scores are not ideal for comparing current neural systems which on average produce different lengths. This is due to a non-linear pattern between ROUGE F1 and summary length. To alleviate the effect of length during evaluation, we have proposed a new method which normalizes the ROUGE F1 scores of a system by that of a random system with same average output length. A pilot human evaluation has shown that humans prefer short summaries in terms of the verbosity of a summary but overall consider longer summaries to be of higher quality. While human evaluations are more expensive in time and resources, it is clear that normalization, such as the one we proposed for automatic evaluation, will make human evaluations more meaningful."
W19-2013,Multi-Context Term Embeddings: the Use Case of Corpus-based Term Set Expansion,2019,27,0,4,1,2522,jonathan mamou,Proceedings of the 3rd Workshop on Evaluating Vector Space Representations for {NLP},0,"In this paper, we present a novel algorithm that combines multi-context term embeddings using a neural classifier and we test this approach on the use case of corpus-based term set expansion. In addition, we present a novel and unique dataset for intrinsic evaluation of corpus-based term set expansion algorithms. We show that, over this dataset, our algorithm provides up to 5 mean average precision points over the best baseline."
Q19-1027,Still a Pain in the Neck: Evaluating Text Representations on Lexical Composition,2019,63,0,2,1,954,vered shwartz,Transactions of the Association for Computational Linguistics,0,"Building meaningful phrase representations is challenging because phrase meanings are not simply the sum of their constituent meanings. Lexical composition can shift the meanings of the constituent words and introduce implicit information. We tested a broad range of textual representations for their capacity to address these issues. We found that, as expected, contextualized word representations perform better than static word embeddings, more so on detecting meaning shift than in recovering implicit information, in which their performance is still far from that of humans. Our evaluation suite, consisting of six tasks related to lexical composition effects, can serve future research aiming to improve representations."
P19-1213,Ranking Generated Summaries by Correctness: An Interesting but Challenging Application for Natural Language Inference,2019,0,16,4,0.47619,2917,tobias falke,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"While recent progress on abstractive summarization has led to remarkably fluent summaries, factual errors in generated summaries still severely limit their use in practice. In this paper, we evaluate summaries produced by state-of-the-art models via crowdsourcing and show that such errors occur frequently, in particular with more abstractive models. We study whether textual entailment predictions can be used to detect such errors and if they can be reduced by reranking alternative predicted summaries. That leads to an interesting downstream application for entailment models. In our experiments, we find that out-of-the-box entailment models trained on NLI datasets do not yet offer the desired performance for the downstream task and we therefore release our annotations as additional test data for future extrinsic evaluations of NLI."
P19-1409,Revisiting Joint Modeling of Cross-document Entity and Event Coreference Resolution,2019,27,0,6,0,25797,shany barhom,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"Recognizing coreferring events and entities across multiple texts is crucial for many NLP applications. Despite the task{'}s importance, research focus was given mostly to within-document entity coreference, with rather little attention to the other variants. We propose a neural architecture for cross-document coreference resolution. Inspired by Lee et al. (2012), we jointly model entity and event coreference. We represent an event (entity) mention using its lexical span, surrounding context, and relation to entity (event) mentions via predicate-arguments structures. Our model outperforms the previous state-of-the-art event coreference model on ECB+, while providing the first entity coreference results on this corpus. Our analysis confirms that all our representation elements, including the mention span itself, its context, and the relation to other mentions contribute to the model{'}s success."
N19-1072,Crowdsourcing Lightweight Pyramids for Manual Summary Evaluation,2019,0,3,8,1,3395,ori shapira,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",0,"Conducting a manual evaluation is considered an essential part of summary evaluation methodology. Traditionally, the Pyramid protocol, which exhaustively compares system summaries to references, has been perceived as very reliable, providing objective scores. Yet, due to the high cost of the Pyramid method and the required expertise, researchers resorted to cheaper and less thorough manual evaluation methods, such as Responsiveness and pairwise comparison, attainable via crowdsourcing. We revisit the Pyramid approach, proposing a lightweight sampling-based version that is crowdsourcable. We analyze the performance of our method in comparison to original expert-based Pyramid evaluations, showing higher correlation relative to the common Responsiveness method. We release our crowdsourced Summary-Content-Units, along with all crowdsourcing scripts, for future evaluations."
N19-1236,{S}tep-by-Step: {S}eparating Planning from Realization in Neural Data-to-Text Generation,2019,0,17,3,0,5096,amit moryossef,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",0,"Data-to-text generation can be conceptually divided into two parts: ordering and structuring the information (planning), and generating fluent language describing the information (realization). Modern neural generation systems conflate these two steps into a single end-to-end differentiable system. We propose to split the generation process into a symbolic text-planning stage that is faithful to the input, followed by a neural generation stage that focuses only on realization. For training a plan-to-text generator, we present a method for matching reference texts to their corresponding text plans. For inference time, we describe a method for selecting high-quality text plans for new inputs. We implement and evaluate our approach on the WebNLG benchmark. Our results demonstrate that decoupling text planning from neural realization indeed improves the system{'}s reliability and adequacy while maintaining fluent output. We observe improvements both in BLEU scores and in manual evaluations. Another benefit of our approach is the ability to output diverse realizations of the same input, paving the way to explicit control over the generated text structure."
K19-1019,Diversify Your Datasets: Analyzing Generalization via Controlled Variance in Adversarial Datasets,2019,25,0,4,1,952,ohad rozen,Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL),0,"Phenomenon-specific {``}adversarial{''} datasets have been recently designed to perform targeted stress-tests for particular inference types. Recent work (Liu et al., 2019a) proposed that such datasets can be utilized for training NLI and other types of models, often allowing to learn the phenomenon in focus and improve on the challenge dataset, indicating a {``}blind spot{''} in the original training data. Yet, although a model can improve in such a training process, it might still be vulnerable to other challenge datasets targeting the same phenomenon but drawn from a different distribution, such as having a different syntactic complexity level. In this work, we extend this method to drive conclusions about a model{'}s ability to learn and generalize a target phenomenon rather than to {``}learn{''} a dataset, by controlling additional aspects in the adversarial datasets. We demonstrate our approach on two inference phenomena {--} dative alternation and numerical reasoning, elaborating, and in some cases contradicting, the results of Liu et al.. Our methodology enables building better challenge datasets for creating more robust models, and may yield better model understanding and subsequent overarching improvements."
D19-3001,{ABSA}pp: A Portable Weakly-Supervised Aspect-Based Sentiment Extraction System,2019,0,0,5,1,11061,oren pereg,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): System Demonstrations,0,"We present ABSApp, a portable system for weakly-supervised aspect-based sentiment ex- traction. The system is interpretable and user friendly and does not require labeled training data, hence can be rapidly and cost-effectively used across different domains in applied setups. The system flow includes three stages: First, it generates domain-specific aspect and opinion lexicons based on an unlabeled dataset; second, it enables the user to view and edit those lexicons (weak supervision); and finally, it enables the user to select an unlabeled target dataset from the same domain, classify it, and generate an aspect-based sentiment report. ABSApp has been successfully used in a number of real-life use cases, among them movie review analysis and convention impact analysis."
D19-1307,Better Rewards Yield Better Summaries: Learning to Summarise Without References,2019,0,8,5,0,26953,florian bohm,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"Reinforcement Learning (RL)based document summarisation systems yield state-of-the-art performance in terms of ROUGE scores, because they directly use ROUGE as the rewards during training. However, summaries with high ROUGE scores often receive low human judgement. To find a better reward function that can guide RL to generate human-appealing summaries, we learn a reward function from human ratings on 2,500 summaries. Our reward function only takes the document and system summary as input. Hence, once trained, it can be used to train RL based summarisation systems without using any reference summaries. We show that our learned rewards have significantly higher correlation with human ratings than previous approaches. Human evaluation experiments show that, compared to the state-of-the-art supervised-learning systems and ROUGE-as-rewards RL summarisation systems, the RL systems using our learned rewards during training generate summaries with higher human ratings. The learned reward function and our source code are available at https://github.com/yg211/summary-reward-no-reference."
P18-1111,Paraphrase to Explicate: Revealing Implicit Noun-Compound Relations,2018,29,1,2,1,954,vered shwartz,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Revealing the implicit semantic relation between the constituents of a noun-compound is important for many NLP applications. It has been addressed in the literature either as a classification task to a set of pre-defined relations or by producing free text paraphrases explicating the relations. Most existing paraphrasing methods lack the ability to generalize, and have a hard time interpreting infrequent or new noun-compounds. We propose a neural model that generalizes better by representing paraphrases in a continuous space, generalizing for both unseen noun-compounds and rare paraphrases. Our model helps improving performance on both the noun-compound paraphrasing and classification tasks."
P18-1201,Zero-Shot Transfer Learning for Event Extraction,2018,0,11,4,0,9579,lifu huang,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Most previous supervised event extraction methods have relied on features derived from manual annotations, and thus cannot be applied to new event types without extra annotation effort. We take a fresh look at event extraction and model it as a generic grounding problem: mapping each event mention to a specific type in a target event ontology. We design a transferable architecture of structural and compositional neural networks to jointly represent and map event mentions and types into a shared semantic space. Based on this new framework, we can select, for each event mention, the event type which is semantically closest in this space as its type. By leveraging manual annotations available for a small set of existing event types, our framework can be applied to new unseen event types without additional manual annotations. When tested on 23 unseen event types, our zero-shot framework, without manual annotations, achieved performance comparable to a supervised model trained from 3,000 sentences annotated with 500 event mentions."
N18-2089,Crowdsourcing Question-Answer Meaning Representations,2018,0,15,4,0.539216,8370,julian michael,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",0,"We introduce Question-Answer Meaning Representations (QAMRs), which represent the predicate-argument structure of a sentence as a set of question-answer pairs. We develop a crowdsourcing scheme to show that QAMRs can be labeled with very little training, and gather a dataset with over 5,000 sentences and 100,000 questions. A qualitative analysis demonstrates that the crowd-generated question-answer pairs cover the vast majority of predicate-argument relationships in existing datasets (including PropBank, NomBank, and QA-SRL) along with many previously under-resourced ones, including implicit arguments and relations. We also report baseline models for question generation and answering, and summarize a recent approach for using QAMR labels to improve an Open IE system. These results suggest the freely available QAMR data and annotation scheme should support significant future work."
N18-1081,Supervised Open Information Extraction,2018,0,28,4,1,973,gabriel stanovsky,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",0,"We present data and methods that enable a supervised learning approach to Open Information Extraction (Open IE). Central to the approach is a novel formulation of Open IE as a sequence tagging problem, addressing challenges such as encoding multiple extractions for a predicate. We also develop a bi-LSTM transducer, extending recent deep Semantic Role Labeling models to extract Open IE tuples and provide confidence scores for tuning their precision-recall tradeoff. Furthermore, we show that the recently released Question-Answer Meaning Representation dataset can be automatically converted into an Open IE corpus which significantly increases the amount of available training data. Our supervised model outperforms the existing state-of-the-art Open IE systems on benchmark datasets."
L18-1229,Automatic Thesaurus Construction for {M}odern {H}ebrew,2018,0,0,2,1,1654,chaya liebeskind,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
D18-1087,Evaluating Multiple System Summary Lengths: A Case Study,2018,0,2,7,1,3395,ori shapira,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"Practical summarization systems are expected to produce summaries of varying lengths, per user needs. While a couple of early summarization benchmarks tested systems across multiple summary lengths, this practice was mostly abandoned due to the assumed cost of producing reference summaries of multiple lengths. In this paper, we raise the research question of whether reference summaries of a single length can be used to reliably evaluate system summaries of multiple lengths. For that, we have analyzed a couple of datasets as a case study, using several variants of the ROUGE metric that are standard in summarization evaluation. Our findings indicate that the evaluation protocol in question is indeed competitive. This result paves the way to practically evaluating varying-length summaries with simple, possibly existing, summarization benchmarks."
D18-1263,Semantics as a Foreign Language,2018,0,3,2,1,973,gabriel stanovsky,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"We propose a novel approach to semantic dependency parsing (SDP) by casting the task as an instance of multi-lingual machine translation, where each semantic representation is a different foreign dialect. To that end, we first generalize syntactic linearization techniques to account for the richer semantic dependency graph structure. Following, we design a neural sequence-to-sequence framework which can effectively recover our graph linearizations, performing almost on-par with previous SDP state-of-the-art while requiring less parallel training annotations. Beyond SDP, our linearization technique opens the door to integration of graph-based semantic representations as features in neural models for downstream applications."
C18-2013,{S}et{E}xpander: End-to-end Term Set Expansion Based on Multi-Context Term Embeddings,2018,0,0,4,1,2522,jonathan mamou,Proceedings of the 27th International Conference on Computational Linguistics: System Demonstrations,0,"We present SetExpander, a corpus-based system for expanding a seed set of terms into a more complete set of terms that belong to the same semantic class. SetExpander implements an iterative end-to end workflow for term set expansion. It enables users to easily select a seed set of terms, expand it, view the expanded set, validate it, re-expand the validated set and store it, thus simplifying the extraction of domain-specific fine-grained semantic classes. SetExpander has been used for solving real-life use cases including integration in an automated recruitment system and an issues and defects resolution system. A video demo of SetExpander is available at https://drive.google.com/open?id=1e545bB87Autsch36DjnJHmq3HWfSd1Rv ."
W17-6927,Neural Disambiguation of Causal Lexical Markers Based on Context,2017,14,2,4,0,27775,eugenio martinezcamara,{IWCS} 2017 {---} 12th International Conference on Computational Semantics {---} Short papers,0,"We propose a neural network architecture for the task of causality classification. We claim that the encoding of the meaning of a sentence is required for the disambiguation of its causal meaning. Our results show that our claim holds, and we outperform the state-of-the-art."
W17-0902,A Consolidated Open Knowledge Representation for Multiple Texts,2017,53,4,10,0,32101,rachel wities,"Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics",0,"We propose to move from Open Information Extraction (OIE) ahead to Open Knowledge Representation (OKR), aiming to represent information conveyed jointly in a set of texts in an open text-based manner. We do so by consolidating OIE extractions using entity and predicate coreference, while modeling information containment between coreferring elements via lexical entailment. We suggest that generating OKR structures can be a useful step in the NLP pipeline, to give semantic applications an easy handle on consolidated information across multiple texts."
S17-1019,Acquiring Predicate Paraphrases from News Tweets,2017,11,3,3,1,954,vered shwartz,Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*{SEM} 2017),0,"We present a simple method for ever-growing extraction of predicate paraphrases from news headlines in Twitter. Analysis of the output of ten weeks of collection shows that the accuracy of paraphrases with different support levels is estimated between 60-86{\%}. We also demonstrate that our resource is to a large extent complementary to existing resources, providing many novel paraphrases. Our resource is publicly available, continuously expanding based on daily news."
P17-2056,Integrating Deep Linguistic Features in Factuality Prediction over Unified Datasets,2017,0,11,4,1,973,gabriel stanovsky,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Previous models for the assessment of commitment towards a predicate in a sentence (also known as factuality prediction) were trained and tested against a specific annotated dataset, subsequently limiting the generality of their results. In this work we propose an intuitive method for mapping three previously annotated corpora onto a single factuality scale, thereby enabling models to be tested across these corpora. In addition, we design a novel model for factuality prediction by first extending a previous rule-based factuality prediction system and applying it over an abstraction of dependency trees, and then using the output of this system in a supervised classifier. We show that this model outperforms previous methods on all three datasets. We make both the unified factuality corpus and our new model publicly available."
D17-2019,Interactive Abstractive Summarization for Event News Tweets,2017,7,6,6,1,3395,ori shapira,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,0,"We present a novel interactive summarization system that is based on abstractive summarization, derived from a recent consolidated knowledge representation for multiple texts. We incorporate a couple of interaction mechanisms, providing a bullet-style summary while allowing to attain the most important information first and interactively drill down to more specific details. A usability study of our implementation, for event news tweets, suggests the utility of our approach for text exploration."
D17-1198,A Simple Language Model based on {PMI} Matrix Approximations,2017,12,1,2,1,24756,oren melamud,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,0,"In this study, we introduce a new approach for learning language models by training them to estimate word-context pointwise mutual information (PMI), and then deriving the desired conditional probabilities from PMI at test time. Specifically, we show that with minor modifications to word2vec{'}s algorithm, we get principled language models that are closely related to the well-established Noise Contrastive Estimation (NCE) based language models. A compelling aspect of our approach is that our models are trained with the same simple negative sampling objective function that is commonly used in word2vec to learn word embeddings."
W16-5304,Path-based vs. Distributional Information in Recognizing Lexical Semantic Relations,2016,23,7,2,1,954,vered shwartz,Proceedings of the 5th Workshop on Cognitive Aspects of the Lexicon ({C}og{AL}ex - V),0,"Recognizing various semantic relations between terms is beneficial for many NLP tasks. While path-based and distributional information sources are considered complementary for this task, the superior results the latter showed recently suggested that the former{'}s contribution might have become obsolete. We follow the recent success of an integrated neural method for hypernymy detection (Shwartz et al., 2016) and extend it to recognize multiple relations. The empirical results show that this method is effective in the multiclass setting as well. We further show that the path-based information source always contributes to the classification, and analyze the cases in which it mostly complements the distributional information."
W16-5310,{C}og{AL}ex-{V} Shared Task: {L}ex{NET} - Integrated Path-based and Distributional Method for the Identification of Semantic Relations,2016,24,3,2,1,954,vered shwartz,Proceedings of the 5th Workshop on Cognitive Aspects of the Lexicon ({C}og{AL}ex - V),0,"We present a submission to the CogALex 2016 shared task on the corpus-based identification of semantic relations, using LexNET (Shwartz and Dagan, 2016), an integrated path-based and distributional method for semantic relation classification. The reported results in the shared task bring this submission to the third place on subtask 1 (word relatedness), and the first place on subtask 2 (semantic relation classification), demonstrating the utility of integrating the complementary path-based and distributional information sources in recognizing concrete semantic relations. Combined with a common similarity measure, LexNET performs fairly good on the word relatedness task (subtask 1). The relatively low performance of LexNET and all other systems on subtask 2, however, confirms the difficulty of the semantic relation classification task, and stresses the need to develop additional methods for this task."
S16-2013,Adding Context to Semantic Data-Driven Paraphrasing,2016,23,2,2,1,954,vered shwartz,Proceedings of the Fifth Joint Conference on Lexical and Computational Semantics,0,"Recognizing lexical inferences between pairs of terms is a common task in NLP applications, which should typically be performed within a given context. Such context-sensitive inferences have to consider both term meaning in context as well as the fine-grained relation holding between the terms. Hence, to develop suitable lexical inference methods, we need datasets that are annotated with fine-grained semantic relations in-context. Since existing datasets either provide outof-context annotations or refer to coarsegrained relations, we propose a methodology for adding context-sensitive annotations. We demonstrate our methodology by applying it to phrase pairs from PPDB 2.0, creating a novel dataset of finegrained lexical inferences in-context and showing its utility in developing contextsensitive methods."
P16-2041,Annotating Relation Inference in Context via Question Answering,2016,30,3,2,1,3267,omer levy,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,None
P16-2077,Specifying and Annotating Reduced Argument Span Via {QA}-{SRL},2016,9,2,2,1,973,gabriel stanovsky,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Prominent semantic annotations take an inclusive approach to argument span annotation, marking arguments as full constituency subtrees. Some works, however, showed that identifying a reduced argument span can be beneficial for various semantic tasks. While certain practical methods do extract reduced argument spans, such as in Open-IE , these solutions are often ad-hoc and system-dependent, with no commonly accepted standards. In this paper we propose a generic argument reduction criterion, along with an annotation procedure, and show that it can be consistently and intuitively annotated using the recent QA-SRL paradigm."
P16-1119,Annotating and Predicting Non-Restrictive Noun Phrase Modifications,2016,22,0,2,1,973,gabriel stanovsky,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"The distinction between restrictive and non-restrictive modification in noun phrases is a well studied subject in linguistics. Automatically identifying non-restrictive modifiers can provide NLP applications with shorter, more salient arguments, which were found beneficial by several recent works. While previous work showed that restrictiveness can be annotated with high agreement, no large scale corpus was created, hindering the development of suitable classification algorithms. In this work we devise a novel crowdsourcing annotation methodology, and an accompanying large scale corpus. Then, we present a robust automated system which identifies non-restrictive modifiers, notably improving over prior methods."
P16-1226,Improving Hypernymy Detection with an Integrated Path-based and Distributional Method,2016,33,4,3,1,954,vered shwartz,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Detecting hypernymy relations is a key task in NLP, which is addressed in the literature using two complementary approaches. Distributional methods, whose supervised variants are the current best performers, and path-based methods, which received less research attention. We suggest an improved path-based algorithm, in which the dependency paths are encoded using a recurrent neural network, that achieves results comparable to distributional methods. We then extend the approach to integrate both path-based and distributional signals, significantly improving upon the state-of-the-art on this task."
L16-1501,The Negochat Corpus of Human-agent Negotiation Dialogues,2016,11,2,4,0,35222,vasily konovalov,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Annotated in-domain corpora are crucial to the successful development of dialogue systems of automated agents, and in particular for developing natural language understanding (NLU) components of such systems. Unfortunately, such important resources are scarce. In this work, we introduce an annotated natural language human-agent dialogue corpus in the negotiation domain. The corpus was collected using Amazon Mechanical Turk following the {`}Wizard-Of-Oz{'} approach, where a {`}wizard{'} human translates the participants{'} natural language utterances in real time into a semantic language. Once dialogue collection was completed, utterances were annotated with intent labels by two independent annotators, achieving high inter-annotator agreement. Our initial experiments with an SVM classifier show that automatically inferring such labels from the utterances is far from trivial. We make our corpus publicly available to serve as an aid in the development of dialogue systems for negotiation agents, and suggest that analogous corpora can be created following our methodology and using our available source code. To the best of our knowledge this is the first publicly available negotiation dialogue corpus."
K16-1006,context2vec: Learning Generic Context Embedding with Bidirectional {LSTM},2016,14,145,3,1,24756,oren melamud,Proceedings of The 20th {SIGNLL} Conference on Computational Natural Language Learning,0,None
D16-1086,Porting an Open Information Extraction System from {E}nglish to {G}erman,2016,13,8,4,0.47619,2917,tobias falke,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,"Many downstream NLP tasks can benefit from Open Information Extraction (Open IE) as a semantic representation. While Open IE systems are available for English, many other languages lack such tools. In this paper, we present a straightforward approach for adapting PropS, a rule-based predicate-argument analysis for English, to a new language, German. With this approach, we quickly obtain an Open IE system for German covering 89% of the English rule set. It yields 1.6 extractions per sentence with 60% precision, making it readily usable in downstream applications."
D16-1252,Creating a Large Benchmark for Open Information Extraction,2016,17,37,2,1,973,gabriel stanovsky,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,None
C16-1272,Modeling Extractive Sentence Intersection via Subtree Entailment,2016,25,0,2,1,3267,omer levy,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"Sentence intersection captures the semantic overlap of two texts, generalizing over paradigms such as textual entailment and semantic text similarity. Despite its modeling power, it has received little attention because it is difficult for non-experts to annotate. We analyze 200 pairs of similar sentences and identify several underlying properties of sentence intersection. We leverage these insights to design an algorithm that decomposes the sentence intersection task into several simpler annotation tasks, facilitating the construction of a high quality dataset via crowdsourcing. We implement this approach and provide an annotated dataset of 1,764 sentence intersections."
W15-3714,Integrating Query Performance Prediction in Term Scoring for Diachronic Thesaurus,2015,29,0,2,1,1654,chaya liebeskind,"Proceedings of the 9th {SIGHUM} Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities ({L}a{T}e{CH})",0,"A diachronic thesaurus is a lexical resource that aims to map between modern terms and their semantically related terms in earlier periods. In this paper, we investigate the task of collecting a list of relevant modern target terms for a domain-specific diachronic thesaurus. We propose a supervised learning scheme, which integrates features from two closely related fields: Terminology Extraction and Query Performance Prediction (QPP). Our method further expands modern candidate terms with ancient related terms, before assessing their corpus relevancy with QPP measures. We evaluate the empirical benefit of our method for a thesaurus for a diachronic Jewish corpus."
W15-1501,A Simple Word Embedding Model for Lexical Substitution,2015,18,41,3,1,24756,oren melamud,Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing,0,"The lexical substitution task requires identifying meaning-preserving substitutes for a target word instance in a given sentential context. Since its introduction in SemEval-2007, various models addressed this challenge, mostly in an unsupervised setting. In this work we propose a simple model for lexical substitution, which is based on the popular skip-gram word embedding model. The novelty of our approach is in leveraging explicitly the context embeddings generated within the skip-gram model, which were so far considered only as an internal component of the learning process. Our model is efficient, very simple to implement, and at the same time achieves state-ofthe-art results on lexical substitution tasks in an unsupervised setting."
S15-1022,Multi-Level Alignments As An Extensible Representation Basis for Textual Entailment Algorithms,2015,20,9,4,0,37318,taegil noh,Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics,0,"A major problem in research on Textual Entailment (TE) is the high implementation effort for TE systems. Recently, interoperable standards for annotation and preprocessing have been proposed. In contrast, the algorithmic level remains unstandardized, which makes component re-use in this area very difficult in practice. In this paper, we introduce multi-level alignments as a central, powerful representation for TE algorithms that encourages modular, reusable, multilingual algorithm development. We demonstrate that a pilot open-source implementation of multi-level alignment with minimal features competes with state-of-theart open-source TE engines in three languages."
Q15-1016,Improving Distributional Similarity with Lessons Learned from Word Embeddings,2015,30,571,3,1,3267,omer levy,Transactions of the Association for Computational Linguistics,0,"Recent trends suggest that neural-network-inspired word embedding models outperform traditional count-based distributional models on word similarity and analogy detection tasks. We reveal that much of the performance gains of word embeddings are due to certain system design choices and hyperparameter optimizations, rather than the embedding algorithms themselves. Furthermore, we show that these modifications can be transferred to traditional distributional models, yielding similar gains. In contrast to prior reports, we observe mostly local or insignificant performance differences between the methods, with no global advantage to any single approach over the others."
P15-2050,Open {IE} as an Intermediate Structure for Semantic Tasks,2015,26,31,2,1,973,gabriel stanovsky,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"Semantic applications typically extract information from intermediate structures derived from sentences, such as dependency parse or semantic role labeling. In this paper, we study Open Information Extractionxe2x80x99s (Open IE) output as an additional intermediate structure and find that for tasks such as text comprehension, word similarity and word analogy it can be very effective. Specifically, for word analogy, Open IE-based embeddings surpass the state of the art. We suggest that semantic applications will likely benefit from adding Open IE format to their set of potential sentencelevel structures."
P15-2061,Seed-Based Event Trigger Labeling: How far can event descriptions get us?,2015,22,16,2,0,37431,ofer bronstein,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"The task of event trigger labeling is typically addressed in the standard supervised setting: triggers for each target event type are annotated as training data, based on annotation guidelines. We propose an alternative approach, which takes the example trigger terms mentioned in the guidelines as seeds, and then applies an eventindependent similarity-based classifier for trigger labeling. This way we can skip manual annotation for new event types, while requiring only minimal annotated training data for few example events at system setup. Our method is evaluated on the ACE-2005 dataset, achieving 5.7% F1 improvement over a state-of-the-art supervised system which uses the full training data."
N15-1050,Modeling Word Meaning in Context with Substitute Vectors,2015,27,20,2,1,24756,oren melamud,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Context representations are a key element in distributional models of word meaning. In contrast to typical representations based on neighboring words, a recently proposed approach suggests to represent a context of a target word by a substitute vector, comprising the potential fillers for the target word slot in that context. In this work we first propose a variant of substitute vectors, which we find particularly suitable for measuring context similarity. Then, we propose a novel model for representing word meaning in context based on this context representation. Our model outperforms state-of-the-art results on lexical substitution tasks in an unsupervised setting."
N15-1098,Do Supervised Distributional Methods Really Learn Lexical Inference Relations?,2015,27,100,4,1,3267,omer levy,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Distributional representations of words have been recently used in supervised settings for recognizing lexical inference relations between word pairs, such as hypernymy and entailment. We investigate a collection of these state-of-the-art methods, and show that they do not actually learn a relation between two words. Instead, they learn an independent property of a single word in the pair: whether that word is a xe2x80x9cprototypical hypernymxe2x80x9d."
K15-1018,Learning to Exploit Structured Resources for Lexical Inference,2015,31,10,3,1,954,vered shwartz,Proceedings of the Nineteenth Conference on Computational Natural Language Learning,0,"Massive knowledge resources, such as Wikidata, can provide valuable information for lexical inference, especially for proper-names. Prior resource-based approaches typically select the subset of each resourcexe2x80x99s relations which are relevant for a particular given task. The selection process is done manually, limiting these approaches to smaller resources such as WordNet, which lacks coverage of propernames and recent terminology. This paper presents a supervised framework for automatically selecting an optimized subset of resource relations for a given target inference task. Our approach enables the use of large-scale knowledge resources, thus providing a rich source of high-precision inferences over proper-names. 1"
J15-2003,Efficient Global Learning of Entailment Graphs,2015,62,3,3,0.95776,874,jonathan berant,Computational Linguistics,0,"Entailment rules between predicates are fundamental to many semantic-inference applications. Consequently, learning such rules has been an active field of research in recent years. Methods for learning entailment rules between predicates that take into account dependencies between different rules e.g., entailment is a transitive relation have been shown to improve rule quality, but suffer from scalability issues, that is, the number of predicates handled is often quite small. In this article, we present methods for learning transitive graphs that contain tens of thousands of nodes, where nodes represent predicates and edges correspond to entailment rules termed entailment graphs. Our methods are able to scale to a large number of predicates by exploiting structural properties of entailment graphs such as the fact that they exhibit a tree-like property. We apply our methods on two data sets and demonstrate that our methods find high-quality solutions faster than methods proposed in the past, and moreover our methods for the first time scale to large graphs containing 20,000 nodes and more than 100,000 edges."
W14-4504,Proposition Knowledge Graphs,2014,-1,-1,3,1,973,gabriel stanovsky,Proceedings of the First {AHA}!-Workshop on Information Discovery in Text,0,None
W14-2413,Intermediary Semantic Representation through Proposition Structures,2014,15,3,3,1,973,gabriel stanovsky,Proceedings of the {ACL} 2014 Workshop on Semantic Parsing,0,"We propose an intermediary-level semantic representation, providing a higher level of abstraction than syntactic parse trees, while not committing to decisions in cases such as quantification, grounding or verbspecific roles assignments. The proposal is centered around the proposition structure of the text, and includes also implicit propositions which can be inferred from the syntax but are not transparent in parse trees, such as copular relations introduced by appositive constructions. Other benefits over dependency-trees are explicit marking of logical relations between propositions, explicit marking of multiword predicate such as light-verbs, and a consistent representation for syntacticallydifferent but semantically-similar structures. The representation is meant to serve as a useful input layer for semanticoriented applications, as well as to provide a better starting point for further levels of semantic analysis such as semantic-rolelabeling and semantic-parsing."
W14-1610,Focused Entailment Graphs for Open {IE} Propositions,2014,22,27,2,1,3267,omer levy,Proceedings of the Eighteenth Conference on Computational Natural Language Learning,0,"Open IE methods extract structured propositions from text. However, these propositions are neither consolidated nor generalized, and querying them may lead to insufficient or redundant information. This work suggests an approach to organize open IE propositions using entailment graphs. The entailment relation unifies equivalent propositions and induces a specific-to-general structure. We create a large dataset of gold-standard proposition entailment graphs, and provide a novel algorithm for automatically constructing them. Our analysis shows that predicate entailment is extremely context-sensitive, and that current lexical-semantic resources do not capture many of the lexical inferences induced by proposition entailment."
W14-1619,Probabilistic Modeling of Joint-context in Distributional Similarity,2014,32,9,2,1,24756,oren melamud,Proceedings of the Eighteenth Conference on Computational Natural Language Learning,0,"Most traditional distributional similarity models fail to capture syntagmatic patterns that group together multiple word features within the same joint context. In this work we introduce a novel generic distributional similarity scheme under which the power of probabilistic models can be leveraged to effectively model joint contexts. Based on this scheme, we implement a concrete model which utilizes probabilistic n-gram language models. Our evaluations suggest that this model is particularly wellsuited for measuring similarity for verbs, which are known to exhibit richer syntagmatic patterns, while maintaining comparable or better performance with respect to competitive baselines for nouns. Following this, we propose our scheme as a framework for future semantic similarity models leveraging the substantial body of work that exists in probabilistic language modeling."
P14-5008,The Excitement Open Platform for Textual Inferences,2014,13,42,3,0,1501,bernardo magnini,Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations,0,"This paper presents the Excitement Open Platform (EOP), a generic architecture and a comprehensive implementation for textual inference in multiple languages. The platform includes state-of-art algorithms, a large number of knowledge resources, and facilities for experimenting and testing innovative approaches. The EOP is distributed as an open source software."
P14-2120,Recognizing Implied Predicate-Argument Relationships in Textual Inference,2014,19,11,2,1,39101,asher stern,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"We investigate recognizing implied predicate-argument relationships which are not explicitly expressed in syntactic structure. While prior works addressed such relationships as an extension to semantic role labeling, our work investigates them in the context of textual inference scenarios. Such scenarios provide prior information, which substantially eases the task. We provide a large and freely available evaluation dataset for our task setting, and propose methods to cope with it, while obtaining promising results in empirical evaluations. 1 Motivation and Task"
2014.lilt-9.2,The {BIUTTE} Research Platform for Transformation-based Textual Entailment Recognition,2014,34,4,2,1,39101,asher stern,"Linguistic Issues in Language Technology, Volume 9, 2014 - Perspectives on Semantic Representations for Textual Inference",0,"Recent progress in research of the Recognizing Textual Entailment (RTE) task shows a constantly-increasing level of complexity in this research field. A way to avoid having this complexity becoming a barrier for researchers, especially for new-comers in the field, is to provide a freely available RTE system with a high level of flexibility and extensibility. In this paper, we introduce our RTE system, BiuTee2, and suggest it as an effective research framework for RTE. In particular, BiuTee follows the prominent transformation-based paradigm for RTE, and offers an accessible platform for research within this approach. We describe each of BiuTee{'}s components and point out the mechanisms and properties which directly support adaptations and integration of new components. In addition, we describe BiuTee{'}s visual tracing tool, which provides notable assistance for researchers in refining and {``}debugging{''} their knowledge resources and inference components."
W13-3805,Entailment graphs for text exploration,2013,0,2,1,1,955,ido dagan,Proceedings of the Joint Symposium on Semantic Processing. Textual Inference and Structures in Corpora,0,None
W13-2704,Semi-automatic Construction of Cross-period Thesaurus,2013,0,4,2,1,1654,chaya liebeskind,"Proceedings of the 7th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities",0,None
S13-2045,{S}em{E}val-2013 Task 7: The Joint Student Response Analysis and 8th Recognizing Textual Entailment Challenge,2013,26,71,8,0,41055,myroslava dzikovska,"Second Joint Conference on Lexical and Computational Semantics (*{SEM}), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation ({S}em{E}val 2013)",0,"We present the results of the Joint Student Response Analysis and 8th Recognizing Textual Entailment Challenge, aiming to bring together researchers in educational NLP technology and textual entailment. The task of giving feedback on student answers requires semantic inference and therefore is related to recognizing textual entailment. Thus, we offered to the community a 5-way student response labeling task, as well as 3-way and 2way RTE-style tasks on educational data. In addition, a partial entailment task was piloted. We present and compare results from 9 participating teams, and discuss future directions."
S13-2048,{UKP}-{BIU}: Similarity and Entailment Metrics for Student Response Analysis,2013,17,11,3,1,3267,omer levy,"Second Joint Conference on Lexical and Computational Semantics (*{SEM}), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation ({S}em{E}val 2013)",0,"Our system combines text similarity measures with a textual entailment system. In the main task, we focused on the influence of lexicalized versus unlexicalized features, and how they affect performance on unseen questions and domains. We also participated in the pilot partial entailment task, where our system significantly outperforms a strong baseline."
P13-4017,{PLIS}: a Probabilistic Lexical Inference System,2013,14,1,4,1,19731,eyal shnarch,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations,0,"This paper presents PLIS, an open source Probabilistic Lexical Inference System which combines two functionalities: (i) a tool for integrating lexical inference knowledge from diverse resources, and (ii) a framework for scoring textual inferences based on the integrated knowledge. We provide PLIS with two probabilistic implementation of this framework. PLIS is available for download and developers of text processing applications can use it as an off-the-shelf component for injecting lexical knowledge into their applications. PLIS is easily configurable, components can be extended or replaced with user generated ones to enable system customization and further research. PLIS includes an online interactive viewer, which is a powerful tool for investigating lexical inference processes."
P13-2051,Using Lexical Expansion to Learn Inference Rules from Sparse Data,2013,19,4,2,1,24756,oren melamud,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Automatic acquisition of inference rules for predicates is widely addressed by computing distributional similarity scores between vectors of argument words. In this scheme, prior work typically refrained from learning rules for low frequency predicates associated with very sparse argument vectors due to expected low reliability. To improve the learning of such rules in an unsupervised way, we propose to lexically expand sparse argument word vectors with semantically similar words. Our evaluation shows that lexical expansion significantly improves performance in comparison to state-of-the-art baselines."
P13-2080,Recognizing Partial Textual Entailment,2013,12,17,3,1,3267,omer levy,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Textual entailment is an asymmetric relation between two text fragments that describes whether one fragment can be inferred from the other. It thus cannot capture the notion that the target fragment is xe2x80x9calmost entailedxe2x80x9d by the given text. The recently suggested idea of partial textual entailment may remedy this problem. We investigate partial entailment under the faceted entailment model and the possibility of adapting existing textual entailment methods to this setting. Indeed, our results show that these methods are useful for rec- ognizing partial entailment. We also provide a preliminary assessment of how partial entailment may be used for recognizing (complete) textual entailment."
P13-1131,A Two Level Model for Context Sensitive Inference Rules,2013,32,17,3,1,24756,oren melamud,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Automatic acquisition of inference rules for predicates has been commonly addressed by computing distributional similarity between vectors of argument words, operating at the word space level. A recent line of work, which addresses context sensitivity of rules, represented contexts in a latent topic space and computed similarity over topic vectors. We propose a novel two-level model, which computes similarities between word-level vectors that are biased by topic-level context representations. Evaluations on a naturallydistributed dataset show that our model significantly outperforms prior word-level and topic-level models. We also release a first context-sensitive inference rule set."
N13-1091,{T}ruth{T}eller: Annotating Predicate Truth,2013,9,21,3,0,41613,amnon lotan,Proceedings of the 2013 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"We propose a novel semantic annotation type of assigning truth values to predicate occurrences, and present TruthTeller, a standalone publiclyavailable tool that produces such annotations. TruthTeller integrates a range of semantic phenomena, such as negation, modality, presupposition, implicativity, and more, which were dealt only partly in previous works. Empirical evaluations against human annotations show satisfactory results and suggest the usefulness of this new type of tool for NLP."
2013.mtsummit-european.5,Bridges Across the Language Divide {---} {EU}-{BRIDGE} Excitement: Exploring Customer Interactions through Textual {E}ntail{MENT},2013,-1,-1,1,1,955,ido dagan,Proceedings of Machine Translation Summit XIV: European projects,0,None
2013.mtsummit-european.6,Excitement: Exploring Customer Interactions through Textual {E}ntail{MENT},2013,-1,-1,1,1,955,ido dagan,Proceedings of Machine Translation Summit XIV: European projects,0,None
S12-1005,Sentence Clustering via Projection over Term Clusters,2012,25,3,2,1,24131,lili kotlerman,"*{SEM} 2012: The First Joint Conference on Lexical and Computational Semantics {--} Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation ({S}em{E}val 2012)",0,"This paper presents a novel sentence clustering scheme based on projecting sentences over term clusters. The scheme incorporates external knowledge to overcome lexical variability and small corpus size, and outperforms common sentence clustering methods on two real-life industrial datasets."
S12-1009,Statistical Thesaurus Construction for a Morphologically Rich Language,2012,29,5,2,1,1654,chaya liebeskind,"*{SEM} 2012: The First Joint Conference on Lexical and Computational Semantics {--} Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation ({S}em{E}val 2012)",0,"Corpus-based thesaurus construction for Morphologically Rich Languages (MRL) is a complex task, due to the morphological variability of MRL. In this paper we explore alternative term representations, complemented by clustering of morphological variants. We introduce a generic algorithmic scheme for thesaurus construction in MRL, and demonstrate the empirical benefit of our methodology for a Hebrew thesaurus."
S12-1032,A Probabilistic Lexical Model for Ranking Textual Inferences,2012,21,3,2,1,19731,eyal shnarch,"*{SEM} 2012: The First Joint Conference on Lexical and Computational Semantics {--} Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation ({S}em{E}val 2012)",0,"Identifying textual inferences, where the meaning of one text follows from another, is a general underlying task within many natural language applications. Commonly, it is approached either by generative syntactic-based methods or by lightweight heuristic lexical models. We suggest a model which is confined to simple lexical information, but is formulated as a principled generative probabilistic model. We focus our attention on the task of ranking textual inferences and show substantially improved results on a recently investigated question answering data set."
P12-3013,{BIUTEE}: A Modular Open-Source System for Recognizing Textual Entailment,2012,17,21,2,1,39101,asher stern,Proceedings of the {ACL} 2012 System Demonstrations,0,"This paper introduces BiuTee, an open-source system for recognizing textual entailment. Its main advantages are its ability to utilize various types of knowledge resources, and its extensibility by which new knowledge resources and inference components can be easily integrated. These abilities make BiuTee an appealing RTE system for two research communities: (1) researchers of end applications, that can benefit from generic textual inference, and (2) RTE researchers, who can integrate their novel algorithms and knowledge resources into our system, saving the time and effort of developing a complete RTE system from scratch. Notable assistance for these researchers is provided by a visual tracing tool, by which researchers can refine and debug their knowledge resources and inference components."
P12-3014,Entailment-based Text Exploration with Application to the Health-care Domain,2012,12,8,3,0.864276,32102,meni adler,Proceedings of the {ACL} 2012 System Demonstrations,0,"We present a novel text exploration model, which extends the scope of state-of-the-art technologies by moving from standard concept-based exploration to statement-based exploration. The proposed scheme utilizes the textual entailment relation between statements as the basis of the exploration process. A user of our system can explore the result space of a query by drilling down/up from one statement to another, according to entailment relations specified by an entailment graph and an optional concept taxonomy. As a prominent use case, we apply our exploration system and illustrate its benefit on the health-care domain. To the best of our knowledge this is the first implementation of an exploration system at the statement level that is based on the textual entailment relation."
P12-2031,Crowdsourcing Inference-Rule Evaluation,2012,20,29,3,0,42655,naomi zeichner,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"The importance of inference rules to semantic applications has long been recognized and extensive work has been carried out to automatically acquire inference-rule resources. However, evaluating such resources has turned out to be a non-trivial task, slowing progress in the field. In this paper, we suggest a framework for evaluating inference-rule resources. Our framework simplifies a previously proposed instance-based evaluation method that involved substantial annotator training, making it suitable for crowdsourcing. We show that our method produces a large amount of annotations with high inter-annotator agreement for a low cost at a short period of time, without requiring training expert annotators."
P12-1013,Efficient Tree-based Approximation for Entailment Graph Learning,2012,27,14,2,1,874,jonathan berant,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Learning entailment rules is fundamental in many semantic-inference applications and has been an active field of research in recent years. In this paper we address the problem of learning transitive graphs that describe entailment rules between predicates (termed entailment graphs). We first identify that entailment graphs exhibit a tree-like property and are very similar to a novel type of graph termed forest-reducible graph. We utilize this property to develop an iterative efficient approximation algorithm for learning the graph edges, where each iteration takes linear time. We compare our approximation algorithm to a recently-proposed state-of-the-art exact algorithm and show that it is more efficient and scalable both theoretically and empirically, while its output quality is close to that given by the optimal solution of the exact algorithm."
P12-1030,Efficient Search for Transformation-based Inference,2012,30,12,3,1,39101,asher stern,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"This paper addresses the search problem in textual inference, where systems need to infer one piece of text from another. A prominent approach to this task is attempts to transform one text into the other through a sequence of inference-preserving transformations, a.k.a. a proof, while estimating the proof's validity. This raises a search challenge of finding the best possible proof. We explore this challenge through a comprehensive investigation of prominent search algorithms and propose two novel algorithmic components specifically designed for textual inference: a gradient-style evaluation function, and a local-lookahead node expansion method. Evaluations, using the open-source system, BiuTee, show the contribution of these ideas to search efficiency and proof quality."
J12-1003,Learning Entailment Relations by Global Graph Structure Optimization,2012,62,28,2,1,874,jonathan berant,Computational Linguistics,0,"Identifying entailment relations between predicates is an important part of applied semantic inference. In this article we propose a global inference algorithm that learns such entailment rules. First, we define a graph structure over predicates that represents entailment relations as directed edges. Then, we use a global transitivity constraint on the graph to learn the optimal set of edges, formulating the optimization problem as an Integer Linear Program. The algorithm is applied in a setting where, given a target concept, the algorithm learns on the fly all entailment rules between predicates that co-occur with this concept. Results show that our global algorithm improves performance over baseline algorithms by more than 10%."
D12-1018,Learning Verb Inference Rules from Linguistically-Motivated Evidence,2012,36,20,4,0,43615,hila weisman,Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,0,"Learning inference relations between verbs is at the heart of many semantic applications. However, most prior work on learning such rules focused on a rather narrow set of information sources: mainly distributional similarity, and to a lesser extent manually constructed verb co-occurrence patterns. In this paper, we claim that it is imperative to utilize information from various textual scopes: verb co-occurrence within a sentence, verb co-occurrence within a document, as well as overall corpus statistics. To this end, we propose a much richer novel set of linguistically motivated cues for detecting entailment between verbs and combine them as features in a supervised classification framework. We empirically demonstrate that our model significantly outperforms previous methods and that information from each textual scope contributes to the verb entailment learning task."
W11-2402,Towards a Probabilistic Model for Lexical Entailment,2011,26,5,3,1,19731,eyal shnarch,Proceedings of the {T}ext{I}nfer 2011 Workshop on Textual Entailment,0,"While modeling entailment at the lexical-level is a prominent task, addressed by most textual entailment systems, it has been approached mostly by heuristic methods, neglecting some of its important aspects. We present a probabilistic approach for this task which covers aspects such as differentiating various resources by their reliability levels, considering the length of the entailed sentence, the number of its covered terms and the existence of multiple evidence for the entailment of a term. The impact of our model components is validated by evaluations, which also show that its performance is in line with the best published entailment systems."
W11-2403,Classification-based Contextual Preferences,2011,24,1,2,1,27123,shachar mirkin,Proceedings of the {T}ext{I}nfer 2011 Workshop on Textual Entailment,0,This paper addresses context matching in textual inference. We formulate the task under the Contextual Preferences framework which broadly captures contextual aspects of inference. We propose a generic classification-based scheme under this framework which coherently attends to context matching in inference and may be employed in any inference-based task. As a test bed for our scheme we use the Name-based Text Categorization (TC) task. We define an integration of Contextual Preferences into the TC setting and present a concrete self-supervised model which instantiates the generic scheme and is applied to address context matching in the TC task. Experiments on standard TC datasets show that our approach outperforms the state of the art in context modeling for Name-based TC.
R11-1063,A Confidence Model for Syntactically-Motivated Entailment Proofs,2011,24,30,2,1,39101,asher stern,Proceedings of the International Conference Recent Advances in Natural Language Processing 2011,0,"This paper presents a novel method for recognizing textual entailment which derives the hypothesis from the text through a sequence of parse tree transformations. Unlike related approaches based on tree-edit-distance, we employ transformations which better capture linguistic structures of entailment. This is achieved by (a) extending an earlier deterministic knowledge-based algorithm with syntactically-motivated on-the-fly transformations, and (b) by introducing an algorithm that uniformly learns costs for all types of transformations. Our evaluations and analysis support the validity of this approach."
R11-1069,A Support Tool for Deriving Domain Taxonomies from {W}ikipedia,2011,10,4,3,1,24131,lili kotlerman,Proceedings of the International Conference Recent Advances in Natural Language Processing 2011,0,"Organizing data into category hierarchies (taxonomies) is useful for content discovery, search, exploration and analysis. In industrial settings targeted taxonomies for specific domains are mostly created manually, typically by domain experts, which is time consuming and requires a high level of expertise. This paper presents an algorithm and an implemented interactive system for automatically generating target-domain taxonomies based on the Wikipedia Category Hierarchy. The system also enables human post-editing, facilitated by intelligent assistance."
P11-2098,A Probabilistic Modeling Framework for Lexical Entailment,2011,21,13,3,1,19731,eyal shnarch,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,0,"Recognizing entailment at the lexical level is an important and commonly-addressed component in textual inference. Yet, this task has been mostly approached by simplified heuristic methods. This paper proposes an initial probabilistic modeling framework for lexical entailment, with suitable EM-based parameter estimation. Our model considers prominent entailment factors, including differences in lexical-resources reliability and the impacts of transitivity and multiple evidence. Evaluations show that the proposed model outperforms most prior systems while pointing at required future improvements."
P11-1062,Global Learning of Typed Entailment Rules,2011,31,98,2,1,874,jonathan berant,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,1,"Extensive knowledge bases of entailment rules between predicates are crucial for applied semantic inference. In this paper we propose an algorithm that utilizes transitivity constraints to learn a globally-optimal set of entailment rules for typed predicates. We model the task as a graph learning problem and suggest methods that scale the algorithm to larger graphs. We apply the algorithm over a large data set of extracted predicate instances, from which a resource of typed entailment rules has been recently released (Schoenmackers et al., 2010). Our results show that using global transitivity information substantially improves performance over this resource and several baselines, and that our scaling methods allow us to increase the scope of global learning of entailment-rule graphs."
P10-2045,Generating Entailment Rules from {F}rame{N}et,2010,13,38,3,0,45666,roni aharon,Proceedings of the {ACL} 2010 Conference Short Papers,0,"Many NLP tasks need accurate knowledge for semantic inference. To this end, mostly WordNet is utilized. Yet WordNet is limited, especially for inference between predicates. To help filling this gap, we present an algorithm that generates inference rules between predicates from FrameNet. Our experiment shows that the novel resource is effective and complements WordNet in terms of rule coverage."
P10-1123,Assessing the Role of Discourse References in Entailment Inference,2010,38,25,2,1,27123,shachar mirkin,Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,1,"Discourse references, notably coreference and bridging, play an important role in many text understanding applications, but their impact on textual entailment is yet to be systematically understood. On the basis of an in-depth analysis of entailment instances, we argue that discourse references have the potential of substantially improving textual entailment recognition, and identify a number of research directions towards this goal."
P10-1124,Global Learning of Focused Entailment Graphs,2010,33,45,2,1,874,jonathan berant,Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,1,"We propose a global algorithm for learning entailment relations between predicates. We define a graph structure over predicates that represents entailment relations as directed edges, and use a global transitivity constraint on the graph to learn the optimal set of edges, by formulating the optimization problem as an Integer Linear Program. We motivate this graph with an application that provides a hierarchical summary for a set of propositions that focus on a target concept, and show that our global algorithm improves performance by more than 10% over baseline algorithms."
bentivogli-etal-2010-building,Building Textual Entailment Specialized Data Sets: a Methodology for Isolating Linguistic Phenomena Relevant to Inference,2010,9,34,3,0,8246,luisa bentivogli,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"This paper proposes a methodology for the creation of specialized data sets for Textual Entailment, made of monothematic Text-Hypothesis pairs (i.e. pairs in which only one linguistic phenomenon relevant to the entailment relation is highlighted and isolated). The expected benefits derive from the intuition that investigating the linguistic phenomena separately, i.e. decomposing the complexity of the TE problem, would yield an improvement in the development of specific strategies to cope with them. The annotation procedure assumes that humans have knowledge about the linguistic phenomena relevant to inference, and a classification of such phenomena both into fine grained and macro categories is suggested. We experimented with the proposed methodology over a sample of pairs taken from the RTE-5 data set, and investigated critical issues arising when entailment, contradiction or unknown pairs are considered. The result is a new resource, which can be profitably used both to advance the comprehension of the linguistic phenomena relevant to entailment judgments and to make a first step towards the creation of large-scale specialized data sets."
abad-etal-2010-resource,A Resource for Investigating the Impact of Anaphora and Coreference on Inference.,2010,12,7,3,0,32584,azad abad,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"Discourse phenomena play a major role in text processing tasks. However, so far relatively little study has been devoted to the relevance of discourse phenomena for inference. Therefore, an experimental study was carried out to assess the relevance of anaphora and coreference for Textual Entailment (TE), a prominent inference framework. First, the annotation of anaphoric and coreferential links in the RTE-5 Search data set was performed according to a specifically designed annotation scheme. As a result, a new data set was created where all anaphora and coreference instances in the entailing sentences which are relevant to the entailment judgment are solved and annotated.. A by-product of the annotation is a new ÂaugmentedÂ data set, where all the referring expressions which need to be resolved in the entailing sentences are replaced by explicit expressions. Starting from the final output of the annotation, the actual impact of discourse phenomena on inference engines was investigated, identifying the kind of operations that the systems need to apply to address discourse phenomena and trying to find direct mappings between these operation and annotation types."
C10-1087,Recognising Entailment within Discourse,2010,26,7,3,1,27123,shachar mirkin,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),0,"Texts are commonly interpreted based on the entire discourse in which they are situated. Discourse processing has been shown useful for inference-based application; yet, most systems for textual entailment -- a generic paradigm for applied inference -- have only addressed discourse considerations via off-the-shelf coreference resolvers. In this paper we explore various discourse aspects in entailment inference, suggest initial solutions for them and investigate their impact on entailment performance. Our experiments suggest that discourse provides useful information, which significantly improves entailment inference, and should be better addressed by future entailment systems."
W09-3701,It{'}s time for a semantic inference engine,2009,0,0,1,1,955,ido dagan,Proceedings of the Eight International Conference on Computational Semantics,0,None
W09-2504,Augmenting {W}ord{N}et-based Inference with Argument Mapping,2009,30,16,2,1,9890,idan szpektor,Proceedings of the 2009 Workshop on Applied Textual Inference ({T}ext{I}nfer),0,"WordNet is a useful resource for lexical inference in applications. Inference over predicates, however, often requires a change in argument positions, which is not specified in WordNet. We propose a novel framework for augmenting WordNet-based inferences over predicates with corresponding argument mappings. We further present a concrete implementation of this framework, which yields substantial improvement to WordNet-based inference."
P09-2018,Directional Distributional Similarity for Lexical Expansion,2009,24,17,2,1,24131,lili kotlerman,Proceedings of the {ACL}-{IJCNLP} 2009 Conference Short Papers,0,"Distributional word similarity is most commonly perceived as a symmetric relation. Yet, one of its major applications is lexical expansion, which is generally asymmetric. This paper investigates the nature of directional (asymmetric) similarity measures, which aim to quantify distributional feature inclusion. We identify desired properties of such measures, specify a particular one based on averaged precision, and demonstrate the empirical benefit of directional measures for expansion."
P09-1051,Extracting Lexical Reference Rules from {W}ikipedia,2009,29,27,3,1,19731,eyal shnarch,Proceedings of the Joint Conference of the 47th Annual Meeting of the {ACL} and the 4th International Joint Conference on Natural Language Processing of the {AFNLP},1,"This paper describes the extraction from Wikipedia of lexical reference rules, identifying references to term meanings triggered by other terms. We present extraction methods geared to cover the broad range of the lexical reference relation and analyze them extensively. Most extraction methods yield high precision levels, and our rule-base is shown to perform better than other automatically constructed baselines in a couple of lexical expansion and matching tasks. Our rule-base yields comparable performance to Word-Net while providing largely complementary information."
P09-1089,Source-Language Entailment Modeling for Translating Unknown Terms,2009,34,47,4,1,27123,shachar mirkin,Proceedings of the Joint Conference of the 47th Annual Meeting of the {ACL} and the 4th International Joint Conference on Natural Language Processing of the {AFNLP},1,This paper addresses the task of handling unknown terms in SMT. We propose using source-language monolingual models and resources to paraphrase the source text prior to translation. We further present a conceptual extension to prior work by allowing translations of entailed texts rather than paraphrases only. A method for performing this process efficiently is presented and applied to some 2500 sentences with unknown terms. Our experiments show that the proposed approach substantially increases the number of properly translated texts.
N09-2009,Text Categorization from Category Name via Lexical Reference,2009,8,17,2,0,24405,libby barak,"Proceedings of Human Language Technologies: The 2009 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers",0,"Requiring only category names as user input is a highly attractive, yet hardly explored, setting for text categorization. Earlier bootstrapping results relied on similarity in LSA space, which captures rather coarse contextual similarity. We suggest improving this scheme by identifying concrete references to the category name's meaning, obtaining a special variant of lexical expansion."
J09-3004,{A}rticles: Bootstrapping Distributional Feature Vector Quality,2009,54,56,2,0,47174,maayan zhitomirskygeffet,Computational Linguistics,0,"This article presents a novel bootstrapping approach for improving the quality of feature vector weighting in distributional word similarity. The method was motivated by attempts to utilize distributional similarity for identifying the concrete semantic relationship of lexical entailment. Our analysis revealed that a major reason for the rather loose semantic similarity obtained by distributional similarity methods is insufficient quality of the word feature vectors, caused by deficient feature weighting. This observation led to the definition of a bootstrapping scheme which yields improved feature weights, and hence higher quality feature vectors. The underlying idea of our approach is that features which are common to similar words are also most characteristic for their meanings, and thus should be promoted. This idea is realized via a bootstrapping step applied to an initial standard approximation of the similarity space. The superior performance of the bootstrapping method was assessed in two different experiments, one based on direct human gold-standard annotation and the other based on an automatically created disambiguation dataset. These results are further supported by applying a novel quantitative measurement of the quality of feature weighting functions. Improved feature weighting also allows massive feature reduction, which indicates that the most characteristic features for a word are indeed concentrated at the top ranks of its vector. Finally, experiments with three prominent similarity measures and two feature weighting functions showed that the bootstrapping scheme is robust and is independent of the original functions over which it is applied."
E09-1064,Evaluating the Inferential Utility of Lexical-Semantic Resources,2009,30,19,2,1,27123,shachar mirkin,Proceedings of the 12th Conference of the {E}uropean Chapter of the {ACL} ({EACL} 2009),0,"Lexical-semantic resources are used extensively for applied semantic inference, yet a clear quantitative picture of their current utility and limitations is largely missing. We propose system- and application-independent evaluation and analysis methodologies for resources' performance, and systematically apply them to seven prominent resources. Our findings identify the currently limited recall of available resources, and indicate the potential to improve performance by examining non-standard relation types and by distilling the output of distributional methods. Further, our results stress the need to include auxiliary information regarding the lexical and logical contexts in which a lexical inference is valid, as well as its prior validity likelihood."
D09-1110,A Compact Forest for Scalable Inference over Entailment and Paraphrase Rules,2009,26,17,3,1,10390,roy barhaim,Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,0,"A large body of recent research has been investigating the acquisition and application of applied inference knowledge. Such knowledge may be typically captured as entailment rules, applied over syntactic representations. Efficient inference with such knowledge then becomes a fundamental problem. Starting out from a formalism for entailment-rule application we present a novel packed data-structure and a corresponding algorithm for its scalable implementation. We proved the validity of the new algorithm and established its efficiency analytically and empirically."
P08-1078,Contextual Preferences,2008,24,37,2,1,9890,idan szpektor,Proceedings of ACL-08: HLT,1,None
C08-1107,Learning Entailment Rules for Unary Templates,2008,22,83,2,1,9890,idan szpektor,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"Most work on unsupervised entailment rule acquisition focused on rules between templates with two variables, ignoring unary rules - entailment rules between templates with a single variable. In this paper we investigate two approaches for unsupervised learning of such rules and compare the proposed methods with a binary rule learning method. The results show that the learned unary rule-sets outperform the binary rule-set. In addition, a novel directional similarity measure for learning entailment, termed Balanced-Inclusion, is the best performing measure."
W07-1401,The Third {PASCAL} Recognizing Textual Entailment Challenge,2007,28,623,3,0,41190,danilo giampiccolo,Proceedings of the {ACL}-{PASCAL} Workshop on Textual Entailment and Paraphrasing,0,"This paper presents the Third PASCAL Recognising Textual Entailment Challenge (RTE-3), providing an overview of the dataset creating methodology and the submitted systems. In creating this year's dataset, a number of longer texts were introduced to make the challenge more oriented to realistic scenarios. Additionally, a pool of resources was offered so that the participants could share common tools. A pilot task was also set up, aimed at differentiating unknown entailments from identified contradictions and providing justifications for overall system decisions. 26 participants submitted 44 runs, using different approaches and generally presenting new entailment models and achieving higher scores than in the previous challenges."
W07-1422,Semantic Inference at the Lexical-Syntactic Level for Textual Entailment Recognition,2007,7,27,2,1,10390,roy barhaim,Proceedings of the {ACL}-{PASCAL} Workshop on Textual Entailment and Paraphrasing,0,"We present a new framework for textual entailment, which provides a modular integration between knowledge-based exact inference and cost-based approximate matching. Diverse types of knowledge are uniformly represented as entailment rules, which were acquired both manually and automatically. Our proof system operates directly on parse trees, and infers new trees by applying entailment rules, aiming to strictly generate the target hypothesis from the source text. In order to cope with inevitable knowledge gaps, a cost function is used to measure the remaining distance from the hypothesis."
W07-0909,Cross Lingual and Semantic Retrieval for Cultural Heritage Appreciation,2007,23,5,2,1,9890,idan szpektor,Proceedings of the Workshop on Language Technology for Cultural Heritage Data ({L}a{T}e{CH} 2007).,0,"We describe a system which enhances the experience of museum visits by providing users with language-technology-based information retrieval capabilities. The system consists of a cross-lingual search engine, augmented by state of the art semantic expansion technology, specifically designed for the domain of the museum (history and archaeology of Israel). We discuss the technology incorporated in the system, its adaptation to the specific domain and its contribution to cultural heritage appreciation."
P07-1058,Instance-based Evaluation of Entailment Rule Acquisition,2007,14,62,3,1,9890,idan szpektor,Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,1,"Obtaining large volumes of inference knowledge, such as entailment rules, has become a major factor in achieving robust semantic processing. While there has been substantial research on learning algorithms for such knowledge, their evaluation methodology has been problematic, hindering further research. We propose a novel evaluation methodology for entailment rules which explicitly addresses their semantic properties and yields satisfactory human agreement levels. The methodology is used to compare two state of the art learning algorithms, exposing critical issues for future progress."
W06-2907,Investigating Lexical Substitution Scoring for Subtitle Generation,2006,19,10,2,1,49669,oren glickman,Proceedings of the Tenth Conference on Computational Natural Language Learning ({C}o{NLL}-X),0,"This paper investigates an isolated setting of the lexical substitution task of replacing words with their synonyms. In particular, we examine this problem in the setting of subtitle generation and evaluate state of the art scoring methods that predict the validity of a given substitution. The paper evaluates two context independent models and two contextual models. The major findings suggest that distributional similarity provides a useful complementary estimate for the likelihood that two Wordnet synonyms are indeed substitutable, while proper modeling of contextual constraints is still a challenging task for future research."
W06-1621,Lexical Reference: a Semantic Matching Subtask,2006,21,16,3,1,49669,oren glickman,Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,0,"Semantic lexical matching is a prominent subtask within text understanding applications. Yet, it is rarely evaluated in a direct manner. This paper proposes a definition for lexical reference which captures the common goals of lexical matching. Based on this definition we created and analyzed a test dataset that was utilized to directly evaluate, compare and improve lexical matching models. We suggest that such decomposition of the global semantic matching task is critical in order to fully understand and improve individual components."
W06-1102,Semantic Similarity: What for?,2006,0,0,1,1,955,ido dagan,Proceedings of the Workshop on Linguistic Distances,0,"Linguistic similarity has been a prominent notion and tool in computational linguistics and related areas, as elaborated nicely in the announcement of this workshop. Yet, what exactly counts as similarity, or when two linguistic concepts should be regarded as similar, often remains rather vague and ill posed, which is in fact quite typical for unsupervised notions. This talk will focus on similarity at the semantic level, and will explore the perspective that different notions of similarity may be defined relative to concrete modeling goals. In particular, I will refer to the two major goals in semantic modeling: predicting likelihood of occurrence, which is the typical goal in disambiguation and language modeling, and recognizing target meanings, which is the typical semantic goal in text understanding applications such as question answering, information extraction, summarization and information retrieval. We will discuss each goal and present corresponding semantic similarity approaches."
P06-2075,Integrating Pattern-Based and Distributional Similarity Methods for Lexical Entailment Acquisition,2006,46,39,2,1,27123,shachar mirkin,Proceedings of the {COLING}/{ACL} 2006 Main Conference Poster Sessions,0,"This paper addresses the problem of acquiring lexical semantic relationships, applied to the lexical entailment relation. Our main contribution is a novel conceptual integration between the two distinct acquisition paradigms for lexical relations - the pattern-based and the distributional similarity approaches. The integrated method exploits mutual complementary information of the two approaches to obtain candidate relations and informative characterizing features. Then, a small size training set is used to construct a more accurate supervised classifier, showing significant increase in both recall and precision over the original approaches."
P06-1057,Direct Word Sense Matching for Lexical Substitution,2006,20,37,1,1,955,ido dagan,Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,1,"This paper investigates conceptually and empirically the novel sense matching task, which requires to recognize whether the senses of two synonymous words match in context. We suggest direct approaches to the problem, which avoid the intermediate step of explicit word sense disambiguation, and demonstrate their appealing advantages and stimulating potential for future research."
E06-1052,Investigating a Generic Paraphrase-Based Approach for Relation Extraction,2006,14,77,4,0,45587,lorenza romano,11th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"Unsupervised paraphrase acquisition has been an active research field in recent years, but its effective coverage and performance have rarely been evaluated. We propose a generic paraphrase-based approach for Relation Extraction (RE), aiming at a dual goal: obtaining an applicative evaluation scheme for paraphrase acquisition and obtaining a generic and largely unsupervised configuration for RE. We analyze the potential of our approach and evaluate an implemented prototype of it using an RE dataset. Our findings reveal a high potential for unsupervised paraphrase acquisition. We also identify the need for novel robust models for matching paraphrases in texts, which should address syntactic complexity and variability."
W05-1208,A Probabilistic Setting and Lexical Coocurrence Model for Textual Entailment,2005,14,18,2,1,49669,oren glickman,Proceedings of the {ACL} Workshop on Empirical Modeling of Semantic Equivalence and Entailment,0,"This paper proposes a general probabilistic setting that formalizes a probabilistic notion of textual entailment. We further describe a particular preliminary model for lexical-level entailment, based on document cooccurrence probabilities, which follows the general setting. The model was evaluated on two application independent datasets, suggesting the relevance of such probabilistic approaches for entailment modeling."
P05-1014,The Distributional Inclusion Hypotheses and Lexical Entailment,2005,20,147,2,1,49940,maayan geffet,Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics ({ACL}{'}05),1,"This paper suggests refinements for the Distributional Similarity Hypothesis. Our proposed hypotheses relate the distributional behavior of pairs of words to lexical entailment -- a tighter notion of semantic similarity that is required by many NLP applications. To automatically explore the validity of the defined hypotheses we developed an inclusion testing algorithm for characteristic features of two words, which incorporates corpus and web-based feature sampling to overcome data sparseness. The degree of hypotheses validity was then empirically tested and manually analyzed with respect to the word sense level. In addition, the above testing algorithm was exploited to improve lexical entailment acquisition."
H05-1017,Investigating Unsupervised Learning for Text Categorization Bootstrapping,2005,13,20,3,0,3532,alfio gliozzo,Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,0,"We propose a generalized bootstrapping algorithm in which categories are described by relevant seed features. Our method introduces two unsupervised steps that improve the initial categorization step of the bootstrapping scheme: (i) using Latent Semantic space to obtain a generalized similarity measure between instances and features, and (ii) the Gaussian Mixture algorithm, to obtain uniform classification probabilities for unlabeled examples. The algorithm was evaluated on two Text Categorization tasks and obtained state-of-the-art performance using only the category names as initial seeds."
H05-1123,A Generalized Framework for Revealing Analogous Themes across Related Topics,2005,10,1,2,1,51129,zvika marx,Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,0,"This work addresses the task of identifying thematic correspondences across sub-corpora focused on different topics. We introduce an unsupervised algorithmic framework based on distributional data clustering, which generalizes previous initial works on this task. The empirical results reveal interesting commonalities of different religions. We evaluate the results through measuring the overlap of our clusters with clusters compiled manually by experts. The tested variants of our framework are shown to outperform alternative methods applicable to the task."
W04-3206,Scaling Web-based Acquisition of Entailment Relations,2004,15,160,3,1,9890,idan szpektor,Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing,0,"Paraphrase recognition is a critical step for natural language interpretation. Accordingly, many NLP applications would benefit from high coverage knowledge bases of paraphrases. However, the scalability of state-of-the-art paraphrase acquisition approaches is still limited. We present a fully unsupervised learning algorithm for Web-based extraction of entailment relations, an extended model of paraphrases. We focus on increased scalability and generality with respect to prior work, eventually aiming at a full scale knowledge base. Our current implementation of the algorithm takes as its input a verb lexicon and for each verb searches the Web for related syntactic entailment templates. Experiments show promising results with respect to the ultimate goal, achieving much better scalability than prior Web-based methods."
C04-1036,Feature Vector Quality and Distributional Similarity,2004,14,33,2,1,49940,maayan geffet,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"We suggest a new goal and evaluation criterion for word similarity measures. The new criterion - meaning-entailing substitutability - fits the needs of semantic-oriented NLP applications and can be evaluated directly (independent of an application) at a good level of human agreement. Motivated by this semantic criterion we analyze the empirical quality of distributional word feature vectors and its impact on word similarity results, proposing an objective measure for evaluating feature vector quality. Finally, a novel feature weighting and selection function is presented, which yields superior feature vectors and better word similarity performance."
W02-2009,Cross-dataset Clustering: Revealing Corresponding Themes across Multiple Corpora,2002,11,8,1,1,955,ido dagan,{COLING}-02: The 6th Conference on Natural Language Learning 2002 ({C}o{NLL}-2002),0,"We present a method for identifying corresponding themes across several corpora that are focused on related, but distinct, domains. This task is approached through simultaneous clustering of keyword sets extracted from the analyzed corpora. Our algorithm extends the information-bottleneck soft clustering method for a suitable setting consisting of several datasets. Experimentation with topical corpora reveals similar aspects of three distinct religions. The evaluation is by way of comparison to clusters constructed manually by an expert."
P00-1007,Incorporating Compositional Evidence in Memory-Based Partial Parsing,2000,21,5,2,0,48630,yuval krymolowski,Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,1,"In this paper, a memory-based parsing method is extended for handling compositional structures. The method is oriented for learning to parse any selected subset of target syntactic structures. It is local, yet can handle also compositional structures. Parts of speech as well as embedded instances are being used simultaneously. The output is a partial parse in which instances of the target structures are marked."
W99-0907,Detecting Sub-Topic Correspondence through Bipartite Term Clustering,1999,0,0,2,1,51129,zvika marx,Unsupervised Learning in Natural Language Processing,0,"This paper addresses a novel task of detecting sub-topic correspondence in a pair of text fragments, enhancing common notions of text similarity. This task is addressed by coupling corresponding term subsets through bipartite clustering. The paper presents a cost-based clustering scheme and compares it with a bipartite version of the single-link method, providing illustrating results."
P98-1010,A Memory-Based Approach to Learning Shallow Natural Language Patterns,1998,42,74,2,0,49333,shlomo argamon,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 1",1,"Recognizing shallow linguistic patterns, such as basic syntactic relationships between words, is a common task in applied natural language and text processing. The common practice for approaching this task is by tedious manual definition of possible pattern structures, often in the form of regular expressions or finite automata. This paper presents a novel memory-based learning method that recognizes shallow patterns in new text based on a bracketed training corpus. The training data are stored as-is, in efficient suffix-tree data structures. Generalization is performed on-line at recognition time by comparing subsequences of the new text to positive and negative evidence in the corpus. This way, no information in the training is lost, as can happen in other learning systems that construct a single generalized model at the time of training. The paper presents experimental results for recognizing noun phrase, subject-verb and verb-object patterns in English. Since the learning approach enables easy porting to new domains, we plan to apply it to syntactic patterns in other languages and to sub-language patterns for information extraction."
C98-1010,A Memory-Based Approach to Learning Shallow Natural Language Patterns,1998,42,74,2,0,49333,shlomo argamon,{COLING} 1998 Volume 1: The 17th International Conference on Computational Linguistics,0,"Recognizing shallow linguistic patterns, such as basic syntactic relationships between words, is a common task in applied natural language and text processing. The common practice for approaching this task is by tedious manual definition of possible pattern structures, often in the form of regular expressions or finite automata. This paper presents a novel memory-based learning method that recognizes shallow patterns in new text based on a bracketed training corpus. The training data are stored as-is, in efficient suffix-tree data structures. Generalization is performed on-line at recognition time by comparing subsequences of the new text to positive and negative evidence in the corpus. This way, no information in the training is lost, as can happen in other learning systems that construct a single generalized model at the time of training. The paper presents experimental results for recognizing noun phrase, subject-verb and verb-object patterns in English. Since the learning approach enables easy porting to new domains, we plan to apply it to syntactic patterns in other languages and to sub-language patterns for information extraction."
W97-0306,Mistake-Driven Learning in Text Categorization,1997,20,141,1,1,955,ido dagan,Second Conference on Empirical Methods in Natural Language Processing,0,None
P97-1008,Similarity-Based Methods for Word Sense Disambiguation,1997,14,116,1,1,955,ido dagan,35th Annual Meeting of the Association for Computational Linguistics and 8th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,1,We compare four similarity-based estimation methods against back-off and maximum-likelihood estimation methods on a pseudo-word sense disambiguation task in which we controlled for both unigram and bigram frequency. The similarity-based methods perform up to 40% better on this particular task. We also conclude that events that occur only once in the training set have major impact on similarity-based estimates.
P96-1042,Minimizing Manual Annotation Cost in Supervised Training from Corpora,1996,17,66,2,0,55909,sean engelson,34th Annual Meeting of the Association for Computational Linguistics,1,"Corpus-based methods for natural language processing often use supervised training, requiring expensive manual annotation of training corpora. This paper investigates methods for reducing annotation cost by sample selection. In this approach, during training the learning program examines many unlabeled examples and selects for labeling (annotation) only those that are most informative at each stage. This avoids redundantly annotating examples that contribute little new information. This paper extends our previous work on committee-based sample selection for probabilistic classifiers. We describe a family of methods for committee-based sample selection, and report experimental results for the task of stochastic part-of-speech tagging. We find that all variants achieve a significant reduction in annotation cost, though their computational efficiency differs. In particular, the simplest method, which has no parameters to tune, gives excellent results. We also show that sample selection yields a significant reduction in the size of the model used by the tagger."
P94-1038,Similarity-Based Estimation of Word Cooccurrence Probabilities,1994,13,113,1,1,955,ido dagan,32nd Annual Meeting of the Association for Computational Linguistics,1,"In many applications of natural language processing it is necessary to determine the likelihood of a given word combination. For example, a speech recognizer may need to determine which of the two word combinations eat a peach and eat a beach is more likely. Statistical NLP methods determine the likelihood of a word combination according to its frequency in a training corpus. However, the nature of language is such that many word combinations are infrequent and do not occur in a given corpus. In this work we propose a method for estimating the probability of such previously unseen word combinations using available information on most similar words.We describe a probabilistic word association model based on distributional word similarity, and apply it to improving probability estimates for unseen word bigrams in a variant of Katz's back-off model. The similarity-based method yields a 20% perplexity improvement in the prediction of unseen bigrams and statistically significant reductions in speech-recognition error."
J94-4003,Word Sense Disambiguation Using a Second Language Monolingual Corpus,1994,31,292,1,1,955,ido dagan,Computational Linguistics,0,"This paper presents a new approach for resolving lexical ambiguities in one language using statistical data from a monolingual corpus of another language. This approach exploits the differences between mappings of words to senses in different languages. The paper concentrates on the problem of target word selection in machine translation, for which the approach is directly applicable. The presented algorithm identifies syntactic relations between words, using a source language parser, and maps the alternative interpretations of these relations to the target language, using a bilingual lexicon. The preferred senses are then selected according to statistics on lexical relations in the target language. The selection is based on a statistical model and on a constraint propagation algorithm, which simultaneously handles all ambiguities in the sentence. The method was evaluated using three sets of Hebrew and German examples and was found to be very useful for disambiguation. The paper includes a detailed comparative analysis of statistical sense disambiguation methods."
A94-1006,{T}ermight: Identifying and Translating Technical Terminology,1994,27,220,1,1,955,ido dagan,Fourth Conference on Applied Natural Language Processing,0,"We propose a semi-automatic tool, termight, that helps professional translators and terminologists identify technical terms and their translations. The tool makes use of part-of-speech tagging and word-alignment programs to extract candidate terms and their translations. Although the extraction programs are far from perfect, it isn't too hard for the user to filter out the wheat from the chaff. The extraction algorithms emphasize completeness. Alternative proposals are likely to miss important but infrequent terms/translations. To reduce the burden on the user during the filtering phase, candidates are presented in a convenient order, along with some useful concordance evidence, in an interface that is designed to minimize keystrokes. Termight is currently being used by the translators at AT&T Business Translation Services (formerly AT&T Language Line Services)."
W93-0301,Robust Bilingual Word Alignment for Machine Aided Translation,1993,-1,-1,1,1,955,ido dagan,{V}ery {L}arge {C}orpora: Academic and Industrial Perspectives,0,None
P93-1022,Contextual Word Similarity and Estimation From Sparse Data,1993,19,113,1,1,955,ido dagan,31st Annual Meeting of the Association for Computational Linguistics,1,"In recent years there is much interest in word cooccurrence relations, such as n-grams, verb-object combinations, or cooccurrence within a limited context. This paper discusses how to estimate the probability of cooccurrences that do not occur in the training data. We present a method that makes local analogies between each specific unobserved cooccurrence and other cooccurrences that contain similar words, as determined by an appropriate word similarity metric. Our evaluation suggests that this method performs better than existing smoothing methods, and may provide an alternative to class based models."
C92-4201,Automatic Translation of Noun Compounds,1992,7,26,2,0,57073,ulrike rackow,{COLING} 1992 Volume 4: The 14th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"This paper describes the treatment of nominal compounds in a tranarer based machine translation system; it presents a new approach for resolving ambiguities in compound segmentation and Constituent structure selection using a combination of linguistic rules and statistical data. An introduction to the general as well as to the German-English-specific problems of compound translation is given (sect. 1). In section 2, the analysis phase is described with its linguistics as well as its computational aspects. Section 3 deals with the transfer and generation process, focussing on corpus based techniques."
P91-1017,Two Languages Are More Informative Than One,1991,9,158,1,1,955,ido dagan,29th Annual Meeting of the Association for Computational Linguistics,1,"This paper presents a new approach for resolving lexical ambiguities in one language using statistical data on lexical relations in another language. This approach exploits the differences between mappings of words to senses in different languages. We concentrate on the problem of target word selection in machine translation, for which the approach is directly applicable, and employ a statistical model for the selection mechanism. The model was evaluated using two sets of Hebrew and German examples and was found to be very useful for disambiguation."
P91-1048,Lexical Disambiguation: Sources of Information and their Statistical Realization,1991,2,12,1,1,955,ido dagan,29th Annual Meeting of the Association for Computational Linguistics,1,"Lexical disambiguation can be achieved using different sources of information. Aiming at high performance of automatic disambiguation it is important to know the relative importance and applicability of the various sources. In this paper we classify several sources of information and show how some of them can be achieved using statistical data. First evaluations indicate the extreme importance of local information, which mainly represents lexical associations and selectional restrictions for syntactically related words."
C90-3063,Automatic Processing of Large Corpora for the Resolution of Anaphora References,1990,5,110,1,1,955,ido dagan,{COLING} 1990 Volume 3: Papers presented to the 13th International Conference on Computational Linguistics,0,"Manual acquisition of semantic constraints in broad domains is very expensive. This paper presents an automatic scheme for collecting statistics on cooccurrence patterns in a large corpus. To a large extent, these statistics reflect semantic constraints and thus are used to disambiguate anaphora references and syntactic ambiguities. The scheme was implemented by gathering statistics on the output of other linguistic tools. An experiment was performed to resolve references of the pronoun it in sentences that were randomly selected from the corpus. The results of the experiment show that in most of the cases the cooccurrence statistics indeed reflect the semantic constraints and thus provide a basis for a useful disambiguation tool."
