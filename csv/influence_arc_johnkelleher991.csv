2019.gwc-1.18,agirre-etal-2010-exploring,0,0.524563,"esource are also updated. More related to our work are the approaches to learn directly from knowledge resources. Examples include building non-distributional sparse word vectors from lexical resources (Faruqui and Dyer, 2015), building Poincar´e embeddings that represent the structure of the WordNet taxonomy (Nickel and Kiela, 2017) and building embeddings that encode all semantic relationships expressed in a biomedical ontology within a single vector space (Cohen and Widdows, 2017). The latter two methods encode the semantic structure of a knowledge resource in a deterministic manner, while Agirre et al. (2010) follow a stochastic approach based on Personalised PageRank: they compute the probability of reaching a synset from a target word, following a random-walk on a given WordNet relation. Instead of computing random-walk probabilities, Goikoetxea et al. (2015) use an off-theshelf implementation of the word2vec Skip-Gram algorithm to train embeddings on WordNet random walk pseudo-corpora, changing neither the embedding algorithm nor the objective function1 . The resulting embeddings encode WordNet taxonomic information rather than natural word cooccurence. An advantage of the embeddings produced b"
2019.gwc-1.18,N18-1062,0,0.0203009,". While such embeddings have been shown to perform well on semantic relatedness benchmarks (Baroni et al., 2014; Camacho-Collados and Pilehvar, 2018), training on a natural corpus only models one type of semantic relation between words: thematic (i.e. syntagmatic). On the flip side, taxonomic (i.e. paradigmatic) relations are not explicitly contained in natural language corpora, and as such are not included in those embeddings (Kacmajor and Kelleher, 2019). In fact, research suggests that the best measures of taxonomic similarity and thematic relatedness are different in distributional space (Asr et al., 2018). Furthermore, there are many other kinds of relationships between words and concepts that can be found in knowledge engineered resources, such as knowledge bases, ontologies, taxonomies and other semantic networks. Modelling these relations is an important task in building AI with comprehensive natural language understanding abilities, and there have been many efforts to bring knowledge graphs into an embedding space (see Section 2 for details). One such approach is the WordNet random walk algorithm (Goikoetxea et al., 2015): by randomly walking the WordNet knowledge graph and choosing words"
2019.gwc-1.18,P14-1023,0,0.0285523,"maps the words in a vocabulary to dense low-dimensional vectors, by inferring the relative position of each word in a shared multidimensional semantic space from its context of use in a corpus (Mikolov et al., 2013a; Mikolov et al., 2013b). This approach is founded on the distributional hypothesis (Harris, 1954), which states that words which occur in the same contexts tend to have similar meanings. Such word embeddings are created by training a neural network language model on natural language corpora. While such embeddings have been shown to perform well on semantic relatedness benchmarks (Baroni et al., 2014; Camacho-Collados and Pilehvar, 2018), training on a natural corpus only models one type of semantic relation between words: thematic (i.e. syntagmatic). On the flip side, taxonomic (i.e. paradigmatic) relations are not explicitly contained in natural language corpora, and as such are not included in those embeddings (Kacmajor and Kelleher, 2019). In fact, research suggests that the best measures of taxonomic similarity and thematic relatedness are different in distributional space (Asr et al., 2018). Furthermore, there are many other kinds of relationships between words and concepts that can"
2019.gwc-1.18,P04-3031,0,0.686406,"Missing"
2019.gwc-1.18,P15-2076,0,0.211787,"the other hand, examples of the specialisation approach are PARAGRAM (Wieting et al., 2015), Attract-Repel (Mrkˇsi´c et al., 2016), Hypervec (Nguyen et al., 2017) and the work of Nguyen et al. (2016) and Mrkˇsi´c et al. (2017) on synonyms and antonyms. Vuli´c et al. (2018) and Ponti et al. (2018) introduce global specialisation models where vectors for words that are missing in the knowledge resource are also updated. More related to our work are the approaches to learn directly from knowledge resources. Examples include building non-distributional sparse word vectors from lexical resources (Faruqui and Dyer, 2015), building Poincar´e embeddings that represent the structure of the WordNet taxonomy (Nickel and Kiela, 2017) and building embeddings that encode all semantic relationships expressed in a biomedical ontology within a single vector space (Cohen and Widdows, 2017). The latter two methods encode the semantic structure of a knowledge resource in a deterministic manner, while Agirre et al. (2010) follow a stochastic approach based on Personalised PageRank: they compute the probability of reaching a synset from a target word, following a random-walk on a given WordNet relation. Instead of computing"
2019.gwc-1.18,N15-1165,0,0.767808,"similarity and thematic relatedness are different in distributional space (Asr et al., 2018). Furthermore, there are many other kinds of relationships between words and concepts that can be found in knowledge engineered resources, such as knowledge bases, ontologies, taxonomies and other semantic networks. Modelling these relations is an important task in building AI with comprehensive natural language understanding abilities, and there have been many efforts to bring knowledge graphs into an embedding space (see Section 2 for details). One such approach is the WordNet random walk algorithm (Goikoetxea et al., 2015): by randomly walking the WordNet knowledge graph and choosing words from each synset that has been traversed, a pseudo-corpus is generated and used for training word embeddings. The reasoning is that the distributional hypothesis should also apply in this scenario, in the sense that co-occurrence within local contexts in the pseudo-corpus will reflect the connections between words connected in the WordNet graph. Naturally, the shape of the underlying knowledge graph (in terms of node connectivity: i.e. tree, fully-connected, radial etc.) affects the properties of a pseudo-corpus generated via"
2019.gwc-1.18,J15-4004,0,0.236765,"e resulting embeddings encode WordNet taxonomic information rather than natural word cooccurence. An advantage of the embeddings produced by this method is that they can be used as is or can be combined with real-corpus embeddings in order to accomplish enrichment or specialisation (Goikoetxea et al., 2016). Previous work has analysed semantic properties of word embeddings generated by random walk. Goikoetxea et al. (2016), for example, found WordNet random-walk embeddings to outperform corpus-based word embeddings on the strict semantic similarity (taxonomic similarity) SimLex-999 benchmark (Hill et al., 2015), confirming that they encode taxonomic information better than real-corpus word embeddings. Additionally, other researchers have explored different varieties of the random walk algorithm. Most notably, Simov et al. (2017a) drastically enrich the graph structure by using all available relationships between WordNet synsets, while inferring and adding others from outside resources (Simov et al., 2015; Simov et al., 2017b). However, to the best of our knowledge, there has been no work on analysing the properties of the corpora generated by random-walk processes. In particular, there has been no w"
2019.gwc-1.18,P18-1002,0,0.0308467,"well-understood shape (tree-like) which informs the analysis of our results. We find that the pseudo-corpora synthesized from the WordNet taxonomy are not as artificial as one might expect - they exhibit properties and regularities also found in natural corpora, following natural language laws such as Heaps’ law and Zipf’s law. Consequently, we hypothesise that word embeddings trained on such corpora might face the same limitations as those trained on natural corpora would. We explore this notion on the case study of rare (i.e. infrequent) words, which are a known problem for word embeddings (Khodak et al., 2018; Pilehvar and Collier, 2017; Pilehvar et al., 2018). 2 Related work Research on building embeddings from knowledge resources such as WordNet (Fellbaum, 1998), can be broadly categorised into three approaches: i) enrichment, ii) specialisation, and iii) direct learning from knowledge resources. Both enrichment and specialisation modify precomputed, corpus-based word embeddings with information from a knowledge resource to either augment them (enrichment) or to fit them onto the specific semantic relation described by that knowledge resource (specialisation). Retrofitting (Faruqui et al., 2015)"
2019.gwc-1.18,P16-2074,0,0.0150392,"at knowledge resource (specialisation). Retrofitting (Faruqui et al., 2015) is an example of enrichment: it modifies corpus-based embeddings by reducing the distance between words that are directly linked in resources like WordNet, MeSH (Yu et al., 2016) and ConceptNet (Speer and Havasi, 2012). In our own recent related work, we have explored the impact of corpus size on vector enrichment (Maldonado et al., 2019). On the other hand, examples of the specialisation approach are PARAGRAM (Wieting et al., 2015), Attract-Repel (Mrkˇsi´c et al., 2016), Hypervec (Nguyen et al., 2017) and the work of Nguyen et al. (2016) and Mrkˇsi´c et al. (2017) on synonyms and antonyms. Vuli´c et al. (2018) and Ponti et al. (2018) introduce global specialisation models where vectors for words that are missing in the knowledge resource are also updated. More related to our work are the approaches to learn directly from knowledge resources. Examples include building non-distributional sparse word vectors from lexical resources (Faruqui and Dyer, 2015), building Poincar´e embeddings that represent the structure of the WordNet taxonomy (Nickel and Kiela, 2017) and building embeddings that encode all semantic relationships expr"
2019.gwc-1.18,D17-1022,0,0.0577301,"Missing"
2019.gwc-1.18,E17-2062,0,0.165471,"(tree-like) which informs the analysis of our results. We find that the pseudo-corpora synthesized from the WordNet taxonomy are not as artificial as one might expect - they exhibit properties and regularities also found in natural corpora, following natural language laws such as Heaps’ law and Zipf’s law. Consequently, we hypothesise that word embeddings trained on such corpora might face the same limitations as those trained on natural corpora would. We explore this notion on the case study of rare (i.e. infrequent) words, which are a known problem for word embeddings (Khodak et al., 2018; Pilehvar and Collier, 2017; Pilehvar et al., 2018). 2 Related work Research on building embeddings from knowledge resources such as WordNet (Fellbaum, 1998), can be broadly categorised into three approaches: i) enrichment, ii) specialisation, and iii) direct learning from knowledge resources. Both enrichment and specialisation modify precomputed, corpus-based word embeddings with information from a knowledge resource to either augment them (enrichment) or to fit them onto the specific semantic relation described by that knowledge resource (specialisation). Retrofitting (Faruqui et al., 2015) is an example of enrichment"
2019.gwc-1.18,D18-1169,0,0.0184788,"he analysis of our results. We find that the pseudo-corpora synthesized from the WordNet taxonomy are not as artificial as one might expect - they exhibit properties and regularities also found in natural corpora, following natural language laws such as Heaps’ law and Zipf’s law. Consequently, we hypothesise that word embeddings trained on such corpora might face the same limitations as those trained on natural corpora would. We explore this notion on the case study of rare (i.e. infrequent) words, which are a known problem for word embeddings (Khodak et al., 2018; Pilehvar and Collier, 2017; Pilehvar et al., 2018). 2 Related work Research on building embeddings from knowledge resources such as WordNet (Fellbaum, 1998), can be broadly categorised into three approaches: i) enrichment, ii) specialisation, and iii) direct learning from knowledge resources. Both enrichment and specialisation modify precomputed, corpus-based word embeddings with information from a knowledge resource to either augment them (enrichment) or to fit them onto the specific semantic relation described by that knowledge resource (specialisation). Retrofitting (Faruqui et al., 2015) is an example of enrichment: it modifies corpus-bas"
2019.gwc-1.18,D18-1026,0,0.0293648,"Missing"
2019.gwc-1.18,R15-1077,0,0.175484,"Goikoetxea et al. (2016), for example, found WordNet random-walk embeddings to outperform corpus-based word embeddings on the strict semantic similarity (taxonomic similarity) SimLex-999 benchmark (Hill et al., 2015), confirming that they encode taxonomic information better than real-corpus word embeddings. Additionally, other researchers have explored different varieties of the random walk algorithm. Most notably, Simov et al. (2017a) drastically enrich the graph structure by using all available relationships between WordNet synsets, while inferring and adding others from outside resources (Simov et al., 2015; Simov et al., 2017b). However, to the best of our knowledge, there has been no work on analysing the properties of the corpora generated by random-walk processes. In particular, there has been no work on comparing their statistical properties with those of natural corpora. 1 http://ixa2.si.ehu.eus/ukb/ 3 3.1 Pseudo-corpora Random walk pseudo corpus generation Our pseudo-corpus generation process is inspired by the work of Goikoetxea et al. (2015). They performed random walks over the full WordNet knowledge base as an undirected graph of interlinked synsets. Their method first chooses a synse"
2019.gwc-1.18,simov-etal-2017-towards,0,0.209598,"mbeddings in order to accomplish enrichment or specialisation (Goikoetxea et al., 2016). Previous work has analysed semantic properties of word embeddings generated by random walk. Goikoetxea et al. (2016), for example, found WordNet random-walk embeddings to outperform corpus-based word embeddings on the strict semantic similarity (taxonomic similarity) SimLex-999 benchmark (Hill et al., 2015), confirming that they encode taxonomic information better than real-corpus word embeddings. Additionally, other researchers have explored different varieties of the random walk algorithm. Most notably, Simov et al. (2017a) drastically enrich the graph structure by using all available relationships between WordNet synsets, while inferring and adding others from outside resources (Simov et al., 2015; Simov et al., 2017b). However, to the best of our knowledge, there has been no work on analysing the properties of the corpora generated by random-walk processes. In particular, there has been no work on comparing their statistical properties with those of natural corpora. 1 http://ixa2.si.ehu.eus/ukb/ 3 3.1 Pseudo-corpora Random walk pseudo corpus generation Our pseudo-corpus generation process is inspired by the"
2019.gwc-1.18,speer-havasi-2012-representing,0,0.0291988,"e approaches: i) enrichment, ii) specialisation, and iii) direct learning from knowledge resources. Both enrichment and specialisation modify precomputed, corpus-based word embeddings with information from a knowledge resource to either augment them (enrichment) or to fit them onto the specific semantic relation described by that knowledge resource (specialisation). Retrofitting (Faruqui et al., 2015) is an example of enrichment: it modifies corpus-based embeddings by reducing the distance between words that are directly linked in resources like WordNet, MeSH (Yu et al., 2016) and ConceptNet (Speer and Havasi, 2012). In our own recent related work, we have explored the impact of corpus size on vector enrichment (Maldonado et al., 2019). On the other hand, examples of the specialisation approach are PARAGRAM (Wieting et al., 2015), Attract-Repel (Mrkˇsi´c et al., 2016), Hypervec (Nguyen et al., 2017) and the work of Nguyen et al. (2016) and Mrkˇsi´c et al. (2017) on synonyms and antonyms. Vuli´c et al. (2018) and Ponti et al. (2018) introduce global specialisation models where vectors for words that are missing in the knowledge resource are also updated. More related to our work are the approaches to lear"
2019.gwc-1.18,N18-1048,0,0.0317733,"Missing"
2019.gwc-1.18,W16-6106,0,0.0216959,"be broadly categorised into three approaches: i) enrichment, ii) specialisation, and iii) direct learning from knowledge resources. Both enrichment and specialisation modify precomputed, corpus-based word embeddings with information from a knowledge resource to either augment them (enrichment) or to fit them onto the specific semantic relation described by that knowledge resource (specialisation). Retrofitting (Faruqui et al., 2015) is an example of enrichment: it modifies corpus-based embeddings by reducing the distance between words that are directly linked in resources like WordNet, MeSH (Yu et al., 2016) and ConceptNet (Speer and Havasi, 2012). In our own recent related work, we have explored the impact of corpus size on vector enrichment (Maldonado et al., 2019). On the other hand, examples of the specialisation approach are PARAGRAM (Wieting et al., 2015), Attract-Repel (Mrkˇsi´c et al., 2016), Hypervec (Nguyen et al., 2017) and the work of Nguyen et al. (2016) and Mrkˇsi´c et al. (2017) on synonyms and antonyms. Vuli´c et al. (2018) and Ponti et al. (2018) introduce global specialisation models where vectors for words that are missing in the knowledge resource are also updated. More relate"
2020.coling-main.174,W05-0909,0,0.35923,"Missing"
2020.coling-main.174,H92-1022,0,0.327446,"Missing"
2020.coling-main.174,P15-2017,0,0.136728,"dditionally demonstrating a considerably larger effective vocabulary size. 1 Introduction Image Captioning brings together the two fields of Computer Vision and Natural Language Generation into a task where the model needs to translate an input image into an appropriate natural language text description. The task leaves some ambiguity regarding which parts of the image should be mentioned and which ones can be excluded. This has led to a common problem where models tend to generate overly generic descriptions that seem to focus more on the category of the image than on its individual content (Devlin et al., 2015; Madhyastha et al., 2018). Recently, Cornia et al. (2019) introduced Controllable Image Captioning as a new sub-task of Image Captioning with a stronger focus on image details. In this task, the input to the model is an image along with bounding box coordinates for a sequence of regions (where each region can consist of one or more bounding boxes) that must be explicitly described in the candidate caption. Thus, in contrast to standard Image Captioning, a generic candidate caption would not meet the criteria of a suitable caption even if it contained no factual errors; this is reflected in th"
2020.coling-main.174,P04-1077,0,0.139133,"Missing"
2020.coling-main.174,P02-1040,0,0.108346,"Missing"
2020.coling-main.174,Q14-1006,0,0.533036,"l is an image along with bounding box coordinates for a sequence of regions (where each region can consist of one or more bounding boxes) that must be explicitly described in the candidate caption. Thus, in contrast to standard Image Captioning, a generic candidate caption would not meet the criteria of a suitable caption even if it contained no factual errors; this is reflected in the evaluation process where the candidate caption is only compared to those ground-captions that share the same sequence of regions. Fig. 1 shows an image and two corresponding captions from the Flickr30k dataset (Young et al., 2014) along with complementary data from Flickr30k Entities (Plummer et al., 2017) which provides annotations that link entities in the captions to region bounding boxes in the corresponding image. In terms of practical use, a Controllable Image Captioning model provides more flexibility and usercontrol over the generated captions, without having to retrain the model. Since the region selection process is not entangled with the caption generation components, the former can be swapped out to adapt to different scenarios (e.g. when applied to social media images, it could target regions where a facia"
2020.coling-main.197,P18-1198,0,0.0223941,"rows a and c of the Table 1 and so we interpret them as confirming the validity of the automatic metrics selected in our work. 7 Probing the Relationship between Style and Content In Section 6.3 we observed that reinforcing the latent representation of the input (z) during generation could result in improved content preservation but also consistently resulted in a deterioration in styleshift power. In this section we examine whether this trade-off is a result of stylistic information being encoded within the latent representation of the input. Inspired by the probing experiments described by Conneau et al. (2018), we designed a probing experiment to examine what style information each of our models encoded in their latent representations. To do the probing classification experiment, for each style-transfer framework we trained a classifier that took the z representation generated by the encoder of the framework and predicted the style of the input sequence that had been feed into the encoder. The idea here is that if we can train a classifier to accurately predict the style of an input from the z representation of an input sequence generated by the encoder, then this z vector must have information rel"
2020.coling-main.197,Q17-1024,0,0.0645459,"Missing"
2020.coling-main.197,D14-1181,0,0.00256665,"pect of textual style transfer: content preservation, style transfer strength and fluency. Style shift power Style shift power investigates how well the output of a model fits with the target style. One approach is to employ a pre-trained classifier to measure the percentage of the style-shifted sequences which are labeled with the desired style (Fu et al., 2018; Li et al., 2018; Leeftink and Spanakis, 2019; Singh and Palod, 2018; Prabhumoye et al., 2018; Shen et al., 2017; John et al., 2018; Hu et al., 2017). Here, we employ the same classifier type as (Shen et al., 2017), the TextCNN model (Kim, 2014) and train it on the Yelp dataset. It achieves an accuracy of %97 on predicting sentiment (positive and negative classes) on the test set of the Yelp dataset. 2170 Content preservation Many approaches, usually inherited from historical NLG tasks, have been employed to compare how well the generated output preserves the meaning of the original sequence. We compute a content preservation rate between an input sequence and a generated sequence as the cosine similarity between their embedded representations. The process we use to generated the embedding of a sequence follows the method proposed by"
2020.coling-main.197,N18-1169,0,0.377219,"s, and Section 5 describes the dataset and our experimental methodology. In Section 6 we evaluate the style-transfer models, and in Section 7 probe their representations of the input to examine what stylistic information they encode. In Section 8 we set out our conclusions and future work. 2 Literature review To deal with textual style transfer, a first group of approaches consider a two-step process where markers of the source style are removed before generating the target style output sequence. Marking and removing the original style features can be achieved through frequency-based methods (Li et al., 2018) or neural networks that integrate attention modules (Leeftink and Spanakis, 2019). The generation step relies on techniques such as the retrieval of textual segments from a corpus of the target style (Ramos, 1999; Leeftink and Spanakis, 2019), or employing neural network generation techniques (Li et al., 2018). The second group of approaches which have been mostly implemented in the literature focus on endto-end learning strategies to frame the task of style transfer. End-to-end learning approaches enable learning of a latent representations of an input (Kelleher, 2019), encoding potentially"
2020.coling-main.197,D14-1162,0,0.0842501,"usually inherited from historical NLG tasks, have been employed to compare how well the generated output preserves the meaning of the original sequence. We compute a content preservation rate between an input sequence and a generated sequence as the cosine similarity between their embedded representations. The process we use to generated the embedding of a sequence follows the method proposed by Fu et al. (2018). Given a sequence of words x = [x1 . . . wN ], each token xi is represented by an embedding ei ∈ R100 . These embeddings are generated using a pre-trained 100-dimensional GloVe model (Pennington et al., 2014). Then, we calculate the min, mean and max pooling of these word embeddings, namely m, µ, M as follows:   m = min ei,j (1) 1≤i≤N P 1≤j≤100 1≤i≤N ei,j µ = N   M = max ei,j 1≤j≤100  (2) 1≤j≤100 (3) 1≤i≤N The embedding of the sequence x is then created by the concatenation of m, µ and M . In practice, sentiment markers1 are removed from the sequence x to make sure the content preservation metric indeed measures the content similarity. Finally, when processing a dataset, content preservation is averaged over all its sentences. Fluency Following Zhao et al. (2018) and John et al.(2018), we me"
2020.coling-main.197,N18-1202,0,0.040617,"reconstructed outputs are considered as “transferred” and “preserved” respectively. For each style s, we train Ds to maximize the probability of assigning these true labels to the output sequences by minimizing this loss. LDs (s) (s) = − log Ds (e x1 )− log(1 − Ds (e x2 )) (6) The backpropagation for the encoder-generator couple (E, G) is carried out using the following: Ltotal = Lrec + Ladv,s1 + Ladv,s2 2172 (7) Figure 2: Detail of the ELMo-based encoding. 4.2 Encoder variant: ELMo-based encoder ELMo is a state-of-the-art NLP framework which provides deep contextualized word representations (Peters et al., 2018) and has proved efficient in many NLP-related tasks recently. Here, we focus on the encoder architecture of the baseline model and replace it with a pre-trained ELMo model. As shown by Figure 2, in this model embeddings for the tokens in the input sequence are generated using a pre-trained ELMo model (Peters et al., 2018) and the embedding for the sequence is the generated from these token embeddings using the min-mean-max representation described in Section 3. The training of this model is the same as for the base model, except that the encoder (the pre-trained ELMo model) is not modified dur"
2020.coling-main.197,P18-1080,0,0.0521323,"15). Ma and Sun (2017) applied this approach to text simplification and summarization (considering the style here as “verbosity”). The key idea is that the text embedding vector generated by the encoder is a style-free representation of the input text (Rabinovich et al., 2017). A major limitation of this approach is the need for parallel data. In some scenarios this data challenge can be addressed by either relying on intermediate resources–e.g., using “zero-shot translation” (Johnson et al., 2017; Carlson et al., 2017)–or monolingual data–e.g. using “back translation” (Sennrich et al., 2016; Prabhumoye et al., 2018). Alternatively, Generative Adversarial Networks (Goodfellow et al., 2014) have been implemented for style transfer, removing the need for parallel data. These models include a generator block which generates two sequences for each input sequence (a style transferred sequence and a reconstructed sequence), and a discriminator block (a classifier) which tries to guess whether a given sequence is style transferred or reconstructed. Generators are usually standard encoder-decoders (Shen et al., 2017; Singh and Palod, 2018; Fu et al., 2018; Romanov et al., 2019), but various refinements have been"
2020.coling-main.197,E17-1101,0,0.0221446,"sentations of an input (Kelleher, 2019), encoding potentially both the input content and style. Historically, such style transfer approaches have been inspired by the Neural Machine Translation techniques, basically a sequence-to-sequence encoder-decoder architecture based on recurrent neural networks (Sutskever et al., 2014; Bahdanau et al., 2015). Ma and Sun (2017) applied this approach to text simplification and summarization (considering the style here as “verbosity”). The key idea is that the text embedding vector generated by the encoder is a style-free representation of the input text (Rabinovich et al., 2017). A major limitation of this approach is the need for parallel data. In some scenarios this data challenge can be addressed by either relying on intermediate resources–e.g., using “zero-shot translation” (Johnson et al., 2017; Carlson et al., 2017)–or monolingual data–e.g. using “back translation” (Sennrich et al., 2016; Prabhumoye et al., 2018). Alternatively, Generative Adversarial Networks (Goodfellow et al., 2014) have been implemented for style transfer, removing the need for parallel data. These models include a generator block which generates two sequences for each input sequence (a sty"
2020.coling-main.197,N19-1088,0,0.0286094,"slation” (Sennrich et al., 2016; Prabhumoye et al., 2018). Alternatively, Generative Adversarial Networks (Goodfellow et al., 2014) have been implemented for style transfer, removing the need for parallel data. These models include a generator block which generates two sequences for each input sequence (a style transferred sequence and a reconstructed sequence), and a discriminator block (a classifier) which tries to guess whether a given sequence is style transferred or reconstructed. Generators are usually standard encoder-decoders (Shen et al., 2017; Singh and Palod, 2018; Fu et al., 2018; Romanov et al., 2019), but various refinements have been proposed. For instance, several generators (decoders) can be used, one for each style (Fu et al., 2018), or a variational encoder can be used for encoding (Hu et al., 2017; John et al., 2018). In most cases, the decoders are conditioned on a latent representation (embedding) of the input sequence concatenated with an embedding of the target output style. The concept of discriminator can also be extended, for instance in order to force encoded representations of the input to either focus on its style or meaning or both (Romanov et al., 2019; John et al., 2018"
2020.coling-main.197,P16-1009,0,0.0260545,"14; Bahdanau et al., 2015). Ma and Sun (2017) applied this approach to text simplification and summarization (considering the style here as “verbosity”). The key idea is that the text embedding vector generated by the encoder is a style-free representation of the input text (Rabinovich et al., 2017). A major limitation of this approach is the need for parallel data. In some scenarios this data challenge can be addressed by either relying on intermediate resources–e.g., using “zero-shot translation” (Johnson et al., 2017; Carlson et al., 2017)–or monolingual data–e.g. using “back translation” (Sennrich et al., 2016; Prabhumoye et al., 2018). Alternatively, Generative Adversarial Networks (Goodfellow et al., 2014) have been implemented for style transfer, removing the need for parallel data. These models include a generator block which generates two sequences for each input sequence (a style transferred sequence and a reconstructed sequence), and a discriminator block (a classifier) which tries to guess whether a given sequence is style transferred or reconstructed. Generators are usually standard encoder-decoders (Shen et al., 2017; Singh and Palod, 2018; Fu et al., 2018; Romanov et al., 2019), but vari"
2020.lrec-1.602,agirre-etal-2010-exploring,0,0.0329304,"e approaches taken in research on training embeddings from scratch are more diverse. For example, Faruqui and Dyer (2015) build non-distributional sparse word vectors from knowledge resources, with each dimension representing whether the word belongs to a particular synset, holds a particular taxonomic relation, etc. Another approach is introduced by Nickel and Kiela (2017), who develop Poincar´e embeddings that represent the structure of the WordNet taxonomy. This method seeks to encode the semantic structure of a knowledge resource, however it does so in a deterministic manner. By contrast, Agirre et al. (2010) follow a stochastic approach based on Personalised PageRank: they compute the probability of reaching a synset from a target word, following a random-walk on a given WordNet relation. Instead of computing random-walk probabilities, Goikoetxea et al. (2015) use an off-the-shelf implementation of the word2vec Skip-Gram algorithm to train embeddings on pseudo-corpora generated from WordNet random walks. Neither the embedding algorithm nor the objective function is changed in any way. By training on sequences of words that hold taxonomic relations, instead of naturally co-occurring words as in re"
2020.lrec-1.602,P04-3031,0,0.183888,"hich can propagate down the line if integrated into various machine learning and language modelling pipelines. 5. Resource publication Goikoetxea et al. provide an implementation of their pseudo-corpus generation algorithm3 . However, due to the significant differences our algorithm has introduced, as outlined in Section 3., and the the special use cases required for our research which focused on analysing how the shape of 3 4900 http://ixa2.si.ehu.eus/ukb/ knowledge graph affects the properties of the synthesized corpora, we reimplemented the algorithm using NLTK’s Python version of WordNet (Bird and Loper, 2004)4 . We have also made our random walk code publicly available via GitHub5 , and have included a detailed guide on how to use the provided scripts. In addition to a script for generating pseudo-corpora with varying hyperparameters, there is also a script for calculating basic corpus statistics, and a script for calculating a word similarity score using word embeddings and cosine similarity. As far as our corpora, we have published all resources related to our research on Arrow@TUDublin6 , which is Technological University Dublin’s official archive and data repository. This includes an archive o"
2020.lrec-1.602,N19-1423,0,0.0158181,"Missing"
2020.lrec-1.602,P15-2076,0,0.0180167,"ated resources, while Section 5. points to the published resources. 2. Related work Recently there has been an increase in the amount of research on building embeddings from knowledge resources such as WordNet. Prior work shows that embeddings can be enriched with taxonomic knowledge, specialised to better reflect that semantic dimension, or trained from scratch on appropriate taxonomic resources. Work on enrichment and specialisation tends to focus on the Skip-Gram family of algorithms whereas the approaches taken in research on training embeddings from scratch are more diverse. For example, Faruqui and Dyer (2015) build non-distributional sparse word vectors from knowledge resources, with each dimension representing whether the word belongs to a particular synset, holds a particular taxonomic relation, etc. Another approach is introduced by Nickel and Kiela (2017), who develop Poincar´e embeddings that represent the structure of the WordNet taxonomy. This method seeks to encode the semantic structure of a knowledge resource, however it does so in a deterministic manner. By contrast, Agirre et al. (2010) follow a stochastic approach based on Personalised PageRank: they compute the probability of reachin"
2020.lrec-1.602,N15-1165,0,0.195029,"se terms falls beyond the scope of this paper, so we lean on the terminology used by Kacmajor and Kelleher (2019). solely rely on natural corpora as their main training resource (Mikolov et al., 2013; Salton et al., 2017; Devlin et al., 2018; Peters et al., 2018; Pagliardini et al., 2018). That said, there have been many efforts to transfer and integrate the taxonomic information encoded in knowledge resources into distributed vector embedding representations of lexical semantics (see Section 2. for details). The approach that we have explored in our work is the WordNet random walk algorithm (Goikoetxea et al., 2015): by randomly walking the WordNet knowledge graph and choosing words from each synset that has been traversed, a pseudo-corpus can be generated and used for training word embeddings, in the same way one would train on a natural language corpus. The reasoning behind this approach is that co-occurrence within local contexts in the pseudocorpus will reflect the connections between words connected in the WordNet graph. In other words, using this approach flattens out WordNet, turning it into a sequential format similar to a natural corpus, where the same implicit connection - co-occurrence - refle"
2020.lrec-1.602,C92-2082,0,0.252893,"often implies having different, albeit complementary, features and functions (Kacmajor and Kelleher, 2019)1 . When it comes to language and language resources, as a rule of thumb the two semantic relationships are explicitly encoded in two different kinds of resources: a natural language corpus primarily reflects the thematic relationships between words by way of word co-occurrence. Taxonomic relations, on the other hand, are rarely overtly expressed in examples of natural language. Though research has shown that such relationships can be automatically extracted from natural language corpora (Hearst, 1992), they are more accessible and more commonly modeled in the form of knowledge-engineered language resources such as knowledge bases, ontologies, taxonomies and similar semantic networks, where relationships are reflected via explicit links between entities (i.e. nodes) in the knowledge graph. Modelling both kinds of relationships is an important task in building AI with comprehensive natural language understanding abilities, yet most NLP models and systems, especially language models and word/sentence embeddings, 1 In the linguistics literature, the concepts of taxonomic and thematic relatedne"
2020.lrec-1.602,J15-4004,0,0.012551,"Net random-walk embeddings with real-corpus embeddings in order to accomplish enrichment or specialisation. For example, Goikoetxea et al. (2016) found that simply concatenating real-corpus word embeddings and WordNet random-walk embeddings gave the best performance on various similarity benchmarks, compared with more sophisticated combination methods. In their work they have also analysed the semantic properties of WordNet randomwalk embeddings, and at the time found them to outperform corpus-based word embeddings on the strict semantic similarity (taxonomic similarity) SimLex-999 benchmark (Hill et al., 2015), confirming that they encode taxonomic information better than real-corpus word embeddings. Rather than training word embeddings, Simov et al. (2015) leverage taxonomic knowledge to tackle the task of Word Sense Disambiguation. They pour significant efforts into techniques for enriching the WordNet graph with additional semantic connections (Simov et al., 2016a; Simov et al., 2016b). In their later work, Simov et al. (2017b) build directly on the work of Goikoetxea et al. (2015) and explore how various different varieties of the random walk algorithm impact performance of trained word embeddi"
2020.lrec-1.602,2019.gwc-1.18,1,0.861794,"Missing"
2020.lrec-1.602,N18-1049,0,0.0158309,"c relatedness would roughly correspond to what are respectively called paradigmatic and syntagmatic relations, and there is a nuanced discussion to be had about the extent of the overlap in the terminology. However, as we are focused on resources modeling taxonomic relations exclusively, delving deeper into the differences between these terms falls beyond the scope of this paper, so we lean on the terminology used by Kacmajor and Kelleher (2019). solely rely on natural corpora as their main training resource (Mikolov et al., 2013; Salton et al., 2017; Devlin et al., 2018; Peters et al., 2018; Pagliardini et al., 2018). That said, there have been many efforts to transfer and integrate the taxonomic information encoded in knowledge resources into distributed vector embedding representations of lexical semantics (see Section 2. for details). The approach that we have explored in our work is the WordNet random walk algorithm (Goikoetxea et al., 2015): by randomly walking the WordNet knowledge graph and choosing words from each synset that has been traversed, a pseudo-corpus can be generated and used for training word embeddings, in the same way one would train on a natural language corpus. The reasoning behind"
2020.lrec-1.602,N18-1202,0,0.0111859,"taxonomic and thematic relatedness would roughly correspond to what are respectively called paradigmatic and syntagmatic relations, and there is a nuanced discussion to be had about the extent of the overlap in the terminology. However, as we are focused on resources modeling taxonomic relations exclusively, delving deeper into the differences between these terms falls beyond the scope of this paper, so we lean on the terminology used by Kacmajor and Kelleher (2019). solely rely on natural corpora as their main training resource (Mikolov et al., 2013; Salton et al., 2017; Devlin et al., 2018; Peters et al., 2018; Pagliardini et al., 2018). That said, there have been many efforts to transfer and integrate the taxonomic information encoded in knowledge resources into distributed vector embedding representations of lexical semantics (see Section 2. for details). The approach that we have explored in our work is the WordNet random walk algorithm (Goikoetxea et al., 2015): by randomly walking the WordNet knowledge graph and choosing words from each synset that has been traversed, a pseudo-corpus can be generated and used for training word embeddings, in the same way one would train on a natural language c"
2020.lrec-1.602,I17-1045,1,0.825998,"e linguistics literature, the concepts of taxonomic and thematic relatedness would roughly correspond to what are respectively called paradigmatic and syntagmatic relations, and there is a nuanced discussion to be had about the extent of the overlap in the terminology. However, as we are focused on resources modeling taxonomic relations exclusively, delving deeper into the differences between these terms falls beyond the scope of this paper, so we lean on the terminology used by Kacmajor and Kelleher (2019). solely rely on natural corpora as their main training resource (Mikolov et al., 2013; Salton et al., 2017; Devlin et al., 2018; Peters et al., 2018; Pagliardini et al., 2018). That said, there have been many efforts to transfer and integrate the taxonomic information encoded in knowledge resources into distributed vector embedding representations of lexical semantics (see Section 2. for details). The approach that we have explored in our work is the WordNet random walk algorithm (Goikoetxea et al., 2015): by randomly walking the WordNet knowledge graph and choosing words from each synset that has been traversed, a pseudo-corpus can be generated and used for training word embeddings, in the same w"
2020.lrec-1.602,R15-1077,0,0.0124816,"nd that simply concatenating real-corpus word embeddings and WordNet random-walk embeddings gave the best performance on various similarity benchmarks, compared with more sophisticated combination methods. In their work they have also analysed the semantic properties of WordNet randomwalk embeddings, and at the time found them to outperform corpus-based word embeddings on the strict semantic similarity (taxonomic similarity) SimLex-999 benchmark (Hill et al., 2015), confirming that they encode taxonomic information better than real-corpus word embeddings. Rather than training word embeddings, Simov et al. (2015) leverage taxonomic knowledge to tackle the task of Word Sense Disambiguation. They pour significant efforts into techniques for enriching the WordNet graph with additional semantic connections (Simov et al., 2016a; Simov et al., 2016b). In their later work, Simov et al. (2017b) build directly on the work of Goikoetxea et al. (2015) and explore how various different varieties of the random walk algorithm impact performance of trained word embeddings, similar to our work on the topic (Klubiˇcka et al., 2019). However, rather than constraining the walk to just the taxonomy, they look for additio"
2020.lrec-1.602,2016.gwc-1.55,0,0.0108114,"work they have also analysed the semantic properties of WordNet randomwalk embeddings, and at the time found them to outperform corpus-based word embeddings on the strict semantic similarity (taxonomic similarity) SimLex-999 benchmark (Hill et al., 2015), confirming that they encode taxonomic information better than real-corpus word embeddings. Rather than training word embeddings, Simov et al. (2015) leverage taxonomic knowledge to tackle the task of Word Sense Disambiguation. They pour significant efforts into techniques for enriching the WordNet graph with additional semantic connections (Simov et al., 2016a; Simov et al., 2016b). In their later work, Simov et al. (2017b) build directly on the work of Goikoetxea et al. (2015) and explore how various different varieties of the random walk algorithm impact performance of trained word embeddings, similar to our work on the topic (Klubiˇcka et al., 2019). However, rather than constraining the walk to just the taxonomy, they look for additional ways of enriching the graph structure and populating WordNet with as many connections as possible, exploiting all available relationships between WordNet synsets, as well as adding and inferring more from outs"
2020.lrec-1.602,simov-etal-2017-towards,0,0.0135579,"randomwalk embeddings, and at the time found them to outperform corpus-based word embeddings on the strict semantic similarity (taxonomic similarity) SimLex-999 benchmark (Hill et al., 2015), confirming that they encode taxonomic information better than real-corpus word embeddings. Rather than training word embeddings, Simov et al. (2015) leverage taxonomic knowledge to tackle the task of Word Sense Disambiguation. They pour significant efforts into techniques for enriching the WordNet graph with additional semantic connections (Simov et al., 2016a; Simov et al., 2016b). In their later work, Simov et al. (2017b) build directly on the work of Goikoetxea et al. (2015) and explore how various different varieties of the random walk algorithm impact performance of trained word embeddings, similar to our work on the topic (Klubiˇcka et al., 2019). However, rather than constraining the walk to just the taxonomy, they look for additional ways of enriching the graph structure and populating WordNet with as many connections as possible, exploiting all available relationships between WordNet synsets, as well as adding and inferring more from outside resources (Simov et al., 2017a). 3. Resource generation algo"
2020.lrec-1.602,J19-3003,0,0.0137071,"as follows: v(n) ∝ nβ , ∀ n  1 (2) where the exponent for the Heaps’ law for natural languages is found to be 0 < β < 1. In other words, Heaps’ law means that as more instances of natural text are gathered, there will be diminishing returns in terms of discovery of the full vocabulary from which the distinct terms are drawn, i.e. as the text gets bigger, there will be less and less new additions to the vocabulary. We also consider Ebeling’s Law, which studies the growth of variance of individual components (e.g. letters or words in text) in relation to the subsequence length l. Described by Takahashi and Tanaka-Ishii (2019), for a set of words W , let y(k, l) be the number of occurrences of word wk ∈ W for all subsequences of length l of the original dataset. Then, m(l) = |W | X m2 (k, l) ∝ lη (3) k=1 m2 (k, l) is the variance of y(k, l). Here, m(l) relates to l with a power-law relationship with exponent η. Ebeling and P¨oschel (1994) showed that the Bible has η = 1.69. In other words, there is a specific relationship between the size of a sequence of natural text and the variance of words that occur in that sequence. It can be seen as describing the variety of words found in a text, which becomes higher as the"
2020.spnlp-1.5,D18-1547,0,0.0505671,"Missing"
2020.spnlp-1.5,2020.sigdial-1.4,0,0.0342708,"Missing"
2020.spnlp-1.5,W19-5910,1,0.698252,"ate-of-the-art dialogue state trackers have treated the task as a set of individual domaindependent classification problems (Heck et al., 2020; Wu et al., 2019; Zhou and Small, 2019). However, we argue that such approaches leave room for improvement; particularly with the consideration of the nature of human-machine interactions (Landragin, 2013). Specially, we argue that the multi-task classification methodology usually does not take into account the relationships between dialogue slot variables, despite the fact that these factors can play an essential part in the dialogue state prediction (Trinh et al., 2019a). Therefore, we propose to explicitly incorporate dialogue variable associations into the prediction process in a multi-domain dialogue environment, thus casting the dialogue state tracking task a structured prediction problem. Introduction Task-oriented dialogue systems have been developed to assist users in many fields (Brixey et al., 2017; Zhao et al., 2019). In recent years it is a rising trend to scale-up task-oriented dialogue systems from single domain to multiple domains to improve the generalisability of models and support transfer of knowledge across domains. This leads to a new ch"
2020.spnlp-1.5,W19-4109,1,0.849226,"ate-of-the-art dialogue state trackers have treated the task as a set of individual domaindependent classification problems (Heck et al., 2020; Wu et al., 2019; Zhou and Small, 2019). However, we argue that such approaches leave room for improvement; particularly with the consideration of the nature of human-machine interactions (Landragin, 2013). Specially, we argue that the multi-task classification methodology usually does not take into account the relationships between dialogue slot variables, despite the fact that these factors can play an essential part in the dialogue state prediction (Trinh et al., 2019a). Therefore, we propose to explicitly incorporate dialogue variable associations into the prediction process in a multi-domain dialogue environment, thus casting the dialogue state tracking task a structured prediction problem. Introduction Task-oriented dialogue systems have been developed to assist users in many fields (Brixey et al., 2017; Zhao et al., 2019). In recent years it is a rising trend to scale-up task-oriented dialogue systems from single domain to multiple domains to improve the generalisability of models and support transfer of knowledge across domains. This leads to a new ch"
2020.spnlp-1.5,2020.acl-main.53,0,0.151701,"nce ‡ Information, Communications & Entertainment Institute Technological University Dublin, Ireland {anhduong.trinh, robert.ross, john.d.kelleher}@tudublin.ie Abstract ios, an increase in the number of domains is equivalent to an increase in the number of slots, this in turn enlarges the models and makes the task more challenging. While traditionally one can develop a number of models to track dialogue states in each domain separately, recent advanced techniques tend to train dialogue state trackers in the multi-domain environment. Such multi-domain trackers produce state-of-the-art results (Kim et al., 2020; Heck et al., 2020). Scaling up dialogue state tracking to multiple domains is challenging due to the growth in the number of variables being tracked. Furthermore, dialog state tracking models do not yet explicitly make use of relationships between dialogue variables, such as slots across domains. We propose using energy-based structure prediction methods for large-scale dialogue state tracking task in two multiple domain dialogue datasets. Our results indicate that: (i) modelling variable dependencies yields better results; and (ii) the structured prediction output aligns with the dialogue s"
2020.spnlp-1.5,P19-1078,0,0.272804,"d structure prediction methods for large-scale dialogue state tracking task in two multiple domain dialogue datasets. Our results indicate that: (i) modelling variable dependencies yields better results; and (ii) the structured prediction output aligns with the dialogue slot-value constraint principles. This leads to promising directions to improve stateof-the-art models by incorporating variable dependencies into their prediction process. 1 To date state-of-the-art dialogue state trackers have treated the task as a set of individual domaindependent classification problems (Heck et al., 2020; Wu et al., 2019; Zhou and Small, 2019). However, we argue that such approaches leave room for improvement; particularly with the consideration of the nature of human-machine interactions (Landragin, 2013). Specially, we argue that the multi-task classification methodology usually does not take into account the relationships between dialogue slot variables, despite the fact that these factors can play an essential part in the dialogue state prediction (Trinh et al., 2019a). Therefore, we propose to explicitly incorporate dialogue variable associations into the prediction process in a multi-domain dialogue env"
2020.spnlp-1.5,P15-2130,0,0.0477374,"Missing"
2020.spnlp-1.5,D19-1196,0,0.0330505,"Missing"
2021.acl-long.147,D16-1019,0,0.18033,"pression is true. We model the soft truth score of grounded patterns using t-norm based fuzzy logics (H´ajek, 1998). 1878 The score fsro of an individual atom (i.e. triple) is computed using the KGE model’s scoring function. We use the sigmoid function σ(x) = 1/(1 + exp(−x)) to map this score to a continuous truth value in the range (0, 1). Hence, the soft truth score for an individual atom is φ(s, r, o) = σ(fsro ). The soft truth score for the grounding of a pattern can then be expressed through logical composition (e.g. ∧ and ⇒) of the scores of individual atoms in the grounding. We follow (Guo et al., 2016, 2018) and define the following compositions for logical conjunction (∧), disjunction (∨), and negation (¬): Adversarial Attack Step Sym Inv Com Determine Adversarial Relations n/a Alg Alg Determine Decoy Entities Sft Rnk Cos Sft Rnk Cos Sft Rnk Cos Determine Adversarial Entities n/a n/a Sft Table 2: A summary of heuristic approaches used for different steps of the adversarial attack with symmetry (Sym), inversion (Inv) and composition (Com) pattern. Alg denotes the algebraic model for inference patterns; Sft denotes the soft truth score; Rnk denotes the KGE ranks; and Cos denotes the cosine"
2021.emnlp-main.648,W17-2609,0,0.0268916,"sion We propose data poisoning attacks against KGE models using instance attribution methods and demonstrate that the proposed attacks outperform the state-of-art attacks. We observe that the attacks are particularly effective when the KGE model relies on few training instances to make predictions, i.e. when the input graph is sparse. We also observe that shallow neural architectures like DistMult, ComplEx and TransE are vulnerable to naive attacks based on Instance Similarity. These models have shown competitive predictive performance by proper hyperparameter tuning (Ruffinelli et al., 2020; Kadlec et al., 2017), making them promising candidates for use in production pipelines. But our research shows that these performance gains can be brittle. This calls for improved KGE model evaluation that accounts for adversarial robustness in addition to predictive performance. Additionally, as in Bhardwaj (2020); Bhardwaj et al. (2021), we call for future proposals to defend against the security vulnerabilities of KGE models. Some promising directions might be to use adversarial training techniques or train ensembles of models over subsets of training data to prevent the model predictions being influenced by a"
2021.emnlp-main.648,2021.naacl-main.75,0,0.153218,"entify the training instances that are influential to 8225 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8225–8239 c November 7–11, 2021. 2021 Association for Computational Linguistics model predictions, that is, deleting the instances from the training data would considerably change the model parameters or predictions. These methods are widely used to generate post-hoc examplebased explanations for deep neural networks on images (Koh and Liang, 2017; Hanawa et al., 2021; Charpiat et al., 2019) and text (Han et al., 2020; Han and Tsvetkov, 2020; Pezeshkpour et al., 2021). Since the KGE models have relatively shallow neural architectures and the instance attribution metrics are independent of the black-box models and the input domain, they are a promising approach to estimate the influence of training triples on the KGE model predictions. Yet, despite their promise, they have not been used on KGE models so far. We use the instance attribution methods to address the challenge of measuring the impact of a candidate adversarial deletion on the model predictions. We focus on the adversarial goal of degrading the KGE model prediction on a given target triple. To ac"
2021.emnlp-main.648,N19-1337,0,0.0395134,"wledge Graph Embeddings via Instance Attribution We consider an adversarial attacker that aims to degrade the KGE model’s predictive performance on a set of missing triples that have been ranked highly plausible by the model. We denote these target triples as Z := {z := (zs , zr , zo )}. Since the predicted ranks are based on the predicted scores; to reduce the predicted rank of a target triple, we craft perturbations to the training data that aim to reduce the predicted score of the target triple. Threat Model: We use the same threat model as the state-of-art poisoning attacks on KGE models (Pezeshkpour et al., 2019; Zhang et al., 2019a). We focus on the white-box attack setting where the attacker has full knowledge of the victim model architecture and access to the learned embeddings. However, they cannot perturb the architecture or the embeddings directly; but only through perturbations in the training data. We study both adversarial additions and adversarial deletions. In both settings, the attacker is restricted to making only one edit in the neighbourhood of the target triple. The neighbourhood of the target triple z := (zs , zr , zo ) is the set of triples that have the same subject or the same obj"
2021.mwe-1.7,N19-1423,0,0.173892,"nonical form (lexical and syntactic) of idiomatic expressions. The related work on idiom token classification at a sentence level includes (Sporleder and Li, 2009; Li and Sporleder, 2010a,b; Peng and Feldman, 2017; Fazly et al., 2009; Salton et al., 2016, 2017). Of particular relevance is Salton et al. (2016) which demonstrated that it is possible to train a generic (as distinct to expression specific) idiom token classifier using distributed sentence embeddings. Of note here is that Salton et al. (2016) used Skip-Thought vectors rather than the more recent contextual embeddings such as BERT (Devlin et al., 2019) and also that these results indicate 57 Proceedings of the 17th Workshop on Multiword Expressions, pages 57–62 Bangkok, Thailand (online), August 6, 2021. ©2021 Association for Computational Linguistics 2 Data, Embeddings, and Models experiment we have run the experiment independently on each of the 20 down sampled versions of the dataset, and then calculated the macro average score across these 20 independent runs. For each down sampled version of the dataset we used a bert-base-uncased pretrained BERT model1 to generate sentence embeddings (Devlin et al., 2019). We use this version of BERT"
2021.mwe-1.7,J09-1005,0,0.0391188,"it takes. Our results indicate that BERT’s idiomatic key is primarily found within an idiomatic expression, but also draws on information from the surrounding context. Also, BERT can distinguish between the disruption in a sentence caused by words missing and the incongruity caused by idiomatic usage. 1 Introduction Idioms occur in almost all languages, however the processing of idioms by NLP systems remains extremely challenging (Villavicencio et al., 2005; Sporleder and Li, 2009; Salton et al., 2014). One reason for this is that many expressions can be used both literally or idiomatically. Fazly et al. (2009) distinguish between identifying whether an expression has an idiomatic sense (idiom type classification) and identifying whether a particular usage of an expression is idiomatic (idiom token classification), and focus their work on analysing the canonical form (lexical and syntactic) of idiomatic expressions. The related work on idiom token classification at a sentence level includes (Sporleder and Li, 2009; Li and Sporleder, 2010a,b; Peng and Feldman, 2017; Fazly et al., 2009; Salton et al., 2016, 2017). Of particular relevance is Salton et al. (2016) which demonstrated that it is possible t"
2021.mwe-1.7,C10-2078,0,0.0372482,"ng (Villavicencio et al., 2005; Sporleder and Li, 2009; Salton et al., 2014). One reason for this is that many expressions can be used both literally or idiomatically. Fazly et al. (2009) distinguish between identifying whether an expression has an idiomatic sense (idiom type classification) and identifying whether a particular usage of an expression is idiomatic (idiom token classification), and focus their work on analysing the canonical form (lexical and syntactic) of idiomatic expressions. The related work on idiom token classification at a sentence level includes (Sporleder and Li, 2009; Li and Sporleder, 2010a,b; Peng and Feldman, 2017; Fazly et al., 2009; Salton et al., 2016, 2017). Of particular relevance is Salton et al. (2016) which demonstrated that it is possible to train a generic (as distinct to expression specific) idiom token classifier using distributed sentence embeddings. Of note here is that Salton et al. (2016) used Skip-Thought vectors rather than the more recent contextual embeddings such as BERT (Devlin et al., 2019) and also that these results indicate 57 Proceedings of the 17th Workshop on Multiword Expressions, pages 57–62 Bangkok, Thailand (online), August 6, 2021. ©2021 Asso"
2021.mwe-1.7,N10-1039,0,0.0309093,"ng (Villavicencio et al., 2005; Sporleder and Li, 2009; Salton et al., 2014). One reason for this is that many expressions can be used both literally or idiomatically. Fazly et al. (2009) distinguish between identifying whether an expression has an idiomatic sense (idiom type classification) and identifying whether a particular usage of an expression is idiomatic (idiom token classification), and focus their work on analysing the canonical form (lexical and syntactic) of idiomatic expressions. The related work on idiom token classification at a sentence level includes (Sporleder and Li, 2009; Li and Sporleder, 2010a,b; Peng and Feldman, 2017; Fazly et al., 2009; Salton et al., 2016, 2017). Of particular relevance is Salton et al. (2016) which demonstrated that it is possible to train a generic (as distinct to expression specific) idiom token classifier using distributed sentence embeddings. Of note here is that Salton et al. (2016) used Skip-Thought vectors rather than the more recent contextual embeddings such as BERT (Devlin et al., 2019) and also that these results indicate 57 Proceedings of the 17th Workshop on Multiword Expressions, pages 57–62 Bangkok, Thailand (online), August 6, 2021. ©2021 Asso"
2021.mwe-1.7,P18-1198,0,0.0298647,"tly local to the expression or not? Note, that here we are using a broader concept of idiomatic key than that proposed by Cacciari and Tabossi (1988): they limit the idiomatic key to be a part of an expression, whereas we use the concept of idiomatic key to be the part of a sentence that provides BERT with a signal that an expression is being using idiomatically. Answering the question of where BERT’s idiomatic key is can provide insight into how BERT, and similar systems work, and also into human language processing. In this paper we address this question by using a probing style experiment (Conneau et al., 2018) combined with various input masking techniques. Section 2 describes the dataset, embeddings, and model types that we use. Section 3 reports baseline experiments that examine the strength of the idiomatic usage signal encoded in BERT embeddings, and Section 4 reports a second set of experiments where various masking techniques are used to analyse where in a sentence BERT’s idiomatic key is located. Section 5 sets out our conclusions. Sentence embeddings encode information relating to the usage of idioms in a sentence. This paper reports a set of experiments that combine a probing methodology w"
2021.mwe-1.7,W14-1007,1,0.816619,"dology with input masking to analyse where in a sentence this idiomatic information is taken from, and what form it takes. Our results indicate that BERT’s idiomatic key is primarily found within an idiomatic expression, but also draws on information from the surrounding context. Also, BERT can distinguish between the disruption in a sentence caused by words missing and the incongruity caused by idiomatic usage. 1 Introduction Idioms occur in almost all languages, however the processing of idioms by NLP systems remains extremely challenging (Villavicencio et al., 2005; Sporleder and Li, 2009; Salton et al., 2014). One reason for this is that many expressions can be used both literally or idiomatically. Fazly et al. (2009) distinguish between identifying whether an expression has an idiomatic sense (idiom type classification) and identifying whether a particular usage of an expression is idiomatic (idiom token classification), and focus their work on analysing the canonical form (lexical and syntactic) of idiomatic expressions. The related work on idiom token classification at a sentence level includes (Sporleder and Li, 2009; Li and Sporleder, 2010a,b; Peng and Feldman, 2017; Fazly et al., 2009; Salto"
2021.mwe-1.7,salton-etal-2017-idiom,1,0.856062,"Missing"
2021.mwe-1.7,E09-1086,0,0.0152716,"combine a probing methodology with input masking to analyse where in a sentence this idiomatic information is taken from, and what form it takes. Our results indicate that BERT’s idiomatic key is primarily found within an idiomatic expression, but also draws on information from the surrounding context. Also, BERT can distinguish between the disruption in a sentence caused by words missing and the incongruity caused by idiomatic usage. 1 Introduction Idioms occur in almost all languages, however the processing of idioms by NLP systems remains extremely challenging (Villavicencio et al., 2005; Sporleder and Li, 2009; Salton et al., 2014). One reason for this is that many expressions can be used both literally or idiomatically. Fazly et al. (2009) distinguish between identifying whether an expression has an idiomatic sense (idiom type classification) and identifying whether a particular usage of an expression is idiomatic (idiom token classification), and focus their work on analysing the canonical form (lexical and syntactic) of idiomatic expressions. The related work on idiom token classification at a sentence level includes (Sporleder and Li, 2009; Li and Sporleder, 2010a,b; Peng and Feldman, 2017; Faz"
I17-1045,H94-1020,0,0.230156,"ext. Using this scoring, the model can decide whether it should pay more attention to the current state, to a previous state or use past states to “supplement” the information for the next prediction. In §5 we present and analysis of how the model attends to different parts of its history as it generates a sequence of predictions. (8) Single score. This score is calculated for each hi using just the information stored the state in itself. The score single(hi ) is defined as single(hi ) = vs tanh(Ws hi ) (9) 4 Experiments To evaluate our Attentive RNN-LMs we conducted experiments over the PTB (Marcus et al., 1994) and wikitext2 (Merity et al., 2017) datasets. We first describe the setup of our Attentive RNN-LM for the PTB (§4.1) and wikitext2 (§4.2) datasets and then discuss the results (§4.3). We compare our results on PTB to Zaremba et al. (2015) and Press and Wolf (2016) the best performing LSTM-LMs on the PTB, two memory augmented networks (Grave et al. (2017) and Merity et al. (2017)) and PTB state-of-the-art ensemble models of Zaremba et al. (2015). On wikitext2 we take (Merity et al., 2017), the creators of the dataset, and (Grave et al., 2017), the current state-of-theart, as our baselines. whe"
I17-1045,D16-1096,0,0.0281932,"ith the decoder RNN’s state ht−1 to compute a context vector that, in turn, is used to compute the state ht . In Luong et al. (2015) a generalization of the model of Bahdanau et al. (2015) is presented which uses the decoder RNN‘s state, in this instance ht rather than ht−1 , along with the outputs of the encoder RNN to compute a context vector that it then concatenated with ht before making the next prediction. Both models have similar performance and achieve state-of-theart performance for some language pairs; however, they suffer from repeating words or dropping translations at the output (Mi et al., 2016). There is previous work on using past information to improve RNN-LMs. Tran et al. (2016) propose an extension to LSTM cells to include memory areas, which depend on input words, at the output of every hidden layer. The model produces good results but the dependency on input words expands the number of parameters in each LSTM cell in proportion to the vocabulary size in use. These recent models have a number of drawbacks. The systems that extend the architecture of LSTM units struggle to process large vocabularies because the system memory expands to the size of the vocabulary. For systems tha"
I17-1045,D16-1053,0,0.0205127,"Missing"
I17-1045,W12-2702,0,0.0687209,"work Language Models (RNN-LMs) with an attention mechanism. We show that an Attentive RNN-LM (with 14.5M parameters) achieves a better perplexity than larger RNN-LMs (with 66M parameters) and achieves performance comparable to an ensemble of 10 similar sized RNN-LMs. We also show that an Attentive RNN-LM needs less contextual information to achieve similar results to the stateof-the-art on the wikitext2 dataset. 1 Introduction Language Models (LMs) are an essential component in a range of Natural Language Processing applications, such as Statistical Machine Translation and Speech Recognition (Schwenk et al., 2012). An LM provides a probability for a sequence of words in a given language, reflecting fluency and the likelihood of that word sequence occurring in that language. In recent years Recurrent Neural Networks (RNNs) have improved the state-of-the-art in LM research (J´ozefowicz et al., 2016). Sequential data prediction, however, is still considered a challenge in Artificial Intelligence (Mikolov et al., 2010) given that, in general, prediction accuracy degrades as the size of sequences increase. RNN-LMs sequentially propagate forward a context vector by integrating the information generated by ea"
J09-2005,P02-1041,0,0.0222565,"Missing"
J09-2005,E03-1036,0,0.0207767,"Missing"
J09-2005,P04-1041,0,0.0291521,"Missing"
J09-2005,J97-1002,0,0.0179474,"n area of ongoing research (see Gawron 1986; Tseng 2000; Beermann and Hellan 2004). The analysis presented here of the prepositional phrase near the ball is intended to illustrate some of the semantic features that prepositions may introduce into a grammar and is not intended as a comprehensive account of how prepositions should be grammatically represented. The ﬁnal stage in the interpretation pipeline is to categorize how the utterance relates to the current dialog context. This categorization is driven by the dialog manager and involves interpreting an utterance as a dialog act (Bunt 1994; Carletta et al. 1997; Klein 1999). One of the important tasks in this process is resolving the references in the input. Consequently, the dialog manager may invoke the reference resolution component. Reference resolution is one of two functions in the architecture where spatial reasoning plays an important role. From a computational perspective, reference resolution involves two main tasks: 1. Creating and maintaining a model of what the system considers as mutual knowledge (this model should contain all the objects that are available for reference and their properties) 2. Matching the representation introduced b"
J09-2005,E91-1028,0,0.0263812,"d earlier, the GRE component is triggered by the content manager. Similar to reference resolution, GRE will ﬁrst retrieve the context from the context model and generate the reference relative to this context. If a locative expression is necessary the GRE component has three things to decide: (1) what properties of the target object to include, (2) which object in the scene should be used as a landmark and how should that be described, and (3) which spatial relation to use (and hence which preposition to use). Several GRE algorithms have addressed the issue of generating locative expressions (Dale and Haddock 1991; Horacek 1997; Gardent 2002; Krahmer and Theune 2002; Figure 26 Interpreting the blue ball near the red ball. 297 Computational Linguistics Volume 35, Number 2 Varges 2004). However, all these algorithms assume the GRE component has access to a predeﬁned scene model that deﬁnes all the spatial relations between all the entities in the scene. For many visually situated dialog systems, in particular robotic dialog systems, this assumption is a serious drawback for these algorithms. If an agent wishes to generate a contextually appropriate reference it cannot assume the availability of a domain"
J09-2005,P02-1013,0,0.0117776,"ered by the content manager. Similar to reference resolution, GRE will ﬁrst retrieve the context from the context model and generate the reference relative to this context. If a locative expression is necessary the GRE component has three things to decide: (1) what properties of the target object to include, (2) which object in the scene should be used as a landmark and how should that be described, and (3) which spatial relation to use (and hence which preposition to use). Several GRE algorithms have addressed the issue of generating locative expressions (Dale and Haddock 1991; Horacek 1997; Gardent 2002; Krahmer and Theune 2002; Figure 26 Interpreting the blue ball near the red ball. 297 Computational Linguistics Volume 35, Number 2 Varges 2004). However, all these algorithms assume the GRE component has access to a predeﬁned scene model that deﬁnes all the spatial relations between all the entities in the scene. For many visually situated dialog systems, in particular robotic dialog systems, this assumption is a serious drawback for these algorithms. If an agent wishes to generate a contextually appropriate reference it cannot assume the availability of a domain model, rather it must dynami"
J09-2005,P97-1027,0,0.0231247,"onent is triggered by the content manager. Similar to reference resolution, GRE will ﬁrst retrieve the context from the context model and generate the reference relative to this context. If a locative expression is necessary the GRE component has three things to decide: (1) what properties of the target object to include, (2) which object in the scene should be used as a landmark and how should that be described, and (3) which spatial relation to use (and hence which preposition to use). Several GRE algorithms have addressed the issue of generating locative expressions (Dale and Haddock 1991; Horacek 1997; Gardent 2002; Krahmer and Theune 2002; Figure 26 Interpreting the blue ball near the red ball. 297 Computational Linguistics Volume 35, Number 2 Varges 2004). However, all these algorithms assume the GRE component has access to a predeﬁned scene model that deﬁnes all the spatial relations between all the entities in the scene. For many visually situated dialog systems, in particular robotic dialog systems, this assumption is a serious drawback for these algorithms. If an agent wishes to generate a contextually appropriate reference it cannot assume the availability of a domain model, rather"
J09-2005,P06-1131,1,0.572745,"tial prepositional semantics. Section 6 presents our computational models accounting for the semantics of spatial prepositions and the inﬂuence of visual context on those semantics, and Section 7 presents psycholinguistic evaluation of these models. Section 8 presents applications of the models in implemented systems. Section 8.1 presents an application of our models to the interpretation of locative expressions, based on Kelleher, Kruijff, and Costello (2006), and Section 8.2 presents algorithms which use these models to generate locative expressions to identify objects in visual scenes from Kelleher and Kruijff (2006). 2. Terminology Our computational models are designed to interpret and generate locative expressions involving spatial prepositions. The term locative expression describes “an expression involving a locative prepositional phrase together with whatever the phrase modiﬁes (noun, clause, etc.)” (Herskovits 1986, page 7). In this article we use the term target (T) to refer to the object that is being located by a locative expression and the term landmark1 (L) to refer to the object relative to which the target’s location is described; see Example (1). We will use the term distractor to describe a"
J09-2005,P06-1094,1,0.871092,"Missing"
J09-2005,J09-2001,0,\N,Missing
L18-1317,ar-etal-2012-cost,0,0.148223,"stimating the cost of corpus annotation as a step towards selecting which annotation environments are most appropriate for a given project. They perform an analysis of annotation costs on the task of correcting part of speech tags in an automatically annotated corpus. Based on these findings, they present a linear model for estimating the hourly cost of annotation for annotators of various skill levels, as well as a model for two granularities of annotation (sentence at a time and word at a time), thus providing informative guidelines for choosing an optimal annotation environment. Similarly, Balamurali et al. (2012) present an economic model to asses the benefit accruing from the increase in project cost by performing annotation. They examine the relationship between the additional investment in annotation of WordNet senses and the subsequent increase in accuracy scores on the task of sentiment analysis. Instead of evaluating the predictive models conventionally, by comparing their accuracies, they compare expected profits, which are set up in terms of costs and expected returns. They make a comparison of approaches from different economic perspectives - namely which approach yields maximum expected prof"
L18-1317,J09-1005,0,0.17584,"ifying instances of a rare linguistic phenomenon. In these contexts the outcome of particular interest is known as the positive class and there are a number of evaluation metrics designed to emphasize the ability of a model to correctly identify instances of this positive class. The F-score is an example of this type of evaluation metric2 The literature on the task of automatic type identification of idioms, more specifically verb and noun idiomatic combinations (VNIC), illustrates the use of these standard model evaluation metrics. Most of the work in this field either uses accuracy (used by Fazly et al. (2009)) or F-score (used by Muzny and Zettlemoyer (2013), Senaldi et al. (2016), Salton et al. (2017)) to compare model performance. These measures provide an appreciable sense of the reliability of a given model, which is why they are commonly used as evaluation metrics. However, because they focus on evaluation accuracy they are evaluating both the model’s ability to score an instance appropriately and the threshold the model uses. We argue that for evaluating a model that will be deployed for pre-filtering in an annotation project this focus is not appropriate. In these contexts the best model is"
L18-1317,D13-1145,0,0.0132377,"nomenon. In these contexts the outcome of particular interest is known as the positive class and there are a number of evaluation metrics designed to emphasize the ability of a model to correctly identify instances of this positive class. The F-score is an example of this type of evaluation metric2 The literature on the task of automatic type identification of idioms, more specifically verb and noun idiomatic combinations (VNIC), illustrates the use of these standard model evaluation metrics. Most of the work in this field either uses accuracy (used by Fazly et al. (2009)) or F-score (used by Muzny and Zettlemoyer (2013), Senaldi et al. (2016), Salton et al. (2017)) to compare model performance. These measures provide an appreciable sense of the reliability of a given model, which is why they are commonly used as evaluation metrics. However, because they focus on evaluation accuracy they are evaluating both the model’s ability to score an instance appropriately and the threshold the model uses. We argue that for evaluating a model that will be deployed for pre-filtering in an annotation project this focus is not appropriate. In these contexts the best model is the model that can sequence instances correctly w"
L18-1317,ringger-etal-2008-assessing,0,0.226744,"We show that in our scenario, given the choice of three systems with varying F-scores, the system with the highest F-score does not yield the highest profits. In other words, we show that the cost-benefit trade off can be more favorable if a system with a lower F-score is employed. Keywords: model evaluation, gain, budget, linguistic resource creation, idiom identification, idiom dictionary, F-score 1. Introduction Building linguistic resources, such as corpora or dictionaries, can be very labor-intensive, requiring great amounts of work-hours and expert annotation. However, as pointed out by Ringger et al. (2008), fixed budgets constrain the amount of annotation that can go into the construction of linguistic resources. In many cases, the amount of available data far exceeds the time and money that is available for annotation, so one can only afford to label a subset of the data. Furthermore, for some linguistic resources only a particular (and sometimes rare) subset of the available data is relevant. Indeed, when the target linguistic phenomenon is relatively rare, the amount of time wasted filtering nonrelevant data can be considerable. For example, consider the scenario of creating an idiom diction"
L18-1317,salton-etal-2017-idiom,1,0.846794,"interest is known as the positive class and there are a number of evaluation metrics designed to emphasize the ability of a model to correctly identify instances of this positive class. The F-score is an example of this type of evaluation metric2 The literature on the task of automatic type identification of idioms, more specifically verb and noun idiomatic combinations (VNIC), illustrates the use of these standard model evaluation metrics. Most of the work in this field either uses accuracy (used by Fazly et al. (2009)) or F-score (used by Muzny and Zettlemoyer (2013), Senaldi et al. (2016), Salton et al. (2017)) to compare model performance. These measures provide an appreciable sense of the reliability of a given model, which is why they are commonly used as evaluation metrics. However, because they focus on evaluation accuracy they are evaluating both the model’s ability to score an instance appropriately and the threshold the model uses. We argue that for evaluating a model that will be deployed for pre-filtering in an annotation project this focus is not appropriate. In these contexts the best model is the model that can sequence instances correctly with a clear separation between positive insta"
L18-1317,W16-1803,0,0.0136275,"ticular interest is known as the positive class and there are a number of evaluation metrics designed to emphasize the ability of a model to correctly identify instances of this positive class. The F-score is an example of this type of evaluation metric2 The literature on the task of automatic type identification of idioms, more specifically verb and noun idiomatic combinations (VNIC), illustrates the use of these standard model evaluation metrics. Most of the work in this field either uses accuracy (used by Fazly et al. (2009)) or F-score (used by Muzny and Zettlemoyer (2013), Senaldi et al. (2016), Salton et al. (2017)) to compare model performance. These measures provide an appreciable sense of the reliability of a given model, which is why they are commonly used as evaluation metrics. However, because they focus on evaluation accuracy they are evaluating both the model’s ability to score an instance appropriately and the threshold the model uses. We argue that for evaluating a model that will be deployed for pre-filtering in an annotation project this focus is not appropriate. In these contexts the best model is the model that can sequence instances correctly with a clear separation"
P06-1094,C04-1181,0,0.0201497,"ticipants, computed in Equation 1, and computed in Equation 3, the values displayed in Figure 7 are normalised so that proximity values have a mean of 0 and a standard deviation of 1. This normalisation simply means that all values fall in the same region of the scale, and can be easily compared visually. 750 and Roy (2002) use machine learning to obtain a statistical mapping between visual and linguistic features. Gorniak and Roy (2004) use manually constructed mappings between linguistic constructions, and probabilistic functions which evaluate whether an object can act as referent, whereas DeVault and Stone (2004) use symbolic constraint resolution. Our approach to visual grounding of language is similar to the latter two approaches. We use a Combinatory Categorial Grammar (CCG) (Baldridge and Kruijff, 2003) to describe the relation between the syntactic structure of an utterance and its meaning. We model meaning as an ontologically richly sorted, relational structure, using a description logic-like framework (Baldridge and Kruijff, 2002). We use OpenCCG for parsing and realization.3 (2) interpretation using ontology-based spatial reasoning. This yields several inferences that need to hold for the scen"
P06-1094,P06-1131,1,0.852455,"Missing"
P06-1094,P02-1041,1,0.780741,"d Roy (2004) use manually constructed mappings between linguistic constructions, and probabilistic functions which evaluate whether an object can act as referent, whereas DeVault and Stone (2004) use symbolic constraint resolution. Our approach to visual grounding of language is similar to the latter two approaches. We use a Combinatory Categorial Grammar (CCG) (Baldridge and Kruijff, 2003) to describe the relation between the syntactic structure of an utterance and its meaning. We model meaning as an ontologically richly sorted, relational structure, using a description logic-like framework (Baldridge and Kruijff, 2002). We use OpenCCG for parsing and realization.3 (2) interpretation using ontology-based spatial reasoning. This yields several inferences that need to hold for the scene, like DeVault and Stone (2004). Where we differ is in how we check whether these inferences hold. Like Gorniak and Roy (2004), we map these conditions onto the energy landscape computed by the proximity field functions. This enables us to take into account inhibition effects arising in the actual situated context, unlike Gorniak & Roy or DeVault & Stone. We convert relative proximity fields into proximal regions anchored to lan"
P06-1094,E03-1036,1,0.859307,"lisation simply means that all values fall in the same region of the scale, and can be easily compared visually. 750 and Roy (2002) use machine learning to obtain a statistical mapping between visual and linguistic features. Gorniak and Roy (2004) use manually constructed mappings between linguistic constructions, and probabilistic functions which evaluate whether an object can act as referent, whereas DeVault and Stone (2004) use symbolic constraint resolution. Our approach to visual grounding of language is similar to the latter two approaches. We use a Combinatory Categorial Grammar (CCG) (Baldridge and Kruijff, 2003) to describe the relation between the syntactic structure of an utterance and its meaning. We model meaning as an ontologically richly sorted, relational structure, using a description logic-like framework (Baldridge and Kruijff, 2002). We use OpenCCG for parsing and realization.3 (2) interpretation using ontology-based spatial reasoning. This yields several inferences that need to hold for the scene, like DeVault and Stone (2004). Where we differ is in how we check whether these inferences hold. Like Gorniak and Roy (2004), we map these conditions onto the energy landscape computed by the pro"
P06-1131,P06-1094,1,0.885157,"ogical relative < projective constrastive < projective relative. For each level of this hierarchy we require a computational model of the semantics of the relation at that level that accomodates both contrastive and relative representations. In §2 we noted that the distinctions between the semantics of the different topological prepositions is often based on functional and pragmatic issues.3 Currently, however, more psycholinguistic data is required to distinguish the cognitive load associated with the different topological prepositions. We use the model of topological proximity developed in (Kelleher et al., 2006) to model all the relations at this level. Using this model we can define the extent of a region proximal to an object. If the target or one of the distractor objects is the only object within the region of proximity around a given landmark this is taken to model a contrastive use of a topological relation relative to that landmark. If the landmark’s region of proximity contains more than one object from the target and distractor object set then it is a relative use of a topological relation. We handle the issue of frame of reference ambiguity and model the semantics of projective prepostions"
P06-1131,P82-1030,0,0.427261,"e is target landmark that can be distinguished from all the members of the set of distractor landmarks under the relation used in the locative. 3.3 Algorithm We first try to generate a distinguishing description using Algorithm 1. If this fails, we divide the context into three components: the target, the distractor objects, and the set of candidate landmarks. We then iterate through the set of candidate landmarks (using a salience ordering if there is more than one, cf. Equation 1) and try to create a distinguishing locative description. The salience ordering of the landmarks is inspired by (Conklin and McDonald, 1982) who found that the higher the salience of an object the more likely it appears in the description of the scene it was embedded in. For each candidate landmark we iterate through the hierarchy of relations, checking for each relation whether the candidate can function as a target landmark under that relation. If so we create a context model that defines the set of target and distractor landmarks. We create a distinguishing locative description by using the basic incremental algorithm to distinguish the target landmark from the distractor landmarks. If we succeed in generating a distinguishing"
P06-1131,E91-1028,0,0.948317,"tinguishes it from the other objects in the domain. We use distractor objects to indicate the ∗ The research reported here was supported by the CoSy project, EU FP6 IST ”Cognitive Systems” FP6-004250-IP. Geert-Jan M. Kruijff DFKI GmbH Saarbr¨ucken, Germany gj@dfki.de objects in the context excluding the target that at a given point in processing fulfill the description of the target object that has been generated. The description generated is said to be distinguishing if the set of distractor objects is empty. Several GRE algorithms have addressed the issue of generating locative expressions (Dale and Haddock, 1991; Horacek, 1997; Gardent, 2002; Krahmer and Theune, 2002; Varges, 2004). However, all these algorithms assume the GRE component has access to a predefined scene model. For a conversational robot operating in dynamic environments this assumption is unrealistic. If a robot wishes to generate a contextually appropriate reference it cannot assume the availability of a fixed scene model, rather it must dynamically construct one. However, constructing a model containing all the relationships between all the entities in the domain is prone to combinatorial explosion, both in terms of the number objec"
P06-1131,P02-1013,0,0.647055,"the domain. We use distractor objects to indicate the ∗ The research reported here was supported by the CoSy project, EU FP6 IST ”Cognitive Systems” FP6-004250-IP. Geert-Jan M. Kruijff DFKI GmbH Saarbr¨ucken, Germany gj@dfki.de objects in the context excluding the target that at a given point in processing fulfill the description of the target object that has been generated. The description generated is said to be distinguishing if the set of distractor objects is empty. Several GRE algorithms have addressed the issue of generating locative expressions (Dale and Haddock, 1991; Horacek, 1997; Gardent, 2002; Krahmer and Theune, 2002; Varges, 2004). However, all these algorithms assume the GRE component has access to a predefined scene model. For a conversational robot operating in dynamic environments this assumption is unrealistic. If a robot wishes to generate a contextually appropriate reference it cannot assume the availability of a fixed scene model, rather it must dynamically construct one. However, constructing a model containing all the relationships between all the entities in the domain is prone to combinatorial explosion, both in terms of the number objects in the context (the locatio"
P06-1131,P97-1027,0,\N,Missing
P16-1019,N10-1039,0,0.260628,"omatic and literal usages of potentially idiomatic phrases (Fazly et al., 2009). In this paper we focus on this second task, idiom token classification. Previous work on idiom token classification, such as (Sporleder and Li, 2009) and (Peng et al., 2014), often frame the problem in terms of modelling the global lexical context. For example, these models try to capture the fact that the idiomatic expression break the ice is likely to have a literal meaning in a context containing words such as cold, frozen or water and an idiomatic meaning in a context containing words such as meet or discuss (Li and Sporleder, 2010a). Frequently these global lexical models create a different idiom token classifier for each phrase. However, a number of papers on idiom type and token classification have pointed to a range of other features that could be useful for idiom token classification; including local syntactic and lexical patterns (Fazly et al., 2009) and cue words (Li and Sporleder, 2010a). However, in most cases these non-global features are specific to a particular phrase. So a key challenge is to identify from a range of features which features are the correct features to use for idiom token classification for"
P16-1019,D14-1179,0,0.0335165,"Missing"
P16-1019,N13-1090,0,0.0185219,"al., 2014). Idioms are pervasive across almost all languages and text genres and as a result broad cov194 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 194–204, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics did not help on the task. Socher et al. (2013), Kalchbrenner et al. (2014) and Kim (2014)). These representations are automatically trained from data and can simultaneously encode multiple linguistics features. For example, word embeddings can encode gender distinctions and plural-singular distinctions (Mikolov et al., 2013b) and the representations generated in sequence to sequence mappings have been shown to be sensitive to word order (Sutskever et al., 2014). The recent development of Skip-Thought Vectors (or Sent2Vec) (Kiros et al., 2015) has provided an approach to learn distributed representations of sentences in an unsupervised manner. In this paper we explore whether the representations generated by Sent2Vec encodes features that are useful for idiom token classification. This question is particularly interesting because the Sent2Vec based models only use the sentence containing the phrase as input where"
P16-1019,D14-1216,0,0.0742545,"s are more frequent, but for some expressions the literal meaning may be more common (Li and Sporleder, 2010a). As a result, there are two fundamental tasks in NLP idiom processing: idiom type classification is the task of identifying expressions that have possible idiomatic interpretations and idiom token classification is the task of distinguishing between idiomatic and literal usages of potentially idiomatic phrases (Fazly et al., 2009). In this paper we focus on this second task, idiom token classification. Previous work on idiom token classification, such as (Sporleder and Li, 2009) and (Peng et al., 2014), often frame the problem in terms of modelling the global lexical context. For example, these models try to capture the fact that the idiomatic expression break the ice is likely to have a literal meaning in a context containing words such as cold, frozen or water and an idiomatic meaning in a context containing words such as meet or discuss (Li and Sporleder, 2010a). Frequently these global lexical models create a different idiom token classifier for each phrase. However, a number of papers on idiom type and token classification have pointed to a range of other features that could be useful"
P16-1019,J09-1005,0,0.763987,"idioms (Villavicencio et al., 2005). A complicating factor, however, is that many idiomatic expressions can be used both literally or figuratively. In general, idiomatic usages are more frequent, but for some expressions the literal meaning may be more common (Li and Sporleder, 2010a). As a result, there are two fundamental tasks in NLP idiom processing: idiom type classification is the task of identifying expressions that have possible idiomatic interpretations and idiom token classification is the task of distinguishing between idiomatic and literal usages of potentially idiomatic phrases (Fazly et al., 2009). In this paper we focus on this second task, idiom token classification. Previous work on idiom token classification, such as (Sporleder and Li, 2009) and (Peng et al., 2014), often frame the problem in terms of modelling the global lexical context. For example, these models try to capture the fact that the idiomatic expression break the ice is likely to have a literal meaning in a context containing words such as cold, frozen or water and an idiomatic meaning in a context containing words such as meet or discuss (Li and Sporleder, 2010a). Frequently these global lexical models create a diffe"
P16-1019,W14-1007,1,0.920791,"Missing"
P16-1019,D08-1104,0,0.171886,"Missing"
P16-1019,D13-1170,0,0.00481492,"sent many challenges for Natural Language Processing (NLP) systems. For example, in Statistical Machine Translation (SMT) it has been shown that translations of sentences containing idioms receive lower scores than translations of sentences that do not contain idioms (Salton et al., 2014). Idioms are pervasive across almost all languages and text genres and as a result broad cov194 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 194–204, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics did not help on the task. Socher et al. (2013), Kalchbrenner et al. (2014) and Kim (2014)). These representations are automatically trained from data and can simultaneously encode multiple linguistics features. For example, word embeddings can encode gender distinctions and plural-singular distinctions (Mikolov et al., 2013b) and the representations generated in sequence to sequence mappings have been shown to be sensitive to word order (Sutskever et al., 2014). The recent development of Skip-Thought Vectors (or Sent2Vec) (Kiros et al., 2015) has provided an approach to learn distributed representations of sentences in an unsupervised man"
P16-1019,P14-1062,0,0.0169599,"or Natural Language Processing (NLP) systems. For example, in Statistical Machine Translation (SMT) it has been shown that translations of sentences containing idioms receive lower scores than translations of sentences that do not contain idioms (Salton et al., 2014). Idioms are pervasive across almost all languages and text genres and as a result broad cov194 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 194–204, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics did not help on the task. Socher et al. (2013), Kalchbrenner et al. (2014) and Kim (2014)). These representations are automatically trained from data and can simultaneously encode multiple linguistics features. For example, word embeddings can encode gender distinctions and plural-singular distinctions (Mikolov et al., 2013b) and the representations generated in sequence to sequence mappings have been shown to be sensitive to word order (Sutskever et al., 2014). The recent development of Skip-Thought Vectors (or Sent2Vec) (Kiros et al., 2015) has provided an approach to learn distributed representations of sentences in an unsupervised manner. In this paper we explor"
P16-1019,D14-1181,0,0.0161754,"(NLP) systems. For example, in Statistical Machine Translation (SMT) it has been shown that translations of sentences containing idioms receive lower scores than translations of sentences that do not contain idioms (Salton et al., 2014). Idioms are pervasive across almost all languages and text genres and as a result broad cov194 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 194–204, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics did not help on the task. Socher et al. (2013), Kalchbrenner et al. (2014) and Kim (2014)). These representations are automatically trained from data and can simultaneously encode multiple linguistics features. For example, word embeddings can encode gender distinctions and plural-singular distinctions (Mikolov et al., 2013b) and the representations generated in sequence to sequence mappings have been shown to be sensitive to word order (Sutskever et al., 2014). The recent development of Skip-Thought Vectors (or Sent2Vec) (Kiros et al., 2015) has provided an approach to learn distributed representations of sentences in an unsupervised manner. In this paper we explore whether the r"
P16-1019,E09-1086,0,0.504403,". In general, idiomatic usages are more frequent, but for some expressions the literal meaning may be more common (Li and Sporleder, 2010a). As a result, there are two fundamental tasks in NLP idiom processing: idiom type classification is the task of identifying expressions that have possible idiomatic interpretations and idiom token classification is the task of distinguishing between idiomatic and literal usages of potentially idiomatic phrases (Fazly et al., 2009). In this paper we focus on this second task, idiom token classification. Previous work on idiom token classification, such as (Sporleder and Li, 2009) and (Peng et al., 2014), often frame the problem in terms of modelling the global lexical context. For example, these models try to capture the fact that the idiomatic expression break the ice is likely to have a literal meaning in a context containing words such as cold, frozen or water and an idiomatic meaning in a context containing words such as meet or discuss (Li and Sporleder, 2010a). Frequently these global lexical models create a different idiom token classifier for each phrase. However, a number of papers on idiom type and token classification have pointed to a range of other featur"
P16-1019,sporleder-etal-2010-idioms,0,0.101686,". We show that classifiers using these representations have competitive performance compared with the state of the art in idiom token classification. Importantly, however, our models use only the sentence containing the target phrase as input and are thus less dependent on a potentially inaccurate or incomplete model of discourse context. We further demonstrate the feasibility of using these representations to train a competitive general idiom token classifier. 1 Introduction Idioms are a class of multiword expressions (MWEs) whose meaning cannot be derived from their individual constituents (Sporleder et al., 2010). Idioms often present idiosyncratic behaviour such as violating selection restrictions or changing the default semantic roles of syntactic categories (Sporleder and Li, 2009). Consequently, they present many challenges for Natural Language Processing (NLP) systems. For example, in Statistical Machine Translation (SMT) it has been shown that translations of sentences containing idioms receive lower scores than translations of sentences that do not contain idioms (Salton et al., 2014). Idioms are pervasive across almost all languages and text genres and as a result broad cov194 Proceedings of t"
P16-1019,C10-2078,0,0.263833,"omatic and literal usages of potentially idiomatic phrases (Fazly et al., 2009). In this paper we focus on this second task, idiom token classification. Previous work on idiom token classification, such as (Sporleder and Li, 2009) and (Peng et al., 2014), often frame the problem in terms of modelling the global lexical context. For example, these models try to capture the fact that the idiomatic expression break the ice is likely to have a literal meaning in a context containing words such as cold, frozen or water and an idiomatic meaning in a context containing words such as meet or discuss (Li and Sporleder, 2010a). Frequently these global lexical models create a different idiom token classifier for each phrase. However, a number of papers on idiom type and token classification have pointed to a range of other features that could be useful for idiom token classification; including local syntactic and lexical patterns (Fazly et al., 2009) and cue words (Li and Sporleder, 2010a). However, in most cases these non-global features are specific to a particular phrase. So a key challenge is to identify from a range of features which features are the correct features to use for idiom token classification for"
R19-1121,D16-1053,0,0.0869892,"Missing"
R19-1121,H94-1020,0,0.783436,"Merity et al. (2017), Grave et al. (2017) and Salton et al. (2017)) In this paper, we demonstrate that an efficient and effective mechanism for a memory augmented LSTM based LM (LSTM-LM) to retrieve important information from its history is to construct a representation of the LSTM unit state history that weights information in proportion to the number of timesteps the unit persisted the information. Using this strategy reinforces the decisions of the LSTM gating mechanism at each timestep regarding what is important in a sequence. Our models achieve competitive results on the Penn Treebank (Marcus et al., 1994) and on the wikitext2 (Merity et al., 2017). Structure: §2 presents the architecture of LSTMs; §3 discusses the effect of uniformly weighting the hidden states of an LSTM; §4 illustrates persistence of information in an LSTM and describes our memory augmented LSTM-LM; §5 presents experiments and results; §6 contextualizes our findings; and §7 our conclusions. 2 Long Short-Therm Memory LSTM units (aka. LSTM cells) are now a normal building block for neural based NLP systems (Bradbury et al., 2017; Murdoch and Szlam, 2017). LSTMs retain and propagate information through the dynamics of the LSTM"
R19-1121,D15-1166,0,0.132858,"Missing"
R19-1121,I17-1045,1,0.208786,"he next prediction. Consequently, the information that is captured at the beginning of a sequence containing a long-distance dependency ∗ Work done while the author was at the ADAPT Center and the Dublin Institute of Technology. is likely to have faded from the context by the time the model spans that dependency. To address these limitations, several “memory-augmented” RNNLMs architectures have been developed that attempt to retrieve relevant information from its past timesteps (e.g., Tran et al. (2016), Cheng et al. (2016), Daniluk et al. (2017), Merity et al. (2017), Grave et al. (2017) and Salton et al. (2017)) In this paper, we demonstrate that an efficient and effective mechanism for a memory augmented LSTM based LM (LSTM-LM) to retrieve important information from its history is to construct a representation of the LSTM unit state history that weights information in proportion to the number of timesteps the unit persisted the information. Using this strategy reinforces the decisions of the LSTM gating mechanism at each timestep regarding what is important in a sequence. Our models achieve competitive results on the Penn Treebank (Marcus et al., 1994) and on the wikitext2 (Merity et al., 2017). St"
R19-1121,W12-2702,0,0.0205512,"these models still struggle to process long sequences which are more likely to contain long-distance dependencies because of information fading and a bias towards more recent information. In this paper we demonstrate an effective mechanism for retrieving information in a memory augmented LSTM LM based on attending to information in memory in proportion to the number of timesteps the LSTM gating mechanism persisted the information. 1 Introduction Language Models (LM) are important components in Natural Language Processing systems, such as Statistical Machine Translation and Speech Recognition (Schwenk et al., 2012). An LM is generally used to compute the likelihood of a sequence of words appearing in a given language. Recently, Recurrent Neural Networks LMs (RNNLMs) have became the state-of-the-art approach to LMs (J´ozefowicz et al., 2016). However, RNNLMs struggle to keep their level of performance as the length of the input increases. A typical RNN-LM propagates a context vector that integrates information about previous inputs to use for the next prediction. Consequently, the information that is captured at the beginning of a sequence containing a long-distance dependency ∗ Work done while the autho"
R19-1121,D18-1489,0,0.0216856,"m the previous timesteps, the Averaging RNN-LM achieves the same level of performance of more complex models with less computation overhead. Table 2 presents the results in terms of perplexity of the models trained over the wikitext2 dataset. Although the Averaging RNN-LM is still behind the Attentive RNN-LMs and the Neural cache model of Grave et al. (2017) on this dataset, the results are encouraging given the simplicity of the Averaging RNN-LM. However, we should note that none of these models perform at the same level of the state-ofthe-art models such as those of Merity et al. (2017) and Takase et al. (2018) as we can see in Tables 1 and 2. These models use advanced regularization techniques and matrix factorization for training the RNN-LMs whilst our Averaging RNN-LM use standard LSTM trainig regime and regularization techniques. Nevertheless, we believe that by adding the regularization scheme of the AWDLSTM and the direct output connection of AWDLSTM-DOC to our models we can bridge that performance gap. 6 Discussion The Averaging LSTM-LM achieves the lowest perplexity for a single model on the PTB (see Table 1). Given the similarity of the results between the Attentive RNN-LMs of Salton et al."
R19-1150,N10-1021,0,0.0967004,"Missing"
R19-1150,N12-1034,0,0.058836,"Missing"
R19-1150,D15-1310,0,0.0146812,"ghbour-based (Brants et al., 2003; Petrovi´c et al., 2010, 2012; Moran et al., 2016; Kannan et al., 2018). In P2C models or P2A models, the novelty score is defined respectively as the distance from the new story to a cluster of existing stories (also can be considered as the distance to an existing event), or to all the existing stories. The former is usually clustering-based (Yang et al., 1998; Allan et al., 2000b; Li et al., 2017), and the latter uses all the existing data to build a system, and applies this system to the incoming story to generate a novelty score (Sch¨olkopf et al., 2001; Wurzer et al., 2015). Based on previous literature and research on FSD, it has been shown that nearest neighbour-based P2P models perform the best among all these three categories of FSD models (Wang et al., 2018). 2.1 Term Vector Models for First Story Detection As presented above, the novelty score in a P2P model is calculated by comparing the incoming story to previous stories and then finding its (approximate) nearest neighbour and the corresponding closest distance. When implementing a P2P model, the first step is to convert the raw stories to document representation vectors that can be fed into the detectio"
S14-2037,S14-2003,0,0.0219224,"oth texts, and (2) leveraging WordNet information to increase usefulness of cosine comparisons of short texts. In our experiments, text summarisation using a graph-based algorithm did not prove to be helpful. Semantic and lexical expansion based upon word relationships defined in WordNet increased the agreement of cosine similarity values with human similarity judgements. 1 Introduction This paper describes a system that addresses the problem of assessing semantic similarity between two different-sized texts. The system has been applied to SemEval-2014 Task 3, Cross-Level Semantic Similarity (Jurgens et al, 2014). The application is limited to a single comparison type, that is, paragraph to sentence. The general approach taken can be characterised as text summarisation followed by a process of semantic expansion and finally similarity computation using cosine similarity. The rationale for applying summarisation is to focus the comparison on the most important elements of the text by selecting key words to be used in the similarity comparison. This summarisation approach is based on the assumption that if summary of a paragraph is similar to the summary sentence paired with the paragraph in the task da"
S14-2037,W04-3252,0,\N,Missing
salton-etal-2017-idiom,C10-2120,0,\N,Missing
salton-etal-2017-idiom,copestake-etal-2002-multiword,0,\N,Missing
salton-etal-2017-idiom,W04-0411,0,\N,Missing
salton-etal-2017-idiom,P98-2127,0,\N,Missing
salton-etal-2017-idiom,C98-2122,0,\N,Missing
salton-etal-2017-idiom,D15-1201,0,\N,Missing
salton-etal-2017-idiom,W16-1803,0,\N,Missing
salton-etal-2017-idiom,D14-1216,0,\N,Missing
W05-1607,E91-1028,0,0.0879744,"Missing"
W05-1607,P02-1013,0,0.0382343,"Missing"
W05-1607,P97-1027,0,0.0597627,"Missing"
W14-0806,W11-0815,0,0.0169237,"ped the question are highly fixed with syntactic and lexical variations considered unacceptable usage. On the other hand idioms such as hold fire are less fixed with variations such as hold one’s fire and held fire considered to be acceptable instances of the idiom type. For reasons such as those outlined above idioms can be challenging to human speakers; but they also pose a great challenge to a range of Natural Language Processing (NLP) applications (Sag et al., 2002). While idiomatic expressions, and more generally multiword expressions, have been widely studied in a number of NLP domains (Acosta et al., 2011; Moreno-Ortiz et al., 2013), their investigation in the context of machine translation has been more limited (Bouamor et al., 2011; Salton et al., 2014). The broad goal of our work is to advance machine translation by improving the processing of idiomatic expressions. To that end, in this paper we introduce and evaluate our initial approach to the problem. We begin in the next section by giving a brief review of the problem of idiom processing in a Statistical Machine Translation (SMT) context. Following that we outline our substitution based solution to idiom processing in SMT. We then outli"
W14-0806,P05-1066,0,0.168394,"Missing"
W14-0806,P07-2045,0,0.00703147,"Missing"
W14-0806,J10-4005,0,0.038496,"slation & Idiomatic Expressions The current state-of-the-art in machine translation is phrase-based SMT (Collins et al., 2005). Phrase-based SMT systems extend basic word-byword SMT by splitting the translation process into 3 steps: the input source sentence is segmented 38 Proceedings of the 10th Workshop on Multiword Expressions (MWE 2014), pages 38–42, c Gothenburg, Sweden, 26-27 April 2014. 2014 Association for Computational Linguistics into “phrases” or multiword units; these phrases are then translated into the target language; and finally the translated phrases are reordered if needed (Koehn, 2010). Although the term phrase-based translation might imply the system works at the semantic or grammatical phrasal level, it is worth noting that the concept of a phrase in SMT is simply a frequently occurring sequence of words. Hence, standard SMT systems do not model idioms explicitly (Bouamor et al., 2011). Given the above, the question arises as to how SMT systems can best be enhanced to account for idiom usage and other similar multiword expressions. One direct way is to use a translation dictionary to insert the idiomatic MWE along with its appropriate translation into the SMT model phrase"
W14-0806,W13-1001,0,0.037184,"Missing"
W14-0806,P02-1040,0,0.0956196,"the literal wording in the target translation is then replaced with an idiomatic phrase from the target language. However if in the post-processing step the original idiom substitution is not found, or if there are no corresponding idioms in the target language, then the post-processing software does nothing. 4 Study Design We have developed an initial implementation of our substitution approach to SMT based idiom translation for the language pair English/Brazillian-Portugese. To evaluate our method we created test corpora where each sentence contained an idiom, and compared the BLEU scores (Papineni et al., 2002) of a baseline SMT system when run on these test corpora with the BLEU scores for the same SMT system when we applied our pre and post processing steps. No sentences with literal uses of the selected idiom form were used in this experiment. Consequently, three corpora were required for this experiment in addition to the three idiomatic resources introduced in the last section. The first corpus was an initial large sentence-aligned bilingual corpus that was used to build a SMT model for the language pair English/BrazilianPortuguese. The second corpus was the first of two test corpora. This corp"
W14-0806,W14-1007,1,0.831043,"Missing"
W14-0806,J09-1005,0,\N,Missing
W14-0806,W11-4533,0,\N,Missing
W14-1007,W13-1003,0,0.0297276,"Missing"
W14-1007,P05-1066,0,0.219926,"Missing"
W14-1007,P07-2045,0,0.00486597,"Missing"
W14-1007,W08-0318,0,0.0394788,"Missing"
W14-1007,J10-4005,0,0.0261133,"LP systems need to distinguish between these types of usages (Fazly et al., 2008). One of the most important NLP applications that is negatively affected by idioms is Statistical Machine Translation (SMT) systems. The current state-of-the-art in SMT are phrase-based systems (Collins et al., 2005). Phrase-based SMT systems extend the basic SMT word-by-word approach by splitting the translation process into 3 steps: the input source sentence is segmented into “phrases” or multi-word units; these phrases are translated into the target language; and the translated phrases are reordered if needed (Koehn, 2010). It is worth highlighting that although the term phrase-based translation seems to imply the system works at a phrasal level, the concept of a phrase to these systems is simply a frequently occurring sequence of words and not necessarily a semantic or grammatical phrase. These systems thus limit themselves to a direct translation of phrases without any syntactic or semantic context. Hence, standard phrase-based SMT systems do not model idioms explicitly (Bouamor et al., 2011). Unfortunately modelling idioms in order to improve SMT is not well studied (Ren et al., 2009) and examples of the dif"
W14-1007,W09-2907,0,0.171393,"ses are reordered if needed (Koehn, 2010). It is worth highlighting that although the term phrase-based translation seems to imply the system works at a phrasal level, the concept of a phrase to these systems is simply a frequently occurring sequence of words and not necessarily a semantic or grammatical phrase. These systems thus limit themselves to a direct translation of phrases without any syntactic or semantic context. Hence, standard phrase-based SMT systems do not model idioms explicitly (Bouamor et al., 2011). Unfortunately modelling idioms in order to improve SMT is not well studied (Ren et al., 2009) and examples of the difficulties in translating these expressions can be seen in the quality of the resultant output of most Machine Translation This paper describes an experiment to evaluate the impact of idioms on Statistical Machine Translation (SMT) process using the language pair English/BrazilianPortuguese. Our results show that on sentences containing idioms a standard SMT system achieves about half the BLEU score of the same system when applied to sentences that do not contain idioms. We also provide a short error analysis and outline our planned work to overcome this limitation. 1 In"
W14-1007,W14-0806,1,0.763853,"Translation system. Given the results of our experiments it is evident that the problem in translating idioms has not been solved using a standard SMT system. Such evidences and the relatively small amount of current related work on idiomatic expression translation, when compared with the amount of work on other MT aspects, indicates that there is likely not a trivial solution. To start addressing these problems, we propose a hybrid method inspired by the work developed by Okuma et al. (2008) for translating unseen words using bilingual dictionaries. Acknowledgments Our method, introduced in Salton et al. (2014), work as a pre and post-processing step. We first identify idioms in source sentences using an idiom dictionary. Then, we substitute the idiom in the source sentence with its literal meaning, taken from the dictionary and record the fact that this sentence contained a substituted idiom. For all sentences that are recorded as containing a substitution, after the translation we check if the original idiom that occurred in the source sentence has a corresponding idiom in the target language by consulting a separate bilingual dictionary. If there is a corresponding idiom in the target language th"
W14-1007,J09-1005,0,\N,Missing
W14-5401,W05-1606,0,0.0332947,"r work based on a corpus of human dialogue (Schuette et al., 2012) and are currently moving toward the same problem in human-computer dialogue. The problem of misunderstandings in human-computer dialogue has previously mostly been addressed under the aspect of problems arising from problems in speech recognition or language understanding (e.g. (Aberdeen and Ferro, 2003; Shin et al., 2002; L´opez-C´ozar et al., 2010)). The problem of producing referring expressions when it is not certain that the other participant shares the same perception and understanding of the scene has been addressed by (Horacek, 2005). More recently (Liu et al., 2012) performed a similar experiment in the context of human-human interaction. Their work was chiefly concerned with the generation of referring expressions. We report on a work in progress in which we investigate the effect of sensor problems on humancomputer dialogue using a dialogue system for a simulated robot. We describe two experiments we performed so far. Both experiments are based on a shared experimental platform. In the first experiment participants interact with a simulated robot using a text based dialogue interface to complete a series of tasks. In t"
W14-5401,P03-1054,0,0.00463554,"e first experiment. It should be emphasized that the goal of the experiments is not to evaluate the performance of the dialogue system, but to investigate the effect of perception errors on the dialogues. 2 Experiment Methodology The experiments were performed using an experiment system that was developed for this experiment. It consists of a simulated world and a dialogue system. The world contains a number of objects such as boxes and balls. These object can be manipulated by an abstract simulated robot arm. The dialogue system is a frame based dialogue system that uses the Stanford Parser (Klein and Manning, 2003) for parsing. The simulation environment was implement using Microsoft Robotics Studio. The system is capable of understanding and performing a range of simple to complicated spatial action instructions such as “Put the ball behind the red box” or “Pick up the red ball between the green box and the yellow box”. The participants interact with the system through the user interface shown in Figure 1. It consists of two elements. The simulation window shows a rendering of the simulation world that is updated in real time. The interaction window provides access to a text based chat interface that t"
W14-5401,W12-1621,0,0.0318274,"an dialogue (Schuette et al., 2012) and are currently moving toward the same problem in human-computer dialogue. The problem of misunderstandings in human-computer dialogue has previously mostly been addressed under the aspect of problems arising from problems in speech recognition or language understanding (e.g. (Aberdeen and Ferro, 2003; Shin et al., 2002; L´opez-C´ozar et al., 2010)). The problem of producing referring expressions when it is not certain that the other participant shares the same perception and understanding of the scene has been addressed by (Horacek, 2005). More recently (Liu et al., 2012) performed a similar experiment in the context of human-human interaction. Their work was chiefly concerned with the generation of referring expressions. We report on a work in progress in which we investigate the effect of sensor problems on humancomputer dialogue using a dialogue system for a simulated robot. We describe two experiments we performed so far. Both experiments are based on a shared experimental platform. In the first experiment participants interact with a simulated robot using a text based dialogue interface to complete a series of tasks. In the second experiment the participa"
W14-5401,P97-1035,0,0.156047,"offered the option to abandon a scene and go on to the next one if they thought they would not be able to complete the current scene. All utterances by the participant and the system are transcribed and annotated with their semantic 2 (a) Scene 1 (b) Target scene 1 (c) Scene 4 (d) Target scene 4 Figure 2: Two scenes from Experiment 1 and their target scenes. interpretation. The system also logs metrics that are used in the evaluation of dialogue systems to describe the cost of a dialogue, such as the task completion rate, the number of utterances, the completion time and the number of errors (Walker et al., 1997). In the following we describe two experiments we performed with this setup so far. In the first experiment participants completed a series of tasks. In the second experiment, participants also completed a series of tasks. In this iteration however, errors were introduced into the system’s perception. 3 Experiment 1 The first experiment uses the basic version of the experiment system. The purpose of the experiment was to establish how difficult the basic experiment task would be and to create a set of performance measurements that could be used to compare this version of the system to later on"
W14-5405,de-marneffe-etal-2006-generating,0,0.0419479,"Missing"
W14-5405,W10-0721,0,0.0220173,"repositions may be more sensitive to the satisfaction of the geometric constraint and hence we expect that they will co-occur with objects of more general semantic types. 3 Datasets and extraction of spatial descriptions The goal of this work is to analyse the semantics of linguistic expressions that are used to describe the relative location of objects in visual contexts. We base our analysis on two corpora of image descriptions: specifically, the IAPR TC-12 Benchmark corpus (Grubinger et al., 2006)1 which contains 20,000 images and multi-sentence descriptions and the 8K ImageFlickr dataset (Rashtchian et al., 2010)2 which contains 8108 images. In both corpora the situations and events represented by images are described by several sentences which contain spatial descriptions with prepositions: in the first case all sentences are by a single annotator and in the second case each sentence is by a different annotator. The descriptions are geometrically constrained by the visual context. On the other hand, the describers’ choice of the target and the landmarks objects and the preposition in these descriptions will tell us about their functional semantics. The main pre-processing step was to extract parts of"
W14-5405,N03-1036,0,0.029562,"elong to, WordNet (Fellbaum, 1998) appears to be an ideal tool. It contains taxonomies of words that were constructed by humans using their intuitions. While certain classification of words in the ontology are not entirely unproblematic it is nonetheless considered a gold-standard resource for lexical semantics. In particular, we are interested in finding out given a certain preposition what are the possible semantic classes of its target and landmark objects. To determine the class synset (a sense in WordNet terminology) that covers a bag of words best we use the class-labelling algorithm of Widdows (2003). Given a list of words, this algorithm finds hypernyms which subsume as many as possible words in the list, as closely as possible. The algorithm works by first defining the set of all possible hypernyms for the words in the list. In then computes a score for each of the hypernyms: a hypernym score is incremented by a small positive value for each word it subsumes (this positive value is defined as 1 divided by the square of the vertical distance in the hierarchy between the hypernym and the word) and decremented by a small negative value g for each word it does not subsume. The algorithm ret"
W14-5405,W10-0707,0,\N,Missing
W18-1401,W14-5405,1,0.751418,"f targets and landmarks per relation and (ii) log likelihood ratio to predict the strength of association of targetlandmark pairs with a spatial relation and presents ranked lists of relations by the degree of argument variation or strength of the association respectively. The approach hypothesises that functionally biased relations are more selective in the kind of targets and landmarks they co-occur with. The reasoning behind this is that geometrically it is possible to relate a wider range of objects than in the case where additional functional constrains between objects are also applied. (Dobnik and Kelleher, 2014) generalises over landmarks and targets in WordNet hierarchy and estimates the generality of the types of landmark. Again, the work hypothesises that functional relations are more restricted in their choice of target and landmark objects and therefore are generally more specific in terms of the WordNet hierarchy. Both papers present results compatible with the hypotheses where the functional or geometric nature of prepositions is predicted in line with the experimental studies (Garrod et al., 1999; Coventry et al., 2001). Sensitive to the fact that relations such as in and on not only have spa"
W18-1401,D15-1022,0,0.158976,"ometrically with several spatial relations, the knowledge of functional bias may be used as a filter, prioritising those relations that are more likely for a particular pair of objects, thereby incorporating functional knowledge. This approach to generation of spatial descriptions is therefore similar to the approach that introduces a cognitive load based hierarchy of spatial relations (Kelleher and Kruijff, 2006) or a classification-based approach that combines geometric (related to the bounding box), textual (word2vec embeddings) and visual features (final layer of a convolutional network) (Ramisa et al., 2015). The functional geometric bias of spatial relations could also be used to inform semantic parsing, for example in prepositional phrase attachment resolution (Christie et al., 2016; Delecraz et al., 2017). Previous work has investigated metrics of the semantic bias of spatial prepositions, see (Dobnik and Kelleher, 2013, 2014). (Dobnik and Kelleher, 2013) uses (i) normalised entropy of targetlandmark pairs to estimate variation of targets and landmarks per relation and (ii) log likelihood ratio to predict the strength of association of targetlandmark pairs with a spatial relation and presents"
W19-3904,E09-2008,0,0.0148146,"exhibit some types of LDDs occurring in natural datasets. Moreover, by modifying the SPk grammar we can control the LDD characteristics within a dataset generated by the grammar. To understand and validate the interaction between an SPk grammar and the characteristics of the data it generates, we used a number of datasets of SPk grammar and analyzed the properties of these datasets. Every dataset is a collection of strings and these strings strictly follow the grammar. Hence the size of the dataset (|dataset|) is the sum of the size of all the strings. The datasets were generated using foma (Hulden, 2009) and python (Avcu et al., 2017; Mahalunkar and Kelleher, 2018)2 . Below we analyze the impact of various factors on the resulting LDD characteristics. (4) i=1 where N Pi is the frequency of unique symbol i, N = Ni , K is the number of unique symbols, and ψ(Ni ) is the logarithmic derivative of the gamma function of Ni . In order to measure dependence between any two symbols at a distance D in a sequence, we design random variables X and Y so that X holds the subsequence of the original sequence from index 0 till |dataset |− 1 − D, and Y holds the subsequence from index D till |dataset |− 1; wh"
W19-3904,P10-1091,0,0.108309,"e next symbol in a sequence, and the lower the perplexity of a model the better the performance of the model. 2 3.1 Preliminaries Strictly k-Piecewise Languages (SPk) SPk languages form a subclass of regular languages. Subregular languages can be identified by mechanisms much less complicated than FiniteState Automata. Many aspects of human language such as local and non-local dependencies are similar to subregular languages (Jager and Rogers, 2012). More importantly, there are certain types of long distance (non-local) dependencies in human language which allow finite-state characterization (Heinz and Rogers, 2010). These type of LDDs can easily be characterized by SPk languages and can be easily extended to other processes. A language L, is described by a finite set of unique symbols Σ and Σ* (free monoid) is a set of finite sequences or strings of zero or more elements from Σ. Related Work: Neural Networks and Artificial Grammars Formal Language Theory, primarily developed to study the computational basis of human language is now being used extensively to analyze any rulegoverned system (Chomsky, 1956, 1959; Fitch and Friederici, 2012). Formal languages have previously been used to train RNNs and inve"
W19-4109,I17-1074,0,0.0690967,"ds. 1 Introduction Dialogue processing is a challenging task due to the nature of human conversations. Currently most Spoken Dialogue Systems (SDS) have a core component called the Dialogue Manager that is responsible for: (a) handling dialogue context and understanding user utterances by tracking dialogue states; and (b) generating useful contributions through the use of an appropriate dialogue policy. The dialogue manager component can be developed independently (Budzianowski et al., 2017; Su et al., 2017; Zhao and Eskenazi, 2016) or in an end-to-end dialogue fashion (Williams et al., 2017; Li et al., 2017; Serban et al., 2016). Between the two dialogue manager components, the dialogue state tracker is arguably the more challenging to perfect, as its performance depends on the quality of the speech recognition component, the complexity of natural language used by users, 77 Proceedings of the 1st Workshop on NLP for Conversational AI, pages 77–86 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics we focus on the remaining three slots only. The DSTC2 dataset contains 1612 dialogues in a training set, 506 in a development (validation) set, and 1117 in a test set. tur"
W19-4109,P16-1101,0,0.113366,"price range, area, and name. At every turn of the dialogue, each slot must be assigned a value from its set of possible values detailed in the task ontology. However, the analysis shows that the slot name rarely appears in the dataset (see Appendix A.1). Therefore following the approach of a number of other researchers, 3 Energy-Based Learning Energy-Based Learning is a branch of machine learning that is notable for its usefulness in structured prediction tasks. Energy-based structured prediction methods have been applied in tasks ranging from Part-of-Speech (POS) tagging (Voutilainen, 1995; Ma and Hovy, 2016) through 78 to instance segmentation tasks in computer vision (Corso et al., 2004; Li and Zhao, 2009; Ngiam et al., 2011). In all of these tasks the output is not a highly structured object, but is rather a set of labels that are not assumed to be independent of each other. The main intuition behind energy-based methods is that it is too challenging to learn a structured output Y for a given input vector X, and that instead we should learn a function that essentially assesses the goodness of fit between a given structured output Y and the input vector X. In practice we often assume that the ra"
W19-4109,W14-4337,0,0.0693155,"t also tackle the relationships between variables such as EncDec Framework (Platek et al., 2016), MTL-based model (Trinh et al., 2018), and Conditional Random Field (CRF) tracker (Kim and Banchs, 2014). When comparing our energy-based model with those, we observe that our work achieves higher accuracy than those for the DSTC2 test set. Two out of three trackers, namely the MTL-based model and EncDec Framework, try to track Dialogue States within the incremental dialogue context, that limited their performances in general. Our work does not include the incrementality phenomenon. Kim and Banchs (2014) manually define input features in their work, that do not perform well. In our work we set up the model to learn these features automatically, and see improved results. Among the state-of-the-art DSTC2 trackers, the Hybrid model (Vodolan et al., 2015, 2017) is the most similar in architecture to our work. Both approaches use a deep learning model as a feature network. The difference between their and our trackers lies in the algorithms applied on top of the feature network. For the hybrid tracker the authors apply a set of manual rule-based differentiable calculations to predict the dialogue"
W19-4109,W14-4340,0,0.0636109,"signal X. Given this intuition, energy-based approaches essentially attempt to learn a function that estimates the goodness of fit between some input feature variable X and an output hypothesis Y . Given such a trained function, a gradient descent-based inference process then searches for an appropriate Y at run-time that demonstrates the best fit to a new input vector X. To investigate the appropriateness of this method, in this paper we apply a variant of the Structured Prediction Energy Network (SPEN) (Belanger and McCallum, 2016) to the Dialogue State Tracking Challenge (DSTC) 2 dataset (Henderson et al., 2014a). To our knowledge, this is the first attempt to apply this formulation of modelling to the DST task. We benchmark our work by comparing it against a number of other dialogue state trackers including the state-of-the-art hybrid dialogue state tracker (Vodolan et al., 2015, 2017). 2 2.2 We conducted a data analysis on the DSTC2 data using the chi-square test to examine the dependencies between target variables. The chisquare test;is an important statistical test to detect associations between variables; however, this test can only give the answer to the question of whether there exist depende"
W19-4109,W17-5518,0,0.039041,"the-art results without the need for many of the other steps required by current state-of-the-art methods. 1 Introduction Dialogue processing is a challenging task due to the nature of human conversations. Currently most Spoken Dialogue Systems (SDS) have a core component called the Dialogue Manager that is responsible for: (a) handling dialogue context and understanding user utterances by tracking dialogue states; and (b) generating useful contributions through the use of an appropriate dialogue policy. The dialogue manager component can be developed independently (Budzianowski et al., 2017; Su et al., 2017; Zhao and Eskenazi, 2016) or in an end-to-end dialogue fashion (Williams et al., 2017; Li et al., 2017; Serban et al., 2016). Between the two dialogue manager components, the dialogue state tracker is arguably the more challenging to perfect, as its performance depends on the quality of the speech recognition component, the complexity of natural language used by users, 77 Proceedings of the 1st Workshop on NLP for Conversational AI, pages 77–86 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics we focus on the remaining three slots only. The DSTC2 dataset contai"
W19-4109,W14-4345,0,0.057479,"ps between slots are not properly taken into account. In order to account for such relationships in the dialogue context, it is appropriate to consider the problem not as a multi-task classification problem, as is currently common, but as a structured prediction problem. This insight is not in itself novel, as there have been several attempts in the research community to investigate the variable dependencies in dialogue state tracking such as in the multi-task learning model (Trinh et al., 2018), the language modelling tracker (Platek et al., 2016), work building on Conditional Random Fields (Kim and Banchs, 2014), work on Attention-based Sequence-to-Sequence models (Hori et al., 2016) and the work by Williams (2010). Although these architectures are good attempts to engage variable dependencies at different levels of abstraction into the dialogue state tracking process, they have not yet achieved state-of-the-art results and do not provide a clear analysis of the relationships between variables. Performing prediction of dialogue states where we acknowledge the relationship between slot values casts the problem into a structured prediction task; this is similar to how both image segmentation and part-o"
W19-4109,E17-2033,0,0.0261317,"015, 2017) is the most similar in architecture to our work. Both approaches use a deep learning model as a feature network. The difference between their and our trackers lies in the algorithms applied on top of the feature network. For the hybrid tracker the authors apply a set of manual rule-based differentiable calculations to predict the dialogue states, while in our work we implement an energy network, that is also deep learning-based. The Word-based tracker (Henderson et al., 2014b) is a fully RNN-based model, that is notable for its high performance and the feature extraction technique. Vodolan et al. (2017) as well as our work adopts this technique Results & Discussions We selected the best fitting set of hyper-parameters and the highest accuracy checkpoint from validation for use on the test set. We report our results against the DSTC2 baseline and other state-of-theart trackers (see Table 4). We choose reference dialogue state trackers that are related to our work in different aspects such as their investigation of variable dependencies or because the network architecture is similar to or inspired that which we use. The evaluation metric used on test results is the accuracy provided by the DST"
W19-4109,E95-1022,0,0.0435074,"slots; namely food, price range, area, and name. At every turn of the dialogue, each slot must be assigned a value from its set of possible values detailed in the task ontology. However, the analysis shows that the slot name rarely appears in the dataset (see Appendix A.1). Therefore following the approach of a number of other researchers, 3 Energy-Based Learning Energy-Based Learning is a branch of machine learning that is notable for its usefulness in structured prediction tasks. Energy-based structured prediction methods have been applied in tasks ranging from Part-of-Speech (POS) tagging (Voutilainen, 1995; Ma and Hovy, 2016) through 78 to instance segmentation tasks in computer vision (Corso et al., 2004; Li and Zhao, 2009; Ngiam et al., 2011). In all of these tasks the output is not a highly structured object, but is rather a set of labels that are not assumed to be independent of each other. The main intuition behind energy-based methods is that it is too challenging to learn a structured output Y for a given input vector X, and that instead we should learn a function that essentially assesses the goodness of fit between a given structured output Y and the input vector X. In practice we ofte"
W19-4109,P17-1062,0,0.0416625,"state-of-the-art methods. 1 Introduction Dialogue processing is a challenging task due to the nature of human conversations. Currently most Spoken Dialogue Systems (SDS) have a core component called the Dialogue Manager that is responsible for: (a) handling dialogue context and understanding user utterances by tracking dialogue states; and (b) generating useful contributions through the use of an appropriate dialogue policy. The dialogue manager component can be developed independently (Budzianowski et al., 2017; Su et al., 2017; Zhao and Eskenazi, 2016) or in an end-to-end dialogue fashion (Williams et al., 2017; Li et al., 2017; Serban et al., 2016). Between the two dialogue manager components, the dialogue state tracker is arguably the more challenging to perfect, as its performance depends on the quality of the speech recognition component, the complexity of natural language used by users, 77 Proceedings of the 1st Workshop on NLP for Conversational AI, pages 77–86 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics we focus on the remaining three slots only. The DSTC2 dataset contains 1612 dialogues in a training set, 506 in a development (validation) set, and 1117 i"
W19-5910,W17-5526,0,0.0531723,"Missing"
W19-5910,N18-1187,0,0.0131995,"oughout the conversation history (El Asri et al., 2017). By casting the dialogue representation as a set of slots to be tracked, the dialogue state tracking process itself is most frequently tackled as a multi-task classification problem. In recent years, various deep learning approaches that track dialogue states as a combination of individual classification tasks have been proposed (Ren et al., 2018; Perez and Liu, 2017; Vodolan et al., 2017; Mrksic et al., 2017; Rastogi et al., 2017). However, while these systems achieve state-of-the-art results, there remains notable room for improvement (Liu et al., 2018). Our work begins with the hypothesis that by treating dialogue state tracking as a simple multi-label classification task, we are not taking into account the relationships between dialogue state slot variables. This hypothesis is based in part on experience from other applications of machine learning that have demonstrated that taking target variable dependencies into account is useful, but is also based on the intuition that a human interlocutor would of course take multiple target variables into account while interpreting language (Landragin, 2013). Given the above argument, in this paper w"
W19-5910,P15-2130,0,0.367643,"Missing"
W19-5910,W14-4337,0,0.0532114,"Missing"
W19-5910,P17-1163,0,0.0300784,"Missing"
W19-5910,W13-4065,0,0.033139,"Missing"
W19-5910,E17-1029,0,0.016054,"dialogue state tracking datasets such as Let’s Go (Raux et al., 2005), though the structure can also be made more complex as is the case in the tracking of multiple frames of dialogue states throughout the conversation history (El Asri et al., 2017). By casting the dialogue representation as a set of slots to be tracked, the dialogue state tracking process itself is most frequently tackled as a multi-task classification problem. In recent years, various deep learning approaches that track dialogue states as a combination of individual classification tasks have been proposed (Ren et al., 2018; Perez and Liu, 2017; Vodolan et al., 2017; Mrksic et al., 2017; Rastogi et al., 2017). However, while these systems achieve state-of-the-art results, there remains notable room for improvement (Liu et al., 2018). Our work begins with the hypothesis that by treating dialogue state tracking as a simple multi-label classification task, we are not taking into account the relationships between dialogue state slot variables. This hypothesis is based in part on experience from other applications of machine learning that have demonstrated that taking target variable dependencies into account is useful, but is also based"
W19-5910,W14-4340,0,0.0204828,"ep learning network to process raw DSTC dialogue data into a representation that is suitable for feeding into the energy network. As DSTC dialogues contain different input channels we implement different techniques to accommodate the variety of input variables. In detail, each input of a dialogue turn consists E(x, y, θ) = Eglobal (y, θ) + Elocal (x, y, θ) (3) 78 Figure 1: Deep Value Network-based Dialogue State Tracking Model. of machine acts in a semantic format and user utterance transcribed by an automatic speech recognizer. We parse the machine dialogue acts with the parsing technique by Henderson et al. (2014d) before reducing the dimensionality of the machine act vectors with two dense neural layers. Meanwhile, all the words in user utterances are embedded with an online trained embedding layer, then passed into a bidirectional LSTM layer (Hochreiter and Schmidhuber, 1997). The output vectors of this bidirectional LSTM layer represent user utterances as real-valued tensors. Following that, the machine act and utterance vectors are concatenated, and fed into a unidirectional LSTM layer that processes dialogue by turn and returns fixedsize dialogue vector representations. We pre-train this feature"
W19-5910,W13-4071,0,0.0161267,"ture function F (X) that transforms the input to an appropriate representation form that better supports the inference process. Thus more commonly we denote the energy function as E(F (X), Y ). Both the feature representation and the energy function itself can be trained through a deep neural network model either dependently or independently. in itself should not be a surprise to the research community, as several researchers have built dialogue state trackers around models that can in principle be thought of as structured classifiers (Zhong et al., 2018; Hori et al., 2016; Jang et al., 2016; Ren et al., 2013). One of the challenges for previous approaches to structured prediction for dialogue state classification is that they relied on methods that had difficulty integrating a structural component that took inter-slot dependencies into account with a robust underlying classifier that facilitated powerful feature representations from individual contributions to the dialogue. Recently the application of energy-based methods that are implemented through neural architectures have provided one promising avenue for structured prediction. The idea underpinning this approach is that we learn to rate the a"
W19-5910,E17-2033,0,0.0703479,"ng datasets such as Let’s Go (Raux et al., 2005), though the structure can also be made more complex as is the case in the tracking of multiple frames of dialogue states throughout the conversation history (El Asri et al., 2017). By casting the dialogue representation as a set of slots to be tracked, the dialogue state tracking process itself is most frequently tackled as a multi-task classification problem. In recent years, various deep learning approaches that track dialogue states as a combination of individual classification tasks have been proposed (Ren et al., 2018; Perez and Liu, 2017; Vodolan et al., 2017; Mrksic et al., 2017; Rastogi et al., 2017). However, while these systems achieve state-of-the-art results, there remains notable room for improvement (Liu et al., 2018). Our work begins with the hypothesis that by treating dialogue state tracking as a simple multi-label classification task, we are not taking into account the relationships between dialogue state slot variables. This hypothesis is based in part on experience from other applications of machine learning that have demonstrated that taking target variable dependencies into account is useful, but is also based on the intuition that"
W19-5910,W14-4339,0,0.028265,"Missing"
