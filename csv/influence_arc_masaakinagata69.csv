2003.mtsummit-papers.56,C00-1004,0,0.0280419,"rbi alignment. Recall is defined as the number of correct word pairs extracted by Viterbi alignment divided by the number of word pairs in the reference data. Although our method can be applied to many 3 This number equals the smallest number of sentences in the three corpora considered. TMs, in these experiments we employed the TM described in (Yamada and Knight, 2001). Because Japanese structure is quite different from English structure. We used the English part of speech tagger described in (Brill, 2000), the English parser in (Collins, 1999) for English, and the morphological analyzer in (Asahara and Matsumoto, 2000), the parser in (Kudo and Matsumoto, 2002) for Japanese analysis. Additionally, we changed the English parse tree as in (Yamada and Knight, 2001) and slightly flattened the Japanese parse tree. The results of the experiment indicate that our approach can improve word level TM quality regardless of the domain and TM, but its effectiveness depends on the amount of training data. Moreover, we can say that common word pairs are as useful as dictionary entries. Figure 7: Second E-J alignment Figure 8: Second J-E alignment 3.1 The individual domains This section shows that the proposed method can im"
2003.mtsummit-papers.56,C88-1016,0,0.0541435,"Missing"
2003.mtsummit-papers.56,W99-0613,0,0.0250924,"e other input string in language B is S B = b1 ...b j ...bn . It follows that word pairs extracted by alignment using P(A|B) are Wba = {(b j , ai )|i = 1..m, j = 1..n} and the word pairs extracted by alignment using P(B|A) are Wab = {(ai , b j )|i = 1..m, j = 1..n}. Let one set of all word pairs for language A be WA and the corresponding set of all word pairs for language B be WB , the common word pairs are then defined as Wab ∩ Wba where (b j , ai ) = (ai , b j ) in WA and WB . Our approach to using common word pairs is similar to co-training methods (Yarowsky, 1995; Blum and Mitchell, 1998; Collins and Singer, 1999). This is because our method can be considered as using the common word pairs as seeds in co-training, and that common word pairs can be extracted by asymmetrical learning. Both translation model P(A|B) and P(B|A) are improved by the use of common word pairs. Although we can easily apply our method to many of the TMs that have asymmetrical learning between source and target languages, this paper examines the TM in (Yamada and Knight, 2001) in order to challenge the method with languages whose structures are quite different, i.e. English and Japanese. Performance was evaluated in terms of the f"
2003.mtsummit-papers.56,W02-2016,0,0.0222465,"r of correct word pairs extracted by Viterbi alignment divided by the number of word pairs in the reference data. Although our method can be applied to many 3 This number equals the smallest number of sentences in the three corpora considered. TMs, in these experiments we employed the TM described in (Yamada and Knight, 2001). Because Japanese structure is quite different from English structure. We used the English part of speech tagger described in (Brill, 2000), the English parser in (Collins, 1999) for English, and the morphological analyzer in (Asahara and Matsumoto, 2000), the parser in (Kudo and Matsumoto, 2002) for Japanese analysis. Additionally, we changed the English parse tree as in (Yamada and Knight, 2001) and slightly flattened the Japanese parse tree. The results of the experiment indicate that our approach can improve word level TM quality regardless of the domain and TM, but its effectiveness depends on the amount of training data. Moreover, we can say that common word pairs are as useful as dictionary entries. Figure 7: Second E-J alignment Figure 8: Second J-E alignment 3.1 The individual domains This section shows that the proposed method can improve word alignment quality by a few perc"
2003.mtsummit-papers.56,P01-1050,0,0.0576258,"t powerful enough to treat languages that have quite different structures, such as English and Japanese. To solve this problem, several methods (Yamada and Knight, 2001; Watanabe et al., 2002) that use structural information have been proposed. However, their TMs are still not strong enough. One simple approach to improving the quality is to add dictionary entries to training data. However, it is rare for a dictionary to include all word pairs appearing the many training sentences available. One interesting approach is seen in the TMs described in (Vogel et al., 1996; Yamada and Knight, 2001; Marcu, 2001; Watanabe et al., 2002); they set correspondences from one language to the other. Accord‡ Current affiliation is Xerox Research Centre Europe. ingly, we paid attention to both training languages. If the training languages are A and B, both P(A|B) and P(B|A) can be used to enhance TM performance. Although P(A|B) is not the same as P(B|A), we can expect that the word pairs yielded by each TM are the same. Och et al. proposed a method that uses alignment templates from both TMs (Och et al., 1999). They improved alignment quality by combining the two alignments using a heuristic. They reported th"
2003.mtsummit-papers.56,W99-0604,0,0.540472,"le. One interesting approach is seen in the TMs described in (Vogel et al., 1996; Yamada and Knight, 2001; Marcu, 2001; Watanabe et al., 2002); they set correspondences from one language to the other. Accord‡ Current affiliation is Xerox Research Centre Europe. ingly, we paid attention to both training languages. If the training languages are A and B, both P(A|B) and P(B|A) can be used to enhance TM performance. Although P(A|B) is not the same as P(B|A), we can expect that the word pairs yielded by each TM are the same. Och et al. proposed a method that uses alignment templates from both TMs (Och et al., 1999). They improved alignment quality by combining the two alignments using a heuristic. They reported that their approach improved the translation results. However, no information was provided on the TM improvement possible by using only P(A|B) and P(B|A). This paper describes several experiments conducted to elucidate this effect. In this paper, we call the word pairs extracted by asymmetrical learning common word pairs. Given that one input string in language A is S A = a1 ...ai ...am , the other input string in language B is S B = b1 ...b j ...bn . It follows that word pairs extracted by align"
2003.mtsummit-papers.56,C02-1032,0,0.0286185,"o make the improvement more effective, we might need to directly utilize TM probability as is described in (Och et al., 1999). This is because the TM has many useless word pairs. For instance, it is possible to extend the proposed approach by weighting each common word pair with a probability according to the part of speech involved. The above text discussed only word alignment, but we can also imagine expanding our approach to cover phrase alignment or n-to-n word alignment. Phrase alignment or n-to-n word alignment approaches for statistical translation (e.g. (Och et al., 1999; Marcu, 2001; Varea et al., 2002; Watanabe et al., 2002)) have been proposed. Adding the proposed approach to these algorithms would improve their performance. 5 Conclusion We have proposed a method that improves translation model quality by using common word pairs extracted by asymmetric learning. If P(A|B) and P(B|A) are training models (TMs), we can get common word pairs from both results of Viterbi alignment using P(A|B) and P(B|A). By retraining P(A|B) and P(B|A) using the original training sentences plus the above common word pairs, the quality of both P(A|B) and P(B|A) are improved. We conducted experiments on English"
2003.mtsummit-papers.56,C96-2141,0,0.0612174,"tring-to-string noisy channel model, it is not powerful enough to treat languages that have quite different structures, such as English and Japanese. To solve this problem, several methods (Yamada and Knight, 2001; Watanabe et al., 2002) that use structural information have been proposed. However, their TMs are still not strong enough. One simple approach to improving the quality is to add dictionary entries to training data. However, it is rare for a dictionary to include all word pairs appearing the many training sentences available. One interesting approach is seen in the TMs described in (Vogel et al., 1996; Yamada and Knight, 2001; Marcu, 2001; Watanabe et al., 2002); they set correspondences from one language to the other. Accord‡ Current affiliation is Xerox Research Centre Europe. ingly, we paid attention to both training languages. If the training languages are A and B, both P(A|B) and P(B|A) can be used to enhance TM performance. Although P(A|B) is not the same as P(B|A), we can expect that the word pairs yielded by each TM are the same. Och et al. proposed a method that uses alignment templates from both TMs (Och et al., 1999). They improved alignment quality by combining the two alignmen"
2003.mtsummit-papers.56,2002.tmi-papers.20,0,0.112773,"ng domain and the translation model. Moreover, we show that common word pairs are almost as useful as regular dictionary entries for training purposes. 1 Introduction The statistical Machine Translation model was proposed by (Brown et al., 1988). This model has two components: a translation model(TM) and a language model. However, since the TM in (Brown et al., 1988) is based on a string-to-string noisy channel model, it is not powerful enough to treat languages that have quite different structures, such as English and Japanese. To solve this problem, several methods (Yamada and Knight, 2001; Watanabe et al., 2002) that use structural information have been proposed. However, their TMs are still not strong enough. One simple approach to improving the quality is to add dictionary entries to training data. However, it is rare for a dictionary to include all word pairs appearing the many training sentences available. One interesting approach is seen in the TMs described in (Vogel et al., 1996; Yamada and Knight, 2001; Marcu, 2001; Watanabe et al., 2002); they set correspondences from one language to the other. Accord‡ Current affiliation is Xerox Research Centre Europe. ingly, we paid attention to both trai"
2003.mtsummit-papers.56,P01-1067,1,0.943996,"independent of the training domain and the translation model. Moreover, we show that common word pairs are almost as useful as regular dictionary entries for training purposes. 1 Introduction The statistical Machine Translation model was proposed by (Brown et al., 1988). This model has two components: a translation model(TM) and a language model. However, since the TM in (Brown et al., 1988) is based on a string-to-string noisy channel model, it is not powerful enough to treat languages that have quite different structures, such as English and Japanese. To solve this problem, several methods (Yamada and Knight, 2001; Watanabe et al., 2002) that use structural information have been proposed. However, their TMs are still not strong enough. One simple approach to improving the quality is to add dictionary entries to training data. However, it is rare for a dictionary to include all word pairs appearing the many training sentences available. One interesting approach is seen in the TMs described in (Vogel et al., 1996; Yamada and Knight, 2001; Marcu, 2001; Watanabe et al., 2002); they set correspondences from one language to the other. Accord‡ Current affiliation is Xerox Research Centre Europe. ingly, we pai"
2003.mtsummit-papers.56,P95-1026,0,0.00645062,"n language A is S A = a1 ...ai ...am , the other input string in language B is S B = b1 ...b j ...bn . It follows that word pairs extracted by alignment using P(A|B) are Wba = {(b j , ai )|i = 1..m, j = 1..n} and the word pairs extracted by alignment using P(B|A) are Wab = {(ai , b j )|i = 1..m, j = 1..n}. Let one set of all word pairs for language A be WA and the corresponding set of all word pairs for language B be WB , the common word pairs are then defined as Wab ∩ Wba where (b j , ai ) = (ai , b j ) in WA and WB . Our approach to using common word pairs is similar to co-training methods (Yarowsky, 1995; Blum and Mitchell, 1998; Collins and Singer, 1999). This is because our method can be considered as using the common word pairs as seeds in co-training, and that common word pairs can be extracted by asymmetrical learning. Both translation model P(A|B) and P(B|A) are improved by the use of common word pairs. Although we can easily apply our method to many of the TMs that have asymmetrical learning between source and target languages, this paper examines the TM in (Yamada and Knight, 2001) in order to challenge the method with languages whose structures are quite different, i.e. English and J"
2005.iwslt-1.16,2003.mtsummit-papers.53,0,\N,Missing
2005.iwslt-1.16,koen-2004-pharaoh,0,\N,Missing
2005.iwslt-1.16,W03-1506,1,\N,Missing
2005.iwslt-1.16,W02-1021,0,\N,Missing
2005.iwslt-1.16,J04-4002,0,\N,Missing
2005.iwslt-1.16,J03-1002,0,\N,Missing
2006.iwslt-evaluation.11,P05-1033,0,0.0533135,"cal machine translation method. In the following sections, we ﬁrst explain our translation model and phrase reordering model. We then report the experiments’ results using our phrase reordering model based on predicate-argument structure. 1. Introduction 2. Baseline Translation Model Recently, phrase-based statistical machine translation model has become the mainstream in the machine translation community. Phrase-based approaches are capable of constructing better context-dependent word selection model than wordbased approaches. Though the unit of translation is still under active development [1], there is no approach more widely used than phrase-based one. Statistical machine translation, however, uses less linguistic knowledge such as syntax and semantics than conventional rule-based machine translation systems. For instance, the chunk-based approach in [2] does not rely on monolingual chunker and the hierarchical phrase-based approach in [1] does not use any kind of syntactic information except for a synchronous context-free grammar. Some SMT systems, however, try to incorporate syntactic knowledge, such as [3], yet it is hard to use it effectively as described in [4]. Another issu"
2006.iwslt-evaluation.11,P03-1039,0,0.0209387,"ranslation Model Recently, phrase-based statistical machine translation model has become the mainstream in the machine translation community. Phrase-based approaches are capable of constructing better context-dependent word selection model than wordbased approaches. Though the unit of translation is still under active development [1], there is no approach more widely used than phrase-based one. Statistical machine translation, however, uses less linguistic knowledge such as syntax and semantics than conventional rule-based machine translation systems. For instance, the chunk-based approach in [2] does not rely on monolingual chunker and the hierarchical phrase-based approach in [1] does not use any kind of syntactic information except for a synchronous context-free grammar. Some SMT systems, however, try to incorporate syntactic knowledge, such as [3], yet it is hard to use it effectively as described in [4]. Another issue in statistical machine translation is reordering. Global reordering is essential to translation of languages with different word orders [5], and some aspects of global reordering in translation between German and English was stated in [6] and [7]. They used some heu"
2006.iwslt-evaluation.11,P01-1067,0,0.0677036,"aches. Though the unit of translation is still under active development [1], there is no approach more widely used than phrase-based one. Statistical machine translation, however, uses less linguistic knowledge such as syntax and semantics than conventional rule-based machine translation systems. For instance, the chunk-based approach in [2] does not rely on monolingual chunker and the hierarchical phrase-based approach in [1] does not use any kind of syntactic information except for a synchronous context-free grammar. Some SMT systems, however, try to incorporate syntactic knowledge, such as [3], yet it is hard to use it effectively as described in [4]. Another issue in statistical machine translation is reordering. Global reordering is essential to translation of languages with different word orders [5], and some aspects of global reordering in translation between German and English was stated in [6] and [7]. They used some heuristics to pre-process German corpus and reported successful results. In this paper, we present a novel phrase reordering model based on a predicate-argument structure analyzer. Given predicate-argument structure information from the analyzer, source sentence"
2006.iwslt-evaluation.11,N03-1017,0,0.0124383,"e development [1], there is no approach more widely used than phrase-based one. Statistical machine translation, however, uses less linguistic knowledge such as syntax and semantics than conventional rule-based machine translation systems. For instance, the chunk-based approach in [2] does not rely on monolingual chunker and the hierarchical phrase-based approach in [1] does not use any kind of syntactic information except for a synchronous context-free grammar. Some SMT systems, however, try to incorporate syntactic knowledge, such as [3], yet it is hard to use it effectively as described in [4]. Another issue in statistical machine translation is reordering. Global reordering is essential to translation of languages with different word orders [5], and some aspects of global reordering in translation between German and English was stated in [6] and [7]. They used some heuristics to pre-process German corpus and reported successful results. In this paper, we present a novel phrase reordering model based on a predicate-argument structure analyzer. Given predicate-argument structure information from the analyzer, source sentence is reordered according to match that of the target languag"
2006.iwslt-evaluation.11,P05-1066,0,0.122301,"the chunk-based approach in [2] does not rely on monolingual chunker and the hierarchical phrase-based approach in [1] does not use any kind of syntactic information except for a synchronous context-free grammar. Some SMT systems, however, try to incorporate syntactic knowledge, such as [3], yet it is hard to use it effectively as described in [4]. Another issue in statistical machine translation is reordering. Global reordering is essential to translation of languages with different word orders [5], and some aspects of global reordering in translation between German and English was stated in [6] and [7]. They used some heuristics to pre-process German corpus and reported successful results. In this paper, we present a novel phrase reordering model based on a predicate-argument structure analyzer. Given predicate-argument structure information from the analyzer, source sentence is reordered according to match that of the target language. The translation model trained on a reWe followed the noisy channel approach to machine translation. In this approach, we search for the target (English) sentence by maximizing the probability of the target sentence eˆ given the source (foreign) senten"
2006.iwslt-evaluation.11,2001.mtsummit-papers.45,0,0.178026,"k-based approach in [2] does not rely on monolingual chunker and the hierarchical phrase-based approach in [1] does not use any kind of syntactic information except for a synchronous context-free grammar. Some SMT systems, however, try to incorporate syntactic knowledge, such as [3], yet it is hard to use it effectively as described in [4]. Another issue in statistical machine translation is reordering. Global reordering is essential to translation of languages with different word orders [5], and some aspects of global reordering in translation between German and English was stated in [6] and [7]. They used some heuristics to pre-process German corpus and reported successful results. In this paper, we present a novel phrase reordering model based on a predicate-argument structure analyzer. Given predicate-argument structure information from the analyzer, source sentence is reordered according to match that of the target language. The translation model trained on a reWe followed the noisy channel approach to machine translation. In this approach, we search for the target (English) sentence by maximizing the probability of the target sentence eˆ given the source (foreign) sentence fˆ. B"
2006.iwslt-evaluation.11,J03-1002,0,0.0120577,"of the source phrase phrase that was translated into the ith target phrase and bi−1 denotes the end position of the source phrase translated into the (i − 1)th target phrase. 77 I P (f 1 |eI1 ) = ΠIi=1 φ(f i |ei )d(ai a predicate-argument structure analyzer [12] on our corpus, which assigns these three cases to the arguments of predicates given a sentence. Figure 1 describes Japanese predicate-argument structure analysis of the following sentence: − bi−1 ) Translation probability is obtained from the relative frequency of the source phrase given the target phrase aligned by the GIZA++ toolkit [9]. count(f , e) φ(f |e) = ∑ f count(f , e) 住所/address を/WO-ACC ここ/here に/NILOC 書い/write て/PARTICLE 下さい/please where count(f , e) gives the source phrase f aligned to the target phrase e in the parallel corpus. The distortion model can be deﬁned as follows with an appropriate value for the parameter α: In this case, “書い/write て/PARTICLE 下さい/please” is identiﬁed as a predicate, “住所/address を/WO-ACC” is assigned WO case, and “ここ/here に/NI-LOC” is assigned NI case, respectively. Our predicate-argument structure analyzer does not only use dependency information and explicit case markers, but also us"
2006.iwslt-evaluation.11,P06-1079,1,0.810285,"f 1 . Each I source phrase f i in f 1 is translated into a a target phrase ei . The target phrases may be reordered. Phrase translation is then modeled by a probability distribution φ(f i |ei ) and reordering of target phrases is modeled by a relative distortion probability distribution d(ai − bi−1 ), where ai denotes the starting position of the source phrase phrase that was translated into the ith target phrase and bi−1 denotes the end position of the source phrase translated into the (i − 1)th target phrase. 77 I P (f 1 |eI1 ) = ΠIi=1 φ(f i |ei )d(ai a predicate-argument structure analyzer [12] on our corpus, which assigns these three cases to the arguments of predicates given a sentence. Figure 1 describes Japanese predicate-argument structure analysis of the following sentence: − bi−1 ) Translation probability is obtained from the relative frequency of the source phrase given the target phrase aligned by the GIZA++ toolkit [9]. count(f , e) φ(f |e) = ∑ f count(f , e) 住所/address を/WO-ACC ここ/here に/NILOC 書い/write て/PARTICLE 下さい/please where count(f , e) gives the source phrase f aligned to the target phrase e in the parallel corpus. The distortion model can be deﬁned as follows with"
2006.iwslt-evaluation.11,P06-1132,0,0.0301701,"Missing"
2006.iwslt-evaluation.11,J05-1004,0,0.0289442,"Missing"
2006.iwslt-evaluation.11,W03-1707,0,0.0504988,"Missing"
2006.iwslt-evaluation.11,W02-2016,1,0.823108,"error rate training (MERT) tool provided by CMU [20] with 500 normal order sentences to tune 4. Experiments and Discussions 4.1. Corpus and Tools We participated in Open Data Track in Japanese-English translation because we have built only Japanese predicateargument structure analyzer and thus source language is limited to Japanese in our phrase reordering model. We used ChaSen [16] for Word segmentation and POS tagging for Japanese. We did not use the original word segmentation information of Japanese because we used another POS tagger, ChaSen, instead. Dependency parsing was done by CaboCha [17]. We used tokenizer.sed from LDC to tokenize English sentences, and MXPOST [18] for POS tagging. Word translation probabilities were calculated by GIZA++ [9]. English words were lowercased for training and testing. We used a back-off word trigram model for the language model. It is trained on the lowercased English side of the parallel corpus by Palmkit [19]. We ﬁrst manually aligned English and Japanese sentences and obtained parallel corpus of 45,909 JapaneseEnglish sentences from 39,953 conversations. We then reordered Japanese sentences by using the predicate-argument structure analyzer. W"
2006.iwslt-evaluation.11,koen-2004-pharaoh,0,\N,Missing
2006.iwslt-evaluation.11,2005.iwslt-1.16,1,\N,Missing
2011.mtsummit-papers.34,2008.iwslt-papers.1,0,0.0222347,"presented a method to obtain more reliable translation estimates from small data sets using multi-parallel data. Different from the previous approaches, we work on a black-boxed translation system, which means generation of the virtual data can be performed on any kind of translation systems including rule based, statistical based or even human translation. The approach introduced in (Leusch et al., 2010) can combine the translation output of a test set produced by any pivot MTs per different languages, however the individual systems are not improved and novel training data is not exploited. Bertoldi et al. (2008) evaluated several methods of pivot languages but did apply the global corpus ﬁltering i.e. compatibility measure to control the quality of data. Our purpose is not only to improve the translation quality but also to provide useful linguistic resources for other NLP tasks. 5 Conclusion and Future Work Thousands of human languages are recognized in the world, and building up millions of translation systems between these language pairs suffers greatly on the scarce resource, such as parallel data. We introduced the idea of compatibility, where all languages can be mapped to the same semantic mea"
2011.mtsummit-papers.34,J93-2003,0,0.0230862,"f each sentence pair and choose a certain percentage of the best scored sentences for training. In order to include information from various resources, the quality of a sentence pair is measured using a log-linear model combining different sub-models. Let (f1J , eI1 ) be a bilingual sentence, the evaluation is performed using the following Equation: H(f1J , eI1 ) = M  λm hm (f1J , eI1 ) m=1 409 hm (f1J , eI1 ) is a score evaluated on this sentence pair using sub-model m. Each model m is assigned with a feature weight λm . For simplicity, we only include the negative logarithm of IBM model 1 (Brown et al., 1993) in normal and inverse direction as submodels. We combine the IBM model 1 in both directions in the log-linear model with an equal weight for each direction. We use the training software GIZA++ (Och and Ney, 2003) to obtain the lexicon probability. 3 Experimental Results 3.1 MT Setup We apply Moses (Koehn et al., 2007) as our baseline translation system and train standard alignment models in both directions with GIZA++ (Och and Ney, 2003) using models of IBM-1 (Brown et al., 1993), HMM (Vogel et al., 1996) and IBM-4 (Brown et al., 1993) which brings us the optimal translation performance and e"
2011.mtsummit-papers.34,2002.tmi-tutorials.2,0,0.0607427,"n the idea of compatibility. Generally speaking, the quality of compatible predictions provided by multiple systems is more reliable. For simple classiﬁcation problems, it is reasonable to take a prediction as good which the multiple systems agree on. This idea is widely used in ensemble learning and semi-supervised learning. Take Bootstrap aggregating, a meta-algorithm for ensemble learning as an example, multiple models are separately trained on randomly generated sub-samples, and then vote to achieve ﬁnal predictions. Another example closely related to our method is co-training such as in (Callison-Burch, 2002). One way to select automatic predictions for re-training in co-training is to choose the agreed ones. Different from simple classiﬁcation problems, even complex structured prediction problems such as parsing, the output of MT is in human languages, which may be the most complicated way to represent the meaning of another human language. It is too strict to ask multiple systems to provide exactly the same translated sentence for an input. We extend the agreement idea to the compatibility idea. Informally, two sentences are called compatible if they express the same meaning to some extent. We c"
2011.mtsummit-papers.34,P07-1092,0,0.0141798,"language. The cognate string edit distance was applied instead of a general MT system, so that the vocabulary learning is limited to mostly European languages. For bridge or pivot languages in MT, Kumar et al. (2007) described a method to improve word alignment quality using multiple bridge languages. In (Wu and Wang, 2007) and (Habash and Hu, 2009) phrase translation tables are improved using the phrase tables obtained from pivot languages in different ways, and in (Eisele et al., 2008) a hybrid method combining RBMT and SMT systems is introduced to ﬁll up the data gap for pivot translation. Cohn and Lapata (2007) presented a method to obtain more reliable translation estimates from small data sets using multi-parallel data. Different from the previous approaches, we work on a black-boxed translation system, which means generation of the virtual data can be performed on any kind of translation systems including rule based, statistical based or even human translation. The approach introduced in (Leusch et al., 2010) can combine the translation output of a test set produced by any pivot MTs per different languages, however the individual systems are not improved and novel training data is not exploited."
2011.mtsummit-papers.34,P08-1010,0,0.0304425,"Missing"
2011.mtsummit-papers.34,2008.eamt-1.6,0,0.0108565,"d Yarowsky (2001) presented a method to induce translation lexicon based on transduction 412 models of cognate pairs via bridge language. The cognate string edit distance was applied instead of a general MT system, so that the vocabulary learning is limited to mostly European languages. For bridge or pivot languages in MT, Kumar et al. (2007) described a method to improve word alignment quality using multiple bridge languages. In (Wu and Wang, 2007) and (Habash and Hu, 2009) phrase translation tables are improved using the phrase tables obtained from pivot languages in different ways, and in (Eisele et al., 2008) a hybrid method combining RBMT and SMT systems is introduced to ﬁll up the data gap for pivot translation. Cohn and Lapata (2007) presented a method to obtain more reliable translation estimates from small data sets using multi-parallel data. Different from the previous approaches, we work on a black-boxed translation system, which means generation of the virtual data can be performed on any kind of translation systems including rule based, statistical based or even human translation. The approach introduced in (Leusch et al., 2010) can combine the translation output of a test set produced by"
2011.mtsummit-papers.34,W09-0431,0,0.013434,"e, while their learning and application are constrained in a bilingual way without introducing any information from a third language. Mann and Yarowsky (2001) presented a method to induce translation lexicon based on transduction 412 models of cognate pairs via bridge language. The cognate string edit distance was applied instead of a general MT system, so that the vocabulary learning is limited to mostly European languages. For bridge or pivot languages in MT, Kumar et al. (2007) described a method to improve word alignment quality using multiple bridge languages. In (Wu and Wang, 2007) and (Habash and Hu, 2009) phrase translation tables are improved using the phrase tables obtained from pivot languages in different ways, and in (Eisele et al., 2008) a hybrid method combining RBMT and SMT systems is introduced to ﬁll up the data gap for pivot translation. Cohn and Lapata (2007) presented a method to obtain more reliable translation estimates from small data sets using multi-parallel data. Different from the previous approaches, we work on a black-boxed translation system, which means generation of the virtual data can be performed on any kind of translation systems including rule based, statistical b"
2011.mtsummit-papers.34,P07-2045,0,0.00524468,"d using the following Equation: H(f1J , eI1 ) = M  λm hm (f1J , eI1 ) m=1 409 hm (f1J , eI1 ) is a score evaluated on this sentence pair using sub-model m. Each model m is assigned with a feature weight λm . For simplicity, we only include the negative logarithm of IBM model 1 (Brown et al., 1993) in normal and inverse direction as submodels. We combine the IBM model 1 in both directions in the log-linear model with an equal weight for each direction. We use the training software GIZA++ (Och and Ney, 2003) to obtain the lexicon probability. 3 Experimental Results 3.1 MT Setup We apply Moses (Koehn et al., 2007) as our baseline translation system and train standard alignment models in both directions with GIZA++ (Och and Ney, 2003) using models of IBM-1 (Brown et al., 1993), HMM (Vogel et al., 1996) and IBM-4 (Brown et al., 1993) which brings us the optimal translation performance and efﬁciency based on empirical evaluations. Features in the log-linear model include translation models in two directions, a language model, a distortion model and a sentence length penalty. The language model is a statistical 5-gram model with modiﬁed Kneser-Ney smoothing estimated using SRI-LM toolkit (Stolcke, 2002). E"
2011.mtsummit-papers.34,2005.mtsummit-papers.11,0,0.0225646,"ll scarce resourced language pairs. The generated virtual parallel corpus can not only be applied into MT but also other NLP tasks. 2 2.1 Generating Virtual Parallel Data Background and Motivation There are only a few parallel corpora publicly available for some languages we work on. The JRC-Acquis(JRC) is a huge collection of European Union legislative documents translated into more than twenty ofﬁcial European languages (Steinberger et al., 2006). The European Parliament Proceedings Parallel Corpus (Europarl corpus) was extracted from the proceedings of the European Parliament (1996-today) (Koehn, 2005). News Commentary(NC) (SMT, 2011) and SETimes (SETIMES, 2011) are corpora collected from the news domains. In this paper, we are concerned with generating high-quality, virtual parallel data for machine translation. To do this, we exploit multiple parallel corpora in different language pairs. In particular, we generate parallel corpora for scarce resourced languages, taking Romanian to German as a case study for simplicity. We can also take German to Romanian or other language directions. In order to ﬁnd out the gap between the translation quality on better studied language pairs and that on l"
2011.mtsummit-papers.34,D07-1005,0,0.0133387,"Section 1 and Section 2.3. Uefﬁng et al. (2009) explored model adaptation methods to use the monolingual data from the source language, while their learning and application are constrained in a bilingual way without introducing any information from a third language. Mann and Yarowsky (2001) presented a method to induce translation lexicon based on transduction 412 models of cognate pairs via bridge language. The cognate string edit distance was applied instead of a general MT system, so that the vocabulary learning is limited to mostly European languages. For bridge or pivot languages in MT, Kumar et al. (2007) described a method to improve word alignment quality using multiple bridge languages. In (Wu and Wang, 2007) and (Habash and Hu, 2009) phrase translation tables are improved using the phrase tables obtained from pivot languages in different ways, and in (Eisele et al., 2008) a hybrid method combining RBMT and SMT systems is introduced to ﬁll up the data gap for pivot translation. Cohn and Lapata (2007) presented a method to obtain more reliable translation estimates from small data sets using multi-parallel data. Different from the previous approaches, we work on a black-boxed translation sys"
2011.mtsummit-papers.34,2010.iwslt-papers.12,0,0.0117409,"les obtained from pivot languages in different ways, and in (Eisele et al., 2008) a hybrid method combining RBMT and SMT systems is introduced to ﬁll up the data gap for pivot translation. Cohn and Lapata (2007) presented a method to obtain more reliable translation estimates from small data sets using multi-parallel data. Different from the previous approaches, we work on a black-boxed translation system, which means generation of the virtual data can be performed on any kind of translation systems including rule based, statistical based or even human translation. The approach introduced in (Leusch et al., 2010) can combine the translation output of a test set produced by any pivot MTs per different languages, however the individual systems are not improved and novel training data is not exploited. Bertoldi et al. (2008) evaluated several methods of pivot languages but did apply the global corpus ﬁltering i.e. compatibility measure to control the quality of data. Our purpose is not only to improve the translation quality but also to provide useful linguistic resources for other NLP tasks. 5 Conclusion and Future Work Thousands of human languages are recognized in the world, and building up millions o"
2011.mtsummit-papers.34,N01-1020,0,0.0187021,"rget languages. Callison-Burch (2002) presented a co-training method for SMT, the agreement of multiple translation systems is explored to ﬁnd the best translation for re-training. We applied compatibility instead of agreement based approach, detailed description on the difference between compatibility and agreement is referred to Section 1 and Section 2.3. Uefﬁng et al. (2009) explored model adaptation methods to use the monolingual data from the source language, while their learning and application are constrained in a bilingual way without introducing any information from a third language. Mann and Yarowsky (2001) presented a method to induce translation lexicon based on transduction 412 models of cognate pairs via bridge language. The cognate string edit distance was applied instead of a general MT system, so that the vocabulary learning is limited to mostly European languages. For bridge or pivot languages in MT, Kumar et al. (2007) described a method to improve word alignment quality using multiple bridge languages. In (Wu and Wang, 2007) and (Habash and Hu, 2009) phrase translation tables are improved using the phrase tables obtained from pivot languages in different ways, and in (Eisele et al., 20"
2011.mtsummit-papers.34,N04-1034,0,0.0782549,"Missing"
2011.mtsummit-papers.34,J03-1002,0,0.00497387,"model combining different sub-models. Let (f1J , eI1 ) be a bilingual sentence, the evaluation is performed using the following Equation: H(f1J , eI1 ) = M  λm hm (f1J , eI1 ) m=1 409 hm (f1J , eI1 ) is a score evaluated on this sentence pair using sub-model m. Each model m is assigned with a feature weight λm . For simplicity, we only include the negative logarithm of IBM model 1 (Brown et al., 1993) in normal and inverse direction as submodels. We combine the IBM model 1 in both directions in the log-linear model with an equal weight for each direction. We use the training software GIZA++ (Och and Ney, 2003) to obtain the lexicon probability. 3 Experimental Results 3.1 MT Setup We apply Moses (Koehn et al., 2007) as our baseline translation system and train standard alignment models in both directions with GIZA++ (Och and Ney, 2003) using models of IBM-1 (Brown et al., 1993), HMM (Vogel et al., 1996) and IBM-4 (Brown et al., 1993) which brings us the optimal translation performance and efﬁciency based on empirical evaluations. Features in the log-linear model include translation models in two directions, a language model, a distortion model and a sentence length penalty. The language model is a s"
2011.mtsummit-papers.34,P02-1040,0,0.0841189,"Missing"
2011.mtsummit-papers.34,N10-1063,0,0.039881,"Missing"
2011.mtsummit-papers.34,W11-2100,0,0.0454475,"The generated virtual parallel corpus can not only be applied into MT but also other NLP tasks. 2 2.1 Generating Virtual Parallel Data Background and Motivation There are only a few parallel corpora publicly available for some languages we work on. The JRC-Acquis(JRC) is a huge collection of European Union legislative documents translated into more than twenty ofﬁcial European languages (Steinberger et al., 2006). The European Parliament Proceedings Parallel Corpus (Europarl corpus) was extracted from the proceedings of the European Parliament (1996-today) (Koehn, 2005). News Commentary(NC) (SMT, 2011) and SETimes (SETIMES, 2011) are corpora collected from the news domains. In this paper, we are concerned with generating high-quality, virtual parallel data for machine translation. To do this, we exploit multiple parallel corpora in different language pairs. In particular, we generate parallel corpora for scarce resourced languages, taking Romanian to German as a case study for simplicity. We can also take German to Romanian or other language directions. In order to ﬁnd out the gap between the translation quality on better studied language pairs and that on less studied language pairs, we co"
2011.mtsummit-papers.34,steinberger-etal-2006-jrc,0,0.0697896,"Missing"
2011.mtsummit-papers.34,C96-2141,0,0.280846,"ature weight λm . For simplicity, we only include the negative logarithm of IBM model 1 (Brown et al., 1993) in normal and inverse direction as submodels. We combine the IBM model 1 in both directions in the log-linear model with an equal weight for each direction. We use the training software GIZA++ (Och and Ney, 2003) to obtain the lexicon probability. 3 Experimental Results 3.1 MT Setup We apply Moses (Koehn et al., 2007) as our baseline translation system and train standard alignment models in both directions with GIZA++ (Och and Ney, 2003) using models of IBM-1 (Brown et al., 1993), HMM (Vogel et al., 1996) and IBM-4 (Brown et al., 1993) which brings us the optimal translation performance and efﬁciency based on empirical evaluations. Features in the log-linear model include translation models in two directions, a language model, a distortion model and a sentence length penalty. The language model is a statistical 5-gram model with modiﬁed Kneser-Ney smoothing estimated using SRI-LM toolkit (Stolcke, 2002). Each language model is trained with the target side of the parallel data. We do not apply any zmert tuning in EMS because it does not improve our translation results on the evaluation set. Imp"
2011.mtsummit-papers.34,P07-1108,0,0.0135407,"from the source language, while their learning and application are constrained in a bilingual way without introducing any information from a third language. Mann and Yarowsky (2001) presented a method to induce translation lexicon based on transduction 412 models of cognate pairs via bridge language. The cognate string edit distance was applied instead of a general MT system, so that the vocabulary learning is limited to mostly European languages. For bridge or pivot languages in MT, Kumar et al. (2007) described a method to improve word alignment quality using multiple bridge languages. In (Wu and Wang, 2007) and (Habash and Hu, 2009) phrase translation tables are improved using the phrase tables obtained from pivot languages in different ways, and in (Eisele et al., 2008) a hybrid method combining RBMT and SMT systems is introduced to ﬁll up the data gap for pivot translation. Cohn and Lapata (2007) presented a method to obtain more reliable translation estimates from small data sets using multi-parallel data. Different from the previous approaches, we work on a black-boxed translation system, which means generation of the virtual data can be performed on any kind of translation systems including"
2011.mtsummit-papers.36,2009.mtsummit-posters.1,0,0.0193042,"rules on syntactic parse trees. There are a lot of pre-ordering studies, but this is the ﬁrst work of post-ordering to our knowledge. The problem can be regarded as a variant of string-to-tree SMT, from Japanese sentences to English trees. We divide the string-to-tree problem into two simpliﬁed problems, which can be solved efﬁciently with less computational cost than a string-to-tree SMT. Post-ordering is also highly related to post-editing technologies, which aim to correct errors in a rulebased translation (Simard et al., 2007; Dugast et al., 2007; Ehara, 2007) or a different type of SMT (Aikawa and Ruopp, 2009). There is a major difference of the post-ordering from such an post-editing framework; in the post-editing framework, the preceding translation process is a complete source-totarget translation, and the post-editing itself works as an additional process to ﬁx errors. In contrast, the post-ordering framework divides the whole translation process into two sub-processes focusing on translation and reordering. It has an advantage that the sub-processes are simpliﬁed and easy to solve compared to a complete translation process in the post-editing. 3 Proposed method This section presents the propos"
2011.mtsummit-papers.36,J93-2003,0,0.0259478,"ainder of this paper is organized as follows. Section 2 brieﬂy reviews related studies on the reordering problem and another related technology called post-editing. Section 3 presents the proposed method in detail taking Japanese-to-English translation as a test case. Section 4 reports our experiments and discusses the results. Section 5 concludes this paper with our prospects for future work. 2 Related Work Reordering is a both theoretically and practically challenging problem in SMT. In the early period of SMT studies, reordering is modeled by distancebased constraints in translation model (Brown et al., 1993; Koehn et al., 2003). This reordering model is easy to compute and also works well in relatively similar language pair like French-to-English. The distance-based reordering constraint is not reasonable in some language pair such as English-toJapanese, because they have very different word ordering and appropriate reordering distances of words and phrases highly depend on their syntactic roles and contexts. Tillmann (2004) proposed a lexicalized reordering model that models orientation of phrases by monotone, swap, and discontinuous. This can directly model reordering of adjacent phrases but m"
2011.mtsummit-papers.36,J07-2003,0,0.106973,"Graehl and Knight, 2004) is a theoretically good solution. Reordering in syntax-based SMT is modeled in a similar manner as reordering of tree nodes in the same level (siblings), regardless of their reordering distance. Although this approach have some shortcomings with parse errors and its too strong constraints, syntactic information is expected to be effective in some language pairs. Another syntactic approach, originally proposed by Wu (1997), uses formally-syntactic structure between source and target language sentences. This framework was extended as the hierarchical phrase-based SMT by Chiang (2007) and is convincing alternative in recent SMT research. The reordering models mentioned above are applied in SMT decoding and solved simultaneously with phrase translation. Xiong et al. extended the hierarchical SMT by lexicalized reordering (Xiong et al., 2006; Xiong et al., 2008). However, the integrated search requires a large computational cost both in time and space. To keep the search tractable, we constrain reordering search by its reordering distance, as so-called distortion limit (or maximum span in tree-based decoder). It effectively reduces the computational cost but it also give up"
2011.mtsummit-papers.36,P05-1066,0,0.719105,"Missing"
2011.mtsummit-papers.36,W06-1609,0,0.0517088,"Missing"
2011.mtsummit-papers.36,W07-0732,0,0.0604907,"k for Japanese-to-English translation using English reordering rules on syntactic parse trees. There are a lot of pre-ordering studies, but this is the ﬁrst work of post-ordering to our knowledge. The problem can be regarded as a variant of string-to-tree SMT, from Japanese sentences to English trees. We divide the string-to-tree problem into two simpliﬁed problems, which can be solved efﬁciently with less computational cost than a string-to-tree SMT. Post-ordering is also highly related to post-editing technologies, which aim to correct errors in a rulebased translation (Simard et al., 2007; Dugast et al., 2007; Ehara, 2007) or a different type of SMT (Aikawa and Ruopp, 2009). There is a major difference of the post-ordering from such an post-editing framework; in the post-editing framework, the preceding translation process is a complete source-totarget translation, and the post-editing itself works as an additional process to ﬁx errors. In contrast, the post-ordering framework divides the whole translation process into two sub-processes focusing on translation and reordering. It has an advantage that the sub-processes are simpliﬁed and easy to solve compared to a complete translation process in th"
2011.mtsummit-papers.36,N10-1128,0,0.0291627,"ore phrase translation options efﬁciently. The pre-ordering is based on syntactic parse and can be regarded as a sub-problem of tree-to-string translation. On the other hand, there are several studies on pre-ordering without syntactic parsing. Costa-juss`a and Fonollosa (2006) tackled the pre-ordering problem as SMT, using reordering tables derived from phrase tables. Tromble and Eisner (2009) applied linear ordering models to preordering. Their techniques can be applied to any language pairs but rely on noisy automatic word alignment results as the reference of the reordering model training. Dyer and Resnik (2010) advanced such a pre-ordering-based translation to a novel uniﬁed approach of long-distance pre-ordering and decoding, with discriminative context-free reordering and ﬁnite-state phrase translation. In this paper, we reverse the pre-ordering SMT framework for Japanese-to-English translation using English reordering rules on syntactic parse trees. There are a lot of pre-ordering studies, but this is the ﬁrst work of post-ordering to our knowledge. The problem can be regarded as a variant of string-to-tree SMT, from Japanese sentences to English trees. We divide the string-to-tree problem into t"
2011.mtsummit-papers.36,2007.mtsummit-wpt.4,0,0.0178607,"lish translation using English reordering rules on syntactic parse trees. There are a lot of pre-ordering studies, but this is the ﬁrst work of post-ordering to our knowledge. The problem can be regarded as a variant of string-to-tree SMT, from Japanese sentences to English trees. We divide the string-to-tree problem into two simpliﬁed problems, which can be solved efﬁciently with less computational cost than a string-to-tree SMT. Post-ordering is also highly related to post-editing technologies, which aim to correct errors in a rulebased translation (Simard et al., 2007; Dugast et al., 2007; Ehara, 2007) or a different type of SMT (Aikawa and Ruopp, 2009). There is a major difference of the post-ordering from such an post-editing framework; in the post-editing framework, the preceding translation process is a complete source-totarget translation, and the post-editing itself works as an additional process to ﬁx errors. In contrast, the post-ordering framework divides the whole translation process into two sub-processes focusing on translation and reordering. It has an advantage that the sub-processes are simpliﬁed and easy to solve compared to a complete translation process in the post-editing"
2011.mtsummit-papers.36,N04-1035,0,0.0641016,"rdering constraint is not reasonable in some language pair such as English-toJapanese, because they have very different word ordering and appropriate reordering distances of words and phrases highly depend on their syntactic roles and contexts. Tillmann (2004) proposed a lexicalized reordering model that models orientation of phrases by monotone, swap, and discontinuous. This can directly model reordering of adjacent phrases but may not work for long distance reordering, because discontinuous supplies few constraints for reordering. On the other hand, syntaxbased SMT (Yamada and Knight, 2001; Galley et al., 2004; Graehl and Knight, 2004) is a theoretically good solution. Reordering in syntax-based SMT is modeled in a similar manner as reordering of tree nodes in the same level (siblings), regardless of their reordering distance. Although this approach have some shortcomings with parse errors and its too strong constraints, syntactic information is expected to be effective in some language pairs. Another syntactic approach, originally proposed by Wu (1997), uses formally-syntactic structure between source and target language sentences. This framework was extended as the hierarchical phrase-based SMT b"
2011.mtsummit-papers.36,C10-1043,0,0.193651,"ed English. The experiments on Japanese-to-English patent translation show the signiﬁcant advantage of postordering over baseline phrase-based, hierarchical phrase-based, and syntax-based translation methods by 1.56, 0.76, and 2.77 points in BLEU, respectively. 1 A recently attractive approach for this challenge is called pre-ordering, which reorders source language sentences into the target language word order prior to SMT decoding (Xia and McCord, 2004; Collins et al., 2005; Costa-juss`a and Fonollosa, 2006; Li et al., 2007; Wang et al., 2007; Tromble and Eisner, 2009; Isozaki et al., 2010; Genzel, 2010). The pre-ordering approach is able to reorder source language words in long distance by some reordering rules or models. This effectively solves the complex reordering problem and achieves good translation performance especially in language pairs with very different word ordering. A crucial issue on the pre-ordering is to develop good reordering methods in the source language. Introduction Statistical Machine Translation (SMT) consists of two major problems, translation of words or phrases and their reordering. Recent research efforts developed novel technologies such as phrase-based SMT with"
2011.mtsummit-papers.36,N04-1014,0,0.0802119,"not reasonable in some language pair such as English-toJapanese, because they have very different word ordering and appropriate reordering distances of words and phrases highly depend on their syntactic roles and contexts. Tillmann (2004) proposed a lexicalized reordering model that models orientation of phrases by monotone, swap, and discontinuous. This can directly model reordering of adjacent phrases but may not work for long distance reordering, because discontinuous supplies few constraints for reordering. On the other hand, syntaxbased SMT (Yamada and Knight, 2001; Galley et al., 2004; Graehl and Knight, 2004) is a theoretically good solution. Reordering in syntax-based SMT is modeled in a similar manner as reordering of tree nodes in the same level (siblings), regardless of their reordering distance. Although this approach have some shortcomings with parse errors and its too strong constraints, syntactic information is expected to be effective in some language pairs. Another syntactic approach, originally proposed by Wu (1997), uses formally-syntactic structure between source and target language sentences. This framework was extended as the hierarchical phrase-based SMT by Chiang (2007) and is con"
2011.mtsummit-papers.36,W10-1736,1,0.779822,"d into correctly-ordered English. The experiments on Japanese-to-English patent translation show the signiﬁcant advantage of postordering over baseline phrase-based, hierarchical phrase-based, and syntax-based translation methods by 1.56, 0.76, and 2.77 points in BLEU, respectively. 1 A recently attractive approach for this challenge is called pre-ordering, which reorders source language sentences into the target language word order prior to SMT decoding (Xia and McCord, 2004; Collins et al., 2005; Costa-juss`a and Fonollosa, 2006; Li et al., 2007; Wang et al., 2007; Tromble and Eisner, 2009; Isozaki et al., 2010; Genzel, 2010). The pre-ordering approach is able to reorder source language words in long distance by some reordering rules or models. This effectively solves the complex reordering problem and achieves good translation performance especially in language pairs with very different word ordering. A crucial issue on the pre-ordering is to develop good reordering methods in the source language. Introduction Statistical Machine Translation (SMT) consists of two major problems, translation of words or phrases and their reordering. Recent research efforts developed novel technologies such as phrase"
2011.mtsummit-papers.36,N03-1017,0,0.171751,"is able to reorder source language words in long distance by some reordering rules or models. This effectively solves the complex reordering problem and achieves good translation performance especially in language pairs with very different word ordering. A crucial issue on the pre-ordering is to develop good reordering methods in the source language. Introduction Statistical Machine Translation (SMT) consists of two major problems, translation of words or phrases and their reordering. Recent research efforts developed novel technologies such as phrase-based SMT with phrase reordering models (Koehn et al., 2003; Tillmann, 2004), and tree-based (or syntax-based) SMT (Yamada and Knight, 2001; Galley et al., 316 In contrast, what can we do in the translation in the opposite direction? This is a non-trivial problem because the pre-ordering techniques are usually language dependent. Even if we have a good preordering technique in A-to-B translation such as reordering rules for syntactic parse trees, it cannot be used directly in B-to-A translation. Developing Bto-A pre-ordering is a different problem from A-toB, which may require a syntactic parser and/or linguistic insights. For example in Japanese-to-E"
2011.mtsummit-papers.36,P07-1091,0,0.280387,"rst translated into foreign-ordered English, and then reordered into correctly-ordered English. The experiments on Japanese-to-English patent translation show the signiﬁcant advantage of postordering over baseline phrase-based, hierarchical phrase-based, and syntax-based translation methods by 1.56, 0.76, and 2.77 points in BLEU, respectively. 1 A recently attractive approach for this challenge is called pre-ordering, which reorders source language sentences into the target language word order prior to SMT decoding (Xia and McCord, 2004; Collins et al., 2005; Costa-juss`a and Fonollosa, 2006; Li et al., 2007; Wang et al., 2007; Tromble and Eisner, 2009; Isozaki et al., 2010; Genzel, 2010). The pre-ordering approach is able to reorder source language words in long distance by some reordering rules or models. This effectively solves the complex reordering problem and achieves good translation performance especially in language pairs with very different word ordering. A crucial issue on the pre-ordering is to develop good reordering methods in the source language. Introduction Statistical Machine Translation (SMT) consists of two major problems, translation of words or phrases and their reordering."
2011.mtsummit-papers.36,J08-1002,0,0.0320343,"node. The determiners “the” and “a” are eliminated by the rules, and a pseudo-particle “ va0” is inserted after the subject. 4 • Japanese tokenizer: Mecab3 (with ipadic-2.7.0) Experiment We investigated the advantage of our post-ordering method by the following Japanese-to-English translation experiment with the post-ordering and baseline SMTs. 4.1 Setup We used NTCIR-9 PatentMT (NTCIR-9, 2011) English and Japanese dataset for this experiment. Some statistics of this dataset are shown in Table 1. We preprocessed the dataset by the following softwares: • English syntactic (HPSG) parser: Enju2 (Miyao and Tsujii, 2008) • English tokenizer: stepp (included in Enju package) 2 http://www-tsujii.is.s.u-tokyo.ac.jp/enju/index.html 320 Word alignment was automatically estimated using MGIZA++4 using bitexts of 64 or less words in the training set to avoid a problematic underﬂow. Language models are word 5-gram models of English and HFE, trained with SRILM5 . 4.2 Compared methods We compared the proposed post-ordering with three baseline SMTs: a standard phrase-based SMT (PBMT) with lexicalized reordering, a hierarchical phrase-based SMT (HPBMT), and a string-to-tree syntax-based SMT (SBMT), included in Moses6 . 3"
2011.mtsummit-papers.36,P03-1021,0,0.0766118,"DQGDVHFRQGOHQV Figure 5: An example of the post-ordering translation. English parse trees used in SBMT were identical to the ones used for generating HFE sentences in the post-ordering. The post-ordering used two Moses phrase-based decoders, one for Japanese-to-HFE and the other for HFE-to-English. The models for these decoders were trained in the standard manner with Moses, grow-diag-final-and heuristics for symmetric word alignment, msd-bidirectional-fe lexicalized reordering (in PBMT and the postordering). The parameter values are optimized by minimum error rate training (MERT) (Och, 2003) with mert-moses.pl. One difference among conﬁgurations of the decoders was distortion limit. The Japanese-to-HFE decoder did not require long distance reordering, so we compared two conditions with the values of 0 (monotone) and 6. The HFE-to-English and PBMT decoders had to drastically reorder phrases so we used the values of 12 and 20. In the HPBMT and SBMT decoders, we used 15 for its max-chart-span option. The other 321 Table 2 shows the results in BLEU (Papineni et al., 2002) in case-insensitive evaluation and average decoding times7 (on a Xeon 7460 2.66GHz computer) with the compared me"
2011.mtsummit-papers.36,P02-1040,0,0.102487,"icalized reordering (in PBMT and the postordering). The parameter values are optimized by minimum error rate training (MERT) (Och, 2003) with mert-moses.pl. One difference among conﬁgurations of the decoders was distortion limit. The Japanese-to-HFE decoder did not require long distance reordering, so we compared two conditions with the values of 0 (monotone) and 6. The HFE-to-English and PBMT decoders had to drastically reorder phrases so we used the values of 12 and 20. In the HPBMT and SBMT decoders, we used 15 for its max-chart-span option. The other 321 Table 2 shows the results in BLEU (Papineni et al., 2002) in case-insensitive evaluation and average decoding times7 (on a Xeon 7460 2.66GHz computer) with the compared methods. The proposed post-ordering translation (with monotone Japaneseto-HFE translation) achieved 0.2963 in BLEU, better than the best HPBMT baseline (0.2887) by 0.76 points and the standard PBMT baseline (0.2806) by 1.57 points. The differences were statistically signiﬁcant according to the bootstrap sampling test (p &lt; 0.05 with HPBMT and p &lt; 0.01 with PBMT, 1,000 samples) (Zhang et al., 2004), and it was consistent among all post-ordering conditions. In the Japanese-to-HFE transl"
2011.mtsummit-papers.36,N07-1064,0,0.0204361,"ordering SMT framework for Japanese-to-English translation using English reordering rules on syntactic parse trees. There are a lot of pre-ordering studies, but this is the ﬁrst work of post-ordering to our knowledge. The problem can be regarded as a variant of string-to-tree SMT, from Japanese sentences to English trees. We divide the string-to-tree problem into two simpliﬁed problems, which can be solved efﬁciently with less computational cost than a string-to-tree SMT. Post-ordering is also highly related to post-editing technologies, which aim to correct errors in a rulebased translation (Simard et al., 2007; Dugast et al., 2007; Ehara, 2007) or a different type of SMT (Aikawa and Ruopp, 2009). There is a major difference of the post-ordering from such an post-editing framework; in the post-editing framework, the preceding translation process is a complete source-totarget translation, and the post-editing itself works as an additional process to ﬁx errors. In contrast, the post-ordering framework divides the whole translation process into two sub-processes focusing on translation and reordering. It has an advantage that the sub-processes are simpliﬁed and easy to solve compared to a complete tran"
2011.mtsummit-papers.36,N04-4026,0,0.440185,"source language words in long distance by some reordering rules or models. This effectively solves the complex reordering problem and achieves good translation performance especially in language pairs with very different word ordering. A crucial issue on the pre-ordering is to develop good reordering methods in the source language. Introduction Statistical Machine Translation (SMT) consists of two major problems, translation of words or phrases and their reordering. Recent research efforts developed novel technologies such as phrase-based SMT with phrase reordering models (Koehn et al., 2003; Tillmann, 2004), and tree-based (or syntax-based) SMT (Yamada and Knight, 2001; Galley et al., 316 In contrast, what can we do in the translation in the opposite direction? This is a non-trivial problem because the pre-ordering techniques are usually language dependent. Even if we have a good preordering technique in A-to-B translation such as reordering rules for syntactic parse trees, it cannot be used directly in B-to-A translation. Developing Bto-A pre-ordering is a different problem from A-toB, which may require a syntactic parser and/or linguistic insights. For example in Japanese-to-English translatio"
2011.mtsummit-papers.36,D09-1105,0,0.249521,"English, and then reordered into correctly-ordered English. The experiments on Japanese-to-English patent translation show the signiﬁcant advantage of postordering over baseline phrase-based, hierarchical phrase-based, and syntax-based translation methods by 1.56, 0.76, and 2.77 points in BLEU, respectively. 1 A recently attractive approach for this challenge is called pre-ordering, which reorders source language sentences into the target language word order prior to SMT decoding (Xia and McCord, 2004; Collins et al., 2005; Costa-juss`a and Fonollosa, 2006; Li et al., 2007; Wang et al., 2007; Tromble and Eisner, 2009; Isozaki et al., 2010; Genzel, 2010). The pre-ordering approach is able to reorder source language words in long distance by some reordering rules or models. This effectively solves the complex reordering problem and achieves good translation performance especially in language pairs with very different word ordering. A crucial issue on the pre-ordering is to develop good reordering methods in the source language. Introduction Statistical Machine Translation (SMT) consists of two major problems, translation of words or phrases and their reordering. Recent research efforts developed novel techn"
2011.mtsummit-papers.36,D07-1077,0,0.0514298,"Missing"
2011.mtsummit-papers.36,J97-3002,0,0.389798,"e reordering, because discontinuous supplies few constraints for reordering. On the other hand, syntaxbased SMT (Yamada and Knight, 2001; Galley et al., 2004; Graehl and Knight, 2004) is a theoretically good solution. Reordering in syntax-based SMT is modeled in a similar manner as reordering of tree nodes in the same level (siblings), regardless of their reordering distance. Although this approach have some shortcomings with parse errors and its too strong constraints, syntactic information is expected to be effective in some language pairs. Another syntactic approach, originally proposed by Wu (1997), uses formally-syntactic structure between source and target language sentences. This framework was extended as the hierarchical phrase-based SMT by Chiang (2007) and is convincing alternative in recent SMT research. The reordering models mentioned above are applied in SMT decoding and solved simultaneously with phrase translation. Xiong et al. extended the hierarchical SMT by lexicalized reordering (Xiong et al., 2006; Xiong et al., 2008). However, the integrated search requires a large computational cost both in time and space. To keep the search tractable, we constrain reordering search by"
2011.mtsummit-papers.36,C04-1073,0,0.779624,"in the opposite direction and proposes post-ordering; foreign sentences are ﬁrst translated into foreign-ordered English, and then reordered into correctly-ordered English. The experiments on Japanese-to-English patent translation show the signiﬁcant advantage of postordering over baseline phrase-based, hierarchical phrase-based, and syntax-based translation methods by 1.56, 0.76, and 2.77 points in BLEU, respectively. 1 A recently attractive approach for this challenge is called pre-ordering, which reorders source language sentences into the target language word order prior to SMT decoding (Xia and McCord, 2004; Collins et al., 2005; Costa-juss`a and Fonollosa, 2006; Li et al., 2007; Wang et al., 2007; Tromble and Eisner, 2009; Isozaki et al., 2010; Genzel, 2010). The pre-ordering approach is able to reorder source language words in long distance by some reordering rules or models. This effectively solves the complex reordering problem and achieves good translation performance especially in language pairs with very different word ordering. A crucial issue on the pre-ordering is to develop good reordering methods in the source language. Introduction Statistical Machine Translation (SMT) consists of t"
2011.mtsummit-papers.36,P06-1066,0,0.0589586,"Missing"
2011.mtsummit-papers.36,I08-1066,0,0.0191332,"e errors and its too strong constraints, syntactic information is expected to be effective in some language pairs. Another syntactic approach, originally proposed by Wu (1997), uses formally-syntactic structure between source and target language sentences. This framework was extended as the hierarchical phrase-based SMT by Chiang (2007) and is convincing alternative in recent SMT research. The reordering models mentioned above are applied in SMT decoding and solved simultaneously with phrase translation. Xiong et al. extended the hierarchical SMT by lexicalized reordering (Xiong et al., 2006; Xiong et al., 2008). However, the integrated search requires a large computational cost both in time and space. To keep the search tractable, we constrain reordering search by its reordering distance, as so-called distortion limit (or maximum span in tree-based decoder). It effectively reduces the computational cost but it also give up long distance reordering exceeding the speciﬁed distortion limit. A novel alternative to the reordering problem, called pre-ordering, has been studied over recent years (Xia and McCord, 2004; Collins et al., 2005; Li et al., 2007; Genzel, 2010). Xia and McCord (2004) proposed auto"
2011.mtsummit-papers.36,P01-1067,0,0.583016,"ng rules or models. This effectively solves the complex reordering problem and achieves good translation performance especially in language pairs with very different word ordering. A crucial issue on the pre-ordering is to develop good reordering methods in the source language. Introduction Statistical Machine Translation (SMT) consists of two major problems, translation of words or phrases and their reordering. Recent research efforts developed novel technologies such as phrase-based SMT with phrase reordering models (Koehn et al., 2003; Tillmann, 2004), and tree-based (or syntax-based) SMT (Yamada and Knight, 2001; Galley et al., 316 In contrast, what can we do in the translation in the opposite direction? This is a non-trivial problem because the pre-ordering techniques are usually language dependent. Even if we have a good preordering technique in A-to-B translation such as reordering rules for syntactic parse trees, it cannot be used directly in B-to-A translation. Developing Bto-A pre-ordering is a different problem from A-toB, which may require a syntactic parser and/or linguistic insights. For example in Japanese-to-English translation, pre-ordering of Japanese parse trees into English word order"
2011.mtsummit-papers.36,zhang-etal-2004-interpreting,0,0.0319715,"ed 15 for its max-chart-span option. The other 321 Table 2 shows the results in BLEU (Papineni et al., 2002) in case-insensitive evaluation and average decoding times7 (on a Xeon 7460 2.66GHz computer) with the compared methods. The proposed post-ordering translation (with monotone Japaneseto-HFE translation) achieved 0.2963 in BLEU, better than the best HPBMT baseline (0.2887) by 0.76 points and the standard PBMT baseline (0.2806) by 1.57 points. The differences were statistically signiﬁcant according to the bootstrap sampling test (p &lt; 0.05 with HPBMT and p &lt; 0.01 with PBMT, 1,000 samples) (Zhang et al., 2004), and it was consistent among all post-ordering conditions. In the Japanese-to-HFE translation, the monotone conﬁguration was slightly better than the reordering with the distortion limit of 6 but the difference was not signiﬁcant. In the HFE-to-English translation, the difference in the distortion limit did not affect the ﬁnal results. Among the baseline methods, HPBMT was better than other baselines by 0.5 points. 4.4 Discussion The proposed post-ordering method was consistently better than the baseline methods in the experiment. To investigate the results in detail, we analyzed the Japanese"
2014.amta-researchers.18,W08-0336,0,0.177985,"ey reported the improvement in word segmentation, and did not report its effect on the patent MT. Their work can be seen an application of a semisupervised learning method (Sun and Xu, 2011) to the domain adaptation. Such an approach is appropriate for the patent domain where a huge number of patent documents are publicly available. We extend their domain adaptation by more effective and easy-to-use features, and also incorporate additional Japanese-oriented features to improve the Japanese word segmentation in the patent domain. With respect to the relation between word segmentation and SMT, Chang et al. (2008) reported consistency and granularity of word segmentation is important in Chinese-to-English MT and modified their Chinese word segmenter to optimize the translation performance. Dyer et al. (2008) and Zhang et al. (2008) used multiple word segmentation results to overcome the problem of different word segmentation standards. Xu et al. (2008) optimized Chinese word segmentation for Chinese-to-English SMT using an extended Bayesian word segmentation method with bilingual correspondence. These studies aim to optimize word segmentation using bilingual correspondence and are different from the do"
2014.amta-researchers.18,P08-1115,0,0.0286095,"omain adaptation. Such an approach is appropriate for the patent domain where a huge number of patent documents are publicly available. We extend their domain adaptation by more effective and easy-to-use features, and also incorporate additional Japanese-oriented features to improve the Japanese word segmentation in the patent domain. With respect to the relation between word segmentation and SMT, Chang et al. (2008) reported consistency and granularity of word segmentation is important in Chinese-to-English MT and modified their Chinese word segmenter to optimize the translation performance. Dyer et al. (2008) and Zhang et al. (2008) used multiple word segmentation results to overcome the problem of different word segmentation standards. Xu et al. (2008) optimized Chinese word segmentation for Chinese-to-English SMT using an extended Bayesian word segmentation method with bilingual correspondence. These studies aim to optimize word segmentation using bilingual correspondence and are different from the domain adaptation. Machine transliteration is an important problem for translating names and other imported words (Knight and Graehl, 1998). Conventional methods need to prepare parallel transliterati"
2014.amta-researchers.18,J04-1004,0,0.0347997,"ses B, M, E (beginning/middle/end of a word), and S (single-character word)2 , as Sun and Xu (2011). Our baseline features follow the work of Japanese word segmentation by Neubig et al. (2011): label bigrams, character n-grams (n=1, 2), and character type n-grams (n=1, 2, 3). We use the n-gram features within [i-2, i+2] for classifying the word at the position i. The character types are kanji, katakana, hiragana, digits, roman characters, and others. 4.2 Conventional Method: Word Segmentation Adaptation using Accessor Variety Sun and Xu (2011) and Guo et al. (2012) used Accessor Variety (AV) (Feng et al., 2004) derived from unlabeled corpora as word segmentation features. AV is a word extraction criterion from un-segmented corpora, focusing on the number of distinct characters appearing around a string. The AV of a string xn is defined as AV (xn ) = min {AVL (xn ), AVR (xn )} , where AVL (xn ) is the left AV (the number of distinct predecessor characters) and AVR (xn ) is the right AV (the number of distinct successor characters). The AV-based word extraction is based on an intuitive assumption; a word appears in many different context so that there is a large variation of its accessor characters. I"
2014.amta-researchers.18,N04-1035,0,0.0159661,"© The Authors 234 Finalization (Isozaki et al., 2010), the reordering in Japanese-to-English is not so straightforward. The second problem results from the Japanese orthography in which there are no explicit word boundaries. General-purpose word segmenters often fail to segment the domain-specific words and those words are translated incorrectly or remain untranslated. Some domain-specific words cannot be translated as unknown words even if they are segmented correctly, due to limited SMT training data. The first problem has been addressed by a syntax-based approach (Yamada and Knight, 2002; Galley et al., 2004; Zollmann and Venugopal, 2006), while most previous studies did not deal with the second problem. Since the lexical translation also affects the reordering based on a lexicalized reordering model and an n-gram language model, considering both problems is important for an overall SMT system. The goal of this work is to improve the Japanese-to-English patent SMT by tackling both problems at the same time. The domain-specific words have important roles in the patents and should be translated carefully for meaningful translations. We propose a novel domain adaptation method for the word segmentat"
2014.amta-researchers.18,P06-1085,0,0.0412818,"s Vancouver, BC © The Authors 237 4 Domain Adaptation of Japanese Word Segmentation for Patents We aim to improve the Japanese-to-English translation performance further by the word segmentation adaptation for patent-specific words and technical terms. We use the large-scale monolingual Japanese patent corpora for the domain adaptation, by the semi-supervised approach as Sun and Xu (2011) and Guo et al. (2012). There are also active learning-based supervised domain adaptation (Tsuboi et al., 2008; Neubig et al., 2011) and unsupervised word segmentation (Kempe, 1999; Kubota Ando and Lee, 2003; Goldwater et al., 2006, and many others) approaches, but the semi-supervised approach is expected to be effective; the active learning method is not easy to utilize for such large-scale corpora and the unsupervised method is not so accurate as existing supervised word segmenters. 4.1 Baseline Word Segmentation based on Conditional Random Fields We use a character-based word segmenter based on CRFs (Peng et al., 2004; Tseng et al., 2005). It solves a character-based sequential labeling problem. In this work we employ four classes B, M, E (beginning/middle/end of a word), and S (single-character word)2 , as Sun and X"
2014.amta-researchers.18,I13-1147,1,0.845609,"translation experiments. 2 Related Work The patent MT between Japanese and English has been studied actively on shared tasks in NTCIR (Fujii et al., 2008, 2010; Goto et al., 2011, 2013). Recent important achievements in these studies are on the reordering problem especially in English-to-Japanese direction. Isozaki et al. (2010) proposed a very simple but effective rule-based syntactic pre-ordering method called Head Finalization. It is very effective for the long distance reordering. On the other hand, the Japanese-to-English direction is more difficult due to the lack of such simple rules. Hoshino et al. (2013) proposed an effective rule-based syntactic pre-ordering based on predicate-argument structures. Sudoh et al. (2013b) proposed a different approach called postordering for the Japanese-to-English patent MT, and achieved high translation performance by an efficient syntax-based translation. Our system uses the latter approach based on English syntax rather than the former one based on Japanese syntax. This is because our word segmentation adaptation can be applied directly to it without the Japanese parser adaptation as described earlier. General-purpose Japanese parsers do not work well in the"
2014.amta-researchers.18,W10-1736,1,0.907546,"erence from general-purpose MT. This work focuses on statistical MT (SMT) for patents, from Japanese to English. It is more difficult than English-to-Japanese in: 1) long distance reordering, and 2) word segmentation and lexical translation of domain-specific terms. The first problem is due to large syntactic differences between Japanese and English. Although the reordering in the English-to-Japanese direction can be solved effectively by very simple heuristics called Head Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 234 Finalization (Isozaki et al., 2010), the reordering in Japanese-to-English is not so straightforward. The second problem results from the Japanese orthography in which there are no explicit word boundaries. General-purpose word segmenters often fail to segment the domain-specific words and those words are translated incorrectly or remain untranslated. Some domain-specific words cannot be translated as unknown words even if they are segmented correctly, due to limited SMT training data. The first problem has been addressed by a syntax-based approach (Yamada and Knight, 2002; Galley et al., 2004; Zollmann and Venugopal, 2006), wh"
2014.amta-researchers.18,P06-2056,0,0.0397128,"Missing"
2014.amta-researchers.18,W99-0702,0,0.0703583,"ings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 237 4 Domain Adaptation of Japanese Word Segmentation for Patents We aim to improve the Japanese-to-English translation performance further by the word segmentation adaptation for patent-specific words and technical terms. We use the large-scale monolingual Japanese patent corpora for the domain adaptation, by the semi-supervised approach as Sun and Xu (2011) and Guo et al. (2012). There are also active learning-based supervised domain adaptation (Tsuboi et al., 2008; Neubig et al., 2011) and unsupervised word segmentation (Kempe, 1999; Kubota Ando and Lee, 2003; Goldwater et al., 2006, and many others) approaches, but the semi-supervised approach is expected to be effective; the active learning method is not easy to utilize for such large-scale corpora and the unsupervised method is not so accurate as existing supervised word segmenters. 4.1 Baseline Word Segmentation based on Conditional Random Fields We use a character-based word segmenter based on CRFs (Peng et al., 2004; Tseng et al., 2005). It solves a character-based sequential labeling problem. In this work we employ four classes B, M, E (beginning/middle/end of a w"
2014.amta-researchers.18,W04-3250,0,0.0288226,"and the large-scale unlabeled patent corpus with the BE and PD features • KyTea, MeCab, and JUMAN10 : publicly available Japanese morphological analyzers We also compared the results by the post-ordering with those by standard SAMT and PBMT. The search space parameters of the standard SAMT were set to the same value as the HFE-toEnglish SAMT, to compare the performance with similar computation time11 . 5.2.3 Results and Discussion Table 4 shows the translation performance in BLEU and TER (Snover et al., 2006) with the results of statistical significance tests (p=0.05) by bootstrap resampling (Koehn, 2004), in which our overall system resulted in the best. The table also shows the results of intermediate Japanese-to-HFE translation. The advantage of our system can be attributed to three techniques included in the system: domain adaption of word segmentation, katakana unknown word transliteration, and post-ordering. 9 It exceeded the maximum sentence length in the development and test sets. 10 http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JUMAN 11 Actually the post-ordering needs the time for the first monotone PBMT but it ran very fast and did not affect so much (Sudoh et al., 2013b). Al-Onaizan"
2014.amta-researchers.18,P11-2093,1,0.789822,"e monolingual corpora. Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 237 4 Domain Adaptation of Japanese Word Segmentation for Patents We aim to improve the Japanese-to-English translation performance further by the word segmentation adaptation for patent-specific words and technical terms. We use the large-scale monolingual Japanese patent corpora for the domain adaptation, by the semi-supervised approach as Sun and Xu (2011) and Guo et al. (2012). There are also active learning-based supervised domain adaptation (Tsuboi et al., 2008; Neubig et al., 2011) and unsupervised word segmentation (Kempe, 1999; Kubota Ando and Lee, 2003; Goldwater et al., 2006, and many others) approaches, but the semi-supervised approach is expected to be effective; the active learning method is not easy to utilize for such large-scale corpora and the unsupervised method is not so accurate as existing supervised word segmenters. 4.1 Baseline Word Segmentation based on Conditional Random Fields We use a character-based word segmenter based on CRFs (Peng et al., 2004; Tseng et al., 2005). It solves a character-based sequential labeling problem. In this work we employ f"
2014.amta-researchers.18,P03-1021,0,0.0420166,"implemented with Moses-chart and trained using the HFE sentences and the corresponding English parse trees. Its reordering parameter max-chart-span was set to 200 to allow arbitrary distance reordering for accurate Japanse-toEnglish translation9 . The search space parameter cube-pruning-pop-limit was set to 32 for efficiency, according to Sudoh et al. (2013b). Their language models were word 6-gram models trained using a large-scale English patent corpus with more than 300 million sentences. Model weights were optimized in BLEU (Papineni et al., 2002) using Minimum Error Rate Training (MERT) (Och, 2003). We chose the best weights among ten individual runs of MERT. The katakana transliteration was implemented as a Moses-based monotone PBMT in the character level, trained using transliteration pairs mined from the Japanese-English phrase table entries whose Japanese part consisted of katakana only. Its character-level language model was character 9-gram models trained using the large-scale English patent corpus which is used for the word-level language models described above. It was used to replace katakana words remained in the intermediate results in HFE with their transliteration results. 5"
2014.amta-researchers.18,P02-1040,0,0.0899612,"strain adjacent phrase translations. The HFE-to-English SAMT was implemented with Moses-chart and trained using the HFE sentences and the corresponding English parse trees. Its reordering parameter max-chart-span was set to 200 to allow arbitrary distance reordering for accurate Japanse-toEnglish translation9 . The search space parameter cube-pruning-pop-limit was set to 32 for efficiency, according to Sudoh et al. (2013b). Their language models were word 6-gram models trained using a large-scale English patent corpus with more than 300 million sentences. Model weights were optimized in BLEU (Papineni et al., 2002) using Minimum Error Rate Training (MERT) (Och, 2003). We chose the best weights among ten individual runs of MERT. The katakana transliteration was implemented as a Moses-based monotone PBMT in the character level, trained using transliteration pairs mined from the Japanese-English phrase table entries whose Japanese part consisted of katakana only. Its character-level language model was character 9-gram models trained using the large-scale English patent corpus which is used for the word-level language models described above. It was used to replace katakana words remained in the intermediate"
2014.amta-researchers.18,C04-1081,0,0.0601299,"al. (2012). There are also active learning-based supervised domain adaptation (Tsuboi et al., 2008; Neubig et al., 2011) and unsupervised word segmentation (Kempe, 1999; Kubota Ando and Lee, 2003; Goldwater et al., 2006, and many others) approaches, but the semi-supervised approach is expected to be effective; the active learning method is not easy to utilize for such large-scale corpora and the unsupervised method is not so accurate as existing supervised word segmenters. 4.1 Baseline Word Segmentation based on Conditional Random Fields We use a character-based word segmenter based on CRFs (Peng et al., 2004; Tseng et al., 2005). It solves a character-based sequential labeling problem. In this work we employ four classes B, M, E (beginning/middle/end of a word), and S (single-character word)2 , as Sun and Xu (2011). Our baseline features follow the work of Japanese word segmentation by Neubig et al. (2011): label bigrams, character n-grams (n=1, 2), and character type n-grams (n=1, 2, 3). We use the n-gram features within [i-2, i+2] for classifying the word at the position i. The character types are kanji, katakana, hiragana, digits, roman characters, and others. 4.2 Conventional Method: Word Seg"
2014.amta-researchers.18,P12-1049,0,0.166158,"nsidering both problems is important for an overall SMT system. The goal of this work is to improve the Japanese-to-English patent SMT by tackling both problems at the same time. The domain-specific words have important roles in the patents and should be translated carefully for meaningful translations. We propose a novel domain adaptation method for the word segmentation, using effective features derived from a large-scale patent corpus. We also incorporate machine transliteration for the unknown Japanese words written in katakana (Japanese phonograms), bootstrapped from the parallel corpus (Sajjad et al., 2012; Sudoh et al., 2013a). Our SMT system integrates these techniques with a post-ordering framework (Sudoh et al., 2013b), which divides the SMT problem explicitly into two sub-problems of the lexical translation and the reordering. In the post-ordering framework, the lexical translation precedes the reordering, different from pre-ordering in which the reordering precedes the lexical translation (Xia and McCord, 2004; Isozaki et al., 2010). An advantage of the post-ordering is that it is easy to integrate the domain-adapted word segmentation and the unknown word transliteration in its lexical tr"
2014.amta-researchers.18,2006.amta-papers.25,0,0.0160653,"tation experiments above • Proposed: the patent-adapted segmenter using the labeled general-domain corpus and the large-scale unlabeled patent corpus with the BE and PD features • KyTea, MeCab, and JUMAN10 : publicly available Japanese morphological analyzers We also compared the results by the post-ordering with those by standard SAMT and PBMT. The search space parameters of the standard SAMT were set to the same value as the HFE-toEnglish SAMT, to compare the performance with similar computation time11 . 5.2.3 Results and Discussion Table 4 shows the translation performance in BLEU and TER (Snover et al., 2006) with the results of statistical significance tests (p=0.05) by bootstrap resampling (Koehn, 2004), in which our overall system resulted in the best. The table also shows the results of intermediate Japanese-to-HFE translation. The advantage of our system can be attributed to three techniques included in the system: domain adaption of word segmentation, katakana unknown word transliteration, and post-ordering. 9 It exceeded the maximum sentence length in the development and test sets. 10 http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JUMAN 11 Actually the post-ordering needs the time for the firs"
2014.amta-researchers.18,D13-1021,1,0.223791,"ms is important for an overall SMT system. The goal of this work is to improve the Japanese-to-English patent SMT by tackling both problems at the same time. The domain-specific words have important roles in the patents and should be translated carefully for meaningful translations. We propose a novel domain adaptation method for the word segmentation, using effective features derived from a large-scale patent corpus. We also incorporate machine transliteration for the unknown Japanese words written in katakana (Japanese phonograms), bootstrapped from the parallel corpus (Sajjad et al., 2012; Sudoh et al., 2013a). Our SMT system integrates these techniques with a post-ordering framework (Sudoh et al., 2013b), which divides the SMT problem explicitly into two sub-problems of the lexical translation and the reordering. In the post-ordering framework, the lexical translation precedes the reordering, different from pre-ordering in which the reordering precedes the lexical translation (Xia and McCord, 2004; Isozaki et al., 2010). An advantage of the post-ordering is that it is easy to integrate the domain-adapted word segmentation and the unknown word transliteration in its lexical translation step and t"
2014.amta-researchers.18,D11-1090,0,0.0334295,"Missing"
2014.amta-researchers.18,I05-3027,0,0.0216655,"are also active learning-based supervised domain adaptation (Tsuboi et al., 2008; Neubig et al., 2011) and unsupervised word segmentation (Kempe, 1999; Kubota Ando and Lee, 2003; Goldwater et al., 2006, and many others) approaches, but the semi-supervised approach is expected to be effective; the active learning method is not easy to utilize for such large-scale corpora and the unsupervised method is not so accurate as existing supervised word segmenters. 4.1 Baseline Word Segmentation based on Conditional Random Fields We use a character-based word segmenter based on CRFs (Peng et al., 2004; Tseng et al., 2005). It solves a character-based sequential labeling problem. In this work we employ four classes B, M, E (beginning/middle/end of a word), and S (single-character word)2 , as Sun and Xu (2011). Our baseline features follow the work of Japanese word segmentation by Neubig et al. (2011): label bigrams, character n-grams (n=1, 2), and character type n-grams (n=1, 2, 3). We use the n-gram features within [i-2, i+2] for classifying the word at the position i. The character types are kanji, katakana, hiragana, digits, roman characters, and others. 4.2 Conventional Method: Word Segmentation Adaptation"
2014.amta-researchers.18,C08-1113,1,0.738883,"are trained using the monolingual corpora. Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 237 4 Domain Adaptation of Japanese Word Segmentation for Patents We aim to improve the Japanese-to-English translation performance further by the word segmentation adaptation for patent-specific words and technical terms. We use the large-scale monolingual Japanese patent corpora for the domain adaptation, by the semi-supervised approach as Sun and Xu (2011) and Guo et al. (2012). There are also active learning-based supervised domain adaptation (Tsuboi et al., 2008; Neubig et al., 2011) and unsupervised word segmentation (Kempe, 1999; Kubota Ando and Lee, 2003; Goldwater et al., 2006, and many others) approaches, but the semi-supervised approach is expected to be effective; the active learning method is not easy to utilize for such large-scale corpora and the unsupervised method is not so accurate as existing supervised word segmenters. 4.1 Baseline Word Segmentation based on Conditional Random Fields We use a character-based word segmenter based on CRFs (Peng et al., 2004; Tseng et al., 2005). It solves a character-based sequential labeling problem. In"
2014.amta-researchers.18,2007.mtsummit-papers.63,0,0.0507407,"s an extended transliteration mining method for Japanese compound words (Sudoh et al., 2013a). 3 System Overview Our Japanese-to-English patent SMT is based on large-scale language resources in the patent domain. This work uses NTCIR PatentMT dataset (Goto et al., 2011, 2013) including a Japanese-English parallel corpus of 3.2 million sentences and monolingual corpora of more than 300 million sentences of Japanese and English. The parallel corpus was developed by an automatic sentence alignment over patent documents in the Japan Patent Office and the United States Patent and Trademark Office (Utiyama and Isahara, 2007). The workflow of our SMT system is illustrated in Figure 1. The translation is divided into the following four processes. 1. Japanese word segmentation using a patent-adapted word segmentation model 2. Translation into an intermediate language, Head Final English (HFE), by a monotone phrase-based SMT 3. Transliteration of untranslated Japanese katakana words (i.e. unknown words in the previous process) into English words, by a monotone phrase-based SMT in the character level 4. Post-ordering into English by a syntax-based SMT Here, HFE is Japanese-ordered English, which was proposed by Isozak"
2014.amta-researchers.18,C04-1073,0,0.0524449,"arge-scale patent corpus. We also incorporate machine transliteration for the unknown Japanese words written in katakana (Japanese phonograms), bootstrapped from the parallel corpus (Sajjad et al., 2012; Sudoh et al., 2013a). Our SMT system integrates these techniques with a post-ordering framework (Sudoh et al., 2013b), which divides the SMT problem explicitly into two sub-problems of the lexical translation and the reordering. In the post-ordering framework, the lexical translation precedes the reordering, different from pre-ordering in which the reordering precedes the lexical translation (Xia and McCord, 2004; Isozaki et al., 2010). An advantage of the post-ordering is that it is easy to integrate the domain-adapted word segmentation and the unknown word transliteration in its lexical translation step and that the reordering can use the improved lexical translation results. If we are to do the same thing in the pre-ordering, we need domain adaptation of its Japanese syntactic parser in addition to the word segmenter, and have to integrate the transliteration process with the SMT decoder as Durrani et al. (2014). Our system shows better translation accuracy in BLEU and TER than baseline methods in"
2014.amta-researchers.18,C08-1128,0,0.022744,"ir domain adaptation by more effective and easy-to-use features, and also incorporate additional Japanese-oriented features to improve the Japanese word segmentation in the patent domain. With respect to the relation between word segmentation and SMT, Chang et al. (2008) reported consistency and granularity of word segmentation is important in Chinese-to-English MT and modified their Chinese word segmenter to optimize the translation performance. Dyer et al. (2008) and Zhang et al. (2008) used multiple word segmentation results to overcome the problem of different word segmentation standards. Xu et al. (2008) optimized Chinese word segmentation for Chinese-to-English SMT using an extended Bayesian word segmentation method with bilingual correspondence. These studies aim to optimize word segmentation using bilingual correspondence and are different from the domain adaptation. Machine transliteration is an important problem for translating names and other imported words (Knight and Graehl, 1998). Conventional methods need to prepare parallel transliteration pairs for training. Sajjad et al. (2012) proposed an unsupervised transliteration mining from standard parallel corpora for bootstrapping machin"
2014.amta-researchers.18,P02-1039,0,0.0604915,"Researchers Vancouver, BC © The Authors 234 Finalization (Isozaki et al., 2010), the reordering in Japanese-to-English is not so straightforward. The second problem results from the Japanese orthography in which there are no explicit word boundaries. General-purpose word segmenters often fail to segment the domain-specific words and those words are translated incorrectly or remain untranslated. Some domain-specific words cannot be translated as unknown words even if they are segmented correctly, due to limited SMT training data. The first problem has been addressed by a syntax-based approach (Yamada and Knight, 2002; Galley et al., 2004; Zollmann and Venugopal, 2006), while most previous studies did not deal with the second problem. Since the lexical translation also affects the reordering based on a lexicalized reordering model and an n-gram language model, considering both problems is important for an overall SMT system. The goal of this work is to improve the Japanese-to-English patent SMT by tackling both problems at the same time. The domain-specific words have important roles in the patents and should be translated carefully for meaningful translations. We propose a novel domain adaptation method f"
2014.amta-researchers.18,W08-0335,0,0.0231886,"an approach is appropriate for the patent domain where a huge number of patent documents are publicly available. We extend their domain adaptation by more effective and easy-to-use features, and also incorporate additional Japanese-oriented features to improve the Japanese word segmentation in the patent domain. With respect to the relation between word segmentation and SMT, Chang et al. (2008) reported consistency and granularity of word segmentation is important in Chinese-to-English MT and modified their Chinese word segmenter to optimize the translation performance. Dyer et al. (2008) and Zhang et al. (2008) used multiple word segmentation results to overcome the problem of different word segmentation standards. Xu et al. (2008) optimized Chinese word segmentation for Chinese-to-English SMT using an extended Bayesian word segmentation method with bilingual correspondence. These studies aim to optimize word segmentation using bilingual correspondence and are different from the domain adaptation. Machine transliteration is an important problem for translating names and other imported words (Knight and Graehl, 1998). Conventional methods need to prepare parallel transliteration pairs for training. S"
2014.amta-researchers.18,Y06-1012,0,0.0222236,"proportional to the corpus size in general. Previous studies use several frequency classes with corresponding threshold values tuned according to the corpus, but it is not straightforward to determine appropriate classes and threshold values. Sun and Xu (2011) used the following features based on the left and right AVs of character n-grams for classifying xi , which imply word boundaries around xi , as illustrated in Figure 4. • Left AV of n-gram starting from xi : AVL (xi , ..., xi+n−1 ) 2 Guo et al. (2012) used six classes including B2, B3 (second and third character in a word) proposed by Zhao et al. (2006) for Chinese word segmentation. This paper uses the four classes, because the six classes did not improve the word segmentation accuracy in our pilot test. Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 238 と前記複数の の前後方向で と前記ノード 直前に、その と前後で交差 predecessors: と: 3 の: 1 直: 1 Left AV of 前: 3 Right AV of 前: 3 Left BE of 前: 3 3 log 5 5 successors: 記: 2 後: 2 に: 1 2⇥ 1 1 log = 1.057 5 5 Right BE of 前: 2⇥ 2 3 log 5 5 1 1 log = 1.308 5 5 Figure 3: Example of accessor variety (AV) and branching entropy (BE) for a character “前”. character to be classi"
2014.amta-researchers.18,W06-3119,0,0.01628,"nalization (Isozaki et al., 2010), the reordering in Japanese-to-English is not so straightforward. The second problem results from the Japanese orthography in which there are no explicit word boundaries. General-purpose word segmenters often fail to segment the domain-specific words and those words are translated incorrectly or remain untranslated. Some domain-specific words cannot be translated as unknown words even if they are segmented correctly, due to limited SMT training data. The first problem has been addressed by a syntax-based approach (Yamada and Knight, 2002; Galley et al., 2004; Zollmann and Venugopal, 2006), while most previous studies did not deal with the second problem. Since the lexical translation also affects the reordering based on a lexicalized reordering model and an n-gram language model, considering both problems is important for an overall SMT system. The goal of this work is to improve the Japanese-to-English patent SMT by tackling both problems at the same time. The domain-specific words have important roles in the patents and should be translated carefully for meaningful translations. We propose a novel domain adaptation method for the word segmentation, using effective features d"
2014.amta-researchers.18,E14-4029,0,\N,Missing
2014.amta-researchers.18,J98-4003,0,\N,Missing
2014.iwslt-papers.16,P01-1067,0,0.149499,"lly annotated treebank of TED talks that we hope will prove useful for investigation into the interaction between syntax and these speechrelated applications. The first version of the corpus includes 1,217 sentences and 23,158 words manually annotated with parse trees, and aligned with translations in 26-43 different languages. In this paper we describe the collection of the corpus, and an analysis of its various characteristics. 1. Introduction Syntactic parsing is widely considered as a useful component of natural language processing systems, not the least of which being machine translation [1, 2]. While a large part of the work on these applications has focused on the written word, we can assume that the fundamental principles behind syntax’s success in these applications will also carry over to spoken language as well. The great majority of recent work on syntactic parsing has been based on the statistical paradigm, in which the parameters of the parser are estimated from treebanks of manually annotated parse trees. In English, the standard data set for estimating these parsers is the Wall Street Journal section of the Penn Treebank [3], consisting of written language from newspapers"
2014.iwslt-papers.16,P14-2024,1,0.799534,"lly annotated treebank of TED talks that we hope will prove useful for investigation into the interaction between syntax and these speechrelated applications. The first version of the corpus includes 1,217 sentences and 23,158 words manually annotated with parse trees, and aligned with translations in 26-43 different languages. In this paper we describe the collection of the corpus, and an analysis of its various characteristics. 1. Introduction Syntactic parsing is widely considered as a useful component of natural language processing systems, not the least of which being machine translation [1, 2]. While a large part of the work on these applications has focused on the written word, we can assume that the fundamental principles behind syntax’s success in these applications will also carry over to spoken language as well. The great majority of recent work on syntactic parsing has been based on the statistical paradigm, in which the parameters of the parser are estimated from treebanks of manually annotated parse trees. In English, the standard data set for estimating these parsers is the Wall Street Journal section of the Penn Treebank [3], consisting of written language from newspapers"
2014.iwslt-papers.16,J93-2004,0,0.047231,"the least of which being machine translation [1, 2]. While a large part of the work on these applications has focused on the written word, we can assume that the fundamental principles behind syntax’s success in these applications will also carry over to spoken language as well. The great majority of recent work on syntactic parsing has been based on the statistical paradigm, in which the parameters of the parser are estimated from treebanks of manually annotated parse trees. In English, the standard data set for estimating these parsers is the Wall Street Journal section of the Penn Treebank [3], consisting of written language from newspapers. However, as there are large differences between written language and spoken language, there have also been some efforts to create resources for spoken language, including the Penn Treebank annotations of ATIS travel conversation and Switchboard telephone conversation data, as well as the OntoNotes [4] annotation of broadcast news and commentary. While these corpora mainly focus on informal speech or news, spoken monologue in the form of talks presented to an audience is also an attractive target for speech processing applications. In particular"
2014.iwslt-papers.16,N06-2015,0,0.00919081,"the statistical paradigm, in which the parameters of the parser are estimated from treebanks of manually annotated parse trees. In English, the standard data set for estimating these parsers is the Wall Street Journal section of the Penn Treebank [3], consisting of written language from newspapers. However, as there are large differences between written language and spoken language, there have also been some efforts to create resources for spoken language, including the Penn Treebank annotations of ATIS travel conversation and Switchboard telephone conversation data, as well as the OntoNotes [4] annotation of broadcast news and commentary. While these corpora mainly focus on informal speech or news, spoken monologue in the form of talks presented to an audience is also an attractive target for speech processing applications. In particular, the talk data from TED1 has been used as a target for much research, most notably the IWSLT evaluation campaigns [5]. In this work, we present the NAIST-NTT TED Talk Treebank, a new manually annotated treebank of TED talks that we hope will prove useful for investigation into the interaction between syntax and speech-related applications such as sp"
2014.iwslt-papers.16,2012.eamt-1.60,0,0.282225,"License at http://ahclab.naist.jp/resource/tedtreebank 2. Corpus Data In this section, we describe the data used as material for the corpus. 2.1. English Data Table 1: Details of the annotated data. Set All Train Test Talk 10 7 3 Min. 125.07 87.23 37.84 Sent. 1,217 822 395 Word 23,158 16,063 7,095 The English text and speech data were gathered from TED Talks. Specifically, we gathered data starting with the beginning of 1 http://www.ted.com 265 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 the May 2012 version of the WIT3 [6] training corpus for EnglishJapanese. From this data, for the first version of the treebank we chose 10 talks, the details of which are shown in Table 1.2 As the original TED data is subtitles, it is necessary to group these subtitles into sentences before performing annotation. In the creation of the corpus, we used the standard English sentence segmentation provided by the WIT3 data.3 In addition, when using a corpus for experiments, it is desirable to have a “standard” split between the training and testing data. As this standard, we designated a split of the first 7 talks as training data,"
2014.iwslt-papers.16,P06-1055,0,0.0347901,"lso the first multilingually aligned treebank of the spoken word. We hope that this data will be of use for investigations into the effect of syntax on speech translation and other cross-lingual tasks. Treebank annotation is an extremely time consuming process, particularly when the entirety of the tree has to be created from scratch. Fortunately, relatively accurate treebank parsers already exist, allowing us to create an initial parse first using an off-the-shelf parser, then have annotators spend their time fixing the errors of the existing parser. In this case, we use the Berkeley Parser6 [8] to create an initial parse. After this, we hired annotators to go through the trees and annotated them based on the standard described in the previous section. The annotators are well versed in annotation of linguistic data, and were given the standard and asked to follow it closely. After receiving this initial annotation result, the first author of the paper went through the entirety of the corpus, checking once more for any remaining errors. Finally, the trees were automatically checked for inconsistencies such as duplicated unary rules, or trees that were judged as a warning or error acco"
2014.iwslt-papers.16,W07-2416,0,0.0146745,"ugh the trees and annotated them based on the standard described in the previous section. The annotators are well versed in annotation of linguistic data, and were given the standard and asked to follow it closely. After receiving this initial annotation result, the first author of the paper went through the entirety of the corpus, checking once more for any remaining errors. Finally, the trees were automatically checked for inconsistencies such as duplicated unary rules, or trees that were judged as a warning or error according to the phrase structure conversion tools of Johansson and Nugues [9]. 3. Creation of Parse Trees 4. Speech Time Alignment The first, and most labor-intensive annotation task was the creation of manual parse trees for the English sentences. Because the treebank described in this paper is of spoken language, the correspondence between syntactic trees and features of the speech is of particular interest. For example, it has been previously noted that prosody and syntax have a close relationship [10], and this corpus could be used to perform further investigations into these and other issues. In order to create the time alignment of each word in the speech, Table"
2014.iwslt-papers.16,2013.iwslt-evaluation.23,1,0.804544,"Missing"
2014.iwslt-papers.16,W07-1001,0,0.0638861,"Missing"
2020.coling-main.418,2020.coling-main.418,1,0.0530913,"Missing"
2020.coling-main.418,Q19-1038,0,0.0931014,"Missing"
2020.coling-main.418,P08-2007,0,0.697497,"Missing"
2020.coling-main.418,N19-1423,0,0.0368242,"Missing"
2020.coling-main.418,J93-1004,0,0.874422,"Missing"
2020.coling-main.418,C18-1122,0,0.144486,"Missing"
2020.coling-main.418,W18-2709,0,0.0657578,"Missing"
2020.coling-main.418,D18-2012,0,0.0574812,"Missing"
2020.coling-main.418,2020.lrec-1.443,1,0.687224,"Missing"
2020.coling-main.418,2020.emnlp-main.41,1,0.565805,"Missing"
2020.coling-main.418,N19-4009,0,0.0610293,"Missing"
2020.coling-main.418,P02-1040,0,0.107952,"Missing"
2020.coling-main.418,W18-6319,0,0.029882,"Missing"
2020.coling-main.418,P13-1061,0,0.218477,"Missing"
2020.coling-main.418,D16-1264,0,0.10539,"Missing"
2020.coling-main.418,W11-4624,0,0.807774,"Missing"
2020.coling-main.418,D19-1136,0,0.0773906,"Missing"
2020.coling-main.418,P03-1010,0,0.610019,"Missing"
2020.coling-main.418,C94-2175,0,0.865617,"Missing"
2020.emnlp-main.41,J93-2003,0,0.151287,"iﬁcantly outperformed previous supervised and unsupervised word alignment methods without any bitexts for pretraining. For example, we achieved 86.7 F1 score for the ChineseEnglish data, which is 13.3 points higher than the previous state-of-the-art supervised method.1 1 Introduction Over the last several years, machine translation accuracy has been greatly improved by neural networks (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Vaswani et al., 2017). However, word alignment tools, which were developed during the age of statistical machine translation (Brown et al., 1993; Koehn et al., 2007) such as GIZA++ (Och and Ney, 2003), MGIZA (Gao and Vogel, 2008) and FastAlign (Dyer et al., 2013), remain 1 Our implementation is available https://github.com/nttcslab-nlp/word align/ at 555 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 555–565, c November 16–20, 2020. 2020 Association for Computational Linguistics million parallel sentences to pretrain their models. Applying these methods to low-resource language pairs and domains is difﬁcult. In this paper, we present a novel supervised word alignment method that requires"
2020.emnlp-main.41,P19-4007,0,0.0637466,"Missing"
2020.emnlp-main.41,D15-1166,0,0.0392179,"ments using ﬁve word alignment datasets from among Chinese, Japanese, German, Romanian, French, and English, we show that our proposed method signiﬁcantly outperformed previous supervised and unsupervised word alignment methods without any bitexts for pretraining. For example, we achieved 86.7 F1 score for the ChineseEnglish data, which is 13.3 points higher than the previous state-of-the-art supervised method.1 1 Introduction Over the last several years, machine translation accuracy has been greatly improved by neural networks (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Vaswani et al., 2017). However, word alignment tools, which were developed during the age of statistical machine translation (Brown et al., 1993; Koehn et al., 2007) such as GIZA++ (Och and Ney, 2003), MGIZA (Gao and Vogel, 2008) and FastAlign (Dyer et al., 2013), remain 1 Our implementation is available https://github.com/nttcslab-nlp/word align/ at 555 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 555–565, c November 16–20, 2020. 2020 Association for Computational Linguistics million parallel sentences to pretrain their models. Applying these"
2020.emnlp-main.41,P08-2007,0,0.116444,"5: Test set performance when trained on subsamples of Chinese gold word alignment data is 59.3, which is only slightly higher than that of GIZA++, 57.6. If we add a short context, namely, the two preceding words and the two following words, the F1 score is improved by more than 10 points to 72.0. If we use the whole source sentence as the context, the F1 score is improved by 5.6 points to 77.6. It is nontrivial to obtain accurate alignments from a set of independently predicted spans. In preliminary experiments, we used Integer Linear Programming (ILP) to optimize the span predictions as in (DeNero and Klein, 2008). We found that using context is simple and more effective. 4.3 Learning Curve Table5 shows the learning curve of the proposed method using the Zh-En data. Compared to previous methods, our method achieved higher accuracy using less training data. Even for 300 sentences, the F1 score of our method was 79.6, which is 6.2 points higher than that of (Stengel-Eskin et al., 2019) (73.4), which used more than 4800 sentences for training. A supervised model trained on hand-aligned data must learn the idiosyncrasies of the annotation standard, which varies widely from language to language and across d"
2020.emnlp-main.41,W03-0301,0,0.179175,"are greatly affected by the orthography of the target language. For languages whose words are not delimited by white spaces, such as Chinese and Japanese, the span prediction accuracy “to English” is signiﬁcantly higher than that of “from English”. In this case, grow-diagﬁnal outperforms bidi-avg. By contract, for languages with spaces between words, such as Ger10 In En-Fr data, all “phrasal correspondence” are annotated as possible alignments. For example, if a phrase with three words and a phrase with four words are regarded as mutual translations, 12 word alignments are marked as possible (Mihalcea and Pedersen, 2003). There are 4,038 sure alignments and 13,400 possible alignments in the En-Fr data (447 sentences). If our model is trained on both sure and possible, such numerous possible alignments function as noise, which results in low precision. 561 Test set Zh-En Ja-En De-En Ro-En En-Fr Method Zh to En En to Zh intersection union grow-diag-ﬁnal bidi-avg Ja to En En to Ja intersection union grow-diag-ﬁnal bidi-avg De to En En to De intersection union grow-diag-ﬁnal bidi-avg Ro to En En to Ro intersection union grow-diag-ﬁnal bidi-avg En to Fr Fr to En intersection union grow-diag-ﬁnal bidi-avg P 89.9 82"
2020.emnlp-main.41,N19-1423,0,0.585401,"Methods in Natural Language Processing, pages 555–565, c November 16–20, 2020. 2020 Association for Computational Linguistics million parallel sentences to pretrain their models. Applying these methods to low-resource language pairs and domains is difﬁcult. In this paper, we present a novel supervised word alignment method that requires no parallel sentences for pretraining and can be trained from fewer gold word alignments (150-300 sentences). It formalizes word alignment as a collection of SQuAD-style span prediction problems (Rajpurkar et al., 2016) and solves them with multilingual BERT (Devlin et al., 2019). We experimentally show that our proposed model signiﬁcantly outperformed both (Garg et al., 2019) and (Stengel-Eskin et al., 2019). Our main contribution is that we make supervised word alignment more practical. Stengel-Eskin et al. (2019) argued that supervised word alignment is a viable option. They concluded that alignment annotation could be performed rapidly 4.4 sentences per minute by annotators with minimal experience using a web-based crowd-sourcing interface. Assuming that a small amount of gold word alignment data, which can be annotated in a couple of hours, our proposed method co"
2020.emnlp-main.41,N13-1073,0,0.154005,"For example, we achieved 86.7 F1 score for the ChineseEnglish data, which is 13.3 points higher than the previous state-of-the-art supervised method.1 1 Introduction Over the last several years, machine translation accuracy has been greatly improved by neural networks (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Vaswani et al., 2017). However, word alignment tools, which were developed during the age of statistical machine translation (Brown et al., 1993; Koehn et al., 2007) such as GIZA++ (Och and Ney, 2003), MGIZA (Gao and Vogel, 2008) and FastAlign (Dyer et al., 2013), remain 1 Our implementation is available https://github.com/nttcslab-nlp/word align/ at 555 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 555–565, c November 16–20, 2020. 2020 Association for Computational Linguistics million parallel sentences to pretrain their models. Applying these methods to low-resource language pairs and domains is difﬁcult. In this paper, we present a novel supervised word alignment method that requires no parallel sentences for pretraining and can be trained from fewer gold word alignments (150-300 sentences). It formal"
2020.emnlp-main.41,P00-1056,0,0.321379,"riments: Chinese-English (Zh-En), Japanese-English (Ja-En), GermanEnglish (De-En), Romanian-English (Ro-En), and English-French (En-Fr). Stengel-Eskin et al. (2019) used the Zh-En dataset and Garg et al. (2019) used the De-En, RoEn, and En-Fr datasets. We added a Ja-En dataset 3 Stengel-Eskin et al. (2019) also used an Arabic-English (Ar-En) dataset. We did not use it here due to time constraints 4 http://www.phontron.com/kftt/index.html 5 https://github.com/lilt/alignment-scripts 6 https://www-i6.informatik.rwthaachen.de/goldAlignment/ 559 2003)7 . The En-Fr data were originally provided by (Och and Ney, 2000). The numbers of the test sentences in the De-En, Ro-En, and En-Fr datasets are 508, 248, and 447. In De-En and En-Fr, we used 300 sentences for training. In Ro-En, we used 150 sentences for training. The other sentences were used for testing. If necessary, we also used alignment error rate (AER) (Och and Ney, 2003) because some previous works only reported it. Let quality of alignment A be measured against a gold word alignment that contains sure (S) and possible(P ) alignments (S ⊆ P ). Precision, recall, and AER are deﬁned as follows: 3.2 Implementation Details P recision(A, P ) = We used B"
2020.emnlp-main.41,J07-3002,0,0.043853,"atasets, Garg et al. (2019), which is the state-of-the-art unsupervised method, only reported AER in their paper. We classiﬁed their method as unsupervised because they did not use manually created word alignment data. Their method used the GIZA++ output for supervision. For reference, we show the precision, recall, and AER of MGIZA (Zenkel et al., 2019)9 , the AER of We evaluated the quality of the word alignment using an F1 score that assigns equal weights to precision (P) and recall (R): 8 |S ∩ A |+ |P ∩ A| |S |+ |A| (7) 3.4 Results 3.3 Measures for Word Alignment Quality 7 |S ∩ A| |S| (6) Fraser and Marcu (2007) pointed out that since AER is broken in a way that favors precision, it should be used sparingly. In previous works, Stengel-Eskin et al. (2019) used precision, recall, and F1, while Garg et al. (2019) and Zenkel et al. (2019) used precision, recall, and AER. Note that, if we distinguish between sure and possible alignments, precision and recall are different from those when we do not make such a distinction. Among our ﬁve datasets, De-En and En-FR make a distinction between sure and possible alignments. Here, if the difference of the scores of best nonnull span sˆij and null (no-answer) span"
2020.emnlp-main.41,J03-1002,0,0.217183,"ed word alignment methods without any bitexts for pretraining. For example, we achieved 86.7 F1 score for the ChineseEnglish data, which is 13.3 points higher than the previous state-of-the-art supervised method.1 1 Introduction Over the last several years, machine translation accuracy has been greatly improved by neural networks (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Vaswani et al., 2017). However, word alignment tools, which were developed during the age of statistical machine translation (Brown et al., 1993; Koehn et al., 2007) such as GIZA++ (Och and Ney, 2003), MGIZA (Gao and Vogel, 2008) and FastAlign (Dyer et al., 2013), remain 1 Our implementation is available https://github.com/nttcslab-nlp/word align/ at 555 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 555–565, c November 16–20, 2020. 2020 Association for Computational Linguistics million parallel sentences to pretrain their models. Applying these methods to low-resource language pairs and domains is difﬁcult. In this paper, we present a novel supervised word alignment method that requires no parallel sentences for pretraining and can be trained"
2020.emnlp-main.41,P19-1467,0,0.192383,"Missing"
2020.emnlp-main.41,W08-0509,0,0.0291561,"ithout any bitexts for pretraining. For example, we achieved 86.7 F1 score for the ChineseEnglish data, which is 13.3 points higher than the previous state-of-the-art supervised method.1 1 Introduction Over the last several years, machine translation accuracy has been greatly improved by neural networks (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Vaswani et al., 2017). However, word alignment tools, which were developed during the age of statistical machine translation (Brown et al., 1993; Koehn et al., 2007) such as GIZA++ (Och and Ney, 2003), MGIZA (Gao and Vogel, 2008) and FastAlign (Dyer et al., 2013), remain 1 Our implementation is available https://github.com/nttcslab-nlp/word align/ at 555 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 555–565, c November 16–20, 2020. 2020 Association for Computational Linguistics million parallel sentences to pretrain their models. Applying these methods to low-resource language pairs and domains is difﬁcult. In this paper, we present a novel supervised word alignment method that requires no parallel sentences for pretraining and can be trained from fewer gold word alignme"
2020.emnlp-main.41,P18-2124,0,0.0791324,"Missing"
2020.emnlp-main.41,D19-1453,0,0.0628775,"L markups (Hashimoto et al., 2019), and enforcing terminology constraints (pre-speciﬁed translation) (Song et al., 2019). We could also use it for the user interfaces of post-editing to detect such problems as under-translation (Tu et al., 2016). Word alignment has a long research history. Here, we focus on approaches that use neural networks because they are the state-of-the art. Most previous works that use them for word alignment (Yang et al., 2013; Tamura et al., 2014; Legrand et al., 2016) achieved accuracies that are basically comparable to GIZA++. However, the accuracy of recent works (Garg et al., 2019; Stengel-Eskin et al., 2019; Zenkel et al., 2020) based on the Transformer (Vaswani et al., 2017), which is the state-of-the art neural machine translation model, have started to outperform GIZA++. Garg et al. (2019) made the attention of the Transformer more closely resembled the word alignment, and achieved better accuracy than GIZA++ when they used alignments obtained from it for supervision. Zenkel et al. (2020) added an alignment layer using a full target context on top of the Transformer and trained it with a loss function that encouraged contiguous alignment and bidirectional agreement"
2020.emnlp-main.41,D16-1264,0,0.255037,"rd align/ at 555 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 555–565, c November 16–20, 2020. 2020 Association for Computational Linguistics million parallel sentences to pretrain their models. Applying these methods to low-resource language pairs and domains is difﬁcult. In this paper, we present a novel supervised word alignment method that requires no parallel sentences for pretraining and can be trained from fewer gold word alignments (150-300 sentences). It formalizes word alignment as a collection of SQuAD-style span prediction problems (Rajpurkar et al., 2016) and solves them with multilingual BERT (Devlin et al., 2019). We experimentally show that our proposed model signiﬁcantly outperformed both (Garg et al., 2019) and (Stengel-Eskin et al., 2019). Our main contribution is that we make supervised word alignment more practical. Stengel-Eskin et al. (2019) argued that supervised word alignment is a viable option. They concluded that alignment annotation could be performed rapidly 4.4 sentences per minute by annotators with minimal experience using a web-based crowd-sourcing interface. Assuming that a small amount of gold word alignment data, which"
2020.emnlp-main.41,P09-1104,0,0.0405738,"nkel et al., 2019) GIZA++ (BPE, Grow-Diag) (Zenkel et al., 2020) Alignment layer, bidi, unsupervised (Zenkel et al., 2020) Align and translate, GIZA++ supervised (Garg et al., 2019) Our method (trained on sure + possible) MGIZA (BPE, Grow-Diag-Final) (Zenkel et al., 2019) GIZA++ (BPE, Grow-Diag) (Zenkel et al., 2020) Alignment layer, bidi, unsupervised (Zenkel et al., 2020) Align and translate, GIZA++ supervised (Garg et al., 2019) Our method MGIZA (BPE, Grow-Diag) (Zenkel et al., 2019) GIZA++ (BPE, Grow-Diag) (Zenkel et al., 2020) Discriminative matching (Taskar et al., 2005) Supervised ITG (Haghighi et al., 2009) Alignment layer, bidi, unsupervised (Zenkel et al., 2020) Align and translate, GIZA++ supervised (Garg et al., 2019) Our method (only trained on sure) P 80.5 72.9 84.4 59.5 77.3 89.9 90.4 79.6 91.3 89.9 90.9 90.4 97.5 95.5 97.7 R 50.5 74.0 89.2 55.6 78.0 81.7 85.3 93.9 70.2 87.3 61.8 85.3 89.7 94.2 93.9 F1 62.0 73.4 86.7 57.6 77.6 85.6 87.8 86.2 - AER 20.6 18.7 16.3 16.0 11.4 26.4 26.5 23.4 23.1 12.2 5.9 5.5 5.4 5.0 5.0 4.6 4.0 Table 2: Best-effort comparison of proposed method with previous works GIZA++ (Zenkel et al., 2020), as well as the accuracies of previous methods (Taskar et al., 2005"
2020.emnlp-main.41,W19-5212,0,0.0815115,"ignment Method based on Cross-Language Span Prediction using Multilingual BERT Masaaki Nagata and Katsuki Chousa and Masaaki Nishino NTT Communication Science Laboratories, NTT Corporation 2-4, Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237, Japan {masaaki.nagata.et,katsuki.chousa.bg,masaaki.nishino.uh}@hco.ntt.co.jp Abstract widely used because the improvement of word alignment accuracy has become stagnant. This situation is unfortunate because word alignment could be used for many downstream tasks including projecting linguistic annotation (Yarowsky et al., 2001), projecting XML markups (Hashimoto et al., 2019), and enforcing terminology constraints (pre-speciﬁed translation) (Song et al., 2019). We could also use it for the user interfaces of post-editing to detect such problems as under-translation (Tu et al., 2016). Word alignment has a long research history. Here, we focus on approaches that use neural networks because they are the state-of-the art. Most previous works that use them for word alignment (Yang et al., 2013; Tamura et al., 2014; Legrand et al., 2016) achieved accuracies that are basically comparable to GIZA++. However, the accuracy of recent works (Garg et al., 2019; Stengel-Eskin e"
2020.emnlp-main.41,N19-1044,0,0.0145192,"ta and Katsuki Chousa and Masaaki Nishino NTT Communication Science Laboratories, NTT Corporation 2-4, Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237, Japan {masaaki.nagata.et,katsuki.chousa.bg,masaaki.nishino.uh}@hco.ntt.co.jp Abstract widely used because the improvement of word alignment accuracy has become stagnant. This situation is unfortunate because word alignment could be used for many downstream tasks including projecting linguistic annotation (Yarowsky et al., 2001), projecting XML markups (Hashimoto et al., 2019), and enforcing terminology constraints (pre-speciﬁed translation) (Song et al., 2019). We could also use it for the user interfaces of post-editing to detect such problems as under-translation (Tu et al., 2016). Word alignment has a long research history. Here, we focus on approaches that use neural networks because they are the state-of-the art. Most previous works that use them for word alignment (Yang et al., 2013; Tamura et al., 2014; Legrand et al., 2016) achieved accuracies that are basically comparable to GIZA++. However, the accuracy of recent works (Garg et al., 2019; Stengel-Eskin et al., 2019; Zenkel et al., 2020) based on the Transformer (Vaswani et al., 2017), whi"
2020.emnlp-main.41,P07-2045,0,0.0132993,"d previous supervised and unsupervised word alignment methods without any bitexts for pretraining. For example, we achieved 86.7 F1 score for the ChineseEnglish data, which is 13.3 points higher than the previous state-of-the-art supervised method.1 1 Introduction Over the last several years, machine translation accuracy has been greatly improved by neural networks (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Vaswani et al., 2017). However, word alignment tools, which were developed during the age of statistical machine translation (Brown et al., 1993; Koehn et al., 2007) such as GIZA++ (Och and Ney, 2003), MGIZA (Gao and Vogel, 2008) and FastAlign (Dyer et al., 2013), remain 1 Our implementation is available https://github.com/nttcslab-nlp/word align/ at 555 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 555–565, c November 16–20, 2020. 2020 Association for Computational Linguistics million parallel sentences to pretrain their models. Applying these methods to low-resource language pairs and domains is difﬁcult. In this paper, we present a novel supervised word alignment method that requires no parallel sentences"
2020.emnlp-main.41,D19-1084,0,0.249735,"Missing"
2020.emnlp-main.41,W16-2207,0,0.196787,"Missing"
2020.emnlp-main.41,P14-1138,0,0.730445,"ecause word alignment could be used for many downstream tasks including projecting linguistic annotation (Yarowsky et al., 2001), projecting XML markups (Hashimoto et al., 2019), and enforcing terminology constraints (pre-speciﬁed translation) (Song et al., 2019). We could also use it for the user interfaces of post-editing to detect such problems as under-translation (Tu et al., 2016). Word alignment has a long research history. Here, we focus on approaches that use neural networks because they are the state-of-the art. Most previous works that use them for word alignment (Yang et al., 2013; Tamura et al., 2014; Legrand et al., 2016) achieved accuracies that are basically comparable to GIZA++. However, the accuracy of recent works (Garg et al., 2019; Stengel-Eskin et al., 2019; Zenkel et al., 2020) based on the Transformer (Vaswani et al., 2017), which is the state-of-the art neural machine translation model, have started to outperform GIZA++. Garg et al. (2019) made the attention of the Transformer more closely resembled the word alignment, and achieved better accuracy than GIZA++ when they used alignments obtained from it for supervision. Zenkel et al. (2020) added an alignment layer using a full"
2020.emnlp-main.41,H05-1010,0,0.23195,"Missing"
2020.emnlp-main.41,P16-1008,0,0.0150457,"raku-gun, Kyoto, 619-0237, Japan {masaaki.nagata.et,katsuki.chousa.bg,masaaki.nishino.uh}@hco.ntt.co.jp Abstract widely used because the improvement of word alignment accuracy has become stagnant. This situation is unfortunate because word alignment could be used for many downstream tasks including projecting linguistic annotation (Yarowsky et al., 2001), projecting XML markups (Hashimoto et al., 2019), and enforcing terminology constraints (pre-speciﬁed translation) (Song et al., 2019). We could also use it for the user interfaces of post-editing to detect such problems as under-translation (Tu et al., 2016). Word alignment has a long research history. Here, we focus on approaches that use neural networks because they are the state-of-the art. Most previous works that use them for word alignment (Yang et al., 2013; Tamura et al., 2014; Legrand et al., 2016) achieved accuracies that are basically comparable to GIZA++. However, the accuracy of recent works (Garg et al., 2019; Stengel-Eskin et al., 2019; Zenkel et al., 2020) based on the Transformer (Vaswani et al., 2017), which is the state-of-the art neural machine translation model, have started to outperform GIZA++. Garg et al. (2019) made the a"
2020.emnlp-main.41,2006.iwslt-papers.7,0,0.130105,"Missing"
2020.emnlp-main.41,C96-2141,0,0.790603,"es, we used the heuristics as the default symmetrization method in our experiments. 4.2 Importance of Source Context Table 4 shows the word alignment accuracies for questions of different source contexts. We used the Ja-En data and found that the source context information is critical for predicting the target span. Without it, the F1 score of the proposed method 5 Related Works For several years, word alignment methods using neural networks (Yang et al., 2013; Tamura et al., 2014; Legrand et al., 2016) failed to signiﬁ562 cantly outperform those using statistical methods (Brown et al., 1993; Vogel et al., 1996). tion that maps a word representation to its translation with both contexts. Recently, Stengel-Eskin et al. (2019) proposed a supervised method using a small amount of annotated data (1.7K-5K sentences) and signiﬁcantly outperformed the accuracy of GIZA++. They ﬁrst mapped the source and target word representations obtained from the encoder and decoder of the Transformer to a shared space using a three-layer feed-forward neural network. They then applied 3 × 3 convolution and softmax to obtain the alignment scores between the source and target words. They used 4M parallel sentences to pretrai"
2020.emnlp-main.41,P13-1017,0,0.688577,"on is unfortunate because word alignment could be used for many downstream tasks including projecting linguistic annotation (Yarowsky et al., 2001), projecting XML markups (Hashimoto et al., 2019), and enforcing terminology constraints (pre-speciﬁed translation) (Song et al., 2019). We could also use it for the user interfaces of post-editing to detect such problems as under-translation (Tu et al., 2016). Word alignment has a long research history. Here, we focus on approaches that use neural networks because they are the state-of-the art. Most previous works that use them for word alignment (Yang et al., 2013; Tamura et al., 2014; Legrand et al., 2016) achieved accuracies that are basically comparable to GIZA++. However, the accuracy of recent works (Garg et al., 2019; Stengel-Eskin et al., 2019; Zenkel et al., 2020) based on the Transformer (Vaswani et al., 2017), which is the state-of-the art neural machine translation model, have started to outperform GIZA++. Garg et al. (2019) made the attention of the Transformer more closely resembled the word alignment, and achieved better accuracy than GIZA++ when they used alignments obtained from it for supervision. Zenkel et al. (2020) added an alignmen"
2020.emnlp-main.41,H01-1035,0,0.377901,"Missing"
2020.emnlp-main.41,2020.acl-main.146,0,0.772635,"ng terminology constraints (pre-speciﬁed translation) (Song et al., 2019). We could also use it for the user interfaces of post-editing to detect such problems as under-translation (Tu et al., 2016). Word alignment has a long research history. Here, we focus on approaches that use neural networks because they are the state-of-the art. Most previous works that use them for word alignment (Yang et al., 2013; Tamura et al., 2014; Legrand et al., 2016) achieved accuracies that are basically comparable to GIZA++. However, the accuracy of recent works (Garg et al., 2019; Stengel-Eskin et al., 2019; Zenkel et al., 2020) based on the Transformer (Vaswani et al., 2017), which is the state-of-the art neural machine translation model, have started to outperform GIZA++. Garg et al. (2019) made the attention of the Transformer more closely resembled the word alignment, and achieved better accuracy than GIZA++ when they used alignments obtained from it for supervision. Zenkel et al. (2020) added an alignment layer using a full target context on top of the Transformer and trained it with a loss function that encouraged contiguous alignment and bidirectional agreement. They outperformed GIZA++ without GIZA++ output f"
2020.emnlp-main.41,P16-1162,0,\N,Missing
2020.findings-emnlp.77,D19-1371,0,0.10446,"ing the context in the abstract. To this end, some statistical methods with hand-engineered features have been proposed, including Hidden Markov Models (HMMs) (Lin et al., 2006) and Conditional Random Fields (CRFs) (Hirohata et al., 2008; Kim et al., 2011; Hassanzadeh et al., 2014). Recently, with the success of neural network models for NLP tasks, Dernoncourt et al. (2017) and Jin and Szolovits (2018) have employed BiLSTMs to obtain sentence embeddings based on word embeddings and CRFs for assigning labels to the sentences. Cohan et al. (2019) employed a pre-trained language model, S CIBERT (Beltagy et al., 2019), which is a variant of BERT (Devlin et al., 2019) trained with scientific papers, to improve the performance of classification without CRFs. Previous methods cast the segmentation with the labeling as a sentence classification. However, such methods have a critical problem: their performances on longer spans1 is not so good since they are designed to maximize the prediction of rhetorical roles for a small context. To tackle the problem, we propose a novel approach, neural sequential span classification, that directly gives the labels for the spans while considering all possible spans of vario"
2020.findings-emnlp.77,D19-1383,0,0.277241,"abels with a B(egin)/I(nside) tag set to each sentence while considering the context in the abstract. To this end, some statistical methods with hand-engineered features have been proposed, including Hidden Markov Models (HMMs) (Lin et al., 2006) and Conditional Random Fields (CRFs) (Hirohata et al., 2008; Kim et al., 2011; Hassanzadeh et al., 2014). Recently, with the success of neural network models for NLP tasks, Dernoncourt et al. (2017) and Jin and Szolovits (2018) have employed BiLSTMs to obtain sentence embeddings based on word embeddings and CRFs for assigning labels to the sentences. Cohan et al. (2019) employed a pre-trained language model, S CIBERT (Beltagy et al., 2019), which is a variant of BERT (Devlin et al., 2019) trained with scientific papers, to improve the performance of classification without CRFs. Previous methods cast the segmentation with the labeling as a sentence classification. However, such methods have a critical problem: their performances on longer spans1 is not so good since they are designed to maximize the prediction of rhetorical roles for a small context. To tackle the problem, we propose a novel approach, neural sequential span classification, that directly gives"
2020.findings-emnlp.77,I17-2052,0,0.507361,"qKdgxjFBP83rNYwDJWkOFzPZzgHBexF2VQGVGSjVQlFmkG8CWUyQ9Xfo67&lt;/latexit&gt; F gure 1 Overv ew of he neura sequen a span c ass fica on In he examp e five rhe or ca abe s “Background” (B) “Ob ec ve” (O) “Me hods” (M) “Resu s” (R) and “Conc us ons” (C) can be ass gned o spans The abs rac ha cons s s of five sen ences s segmen ed n o four spans span(1 1) span(2 2) span(3 4) and span(5 5) and hese spans are abe ed as B M R and C respec ve y 2019) to handle spans of the different lengths To demonstrate the effectiveness of method we conduc ed exper men a eva ua ons on wo benchmark datasets PubMed 20k RCT (Dernoncourt and Lee 2017) and NICTA-PIBOSO (K m e a 2011) The results show that our method achieved the best micro sentence-F1 score of 93 1 and micro span-F1 score of 84 3 n he PubMed 20k RCT da ase and the best micro sentence-F1 score of 84 4 and micro span-F1 score of 58 7 in the NICTA-PIBOSO dataset 2 Proposed Method follows: −−−−→ ←−−−− f = LSTM(f −1 , s ), b = LSTM(b +1 , s ). (1) Here s represents the embedding of the i-th sentence To obtain s we utilize BERT which has been pre-trained with PubMed (Peng et al 2019) We insert [CLS] tokens at the beginning and [SEP] tokens at the end of sentences and then extract"
2020.findings-emnlp.77,E17-2110,0,0.0160424,"on-answering (Guo et al., 2013). Most previous methods in PubMed have regarded the task as a sequence labeling, namely sequential sentence classification, that assigns rhetorical labels with a B(egin)/I(nside) tag set to each sentence while considering the context in the abstract. To this end, some statistical methods with hand-engineered features have been proposed, including Hidden Markov Models (HMMs) (Lin et al., 2006) and Conditional Random Fields (CRFs) (Hirohata et al., 2008; Kim et al., 2011; Hassanzadeh et al., 2014). Recently, with the success of neural network models for NLP tasks, Dernoncourt et al. (2017) and Jin and Szolovits (2018) have employed BiLSTMs to obtain sentence embeddings based on word embeddings and CRFs for assigning labels to the sentences. Cohan et al. (2019) employed a pre-trained language model, S CIBERT (Beltagy et al., 2019), which is a variant of BERT (Devlin et al., 2019) trained with scientific papers, to improve the performance of classification without CRFs. Previous methods cast the segmentation with the labeling as a sentence classification. However, such methods have a critical problem: their performances on longer spans1 is not so good since they are designed to m"
2020.findings-emnlp.77,N19-1423,0,0.00896325,"statistical methods with hand-engineered features have been proposed, including Hidden Markov Models (HMMs) (Lin et al., 2006) and Conditional Random Fields (CRFs) (Hirohata et al., 2008; Kim et al., 2011; Hassanzadeh et al., 2014). Recently, with the success of neural network models for NLP tasks, Dernoncourt et al. (2017) and Jin and Szolovits (2018) have employed BiLSTMs to obtain sentence embeddings based on word embeddings and CRFs for assigning labels to the sentences. Cohan et al. (2019) employed a pre-trained language model, S CIBERT (Beltagy et al., 2019), which is a variant of BERT (Devlin et al., 2019) trained with scientific papers, to improve the performance of classification without CRFs. Previous methods cast the segmentation with the labeling as a sentence classification. However, such methods have a critical problem: their performances on longer spans1 is not so good since they are designed to maximize the prediction of rhetorical roles for a small context. To tackle the problem, we propose a novel approach, neural sequential span classification, that directly gives the labels for the spans while considering all possible spans of various lengths in the abstract. That is, our method is"
2020.findings-emnlp.77,W19-5006,0,0.23808,"s of method we conduc ed exper men a eva ua ons on wo benchmark datasets PubMed 20k RCT (Dernoncourt and Lee 2017) and NICTA-PIBOSO (K m e a 2011) The results show that our method achieved the best micro sentence-F1 score of 93 1 and micro span-F1 score of 84 3 n he PubMed 20k RCT da ase and the best micro sentence-F1 score of 84 4 and micro span-F1 score of 58 7 in the NICTA-PIBOSO dataset 2 Proposed Method follows: −−−−→ ←−−−− f = LSTM(f −1 , s ), b = LSTM(b +1 , s ). (1) Here s represents the embedding of the i-th sentence To obtain s we utilize BERT which has been pre-trained with PubMed (Peng et al 2019) We insert [CLS] tokens at the beginning and [SEP] tokens at the end of sentences and then extract vectors corresponding to [CLS] tokens in the penultimate layer as sentence vectors Finally we represent a span from the i-th sentence to the j- h sen ence as a vec or vspan( j) wh ch s a concatenation of four vectors as follows: To perform sequen a span c ass fica on n an endto-end manner we need to represent spans as vecors and hand e a poss b e sequences w h var ous lengths in the abstract To this end we introduce BiLSTMs and Semi-Markov CRFs (SCRFs) Figure 1 shows an overview of our method The"
2020.findings-emnlp.77,P16-1111,0,0.022528,"sing (NLP). For example, abstracts in PubMed, a database of the biomedical literature, can be divided into rhetorical segments such as “Objective”, “Methods”, “Results”, and “Conclusions”. Abstracts segmented for each rhetorical role allows us to exploit advanced search. That is, researchers can easily find information by utilizing the structured queries such as “find abstracts that contain ‘Covid19’ in ‘Objective’ and ‘Remdesivir’ in ‘Methods’”. Furthermore, the technique can also be used for NLP applications such as academic writing support (Huang and Chen, 2017), scientific trend analysis (Prabhakaran et al., 2016), and question-answering (Guo et al., 2013). Most previous methods in PubMed have regarded the task as a sequence labeling, namely sequential sentence classification, that assigns rhetorical labels with a B(egin)/I(nside) tag set to each sentence while considering the context in the abstract. To this end, some statistical methods with hand-engineered features have been proposed, including Hidden Markov Models (HMMs) (Lin et al., 2006) and Conditional Random Fields (CRFs) (Hirohata et al., 2008; Kim et al., 2011; Hassanzadeh et al., 2014). Recently, with the success of neural network models for"
2020.findings-emnlp.77,I08-1050,0,0.558478,"applications such as academic writing support (Huang and Chen, 2017), scientific trend analysis (Prabhakaran et al., 2016), and question-answering (Guo et al., 2013). Most previous methods in PubMed have regarded the task as a sequence labeling, namely sequential sentence classification, that assigns rhetorical labels with a B(egin)/I(nside) tag set to each sentence while considering the context in the abstract. To this end, some statistical methods with hand-engineered features have been proposed, including Hidden Markov Models (HMMs) (Lin et al., 2006) and Conditional Random Fields (CRFs) (Hirohata et al., 2008; Kim et al., 2011; Hassanzadeh et al., 2014). Recently, with the success of neural network models for NLP tasks, Dernoncourt et al. (2017) and Jin and Szolovits (2018) have employed BiLSTMs to obtain sentence embeddings based on word embeddings and CRFs for assigning labels to the sentences. Cohan et al. (2019) employed a pre-trained language model, S CIBERT (Beltagy et al., 2019), which is a variant of BERT (Devlin et al., 2019) trained with scientific papers, to improve the performance of classification without CRFs. Previous methods cast the segmentation with the labeling as a sentence cla"
2020.findings-emnlp.77,D18-1191,0,0.0278845,"anner we need to represent spans as vecors and hand e a poss b e sequences w h var ous lengths in the abstract To this end we introduce BiLSTMs and Semi-Markov CRFs (SCRFs) Figure 1 shows an overview of our method The BiLSTMs layer generates span vectors from sentence vectors incorporating the context in the abstract and the SCRFs layer learns the labeling of span sequences by cons der ng a poss b e sequences of various lengths The details are described below 21 Span Representation B LSTMs have been successfu y used o represen spans as vectors in many NLP tasks such as semantic role labeling (Ouchi et al 2018) syntactic parsing (Stern et al 2017) and coreference resolution (Lee et al 2017) BiLSTMs use a forward− −−− → LSTM func on LSTM and backward-LSTM func←−−−− tion LSTM where the forward and backward hidden states of the i-th sentence are represented as 872 vspan( 22 j) = [f −1 ; b ; fj ; bj+1 ]. (2) Neural Semi-Markov CRFs Neural SCRFs (Ye and Ling 2018; Kemos et al 2019) learn parameters the logPN to maximize ∗ likelihood function j=1 log P (yj |Xj ) where N is the number of training data yj∗ is the correctly labeled sequence of spans for the j-th abstract in the training data and X is the seq"
2020.iwslt-1.17,N12-1047,0,0.0426694,"NMT model as T2S feature. In addition, we also use the score generated by a target-to-source right-to-left model as a reranking feature. • Length Feature: We also design a length feature that quantifies the difference between the ratio of each sentence pair and the optimal ratio. The optimal ratio is determined according to the training parallel corpus. 3.4 Reranking Reranking is a method of improving translation quality by rescoring a list of n-best hypotheses. For our submissions, we generate n-best hypotheses through a source-to-target NMT model and then train a reranker using k-best MIRA (Cherry and Foster, 2012). The features we use for reranking are: 3.5 Postprocessing Since we perform NFKC-based text normalization on the training corpus, we also employ a postprocessing algorithm on the generated hypothesis. To be more specific, we change half-width punctuations to full-width punctuations. • Left-to-right NMT Feature: We keep the original perplexity by the original translation model as a L2R reranking feature. 4 Results • Right-to-left NMT Feature: In order to address exposure bias problem, we train a rightto-left (R2L) NMT model using the same Results and ablations for Ja→Zh and Zh→Ja are shown in"
2020.iwslt-1.17,D18-1045,0,0.104694,"uning. For data preprocessing, firstly, various orthodox methods including punctuation normalization, tokenization as well as byte pair encoding (Sennrich et al., 2016a) which have been widely used in recent researches are applied. Besides, we also apply manual rules, aiming to clean the provided parallel data, the monolingual data and the synthetic data which is generated by ourselves for data augmentation. For the sake of a better use of all provided data, we do a back-translation for either the source-side and the target-side monolingual data. Meanwhile, inspired by noised training method (Edunov et al., 2018; Wu et al., 2019; He et al., 2020), we add noise to the source sentences of the synthetic parallel corpus to make the translation models more robust and to improve its generalization ability. In addition, in inference phrase, we apply the model ensemble strategy while top n-best hypotheses are kept for further multi-features reranking process. At last, post-processing is applied to correct the inconsistent punctuation form. This paper is organized as follows: in Section 2, we describe our data preprocessing and data filtering. Details of each component of our systems are described in Section"
2020.iwslt-1.17,N19-4009,0,0.0181098,"rules: 3.1 Baseline System We adopt the base Transformer as our machine translation system following the settings as described in Vaswani et al. (2017), consisting of 6 encoder layers, 6 decoder layers, 8 heads, with an embedding dimension of 512 and feed-forward network dimension of 2048. The dropout probability is 0.2. For all experiments, we adopt the Adam optimizer (Kingma and Ba, 2014) using β1 = 0.9, β2 = 0.98, and "" = 1e-8. The learning rate is scheduled using inverse square root schedule with a maximum learning rate 0.0005 and 4000 warmup steps. We train all our models using fairseq5 (Ott et al., 2019) on two NIVIDA 2080Ti GPUs with a batch size of around 4096 tokens. During training, we employ label smoothing of value 0.1. We average the last 5 model checkpoints and use it for decoding. • Filter out duplicate sentence pairs. • Filter out sentence pairs which have identical source-side and target-side sentences. • Filter out sentence pairs with more than 10 punctuations or imbalanced punctuation ratio. • Filter out sentence pairs which contains half or more tokens that are numbers or letters. • Filter out sentence pairs which contain HTML tags or emoji. • Filter out sentence pairs with wron"
2020.iwslt-1.17,P16-1009,0,0.24387,".) + Filtering(in subwords.) Monolingual data(in sents.) + Filtering(in sents) + Filtering(in subwords.) 2.1 Data Preprocessing The provided parallel training data contains different forms of characters, for example, full-width form and half-width form. To get a normalized form, we remove all the spaces between characters and perform NFKC-based text normalization. Chinese sentences are segmented with the default mode of Jieba1 and Japanese sentences are segmented with Mecab2 using mecab-ipadicNEologd3 dictionary. To limit the size of vocabularies of NMT models, we use byte pair encoding(BPE) (Sennrich et al., 2016b) with 32K split operations separately for both side. Ja 20.9M 9.8M 164.5M 161.5M 17.8M 308.3M Zh 20.9M 9.8M 128.1M 161.5M 17.6M 254.9M Table 1: Statistics of the provided data. Notice that we treat two sides of the provided unfiltered dataset separately as monolingual data, therefore the number of monolingual data in terms of sentence pairs are identical as before data filtering. The same data filtering strategies except those designed for sentence pairs are also employed on monolingual data. Details of the preprocessed dataset in terms of the amount of sentences and BPE subwords are listed"
2020.iwslt-1.17,P16-1162,0,0.353045,".) + Filtering(in subwords.) Monolingual data(in sents.) + Filtering(in sents) + Filtering(in subwords.) 2.1 Data Preprocessing The provided parallel training data contains different forms of characters, for example, full-width form and half-width form. To get a normalized form, we remove all the spaces between characters and perform NFKC-based text normalization. Chinese sentences are segmented with the default mode of Jieba1 and Japanese sentences are segmented with Mecab2 using mecab-ipadicNEologd3 dictionary. To limit the size of vocabularies of NMT models, we use byte pair encoding(BPE) (Sennrich et al., 2016b) with 32K split operations separately for both side. Ja 20.9M 9.8M 164.5M 161.5M 17.8M 308.3M Zh 20.9M 9.8M 128.1M 161.5M 17.6M 254.9M Table 1: Statistics of the provided data. Notice that we treat two sides of the provided unfiltered dataset separately as monolingual data, therefore the number of monolingual data in terms of sentence pairs are identical as before data filtering. The same data filtering strategies except those designed for sentence pairs are also employed on monolingual data. Details of the preprocessed dataset in terms of the amount of sentences and BPE subwords are listed"
2020.iwslt-1.17,D19-1430,0,0.0483611,"ocessing, firstly, various orthodox methods including punctuation normalization, tokenization as well as byte pair encoding (Sennrich et al., 2016a) which have been widely used in recent researches are applied. Besides, we also apply manual rules, aiming to clean the provided parallel data, the monolingual data and the synthetic data which is generated by ourselves for data augmentation. For the sake of a better use of all provided data, we do a back-translation for either the source-side and the target-side monolingual data. Meanwhile, inspired by noised training method (Edunov et al., 2018; Wu et al., 2019; He et al., 2020), we add noise to the source sentences of the synthetic parallel corpus to make the translation models more robust and to improve its generalization ability. In addition, in inference phrase, we apply the model ensemble strategy while top n-best hypotheses are kept for further multi-features reranking process. At last, post-processing is applied to correct the inconsistent punctuation form. This paper is organized as follows: in Section 2, we describe our data preprocessing and data filtering. Details of each component of our systems are described in Section 3. The results of"
2020.iwslt-1.17,D16-1160,0,0.0266344,"t sentence pairs which have identical source-side and target-side sentences. • Filter out sentence pairs with more than 10 punctuations or imbalanced punctuation ratio. • Filter out sentence pairs which contains half or more tokens that are numbers or letters. • Filter out sentence pairs which contain HTML tags or emoji. • Filter out sentence pairs with wrong languages identified by langid.4 3.2 Large-scale Noised Training It is widely known that the performance of a NMT system relies heavily on the amount of parallel training data. Back-translation (Sennrich et al., 2016a) and Self-training (Zhang and Zong, 2016) are effective and commonly used data augmentation techniques to leverage the monolingual data to augment the original parallel dataset. In our case, we leverage both the source-side and target-side monolingual data to help the train• Filter out sentence pairs exceeding length ratio 1.5. • Filter out sentence pairs with less than 3 words or more than 100 word. 1 https://github.com/fxsjy/jieba https://taku910.github.io/mecab 3 https://github.com/neologd/ mecab-ipadic-neologd 4 https://github.com/saffsd/langid.py 2 5 146 https://github.com/pytorch/fairseq ing. Specifically, we train a baseline N"
2020.lrec-1.443,W18-6401,0,0.0525748,"Missing"
2020.lrec-1.443,2012.eamt-1.60,0,0.0283866,"llion parallel sentences. 4.1. Training NMT with JParaCrawl In this section, we trained NMT models with JParaCrawl and tested the models on several test sets to see how our corpus covers a broader range of domains. 4.1.1. Experimental Settings Data To see how our corpus covers a broader range of domains, we used four test sets: scientific paper excerpts (ASPEC, Nakazawa et al. (2016)), movie subtitles (JESC, Pryzant et al. (2017)), texts on Wikipedia articles related to Kyoto (KFTT, Neubig (2011)), and TED talks (tst2015, provided for the IWSLT 2017 evaluation campaign, Cettolo et al. (2017), Cettolo et al. (2012)). Table 2 shows the details of the test sets. During training, we used the ASPEC dev set as a validation set. We preprocessed the data with sentencepiece (Kudo and Richardson, 2018) to split the sentences into subwords. We set the vocabulary size to 32,000 and removed sentences whose length exceeded 250 subwords from the training data. Since JParaCrawl was NFKC-normalized, we also normalized the test sets. For comparison, we trained NMT models with domainspecific bitexts. Table 3 shows the number of sentences and words in the domain-specific training sets. Since the sentences in ASPEC are ord"
2020.lrec-1.443,W18-6415,0,0.0184478,"org/ 3603 ParaCrawl5 project, which is building parallel corpora by crawling the web. Their objective is to build parallel corpora to/from English for the 24 official languages of the European Union. They released an earlier version of the corpora, and this version is already quite large6 . This early release has already been used for previous WMT shared translation tasks for some language pairs (Bojar et al., 2018; Barrault et al., 2019), and task participants reported that ParaCrawl significantly improved the translation performance when it was used with a careful corpus cleaning technique (Junczys-Dowmunt, 2018). We extend this work to mine English-Japanese parallel sentences. 3. JParaCrawl To collect English-Japanese parallel sentences from the web, we took a similar approach to that used in the ParaCrawl project. Figure 1 shows how we mined parallel corpora from the web. First, we selected candidate domains to crawl that might contain English-Japanese parallel sentences from Common Crawl data (Section 3.1). Then, we crawled the candidate domains (Section 3.2). Finally, we aligned the parallel sentences from the crawled data and filtered out noisy sentence pairs (Section 3.3). 3.1. Crawling Domain S"
2020.lrec-1.443,P07-2045,0,0.00878145,"side in JParaCrawl corpus We crawled a large number of websites, but some were too small to mine for parallel sentences. Therefore, we filtered out those domains whose compressed archive size was less than 1 MB, and only 39,936 domains remained. To align parallel sentences, we used the Bitextor toolkit10 provided by the ParaCrawl project. Our experiment was based on version 7.0, and we fixed several components for Japanese sentences. To extract text from HTML files, we used extractontent11 , which was developed for Japanese text. We used split-sentences.perl12 contained in the Moses toolkit (Koehn et al., 2007) to split a text into sentences. We fixed the script to deal with Japanese end-of-sentence tokens. There are two primary approaches to align parallel text. One algorithm uses a bilingual lexicon to generate crude translations along with such other features as sentence length to find the best sentence pair (Varga et al., 2005). The other algorithm uses an external MT system to translate one language into the other and find a sentence pair that maximizes BLEU scores (Sennrich and Volk, 2011; Papineni et al., 2002). Since the latter approach needs more computational resources for external MT, we"
2020.lrec-1.443,2005.mtsummit-papers.11,0,0.248187,"onducted experiments to show 1 Currently the largest French-English parallel corpus is ParaCrawl v5, which contains 51.3 million training data. how effectively our corpus and pre-trained models worked with typical NMT training settings. These experimental settings and results are shown in Section 4. Finally, we conclude with a brief discussion of future work in Section 5. JParaCrawl and the NMT models pre-trained with it are freely available online2 for research purposes3 . 2. Related Work One typical kind of source for parallel texts is the documents of international organizations. Europarl (Koehn, 2005) is an example of the early success of creating a large parallel corpus by automatically aligning parallel texts from the proceedings of the European Parliament. The United Nations Parallel Corpus (Ziemski et al., 2016) is another similar example that was created from UN documents. These texts were translated by professionals, and aligning the documents is easy because they often have such metainformation as the identities of speakers, although their domains and language pairs are limited. Another important source of parallel texts is the web. Uszkoreit et al. (2010) proposed a large-scale dis"
2020.lrec-1.443,D18-2012,0,0.0538961,"us covers a broader range of domains. 4.1.1. Experimental Settings Data To see how our corpus covers a broader range of domains, we used four test sets: scientific paper excerpts (ASPEC, Nakazawa et al. (2016)), movie subtitles (JESC, Pryzant et al. (2017)), texts on Wikipedia articles related to Kyoto (KFTT, Neubig (2011)), and TED talks (tst2015, provided for the IWSLT 2017 evaluation campaign, Cettolo et al. (2017), Cettolo et al. (2012)). Table 2 shows the details of the test sets. During training, we used the ASPEC dev set as a validation set. We preprocessed the data with sentencepiece (Kudo and Richardson, 2018) to split the sentences into subwords. We set the vocabulary size to 32,000 and removed sentences whose length exceeded 250 subwords from the training data. Since JParaCrawl was NFKC-normalized, we also normalized the test sets. For comparison, we trained NMT models with domainspecific bitexts. Table 3 shows the number of sentences and words in the domain-specific training sets. Since the sentences in ASPEC are ordered by their alignment confidence scores, the former sentences tended to be clean and the latter might contain noisy sentence pairs. Based on previous work (Neubig, 2014), we used o"
2020.lrec-1.443,C96-2195,0,0.091788,"o primary approaches to align parallel text. One algorithm uses a bilingual lexicon to generate crude translations along with such other features as sentence length to find the best sentence pair (Varga et al., 2005). The other algorithm uses an external MT system to translate one language into the other and find a sentence pair that maximizes BLEU scores (Sennrich and Volk, 2011; Papineni et al., 2002). Since the latter approach needs more computational resources for external MT, we used the bilingual lexicon-based algorithm. We used the EDR EnglishJapanese dictionary as a bilingual lexicon (Miyoshi et al., 1996). Table 1 shows the number of collected parallel sentences and words. After the bitext alignment process, we mined over 27 million parallel sentences. However, these collected sentences contained many noisy pairs. Therefore, we filtered the corpus with Bicleaner13 (SánchezCartagena et al., 2018). The filtering model was trained with our in-house English-Japanese parallel corpora. After removing sentence pairs whose scores were lower than 0.5, we retained around 8.7 million sentences. As our initial corpus release, we open-filtered 8.7 million parallel sentences to the public. However, we still"
2020.lrec-1.443,L16-1350,0,0.159048,"lel corpus, machine translation, English, Japanese 1. Introduction Since current machine translation (MT) approaches are mainly data-driven, one key bottleneck has been the lack of parallel corpora. This problem continues with the recent neural machine translation (NMT) architecture. As the amount of training data increases, NMT performance improves (Sennrich and Zhang, 2019). Our goal is to create large parallel corpora to/from Japanese. In our first attempt, we focused on the English-Japanese language pair. Currently, ASPEC is the largest publicly available English-Japanese parallel corpus (Nakazawa et al., 2016), which contains 3.0 million sentences for training. Unfortunately, this is relatively small compared to such resource-rich language pairs as FrenchEnglish1 . Also, available domains remain limited. We address this problem, which hinders the progress of EnglishJapanese translation research, by crawling the web to mine for English-Japanese parallel sentences. Current NMT training requires a great deal of computational time, which complicates running experiments with few computational resources. We alleviate this problem by providing NMT models trained with our corpus. Since our web-based parall"
2020.lrec-1.443,W14-7002,0,0.0529333,"4.2. We also trained a model for a specific domain without fine-tuning in Section 4.3. Data ASPEC JESC KFTT IWSLT # sentences # words 1,812 2,000 1,160 1,194 39,573 13,617 22,063 20,367 Table 2: Number of sentences and words on English side in test sets Data ASPEC JESC KFTT IWSLT # sentences # words 3,008,500 2,797,388 440,288 223,108 68,929,413 19,339,040 9,737,715 3,877,868 Table 3: Number of sentences and words on English side in training sets. The original version of ASPEC contains 3.0 million sentences, but we used only the first 2.0 million sentences for training based on previous work (Neubig, 2014). with an existing corpus in Section 4.2. Finally, we trained a model for a specific domain with JParaCrawl and other corpora without fine-tuning in Section 4.3. In the following experiments, we used a filtered JParaCrawl corpus that contained 8.7 million parallel sentences. 4.1. Training NMT with JParaCrawl In this section, we trained NMT models with JParaCrawl and tested the models on several test sets to see how our corpus covers a broader range of domains. 4.1.1. Experimental Settings Data To see how our corpus covers a broader range of domains, we used four test sets: scientific paper exc"
2020.lrec-1.443,W18-6301,0,0.0530583,"ed dropout with a probability of 0.3 (Srivastava et al., 2014). We used big settings for ASPEC and JESC, base settings for KFTT, and small settings for IWSLT. As an optimizer, we used Adam with α = 0.001, β1 = 0.9, and β2 = 0.98. We used a root-square decay learning rate schedule with a linear warmup of 4000 steps (Vaswani et al., 2017). We clipped gradients to avoid exceeding their norm 1.0 to stabilize the training (Pascanu et al., 2013). For the base and small settings, each mini-batch contained about 5,000 tokens (subwords), and we accumulated the gradients of 64 mini-batches for updates (Ott et al., 2018). For the big settings, we set the mini-batch size to 2,000 tokens and accumulated 160 mini-batches for updates. We trained the model with 24,000 iterations, saved the model parameters every 200 iterations, and averaged the last eight models. To achieve maximum performance with the latest GPUs, we used mixed-precision training (Micikevicius et al., 2018). When decoding, we used a beam search with a size of six and length normalization by dividing the scores by their lengths. We slightly changed the settings for the in-domain baseline training because some of the above settings are inappropriat"
2020.lrec-1.443,N19-4009,0,0.032921,"aCrawl was NFKC-normalized, we also normalized the test sets. For comparison, we trained NMT models with domainspecific bitexts. Table 3 shows the number of sentences and words in the domain-specific training sets. Since the sentences in ASPEC are ordered by their alignment confidence scores, the former sentences tended to be clean and the latter might contain noisy sentence pairs. Based on previous work (Neubig, 2014), we used only the first 2.0 million sentences for training, although the original ASPEC corpus contained 3.0 million sentences. NMT Models We trained an NMT model with fairseq (Ott et al., 2019). Our model was based on Transformer (Vaswani et al., 2017). We trained three models for each direction by varying the hyper-parameters: small, base, and big settings. The base and big settings are based on Vaswani et al. (2017). For the base settings, we used an encoder/decoder with six layers. We set their embedding size to 512 and their feed-forward embedding size to 2048. We used eight attention heads for both the encoder and the decoder. For the big settings, we changed their embedding size to 1024 and their feed-forward embedding size to 4096. We also used 16 attention heads for both the"
2020.lrec-1.443,P02-1040,0,0.107238,"ped for Japanese text. We used split-sentences.perl12 contained in the Moses toolkit (Koehn et al., 2007) to split a text into sentences. We fixed the script to deal with Japanese end-of-sentence tokens. There are two primary approaches to align parallel text. One algorithm uses a bilingual lexicon to generate crude translations along with such other features as sentence length to find the best sentence pair (Varga et al., 2005). The other algorithm uses an external MT system to translate one language into the other and find a sentence pair that maximizes BLEU scores (Sennrich and Volk, 2011; Papineni et al., 2002). Since the latter approach needs more computational resources for external MT, we used the bilingual lexicon-based algorithm. We used the EDR EnglishJapanese dictionary as a bilingual lexicon (Miyoshi et al., 1996). Table 1 shows the number of collected parallel sentences and words. After the bitext alignment process, we mined over 27 million parallel sentences. However, these collected sentences contained many noisy pairs. Therefore, we filtered the corpus with Bicleaner13 (SánchezCartagena et al., 2018). The filtering model was trained with our in-house English-Japanese parallel corpora. Af"
2020.lrec-1.443,W18-6319,0,0.0240423,"decoding, we used a beam search with a size of six and length normalization by dividing the scores by their lengths. We slightly changed the settings for the in-domain baseline training because some of the above settings are inappropriate for smaller datasets. For IWSLT, we accumulated 16 mini-batches per update instead of 64. Since we confirmed that the model had already converged based on the validation loss, we stopped training at 20,000 iterations for all the in-domain baselines. Evaluation To evaluate the performance, we calculated the BLEU scores (Papineni et al., 2002) with sacreBLEU (Post, 2018). Since sacreBLEU does not internally tokenize Japanese text, we tokenized both the hypothesis and reference texts using MeCab14 with an IPA dictionary when evaluating the English-Japanese translations. 4.1.2. Experimental Results and Analysis Table 4 shows the BLEU scores of the in-domain and JParaCrawl NMT models (see in-domain and JParaCrawl columns). Since JParaCrawl is not supposed to contain a specific domain, its BLEU scores were lower than those of the model trained with the in-domain corpus. However, we expect that these in-domain models only focus on a specific domain and do not work"
2020.lrec-1.443,W18-6488,0,0.05153,"Missing"
2020.lrec-1.443,W11-4624,0,0.0822405,"tent11 , which was developed for Japanese text. We used split-sentences.perl12 contained in the Moses toolkit (Koehn et al., 2007) to split a text into sentences. We fixed the script to deal with Japanese end-of-sentence tokens. There are two primary approaches to align parallel text. One algorithm uses a bilingual lexicon to generate crude translations along with such other features as sentence length to find the best sentence pair (Varga et al., 2005). The other algorithm uses an external MT system to translate one language into the other and find a sentence pair that maximizes BLEU scores (Sennrich and Volk, 2011; Papineni et al., 2002). Since the latter approach needs more computational resources for external MT, we used the bilingual lexicon-based algorithm. We used the EDR EnglishJapanese dictionary as a bilingual lexicon (Miyoshi et al., 1996). Table 1 shows the number of collected parallel sentences and words. After the bitext alignment process, we mined over 27 million parallel sentences. However, these collected sentences contained many noisy pairs. Therefore, we filtered the corpus with Bicleaner13 (SánchezCartagena et al., 2018). The filtering model was trained with our in-house English-Japan"
2020.lrec-1.443,P19-1021,0,0.0195903,"ning time. Additionally, we trained the model with an in-domain dataset and JParaCrawl to show how we achieved the best performance with them. JParaCrawl and the pre-trained models are freely available online for research purposes. Keywords: parallel corpus, machine translation, English, Japanese 1. Introduction Since current machine translation (MT) approaches are mainly data-driven, one key bottleneck has been the lack of parallel corpora. This problem continues with the recent neural machine translation (NMT) architecture. As the amount of training data increases, NMT performance improves (Sennrich and Zhang, 2019). Our goal is to create large parallel corpora to/from Japanese. In our first attempt, we focused on the English-Japanese language pair. Currently, ASPEC is the largest publicly available English-Japanese parallel corpus (Nakazawa et al., 2016), which contains 3.0 million sentences for training. Unfortunately, this is relatively small compared to such resource-rich language pairs as FrenchEnglish1 . Also, available domains remain limited. We address this problem, which hinders the progress of EnglishJapanese translation research, by crawling the web to mine for English-Japanese parallel senten"
2020.lrec-1.443,P13-1135,0,0.0240656,"el corpus by automatically aligning parallel texts from the proceedings of the European Parliament. The United Nations Parallel Corpus (Ziemski et al., 2016) is another similar example that was created from UN documents. These texts were translated by professionals, and aligning the documents is easy because they often have such metainformation as the identities of speakers, although their domains and language pairs are limited. Another important source of parallel texts is the web. Uszkoreit et al. (2010) proposed a large-scale distributed system to mine parallel text from the web and books. Smith et al. (2013) proposed an algorithm that creates a parallel corpus by mining Common Crawl4 , which is a free web crawl archive. Schwenk et al. (2019) mined Wikipedia and created a parallel corpus of 1,620 language pairs. The web, which includes a broad range of domains and many language pairs, is rapidly and continually growing. Thus, it has huge potential as a source of parallel corpora, although identifying correct parallel sentences is difficult. Our work was inspired by the recent success of the 2 http://www.kecl.ntt.co.jp/icl/lirg/ jparacrawl/ 3 This research is based on JParaCrawl v1.0. During the re"
2020.lrec-1.443,C10-1124,0,0.0337521,"nternational organizations. Europarl (Koehn, 2005) is an example of the early success of creating a large parallel corpus by automatically aligning parallel texts from the proceedings of the European Parliament. The United Nations Parallel Corpus (Ziemski et al., 2016) is another similar example that was created from UN documents. These texts were translated by professionals, and aligning the documents is easy because they often have such metainformation as the identities of speakers, although their domains and language pairs are limited. Another important source of parallel texts is the web. Uszkoreit et al. (2010) proposed a large-scale distributed system to mine parallel text from the web and books. Smith et al. (2013) proposed an algorithm that creates a parallel corpus by mining Common Crawl4 , which is a free web crawl archive. Schwenk et al. (2019) mined Wikipedia and created a parallel corpus of 1,620 language pairs. The web, which includes a broad range of domains and many language pairs, is rapidly and continually growing. Thus, it has huge potential as a source of parallel corpora, although identifying correct parallel sentences is difficult. Our work was inspired by the recent success of the"
2020.lrec-1.443,L16-1561,0,0.0369002,"cal NMT training settings. These experimental settings and results are shown in Section 4. Finally, we conclude with a brief discussion of future work in Section 5. JParaCrawl and the NMT models pre-trained with it are freely available online2 for research purposes3 . 2. Related Work One typical kind of source for parallel texts is the documents of international organizations. Europarl (Koehn, 2005) is an example of the early success of creating a large parallel corpus by automatically aligning parallel texts from the proceedings of the European Parliament. The United Nations Parallel Corpus (Ziemski et al., 2016) is another similar example that was created from UN documents. These texts were translated by professionals, and aligning the documents is easy because they often have such metainformation as the identities of speakers, although their domains and language pairs are limited. Another important source of parallel texts is the web. Uszkoreit et al. (2010) proposed a large-scale distributed system to mine parallel text from the web and books. Smith et al. (2013) proposed an algorithm that creates a parallel corpus by mining Common Crawl4 , which is a free web crawl archive. Schwenk et al. (2019) m"
2020.lrec-1.457,D14-1179,0,0.00666171,"Missing"
2020.lrec-1.457,N06-2015,0,0.296565,"Missing"
2020.lrec-1.457,Q17-1024,0,0.0316991,"Missing"
2020.lrec-1.457,W19-6616,1,0.796364,"ores For the 1-to-1 model, the log probability (-8.48) of the correct sentence is smaller than the incorrect sentence (-7.44), which is categorized as a wrong answer. The translation output abandons the translation of the subordinate clause, which is a typical behavior of neural machine translation. 4. Conclusions We developed a contrastive test set for evaluating the power of discourse translation models for Japanese-to-English translation and found that the 2-to-2 discourse translation model’s improvement is mainly caused by better translation of Japanese zero pronouns. As was also shown in Kimura et al. (2019), Japanese zero pronouns can basically be effectively handled by context-aware neural machine translation. In future work, we want to build a test set for English-toJapanese discourse translation to focus on Japanese empathy and honoriﬁcs. We also want to develop a more sophisticated context-aware neural machine translation method that can appropriately handle coherence. Moreover, we want to expand our test set to a similar size of a previous work (M¨uller et al., 2018). They built a largescale test set from German-English bilingual texts using coreference resolution and word alignment tools."
2020.lrec-1.457,P07-2045,0,0.00766272,"96 18.03 18.56* 19.22 19.69* 12.92 13.38 27.80 28.08 21.89 21.76 17.87 18.07* Table 2: Translation accuracies of each dataset for 1-to-1 and 2-to-2 models by two translation methods. * indicates statistically signiﬁcant difference (p≤0.01). at The Japan News (formerly the Daily Yomiuri), which is the newspaper’s English edition. We purchased CD-ROMs published for research purposes 11 , and obtained document and sentence alignments using the algorithm of Utiyama and Isahara (2003). 3.2. Tools For preprocessing, the English sentences are tokenized and lowercased by the scripts in Moses toolkit (Koehn et al., 2007). Japanese sentences were normalized by NFKC (a unicode normalization form) and word segmented by MeCab12 with UniDic. Both Japanese and English sentences were further tokenized into subwords using byte pair encoding (Sennrich et al., 2016) with 32k shared merge operations. For neural machine translation, we used OpenNMT-lua13 for RNN encoder-decoder (Luong et al., 2015) and fairseq toolkit14 for the Transformer (Vaswani et al., 2017). We used default settings unless otherwise speciﬁed. The translation accuracy was measured by BLEU (Papineni et al., 2002) using multi-bleu.perl in the Moses too"
2020.lrec-1.457,C18-1050,0,0.0167301,"and Scherrer, 2017). This method, which is called 2-to-2, is known to be a reasonably strong baseline for discourse translation (Bawden et al., 2018; Voita et al., 2018). In this context, ordinary sentence-based translation is called 1-to-1. Some recent proposals have focused on multiple-encoders, where the input sentence and the context have different encoders. Bawden et al. (2018) used an RNN-based encoder-decoder and Voita et al. (2018) used the Transformer (Vaswani et al., 2017) for multipleencoders. Others used a cache-based model to exploit document-level information (Wang et al., 2017; Kuang et al., 2018; Tu et al., 2018). One problem in the study of discourse translation is that by only using automatic accuracy measures such as BLEU, we cannot distinguish between what is solved and not solved by the proposed method. Bawden et al. (2018) therefore proposed discourse test sets for English-to-French translation to evaluate the power of generating appropriate sentences based on the context information using the framework of a contrastive test set (Sennrich, 2017). For discourse phenomena, they targeted coreference and coherence, and manually created a test set based on real examples in bilingual"
2020.lrec-1.457,P14-2091,0,0.0194931,"Missing"
2020.lrec-1.457,L18-1275,0,0.0308548,"Missing"
2020.lrec-1.457,D15-1166,0,0.152214,"est sets for English-to-French discourse translation (Bawden et al., 2018), we needed different approaches to make the data because Japanese has zero pronouns and represents different senses in different characters. We improved the translation accuracy using context-aware neural machine translation, and the improvement mainly reﬂects the betterment of the translation of zero pronouns. Keywords: discourse, translation, test 1. Introduction Translation accuracy for sentences has been greatly improved by neural machine translation (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Vaswani et al., 2017). Although the accuracy of sentence-level machine translation is approaching human parity (Hassan et al., 2018), human evaluators prefer the translations of human translators over those of machine translation in document-level evaluations (Samuel L¨aubli, 2018). For further improvement, research using discourse information is gaining attention (Tiedemann and Scherrer, 2017; Bawden et al., 2018; Voita et al., 2018; Maruf and Haffari, 2018; Miculicich et al., 2018; Voita et al., 2019a). The easiest method to use discourse information is to concatenate both the previous and"
2020.lrec-1.457,P18-1118,0,0.056101,"racy for sentences has been greatly improved by neural machine translation (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Vaswani et al., 2017). Although the accuracy of sentence-level machine translation is approaching human parity (Hassan et al., 2018), human evaluators prefer the translations of human translators over those of machine translation in document-level evaluations (Samuel L¨aubli, 2018). For further improvement, research using discourse information is gaining attention (Tiedemann and Scherrer, 2017; Bawden et al., 2018; Voita et al., 2018; Maruf and Haffari, 2018; Miculicich et al., 2018; Voita et al., 2019a). The easiest method to use discourse information is to concatenate both the previous and current sentences with a special token &lt;CONCAT&gt; as a separator and translate them using an ordinary sentence-based translation system (Tiedemann and Scherrer, 2017). This method, which is called 2-to-2, is known to be a reasonably strong baseline for discourse translation (Bawden et al., 2018; Voita et al., 2018). In this context, ordinary sentence-based translation is called 1-to-1. Some recent proposals have focused on multiple-encoders, where the input sen"
2020.lrec-1.457,D18-1325,0,0.0374101,"en greatly improved by neural machine translation (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Vaswani et al., 2017). Although the accuracy of sentence-level machine translation is approaching human parity (Hassan et al., 2018), human evaluators prefer the translations of human translators over those of machine translation in document-level evaluations (Samuel L¨aubli, 2018). For further improvement, research using discourse information is gaining attention (Tiedemann and Scherrer, 2017; Bawden et al., 2018; Voita et al., 2018; Maruf and Haffari, 2018; Miculicich et al., 2018; Voita et al., 2019a). The easiest method to use discourse information is to concatenate both the previous and current sentences with a special token &lt;CONCAT&gt; as a separator and translate them using an ordinary sentence-based translation system (Tiedemann and Scherrer, 2017). This method, which is called 2-to-2, is known to be a reasonably strong baseline for discourse translation (Bawden et al., 2018; Voita et al., 2018). In this context, ordinary sentence-based translation is called 1-to-1. Some recent proposals have focused on multiple-encoders, where the input sentence and the context hav"
2020.lrec-1.457,W18-6307,0,0.104996,"Missing"
2020.lrec-1.457,P02-1040,0,0.106976,"Missing"
2020.lrec-1.457,L16-1144,0,0.0390114,"Missing"
2020.lrec-1.457,D18-1512,0,0.064336,"Missing"
2020.lrec-1.457,P16-1162,0,0.0134638,"1). at The Japan News (formerly the Daily Yomiuri), which is the newspaper’s English edition. We purchased CD-ROMs published for research purposes 11 , and obtained document and sentence alignments using the algorithm of Utiyama and Isahara (2003). 3.2. Tools For preprocessing, the English sentences are tokenized and lowercased by the scripts in Moses toolkit (Koehn et al., 2007). Japanese sentences were normalized by NFKC (a unicode normalization form) and word segmented by MeCab12 with UniDic. Both Japanese and English sentences were further tokenized into subwords using byte pair encoding (Sennrich et al., 2016) with 32k shared merge operations. For neural machine translation, we used OpenNMT-lua13 for RNN encoder-decoder (Luong et al., 2015) and fairseq toolkit14 for the Transformer (Vaswani et al., 2017). We used default settings unless otherwise speciﬁed. The translation accuracy was measured by BLEU (Papineni et al., 2002) using multi-bleu.perl in the Moses toolkit. 3.3. Context Boundaries Unlike spoken language datasets, written language datasets have obvious structures including documents, sections, and paragraphs, all of which can be candidates for context boundaries. To the best of our knowle"
2020.lrec-1.457,E17-2060,0,0.0563721,"swani et al., 2017) for multipleencoders. Others used a cache-based model to exploit document-level information (Wang et al., 2017; Kuang et al., 2018; Tu et al., 2018). One problem in the study of discourse translation is that by only using automatic accuracy measures such as BLEU, we cannot distinguish between what is solved and not solved by the proposed method. Bawden et al. (2018) therefore proposed discourse test sets for English-to-French translation to evaluate the power of generating appropriate sentences based on the context information using the framework of a contrastive test set (Sennrich, 2017). For discourse phenomena, they targeted coreference and coherence, and manually created a test set based on real examples in bilingual corpora. Voita et al. (2019b) made a contrastive test set for English-to-Russian discourse translation. To make Japanese-to-English discourse translation test sets, we also targeted on coreference and coherence. We ﬁrst looked for examples in bilingual corpora. However, since we found that this approach is inefﬁcient, we chose a different approach using linguistically annotated corpora. The following are this paper’s contributions: • we created a novel test se"
2020.lrec-1.457,W12-4213,1,0.874935,"Missing"
2020.lrec-1.457,W16-4615,1,0.875651,"Missing"
2020.lrec-1.457,W17-4811,0,0.0466245,"Keywords: discourse, translation, test 1. Introduction Translation accuracy for sentences has been greatly improved by neural machine translation (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Vaswani et al., 2017). Although the accuracy of sentence-level machine translation is approaching human parity (Hassan et al., 2018), human evaluators prefer the translations of human translators over those of machine translation in document-level evaluations (Samuel L¨aubli, 2018). For further improvement, research using discourse information is gaining attention (Tiedemann and Scherrer, 2017; Bawden et al., 2018; Voita et al., 2018; Maruf and Haffari, 2018; Miculicich et al., 2018; Voita et al., 2019a). The easiest method to use discourse information is to concatenate both the previous and current sentences with a special token &lt;CONCAT&gt; as a separator and translate them using an ordinary sentence-based translation system (Tiedemann and Scherrer, 2017). This method, which is called 2-to-2, is known to be a reasonably strong baseline for discourse translation (Bawden et al., 2018; Voita et al., 2018). In this context, ordinary sentence-based translation is called 1-to-1. Some recen"
2020.lrec-1.457,tiedemann-2012-parallel,0,0.0178619,"experiments. They include spoken language datasets of about 2M sentences and written language datasets of about 1M sentences. IWSLT-2017 (Cettolo et al., 2017) is a dataset for JapaneseEnglish Tasks of the International Workshop on Spoken Language Translation and consists of the transcriptions of TED Talks and their translations. OpenSubtitles2018 (Lison et al., 2018) is a collection of movie subtitles and their translations. Global Voices is a multilingual corpus created from Global Voices websites that translate social media and blogs (Prokopidis et al., 2016). Both are available from OPUS (Tiedemann, 2012). Hiragana Times Books are a collection of bilingual books in Japan and the Hiragana Times is a monthly bilingual magazine for Japanese learners.6 Both can be purchased at the publishing company. The English-Japanese Translation Alignment Data (Utiyama and Takahashi, ) (NICT align) is a collection of publicly available books from such archives as Aozora Bunko7 and Project Gutenberg8 . Their document and sentence alignments were provided by NICT. The Japanese-English Bilingual Corpus of Wikipedia’s Kyoto Articles9 (Wikipedia Kyoto) is a collection of Japanese Wikipedia articles on Kyoto and the"
2020.lrec-1.457,Q18-1029,0,0.0449237,"This method, which is called 2-to-2, is known to be a reasonably strong baseline for discourse translation (Bawden et al., 2018; Voita et al., 2018). In this context, ordinary sentence-based translation is called 1-to-1. Some recent proposals have focused on multiple-encoders, where the input sentence and the context have different encoders. Bawden et al. (2018) used an RNN-based encoder-decoder and Voita et al. (2018) used the Transformer (Vaswani et al., 2017) for multipleencoders. Others used a cache-based model to exploit document-level information (Wang et al., 2017; Kuang et al., 2018; Tu et al., 2018). One problem in the study of discourse translation is that by only using automatic accuracy measures such as BLEU, we cannot distinguish between what is solved and not solved by the proposed method. Bawden et al. (2018) therefore proposed discourse test sets for English-to-French translation to evaluate the power of generating appropriate sentences based on the context information using the framework of a contrastive test set (Sennrich, 2017). For discourse phenomena, they targeted coreference and coherence, and manually created a test set based on real examples in bilingual corpora. Voita et"
2020.lrec-1.457,P03-1010,0,0.0785205,"0.66 10.80 12.91 13.23 13.23 13.32 9.36 9.64 23.09 23.12 14.53 15.26* 12.77 13.02* Transformer 1-to-1 2-to-2 17.72 17.67 15.07 15.48 14.86 14.96 18.03 18.56* 19.22 19.69* 12.92 13.38 27.80 28.08 21.89 21.76 17.87 18.07* Table 2: Translation accuracies of each dataset for 1-to-1 and 2-to-2 models by two translation methods. * indicates statistically signiﬁcant difference (p≤0.01). at The Japan News (formerly the Daily Yomiuri), which is the newspaper’s English edition. We purchased CD-ROMs published for research purposes 11 , and obtained document and sentence alignments using the algorithm of Utiyama and Isahara (2003). 3.2. Tools For preprocessing, the English sentences are tokenized and lowercased by the scripts in Moses toolkit (Koehn et al., 2007). Japanese sentences were normalized by NFKC (a unicode normalization form) and word segmented by MeCab12 with UniDic. Both Japanese and English sentences were further tokenized into subwords using byte pair encoding (Sennrich et al., 2016) with 32k shared merge operations. For neural machine translation, we used OpenNMT-lua13 for RNN encoder-decoder (Luong et al., 2015) and fairseq toolkit14 for the Transformer (Vaswani et al., 2017). We used default settings"
2020.lrec-1.457,P18-1117,0,0.179029,"ion Translation accuracy for sentences has been greatly improved by neural machine translation (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Vaswani et al., 2017). Although the accuracy of sentence-level machine translation is approaching human parity (Hassan et al., 2018), human evaluators prefer the translations of human translators over those of machine translation in document-level evaluations (Samuel L¨aubli, 2018). For further improvement, research using discourse information is gaining attention (Tiedemann and Scherrer, 2017; Bawden et al., 2018; Voita et al., 2018; Maruf and Haffari, 2018; Miculicich et al., 2018; Voita et al., 2019a). The easiest method to use discourse information is to concatenate both the previous and current sentences with a special token &lt;CONCAT&gt; as a separator and translate them using an ordinary sentence-based translation system (Tiedemann and Scherrer, 2017). This method, which is called 2-to-2, is known to be a reasonably strong baseline for discourse translation (Bawden et al., 2018; Voita et al., 2018). In this context, ordinary sentence-based translation is called 1-to-1. Some recent proposals have focused on multiple-enco"
2020.lrec-1.457,D19-1081,0,0.331148,"ural machine translation (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Vaswani et al., 2017). Although the accuracy of sentence-level machine translation is approaching human parity (Hassan et al., 2018), human evaluators prefer the translations of human translators over those of machine translation in document-level evaluations (Samuel L¨aubli, 2018). For further improvement, research using discourse information is gaining attention (Tiedemann and Scherrer, 2017; Bawden et al., 2018; Voita et al., 2018; Maruf and Haffari, 2018; Miculicich et al., 2018; Voita et al., 2019a). The easiest method to use discourse information is to concatenate both the previous and current sentences with a special token &lt;CONCAT&gt; as a separator and translate them using an ordinary sentence-based translation system (Tiedemann and Scherrer, 2017). This method, which is called 2-to-2, is known to be a reasonably strong baseline for discourse translation (Bawden et al., 2018; Voita et al., 2018). In this context, ordinary sentence-based translation is called 1-to-1. Some recent proposals have focused on multiple-encoders, where the input sentence and the context have different encoders"
2020.lrec-1.457,P19-1116,0,0.444493,"ural machine translation (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Vaswani et al., 2017). Although the accuracy of sentence-level machine translation is approaching human parity (Hassan et al., 2018), human evaluators prefer the translations of human translators over those of machine translation in document-level evaluations (Samuel L¨aubli, 2018). For further improvement, research using discourse information is gaining attention (Tiedemann and Scherrer, 2017; Bawden et al., 2018; Voita et al., 2018; Maruf and Haffari, 2018; Miculicich et al., 2018; Voita et al., 2019a). The easiest method to use discourse information is to concatenate both the previous and current sentences with a special token &lt;CONCAT&gt; as a separator and translate them using an ordinary sentence-based translation system (Tiedemann and Scherrer, 2017). This method, which is called 2-to-2, is known to be a reasonably strong baseline for discourse translation (Bawden et al., 2018; Voita et al., 2018). In this context, ordinary sentence-based translation is called 1-to-1. Some recent proposals have focused on multiple-encoders, where the input sentence and the context have different encoders"
2020.lrec-1.457,N16-1113,0,0.0270109,"Missing"
2020.lrec-1.457,D17-1301,0,0.0430898,"system (Tiedemann and Scherrer, 2017). This method, which is called 2-to-2, is known to be a reasonably strong baseline for discourse translation (Bawden et al., 2018; Voita et al., 2018). In this context, ordinary sentence-based translation is called 1-to-1. Some recent proposals have focused on multiple-encoders, where the input sentence and the context have different encoders. Bawden et al. (2018) used an RNN-based encoder-decoder and Voita et al. (2018) used the Transformer (Vaswani et al., 2017) for multipleencoders. Others used a cache-based model to exploit document-level information (Wang et al., 2017; Kuang et al., 2018; Tu et al., 2018). One problem in the study of discourse translation is that by only using automatic accuracy measures such as BLEU, we cannot distinguish between what is solved and not solved by the proposed method. Bawden et al. (2018) therefore proposed discourse test sets for English-to-French translation to evaluate the power of generating appropriate sentences based on the context information using the framework of a contrastive test set (Sennrich, 2017). For discourse phenomena, they targeted coreference and coherence, and manually created a test set based on real e"
2020.lrec-1.457,D18-1333,0,0.0321588,"Missing"
2020.wmt-1.1,2020.nlpcovid19-2.5,1,0.796341,"tails of the evaluation. 4.1.1 Covid Test Suite TICO-19 The TICO-19 test suite was developed to evaluate how well can MT systems handle the newlyemerged topic of COVID-19. Accurate automatic translation can play an important role in facilitating communication in order to protect at-risk populations and combat the infodemic of misinformation, as described by the World Health Organization. The test suite has no corresponding paper so its authors provided an analysis of the outcomes directly here. The submitted systems were evaluated using the test set from the recently-released TICO-19 dataset (Anastasopoulos et al., 2020). The dataset provides manually created translations of COVID19 related data. The test set consists of PubMed articles (678 sentences from 5 scientific articles), patient-medical professional conversations (104 sentences), as well as related Wikipedia articles (411 sentences), announcements (98 sentences from Wikisource), and news items (67 sentences from Wikinews), for a total of 2100 sentences. Table 15 outlines the BLEU scores by each submitted system in the English-to-X directions, also breaking down the results per domain. The analysis shows that some systems are significantly more prepar"
2020.wmt-1.1,2020.wmt-1.6,0,0.0647231,"AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) SJTU-NICT (Li et al., 2020) Samsung Research Poland (Krubi´nski et al., 2020) TALP UPC (Escolano et al., 2020) Tencent Translation (Wu et al., 2020b) NLP Lab at Tsinghua Universit"
2020.wmt-1.1,2020.wmt-1.38,0,0.0746945,"Missing"
2020.wmt-1.1,2020.wmt-1.54,1,0.802974,"Missing"
2020.wmt-1.1,W07-0718,1,0.671054,"Missing"
2020.wmt-1.1,W08-0309,1,0.762341,"Missing"
2020.wmt-1.1,W12-3102,1,0.500805,"Missing"
2020.wmt-1.1,2020.lrec-1.461,0,0.0795779,"Missing"
2020.wmt-1.1,2012.eamt-1.60,0,0.124643,"tted systems, for those where the authors provided such details. The training corpus for Inuktitut↔English is the recently released Nunavut Hansard Inuktitut– English Parallel Corpus 3.0 (Joanis et al., 2020). For the Japanese↔English tasks, we added several freely available parallel corpora to the training data. It includes JParaCrawl v2.0 (Morishita et al., 2020), a large web-based parallel corpus, Japanese-English Subtitle Corpus (JESC) (Pryzant et al., 2017), the Kyoto Free Translation Task (KFTT) corpus (Neubig, 2011), constructed from the Kyoto-related Wikipedia articles, and TED Talks (Cettolo et al., 2012). The monolingual data we provided was similar to last year’s, with a 2019 news crawl added to all the news corpora. In addition, we provided versions of the news corpora for Czech, English and German, with both the document and paragraph structure retained. In other words, we did not apply sentence splitting to these corpora, and we retained the document boundaries and text ordering of the originals. Training, development, and test data for Pashto↔English and Khmer↔English are shared with the Parallel Corpus Filtering Shared Task (Koehn et al., 2020). The training data mostly comes from OPUS"
2020.wmt-1.1,2020.wmt-1.3,0,0.0731913,"on Machine Translation (WMT20)1 was held online with EMNLP 2020 and hosted a number of shared tasks on various aspects of machine translation. This conference built on 14 previous editions of WMT as workshops and conferences (Koehn and Monz, 2006; CallisonBurch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016, 2017, 2018; Barrault et al., 2019). This year we conducted several official tasks. We report in this paper on the news and similar translation tasks. Additional shared tasks are described in separate papers in these proceedings: • automatic post-editing (Chatterjee et al., 2020) • biomedical translation (Bawden et al., 2020b) • chat translation (Farajian et al., 2020) • lifelong learning (Barrault et al., 2020) 1 Makoto Morishita NTT Santanu Pal WIPRO AI Abstract 1 Philipp Koehn JHU http://www.statmt.org/wmt20/ 1 Proceedings of the 5th Conference on Machine Translation (WMT), pages 1–55 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics as “direct assessment”) that we explored in the previous years with convincing results in terms of the trade-off between annotation effort and reliable distinctions between systems. The primary objectives o"
2020.wmt-1.1,2020.wmt-1.8,0,0.0898111,"D D I D I -NLP DONG - NMT ENMT E T RANSLATION FACEBOOK AI G RONINGEN GTCOM H ELSINKI NLP H UAWEI TSC IIE M ICROSOFT STC I NDIA NICT-K YOTO NICT-RUI N IU T RANS NRC OPPO PROMT SJTU-NICT SRPOL TALP UPC T ENCENT T RANSLATION THUNLP T ILDE T OHOKU -AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Know"
2020.wmt-1.1,2009.freeopmt-1.3,0,0.088081,"ve (CONTRASTIVE) or primary (PRIMARY), and the BLEU, RIBES and TER results. The scores are sorted by BLEU. In general, primary systems tend to be better than contrastive systems, as expected, but there are some exceptions. This year we recived major number of participants for the case of Indo-Aryan language group NUST-FJWU NUST-FJWU system is an extension of state-of-the-art Transformer model with hierarchical attention networks to incorporate contextual information. During training the model used back-translation. Prompsit This team is participating with a rulebased system based on Apertium (Forcada et al., 2009-11). Apertium is a free/open-source platform for developing rule-based machine translation systems and language technology that was first released in 2005. Apertium is hosted in Github where both language data and code are licensed under the GNU GPL. It is a research and business platform with a very active community that loves small languages. Language pairs are at a very different level of development and output quality in the platform, depending on two main variables: how much funded or in-kind effort has 32 5.4 i.e. Hindi–Marathi (in both directions). We received 22 submissions from 14 te"
2020.wmt-1.1,2020.wmt-1.80,0,0.0933589,"airs, to be evaluated on test sets consisting mainly of news stories. The task was also opened up to additional test suites to probe specific aspects of translation. In the similar language translation task, participants built machine translation systems for translating between closely related pairs of languages. Chi-kiu Lo NRC Masaaki Nagata NTT Marcos Zampieri Rochester Institute of Technology metrics (Mathur et al., 2020) parallel corpus filtering (Koehn et al., 2020) quality estimation (Specia et al., 2020a) robustness (Specia et al., 2020b) unsupervised and very low-resource translation (Fraser, 2020) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (“constrained” condition). We included 22 translation directions this year, with translation between English and each of Chinese, Czech, German and Russian, as well as French to and from German being repeated from last year, and English to and from Inuktitut, Japanese, Polish and Tamil being new for this year. Furthermore, English to and from Khmer and Pashto were included, using the same test sets as in the corpus filtering task. Th"
2020.wmt-1.1,W19-5204,0,0.0543621,"Missing"
2020.wmt-1.1,2020.emnlp-main.5,0,0.0410594,"luation of out-ofEnglish translations, HITs were generated using the same method as described for the SR+DC evaluation of into-English translations in Section 3.2.1 with minor modifications. Source-based DA allows to include human references in the evaluation as another system to provide an estimate of human performance. Human references were added to the pull of system outputs prior to sampling documents for tasks generation. If multiple references are available, which is the case for English→German (3 alternative reference translations, including 1 generated using the paraphrasing method of Freitag et al. (2020)) and English→Chinese (2 translations), each reference is assessed individually. Since the annotations are made by researchers and professional translators who ensure a betTable 11: Amount of data collected in the WMT20 manual document- and segment-level evaluation campaigns for bilingual/source-based evaluation out of English and nonEnglish pairs. et al., 2020; Laubli et al., 2020). It differs from SR+DC DA introduced in WMT19 (Bojar et al., 2019), and still used in into-English human evaluation this year, where a single segment from a document is provided on a screen at a time, followed by s"
2020.wmt-1.1,W18-3931,1,0.874637,"ese improvements to increased capacity (which allows increased sensitivity to long-range relations) of the models. 5 Similar Language Translation Most shared tasks at WMT (e.g. News, Biomedical) have historically dealt with translating texts from and to English. In recent years, we observed a growing interest in training systems to translate between languages other than English. This includes a number of papers applying MT to translate between pairs of closely-related languages, national language varieties, and dialects 27 of the same language (Zhang, 1998; Marujo et al., 2011; Hassani, 2017; Costa-jussà et al., 2018; Popovi´c et al., 2020). To address this topic, the first Similar Language Translation (SLT) shared task at WMT 2019 has been organized. It featured data from three pairs of closely-related languages from different language families: Spanish - Portuguese (Romance languages), Czech - Polish (Slavic languages), and Hindi - Nepali (IndoAryan languages). Following the success of the first SLT shared task at WMT 2019 and the interest of the community in this topic, we organize, for the second time at WMT, this shared task to evaluate the performance of state-of-the-art translation systems on trans"
2020.wmt-1.1,2020.wmt-1.18,0,0.0913945,"2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) SJTU-NICT (Li et al., 2020) Samsung Research Poland (Krubi´nski et al., 2020) TALP UPC (Escolano et al., 2020) Tencent Translation (Wu et al., 2020b) NLP Lab at Tsinghua University (no associated paper) Tilde (Krišlauks and Pinnis, 2020) Tohoku-AIP-NTT (Kiyono et al., 2020) Ubiqus (Hernandez and Nguyen, 2020) University of Edinburgh (Bawden et al., 2020a; Germann, 2020) University of Edinburgh and Charles University (Germann et al., 2020) Université du Québec à Montréal (no associated paper) ByteDance AI Lab (Wu et al., 2020a) WeChat (Meng et al., 2020) Baseline System from Biomedical Task (Bawden et al., 2020b) American University of Beirut (no associated paper) Zoho Corporation (no associated paper) Table 6: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion"
2020.wmt-1.1,2020.wmt-1.43,0,0.0835067,"Missing"
2020.wmt-1.1,2020.wmt-1.9,0,0.0939415,"Missing"
2020.wmt-1.1,2020.wmt-1.19,0,0.0674131,"Missing"
2020.wmt-1.1,2009.mtsummit-btm.6,0,0.103443,"Missing"
2020.wmt-1.1,W13-2305,1,0.929934,"work which can be well applied to different translation. directions. Techniques used in the submitted systems include optional multilingual pre-training (mRASP) for low resource languages, very deep Transformer or dynamic convolution models up to 50 encoder layers, iterative backtranslation, knowledge distillation, model ensemble and development set fine-tuning. The key ingredient of the process seems the strong focus on diversification of the (synthetic) training data, using multiple scalings of the Transformer model 3.1 Direct Assessment Since running a comparison of direct assessments (DA, Graham et al., 2013, 2014, 2016) and relative ranking in 2016 (Bojar et al., 2016) and verifying a high correlation of system rankings for the two methods, as well as the advantages of DA, such as quality controlled crowd-sourcing and linear growth relative to numbers of submissions, we have employed DA as the primary mechanism for evaluating systems. With DA human evaluation, 15 human assessors are asked to rate a given translation by how adequately it expresses the meaning of the corresponding reference translation or source language input on an analogue scale, which corresponds to an underlying absolute 0–100"
2020.wmt-1.1,E14-1047,1,0.888167,"Missing"
2020.wmt-1.1,2020.lrec-1.312,1,0.804196,"A screenshot of OCELoT is shown in Figure 5. For presentation of the results, systems are treated as either constrained or unconstrained, depending on whether their models were trained only on the provided data. Since we do not know how they were built, the online systems are treated as unconstrained during the automatic and human evaluations. In the rest of this section, we provide brief details of the submitted systems, for those where the authors provided such details. The training corpus for Inuktitut↔English is the recently released Nunavut Hansard Inuktitut– English Parallel Corpus 3.0 (Joanis et al., 2020). For the Japanese↔English tasks, we added several freely available parallel corpora to the training data. It includes JParaCrawl v2.0 (Morishita et al., 2020), a large web-based parallel corpus, Japanese-English Subtitle Corpus (JESC) (Pryzant et al., 2017), the Kyoto Free Translation Task (KFTT) corpus (Neubig, 2011), constructed from the Kyoto-related Wikipedia articles, and TED Talks (Cettolo et al., 2012). The monolingual data we provided was similar to last year’s, with a 2019 news crawl added to all the news corpora. In addition, we provided versions of the news corpora for Czech, Engli"
2020.wmt-1.1,2020.emnlp-main.6,1,0.839606,"shown in Table 3, where the first and second are simple merges or splits, whereas the third is a rare case of more complex reordering. We leave a detailed analysis of the translators’ treatment of paragraph-split data for future work. development set is provided, it is a mixture of both “source-original” and “target-original” texts, in order to maximise its size, although the original language is always marked in the sgm file, except for Inuktitut↔English. The consequences of directionality in test sets has been discussed recently in the literature (Freitag et al., 2019; Laubli et al., 2020; Graham et al., 2020), and the conclusion is that it can have an effect on detrimental effect on the accuracy of system evaluation. We use “source-original” parallel sentences wherever possible, on the basis that it is the more realistic scenario for practical MT usage. Exception: the test sets for the two Inuktitut↔English translation directions contain the same data, without regard to original direction. For most news text in the test and development sets, English was the original language and Inuktitut the translation, while the parliamentary data mixes the two directions. The origins of the news test documents"
2020.wmt-1.1,2020.wmt-1.11,0,0.0940191,"N GTCOM H ELSINKI NLP H UAWEI TSC IIE M ICROSOFT STC I NDIA NICT-K YOTO NICT-RUI N IU T RANS NRC OPPO PROMT SJTU-NICT SRPOL TALP UPC T ENCENT T RANSLATION THUNLP T ILDE T OHOKU -AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) S"
2020.wmt-1.1,D19-1632,1,0.881933,"ent and paragraph structure retained. In other words, we did not apply sentence splitting to these corpora, and we retained the document boundaries and text ordering of the originals. Training, development, and test data for Pashto↔English and Khmer↔English are shared with the Parallel Corpus Filtering Shared Task (Koehn et al., 2020). The training data mostly comes from OPUS (software localization, Tatoeba, Global Voices), the Bible, and specialprepared corpora from TED Talks and the Jehova Witness web site (JW300). The development and test sets were created as part of the Flores initiative (Guzmán et al., 2019) by professional translation of Wikipedia content with careful vetting of the translations. Please refer the to the Parallel Corpus Filtering Shared Task overview paper for details on these corpora. 2.3.1 AFRL (Gwinnup and Anderson, 2020) AFRL - SYSCOMB 20 is a system combination consisting of two Marian transformer ensembles, one OpenNMT transformer system and a Moses phrase-based system. AFRL - FINETUNE is an OpenNMT transformer system fine-tuned on newstest2014-2017. 2.3.2 (Xv, 2020) ARIEL XV is a Transformer base model trained with the Sockeye sequence modeling toolkit usSome statistics ab"
2020.wmt-1.1,2020.wmt-1.12,1,0.754946,"Missing"
2020.wmt-1.1,2020.wmt-1.13,0,0.0737827,"2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) SJTU-NICT (Li et al., 2020) Samsung Research Poland (Krubi´nski et al., 2020) TALP UPC (Escolano et al., 2020) Tencent Translation (Wu et al., 2020b) NLP Lab at Tsinghua University (no associated paper) Tilde (Krišlauks and Pinnis, 2020) Tohoku-AIP-NTT (Kiyono et al., 2020) Ubiqus (Hernandez and Nguyen, 2020) University of Edinburgh (Bawden et al., 2020a; Germann, 2020) University of Edinburgh and Charles University (Germann et al., 2020) Université du Québec à Montréal (no associated paper) ByteDance AI Lab (Wu et al., 2020a) WeChat (Meng et al"
2020.wmt-1.1,2020.wmt-1.20,0,0.057602,"Missing"
2020.wmt-1.1,2020.wmt-1.14,1,0.820019,"set, and the origlang tag indicates the original source language. 9 Team AFRL ARIEL XV CUNI DCU D EEP M IND D I D I -NLP DONG - NMT ENMT E T RANSLATION FACEBOOK AI G RONINGEN GTCOM H ELSINKI NLP H UAWEI TSC IIE M ICROSOFT STC I NDIA NICT-K YOTO NICT-RUI N IU T RANS NRC OPPO PROMT SJTU-NICT SRPOL TALP UPC T ENCENT T RANSLATION THUNLP T ILDE T OHOKU -AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Mari"
2020.wmt-1.1,2020.wmt-1.39,1,0.812433,"kables was collected in the first phase of the annotation, which amounted to 4k assessments across the systems. The second annotation phase with 6.5k assessments compared markable translations, always checking outputs of all the 13 competing MT systems but still considering the document-level context of each of them. Among other things, the observations indicate that the better the system, the lower the variance in manual scores. Markables annotation then confirms that frequent errors like bad translation of a term need not be the most severe and conversely, 4.1.3 Gender Coreference and Bias (Kocmi et al., 2020) The test suite by Kocmi et al. (2020) focuses on the gender bias in professions (e.g. physician, teacher, secretary) for the translation from English into Czech, German, Polish and Russian. These nouns are ambiguous with respect to gender in English but exhibit gender in the examined target languages. The test suite is based on the fact that a pronoun referring to the ambiguous noun can reveal the gender of the noun in the English source sentence. Once disambiguated, the gender needs to be preserved in translation. To correctly translate the given noun, the translation system thus has to corr"
2020.wmt-1.1,2020.wmt-1.53,0,0.089538,"Missing"
2020.wmt-1.1,2020.wmt-1.78,1,0.815194,"rence on Machine Translation (WMT) 2020. In the news task, participants were asked to build machine translation systems for any of 11 language pairs, to be evaluated on test sets consisting mainly of news stories. The task was also opened up to additional test suites to probe specific aspects of translation. In the similar language translation task, participants built machine translation systems for translating between closely related pairs of languages. Chi-kiu Lo NRC Masaaki Nagata NTT Marcos Zampieri Rochester Institute of Technology metrics (Mathur et al., 2020) parallel corpus filtering (Koehn et al., 2020) quality estimation (Specia et al., 2020a) robustness (Specia et al., 2020b) unsupervised and very low-resource translation (Fraser, 2020) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (“constrained” condition). We included 22 translation directions this year, with translation between English and each of Chinese, Czech, German and Russian, as well as French to and from German being repeated from last year, and English to and from Inuktitut, Japanese, Polish and Tamil being new fo"
2020.wmt-1.1,W17-1208,0,0.0524248,"ttribute all these improvements to increased capacity (which allows increased sensitivity to long-range relations) of the models. 5 Similar Language Translation Most shared tasks at WMT (e.g. News, Biomedical) have historically dealt with translating texts from and to English. In recent years, we observed a growing interest in training systems to translate between languages other than English. This includes a number of papers applying MT to translate between pairs of closely-related languages, national language varieties, and dialects 27 of the same language (Zhang, 1998; Marujo et al., 2011; Hassani, 2017; Costa-jussà et al., 2018; Popovi´c et al., 2020). To address this topic, the first Similar Language Translation (SLT) shared task at WMT 2019 has been organized. It featured data from three pairs of closely-related languages from different language families: Spanish - Portuguese (Romance languages), Czech - Polish (Slavic languages), and Hindi - Nepali (IndoAryan languages). Following the success of the first SLT shared task at WMT 2019 and the interest of the community in this topic, we organize, for the second time at WMT, this shared task to evaluate the performance of state-of-the-art tr"
2020.wmt-1.1,2020.wmt-1.21,0,0.0791885,"Missing"
2020.wmt-1.1,2020.wmt-1.23,0,0.0607352,"020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) SJTU-NICT (Li et al., 2020) Samsung Research Poland (Krubi´nski et al., 2020) TALP UPC (Escolano et al., 2020) Tencent Translation (Wu et al., 2020b) NLP Lab at Tsinghua University (no associated paper) Tilde (Krišlauks and Pinnis, 2020) Tohoku-AIP-NTT (Kiyono et al., 2020) Ubiqus (Hernandez and Nguyen, 2020) University of Edinburgh (Bawden et al., 2020a; Germann, 2020) University of Edinburgh and Charles University (Germann et al., 2"
2020.wmt-1.1,2020.wmt-1.77,1,0.84299,"slation task, both organised alongside the Conference on Machine Translation (WMT) 2020. In the news task, participants were asked to build machine translation systems for any of 11 language pairs, to be evaluated on test sets consisting mainly of news stories. The task was also opened up to additional test suites to probe specific aspects of translation. In the similar language translation task, participants built machine translation systems for translating between closely related pairs of languages. Chi-kiu Lo NRC Masaaki Nagata NTT Marcos Zampieri Rochester Institute of Technology metrics (Mathur et al., 2020) parallel corpus filtering (Koehn et al., 2020) quality estimation (Specia et al., 2020a) robustness (Specia et al., 2020b) unsupervised and very low-resource translation (Fraser, 2020) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (“constrained” condition). We included 22 translation directions this year, with translation between English and each of Chinese, Czech, German and Russian, as well as French to and from German being repeated from last year, and English to and from Inu"
2020.wmt-1.1,2020.wmt-1.24,0,0.0435945,"Missing"
2020.wmt-1.1,D18-1512,0,0.05415,"Missing"
2020.wmt-1.1,2020.wmt-1.47,1,0.740067,"Missing"
2020.wmt-1.1,W18-3601,1,0.891679,", we have employed DA as the primary mechanism for evaluating systems. With DA human evaluation, 15 human assessors are asked to rate a given translation by how adequately it expresses the meaning of the corresponding reference translation or source language input on an analogue scale, which corresponds to an underlying absolute 0–100 rating scale.5 No sentence or document length restriction is applied during manual evaluation. Direct Assessment is also employed for evaluation of video captioning systems at TRECvid (Graham et al., 2018; Awad et al., 2019) and multilingual surface realisation (Mille et al., 2018, 2019). 3.1.1 tion 2, most of our test sets do not include reversecreated sentence pairs, except when there were resource constraints on the creation of the test sets. 3.1.3 Prior to WMT19, the issue of including document context was raised within the community (Läubli et al., 2018; Toral et al., 2018) and at WMT19 a range of DA styles were subsequently tested that included document context. In WMT19, two options were run, firstly, an evaluation that included the document context “+DC” (with document context), and secondly, a variation that omitted document context “−DC” (without document con"
2020.wmt-1.1,2020.wmt-1.27,0,0.247738,"he original source language. 9 Team AFRL ARIEL XV CUNI DCU D EEP M IND D I D I -NLP DONG - NMT ENMT E T RANSLATION FACEBOOK AI G RONINGEN GTCOM H ELSINKI NLP H UAWEI TSC IIE M ICROSOFT STC I NDIA NICT-K YOTO NICT-RUI N IU T RANS NRC OPPO PROMT SJTU-NICT SRPOL TALP UPC T ENCENT T RANSLATION THUNLP T ILDE T OHOKU -AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans"
2020.wmt-1.1,D19-6301,1,0.888512,"Missing"
2020.wmt-1.1,W18-6424,0,0.0431841,"(Kocmi, 2020) combines transfer learning from a high-resource language pair Czech–English into the low-resource Inuktitut-English with an additional backtranslation step. Surprising behaviour is noticed when using synthetic data, which can be possibly attributed to a narrow domain of training and test data. The system is the Transformer model in a constrained submission. 2.3.3 Charles University (CUNI) CUNI-D OC T RANSFORMER (Popel, 2020) is similar to the sentence-level version (CUNI-T2T2018, CUBBITT), but trained on sequences with multiple sentences of up to 3000 characters. CUNI-T2T-2018 (Popel, 2018), also called CUBBITT, is exactly the same system as in WMT2018. It is the Transformer model trained according to Popel and Bojar (2018) plus a novel concat-regime backtranslation with checkpoint averaging (Popel et al., 2020), tuned separately for CZ-domain and non CZ-domain articles, possibly handling also translation-direction (“translationese”) issues. For cs→en also a coreference preprocessing was used adding the female-gender CUNI-T RANSFORMER (Popel, 2020) is similar to the WMT2018 version of CUBBITT, but with 12 encoder layers instead of 6 and trained on CzEng 2.0 instead of CzEng 1.7."
2020.wmt-1.1,2020.wmt-1.25,0,0.094349,"Missing"
2020.wmt-1.1,2020.wmt-1.28,0,0.0792624,"cument in the test set, and the origlang tag indicates the original source language. 9 Team AFRL ARIEL XV CUNI DCU D EEP M IND D I D I -NLP DONG - NMT ENMT E T RANSLATION FACEBOOK AI G RONINGEN GTCOM H ELSINKI NLP H UAWEI TSC IIE M ICROSOFT STC I NDIA NICT-K YOTO NICT-RUI N IU T RANS NRC OPPO PROMT SJTU-NICT SRPOL TALP UPC T ENCENT T RANSLATION THUNLP T ILDE T OHOKU -AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 20"
2020.wmt-1.1,2020.lrec-1.443,1,0.79707,"er their models were trained only on the provided data. Since we do not know how they were built, the online systems are treated as unconstrained during the automatic and human evaluations. In the rest of this section, we provide brief details of the submitted systems, for those where the authors provided such details. The training corpus for Inuktitut↔English is the recently released Nunavut Hansard Inuktitut– English Parallel Corpus 3.0 (Joanis et al., 2020). For the Japanese↔English tasks, we added several freely available parallel corpora to the training data. It includes JParaCrawl v2.0 (Morishita et al., 2020), a large web-based parallel corpus, Japanese-English Subtitle Corpus (JESC) (Pryzant et al., 2017), the Kyoto Free Translation Task (KFTT) corpus (Neubig, 2011), constructed from the Kyoto-related Wikipedia articles, and TED Talks (Cettolo et al., 2012). The monolingual data we provided was similar to last year’s, with a 2019 news crawl added to all the news corpora. In addition, we provided versions of the news corpora for Czech, English and German, with both the document and paragraph structure retained. In other words, we did not apply sentence splitting to these corpora, and we retained t"
2020.wmt-1.1,2020.wmt-1.48,0,0.090424,"Missing"
2020.wmt-1.1,2020.wmt-1.49,0,0.0519886,"Missing"
2020.wmt-1.1,W19-6712,0,0.0573476,"tly crawled multilingual parallel corpora from Indian government websites (Haddow and Kirefu, 2020; Siripragada et al., 2020), the Tanzil corpus (Tiedemann, 2009), the Pavlick dicParagraph-split Test Sets For the language pairs English↔Czech, English↔German and English→Chinese, we provided the translators with paragraph-split texts, instead of sentence-split texts. We did this in order to provide the translators with greater freedom and, hopefully, to improve the quality of the translation. Allowing translators to merge and split sentences removes one of the “translation shifts” identified by Popovic (2019), which can make translations create solely for MT evaluation different from translations produced for other purposes. We first show some descriptive statistics of the source texts, for Czech, English and German, in 3 Europarl Parallel Corpus Czech ↔ English German ↔ English Polish↔ English German ↔ French Sentences 645,241 1,825,745 632,435 1,801,076 Words 14,948,900 17,380,340 48,125,573 50,506,059 14,691,199 16,995,232 47,517,102 55,366,136 Distinct words 172,452 63,289 371,748 113,960 170,271 62,694 368,585 134,762 News Commentary Parallel Corpus Czech ↔ English 248,927 5,570,734 6,156,063"
2020.wmt-1.1,2020.wmt-1.26,0,0.0845151,"Missing"
2020.wmt-1.1,2020.vardial-1.10,0,0.0933804,"Missing"
2020.wmt-1.1,W18-6301,0,0.038239,"Missing"
2020.wmt-1.1,2020.wmt-1.51,0,0.0917045,"Missing"
2020.wmt-1.1,2020.wmt-1.50,1,0.78567,"Missing"
2020.wmt-1.1,2020.wmt-1.52,0,0.0485419,"Missing"
2020.wmt-1.1,P19-1164,0,0.0581517,"26 26.37 25.51 24.82 28.33 23.33 21.13 21.96 20.43 22.90 22.58 21.90 22.17 22.17 20.53 19.40 20.01 40.44 32.39 30.39 37.04 32.27 27.54 25.97 26.09 46.38 37.30 36.05 35.96 33.76 33.07 27.20 27.07 Table 15: TICO-19 test suite results on the English-to-X WMT20 translation directions. 26 4.1.5 antecedent (a less common direction of information flow), and then correctly express the noun in the target language. The success of the MT system in this test can be established automatically, whenever the gender of the target word can be automatically identified. Kocmi et al. (2020) build upon the WinoMT (Stanovsky et al., 2019) test set, which provides exactly the necessary type of sentences containing an ambiguous profession noun and a personal pronoun which unambiguously (for the human eye) refers to it based the situation described. When extending WinMT with Czech and Polish, Stanovsky et al. have to disregard some test patterns but the principle remains. The results indicate that all MT systems fail in this test, following gender bias (stereotypical patterns attributing the masculine gender to some professions and feminine gender to others) rather than the coreference link. Word Sense Disambiguation (Scherrer et"
2020.wmt-1.1,2020.wmt-1.31,0,0.0881792,"morphological segmentation of the polysynthetic Inuktitut, testing rule-based, supervised, semi-supervised as well as unsupervised word segmentation methods, (2) whether or not adding data from a related language (Greenlandic) helps, and (3) whether contextual word embeddings (XLM) improve translation. G RONINGEN - ENIU use Transformer implemented in Marian with the default setting, improving the performance also with tagged backtranslation, domain-specific data, ensembling and finetuning. 2.3.7 DONG - NMT (no associated paper) No description provided. 2.3.8 ENMT (Kim et al., 2020) Kim et al. (2020) base their approach on transferring knowledge of domain and linguistic characteristics by pre-training the encoder-decoder model with large amount of in-domain monolingual data through unsupervised and supervised prediction task. The model is then fine-tuned with parallel data and in-domain synthetic data, generated with iterative back-translation. For additional gain, final results are generated with an ensemble model and re-ranked with averaged models and language models. G RONINGEN - ENTAM (Dhar et al., 2020) study the effects of various techniques such as linguistically motivated segmenta"
2020.wmt-1.1,2020.wmt-1.32,0,0.0839317,"- NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) SJTU-NICT (Li et al., 2020) Samsung Research Poland (Krubi´nski et al., 2020) TALP UPC (Escolano et al., 2020) Tencent Translation (Wu et al., 2020b) NLP Lab at Tsinghua University (no associated paper) Tilde (Krišlauks and Pinnis, 2020) Tohoku-AIP-NTT (Kiyono et al., 2020) Ub"
2020.wmt-1.1,2020.wmt-1.33,0,0.080166,"Missing"
2020.wmt-1.1,2020.wmt-1.34,0,0.0803867,"Missing"
2020.wmt-1.1,2020.wmt-1.35,0,0.0951745,"ss web site (JW300). The development and test sets were created as part of the Flores initiative (Guzmán et al., 2019) by professional translation of Wikipedia content with careful vetting of the translations. Please refer the to the Parallel Corpus Filtering Shared Task overview paper for details on these corpora. 2.3.1 AFRL (Gwinnup and Anderson, 2020) AFRL - SYSCOMB 20 is a system combination consisting of two Marian transformer ensembles, one OpenNMT transformer system and a Moses phrase-based system. AFRL - FINETUNE is an OpenNMT transformer system fine-tuned on newstest2014-2017. 2.3.2 (Xv, 2020) ARIEL XV is a Transformer base model trained with the Sockeye sequence modeling toolkit usSome statistics about the training and test materials are given in Figures 1, 2, 3 and 4. 4 8 ARIEL XV https://github.com/AppraiseDev/OCELoT English I English II English III Chinese Czech German Inuktitut Japanese Polish Russian Tamil ABC News (2), All Africa (5), Brisbane Times (1), CBS LA (1), CBS News (1), CNBC (3), CNN (2), Daily Express (1), Daily Mail (2), Fox News (1), Gateway (1), Guardian (3), Huffington Post (2), London Evening Standard (2), Metro (2), NDTV (7), RTE (7), Reuters (4), STV (2), S"
2020.wmt-1.1,2020.wmt-1.55,0,0.0877526,"Missing"
2020.wmt-1.1,P17-4012,0,0.0273892,"o SJTU-NICT using large XLM model to improve NMT but the exact relation is unclear. 2.3.14 H UAWEI TSC (Wei et al., 2020a) H UAWEI TSC use Transformer-big with a further increased model size, focussing on standard techniques of careful pre-processing and filtering, back-translation and forward translation, including self-training, i.e. translating one of the sides of the original parallel data. Ensembling of individual training runs is used in the forward as well as backward translation, and single models are created from the ensembles using knowledge distillation. The submission uses THUNMT (Zhang et al., 2017) open-source engine. 2.3.19 N IU T RANS (Zhang et al., 2020) N IU T RANS gain their performance from focussed attention to six areas: (1) careful data preprocessing and filtering, (2) iterative back-translation to generate additional training data, (3) using different model architectures, such as wider and/or deeper models, relative position representation and relative length, to enhance the diversity of translations, (4) iterative knowledge distillation by in-domain monolingual data, (5) iterative finetuning for domain adaptation using small training batches, (6) rule-based post-processing of"
2020.wmt-1.1,P98-2238,0,0.590812,"and punctuation, and we tend to attribute all these improvements to increased capacity (which allows increased sensitivity to long-range relations) of the models. 5 Similar Language Translation Most shared tasks at WMT (e.g. News, Biomedical) have historically dealt with translating texts from and to English. In recent years, we observed a growing interest in training systems to translate between languages other than English. This includes a number of papers applying MT to translate between pairs of closely-related languages, national language varieties, and dialects 27 of the same language (Zhang, 1998; Marujo et al., 2011; Hassani, 2017; Costa-jussà et al., 2018; Popovi´c et al., 2020). To address this topic, the first Similar Language Translation (SLT) shared task at WMT 2019 has been organized. It featured data from three pairs of closely-related languages from different language families: Spanish - Portuguese (Romance languages), Czech - Polish (Slavic languages), and Hindi - Nepali (IndoAryan languages). Following the success of the first SLT shared task at WMT 2019 and the interest of the community in this topic, we organize, for the second time at WMT, this shared task to evaluate th"
2020.wmt-1.1,2020.wmt-1.41,1,0.814291,"Missing"
2021.acl-srw.34,2020.crac-1.3,0,0.0358749,"used parse trees as input and detected empty categories, including ZPs, by labeling a node representing the maximal projection of a predicate, namely IP or VP. Song et al. (2020) proposed jointly learning 331 Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Student Research Workshop, pages 331–336 August 5–6, 2021. ©2021 Association for Computational Linguistics ZP resolution and ZP identification by treating it as sequence labeling on every word boundary. Aloraini and Poesio (2020) considered word positions before or after each VP node as ZP location candidates and predicted whether the candidate has ZP or not as a binary classification task. To the best of our knowledge, our approach is the first work that formalizes ZP identification as a QA task. In recent years, approaches for solving various tasks as QA-based span prediction problems have been proposed. Li et al. (2020) made questions corresponding to NER entity tags. Then, their model predicted the entity span giving the question and a sentence as QA tasks to tackle the nested NER problem. In the coreference resol"
2021.acl-srw.34,N19-1423,0,0.0907288,"Science and Technology 2 NTT Communication Science Laboratories, NTT Corporation {iwata.sei.is6,taro}@is.naist.jp masaaki.nagata.et@hco.ntt.co.jp Abstract When identifying a ZP from the sentence where the argument is omitted, the predicate information is the key. The ZP identification is solved in many previous works as a labeling task for input sentence tokens (Aloraini and Poesio, 2020; Song et al., 2020) or nodes in a parse tree (Xiang et al., 2013; Takeno et al., 2015). In this study, we treat ZP identification as an instance of span prediction tasks inspired by the QA method proposed in Devlin et al. (2019). There are two steps to solve the ZP identification in our approach. 1) Given a predicate as a query, our model extracts each argument, such as subject or object, as the answer from the input sentence. 2) If our model cannot extract any corresponding argument from the input sentence, the model predicts whether or not it is a ZP. In the above example, given a predicate 気に入った “like”, our model should predict that the subject argument is 私は “I” in the sentence and the object argument is a ZP. By explicitly providing predicates as queries in this way, our approach allows the model to capture info"
2021.acl-srw.34,2020.acl-main.622,0,0.0295219,"word positions before or after each VP node as ZP location candidates and predicted whether the candidate has ZP or not as a binary classification task. To the best of our knowledge, our approach is the first work that formalizes ZP identification as a QA task. In recent years, approaches for solving various tasks as QA-based span prediction problems have been proposed. Li et al. (2020) made questions corresponding to NER entity tags. Then, their model predicted the entity span giving the question and a sentence as QA tasks to tackle the nested NER problem. In the coreference resolution task, Wu et al. (2020) generated queries based on each mention and extracted the text spans of coreferences as answers to given queries. Nagata et al. (2020) improved the performance of word alignment task by giving a word in the source language sentence as a question and predicting its corresponding word span in the target language sentence. 3 Our argument span prediction is inspired by BERT fine-tuning for the QA task (Devlin et al., 2019). Inputs follow a BERT style formulated as “[CLS] query [SEP] sentence [SEP]”, where [CLS] is a special token to output the classification result and [SEP] denotes the boundary"
2021.acl-srw.34,P13-1081,0,0.0341127,"the object argument (OBJ) is omitted from the second sentence because Japanese speakers can predict from the context that the OBJ is “it”, and the omission is natural for the Japanese speakers. Downstream tasks involving pro-drop languages could easily suffer from the existence of ZPs. In the machine translation task, it has been reported that supplementing the ZP information when translating from pro-drop languages to non-pro-drop languages improves the performance (Wang et al., 2019). 2 Related work Most of the researchers considered the ZP detection or ZP identification as a labeling task. Xiang et al. (2013) and Takeno et al. (2015) used parse trees as input and detected empty categories, including ZPs, by labeling a node representing the maximal projection of a predicate, namely IP or VP. Song et al. (2020) proposed jointly learning 331 Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Student Research Workshop, pages 331–336 August 5–6, 2021. ©2021 Association for Computational Linguistics ZP resolution and ZP identification by treating it as sequence labelin"
2021.acl-srw.34,P11-1081,0,0.0276918,"f the baseline and QAZP, our proposal model for three sentences in a Japanese ZP identification task. Each line represents either the prediction of one of the both models, or the Gold data for the argument of a predicate covered by the lines. The first and third examples are predictions for SBJ arguments, and the second example is a prediction result for the OBJ arguments by the baseline and QAZP. Model Baseline QAZP Arg SBJ ALL SBJ ALL Arg span accuracy 88.7 88.3 ZP F1 71.5 71.4 80.6 80.5 ZP pre 72.5 72.6 81.2 81.0 ZP recall 70.5 70.3 80.6 80.4 Integer Linear Programming as in the method of (Iida and Poesio, 2011). Example 3 is the case when the predictions of both models are incorrect. In this example sentence, the gold ZP class is the first person “speaker”, but it is impossible to identify the ZP without knowing the context before and after the input sentence. We expect our model will capture context information by extending the input unit to multiple sentences instead of a single sentence. Table 5: Argument(Arg) span accuracy and ZP detection on OntoNotes5.0. for “pro” class. The row of ALL indiciataes the value for SBJ, OBJ and IO2 arguments. cates お 話し “speak” and 復習 “review” are different. While"
2021.acl-srw.34,2020.acl-main.519,0,0.0429663,"ch Workshop, pages 331–336 August 5–6, 2021. ©2021 Association for Computational Linguistics ZP resolution and ZP identification by treating it as sequence labeling on every word boundary. Aloraini and Poesio (2020) considered word positions before or after each VP node as ZP location candidates and predicted whether the candidate has ZP or not as a binary classification task. To the best of our knowledge, our approach is the first work that formalizes ZP identification as a QA task. In recent years, approaches for solving various tasks as QA-based span prediction problems have been proposed. Li et al. (2020) made questions corresponding to NER entity tags. Then, their model predicted the entity span giving the question and a sentence as QA tasks to tackle the nested NER problem. In the coreference resolution task, Wu et al. (2020) generated queries based on each mention and extracted the text spans of coreferences as answers to given queries. Nagata et al. (2020) improved the performance of word alignment task by giving a word in the source language sentence as a question and predicting its corresponding word span in the target language sentence. 3 Our argument span prediction is inspired by BERT"
2021.acl-srw.34,2020.emnlp-main.41,1,0.867501,"Missing"
2021.acl-srw.34,2020.acl-main.482,0,0.0181744,"tasks involving pro-drop languages could easily suffer from the existence of ZPs. In the machine translation task, it has been reported that supplementing the ZP information when translating from pro-drop languages to non-pro-drop languages improves the performance (Wang et al., 2019). 2 Related work Most of the researchers considered the ZP detection or ZP identification as a labeling task. Xiang et al. (2013) and Takeno et al. (2015) used parse trees as input and detected empty categories, including ZPs, by labeling a node representing the maximal projection of a predicate, namely IP or VP. Song et al. (2020) proposed jointly learning 331 Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Student Research Workshop, pages 331–336 August 5–6, 2021. ©2021 Association for Computational Linguistics ZP resolution and ZP identification by treating it as sequence labeling on every word boundary. Aloraini and Poesio (2020) considered word positions before or after each VP node as ZP location candidates and predicted whether the candidate has ZP or not as a binary classifi"
2021.acl-srw.34,D15-1156,1,0.832364,") is omitted from the second sentence because Japanese speakers can predict from the context that the OBJ is “it”, and the omission is natural for the Japanese speakers. Downstream tasks involving pro-drop languages could easily suffer from the existence of ZPs. In the machine translation task, it has been reported that supplementing the ZP information when translating from pro-drop languages to non-pro-drop languages improves the performance (Wang et al., 2019). 2 Related work Most of the researchers considered the ZP detection or ZP identification as a labeling task. Xiang et al. (2013) and Takeno et al. (2015) used parse trees as input and detected empty categories, including ZPs, by labeling a node representing the maximal projection of a predicate, namely IP or VP. Song et al. (2020) proposed jointly learning 331 Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Student Research Workshop, pages 331–336 August 5–6, 2021. ©2021 Association for Computational Linguistics ZP resolution and ZP identification by treating it as sequence labeling on every word boundary."
2021.acl-srw.34,D19-1085,0,0.0219258,"linguistics. JA こ の ケ ー キ は 美 味 し い 。私は (pro-OBJ) 気に入った． EN This cake is delicious. I like (it). In the Japanese example above, the object argument (OBJ) is omitted from the second sentence because Japanese speakers can predict from the context that the OBJ is “it”, and the omission is natural for the Japanese speakers. Downstream tasks involving pro-drop languages could easily suffer from the existence of ZPs. In the machine translation task, it has been reported that supplementing the ZP information when translating from pro-drop languages to non-pro-drop languages improves the performance (Wang et al., 2019). 2 Related work Most of the researchers considered the ZP detection or ZP identification as a labeling task. Xiang et al. (2013) and Takeno et al. (2015) used parse trees as input and detected empty categories, including ZPs, by labeling a node representing the maximal projection of a predicate, namely IP or VP. Song et al. (2020) proposed jointly learning 331 Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Student Research Workshop, pages 331–336 August"
2021.eacl-main.214,W19-6721,0,0.0143792,"✓, ) + LMB ( ), To use inter-sentence information, we modify the NMT model by adding MBE to the input: y = f (x, z; ✓). B2D0 (x,y)2B (2) where e is a hidden dimension of the NMT model. We use mean pooling2 to make both sentence embeddings vi and MBE z. By adopting this procedure, we expect MBE z to have inter-sentence context features, which is desirable for a contextaware NMT. Note that we ignore the order of sentences in a document. This is a beneficial trait because this method is also applicable to corpora with document boundaries but without in-document sentence order, such as ParaCrawl (Esplà et al., 2019). 3.2 We concatenated MBE to the input word embeddings, and the model uses MBE as the first input token (Fig. 1). Now the encoder/decoder takes s+1 and t + 1 embeddings. The Transformer encoder layer for MBE was jointly trained with the NMT model by modifying the loss function in Eq. (1): X X LNMT (✓, ) = log P (y|x, B; ✓, ), (7) where is a hyperparameter used to control the weight of the classifier loss. We use the value predicted by the classifier as a gate. Our new weighted MBE is z˜ = ↵z, (8) where ↵ = P (d = 1|z), and we change z in Eqs. (3) to z˜. 2514 <pad> Transformer encoder <pad> Tra"
2021.eacl-main.214,D18-1325,0,0.076055,"l., 2015; Vaswani et al., 2017) translate sentences in a sentence-by-sentence manner. However, some have argued that it is critical to consider the inter-sentence context in handling discourse phenomena (Hardmeier, 2012), which include coherence, cohesion, coreference (Bawden et al., 2018; Nagata and Morishita, 2020), and writing style (Yamagishi et al., 2016). To correctly translate these linguistic features, some works provide additional context information to an NMT model by concatenating the previous sentence (Tiedemann and Scherrer, 2017), applying a context encoder (Bawden et al., 2018; Miculicich et al., 2018; Voita et al., 2018), or using a cachebased network (Tu et al., 2018; Kuang et al., 2018). Most of the previous studies have considered only a few previous context sentences. Several methods, such as the cache-based network, consider long-range context but heavily modify the standard NMT models and require additional training/decoding steps. Our goal is to make a simple but effective context-aware NMT model, which does not require heavy modification to standard NMT models and can handle a wider inter-sentence context. To this end, we propose a method to create an embedding that represents the"
2021.eacl-main.214,2020.lrec-1.443,1,0.759612,"6 Enc-Layers but uses MBE in the encoder. 3 Since our training data have document boundaries but the in-document sentence orders were shuffled, we randomly selected one in-document sentence and used it as previous context. For dev/test sets, we used the original sentence order. MBE Enc w/o Gate resembles MBE Enc, but it does not use the MBE gate described in Section 3.3. MBE Dec uses MBE in the decoder. MBE Enc/Dec uses MBE in both the encoder and the decoder. 4.2 Experimental Settings Datasets/Evaluation We trained JapaneseEnglish NMT models. As training data, we used the JParaCrawl corpus (Morishita et al., 2020). JParaCrawl was created by crawling the web and aligning parallel sentences, and each sentence-pair has a URL from which the sentences were taken. In this experiment, we regarded the sentences from the same URL as a document. We used several test sets with document boundaries: (i) scientific paper excerpts (ASPEC (Nakazawa et al., 2016)), (ii) news (newsdev2020 from WMT20 news translation shared task4 ), and (iii) TED talks (tst2012 from IWSLT translation shared task (Cettolo et al., 2012)). As a dev set to tune the NMT model, we used the ASPEC dev split. See Section A.1 in the Appendix for c"
2021.eacl-main.214,W18-6301,0,0.0495336,"Missing"
2021.eacl-main.214,P02-1040,0,0.109482,"his experiment, we regarded the sentences from the same URL as a document. We used several test sets with document boundaries: (i) scientific paper excerpts (ASPEC (Nakazawa et al., 2016)), (ii) news (newsdev2020 from WMT20 news translation shared task4 ), and (iii) TED talks (tst2012 from IWSLT translation shared task (Cettolo et al., 2012)). As a dev set to tune the NMT model, we used the ASPEC dev split. See Section A.1 in the Appendix for corpus statistics and detailed preprocessing steps. To evaluate the translation performance, we used sacreBLEU5 (Post, 2018) and report the BLEU scores (Papineni et al., 2002). Model Configurations We used the Transformer model as an NMT model (Vaswani et al., 2017). Our hyperparameters were based on the “big” settings defined by Vaswani et al. (2017). For the MBE experiments, we set in Eq. (7) to 1.0. We set the mini-batch size to 3,000 tokens. If the tokens in a document were larger than this size, we 4 We used newsdev2020 as a test set because no official test set for English-Japanese was available at the time of writing. Since we did not use newsdev2020 for tuning the model, there is no problem with using it as a test set. 5 The signature is BLEU+case.mixed+lan"
2021.eacl-main.214,W18-6319,0,0.0117096,"rom which the sentences were taken. In this experiment, we regarded the sentences from the same URL as a document. We used several test sets with document boundaries: (i) scientific paper excerpts (ASPEC (Nakazawa et al., 2016)), (ii) news (newsdev2020 from WMT20 news translation shared task4 ), and (iii) TED talks (tst2012 from IWSLT translation shared task (Cettolo et al., 2012)). As a dev set to tune the NMT model, we used the ASPEC dev split. See Section A.1 in the Appendix for corpus statistics and detailed preprocessing steps. To evaluate the translation performance, we used sacreBLEU5 (Post, 2018) and report the BLEU scores (Papineni et al., 2002). Model Configurations We used the Transformer model as an NMT model (Vaswani et al., 2017). Our hyperparameters were based on the “big” settings defined by Vaswani et al. (2017). For the MBE experiments, we set in Eq. (7) to 1.0. We set the mini-batch size to 3,000 tokens. If the tokens in a document were larger than this size, we 4 We used newsdev2020 as a test set because no official test set for English-Japanese was available at the time of writing. Since we did not use newsdev2020 for tuning the model, there is no problem with using it as"
2021.eacl-main.214,D19-1081,0,0.136274,"yer, this model has a comparable number of parameters as the following MBE models. 2-to-1 is the context-aware translation model proposed by Tiedemann and Scherrer (2017) that translates a pair of previous and current source sentences into a target sentence. Two source sentences are concatenated with a special sentence boundary token. This method is known as a strong baseline for context-aware NMT (Bawden et al., 2018; Voita et al., 2018). Other settings are identical to those of Baseline 6 EncLayers3 . DocRepair is another recent context-aware translation model, which uses two-step decoding (Voita et al., 2019). The first step generates 1-best translation with a sentence-level NMT model given a single sentence. The second step generates document-level translation given 1best translations of four consecutive sentences concatenated with a special token. We compared our proposed methods with the following settings: MBE Enc resembles Baseline 6 Enc-Layers but uses MBE in the encoder. 3 Since our training data have document boundaries but the in-document sentence orders were shuffled, we randomly selected one in-document sentence and used it as previous context. For dev/test sets, we used the original se"
2021.eacl-main.214,P18-1117,0,0.0742115,"2017) translate sentences in a sentence-by-sentence manner. However, some have argued that it is critical to consider the inter-sentence context in handling discourse phenomena (Hardmeier, 2012), which include coherence, cohesion, coreference (Bawden et al., 2018; Nagata and Morishita, 2020), and writing style (Yamagishi et al., 2016). To correctly translate these linguistic features, some works provide additional context information to an NMT model by concatenating the previous sentence (Tiedemann and Scherrer, 2017), applying a context encoder (Bawden et al., 2018; Miculicich et al., 2018; Voita et al., 2018), or using a cachebased network (Tu et al., 2018; Kuang et al., 2018). Most of the previous studies have considered only a few previous context sentences. Several methods, such as the cache-based network, consider long-range context but heavily modify the standard NMT models and require additional training/decoding steps. Our goal is to make a simple but effective context-aware NMT model, which does not require heavy modification to standard NMT models and can handle a wider inter-sentence context. To this end, we propose a method to create an embedding that represents the contextual informati"
2021.eacl-main.214,W16-4620,0,0.0227936,"erforms the translation capabilities of strong baselines and improves writing style or terminology to fit the document’s context.1 1 Introduction Current standard neural machine translation (NMT) models (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Vaswani et al., 2017) translate sentences in a sentence-by-sentence manner. However, some have argued that it is critical to consider the inter-sentence context in handling discourse phenomena (Hardmeier, 2012), which include coherence, cohesion, coreference (Bawden et al., 2018; Nagata and Morishita, 2020), and writing style (Yamagishi et al., 2016). To correctly translate these linguistic features, some works provide additional context information to an NMT model by concatenating the previous sentence (Tiedemann and Scherrer, 2017), applying a context encoder (Bawden et al., 2018; Miculicich et al., 2018; Voita et al., 2018), or using a cachebased network (Tu et al., 2018; Kuang et al., 2018). Most of the previous studies have considered only a few previous context sentences. Several methods, such as the cache-based network, consider long-range context but heavily modify the standard NMT models and require additional training/decoding s"
2021.eacl-main.214,D19-1410,0,0.0129684,"ranslate the mini-batch or the MBE fails to contain important information for translation. To deal with such cases, we aim to make the model estimate how important MBE is for each mini-batch. Thus we added a mini-batch embedding gate to determine MBE’s importance. In this setting, we prepared two types of minibatches for training: (i) sentences from the same document and (ii) sentences from different documents. Then we trained a binary classifier that predicts whether the sentences in the mini-batch are selected from the same document: P (d|z) = softmax(W z), (3) This approach was inspired by Reimers and Gurevych (2019), who successfully created sentence embeddings from BERT embeddings (Devlin et al., 2019) by mean pooling. (5) where W 2 R2⇥e is a parameter matrix and d is a binary value that takes 1 if the sentences in the mini-batch are selected from the same document. To train the classifier, we minimize the loss function: X LMB ( ) = log P (d|B; ), (6) (d,B)2D0 where is a set of parameters for the classifier. For training, we mix the two types of mini-batches at the same ratio. Concretely, we jointly minimize the NMT and the classifier loss functions: L(✓, , ) = LNMT (✓, ) + LMB ( ), To use inter-sentenc"
2021.eacl-main.214,P16-1009,0,0.0297422,"requires only a small computational cost. NMT with Tags We used an MBE as the first input of the encoder/decoder. Our approach is similar to the work that uses special tags to control or provide additional information to NMT (Johnson et al., 2016; Takeno et al., 2017; Caswell et al., 2019). Johnson et al. (2016) added tags to a source sentence for indicating the target language in multilingual NMT models. Takeno et al. (2017) proposed a method that controls the target length or the domain by adding a tag to the decoder inputs. Caswell et al. (2019) used a tag to indicate the synthetic corpus (Sennrich et al., 2016). Our work, which automatically generates a tag (MBE) with the sentence in a mini-batch and uses a gate to control the importance of MBE, is different from the previous studies. 6 Conclusion We proposed mini-batch embedding (MBE), which is a simple but effective method to represent contextual information across documents. We incorporated MBE in the NMT model, which enabled it to outperform competitive baselines. We found that our NMT model could choose the appropriate word and writing style to match the document context. An analysis showed that our model’s performance improves with a large con"
2021.eacl-main.214,W17-5702,1,0.830148,"ment-level model that outputs document-level translation. Although this is a promising method, it requires training of three sequence-to-sequence models to translate a single direction and needs two decoding steps, which slows down the translation. Our method has an advantage in that it only trains a single model and uses single-step decoding, which requires only a small computational cost. NMT with Tags We used an MBE as the first input of the encoder/decoder. Our approach is similar to the work that uses special tags to control or provide additional information to NMT (Johnson et al., 2016; Takeno et al., 2017; Caswell et al., 2019). Johnson et al. (2016) added tags to a source sentence for indicating the target language in multilingual NMT models. Takeno et al. (2017) proposed a method that controls the target length or the domain by adding a tag to the decoder inputs. Caswell et al. (2019) used a tag to indicate the synthetic corpus (Sennrich et al., 2016). Our work, which automatically generates a tag (MBE) with the sentence in a mini-batch and uses a gate to control the importance of MBE, is different from the previous studies. 6 Conclusion We proposed mini-batch embedding (MBE), which is a sim"
2021.eacl-main.214,W17-4811,0,0.401327,"nslation (NMT) models (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Vaswani et al., 2017) translate sentences in a sentence-by-sentence manner. However, some have argued that it is critical to consider the inter-sentence context in handling discourse phenomena (Hardmeier, 2012), which include coherence, cohesion, coreference (Bawden et al., 2018; Nagata and Morishita, 2020), and writing style (Yamagishi et al., 2016). To correctly translate these linguistic features, some works provide additional context information to an NMT model by concatenating the previous sentence (Tiedemann and Scherrer, 2017), applying a context encoder (Bawden et al., 2018; Miculicich et al., 2018; Voita et al., 2018), or using a cachebased network (Tu et al., 2018; Kuang et al., 2018). Most of the previous studies have considered only a few previous context sentences. Several methods, such as the cache-based network, consider long-range context but heavily modify the standard NMT models and require additional training/decoding steps. Our goal is to make a simple but effective context-aware NMT model, which does not require heavy modification to standard NMT models and can handle a wider inter-sentence context. T"
2021.eacl-main.214,Q18-1029,0,0.0867773,"manner. However, some have argued that it is critical to consider the inter-sentence context in handling discourse phenomena (Hardmeier, 2012), which include coherence, cohesion, coreference (Bawden et al., 2018; Nagata and Morishita, 2020), and writing style (Yamagishi et al., 2016). To correctly translate these linguistic features, some works provide additional context information to an NMT model by concatenating the previous sentence (Tiedemann and Scherrer, 2017), applying a context encoder (Bawden et al., 2018; Miculicich et al., 2018; Voita et al., 2018), or using a cachebased network (Tu et al., 2018; Kuang et al., 2018). Most of the previous studies have considered only a few previous context sentences. Several methods, such as the cache-based network, consider long-range context but heavily modify the standard NMT models and require additional training/decoding steps. Our goal is to make a simple but effective context-aware NMT model, which does not require heavy modification to standard NMT models and can handle a wider inter-sentence context. To this end, we propose a method to create an embedding that represents the contextual information of a document. To create this embedding, we f"
2021.naacl-main.127,P19-1061,0,0.0150032,"aim at improving the Span and Nuclearity scores. Unsupervised RST parsing methods have also been proposed recently (Kobayashi et al., 2019; Nishida and Nakayama, 2020). Since they are unsupervised, they do not require any annotated corpora. However, they can predict only tree structures and 1601 2 Nguyen et al. (2020) proposed a similar approach in NMT and introduced a method named data diversification: it diversifies the training data by using multiple forward and backward translation models. We can find some weak supervision approaches for other discourse representation formalisms such as (Badene et al., 2019). Teacher parsers Autoparsed trees Ptch=1 Ptch=k Training Unlabeled Documents CNN … … Gold data: RST-DT Autoparsed trees Silver data: Agreement subtrees Student parser Pstu Pre-training Gold data: RST-DT Pstu Fine-tuning Figure 1: Overview of proposed method. In the subtree extraction step, the teacher RST parsers first annotate trees to unlabeled documents, and then the proposed subtree extraction method constructs large silver data. In the training step, the student parser is trained through pre-training and fine-tuning. cannot predict nucleus and relation labels. Therefore, the predicted tr"
2021.naacl-main.127,E17-1028,0,0.0165477,". Kobayashi et al. (2020) proposed another top-down RST parsing method exploiting multiple granularity levels in a document and achieved the best Span and Nuclearity scores on the RST-DT, i.e., F1 of 87.0 and 74.6, respectively. Since the RST-DT, the largest treebank, contains only 385 documents, several studies have been conducted on overcoming the problem of a limited number of training data. Braud et al. (2016) leveraged multi-task learning not only with 13 related tasks as an auxiliary task but also for multiple views of discourse structures, such as Constituent, Nuclearity, and Relation. Braud et al. (2017) used multilingual RST discourse datasets that share the same underlying linguistic theory. Huber and Carenini (2019) adopted distant supervision with an auxiliary task of sentiment classification to create largescale training data, i.e., they trained a two-stage RST parser (Wang et al., 2017a) with RST trees automatically built based on attention and sentiment scores from the Multiple-Instance Learning network, which was trained with a review dataset. However, these studies need other annotated corpora than the RST-DT, which means we still face the problem of being dependent on costly annotat"
2021.naacl-main.127,W01-1605,0,0.226951,"r ones or merging two adjacent spans into a larger one, (2) nuclearity status prediction for two adjacent spans by solving a 3-class classification problem, and (3) relation label prediction for two adjacent spans by solving an 18-class classification problem (see Section 3.3 for details). However, it is costly to annotate RST trees for a huge collection of documents, and thus it is difficult to obtain a large amount of human-annotated data for RST parsing. As a result, research on RST parsing has focused on English, with the largest annotated corpus being the RST Discourse Treebank (RST-DT) (Carlson et al., 2001), although even this is still small with only 385 documents.1 Many RST parsing methods have recently been developed based on neural models (Ji and Eisenstein, 2014; Li et al., 2014a, 2016; Liu and Lapata, 2017; Braud et al., 2016, 2017). Among them, Kobayashi et al. (2020) is the current state-of-theart system and is based on the neural top-down method. While its Span and Nuclearity scores achieved the highest level, its Relation score still has room for improvement. One of the reasons for its poor Relation score might be its small amount of training data for solving the 18-class classificatio"
2021.naacl-main.127,W11-0401,0,0.0365009,"d theories for representing the discourse structure of a text as a tree. RST trees are a kind of constituent tree, whose leaves are Elementary Discourse Units (EDUs), i.e., clause-like units, and whose non-terminal nodes cover text spans consisting of either a sequence of EDUs or a single EDU. The label of a non-terminal node represents the attribution of a text span, i.e., nucleus (N) or satellite (S). A discourse relation is also assigned between two adjacent non-terminal nodes. 1 We can find some exceptions for other languages such In most cases, RST parsers have been devel- as Spanish (da Cunha et al., 2011) and German (Stede and oped on the basis of supervised learning algorithms Neumann, 2014). 1600 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1600–1612 June 6–11, 2021. ©2021 Association for Computational Linguistics (Sennrich et al., 2016) introduced a simple learning framework: first pre-train an NMT model with silver data, i.e., pseudo-parallel data generated by automatic back-translation, and then fine-tune it with gold data, i.e., real parallel data, to overcome the data sparseness prob"
2021.naacl-main.127,P14-1048,0,0.0273741,"cale data. We first pre-train the student parser by using the obtained silver data. We then fine-tune parameters of the parser on gold data, using the RST-DT. Experimental results on the RST-DT clearly indicate the effectiveness of our silver data. Our method obtained remarkable Nuclearity and Relation F1 scores of 75.0 and 63.2, respectively. 2 Related Work Early studies on RST parsing were based on traditional supervised learning methods with handcrafted features and the shift-reduce or CKY-like parsing algorithms (duVerle and Prendinger, 2009; Feng and Hirst, 2012; Joty et al., 2013, 2015; Feng and Hirst, 2014). Recently, Wang et al. (2017b) proposed a shift-reduce parser based on SVMs and achieved the current best results in classical statistical models on the RST-DT. The method first built nuclearity-labeled RST trees and then assigned relation labels between two adjacent spans consisting of a single or multiple EDUs. Inspired by the success of neural networks in many NLP tasks, several neural network-based models have been proposed for RST parsing (Ji and Eisenstein, 2014; Li et al., 2014a, 2016; Liu and Lapata, 2017). Yu et al. (2018) proposed a shift-reduce parser based on neural networks and l"
2021.naacl-main.127,D19-1235,0,0.0143347,"n a document and achieved the best Span and Nuclearity scores on the RST-DT, i.e., F1 of 87.0 and 74.6, respectively. Since the RST-DT, the largest treebank, contains only 385 documents, several studies have been conducted on overcoming the problem of a limited number of training data. Braud et al. (2016) leveraged multi-task learning not only with 13 related tasks as an auxiliary task but also for multiple views of discourse structures, such as Constituent, Nuclearity, and Relation. Braud et al. (2017) used multilingual RST discourse datasets that share the same underlying linguistic theory. Huber and Carenini (2019) adopted distant supervision with an auxiliary task of sentiment classification to create largescale training data, i.e., they trained a two-stage RST parser (Wang et al., 2017a) with RST trees automatically built based on attention and sentiment scores from the Multiple-Instance Learning network, which was trained with a review dataset. However, these studies need other annotated corpora than the RST-DT, which means we still face the problem of being dependent on costly annotated corpora. Jiang et al. (2016) proposed a framework for enriching training data based on co-training to improve the"
2021.naacl-main.127,P14-1002,0,0.199435,"(3) relation label prediction for two adjacent spans by solving an 18-class classification problem (see Section 3.3 for details). However, it is costly to annotate RST trees for a huge collection of documents, and thus it is difficult to obtain a large amount of human-annotated data for RST parsing. As a result, research on RST parsing has focused on English, with the largest annotated corpus being the RST Discourse Treebank (RST-DT) (Carlson et al., 2001), although even this is still small with only 385 documents.1 Many RST parsing methods have recently been developed based on neural models (Ji and Eisenstein, 2014; Li et al., 2014a, 2016; Liu and Lapata, 2017; Braud et al., 2016, 2017). Among them, Kobayashi et al. (2020) is the current state-of-theart system and is based on the neural top-down method. While its Span and Nuclearity scores achieved the highest level, its Relation score still has room for improvement. One of the reasons for its poor Relation score might be its small amount of training data for solving the 18-class classification problem. Currently, we can refer to various studies on improving neural models for NLP tasks through acquiring large-scale synthetic training data, sometimes cal"
2021.naacl-main.127,C16-1245,0,0.0171105,"ingual RST discourse datasets that share the same underlying linguistic theory. Huber and Carenini (2019) adopted distant supervision with an auxiliary task of sentiment classification to create largescale training data, i.e., they trained a two-stage RST parser (Wang et al., 2017a) with RST trees automatically built based on attention and sentiment scores from the Multiple-Instance Learning network, which was trained with a review dataset. However, these studies need other annotated corpora than the RST-DT, which means we still face the problem of being dependent on costly annotated corpora. Jiang et al. (2016) proposed a framework for enriching training data based on co-training to improve the performance for infrequent relation labels. However, the method failed to improve the overall Relation score, while they did not aim at improving the Span and Nuclearity scores. Unsupervised RST parsing methods have also been proposed recently (Kobayashi et al., 2019; Nishida and Nakayama, 2020). Since they are unsupervised, they do not require any annotated corpora. However, they can predict only tree structures and 1601 2 Nguyen et al. (2020) proposed a similar approach in NMT and introduced a method named"
2021.naacl-main.127,P13-1048,0,0.0360339,"ubtrees to handle large-scale data. We first pre-train the student parser by using the obtained silver data. We then fine-tune parameters of the parser on gold data, using the RST-DT. Experimental results on the RST-DT clearly indicate the effectiveness of our silver data. Our method obtained remarkable Nuclearity and Relation F1 scores of 75.0 and 63.2, respectively. 2 Related Work Early studies on RST parsing were based on traditional supervised learning methods with handcrafted features and the shift-reduce or CKY-like parsing algorithms (duVerle and Prendinger, 2009; Feng and Hirst, 2012; Joty et al., 2013, 2015; Feng and Hirst, 2014). Recently, Wang et al. (2017b) proposed a shift-reduce parser based on SVMs and achieved the current best results in classical statistical models on the RST-DT. The method first built nuclearity-labeled RST trees and then assigned relation labels between two adjacent spans consisting of a single or multiple EDUs. Inspired by the success of neural networks in many NLP tasks, several neural network-based models have been proposed for RST parsing (Ji and Eisenstein, 2014; Li et al., 2014a, 2016; Liu and Lapata, 2017). Yu et al. (2018) proposed a shift-reduce parser b"
2021.naacl-main.127,J15-3002,0,0.0619835,"Missing"
2021.naacl-main.127,W04-3250,0,0.109255,"ur model is statistically significantly better than underlined scores at p-level &lt; 0.01 in pairwise comparison.8 cannot handle, by acquiring training instances with the help of the teacher parser. 5.4 5.3 S 80 Comparison with state-of-the-art parsers Detailed Analysis of Relation Labeling Finally, we compare our SBP+AST with the enTo investigate the effectiveness of SBP+AST in semble to current state-of-the-art parsers. Table more detail, we show Relation F1 scores for re- 3 shows the micro-averaged F scores. We used 1 lation labels with SBP, SBP+AST, and the two- Paired Bootstrap Resampling (Koehn, 2004) for stage parser in Figure 4. The results of SBP and the significance test. We can see that our method SBP+AST were obtained from a five-model ensem- achieved the best scores except for Span. The gains ble. In most relation labels, since the two-stage against the previous best scores were 0.4, 3.0, and parser, the teacher parser, is comparable or supe- 2.7 points for Nuclearity, Relation, and Full, rerior to SBP, i.e., the student parser, the performance spectively. In particular, the gains for Relation and of SBP+AST can be improved. It finally outper- Full are remarkable. formed the two-sta"
2021.naacl-main.127,D14-1220,0,0.375346,"tion for two adjacent spans by solving an 18-class classification problem (see Section 3.3 for details). However, it is costly to annotate RST trees for a huge collection of documents, and thus it is difficult to obtain a large amount of human-annotated data for RST parsing. As a result, research on RST parsing has focused on English, with the largest annotated corpus being the RST Discourse Treebank (RST-DT) (Carlson et al., 2001), although even this is still small with only 385 documents.1 Many RST parsing methods have recently been developed based on neural models (Ji and Eisenstein, 2014; Li et al., 2014a, 2016; Liu and Lapata, 2017; Braud et al., 2016, 2017). Among them, Kobayashi et al. (2020) is the current state-of-theart system and is based on the neural top-down method. While its Span and Nuclearity scores achieved the highest level, its Relation score still has room for improvement. One of the reasons for its poor Relation score might be its small amount of training data for solving the 18-class classification problem. Currently, we can refer to various studies on improving neural models for NLP tasks through acquiring large-scale synthetic training data, sometimes called silver data."
2021.naacl-main.127,D16-1035,0,0.0452543,"Missing"
2021.naacl-main.127,P14-1043,0,0.253332,"tion for two adjacent spans by solving an 18-class classification problem (see Section 3.3 for details). However, it is costly to annotate RST trees for a huge collection of documents, and thus it is difficult to obtain a large amount of human-annotated data for RST parsing. As a result, research on RST parsing has focused on English, with the largest annotated corpus being the RST Discourse Treebank (RST-DT) (Carlson et al., 2001), although even this is still small with only 385 documents.1 Many RST parsing methods have recently been developed based on neural models (Ji and Eisenstein, 2014; Li et al., 2014a, 2016; Liu and Lapata, 2017; Braud et al., 2016, 2017). Among them, Kobayashi et al. (2020) is the current state-of-theart system and is based on the neural top-down method. While its Span and Nuclearity scores achieved the highest level, its Relation score still has room for improvement. One of the reasons for its poor Relation score might be its small amount of training data for solving the 18-class classification problem. Currently, we can refer to various studies on improving neural models for NLP tasks through acquiring large-scale synthetic training data, sometimes called silver data."
2021.naacl-main.127,P19-1410,0,0.06264,"y using a state-of-the-art RST parser. To obtain high-quality silver data, we extract agreement subtrees from RST trees for documents built using the RST parsers. We then pre-train a neural RST parser with the obtained silver data and fine-tune it on the RST-DT. Experimental results show that our method achieved the best micro-F1 scores for Nuclearity and Relation at 75.0 and 63.2, respectively. Furthermore, we obtained a remarkable gain in the Relation score, 3.0 points, against the previous state-of-the-art parser. 1 Introduction (Wang et al., 2017b; Yu et al., 2018; Kobayashi et al., 2020; Lin et al., 2019; Zhang et al., 2020), which require a high-quality annotated corpus of sufficient size. Generally, they train the following three components of the RST parsing: (1) structure prediction by splitting a text span consisting of contiguous EDUs into two smaller ones or merging two adjacent spans into a larger one, (2) nuclearity status prediction for two adjacent spans by solving a 3-class classification problem, and (3) relation label prediction for two adjacent spans by solving an 18-class classification problem (see Section 3.3 for details). However, it is costly to annotate RST trees for a hu"
2021.naacl-main.127,D17-1133,0,0.0881458,"ans by solving an 18-class classification problem (see Section 3.3 for details). However, it is costly to annotate RST trees for a huge collection of documents, and thus it is difficult to obtain a large amount of human-annotated data for RST parsing. As a result, research on RST parsing has focused on English, with the largest annotated corpus being the RST Discourse Treebank (RST-DT) (Carlson et al., 2001), although even this is still small with only 385 documents.1 Many RST parsing methods have recently been developed based on neural models (Ji and Eisenstein, 2014; Li et al., 2014a, 2016; Liu and Lapata, 2017; Braud et al., 2016, 2017). Among them, Kobayashi et al. (2020) is the current state-of-theart system and is based on the neural top-down method. While its Span and Nuclearity scores achieved the highest level, its Relation score still has room for improvement. One of the reasons for its poor Relation score might be its small amount of training data for solving the 18-class classification problem. Currently, we can refer to various studies on improving neural models for NLP tasks through acquiring large-scale synthetic training data, sometimes called silver data. Among them, one of the studie"
2021.naacl-main.127,N06-1020,0,0.169545,"fferent parsers that rely on different parsing algorithms is that we can acquire instances that the student parser cannot correctly parse yet the teacher parser can parse as the training data. Second, using multiple different RST parsers in a semi-supervised manner in our work might seem reminiscent of co- or tritraining. While co- or tri-training is attractive, it is time consuming to repeat the step of alternately training multiple different neural network-based parsers many times. Thus, previous studies have focused on simplifying the repetition step in constituency and dependency parsing (McClosky et al., 2006; Yu et al., 2015; Pekar et al., 2014; Weiss et al., 2015; Li et al., 2014b). We believe our method is similar to these simplified version as a semi-supervised framework with two different RST parsers. adopt a simple pre-training and fine-tuning strategy, which is inspired by the NMT research (Sennrich et al., 2016), to train a student RST parser. Since early statistical RST parsing methods relied on handcrafted features, i.e., sentence-level features obtained from parse trees and documentlevel features, they require complete documents with complete sentences for their feature extraction. On t"
2021.naacl-main.127,D17-1136,0,0.0133782,"trees and evaluated system results with RST-DT; this setting is without any silver data. micro-averaged F1 scores of Span, Nuclearity, ReWith AST as the silver data, performance in all lation, and Full, based on RST-Parseval (Marcu, metrics improved against the baseline. In most met2000). Span, Nuclearity, Relation, and Full were rics, AST achieved the best scores. In particular, used to evaluate unlabeled, nuclearity-labeled, the gains in Relation and Full were impressive. DT relation-labeled, and fully-labeled tree structures, and ADT, which consist of document-level RST respectively. Since Morey et al. (2017) made a trees, also outperformed the baseline. However, the suggestion to use a standard parseEval toolkit for gains against the baseline were smaller than those evaluation, we also report the results using this in by AST. We believe this is related to the size and Appendix C. quality of the silver data. The number of trees and nodes in ADT is only 2,142 and 57,940, respec4.4 Compared Methods tively, while AST has 175,709 trees and 2,279,275 To demonstrate the effectiveness of our proposed nodes. Thus, a small number of silver data for method, we pre-trained the span-based neural top- pre-trai"
2021.naacl-main.127,2020.tacl-1.15,0,0.0140569,"le-Instance Learning network, which was trained with a review dataset. However, these studies need other annotated corpora than the RST-DT, which means we still face the problem of being dependent on costly annotated corpora. Jiang et al. (2016) proposed a framework for enriching training data based on co-training to improve the performance for infrequent relation labels. However, the method failed to improve the overall Relation score, while they did not aim at improving the Span and Nuclearity scores. Unsupervised RST parsing methods have also been proposed recently (Kobayashi et al., 2019; Nishida and Nakayama, 2020). Since they are unsupervised, they do not require any annotated corpora. However, they can predict only tree structures and 1601 2 Nguyen et al. (2020) proposed a similar approach in NMT and introduced a method named data diversification: it diversifies the training data by using multiple forward and backward translation models. We can find some weak supervision approaches for other discourse representation formalisms such as (Badene et al., 2019). Teacher parsers Autoparsed trees Ptch=1 Ptch=k Training Unlabeled Documents CNN … … Gold data: RST-DT Autoparsed trees Silver data: Agreement subt"
2021.naacl-main.127,W14-6105,0,0.0180731,"arsing algorithms is that we can acquire instances that the student parser cannot correctly parse yet the teacher parser can parse as the training data. Second, using multiple different RST parsers in a semi-supervised manner in our work might seem reminiscent of co- or tritraining. While co- or tri-training is attractive, it is time consuming to repeat the step of alternately training multiple different neural network-based parsers many times. Thus, previous studies have focused on simplifying the repetition step in constituency and dependency parsing (McClosky et al., 2006; Yu et al., 2015; Pekar et al., 2014; Weiss et al., 2015; Li et al., 2014b). We believe our method is similar to these simplified version as a semi-supervised framework with two different RST parsers. adopt a simple pre-training and fine-tuning strategy, which is inspired by the NMT research (Sennrich et al., 2016), to train a student RST parser. Since early statistical RST parsing methods relied on handcrafted features, i.e., sentence-level features obtained from parse trees and documentlevel features, they require complete documents with complete sentences for their feature extraction. On the other hand, recent neural models d"
2021.naacl-main.127,W05-1513,0,0.0873679,"Missing"
2021.naacl-main.127,P16-1009,0,0.215174,"e attribution of a text span, i.e., nucleus (N) or satellite (S). A discourse relation is also assigned between two adjacent non-terminal nodes. 1 We can find some exceptions for other languages such In most cases, RST parsers have been devel- as Spanish (da Cunha et al., 2011) and German (Stede and oped on the basis of supervised learning algorithms Neumann, 2014). 1600 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1600–1612 June 6–11, 2021. ©2021 Association for Computational Linguistics (Sennrich et al., 2016) introduced a simple learning framework: first pre-train an NMT model with silver data, i.e., pseudo-parallel data generated by automatic back-translation, and then fine-tune it with gold data, i.e., real parallel data, to overcome the data sparseness problem. Since the frameworks successfully improved the NMT systems, it has become a standard approach. Inspired by the above research, we propose a method for improving a student neural parser by exploiting large-scale silver data, thus generating RST trees using an automatic RST parser.2 Specifically, we improve the state-of-the-art neural RST"
2021.naacl-main.127,P17-2029,0,0.258284,"We create large-scale silver data from an unlabeled corpus by using a state-of-the-art RST parser. To obtain high-quality silver data, we extract agreement subtrees from RST trees for documents built using the RST parsers. We then pre-train a neural RST parser with the obtained silver data and fine-tune it on the RST-DT. Experimental results show that our method achieved the best micro-F1 scores for Nuclearity and Relation at 75.0 and 63.2, respectively. Furthermore, we obtained a remarkable gain in the Relation score, 3.0 points, against the previous state-of-the-art parser. 1 Introduction (Wang et al., 2017b; Yu et al., 2018; Kobayashi et al., 2020; Lin et al., 2019; Zhang et al., 2020), which require a high-quality annotated corpus of sufficient size. Generally, they train the following three components of the RST parsing: (1) structure prediction by splitting a text span consisting of contiguous EDUs into two smaller ones or merging two adjacent spans into a larger one, (2) nuclearity status prediction for two adjacent spans by solving a 3-class classification problem, and (3) relation label prediction for two adjacent spans by solving an 18-class classification problem (see Section 3.3 for de"
2021.naacl-main.127,D18-1116,0,0.0479865,"Missing"
2021.naacl-main.127,P15-1032,0,0.0260396,"that we can acquire instances that the student parser cannot correctly parse yet the teacher parser can parse as the training data. Second, using multiple different RST parsers in a semi-supervised manner in our work might seem reminiscent of co- or tritraining. While co- or tri-training is attractive, it is time consuming to repeat the step of alternately training multiple different neural network-based parsers many times. Thus, previous studies have focused on simplifying the repetition step in constituency and dependency parsing (McClosky et al., 2006; Yu et al., 2015; Pekar et al., 2014; Weiss et al., 2015; Li et al., 2014b). We believe our method is similar to these simplified version as a semi-supervised framework with two different RST parsers. adopt a simple pre-training and fine-tuning strategy, which is inspired by the NMT research (Sennrich et al., 2016), to train a student RST parser. Since early statistical RST parsing methods relied on handcrafted features, i.e., sentence-level features obtained from parse trees and documentlevel features, they require complete documents with complete sentences for their feature extraction. On the other hand, recent neural models do not necessarily ne"
2021.naacl-main.127,W15-2201,0,0.0218308,"ly on different parsing algorithms is that we can acquire instances that the student parser cannot correctly parse yet the teacher parser can parse as the training data. Second, using multiple different RST parsers in a semi-supervised manner in our work might seem reminiscent of co- or tritraining. While co- or tri-training is attractive, it is time consuming to repeat the step of alternately training multiple different neural network-based parsers many times. Thus, previous studies have focused on simplifying the repetition step in constituency and dependency parsing (McClosky et al., 2006; Yu et al., 2015; Pekar et al., 2014; Weiss et al., 2015; Li et al., 2014b). We believe our method is similar to these simplified version as a semi-supervised framework with two different RST parsers. adopt a simple pre-training and fine-tuning strategy, which is inspired by the NMT research (Sennrich et al., 2016), to train a student RST parser. Since early statistical RST parsing methods relied on handcrafted features, i.e., sentence-level features obtained from parse trees and documentlevel features, they require complete documents with complete sentences for their feature extraction. On the other hand, re"
2021.naacl-main.127,C18-1047,0,0.215639,"le silver data from an unlabeled corpus by using a state-of-the-art RST parser. To obtain high-quality silver data, we extract agreement subtrees from RST trees for documents built using the RST parsers. We then pre-train a neural RST parser with the obtained silver data and fine-tune it on the RST-DT. Experimental results show that our method achieved the best micro-F1 scores for Nuclearity and Relation at 75.0 and 63.2, respectively. Furthermore, we obtained a remarkable gain in the Relation score, 3.0 points, against the previous state-of-the-art parser. 1 Introduction (Wang et al., 2017b; Yu et al., 2018; Kobayashi et al., 2020; Lin et al., 2019; Zhang et al., 2020), which require a high-quality annotated corpus of sufficient size. Generally, they train the following three components of the RST parsing: (1) structure prediction by splitting a text span consisting of contiguous EDUs into two smaller ones or merging two adjacent spans into a larger one, (2) nuclearity status prediction for two adjacent spans by solving a 3-class classification problem, and (3) relation label prediction for two adjacent spans by solving an 18-class classification problem (see Section 3.3 for details). However, i"
2021.naacl-main.127,2020.acl-main.569,0,0.0432201,"-the-art RST parser. To obtain high-quality silver data, we extract agreement subtrees from RST trees for documents built using the RST parsers. We then pre-train a neural RST parser with the obtained silver data and fine-tune it on the RST-DT. Experimental results show that our method achieved the best micro-F1 scores for Nuclearity and Relation at 75.0 and 63.2, respectively. Furthermore, we obtained a remarkable gain in the Relation score, 3.0 points, against the previous state-of-the-art parser. 1 Introduction (Wang et al., 2017b; Yu et al., 2018; Kobayashi et al., 2020; Lin et al., 2019; Zhang et al., 2020), which require a high-quality annotated corpus of sufficient size. Generally, they train the following three components of the RST parsing: (1) structure prediction by splitting a text span consisting of contiguous EDUs into two smaller ones or merging two adjacent spans into a larger one, (2) nuclearity status prediction for two adjacent spans by solving a 3-class classification problem, and (3) relation label prediction for two adjacent spans by solving an 18-class classification problem (see Section 3.3 for details). However, it is costly to annotate RST trees for a huge collection of docu"
2021.naacl-main.127,stede-neumann-2014-potsdam,0,0.0608475,"Missing"
2021.naacl-main.127,P17-2041,0,0.347521,"We create large-scale silver data from an unlabeled corpus by using a state-of-the-art RST parser. To obtain high-quality silver data, we extract agreement subtrees from RST trees for documents built using the RST parsers. We then pre-train a neural RST parser with the obtained silver data and fine-tune it on the RST-DT. Experimental results show that our method achieved the best micro-F1 scores for Nuclearity and Relation at 75.0 and 63.2, respectively. Furthermore, we obtained a remarkable gain in the Relation score, 3.0 points, against the previous state-of-the-art parser. 1 Introduction (Wang et al., 2017b; Yu et al., 2018; Kobayashi et al., 2020; Lin et al., 2019; Zhang et al., 2020), which require a high-quality annotated corpus of sufficient size. Generally, they train the following three components of the RST parsing: (1) structure prediction by splitting a text span consisting of contiguous EDUs into two smaller ones or merging two adjacent spans into a larger one, (2) nuclearity status prediction for two adjacent spans by solving a 3-class classification problem, and (3) relation label prediction for two adjacent spans by solving an 18-class classification problem (see Section 3.3 for de"
C10-1038,W09-3401,1,0.867311,"Missing"
C10-1038,C04-1193,1,0.819328,"itself or a ﬁeld of onions, therefore the evaluation is M. One point of judgment, speciﬁcally between T and M, is whether the image is typical or not. With onion, most typical images are similar to (1), (4) and (5). The image (3) may not be typical but is helpful for understanding, and (2) may lead to a misunderstanding if this is the only image shown to the dictionary user. This is why (3) is judged to be M and (2) is judged to be F. We evaluated 200 target senses for Lexeed, and 100 for Wikipedia.10 8WF&apos; 8WF&apos; 3.2 Experiment: Lexeed In this paper, we expand queries using the Hinoki Ontology (Bond et al., 2004), which includes related words extracted from the deﬁnition sentences. Table 2 shows the data for the Hinoki Ontology. For SYN, we expand queries using synonyms, abbreviations, other names in Table 2, and vari10 We Lexeed, Example Related Word Lemma performed an image search in September 2009 for and in December 2009 for Wikipedia. Tù ·ÀÊ homer  baseball 7d sell î ﬁsh meat yy Asia ÊÊ plug outlet  ﬁsh ant spellings found in the dictionary. On the other hand, for LNK, we use all the remaining relations, namely hypernyms, domains, etc. Additionally, we use only normal spellings with no"
C10-1038,P08-1032,0,0.0312082,"stive communication tools especially for children, language learners, speakers of different languages, and people with disabilities such as dyslexia (Mihalcea and Leong, 2008; Goldberg et al., 2009). Additionally, a database of typical images connected to meanings has the potential to ﬁll the gaps between images and meanings (semantic gap). There are many studies which aim to cross the semantic gap (Ide and Yanai, 2009; Smeulders et al., 2000; Barnard et al., 2003) from the point of view of image recognition. However the semantic classes of target images are limited (e.g. Caltech-101, 2561 ). Yansong and Lapata (2008) tried to construct image databases annotated with keywords from Web news images with their captions and articles, though the semantic coverage is 1 http://www.vision.caltech.edu/Image 256/ unknown. In this paper, we aim to supply several suitable images for dictionary deﬁnitions. We propose a simple but effective method based on an Internet image search. There have been several studies related to supplying images for a dictionary or thesaurus. Bond et al. (2009) applied images obtained from the Open Clip Art Library (OCAL) to Japanese WordNet.2 They obtained candidate images by comparing the"
C12-2084,J96-1002,0,0.0357184,"rtion-limit for Moses, therefore we chose the edit distance to be smaller than the distortion-limit. 866 4.1 Tools and experimental data We used Moses 2010-08-13 5 with default parameters as a decoder and GIZA++ 1.0.5 6 as an alignment tool to implement an error correction system with phrase-based SMT. We applied growdiag-ﬁnal-and (Och and Ney, 2003) heuristics for phrase extraction. The number of extracted phrases are 1,050,070 (245 MB) using all data of Lang-8 Corpus. We used 3-gram as a language model trained on the corrected text of Lang-8 Corpus. Next, we built the maximum entropy model (Berger et al., 1996) as a multi-class classiﬁer baseline for preposition error correction (Sakaguchi et al., 2012). We used the implementation of Maximum Entropy Modeling Toolkit 7 with its default parameters. We incorporated surface, POS, WordNet, parse and language model features described in (Tetreault et al., 2010) and (De Felice and Pulman, 2008). POS and parse features were extracted using the Stanford Parser 2.0.2. This system achieves recall of 18.44, precision of 34.88 and F-measure of 24.12 trained and tested on the CLC FCE dataset (Yannakoudakis et al., 2011), which ranked the 4th out of 13 systems at"
C12-2084,P06-1032,0,0.289136,"in preposition error correction when we trained a phrase-based SMT system on a small learner corpus. However, in this work, we exploit a large scale error-annotated corpus extracted from the web to overcome the data sparseness problem. 3 3.1 Using a large scale learner corpus with phrase-based SMT for grammatical error correction Error correction with phrase-based SMT We use phrase-based statistical machine translation (Koehn et al., 2003) to conduct unrestricted error correction. There are several studies about grammatical error correction using phrase-based statistical machine translation (Brockett et al., 2006; Mizumoto et al., 2011; Ehsan and Faili, 2012). Although Brockett et al. (2006) corrected English learners’ error using phrase-based statistical machine translation, they only targeted mass noun errors. Mizumoto et al. (2011) dealt with un865 restricted types of learners’ errors, but their target is not English but Japanese. Ehsan and Faili (2012) applied an SMT framework to English and Persian grammatical error correction, but used artiﬁcially created learner corpora. The well-known statistical machine translation formulation using a log-linear model (Och and Ney, 2002) is deﬁned by: eˆ = ar"
C12-2084,P11-1092,0,0.0143922,"ount for the most errors, but it should be noted that there are many different types of errors in learner corpus. Thus far, a lot of studies have been made on automated error correction in regard to errors ESL learners make. However, most previous studies of second language learning deal with one or a few restricted types of learners’ errors. For example, there are studies on preposition errors (Rozovskaya and Roth, 2011), verb selection errors (Liu et al., 2011), tense errors (Tajiri et al., 2012), verb form errors (agreement and tense) (Lee and Seneff, 2008), preposition and article errors (Dahlmeier and Ng, 2011) and spelling, article, preposition and word form (agreement and tense) errors (Park and Levy, 2011). Recently, Swanson and Yamangil (2012) presented a detailed analysis on correcting all types of errors in the Cambridge Learner Corpus, but their task is different from the others in that their goal is to detect errors and select error types given both the original and corrected text, which is not often available in practice. Some types of errors like agreement errors can be corrected by simple rules using heuristics, while others like preposition errors are difﬁcult to correct without statisti"
C12-2084,D12-1052,0,0.085074,"rror-tagged corpus annotated by the wisdom of crowds. In addition, they targeted only spelling, article, preposition and word form errors, while we do not restrict error types. Third, Han et al. (2010) developed a preposition correction system using a large scale error-tagged corpus of learner English. They built a maximum entropy-based model for preposition errors trained on learner and native corpora. We also take advantage of a large scale error-tagged corpus of learner English, but use phrase-based SMT to deal with various kinds of errors and to fully exploit the learner corpus. Recently, Dahlmeier and Ng (2012) presented a beam-search decoder for correcting spelling, article, preposition, punctuation and noun number errors. They reported that their discriminative model achieves considerably better results than an SMT baseline trained on a few hundreds of sentences. As we will see later, we observed a similar tendency in preposition error correction when we trained a phrase-based SMT system on a small learner corpus. However, in this work, we exploit a large scale error-annotated corpus extracted from the web to overcome the data sparseness problem. 3 3.1 Using a large scale learner corpus with phras"
C12-2084,W12-2006,0,0.0189159,"r baseline for preposition error correction (Sakaguchi et al., 2012). We used the implementation of Maximum Entropy Modeling Toolkit 7 with its default parameters. We incorporated surface, POS, WordNet, parse and language model features described in (Tetreault et al., 2010) and (De Felice and Pulman, 2008). POS and parse features were extracted using the Stanford Parser 2.0.2. This system achieves recall of 18.44, precision of 34.88 and F-measure of 24.12 trained and tested on the CLC FCE dataset (Yannakoudakis et al., 2011), which ranked the 4th out of 13 systems at the HOO 2012 Shared Task (Dale et al., 2012). We use KJ Corpus as a test data. KJ Corpus consist of 170 essays, containing 2,411 sentences. When we experiment on a system using KJ Corpus, we perform 5-fold cross validation. 4.2 Evaluation metrics For the evaluation metrics, we use automatic evaluation criteria. To be precise, we use recall, precision and F-measure. Recall and precision for each type of errors are calculated from true positive, false positive and false negative based on error tags in KJ Corpus. The word which does not have any tag in KJ Corpus does not affect precision for each type of errors 8 . For example, let us cons"
C12-2084,C08-1022,0,0.0206406,"Missing"
C12-2084,han-etal-2010-using,0,0.0387109,"to emphasize that our work is the ﬁrst attempt to use a real world large learner corpus with phrase-based SMT technique. We will show that phrase-based SMT especially suffers from data sparseness. Second, Park and Levy (2011) attempted to correct various kinds of errors with a noisy channel model using a large scale unannotated corpus of learner English. Ours differs from their work in that we use a large scale error-tagged corpus annotated by the wisdom of crowds. In addition, they targeted only spelling, article, preposition and word form errors, while we do not restrict error types. Third, Han et al. (2010) developed a preposition correction system using a large scale error-tagged corpus of learner English. They built a maximum entropy-based model for preposition errors trained on learner and native corpora. We also take advantage of a large scale error-tagged corpus of learner English, but use phrase-based SMT to deal with various kinds of errors and to fully exploit the learner corpus. Recently, Dahlmeier and Ng (2012) presented a beam-search decoder for correcting spelling, article, preposition, punctuation and noun number errors. They reported that their discriminative model achieves conside"
C12-2084,N03-1017,0,0.023552,"eir discriminative model achieves considerably better results than an SMT baseline trained on a few hundreds of sentences. As we will see later, we observed a similar tendency in preposition error correction when we trained a phrase-based SMT system on a small learner corpus. However, in this work, we exploit a large scale error-annotated corpus extracted from the web to overcome the data sparseness problem. 3 3.1 Using a large scale learner corpus with phrase-based SMT for grammatical error correction Error correction with phrase-based SMT We use phrase-based statistical machine translation (Koehn et al., 2003) to conduct unrestricted error correction. There are several studies about grammatical error correction using phrase-based statistical machine translation (Brockett et al., 2006; Mizumoto et al., 2011; Ehsan and Faili, 2012). Although Brockett et al. (2006) corrected English learners’ error using phrase-based statistical machine translation, they only targeted mass noun errors. Mizumoto et al. (2011) dealt with un865 restricted types of learners’ errors, but their target is not English but Japanese. Ehsan and Faili (2012) applied an SMT framework to English and Persian grammatical error correc"
C12-2084,P08-1021,0,0.0105552,"It is not surprising that frequent types of errors account for the most errors, but it should be noted that there are many different types of errors in learner corpus. Thus far, a lot of studies have been made on automated error correction in regard to errors ESL learners make. However, most previous studies of second language learning deal with one or a few restricted types of learners’ errors. For example, there are studies on preposition errors (Rozovskaya and Roth, 2011), verb selection errors (Liu et al., 2011), tense errors (Tajiri et al., 2012), verb form errors (agreement and tense) (Lee and Seneff, 2008), preposition and article errors (Dahlmeier and Ng, 2011) and spelling, article, preposition and word form (agreement and tense) errors (Park and Levy, 2011). Recently, Swanson and Yamangil (2012) presented a detailed analysis on correcting all types of errors in the Cambridge Learner Corpus, but their task is different from the others in that their goal is to detect errors and select error types given both the original and corrected text, which is not often available in practice. Some types of errors like agreement errors can be corrected by simple rules using heuristics, while others like pr"
C12-2084,I11-1017,1,0.927515,"orrection when we trained a phrase-based SMT system on a small learner corpus. However, in this work, we exploit a large scale error-annotated corpus extracted from the web to overcome the data sparseness problem. 3 3.1 Using a large scale learner corpus with phrase-based SMT for grammatical error correction Error correction with phrase-based SMT We use phrase-based statistical machine translation (Koehn et al., 2003) to conduct unrestricted error correction. There are several studies about grammatical error correction using phrase-based statistical machine translation (Brockett et al., 2006; Mizumoto et al., 2011; Ehsan and Faili, 2012). Although Brockett et al. (2006) corrected English learners’ error using phrase-based statistical machine translation, they only targeted mass noun errors. Mizumoto et al. (2011) dealt with un865 restricted types of learners’ errors, but their target is not English but Japanese. Ehsan and Faili (2012) applied an SMT framework to English and Persian grammatical error correction, but used artiﬁcially created learner corpora. The well-known statistical machine translation formulation using a log-linear model (Och and Ney, 2002) is deﬁned by: eˆ = arg max P(e |f ) = arg ma"
C12-2084,P02-1038,0,0.0170949,"chine translation (Brockett et al., 2006; Mizumoto et al., 2011; Ehsan and Faili, 2012). Although Brockett et al. (2006) corrected English learners’ error using phrase-based statistical machine translation, they only targeted mass noun errors. Mizumoto et al. (2011) dealt with un865 restricted types of learners’ errors, but their target is not English but Japanese. Ehsan and Faili (2012) applied an SMT framework to English and Persian grammatical error correction, but used artiﬁcially created learner corpora. The well-known statistical machine translation formulation using a log-linear model (Och and Ney, 2002) is deﬁned by: eˆ = arg max P(e |f ) = arg max e e M ∑ λm hm (e, f ) (1) m=1 where e represents target sentences (corrected sentences) and f represents source sentences (sentences written by learners). hm (e, f ) is a feature function and λm is a model parameter for each feature function. This formulation ﬁnds a target sentence e that maximizes a weighted linear combination of feature functions for source sentence f . A translation model and a language model can be used as feature functions. The translation model is commonly represented as conditional probability P( f |e) factored into the tra"
C12-2084,J03-1002,0,0.0251172,"the effect of error correction methods, we also experimented on the preposition error correction task using a maximum entropy model as a discriminative baseline and SMT-based models as our proposal for all error correction. 3 http://lang-8.com/ 4 We use 6 as a distortion-limit for Moses, therefore we chose the edit distance to be smaller than the distortion-limit. 866 4.1 Tools and experimental data We used Moses 2010-08-13 5 with default parameters as a decoder and GIZA++ 1.0.5 6 as an alignment tool to implement an error correction system with phrase-based SMT. We applied growdiag-ﬁnal-and (Och and Ney, 2003) heuristics for phrase extraction. The number of extracted phrases are 1,050,070 (245 MB) using all data of Lang-8 Corpus. We used 3-gram as a language model trained on the corrected text of Lang-8 Corpus. Next, we built the maximum entropy model (Berger et al., 1996) as a multi-class classiﬁer baseline for preposition error correction (Sakaguchi et al., 2012). We used the implementation of Maximum Entropy Modeling Toolkit 7 with its default parameters. We incorporated surface, POS, WordNet, parse and language model features described in (Tetreault et al., 2010) and (De Felice and Pulman, 2008"
C12-2084,P11-1094,0,0.0394033,"er corpus. Thus far, a lot of studies have been made on automated error correction in regard to errors ESL learners make. However, most previous studies of second language learning deal with one or a few restricted types of learners’ errors. For example, there are studies on preposition errors (Rozovskaya and Roth, 2011), verb selection errors (Liu et al., 2011), tense errors (Tajiri et al., 2012), verb form errors (agreement and tense) (Lee and Seneff, 2008), preposition and article errors (Dahlmeier and Ng, 2011) and spelling, article, preposition and word form (agreement and tense) errors (Park and Levy, 2011). Recently, Swanson and Yamangil (2012) presented a detailed analysis on correcting all types of errors in the Cambridge Learner Corpus, but their task is different from the others in that their goal is to detect errors and select error types given both the original and corrected text, which is not often available in practice. Some types of errors like agreement errors can be corrected by simple rules using heuristics, while others like preposition errors are difﬁcult to correct without statistical model trained on native corpora and/or learner corpora. It was not until recently that large sca"
C12-2084,P11-1093,0,0.0268429,"s the distribution of errors found in KJ Corpus2 . The most frequent error type is article errors, followed by noun number and preposition errors. It is not surprising that frequent types of errors account for the most errors, but it should be noted that there are many different types of errors in learner corpus. Thus far, a lot of studies have been made on automated error correction in regard to errors ESL learners make. However, most previous studies of second language learning deal with one or a few restricted types of learners’ errors. For example, there are studies on preposition errors (Rozovskaya and Roth, 2011), verb selection errors (Liu et al., 2011), tense errors (Tajiri et al., 2012), verb form errors (agreement and tense) (Lee and Seneff, 2008), preposition and article errors (Dahlmeier and Ng, 2011) and spelling, article, preposition and word form (agreement and tense) errors (Park and Levy, 2011). Recently, Swanson and Yamangil (2012) presented a detailed analysis on correcting all types of errors in the Cambridge Learner Corpus, but their task is different from the others in that their goal is to detect errors and select error types given both the original and corrected text, which is not of"
C12-2084,W12-2033,1,0.810438,"on-limit. 866 4.1 Tools and experimental data We used Moses 2010-08-13 5 with default parameters as a decoder and GIZA++ 1.0.5 6 as an alignment tool to implement an error correction system with phrase-based SMT. We applied growdiag-ﬁnal-and (Och and Ney, 2003) heuristics for phrase extraction. The number of extracted phrases are 1,050,070 (245 MB) using all data of Lang-8 Corpus. We used 3-gram as a language model trained on the corrected text of Lang-8 Corpus. Next, we built the maximum entropy model (Berger et al., 1996) as a multi-class classiﬁer baseline for preposition error correction (Sakaguchi et al., 2012). We used the implementation of Maximum Entropy Modeling Toolkit 7 with its default parameters. We incorporated surface, POS, WordNet, parse and language model features described in (Tetreault et al., 2010) and (De Felice and Pulman, 2008). POS and parse features were extracted using the Stanford Parser 2.0.2. This system achieves recall of 18.44, precision of 34.88 and F-measure of 24.12 trained and tested on the CLC FCE dataset (Yannakoudakis et al., 2011), which ranked the 4th out of 13 systems at the HOO 2012 Shared Task (Dale et al., 2012). We use KJ Corpus as a test data. KJ Corpus consi"
C12-2084,N12-1037,0,0.0216011,"udies have been made on automated error correction in regard to errors ESL learners make. However, most previous studies of second language learning deal with one or a few restricted types of learners’ errors. For example, there are studies on preposition errors (Rozovskaya and Roth, 2011), verb selection errors (Liu et al., 2011), tense errors (Tajiri et al., 2012), verb form errors (agreement and tense) (Lee and Seneff, 2008), preposition and article errors (Dahlmeier and Ng, 2011) and spelling, article, preposition and word form (agreement and tense) errors (Park and Levy, 2011). Recently, Swanson and Yamangil (2012) presented a detailed analysis on correcting all types of errors in the Cambridge Learner Corpus, but their task is different from the others in that their goal is to detect errors and select error types given both the original and corrected text, which is not often available in practice. Some types of errors like agreement errors can be corrected by simple rules using heuristics, while others like preposition errors are difﬁcult to correct without statistical model trained on native corpora and/or learner corpora. It was not until recently that large scale learner corpora became widely availa"
C12-2084,P12-2039,1,0.760617,"rticle errors, followed by noun number and preposition errors. It is not surprising that frequent types of errors account for the most errors, but it should be noted that there are many different types of errors in learner corpus. Thus far, a lot of studies have been made on automated error correction in regard to errors ESL learners make. However, most previous studies of second language learning deal with one or a few restricted types of learners’ errors. For example, there are studies on preposition errors (Rozovskaya and Roth, 2011), verb selection errors (Liu et al., 2011), tense errors (Tajiri et al., 2012), verb form errors (agreement and tense) (Lee and Seneff, 2008), preposition and article errors (Dahlmeier and Ng, 2011) and spelling, article, preposition and word form (agreement and tense) errors (Park and Levy, 2011). Recently, Swanson and Yamangil (2012) presented a detailed analysis on correcting all types of errors in the Cambridge Learner Corpus, but their task is different from the others in that their goal is to detect errors and select error types given both the original and corrected text, which is not often available in practice. Some types of errors like agreement errors can be c"
C12-2084,P10-2065,0,0.0129726,"d SMT. We applied growdiag-ﬁnal-and (Och and Ney, 2003) heuristics for phrase extraction. The number of extracted phrases are 1,050,070 (245 MB) using all data of Lang-8 Corpus. We used 3-gram as a language model trained on the corrected text of Lang-8 Corpus. Next, we built the maximum entropy model (Berger et al., 1996) as a multi-class classiﬁer baseline for preposition error correction (Sakaguchi et al., 2012). We used the implementation of Maximum Entropy Modeling Toolkit 7 with its default parameters. We incorporated surface, POS, WordNet, parse and language model features described in (Tetreault et al., 2010) and (De Felice and Pulman, 2008). POS and parse features were extracted using the Stanford Parser 2.0.2. This system achieves recall of 18.44, precision of 34.88 and F-measure of 24.12 trained and tested on the CLC FCE dataset (Yannakoudakis et al., 2011), which ranked the 4th out of 13 systems at the HOO 2012 Shared Task (Dale et al., 2012). We use KJ Corpus as a test data. KJ Corpus consist of 170 essays, containing 2,411 sentences. When we experiment on a system using KJ Corpus, we perform 5-fold cross validation. 4.2 Evaluation metrics For the evaluation metrics, we use automatic evaluati"
C12-2084,P11-1019,0,0.0348522,"Corpus. Next, we built the maximum entropy model (Berger et al., 1996) as a multi-class classiﬁer baseline for preposition error correction (Sakaguchi et al., 2012). We used the implementation of Maximum Entropy Modeling Toolkit 7 with its default parameters. We incorporated surface, POS, WordNet, parse and language model features described in (Tetreault et al., 2010) and (De Felice and Pulman, 2008). POS and parse features were extracted using the Stanford Parser 2.0.2. This system achieves recall of 18.44, precision of 34.88 and F-measure of 24.12 trained and tested on the CLC FCE dataset (Yannakoudakis et al., 2011), which ranked the 4th out of 13 systems at the HOO 2012 Shared Task (Dale et al., 2012). We use KJ Corpus as a test data. KJ Corpus consist of 170 essays, containing 2,411 sentences. When we experiment on a system using KJ Corpus, we perform 5-fold cross validation. 4.2 Evaluation metrics For the evaluation metrics, we use automatic evaluation criteria. To be precise, we use recall, precision and F-measure. Recall and precision for each type of errors are calculated from true positive, false positive and false negative based on error tags in KJ Corpus. The word which does not have any tag in"
C16-1021,J08-1001,0,0.801524,"nguages without much efforts. It also provides inspiration as regards other tasks that require computers to generate coherent text. The rest of the papers is organized as follows: Section 2 presents the centering theory and a coherence model based on entities. Section 3 presents our model. Section 4 describes the experiments and results. Section 5 presents some previous work and Section 6 concludes this paper. 2 Centering Theory and Coherence Modelling The centering theory (Grosz et al., 1995) as a popular theory on discourse analysis, serves as the basis of some coherence evaluation methods (Barzilay and Lapata, 2008; Burstein et al., 2010; Li and Hovy, 2014; Li and Jurafsky, 2016) and enables us to measure the coherence score of any given text without discourse parsing solely based on the reappearance of entities. Entities here refer to noun/pronoun 214 word/phrases 2 . According to the centering theory, we have the following assumptions: 1. Text that contains successive mentions of the same entities would be more coherent. 2. The main entities that are focused on tend to play an important grammatical role, such as the subject or object of the sentences. Therefore the key to the coherence of a text lies"
C16-1021,H01-1065,0,0.106884,"to be representative of the corpus, and rewarding diversity separately. Other methods that have been applied to summarization include centroid-based methods (Radev et al., 2004b; Saggion and Gaizauskas, 2004), and minimum dominating set methods (Shen and Li, 2010). All these methods suffer in coherence. 5.2 Coherence-Focused Method Sentence reordering methods are developed to correct the salience-focused models. Sentence reordering tries to generate a more coherent text by reordering its contents. Rich semantic and syntactic features are used to find a better permutation for input sentences (Barzilay et al., 2001; Bollegala et al., 2010; Okazaki et al., 2004). The drawback to sentence reordering is obvious. The preceding sentence selection focuses solely on informativeness and totally neglects coherence. Thus it prevents the improvements expected from permutation. This is confirmed by the fact that the above methods all reports limited improvement. A consideration of coherence during sentence selection leads to new methods, and these are mainly discourse driven models. Some of the summarization methods encode discourse analysis results in feature presentations together with other frequency based featu"
C16-1021,N10-1099,0,0.0837394,"s. It also provides inspiration as regards other tasks that require computers to generate coherent text. The rest of the papers is organized as follows: Section 2 presents the centering theory and a coherence model based on entities. Section 3 presents our model. Section 4 describes the experiments and results. Section 5 presents some previous work and Section 6 concludes this paper. 2 Centering Theory and Coherence Modelling The centering theory (Grosz et al., 1995) as a popular theory on discourse analysis, serves as the basis of some coherence evaluation methods (Barzilay and Lapata, 2008; Burstein et al., 2010; Li and Hovy, 2014; Li and Jurafsky, 2016) and enables us to measure the coherence score of any given text without discourse parsing solely based on the reappearance of entities. Entities here refer to noun/pronoun 214 word/phrases 2 . According to the centering theory, we have the following assumptions: 1. Text that contains successive mentions of the same entities would be more coherent. 2. The main entities that are focused on tend to play an important grammatical role, such as the subject or object of the sentences. Therefore the key to the coherence of a text lies in what entities it con"
C16-1021,N13-1136,0,0.275446,"ion. Another kind of widely adopted method takes discourse relations into consideration when selecting sentences, as discourse relations are believed to be essential for maintaining textual coherence. Hirao et al. (2013) formulated single document summarization as to extract a sub tree from the complete discourse tree and thus preserve the relations between extracted document units to form a readable text. Wang et al. (2015) extended it to multi-document summarization by regarding a document set as one document and developed a model which combined discourse parsing and summarization together. Christensen et al. (2013) proposed a graph-based model to bypass the tree constraints. They employed rich textual features to build a discourse relation graph for source documents with the aim of representing the relations between sentences (both inter and intra-document relations). Christensen et al. (2013) reported ROUGE scores lower than some baselines. This is because that, they claim, ROUGE is salience-focused and fails to notice the improvement in coherence. In a further human evaluation, they reported improvements in readability. These discourse-based methods without exception have discourse analysis as a prere"
C16-1021,P02-1057,0,0.0628032,"Missing"
C16-1021,W04-1017,0,0.029919,"of T (Barzilay and Lapata, 2008). To calculate the coherence score of T , Barzilay and Lapata (2008) used M (T ) as a feature vector. They calculated the transition probability for |{s(subj), o(bj), x(others), −(absent)}2 |= 16 transition patterns from M (T ) without distinguishing between entities, to form a vector f (T ) for T , and a weight vector w was then learnt from training data so that w ∗ f (T ) can be used as the coherence score for T . This kind of method has been adopted in many studies (Filippova and Strube, 2007; Barzilay and Lapata, 2008; Burstein et al., 2010). In particular, Filatova and Hatzivassiloglou (2004) extends entity grids to model semantical relations between entities, which provides a possible further improvement for our models. 3 Modeling Summarization The above model can only be used to measure coherence but summarization is much complex as it involves not only coherence bust also informativeness and redundancy. We design a much more sophisticated models leveraging entities. Two models are presented below. Both of them are based on entities and consider coherence as well as informativeness. The first one is based on global coherence and the second one local coherence. The global coheren"
C16-1021,W07-2321,0,0.0377804,"to the grammatical roles of Entity j in Sentence i. M (T ) is referred to as the Entity Grid of T (Barzilay and Lapata, 2008). To calculate the coherence score of T , Barzilay and Lapata (2008) used M (T ) as a feature vector. They calculated the transition probability for |{s(subj), o(bj), x(others), −(absent)}2 |= 16 transition patterns from M (T ) without distinguishing between entities, to form a vector f (T ) for T , and a weight vector w was then learnt from training data so that w ∗ f (T ) can be used as the coherence score for T . This kind of method has been adopted in many studies (Filippova and Strube, 2007; Barzilay and Lapata, 2008; Burstein et al., 2010). In particular, Filatova and Hatzivassiloglou (2004) extends entity grids to model semantical relations between entities, which provides a possible further improvement for our models. 3 Modeling Summarization The above model can only be used to measure coherence but summarization is much complex as it involves not only coherence bust also informativeness and redundancy. We design a much more sophisticated models leveraging entities. Two models are presented below. Both of them are based on entities and consider coherence as well as informativ"
C16-1021,P15-1056,0,0.0147264,"Hence the summarization systems need to identify important information and keep as much of it as possible. Most existing research follows such a guideline and takes salience as its sole focus. Salience-focused systems cannot guarantee the readability of the generated text as they fail to take coherence into consideration. Sentence reordering, as a post processing task has began to develop. Apparently, it cannot make up for the flaws of salience-focused systems because it is simply a reorganization of sentences. Besides, it also faces problems when dealing with temporal text (Yan et al., 2011; Ge et al., 2015). A better solution is to consider coherence when selecting sentences. Such comprehensive models have been proposed. Most of them are discourse driven and sacrifice informativeness for coherence. In this sense, our model is novel in dealing with coherence without discourse analysis. 219 5.1 Salience-Focused Method As stated, the summarization systems need to identify the important information and keep as much of it in the generated summaries as possible. One straightforward method is Maximum Marginal Relevance (Carbonell and Goldstein, 1998) (MMR). It is a greedy method, and is proposed to sel"
C16-1021,W09-1802,0,0.11699,"07; Wang et al., 2012)), which construct a graph between text units and use ranking algorithms to select top sentences to build summaries. Another kind is the optimization method. Our work is one of this kind. It formulates summarization as finding a subset that optimizes certain objective functions without violating certain constraints. To find such an optimal subset is a combinatorial optimization problem, which is an NP hard problem and hence cannot be solved in linear time (McDonald, 2007). Recently, maximum coverage methods have been proposed and yield good results (Gillick et al., 2009; Gillick and Favre, 2009; Takamura and Okumura, 2009). Maximum coverage methods formulate summarization as a maximum knapsack problem (MKMC). In MKMC methods, the meanings of sentences are believed to be made up by concepts, which usually refer to content words. And summarization involves extracting a subset of sentences that covers as many important concepts as possible without violating the length constraint. It is usually formulated as an integer linear problem. And some algorithms are proposed for obtaining approximated solutions (Takamura and Okumura, 2009; Gillick et al., 2009). Lin and Bilmes (2011) design a c"
C16-1021,J95-2003,0,0.89001,"ted improvements in readability. These discourse-based methods without exception have discourse analysis as a prerequisite. As we all know, discourse analysis is still under development thus preventing the expected improvement. Furthermore, languages other than English do not enjoy plenty of ready-to-use discourse analysis tools. This also limits the usage of these discourse-based methods. Is it possible to consider coherence in summarization without discourse analysis? Before answering this question, we need to find out what is the key to coherence in text. According to the centering theory (Grosz et al., 1995; Walker et al., 1998), the coherence of text is to a large extent maintained by entities and the relations between them. This indicates that discourse analysis is not a must to preserve coherence; we can directly take advantages of entities and their relations to generate coherence text. Based on this point, we design a novel graph-based model for multi-document summarization that eliminates the effort of conducting discourse relation analysis (inter or intra document) and generates informative and readable summaries. We formulate the document set as a graph whose nodes corresponds to sentenc"
C16-1021,D13-1158,1,0.937019,"uation (Lin, 2004)). 213 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 213–223, Osaka, Japan, December 11-17 2016. Existing work addresses coherence in summarization from different aspects. One kind of method employs reordering after selecting sentences, and the drawback is evident: coherence is considered after sentence selection. Another kind of widely adopted method takes discourse relations into consideration when selecting sentences, as discourse relations are believed to be essential for maintaining textual coherence. Hirao et al. (2013) formulated single document summarization as to extract a sub tree from the complete discourse tree and thus preserve the relations between extracted document units to form a readable text. Wang et al. (2015) extended it to multi-document summarization by regarding a document set as one document and developed a model which combined discourse parsing and summarization together. Christensen et al. (2013) proposed a graph-based model to bypass the tree constraints. They employed rich textual features to build a discourse relation graph for source documents with the aim of representing the relatio"
C16-1021,hong-etal-2014-repository,0,0.0620989,"multi-document summarization systems using ROUGE and human evaluation. The former aims to evaluate informativeness and the latter targets readability. ROUGE Evaluation MCKP is the maximum coverage methods proposed by Takamura and Okumura (2009). Lin is a model that uses a class of submodular functions (Lin and Bilmes, 2011). Christ is a graph based model proposed by Christensen et al. (2013). DPP is the determinantal point processes model Borodin (2009) and ICSI is another model based on maximum coverage Gillick et al. (2008). The results of DPP and ICSI comes from the repository presented in Hong et al. (2014). M1 is our model described in Section 3.1. M2 is the model described in Section 3.3, which is resolved using an ILP method. MEAD Radev et al. (2004a) is a baseline that employs ranking algorithms to generate multi-document summaries. The results are shown in Table 1. As we can see, our system (M1 and M2) produces comparable results to the state-of-the-art systems. With the MCKP method, all content words are used as concepts. But in 4 http://www-03.ibm.com/software/products/en/ibmilogcpleoptistud/ This method was first proposed by Yih et al. (2007) and then improved by Takamura and Okumura (20"
C16-1021,P14-1002,0,0.0177133,"s viewed summarization as a knapsack problem on trees, and uses an integer linear problem (ILP) to formulate it. A sub tree that maximizes some objective function and obeys some given constraints is extracted from the original parse tree as the summary. Discourse tree based methods cannot be extended to multi-document summarization. Christensen et al. (2013) propose a graph model that bypasses the tree constraints. They build a graph to represent discourse relations between sentences and then extract summaries accordingly. Recently the neural network based discourse analysis (Li et al., 2014; Ji and Eisenstein, 2014) provides 220 us with an alternative way of conducting discourse analysis without traditional feature engineering. It can be used in our future work of modelling coherence using semantic relations. 6 Conclusion Previous summarization methods have usually focused on salience and neglected coherence. This work proposed a novel summarization system that combines coherence with salience. By taking entities and links between them into consideration, our weighted longest path model successfully improves the quality of summaries. The proposed model does not require discourse analysis and hence can be"
C16-1021,D14-1218,0,0.019942,"piration as regards other tasks that require computers to generate coherent text. The rest of the papers is organized as follows: Section 2 presents the centering theory and a coherence model based on entities. Section 3 presents our model. Section 4 describes the experiments and results. Section 5 presents some previous work and Section 6 concludes this paper. 2 Centering Theory and Coherence Modelling The centering theory (Grosz et al., 1995) as a popular theory on discourse analysis, serves as the basis of some coherence evaluation methods (Barzilay and Lapata, 2008; Burstein et al., 2010; Li and Hovy, 2014; Li and Jurafsky, 2016) and enables us to measure the coherence score of any given text without discourse parsing solely based on the reappearance of entities. Entities here refer to noun/pronoun 214 word/phrases 2 . According to the centering theory, we have the following assumptions: 1. Text that contains successive mentions of the same entities would be more coherent. 2. The main entities that are focused on tend to play an important grammatical role, such as the subject or object of the sentences. Therefore the key to the coherence of a text lies in what entities it contains and how their"
C16-1021,P13-2099,0,0.0150176,"d methods face some major challenges. One is informativeness, which means we need to maintain the important information of source documents in summaries. This is the focus of almost all research on summarization. Another challenge is presentation, namely that the extracted text should be well presented, i.e., it should contain little redundancy and be coherent so as to be readily understandable. Previous work has addressed the problem of redundancy, and some successful solutions like Maximum Marginal Relevance (MMR) (Carbonell and Goldstein, 1998) have been proposed and widely adopted (e.g., (Li and Li, 2013)), but very few try to deal with coherence. Therefore the generated summaries generally suffer as regards readability and are very difficult to use for practical applications. In the report of the TAC 2011 summarization task (Owczarzak and Dang, 2011), it is stated that “in general, automatic summaries are better than baselines1 , except Readability.” Such a statement suggests, as for summarization, coherence should be treated with the same as salience and redundancy. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http: //creativecommons."
C16-1021,D14-1220,0,0.0209026,"et al. (2013) has viewed summarization as a knapsack problem on trees, and uses an integer linear problem (ILP) to formulate it. A sub tree that maximizes some objective function and obeys some given constraints is extracted from the original parse tree as the summary. Discourse tree based methods cannot be extended to multi-document summarization. Christensen et al. (2013) propose a graph model that bypasses the tree constraints. They build a graph to represent discourse relations between sentences and then extract summaries accordingly. Recently the neural network based discourse analysis (Li et al., 2014; Ji and Eisenstein, 2014) provides 220 us with an alternative way of conducting discourse analysis without traditional feature engineering. It can be used in our future work of modelling coherence using semantic relations. 6 Conclusion Previous summarization methods have usually focused on salience and neglected coherence. This work proposed a novel summarization system that combines coherence with salience. By taking entities and links between them into consideration, our weighted longest path model successfully improves the quality of summaries. The proposed model does not require discourse"
C16-1021,P11-1052,0,0.0900013,"., 2009; Gillick and Favre, 2009; Takamura and Okumura, 2009). Maximum coverage methods formulate summarization as a maximum knapsack problem (MKMC). In MKMC methods, the meanings of sentences are believed to be made up by concepts, which usually refer to content words. And summarization involves extracting a subset of sentences that covers as many important concepts as possible without violating the length constraint. It is usually formulated as an integer linear problem. And some algorithms are proposed for obtaining approximated solutions (Takamura and Okumura, 2009; Gillick et al., 2009). Lin and Bilmes (2011) design a class of submodular functions for document summarization. The functions they use combine two parts, encouraging the summary to be representative of the corpus, and rewarding diversity separately. Other methods that have been applied to summarization include centroid-based methods (Radev et al., 2004b; Saggion and Gaizauskas, 2004), and minimum dominating set methods (Shen and Li, 2010). All these methods suffer in coherence. 5.2 Coherence-Focused Method Sentence reordering methods are developed to correct the salience-focused models. Sentence reordering tries to generate a more coher"
C16-1021,W04-1013,0,0.0375736,". In the report of the TAC 2011 summarization task (Owczarzak and Dang, 2011), it is stated that “in general, automatic summaries are better than baselines1 , except Readability.” Such a statement suggests, as for summarization, coherence should be treated with the same as salience and redundancy. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http: //creativecommons.org/licenses/by/4.0/ 1 The baseline they used is the lead paragraph method and summaries are evaluated by human and ROUGE (Recall-Oriented Understudy for Gisting Evaluation (Lin, 2004)). 213 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 213–223, Osaka, Japan, December 11-17 2016. Existing work addresses coherence in summarization from different aspects. One kind of method employs reordering after selecting sentences, and the drawback is evident: coherence is considered after sentence selection. Another kind of widely adopted method takes discourse relations into consideration when selecting sentences, as discourse relations are believed to be essential for maintaining textual coherence. Hirao et al. (2013"
C16-1021,P14-5010,0,0.00489084,"Missing"
C16-1021,W98-1124,0,0.0172051,"s for sentence selection/compression. The problem is that these discourse based features usually play secondary roles, because the models all try to improve information coverage, which are evaluated by ROUGE. And ROUGE, as is commonly known, is not sensitive to coherence. Some others work directly on discourse analysis results, and they usually try to derive a passage from a given parse tree. The problem of summarization is regarded as finding a text T so that T = arg max F (T |T r) for a given tree T r. Here F is the objective function. Early representative work of this kind includes that of Marcu (1998) and that of Daum´e III and Marcu (2002). Recently, Hirao et al. (2013) has viewed summarization as a knapsack problem on trees, and uses an integer linear problem (ILP) to formulate it. A sub tree that maximizes some objective function and obeys some given constraints is extracted from the original parse tree as the summary. Discourse tree based methods cannot be extended to multi-document summarization. Christensen et al. (2013) propose a graph model that bypasses the tree constraints. They build a graph to represent discourse relations between sentences and then extract summaries accordingl"
C16-1021,C04-1108,0,0.024334,"ing diversity separately. Other methods that have been applied to summarization include centroid-based methods (Radev et al., 2004b; Saggion and Gaizauskas, 2004), and minimum dominating set methods (Shen and Li, 2010). All these methods suffer in coherence. 5.2 Coherence-Focused Method Sentence reordering methods are developed to correct the salience-focused models. Sentence reordering tries to generate a more coherent text by reordering its contents. Rich semantic and syntactic features are used to find a better permutation for input sentences (Barzilay et al., 2001; Bollegala et al., 2010; Okazaki et al., 2004). The drawback to sentence reordering is obvious. The preceding sentence selection focuses solely on informativeness and totally neglects coherence. Thus it prevents the improvements expected from permutation. This is confirmed by the fact that the above methods all reports limited improvement. A consideration of coherence during sentence selection leads to new methods, and these are mainly discourse driven models. Some of the summarization methods encode discourse analysis results in feature presentations together with other frequency based features for sentence selection/compression. The pro"
C16-1021,radev-etal-2004-mead,0,0.346146,"y. ROUGE Evaluation MCKP is the maximum coverage methods proposed by Takamura and Okumura (2009). Lin is a model that uses a class of submodular functions (Lin and Bilmes, 2011). Christ is a graph based model proposed by Christensen et al. (2013). DPP is the determinantal point processes model Borodin (2009) and ICSI is another model based on maximum coverage Gillick et al. (2008). The results of DPP and ICSI comes from the repository presented in Hong et al. (2014). M1 is our model described in Section 3.1. M2 is the model described in Section 3.3, which is resolved using an ILP method. MEAD Radev et al. (2004a) is a baseline that employs ranking algorithms to generate multi-document summaries. The results are shown in Table 1. As we can see, our system (M1 and M2) produces comparable results to the state-of-the-art systems. With the MCKP method, all content words are used as concepts. But in 4 http://www-03.ibm.com/software/products/en/ibmilogcpleoptistud/ This method was first proposed by Yih et al. (2007) and then improved by Takamura and Okumura (2009). Here we follow the same steps with Takamura and Okumura (2009). 5 218 our systems, only nouns and pronouns are regarded as entities. There are"
C16-1021,C10-1111,0,0.0287562,"he length constraint. It is usually formulated as an integer linear problem. And some algorithms are proposed for obtaining approximated solutions (Takamura and Okumura, 2009; Gillick et al., 2009). Lin and Bilmes (2011) design a class of submodular functions for document summarization. The functions they use combine two parts, encouraging the summary to be representative of the corpus, and rewarding diversity separately. Other methods that have been applied to summarization include centroid-based methods (Radev et al., 2004b; Saggion and Gaizauskas, 2004), and minimum dominating set methods (Shen and Li, 2010). All these methods suffer in coherence. 5.2 Coherence-Focused Method Sentence reordering methods are developed to correct the salience-focused models. Sentence reordering tries to generate a more coherent text by reordering its contents. Rich semantic and syntactic features are used to find a better permutation for input sentences (Barzilay et al., 2001; Bollegala et al., 2010; Okazaki et al., 2004). The drawback to sentence reordering is obvious. The preceding sentence selection focuses solely on informativeness and totally neglects coherence. Thus it prevents the improvements expected from"
C16-1021,E09-1089,0,0.42513,"we assign a cost score, which is the number of words the corresponding sentence contains. To each path in the directed graph, we assign a gain score. The gain score is a comprehensive evaluation of the informativeness and coherence of the sequence of sentences represented by the path. The problem of extracting a good summary becomes the problem of extracting the best path. Note that it is an asymmetric graph. Gain scores for A → B → C and C → B → A are different. The direction determines the positions of corresponding sentences in the generated text. 2 In some previous work on summarization (Takamura and Okumura, 2009; Hirao et al., 2013), concepts are used to measure informativeness. Concepts can be used to refer any non functional words, including adjectives, adverbs. All the entities can be regarded as concepts, but some concept words (non-nominal words) are not entities. Entity is a subset of Concept. 215 One more thing to consider is the redundancy. Instead of formulating redundancy explicitly, we remove edges connecting similar sentences to turn the complete graph into an incomplete graph. This ensures that similar sentences do not occupy adjacent positions in the generated summaries and thus reduce"
C16-1021,P07-1070,0,0.0269097,"ant information and keep as much of it in the generated summaries as possible. One straightforward method is Maximum Marginal Relevance (Carbonell and Goldstein, 1998) (MMR). It is a greedy method, and is proposed to select sentences that are most relevant but not too similar to the already selected ones. It tries to keep a balance between relevance and redundancy. MMR is also widely employed to avoid redundancy in summarization systems. Among existing research, one popular kind is the ranking method (e.g., Textrank (Mihalcea and Tarau, 2004), Lexrank (Erkan and Radev, 2004) and its variants (Wan et al., 2007; Wang et al., 2012)), which construct a graph between text units and use ranking algorithms to select top sentences to build summaries. Another kind is the optimization method. Our work is one of this kind. It formulates summarization as finding a subset that optimizes certain objective functions without violating certain constraints. To find such an optimal subset is a combinatorial optimization problem, which is an NP hard problem and hence cannot be solved in linear time (McDonald, 2007). Recently, maximum coverage methods have been proposed and yield good results (Gillick et al., 2009; Gi"
C16-1021,W04-3252,0,\N,Missing
C18-1052,2012.eamt-1.60,0,0.0152173,"r, increasing the memory requirement and runtime is limited, which allows us to run almost the same speed and memory requirement as the baseline system3 . We can simultaneously use several subword features. In Fig. 1, we use both BPE (m =1k) and BPE (m =300) for the encoder side. By adding several subword features, the model can use more information with which we expect to improve the task performance. 4 Experiments 4.1 Setup In this paper, we focused on from/to English (EN) to/from French (FR), German (DE) translations. We carried out our experiments on the IWSLT evaluation campaign dataset (Cettolo et al., 2012), which is based on a TED talk that has been translated into several languages. We used the IWSLT 2016 training set for the training models, tst2012 as the development set, and tst2013 and tst2014 as the test sets. For preprocessing, we used the Moses tokenizer4 and the truecaser5 . For the training set, we removed sentences over 50 words to clean the corpus. Table 1 shows the cleaned corpora statistics on the IWSLT datasets. To split words into subwords, we used the scripts6 provided by Sennrich et al. (2016). As an NMT framework, we used almost the same structure as Luong et al. (2015), exce"
C18-1052,W16-4616,0,0.0249753,"ture (g) + Encoder side BPE 300 feature (g) + Encoder side BPE 1k, 300 features (a) + Decoder side BPE 1k feature (a) + Decoder side BPE 300 feature (a) + Decoder side BPE 1k, 300 features (f) + (n) (k) + (n) Table 3: Compared experimental settings learning rate to 1.0, but after 30 epochs we multiplied it by 0.8 for every epoch and continued training until 40 epochs. For decoding, we performed a beam search with a beam size of 20. To prevent the model from outputting short sentences, we applied the length normalization technique by dividing the negative log-likelihood by the sentence length (Cromieres et al., 2016; Morishita et al., 2017). As evaluation metrics, we used case-sensitive7 BLEU scores (Papineni et al., 2002) using multi-bleu.perl8 . To fix the experimental settings, we carried out a preliminary analysis to find the relation between the sentence length and the vocabulary size (≃ the number of BPE merge operations). Fig. 2 shows the results on the English sentences of the IWSLT 2016 German-English training set. When we reduce the vocabulary size, the average sentence length rapidly increases. Unfortunately, longer sentences require more computational cost and are time-consuming. Thus in our"
C18-1052,W17-3204,0,0.0348867,"tors, which are shown respectively in Eqs. 2 and 3. For the encoder embedding vectors, we introduce the following operator: $ ei = Eq φq (xi ), (5) q where φq (xi ) is a mapping function that returns a binary vector that corresponds to xi . For the decoder embedding vectors, we obtain fj by the following equation: fj = $ Fr ψr (yj−1 ), (6) r where ψr (yj−1 ) is a mapping function that returns a binary vector that corresponds to previously estimated result yj−1 . For example, if we get record as an estimation result of BPE(m =16k), mapping 1 NMT lacks the ability to translate longer sentences (Koehn and Knowles, 2017). 621 train tst2012 (development) tst2013 (test) tst2014 (test) DE-EN Tokens Sentences 3,496,036 189,318 30,900 1,700 21,037 993 24,950 1,305 FR-EN Tokens Sentences 3,800,613 208,323 21,653 1,124 21,894 1,024 24,950 1,305 Table 1: Cleaned corpora statistics on IWSLT datasets. Number of tokens is English side. Configurations Embedding dimension D Hidden dimension H Attention dimension Encoder layer Decoder layer Values 512 512 512 2 2 Configurations Optimizer Initial learning rate Gradient clipping Dropout rate Mini-batch size Values SGD 1.0 5.0 0.3 128 sent Table 2: Model and optimization conf"
C18-1052,P18-1007,0,0.192089,"618 Proceedings of the 27th International Conference on Computational Linguistics, pages 618–629 Santa Fe, New Mexico, USA, August 20-26, 2018. A subword is a fraction of a word, determined by Byte Pair Encoding (BPE) operations. By the BPE operation, a word that appears frequently can be one unit, and rare or uncommon words can be expressed by the combination of several subword units. Thus we can express any words by the combination of small subword vocabularies to alleviate the out-of-vocabulary problem. Several similar works exist to make subword units (e.g., (Schuster and Nakajima, 2012; Kudo, 2018)), but in the following, we denote the units segmented by BPE as subword units unless otherwise noted. The primary reason why we use subword units is to generate rare or unknown words on the decoder side. In other words, our purpose is to change the vocabulary at the decoder output layer into subwords. Once we decide the vocabulary in the decoder output layer, it is natural to use the same vocabulary in the decoder embedding layer. We also use subword units in the encoder side to maintain consistency with the decoder. As described, NMT has three layers that are related to subwords: the encoder"
C18-1052,Q17-1026,0,0.0318171,"larization method is significantly effective when testing accuracy with a different dataset than the training set. This means that their method is effective with open-domain settings. Our method might have a similar tendency, but we will investigate in future work. It might be interesting to verify the effect of combining our method and the subword regularization method. Several studies incorporated the Recurrent Neural Network (RNN) or the Convolutional Neural Network (CNN) into the embedding layer for encoding character-, subword-, or morpheme-level informa627 tion (Luong and Manning, 2016; Lee et al., 2017). Comparing with these approaches, our work has a significant advantage in terms of the fast computation since our method increases negligible computational cost as clarified in Section 4.2.1. 6 Conclusion In this paper, we focused on neural machine translation with subword units and experimented with hierarchical subword features. Our experiments confirmed that adding hierarchical subword features to the encoder side consistently improved the BLEU scores. Our method, which is quite simple and easy to adapt to any models that use subwords as a unit (such as text summarization and language mode"
C18-1052,P16-1100,0,0.0319877,"wed that the subword regularization method is significantly effective when testing accuracy with a different dataset than the training set. This means that their method is effective with open-domain settings. Our method might have a similar tendency, but we will investigate in future work. It might be interesting to verify the effect of combining our method and the subword regularization method. Several studies incorporated the Recurrent Neural Network (RNN) or the Convolutional Neural Network (CNN) into the embedding layer for encoding character-, subword-, or morpheme-level informa627 tion (Luong and Manning, 2016; Lee et al., 2017). Comparing with these approaches, our work has a significant advantage in terms of the fast computation since our method increases negligible computational cost as clarified in Section 4.2.1. 6 Conclusion In this paper, we focused on neural machine translation with subword units and experimented with hierarchical subword features. Our experiments confirmed that adding hierarchical subword features to the encoder side consistently improved the BLEU scores. Our method, which is quite simple and easy to adapt to any models that use subwords as a unit (such as text summarizatio"
C18-1052,D15-1166,0,0.80111,"で部分単語が用いられるが、それぞれの層で適切な部分単 語単位は異なるという仮説を立てた。我々は、Sennrich et al. (2016) に基づく部分単語は、 大きな語彙集合が小さい語彙集合を必ず包含するという特徴を利用して、複数の異なる部 分単語列を同時に一つの埋め込み層として扱えるよう NMT モデルを改良する。以降、こ の小さな語彙集合特徴を階層的部分単語特徴と呼ぶ。本仮説を検証するために、様々な部 分単語単位や階層的部分単語特徴をエンコーダ・デコーダの埋め込み層に適用して、その 精度の変化を確認する。IWSLT 評価セットを用いた実験により、エンコーダ側で階層的な 部分単語を用いたモデルは BLEU スコアが一貫して向上することが確認できた。 1 Introduction The approach of end-to-end Neural Machine Translation (NMT) continues to make rapid progress. A simple encoder-decoder model was proposed by Sutskever et al. (2014), and an attentional mechanism was added to better exploit the encoder-side information (Luong et al., 2015; Bahdanau et al., 2015). Compared to traditional Statistical Machine Translation (SMT), NMT has relatively simple architecture, which only uses a large single neural network, but its accuracy surpasses SMT (Junczys-Dowmunt et al., 2016). A conventional NMT uses a limited vocabulary and a decoder generates a “word” in the vocabulary at each time step, but a problem occurs when it encounters an out-of-vocabulary word. Since NMT cannot correctly encode and generate such out-of-vocabulary words, the task performance is degraded. To solve this problem, Sennrich et al. (2016) proposed a method that"
C18-1052,P02-1040,0,0.102586,"ture (a) + Decoder side BPE 300 feature (a) + Decoder side BPE 1k, 300 features (f) + (n) (k) + (n) Table 3: Compared experimental settings learning rate to 1.0, but after 30 epochs we multiplied it by 0.8 for every epoch and continued training until 40 epochs. For decoding, we performed a beam search with a beam size of 20. To prevent the model from outputting short sentences, we applied the length normalization technique by dividing the negative log-likelihood by the sentence length (Cromieres et al., 2016; Morishita et al., 2017). As evaluation metrics, we used case-sensitive7 BLEU scores (Papineni et al., 2002) using multi-bleu.perl8 . To fix the experimental settings, we carried out a preliminary analysis to find the relation between the sentence length and the vocabulary size (≃ the number of BPE merge operations). Fig. 2 shows the results on the English sentences of the IWSLT 2016 German-English training set. When we reduce the vocabulary size, the average sentence length rapidly increases. Unfortunately, longer sentences require more computational cost and are time-consuming. Thus in our experiments, we set the baseline system’s vocabulary size to 16,000, which balances the sentence length witho"
C18-1052,W16-2209,0,0.0235885,"ux peut-ˆetre me l’apprendre a` moi. ✿✿✿✿✿✿✿✿✿✿✿✿ I was like, “Well I’m not ✿✿✿✿✿✿✿ Britney✿✿✿✿✿✿ Spears, but maybe you could teach me. I said, “I’m not ✿✿✿✿✿✿ British✿✿✿✿✿✿✿ Speney✿✿✿✿✿✿ Spears, but maybe you can teach me. I said, “I’m not ✿✿✿✿✿✿✿ Britney ✿✿✿✿✿✿ Spears, but maybe you can teach me. Table 9: Example translation from French to English. Proposed method correctly translated rare words: “Britney Spears.” Merge operations 16k 1k 300 Subword segmentation Bri t ney S pe ars B ri t n e y S pe ars B ri t n e y S p e ar s Table 10: Example segmentation of “Britney Spears” 5 Related Work Sennrich and Haddow (2016) added linguistic features to NMT embedding layer and achieved significant improvement. They modified the embedding layer to exploit several features, such as part-of-speech tags and syntactic dependency labels. This method resembles our work in terms of providing more information to the embedding layer. However, to use these linguistic features, since we need to prepare a morphological analyzer and/or a dependency parser, applicable languages are limited. In our method, BPE features are language independent and applicable to all languages. Our method can also be interpreted as a regularizer t"
C18-1052,P16-1162,0,0.80135,"roving Neural Machine Translation by Incorporating Hierarchical Subword Features Makoto Morishita, Jun Suzuki∗ and Masaaki Nagata NTT Communication Science Laboratories, NTT Corporation, Japan {morishita.makoto, nagata.masaaki}@lab.ntt.co.jp jun.suzuki@ecei.tohoku.ac.jp Abstract This paper focuses on subword-based Neural Machine Translation (NMT). We hypothesize that in the NMT model, the appropriate subword units for the following three modules (layers) can differ: (1) the encoder embedding layer, (2) the decoder embedding layer, and (3) the decoder output layer. We find the subword based on Sennrich et al. (2016) has a feature that a large vocabulary is a superset of a small vocabulary and modify the NMT model enables the incorporation of several different subword units in a single embedding layer. We refer these small subword features as hierarchical subword features. To empirically investigate our assumption, we compare the performance of several different subword units and hierarchical subword features for both the encoder and decoder embedding layers. We confirmed that incorporating hierarchical subword features in the encoder consistently improves BLEU scores on the IWSLT evaluation datasets. Tit"
C92-1030,W89-0214,0,\N,Missing
C92-1030,C90-2039,0,\N,Missing
C92-1030,C86-1016,0,\N,Missing
C92-1030,P88-1035,0,\N,Missing
C92-1030,P91-1040,0,\N,Missing
C92-1030,P85-1016,0,\N,Missing
C92-1030,P91-1041,0,\N,Missing
C92-1030,1991.iwpt-1.19,0,\N,Missing
C92-1030,P87-1033,0,\N,Missing
C92-1030,C90-2018,0,\N,Missing
C92-1030,P85-1017,0,\N,Missing
C92-1030,C90-3013,0,\N,Missing
C92-1030,P91-1042,0,\N,Missing
C92-1030,P90-1023,0,\N,Missing
C92-1030,W89-0203,0,\N,Missing
C92-3164,P91-1041,0,0.0921449,"Missing"
C92-3164,C92-1030,1,0.883308,"Missing"
C92-3164,C92-1009,1,0.823517,"Missing"
C92-3164,C88-2118,0,\N,Missing
C92-3164,1991.iwpt-1.19,0,\N,Missing
C94-1032,H91-1060,0,0.0852544,"niformly within the statistical approach. Using character trigrams ms tim word model, it generates the N-best word hypotheses that match the leftmost substrings starting at a given position in the input s e n t e n ce. Moreover, we propose a novel method for evaluating the performance of morphological analyzers. Unlike English, Japanese does not place spaces between words. It is difficult, even for native Japanese, to place word boundaries consistently because of the agglutinative nature of the language. Thus, there were no standard performance metrics. We applied bracketing accuracy measures [1], which is originally used for English parsers, to Japanese morphological analyzers. We also slightly extended the original definition to describe the accuracy of tile N-best candidates. In the following sections, we first describe the techniques used in the proposed morphological analyzer, we then explain the cwduation metrics and show the system's performance by experimental results. 2 2.1 Tagging M o d e l Tri-POS Model and quency Training Relative FreWe used the tri-POS (or triclass, tri-tag, tri-Ggram etc.) model ~Ls tile tagging model for Japanese. Consider a word segmentation of the inp"
C94-1032,A92-1018,0,0.10949,"Missing"
C94-1032,A88-1019,0,\N,Missing
C96-2136,W96-0205,1,\N,Missing
C96-2136,C90-2036,0,\N,Missing
C96-2136,C94-1032,1,\N,Missing
C96-2136,A92-1018,0,\N,Missing
C96-2136,A88-1019,0,\N,Missing
C96-2136,H90-1056,0,\N,Missing
C98-2147,P96-1010,0,0.0233583,"daries are placed by white spaces. If the tokenized string is not in the dictionary, it is a nonword. For a non-word, correction candidates are retrieved fl&apos;om the dictionary by approximate string 1 922 match techniques using context-independent word distance measures such as edit distance (Wagner and Fischer, 1974) and ngram distance (Angell et al., 1983). Recently, statistical language models and featurebased method have been used for context-sensitive spelling correction, where errors are corrected considering the context in which the error occurs (Church and Gale, 1991; Mays et al., 1991; Golding and Schabes, 1996). Similar techniques are used for correcting the output of English OCRs (Tong and Evans, 1996) and English speech recognizers (Ringger and Allen, 1996). There are two problems in Japanese (and Chinese) spelling correction. The first is the word boundary problem. It is impossible to use isolated word error correction techniques because there are no delimiters between words. The second is the short word problem. Word distance measures are useless because the average word length is short (< 2), and the character set is large (&gt; 3000). There are a much larger number of one edit distance neighbors"
C98-2147,C96-2136,1,0.554572,"r correction techniques because there are no delimiters between words. The second is the short word problem. Word distance measures are useless because the average word length is short (< 2), and the character set is large (&gt; 3000). There are a much larger number of one edit distance neighbors for a word, compared with English. Recently, the first problem was solved by selecting the most likely word sequence from all combinations of exactly and approximately matched words using a Viterbi-like word segmentation algorithm and a statistical language model considering unknown words and non-words (Nagata, 1996). However, the second problem is not solved yet, at least elegantly. The solution presented in (Nagata, 1996) which sorts a list of one edit distance words considering the context in which it will be placed is inaccurate because the context itself might include some errors. In this paper, we present a context-independent approximate word match method using character shape similarity. This is suitable for languages with large character sets, such as Japanese and Chinese. We also present a method to build a statistical OCR model by smoothing the character confusion probability using character sh"
C98-2147,W96-0108,0,\N,Missing
D08-1055,Y01-1001,0,0.0459586,"Missing"
D08-1055,W02-1003,0,0.0416016,"Missing"
D08-1055,P06-1079,0,0.0196042,"es Tom fix dinner),’ the basic form of the causative verb, ‘作らせる (make fix)’ is ‘作る (fix),’ and its nominative is ‘トム (Tom)’ and the accusative case role (wo-case) is ‘夕食 (dinner),’ although the surface case particle is ni (dative). We must deal with syntactic transformations in passive, causative, and benefactive constructions when analyzing the corpus. As regards iii) and iv), in Japanese, zero pronouns often occur, especially when the argument has already been mentioned in previous sentences. There have been many studies of zero-pronoun identification (Walker et al., 1994) (Nakaiwa, 1997) (Iida et al., 2006). In this paper, we present a general procedure for handling both the case role assignment of predicates and event nouns, and zero-pronoun identification. We use the decision list learning of rules to find the closest words with various constraints, because with decision lists the readability of learned lists is high and the learning is fast. The rest of this paper is organized as follows. We describe the NAIST Text Corpus, which is our target corpus in Section 2. We describe our proposed method in Section 3. The result of experiments using the NAIST Text Corpus and our method are reported in"
D08-1055,W07-1522,0,0.500179,"searchers because this information can increase the precision of text processing tasks, such as machine translation, information extraction (Hirschman et al., 1999), question answering (Narayanan and Harabagiu, 2004) (Shen and Lapata, 2007), and summarization (Melli et al., 2005). In English predicate argument structure analysis, large corpora such as FrameNet (Fillmore et al., 2001), PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004) have been created and utilized. Recently, the GDA Corpus (Hashida, 2005), Kyoto Text Corpus Ver.4.0 (Kawahara et al., 2002) and NAIST Text Corpus (Iida et al., 2007) were constructed in Japanese, and these corpora have become the target of an automatic Japanese predicate argument structure analysis system. We conducted Japanese predicate argument structure (PAS) analysis for the NAIST Text Corpus, which is the largest of these three corpora, and, as far as we know, this is the first time PAS analysis has been conducted for whole articles of the corpus. The NAIST Text Corpus has the following characteristics, i) semantic roles for both predicates and event nouns are annotated in the corpus, ii) three major case roles,1 namely the ga, wo and ni-cases in Jap"
D08-1055,W06-1617,0,0.0216272,"rpus. The NAIST Text Corpus has the following characteristics, i) semantic roles for both predicates and event nouns are annotated in the corpus, ii) three major case roles,1 namely the ga, wo and ni-cases in Japanese are annotated for the base form of predicates and event nouns, iii) both the case roles in sentences containing the target predicates and those outside the sentences (zero-pronouns) are annotated, and iv) coreference relations are also annotated. As regards i), recently there has been an increase in the number of papers dealing with nominalized predicates (Pradhan et al., 2004) (Jiang and Ng, 2006) (Xue, 2006) (Liu and Ng, 2007). For example, ‘trip’ in the sentence “During my trip to Italy, I met him.” refers not only to the event “I met him” but also to the event “I traveled to Italy.” As in this example, nouns sometimes have argument structures referring to an event. Such nouns are called event nouns (Komachi et al., 2007) in the NAIST Text Corpus. At the same time, the problems related to compound nouns are also important. In Japanese, a compound noun sometimes simultaneously contains both an event noun and its arguments. For example, the compound noun, ‘企業買収 (corporate buyout)’ cont"
D08-1055,kawahara-etal-2002-construction,0,0.011374,"ure analysis has attracted the attention of researchers because this information can increase the precision of text processing tasks, such as machine translation, information extraction (Hirschman et al., 1999), question answering (Narayanan and Harabagiu, 2004) (Shen and Lapata, 2007), and summarization (Melli et al., 2005). In English predicate argument structure analysis, large corpora such as FrameNet (Fillmore et al., 2001), PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004) have been created and utilized. Recently, the GDA Corpus (Hashida, 2005), Kyoto Text Corpus Ver.4.0 (Kawahara et al., 2002) and NAIST Text Corpus (Iida et al., 2007) were constructed in Japanese, and these corpora have become the target of an automatic Japanese predicate argument structure analysis system. We conducted Japanese predicate argument structure (PAS) analysis for the NAIST Text Corpus, which is the largest of these three corpora, and, as far as we know, this is the first time PAS analysis has been conducted for whole articles of the corpus. The NAIST Text Corpus has the following characteristics, i) semantic roles for both predicates and event nouns are annotated in the corpus, ii) three major case rol"
D08-1055,P07-1027,0,0.0203787,"he following characteristics, i) semantic roles for both predicates and event nouns are annotated in the corpus, ii) three major case roles,1 namely the ga, wo and ni-cases in Japanese are annotated for the base form of predicates and event nouns, iii) both the case roles in sentences containing the target predicates and those outside the sentences (zero-pronouns) are annotated, and iv) coreference relations are also annotated. As regards i), recently there has been an increase in the number of papers dealing with nominalized predicates (Pradhan et al., 2004) (Jiang and Ng, 2006) (Xue, 2006) (Liu and Ng, 2007). For example, ‘trip’ in the sentence “During my trip to Italy, I met him.” refers not only to the event “I met him” but also to the event “I traveled to Italy.” As in this example, nouns sometimes have argument structures referring to an event. Such nouns are called event nouns (Komachi et al., 2007) in the NAIST Text Corpus. At the same time, the problems related to compound nouns are also important. In Japanese, a compound noun sometimes simultaneously contains both an event noun and its arguments. For example, the compound noun, ‘企業買収 (corporate buyout)’ contains an event noun ‘買収 (buyout)"
D08-1055,W04-2705,0,0.0409732,"mpared with a baseline method in a sentence level performance analysis. 1 Introduction Recently, predicate argument structure analysis has attracted the attention of researchers because this information can increase the precision of text processing tasks, such as machine translation, information extraction (Hirschman et al., 1999), question answering (Narayanan and Harabagiu, 2004) (Shen and Lapata, 2007), and summarization (Melli et al., 2005). In English predicate argument structure analysis, large corpora such as FrameNet (Fillmore et al., 2001), PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004) have been created and utilized. Recently, the GDA Corpus (Hashida, 2005), Kyoto Text Corpus Ver.4.0 (Kawahara et al., 2002) and NAIST Text Corpus (Iida et al., 2007) were constructed in Japanese, and these corpora have become the target of an automatic Japanese predicate argument structure analysis system. We conducted Japanese predicate argument structure (PAS) analysis for the NAIST Text Corpus, which is the largest of these three corpora, and, as far as we know, this is the first time PAS analysis has been conducted for whole articles of the corpus. The NAIST Text Corpus has the following"
D08-1055,W97-0114,0,0.0467601,"食を作らせる (Mary makes Tom fix dinner),’ the basic form of the causative verb, ‘作らせる (make fix)’ is ‘作る (fix),’ and its nominative is ‘トム (Tom)’ and the accusative case role (wo-case) is ‘夕食 (dinner),’ although the surface case particle is ni (dative). We must deal with syntactic transformations in passive, causative, and benefactive constructions when analyzing the corpus. As regards iii) and iv), in Japanese, zero pronouns often occur, especially when the argument has already been mentioned in previous sentences. There have been many studies of zero-pronoun identification (Walker et al., 1994) (Nakaiwa, 1997) (Iida et al., 2006). In this paper, we present a general procedure for handling both the case role assignment of predicates and event nouns, and zero-pronoun identification. We use the decision list learning of rules to find the closest words with various constraints, because with decision lists the readability of learned lists is high and the learning is fast. The rest of this paper is organized as follows. We describe the NAIST Text Corpus, which is our target corpus in Section 2. We describe our proposed method in Section 3. The result of experiments using the NAIST Text Corpus and our met"
D08-1055,C04-1100,0,0.0186387,"Missing"
D08-1055,J05-1004,0,0.195225,"Missing"
D08-1055,N04-1030,0,0.0247891,"hole articles of the corpus. The NAIST Text Corpus has the following characteristics, i) semantic roles for both predicates and event nouns are annotated in the corpus, ii) three major case roles,1 namely the ga, wo and ni-cases in Japanese are annotated for the base form of predicates and event nouns, iii) both the case roles in sentences containing the target predicates and those outside the sentences (zero-pronouns) are annotated, and iv) coreference relations are also annotated. As regards i), recently there has been an increase in the number of papers dealing with nominalized predicates (Pradhan et al., 2004) (Jiang and Ng, 2006) (Xue, 2006) (Liu and Ng, 2007). For example, ‘trip’ in the sentence “During my trip to Italy, I met him.” refers not only to the event “I met him” but also to the event “I traveled to Italy.” As in this example, nouns sometimes have argument structures referring to an event. Such nouns are called event nouns (Komachi et al., 2007) in the NAIST Text Corpus. At the same time, the problems related to compound nouns are also important. In Japanese, a compound noun sometimes simultaneously contains both an event noun and its arguments. For example, the compound noun, ‘企業買収 (co"
D08-1055,D07-1002,0,0.0647834,"Missing"
D08-1055,J94-2003,0,0.169479,"e sentence, ‘メアリーはトムに夕食を作らせる (Mary makes Tom fix dinner),’ the basic form of the causative verb, ‘作らせる (make fix)’ is ‘作る (fix),’ and its nominative is ‘トム (Tom)’ and the accusative case role (wo-case) is ‘夕食 (dinner),’ although the surface case particle is ni (dative). We must deal with syntactic transformations in passive, causative, and benefactive constructions when analyzing the corpus. As regards iii) and iv), in Japanese, zero pronouns often occur, especially when the argument has already been mentioned in previous sentences. There have been many studies of zero-pronoun identification (Walker et al., 1994) (Nakaiwa, 1997) (Iida et al., 2006). In this paper, we present a general procedure for handling both the case role assignment of predicates and event nouns, and zero-pronoun identification. We use the decision list learning of rules to find the closest words with various constraints, because with decision lists the readability of learned lists is high and the learning is fast. The rest of this paper is organized as follows. We describe the NAIST Text Corpus, which is our target corpus in Section 2. We describe our proposed method in Section 3. The result of experiments using the NAIST Text Co"
D08-1055,N06-1055,0,0.0180762,"Corpus has the following characteristics, i) semantic roles for both predicates and event nouns are annotated in the corpus, ii) three major case roles,1 namely the ga, wo and ni-cases in Japanese are annotated for the base form of predicates and event nouns, iii) both the case roles in sentences containing the target predicates and those outside the sentences (zero-pronouns) are annotated, and iv) coreference relations are also annotated. As regards i), recently there has been an increase in the number of papers dealing with nominalized predicates (Pradhan et al., 2004) (Jiang and Ng, 2006) (Xue, 2006) (Liu and Ng, 2007). For example, ‘trip’ in the sentence “During my trip to Italy, I met him.” refers not only to the event “I met him” but also to the event “I traveled to Italy.” As in this example, nouns sometimes have argument structures referring to an event. Such nouns are called event nouns (Komachi et al., 2007) in the NAIST Text Corpus. At the same time, the problems related to compound nouns are also important. In Japanese, a compound noun sometimes simultaneously contains both an event noun and its arguments. For example, the compound noun, ‘企業買収 (corporate buyout)’ contains an even"
D08-1055,P94-1013,0,0.0405829,"Missing"
D13-1021,2010.iwslt-papers.7,0,0.33004,"data (in case of sample-wise noise, we assume the noise is in the beginning). This assumption derives a constraint between signal and noise parts that helps to avoid a welter of transliteration and non-transliteration parts. It also has a shortcoming that it is generally noise t h エ noise e sp e ッ t c チ ン グ h i n sp g マ sp ス m ク a s t noiseB t noiseB noise k s s t noiseB t noiseB s s s t signal t signal Figure 2: Example of many-to-many alignment with partial noise in the beginning and end. “noise” stands for the noise symbol and “sp” stands for a white space. s 3.3 Constrained Gibbs sampling Finch and Sumita (2010) used a blocked Gibbs sampling algorithm with forward-filtering backwardsampling (FFBS) (Mochihashi et al., 2009). We extend their algorithm for our noise-aware model using a state-based calculation over the three states: non-transliteration part in the beginning (noiseB), transliteration part (signal), non-transliteration part in the end (noiseE). Figure 3 illustrates our FFBS steps. At first in the forward filtering, we begin with transition to noiseB and signal. The calculation of forward probabilities itself is almost the same as Finch and Sumita (2010) except for state transition constrai"
D13-1021,P09-1012,0,0.0328726,"nt between signal and noise parts that helps to avoid a welter of transliteration and non-transliteration parts. It also has a shortcoming that it is generally noise t h エ noise e sp e ッ t c チ ン グ h i n sp g マ sp ス m ク a s t noiseB t noiseB noise k s s t noiseB t noiseB s s s t signal t signal Figure 2: Example of many-to-many alignment with partial noise in the beginning and end. “noise” stands for the noise symbol and “sp” stands for a white space. s 3.3 Constrained Gibbs sampling Finch and Sumita (2010) used a blocked Gibbs sampling algorithm with forward-filtering backwardsampling (FFBS) (Mochihashi et al., 2009). We extend their algorithm for our noise-aware model using a state-based calculation over the three states: non-transliteration part in the beginning (noiseB), transliteration part (signal), non-transliteration part in the end (noiseE). Figure 3 illustrates our FFBS steps. At first in the forward filtering, we begin with transition to noiseB and signal. The calculation of forward probabilities itself is almost the same as Finch and Sumita (2010) except for state transition constraints: from noiseB to signal, from signal to noiseE. The backward-sampling traverses a path by probabilitybased sam"
D13-1021,P02-1040,0,0.0872961,"Missing"
D13-1021,P12-1049,0,0.282575,"ration in patent domain using patent bilingual corpora. 1 Introduction Transliteration is used for providing translations for source language words that have no appropriate counterparts in target language, such as some technical terms and named entities. Statistical machine transliteration (Knight and Graehl, 1998) is a technology to solve it in a statistical manner. Bilingual dictionaries can be used to train its model, but many of their entries are actually translation but not transliteration. Such non-transliteration pairs hurt the transliteration model and should be eliminated beforehand. Sajjad et al. (2012) proposed a method to identify such non-transliteration pairs, and applied it successfully to noisy word pairs obtained from automatic word alignment on bilingual corpora. It enables the statistical machine transliteration to be bootstrapped from bilingual corpora. This approach is beneficial because it does not require carefullydeveloped bilingual transliteration dictionaries and it can learn domain-specific transliteration patterns This paper proposes a novel transliteration mining method for such partial transliterations. The method uses a noise-aware character alignment model that distingu"
D13-1021,W10-2402,0,\N,Missing
D13-1021,J98-4003,0,\N,Missing
D13-1139,P05-1033,0,0.11802,"uses rich syntax parsing features for word reordering and runs in linear time. We apply it to postordering of phrase-based machine translation (PBMT) for Japanese-to-English patent tasks. Our experimental results show that our method achieves a significant improvement of +3.1 BLEU scores against 30.15 BLEU scores of the baseline PBMT system. 1 Sourceordered. Target Sentence (HFE) Target Sentence (E) reordering Figure 1: A description of the postordering MT system. Introduction Even though phrase-based machine translation (PBMT) (Koehn et al., 2007) and tree-based MT (Graehl and Knight, 2004; Chiang, 2005; Galley et al., 2006) systems have achieved great success, many problems remain for distinct language pairs, including long-distant word reordering. To improve such word reordering, one promising way is to separate it from the translation process as preordering (Collins et al., 2005; DeNero and Uszkoreit, 2011) or postordering (Sudoh et al., 2011; Goto et al., 2012). Many studies utilize a rulebased or a probabilistic model to perform a reordering decision at each node of a syntactic parse tree. This paper presents a parser-based word reordering model that employs a shift-reduce parser for in"
D13-1139,P04-1015,0,0.056235,"wlef t1 of Y, and the rightmost word of phrase X is set to rightmost word wright0 of Z. Variable a is set to a1 of Y. The difference between reduce-MR-X and reduce-SR-X actions is new stack element s0 . The reduce-SR-X action generates s0 by combining s′0 and s′1 with binary rule X# →Y Z: s0 = {X# , h0 , wlef t0 , wright1 , a0 }. This action expands Y and Z in a reverse order, and the leftmost word of X# is set to wlef t0 of Z, and the rightmost word of X# is set to wright1 of Y. Variable a is set to a0 of Z. We use a linear model that is discriminatively trained with the averaged perceptron (Collins and Roark, 2004). Table 1 shows the feature templates used in our experiments and we call the features in the bottom two rows “non-local” features. # of sent. ave. leng. (J) ave. leng. (E) train dev test9 test10 3,191,228 36.4 33.3 2,000 36.6 33.3 2,000 37.0 33.7 2,300 43.1 39.6 Table 2: NTCIR-9 and 10 data statistics. 4 Experiments 4.1 Experimental Setups We conducted experiments for NTCIR-9 and 10 patent data using a Japanese-English language pair. Mecab2 was used for the Japanese morphological analysis. The data are summarized in Table 2. We used Enju (Miyao and Tsujii, 2008) for parsing the English traini"
D13-1139,P05-1066,0,0.27903,"Missing"
D13-1139,D11-1018,0,0.218258,"30.15 BLEU scores of the baseline PBMT system. 1 Sourceordered. Target Sentence (HFE) Target Sentence (E) reordering Figure 1: A description of the postordering MT system. Introduction Even though phrase-based machine translation (PBMT) (Koehn et al., 2007) and tree-based MT (Graehl and Knight, 2004; Chiang, 2005; Galley et al., 2006) systems have achieved great success, many problems remain for distinct language pairs, including long-distant word reordering. To improve such word reordering, one promising way is to separate it from the translation process as preordering (Collins et al., 2005; DeNero and Uszkoreit, 2011) or postordering (Sudoh et al., 2011; Goto et al., 2012). Many studies utilize a rulebased or a probabilistic model to perform a reordering decision at each node of a syntactic parse tree. This paper presents a parser-based word reordering model that employs a shift-reduce parser for inversion transduction grammars (ITG) (Wu, 1997). To the best of our knowledge, this is the first study on a shift-reduce parser for word reordering. The parser-based reordering approach uses rich syntax parsing features for reordering decisions. Our propoesd method can also easily define such non-local features a"
D13-1139,P06-1121,0,0.202823,"tax parsing features for word reordering and runs in linear time. We apply it to postordering of phrase-based machine translation (PBMT) for Japanese-to-English patent tasks. Our experimental results show that our method achieves a significant improvement of +3.1 BLEU scores against 30.15 BLEU scores of the baseline PBMT system. 1 Sourceordered. Target Sentence (HFE) Target Sentence (E) reordering Figure 1: A description of the postordering MT system. Introduction Even though phrase-based machine translation (PBMT) (Koehn et al., 2007) and tree-based MT (Graehl and Knight, 2004; Chiang, 2005; Galley et al., 2006) systems have achieved great success, many problems remain for distinct language pairs, including long-distant word reordering. To improve such word reordering, one promising way is to separate it from the translation process as preordering (Collins et al., 2005; DeNero and Uszkoreit, 2011) or postordering (Sudoh et al., 2011; Goto et al., 2012). Many studies utilize a rulebased or a probabilistic model to perform a reordering decision at each node of a syntactic parse tree. This paper presents a parser-based word reordering model that employs a shift-reduce parser for inversion transduction g"
D13-1139,P12-2061,0,0.351117,"arget Sentence (HFE) Target Sentence (E) reordering Figure 1: A description of the postordering MT system. Introduction Even though phrase-based machine translation (PBMT) (Koehn et al., 2007) and tree-based MT (Graehl and Knight, 2004; Chiang, 2005; Galley et al., 2006) systems have achieved great success, many problems remain for distinct language pairs, including long-distant word reordering. To improve such word reordering, one promising way is to separate it from the translation process as preordering (Collins et al., 2005; DeNero and Uszkoreit, 2011) or postordering (Sudoh et al., 2011; Goto et al., 2012). Many studies utilize a rulebased or a probabilistic model to perform a reordering decision at each node of a syntactic parse tree. This paper presents a parser-based word reordering model that employs a shift-reduce parser for inversion transduction grammars (ITG) (Wu, 1997). To the best of our knowledge, this is the first study on a shift-reduce parser for word reordering. The parser-based reordering approach uses rich syntax parsing features for reordering decisions. Our propoesd method can also easily define such non-local features as the N -gram words of reordered strings. Even when usin"
D13-1139,N04-1014,0,0.0387663,"ction grammars. Our model uses rich syntax parsing features for word reordering and runs in linear time. We apply it to postordering of phrase-based machine translation (PBMT) for Japanese-to-English patent tasks. Our experimental results show that our method achieves a significant improvement of +3.1 BLEU scores against 30.15 BLEU scores of the baseline PBMT system. 1 Sourceordered. Target Sentence (HFE) Target Sentence (E) reordering Figure 1: A description of the postordering MT system. Introduction Even though phrase-based machine translation (PBMT) (Koehn et al., 2007) and tree-based MT (Graehl and Knight, 2004; Chiang, 2005; Galley et al., 2006) systems have achieved great success, many problems remain for distinct language pairs, including long-distant word reordering. To improve such word reordering, one promising way is to separate it from the translation process as preordering (Collins et al., 2005; DeNero and Uszkoreit, 2011) or postordering (Sudoh et al., 2011; Goto et al., 2012). Many studies utilize a rulebased or a probabilistic model to perform a reordering decision at each node of a syntactic parse tree. This paper presents a parser-based word reordering model that employs a shift-reduce"
D13-1139,P10-1110,0,0.0256258,"j, S|s0 ⟩ : π where s′0 is {X, h, wlef t , wright , a} and s0 is {X, h, wlef t , wright , x} (i ≤ h, lef t, right < j). The side condition prevents the parser from inserting articles into phrase X more than twice. During parsing, articles are not explicitly inserted into the input string: they are inserted into it when backtracking to generate a reordered string after parsing. The reduce-MR-X action has a deduction rule: X→YZ∈P ∧q ∈π z q }| { : ⟨k, i, S|s′2 |s′1 ⟩ : π ′ ℓ : ⟨i, j, S|s′1 |s′0 ⟩ : π ℓ + 1 : ⟨k, j, S|s′2 |s0 ⟩ : π ′ 1 Since our notion of predictor states is identical to that in (Huang and Sagae, 2010), we omit the details here. 1384 s0 = {X, h0 , wlef t1 , wright0 , a1 }. New nonterminal X is lexicalized with head word wh0 of right nonterminal Z. This action expands Y and Z in a straight order. The leftmost word of phrase X is set to leftmost word wlef t1 of Y, and the rightmost word of phrase X is set to rightmost word wright0 of Z. Variable a is set to a1 of Y. The difference between reduce-MR-X and reduce-SR-X actions is new stack element s0 . The reduce-SR-X action generates s0 by combining s′0 and s′1 with binary rule X# →Y Z: s0 = {X# , h0 , wlef t0 , wright1 , a0 }. This action expa"
D13-1139,P07-2045,0,0.0104477,"ift-reduce parser for inversion transduction grammars. Our model uses rich syntax parsing features for word reordering and runs in linear time. We apply it to postordering of phrase-based machine translation (PBMT) for Japanese-to-English patent tasks. Our experimental results show that our method achieves a significant improvement of +3.1 BLEU scores against 30.15 BLEU scores of the baseline PBMT system. 1 Sourceordered. Target Sentence (HFE) Target Sentence (E) reordering Figure 1: A description of the postordering MT system. Introduction Even though phrase-based machine translation (PBMT) (Koehn et al., 2007) and tree-based MT (Graehl and Knight, 2004; Chiang, 2005; Galley et al., 2006) systems have achieved great success, many problems remain for distinct language pairs, including long-distant word reordering. To improve such word reordering, one promising way is to separate it from the translation process as preordering (Collins et al., 2005; DeNero and Uszkoreit, 2011) or postordering (Sudoh et al., 2011; Goto et al., 2012). Many studies utilize a rulebased or a probabilistic model to perform a reordering decision at each node of a syntactic parse tree. This paper presents a parser-based word r"
D13-1139,J08-1002,0,0.0206303,"with the averaged perceptron (Collins and Roark, 2004). Table 1 shows the feature templates used in our experiments and we call the features in the bottom two rows “non-local” features. # of sent. ave. leng. (J) ave. leng. (E) train dev test9 test10 3,191,228 36.4 33.3 2,000 36.6 33.3 2,000 37.0 33.7 2,300 43.1 39.6 Table 2: NTCIR-9 and 10 data statistics. 4 Experiments 4.1 Experimental Setups We conducted experiments for NTCIR-9 and 10 patent data using a Japanese-English language pair. Mecab2 was used for the Japanese morphological analysis. The data are summarized in Table 2. We used Enju (Miyao and Tsujii, 2008) for parsing the English training data and converted parse trees into HFE trees by a head-finalization scheme. We extracted grammar rules from all the HFE trees and randomly selected 500,000 HFE trees to train the shift-reduce parser. We used Moses (Koehn et al., 2007) with lexicalized reordering and a 6-gram language model (LM) trained using SRILM (Stolcke et al., 2011) to translate the Japanese sentences into HFE sentences. To recover the English sentences, our shift-reduce parser reordered only the 1-best HFE sentence. Our strategy is much simpler than Goto et al. (2012)’s because they used"
D13-1139,N07-1051,0,0.0337815,"articles “ga” “wo” “wa” into English sentences. We privilege the nonterminals of a phrase modified by a deleted article to determine which “the” “a/an” or “no articles” should be inserted at the front of the phrase. Note that an original English sentence can be recovered from its HFE tree by using # symbols and annotated articles and deleting Japanese particles. As well as Goto et al. (2012), we solve postordering by a parser whose model is trained with a set of HFE trees. The main difference between Goto et al. (2012)’s model and ours is that while the former simply used the Berkeley parser (Petrov and Klein, 2007), our shift-reduce parsing model can use such non-local task specific features as the N -gram words of reordered strings without sacrificing efficiency. Our method integrates postediting (Knight and Chander, 1994) with reordering and inserts articles into English translations by learning an additional “insert” action of the parser. Goto et al. (2012) solved the article generation problem by using an 1383 N -gram language model, but this somewhat complicates their approach. Compared with other parsers, one advantage of the shift-reduce parser is to easily define such additional operations as “i"
D13-1139,2011.mtsummit-papers.36,1,0.941844,". 1 Sourceordered. Target Sentence (HFE) Target Sentence (E) reordering Figure 1: A description of the postordering MT system. Introduction Even though phrase-based machine translation (PBMT) (Koehn et al., 2007) and tree-based MT (Graehl and Knight, 2004; Chiang, 2005; Galley et al., 2006) systems have achieved great success, many problems remain for distinct language pairs, including long-distant word reordering. To improve such word reordering, one promising way is to separate it from the translation process as preordering (Collins et al., 2005; DeNero and Uszkoreit, 2011) or postordering (Sudoh et al., 2011; Goto et al., 2012). Many studies utilize a rulebased or a probabilistic model to perform a reordering decision at each node of a syntactic parse tree. This paper presents a parser-based word reordering model that employs a shift-reduce parser for inversion transduction grammars (ITG) (Wu, 1997). To the best of our knowledge, this is the first study on a shift-reduce parser for word reordering. The parser-based reordering approach uses rich syntax parsing features for reordering decisions. Our propoesd method can also easily define such non-local features as the N -gram words of reordered str"
D13-1139,J97-3002,0,0.358339,"chieved great success, many problems remain for distinct language pairs, including long-distant word reordering. To improve such word reordering, one promising way is to separate it from the translation process as preordering (Collins et al., 2005; DeNero and Uszkoreit, 2011) or postordering (Sudoh et al., 2011; Goto et al., 2012). Many studies utilize a rulebased or a probabilistic model to perform a reordering decision at each node of a syntactic parse tree. This paper presents a parser-based word reordering model that employs a shift-reduce parser for inversion transduction grammars (ITG) (Wu, 1997). To the best of our knowledge, this is the first study on a shift-reduce parser for word reordering. The parser-based reordering approach uses rich syntax parsing features for reordering decisions. Our propoesd method can also easily define such non-local features as the N -gram words of reordered strings. Even when using these non-local features, the complexity of the shift-reduce parser does not increase at all due to give up achieving an optimal solution. Therefore, it works much more efficient. In our experiments, we apply our proposed method to postordering for J-to-E patent tasks becaus"
D13-1158,W01-1605,0,0.882141,"Missing"
D13-1158,P02-1057,0,0.236537,"Missing"
D13-1158,P09-1075,0,0.0187614,"age model (MCP) and a lead method (LEAD). MCP is known to be a state-of-the-art method for multiple document summarization and we believe that MCP also performs well in terms of single document summarization. LEAD is also a widely used summarizer that simply takes the first K textual units of the document. Although this is a simple heuristic rule, it is known as a state-of-the-art summarizer (Nenkova and McKeown, 2011). For our method, we examined two types of DEP-DT. One was obtained from the gold RSTDT. The other was obtained from the RST-DT produced by a state-of-the-art RST parser, HILDA (duVerle and Prendinger, 2009; Hernault et al., 2010). For Marcu’s method, we examined both the gold RST-DT and HILDA’s RST-DT. We re-implemented HILDA and re-trained it on the RST-DT Corpus excluding the 30 documents used in the evaluation. The F-score of the parser was around 0.5. For KP, we exclude constraint (7) from the ILP formulation of TKP and set the depth of all EDUs in equations (3) and (5) at 1. For MCP, we use tf (equation (4)) as the word weight. We evaluated the summarization systems with ROUGE version 1.5.5 4 . Performance metrics were the recall (R) and F-score (F) of ROUGE -1,2. 4.2 Results and Discussio"
D13-1158,C04-1057,0,0.027106,"Missing"
D13-1158,W08-1105,0,0.0274936,"minals of an RST-DT are eliminated. However, we believe that these relations are no needed for the summarization that we are attempting to realize. 1517 We formulate the optimization problem in the previous section as a Tree Knapsack Problem, which is a kind of Precedence-Constrained Knapsack Problem (Samphaiboon and Yamada, 2000) and we can obtain the optimal rooted subtree by solving the following ILP problem2 : maximize x s.t. N ∑ W(ei ) xi Depth(ei ) i=1 N ∑ ℓi x i ≤ L (5) (6) i=1 2 ∀i : xparent(i) ≥ xi (7) ∀i : xi ∈ {0, 1}, (8) A similar approach has been applied to sentence compression (Filippova and Strube, 2008). TKP(G) TKP(H) Marcu(G) Marcu(H) MCP KP LEAD ROUGE -1 F R .310H,K,L .321G,H,K,L .281H .284H H .291 .272H .236 .219 .279 .295H .251 .266H .255 .240 ROUGE -2 F R .108 .112H .092 .093 .101 .093 .073 .068 .073 .077 .071 .075 .092 .086 Table 1: ROUGE scores of the RST discourse treebank dataset. In the table, G,H,K,L indicate a method statistically significant against Marcu(G), Marcu(H), KP, LEAD, respectively. where x is an N -dimensional binary vector that represents the summary, i.e. xi =1 denotes that the ith EDU is included in the summary. N is the number of EDUs in a document, ℓi is the leng"
D13-1158,W04-1013,0,0.0694284,"nce. This paper proposes a single document summarization method based on the trimming of a discourse tree. This is a two-fold process. First, we propose rules for transforming a rhetorical structure theorybased discourse tree into a dependency-based discourse tree, which allows us to take a treetrimming approach to summarization. Second, we formulate the problem of trimming a dependency-based discourse tree as a Tree Knapsack Problem, then solve it with integer linear programming (ILP). Evaluation results showed that our method improved ROUGE scores. These methods successfully improved ROUGE (Lin, 2004) scores, but they still have one critical shortcoming. Since these methods are based on subset selection, the summaries they generate cannot preserve the rhetorical structure of the textual units of a source document. Thus, the resulting summary may lack coherence and may not include significant textual units from a source document. 1 Introduction State-of-the-art extractive text summarization methods regard a document (or a document set) as a set of textual units (e.g. sentences, clauses, phrases) and formulate summarization as a combinatorial optimization problem, i.e. selecting a subset of"
D13-1158,W98-1124,0,0.316243,"Introduction State-of-the-art extractive text summarization methods regard a document (or a document set) as a set of textual units (e.g. sentences, clauses, phrases) and formulate summarization as a combinatorial optimization problem, i.e. selecting a subset of the set of textual units that maximizes an objective without violating a length constraint. For example, McDonald (2007) formulated text summarization as a Knapsack Problem, where he selects a set of textual One powerful and potential way to overcome the problem is to include discourse tree constraints in the summarization procedure. Marcu (1998) regarded a document as a Rhetorical Structure Theory (RST) (William Charles, Mann and Sandra Annear, Thompson, 1988)-based discourse tree (RSTDT) and selected textual units according to a preference ranking derived from the tree structure to make a summary. Daum´e et al. (2002) proposed a document compression method that directly models the probability of a summary given an RST-DT by using a noisy-channel model. These methods generate well-organized summaries, however, since they do not formulate summarizations as combinatorial op1515 Proceedings of the 2013 Conference on Empirical Methods in"
D13-1158,E09-1089,0,0.0923356,"Missing"
D13-1158,W01-0100,0,\N,Missing
D14-1196,P02-1057,0,0.242742,"Missing"
D14-1196,D13-1158,1,0.413854,"Missing"
D14-1196,P14-2052,1,0.463564,"an automatically parsed RST-DT. To improve the ROUGE score, we propose a novel discourse parser that directly generates the DEP-DT. The evaluation results showed that the TKP with our parser outperformed that with the state-of-the-art RST-DT parser, and achieved almost equivalent ROUGE scores to the TKP with the gold DEP-DT. 1 Introduction Discourse structures of documents are believed to be highly beneficial for generating informative and coherent summaries. Several discoursebased summarization methods have been developed, such as (Marcu, 1998; Daum´e III and Marcu, 2002; Hirao et al., 2013; Kikuchi et al., 2014). Moreover, the current best ROUGE score for the summarization benchmark data of the RSTdiscourse Treebank (Carlson et al., 2002) has been provided by (Hirao et al., 2013), whose method also utilizes discourse trees. Thus, the discoursebased summarization approach is one promising way to obtain high-quality summaries. One possible weakness of discourse-based summarization techniques is that they rely greatly on the accuracy of the discourse parser they use. For example, the above discourse-based summarization methods utilize discourse trees based on the Rhetorical Structure Theory (RST) (Mann"
D14-1196,P14-1003,0,0.218582,"Nucleus, we obtain an incorrect DEP-DT in Figure 1(b) because the transformation algorithm uses the Nucleus-Satellite relationships in the RST-DT. The dependency relationships in Figure 1(b) are quite different from that of the correct DEP-DT in Figure 1(c). In this example, the parser failed to determine the most salient EDU e2 , that is the root EDU of the gold DEP-DT. Thus, the summary extracted from this DEP-DT will have a low ROUGE score. The results motivated us to design a new discourse parser fully trained on the DEP-DTs and 1 Li et al. also defined a similar transformation algorithm (Li et al., 2014). In this paper, we follow the transformation algorithm defined in (Hirao et al., 2013). 1835 Parser)Training)Phase Transforma;on)Algorithm Elabora7on$ N$ Root$ Elabora7on$ Root$ Example$ Elabora7on$ S$ N$ Elabora7on$ S$ S$ N$ Example$ Elabora7on$ S$ N$ Background$ Concession$ S$ S$ Example$An7thesis$ N$ N$ Elabora7on$ Elabora7on$ Background$ N$ N$ S$ N$ Elabora7on$ S$ S$ e1$ e2$ e1$ e1$ S$ e3$ e4$ e2$ e3$ e2$ e5$ N$ e4$ e5$ e3$ e4$ N$ N$ S$ Concession$ S$ Contrast$ Background$ N$ N$ S$ Elabora7on$ N$ Contrast$ N$ N$ N$ S$ Evidence$ N$ Contrast$N$ N$ S$ Evidence$ N$ RST=DTs Root$ Elabora7on$ R"
D14-1196,W98-1124,0,0.568676,"with a gold DEP-DT and a system with a DEP-DT obtained from an automatically parsed RST-DT. To improve the ROUGE score, we propose a novel discourse parser that directly generates the DEP-DT. The evaluation results showed that the TKP with our parser outperformed that with the state-of-the-art RST-DT parser, and achieved almost equivalent ROUGE scores to the TKP with the gold DEP-DT. 1 Introduction Discourse structures of documents are believed to be highly beneficial for generating informative and coherent summaries. Several discoursebased summarization methods have been developed, such as (Marcu, 1998; Daum´e III and Marcu, 2002; Hirao et al., 2013; Kikuchi et al., 2014). Moreover, the current best ROUGE score for the summarization benchmark data of the RSTdiscourse Treebank (Carlson et al., 2002) has been provided by (Hirao et al., 2013), whose method also utilizes discourse trees. Thus, the discoursebased summarization approach is one promising way to obtain high-quality summaries. One possible weakness of discourse-based summarization techniques is that they rely greatly on the accuracy of the discourse parser they use. For example, the above discourse-based summarization methods utiliz"
D14-1196,P05-1012,0,0.102477,"-DT. (b) Overview of (Hirao et al., 2013). In the parser training phase, the parser is trained on RST-DTs, and in the summarization phase, the document is parsed into the RST-DT, and then transformed into the DEP-DT. that could directly generate the DEP-DT. Figure 2(a) shows an overview of the TKP combined with our DEP-DT parser. In the parser training phase, we transform RST-DTs into DEP-DTs, and directly train our parser with the DEP-DTs. In the summarization phase, our method parses a raw document directly into a DEP-DT, and generates a summary with the TKP. fused Relaxed Algorithm (MIRA) (McDonald et al., 2005a; Crammer et al., 2006). We denote s(w, y) = wT fy as a score function given a weight vector w and a DEP-DT y. L(y, y? ) is a loss function, and we define it as the number of EDUs that have an incorrect parent EDU in a predicted DEP-DT y? = arg max s(w, y). Then, we y solve the following optimization problem: 3.2 Description of Discourse Dependency Parser min ||w − w(t) || w Our parser is based on the first-order Maximum Spanning Tree (MST) algorithm (McDonald et al., 2005b). Our parser extracts the features from the EDU ei and the EDU ej . We use almost the features as those shown in (Hernau"
D14-1196,H05-1066,0,0.071798,"-DT. (b) Overview of (Hirao et al., 2013). In the parser training phase, the parser is trained on RST-DTs, and in the summarization phase, the document is parsed into the RST-DT, and then transformed into the DEP-DT. that could directly generate the DEP-DT. Figure 2(a) shows an overview of the TKP combined with our DEP-DT parser. In the parser training phase, we transform RST-DTs into DEP-DTs, and directly train our parser with the DEP-DTs. In the summarization phase, our method parses a raw document directly into a DEP-DT, and generates a summary with the TKP. fused Relaxed Algorithm (MIRA) (McDonald et al., 2005a; Crammer et al., 2006). We denote s(w, y) = wT fy as a score function given a weight vector w and a DEP-DT y. L(y, y? ) is a loss function, and we define it as the number of EDUs that have an incorrect parent EDU in a predicted DEP-DT y? = arg max s(w, y). Then, we y solve the following optimization problem: 3.2 Description of Discourse Dependency Parser min ||w − w(t) || w Our parser is based on the first-order Maximum Spanning Tree (MST) algorithm (McDonald et al., 2005b). Our parser extracts the features from the EDU ei and the EDU ej . We use almost the features as those shown in (Hernau"
D14-1196,N03-1030,0,0.477924,"Missing"
D15-1156,P11-2037,0,0.107753,"ank (Marcus et al., 1993) has annotations on PRO and trace, they provide only labeled bracketing. Johnson (2002) proposed a statistical pattern-matching algorithm for post-processing the results of syntactic parsing based on minimal unlexicalized tree fragments from empty node to its antecedent. Dienes and Dubey (2003) proposed a machine learning-based “trace tagger” as a preprocess of parsing. Campbell (2004) proposed a rule-based post-processing method based on linguistically motivated rules. Gabbard et al. (2006) replaced the rules with machine learning-based classifiers. Schmid (2006) and Cai et al. (2011) integrated empty category detection with the syntactic parsing. Empty category detection for pro (dropped pronouns or zero pronoun) has begun to receive attention as the Chinese Penn Treebank (Xue et al., 2005) has annotations for pro as well as PRO and trace. Xue and Yang (2013) formalized the problem as classifying each pair of the location of empty category and its head word in the dependency structure. Wang et al. (2015) proposed a joint embedding of empty categories and their contexts on dependency structure. Xiang et al. (2013) formalized the problem as classifying each IP node (roughly"
D15-1156,P03-1055,0,0.607948,"proved the accuracy of machine translation both in Korean and in Chinese. Kudo et al. (2014) showed that generating zero subjects in Japanese improved the accuracy of preorderingbased translation. State-of-the-art statistical syntactic parsers had typically ignored empty categories. Although Penn Treebank (Marcus et al., 1993) has annotations on PRO and trace, they provide only labeled bracketing. Johnson (2002) proposed a statistical pattern-matching algorithm for post-processing the results of syntactic parsing based on minimal unlexicalized tree fragments from empty node to its antecedent. Dienes and Dubey (2003) proposed a machine learning-based “trace tagger” as a preprocess of parsing. Campbell (2004) proposed a rule-based post-processing method based on linguistically motivated rules. Gabbard et al. (2006) replaced the rules with machine learning-based classifiers. Schmid (2006) and Cai et al. (2011) integrated empty category detection with the syntactic parsing. Empty category detection for pro (dropped pronouns or zero pronoun) has begun to receive attention as the Chinese Penn Treebank (Xue et al., 2005) has annotations for pro as well as PRO and trace. Xue and Yang (2013) formalized the proble"
D15-1156,N06-1024,0,0.121015,"of-the-art statistical syntactic parsers had typically ignored empty categories. Although Penn Treebank (Marcus et al., 1993) has annotations on PRO and trace, they provide only labeled bracketing. Johnson (2002) proposed a statistical pattern-matching algorithm for post-processing the results of syntactic parsing based on minimal unlexicalized tree fragments from empty node to its antecedent. Dienes and Dubey (2003) proposed a machine learning-based “trace tagger” as a preprocess of parsing. Campbell (2004) proposed a rule-based post-processing method based on linguistically motivated rules. Gabbard et al. (2006) replaced the rules with machine learning-based classifiers. Schmid (2006) and Cai et al. (2011) integrated empty category detection with the syntactic parsing. Empty category detection for pro (dropped pronouns or zero pronoun) has begun to receive attention as the Chinese Penn Treebank (Xue et al., 2005) has annotations for pro as well as PRO and trace. Xue and Yang (2013) formalized the problem as classifying each pair of the location of empty category and its head word in the dependency structure. Wang et al. (2015) proposed a joint embedding of empty categories and their contexts on depen"
D15-1156,P02-1018,0,0.502528,"es such as Japanese, in particular, for the machine translation from pro-drop languages to nonpro-drop languages such as English. Chung and Gildea (2010) reported their recover of empty categories improved the accuracy of machine translation both in Korean and in Chinese. Kudo et al. (2014) showed that generating zero subjects in Japanese improved the accuracy of preorderingbased translation. State-of-the-art statistical syntactic parsers had typically ignored empty categories. Although Penn Treebank (Marcus et al., 1993) has annotations on PRO and trace, they provide only labeled bracketing. Johnson (2002) proposed a statistical pattern-matching algorithm for post-processing the results of syntactic parsing based on minimal unlexicalized tree fragments from empty node to its antecedent. Dienes and Dubey (2003) proposed a machine learning-based “trace tagger” as a preprocess of parsing. Campbell (2004) proposed a rule-based post-processing method based on linguistically motivated rules. Gabbard et al. (2006) replaced the rules with machine learning-based classifiers. Schmid (2006) and Cai et al. (2011) integrated empty category detection with the syntactic parsing. Empty category detection for p"
D15-1156,kawahara-kurohashi-2006-case,0,0.0838813,"Missing"
D15-1156,P14-2091,0,0.145002,"Missing"
D15-1156,P04-1082,0,0.775457,"hat generating zero subjects in Japanese improved the accuracy of preorderingbased translation. State-of-the-art statistical syntactic parsers had typically ignored empty categories. Although Penn Treebank (Marcus et al., 1993) has annotations on PRO and trace, they provide only labeled bracketing. Johnson (2002) proposed a statistical pattern-matching algorithm for post-processing the results of syntactic parsing based on minimal unlexicalized tree fragments from empty node to its antecedent. Dienes and Dubey (2003) proposed a machine learning-based “trace tagger” as a preprocess of parsing. Campbell (2004) proposed a rule-based post-processing method based on linguistically motivated rules. Gabbard et al. (2006) replaced the rules with machine learning-based classifiers. Schmid (2006) and Cai et al. (2011) integrated empty category detection with the syntactic parsing. Empty category detection for pro (dropped pronouns or zero pronoun) has begun to receive attention as the Chinese Penn Treebank (Xue et al., 2005) has annotations for pro as well as PRO and trace. Xue and Yang (2013) formalized the problem as classifying each pair of the location of empty category and its head word in the depende"
D15-1156,levy-andrew-2006-tregex,0,0.0981205,"Missing"
D15-1156,D10-1062,0,0.0906353,"Missing"
D15-1156,D14-1162,0,0.0853881,"and HEAD is:IP-MAT× 連れ (tsure) and the combinations of PATH and CHILD are: IP-MAT×PP-を (wo), IP-MAT×VB, IP-MAT×VB2, IP-MAT×AXD-た (ta) and IP-MAT×PU-。 . 3.1 Using Word Embedding to approximate Case Frame Lexicon A case frame lexicon would be obviously useful for empty category detection because it provides information on the type of argument the verb in question takes. The problem is that case frame lexicon is not usually readily available. We propose a novel method to approximate case frame lexicon for languages with explicit case marking such as Japanese using word embeddings. According to (Pennington et al., 2014), they designed their embedding model GloVe so that the dot product of two word embeddings approximates the logarithm of their co-occurrence counts. Using this characteristic, we can easily make a feature that approximate the case frame of a verb. Given a set of word embeddings for case particles q1 , q2 , · · · , qN ∈ Q, the distributed case frame feature (DCF) for a verb wi is defined as: v˜i = wi · (q1 , q2 , · · · , qN ) v˜i vi = ||v˜i || Proposed model In the proposed model, we use combinations of path features and three other features, namely head word feature, child feature and empty ca"
D15-1156,N07-1051,0,0.0334814,"Missing"
D15-1156,C04-1024,0,0.0843671,"Missing"
D15-1156,P06-1023,0,0.533195,"lthough Penn Treebank (Marcus et al., 1993) has annotations on PRO and trace, they provide only labeled bracketing. Johnson (2002) proposed a statistical pattern-matching algorithm for post-processing the results of syntactic parsing based on minimal unlexicalized tree fragments from empty node to its antecedent. Dienes and Dubey (2003) proposed a machine learning-based “trace tagger” as a preprocess of parsing. Campbell (2004) proposed a rule-based post-processing method based on linguistically motivated rules. Gabbard et al. (2006) replaced the rules with machine learning-based classifiers. Schmid (2006) and Cai et al. (2011) integrated empty category detection with the syntactic parsing. Empty category detection for pro (dropped pronouns or zero pronoun) has begun to receive attention as the Chinese Penn Treebank (Xue et al., 2005) has annotations for pro as well as PRO and trace. Xue and Yang (2013) formalized the problem as classifying each pair of the location of empty category and its head word in the dependency structure. Wang et al. (2015) proposed a joint embedding of empty categories and their contexts on dependency structure. Xiang et al. (2013) formalized the problem as classifying"
D15-1156,P13-1081,0,0.276745,"es with machine learning-based classifiers. Schmid (2006) and Cai et al. (2011) integrated empty category detection with the syntactic parsing. Empty category detection for pro (dropped pronouns or zero pronoun) has begun to receive attention as the Chinese Penn Treebank (Xue et al., 2005) has annotations for pro as well as PRO and trace. Xue and Yang (2013) formalized the problem as classifying each pair of the location of empty category and its head word in the dependency structure. Wang et al. (2015) proposed a joint embedding of empty categories and their contexts on dependency structure. Xiang et al. (2013) formalized the problem as classifying each IP node (roughly corresponds to S and SBAR in Penn Treebank) in the phrase structure. In this paper, we propose a novel method for empty category detection for Japanese that uses conjunction features on phrase structure and word embeddings. We use the Keyaki Treebank (Butler et al., 2012), which is a recent development. As it has annotations for pro and trace, we show our method has substantial improvements over the state-of-the-art machine learning-based method (Xiang et al., 2013) for Chinese empty category detection as well as linguistically-motiv"
D15-1156,N13-1125,0,0.161969,"o its antecedent. Dienes and Dubey (2003) proposed a machine learning-based “trace tagger” as a preprocess of parsing. Campbell (2004) proposed a rule-based post-processing method based on linguistically motivated rules. Gabbard et al. (2006) replaced the rules with machine learning-based classifiers. Schmid (2006) and Cai et al. (2011) integrated empty category detection with the syntactic parsing. Empty category detection for pro (dropped pronouns or zero pronoun) has begun to receive attention as the Chinese Penn Treebank (Xue et al., 2005) has annotations for pro as well as PRO and trace. Xue and Yang (2013) formalized the problem as classifying each pair of the location of empty category and its head word in the dependency structure. Wang et al. (2015) proposed a joint embedding of empty categories and their contexts on dependency structure. Xiang et al. (2013) formalized the problem as classifying each IP node (roughly corresponds to S and SBAR in Penn Treebank) in the phrase structure. In this paper, we propose a novel method for empty category detection for Japanese that uses conjunction features on phrase structure and word embeddings. We use the Keyaki Treebank (Butler et al., 2012), which"
D15-1156,J93-2004,0,\N,Missing
D16-1112,W13-2322,0,0.0542076,"ted edges represent a relationship between nodes. Concepts &lt;s&gt; canada od m a1 a3 country na m e a2 nato prime op 1 name … summary AMR of the input sentence “canadian” yi+1 encAMR aj … announce … … a1 &lt;s&gt; E 0 yi C+1 canada … nato headline prime op 1 name a3 country od m na m e a2 E 0 yi ABS “canadian” AMR of the input sentence Figure 2: Model structure of our proposed attentionbased AMR encoder; it outputs a headline using ABS and encoded AMR with attention. consist of English words, PropBank event predicates, and special labels such as “person”. For edges, AMR has approximately 100 relations (Banarescu et al., 2013) including semantic roles based on the PropBank annotations in OntoNotes (Hovy et al., 2006). To acquire AMRs for input sentences, we use the state-of-the-art transition-based AMR parser (Wang et al., 2015). 3.2 Attention-Based AMR Encoder Figure 2 shows a brief sketch of the model structure of our attention-based AMR encoder model. We utilize a variant of child-sum Tree-LSTM originally proposed in (Tai et al., 2015) to encode syntactic and semantic information obtained from output of the AMR parser into certain fixed-length embedding vectors. To simplify the computation, we transform a DAG st"
D16-1112,N16-1012,0,0.30206,"e 3 supports this observation. For example, ABS+AMR successfully added the correct modifier ‘saudi’ to “crown prince” in the first example. Moreover, ABS+AMR generated a consistent subject in the third example. The comparison between ABS+AMR(w/o attn) and ABS+AMR (with attention) suggests that the attention mechanism is necessary for AMR encoding. In other words, the encoder without the attention mechanism tends to be overfitting. 1058 5 Related Work Recently, the Recurrent Neural Network (RNN) and its variant have been applied successfully to various NLP tasks. For headline generation tasks, Chopra et al. (2016) exploited the RNN decoder (and its variant) with the attention mechanism instead of the method of Rush et al. (2015): the combination of the feed-forward neural network language model and attention-based sentence encoder. Nallapati et al. (2016) also adapted the RNN encoder-decoder with attention for headline generation tasks. Moreover, they made some efforts such as hierarchical attention to improve the performance. In addition to using a variant of RNN, Gulcehre et al. (2016) proposed a method to handle infrequent words in natural language generation. Note that these recent developments do"
D16-1112,P16-1014,0,0.0597202,"ural Network (RNN) and its variant have been applied successfully to various NLP tasks. For headline generation tasks, Chopra et al. (2016) exploited the RNN decoder (and its variant) with the attention mechanism instead of the method of Rush et al. (2015): the combination of the feed-forward neural network language model and attention-based sentence encoder. Nallapati et al. (2016) also adapted the RNN encoder-decoder with attention for headline generation tasks. Moreover, they made some efforts such as hierarchical attention to improve the performance. In addition to using a variant of RNN, Gulcehre et al. (2016) proposed a method to handle infrequent words in natural language generation. Note that these recent developments do not conflict with our method using the AMR encoder. This is because the AMR encoder can be straightforwardly incorporated into their methods as we have done in this paper, incorporating the AMR encoder into the baseline. We believe that our AMR encoder can possibly further improve the performance of their methods. We will test that hypothesis in future study. 6 Conclusion This paper mainly discussed the usefulness of incorporating structural syntactic and semantic information in"
D16-1112,N06-2015,0,0.0110087,"2 nato prime op 1 name … summary AMR of the input sentence “canadian” yi+1 encAMR aj … announce … … a1 &lt;s&gt; E 0 yi C+1 canada … nato headline prime op 1 name a3 country od m na m e a2 E 0 yi ABS “canadian” AMR of the input sentence Figure 2: Model structure of our proposed attentionbased AMR encoder; it outputs a headline using ABS and encoded AMR with attention. consist of English words, PropBank event predicates, and special labels such as “person”. For edges, AMR has approximately 100 relations (Banarescu et al., 2013) including semantic roles based on the PropBank annotations in OntoNotes (Hovy et al., 2006). To acquire AMRs for input sentences, we use the state-of-the-art transition-based AMR parser (Wang et al., 2015). 3.2 Attention-Based AMR Encoder Figure 2 shows a brief sketch of the model structure of our attention-based AMR encoder model. We utilize a variant of child-sum Tree-LSTM originally proposed in (Tai et al., 2015) to encode syntactic and semantic information obtained from output of the AMR parser into certain fixed-length embedding vectors. To simplify the computation, we transform a DAG structure of AMR parser output to a tree structure, which we refer to as “tree-converted AMR s"
D16-1112,W04-1013,0,0.222018,"esults if we observed statistical difference (p &lt; 0.05) between ABS (re-run) and ABS+AMR on the t-test. (R-1: ROUGE-1, R-2: ROUGE-2, R-L: ROUGE-L) For a fair comparison, we followed their evaluation setting. The training data was obtained from the first sentence and the headline of a document in the annotated Gigaword corpus (Napoles et al., 2012)4 . The development data is DUC-2003 data, and test data are both DUC-2004 (Over et al., 2007) and sentence-headline pairs obtained from the annotated Gigaword corpus as well as training data5 . All of the generated headlines were evaluated by ROUGE (Lin, 2004)6 . For evaluation on DUC2004, we removed strings after 75-characters for each generated headline as described in the DUC2004 evaluation. For evaluation on Gigaword, we forced the system outputs to be at most 8 words as in Rush et al. (2015) since the average length of headline in Gigaword is 8.3 words. For the preprocessing for all data, all letters were converted to lower case, all digits were replaced with ‘#’, and words appearing less than five times with ‘UNK’. Note that, for further evaluation, we prepared 2,000 sentence-headline pairs randomly sampled from the test data section of the G"
D16-1112,K16-1028,0,0.0442533,"/o attn) and ABS+AMR (with attention) suggests that the attention mechanism is necessary for AMR encoding. In other words, the encoder without the attention mechanism tends to be overfitting. 1058 5 Related Work Recently, the Recurrent Neural Network (RNN) and its variant have been applied successfully to various NLP tasks. For headline generation tasks, Chopra et al. (2016) exploited the RNN decoder (and its variant) with the attention mechanism instead of the method of Rush et al. (2015): the combination of the feed-forward neural network language model and attention-based sentence encoder. Nallapati et al. (2016) also adapted the RNN encoder-decoder with attention for headline generation tasks. Moreover, they made some efforts such as hierarchical attention to improve the performance. In addition to using a variant of RNN, Gulcehre et al. (2016) proposed a method to handle infrequent words in natural language generation. Note that these recent developments do not conflict with our method using the AMR encoder. This is because the AMR encoder can be straightforwardly incorporated into their methods as we have done in this paper, incorporating the AMR encoder into the baseline. We believe that our AMR e"
D16-1112,W12-3018,0,0.0606696,"Missing"
D16-1112,D15-1044,0,0.311302,"ral attention-based model. We encode results obtained from an abstract meaning representation (AMR) parser using a modified version of Tree-LSTM. Our proposed attention-based AMR encoder-decoder model improves headline generation benchmarks compared with the baseline neural attention-based model. 1 Introduction Neural network-based encoder-decoder models are cutting-edge methodologies for tackling natural language generation (NLG) tasks, i.e., machine translation (Cho et al., 2014), image captioning (Vinyals et al., 2015), video description (Venugopalan et al., 2015), and headline generation (Rush et al., 2015). This paper also shares a similar goal and motivation to previous work: improving the encoderdecoder models for natural language generation. There are several directions for enhancement. This paper respects the fact that NLP researchers have expended an enormous amount of effort to develop fundamental NLP techniques such as POS tagging, dependency parsing, named entity recognition, and semantic role labeling. Intuitively, this structural, syntactic, and semantic information underlying input text has the potential for improving the quality of NLG tasks. However, to the best of our knowledge, t"
D16-1112,P15-1150,0,0.0400517,"ing the quality of NLG tasks. However, to the best of our knowledge, there is no clear evidence that syntactic and semantic information can enhance the recently developed encoder-decoder models in NLG tasks. To answer this research question, this paper proposes and evaluates a headline generation method based on an encoder-decoder architecture on Abstract Meaning Representation (AMR). The method is essentially an extension of attention-based summarization (ABS) (Rush et al., 2015). Our proposed method encodes results obtained from an AMR parser by using a modified version of TreeLSTM encoder (Tai et al., 2015) as additional information of the baseline ABS model. Conceptually, the reason for using AMR for headline generation is that information presented in AMR, such as predicate-argument structures and named entities, can be effective clues when producing shorter summaries (headlines) from original longer sentences. We expect that the quality of headlines will improve with this reasonable combination (ABS and AMR). 2 Attention-based summarization (ABS) ABS proposed in Rush et al. (2015) has achieved state-of-the-art performance on the benchmark data of headline generation including the DUC-2004 dat"
D16-1112,N15-1173,0,0.0191451,"ormation additionally incorporated in a baseline neural attention-based model. We encode results obtained from an abstract meaning representation (AMR) parser using a modified version of Tree-LSTM. Our proposed attention-based AMR encoder-decoder model improves headline generation benchmarks compared with the baseline neural attention-based model. 1 Introduction Neural network-based encoder-decoder models are cutting-edge methodologies for tackling natural language generation (NLG) tasks, i.e., machine translation (Cho et al., 2014), image captioning (Vinyals et al., 2015), video description (Venugopalan et al., 2015), and headline generation (Rush et al., 2015). This paper also shares a similar goal and motivation to previous work: improving the encoderdecoder models for natural language generation. There are several directions for enhancement. This paper respects the fact that NLP researchers have expended an enormous amount of effort to develop fundamental NLP techniques such as POS tagging, dependency parsing, named entity recognition, and semantic role labeling. Intuitively, this structural, syntactic, and semantic information underlying input text has the potential for improving the quality of NLG ta"
D16-1112,N15-1040,0,0.0546549,"ed method encodes results obtained from an AMR parser by using a modified version of TreeLSTM encoder (Tai et al., 2015) as additional information of the baseline ABS model. Conceptually, the reason for using AMR for headline generation is that information presented in AMR, such as predicate-argument structures and named entities, can be effective clues when producing shorter summaries (headlines) from original longer sentences. We expect that the quality of headlines will improve with this reasonable combination (ABS and AMR). 2 Attention-based summarization (ABS) ABS proposed in Rush et al. (2015) has achieved state-of-the-art performance on the benchmark data of headline generation including the DUC-2004 dataset (Over et al., 2007). Figure 1 illustrates the model structure of ABS. The model predicts a word sequence (summary) based on the combination of the neural network language model and an input sentence encoder. Let V be a vocabulary. xi is the i-th indicator vector corresponding to the i-th word in the input sentence. Suppose we have M words of an input sentence. X represents an input sentence, which 1054 Proceedings of the 2016 Conference on Empirical Methods in Natural Language"
D18-1450,hovy-etal-2006-automated,0,0.693296,"t consist of Elementary Discourse Units (EDUs) obtained from source documents and then weight every EDU by counting the number of extractive reference summaries that contain the EDU. A summary is scored by the correspondences between EDUs in the summary and those in the pyramid. Experiments on DUC and TAC data sets show that our methods strongly correlate with various manual evaluations. 1 Introduction To develop high quality summarization systems, we need accurate automatic content evaluation. Although, various evaluation measures have been proposed, ROUGE-N (Lin, 2004), Basic Elements (BE) (Hovy et al., 2006) remain the de facto standard measures since they strongly correlate with various manual evaluations and are easy to use. However, the evaluation scores computed by these automatic measures are not so useful for improving system performance because they merely confirm if the summary contains small textual fragments and so they do not address semantic correctness. The pyramid method was proposed as a manual evaluation that well supports the improvement of summarization systems (Nenkova and Passonneau, 2004; Nenkova et al., 2007). First, the method identifies conceptual contents, Summary Content"
D18-1450,P15-1162,0,0.0742783,"Missing"
D18-1450,N16-1030,0,0.0177166,"d for multi document summarization tasks in DUC-2003 to 2007 and TAC-2008 to 2011. Table 1 and Table 2 show the statistics of the data sets. 4.2 EDU Segmenter We regard decomposing a sentence into EDUs as a sequential tagging problem and implement a neural EDU segmenter that classifies each word in a sentence as the boundary of EDU or not based on 3-layer bi-LSTM (Wang et al., 2015). The size of word embeddings and hidden layers of the LSTM were set to 100 and 256, respectively. To handle low-frequency words, all words are encoded to 40 dimension hidden state by using character-based bi-LSTM (Lample et al., 2016). To utilize entire words in a corpus, we integrated word dropout (Iyyer et al., 2015) into our models with smoothing rate, 1.0. Moreover, to avoid overfitting the training data, dropout layer was adopted to the input of the LSTMs with the ratio 0.3. The segmenter was trained by utilizing the training data of RST Discourse Treebank corpus (Carlson et al., 2001). The macro-averaged Fmeasure of boundary detection on the test data of the corpus is 0.917. The source documents, system summaries and reference summaries utilized 4182 2003 Cov. ROUGE-2 .906/.821/.617 ROUGE-SU4 .782/.774/.600 BE .927/."
D18-1450,W16-3617,0,0.0220435,"e documents. In other words, we regard EDUs as alternatives to SCUs. To construct the pyramid, we transform human-made reference summaries into EDU-based extractive reference summaries and then weight every EDU by counting the number of the extractive reference summaries that contain the EDU. The rea4177 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4177–4186 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics son why we derive extractive reference summaries whose SCUs are EDUs is as follows. First, Li et al. (2016) reported that EDUs are very similar to SCUs. Second, the performance of EDU segmenter is sufficient to satisfy practical requirements (see Section 2). Third, we do not need measure any semantic similarity to identify EDUs common to the extractive reference summaries. We also examine two types of extractive reference summary. One is based on the alignment between EDUs in reference summary and source documents. The other is based on the extractive oracle summary (Hirao et al., 2017). We conducted experiments on the Document Understanding Conference (DUC) 2003 to 2007 data sets and Text Analysis"
D18-1450,W01-1605,0,0.107287,"STM (Wang et al., 2015). The size of word embeddings and hidden layers of the LSTM were set to 100 and 256, respectively. To handle low-frequency words, all words are encoded to 40 dimension hidden state by using character-based bi-LSTM (Lample et al., 2016). To utilize entire words in a corpus, we integrated word dropout (Iyyer et al., 2015) into our models with smoothing rate, 1.0. Moreover, to avoid overfitting the training data, dropout layer was adopted to the input of the LSTMs with the ratio 0.3. The segmenter was trained by utilizing the training data of RST Discourse Treebank corpus (Carlson et al., 2001). The macro-averaged Fmeasure of boundary detection on the test data of the corpus is 0.917. The source documents, system summaries and reference summaries utilized 4182 2003 Cov. ROUGE-2 .906/.821/.617 ROUGE-SU4 .782/.774/.600 BE .927/.862/.617 − PEAK Prop(BE) .936/.909/.750 Prop(ROUGE) .908/.874/.750 Prop(AL) .831/.841/.633 2004 Cov. .909/.838/.691 .854/.772/.559 .936/.868/.721 − .929/.892/.750 .938/.814/.676 .904/.855/.735 2005 Resp. .932/.931/.792 .925/.893/.731 .897/.867/.714 − .845/.819/.657 .864/.809/.629 .821/.757/.567 2006 Resp. Pyr. .836/.767/.584 .905/.884/.740 .849/.790/.601 .885/."
D18-1450,W04-1013,0,0.0808668,"xtractive reference summaries that consist of Elementary Discourse Units (EDUs) obtained from source documents and then weight every EDU by counting the number of extractive reference summaries that contain the EDU. A summary is scored by the correspondences between EDUs in the summary and those in the pyramid. Experiments on DUC and TAC data sets show that our methods strongly correlate with various manual evaluations. 1 Introduction To develop high quality summarization systems, we need accurate automatic content evaluation. Although, various evaluation measures have been proposed, ROUGE-N (Lin, 2004), Basic Elements (BE) (Hovy et al., 2006) remain the de facto standard measures since they strongly correlate with various manual evaluations and are easy to use. However, the evaluation scores computed by these automatic measures are not so useful for improving system performance because they merely confirm if the summary contains small textual fragments and so they do not address semantic correctness. The pyramid method was proposed as a manual evaluation that well supports the improvement of summarization systems (Nenkova and Passonneau, 2004; Nenkova et al., 2007). First, the method identi"
D18-1450,de-marneffe-etal-2006-generating,0,0.183189,"Missing"
D18-1450,P07-1062,0,0.0425633,"on Alignment between EDUs e50 e41 maximize W=3 φ(ej , mk )aj,k (1) j=1 k=1 W=2 e30 |E ||M| X X s.t. W=1 |E ||M| X X `(ej )aj,k ≤ Lmax (2) j=1 k=1 |E| X Figure 1: Overview of our pyramid construction. aj,k ≤ 1 ∀k (3) aj,k ≤ 1 ∀j (4) ∀j, k. (5) j=1 |M| (Pilehvar et al., 2013). As a result, the resultant pyramids have insufficient quality to be practical. Clearly, further improvement is necessary. While our method is required to decompose a document into EDUs accurately, the EDU segmenter offers accurate decomposition performance; existing EDU boundary detection methods have F-measures over 0.9 (Fisher and Roark, 2007; Feng and Hirst, 2014). Moreover, since extractive reference summaries are set of EDUs from the source documents, we do not need semantic similarity to identify EDUs that have the same meaning. Thus, we can easily construct a pyramid by simply counting the number of extractive reference summaries that contains each EDU. 3 Automatic Pyramid Evaluation First, we transform human-made reference summaries into extractive reference summaries; the EDUs in the source documents are used as the atomic units. Second, we construct a pyramid by weighting EDUs in the extractive reference summaries. EDU wei"
D18-1450,N04-1019,0,0.9025,"Although, various evaluation measures have been proposed, ROUGE-N (Lin, 2004), Basic Elements (BE) (Hovy et al., 2006) remain the de facto standard measures since they strongly correlate with various manual evaluations and are easy to use. However, the evaluation scores computed by these automatic measures are not so useful for improving system performance because they merely confirm if the summary contains small textual fragments and so they do not address semantic correctness. The pyramid method was proposed as a manual evaluation that well supports the improvement of summarization systems (Nenkova and Passonneau, 2004; Nenkova et al., 2007). First, the method identifies conceptual contents, Summary Content Units (SCUs), in reference summaries and then constructs a pyramid by collecting semantically equivalent SCUs. The weight of an SCU in the pyramid is defined as the number of reference summaries that contain the SCU. Thus, an SCU shared by many reference summaries is given higher weight. Second, a system summary is scored by the correspondences between SCUs in the summary and the pyramid. Its results are very useful for system improvement, i.e., we can know which important SCUs the system could or could"
D18-1450,P13-2026,0,0.102162,"hy a summary was given a good or bad score. During the past few years, studies have focused on the automatic scoring of summaries based on manually generated pyramids. Harnly et al. (2005) proposed a scoring method that matches SCUs in the pyramid with possible textual fragments in the summary. They enumerate all possible textual fragments within a sentence in the summary and compute similarity scores between the fragments and the SCUs in the pyramid based on unigram overlap. Then, they find the optimal correspondences between SCUs and the fragments that maximize the sum of similarity scores. Passonneau et al. (2013) extended the method by introducing distributional semantics to compute the similarity scores between SCUs and the fragments. Recently, Yang et al. (2016) proposed the first automatic pyramid method, Pyramid Evaluation via Automated Knowledge Extraction (PEAK). PEAK employs subject-predicate-object triples extracted by ClausIE (Del Corro and Gemulla, 2013) as SCUs, and constructs pyramids by cutting a graph whose vertices represent the triples and whose edges represent semantic similarity scores between the triples computed by Align, Disambiguate and Walk (ADW) (Pilehvar et al., 2013). When ev"
D18-1450,P13-1132,0,0.210488,"cores. Passonneau et al. (2013) extended the method by introducing distributional semantics to compute the similarity scores between SCUs and the fragments. Recently, Yang et al. (2016) proposed the first automatic pyramid method, Pyramid Evaluation via Automated Knowledge Extraction (PEAK). PEAK employs subject-predicate-object triples extracted by ClausIE (Del Corro and Gemulla, 2013) as SCUs, and constructs pyramids by cutting a graph whose vertices represent the triples and whose edges represent semantic similarity scores between the triples computed by Align, Disambiguate and Walk (ADW) (Pilehvar et al., 2013). When evaluating a summary, PEAK constructs a weighted bipartite graph whose vertices represent subject-predicate-object triples extracted from the pyramid and the summary, respectively; the edges represent the similarity scores between the triples as computed by ADW. It scores the summary by solving the Linear Assignment Problem which involves maximizing the sum of the similarity scores on the bipartite graph. The major difference between PEAK and our method is that the former regards the reference summary as a set of subject-predicate-object triples while the latter regards a reference summ"
D18-1489,A00-2018,0,0.377682,") and reranking them based on the perplexity obtained by a neural language model. To investigate the effectiveness of DOC, we evaluate our language models following their configurations. 7.1 Dataset We used the Wall Street Journal of the Penn Treebank dataset. We used the section 2-21 for training, 22 for validation, and 23 for testing. We applied the preprocessing codes of Choe and Charniak (2016)12 to the dataset and converted a token that appears fewer than ten times in the training dataset into a special token unk. For reranking, we prepared 500 candidates obtained by the Charniak parser (Charniak, 2000). 7.2 Models We compare AWD-LSTM-DOC with AWDLSTM (Merity et al., 2018) and AWD-LSTMMoS (Yang et al., 2018). We trained each model with the same hyperparameters from our language modeling experiments (Section 5). We selected the model that achieved the best perplexity on the validation set during the training. 7.3 Results Table 11 shows the bracketing F1 scores on the PTB test set. This table is divided into three parts by horizontal lines; the upper part describes the scores by single language modeling based rerankers, the middle part shows the results by ensembling five rerankers, and the lo"
D18-1489,P96-1041,0,0.345939,"ev and Klein, 2018). 8 Related Work Bengio et al. (2003) are pioneers of neural language models. To address the curse of dimensionality in language modeling, they proposed a method using word embeddings and a feed-forward neural network (FFNN). They demonstrated that their approach outperformed n-gram language models, but FFNN can only handle fixed-length contexts. Instead of FFNN, Mikolov et al. (2010) applied RNN (Elman, 1990) to language modeling to address the entire given sequence as a context. Their method outperformed the Kneser-Ney smoothed 5-gram language model (Kneser and Ney, 1995; Chen and Goodman, 1996). Researchers continue to try to improve the performance of RNN language models. Zaremba et al. (2014) used LSTM (Hochreiter and Schmidhuber, 1997) instead of a simple RNN for language modeling and significantly improved an RNN language model by applying dropout (Srivastava et al., 2014) to all the connections except for the recurrent connections. To regularize the recurrent connections, Gal and Ghahramani (2016) proposed variational inference-based dropout. Their method uses the same dropout mask at each timestep. Zolna et al. (2018) proposed fraternal dropout, which minimizes the differences"
D18-1489,D16-1257,0,0.358111,"ps://github.com/nttcslabnlp/doc lm. 1 p(w1:T ) = p(w1 ) TY −1 p(wt+1 |w1:t ). (1) t=1 Introduction Neural network language models have played a central role in recent natural language processing (NLP) advances. For example, neural encoderdecoder models, which were successfully applied to various natural language generation tasks including machine translation (Sutskever et al., 2014), summarization (Rush et al., 2015), and dialogue (Wen et al., 2015), can be interpreted as conditional neural language models. Neural language models also positively influence syntactic parsing (Dyer et al., 2016; Choe and Charniak, 2016). Moreover, such word embedding methods as Skipgram (Mikolov et al., 2013) and vLBL (Mnih and Kavukcuoglu, 2013) originated from neural language models designed to handle much larger vocabulary and data sizes. Neural language models can also be used as contextualized word representations (Peters et al., 2018). Thus, language modeling is a good benchmark task for investigating the general frameworks of neural methods in NLP field. p(w1 ) is generally assumed to be 1 in this literature, that is, p(w1 ) = 1, and thus we can ignore its calculation. See the implementation of Zaremba et al. (2014)1"
D18-1489,N16-1024,0,0.259497,"y available at: https://github.com/nttcslabnlp/doc lm. 1 p(w1:T ) = p(w1 ) TY −1 p(wt+1 |w1:t ). (1) t=1 Introduction Neural network language models have played a central role in recent natural language processing (NLP) advances. For example, neural encoderdecoder models, which were successfully applied to various natural language generation tasks including machine translation (Sutskever et al., 2014), summarization (Rush et al., 2015), and dialogue (Wen et al., 2015), can be interpreted as conditional neural language models. Neural language models also positively influence syntactic parsing (Dyer et al., 2016; Choe and Charniak, 2016). Moreover, such word embedding methods as Skipgram (Mikolov et al., 2013) and vLBL (Mnih and Kavukcuoglu, 2013) originated from neural language models designed to handle much larger vocabulary and data sizes. Neural language models can also be used as contextualized word representations (Peters et al., 2018). Thus, language modeling is a good benchmark task for investigating the general frameworks of neural methods in NLP field. p(w1 ) is generally assumed to be 1 in this literature, that is, p(w1 ) = 1, and thus we can ignore its calculation. See the implementation"
D18-1489,P17-2025,0,0.0638382,"Missing"
D18-1489,D15-1166,0,0.0301067,"ted sentenceheadline pairs into three parts: training, validation, and test sets. The training set contains about 3.8M sentence-headline pairs. For our evaluation, we used the test set constructed by Zhou et al. (2017) because the one constructed by Rush et al. (2015) contains some invalid instances, as reported in Zhou et al. (2017). 6.2 Encoder-Decoder Model For the base model, we adopted an encoder-decoder with an attention mechanism described in Kiyono et al. (2017). The encoder consists of a 2-layer bidirectional LSTM, and the decoder consists of a 2-layer LSTM with attention proposed by Luong et al. (2015). We interpreted the layer after computing the attention as the 3rd layer of the decoder. We refer to this encoder-decoder as EncDec. For the hyperparameters, we followed the setting of Kiyono et al. (2017) except for the sizes of hidden 9 De-En 28.18 29.12 29.33 En-Fr 34.37 36.09 36.11 Fr-En 34.07 34.41 34.72 Table 9: BLEU scores on test sets in the IWSLT 2016 dataset. We report averages of three runs. Experiments on Application Tasks 6.1 En-De 23.05 23.62 23.97 RG-1 46.77 46.91 46.99 37.41 46.86 46.34 RG-2 24.87 24.91 25.29 15.87 24.58 24.85 RG-L 43.58 43.73 43.83 34.70 43.53 43.49 Table 10:"
D18-1489,J93-2004,0,0.0612474,"for a mini-batch consisting of wb , wb+1 , ..., w˜b : B= ˜b X πct Vocab Train #Token Valid Test β= std(B) avg(B) Hyperparameter Learning rate Batch size Non-monotone interval De Dh1 Dh2 Dh3 Dropout rate for xt Dropout rate for h0t Dropout rate for h1t , h2t Dropout rate for h3t Dropout rate for kj,ct Recurrent weight dropout (12) 2 , (13) where functions std(·) and avg(·) are functions that respectively return an input’s standard deviation and its average. In the training step, we add λβ multiplied by weight coefficient β to the loss function. 5 5.1 Datasets We used the Penn Treebank (PTB) (Marcus et al., 1993) and WikiText-2 (Merity et al., 2017) datasets, which are the standard benchmark datasets for the word-level language modeling task. Mikolov et al. (2010) and Merity et al. (2017) respectively published preprocessed PTB3 and WikiText-24 datasets. Table 1 describes their statistics. We used these preprocessed datasets for fair comparisons with previous studies. 5.2 Hyperparameters Our implementation is based on the averaged stochastic gradient descent Weight-Dropped LSTM (AWD-LSTM)5 proposed by Merity et al. (2018). AWD-LSTM consists of three LSTMs with various regularizations. For the hyperpar"
D18-1489,P18-1249,0,0.0249447,"ing order because it samples candidates based on their scores. Thus, we prepared more candidates (i.e., 700) to be able to obtain correct instances as candidates. 14 We used the deep bidirectional encoder described at http://opennmt.net/OpenNMT/training/models/ instead of a basic bidirectional encoder. 4606 the probability distributions in the decoder. The retrained parser achieved 93.12 F1 score. Finally, we achieved 94.47 F1 score by reranking its candidates with AWD-LSTM-DOC. We expect that we can achieve even better score by replacing the base parser with the current state-of-the-art one (Kitaev and Klein, 2018). 8 Related Work Bengio et al. (2003) are pioneers of neural language models. To address the curse of dimensionality in language modeling, they proposed a method using word embeddings and a feed-forward neural network (FFNN). They demonstrated that their approach outperformed n-gram language models, but FFNN can only handle fixed-length contexts. Instead of FFNN, Mikolov et al. (2010) applied RNN (Elman, 1990) to language modeling to address the entire given sequence as a context. Their method outperformed the Kneser-Ney smoothed 5-gram language model (Kneser and Ney, 1995; Chen and Goodman, 1"
D18-1489,W12-3018,0,0.0661917,"Missing"
D18-1489,N18-1202,0,0.0245701,"generation tasks including machine translation (Sutskever et al., 2014), summarization (Rush et al., 2015), and dialogue (Wen et al., 2015), can be interpreted as conditional neural language models. Neural language models also positively influence syntactic parsing (Dyer et al., 2016; Choe and Charniak, 2016). Moreover, such word embedding methods as Skipgram (Mikolov et al., 2013) and vLBL (Mnih and Kavukcuoglu, 2013) originated from neural language models designed to handle much larger vocabulary and data sizes. Neural language models can also be used as contextualized word representations (Peters et al., 2018). Thus, language modeling is a good benchmark task for investigating the general frameworks of neural methods in NLP field. p(w1 ) is generally assumed to be 1 in this literature, that is, p(w1 ) = 1, and thus we can ignore its calculation. See the implementation of Zaremba et al. (2014)1 , for an example. RNN language models obtain conditional probability p(wt+1 |w1:t ) from the probability distribution of each word. To compute the probability distribution, RNN language models encode sequence w1:t into a fixed-length vector and apply a transformation matrix and the softmax function. Previous"
D18-1489,E17-2025,0,0.0351521,"To regularize the recurrent connections, Gal and Ghahramani (2016) proposed variational inference-based dropout. Their method uses the same dropout mask at each timestep. Zolna et al. (2018) proposed fraternal dropout, which minimizes the differences between outputs from different dropout masks to be invariant to the dropout mask. Melis et al. (2018) used black-box optimization to find appropriate hyperparameters for RNN language models and demonstrated that the standard LSTM with proper regularizations can outperform other architectures. Apart from dropout techniques, Inan et al. (2017) and Press and Wolf (2017) proposed the word tying method (WT), which unifies word embeddings (E in Equation 4) with the weight matrix to compute probability distributions (W in Equation 2). In addition to quantitative evaluation, Inan et al. (2017) provided a theoretical justification for WT and proposed the augmented loss technique (AL), which computes an objective probability based on word embeddings. In addition to these regularization techniques, Merity et al. (2018) used DropConnect (Wan et al., 2013) and averaged SGD (Polyak and Juditsky, 1992) for an LSTM language model. Their AWD-LSTM achieved lower perplexity"
D18-1489,D15-1044,0,0.43792,"e the standard benchmark datasets. Moreover, we indicate our proposed method contributes to two application tasks: machine translation and headline generation. Our code is publicly available at: https://github.com/nttcslabnlp/doc lm. 1 p(w1:T ) = p(w1 ) TY −1 p(wt+1 |w1:t ). (1) t=1 Introduction Neural network language models have played a central role in recent natural language processing (NLP) advances. For example, neural encoderdecoder models, which were successfully applied to various natural language generation tasks including machine translation (Sutskever et al., 2014), summarization (Rush et al., 2015), and dialogue (Wen et al., 2015), can be interpreted as conditional neural language models. Neural language models also positively influence syntactic parsing (Dyer et al., 2016; Choe and Charniak, 2016). Moreover, such word embedding methods as Skipgram (Mikolov et al., 2013) and vLBL (Mnih and Kavukcuoglu, 2013) originated from neural language models designed to handle much larger vocabulary and data sizes. Neural language models can also be used as contextualized word representations (Peters et al., 2018). Thus, language modeling is a good benchmark task for investigating the general frame"
D18-1489,P16-1009,0,0.0259373,"in the IWSLT 2016 dataset. We report averages of three runs. Experiments on Application Tasks 6.1 En-De 23.05 23.62 23.97 RG-1 46.77 46.91 46.99 37.41 46.86 46.34 RG-2 24.87 24.91 25.29 15.87 24.58 24.85 RG-L 43.58 43.73 43.83 34.70 43.53 43.49 Table 10: ROUGE F1 scores in headline generation test data provided by Zhou et al. (2017). RG in table denotes ROUGE. For our implementations (the upper part), we report averages of three runs. states and embeddings. We used 500 for machine translation and 400 for headline generation. We constructed a vocabulary set by using Byte-PairEncoding10 (BPE) (Sennrich et al., 2016). We set the number of BPE merge operations at 16K for the machine translation and 5K for the headline generation. In this experiment, we compare DOC to the base EncDec. We prepared two DOC settings: using only the final layer, that is, a setting that is identical to MoS, and using both the final and middle layers. We used the 2nd and 3rd layers in the latter setting because this case achieved the best performance on the language modeling task in Section 5.3. We set i3 = 2 and i2 = 2, i3 = 2. For this experiment, we modified a publicly available encode-decoder implementation11 . 6.3 Results Ta"
D18-1489,P18-2097,1,0.851186,"esults indicate that DOC positively influences a neural encoder-decoder model. Using the middle layer also yields further improvement because EncDec+DOC (i3 = i2 = 2) outperformed EncDec+DOC (i3 = 2). 7 Model Base Reranking with single model Choe and Charniak (2016) 89.7 AWD-LSTM 89.7 AWD-LSTM-MoS 89.7 AWD-LSTM-DOC 89.7 Reranking with model ensemble AWD-LSTM × 5 (ensemble) 89.7 AWD-LSTM-MoS × 5 (ensemble) 89.7 AWD-LSTM-DOC × 5 (ensemble) 89.7 AWD-LSTM-DOC × 5 (ensemble) 91.2 AWD-LSTM-DOC × 5 (ensemble) 93.12 State-of-the-art results Dyer et al. (2016) 91.7 Fried et al. (2017) (ensemble) 92.72 Suzuki et al. (2018) (ensemble) 92.74 Kitaev and Klein (2018) 95.13 Experiments on Constituency Parsing Choe and Charniak (2016) achieved high F1 scores on the Penn Treebank constituency parsing task by transforming candidate trees into a symbol sequence (S-expression) and reranking them based on the perplexity obtained by a neural language model. To investigate the effectiveness of DOC, we evaluate our language models following their configurations. 7.1 Dataset We used the Wall Street Journal of the Penn Treebank dataset. We used the section 2-21 for training, 22 for validation, and 23 for testing. We applied th"
D18-1489,I17-2008,1,0.256626,"the number of kj,ct from hnt . Then we PNdefine the sum of in for all n as J; that is, n=0 in = J. In short, DOC computes J probability distributions from all the layers, including the input embedding (h0 ). For iN = J, DOC becomes identical to MoS. In addition to increasing the rank, we expect that DOC weakens the vanishing gradient problem during backpropagation because a middle layer is directly connected to the output, such as with the auxiliary classifiers described in Szegedy et al. (2015). For a network that computes the weights for several vectors, such as Equation 10, Shazeer et al. (2017) indicated that it often converges to a state where it always produces large weights for few vectors. In fact, we observed that DOC tends to assign large weights to shallow layers. To prevent this phenomenon, we compute the coefficient of 4601 variation of Equation 10 in each mini-batch as a regularization term following Shazeer et al. (2017). In other words, we try to adjust the sum of the weights for each probability distribution with identical values in each mini-batch. Formally, we compute the following equation for a mini-batch consisting of wb , wb+1 , ..., w˜b : B= ˜b X πct Vocab Train"
D18-1489,D15-1199,0,0.0428095,"Missing"
D18-1489,P17-1101,0,0.0291304,"En), from English to French (En-Fr), and its reverse (Fr-En). Headline generation is a task that creates a short summarization of an input sentence(Rush et al., 2015). Rush et al. (2015) constructed a headline generation dataset by extracting pairs of first sentences of news articles and their headlines from the annotated English Gigaword corpus (Napoles et al., 2012). They also divided the extracted sentenceheadline pairs into three parts: training, validation, and test sets. The training set contains about 3.8M sentence-headline pairs. For our evaluation, we used the test set constructed by Zhou et al. (2017) because the one constructed by Rush et al. (2015) contains some invalid instances, as reported in Zhou et al. (2017). 6.2 Encoder-Decoder Model For the base model, we adopted an encoder-decoder with an attention mechanism described in Kiyono et al. (2017). The encoder consists of a 2-layer bidirectional LSTM, and the decoder consists of a 2-layer LSTM with attention proposed by Luong et al. (2015). We interpreted the layer after computing the attention as the 3rd layer of the decoder. We refer to this encoder-decoder as EncDec. For the hyperparameters, we followed the setting of Kiyono et al."
D19-1587,D15-1263,0,0.141551,"similarity score function that is defined for merging two adjacent spans into a large one. Experimental results on English and German RST treebanks showed that our parser based on span merging achieved the best score, around 0.8 F1 score, which is close to the scores of the previous supervised parsers. 1 Introduction Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) is one of the theories that are most widely utilized for representing a discourse structure of a text in downstream NLP applications, such as automatic summarization (Marcu, 1998; Hirao et al., 2013), sentiment analysis (Bhatia et al., 2015), and text categorization (Ji and Smith, 2017). RST represents a text as a kind of constituent tree, whose leaves are Elementary Discourse Units (EDUs), clause-like units, and whose non-terminal nodes cover text spans consisting of a sequence of EDUs or a singleton EDU. The label of a non-terminal node represents the attribution of a text span, nucleus or satellite. A discourse relation is also assigned between two adjacent nonterminal nodes. Since the RST tree can be regarded as a standard constituent (phrase structure) tree, syntactic parsing models for constituency parsing can also be succe"
D19-1587,E17-1028,0,0.116463,"it is significant that our method outperformed baselines on D2S2E settings. Moreover, we compared our method with some supervised RST parsers on the test set of RSTDT. Table 3 shows the results. In the table, HHN16 denotes a simple transition-based RST parser (Hayashi et al., 2016), and WLW17 denotes a current state-of-the-art transition-based RST parser (Wang et al., 2017). Although the score of our method is lower than the scores of the supervised parsers, the score is close to that of HHN16. The results demonstrated the effectiveness of our unsupervised RST parsing method. For reference, (Braud et al., 2017) reported that their supervised RST parser obtained .802 span F1 score on PCC. However, we cannot compare the score with our score because the test set differs from ours. 4 Conclusion This paper proposed two kinds of unsupervised RST parsing methods based on dynamic programming that can build the optimal RST tree in terms of either a span splitting score or a span merging score. We regarded a document as a text span consisting of three different granularity levels and built trees at each level, a document tree, paragraph trees for each paragraph, and sentence trees for each sentence. Then, we"
D19-1587,W01-1605,0,0.0730738,"of a text span, nucleus or satellite. A discourse relation is also assigned between two adjacent nonterminal nodes. Since the RST tree can be regarded as a standard constituent (phrase structure) tree, syntactic parsing models for constituency parsing can also be successfully applied to RST parsing. In most cases, RST parsers have been developed on the basis of supervised learning algorithms, which require a high quality annotated corpus of sufficient size. As a result, research on RST parsing has focused on English, with the largest annotated corpus being the RST Discourse Treebank (RST-DT) (Carlson et al., 2001). These supervised RST parsing methods might not be applied to languages with only a small-size corpus. This paper presents two types of language independent unsupervised RST parsing methods based on the CKY-like dynamic programming-based approach. One method builds the most likely parse tree in terms of a dissimilarity score defined for splitting a text span into two smaller ones. The other builds the optimal tree in terms of a similarity score defined for merging two adjacent text spans into a larger one. The similarity and dissimilarity scores between text spans are calculated on the basis"
D19-1587,W16-3616,1,0.857101,"T trees contain the multi-nuclear relations is larger. In fact, the ratio of the documents whose RST trees contain multi-nuclear relations in PCC is 0.712, while that in RST-DT is 0.464. We can obtain the information of sentences boundaries in most cases, however, sometimes we cannot obtain the information of paragraph boundaries. Thus, it is significant that our method outperformed baselines on D2S2E settings. Moreover, we compared our method with some supervised RST parsers on the test set of RSTDT. Table 3 shows the results. In the table, HHN16 denotes a simple transition-based RST parser (Hayashi et al., 2016), and WLW17 denotes a current state-of-the-art transition-based RST parser (Wang et al., 2017). Although the score of our method is lower than the scores of the supervised parsers, the score is close to that of HHN16. The results demonstrated the effectiveness of our unsupervised RST parsing method. For reference, (Braud et al., 2017) reported that their supervised RST parser obtained .802 span F1 score on PCC. However, we cannot compare the score with our score because the test set differs from ours. 4 Conclusion This paper proposed two kinds of unsupervised RST parsing methods based on dynam"
D19-1587,D13-1158,1,0.930181,"Missing"
D19-1587,P14-1002,0,0.17613,"he left span ` from i-th to k-th atomic text unit1 and the right span r from k+1 to j-th atomic text unit are given, we define the similarity score between them as follows: −→ −−→ 1 sim(`i:k , − rk+1:j )= 2 ( −→ ) −−→ `i:k · − rk+1:j −→ −−→ + 1 . r k k` kk− i:k −→ −−→ indicate the vector represenHere, `i:k and − rk+1:j tations of the left and right spans, which are defined as a concatenation of two vectors for the left most atomic unit and the right most atomic unit as follows: −→ − →], `i:k = [→ ui ; − u k (2) − − − → − − → − r = [u ; → u ]. k+1:j k+1 j The definition is inspired by that of (Ji and Eisenstein, 2014), which employs words at the beginning and end of a text span as important features to build an RST tree. → − ut is the vector representation of a t-th atomic unit ut , which is defined on the basis of SIF (Arora et al., 2017)2 as follows: X a → − → − ut = w. (3) p(w) + a w∈Wt − p(w) is the occurrence probability for word w, → w is the vector representation of the word and Wt is a set of words in ut . We use a concatenation of two word vectors obtained from ELMo (Peters et al., 2018) and Glove (Pennington et al., 2014) as a vector representation of a word. a is a parameter to decay the score o"
D19-1587,P17-1092,0,0.0131247,"erging two adjacent spans into a large one. Experimental results on English and German RST treebanks showed that our parser based on span merging achieved the best score, around 0.8 F1 score, which is close to the scores of the previous supervised parsers. 1 Introduction Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) is one of the theories that are most widely utilized for representing a discourse structure of a text in downstream NLP applications, such as automatic summarization (Marcu, 1998; Hirao et al., 2013), sentiment analysis (Bhatia et al., 2015), and text categorization (Ji and Smith, 2017). RST represents a text as a kind of constituent tree, whose leaves are Elementary Discourse Units (EDUs), clause-like units, and whose non-terminal nodes cover text spans consisting of a sequence of EDUs or a singleton EDU. The label of a non-terminal node represents the attribution of a text span, nucleus or satellite. A discourse relation is also assigned between two adjacent nonterminal nodes. Since the RST tree can be regarded as a standard constituent (phrase structure) tree, syntactic parsing models for constituency parsing can also be successfully applied to RST parsing. In most cases,"
D19-1587,W04-3250,0,0.0351274,"PCC contains two invalid RST trees, maz-12666 and maz-8838, we excluded them. Dataset Gran. Split Merge RB D2E .602 RST-DT D2S2E .755 D2P2S2E .793 .656 .788 .811 .545 .751 .803 D2E .656 D2S2E .757 D2P2S2E .787 .669 .760 .784 .626 .749 .789 PCC Table 2: Micro Span F1 scores for RST-DT and PCC. 3.2 Results Table 2 shows the evaluation results. Merge outperformed Split in most cases, and D2P2S2E achieved the best scores among our variants on both RST-DT and PCC. To clearly show the differences between our proposed method and RB, we performed significance tests, using paired bootstrap resampling (Koehn, 2004) at significance level=0.05. The results showed that there were significant differences between our method and RB at all the settings (D2E, D2S2E, D2P2S2E) for English, and at D2E and D2S2E for German, while there were no significant differences at D2P2S2E for German. The results imply that merging two adjacent spans and dividing with three granularity levels is suitable for unsupervised RST parsing. Comparing our methods with the baseline, right-branching (RB), we could find larger differences without considering the granularity levels of a document, whereas the differences became smaller by"
D19-1587,W98-1124,0,0.390101,"nes. The second builds the optimal tree in terms of a similarity score function that is defined for merging two adjacent spans into a large one. Experimental results on English and German RST treebanks showed that our parser based on span merging achieved the best score, around 0.8 F1 score, which is close to the scores of the previous supervised parsers. 1 Introduction Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) is one of the theories that are most widely utilized for representing a discourse structure of a text in downstream NLP applications, such as automatic summarization (Marcu, 1998; Hirao et al., 2013), sentiment analysis (Bhatia et al., 2015), and text categorization (Ji and Smith, 2017). RST represents a text as a kind of constituent tree, whose leaves are Elementary Discourse Units (EDUs), clause-like units, and whose non-terminal nodes cover text spans consisting of a sequence of EDUs or a singleton EDU. The label of a non-terminal node represents the attribution of a text span, nucleus or satellite. A discourse relation is also assigned between two adjacent nonterminal nodes. Since the RST tree can be regarded as a standard constituent (phrase structure) tree, synt"
D19-1587,D14-1162,0,0.0979854,"− − → − r = [u ; → u ]. k+1:j k+1 j The definition is inspired by that of (Ji and Eisenstein, 2014), which employs words at the beginning and end of a text span as important features to build an RST tree. → − ut is the vector representation of a t-th atomic unit ut , which is defined on the basis of SIF (Arora et al., 2017)2 as follows: X a → − → − ut = w. (3) p(w) + a w∈Wt − p(w) is the occurrence probability for word w, → w is the vector representation of the word and Wt is a set of words in ut . We use a concatenation of two word vectors obtained from ELMo (Peters et al., 2018) and Glove (Pennington et al., 2014) as a vector representation of a word. a is a parameter to decay the score of frequent words and is defined as a = (1 − α)/(αZ), where α is a hyper parameter and Z is the total number of words. 2.3 Dynamic Programming-Based Approach for Building Optimal Trees We propose a dynamic programming-based approach to obtain the optimal tree in terms of either the total split or merge score from all the possible trees. We first illustrate the algorithm with a merge (similarity) score. We define V [b][e], which stores the maximum merge score for a span ub:e consisting from b-th unit to e-th unit, as fol"
D19-1587,N18-1202,0,0.0113315,"`i:k = [→ ui ; − u k (2) − − − → − − → − r = [u ; → u ]. k+1:j k+1 j The definition is inspired by that of (Ji and Eisenstein, 2014), which employs words at the beginning and end of a text span as important features to build an RST tree. → − ut is the vector representation of a t-th atomic unit ut , which is defined on the basis of SIF (Arora et al., 2017)2 as follows: X a → − → − ut = w. (3) p(w) + a w∈Wt − p(w) is the occurrence probability for word w, → w is the vector representation of the word and Wt is a set of words in ut . We use a concatenation of two word vectors obtained from ELMo (Peters et al., 2018) and Glove (Pennington et al., 2014) as a vector representation of a word. a is a parameter to decay the score of frequent words and is defined as a = (1 − α)/(αZ), where α is a hyper parameter and Z is the total number of words. 2.3 Dynamic Programming-Based Approach for Building Optimal Trees We propose a dynamic programming-based approach to obtain the optimal tree in terms of either the total split or merge score from all the possible trees. We first illustrate the algorithm with a merge (similarity) score. We define V [b][e], which stores the maximum merge score for a span ub:e consisting"
D19-1587,stede-neumann-2014-potsdam,0,0.0280019,"e calculated on the basis of their distributional representations. Note that since our method is fully unsupervised, the parser predicts only the skeleton of an RST tree. Moreover, we exploit multiple granularity levels of a document: (1) we first independently build three types of trees, a tree whose leaves correspond to a paragraph, a tree whose leaves correspond to a sentence, and a tree whose leaves correspond to an EDU, and then (2) merge them to obtain the whole RST tree. We conducted experimental evaluation on English and German datasets, RST-DT and the Potsdam Commentary Corpus (PCC) (Stede and Neumann, 2014), respectively. The results demonstrated that our method with span merging outperformed our method with span splitting and ob5797 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 5797–5802, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics tained .811 and .784 span scores for RST-DT and PCC, respectively. The scores are close to the scores of early supervised RST parsers. 2 Unsupervised RST Parsing 2.1 Motivation Generally, RST trees with m"
D19-1587,P17-2029,0,0.160955,"RST trees contain multi-nuclear relations in PCC is 0.712, while that in RST-DT is 0.464. We can obtain the information of sentences boundaries in most cases, however, sometimes we cannot obtain the information of paragraph boundaries. Thus, it is significant that our method outperformed baselines on D2S2E settings. Moreover, we compared our method with some supervised RST parsers on the test set of RSTDT. Table 3 shows the results. In the table, HHN16 denotes a simple transition-based RST parser (Hayashi et al., 2016), and WLW17 denotes a current state-of-the-art transition-based RST parser (Wang et al., 2017). Although the score of our method is lower than the scores of the supervised parsers, the score is close to that of HHN16. The results demonstrated the effectiveness of our unsupervised RST parsing method. For reference, (Braud et al., 2017) reported that their supervised RST parser obtained .802 span F1 score on PCC. However, we cannot compare the score with our score because the test set differs from ours. 4 Conclusion This paper proposed two kinds of unsupervised RST parsing methods based on dynamic programming that can build the optimal RST tree in terms of either a span splitting score o"
D19-1674,D18-1443,0,0.0214465,"n search efficiency in finding word combinations that can form anagrams (Jordan and Monteiro, 2003). However, they place little consideration on the word orders that seem natural to humans. Fortunately, recent progress in NLP techniques now enables natural sentences to be generated in many ∗ This work was conducted while the author was at NTT Communication Science Laboratories. 1 NP-hardness of anagram generation can be proved by using the fact that it contains an exact cover problem, which has been proven to be NP-complete (Karp, 1972), as a special case. NLP tasks (J´ozefowicz et al., 2016; Gehrmann et al., 2018; Skadina and Pinnis, 2017). However, beam search, the de-facto decoding algorithm used in many language generation methods, cannot be used as-is to generate anagrams since it cannot ensure that the generated sentences are anagrams. Although beam search variants that can generate sentences that satisfy some constraints have been proposed (Anderson et al., 2017; Hokamp and Liu, 2017), applying them to anagram generation tasks soon becomes intractable. To summarize, generating natural anagrams is a challenging task that cannot be solved by simply applying existing methods. In this paper, we prop"
D19-1674,P17-1141,0,0.16024,"-hardness of anagram generation can be proved by using the fact that it contains an exact cover problem, which has been proven to be NP-complete (Karp, 1972), as a special case. NLP tasks (J´ozefowicz et al., 2016; Gehrmann et al., 2018; Skadina and Pinnis, 2017). However, beam search, the de-facto decoding algorithm used in many language generation methods, cannot be used as-is to generate anagrams since it cannot ensure that the generated sentences are anagrams. Although beam search variants that can generate sentences that satisfy some constraints have been proposed (Anderson et al., 2017; Hokamp and Liu, 2017), applying them to anagram generation tasks soon becomes intractable. To summarize, generating natural anagrams is a challenging task that cannot be solved by simply applying existing methods. In this paper, we propose an anagram generation algorithm. The proposed algorithm is very simple; we run depth-first search to enumerate anagrams. As to the generation procedure, we use a neural language model and character frequency to suppress unnecessary search operations. Due to the power of the currently available pre-trained neural language models, this simple approach works surprisingly well. In e"
D19-1674,D17-1098,0,0.127442,"ence Laboratories. 1 NP-hardness of anagram generation can be proved by using the fact that it contains an exact cover problem, which has been proven to be NP-complete (Karp, 1972), as a special case. NLP tasks (J´ozefowicz et al., 2016; Gehrmann et al., 2018; Skadina and Pinnis, 2017). However, beam search, the de-facto decoding algorithm used in many language generation methods, cannot be used as-is to generate anagrams since it cannot ensure that the generated sentences are anagrams. Although beam search variants that can generate sentences that satisfy some constraints have been proposed (Anderson et al., 2017; Hokamp and Liu, 2017), applying them to anagram generation tasks soon becomes intractable. To summarize, generating natural anagrams is a challenging task that cannot be solved by simply applying existing methods. In this paper, we propose an anagram generation algorithm. The proposed algorithm is very simple; we run depth-first search to enumerate anagrams. As to the generation procedure, we use a neural language model and character frequency to suppress unnecessary search operations. Due to the power of the currently available pre-trained neural language models, this simple approach works"
D19-1674,N18-1202,0,0.0210559,"s) for s = “christmas”. The anagram generation problem is the problem of finding sentence t given input sentence s, where t is an anagram of s. We use a language model when generating anagrams. Let p(s) be the probability that sentence s = (w1 , w2 , . . . , w|s |) appears. A language model is a stochastic model that gives conditional probability p(wi+1 |w1 , . . . , wi ). A language model derives probability p(s) by |s|−1 p(s) = p(w1 ) Y p(wi+1 |w1 , . . . , wi ). (1) i=1 While our anagram generation algorithm can use any language model, our experiments use the neural language model of ELMo (Peters et al., 2018). 4 Anagram Generation Our anagram generation algorithm uses simple DFS. Given input string s, we start the search with the initial string t =“hSi” and then try to append a word w ∈ V to the tail of t for each step to search for anagrams of s. If we find that t yields no solution by appending any words to it, we pop back the last word from t and continue the search by appending a different word to the tail of t. Since there are |V |N possible sentences consisting of N words, naive DFS is intractable. Thus we need to reduce the search steps. For this we introduce the following two criteria. The"
D19-1674,I17-1038,0,0.0191609,"finding word combinations that can form anagrams (Jordan and Monteiro, 2003). However, they place little consideration on the word orders that seem natural to humans. Fortunately, recent progress in NLP techniques now enables natural sentences to be generated in many ∗ This work was conducted while the author was at NTT Communication Science Laboratories. 1 NP-hardness of anagram generation can be proved by using the fact that it contains an exact cover problem, which has been proven to be NP-complete (Karp, 1972), as a special case. NLP tasks (J´ozefowicz et al., 2016; Gehrmann et al., 2018; Skadina and Pinnis, 2017). However, beam search, the de-facto decoding algorithm used in many language generation methods, cannot be used as-is to generate anagrams since it cannot ensure that the generated sentences are anagrams. Although beam search variants that can generate sentences that satisfy some constraints have been proposed (Anderson et al., 2017; Hokamp and Liu, 2017), applying them to anagram generation tasks soon becomes intractable. To summarize, generating natural anagrams is a challenging task that cannot be solved by simply applying existing methods. In this paper, we propose an anagram generation a"
D19-5211,D18-2012,0,0.0992129,"We crawled the listed candidate domains and aligned parallel sentences using bitextor4 . Then we filtered out noisy sentences with bicleaner5 (S´anchez-Cartagena et al., 2018). After corpus cleaning, we retained 7.5M sentences. We named this corpus “JParaCrawl” and we plan to release it publicly with a detailed corpus description paper. further improve the performance: (1) model ensembling and (2) Right-to-Left (R2L) re-ranking. 2.2.3 Data Preprocessing This year, we decided not to employ any external morphological analyzer like KyTea (Neubig et al., 2011). Instead we utilized sentencepiece6 (Kudo and Richardson, 2018), which tokenizes a sentence into a sequence of subwords without requiring any other tokenizers. Note here that we did not apply any filtering method, such as sentence length filtering. The JParaCrawl domain basically differs from the scientific paper task. To effectively incorporate with the JParaCrawl, we first pre-trained the model with the mixed data of ASPEC and JParaCrawl.7 Then we fine-tuned the pre-trained model using only ASPEC. 2.3.1 Ensembling We independently trained four models with different random seeds and simultaneously utilized them for model ensembling to boost the translati"
D19-5211,N19-4009,0,0.0874362,"n exploit both the advantages of the L2R and R2L models and improve their performance. 2.3.3 Model incorporation with JParaCrawl 2.4 Hyper-parameter As a base NMT model, we selected the Transformer model with the “big” hyper-parameter setting. During training, we used mixed-precision training (Micikevicius et al., 2018) that can boost the training speed and reduce the memory consumption. We saved the model each epoch and used the average of the last ten models for decoding. We set the beam size to six and normalized the scores by their length. All implementations are based on fairseq toolkit (Ott et al., 2019). Table 2 shows the selected set of the 2.3 System Details We selected the Transformer model (Vaswani et al., 2017) as our base NMT model. We also incorporated two techniques to 1 http://paracrawl.eu/ https://commoncrawl.org/ 3 https://github.com/paracrawl/ extractor 4 https://github.com/bitextor/bitextor 5 https://github.com/bitextor/bicleaner 6 https://github.com/google/ sentencepiece 2 7 We mixed ASPEC and JParaCrawl by upsampling ASPEC twice. 100 Hyper-parameter Subword (vocabulary) size Gradient clipping Dropout rate Mini-batch size Update frequency Beam search (n-best) Selected Value src"
D19-5211,N16-1046,0,0.030925,"PEC. 2.3.1 Ensembling We independently trained four models with different random seeds and simultaneously utilized them for model ensembling to boost the translation performance. 2.3.2 Right-to-left (R2L) re-ranking The NMT model has auto-regressive architecture in its decoder that uses previously generated tokens for predicting the next token. In other words, we normally decode a sentence from the beginning-of-the-sentence (BOS), which is its left side, to the end-of-the-sentence (EOS), which is on the right. Here we call this normal decoding process as Left-to-Right (L2R) decoding. However, Liu et al. (2016) pointed out that L2R decoding lacks reliability near the EOS tokens because if the previous tokens contain errors, the next prediction might have error as well. To alleviate this problem, Liu et al. (2016) proposed a method that generates the n-best hypotheses with the L2R model and re-ranks them with the R2L model, which decodes the sentences from the EOS tokens to the BOS tokens. By R2L re-ranking, we can exploit both the advantages of the L2R and R2L models and improve their performance. 2.3.3 Model incorporation with JParaCrawl 2.4 Hyper-parameter As a base NMT model, we selected the Tran"
D19-5211,W18-6301,0,0.093188,"ality of the English sentences in the latter half of the provided data looks somewhat awful (not very well). Therefore, we then tried to make synthetic data by using forward-translation instead of the standard back-translation. This means that we used the synthetic data for the En-Ja translation setting as the synthetic data of the Ja-En translation setting. This slightly improved the performance of the Ja-En translation setting. 2.4.3 Mini-batch size/Update frequency According to a previously introduced finding, Transformer models tend to provide better results with a larger mini-batch size (Ott et al., 2018). Based on this observation, we explored the effectiveness of the mini-batch size in our setting. Table 5 shows the results. We found that an overly large mini-batch, i.e., 512, degraded the performance. In our experiments, an update frequency of 128, which means 128 × 4, 000 = 512, 000 tokens per mini-batch, was an appropriate value. 2.4.4 Ensemble and R2L re-ranking Ensembling and re-ranking are currently the standard techniques for further improving the translation quality in the NMT models. Following this public knowledge, we also applied standard ensembling and right-to-left (R2L) re-rank"
D19-5211,P02-1040,0,0.10354,"9 the model for 800 updates. During training, we used mixed-precision training (Micikevicius et al., 2018), like the scientific paper subtasks. We saved the model every 100 updates and used the average of the last eight models for decoding. We set the beam size to six and normalized the scores by length. All implementations are based on fairseq toolkit (Ott et al., 2019). Table 10: Number of sentences in timely disclosure document corpus: We split training set into two categories. See Section 3.2 for details. 3.4 Experimental Results and Analysis Table 11 shows the case-sensitive BLEU scores (Papineni et al., 2002) of the provided blind test sets. All the reported BLEU scores were calculated on the organizers’ submission website. 3.3 Experimental Settings For preprocessing, we only relied on sentencepiece (Kudo and Richardson, 2018), which tokenizes a sentence into subwords without requiring any other tokenizers. We set the vocabulary size to 32k9 . The provided training data were split by their released years, but we concatenated them without distinguishing them. As an NMT model, we used the Transformer (Vaswani et al., 2017) with big hyperparameter settings and dropout (Srivastava et al., 2014) with a"
D19-5211,W17-5706,1,0.859249,"e scientific paper subtask in ∗ # Sentences 3,008,500 (1,500,000) (1,508,500) 1,790 1,784 1,812 2.2 Data and Data Preparation 2.2.1 Provided data: constrained setting As training/dev/test data, the task organizer provided the Asian Scientific Paper Excerpt Corpus (ASPEC) (Nakazawa et al., 2016) whose statistics are shown in Table 1. ASPEC was created by automatically aligning parallel documents and sentences, and the training sentences are ordered by sentence alignment scores. Thus, the previous participants generally removed the latter sentences (Neubig, 2014) or used them as synthetic data (Morishita et al., 2017). This year, we used the former 1.5M training sentences as bitext data and the latter 1.5M as monolingual data and created synthetic data (Sennrich et al., 2016). Equal contribution. 99 Proceedings of the 6th Workshop on Asian Translation, pages 99–105 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics 2.2.2 JParaCrawl: unconstrained setting The ParaCrawl1 project is building parallel corpora by largely crawling the web. Their objective is to build parallel corpora for the 24 official languages of the European Union. They already released earlier versions of t"
D19-5211,W18-6319,0,0.0133432,"n changing mini-batch size (update frequency) for each update in NMT training. Scores here were calculated by sacreBLEU. Table 3: Comparison of translation performance on changing subword size. Scores here were calculated by sacreBLEU. hyper-parameters we used for the final submission. In our preliminary experiments, we evaluated extensive combinations of hyper-parameters and we found that this setting was optimal in our hyper-parameter search. Hereafter, the reported performance in the rest of this paper was obtained using this setting unless otherwise specified. performance using sacreBLEU (Post, 2018) for all the results shown in this section. We clearly observe a tendency that the fewer subwords got better performance. This observation is actually a bit surprising since many recent previous studies in the NMT community often employed a larger amount of subwords like 16,000 or 32,000. 2.4.1 Back- and forward-translation for building synthetic data We first investigated the effectiveness of incorporating synthetic data generated by the backtranslation technique. Table 3 shows the results. We significantly improved the performance of the En-Ja translation setting by adding the synthetic data"
D19-5211,W18-6488,0,0.05847,"Missing"
D19-5211,W18-6421,1,0.837884,"05 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics 2.2.2 JParaCrawl: unconstrained setting The ParaCrawl1 project is building parallel corpora by largely crawling the web. Their objective is to build parallel corpora for the 24 official languages of the European Union. They already released earlier versions of the corpora and they were used on the WMT 2018 news shared translation tasks (Bojar et al., 2018). The WMT shared task participants reported that this corpora boosted translation accuracy when used with careful corpus cleaning (Junczys-Dowmunt, 2018; Morishita et al., 2018). Inspired by these previous works, we constructed a web-based Japanese-English parallel corpus. We followed almost the same procedure as ParaCrawl to make this corpus. First, we listed 100,000 candidate domains that might contain parallel Japanese and English sentences by analyzing the whole Common Crawl text data2 on how each domain contains Japanese or English data with extractor3 . We crawled the listed candidate domains and aligned parallel sentences using bitextor4 . Then we filtered out noisy sentences with bicleaner5 (S´anchez-Cartagena et al., 2018). After corpus cleaning, we retained"
D19-5211,P16-1009,0,0.0329958,"ting As training/dev/test data, the task organizer provided the Asian Scientific Paper Excerpt Corpus (ASPEC) (Nakazawa et al., 2016) whose statistics are shown in Table 1. ASPEC was created by automatically aligning parallel documents and sentences, and the training sentences are ordered by sentence alignment scores. Thus, the previous participants generally removed the latter sentences (Neubig, 2014) or used them as synthetic data (Morishita et al., 2017). This year, we used the former 1.5M training sentences as bitext data and the latter 1.5M as monolingual data and created synthetic data (Sennrich et al., 2016). Equal contribution. 99 Proceedings of the 6th Workshop on Asian Translation, pages 99–105 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics 2.2.2 JParaCrawl: unconstrained setting The ParaCrawl1 project is building parallel corpora by largely crawling the web. Their objective is to build parallel corpora for the 24 official languages of the European Union. They already released earlier versions of the corpora and they were used on the WMT 2018 news shared translation tasks (Bojar et al., 2018). The WMT shared task participants reported that this corpora boo"
D19-5211,L16-1350,0,0.13062,"Missing"
D19-5211,W14-7002,0,0.0181288,"we first explain the systems developed for the scientific paper subtask in ∗ # Sentences 3,008,500 (1,500,000) (1,508,500) 1,790 1,784 1,812 2.2 Data and Data Preparation 2.2.1 Provided data: constrained setting As training/dev/test data, the task organizer provided the Asian Scientific Paper Excerpt Corpus (ASPEC) (Nakazawa et al., 2016) whose statistics are shown in Table 1. ASPEC was created by automatically aligning parallel documents and sentences, and the training sentences are ordered by sentence alignment scores. Thus, the previous participants generally removed the latter sentences (Neubig, 2014) or used them as synthetic data (Morishita et al., 2017). This year, we used the former 1.5M training sentences as bitext data and the latter 1.5M as monolingual data and created synthetic data (Sennrich et al., 2016). Equal contribution. 99 Proceedings of the 6th Workshop on Asian Translation, pages 99–105 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics 2.2.2 JParaCrawl: unconstrained setting The ParaCrawl1 project is building parallel corpora by largely crawling the web. Their objective is to build parallel corpora for the 24 official languages of the Eur"
D19-5211,N19-4007,0,0.017437,"ategories. This means that the model already outputs quite similar hypotheses as references. 3.4.2 Fine-tuning with a specific category We found that fine-tuning with specific category data significantly increased the BLEU scores: +3.65 points for texts and +1.56 points for the items categories. Table 13 shows the example translations of the baseline and fine-tuned systems10 . The fine-tuned system perfectly gener9 10 In contrast to the scientific paper subtasks, we did not see improvement with a smaller vocabulary in the preliminary experiments. For finding good examples, we used compare-mt (Neubig et al., 2019), which is a toolkit that compares two MT outputs. 103 Task Texts Items Auto Eval BLEU (Rank) 61.19 (1) 57.34 (1) Pairwise 55.50 34.00 Human Eval (Rank) Adequacy (1) 4.46 (2) 4.47 (Rank) (1) (1) Table 12: Official results of our submitted systems for timely disclosure subtask: Shown rank is only ordered among constrained submissions. Input 実績値、類似建物の修繕費水準、エンジニアリング・レポートの修繕更新費等を考慮し査定 Reference Based on historical data, comparable assets and estimates in the engineering report Baseline Assessed by taking into account the actual results, the level of repair expenses of similar buildings, the level"
D19-5622,K18-1010,0,0.0387704,"Missing"
D19-5622,P19-2030,1,0.769292,"prove the self-attention in a systematic and multifaceted perspective, rather than just paying attention to one specific characteristic. 6 Compared to a conventional NMT model with only a single head, multi-head is assumed to have a stronger ability to extract different features in different subspaces. However, there are no explicit mechanism that make them distinct (Voita et al., 2019; Michel et al., 2019). Li et al. (2018) had shown that using a disagreement regularization to encourage different attention heads to have different behaviors can improve the performance of multi-head attention. Iida et al. (2019) proposed a multi-hop attention where the second-hop serves as a head gate function to normalize the attentional context of each head. Not only limited in the field of neural machine translation, Strubell et al. (2018) combined multi-head self-attention with multi-task learning, this led to a promising result for semantic role labeling. Similar to the above studies, we also attempt to model diversity for multi-head attention. In this work, we apply difRelated Work In the field of neural machine translation, the two most used attention mechanisms are additive attention (Bahdanau et al., 2015) a"
D19-5622,W18-2709,0,0.0240383,"our mixed multihead self-attention. For a fair comparison, we apply each attention function twice in base model. By doing this, our Transformer MMA have the same number of parameters as the original Transformer. For evaluation, we use a beam size of 5 for beam search, translation quality is reported via BLEU (Papineni et al., 2002) and statistical significance test is conducted by paired bootstrap resampling method (Koehn, 2004). 5.1 Effectiveness of MMA Neural machine translation must consider the correlated ordering of words, where order has a lot of influence on the meaning of a sentence (Khayrallah and Koehn, 2018). In vanilla Transformer, the position embedding is a deterministic function of position and it allows the model to be aware of the order of the sequence (Yang et al., 2019). As shown in Table 3, Transformer without position embedding fails on translation task, resulting in a decrease of 17.91 BLEU score. With the help of proposed MMA, the performance is only reduced by 0.75 BLEU score without position embedding, and 18.11 points higher than the Transformer baseline. The same result holds true for a distant language pair Japanese-English where word oder is completely different. When removing p"
D19-5622,W04-3250,0,0.140022,"20 epochs, the small model are trained for 45 epochs. The number of heads are 8 for base model and 4 for small model. We replace multi-head selfattention in the encoder layers by our mixed multihead self-attention. For a fair comparison, we apply each attention function twice in base model. By doing this, our Transformer MMA have the same number of parameters as the original Transformer. For evaluation, we use a beam size of 5 for beam search, translation quality is reported via BLEU (Papineni et al., 2002) and statistical significance test is conducted by paired bootstrap resampling method (Koehn, 2004). 5.1 Effectiveness of MMA Neural machine translation must consider the correlated ordering of words, where order has a lot of influence on the meaning of a sentence (Khayrallah and Koehn, 2018). In vanilla Transformer, the position embedding is a deterministic function of position and it allows the model to be aware of the order of the sequence (Yang et al., 2019). As shown in Table 3, Transformer without position embedding fails on translation task, resulting in a decrease of 17.91 BLEU score. With the help of proposed MMA, the performance is only reduced by 0.75 BLEU score without position"
D19-5622,D18-1317,0,0.0717605,"cy of the Transformer with only one head and eight has occurred in multiple heads. In this paheads (Vaswani et al., 2017; Chen et al., 2018). per, we argue that using the same global atHowever, all encoder self-attention heads fully tention in multiple heads limits multi-head self-attention’s capacity for learning distinct take global information into account, there is features. In order to improve the expresno explicit mechanism to ensure that differsiveness of multi-head self-attention, we proent attention heads indeed capture different feapose a novel Mixed Multi-Head Self-Attention tures (Li et al., 2018). Concerning the results pre(MMA) which models not only global and losented by some latest researches, the majority cal attention but also forward and backward atof the encoder self-attention heads, can even be tention in different attention heads. This enpruned away without substantially hurting model’s ables the model to learn distinct representations explicitly among multiple heads. In our performance (Voita et al., 2019; Michel et al., experiments on both WAT17 English-Japanese 2019). Moreover, the ability of multi-head selfas well as IWSLT14 German-English transattention, in which lacking"
D19-5622,D15-1166,0,0.833569,"e latest researches, the majority cal attention but also forward and backward atof the encoder self-attention heads, can even be tention in different attention heads. This enpruned away without substantially hurting model’s ables the model to learn distinct representations explicitly among multiple heads. In our performance (Voita et al., 2019; Michel et al., experiments on both WAT17 English-Japanese 2019). Moreover, the ability of multi-head selfas well as IWSLT14 German-English transattention, in which lacking capacity to capture lolation task, we show that, without increascal information (Luong et al., 2015; Yang et al., ing the number of parameters, our models 2018; Wu et al., 2019) and sequential informayield consistent and significant improvements tion (Shaw et al., 2018; Dehghani et al., 2019), (0.9 BLEU scores on average) over the strong 1 has recently come into question (Tang et al., Transformer baseline. 2018). Motivated by above findings, we attribute the 1 Introduction redundancy arising in encoder self-attention heads Neural machine translation (NMT) has made to the using of same global self-attention among promising progress in recent years with differall attention heads. Additionally"
D19-5622,P18-1008,0,0.0242936,"llow the model of its high-performance is the multi-head selfto independently attend to information from attention which allows the model to jointly attend different representation subspaces. However, to information from different representation subthere is no explicit mechanism to ensure that spaces at different positions. There is a huge gap different attention heads indeed capture dif(around 1 BLEU score) between the performance ferent features, and in practice, redundancy of the Transformer with only one head and eight has occurred in multiple heads. In this paheads (Vaswani et al., 2017; Chen et al., 2018). per, we argue that using the same global atHowever, all encoder self-attention heads fully tention in multiple heads limits multi-head self-attention’s capacity for learning distinct take global information into account, there is features. In order to improve the expresno explicit mechanism to ensure that differsiveness of multi-head self-attention, we proent attention heads indeed capture different feapose a novel Mixed Multi-Head Self-Attention tures (Li et al., 2018). Concerning the results pre(MMA) which models not only global and losented by some latest researches, the majority cal atte"
D19-5622,N19-4009,0,0.0214542,"K, 1.8K sentence pairs respectively. We adopt the official 16K vocabularies preprocessed by sentencepiece.2 IWSLT14 German-English: We use the TED data from the IWSLT14 German-English shared translation task (Cettolo et al., 2014) which contains 160K training sentences and 7K validation sentences randomly sampled from the training data. We test on the concatenation of tst2010, tst2011, tst2012, tst2013 and dev2010. For this benchmark, data is lowercased and tokenized with byte pair encoding (BPE) (Sennrich et al., 2016). 4.2 Setup Our implementation is built upon open-source toolkit fairseq3 (Ott et al., 2019). For WAT17 dataset and IWSLT14 dataset, we use the configurations of the Transformer base and small model respectively. Both of them consist of a 6-layer encoder and 6-layer decoder, the size of hidden state and word embedding are set to 512. The dimensionality of inner feed-forward layer is 2048 for base and 1024 for small model. The dropout probability is 0.1 and 0.3 for base and small model. Models are optimized with Adam (Kingma and Ba, 2014). We use the same warmup and decay strategy for learning rate as Vaswani et al. (2017) with 4000 warmup steps. Experiments 4.1 Datasets To test the p"
D19-5622,P02-1040,0,0.104861,"trained on a single NVIDIA RTX2080Ti with a batch size of around 4096 tokens. The base model are trained for 20 epochs, the small model are trained for 45 epochs. The number of heads are 8 for base model and 4 for small model. We replace multi-head selfattention in the encoder layers by our mixed multihead self-attention. For a fair comparison, we apply each attention function twice in base model. By doing this, our Transformer MMA have the same number of parameters as the original Transformer. For evaluation, we use a beam size of 5 for beam search, translation quality is reported via BLEU (Papineni et al., 2002) and statistical significance test is conducted by paired bootstrap resampling method (Koehn, 2004). 5.1 Effectiveness of MMA Neural machine translation must consider the correlated ordering of words, where order has a lot of influence on the meaning of a sentence (Khayrallah and Koehn, 2018). In vanilla Transformer, the position embedding is a deterministic function of position and it allows the model to be aware of the order of the sequence (Yang et al., 2019). As shown in Table 3, Transformer without position embedding fails on translation task, resulting in a decrease of 17.91 BLEU score."
D19-5622,W18-5431,0,0.0369679,"MA achieves the best result. One possible reason is that, in the case where there are already global features captured by global attention, the smaller the attention scope, the more local features can be learned by local attention. 5.4 Attention Visualization To further explore the behavior of our Transformer MMA, we observe the distribution of encoder attention weights in our models and show an example of Japanese sentence as plotted in Figure 3. The first discovery is that we find the word overlooks itself on the first layer in the global attention head. This contrasts with the results from Raganato and Tiedemann (2018). They find that, on the first layer of original Transformer, more en5.3 Ablation Study For ablation study, the primary question is whether the Transformer benefits from the integration of different attention equally. To do evaluate the impact of various attention functions, we keep global self-attention head unchanged, and next we replace other heads with different attention function. 211 Figure 3: Visualization of the attention weights of Japanese sentence “これらは 腰椎 装具 装用 または 運動 制 限 により 全 症例 軽快した 。” (meaning “These persons were improved in all cases by wearing lumbar braces or limiting exerci"
D19-5622,D18-1475,0,0.107174,"us on the word itself. This change is in line with our assumption that, due to the existence of other attention heads, global attention head can focus more on capturing global information. The second discovery is that, on the upper layers, forward and backward attention heads move the attention more on distant words. This suggests forward and backward attention is able to serve as a complement to capturing long-range dependency. with a global receptive field, the ability of selfattention recently came into question (Tang et al., 2018). And modeling localness, either restricting context sizes (Yang et al., 2018; Wu et al., 2019; Child et al., 2019) or balancing the contribution of local and global information (Xu et al., 2019), has been shown to be able to improve the expressiveness of self-attention. In contrast to these studies, we aim to improve the self-attention in a systematic and multifaceted perspective, rather than just paying attention to one specific characteristic. 6 Compared to a conventional NMT model with only a single head, multi-head is assumed to have a stronger ability to extract different features in different subspaces. However, there are no explicit mechanism that make them dis"
D19-5622,P19-1354,0,0.0126744,"s as the original Transformer. For evaluation, we use a beam size of 5 for beam search, translation quality is reported via BLEU (Papineni et al., 2002) and statistical significance test is conducted by paired bootstrap resampling method (Koehn, 2004). 5.1 Effectiveness of MMA Neural machine translation must consider the correlated ordering of words, where order has a lot of influence on the meaning of a sentence (Khayrallah and Koehn, 2018). In vanilla Transformer, the position embedding is a deterministic function of position and it allows the model to be aware of the order of the sequence (Yang et al., 2019). As shown in Table 3, Transformer without position embedding fails on translation task, resulting in a decrease of 17.91 BLEU score. With the help of proposed MMA, the performance is only reduced by 0.75 BLEU score without position embedding, and 18.11 points higher than the Transformer baseline. The same result holds true for a distant language pair Japanese-English where word oder is completely different. When removing position embedding, the Transformer baseline drops to 12.83 BLEU score. However, our model still achieves 23.80 in terms of BLEU score, with 10.97 points improvement over the"
D19-5622,P16-1162,0,0.0826212,"nd does not affect the training efficiency. 4 Training, validation and test sets comprise 2M, 1.8K, 1.8K sentence pairs respectively. We adopt the official 16K vocabularies preprocessed by sentencepiece.2 IWSLT14 German-English: We use the TED data from the IWSLT14 German-English shared translation task (Cettolo et al., 2014) which contains 160K training sentences and 7K validation sentences randomly sampled from the training data. We test on the concatenation of tst2010, tst2011, tst2012, tst2013 and dev2010. For this benchmark, data is lowercased and tokenized with byte pair encoding (BPE) (Sennrich et al., 2016). 4.2 Setup Our implementation is built upon open-source toolkit fairseq3 (Ott et al., 2019). For WAT17 dataset and IWSLT14 dataset, we use the configurations of the Transformer base and small model respectively. Both of them consist of a 6-layer encoder and 6-layer decoder, the size of hidden state and word embedding are set to 512. The dimensionality of inner feed-forward layer is 2048 for base and 1024 for small model. The dropout probability is 0.1 and 0.3 for base and small model. Models are optimized with Adam (Kingma and Ba, 2014). We use the same warmup and decay strategy for learning"
D19-5622,P19-1021,0,0.051077,"Missing"
D19-5622,N18-2074,0,0.24625,"enpruned away without substantially hurting model’s ables the model to learn distinct representations explicitly among multiple heads. In our performance (Voita et al., 2019; Michel et al., experiments on both WAT17 English-Japanese 2019). Moreover, the ability of multi-head selfas well as IWSLT14 German-English transattention, in which lacking capacity to capture lolation task, we show that, without increascal information (Luong et al., 2015; Yang et al., ing the number of parameters, our models 2018; Wu et al., 2019) and sequential informayield consistent and significant improvements tion (Shaw et al., 2018; Dehghani et al., 2019), (0.9 BLEU scores on average) over the strong 1 has recently come into question (Tang et al., Transformer baseline. 2018). Motivated by above findings, we attribute the 1 Introduction redundancy arising in encoder self-attention heads Neural machine translation (NMT) has made to the using of same global self-attention among promising progress in recent years with differall attention heads. Additionally, it is because of ent architectures, ranging from recurrent neuthe redundancy, multi-head self-attention is unral networks (Sutskever et al., 2014; Cho et al., able to l"
D19-5622,D18-1548,0,0.0377067,"assumed to have a stronger ability to extract different features in different subspaces. However, there are no explicit mechanism that make them distinct (Voita et al., 2019; Michel et al., 2019). Li et al. (2018) had shown that using a disagreement regularization to encourage different attention heads to have different behaviors can improve the performance of multi-head attention. Iida et al. (2019) proposed a multi-hop attention where the second-hop serves as a head gate function to normalize the attentional context of each head. Not only limited in the field of neural machine translation, Strubell et al. (2018) combined multi-head self-attention with multi-task learning, this led to a promising result for semantic role labeling. Similar to the above studies, we also attempt to model diversity for multi-head attention. In this work, we apply difRelated Work In the field of neural machine translation, the two most used attention mechanisms are additive attention (Bahdanau et al., 2015) and dot attention (Luong et al., 2015). Based on the latter, Vaswani et al. (2017) proposed a multi-head selfattention, that is not only highly parallelizable but also with better performance. However, self-attention, w"
D19-5622,D18-1458,0,0.164173,"Missing"
D19-5622,P19-1580,0,0.310539,"echanism to ensure that differsiveness of multi-head self-attention, we proent attention heads indeed capture different feapose a novel Mixed Multi-Head Self-Attention tures (Li et al., 2018). Concerning the results pre(MMA) which models not only global and losented by some latest researches, the majority cal attention but also forward and backward atof the encoder self-attention heads, can even be tention in different attention heads. This enpruned away without substantially hurting model’s ables the model to learn distinct representations explicitly among multiple heads. In our performance (Voita et al., 2019; Michel et al., experiments on both WAT17 English-Japanese 2019). Moreover, the ability of multi-head selfas well as IWSLT14 German-English transattention, in which lacking capacity to capture lolation task, we show that, without increascal information (Luong et al., 2015; Yang et al., ing the number of parameters, our models 2018; Wu et al., 2019) and sequential informayield consistent and significant improvements tion (Shaw et al., 2018; Dehghani et al., 2019), (0.9 BLEU scores on average) over the strong 1 has recently come into question (Tang et al., Transformer baseline. 2018). Motivated"
D19-5622,P19-1295,0,0.593715,"ly describe the Transformer architecture (Vaswani et al., 2017) which includes 207 ATT(·) is computed by: ei = Qi K ⊤ √ d ATT(Q, K, V ) = Softmax(ei )V receptive field which is used to connect with arbitrary words directly. Under our framework, we define the hard mask for global attention as follows: (5) G Mi,j =0 But global attention may be less powerful and can potentially render it impractical for longer sequences (Luong et al., 2015). On the other hand, self-attention can be enhanced by local attention which focuses more on restricted scope rather than the entire context (Wu et al., 2019; Xu et al., 2019). Based on the above findings, we also define a local attention which simply employs a hard mask to restrict the attention scope by: { 0, i−w ≤j ≤i+w L Mi,j = (9) −∞, otherwise (6) where ei is the i-th energy and d is the dimension of hidden state. The decoder is also composed of N identical layers and it contains a third sublayer, which performs attention over the output of the encoder between the self-attention sublayer and feed-forward network sublayer. 3 Proposed Architecture Our proposed approach is mainly motivated by the fact that redundancy has occurred in multiheads (Voita et al., 201"
D19-6505,W04-3250,0,0.134895,"Missing"
D19-6505,L18-1275,0,0.138657,"ce to be translated, by constructing an encoder that is based on explicit coreference relations. The proposed model can directly take into account relationships between sentences via a graph structured encoder constructed with a coreference resolution toolkit. Therefore, it does not need to attend to all input tokens. This characteristic enables our proposed model to handle more sentences in a step, compared with the previous models, and it may improve translation quality when a source text has many sentences. Experimental results on English-to-Japanese translation pairs in OpenSubtitles2018 (Lison et al., 2018) show that our proposed model can significantly improve the previous model in terms of BLEU scores. In addition, we observe that our model is especially effective in translating a sentence which is a part of a long text, compared to the previous model. We present neural machine translation models for translating a sentence in a text by using a graph-based encoder which can consider coreference relations provided within the text explicitly. The graph-based encoder can dynamically encode the source text without attending to all tokens in the text. In experiments, our proposed models provide stat"
D19-6505,D15-1166,0,0.0610526,"Ti . The concatenated token sequence is represented as: p (yi |y1 , . . . , yi−1 , x) = sof tmax(g (si , di )), si = dec (si−1 , emb(yi−1 ), di ) , di = Tx X a (si−1 , hj ) hj , (1) j=1 ht = enc (emb(xt ), ht−1 , ht+1 ) , N (x11 , · · · , x1T1 , x21 , · · · , x2T2 , · · · , xN 1 , · · · , xTN ). where i is the position of an output token, t is the position of an input token, emb(·) is a function that returns the embedding of an input word, g is a 2-layer feedforward neural network (FFNN), dec is a decoder forward-LSTM, enc is an encoder bidirectional-LSTM (Bi-LSTM), and a is a dot attention (Luong et al., 2015) for calculating the attention weight. 3 Coreference Resolution (2) The coreference resolution system extracts Nc clusters of coreferring mentions (c1 , · · · , cNc ), which are defined as: ck = (maink , subk ), (3) where maink is a span of the representative mention in a cluster of coreferring mentions, and subk is a span of another mention in the cluster.2 In general, because many mentions are in a single cluster, the same maink is sometimes paired to different mentions. To use coreference relations in our graph-based encoder, we need to consider word-based coreference relations. Let head(·)"
D19-6505,D18-1325,0,0.0176744,"anslated one by one. In contrast to this premise, real sentences are often an element of a larger unit, such as a document. This means that a sentence is not always semantically self-contained in itself. To correctly interpret a sentence which is a part of a document, it is important to consider its context, preceding and/or succeeding sentences. In order to tackle the problem, Seq2Seq models that can receive two sentences (Tiedemann and Scherrer, 2017; Bawden et al., 2018; Voita et al., 2018; Wang et al., 2017) have been utilized. For capturing multiple-sentence information more effectively, Miculicich et al. (2018); Zhang et al. (2018) incorporated document-level attention modules into Seq2Seq models. Stojanovski and Fraser (2018) proposed a Seq2Seq model which can capture antecedents of pronouns 2 Sequence-to-Sequence Model In this section, we explain the standard Seq2Seq model proposed by Bahdanau et al. (2014), which 45 Proceedings of the Fourth Workshop on Discourse in Machine Translation (DiscoMT 2019), pages 45–50 c Hong Kong, China, November 3, 2019. 2019 Association for Computational Linguistics y11 → −1 s1 Output Sentences Decoder y21 → −1 s2 y31 → −1 s3 y12 → −2 s1 y22 → −2 s2 − → h21 ← − h21"
D19-6505,N18-1118,0,0.219887,"-toSequence (Seq2Seq) models (Bahdanau et al., 2014). Most Seq2Seq models are used based on the premise that each sentence is independently translated one by one. In contrast to this premise, real sentences are often an element of a larger unit, such as a document. This means that a sentence is not always semantically self-contained in itself. To correctly interpret a sentence which is a part of a document, it is important to consider its context, preceding and/or succeeding sentences. In order to tackle the problem, Seq2Seq models that can receive two sentences (Tiedemann and Scherrer, 2017; Bawden et al., 2018; Voita et al., 2018; Wang et al., 2017) have been utilized. For capturing multiple-sentence information more effectively, Miculicich et al. (2018); Zhang et al. (2018) incorporated document-level attention modules into Seq2Seq models. Stojanovski and Fraser (2018) proposed a Seq2Seq model which can capture antecedents of pronouns 2 Sequence-to-Sequence Model In this section, we explain the standard Seq2Seq model proposed by Bahdanau et al. (2014), which 45 Proceedings of the Fourth Workshop on Discourse in Machine Translation (DiscoMT 2019), pages 45–50 c Hong Kong, China, November 3, 2019. 2"
D19-6505,P02-1040,0,0.10417,"ces and generating a single sentence, proposed by Bawden et al. (2018) (Concat)5 . We compared our proposed models, Coref-mean (Cor-m) and Coref-gate (Cor-g), with the baseline. In order to evaluate the effectiveness of succeeding sentences, we also experimented with the cases of inputting the same number of preceding and succeeding sentences for the target sentence to be translated at the center, for Cor-g. We denote this setting as Coref-gatecentered (Cor-g-c). The number of weight parameters for each model is 111,057k for the baseline and Cor-m, and 111,558k for Cor-g. We used BLEU scores (Papineni et al., 2002) to evaluate the translation performance for each model. All reported BLEU scores in the experiments are averages for three times and are based on MeCab tokenization. Significance tests were conducted by paired bootstrap resampling (Koehn, 2004) with multevel (Clark et al., 2011)6 . 5 In our preliminary comparison, there are no statistically significant differences in translation performances between Concat and the method of inputting and outputting concatenated multiple sentences, also proposed by Bawden et al. (2018). From the computational efficiency perspective, therefore, we chose Concat"
D19-6505,P11-2031,0,0.0309604,"putting the same number of preceding and succeeding sentences for the target sentence to be translated at the center, for Cor-g. We denote this setting as Coref-gatecentered (Cor-g-c). The number of weight parameters for each model is 111,057k for the baseline and Cor-m, and 111,558k for Cor-g. We used BLEU scores (Papineni et al., 2002) to evaluate the translation performance for each model. All reported BLEU scores in the experiments are averages for three times and are based on MeCab tokenization. Significance tests were conducted by paired bootstrap resampling (Koehn, 2004) with multevel (Clark et al., 2011)6 . 5 In our preliminary comparison, there are no statistically significant differences in translation performances between Concat and the method of inputting and outputting concatenated multiple sentences, also proposed by Bawden et al. (2018). From the computational efficiency perspective, therefore, we chose Concat as our baseline. 6 https://github.com/jhclark/multeval 7 These results are close to the reported BLEU scores of the Ja-En caption translations in Pryzant et al. (2018) 48 are anaphora, and cataphora is rarely observed in the test set. Ignoring the succeeding sentences, Cor-g-c at"
D19-6505,L18-1182,0,0.0667736,"Missing"
D19-6505,D16-1245,0,0.0257192,"nce relationships can be effectively utilized. Figure1 shows the network structure of our proposed model. At first, input sentences are analyzed by using a coreference resolution system. After that, the encoder part is structured based on the coreference resolution results, and the input text is encoded into hidden states. Then, the hidden states are converted to a translated text via attention distributions and the decoder. During the translation, the attention distri0 xij 0 = tail(maink ), xij = head(subk ). (4) 1 https://github.com/huggingface/neuralcoref. This code is based on the work by Clark and Manning (2016). 2 We treat a nominal noun which is the antecedent of a pronoun or a proper noun as a representative mention. 46 refers to a word tail(main1 ) → − i0 i0 h j 0 . βj 0 is calculated as follows: head(sub1 ) I have two daughters . They are · · · refers to a span main1 → − 0 → − 0 βji 0 = sigmoid(Wt h ij 0 + Ws h it−1 ), sub1 Figure 2: An example of a word-based coreference relation. where Wt and Ws are weight matrices. The backward encoding is similarly processed by replacing reff with refb . Finally, the forward and backward hidden states are concatenated to → − ← − hit = [ h it ; h it ] for eac"
D19-6505,W17-4811,0,0.0294308,"tically improved with Sequence-toSequence (Seq2Seq) models (Bahdanau et al., 2014). Most Seq2Seq models are used based on the premise that each sentence is independently translated one by one. In contrast to this premise, real sentences are often an element of a larger unit, such as a document. This means that a sentence is not always semantically self-contained in itself. To correctly interpret a sentence which is a part of a document, it is important to consider its context, preceding and/or succeeding sentences. In order to tackle the problem, Seq2Seq models that can receive two sentences (Tiedemann and Scherrer, 2017; Bawden et al., 2018; Voita et al., 2018; Wang et al., 2017) have been utilized. For capturing multiple-sentence information more effectively, Miculicich et al. (2018); Zhang et al. (2018) incorporated document-level attention modules into Seq2Seq models. Stojanovski and Fraser (2018) proposed a Seq2Seq model which can capture antecedents of pronouns 2 Sequence-to-Sequence Model In this section, we explain the standard Seq2Seq model proposed by Bahdanau et al. (2014), which 45 Proceedings of the Fourth Workshop on Discourse in Machine Translation (DiscoMT 2019), pages 45–50 c Hong Kong, China"
D19-6505,P18-1117,0,0.019327,"models (Bahdanau et al., 2014). Most Seq2Seq models are used based on the premise that each sentence is independently translated one by one. In contrast to this premise, real sentences are often an element of a larger unit, such as a document. This means that a sentence is not always semantically self-contained in itself. To correctly interpret a sentence which is a part of a document, it is important to consider its context, preceding and/or succeeding sentences. In order to tackle the problem, Seq2Seq models that can receive two sentences (Tiedemann and Scherrer, 2017; Bawden et al., 2018; Voita et al., 2018; Wang et al., 2017) have been utilized. For capturing multiple-sentence information more effectively, Miculicich et al. (2018); Zhang et al. (2018) incorporated document-level attention modules into Seq2Seq models. Stojanovski and Fraser (2018) proposed a Seq2Seq model which can capture antecedents of pronouns 2 Sequence-to-Sequence Model In this section, we explain the standard Seq2Seq model proposed by Bahdanau et al. (2014), which 45 Proceedings of the Fourth Workshop on Discourse in Machine Translation (DiscoMT 2019), pages 45–50 c Hong Kong, China, November 3, 2019. 2019 Association for"
D19-6505,D17-1301,0,0.0191692,"al., 2014). Most Seq2Seq models are used based on the premise that each sentence is independently translated one by one. In contrast to this premise, real sentences are often an element of a larger unit, such as a document. This means that a sentence is not always semantically self-contained in itself. To correctly interpret a sentence which is a part of a document, it is important to consider its context, preceding and/or succeeding sentences. In order to tackle the problem, Seq2Seq models that can receive two sentences (Tiedemann and Scherrer, 2017; Bawden et al., 2018; Voita et al., 2018; Wang et al., 2017) have been utilized. For capturing multiple-sentence information more effectively, Miculicich et al. (2018); Zhang et al. (2018) incorporated document-level attention modules into Seq2Seq models. Stojanovski and Fraser (2018) proposed a Seq2Seq model which can capture antecedents of pronouns 2 Sequence-to-Sequence Model In this section, we explain the standard Seq2Seq model proposed by Bahdanau et al. (2014), which 45 Proceedings of the Fourth Workshop on Discourse in Machine Translation (DiscoMT 2019), pages 45–50 c Hong Kong, China, November 3, 2019. 2019 Association for Computational Lingui"
D19-6505,D18-1049,0,0.0313502,"Missing"
E17-1037,C02-1053,1,0.520056,"mploys another summarization approach, the extractive summarization paradigm is worthwhile to leverage research resources. As another benefit, identifying an oracle summary for a set of reference summaries allows us to utilize yet another evaluation measure. Since both oracle and extractive summaries are sets of sentences, it is easy to check whether a system summary contains sentences in the oracle summary. As a result, F-measures, which are available to evaluate a system summary, are useful for evaluating classification-based extractive summarization (Mani and Bloedorn, 1998; Osborne, 2002; Hirao et al., 2002). Since ROUGEn evaluation does not identify which sentence is important, an Fmeasure conveys useful information in terms of “important sentence extraction.” Thus, combining ROUGEn and an F-measure allows us to scrutinize the failure analysis of systems. Note that more than one oracle summary might exist for a set of reference summaries because ROUGEn scores are based on the unweighted counting of n-grams. As a result, an F-measure might not be identical among multiple oracle summaries. Thus, we need to enumerate the oracle summaries for a set of reference summaries and compute the F-measures b"
E17-1037,P13-1020,0,0.0615489,"maries to exploit F-measures that evaluate which system summaries contain how many sentences that are extracted as an oracle summary. Our experimental results obtained from Document Understanding Conference (DUC) corpora demonstrated the following: (1) room still exists to improve the performance of extractive summarization; (2) the F-measures derived from the enumerated oracle summaries have significantly stronger correlations with human judgment than those derived from single oracle summaries. 1 Introduction Recently, compressive and abstractive summarization are attracting attention (e.g., Almeida and Martins (2013), Qian and Liu (2013), Yao et al. (2015), Banerjee et al. (2015), Bing et al. (2015)). However, extractive summarization remains a primary research topic because the linguistic quality of the resultant summaries is guaranteed, at least at the sentence level, which is a key requirement for practical use (e.g., Hong and Nenkova (2014), Hong et al. (2015), Yogatama et al. (2015), Parveen et al. (2015)). The summarization research community is experiencing a paradigm shift from extractive to compressive or abstractive summarization. Currently our question is: “Is extractive summariza386 Proceeding"
E17-1037,E14-1075,0,0.0408382,"measures derived from the enumerated oracle summaries have significantly stronger correlations with human judgment than those derived from single oracle summaries. 1 Introduction Recently, compressive and abstractive summarization are attracting attention (e.g., Almeida and Martins (2013), Qian and Liu (2013), Yao et al. (2015), Banerjee et al. (2015), Bing et al. (2015)). However, extractive summarization remains a primary research topic because the linguistic quality of the resultant summaries is guaranteed, at least at the sentence level, which is a key requirement for practical use (e.g., Hong and Nenkova (2014), Hong et al. (2015), Yogatama et al. (2015), Parveen et al. (2015)). The summarization research community is experiencing a paradigm shift from extractive to compressive or abstractive summarization. Currently our question is: “Is extractive summariza386 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 386–396, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics summary from a set of reference summaries and a source document(s). To the best of our knowledge, this is the firs"
E17-1037,hong-etal-2014-repository,0,0.410501,"Missing"
E17-1037,P15-1153,0,0.0190065,"that are extracted as an oracle summary. Our experimental results obtained from Document Understanding Conference (DUC) corpora demonstrated the following: (1) room still exists to improve the performance of extractive summarization; (2) the F-measures derived from the enumerated oracle summaries have significantly stronger correlations with human judgment than those derived from single oracle summaries. 1 Introduction Recently, compressive and abstractive summarization are attracting attention (e.g., Almeida and Martins (2013), Qian and Liu (2013), Yao et al. (2015), Banerjee et al. (2015), Bing et al. (2015)). However, extractive summarization remains a primary research topic because the linguistic quality of the resultant summaries is guaranteed, at least at the sentence level, which is a key requirement for practical use (e.g., Hong and Nenkova (2014), Hong et al. (2015), Yogatama et al. (2015), Parveen et al. (2015)). The summarization research community is experiencing a paradigm shift from extractive to compressive or abstractive summarization. Currently our question is: “Is extractive summariza386 Proceedings of the 15th Conference of the European Chapter of the Association for Computationa"
E17-1037,D15-1011,0,0.027651,"Missing"
E17-1037,N10-1133,0,0.0286592,"here the ROUGEn scores of the oracle summaries are significantly higher than those of the state-ofthe-art summarization systems. 3 Definition of Extractive Oracle Summaries We first briefly describe ROUGEn . Given set of reference summaries R and system summary S, ROUGEn is defined as follows: ROUGEn (R, S) = |R ||U (Rk )| X X k=1 min{N (gjn , Rk ), N (gjn , S)} j=1 |R ||U (Rk )| X X k=1 . Related Work Lin and Hovy (2003) utilized a naive exhaustive search method to obtain oracle summaries in terms of ROUGEn and exploited them to understand the limitations of extractive summarization systems. Ceylan et al. (2010) proposed another naive exhaustive search method to derive a probability density function from the ROUGEn scores of oracle summaries for the domains to which source documents belong. The computational complexity of naive exhaustive methods is exponential to the size of the sentence set. Thus, it may be possible to apply them to single document summarization tasks involving a dozen sentences, but it is infeasible to apply them to multiple document summarization tasks that involve several hundred sentences. To describe the difference between the ROUGEn scores of oracle and system summaries in mu"
E17-1037,P11-1052,0,0.204221,"nt Lmax (line 11). When we do not have room to add w, we update U by adding the score obtained by multiplying the density of w by the remaining length, Lmax (line 13), and exit the while loop. 5.3 Initial Score for Search 1. ROUGEn (R, V ) ≥ τ ; 2. ROUGEn (R, V ) < τ , R OUGE n (R, V ) < τ ; 3. ROUGEn (R, V ) < τ , R OUGE n (R, V ) ≥ τ . Since the branch and bound technique prunes the search by comparing the best solution found so far with the upper bounds, obtaining a good solution in the early stage is critical for raising search efficiency. Since ROUGEn is a monotone submodular function (Lin and Bilmes, 2011), we can obtain a good approximate solution by a greedy algorithm (Khuller et al., 1999). It is guaranteed that the score of the obtained approximate solution is larger than 12 (1 − 1e )OPT, where OPT is the score of the optimal solution. We employ the solution as the initial ROUGEn score of the candidate oracle summary. Algorithm 2 shows the greedy algorithm. In it, S denotes a summary and D denotes a set of sentences. The algorithm iteratively adds sentence s∗ that yields the largest gain in the ROUGEn score to current summary S, provided the length of the summary does not violate length con"
E17-1037,W13-3108,0,0.06166,"Missing"
E17-1037,W03-0510,0,0.049582,"inatorial optimization problem, and no polynomial time algorithms exist that can attain an optimal solution. 1. Room still exists for the further improvement of extractive summarization, i.e., where the ROUGEn scores of the oracle summaries are significantly higher than those of the state-ofthe-art summarization systems. 3 Definition of Extractive Oracle Summaries We first briefly describe ROUGEn . Given set of reference summaries R and system summary S, ROUGEn is defined as follows: ROUGEn (R, S) = |R ||U (Rk )| X X k=1 min{N (gjn , Rk ), N (gjn , S)} j=1 |R ||U (Rk )| X X k=1 . Related Work Lin and Hovy (2003) utilized a naive exhaustive search method to obtain oracle summaries in terms of ROUGEn and exploited them to understand the limitations of extractive summarization systems. Ceylan et al. (2010) proposed another naive exhaustive search method to derive a probability density function from the ROUGEn scores of oracle summaries for the domains to which source documents belong. The computational complexity of naive exhaustive methods is exponential to the size of the sentence set. Thus, it may be possible to apply them to single document summarization tasks involving a dozen sentences, but it is"
E17-1037,W09-1802,0,0.38159,"Missing"
E17-1037,W02-0401,0,0.114143,"a system that employs another summarization approach, the extractive summarization paradigm is worthwhile to leverage research resources. As another benefit, identifying an oracle summary for a set of reference summaries allows us to utilize yet another evaluation measure. Since both oracle and extractive summaries are sets of sentences, it is easy to check whether a system summary contains sentences in the oracle summary. As a result, F-measures, which are available to evaluate a system summary, are useful for evaluating classification-based extractive summarization (Mani and Bloedorn, 1998; Osborne, 2002; Hirao et al., 2002). Since ROUGEn evaluation does not identify which sentence is important, an Fmeasure conveys useful information in terms of “important sentence extraction.” Thus, combining ROUGEn and an F-measure allows us to scrutinize the failure analysis of systems. Note that more than one oracle summary might exist for a set of reference summaries because ROUGEn scores are based on the unweighted counting of n-grams. As a result, an F-measure might not be identical among multiple oracle summaries. Thus, we need to enumerate the oracle summaries for a set of reference summaries and com"
E17-1037,W12-2601,0,0.0227224,"sentence from V and return to the top of the recurrence. 6 6.1 Experiments Experimental Setting We conducted experiments on the corpora developed for a multiple document summarization task in DUC 2001 to 2007. Table 1 show the statistics of the data. In particular, the DUC-2005 to -2007 data sets not only have very large numbers of sentences and words but also a long target length (the reference summary length) of 250 words. All the words in the documents were stemmed by Porter’s stemmer (Porter, 1980). We computed ROUGE1 scores, excluding stopwords, and computed ROUGE2 scores, keeping them. Owczarzak et al. (2012) suggested using ROUGE1 and keeping stopwords. However, as Takamura et al. argued (Takamura and Okumura, 2009), the summaries optimized with non-content words failed to consider the actual quality. Thus, we excluded stopwords for computing the ROUGE1 scores. We enumerated the following two types of oracle summaries: those for a set of references for a given topic and those for each reference in the set of references. 6.2 6.2.1 Results and Discussion Impact of Oracle ROUGEn scores Table 2 shows the average ROUGE1,2 scores of the oracle summaries obtained from both a set of references and each r"
E17-1037,D15-1226,0,0.0258891,"tly stronger correlations with human judgment than those derived from single oracle summaries. 1 Introduction Recently, compressive and abstractive summarization are attracting attention (e.g., Almeida and Martins (2013), Qian and Liu (2013), Yao et al. (2015), Banerjee et al. (2015), Bing et al. (2015)). However, extractive summarization remains a primary research topic because the linguistic quality of the resultant summaries is guaranteed, at least at the sentence level, which is a key requirement for practical use (e.g., Hong and Nenkova (2014), Hong et al. (2015), Yogatama et al. (2015), Parveen et al. (2015)). The summarization research community is experiencing a paradigm shift from extractive to compressive or abstractive summarization. Currently our question is: “Is extractive summariza386 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 386–396, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics summary from a set of reference summaries and a source document(s). To the best of our knowledge, this is the first ILP formulation that extracts oracle summaries. Second, since it"
E17-1037,P16-1172,0,0.0151764,"denotes the multiple set of n-grams that appear in system-generated summary S (a set of sentences). N (gjn , Rk ) and N (gjn , S) return the number of occurrences of n-gram gjn in the k-th reference and system summaries, respectively. Function U (·) transforms a multiple set into a normal set. ROUGEn takes values in the range of [0, 1], and when the n-gram occurrences of the system summary agree with those of the reference summary, the value is 1. In this paper, we focus on extractive summarization, employ ROUGEn as an evaluation measure, 387 with oracle summaries found by a greedy algorithm. Peyrard and Eckle-Kohler (2016) proposed a method to find a summary that approximates a ROUGE score based on the ROUGE scores of individual sentences and exploited the framework to train their summarizer. As mentioned above, such summaries do not always agree with the oracle summaries defined in our paper. Thus, the quality of the training data is suspect. Moreover, since these studies fail to consider that a set of reference summaries has multiple oracle summaries, the score of the loss function defined between their oracle and system summaries is not appropriate in most cases. As mentioned above, no known efficient algori"
E17-1037,D13-1156,0,0.0742105,"that evaluate which system summaries contain how many sentences that are extracted as an oracle summary. Our experimental results obtained from Document Understanding Conference (DUC) corpora demonstrated the following: (1) room still exists to improve the performance of extractive summarization; (2) the F-measures derived from the enumerated oracle summaries have significantly stronger correlations with human judgment than those derived from single oracle summaries. 1 Introduction Recently, compressive and abstractive summarization are attracting attention (e.g., Almeida and Martins (2013), Qian and Liu (2013), Yao et al. (2015), Banerjee et al. (2015), Bing et al. (2015)). However, extractive summarization remains a primary research topic because the linguistic quality of the resultant summaries is guaranteed, at least at the sentence level, which is a key requirement for practical use (e.g., Hong and Nenkova (2014), Hong et al. (2015), Yogatama et al. (2015), Parveen et al. (2015)). The summarization research community is experiencing a paradigm shift from extractive to compressive or abstractive summarization. Currently our question is: “Is extractive summariza386 Proceedings of the 15th Confere"
E17-1037,E12-1023,0,0.0998676,"widely used to solve NP-hard combinatorial optimization problems, the solutions are not always optimal. Thus, the summary does not always have a maximum ROUGEn score for the set of reference summaries. Both works called the summary found by their methods the oracle, but it differs from the definition in our paper. Since summarization systems cannot reproduce human-made reference summaries in most cases, oracle summaries, which can be reproduced by summarization systems, have been used as training data to tune the parameters of summarization systems. For example, Kulesza and Tasker (2011) and Sipos et al. (2012) trained their summarizers 2. The F-measures derived from multiple oracle summaries obtain significantly stronger correlations with human judgment than those derived from single oracle summaries. 2 (2) (1) N (gjn , Rk ) j=1 Rk denotes the multiple set of n-grams that occur in k-th reference summary Rk , and S denotes the multiple set of n-grams that appear in system-generated summary S (a set of sentences). N (gjn , Rk ) and N (gjn , S) return the number of occurrences of n-gram gjn in the k-th reference and system summaries, respectively. Function U (·) transforms a multiple set into a normal"
E17-1037,E09-1089,0,0.140371,"ted experiments on the corpora developed for a multiple document summarization task in DUC 2001 to 2007. Table 1 show the statistics of the data. In particular, the DUC-2005 to -2007 data sets not only have very large numbers of sentences and words but also a long target length (the reference summary length) of 250 words. All the words in the documents were stemmed by Porter’s stemmer (Porter, 1980). We computed ROUGE1 scores, excluding stopwords, and computed ROUGE2 scores, keeping them. Owczarzak et al. (2012) suggested using ROUGE1 and keeping stopwords. However, as Takamura et al. argued (Takamura and Okumura, 2009), the summaries optimized with non-content words failed to consider the actual quality. Thus, we excluded stopwords for computing the ROUGE1 scores. We enumerated the following two types of oracle summaries: those for a set of references for a given topic and those for each reference in the set of references. 6.2 6.2.1 Results and Discussion Impact of Oracle ROUGEn scores Table 2 shows the average ROUGE1,2 scores of the oracle summaries obtained from both a set of references and each reference in the set (“multi” and “single”), those of the best conventional system (Peer), and those obtained f"
E17-1037,D15-1228,0,0.0364183,"Missing"
E17-2047,D15-1166,0,0.0367292,"ting generation behavior can become more severe in some NLG tasks than in MT. The very short ABS task in DUC-2003 and 2004 (Over et al., 2007) is a typical example because it requires the generation of a summary in a pre-defined limited output space, such as ten words or 75 bytes. Thus, the repeated output consumes precious limited output space. Unfortunately, the coverage approach cannot be directly applied to ABS tasks since they require us to optimally find salient ideas 2 Baseline RNN-based EncDec Model The baseline of our proposal is an RNN-based EncDec model with an attention mechanism (Luong et al., 2015). In fact, this model has already been used as a strong baseline for ABS tasks (Chopra et al., 2016; Kikuchi et al., 2016) as well as in the NMT literature. More specifically, as a case study we employ a 2-layer bidirectional LSTM encoder and a 2-layer LSTM decoder with a global attention (Bahdanau et al., 2014). We omit a detailed review of the descriptions due to space limitations. The following are the necessary parts for explaining our proposed method. Let X = (xi )Ii=1 and Y = (yj )Jj=1 be input and output sequences, respectively, where xi and 291 Proceedings of the 15th Conference of the"
E17-2047,D14-1179,0,0.0183778,"Missing"
E17-2047,D16-1096,0,0.0741514,"ased encoder-decoder (EncDec) approach has recently been providing significant progress in various natural language generation (NLG) tasks, i.e., machine translation (MT) (Sutskever et al., 2014; Cho et al., 2014) and abstractive summarization (ABS) (Rush et al., 2015). Since a scheme in this approach can be interpreted as a conditional language model, it is suitable for NLG tasks. However, one potential weakness is that it sometimes repeatedly generates the same phrase (or word). This issue has been discussed in the neural MT (NMT) literature as a part of a coverage problem (Tu et al., 2016; Mi et al., 2016). Such repeating generation behavior can become more severe in some NLG tasks than in MT. The very short ABS task in DUC-2003 and 2004 (Over et al., 2007) is a typical example because it requires the generation of a summary in a pre-defined limited output space, such as ten words or 75 bytes. Thus, the repeated output consumes precious limited output space. Unfortunately, the coverage approach cannot be directly applied to ABS tasks since they require us to optimally find salient ideas 2 Baseline RNN-based EncDec Model The baseline of our proposal is an RNN-based EncDec model with an attention"
E17-2047,N16-1012,0,0.176215,"cy of each target vocabulary that can occur in a summary during the encoding process and exploit the estimation to control the output words in each decoding step. We refer to our additional component as a wordfrequency estimation (WFE) sub-model. The WFE sub-model explicitly manages how many times each word has been generated so far and might be generated in the future during the decoding process. Thus, we expect to decisively prohibit excessive generation. Finally, we evaluate the effectiveness of our method on well-studied ABS benchmark data provided by Rush et al. (2015), and evaluated in (Chopra et al., 2016; Nallapati et al., 2016b; Kikuchi et al., 2016; Takase et al., 2016; Ayana et al., 2016; Gulcehre et al., 2016). This paper tackles the reduction of redundant repeating generation that is often observed in RNN-based encoder-decoder models. Our basic idea is to jointly estimate the upper-bound frequency of each target vocabulary in the encoder and control the output words based on the estimation in the decoder. Our method shows significant improvement over a strong RNN-based encoder-decoder baseline and achieved its best results on an abstractive summarization benchmark. 1 Introduction The RNN"
E17-2047,K16-1028,0,0.0150191,"abulary that can occur in a summary during the encoding process and exploit the estimation to control the output words in each decoding step. We refer to our additional component as a wordfrequency estimation (WFE) sub-model. The WFE sub-model explicitly manages how many times each word has been generated so far and might be generated in the future during the decoding process. Thus, we expect to decisively prohibit excessive generation. Finally, we evaluate the effectiveness of our method on well-studied ABS benchmark data provided by Rush et al. (2015), and evaluated in (Chopra et al., 2016; Nallapati et al., 2016b; Kikuchi et al., 2016; Takase et al., 2016; Ayana et al., 2016; Gulcehre et al., 2016). This paper tackles the reduction of redundant repeating generation that is often observed in RNN-based encoder-decoder models. Our basic idea is to jointly estimate the upper-bound frequency of each target vocabulary in the encoder and control the output words based on the estimation in the decoder. Our method shows significant improvement over a strong RNN-based encoder-decoder baseline and achieved its best results on an abstractive summarization benchmark. 1 Introduction The RNN-based encoder-decoder ("
E17-2047,D15-1044,0,0.611359,"intly estimate the upper-bound frequency of each target vocabulary that can occur in a summary during the encoding process and exploit the estimation to control the output words in each decoding step. We refer to our additional component as a wordfrequency estimation (WFE) sub-model. The WFE sub-model explicitly manages how many times each word has been generated so far and might be generated in the future during the decoding process. Thus, we expect to decisively prohibit excessive generation. Finally, we evaluate the effectiveness of our method on well-studied ABS benchmark data provided by Rush et al. (2015), and evaluated in (Chopra et al., 2016; Nallapati et al., 2016b; Kikuchi et al., 2016; Takase et al., 2016; Ayana et al., 2016; Gulcehre et al., 2016). This paper tackles the reduction of redundant repeating generation that is often observed in RNN-based encoder-decoder models. Our basic idea is to jointly estimate the upper-bound frequency of each target vocabulary in the encoder and control the output words based on the estimation in the decoder. Our method shows significant improvement over a strong RNN-based encoder-decoder baseline and achieved its best results on an abstractive summariz"
E17-2047,P16-1159,0,0.0255766,"le 2 and Figure 3. We can expect to further gain the overall performance by improving the performance of the WFE sub-model. Table 4: Confusion matrix of WFE on Gigaword data: only evaluated true frequency ≥ 1. imum risk estimation, while we trained all the models in our experiments with standard (pointwise) log-likelihood maximization. MRT essentially complements our method. We expect to further improve its performance by applying MRT for its training since recent progress of NMT has suggested leveraging a sequence-wise optimization technique for improving performance (Wiseman and Rush, 2016; Shen et al., 2016). We leave this as our future work. 4.3 5 This paper discussed the behavior of redundant repeating generation often observed in neural EncDec approaches. We proposed a method for reducing such redundancy by incorporating a submodel that directly estimates and manages the frequency of each target vocabulary in the output. Experiments on ABS benchmark data showed the effectiveness of our method, EncDec+WFE, for both improving automatic evaluation performance and reducing the actual redundancy. Our method is suitable for lossy compression tasks such as image caption generation tasks. Generation e"
E17-2047,P16-1014,0,0.121269,"on to control the output words in each decoding step. We refer to our additional component as a wordfrequency estimation (WFE) sub-model. The WFE sub-model explicitly manages how many times each word has been generated so far and might be generated in the future during the decoding process. Thus, we expect to decisively prohibit excessive generation. Finally, we evaluate the effectiveness of our method on well-studied ABS benchmark data provided by Rush et al. (2015), and evaluated in (Chopra et al., 2016; Nallapati et al., 2016b; Kikuchi et al., 2016; Takase et al., 2016; Ayana et al., 2016; Gulcehre et al., 2016). This paper tackles the reduction of redundant repeating generation that is often observed in RNN-based encoder-decoder models. Our basic idea is to jointly estimate the upper-bound frequency of each target vocabulary in the encoder and control the output words based on the estimation in the decoder. Our method shows significant improvement over a strong RNN-based encoder-decoder baseline and achieved its best results on an abstractive summarization benchmark. 1 Introduction The RNN-based encoder-decoder (EncDec) approach has recently been providing significant progress in various natural lan"
E17-2047,D16-1140,0,0.224773,"a summary during the encoding process and exploit the estimation to control the output words in each decoding step. We refer to our additional component as a wordfrequency estimation (WFE) sub-model. The WFE sub-model explicitly manages how many times each word has been generated so far and might be generated in the future during the decoding process. Thus, we expect to decisively prohibit excessive generation. Finally, we evaluate the effectiveness of our method on well-studied ABS benchmark data provided by Rush et al. (2015), and evaluated in (Chopra et al., 2016; Nallapati et al., 2016b; Kikuchi et al., 2016; Takase et al., 2016; Ayana et al., 2016; Gulcehre et al., 2016). This paper tackles the reduction of redundant repeating generation that is often observed in RNN-based encoder-decoder models. Our basic idea is to jointly estimate the upper-bound frequency of each target vocabulary in the encoder and control the output words based on the estimation in the decoder. Our method shows significant improvement over a strong RNN-based encoder-decoder baseline and achieved its best results on an abstractive summarization benchmark. 1 Introduction The RNN-based encoder-decoder (EncDec) approach has re"
E17-2047,D16-1112,1,0.505499,"encoding process and exploit the estimation to control the output words in each decoding step. We refer to our additional component as a wordfrequency estimation (WFE) sub-model. The WFE sub-model explicitly manages how many times each word has been generated so far and might be generated in the future during the decoding process. Thus, we expect to decisively prohibit excessive generation. Finally, we evaluate the effectiveness of our method on well-studied ABS benchmark data provided by Rush et al. (2015), and evaluated in (Chopra et al., 2016; Nallapati et al., 2016b; Kikuchi et al., 2016; Takase et al., 2016; Ayana et al., 2016; Gulcehre et al., 2016). This paper tackles the reduction of redundant repeating generation that is often observed in RNN-based encoder-decoder models. Our basic idea is to jointly estimate the upper-bound frequency of each target vocabulary in the encoder and control the output words based on the estimation in the decoder. Our method shows significant improvement over a strong RNN-based encoder-decoder baseline and achieved its best results on an abstractive summarization benchmark. 1 Introduction The RNN-based encoder-decoder (EncDec) approach has recently been providing"
E17-2047,P16-1008,0,0.0948626,"duction The RNN-based encoder-decoder (EncDec) approach has recently been providing significant progress in various natural language generation (NLG) tasks, i.e., machine translation (MT) (Sutskever et al., 2014; Cho et al., 2014) and abstractive summarization (ABS) (Rush et al., 2015). Since a scheme in this approach can be interpreted as a conditional language model, it is suitable for NLG tasks. However, one potential weakness is that it sometimes repeatedly generates the same phrase (or word). This issue has been discussed in the neural MT (NMT) literature as a part of a coverage problem (Tu et al., 2016; Mi et al., 2016). Such repeating generation behavior can become more severe in some NLG tasks than in MT. The very short ABS task in DUC-2003 and 2004 (Over et al., 2007) is a typical example because it requires the generation of a summary in a pre-defined limited output space, such as ten words or 75 bytes. Thus, the repeated output consumes precious limited output space. Unfortunately, the coverage approach cannot be directly applied to ABS tasks since they require us to optimally find salient ideas 2 Baseline RNN-based EncDec Model The baseline of our proposal is an RNN-based EncDec model"
E17-2047,D16-1137,0,0.0591229,"Missing"
E17-2049,N06-1022,0,0.129494,". 1 Introduction The CKY or Viterbi inside algorithm is a wellknown algorithm for PCFG parsing (Jurafsky and Martin, 2000), which is a dynamic programming parser using a chart table to calculate the Viterbi tree. This algorithm is commonly used in natural language parsing, but when the size of the grammar is extremely large, exhaustive parsing becomes impractical. One way to reduce the computational cost of PCFG parsing is to prune the edges produced during parsing. In fact, modern parsers have often employed pruning techniques such as beam search (Ratnaparkhi, 1999) and coarse-tofine search (Charniak et al., 2006). Despite their practical success, both pruning methods are approximate, so the solution of the parser is not always optimal, i.e., the parser does not always output the Viterbi tree. Recently, another line of work has explored A* search algo2 Iterative Viterbi Parsing Following Pauls and Klein (2009), we define some notations. The IVP algorithm takes as input a 305 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 305–310, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics"
E17-2049,D16-1257,0,0.029909,"64), K-best IVP is much faster than Lazy. However, K-best IVP did not work well when setting k to more than 128. We show the reason in Figure 4 where we plot the number of edges in chart table at each K-best IVP iterations for some test sentence with length 28. It is clear that the smaller k is, the earlier it is convergent. Moreover, when setting k too large, it is difficult to compute a tight lower bound, i.e., K-best IVP does not prune unnecessary edges efficiently. However, in practice, this is not likely to be a serious problem since many NLP tasks use only very small kbest parse trees (Choe and Charniak, 2016). Experiments We used the Wall Street Journal (WSJ) part of the English Penn Treebank: Sections 02–21 were used for training, sentences of length 1–35 in Section 22 for testing. We estimated a Chomsky normal form PCFG by maximum likelihood from rightbranching binarized trees without function labels and trace-fillers. Note that while this grammar is a proof-of-concept, CKY on a larger grammar does not work well even for short sentences. Table 1 shows that the number of edges produced by the IVP algorithm is significantly smaller than standard CKY. Moreover, many of the edges are pruned during t"
E17-2049,P12-1024,0,0.0267597,"Missing"
E17-2049,P12-1064,0,0.0458297,"Missing"
E17-2049,P10-1050,0,0.0614102,"Missing"
E17-2049,N03-1016,0,0.0303873,"Missing"
E17-2049,P09-1108,0,0.0113693,"mmar is extremely large, exhaustive parsing becomes impractical. One way to reduce the computational cost of PCFG parsing is to prune the edges produced during parsing. In fact, modern parsers have often employed pruning techniques such as beam search (Ratnaparkhi, 1999) and coarse-tofine search (Charniak et al., 2006). Despite their practical success, both pruning methods are approximate, so the solution of the parser is not always optimal, i.e., the parser does not always output the Viterbi tree. Recently, another line of work has explored A* search algo2 Iterative Viterbi Parsing Following Pauls and Klein (2009), we define some notations. The IVP algorithm takes as input a 305 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 305–310, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics (a) A B C D A B C D (b) X1 X2 A B X2 A B C D A B C D A B X2 A B X2 A B C D A B C D A B X2 A B X2 Level A B C D A B C D X1 X2 A B X2 A B C D X1 C D A B C D 0 Op A B X2 Figure 1: (a) An original chart table consisting of non-terminal symbols only. (b) A coarse chart table consisting of both non-terminal"
E17-2049,N07-1051,0,\N,Missing
E17-2049,P05-1010,0,\N,Missing
I11-1004,J08-1002,0,0.059746,"show that our proposal can significantly improve BLEU scores of 2.47∼3.15 points compared with using the original English sentences. We finally conclude this paper by summarizing our proposal and the experiment results. Specially, for English-to-Japanese translation, Isozaki et al. (2010b) proposed to move syntactic or semantic heads to the end of corresponding phrases or clauses so that to yield head finalized English (HFE) sentences which follow the word order of Japanese. The head information of an English sentence is detected by a head-driven phrase structure grammar (HPSG) parser, Enju1 (Miyao and Tsujii, 2008). In addition, transformation rules were manually written for appending particle seed words, refining POS tags to be used before parsing, and deleting English determiners. Due to the usage of the same parser, we take this HFE approach as one of our baseline systems. The goal in this paper, however, is to learn preordering rules from parallel data in an automatic way. Under this motivation, pre-ordering rules can be extracted in a language-independent manner. A number of researches follow this automatic way. For example, in (Xia and McCord, 2004), a variety of heuristic rules were applied to bi"
I11-1004,J03-1002,0,0.00568597,"ese sentences into SVO-style English sentences. For comparison, our proposal 1) makes use of not only PASs but also the source syntactic tree structures for preordering rule matching, 2) extracts pre-ordering 1 Figure 1 shows a word-aligned HPSG-tree-tostring pair for English-to-Japanese translation. PASs among lexical nodes and their argument nodes in this HPSG tree are described by arrows in thick-lines. For simplicity, we only draw the identifiers for the signs of the nodes in the HPSG tree. Note that the identifiers that start with ‘c’ 2 These word alignments are gained by running GIZA++ (Och and Ney, 2003) on the original parallel sentences. http://www-tsujii.is.s.u-tokyo.ac.jp/enju/index.html 30 c0 &lt;tok id=t0 cat=SC pos=WRB base=when lexentry=[when] pred=conj_arg12 arg1=c16 arg2=c3&gt; c0 c1 c1 c3 &lt;cons id=c16 cat=S xcat= head=c18 sem_head=c18 schema=mod_head&gt; t0 &lt;cons id=c3 cat=S xcat= head=c13 sem_head=c13 schema=subj_head&gt; c18 c6 c21 c8 c13 c10 c7 c5 c3 c2 c16 c4 c2 c16 c9 c11 t0 t1 t2 t3 when the fluid pressure c12 t4 cylinder t6 31 is c23 c15 c17 c20 c14 t5 c19 t7 t8 t9 used , c22 c24 c25 t10 t11 t12 is gradually applied fluid . 流体 圧 シリンダ 31 の 場合 は 流体 が 徐々に 排出 さ れる こと と なる 。 0 1 fluid pressu"
I11-1004,P05-1033,0,0.0598702,"otone or reordering) with probabilities for each bilingual phrase from the training data. For example, by taking lexical information as features, a maximum entropy phrase reordering model was proposed by Xiong et al. (2006). Second, syntax-based models attempt to solve the word ordering problem by employing syntactic structures. For example, linguistically syntaxbased approaches (Galley et al., 2004; Liu et al., 2006) first parse source and/or target sentences and then learn reordering templates from the subtree fragments of the parse trees. In contrast, hierarchical phrase based translation (Chiang, 2005) is a formally syntax-based approach which can automatically extract hierarchical ordering rules from aligned string-string pairs without using additional parsers. These approaches have been proved to be both algorithmically appealing and empirically successful. However, most of current syntax-based SMT systems use IBM models (Brown et al., 1993) and hidden Markov model (HMM) (Vogel et al., 1996) to generate word alignments. These models have a penalty parameter associated with long distance jumps, and tend to misalign words which move far from the window sizes of their expected positions (Xu"
I11-1004,P03-1021,0,0.0213618,"topological order do if n is a terminal node then n.srcPhrase ← E[n.srcSpan[0]] else if n.srcPhrase = NULL then n.srcPhrase ← C ONNECT(n.children().srcPhrase) end if end for 3 Experiments 3.1 Setup We test our proposal by translating from English to Japanese. We use the NTCIR-9 English-Japanese patent corpus4 as our experiment set. Since the reference set of the official test set has not been released yet, we instead split the original development set averagely into two parts, named dev.a and dev.b. In our experiments, we first take dev.a as our development set for minimum-error rate tuning (Och, 2003) and then report the final translation accuracies on dev.b. For direct comparison with other systems in the future, we use the configuration of the official baseline system5 : • Moses6 (Koehn et al., 2007): revision = “3717” as the baseline decoder. Note that we also train Moses using HFE sentences (Isozaki et al., 2010b) and the English sentences pre-ordered by PASs; Algorithm 2 sketches the algorithm for applying pre-ordering rules to a given HPSG tree TE . The algorithm contains three parts: rule matching (Lines 4-12), bottom-up rule applying (Lines 13-19), and sentence collecting (Lines 20"
I11-1004,P05-1066,0,0.180454,"jumps, and tend to misalign words which move far from the window sizes of their expected positions (Xu et al., 2009; Genzel, 2010). The third type tackles the word-order problem in pre-ordering ways. Through the usage of a sequence of pre-ordering rules, the word order of an original source sentence is (approximately) changed into the word order of the target sentence. Here, the pre-ordering rules can be manually or automatically extracted. For manual extraction of pre-ordering rules, linguistic background and expertise are required for predetermined language pairs, such as for GermanEnglish (Collins et al., 2005), Chinese-to-English (Wang et al., 2007), Japanese-to-English (KatzBrown and Collins, 2007), and English-to-SOV languages (Xu et al., 2009). Word ordering remains as an essential problem for translating between languages with substantial structural differences, such as SOV and SVO languages. In this paper, we propose to automatically extract pre-ordering rules from predicateargument structures. A pre-ordering rule records the relative position mapping of a predicate word and its argument phrases from the source language side to the target language side. We propose 1) a lineartime algorithm to"
I11-1004,P02-1040,0,0.0821751,"Missing"
I11-1004,N04-1035,0,0.311955,"mmunication Science Laboratories, NTT Corporation 2-4 Hikaridai Seika-cho, Soraku-gun Kyoto 619-0237 Japan {wu.xianchao,sudoh.katsuhito,kevin.duh,tsukada.hajime,nagata.masaaki}@lab.ntt.co.jp Abstract orientations (monotone or reordering) with probabilities for each bilingual phrase from the training data. For example, by taking lexical information as features, a maximum entropy phrase reordering model was proposed by Xiong et al. (2006). Second, syntax-based models attempt to solve the word ordering problem by employing syntactic structures. For example, linguistically syntaxbased approaches (Galley et al., 2004; Liu et al., 2006) first parse source and/or target sentences and then learn reordering templates from the subtree fragments of the parse trees. In contrast, hierarchical phrase based translation (Chiang, 2005) is a formally syntax-based approach which can automatically extract hierarchical ordering rules from aligned string-string pairs without using additional parsers. These approaches have been proved to be both algorithmically appealing and empirically successful. However, most of current syntax-based SMT systems use IBM models (Brown et al., 1993) and hidden Markov model (HMM) (Vogel et"
I11-1004,C10-1043,0,0.299656,"syntax-based approach which can automatically extract hierarchical ordering rules from aligned string-string pairs without using additional parsers. These approaches have been proved to be both algorithmically appealing and empirically successful. However, most of current syntax-based SMT systems use IBM models (Brown et al., 1993) and hidden Markov model (HMM) (Vogel et al., 1996) to generate word alignments. These models have a penalty parameter associated with long distance jumps, and tend to misalign words which move far from the window sizes of their expected positions (Xu et al., 2009; Genzel, 2010). The third type tackles the word-order problem in pre-ordering ways. Through the usage of a sequence of pre-ordering rules, the word order of an original source sentence is (approximately) changed into the word order of the target sentence. Here, the pre-ordering rules can be manually or automatically extracted. For manual extraction of pre-ordering rules, linguistic background and expertise are required for predetermined language pairs, such as for GermanEnglish (Collins et al., 2005), Chinese-to-English (Wang et al., 2007), Japanese-to-English (KatzBrown and Collins, 2007), and English-to-S"
I11-1004,2007.tmi-papers.21,0,0.143543,"Missing"
I11-1004,D10-1092,1,0.904154,"Missing"
I11-1004,N04-4026,0,0.272773,"Missing"
I11-1004,W10-1736,1,0.754264,", in order to overcome the shortages of traditional distance based distortion models (Brown et al., 1993; Koehn et al., 2007), phrase dependent lexicalized reordering models were proposed by several researchers (Tillman, 2004; Kumar and Byrne, 2005). Lexicalized reordering models learn local 29 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 29–37, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP rules in an automatic way, and 3) use factored representations of syntactic features to refine the preordering rules. Following (Wu et al., 2010a; Isozaki et al., 2010b), we use the HPSG parser Enju to generate the PASs of English sentences. HPSG (Pollard and Sag, 1994) is a lexicalist grammar framework. In HPSG, linguistic entities such as words and phrases are represented by a data structure called a sign. A sign gives a factored representation of the syntactic features of a word/phrase, as well as a representation of their semantic content which corresponds to PASs. In order to record the relative positions among a predicate word and its argument phrases, we propose a linear-time algorithm to extract preordering rules from word-aligned HPSG-tree-tostring"
I11-1004,C96-2141,0,0.339493,"al., 2004; Liu et al., 2006) first parse source and/or target sentences and then learn reordering templates from the subtree fragments of the parse trees. In contrast, hierarchical phrase based translation (Chiang, 2005) is a formally syntax-based approach which can automatically extract hierarchical ordering rules from aligned string-string pairs without using additional parsers. These approaches have been proved to be both algorithmically appealing and empirically successful. However, most of current syntax-based SMT systems use IBM models (Brown et al., 1993) and hidden Markov model (HMM) (Vogel et al., 1996) to generate word alignments. These models have a penalty parameter associated with long distance jumps, and tend to misalign words which move far from the window sizes of their expected positions (Xu et al., 2009; Genzel, 2010). The third type tackles the word-order problem in pre-ordering ways. Through the usage of a sequence of pre-ordering rules, the word order of an original source sentence is (approximately) changed into the word order of the target sentence. Here, the pre-ordering rules can be manually or automatically extracted. For manual extraction of pre-ordering rules, linguistic b"
I11-1004,D07-1077,0,0.403469,"e far from the window sizes of their expected positions (Xu et al., 2009; Genzel, 2010). The third type tackles the word-order problem in pre-ordering ways. Through the usage of a sequence of pre-ordering rules, the word order of an original source sentence is (approximately) changed into the word order of the target sentence. Here, the pre-ordering rules can be manually or automatically extracted. For manual extraction of pre-ordering rules, linguistic background and expertise are required for predetermined language pairs, such as for GermanEnglish (Collins et al., 2005), Chinese-to-English (Wang et al., 2007), Japanese-to-English (KatzBrown and Collins, 2007), and English-to-SOV languages (Xu et al., 2009). Word ordering remains as an essential problem for translating between languages with substantial structural differences, such as SOV and SVO languages. In this paper, we propose to automatically extract pre-ordering rules from predicateargument structures. A pre-ordering rule records the relative position mapping of a predicate word and its argument phrases from the source language side to the target language side. We propose 1) a lineartime algorithm to extract the pre-ordering rules from word"
I11-1004,P10-1034,1,0.926742,"dering ways. First, in order to overcome the shortages of traditional distance based distortion models (Brown et al., 1993; Koehn et al., 2007), phrase dependent lexicalized reordering models were proposed by several researchers (Tillman, 2004; Kumar and Byrne, 2005). Lexicalized reordering models learn local 29 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 29–37, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP rules in an automatic way, and 3) use factored representations of syntactic features to refine the preordering rules. Following (Wu et al., 2010a; Isozaki et al., 2010b), we use the HPSG parser Enju to generate the PASs of English sentences. HPSG (Pollard and Sag, 1994) is a lexicalist grammar framework. In HPSG, linguistic entities such as words and phrases are represented by a data structure called a sign. A sign gives a factored representation of the syntactic features of a word/phrase, as well as a representation of their semantic content which corresponds to PASs. In order to record the relative positions among a predicate word and its argument phrases, we propose a linear-time algorithm to extract preordering rules from word-ali"
I11-1004,P07-2045,0,0.0692904,"eline SMT systems. 1 Introduction Statistical machine translation (SMT) suffers from an essential problem for translating between languages with substantial structural differences, such as between English which is a subject-verbobject (SVO) language and Japanese which is a typical subject-object-verb (SOV) language. Numerous approaches have been consequently proposed to tackle this word-order problem, such as lexicalized reordering methods, syntax-based models, and pre-ordering ways. First, in order to overcome the shortages of traditional distance based distortion models (Brown et al., 1993; Koehn et al., 2007), phrase dependent lexicalized reordering models were proposed by several researchers (Tillman, 2004; Kumar and Byrne, 2005). Lexicalized reordering models learn local 29 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 29–37, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP rules in an automatic way, and 3) use factored representations of syntactic features to refine the preordering rules. Following (Wu et al., 2010a; Isozaki et al., 2010b), we use the HPSG parser Enju to generate the PASs of English sentences. HPSG (Pollard and Sag, 1994) i"
I11-1004,2006.iwslt-evaluation.11,1,0.751145,"is that, predicate-argument structures (PASs) are introduced to extract fine-grained pre-ordering rules. PASs have the following merits for describing reordering phenomena: • predicate words and argument phrases respectively record reordering phenomena in a lexicalized level and an abstract level; • PASs provide a fine-grained classification of the reordering phenomena since they include factored representations of syntactic features of the predicate words and their argument phrases. 2 Pre-ordering Rule Extraction and Application 2.1 An example The idea of using PASs for pre-ordering follows (Komachi et al., 2006). Several reordering operations were manually designed by Komachi et al. (2006) to pre-ordering Japanese sentences into SVO-style English sentences. For comparison, our proposal 1) makes use of not only PASs but also the source syntactic tree structures for preordering rule matching, 2) extracts pre-ordering 1 Figure 1 shows a word-aligned HPSG-tree-tostring pair for English-to-Japanese translation. PASs among lexical nodes and their argument nodes in this HPSG tree are described by arrows in thick-lines. For simplicity, we only draw the identifiers for the signs of the nodes in the HPSG tree."
I11-1004,C04-1073,0,0.419443,"phrase structure grammar (HPSG) parser, Enju1 (Miyao and Tsujii, 2008). In addition, transformation rules were manually written for appending particle seed words, refining POS tags to be used before parsing, and deleting English determiners. Due to the usage of the same parser, we take this HFE approach as one of our baseline systems. The goal in this paper, however, is to learn preordering rules from parallel data in an automatic way. Under this motivation, pre-ordering rules can be extracted in a language-independent manner. A number of researches follow this automatic way. For example, in (Xia and McCord, 2004), a variety of heuristic rules were applied to bilingual parse trees to extract pre-ordering rules for French-English translation. Rottmann and Vogen (2007) learned reordering rules based on sequences of part-of-speech (POS) tags, instead of parse trees. Dependency trees were used by Genzel (2010) to extract source-side reordering rules for translating languages from SVO to SOV, etc.. The novel idea expressed in this paper is that, predicate-argument structures (PASs) are introduced to extract fine-grained pre-ordering rules. PASs have the following merits for describing reordering phenomena:"
I11-1004,P06-1066,0,0.17735,"Missing"
I11-1004,H05-1021,0,0.0214174,"between languages with substantial structural differences, such as between English which is a subject-verbobject (SVO) language and Japanese which is a typical subject-object-verb (SOV) language. Numerous approaches have been consequently proposed to tackle this word-order problem, such as lexicalized reordering methods, syntax-based models, and pre-ordering ways. First, in order to overcome the shortages of traditional distance based distortion models (Brown et al., 1993; Koehn et al., 2007), phrase dependent lexicalized reordering models were proposed by several researchers (Tillman, 2004; Kumar and Byrne, 2005). Lexicalized reordering models learn local 29 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 29–37, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP rules in an automatic way, and 3) use factored representations of syntactic features to refine the preordering rules. Following (Wu et al., 2010a; Isozaki et al., 2010b), we use the HPSG parser Enju to generate the PASs of English sentences. HPSG (Pollard and Sag, 1994) is a lexicalist grammar framework. In HPSG, linguistic entities such as words and phrases are represented by a data structure"
I11-1004,N09-1028,0,0.247032,"05) is a formally syntax-based approach which can automatically extract hierarchical ordering rules from aligned string-string pairs without using additional parsers. These approaches have been proved to be both algorithmically appealing and empirically successful. However, most of current syntax-based SMT systems use IBM models (Brown et al., 1993) and hidden Markov model (HMM) (Vogel et al., 1996) to generate word alignments. These models have a penalty parameter associated with long distance jumps, and tend to misalign words which move far from the window sizes of their expected positions (Xu et al., 2009; Genzel, 2010). The third type tackles the word-order problem in pre-ordering ways. Through the usage of a sequence of pre-ordering rules, the word order of an original source sentence is (approximately) changed into the word order of the target sentence. Here, the pre-ordering rules can be manually or automatically extracted. For manual extraction of pre-ordering rules, linguistic background and expertise are required for predetermined language pairs, such as for GermanEnglish (Collins et al., 2005), Chinese-to-English (Wang et al., 2007), Japanese-to-English (KatzBrown and Collins, 2007), a"
I11-1004,J93-2003,0,\N,Missing
I11-1017,P06-1132,0,0.118427,"learned from a monolingual corpus of the language to be learned. Once we obtain a manuallycorrected corpus of language learners, it is possible to translate erroneous sentences into correct sentences using SMT. The use of SMT for spelling and grammar correction has the following three advantages. (1) It does not require expert knowledge. (2) It is straightforward to apply SMT tools to this task. (3) Error correction using SMT can benefit from the improvement of SMT method. Related work on error correction using phrasebased SMT includes research on English and Japanese (Brockett et al., 2006; Suzuki and Toutanova, 2006). Brockett et al. (2006) proposed to correct mass noun errors using SMT and used 45,000 sentences as training sets randomly extracted from automatically created 346,000 sentences. Our work differs from them in that we (1) do not restrict ourselves to a specific error type such as mass noun; and (2) exploit a large-scale real world data set. Suzuki and Toutanova (2006) proposed a machine learning-based method to preError Correction Using SMT eˆ = arg max P(e |f ) = arg max P(e)P( f |e) 銭湯に行った。 いつ行ったかがある方がいい (1) where e represents target sentences and f represents source sentences. P(e) is the p"
I11-1017,P01-1008,0,0.0146404,"Missing"
I11-1017,C10-2157,0,0.049317,"apanese language learners around the world has increased more than 30-fold in the past three decades. The Japan Foundation reports that more than 3.65 million people in 133 countries and regions are studying Japanese in 2009 1 . However, there are only 50,000 Japanese language teachers overseas, and thus it is in high demand to find good instructors for writers of Japanese as a Second Language (JSL). Recently, natural language processing research has begun to pay attention to second language learning (Rozovskaya and Roth, 2011; Park and Levy, 2011; Liu et al., 2011; Oyama and Matsumoto, 2010; Xue and Hwa, 2010). However, most previous research for second language learning deals with restricted types of learners’ errors. For example, research for JSL learners’ 1 http://www.jpf.go.jp/e/japanese/ survey/result/index.html 147 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 147–155, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP feedbacks from other users of the native language. However, they are not able to write about arbitrary topics. Third, Lang-8 is a “Multi-lingual language learning and language exchange Social Networking Service” 4 , which has"
I11-1017,P06-1032,0,0.745415,"at the character-wise model outperforms the word-wise model. 1 Yuji Matsumoto NAIST, Japan 何で日本語はこんなに難しい な の？ (Why does Japanese are so difficult?) which has a grammatical error of inserting ‘な’ due to literal translation from Chinese. Park and Levy (2011) proposed an EM-based unsupervised approach to perform whole sentence grammar correction, but the types of errors must be predetermined to learn the parameters for their noisy channel model. It requires expert knowledge of L2 teaching, which is often hard to obtain. One promising approach for correcting unrestricted errors of JSL learners is Brockett et al. (2006)’s automated error correction method using statistical machine translation (SMT). The advantage of their method is that it does not require expert knowledge. Instead, it learns a correction model from sentence-aligned corrected learners’ corpora. However, it is not easy to acquire largescale learners’ corpora. In fact, Brockett et al. (2006) used regular expressions to automatically create erroneous corpora from native corpora. To solve the knowledge acquisition bottleneck, we propose a method of mining revision logs to create a large-scale learners’ corpus. The corpus is compiled from error r"
I11-1017,J93-2003,0,0.0463045,"is that annotators may not correct all the errors in a sentence. Table 5 shows an example of JSL learner’s sentence for confusing case markers of “が” (NOM) and “は” (TOP). In this example, “は” and “が” should be corrected to “が” and “は”, respectively. However, the annotator left the second case markers “は” unchanged. Because the number of these cases seems low, we regard it as safe to ignore this issue for creating the corpus. 3 In this study, we attempt to solve the problem of JSL learners’ error correction using the SMT technique. The well-known SMT formulation using the noisy channel model (Brown et al., 1993) is: e e (I went to a public bath. It is better to say when you went.) from sentence-aligned parallel corpus while LM is learned from target language corpus. To adapt SMT to error correction, f can be regarded as the sentences written by Japanese learners, whereas e represents the manually-corrected Japanese sentences. TM can be learned from the sentence-aligned learners’ corpus. LM can be learned from a monolingual corpus of the language to be learned. Once we obtain a manuallycorrected corpus of language learners, it is possible to translate erroneous sentences into correct sentences using S"
I11-1017,P03-1021,0,0.0408272,"the effect of corpus size; (3) the difference of L1 model. We used Moses 2.1 14 as a decoder and GIZA++ 1.0.5 15 as an alignment tool. We used Japanese morphological analyzer MeCab 0.97 with UniDic 1.3.12 16 for word segmentation. We created a word-wise model as baseline. Hereafter, we refer to this as W and also constructed model with entries from UniDic for better alignment, denoted as W+Dic. We used word trigram as LM for W and W+Dic. We built two character-wise models: character 3-gram and 5-gram represented as C3 and C5, respectively. We also conducted minimum error rate training (MERT) (Och, 2003) in all experiments 17 . でもじょずじゃりません The correct counterpart would be: でもじょう ずじゃあ りません (But I am not good at it.) The corrected sentence has “う” and “あ” inserted 12 . These sentences written by a learner and corrected by a native speaker are tokenized as follows by MeCab 13 , which is one of the most popular Japanese Morphological Analyzer: でも じ ょずじゃりません ( but (fragment) (garbled word) ) でも じょうず じゃ あり ません ( but good at be 4.1 Experimental Data not ) All the data was created from 849,894 Japanese sentences extracted from revision logs of Lang8 crawled in December 2010. To see the difference of"
I11-1017,P02-1040,0,0.0983684,"granularity of tokenization Table 6 illustrates the performance with different 18 The W+Dic 0.9083 0.9210 0.9146 0.8101 methods (Training Corpus: L1 = ALL; Test Corpus: L1 = English; TM: 0.3M sentences; LM: 1M sentence). The character-wise models outperform the word-wise model in both recall and precision. C5 achieved the best precision, F and BLEU, while C3 obtained the best recall. As evaluation metrics, we use automatic evaluation criteria. To be precise, we used recall (R) and precision (P) based on longest common subsequence (LCS) (Mori et al., 1999; Aho, 1990) and character-based BLEU (Papineni et al., 2002). Park and Levy (2011) adopted character-based BLEU for automatic assessment of ESL errors. We followed their use of BLEU in the error correction task of JSL learners. Since we perform minimum error rate training using BLEU we can directly compare each model’s performance. Recall and precision based on LCS are defined as follows: Recall = W 0.9043 0.9175 0.9109 0.8072 19 Note that LM was trained from the whole training corpus. We did not change L1 for LM. pronunciation of “わ” is the same as “は”. 153 Table 7: Comparison of the performance (recall, precision, F, BLEU) of error correction trained"
I11-1017,P11-1094,0,0.0783935,"ructors. We also demonstrate that the extracted learners’ corpus of Japanese as a second language can be used as training data for learners’ error correction using an SMT approach. We evaluate different granularities of tokenization to alleviate the problem of word segmentation errors caused by erroneous input from language learners. Experimental results show that the character-wise model outperforms the word-wise model. 1 Yuji Matsumoto NAIST, Japan 何で日本語はこんなに難しい な の？ (Why does Japanese are so difficult?) which has a grammatical error of inserting ‘な’ due to literal translation from Chinese. Park and Levy (2011) proposed an EM-based unsupervised approach to perform whole sentence grammar correction, but the types of errors must be predetermined to learn the parameters for their noisy channel model. It requires expert knowledge of L2 teaching, which is often hard to obtain. One promising approach for correcting unrestricted errors of JSL learners is Brockett et al. (2006)’s automated error correction method using statistical machine translation (SMT). The advantage of their method is that it does not require expert knowledge. Instead, it learns a correction model from sentence-aligned corrected learne"
I11-1017,P11-1093,0,\N,Missing
I11-1073,W08-0304,0,0.0816198,"faster parameter tuning algorithm would have a positive impact on research on all the components of the SMT system. Imagine a researcher designing a new pruning algorithm for decoding, a new word alignment model, or a new domain adaptation method. Any of these methods need to be evaluated in the context of a full SMT system, which requires parameter tuning. If we can reduce the parameter tuning time from 10 hours to 1 hour, this can greatly increase the pace of innovation. Thus our motivation is orthogonal to recent research on improving MERT, such as efforts to escape local maxima problems (Cer et al., 2008; 649 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 649–657, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP proposed by (Och, 2003). There are several variations for updating weights during the iterative tuning process in MERT. The most commonly used algorithm for MERT is usually called Koehncoordinate descent (KCD), which is used in the MERT utility packaged in the popular Moses statistical machine translation system (Koehn et al., 2007). Another choice is Powell’s method that was advocated when MERT was first introduced for SMT (Och, 2"
I11-1073,W09-0439,0,0.549104,"sentences in the tuning dataset with the current weights to generate N -best lists of translations. Then, the method employs the inner loop procedure to optimize the weights based on those N -best lists instead of decoding the tuning dataset. After obtaining the optimal weights from the inner loop, we repeat the outer loop to generate a new N -best list. Figure 1 shows the system outline, which is described in detail elsewhere (Bertoldi et al., 2009). We note here that the method proposed in this paper essentially involves the replacement of the inner loop algorithm of the conventional MERT. Foster and Kuhn, 2009; Moore and Quirk, 2008), or incorporate lattices (Macherey et al., 2008). 2 Parameter Tuning for SMT Systems Most recently developed SMT systems consist of several model components, such as translation models, language models, and reordering models. To combine evidence obtained from these different components, we often define a discriminative (log-)linear model. Suppose the SMT system has D components. The log probabilities of the components are usually treated as features in the discriminative model. We denote the d-th feature, or log probability of the d-th component given a source sentence"
I11-1073,P07-2045,0,0.0612959,"on is orthogonal to recent research on improving MERT, such as efforts to escape local maxima problems (Cer et al., 2008; 649 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 649–657, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP proposed by (Och, 2003). There are several variations for updating weights during the iterative tuning process in MERT. The most commonly used algorithm for MERT is usually called Koehncoordinate descent (KCD), which is used in the MERT utility packaged in the popular Moses statistical machine translation system (Koehn et al., 2007). Another choice is Powell’s method that was advocated when MERT was first introduced for SMT (Och, 2003). Since KCD tends to be marginally more effective at optimizing the MERT objective, and is much simpler to implement than Powell’s method, this paper focuses only on KCD. KCD is a variant of a coordinate ascent (or descent) algorithm. At each iteration, it moves along the coordinate, which allows for the greatest progress of the maximization. The routine performs a trial line maximization along each coordinate to determine which one should be selected. It then updates the weight vector with"
I11-1073,D08-1076,0,0.0441913,"-best lists of translations. Then, the method employs the inner loop procedure to optimize the weights based on those N -best lists instead of decoding the tuning dataset. After obtaining the optimal weights from the inner loop, we repeat the outer loop to generate a new N -best list. Figure 1 shows the system outline, which is described in detail elsewhere (Bertoldi et al., 2009). We note here that the method proposed in this paper essentially involves the replacement of the inner loop algorithm of the conventional MERT. Foster and Kuhn, 2009; Moore and Quirk, 2008), or incorporate lattices (Macherey et al., 2008). 2 Parameter Tuning for SMT Systems Most recently developed SMT systems consist of several model components, such as translation models, language models, and reordering models. To combine evidence obtained from these different components, we often define a discriminative (log-)linear model. Suppose the SMT system has D components. The log probabilities of the components are usually treated as features in the discriminative model. We denote the d-th feature, or log probability of the d-th component given a source sentence f and its translation e, as φd (e, f ). We also denote the d-th weight a"
I11-1073,C08-1074,0,0.0654341,"g dataset with the current weights to generate N -best lists of translations. Then, the method employs the inner loop procedure to optimize the weights based on those N -best lists instead of decoding the tuning dataset. After obtaining the optimal weights from the inner loop, we repeat the outer loop to generate a new N -best list. Figure 1 shows the system outline, which is described in detail elsewhere (Bertoldi et al., 2009). We note here that the method proposed in this paper essentially involves the replacement of the inner loop algorithm of the conventional MERT. Foster and Kuhn, 2009; Moore and Quirk, 2008), or incorporate lattices (Macherey et al., 2008). 2 Parameter Tuning for SMT Systems Most recently developed SMT systems consist of several model components, such as translation models, language models, and reordering models. To combine evidence obtained from these different components, we often define a discriminative (log-)linear model. Suppose the SMT system has D components. The log probabilities of the components are usually treated as features in the discriminative model. We denote the d-th feature, or log probability of the d-th component given a source sentence f and its translation e"
I11-1073,P03-1021,0,0.718943,"rd alignment model, or a new domain adaptation method. Any of these methods need to be evaluated in the context of a full SMT system, which requires parameter tuning. If we can reduce the parameter tuning time from 10 hours to 1 hour, this can greatly increase the pace of innovation. Thus our motivation is orthogonal to recent research on improving MERT, such as efforts to escape local maxima problems (Cer et al., 2008; 649 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 649–657, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP proposed by (Och, 2003). There are several variations for updating weights during the iterative tuning process in MERT. The most commonly used algorithm for MERT is usually called Koehncoordinate descent (KCD), which is used in the MERT utility packaged in the popular Moses statistical machine translation system (Koehn et al., 2007). Another choice is Powell’s method that was advocated when MERT was first introduced for SMT (Och, 2003). Since KCD tends to be marginally more effective at optimizing the MERT objective, and is much simpler to implement than Powell’s method, this paper focuses only on KCD. KCD is a vari"
I11-1073,P02-1040,0,0.0919199,"Missing"
I11-1153,N09-2019,0,0.0494692,"Missing"
I11-1153,P09-1064,0,0.033943,"4 is the N-best approximation commonly used in practice: N (f ) contains the set of hypotheses in the N-best list, and the argmin and sum is only performed within this finite set. There are two difficulties with Eq. 4: 1. The N-best approximation is much smaller than the true space of all English hypotheses in the argmin and sum of Eq. 3. The approximation in the argmin causes search errors, while the approximation in the sum introduces bias. This problem can be somewhat mitigated by increasing the N-best list size or extending this space using lattices and hypergraphs (Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009). We do not address this issue here. 2. The posterior probability p(e|f ) in Eq.3 and Eq. 4 refers to the true posterior probability arising from Ep(e,f ) [·] in the derivation of Eq. 2. In practice, this can only be estimated from the MT decoder’s model scores: P (exp i λi hi (e, f ))α P p(e|f ) ≈ P ′ α e′ ∈N (f ) (exp i λi hi (e , f )) (5) where hi (e, f ) are features, λi are feature weights, and α is a scaling factor that determines the flatness of the posterior distribution (Ehling et al., 2007). It is important to emphasize that we are assuming that the decoder’s sco"
I11-1153,C10-1036,0,0.0320563,"Missing"
I11-1153,P07-2026,0,0.0184181,"extending this space using lattices and hypergraphs (Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009). We do not address this issue here. 2. The posterior probability p(e|f ) in Eq.3 and Eq. 4 refers to the true posterior probability arising from Ep(e,f ) [·] in the derivation of Eq. 2. In practice, this can only be estimated from the MT decoder’s model scores: P (exp i λi hi (e, f ))α P p(e|f ) ≈ P ′ α e′ ∈N (f ) (exp i λi hi (e , f )) (5) where hi (e, f ) are features, λi are feature weights, and α is a scaling factor that determines the flatness of the posterior distribution (Ehling et al., 2007). It is important to emphasize that we are assuming that the decoder’s score is an accurate surrogate for the true posterior distribution p(e|f ). The second difficulty poses a particular problem for system combination. Although the assumption in Eq. 5 is reasonable for single-system MT, it becomes P unclear how to compare the model scores i λi hi (e, f ) in a multi-system setting. To illustrate, consider two MT systems, their 2-best lists, and corresponding model scores: • System A: e1 , score=7; e2 , score=3; • System B: e3 , score=90; e4 , score=10; It is unclear what is the ranking of post"
I11-1153,E06-1005,0,0.124381,"Missing"
I11-1153,P08-1023,0,0.0352485,"satisfy the relations (Eq. 10) in its constraints while allowing for some slack ξ, whose amount depends on hyperparameter c. 1358 4 Experiments We experiment with the NTCIR-9 (2011) Englishto-Japanese Patent Translation task1 . This includes 3 million sentences for training individual MT systems; the official dev set is split into 1000 sentences for MERT of individual systems, 500 for system combination optimization (MBR, GMBR), and 500 for final evaluation. We combine three systems: • Phrase-based Moses with lexical reordering, distortion=6 (Koehn and others, 2007) • Forest-to-string system (Mi et al., 2008) • Weighted finite-state Transducer (WFST) (Zhou et al., 2006) with rule-based reordering as preprocessing (Isozaki et al., 2010b). Each system generates a 100-best list, so our system combination task involves hypothesis selection out of 300 hypotheses. As evaluation measure, we focus on BLEU, Normalized Kendall’s Tau (NKT), a metric that has been shown to correlate well with humans on this language pair (Isozaki et al., 2010a)2 , and a combination thereof. The loss function used for MBR is therefore the sum of BLEU and NKT. For GMBR, the sub-components of this loss function are derived from"
I11-1153,N07-1029,0,0.0560152,"Missing"
I11-1153,D08-1065,0,0.0343379,"BR) decision rule. Eq. 4 is the N-best approximation commonly used in practice: N (f ) contains the set of hypotheses in the N-best list, and the argmin and sum is only performed within this finite set. There are two difficulties with Eq. 4: 1. The N-best approximation is much smaller than the true space of all English hypotheses in the argmin and sum of Eq. 3. The approximation in the argmin causes search errors, while the approximation in the sum introduces bias. This problem can be somewhat mitigated by increasing the N-best list size or extending this space using lattices and hypergraphs (Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009). We do not address this issue here. 2. The posterior probability p(e|f ) in Eq.3 and Eq. 4 refers to the true posterior probability arising from Ep(e,f ) [·] in the derivation of Eq. 2. In practice, this can only be estimated from the MT decoder’s model scores: P (exp i λi hi (e, f ))α P p(e|f ) ≈ P ′ α e′ ∈N (f ) (exp i λi hi (e , f )) (5) where hi (e, f ) are features, λi are feature weights, and α is a scaling factor that determines the flatness of the posterior distribution (Ehling et al., 2007). It is important to emphasize that we are assuming t"
I11-1153,D08-1011,0,0.0385247,"k e Lk (e k=1 θk Ck (e ), where P|e) =′ Ck (e′ ) = L (e |e) represents the come k ′ bined loss for e . So we first compute Ck (·) for all hypotheses, for an O(|N (f )|2 ) run-time. To find the GMBR P decision then requires a ′ search arg mine′ ∈N (f ) K k=1 θk Ck (e ). So in test, GMBR is on the same order as conventional MBR. To tune θ, we first extract all pairs of hypotheses where a difference exists in the true loss, then optimize θ in a formulation similar to RankSVM (Joachims, 2006). The pair-wise nature of Eq. 10 makes the problem amenable to solutions in “learning to rank” literature (He et al., 2008a). The pseudocode is shown in Algorithm 1. The RankSVM (line 8) tries to satisfy the relations (Eq. 10) in its constraints while allowing for some slack ξ, whose amount depends on hyperparameter c. 1358 4 Experiments We experiment with the NTCIR-9 (2011) Englishto-Japanese Patent Translation task1 . This includes 3 million sentences for training individual MT systems; the official dev set is split into 1000 sentences for MERT of individual systems, 500 for system combination optimization (MBR, GMBR), and 500 for final evaluation. We combine three systems: • Phrase-based Moses with lexical reo"
I11-1153,D10-1092,1,0.905756,"Missing"
I11-1153,W10-1736,1,0.870667,"Missing"
I11-1153,P07-2045,0,0.00573936,"Missing"
I11-1153,N04-1022,0,0.0385613,"e loss function in MBR and allows it to be optimized in the given hypothesis space of multiple systems. This extension better approximates the true Bayes Risk decision rule and empirically improves over MBR, even in cases where the combined systems are of mixed quality. 1 Introduction Minimum Bayes Risk (MBR) is a theoreticallyelegant decision rule that has been used for singlesystem decoding and system combination in machine translation (MT). MBR arose in Bayes decision theory (Duda et al., 2000) and has since been applied to speech recognition (Goel and Byrne, 2000) and machine translation (Kumar and Byrne, 2004). The idea is to choose hypotheses that minimize Bayes Risk as oppose to those that maximize posterior probability. This enables the use of taskspecific loss functions (e.g BLEU). However, the definition of Bayes Risk depends critically on the posterior probability of hypotheses. In single-system decoding, one could approximate this probability using model scores. However, for system combination, the various systems 2 The Difficulty with MBR Consider the task of translation from a French sentence (f ) to an English sentence (e). Our goal is to find a decision rule δ(f ) → e′ , which takes f as"
I11-1153,P09-1019,0,0.0167692,"oximation commonly used in practice: N (f ) contains the set of hypotheses in the N-best list, and the argmin and sum is only performed within this finite set. There are two difficulties with Eq. 4: 1. The N-best approximation is much smaller than the true space of all English hypotheses in the argmin and sum of Eq. 3. The approximation in the argmin causes search errors, while the approximation in the sum introduces bias. This problem can be somewhat mitigated by increasing the N-best list size or extending this space using lattices and hypergraphs (Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009). We do not address this issue here. 2. The posterior probability p(e|f ) in Eq.3 and Eq. 4 refers to the true posterior probability arising from Ep(e,f ) [·] in the derivation of Eq. 2. In practice, this can only be estimated from the MT decoder’s model scores: P (exp i λi hi (e, f ))α P p(e|f ) ≈ P ′ α e′ ∈N (f ) (exp i λi hi (e , f )) (5) where hi (e, f ) are features, λi are feature weights, and α is a scaling factor that determines the flatness of the posterior distribution (Ehling et al., 2007). It is important to emphasize that we are assuming that the decoder’s score is an accurate sur"
I13-1147,P05-1066,0,0.15332,"2 Related Work We propose a new rule-based pre-ordering method for Japanese-to-English statistical machine translation that employs heuristic rules in two-stages. This two-stage framework contributes to experimental results that our method outperforms conventional rule-based methods in BLEU and RIBES. 1 Introduction Reordering is an important strategy in statistical machine translation (SMT) to achieve high quality translation. While many reordering methods often fail in long distance reordering due to computational complexity, a promising technology called pre-ordering (Xia and McCord, 2004; Collins et al., 2005) has been successful for distant English-to-Japanese translation (Isozaki et al., 2010b). However, this strong effectiveness has not been shown for Japanese-to-English translation. In this paper, we propose a novel rule-based pre-ordering method for the Japanese-to-English translation. The method utilizes simple heuristic rules in two-stages1 : the inter-chunk and intrachunk levels. Thus the method can achieve more accurate reorderings in Japanese. The translation experiments in patent domain showed that our method outperformed conventional rule-based methods, especially on the word reordering"
I13-1147,W08-0509,0,0.0229865,"million sentence pairs for training, 500 sentence pairs for development, and 2,000 sentence pairs for testing. The Japanese sentences are tokenized by MeCab 0.99445 . In addition, we employed two parser configurations for Japanese parsing: (1) the KNP configuration used KNP 4.016 (Sasano and Kurohashi, 2011) for both dependency and PAS analyzer; (2) the CaboCha+SynCha configuration used CaboCha 0.657 (Kudo and Matsumoto, 2002) for dependency analysis and SynCha 0.38 (Iida and Poesio, 2011) for PAS analysis. For the common SMT system, we used SRILM 1.7.0 (Stolcke et al., 2011), MGIZA++ 0.7.3 (Gao and Vogel, 2008), Moses 0.91 (Koehn et al., 2007)9 , and two popular evaluation metrics for Japanese-to-English: BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010a). For the pre-ordering methods, we implemented rule-based methods proposed by (Komachi et al., 2006) and (Katz-Brown and Collins, 2008). In addition, an implementation10 of statistical method proposed by Neubig et al. (2012) are used11 . We did not use any pre-ordering in the baseline. 4.2 Experimental Results Table 1 shows the experimental results for Japanese-to-English patent document translations that compare the following pre-orderi"
I13-1147,P11-1081,0,0.0681856,"used the NTCIR9 PatentMT Test Collection Japanese-to-English Machine Translation Data3 package that contains approximately 3.2 million sentence pairs for training, 500 sentence pairs for development, and 2,000 sentence pairs for testing. The Japanese sentences are tokenized by MeCab 0.99445 . In addition, we employed two parser configurations for Japanese parsing: (1) the KNP configuration used KNP 4.016 (Sasano and Kurohashi, 2011) for both dependency and PAS analyzer; (2) the CaboCha+SynCha configuration used CaboCha 0.657 (Kudo and Matsumoto, 2002) for dependency analysis and SynCha 0.38 (Iida and Poesio, 2011) for PAS analysis. For the common SMT system, we used SRILM 1.7.0 (Stolcke et al., 2011), MGIZA++ 0.7.3 (Gao and Vogel, 2008), Moses 0.91 (Koehn et al., 2007)9 , and two popular evaluation metrics for Japanese-to-English: BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010a). For the pre-ordering methods, we implemented rule-based methods proposed by (Komachi et al., 2006) and (Katz-Brown and Collins, 2008). In addition, an implementation10 of statistical method proposed by Neubig et al. (2012) are used11 . We did not use any pre-ordering in the baseline. 4.2 Experimental Results Tabl"
I13-1147,D10-1092,1,0.935952,"Missing"
I13-1147,W10-1736,1,0.956068,"statistical machine translation that employs heuristic rules in two-stages. This two-stage framework contributes to experimental results that our method outperforms conventional rule-based methods in BLEU and RIBES. 1 Introduction Reordering is an important strategy in statistical machine translation (SMT) to achieve high quality translation. While many reordering methods often fail in long distance reordering due to computational complexity, a promising technology called pre-ordering (Xia and McCord, 2004; Collins et al., 2005) has been successful for distant English-to-Japanese translation (Isozaki et al., 2010b). However, this strong effectiveness has not been shown for Japanese-to-English translation. In this paper, we propose a novel rule-based pre-ordering method for the Japanese-to-English translation. The method utilizes simple heuristic rules in two-stages1 : the inter-chunk and intrachunk levels. Thus the method can achieve more accurate reorderings in Japanese. The translation experiments in patent domain showed that our method outperformed conventional rule-based methods, especially on the word reorderings. Our claims in this paper are summarized as follows: 1. The inter-chunk pre-ordering"
I13-1147,P03-1021,0,0.0888984,"en-PatentMT.html 4 http://mecab.googlecode.com/svn/ trunk/mecab/doc/index.html 5 Since (unlike English) the Japanese language does not utilize spaces to delineate word boundaries, MeCab was used to perform the required Japanese tokenization. 6 http://nlp.ist.i.kyoto-u.ac.jp/EN/ index.php?KNP 7 http://code.google.com/p/cabocha/ 8 http://www.cl.cs.titech.ac.jp/~ryu-i/ syncha/ Baseline Proposed BLEU 15.03 16.12 RIBES 62.71 69.30 Table 4: Results within a News Domain. 9 the following configurations are used in the system: 6gram for language modeling, msd-bidirectional-fe for reordering, and MERT (Och, 2003) for tuning. After reviewing our preliminary findings, distortion limits were set to 20 for the baseline and (Komachi et al., 2006), and 10 for others. 10 http://www.phontron.com/lader/ 11 Only 10,000 sampled lines were used for training due to its computational complexity: During the training process, it consumed 120 GB of memory space for almost entire month. 1064 improvement in RIBES by adding Rule 2 to Rule 1-2 and Rule 1-3, even thought this combination yields better translations for native speakers. This phenomenon can be explained by the characteristic difference between BLEU and RIBES."
I13-1147,P02-1040,0,0.094191,"e sentences are tokenized by MeCab 0.99445 . In addition, we employed two parser configurations for Japanese parsing: (1) the KNP configuration used KNP 4.016 (Sasano and Kurohashi, 2011) for both dependency and PAS analyzer; (2) the CaboCha+SynCha configuration used CaboCha 0.657 (Kudo and Matsumoto, 2002) for dependency analysis and SynCha 0.38 (Iida and Poesio, 2011) for PAS analysis. For the common SMT system, we used SRILM 1.7.0 (Stolcke et al., 2011), MGIZA++ 0.7.3 (Gao and Vogel, 2008), Moses 0.91 (Koehn et al., 2007)9 , and two popular evaluation metrics for Japanese-to-English: BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010a). For the pre-ordering methods, we implemented rule-based methods proposed by (Komachi et al., 2006) and (Katz-Brown and Collins, 2008). In addition, an implementation10 of statistical method proposed by Neubig et al. (2012) are used11 . We did not use any pre-ordering in the baseline. 4.2 Experimental Results Table 1 shows the experimental results for Japanese-to-English patent document translations that compare the following pre-ordering methods: the baseline (no pre-ordering), Komachi et al. (2006), Katz-Brown and Collins (2008),Neubig et al. (2012), our pr"
I13-1147,I11-1085,0,0.0287568,"ents 4.1 Experimental Setup In order to compare pre-ordering methods, we conducted Japanese-to-English translation experiments on a fixed data set and SMT system. For the common data set, we used the NTCIR9 PatentMT Test Collection Japanese-to-English Machine Translation Data3 package that contains approximately 3.2 million sentence pairs for training, 500 sentence pairs for development, and 2,000 sentence pairs for testing. The Japanese sentences are tokenized by MeCab 0.99445 . In addition, we employed two parser configurations for Japanese parsing: (1) the KNP configuration used KNP 4.016 (Sasano and Kurohashi, 2011) for both dependency and PAS analyzer; (2) the CaboCha+SynCha configuration used CaboCha 0.657 (Kudo and Matsumoto, 2002) for dependency analysis and SynCha 0.38 (Iida and Poesio, 2011) for PAS analysis. For the common SMT system, we used SRILM 1.7.0 (Stolcke et al., 2011), MGIZA++ 0.7.3 (Gao and Vogel, 2008), Moses 0.91 (Koehn et al., 2007)9 , and two popular evaluation metrics for Japanese-to-English: BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010a). For the pre-ordering methods, we implemented rule-based methods proposed by (Komachi et al., 2006) and (Katz-Brown and Collins, 2"
I13-1147,P03-1010,0,0.132275,"Missing"
I13-1147,C04-1073,0,0.317275,"ab.ntt.co.jp Abstract 2 Related Work We propose a new rule-based pre-ordering method for Japanese-to-English statistical machine translation that employs heuristic rules in two-stages. This two-stage framework contributes to experimental results that our method outperforms conventional rule-based methods in BLEU and RIBES. 1 Introduction Reordering is an important strategy in statistical machine translation (SMT) to achieve high quality translation. While many reordering methods often fail in long distance reordering due to computational complexity, a promising technology called pre-ordering (Xia and McCord, 2004; Collins et al., 2005) has been successful for distant English-to-Japanese translation (Isozaki et al., 2010b). However, this strong effectiveness has not been shown for Japanese-to-English translation. In this paper, we propose a novel rule-based pre-ordering method for the Japanese-to-English translation. The method utilizes simple heuristic rules in two-stages1 : the inter-chunk and intrachunk levels. Thus the method can achieve more accurate reorderings in Japanese. The translation experiments in patent domain showed that our method outperformed conventional rule-based methods, especially"
I13-1147,P07-2045,0,0.00751715,"ng, 500 sentence pairs for development, and 2,000 sentence pairs for testing. The Japanese sentences are tokenized by MeCab 0.99445 . In addition, we employed two parser configurations for Japanese parsing: (1) the KNP configuration used KNP 4.016 (Sasano and Kurohashi, 2011) for both dependency and PAS analyzer; (2) the CaboCha+SynCha configuration used CaboCha 0.657 (Kudo and Matsumoto, 2002) for dependency analysis and SynCha 0.38 (Iida and Poesio, 2011) for PAS analysis. For the common SMT system, we used SRILM 1.7.0 (Stolcke et al., 2011), MGIZA++ 0.7.3 (Gao and Vogel, 2008), Moses 0.91 (Koehn et al., 2007)9 , and two popular evaluation metrics for Japanese-to-English: BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010a). For the pre-ordering methods, we implemented rule-based methods proposed by (Komachi et al., 2006) and (Katz-Brown and Collins, 2008). In addition, an implementation10 of statistical method proposed by Neubig et al. (2012) are used11 . We did not use any pre-ordering in the baseline. 4.2 Experimental Results Table 1 shows the experimental results for Japanese-to-English patent document translations that compare the following pre-ordering methods: the baseline (no pre-"
I13-1147,2006.iwslt-evaluation.11,1,0.39014,"ng that relies on PAS analysis contributes to improvements in translation quality. 2. The intra-chunk pre-ordering which converts postpositional phrases into prepositional phrases further improves translation quality. 3. Thus, our two-stage framework is more effective than other pre-ordering methods. Japanese-to-English is challenging because the grammatical forms of the two languages are totally dissimilar. For instance, English is a head-initial language, and utilizes subject-verb-object (SVO) word orders, while Japanese is a pure head-final language, and utilizes subject-object-verb (SOV). Komachi et al. (2006) proposed a rule-based pre-ordering method to convert SOV into SVO via a PAS analyzer. This method pre-orders interchunk level word orders in a single-stage, via the PAS analyzer which produced dependency trees and tagged each S, O, and V label. Then SOV sequences are converted into SVO. However, since the non-labeled words are left untouched, the effectiveness of this method is limited to simple SOV labeled matrix sentences without multiple clauses. Katz-Brown and Collins (2008) proposed a twostage rule-based pre-ordering method. In the first stage, SOV sequences are converted into SVO via th"
I13-1147,W02-2016,0,0.24869,"nts on a fixed data set and SMT system. For the common data set, we used the NTCIR9 PatentMT Test Collection Japanese-to-English Machine Translation Data3 package that contains approximately 3.2 million sentence pairs for training, 500 sentence pairs for development, and 2,000 sentence pairs for testing. The Japanese sentences are tokenized by MeCab 0.99445 . In addition, we employed two parser configurations for Japanese parsing: (1) the KNP configuration used KNP 4.016 (Sasano and Kurohashi, 2011) for both dependency and PAS analyzer; (2) the CaboCha+SynCha configuration used CaboCha 0.657 (Kudo and Matsumoto, 2002) for dependency analysis and SynCha 0.38 (Iida and Poesio, 2011) for PAS analysis. For the common SMT system, we used SRILM 1.7.0 (Stolcke et al., 2011), MGIZA++ 0.7.3 (Gao and Vogel, 2008), Moses 0.91 (Koehn et al., 2007)9 , and two popular evaluation metrics for Japanese-to-English: BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010a). For the pre-ordering methods, we implemented rule-based methods proposed by (Komachi et al., 2006) and (Katz-Brown and Collins, 2008). In addition, an implementation10 of statistical method proposed by Neubig et al. (2012) are used11 . We did not use"
I13-1147,D12-1077,0,0.131501,"is method pre-orders interchunk level word orders in a single-stage, via the PAS analyzer which produced dependency trees and tagged each S, O, and V label. Then SOV sequences are converted into SVO. However, since the non-labeled words are left untouched, the effectiveness of this method is limited to simple SOV labeled matrix sentences without multiple clauses. Katz-Brown and Collins (2008) proposed a twostage rule-based pre-ordering method. In the first stage, SOV sequences are converted into SVO via the dependency analyzer. In the second stage, each chunk word order is naively reversed2 . Neubig et al. (2012) proposed a statistical model that was capable of learning how to pre-order word sequences from human annotated or automatically generated alignment data. However, this method has very large computational complexity to model long distance reordering. 3 Two-stage Pre-ordering Method Here, we describe a new pre-ordering method which employs heuristic rules in two-stages. In the first stage, we reorganize and extend the rules described in (Komachi et al., 2006; Katz-Brown and Collins, 2008). In the second stage, we propose a new rule to consider chunk internal word orders. More precisely, we appl"
I17-2002,2016.amta-researchers.10,0,0.0160326,"end of sentence symbols, respectively. Introduction The sequence-to-sequence (Seq2Seq) model has been successfully used in natural language generation tasks such as machine translation (MT) (Bahdanau et al., 2014) and text summarization (Rush et al., 2015). In the Seq2Seq model, attention, which encodes an input sentence by generating an alignment between output and input words, plays an important role. Without the attention mechanism, the performance of the Seq2Seq model degrades significantly (Bahdanau et al., 2014). To improve the alignment quality, Mi et al. (2016), Liu et al. (2016), and Chen et al. (2016) proposed a method that learns attention with the given alignments in a supervised manner, which is called supervised attention. By utilizing supervised attention, the translation quality of MT is improved. The Seq2Seq model can also be applied to other NLP tasks. We can regard parsing as a translation task from a sentence to an S-expression, and Vinyals et al. (2015) proposed a constituent parsing method based on the Seq2Seq model. Their method achieved the state-of-the-art performance. In their method, based on the alignment between a nonterminal and input words, the attention mechanism has"
I17-2002,P97-1003,0,0.247328,"nd Huang, 2016). We used dropout layers (Srivastava et al., 2014) to • Right word: On the contrary, the syntactic head of a simple English noun phrase is often at the end of the span. The alignment example in Fig. 3b is produced by this method, where an output token is linked to the rightmost word of the span. • Span word: Here, we unify the above two methods. All output tokens are linked to their leftmost word, except the ending bracket tokens, which are linked to their rightmost word. Figure 3c shows an alignment example produced by this method. • Lexical head: Lexicalization (Eisner, 1996; Collins, 1997), which annotates grammar nonterminals with their head words, is useful for resolving the syntactic ambiguities involved by such linguistic phenomena as co1 For head annotations, we used ptbconv 3.0 tool (Yamada and Matsumoto, 2003), which is available from http:// www.jaist.ac.jp/h-yamada/. 9 Setting P WSJ Section 22 R F1 AER P WSJ Section 23 R F1 AER Seq2Seq Seq2Seq+random Seq2Seq+first Seq2Seq+last 88.1 67.1 70.3 66.7 88.0 66.3 69.7 66.1 88.1 66.7 70.0 66.4 – 96.3 0.0 0.0 88.3 66.5 69.6 66.1 87.6 65.5 68.7 64.8 88.0 66.0 69.2 65.4 – 96.3 0.0 0.0 Seq2Seq+head Seq2Seq+left Seq2Seq+right Seq2S"
I17-2002,D16-1001,0,0.0389053,"data into three parts: The Wall Street Journal (WSJ) sections 02-21 for training, section 22 for development and section 23 for testing. In our models, the dimensions of the input word embeddings, the fed label embeddings, the hidden layers, and an attention vector were respectively set to 150, 30, 200, and 200. The LSTM depth was set to 3. Label set Lcon had a size of 61. The input vocabulary size of PTB was set to 42393. Supervised attention rate λ was set to 1.0. To use entire words as a vocabulary, we integrated word dropout (Iyyer et al., 2015) into our models with smoothing rate 0.8375 (Cross and Huang, 2016). We used dropout layers (Srivastava et al., 2014) to • Right word: On the contrary, the syntactic head of a simple English noun phrase is often at the end of the span. The alignment example in Fig. 3b is produced by this method, where an output token is linked to the rightmost word of the span. • Span word: Here, we unify the above two methods. All output tokens are linked to their leftmost word, except the ending bracket tokens, which are linked to their rightmost word. Figure 3c shows an alignment example produced by this method. • Lexical head: Lexicalization (Eisner, 1996; Collins, 1997),"
I17-2002,P15-1030,0,0.0325395,"rd LSTM to encode previously predicted label yt−1 into hidden state st . For each time t, with a 2-layer feed-forward neural network r, encoder and decoder hidden lay→ ers h and − s t are used to calculate the attention weight: → exp(r(hi , − s t )) . αti = ∑n → exp(r(h ′ , − s )) i i′ =1 − t=1 −λ × logP (yt |yt−1 , ..., y0 , x) n ∑ m ∑ i=1 t=1 ait × logαti , to jointly learn the attention and output distributions. All our alignments are represented by oneto-many links between input words x and nonterminals y. 3 Design of our Alignments In the traditional parsing framework (Hall et al., 2014; Durrett and Klein, 2015), lexical features have been proven to be useful in improving parsing performance. Inspired by previous work, we enhance the attention mechanism utilizing the linguistically-motivated annotations between surface words and nonterminals by supervised attention. In this paper, we define four types of alignments for supervised attention. The first three methods use the monolexical properties of heads without incurring any inferential costs of lexicalized annotations. Although the last needs manually constructed annotation schemes, it can capture bilexical relationships along dependency arcs. The f"
I17-2002,C96-1058,0,0.0611496,".8375 (Cross and Huang, 2016). We used dropout layers (Srivastava et al., 2014) to • Right word: On the contrary, the syntactic head of a simple English noun phrase is often at the end of the span. The alignment example in Fig. 3b is produced by this method, where an output token is linked to the rightmost word of the span. • Span word: Here, we unify the above two methods. All output tokens are linked to their leftmost word, except the ending bracket tokens, which are linked to their rightmost word. Figure 3c shows an alignment example produced by this method. • Lexical head: Lexicalization (Eisner, 1996; Collins, 1997), which annotates grammar nonterminals with their head words, is useful for resolving the syntactic ambiguities involved by such linguistic phenomena as co1 For head annotations, we used ptbconv 3.0 tool (Yamada and Matsumoto, 2003), which is available from http:// www.jaist.ac.jp/h-yamada/. 9 Setting P WSJ Section 22 R F1 AER P WSJ Section 23 R F1 AER Seq2Seq Seq2Seq+random Seq2Seq+first Seq2Seq+last 88.1 67.1 70.3 66.7 88.0 66.3 69.7 66.1 88.1 66.7 70.0 66.4 – 96.3 0.0 0.0 88.3 66.5 69.6 66.1 87.6 65.5 68.7 64.8 88.0 66.0 69.2 65.4 – 96.3 0.0 0.0 Seq2Seq+head Seq2Seq+left Seq"
I17-2002,P14-1022,0,0.0132644,"layer stacked forward LSTM to encode previously predicted label yt−1 into hidden state st . For each time t, with a 2-layer feed-forward neural network r, encoder and decoder hidden lay→ ers h and − s t are used to calculate the attention weight: → exp(r(hi , − s t )) . αti = ∑n → exp(r(h ′ , − s )) i i′ =1 − t=1 −λ × logP (yt |yt−1 , ..., y0 , x) n ∑ m ∑ i=1 t=1 ait × logαti , to jointly learn the attention and output distributions. All our alignments are represented by oneto-many links between input words x and nonterminals y. 3 Design of our Alignments In the traditional parsing framework (Hall et al., 2014; Durrett and Klein, 2015), lexical features have been proven to be useful in improving parsing performance. Inspired by previous work, we enhance the attention mechanism utilizing the linguistically-motivated annotations between surface words and nonterminals by supervised attention. In this paper, we define four types of alignments for supervised attention. The first three methods use the monolexical properties of heads without incurring any inferential costs of lexicalized annotations. Although the last needs manually constructed annotation schemes, it can capture bilexical relationships al"
I17-2002,P15-1162,0,0.0359884,"Missing"
I17-2002,C16-1291,0,0.145898,"and &lt;/s&gt; are start and end of sentence symbols, respectively. Introduction The sequence-to-sequence (Seq2Seq) model has been successfully used in natural language generation tasks such as machine translation (MT) (Bahdanau et al., 2014) and text summarization (Rush et al., 2015). In the Seq2Seq model, attention, which encodes an input sentence by generating an alignment between output and input words, plays an important role. Without the attention mechanism, the performance of the Seq2Seq model degrades significantly (Bahdanau et al., 2014). To improve the alignment quality, Mi et al. (2016), Liu et al. (2016), and Chen et al. (2016) proposed a method that learns attention with the given alignments in a supervised manner, which is called supervised attention. By utilizing supervised attention, the translation quality of MT is improved. The Seq2Seq model can also be applied to other NLP tasks. We can regard parsing as a translation task from a sentence to an S-expression, and Vinyals et al. (2015) proposed a constituent parsing method based on the Seq2Seq model. Their method achieved the state-of-the-art performance. In their method, based on the alignment between a nonterminal and input words, the"
I17-2002,D15-1166,0,0.106565,"odel and five-model ensembles. We used the products of the output probabilities for the ensemble. • Last word: All output tokens were linked to the end of the sentence tokens in the input sentence. We evaluated the compared methods using bracketing Precision, Recall and F-measure. We used evalb2 as a parsing evaluation. We also evalAll models were written in C++ on Dynet (Neubig et al., 2017). We compared Seq2Seq models with and with2 10 http://nlp.cs.nyu.edu/evalb/ uated the learned attention using alignment error rate (AER) (Och and Ney, 2003) on their alignments. Following a previous work (Luong et al., 2015), attention evaluation was conducted on gold output. on the English Penn Treebank against the baseline methods. These results emphasize, the effectiveness of our alignments in parsing performances. Acknowledgement We thank the anonymous reviewers for their careful reading and useful comments. 4.2 Results Table 1 shows the results. All our lexical head, left word, right word and span word alignments improved bracket F-measure of baseline on every setting. From the +random, +first, and +last results, only supervised attention itself did not improve the parsing performances. Furthermore, each AER"
I17-2002,D16-1249,0,0.0316216,"utput tokens. &lt;s&gt; and &lt;/s&gt; are start and end of sentence symbols, respectively. Introduction The sequence-to-sequence (Seq2Seq) model has been successfully used in natural language generation tasks such as machine translation (MT) (Bahdanau et al., 2014) and text summarization (Rush et al., 2015). In the Seq2Seq model, attention, which encodes an input sentence by generating an alignment between output and input words, plays an important role. Without the attention mechanism, the performance of the Seq2Seq model degrades significantly (Bahdanau et al., 2014). To improve the alignment quality, Mi et al. (2016), Liu et al. (2016), and Chen et al. (2016) proposed a method that learns attention with the given alignments in a supervised manner, which is called supervised attention. By utilizing supervised attention, the translation quality of MT is improved. The Seq2Seq model can also be applied to other NLP tasks. We can regard parsing as a translation task from a sentence to an S-expression, and Vinyals et al. (2015) proposed a constituent parsing method based on the Seq2Seq model. Their method achieved the state-of-the-art performance. In their method, based on the alignment between a nonterminal an"
I17-2002,J03-1002,0,0.00594689,"size was set to ten. The decoding was performed on both a single model and five-model ensembles. We used the products of the output probabilities for the ensemble. • Last word: All output tokens were linked to the end of the sentence tokens in the input sentence. We evaluated the compared methods using bracketing Precision, Recall and F-measure. We used evalb2 as a parsing evaluation. We also evalAll models were written in C++ on Dynet (Neubig et al., 2017). We compared Seq2Seq models with and with2 10 http://nlp.cs.nyu.edu/evalb/ uated the learned attention using alignment error rate (AER) (Och and Ney, 2003) on their alignments. Following a previous work (Luong et al., 2015), attention evaluation was conducted on gold output. on the English Penn Treebank against the baseline methods. These results emphasize, the effectiveness of our alignments in parsing performances. Acknowledgement We thank the anonymous reviewers for their careful reading and useful comments. 4.2 Results Table 1 shows the results. All our lexical head, left word, right word and span word alignments improved bracket F-measure of baseline on every setting. From the +random, +first, and +last results, only supervised attention it"
I17-2002,D15-1044,0,0.0712984,"rpus showed that the bracketing F-measure was improved by supervised attention. 1 (NP )S )NP (VP XX XX XX the chef cooks )VP (NP )NP XX XX the soup Figure 1: S-expression format for Vinyals et al. (2015)’s Seq2seq constituency parser. The Seq2seq model employs “&lt;s&gt; (S (NP XX XX )NP (VP XX (NP XX XX )NP )VP )S &lt;/s&gt;” as output tokens. &lt;s&gt; and &lt;/s&gt; are start and end of sentence symbols, respectively. Introduction The sequence-to-sequence (Seq2Seq) model has been successfully used in natural language generation tasks such as machine translation (MT) (Bahdanau et al., 2014) and text summarization (Rush et al., 2015). In the Seq2Seq model, attention, which encodes an input sentence by generating an alignment between output and input words, plays an important role. Without the attention mechanism, the performance of the Seq2Seq model degrades significantly (Bahdanau et al., 2014). To improve the alignment quality, Mi et al. (2016), Liu et al. (2016), and Chen et al. (2016) proposed a method that learns attention with the given alignments in a supervised manner, which is called supervised attention. By utilizing supervised attention, the translation quality of MT is improved. The Seq2Seq model can also be a"
I17-2002,W03-3023,0,0.135784,"produced by this method, where an output token is linked to the rightmost word of the span. • Span word: Here, we unify the above two methods. All output tokens are linked to their leftmost word, except the ending bracket tokens, which are linked to their rightmost word. Figure 3c shows an alignment example produced by this method. • Lexical head: Lexicalization (Eisner, 1996; Collins, 1997), which annotates grammar nonterminals with their head words, is useful for resolving the syntactic ambiguities involved by such linguistic phenomena as co1 For head annotations, we used ptbconv 3.0 tool (Yamada and Matsumoto, 2003), which is available from http:// www.jaist.ac.jp/h-yamada/. 9 Setting P WSJ Section 22 R F1 AER P WSJ Section 23 R F1 AER Seq2Seq Seq2Seq+random Seq2Seq+first Seq2Seq+last 88.1 67.1 70.3 66.7 88.0 66.3 69.7 66.1 88.1 66.7 70.0 66.4 – 96.3 0.0 0.0 88.3 66.5 69.6 66.1 87.6 65.5 68.7 64.8 88.0 66.0 69.2 65.4 – 96.3 0.0 0.0 Seq2Seq+head Seq2Seq+left Seq2Seq+right Seq2Seq+span 89.2 89.6 89.2 89.3 88.9 89.4 88.9 89.1 89.1 89.5 89.0 89.2 6.9 1.8 4.7 1.6 89.2 89.4 89.5 89.2 88.1 88.7 88.6 88.4 88.6 89.0 89.1 88.8 6.9 1.7 4.7 1.6 Vinyals et al. (2015) w att† Vinyals et al. (2015) w/o att† Seq2Seq+beam"
I17-2008,D15-1199,0,0.115814,"Missing"
I17-2008,J93-2004,0,0.0604496,"Missing"
I17-2008,E17-2025,0,0.0493625,"IOG reduced the perplexity. In other words, IOG boosted the performance of the baseline models. We emphasize that IOG is not restricted to a neural architecture of a language model because it improved the RHN and LSTM performances. In addition to the comparison with the baselines, Table 2 and Table 3 contain the scores published in previous studies. Merity et al. (2017b) and Grave et al. (2017) proposed similar methods. Their methods, which are called “cache mechanism” (or ‘pointer’), keep multiple hidden states at past timesteps to select words from previous sequences. Inan et al. (2016) and Press and Wolf (2017) introduced a technique that shares word embeddings with the weight matrix of the final layer (represented as ‘WT’ in Table 2). Inan et al. (2016) also proposed using word embeddings to augment loss function (represented as ‘AL’ in Table 2). Zoph and Le (2017) adopted RNNs and reinforcement learning to automatically construct a novel RNN architecture. We expect that IOG will improve these models since it can be combined with any RNN language models. In fact, Table 2 and Table 3 6 In contrast to other comparisons, we used the following implementation by the authors: https://github.com/salesforc"
I17-2008,D15-1044,0,0.0689356,"anguage models. Hereafter, we denote a word sequence with length T , namely, w1 , ..., wT as w1:T for short. Formally, a typical RNN language model computes the joint probability of word sequence w1:T by the product of the conditional probabilities of each timestep t: A neural language model is a central technology of recently developed neural architectures in the natural language processing (NLP) field. For example, neural encoder-decoder models, which were successfully applied to various natural language generation tasks including machine translation (Sutskever et al., 2014), summarization (Rush et al., 2015), and dialogue (Wen et al., 2015), can be interpreted as conditional neural language models. Moreover, word embedding methods, such as Skip-gram (Mikolov et al., 2013) and vLBL (Mnih and Kavukcuoglu, 2013), are also originated from neural language models that aim to handle much larger vocabulary and data sizes. Thus, language modeling is a good benchmark task for investigating the general frameworks of neural methods in the NLP field. In this paper, we address improving the performance on the language modeling task. In particular, we focus on boosting the quality of existing Recurrent Neural N"
K15-2015,W05-0305,0,0.0675271,"ased argument extractors. One extracts both Arg1 and Arg2 from the same sentence (SS). The other extracts Arg1 and Arg2 from adjacent sentences respectively (PS). 3.1 Argument Position Identification 4.1 SS Cases In the argument identification step, following Ghosh et al. (2011), the identifier examines all adjacent sentence pairs within each paragraph. For each pair of sentences (Si , Si+1 ), we identify the existence of a discourse relation. To identify the existence of the relation (binary classification), we used SVM with the following features. 4.1.1 Subordinating Conjunctions We adopted Dinesh et al. (2005)’s tree subtraction method for subordinating conjunctions. This method takes a constituent parse tree as an input and detects argument spans as follows: (1) set a node variable x to the last word of the target connective, • First unigram, last unigram, and first trigram of Si and Si+1 . (2) set x to the parent node of x and repeat until x has label SBAR or S and set a node variable Arg2 to the node of x, • Si (or Si+1 ) contains modality words or not. 96 Con. Arg1 Arg2 Arg1&Arg2 Overall P .924 .658 .768 .566 .348 Dev R .857 .549 .640 .471 .290 F .889 .599 .698 .514 .316 P .918 .719 .587 .488 ."
K15-2015,P14-1003,0,0.0557319,"ural Language Learning: Shared Task, pages 95–99, c Beijing, China, July 26-31, 2015. 2014 Association for Computational Linguistics Type Context Parse Tree Features Cs , POSs , {Wordu }, {POSu } Path(Cs ,root), Parent(Cs ), depth(Cs ), RightSib(Cs ), LeftSib(Cs ) u = s − 5, . . . , s − 1, s + 1, . . . , s + 5 • Word pairs (wi , wi+1 ) ∈ Si × Si+1 • Brown cluster pairs feature defined in Rutherford and Xue (2014) • Sentence-to-sentence discourse dependency tree features including existence of dependency edges and rhetorical relation labels. Discourse dependency trees are defined in Li et al. (Li et al., 2014). Table 1: Features used in connective classifier discourse connective into “same sentence” (SS) or “previous sentence” (PS). SS indicates both Arg1 and Arg2 are located in the same sentence that contains the discourse connective. PS indicates Arg1 is located in the sentence previous to that containing both the discourse connective and Arg2. We utilized context features in Table 1 and the position of the connective Cs : start, middle, or end. We also trained the classifier by using SVM with second-order polynomial kernel. If the identifier identifies that a pair of sentences (Si , Si+1 ) has t"
K15-2015,E14-1068,0,0.182146,"on By following (Ziheng et al., 2014), we implemented an argument position classifier that classifies the location of the arguments of arbitrary 95 Proceedings of the Nineteenth Conference on Computational Natural Language Learning: Shared Task, pages 95–99, c Beijing, China, July 26-31, 2015. 2014 Association for Computational Linguistics Type Context Parse Tree Features Cs , POSs , {Wordu }, {POSu } Path(Cs ,root), Parent(Cs ), depth(Cs ), RightSib(Cs ), LeftSib(Cs ) u = s − 5, . . . , s − 1, s + 1, . . . , s + 5 • Word pairs (wi , wi+1 ) ∈ Si × Si+1 • Brown cluster pairs feature defined in Rutherford and Xue (2014) • Sentence-to-sentence discourse dependency tree features including existence of dependency edges and rhetorical relation labels. Discourse dependency trees are defined in Li et al. (Li et al., 2014). Table 1: Features used in connective classifier discourse connective into “same sentence” (SS) or “previous sentence” (PS). SS indicates both Arg1 and Arg2 are located in the same sentence that contains the discourse connective. PS indicates Arg1 is located in the sentence previous to that containing both the discourse connective and Arg2. We utilized context features in Table 1 and the position"
K15-2015,N15-1081,0,0.0230907,"ces (Si , Si+1 ) has the discourse relation, we heuristically regard Si as Arg1 and Si+1 as Arg2. 3.2 Sense Classification In the sense classification step, we classify the discourse relation between a pair of sentences (Si , Si+1 ) into five senses: “Expansion”, “Contingency”, “Temporal”, “Comparison”, and “EntRel”. To classify the sense of a pair of sentences, we used multi-class SVM. We used the same features described in the argument position identification step. To increase the number of training data, we used the (inter-sentential) explicit training data as the additional training data (Rutherford and Xue, 2015). We removed a connective from each instance in the explicit training data and treated them as implicit training data. The accuracy of classification into five senses is still low because the distribution of the senses is imbalanced. Following Rutherford and Xue (2014), we resampled the instances in the training data of sense classification to balance the distribution of the senses. 2.3 Sense Classification We assign majority sense ℓ∗ for each discourse connective Cs as follows: ℓ∗ = arg maxℓ∈L freq(Cs , ℓ). (1) L is a set of sense labels used in training data and freq returns the frequency of"
N15-1030,P11-2037,0,0.406359,"trube, 2006; Kong and Ng, 2013), long distance dependency relation analysis (Marcus et al., 1993; Xue et al., 2005). Research also uncovers the important role of ECs in machine translation. Some recent work (Chung and Gildea, 2010; Xiang et al., 2013) demonstrates the improvements they manage to obtain through EC detection in Chinese-English translation. To resolve ECs, we need to decide 1) the position and type of the EC and 2) the content of the EC (to which element the EC is linked to if plausible). Existing research mainly focuses on the first problem which is referred to as EC detection (Cai et al., 2011; Yang and Xue, 2010), and so is this paper. As ECs are words or phrases inferable from their context, previous work mainly designs features mining the contexts of ECs and then trains classification models or parsers using these features (Xue and Yang, 2013; Johnson, 2002; Gabbard et al., 2006; Kong and Zhou, 2010). One problem with these human-developed features are that they are not fully capable of representing the semantics and syntax of contexts. Besides, the feature engineering is also time consuming and labor intensive. Recently neural network models have proven their superiority in cap"
N15-1030,D10-1062,0,0.0133031,"”). This kind of grammatical phenomenons is usually caused by the omission or dislocation of nouns or pronouns. Empty categories are the “hidden” parts of text and are essential for syntactic parsing (Gabbard et al., 2006; Yang and Xue, 2010). As a basic problem in NLP, the resolution of ECs also has a huge impact on lots of downstream tasks, such as co-reference resolution (Ponzetto and Strube, 2006; Kong and Ng, 2013), long distance dependency relation analysis (Marcus et al., 1993; Xue et al., 2005). Research also uncovers the important role of ECs in machine translation. Some recent work (Chung and Gildea, 2010; Xiang et al., 2013) demonstrates the improvements they manage to obtain through EC detection in Chinese-English translation. To resolve ECs, we need to decide 1) the position and type of the EC and 2) the content of the EC (to which element the EC is linked to if plausible). Existing research mainly focuses on the first problem which is referred to as EC detection (Cai et al., 2011; Yang and Xue, 2010), and so is this paper. As ECs are words or phrases inferable from their context, previous work mainly designs features mining the contexts of ECs and then trains classification models or parse"
N15-1030,N06-1024,0,0.165919,"on Chinese Treebank prove the effectiveness of the proposed method. We improve the precision by about 6 points on a subset of Chinese Treebank, which is a new state-ofthe-art performance on CTB. 1 Introduction The empty category (EC) is an important concept in linguistic theories. It is used to describe nominal words that do not have explicit phonological forms (they are also called “covert nouns”). This kind of grammatical phenomenons is usually caused by the omission or dislocation of nouns or pronouns. Empty categories are the “hidden” parts of text and are essential for syntactic parsing (Gabbard et al., 2006; Yang and Xue, 2010). As a basic problem in NLP, the resolution of ECs also has a huge impact on lots of downstream tasks, such as co-reference resolution (Ponzetto and Strube, 2006; Kong and Ng, 2013), long distance dependency relation analysis (Marcus et al., 1993; Xue et al., 2005). Research also uncovers the important role of ECs in machine translation. Some recent work (Chung and Gildea, 2010; Xiang et al., 2013) demonstrates the improvements they manage to obtain through EC detection in Chinese-English translation. To resolve ECs, we need to decide 1) the position and type of the EC and"
N15-1030,P14-1136,0,0.0985971,"features. The distributed representations of the words would be placed at the corresponding positions in the feature vector and the remaining are set to 0. Previous work usually involves lots of syntactic and semantic features. In the work of (Xue and Yang, 2013), 6 kinds of features are used, including those derived from constituency parse trees, dependency parse trees, semantic roles and others. Here we use only the dependency parse trees for the feature extraction. The words in dependency paths we use have proven their potential in representing the meanings of text in frame identification (Hermann et al., 2014). 1. The head word (except the dummy root node). Suppose words are represented using d dimension vectors, we need d elements to represent this feature. The distributed representations of the head word would be placed at the corresponding positions. Take the OP in the sentence shown in Fig. 3 for example. For the OP, its head word is “的”, its following word is “告别” and its nephews are “NULL” and “NULL” (ECs are invisible). 2. The following word in the text. This feature is extracted using the same method with head words. −−−−→ 的/DE −−−−−→ OP 3. “Nephews”, the sons of the following word. We choo"
N15-1030,P11-1081,0,0.146437,"eatures but does not reflect the grammatic constraints of languages. For example, simple sentences in Chinese contain one and only one subject, whether it is an EC or not. If it is decided there is an EC as a subject in a certain place, there should be no more ECs as subjects in the same sentence. But such an important property is not reflected in these classification models. Methods that adopt parsing techniques take the whole parse tree as input and output a parse tree with EC anchored. So we can view the sentence as a whole and deal with ECs with regarding to all the words in the sentence. Iida and Poesio (2011) also take the grammar constraints into consideration by formulating EC detection as an ILP problem. But they usually yield poor performances compared with classification methods partly because the methods they use can not fully explore the syntactic and semantic features. 5 Related Work Empty category is a complex problem (Li and Hovy, 2015). Existing methods for EC detection mainly explores syntactic and semantic features using classification models or parsing techniques. Johnson (2002) proposes a simple pattern based algorithm to recover ECs, both the positions and their antecedents in phra"
N15-1030,P02-1018,0,0.110192,"ements they manage to obtain through EC detection in Chinese-English translation. To resolve ECs, we need to decide 1) the position and type of the EC and 2) the content of the EC (to which element the EC is linked to if plausible). Existing research mainly focuses on the first problem which is referred to as EC detection (Cai et al., 2011; Yang and Xue, 2010), and so is this paper. As ECs are words or phrases inferable from their context, previous work mainly designs features mining the contexts of ECs and then trains classification models or parsers using these features (Xue and Yang, 2013; Johnson, 2002; Gabbard et al., 2006; Kong and Zhou, 2010). One problem with these human-developed features are that they are not fully capable of representing the semantics and syntax of contexts. Besides, the feature engineering is also time consuming and labor intensive. Recently neural network models have proven their superiority in capturing features using low dense vector compared with traditional manually designed features in dozens of NLP tasks (Bengio et al., 2006; Collobert and Weston, 2008; Socher et al., 2010; Collobert et al., 2011; Li and Hovy, 2014; Li et al., 2014). This paper demonstrates t"
N15-1030,D13-1028,0,0.0125519,"ction The empty category (EC) is an important concept in linguistic theories. It is used to describe nominal words that do not have explicit phonological forms (they are also called “covert nouns”). This kind of grammatical phenomenons is usually caused by the omission or dislocation of nouns or pronouns. Empty categories are the “hidden” parts of text and are essential for syntactic parsing (Gabbard et al., 2006; Yang and Xue, 2010). As a basic problem in NLP, the resolution of ECs also has a huge impact on lots of downstream tasks, such as co-reference resolution (Ponzetto and Strube, 2006; Kong and Ng, 2013), long distance dependency relation analysis (Marcus et al., 1993; Xue et al., 2005). Research also uncovers the important role of ECs in machine translation. Some recent work (Chung and Gildea, 2010; Xiang et al., 2013) demonstrates the improvements they manage to obtain through EC detection in Chinese-English translation. To resolve ECs, we need to decide 1) the position and type of the EC and 2) the content of the EC (to which element the EC is linked to if plausible). Existing research mainly focuses on the first problem which is referred to as EC detection (Cai et al., 2011; Yang and Xue,"
N15-1030,D10-1086,0,0.159883,"EC detection in Chinese-English translation. To resolve ECs, we need to decide 1) the position and type of the EC and 2) the content of the EC (to which element the EC is linked to if plausible). Existing research mainly focuses on the first problem which is referred to as EC detection (Cai et al., 2011; Yang and Xue, 2010), and so is this paper. As ECs are words or phrases inferable from their context, previous work mainly designs features mining the contexts of ECs and then trains classification models or parsers using these features (Xue and Yang, 2013; Johnson, 2002; Gabbard et al., 2006; Kong and Zhou, 2010). One problem with these human-developed features are that they are not fully capable of representing the semantics and syntax of contexts. Besides, the feature engineering is also time consuming and labor intensive. Recently neural network models have proven their superiority in capturing features using low dense vector compared with traditional manually designed features in dozens of NLP tasks (Bengio et al., 2006; Collobert and Weston, 2008; Socher et al., 2010; Collobert et al., 2011; Li and Hovy, 2014; Li et al., 2014). This paper demonstrates the advantages of distributed representations"
N15-1030,D14-1218,0,0.0167932,"arsers using these features (Xue and Yang, 2013; Johnson, 2002; Gabbard et al., 2006; Kong and Zhou, 2010). One problem with these human-developed features are that they are not fully capable of representing the semantics and syntax of contexts. Besides, the feature engineering is also time consuming and labor intensive. Recently neural network models have proven their superiority in capturing features using low dense vector compared with traditional manually designed features in dozens of NLP tasks (Bengio et al., 2006; Collobert and Weston, 2008; Socher et al., 2010; Collobert et al., 2011; Li and Hovy, 2014; Li et al., 2014). This paper demonstrates the advantages of distributed representations and neural networks in predicting the locations and types of ECs. We formulate the EC detection as an annotation 263 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 263–271, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics task, to assign predefined labels (EC types) to given contexts. Recently, Weston et al. (2011) proposed a system taking advantages of the hidden representations of neural networks for image"
N15-1030,D14-1220,0,0.0146701,"features (Xue and Yang, 2013; Johnson, 2002; Gabbard et al., 2006; Kong and Zhou, 2010). One problem with these human-developed features are that they are not fully capable of representing the semantics and syntax of contexts. Besides, the feature engineering is also time consuming and labor intensive. Recently neural network models have proven their superiority in capturing features using low dense vector compared with traditional manually designed features in dozens of NLP tasks (Bengio et al., 2006; Collobert and Weston, 2008; Socher et al., 2010; Collobert et al., 2011; Li and Hovy, 2014; Li et al., 2014). This paper demonstrates the advantages of distributed representations and neural networks in predicting the locations and types of ECs. We formulate the EC detection as an annotation 263 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 263–271, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics task, to assign predefined labels (EC types) to given contexts. Recently, Weston et al. (2011) proposed a system taking advantages of the hidden representations of neural networks for image annotation which i"
N15-1030,D15-1278,0,0.0213416,"Missing"
N15-1030,J93-2004,0,0.0493905,"tic theories. It is used to describe nominal words that do not have explicit phonological forms (they are also called “covert nouns”). This kind of grammatical phenomenons is usually caused by the omission or dislocation of nouns or pronouns. Empty categories are the “hidden” parts of text and are essential for syntactic parsing (Gabbard et al., 2006; Yang and Xue, 2010). As a basic problem in NLP, the resolution of ECs also has a huge impact on lots of downstream tasks, such as co-reference resolution (Ponzetto and Strube, 2006; Kong and Ng, 2013), long distance dependency relation analysis (Marcus et al., 1993; Xue et al., 2005). Research also uncovers the important role of ECs in machine translation. Some recent work (Chung and Gildea, 2010; Xiang et al., 2013) demonstrates the improvements they manage to obtain through EC detection in Chinese-English translation. To resolve ECs, we need to decide 1) the position and type of the EC and 2) the content of the EC (to which element the EC is linked to if plausible). Existing research mainly focuses on the first problem which is referred to as EC detection (Cai et al., 2011; Yang and Xue, 2010), and so is this paper. As ECs are words or phrases inferab"
N15-1030,N06-1025,0,0.0166719,"rformance on CTB. 1 Introduction The empty category (EC) is an important concept in linguistic theories. It is used to describe nominal words that do not have explicit phonological forms (they are also called “covert nouns”). This kind of grammatical phenomenons is usually caused by the omission or dislocation of nouns or pronouns. Empty categories are the “hidden” parts of text and are essential for syntactic parsing (Gabbard et al., 2006; Yang and Xue, 2010). As a basic problem in NLP, the resolution of ECs also has a huge impact on lots of downstream tasks, such as co-reference resolution (Ponzetto and Strube, 2006; Kong and Ng, 2013), long distance dependency relation analysis (Marcus et al., 1993; Xue et al., 2005). Research also uncovers the important role of ECs in machine translation. Some recent work (Chung and Gildea, 2010; Xiang et al., 2013) demonstrates the improvements they manage to obtain through EC detection in Chinese-English translation. To resolve ECs, we need to decide 1) the position and type of the EC and 2) the content of the EC (to which element the EC is linked to if plausible). Existing research mainly focuses on the first problem which is referred to as EC detection (Cai et al.,"
N15-1030,P13-1081,0,0.0120588,"ical phenomenons is usually caused by the omission or dislocation of nouns or pronouns. Empty categories are the “hidden” parts of text and are essential for syntactic parsing (Gabbard et al., 2006; Yang and Xue, 2010). As a basic problem in NLP, the resolution of ECs also has a huge impact on lots of downstream tasks, such as co-reference resolution (Ponzetto and Strube, 2006; Kong and Ng, 2013), long distance dependency relation analysis (Marcus et al., 1993; Xue et al., 2005). Research also uncovers the important role of ECs in machine translation. Some recent work (Chung and Gildea, 2010; Xiang et al., 2013) demonstrates the improvements they manage to obtain through EC detection in Chinese-English translation. To resolve ECs, we need to decide 1) the position and type of the EC and 2) the content of the EC (to which element the EC is linked to if plausible). Existing research mainly focuses on the first problem which is referred to as EC detection (Cai et al., 2011; Yang and Xue, 2010), and so is this paper. As ECs are words or phrases inferable from their context, previous work mainly designs features mining the contexts of ECs and then trains classification models or parsers using these featur"
N15-1030,N13-1125,0,0.0747831,"onstrates the improvements they manage to obtain through EC detection in Chinese-English translation. To resolve ECs, we need to decide 1) the position and type of the EC and 2) the content of the EC (to which element the EC is linked to if plausible). Existing research mainly focuses on the first problem which is referred to as EC detection (Cai et al., 2011; Yang and Xue, 2010), and so is this paper. As ECs are words or phrases inferable from their context, previous work mainly designs features mining the contexts of ECs and then trains classification models or parsers using these features (Xue and Yang, 2013; Johnson, 2002; Gabbard et al., 2006; Kong and Zhou, 2010). One problem with these human-developed features are that they are not fully capable of representing the semantics and syntax of contexts. Besides, the feature engineering is also time consuming and labor intensive. Recently neural network models have proven their superiority in capturing features using low dense vector compared with traditional manually designed features in dozens of NLP tasks (Bengio et al., 2006; Collobert and Weston, 2008; Socher et al., 2010; Collobert et al., 2011; Li and Hovy, 2014; Li et al., 2014). This paper"
N15-1030,C10-2158,0,0.29338,"ove the effectiveness of the proposed method. We improve the precision by about 6 points on a subset of Chinese Treebank, which is a new state-ofthe-art performance on CTB. 1 Introduction The empty category (EC) is an important concept in linguistic theories. It is used to describe nominal words that do not have explicit phonological forms (they are also called “covert nouns”). This kind of grammatical phenomenons is usually caused by the omission or dislocation of nouns or pronouns. Empty categories are the “hidden” parts of text and are essential for syntactic parsing (Gabbard et al., 2006; Yang and Xue, 2010). As a basic problem in NLP, the resolution of ECs also has a huge impact on lots of downstream tasks, such as co-reference resolution (Ponzetto and Strube, 2006; Kong and Ng, 2013), long distance dependency relation analysis (Marcus et al., 1993; Xue et al., 2005). Research also uncovers the important role of ECs in machine translation. Some recent work (Chung and Gildea, 2010; Xiang et al., 2013) demonstrates the improvements they manage to obtain through EC detection in Chinese-English translation. To resolve ECs, we need to decide 1) the position and type of the EC and 2) the content of th"
N15-1030,W03-1730,0,0.01828,"ian 军队/troops 31 日/31rd 举行/hold 了/past-tense-marker 告别/farewell 德 国/Germany 的/DE 最后/final 仪式/ceremony 。 Figure 3: ECs in a Dependency Tree File #pro #PRO #OP #T #RNR #* #Others Total Train 81-325, 400-454 500-554, 590-596 600-885, 900 1023 1089 2099 1981 91 22 0 6305 Dev 41-80 Test 1-40 901-931 166 210 301 287 15 0 0 979 297 298 575 527 32 19 0 1748 is learned using the word2vec toolkit (Mikolov et al., 2013). We train the model on a large Chinese news copora provided by Sogou2 , which contains about 1 billion words after necessary preprocessing. The text is segmented into words using ICTCLAS(Zhang et al., 2003)3 . 3.2 Experiment Settings Initialization WA is initialized according to unif orm[− din +d24hidden , din +d24hidden ]. And WB is initialized using 24 24 unif orm[− dhidden +dout , dhidden +dout ]. Here din , dhidden and dout are the dimensions of the input layer, the hidden space and the label space. Table 1: Data Division and EC Distribution 3 3.1 Experiments on CTB Data The proposed method can be applied to various kinds of languages as long as annotated corpus are available. In our experiments, we use a subset of Chinese Treebank V7.0. We split the data set into three parts, training, deve"
N15-1049,W01-1605,0,0.014309,"Missing"
N15-1049,P14-1085,0,0.0239079,"ent, we can give theoretical upper bounds when we represent the set of all subtrees of an input tree. ZDD uses O(N log N ) nodes to represent the set of all subtrees of an N node input tree. Hence the DP algorithm runs in O(N L log N ) time. The main virtues of the proposed algorithm are that (1) it can always find an exact solution, (2) its running time is theoretically guaranteed, and (3) it can solve the three known tree trimming problems. Furthermore, our algorithm is fast enough to be practical and scalable. Since text summarization methods are often applied to large scale inputs (e.g., (Christensen et al., 2014; Nakao, 2000)), scalability is important. We compare it to state-of-the-art ILP solvers and confirm that the proposed algorithm can be hundreds of times faster. Since our method assumes known formuations for text summarization, the summary created by our algorithm is exactly the same as that obtained by applying previous methods. However, we believe that algorithmic improvements in computational cost is as important as improvements in accuracy in order to make better practical systems. 2 Tree Trimming Problems We briefly review the three tree trimming formulations used in text summarization a"
N15-1049,C04-1057,0,0.0263461,"exact solutions, and our algorithm is applicable to different tree trimming problems. Moreover, experiments show that our algorithm is faster than state-of-the-art ILP solvers, and that it scales well to handle large summarization problems. 1 Introduction Extractive text summarization and sentence compression are tasks that basically select a subset of the input set of textual units that is appropriate as a summary or a compressed sentence. Current text summarization and sentence compression methods regard the problem of extracting such a subset as a combinatorial optimization problem (e.g., (Filatova and Hatzivassiloglou, 2004; McDonald, 2007; Lin and Bilmes, 2010)). Tree trimming, the problem of finding an optimal subtree of an input tree, is one kind of these combinatorial optimization problems, and it is used in three classes of text summarizations: sentence compression (Filippova and Strube, 2008; Filippova and Altun, 2013), single-document summarization (Hirao et al., 2013), and the combination of sentence compression and single-document summarization (Kikuchi et al., 2014). In these tasks, the set of input textual units is represented as a rooted tree whose nodes correspond to the minimum textual units such a"
N15-1049,D13-1155,0,0.0955036,"asks that basically select a subset of the input set of textual units that is appropriate as a summary or a compressed sentence. Current text summarization and sentence compression methods regard the problem of extracting such a subset as a combinatorial optimization problem (e.g., (Filatova and Hatzivassiloglou, 2004; McDonald, 2007; Lin and Bilmes, 2010)). Tree trimming, the problem of finding an optimal subtree of an input tree, is one kind of these combinatorial optimization problems, and it is used in three classes of text summarizations: sentence compression (Filippova and Strube, 2008; Filippova and Altun, 2013), single-document summarization (Hirao et al., 2013), and the combination of sentence compression and single-document summarization (Kikuchi et al., 2014). In these tasks, the set of input textual units is represented as a rooted tree whose nodes correspond to the minimum textual units such as sentences and words. Next, a subset is made by forming a subtree by trimming the input tree. Since the optimal trimmed subtree preserves the relationships between textual units, it is a concise representation of the original set that preserves linguistic quality. A shortcoming of tree trimming-based meth"
N15-1049,W08-1105,0,0.156311,"d sentence compression are tasks that basically select a subset of the input set of textual units that is appropriate as a summary or a compressed sentence. Current text summarization and sentence compression methods regard the problem of extracting such a subset as a combinatorial optimization problem (e.g., (Filatova and Hatzivassiloglou, 2004; McDonald, 2007; Lin and Bilmes, 2010)). Tree trimming, the problem of finding an optimal subtree of an input tree, is one kind of these combinatorial optimization problems, and it is used in three classes of text summarizations: sentence compression (Filippova and Strube, 2008; Filippova and Altun, 2013), single-document summarization (Hirao et al., 2013), and the combination of sentence compression and single-document summarization (Kikuchi et al., 2014). In these tasks, the set of input textual units is represented as a rooted tree whose nodes correspond to the minimum textual units such as sentences and words. Next, a subset is made by forming a subtree by trimming the input tree. Since the optimal trimmed subtree preserves the relationships between textual units, it is a concise representation of the original set that preserves linguistic quality. A shortcoming"
N15-1049,D13-1158,1,0.928735,"tual units that is appropriate as a summary or a compressed sentence. Current text summarization and sentence compression methods regard the problem of extracting such a subset as a combinatorial optimization problem (e.g., (Filatova and Hatzivassiloglou, 2004; McDonald, 2007; Lin and Bilmes, 2010)). Tree trimming, the problem of finding an optimal subtree of an input tree, is one kind of these combinatorial optimization problems, and it is used in three classes of text summarizations: sentence compression (Filippova and Strube, 2008; Filippova and Altun, 2013), single-document summarization (Hirao et al., 2013), and the combination of sentence compression and single-document summarization (Kikuchi et al., 2014). In these tasks, the set of input textual units is represented as a rooted tree whose nodes correspond to the minimum textual units such as sentences and words. Next, a subset is made by forming a subtree by trimming the input tree. Since the optimal trimmed subtree preserves the relationships between textual units, it is a concise representation of the original set that preserves linguistic quality. A shortcoming of tree trimming-based methods is that they are formulated as integer linear pr"
N15-1049,P14-2052,1,0.861463,"sentence compression methods regard the problem of extracting such a subset as a combinatorial optimization problem (e.g., (Filatova and Hatzivassiloglou, 2004; McDonald, 2007; Lin and Bilmes, 2010)). Tree trimming, the problem of finding an optimal subtree of an input tree, is one kind of these combinatorial optimization problems, and it is used in three classes of text summarizations: sentence compression (Filippova and Strube, 2008; Filippova and Altun, 2013), single-document summarization (Hirao et al., 2013), and the combination of sentence compression and single-document summarization (Kikuchi et al., 2014). In these tasks, the set of input textual units is represented as a rooted tree whose nodes correspond to the minimum textual units such as sentences and words. Next, a subset is made by forming a subtree by trimming the input tree. Since the optimal trimmed subtree preserves the relationships between textual units, it is a concise representation of the original set that preserves linguistic quality. A shortcoming of tree trimming-based methods is that they are formulated as integer linear programming (ILP) problems and so an ILP solver is needed to solve them. Although modern ILP solvers can"
N15-1049,N10-1134,0,0.0631445,"ifferent tree trimming problems. Moreover, experiments show that our algorithm is faster than state-of-the-art ILP solvers, and that it scales well to handle large summarization problems. 1 Introduction Extractive text summarization and sentence compression are tasks that basically select a subset of the input set of textual units that is appropriate as a summary or a compressed sentence. Current text summarization and sentence compression methods regard the problem of extracting such a subset as a combinatorial optimization problem (e.g., (Filatova and Hatzivassiloglou, 2004; McDonald, 2007; Lin and Bilmes, 2010)). Tree trimming, the problem of finding an optimal subtree of an input tree, is one kind of these combinatorial optimization problems, and it is used in three classes of text summarizations: sentence compression (Filippova and Strube, 2008; Filippova and Altun, 2013), single-document summarization (Hirao et al., 2013), and the combination of sentence compression and single-document summarization (Kikuchi et al., 2014). In these tasks, the set of input textual units is represented as a rooted tree whose nodes correspond to the minimum textual units such as sentences and words. Next, a subset i"
N15-1049,P11-1052,0,0.0395879,"properties required. However, their complexity makes it difficult 469 to solve optimization problems efficiently. These problems can be solved by using ILP solvers, however, they may fail to find optimal solutions and they have no guarantee on the running time. Since the proposed method is a DP algorithm and it has a theoretical guarantee, it always find an optimal solution in time proportional to the size of the input tree. Our method also can be seen as a kind of fast text summarization algorithm. Previous fast algorithms are approximate algorithms (Qian and Liu, 2013; Lin and Bilmes, 2010; Lin and Bilmes, 2011; Davis et al., 2012), while our algorithm is an exact algorithm. Of course, there is a difference in task hardness since previous methods were designed for multi-document summarization and ours for single document summarization. Those works suggest the label order used in this paper to give a theoretical bound that only depends on the size of an input tree. Nevertheless, the bound attained with this extension is worse than that shown in this paper. Solution time (ms) 2500 ZDD Gurobi 2000 1500 1000 10 500 0 0 5000 10000 15000 20000 25000 Number of tree nodes Figure 7: Solution time of our algo"
N15-1049,W09-1801,0,0.0186519,"00 1000 1500 Number of tree nodes 2000 (c) Extraction & compression Figure 6: ZDD sizes with number of input tree nodes trimming problem since it is the most complex problem. We make a large artificial nested tree by concatenating outer-trees of the nested trees of 30 RSTDT datasets. The results are shown in Fig. 7, and it shows that out method scales well with large inputs comparing with Gurobi. 9 Related Work Recently proposed text summarization and sentence compression methods solve a task by formulating it as a combinatorial optimization problem (McDonald, 2007; Woodsend and Lapata, 2010; Martins and Smith, 2009; Clarke and Lapata, 2008). These combinatorial optimization-based formulations enable flexible models that can reflect the properties required. However, their complexity makes it difficult 469 to solve optimization problems efficiently. These problems can be solved by using ILP solvers, however, they may fail to find optimal solutions and they have no guarantee on the running time. Since the proposed method is a DP algorithm and it has a theoretical guarantee, it always find an optimal solution in time proportional to the size of the input tree. Our method also can be seen as a kind of fast t"
N15-1049,P00-1039,0,0.0991454,"al upper bounds when we represent the set of all subtrees of an input tree. ZDD uses O(N log N ) nodes to represent the set of all subtrees of an N node input tree. Hence the DP algorithm runs in O(N L log N ) time. The main virtues of the proposed algorithm are that (1) it can always find an exact solution, (2) its running time is theoretically guaranteed, and (3) it can solve the three known tree trimming problems. Furthermore, our algorithm is fast enough to be practical and scalable. Since text summarization methods are often applied to large scale inputs (e.g., (Christensen et al., 2014; Nakao, 2000)), scalability is important. We compare it to state-of-the-art ILP solvers and confirm that the proposed algorithm can be hundreds of times faster. Since our method assumes known formuations for text summarization, the summary created by our algorithm is exactly the same as that obtained by applying previous methods. However, we believe that algorithmic improvements in computational cost is as important as improvements in accuracy in order to make better practical systems. 2 Tree Trimming Problems We briefly review the three tree trimming formulations used in text summarization and sentence co"
N15-1049,D13-1156,0,0.0203098,"able flexible models that can reflect the properties required. However, their complexity makes it difficult 469 to solve optimization problems efficiently. These problems can be solved by using ILP solvers, however, they may fail to find optimal solutions and they have no guarantee on the running time. Since the proposed method is a DP algorithm and it has a theoretical guarantee, it always find an optimal solution in time proportional to the size of the input tree. Our method also can be seen as a kind of fast text summarization algorithm. Previous fast algorithms are approximate algorithms (Qian and Liu, 2013; Lin and Bilmes, 2010; Lin and Bilmes, 2011; Davis et al., 2012), while our algorithm is an exact algorithm. Of course, there is a difference in task hardness since previous methods were designed for multi-document summarization and ours for single document summarization. Those works suggest the label order used in this paper to give a theoretical bound that only depends on the size of an input tree. Nevertheless, the bound attained with this extension is worse than that shown in this paper. Solution time (ms) 2500 ZDD Gurobi 2000 1500 1000 10 500 0 0 5000 10000 15000 20000 25000 Number of tr"
N15-1049,P10-1058,0,0.0272796,"compression 150 2000 0 0 500 1000 1500 Number of tree nodes 2000 (c) Extraction & compression Figure 6: ZDD sizes with number of input tree nodes trimming problem since it is the most complex problem. We make a large artificial nested tree by concatenating outer-trees of the nested trees of 30 RSTDT datasets. The results are shown in Fig. 7, and it shows that out method scales well with large inputs comparing with Gurobi. 9 Related Work Recently proposed text summarization and sentence compression methods solve a task by formulating it as a combinatorial optimization problem (McDonald, 2007; Woodsend and Lapata, 2010; Martins and Smith, 2009; Clarke and Lapata, 2008). These combinatorial optimization-based formulations enable flexible models that can reflect the properties required. However, their complexity makes it difficult 469 to solve optimization problems efficiently. These problems can be solved by using ILP solvers, however, they may fail to find optimal solutions and they have no guarantee on the running time. Since the proposed method is a DP algorithm and it has a theoretical guarantee, it always find an optimal solution in time proportional to the size of the input tree. Our method also can be"
N16-1135,N09-1003,0,0.11674,"Missing"
N16-1135,D15-1159,0,0.0232145,"requirements imposed by many different uses and applications. 1 Introduction Word embedding vectors obtained from ‘neural word embedding methods’, such as SkipGram, continuous bag-of-words (CBoW) and the family of vector log-bilinear (vLBL) models (Mnih and Kavukcuoglu, 2013; Mikolov et al., 2013a; Mikolov et al., 2013c; Mikolov et al., 2013b) have now become an important fundamental resource for tackling many natural language processing (NLP) tasks. These NLP tasks include part-of-speech tagging (Tsuboi, 2014; Ling et al., 2015), dependency parsing (Chen and Manning, 2014; Dyer et al., 2015; Alberti et al., 2015), semantic role labeling (Zhou and Xu, 2015; Woodsend and Lapata, 2015), machine translation (Sutskever et al., 2014), sentiment analysis (Kim et al., 2015), and question answering (Wang and Nyberg, 2015). The main purpose of this paper is to further enhance the ‘usability’ of obtained embedding vectors in actual use. To briefly explain our motivation, we first introduce the following concept: This paper focuses on the fact that the appropriate dimension of embedding vectors strongly depends on applications and uses, and is basically determined based on the performance and memory space (or cal"
N16-1135,D14-1082,0,0.0225864,"tor can manage a wide range of dimensional requirements imposed by many different uses and applications. 1 Introduction Word embedding vectors obtained from ‘neural word embedding methods’, such as SkipGram, continuous bag-of-words (CBoW) and the family of vector log-bilinear (vLBL) models (Mnih and Kavukcuoglu, 2013; Mikolov et al., 2013a; Mikolov et al., 2013c; Mikolov et al., 2013b) have now become an important fundamental resource for tackling many natural language processing (NLP) tasks. These NLP tasks include part-of-speech tagging (Tsuboi, 2014; Ling et al., 2015), dependency parsing (Chen and Manning, 2014; Dyer et al., 2015; Alberti et al., 2015), semantic role labeling (Zhou and Xu, 2015; Woodsend and Lapata, 2015), machine translation (Sutskever et al., 2014), sentiment analysis (Kim et al., 2015), and question answering (Wang and Nyberg, 2015). The main purpose of this paper is to further enhance the ‘usability’ of obtained embedding vectors in actual use. To briefly explain our motivation, we first introduce the following concept: This paper focuses on the fact that the appropriate dimension of embedding vectors strongly depends on applications and uses, and is basically determined based o"
N16-1135,P15-1033,0,0.0173416,"nge of dimensional requirements imposed by many different uses and applications. 1 Introduction Word embedding vectors obtained from ‘neural word embedding methods’, such as SkipGram, continuous bag-of-words (CBoW) and the family of vector log-bilinear (vLBL) models (Mnih and Kavukcuoglu, 2013; Mikolov et al., 2013a; Mikolov et al., 2013c; Mikolov et al., 2013b) have now become an important fundamental resource for tackling many natural language processing (NLP) tasks. These NLP tasks include part-of-speech tagging (Tsuboi, 2014; Ling et al., 2015), dependency parsing (Chen and Manning, 2014; Dyer et al., 2015; Alberti et al., 2015), semantic role labeling (Zhou and Xu, 2015; Woodsend and Lapata, 2015), machine translation (Sutskever et al., 2014), sentiment analysis (Kim et al., 2015), and question answering (Wang and Nyberg, 2015). The main purpose of this paper is to further enhance the ‘usability’ of obtained embedding vectors in actual use. To briefly explain our motivation, we first introduce the following concept: This paper focuses on the fact that the appropriate dimension of embedding vectors strongly depends on applications and uses, and is basically determined based on the performance a"
N16-1135,P12-1092,0,0.0423807,"32.1 ∗ 34.3 ∗ 35.4 SGNS (trunc) 24.2 26.2 29.9 32.1 32.3 GloVe (trunc) 22.8 24.3 26.7 26.7 28.2 SGNS (retrain) 24.8 29.7 ∗ 33.0 32.7 33.9 GloVe (retrain) 26.3 27.3 27.1 28.1 28.1 1000 ∗ 65.6 63.2 ∗ 65.6 – – 1000 62.9 ∗ 64.5 59.9 – – 1000 36.6 36.1 27.7 – – ∗ Table 1: Results of right-truncated embedding vectors (trunc), and standard embedding vectors (retrain). ‘∗ ’ represents the best results in the corresponding column. nine datasets for Similarity (Rubenstein and Goodenough, 1965; Miller and Charles, 1991; Agirre et al., 2009; Agirre et al., 2009; Bruni et al., 2014; Radinsky et al., 2011; Huang et al., 2012; Luong et al., 2013; Hill et al., 2014), three for Analogy (Mikolov et al., 2013a; Mikolov et al., 2013c) , and one for SentComp (Mikolov et al., 2013a). Table 1 shows all the results of our experiments8 . The rows labeled ‘(trunc)’ show the performance of D-right-truncated embedding vectors, whose original vector of dimension is D = 1000. Thus, they were obtained from a single set of embedding vectors with D = 1000 for each corresponding method. Next, the rows labeled ‘(retrain)’ show the performance provided by SGNS or GloVe that were independently constructed with using a standard setting"
N16-1135,D15-1089,0,0.0249387,"ram, continuous bag-of-words (CBoW) and the family of vector log-bilinear (vLBL) models (Mnih and Kavukcuoglu, 2013; Mikolov et al., 2013a; Mikolov et al., 2013c; Mikolov et al., 2013b) have now become an important fundamental resource for tackling many natural language processing (NLP) tasks. These NLP tasks include part-of-speech tagging (Tsuboi, 2014; Ling et al., 2015), dependency parsing (Chen and Manning, 2014; Dyer et al., 2015; Alberti et al., 2015), semantic role labeling (Zhou and Xu, 2015; Woodsend and Lapata, 2015), machine translation (Sutskever et al., 2014), sentiment analysis (Kim et al., 2015), and question answering (Wang and Nyberg, 2015). The main purpose of this paper is to further enhance the ‘usability’ of obtained embedding vectors in actual use. To briefly explain our motivation, we first introduce the following concept: This paper focuses on the fact that the appropriate dimension of embedding vectors strongly depends on applications and uses, and is basically determined based on the performance and memory space (or calculation speed) trade-off. Indeed, the actual dimensions of the previous studies listed above are diverse; often around 50, and at most 1000. It is worth no"
N16-1135,Q15-1016,0,0.0385286,"rocedure of updateParams1D in Fig. 1 using the ACO-based algorithm. this norm constraint: 1 |U |ei  |V| 2 ∀i ||¯ q||p ||¯r||p |V ¯ q||p |U| 1 oj  |V| 2 o˜j = ||¯ q||p ||¯r||p ∀j, ||¯r||p |U| e˜i = (12) which also maintain e˜i o˜j = ei oj , and the objective value. Thus, we can safely apply them at any time during the optimization. Finally, Fig. 4 shows the optimization procedure when using the ACO framework. 4 Experiments As in previously reported neural word embedding papers, our training data was taken from a Wikipedia dump (Aug. 2014). We used hyperwords tool5 for our data preparation (Levy et al., 2015). We compared our method, ITACO, with the widely used conventional methods, SGNS and GloVe. We used the word2vec implementation6 to obtain word embeddings of SGNS, and glove implementation7 for GloVe. Many tunable hyper-parameters were selected based on the recommended default values of each implementation, or suggestion explained in (Levy et al., 2015). For ITACO, we selected the Glove objective to solve Eqs. 6 and 7 since it requires a lower calculation cost than the SGNS objective. We prepared three types of linguistic benchmark tasks, namely word similarity estimation (Similarity), word an"
N16-1135,D15-1176,0,0.0200786,"dding. Therefore, a single embedding vector can manage a wide range of dimensional requirements imposed by many different uses and applications. 1 Introduction Word embedding vectors obtained from ‘neural word embedding methods’, such as SkipGram, continuous bag-of-words (CBoW) and the family of vector log-bilinear (vLBL) models (Mnih and Kavukcuoglu, 2013; Mikolov et al., 2013a; Mikolov et al., 2013c; Mikolov et al., 2013b) have now become an important fundamental resource for tackling many natural language processing (NLP) tasks. These NLP tasks include part-of-speech tagging (Tsuboi, 2014; Ling et al., 2015), dependency parsing (Chen and Manning, 2014; Dyer et al., 2015; Alberti et al., 2015), semantic role labeling (Zhou and Xu, 2015; Woodsend and Lapata, 2015), machine translation (Sutskever et al., 2014), sentiment analysis (Kim et al., 2015), and question answering (Wang and Nyberg, 2015). The main purpose of this paper is to further enhance the ‘usability’ of obtained embedding vectors in actual use. To briefly explain our motivation, we first introduce the following concept: This paper focuses on the fact that the appropriate dimension of embedding vectors strongly depends on applications a"
N16-1135,W13-3512,0,0.0331257,"GNS (trunc) 24.2 26.2 29.9 32.1 32.3 GloVe (trunc) 22.8 24.3 26.7 26.7 28.2 SGNS (retrain) 24.8 29.7 ∗ 33.0 32.7 33.9 GloVe (retrain) 26.3 27.3 27.1 28.1 28.1 1000 ∗ 65.6 63.2 ∗ 65.6 – – 1000 62.9 ∗ 64.5 59.9 – – 1000 36.6 36.1 27.7 – – ∗ Table 1: Results of right-truncated embedding vectors (trunc), and standard embedding vectors (retrain). ‘∗ ’ represents the best results in the corresponding column. nine datasets for Similarity (Rubenstein and Goodenough, 1965; Miller and Charles, 1991; Agirre et al., 2009; Agirre et al., 2009; Bruni et al., 2014; Radinsky et al., 2011; Huang et al., 2012; Luong et al., 2013; Hill et al., 2014), three for Analogy (Mikolov et al., 2013a; Mikolov et al., 2013c) , and one for SentComp (Mikolov et al., 2013a). Table 1 shows all the results of our experiments8 . The rows labeled ‘(trunc)’ show the performance of D-right-truncated embedding vectors, whose original vector of dimension is D = 1000. Thus, they were obtained from a single set of embedding vectors with D = 1000 for each corresponding method. Next, the rows labeled ‘(retrain)’ show the performance provided by SGNS or GloVe that were independently constructed with using a standard setting and corresponding D."
N16-1135,N13-1090,0,0.43353,"Global Vectors. Since our method iteratively generates embedding vectors one dimension at a time, obtained vectors equip a unique property. Namely, any right-truncated vector matches the solution of the corresponding lower-dimensional embedding. Therefore, a single embedding vector can manage a wide range of dimensional requirements imposed by many different uses and applications. 1 Introduction Word embedding vectors obtained from ‘neural word embedding methods’, such as SkipGram, continuous bag-of-words (CBoW) and the family of vector log-bilinear (vLBL) models (Mnih and Kavukcuoglu, 2013; Mikolov et al., 2013a; Mikolov et al., 2013c; Mikolov et al., 2013b) have now become an important fundamental resource for tackling many natural language processing (NLP) tasks. These NLP tasks include part-of-speech tagging (Tsuboi, 2014; Ling et al., 2015), dependency parsing (Chen and Manning, 2014; Dyer et al., 2015; Alberti et al., 2015), semantic role labeling (Zhou and Xu, 2015; Woodsend and Lapata, 2015), machine translation (Sutskever et al., 2014), sentiment analysis (Kim et al., 2015), and question answering (Wang and Nyberg, 2015). The main purpose of this paper is to further enhance the ‘usability’ o"
N16-1135,D14-1162,0,0.0854519,"in this form by a simple reformulation from the original objective of SGNS (Mikolov et al., 2013b). 1146 Input: X : training data, D: maximum number of dimensions (iterations) 1: E(0) ← ∅, O(0) ← ∅, and B(0) ← 0, d ← 0 2: repeat 3: d ← d + 1 4: (¯ qd , ¯rd ) ← updateParams1D(X , B(d−1) ) // Eq. 5 ¯d) 5: E(d) ← appendVec(E(d−1) , q 6: O(d) ← appendVec(O(d−1) , ¯rd ) ¯ d , ¯rd ) // Eq. 4 7: B(d) ← updateBias(B(d−1) , q 8: until d = D Output: (E(D) , O(D) ) Figure 1: An algorithm for solving an iterative additional coordinate optimization formulation for obtaining embedding vectors. lowing form (Pennington et al., 2014): Ψ= 1X βi,j (xi,j − mi,j )2 , 2 (3) (i,j) where mi,j and βi,j represent certain co-occurrence and weighting factors of the i-th input and the jth output words, respectively. For example, βi,j = min(1, (ci,j /xmax )γ ), and mi,j = log(ci,j ) are used in (Pennington et al., 2014), where xmax and γ are tunable hyper-parameters. 3 Incremental Construction of Embedding This section explains our proposed method. The basic idea is very simple and clear: we convert the minimization problem shown in Eq. 1 to a series of minimization problems, each of whose individual problem determines one additional"
N16-1135,P15-2031,1,0.753243,"D = 1000. Thus, they were obtained from a single set of embedding vectors with D = 1000 for each corresponding method. Next, the rows labeled ‘(retrain)’ show the performance provided by SGNS or GloVe that were independently constructed with using a standard setting and corresponding D. Note that the results of ‘ITACO (retrain)’ are identical to those of ‘ITACO (trunc)’. Moreover, ‘GloVe (trunc)’ and ‘GloVe (retrain)’ in D = 1000 are equivalent, as are ‘SGNS (trunc)’ and ‘SGNS (retrain)’. Thus, these results 8 Results for SGNS and GloVe are the average performance of ten runs as suggested in (Suzuki and Nagata, 2015) 1149 were omitted from the table. First, comparing ‘(retrain)’ and ‘(trunc)’ in SGNS and GloVe, our experimental results first explicitly revealed that SGNS and GloVe with the simple truncation approach ‘(trunc)’ cannot provide effective lower-dimensional embedding vectors. This observation strongly supports the significance of existence of our proposed method, ITACO. Second, in most cases ITACO successfully provided almost the same performance level as the best SGNS and GloVe (retrain) results. We emphasize that ITACO constructed embedding vectors ‘just once’, while SGNS and GloVe required u"
N16-1135,D14-1101,0,0.0250268,"mensional embedding. Therefore, a single embedding vector can manage a wide range of dimensional requirements imposed by many different uses and applications. 1 Introduction Word embedding vectors obtained from ‘neural word embedding methods’, such as SkipGram, continuous bag-of-words (CBoW) and the family of vector log-bilinear (vLBL) models (Mnih and Kavukcuoglu, 2013; Mikolov et al., 2013a; Mikolov et al., 2013c; Mikolov et al., 2013b) have now become an important fundamental resource for tackling many natural language processing (NLP) tasks. These NLP tasks include part-of-speech tagging (Tsuboi, 2014; Ling et al., 2015), dependency parsing (Chen and Manning, 2014; Dyer et al., 2015; Alberti et al., 2015), semantic role labeling (Zhou and Xu, 2015; Woodsend and Lapata, 2015), machine translation (Sutskever et al., 2014), sentiment analysis (Kim et al., 2015), and question answering (Wang and Nyberg, 2015). The main purpose of this paper is to further enhance the ‘usability’ of obtained embedding vectors in actual use. To briefly explain our motivation, we first introduce the following concept: This paper focuses on the fact that the appropriate dimension of embedding vectors strongly depen"
N16-1135,P15-2116,0,0.0396451,"Missing"
N16-1135,D15-1295,0,0.0263285,"troduction Word embedding vectors obtained from ‘neural word embedding methods’, such as SkipGram, continuous bag-of-words (CBoW) and the family of vector log-bilinear (vLBL) models (Mnih and Kavukcuoglu, 2013; Mikolov et al., 2013a; Mikolov et al., 2013c; Mikolov et al., 2013b) have now become an important fundamental resource for tackling many natural language processing (NLP) tasks. These NLP tasks include part-of-speech tagging (Tsuboi, 2014; Ling et al., 2015), dependency parsing (Chen and Manning, 2014; Dyer et al., 2015; Alberti et al., 2015), semantic role labeling (Zhou and Xu, 2015; Woodsend and Lapata, 2015), machine translation (Sutskever et al., 2014), sentiment analysis (Kim et al., 2015), and question answering (Wang and Nyberg, 2015). The main purpose of this paper is to further enhance the ‘usability’ of obtained embedding vectors in actual use. To briefly explain our motivation, we first introduce the following concept: This paper focuses on the fact that the appropriate dimension of embedding vectors strongly depends on applications and uses, and is basically determined based on the performance and memory space (or calculation speed) trade-off. Indeed, the actual dimensions of the previou"
N16-1135,P15-1109,0,0.0154058,"applications. 1 Introduction Word embedding vectors obtained from ‘neural word embedding methods’, such as SkipGram, continuous bag-of-words (CBoW) and the family of vector log-bilinear (vLBL) models (Mnih and Kavukcuoglu, 2013; Mikolov et al., 2013a; Mikolov et al., 2013c; Mikolov et al., 2013b) have now become an important fundamental resource for tackling many natural language processing (NLP) tasks. These NLP tasks include part-of-speech tagging (Tsuboi, 2014; Ling et al., 2015), dependency parsing (Chen and Manning, 2014; Dyer et al., 2015; Alberti et al., 2015), semantic role labeling (Zhou and Xu, 2015; Woodsend and Lapata, 2015), machine translation (Sutskever et al., 2014), sentiment analysis (Kim et al., 2015), and question answering (Wang and Nyberg, 2015). The main purpose of this paper is to further enhance the ‘usability’ of obtained embedding vectors in actual use. To briefly explain our motivation, we first introduce the following concept: This paper focuses on the fact that the appropriate dimension of embedding vectors strongly depends on applications and uses, and is basically determined based on the performance and memory space (or calculation speed) trade-off. Indeed, the actu"
N18-1047,W15-4002,0,0.367123,"component of neural network (NN) models, as it determines the feature interaction and thus the expressiveness of models. In addition to the matrix-based mapping, neural tensor networks (NTNs) (Socher et al., 2013a) employ a 3dimensional tensor to capture direct interactions among input features. Due to the large expressive capacity of 3D tensors, NTNs have been successful in an array of natural language processing (NLP) and machine learning tasks, including knowledge graph completion (KGC) (Socher et al., 2013a), sentiment analysis (Socher et al., 2013b), and reasoning with logical semantics (Bowman et al., 2015). However, since a 3D tensor has a large number of parameters, NTNs need longer time to train than other NN models. Moreover, the millions of parameters often make the model suffer from overfitting (Yang et al., 2015). To solve these problems, we propose two new parameter reduction techniques for NTNs. These 2 Background We consider mapping in a neural network (NN) layer that takes two vectors as input, such as recursive neural networks. Recurrent neural networks also has this structure, with one input vector being the hidden state from the previous time step. As a mapping before activation in"
N18-1047,P17-2088,1,0.549793,"applied to more complicated architectures such as those examples.   C(w) =   |E| w2 w3  ..  .   wn w1 for wT = (w1 , . . . , wn ) can be factorized into F−1 diag(F w) F with the Fourier matrix F. By assuming each slice matrix W [i] of W [1:k] is circulant, we get the same scoring [i] function as that in Eq. (3); xT 1 W x2 = ′ −1 xT diag(F w[i] ) F x2 = ℜ(⟨x′1 , w[i] , x2 ′ ⟩) 1 F ′ where x′1 = F x1 , x′2 = F x2 , and w[i] = 1 [i] n n diag(F w ) are complex vectors in C . In this sense, NTN-Comp is equivalent to NTN where slice matrices of the 3D tensor are restricted to be circulant. Hayashi and Shimbo (2017) established a more detailed proof of the equivalence. Lu et al. (2016) employed a Toeplitz-like structured matrix, reducing parameters of LSTM. Chen et al. (2015) used a feature hashing technique to reduce parameters in RNN. Although these techniques can also be extended to reduce the number of tensor-related parameters in NTN, the former needs FFT operations; i.e., O(n log n) computation time, and the latter’s contribution is only a reduction in memory consumption. Models and Loss Function The standard approach to KGC is to design a score function Φ : E × R × E → R that assigns a large value"
N18-1047,W09-3714,0,0.0299169,"ng the protocol described in Bowman et al. (2015), with the exception that the formulas are restricted to CNF or DNF, as mentioned above. We obtained 62,589 training examples, 13,413 validation examples, and 55,150 test examples. Each formula in the training and validation examples contains up to four logical operators, whereas those in the test examples have This task definition basically follows Bowman et al. (2015): Given a pair of artificially generated propositional logic formulas, classify the relation between the formulas into one of the seven basic semantic relations of natural logic (MacCartney and Manning, 2009). Table 5 shows these seven relation types. The formulas consist of propositional variables, negation, and conjunction and disjunction connectives. Although Bowman et al. (2015) generated formulas with no constraint on its form, we restricted them to disjunctive normal 512 Model 1 2 3 4 5 6 7 8 9 10 11 12 Avg. Majority class RNN RNTN 56.0 98.0 99.9 53.0 97.5 99.5 53.4 95.5 98.2 53.2 93.3 95.7 55.9 89.9 92.7 56.5 86.1 88.5 56.5 82.8 84.7 57.8 79.9 81.2 56.5 74.8 78.1 57.7 73.2 77.5 56.8 71.8 74.4 59.9 71.7 74.4 56.1 84.5 87.0 RNTN-SMD (m = 1) RNTN-SMD (m = 2) RNTN-SMD (m = 4) RNTN-SMD (m = 8) R"
N18-1155,P17-2021,0,0.0129318,"ed methods for sentence compression use syntactic features. Filippova et al. (2015) employs the features obtained from automatic parse trees in the LSTM-based encoder-decoder in a pipeline manner. Wang et al. (2017) trims dependency trees based on the scores predicted by an LSTM-based tagger. Although these methods can consider dependency relationships between words, the pipeline approach and the 1st-order dependency relationship fail to compress longer than average sentences. Several recent machine translation studies also utilize syntactic features in Seq2Seq models. Eriguchi et al. (2017); Aharoni and Goldberg (2017) incorporate syntactic features of the target language in the decoder part of Seq2Seq. Both methods outperformed Seq2Seq without syntactic features in terms of translation quality. However, both methods fail to provide an entire parse tree until the decoding phase is finished. Thus, these methods cannot track all possible parents for each word within the decoding process. Similar to HiSAN, Hashimoto and Tsuruoka (2017) use dependency features as attention distributions, but different from HiSAN, they use pre-trained dependency relations, and do not take into account the chains of dependencies."
N18-1155,D17-1209,0,0.011857,"of the target language in the decoder part of Seq2Seq. Both methods outperformed Seq2Seq without syntactic features in terms of translation quality. However, both methods fail to provide an entire parse tree until the decoding phase is finished. Thus, these methods cannot track all possible parents for each word within the decoding process. Similar to HiSAN, Hashimoto and Tsuruoka (2017) use dependency features as attention distributions, but different from HiSAN, they use pre-trained dependency relations, and do not take into account the chains of dependencies. Marcheggiani and Titov (2017); Bastings et al. (2017) consider higherorder dependency relationships in Seq2Seq by incorporating a graph convolution technique (Kipf and Welling, 2016) into the encoder. However, the dependency information of the graph convolution technique is still given in pipeline manner. Unlike the above methods, HiSAN can capture higher-order dependency features using d-length dependency chains without relying on pipeline processing. 7 Conclusion In this paper, we incorporated higher-order dependency features into Seq2Seq to compress sentences of all lengths. Experiments on the Google sentence compression test data showed that"
N18-1155,D17-1012,0,0.114686,"Missing"
N18-1155,P11-1049,0,0.0302536,"eadability and informativeness. 80 F1 85 75 70 65 60 16-20 21-25 26-30 31-35 36-40 Sentence Length 41-45 46-50 51-55 Figure 1: F1 scores of LSTM-based sentence compression method for each sentence length.1 Average Depth 12 11 10 9 8 7 6 5 4 11-15 16-20 21-25 26-30 31-35 36-40 41-45 46-50 51-55 Sentence Length Figure 2: Average tree depths for each sentence length.2 Introduction Sentence compression is the task of compressing long sentences into short and concise ones by deleting words. To generate compressed sentences that are grammatical, many researchers (Jing, 2000; Knight and Marcu, 2000; Berg-Kirkpatrick et al., 2011; Filippova and Altun, 2013) have adopted tree trimming methods. Even though Filippova and Altun (2013) reported the best results on this task, automatic parse errors greatly degrade the performances of these tree trimming methods. 1 11-15 We used an LSTM-based sentence compression method (Filippova et al., 2015) in the evaluation setting as described in Section 4.1. Recently, Filippova et al. (2015) proposed an LSTM sequence-to-sequence (Seq2Seq) based sentence compression method that can generate fluent sentences without utilizing any syntactic features. Therefore, Seq2Seq based sentence com"
N18-1155,W14-4012,0,0.0424489,"Missing"
N18-1155,P17-2012,0,0.0150649,"veral neural network based methods for sentence compression use syntactic features. Filippova et al. (2015) employs the features obtained from automatic parse trees in the LSTM-based encoder-decoder in a pipeline manner. Wang et al. (2017) trims dependency trees based on the scores predicted by an LSTM-based tagger. Although these methods can consider dependency relationships between words, the pipeline approach and the 1st-order dependency relationship fail to compress longer than average sentences. Several recent machine translation studies also utilize syntactic features in Seq2Seq models. Eriguchi et al. (2017); Aharoni and Goldberg (2017) incorporate syntactic features of the target language in the decoder part of Seq2Seq. Both methods outperformed Seq2Seq without syntactic features in terms of translation quality. However, both methods fail to provide an entire parse tree until the decoding phase is finished. Thus, these methods cannot track all possible parents for each word within the decoding process. Similar to HiSAN, Hashimoto and Tsuruoka (2017) use dependency features as attention distributions, but different from HiSAN, they use pre-trained dependency relations, and do not take into accoun"
N18-1155,D15-1042,0,0.183986,"Missing"
N18-1155,D13-1155,0,0.704104,"80 F1 85 75 70 65 60 16-20 21-25 26-30 31-35 36-40 Sentence Length 41-45 46-50 51-55 Figure 1: F1 scores of LSTM-based sentence compression method for each sentence length.1 Average Depth 12 11 10 9 8 7 6 5 4 11-15 16-20 21-25 26-30 31-35 36-40 41-45 46-50 51-55 Sentence Length Figure 2: Average tree depths for each sentence length.2 Introduction Sentence compression is the task of compressing long sentences into short and concise ones by deleting words. To generate compressed sentences that are grammatical, many researchers (Jing, 2000; Knight and Marcu, 2000; Berg-Kirkpatrick et al., 2011; Filippova and Altun, 2013) have adopted tree trimming methods. Even though Filippova and Altun (2013) reported the best results on this task, automatic parse errors greatly degrade the performances of these tree trimming methods. 1 11-15 We used an LSTM-based sentence compression method (Filippova et al., 2015) in the evaluation setting as described in Section 4.1. Recently, Filippova et al. (2015) proposed an LSTM sequence-to-sequence (Seq2Seq) based sentence compression method that can generate fluent sentences without utilizing any syntactic features. Therefore, Seq2Seq based sentence compression is a promising alte"
N18-1155,I17-2002,1,0.704068,"β0,t indicates the weight when the method does not use the dependency features. Context vector ct is calculated as ct = ← − − → → [ h 0, h n, − s t ] using the current decoder hidden → state − s t. The calculated Ωt is concatenated and input to the output layer. In detail, dt in Eq. (5) is replaced → s t ]; furtherby concatenated vector d′t = [ht , Ωt , − ′ more, instead of dt , dt is also fed to the input of the decoder LSTM at t + 1. 3.3 Objective Function To alleviate the influence of parse errors, we jointly update the 1st-order attention distribution α1,t,k and label probability P (y|x) (Kamigaito et al., 2017). The 1st-order attention distribution is learned by dependency parse trees. If at,j = 1 is an edge between parent word wj and child wt on a dependency tree (at,j = 0 denotes that wj is not a parent of wt .), the objective function of our method can be defined as: −logP (y|x) − λ · n ∑ n ∑ j=1 t=1 at,j · logα1,t,j , (14) where λ is a hyper-parameter that controls the importance of the output labels and parse trees in the training dataset. 4 Evaluations 4.1 Evaluation Settings 4.1.1 Dataset This evaluation used the Google sentence compression dataset (Filippova and Altun, 2013)5 . This dataset"
N18-1155,N16-1179,0,0.252118,"82.7, respectively. In particular, HiSAN attained remarkable compression performance with long sentences. In human evaluations, HiSAN also outperformed the baseline methods. 2 Baseline Sequence-to-Sequence Method Sentence compression can be regarded as a tagging task, where given a sequence of input tokens x = (x0 , ..., xn ), a system assigns output label yt , which is one of three types of specific labels (“keep,”“delete,” or“end of sentence”) to each input token xt (1 ≤ t ≤ n). The LSTM-based approaches for sentence compression are mostly based on the bi-LSTM based tagging method (Tagger) (Klerke et al., 2016; Wang et al., 2017; Chen and Pan, 2017) or Seq2Seq (Filippova et al., 2015; Tran et al., 2016). Tagger independently predicts labels in a point estimation manner, whereas Seq2Seq predicts labels by considering previously predicted labels. Since Seq2Seq is more expressive than Tagger, we built HiSAN on the baseline Seq2Seq model. Our baseline Seq2Seq is a version of Filippova et al. (2015) extended through the addition of bi-LSTM, an input feeding approach (Vinyals et al., 2015; Luong et al., 2015), and a monotonic hard attention method (Yao and Zweig, 2015; Tran et al., 2016). As described in"
N18-1155,W17-3204,0,0.0215914,"Missing"
N18-1155,P04-1077,0,0.0109078,"ach training epoch. The clipping threshold of the gradient was set to 5.0. We selected trained models with early stopping based on maximizing per-sentence accuracy (i.e., how many compressions could be fully reproduced) of the development data set. To obtain a compressed sentence, we used greedy decoding, rather than beam decoding, as the latter attained no gain in the development dataset. All methods were written in C++ on Dynet (Neubig et al., 2017). 4.2 Automatic Evaluation In the automatic evaluation, we used token level F1 -measure (F1 ) as well as recall of ROUGE-1, ROUGE-2 and ROUGE-L (Lin and Och, 2004)9 as evaluation measures. We used ∆C = system compression ratio − gold compression ratio to evaluate how close the compression ratio of system outputs was to that of gold compressed sentences. The average compression ratio of the gold compression for input sentence was 39.8. We used micro-average for F1 -measure and compression ratio10 , and macroaverage for ROUGE scores, respectively. To verify the benefits of our methods on long sentences, we additionally report scores on sentences longer than the average sentence length (= 28) in the test set. The average compression ratio of the gold compr"
N18-1155,D15-1166,0,0.539389,"proaches for sentence compression are mostly based on the bi-LSTM based tagging method (Tagger) (Klerke et al., 2016; Wang et al., 2017; Chen and Pan, 2017) or Seq2Seq (Filippova et al., 2015; Tran et al., 2016). Tagger independently predicts labels in a point estimation manner, whereas Seq2Seq predicts labels by considering previously predicted labels. Since Seq2Seq is more expressive than Tagger, we built HiSAN on the baseline Seq2Seq model. Our baseline Seq2Seq is a version of Filippova et al. (2015) extended through the addition of bi-LSTM, an input feeding approach (Vinyals et al., 2015; Luong et al., 2015), and a monotonic hard attention method (Yao and Zweig, 2015; Tran et al., 2016). As described in the evaluations section, this baseline achieved comparable or even better scores than the state-of-the-art scores reported in Filippova et al. (2015). The baseline Seq2Seq model consists of embedding, encoder, decoder, and output layers. In the embedding layer, the input tokens x are converted to the embeddings e. As reported in Wang et al. (2017), syntactic features are important for learning a generalizable embedding for sentence compression. Following their results, we also introduce syntactic"
N18-1155,D17-1159,0,0.0343335,"incorporate syntactic features of the target language in the decoder part of Seq2Seq. Both methods outperformed Seq2Seq without syntactic features in terms of translation quality. However, both methods fail to provide an entire parse tree until the decoding phase is finished. Thus, these methods cannot track all possible parents for each word within the decoding process. Similar to HiSAN, Hashimoto and Tsuruoka (2017) use dependency features as attention distributions, but different from HiSAN, they use pre-trained dependency relations, and do not take into account the chains of dependencies. Marcheggiani and Titov (2017); Bastings et al. (2017) consider higherorder dependency relationships in Seq2Seq by incorporating a graph convolution technique (Kipf and Welling, 2016) into the encoder. However, the dependency information of the graph convolution technique is still given in pipeline manner. Unlike the above methods, HiSAN can capture higher-order dependency features using d-length dependency chains without relying on pipeline processing. 7 Conclusion In this paper, we incorporated higher-order dependency features into Seq2Seq to compress sentences of all lengths. Experiments on the Google sentence compressi"
N18-1155,E17-1063,0,0.0383264,"This is because the output length is the same as the input length, and each xt can be assigned to each yt in a one-to-one correspondence. 1718 1. Parent Attention module calculates Pparent (xj |xt , x), the probability of xj being the parent of xt , by using hj and ht . This probability is calculated for all pairs of xj , xt . The arc in Figure 5 shows the most probable dependency parent for each child token. y1 y6 y7 sof tmax sof tmax sof tmax h 1 Ω1 h 7 Ω7 h 6 Ω6 − → s1 − → s6 y0 x 1 y5 x 6 ← − h0 − → s7 y6 Recursive Attention (d = {1, 2, 3}) d=3 γ3,7 Σ γ2,7 Σ 3.2.1 Parent Attention Module Zhang et al. (2017) formalized dependency parsing as the problem of independently selecting the parent of each word in a sentence. They produced a distribution over possible parents for each child word by using the attention layer on bi-LSTM hidden layers. In a dependency tree, a parent has more than one child. Under this constraint, dependency parsing is represented as follows. Given sentence S = (x0 , x1 , ..., xn ), the parent of xj is selected from S  xi for each token S  x0 . Note that x0 denotes the root node. The probability of token xj being the parent of token xt in sentence x is calculated as follows"
N18-1155,W14-4009,0,0.0338832,"Missing"
N18-1157,P11-1049,0,0.211654,"ften used, which is advantageous in that each compressed sentence preserves its original dependency relations. Specifically, given a set of dependency trees constructed for sentences in the original documents, a summary is obtained by extracting some rooted subtrees; each subtree corresponds to a compressed sentence. Different from the extractive summarization, the dependency relations in each sentence must be taken into account, and hence the aforementioned extractive methods cannot be applied to compressive summarization. A number of methods have been proposed for compressive summarization (Berg-Kirkpatrick et al., 2011; Almeida and Martins, 2013; Morita et al., 2013; Kikuchi et al., 2014; Hirao et al., 2017a). These methods formulate summarization as a type of combinatorial optimization problem with a tree constraint, and they obtain summaries by solving the problem. Unfortunately, the existing methods have two drawbacks: (1) The class of objective functions to which they are applicable is limited; for example, they work only with the linear function or coverage function. As a result, the performance of these methods cannot be improved by elaborating the objective functions. (2) They contain costly procedur"
N18-1157,P16-1046,0,0.0711965,"Missing"
N18-1157,N16-2014,0,0.106628,"ed by Hirao et al. (2017b). In many existing methods, sentences are extracted by solving various subset selection problems: for example, the knapsack problem (McDonald, 2007), maximum coverage problem (Filatova and Hatzivassiloglou, 2004; Takamura and Okumura, 2009a), budgeted median problem (Takamura and Okumura, 2009b), and submodular maximization problem (Lin and Bilmes, 2010). Of particular interest, the method based on submodular maximization has three advantages: (1) Many objective functions used for document summarization are known to be monotone and submodular (Lin and Bilmes, 2011; J Kurisinkel et al., 2016); examples of such functions include the coverage function, diversity reward function, and ROUGE. Therefore, the method can deliver high performance by using monotone submodular objective functions that are suitable for the given tasks. (2) The efficient greedy algorithm is effective for the submodular maximization problem, which provides fast summarization systems. (3) Theoretical performance guarantees of the greedy algorithm can be proved; for example, a 12 (1 − e−1 )approximation guarantee can be obtained. Although the above extractive methods successfully obtain summaries with high ROUGE"
N18-1157,de-marneffe-etal-2006-generating,0,0.245993,"Missing"
N18-1157,C04-1057,0,0.134823,"Missing"
N18-1157,D13-1155,0,0.142678,"objective functions well-suited for document summarization have submodularity and monotonicity; examples of such functions include the coverage function, diversity reward function, and ROUGE, to name a few. 3 Problem Statements We formulate the summarization task as the following subtree extraction problem called STKP hereafter. In what follows, we let [M ] := {1, . . . , M } for any positive integer M . We attempt to summarize document data consisting of N sentences. Each sentence forms a dependency tree, which can be constructed by using existing methods (e.g., (Filippova and Strube, 2008; Filippova and Altun, 2013)). For convenience, we call the dependency tree of a sentence the sentence tree. The i-th sentence (i ∈ [N ]) yields sentence tree Ti = (Vi , Ei ) rooted at ri ∈ Vi , where Vi is a set of textual units (e.g., words or chunks) contained in the i-th sentence, and edges in Ei represent their dependency relations. We define a document tree with a dummy root vertex r as T := ({r} ∪ V, E), where V and E are vertex and edge sets, respectively, defined as follows: V := [ i∈[N ] Vi , E := [ i∈[N ] {Ei ∪ {(r, ri )}}. Namely, V is the set of all textual units contained in the document data, and edges in"
N18-1157,W08-1105,0,0.446485,"me sentences often includes many redundant parts. As a result, if the limitation placed on summary length is tight, the extractive approach cannot yield an informative summary. Compressive summarization is known to be effective in overcoming this problem. With this approach, a summary is constructed with some 1737 Proceedings of NAACL-HLT 2018, pages 1737–1746 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics compressed sentences, and thus we can obtain a concise and informative summary. To make compressed sentences, the dependency-tree-based approach (Filippova and Strube, 2008) is often used, which is advantageous in that each compressed sentence preserves its original dependency relations. Specifically, given a set of dependency trees constructed for sentences in the original documents, a summary is obtained by extracting some rooted subtrees; each subtree corresponds to a compressed sentence. Different from the extractive summarization, the dependency relations in each sentence must be taken into account, and hence the aforementioned extractive methods cannot be applied to compressive summarization. A number of methods have been proposed for compressive summarizat"
N18-1157,P17-2043,1,0.86869,"69; Cheng and Lapata, 2016; Peyrard and Eckle-Kohler, 2017). Owing to the recent advances in data collection, the size of document data to be summarized has been exploding, which has been bringing a drastic increase in the demand for fast summarization systems. Extractive summarization is a widely used approach to designing fast summarization systems. With this approach, we construct a summary by extracting some sentences from the original document(s). The extractive approach is not only fast but also has the potential to achieve state-of-theart ROUGE scores (Lin, 2004), which was revealed by Hirao et al. (2017b). In many existing methods, sentences are extracted by solving various subset selection problems: for example, the knapsack problem (McDonald, 2007), maximum coverage problem (Filatova and Hatzivassiloglou, 2004; Takamura and Okumura, 2009a), budgeted median problem (Takamura and Okumura, 2009b), and submodular maximization problem (Lin and Bilmes, 2010). Of particular interest, the method based on submodular maximization has three advantages: (1) Many objective functions used for document summarization are known to be monotone and submodular (Lin and Bilmes, 2011; J Kurisinkel et al., 2016)"
N18-1157,E17-1037,1,0.939011,"69; Cheng and Lapata, 2016; Peyrard and Eckle-Kohler, 2017). Owing to the recent advances in data collection, the size of document data to be summarized has been exploding, which has been bringing a drastic increase in the demand for fast summarization systems. Extractive summarization is a widely used approach to designing fast summarization systems. With this approach, we construct a summary by extracting some sentences from the original document(s). The extractive approach is not only fast but also has the potential to achieve state-of-theart ROUGE scores (Lin, 2004), which was revealed by Hirao et al. (2017b). In many existing methods, sentences are extracted by solving various subset selection problems: for example, the knapsack problem (McDonald, 2007), maximum coverage problem (Filatova and Hatzivassiloglou, 2004; Takamura and Okumura, 2009a), budgeted median problem (Takamura and Okumura, 2009b), and submodular maximization problem (Lin and Bilmes, 2010). Of particular interest, the method based on submodular maximization has three advantages: (1) Many objective functions used for document summarization are known to be monotone and submodular (Lin and Bilmes, 2011; J Kurisinkel et al., 2016)"
N18-1157,P14-2052,1,0.842528,"original dependency relations. Specifically, given a set of dependency trees constructed for sentences in the original documents, a summary is obtained by extracting some rooted subtrees; each subtree corresponds to a compressed sentence. Different from the extractive summarization, the dependency relations in each sentence must be taken into account, and hence the aforementioned extractive methods cannot be applied to compressive summarization. A number of methods have been proposed for compressive summarization (Berg-Kirkpatrick et al., 2011; Almeida and Martins, 2013; Morita et al., 2013; Kikuchi et al., 2014; Hirao et al., 2017a). These methods formulate summarization as a type of combinatorial optimization problem with a tree constraint, and they obtain summaries by solving the problem. Unfortunately, the existing methods have two drawbacks: (1) The class of objective functions to which they are applicable is limited; for example, they work only with the linear function or coverage function. As a result, the performance of these methods cannot be improved by elaborating the objective functions. (2) They contain costly procedures as their building blocks: integer-linear-programming (ILP) solvers,"
N18-1157,W04-1013,0,0.12033,"trieval (Luhn, 1958; Edmundson, 1969; Cheng and Lapata, 2016; Peyrard and Eckle-Kohler, 2017). Owing to the recent advances in data collection, the size of document data to be summarized has been exploding, which has been bringing a drastic increase in the demand for fast summarization systems. Extractive summarization is a widely used approach to designing fast summarization systems. With this approach, we construct a summary by extracting some sentences from the original document(s). The extractive approach is not only fast but also has the potential to achieve state-of-theart ROUGE scores (Lin, 2004), which was revealed by Hirao et al. (2017b). In many existing methods, sentences are extracted by solving various subset selection problems: for example, the knapsack problem (McDonald, 2007), maximum coverage problem (Filatova and Hatzivassiloglou, 2004; Takamura and Okumura, 2009a), budgeted median problem (Takamura and Okumura, 2009b), and submodular maximization problem (Lin and Bilmes, 2010). Of particular interest, the method based on submodular maximization has three advantages: (1) Many objective functions used for document summarization are known to be monotone and submodular (Lin an"
N18-1157,N10-1134,0,0.464084,"ith this approach, we construct a summary by extracting some sentences from the original document(s). The extractive approach is not only fast but also has the potential to achieve state-of-theart ROUGE scores (Lin, 2004), which was revealed by Hirao et al. (2017b). In many existing methods, sentences are extracted by solving various subset selection problems: for example, the knapsack problem (McDonald, 2007), maximum coverage problem (Filatova and Hatzivassiloglou, 2004; Takamura and Okumura, 2009a), budgeted median problem (Takamura and Okumura, 2009b), and submodular maximization problem (Lin and Bilmes, 2010). Of particular interest, the method based on submodular maximization has three advantages: (1) Many objective functions used for document summarization are known to be monotone and submodular (Lin and Bilmes, 2011; J Kurisinkel et al., 2016); examples of such functions include the coverage function, diversity reward function, and ROUGE. Therefore, the method can deliver high performance by using monotone submodular objective functions that are suitable for the given tasks. (2) The efficient greedy algorithm is effective for the submodular maximization problem, which provides fast summarizatio"
N18-1157,P11-1052,0,0.106124,"Missing"
N18-1157,P13-1101,0,0.0389869,"Missing"
N18-1157,P17-1100,0,0.025609,"Missing"
N18-1157,E09-1089,0,0.324943,"fast summarization systems. Extractive summarization is a widely used approach to designing fast summarization systems. With this approach, we construct a summary by extracting some sentences from the original document(s). The extractive approach is not only fast but also has the potential to achieve state-of-theart ROUGE scores (Lin, 2004), which was revealed by Hirao et al. (2017b). In many existing methods, sentences are extracted by solving various subset selection problems: for example, the knapsack problem (McDonald, 2007), maximum coverage problem (Filatova and Hatzivassiloglou, 2004; Takamura and Okumura, 2009a), budgeted median problem (Takamura and Okumura, 2009b), and submodular maximization problem (Lin and Bilmes, 2010). Of particular interest, the method based on submodular maximization has three advantages: (1) Many objective functions used for document summarization are known to be monotone and submodular (Lin and Bilmes, 2011; J Kurisinkel et al., 2016); examples of such functions include the coverage function, diversity reward function, and ROUGE. Therefore, the method can deliver high performance by using monotone submodular objective functions that are suitable for the given tasks. (2)"
N18-2104,hovy-etal-2006-automated,0,0.0728621,"ally overlapped basic elements based on word similarity. Even though it is simple, pBE outperforms ROUGE in DUC datasets in most cases and achieves the highest rank correlation coefficient in TAC 2011 AESOP task. 1 Introduction Automatic evaluation measures have a significant impact on the research on summarization. Since there is no other practical way to quickly evaluate the quality of system summaries, summarization studies work on raising the scores that are given by automatic evaluation measures. Among the automatic evaluation measures, the most popular ones are ROUGE (Lin, 2004) and BE (Hovy et al., 2006). ROUGE/BE counts the number of ngrams/basic elements1 that match those in manual reference summaries. ROUGE normally employs unigrams or bigrams while BE uses dependency triples (head|modifier|relation) as their units. It is known that both ROUGE and BE are well correlated with human judgment. Their evaluation approach, however, is quite different from humans’ in two ways: they score low-information units higher and ignore the semantic overlap of units. The first problem is 2 Related Work ROUGE-WE (Ng and Abrecht, 2015) and BEwTE (Tratz and Hovy, 2008) are closely related to our method in tha"
N18-2104,W04-1013,0,0.0108104,") reducing semantically overlapped basic elements based on word similarity. Even though it is simple, pBE outperforms ROUGE in DUC datasets in most cases and achieves the highest rank correlation coefficient in TAC 2011 AESOP task. 1 Introduction Automatic evaluation measures have a significant impact on the research on summarization. Since there is no other practical way to quickly evaluate the quality of system summaries, summarization studies work on raising the scores that are given by automatic evaluation measures. Among the automatic evaluation measures, the most popular ones are ROUGE (Lin, 2004) and BE (Hovy et al., 2006). ROUGE/BE counts the number of ngrams/basic elements1 that match those in manual reference summaries. ROUGE normally employs unigrams or bigrams while BE uses dependency triples (head|modifier|relation) as their units. It is known that both ROUGE and BE are well correlated with human judgment. Their evaluation approach, however, is quite different from humans’ in two ways: they score low-information units higher and ignore the semantic overlap of units. The first problem is 2 Related Work ROUGE-WE (Ng and Abrecht, 2015) and BEwTE (Tratz and Hovy, 2008) are closely r"
N18-2104,P14-5010,0,0.00395057,"s of the datasets. “Evaluation” represents manual evaluation methods and “Limit” represents word limits of summarization. Combined with step 1, fully pruned BE is defined as follows: pBE−cnt+cls (R, S) = PK PMk Evaluation coverage coverage responsiveness responsiveness pyramid responsiveness pyramid pyramid method ROUGE-WE. We chose the latest AESOP dataset, TAC 2011, for which ROUGE-WE achieved the highest Spearman coefficient (Ng and Abrecht, 2015). The details of our experimental setup are given in Table 3 and below. Parser: We used the neural-network dependency parser of Stanford CoreNLP (Manning et al., 2014). Dependencies were set to enhanced++ Universal Dependencies (Schuster and Manning, 2016). Clustering: We employed hierarchical clustering, maximum distance method. The number of clusters, N , was set to 0.975 ∗ Q. Word Embeddings: A set of pre-trained Google-News word embeddings10 . It contains 3 million words, each of which has a word embedding of 300 dimensions. (3) Experimental Setup To assess the effectiveness of pBE, we computed the correlation coefficient between pBE scores and human judgments, as well as between the scores of other automatic evaluation measures and manual scores for co"
N18-2104,D15-1222,0,0.0850817,"tic evaluation measures, the most popular ones are ROUGE (Lin, 2004) and BE (Hovy et al., 2006). ROUGE/BE counts the number of ngrams/basic elements1 that match those in manual reference summaries. ROUGE normally employs unigrams or bigrams while BE uses dependency triples (head|modifier|relation) as their units. It is known that both ROUGE and BE are well correlated with human judgment. Their evaluation approach, however, is quite different from humans’ in two ways: they score low-information units higher and ignore the semantic overlap of units. The first problem is 2 Related Work ROUGE-WE (Ng and Abrecht, 2015) and BEwTE (Tratz and Hovy, 2008) are closely related to our method in that they aim to improve unit matching. ROUGE-WE exploits word embeddings to softly match ngrams based on their cosine similarities. Although this also takes semantic correspondence into consideration, it is different from pBE because it does not judge word similarity within one summary, but only between a target sum1 We use “BE” to represent the evaluation method Basic Elements, “basic element(s)” to represent the fragments of Basic Elements and “unit” as a general term of ngrams and basic elements. 661 Proceedings of NAAC"
N18-2104,L16-1376,0,0.0603922,"Missing"
P00-1049,W99-0902,0,\N,Missing
P00-1049,C94-1032,1,\N,Missing
P00-1049,C94-1030,0,\N,Missing
P00-1049,C96-2136,1,\N,Missing
P00-1049,P98-2152,1,\N,Missing
P00-1049,C98-2147,1,\N,Missing
P05-3016,P98-2152,1,0.806927,". To increase the accuracy, we consider all candidates around each estimated location and create a character matrix, an example of which is shown in Figure 4. At each location, we rank the candidates according to their OCR scores, the highest scores occupy the top row. Next, we apply an algorithm that consists of similar character matching, similar word retrieval, and word sequence search using language model scores 63                     Figure 4: A character matrix: Character candidates are bound to each estimated location to make the matrix. Bold characters are true. (Nagata, 1998). The algorithm is applied from the start to the end of the string and examines all possible combinations of the characters in the matrix. At each location, the algorithm finds all words, listed in a word dictionary, that are possible given the location; that is, the first location restricts the word candidates to those that start with this character. Moreover, to counter the case in which the true character is not present in the matrix, the algorithm identifies those words in the dictionary that contain characters similar to the characters in the matrix and outputs those words as word candida"
P05-3016,C98-2147,1,\N,Missing
P06-1090,J03-1002,0,0.0599068,"!     !    !    (6) ! where and  are words in the target and source phrases. The phrase alignment based on Equation (5) can be thought of as an extension of word alignment based on the IBM Model 1 to phrase alignment. Note that bilingual phrase segmentation (phrase extraction) is also done using the same criteria. The approximation in Equation (6) is motivated by (Vogel et al., 2003). Here, we added the second !  term    to cope with the asymmetry between !     !2  and    . The word translation probabilities are estimated using the GIZA++ (Och and Ney, 2003). The above search is implemented in the following way:  1. All source word and target word pairs are considered to be initial phrase pairs. 715 2. If the phrase translation probability of the phrase pair is less than the threshold, it is deleted. _ the_light_was_red 3. Each phrase pair is expanded toward the eight neighboring directions as shown in Figure 3. _ (2) _ the_light 4. If the phrase translation probability of the expanded phrase pair is less than the threshold, it is deleted. (3) _ was_red _ 5. The process of expansion and deletion is repeated until no further expansion is possible"
P06-1090,J04-4002,0,0.0601504,"47 Japan nagata.masaaki@labs.ntt.co.jp, saito.kuniko@labs.ntt.co.jp Kazuhide Yamamoto, Kazuteru Ohashi Nagaoka University of Technology 1603-1, Kamitomioka, Nagaoka City Niigata, 940-2188 Japan ykaz@nlp.nagaokaut.ac.jp, ohashi@nlp.nagaokaut.ac.jp Abstract Standard phrase-based translation systems use a word distance-based reordering model in which non-monotonic phrase alignment is penalized based on the word distance between successively translated source phrases without considering the orientation of the phrase alignment or the identities of the source and target phrases (Koehn et al., 2003; Och and Ney, 2004). (Tillmann and Zhang, 2005) introduced the notion of a block (a pair of source and target phrases that are translations of each other), and proposed the block orientation bigram in which the local reordering of adjacent blocks are expressed as a three-valued orientation, namely Right (monotone), Left (swapped), or Neutral. A block with neutral orientation is supposed to be less strongly linked to its predecessor block: thus in their model, the global reordering is not explicitly modeled. In this paper, we present a global reordering model that explicitly models long distance reordering1 . It"
P06-1090,W99-0604,0,0.0446249,"se-English bilingual sentence. For the estimation of the global phrase reordering model, preliminary tests have shown that the appropriate N-best number is 20. In counting the events for the relative frequency estimation, we treat all N-best phrase alignments equally. For comparison, we also implemented a different N-best phrase alignment method, where                 _ _ _ (1) 4.2 Bilingual Phrase Clustering The second approach to cope with the sparseness in Equation (4) is to group the phrases into equivalence classes. We used a bilingual word clustering tool, mkcls (Och et al., 1999) for this purpose. It forms partitions of the vocabulary of the two languages to maximize the joint probability of the training bilingual corpus. In order to perform bilingual phrase clustering, all words in a phrase are concatenated by an underscore ’ ’ to form a pseudo word. We then use the modified bilingual sentences as the input to mkcls. We treat all N-best phrase alignments equally. Thus, the phrase alignments in Figure 4 are converted to the following three bilingual sentence pairs.  the_light_was_red   _  _ _""! _ # _ ""! _ # the_light  _  was_red _ $! _ #"
P06-1090,2005.iwslt-1.16,1,0.828703,"bilingual sentences as the input to mkcls. We treat all N-best phrase alignments equally. Thus, the phrase alignments in Figure 4 are converted to the following three bilingual sentence pairs.  the_light_was_red   _  _ _""! _ # _ ""! _ # the_light  _  was_red _ $! _ #             the_light was red Preliminary tests have shown that the appropriate number of classes for the estimation of the global phrase reordering model is 20. As a comparison, we also tried two phrase classification methods based on the part of speech of the head word (Ohashi et al., 2005). We defined (arguably) the first word of each English phrase and the last word of each Japanese phrase as the 716 shorthand baseline "" e[0] f[0] e[0]f[0] e[-1]f[0] e[0]f[-1,0] e[-1]f[-1,0] e[-1,0]f[0] e[-1,0]f[-1,0] reordering ->K - model G H IJ  JMLN H '$ !2 '$  ! [ '$   ! A !  '$    !5-  A !  '$    ! A !5-  A !  '$    !5-  A !5-  A ![ '$    !5- A  ! A !   '$    !5- A   ! A ! 5- A ![   '$      Japanese English Vocabulary 9,277 6,956 Language Translation) 2005 is an evaluation campaign for spoken language transl"
P06-1090,P02-1040,0,0.107293,"Missing"
P06-1090,P05-1069,0,0.515194,"ki@labs.ntt.co.jp, saito.kuniko@labs.ntt.co.jp Kazuhide Yamamoto, Kazuteru Ohashi Nagaoka University of Technology 1603-1, Kamitomioka, Nagaoka City Niigata, 940-2188 Japan ykaz@nlp.nagaokaut.ac.jp, ohashi@nlp.nagaokaut.ac.jp Abstract Standard phrase-based translation systems use a word distance-based reordering model in which non-monotonic phrase alignment is penalized based on the word distance between successively translated source phrases without considering the orientation of the phrase alignment or the identities of the source and target phrases (Koehn et al., 2003; Och and Ney, 2004). (Tillmann and Zhang, 2005) introduced the notion of a block (a pair of source and target phrases that are translations of each other), and proposed the block orientation bigram in which the local reordering of adjacent blocks are expressed as a three-valued orientation, namely Right (monotone), Left (swapped), or Neutral. A block with neutral orientation is supposed to be less strongly linked to its predecessor block: thus in their model, the global reordering is not explicitly modeled. In this paper, we present a global reordering model that explicitly models long distance reordering1 . It predicts four type of reorde"
P06-1090,N03-1017,0,0.181266,"pan Kanagawa, 239-0847 Japan nagata.masaaki@labs.ntt.co.jp, saito.kuniko@labs.ntt.co.jp Kazuhide Yamamoto, Kazuteru Ohashi Nagaoka University of Technology 1603-1, Kamitomioka, Nagaoka City Niigata, 940-2188 Japan ykaz@nlp.nagaokaut.ac.jp, ohashi@nlp.nagaokaut.ac.jp Abstract Standard phrase-based translation systems use a word distance-based reordering model in which non-monotonic phrase alignment is penalized based on the word distance between successively translated source phrases without considering the orientation of the phrase alignment or the identities of the source and target phrases (Koehn et al., 2003; Och and Ney, 2004). (Tillmann and Zhang, 2005) introduced the notion of a block (a pair of source and target phrases that are translations of each other), and proposed the block orientation bigram in which the local reordering of adjacent blocks are expressed as a three-valued orientation, namely Right (monotone), Left (swapped), or Neutral. A block with neutral orientation is supposed to be less strongly linked to its predecessor block: thus in their model, the global reordering is not explicitly modeled. In this paper, we present a global reordering model that explicitly models long distan"
P06-1090,2003.mtsummit-papers.53,0,0.0154005,"A      C  /     E    C  !M0    N N (5) Phrase translation probabilities are approximated  !  using word translation probabilities    and !     as follows,  /        !     !    !    (6) ! where and  are words in the target and source phrases. The phrase alignment based on Equation (5) can be thought of as an extension of word alignment based on the IBM Model 1 to phrase alignment. Note that bilingual phrase segmentation (phrase extraction) is also done using the same criteria. The approximation in Equation (6) is motivated by (Vogel et al., 2003). Here, we added the second !  term    to cope with the asymmetry between !     !2  and    . The word translation probabilities are estimated using the GIZA++ (Och and Ney, 2003). The above search is implemented in the following way:  1. All source word and target word pairs are considered to be initial phrase pairs. 715 2. If the phrase translation probability of the phrase pair is less than the threshold, it is deleted. _ the_light_was_red 3. Each phrase pair is expanded toward the eight neighboring directions as shown in Figure 3. _ (2) _ the_light 4. If the phrase tra"
P06-1090,C04-1030,0,0.0254999,", 2003). We call this conventional phrase extraction method “grow-diag-final”, and the proposed phrase extraction method “ppicker” (this is intended to stand for phrase picker). The search for consistent Viterbi phrase alignments can be implemented as a phrase-based decoder using a beam search whose outputs are constrained only to the target sentence. The consistent N-best phrase alignment can be obtained by using A* search as described in (Ueffing et al., 2002). We did not use any reordering constraints, such as IBM constraint and ITG constraint in the search for the N-best phrase alignment (Zens et al., 2004). The thresholds used in the search are the following: the minimum phrase translation probability is 0.0001. The maximum number of translation candidates for each phrase is 20. The beam width is 1e-10, the stack size (for each target candidate word length) is 1000. We found that, compared with the decoding of sentence translation, we have to search significantly larger space for the N-best phrase alignment. Figure 3 shows an example of phrase pair expansion toward eight neighbors. If the current phrase pair is ( , of), the expanded phrase , means of), ( , pairs are ( means of), ( , means of),"
P06-1090,W02-1021,0,\N,Missing
P06-1090,2005.iwslt-1.1,0,\N,Missing
P10-2025,P07-1001,0,0.0172614,"and translation probability, which leads to a significant improvement in SMT performance. Many word alignment approaches based on generative models have been proposed and they learn from bilingual sentences in an unsupervised manner (Vogel et al., 1996; Och and Ney, 2003; Fraser and Marcu, 2007). One way to improve word alignment quality is to add linguistic knowledge derived from a monolingual corpus. This monolingual knowledge makes it easier to determine corresponding words correctly. For instance, functional words in one language tend to correspond to functional words in another language (Deng and Gao, 2007), and the syntactic dependency of words in each language can help the alignment process (Ma et al., 2008). It has been shown that such grammatical 137 Proceedings of the ACL 2010 Conference Short Papers, pages 137–141, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics translation probability from {e to } f under the kth topic: p (f |e, z = k ). T = Ti,i′ is a state transition probability of a first order Markov process. Fig. 1 shows a graphical model of HM-BiTAM. The total likelihood of bilingual sentence pairs {E, F } can be obtained by marginalizing out laten"
P10-2025,D07-1006,0,0.0229483,"roves word alignment quality. 1 Introduction Word alignment is an essential step in most phrase and syntax based statistical machine translation (SMT). It is an inference problem of word correspondences between different languages given parallel sentence pairs. Accurate word alignment can induce high quality phrase detection and translation probability, which leads to a significant improvement in SMT performance. Many word alignment approaches based on generative models have been proposed and they learn from bilingual sentences in an unsupervised manner (Vogel et al., 1996; Och and Ney, 2003; Fraser and Marcu, 2007). One way to improve word alignment quality is to add linguistic knowledge derived from a monolingual corpus. This monolingual knowledge makes it easier to determine corresponding words correctly. For instance, functional words in one language tend to correspond to functional words in another language (Deng and Gao, 2007), and the syntactic dependency of words in each language can help the alignment process (Ma et al., 2008). It has been shown that such grammatical 137 Proceedings of the ACL 2010 Conference Short Papers, pages 137–141, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for C"
P10-2025,W08-0409,0,0.0194292,"t approaches based on generative models have been proposed and they learn from bilingual sentences in an unsupervised manner (Vogel et al., 1996; Och and Ney, 2003; Fraser and Marcu, 2007). One way to improve word alignment quality is to add linguistic knowledge derived from a monolingual corpus. This monolingual knowledge makes it easier to determine corresponding words correctly. For instance, functional words in one language tend to correspond to functional words in another language (Deng and Gao, 2007), and the syntactic dependency of words in each language can help the alignment process (Ma et al., 2008). It has been shown that such grammatical 137 Proceedings of the ACL 2010 Conference Short Papers, pages 137–141, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics translation probability from {e to } f under the kth topic: p (f |e, z = k ). T = Ti,i′ is a state transition probability of a first order Markov process. Fig. 1 shows a graphical model of HM-BiTAM. The total likelihood of bilingual sentence pairs {E, F } can be obtained by marginalizing out latent variables z, a and θ, p (F, E; Ψ) = ∑∑ z p (F, E, z, a, θ; Ψ) dθ, where Ψ = {α, β, T, B} is a paramete"
P10-2025,P00-1056,0,0.0606791,"GIZA++ is an implementation of IBM-model 4 and HMM, and HM-BiTAM corresponds to ζ = 0 in eq. 7. We adopted K = 3 topics, following the setting in (Zhao and Xing, 2006). We trained the word alignment in two directions: English to French, and French to English. The alignment results for both directions were refined with ‘GROW’ heuristics to yield high precision and high recall in accordance with previous work (Och and Ney, 2003; Zhao and Xing, 2006). We evaluated these results for precision, recall, Fmeasure and alignment error rate (AER), which are standard metrics for word alignment accuracy (Och and Ney, 2000). Figure 2: Graphical model of synonym pair generative process correspond to the same word in a different language, thus they make it easy to infer accurate word alignment. HM-BiTAM and the synonym model share parameters in order to incorporate monolingual synonym information into the bilingual word alignment model. This can be achieved e in eq. 3 as, via reparameterizing Ψ ( ) e p f e, k; Ψ ( ) e p e, k; Ψ ≡ p (f |e, k; B ) , (4) ≡ p (e |k; β ) p (k; α) . (5) Overall, we re-define the synonym pair model with the HM-BiTAM parameter set Ψ, { } p( f, f ′ ; Ψ) ∏ ∑ 1 αk βk,e Bf,e,k Bf ′ ,e,k . (6)"
P10-2025,J03-1002,0,0.0178838,"d significantly improves word alignment quality. 1 Introduction Word alignment is an essential step in most phrase and syntax based statistical machine translation (SMT). It is an inference problem of word correspondences between different languages given parallel sentence pairs. Accurate word alignment can induce high quality phrase detection and translation probability, which leads to a significant improvement in SMT performance. Many word alignment approaches based on generative models have been proposed and they learn from bilingual sentences in an unsupervised manner (Vogel et al., 1996; Och and Ney, 2003; Fraser and Marcu, 2007). One way to improve word alignment quality is to add linguistic knowledge derived from a monolingual corpus. This monolingual knowledge makes it easier to determine corresponding words correctly. For instance, functional words in one language tend to correspond to functional words in another language (Deng and Gao, 2007), and the syntactic dependency of words in each language can help the alignment process (Ma et al., 2008). It has been shown that such grammatical 137 Proceedings of the ACL 2010 Conference Short Papers, pages 137–141, c Uppsala, Sweden, 11-16 July 201"
P10-2025,C96-2141,0,0.849951,"t our proposed method significantly improves word alignment quality. 1 Introduction Word alignment is an essential step in most phrase and syntax based statistical machine translation (SMT). It is an inference problem of word correspondences between different languages given parallel sentence pairs. Accurate word alignment can induce high quality phrase detection and translation probability, which leads to a significant improvement in SMT performance. Many word alignment approaches based on generative models have been proposed and they learn from bilingual sentences in an unsupervised manner (Vogel et al., 1996; Och and Ney, 2003; Fraser and Marcu, 2007). One way to improve word alignment quality is to add linguistic knowledge derived from a monolingual corpus. This monolingual knowledge makes it easier to determine corresponding words correctly. For instance, functional words in one language tend to correspond to functional words in another language (Deng and Gao, 2007), and the syntactic dependency of words in each language can help the alignment process (Ma et al., 2008). It has been shown that such grammatical 137 Proceedings of the ACL 2010 Conference Short Papers, pages 137–141, c Uppsala, Swe"
P10-2025,P05-1074,0,0.0405879,"t topic zn ∈ {1, . . . , k, . . . , K}, where K is the number of latent topics. Let N be the number of sentence pairs, and In and Jn be the lengths of En and Fn , respectively. In this framework, all of the bilingual sentence pairs {E, F } = {(En , Fn )}N n=1 are generated as follows. ( ) ∑ ( ) p f, f ′ ∝ p (f |s ) p f ′ |s p (s) . (2) s We define a pair (e, k) as a representation of the sense s, where e and k are a word in a different language E and a latent topic, respectively. It has been shown that a word e in a different language is an appropriate representation of s in synonym modeling (Bannard and Callison-Burch, 2005). We assume that adding a latent topic k for the sense is very useful for disambiguating word meaning, and thus that (e, k) gives us a good approximation of s. Under this assumption, the synonym pair generative model can be defined as follows. 1. θ ∼ Dirichlet (α): sample topic-weight vector 2. For each sentence pair (En , Fn ) (a) zn ∼ M ultinomial (θ): sample the topic (b) en,i:In |zn ∼ p (En |zn ; β ): sample English (c) (1) a words from a monolingual unigram model given topic zn For each position jn = 1, . . . , Jn i. ajn ∼ p (ajn |ajn −1 ; T ): sample an alignment link ajn from a first or"
P10-2025,P06-2124,0,0.0185093,"rom WordNet 2.1 (Miller, 1995) and WOLF 0.1.4 (Sagot and Fiser, 2008), respectively. WOLF is a semantic resource constructed from the Princeton WordNet and various multilingual resources. We selected synonym pairs where both words were included in the bilingual training set. We compared the word alignment performance of our model with that of GIZA++ 1.03 1 (Vogel et al., 1996; Och and Ney, 2003), and HMBiTAM (Zhao and Xing, 2008) implemented by us. GIZA++ is an implementation of IBM-model 4 and HMM, and HM-BiTAM corresponds to ζ = 0 in eq. 7. We adopted K = 3 topics, following the setting in (Zhao and Xing, 2006). We trained the word alignment in two directions: English to French, and French to English. The alignment results for both directions were refined with ‘GROW’ heuristics to yield high precision and high recall in accordance with previous work (Och and Ney, 2003; Zhao and Xing, 2006). We evaluated these results for precision, recall, Fmeasure and alignment error rate (AER), which are standard metrics for word alignment accuracy (Och and Ney, 2000). Figure 2: Graphical model of synonym pair generative process correspond to the same word in a different language, thus they make it easy to infer a"
P10-2025,W03-0301,0,\N,Missing
P10-2030,1993.iwpt-1.3,0,0.0339422,"rom time-type argument is difficult because the arguments of both types are often accompanied with the ‘ni’ case marker. A problem with such statistical learners as SVM is the lack of interpretability; if accuracy is low, we cannot identify the problems in the annotations. We are focusing on transformation-based learning (TBL). An advantage for such learning methods is that we can easily interpret the learned model. The tasks in most previous research are such simple tagging tasks as part-of-speech tagging, insertion and deletion of parentheses in syntactic parsing, and chunking (Brill, 1995; Brill, 1993; Ramshaw and Marcus, 1995). Here we experiment with a complex task: Japanese PASs. TBL can be slow, so we proposed an incremental training method to speed up the training. We experimented with a Japanese PAS corpus with a graph-based TBL. From the experiments, we interrelated the annotation tendency on the dataset. The rest of this paper is organized as follows. Section 2 describes Japanese predicate structure, our graph expression of it, and our improved method. The results of experiments using the NAIST Text Corpus, which is our target corpus, are reported in Section 3, and our conclusion i"
P10-2030,W07-1522,0,0.265388,"2004; Shen and Lapata, 2007), and summarization (Melli et al., 2005). Most recent approaches to predicate argument structure analysis are statistical machine learning methods such as support vector machines (SVMs)(Pradhan et al., 2004). For predicate argument structure analysis, we have the following representative large corpora: FrameNet (Fillmore et al., 2001), PropBank (Palmer et al., 2005), and NomBank (Meyers et al., 2004) in English, the Chinese PropBank (Xue, 2008) in Chinese, the GDA Corpus (Hashida, 2005), Kyoto Text Corpus Ver.4.0 (Kawahara et al., 2002), and the NAIST Text Corpus (Iida et al., 2007) in Japanese. 2 Predicate argument structure and graph transformation learning First, we illustrate the structure of a Japanese sentence in Fig. 1. In Japanese, we can divide a sentence into bunsetsu phrases (BP). A BP usually consists of one or more content words and zero, 162 Proceedings of the ACL 2010 Conference Short Papers, pages 162–167, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics Sentence BP BP Syntactic dependency between bunsetsus BP PRED BP CW FW Kare no He ’s BP BP BP BP CW FW CW FW CW FW CW mise de tabe ta okashi wa kinou eat PAST snack TOP y"
P10-2030,W07-0208,0,0.0398879,"Missing"
P10-2030,kawahara-etal-2002-construction,0,0.0232737,"1999), question answering (Narayanan and Harabagiu, 2004; Shen and Lapata, 2007), and summarization (Melli et al., 2005). Most recent approaches to predicate argument structure analysis are statistical machine learning methods such as support vector machines (SVMs)(Pradhan et al., 2004). For predicate argument structure analysis, we have the following representative large corpora: FrameNet (Fillmore et al., 2001), PropBank (Palmer et al., 2005), and NomBank (Meyers et al., 2004) in English, the Chinese PropBank (Xue, 2008) in Chinese, the GDA Corpus (Hashida, 2005), Kyoto Text Corpus Ver.4.0 (Kawahara et al., 2002), and the NAIST Text Corpus (Iida et al., 2007) in Japanese. 2 Predicate argument structure and graph transformation learning First, we illustrate the structure of a Japanese sentence in Fig. 1. In Japanese, we can divide a sentence into bunsetsu phrases (BP). A BP usually consists of one or more content words and zero, 162 Proceedings of the ACL 2010 Conference Short Papers, pages 162–167, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics Sentence BP BP Syntactic dependency between bunsetsus BP PRED BP CW FW Kare no He ’s BP BP BP BP CW FW CW FW CW FW CW mise"
P10-2030,W02-2016,0,0.138441,"Missing"
P10-2030,W04-2705,0,0.0364277,"m” and is an important base tool for such various text processing tasks as machine translation information extraction (Hirschman et al., 1999), question answering (Narayanan and Harabagiu, 2004; Shen and Lapata, 2007), and summarization (Melli et al., 2005). Most recent approaches to predicate argument structure analysis are statistical machine learning methods such as support vector machines (SVMs)(Pradhan et al., 2004). For predicate argument structure analysis, we have the following representative large corpora: FrameNet (Fillmore et al., 2001), PropBank (Palmer et al., 2005), and NomBank (Meyers et al., 2004) in English, the Chinese PropBank (Xue, 2008) in Chinese, the GDA Corpus (Hashida, 2005), Kyoto Text Corpus Ver.4.0 (Kawahara et al., 2002), and the NAIST Text Corpus (Iida et al., 2007) in Japanese. 2 Predicate argument structure and graph transformation learning First, we illustrate the structure of a Japanese sentence in Fig. 1. In Japanese, we can divide a sentence into bunsetsu phrases (BP). A BP usually consists of one or more content words and zero, 162 Proceedings of the ACL 2010 Conference Short Papers, pages 162–167, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computatio"
P10-2030,C04-1100,0,0.0198359,"arned rules. A disadvantage is that the rule extraction procedure is time-consuming. We present incremental-based, transformation-based learning for semantic processing tasks. As an example, we deal with Japanese predicate argument analysis and show some tendencies of annotators for constructing a corpus with our method. 1 Introduction Automatic predicate argument structure analysis (PAS) provides information of “who did what to whom” and is an important base tool for such various text processing tasks as machine translation information extraction (Hirschman et al., 1999), question answering (Narayanan and Harabagiu, 2004; Shen and Lapata, 2007), and summarization (Melli et al., 2005). Most recent approaches to predicate argument structure analysis are statistical machine learning methods such as support vector machines (SVMs)(Pradhan et al., 2004). For predicate argument structure analysis, we have the following representative large corpora: FrameNet (Fillmore et al., 2001), PropBank (Palmer et al., 2005), and NomBank (Meyers et al., 2004) in English, the Chinese PropBank (Xue, 2008) in Chinese, the GDA Corpus (Hashida, 2005), Kyoto Text Corpus Ver.4.0 (Kawahara et al., 2002), and the NAIST Text Corpus (Iida"
P10-2030,J05-1004,0,0.009844,"information of “who did what to whom” and is an important base tool for such various text processing tasks as machine translation information extraction (Hirschman et al., 1999), question answering (Narayanan and Harabagiu, 2004; Shen and Lapata, 2007), and summarization (Melli et al., 2005). Most recent approaches to predicate argument structure analysis are statistical machine learning methods such as support vector machines (SVMs)(Pradhan et al., 2004). For predicate argument structure analysis, we have the following representative large corpora: FrameNet (Fillmore et al., 2001), PropBank (Palmer et al., 2005), and NomBank (Meyers et al., 2004) in English, the Chinese PropBank (Xue, 2008) in Chinese, the GDA Corpus (Hashida, 2005), Kyoto Text Corpus Ver.4.0 (Kawahara et al., 2002), and the NAIST Text Corpus (Iida et al., 2007) in Japanese. 2 Predicate argument structure and graph transformation learning First, we illustrate the structure of a Japanese sentence in Fig. 1. In Japanese, we can divide a sentence into bunsetsu phrases (BP). A BP usually consists of one or more content words and zero, 162 Proceedings of the ACL 2010 Conference Short Papers, pages 162–167, c Uppsala, Sweden, 11-16 July 20"
P10-2030,N04-1030,0,0.0426591,"s and show some tendencies of annotators for constructing a corpus with our method. 1 Introduction Automatic predicate argument structure analysis (PAS) provides information of “who did what to whom” and is an important base tool for such various text processing tasks as machine translation information extraction (Hirschman et al., 1999), question answering (Narayanan and Harabagiu, 2004; Shen and Lapata, 2007), and summarization (Melli et al., 2005). Most recent approaches to predicate argument structure analysis are statistical machine learning methods such as support vector machines (SVMs)(Pradhan et al., 2004). For predicate argument structure analysis, we have the following representative large corpora: FrameNet (Fillmore et al., 2001), PropBank (Palmer et al., 2005), and NomBank (Meyers et al., 2004) in English, the Chinese PropBank (Xue, 2008) in Chinese, the GDA Corpus (Hashida, 2005), Kyoto Text Corpus Ver.4.0 (Kawahara et al., 2002), and the NAIST Text Corpus (Iida et al., 2007) in Japanese. 2 Predicate argument structure and graph transformation learning First, we illustrate the structure of a Japanese sentence in Fig. 1. In Japanese, we can divide a sentence into bunsetsu phrases (BP). A BP"
P10-2030,W94-0111,0,0.202072,"Missing"
P10-2030,W95-0107,0,0.105855,"argument is difficult because the arguments of both types are often accompanied with the ‘ni’ case marker. A problem with such statistical learners as SVM is the lack of interpretability; if accuracy is low, we cannot identify the problems in the annotations. We are focusing on transformation-based learning (TBL). An advantage for such learning methods is that we can easily interpret the learned model. The tasks in most previous research are such simple tagging tasks as part-of-speech tagging, insertion and deletion of parentheses in syntactic parsing, and chunking (Brill, 1995; Brill, 1993; Ramshaw and Marcus, 1995). Here we experiment with a complex task: Japanese PASs. TBL can be slow, so we proposed an incremental training method to speed up the training. We experimented with a Japanese PAS corpus with a graph-based TBL. From the experiments, we interrelated the annotation tendency on the dataset. The rest of this paper is organized as follows. Section 2 describes Japanese predicate structure, our graph expression of it, and our improved method. The results of experiments using the NAIST Text Corpus, which is our target corpus, are reported in Section 3, and our conclusion is provided in Section 4. Ma"
P10-2030,D07-1002,0,0.0198311,"that the rule extraction procedure is time-consuming. We present incremental-based, transformation-based learning for semantic processing tasks. As an example, we deal with Japanese predicate argument analysis and show some tendencies of annotators for constructing a corpus with our method. 1 Introduction Automatic predicate argument structure analysis (PAS) provides information of “who did what to whom” and is an important base tool for such various text processing tasks as machine translation information extraction (Hirschman et al., 1999), question answering (Narayanan and Harabagiu, 2004; Shen and Lapata, 2007), and summarization (Melli et al., 2005). Most recent approaches to predicate argument structure analysis are statistical machine learning methods such as support vector machines (SVMs)(Pradhan et al., 2004). For predicate argument structure analysis, we have the following representative large corpora: FrameNet (Fillmore et al., 2001), PropBank (Palmer et al., 2005), and NomBank (Meyers et al., 2004) in English, the Chinese PropBank (Xue, 2008) in Chinese, the GDA Corpus (Hashida, 2005), Kyoto Text Corpus Ver.4.0 (Kawahara et al., 2002), and the NAIST Text Corpus (Iida et al., 2007) in Japanes"
P10-2030,J08-2004,0,0.0234173,"processing tasks as machine translation information extraction (Hirschman et al., 1999), question answering (Narayanan and Harabagiu, 2004; Shen and Lapata, 2007), and summarization (Melli et al., 2005). Most recent approaches to predicate argument structure analysis are statistical machine learning methods such as support vector machines (SVMs)(Pradhan et al., 2004). For predicate argument structure analysis, we have the following representative large corpora: FrameNet (Fillmore et al., 2001), PropBank (Palmer et al., 2005), and NomBank (Meyers et al., 2004) in English, the Chinese PropBank (Xue, 2008) in Chinese, the GDA Corpus (Hashida, 2005), Kyoto Text Corpus Ver.4.0 (Kawahara et al., 2002), and the NAIST Text Corpus (Iida et al., 2007) in Japanese. 2 Predicate argument structure and graph transformation learning First, we illustrate the structure of a Japanese sentence in Fig. 1. In Japanese, we can divide a sentence into bunsetsu phrases (BP). A BP usually consists of one or more content words and zero, 162 Proceedings of the ACL 2010 Conference Short Papers, pages 162–167, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics Sentence BP BP Syntactic depe"
P10-2030,Y01-1001,0,\N,Missing
P10-2030,J95-4004,0,\N,Missing
P11-2036,P10-2042,0,0.211575,"r. The base distribution over initial trees is defined as P0 (e |X ), and the base distribution over simple auxiliary trees is defined as P00 (e |X ). An initial tree ei replaces a frontier node with probability p (ei |e−i , X, dX , θX ). On the other hand, a simple auxiliary tree e0 i inserts an internal node  0 , where with probability aX ×p0 e0i e0−i , X, d0X , θX aX is an insertion probability defined for each X. The stopping probabilities are common to both initial and auxiliary trees. 3.2 Grammar Decomposition We develop a grammar decomposition technique, which is an extension of work (Cohn and Blunsom, 2010) on BTSG model, to deal with an insertion operator. The motivation behind grammar decomposition is that it is hard to consider all possible 208 ins (N girl) probability ins (N girl) → the (N girl) → Nins (N (JJ pretty) N*) (1 − aDT ) × aN 1 0 α(N (JJ pretty) N*),N (N girl) Nins (N (JJ pretty) N*) → JJ(JJ pretty) N(N girl) JJ(JJ pretty) → pretty 1 N(N girl) → girl 1 (1 − aJJ ) × 1 Table 1: The rules and probabilities of grammar decomposition for Fig. 2. derivations explicitly since the base distribution assigns non-zero probability to an infinite number of initial and auxiliary trees. Alternati"
P11-2036,N09-1036,0,0.0317833,"0 (ei , |X ) , (1) where αei ,X θX +dX ·t·,X . θX +n−i ·,X = n−i e ,X −dX ·tei ,X i and βX θX +n−i ·,X = e−i = e1 , . . . , ei−1 are previously generated initial trees, and n−i ei ,X is the number of times ei has been used in e−i . tei ,X is the number of taP bles labeled with ei . n−i = e n−i ·,X e,X and t·,X = P e te,X are the total counts of initial trees and tables, respectively. The PYP prior produces “rich get richer” statistics: a few initial trees are often used for derivation while many are rarely used, and this is shown empirically to be well-suited for natural language (Teh, 2006b; Johnson and Goldwater, 2009). The base probability of an initial tree, P0 (e |X ), is given as follows. (a) (b) P0 (e |X ) = Y PMLE (r) × r∈CFG(e) × Y Y sA Figure 1: Example of (a) substitution and (b) insertion (dotted line). A∈LEAF(e) (1 − sB ) , (2) B∈INTER(e) where CFG (e) is a set of decomposed CFG productions of e, PMLE (r) is a maximum likelihood estimate (MLE) of r. LEAF (e) and INTER (e) are sets of leaf and internal symbols of e, respectively. sX is a stopping probability defined for each X. 3 3.1 Insertion Operator for BTSG Tree Insertion Model We propose a model that incorporates an insertion operator in BTSG"
P11-2036,P05-1010,0,0.0343211,"n a top-down manner. 3. Accept or reject the derivation sample by using the MH test. The MH algorithm is described in detail in (Cohn and Blunsom, 2010). The hyperparameters of our model are updated with the auxiliary variable technique (Teh, 2006a). 4 method CFG BTSG BTSG + insertion CFG BTSG BTSG + insertion Experiments We ran experiments on the British National Corpus (BNC) Treebank 3 and the WSJ English Penn Treebank. We did not use a development set since our model automatically updates the hyperparameters for every iteration. The treebank data was binarized using the CENTER-HEAD method (Matsuzaki et al., 2005). We replaced lexical words with counts ≤ 1 in the training set with one of three unknown 1 Results from (Cohn and Blunsom, 2010). Results for length ≤ 40. 3 http://nclt.computing.dcu.ie/~jfoster/resources/ 2 (Cohn and Blunsom, 2010) Table 3: Full Penn Treebank dataset experiments words using lexical features. We trained our model using a training set, and then sampled 10k derivations for each sentence in a test set. Parsing results were obtained with the MER algorithm (Cohn et al., 2011) using the 10k derivation samples. We show the bracketing F1 score of predicted parse trees evaluated by EV"
P11-2036,P06-1055,0,0.264362,"Missing"
P11-2036,P09-2012,0,0.0811452,"d JJ, respectively. Note that the probability of a derivation according to Table 1 is the same as the probability of a derivation obtained from the distribution over the initial and auxiliary trees (i.e. eq. 1 and eq. 3). In Table 1, we assume that the auxiliary tree “(N (JJ pretty) (N*))” is sampled from the first term of eq. 3. When it is sampled from the second term, we alternatively assign the probability 0 β(N (JJ pretty) N*), N . 3.3 Table 2: Small dataset experiments CFG BTSG BTSG + insertion Training # rules (# aux. trees) 35374 (-) 80026 (0) 65099 (25) F1 71.0 85.0 85.3 - 82.62 85.3 (Post and Gildea, 2009) We use a blocked Metropolis-Hastings (MH) algorithm (Cohn and Blunsom, 2010) to train our model. The MH algorithm learns BTSG model parameters efficiently, and it can be applied to our insertion model. The MH algorithm consists of the following three steps. For each sentence, 1. Calculate the inside probability (Lari and Young, 1991) in a bottom-up manner using the grammar decomposition. 2. Sample a derivation tree in a top-down manner. 3. Accept or reject the derivation sample by using the MH test. The MH algorithm is described in detail in (Cohn and Blunsom, 2010). The hyperparameters of ou"
P11-2036,J95-4002,0,0.690892,"that our model outperforms a standard PCFG and BTSG for a small dataset. For a large dataset, our model obtains comparable results to BTSG, making the number of grammar rules much smaller than with BTSG. 2 1 Introduction Tree substitution grammar (TSG) is a promising formalism for modeling language data. TSG generalizes context free grammars (CFG) by allowing nonterminal nodes to be replaced with subtrees of arbitrary size. A natural extension of TSG involves adding an insertion operator for combining subtrees as in tree adjoining grammars (TAG) (Joshi, 1985) or tree insertion grammars (TIG) (Schabes and Waters, 1995). An insertion operator is helpful for expressing various syntax patterns with fewer grammar rules, thus we expect that adding an insertion operator will improve parsing accuracy and realize a compact grammar size. One of the challenges of adding an insertion operator is that the computational cost of grammar induction is high since tree insertion significantly increases the number of possible subtrees. Previous work on TAG and TIG induction (Xia, 1999; Chiang, 2003; Chen et al., 2006) has addressed the problem using language-specific heuristics and a maxiOverview of BTSG Model We briefly revi"
P11-2036,P06-1124,0,0.304811,"ei ,X + βX P0 (ei , |X ) , (1) where αei ,X θX +dX ·t·,X . θX +n−i ·,X = n−i e ,X −dX ·tei ,X i and βX θX +n−i ·,X = e−i = e1 , . . . , ei−1 are previously generated initial trees, and n−i ei ,X is the number of times ei has been used in e−i . tei ,X is the number of taP bles labeled with ei . n−i = e n−i ·,X e,X and t·,X = P e te,X are the total counts of initial trees and tables, respectively. The PYP prior produces “rich get richer” statistics: a few initial trees are often used for derivation while many are rarely used, and this is shown empirically to be well-suited for natural language (Teh, 2006b; Johnson and Goldwater, 2009). The base probability of an initial tree, P0 (e |X ), is given as follows. (a) (b) P0 (e |X ) = Y PMLE (r) × r∈CFG(e) × Y Y sA Figure 1: Example of (a) substitution and (b) insertion (dotted line). A∈LEAF(e) (1 − sB ) , (2) B∈INTER(e) where CFG (e) is a set of decomposed CFG productions of e, PMLE (r) is a maximum likelihood estimate (MLE) of r. LEAF (e) and INTER (e) are sets of leaf and internal symbols of e, respectively. sX is a stopping probability defined for each X. 3 3.1 Insertion Operator for BTSG Tree Insertion Model We propose a model that incorporate"
P11-2036,P00-1058,0,\N,Missing
P11-2075,D08-1014,0,0.30279,"Missing"
P11-2075,W06-1615,0,0.0877732,"(x, y) = ps (y|x)ps (x) be the corresponding source distribution. We assume that one (or both) of the following distributions differ between source and target: • Instance mismatch: ps (x) 6= pt (x). • Labeling mismatch: ps (y|x) 6= pt (y|x). Instance mismatch implies that the input feature vectors have different distribution (e.g. one dataset uses the word “excellent” often, while the other uses the word “awesome”). This degrades performance because classifiers trained on “excellent” might not know how to classify texts with the word “awesome.” The solution is to tie together these features (Blitzer et al., 2006) or re-weight the input distribution (Sugiyama et al., 2008). Under some assumptions (i.e. covariate shift), oracle accuracy can be achieved theoretically (Shimodaira, 2000). Labeling mismatch implies the same input has different labels in different domains. For example, the JP word meaning “excellent” may be mistranslated as “bad” in English. Then, positive JP 5 See “Adapt by Language” columns of Table 2. Note JP+FR+DE condition has 6000 labeled samples, so is not directly comparable to other adaptation scenarios (2000 samples). Nevertheless, mixing languages seem to give good results. 6 See"
P11-2075,N09-1068,0,0.0223374,"he same market and same language domain as the target. “JP+FR+DE” indicates the concatenation of JP, FR, DE as source data. Boldface shows the winner of Supervised vs. Adapted. reviews will be associated with the word “bad”: ps (y = +1|x = bad) will be high, whereas the true conditional distribution should have high pt (y = −1|x = bad) instead. There are several cases for labeling mismatch, depending on how the polarity changes (Table 3). The solution is to filter out these noisy samples (Jiang and Zhai, 2007) or optimize loosely-linked objectives through shared parameters or Bayesian priors (Finkel and Manning, 2009). Which mismatch is responsible for accuracy degradations in cross-lingual adaptation? To measure instance mismatch, we compute statistics between ps (x) and pt (x), or approximations thereof: First, we calculate a (normalized) average feature from all samples of source S, which represents the unigram distribution of MT output. Similarly, the average feature vector for target T approximates the unigram distribution of English reviews pt (x). Then we measure: • Instance mismatch: Systematic MT bias generates word distributions different from naturallyoccurring English. (Translation may be valid"
P11-2075,P07-1034,0,0.247664,"d language mismatch are comparable in several cases (e.g. MUSIC-DE and DVD-EN perform similarly for target MUSIC-EN). Observation 2: The ranking of source language by decreasing accuracy is DE &gt; FR &gt; JP. Does this mean JP-EN is a more difficult language pair for MT? The next section will show that this is not necessarily the case. Certainly, the domain mismatch for JP is larger than DE, but this could be due to phenomenon other than MT errors. 4 Where exactly is the domain mismatch? 4.1 Theory of Domain Adaptation We analyze domain adaptation by the concepts of labeling and instance mismatch (Jiang and Zhai, 2007). Let pt (x, y) = pt (y|x)pt (x) be the target distribution of samples x (e.g. unigram feature vector) and labels y (positive / negative). Let ps (x, y) = ps (y|x)ps (x) be the corresponding source distribution. We assume that one (or both) of the following distributions differ between source and target: • Instance mismatch: ps (x) 6= pt (x). • Labeling mismatch: ps (y|x) 6= pt (y|x). Instance mismatch implies that the input feature vectors have different distribution (e.g. one dataset uses the word “excellent” often, while the other uses the word “awesome”). This degrades performance because"
P11-2075,P10-1114,0,0.0821553,"Missing"
P11-2075,P09-1027,0,0.423747,"Missing"
P11-2075,P10-2048,0,0.0762766,"Missing"
P11-2112,P05-1001,0,0.758669,"orrelation with the best ˆ given by the base supervised model. This output y implies that fn is an informative and potent feature in the model. Then, the distribution of fn has very ˆ if |VD (fn )| small (or no) correlation to determine y is zero or near zero. In this case, fn can be evaluated as an uninformative feature in the model. From this perspective, we treat VD (fn ) as a measure of feature potency in terms of the base supervised model. The essence of this idea, evaluating features against each other on a certain model, is widely used in the context of semi-supervised learning, i.e., (Ando and Zhang, 2005; Suzuki and Isozaki, We define VD∗ (fn ) as VD∗ (fn ) = dδVD0 (fn )e if VD0 (fn ) > 0 and VD∗ (fn ) = bδVD0 (fn )c otherwise, where δ is a positive user-specified constant. Note that VD∗ (fn ) always becomes an integer, that is, VD∗ (fn ) ∈ N where N = {. . . , −2, −1, 0, 1, 2, . . .}. This calculation can be seen as mapping each feature into a discrete (integer) space with respect to VD0 (fn ). δ controls the range of VD0 (fn ) mapping into the same integer. with the same (similar) feature potency are given the same weight by supervised learning since they have ˆ. δ the same potency with reg"
P11-2112,D09-1060,0,0.211446,"e summation of a data-wise calculation (map phase), and VD∗ (fn ) can be calculated independently by each feature (reduce phase). We emphasize that our feature potency estimation can be performed in a ‘single’ map-reduce process. 4 Experiments We conducted experiments on two different NLP tasks, namely NER and dependency parsing. To facilitate comparisons with the performance of previous methods, we adopted the experimental settings used to examine high-performance semi-supervised NLP systems; i.e., NER (Ando and Zhang, 2005; Suzuki and Isozaki, 2008) and dependency parsing (Koo et al., 2008; Chen et al., 2009; Suzuki et al., 2009). For the supervised datasets, we used CoNLL’03 (Tjong Kim Sang and De Meulder, 2003) shared task data for NER, and the Penn Treebank III 639 (PTB) corpus (Marcus et al., 1994) for dependency parsing. We prepared a total of 3.72 billion token text data as unsupervised data following the instructions given in (Suzuki et al., 2009). 4.1 Comparative Methods We mainly compare the effectiveness of COFER with that of CWR derived by the Brown algorithm. The iCWR approach yields the state-of-the-art results with both dependency parsing data derived from PTB-III (Koo et al., 2008)"
P11-2112,P10-1001,0,0.0505475,"Missing"
P11-2112,P08-1068,0,0.618373,"dels of many natural language processing (NLP) systems. One simple but powerful approach for further enhancing the performance is to utilize a large amount of unsupervised data to supplement supervised data. Specifically, an approach that involves incorporating ‘clusteringbased word representations (CWR)’ induced from unsupervised data as additional features of supervised learning has demonstrated substantial performance gains over state-of-the-art supervised learning systems in typical NLP tasks, such as named entity recognition (Lin and Wu, 2009; Turian et al., 2010) and dependency parsing (Koo et al., 2008). We refer to this approach as the iCWR approach, The iCWR approach has become popular for enhancement because of its simplicity and generality. The goal of this paper is to provide yet another simple and general framework, like the iCWR approach, to enhance existing state-of-the-art supervised NLP systems. The differences between the iCWR approach and our method are as follows; suppose F is the original feature set used in supervised learning, C is the CWR feature set, and H is the new feature set generated by our method. Then, with the iCWR approach, C is induced independently from F, and us"
P11-2112,P09-1116,0,0.546608,", supervised learning has become a standard way to train the models of many natural language processing (NLP) systems. One simple but powerful approach for further enhancing the performance is to utilize a large amount of unsupervised data to supplement supervised data. Specifically, an approach that involves incorporating ‘clusteringbased word representations (CWR)’ induced from unsupervised data as additional features of supervised learning has demonstrated substantial performance gains over state-of-the-art supervised learning systems in typical NLP tasks, such as named entity recognition (Lin and Wu, 2009; Turian et al., 2010) and dependency parsing (Koo et al., 2008). We refer to this approach as the iCWR approach, The iCWR approach has become popular for enhancement because of its simplicity and generality. The goal of this paper is to provide yet another simple and general framework, like the iCWR approach, to enhance existing state-of-the-art supervised NLP systems. The differences between the iCWR approach and our method are as follows; suppose F is the original feature set used in supervised learning, C is the CWR feature set, and H is the new feature set generated by our method. Then, w"
P11-2112,D10-1004,0,0.034847,"Missing"
P11-2112,W09-1119,0,0.107647,"Missing"
P11-2112,P08-1076,1,0.954742,"mputing framework. This is because Rn and An can be calculated by the summation of a data-wise calculation (map phase), and VD∗ (fn ) can be calculated independently by each feature (reduce phase). We emphasize that our feature potency estimation can be performed in a ‘single’ map-reduce process. 4 Experiments We conducted experiments on two different NLP tasks, namely NER and dependency parsing. To facilitate comparisons with the performance of previous methods, we adopted the experimental settings used to examine high-performance semi-supervised NLP systems; i.e., NER (Ando and Zhang, 2005; Suzuki and Isozaki, 2008) and dependency parsing (Koo et al., 2008; Chen et al., 2009; Suzuki et al., 2009). For the supervised datasets, we used CoNLL’03 (Tjong Kim Sang and De Meulder, 2003) shared task data for NER, and the Penn Treebank III 639 (PTB) corpus (Marcus et al., 1994) for dependency parsing. We prepared a total of 3.72 billion token text data as unsupervised data following the instructions given in (Suzuki et al., 2009). 4.1 Comparative Methods We mainly compare the effectiveness of COFER with that of CWR derived by the Brown algorithm. The iCWR approach yields the state-of-the-art results with both dep"
P11-2112,D09-1058,1,0.917404,"Missing"
P11-2112,W03-0419,0,0.288854,"Missing"
P11-2112,P09-1054,0,0.1595,"arning. 3.2 Feature potency discounting To discount low potency values, we redefine feature potency as VD0 (fn ) instead of VD (fn ) as follows:   log [Rn +C]−log[An ] if Rn −An &lt; −C if − C ≤ Rn −An ≤ C VD0 (fn ) = 0  log [Rn −C]−log[An ] if C &lt; Rn −An where Rn and An are defined in Figure 2. Note that VD (fn ) = VD+ (fn ) − VD− (fn ) = Rn − An . The difference from VD (fn ) is that we cast it in the log-domain and introduce a non-negative constant C. The introduction of C is inspired by the L1 regularization technique used in supervised learning algorithms such as (Duchi and Singer, 2009; Tsuruoka et al., 2009). C controls how much we discount VD (fn ) toward zero, and is given by the user. 3.3 Feature potency quantization 638 and given input x. We write z ∈ y when the local sub-structure z is a part of output y, assuming that output y is constructed by a set of local substructures. Then formally, the ∑ n-th feature is written as fn (x, z), and fn (x, y) = z∈y fn (x, z) holds. Similarly, we introduce r(x, z), where r(x, z) = 1 ˆ , and r(x, z) = 0 otherwise, namely z ∈ ˆ. if z ∈ y /y We define Z(x) as the set of all local substructures possibly generated for all y in Y(x). Z(x) can be enumerated easi"
P11-2112,P10-1040,0,0.749404,"ing has become a standard way to train the models of many natural language processing (NLP) systems. One simple but powerful approach for further enhancing the performance is to utilize a large amount of unsupervised data to supplement supervised data. Specifically, an approach that involves incorporating ‘clusteringbased word representations (CWR)’ induced from unsupervised data as additional features of supervised learning has demonstrated substantial performance gains over state-of-the-art supervised learning systems in typical NLP tasks, such as named entity recognition (Lin and Wu, 2009; Turian et al., 2010) and dependency parsing (Koo et al., 2008). We refer to this approach as the iCWR approach, The iCWR approach has become popular for enhancement because of its simplicity and generality. The goal of this paper is to provide yet another simple and general framework, like the iCWR approach, to enhance existing state-of-the-art supervised NLP systems. The differences between the iCWR approach and our method are as follows; suppose F is the original feature set used in supervised learning, C is the CWR feature set, and H is the new feature set generated by our method. Then, with the iCWR approach,"
P11-2112,J93-2004,0,\N,Missing
P12-1001,P07-1111,0,0.0175966,"tion of BLEU/TER is the most common technique (Zaidan, 2009), sometimes achieving good results in evaluation campaigns (Dyer et al., 2009). As far as we known, the only work that directly proposes a multi-objective technique is (He and Way, 2009), which modifies MERT to optimize a single metric subject to the constraint that it does not degrade others. These approaches all require some setting of constraint strength or combination weights {pk }. Recent work in MT evaluation has examined combining metrics using machine learning for better correlation with human judgments (Liu and Gildea, 2007; Albrecht and Hwa, 2007; Gimnez and M`arquez, 2008) and may give insights for setting {pk }. We view our Pareto-based approach as orthogonal to these efforts. The tunability of metrics is a problem that is gaining recognition (Liu et al., 2011). If a good evaluation metric could not be used for tuning, it would be a pity. The Tunable Metrics task at WMT2011 concluded that BLEU is still the easiest to tune (Callison-Burch et al., 2011). (Mauser et al., 2008; Cer et al., 2010) report similar observations, in addition citing WER being difficult and BLEU-TER being amenable. One unsolved question is whether metric tunabi"
P12-1001,W11-2103,0,0.162556,"These methods are effective because they tune the system to maximize an automatic evaluation metric such as BLEU, which serve as surrogate objective for translation quality. However, we know that a single metric such as BLEU is not enough. Ideally, we want to tune towards an automatic metric that has perfect correlation with human judgments of translation quality. ∗ *Now at Nara Institute of Science & Technology (NAIST) While many alternatives have been proposed, such a perfect evaluation metric remains elusive. As a result, many MT evaluation campaigns now report multiple evaluation metrics (Callison-Burch et al., 2011; Paul, 2010). Different evaluation metrics focus on different aspects of translation quality. For example, while BLEU (Papineni et al., 2002) focuses on word-based n-gram precision, METEOR (Lavie and Agarwal, 2007) allows for stem/synonym matching and incorporates recall. TER (Snover et al., 2006) allows arbitrary chunk movements, while permutation metrics like RIBES (Isozaki et al., 2010; Birch et al., 2010) measure deviation in word order. Syntax (Owczarzak et al., 2007) and semantics (Pado et al., 2009) also help. Arguably, all these metrics correspond to our intuitions on what is a good t"
P12-1001,N10-1080,0,0.0600026,"in MT evaluation has examined combining metrics using machine learning for better correlation with human judgments (Liu and Gildea, 2007; Albrecht and Hwa, 2007; Gimnez and M`arquez, 2008) and may give insights for setting {pk }. We view our Pareto-based approach as orthogonal to these efforts. The tunability of metrics is a problem that is gaining recognition (Liu et al., 2011). If a good evaluation metric could not be used for tuning, it would be a pity. The Tunable Metrics task at WMT2011 concluded that BLEU is still the easiest to tune (Callison-Burch et al., 2011). (Mauser et al., 2008; Cer et al., 2010) report similar observations, in addition citing WER being difficult and BLEU-TER being amenable. One unsolved question is whether metric tunability is a problem inherent to the metric only, or depends also on the underlying optimization algorithm. Our positive results with PMO suggest that the choice of optimization algorithm can help. Multi-objective ideas are being explored in other NLP areas. (Spitkovsky et al., 2011) describe a technique that alternates between hard and soft EM objectives in order to achieve better local optimum in grammar induction. (Hall et al., 2011) investigates joint"
P12-1001,N09-1025,0,0.0389566,", so the method not only learns to discriminate pareto vs. non-pareto but also also learns to discriminate among competing non-pareto points. Also, like other MT works, in line 5 the N-best list is concatenated to N-best lists from previous iterations, so {h} is a set with i · N elements. General PMO Approach: The strategy we outlined in Section 3.2 can be easily applied to other MT optimization techniques. For example, by replacing the optimization subroutine (line 10, Algorithm 2) with a Powell search (Och, 2003), one can get PMO-MERT4 . Alternatively, by using the largemargin optimizer in (Chiang et al., 2009) and moving it into the for-each loop (lines 4-9), one can get an online algorithm such PMO-MIRA. Virtually all MT optimization algorithms have a place where metric scores feedback into the optimization procedure; the idea of PMO is to replace these raw scores with labels derived from Pareto optimality. 4 Experiments 4.1 Evaluation Methodology We experiment with two datasets: (1) The PubMed task is English-to-Japanese translation of scientific 4 A difference with traditional MERT is the necessity of sentence-BLEU (Liang et al., 2006) in line 6. We use sentenceBLEU for optimization but corpus-B"
P12-1001,W09-0426,0,0.266662,"ts. In this case, we recommend the following trick: Set up a multi-objective problem where one metric is BLEU and the other is 3/4BLEU+1/4RIBES. This encourages PMO to explore the joint metric space but avoid solutions that sacrifice too much BLEU, and should also outperform Linear Combination that searches only on the (3/4,1/4) direction. 5 How many Pareto points? The number of pareto 7 Related Work Multi-objective optimization for MT is a relatively new area. Linear-combination of BLEU/TER is the most common technique (Zaidan, 2009), sometimes achieving good results in evaluation campaigns (Dyer et al., 2009). As far as we known, the only work that directly proposes a multi-objective technique is (He and Way, 2009), which modifies MERT to optimize a single metric subject to the constraint that it does not degrade others. These approaches all require some setting of constraint strength or combination weights {pk }. Recent work in MT evaluation has examined combining metrics using machine learning for better correlation with human judgments (Liu and Gildea, 2007; Albrecht and Hwa, 2007; Gimnez and M`arquez, 2008) and may give insights for setting {pk }. We view our Pareto-based approach as orthogona"
P12-1001,I08-1042,0,0.0478355,"Missing"
P12-1001,D11-1138,0,0.0324528,"auser et al., 2008; Cer et al., 2010) report similar observations, in addition citing WER being difficult and BLEU-TER being amenable. One unsolved question is whether metric tunability is a problem inherent to the metric only, or depends also on the underlying optimization algorithm. Our positive results with PMO suggest that the choice of optimization algorithm can help. Multi-objective ideas are being explored in other NLP areas. (Spitkovsky et al., 2011) describe a technique that alternates between hard and soft EM objectives in order to achieve better local optimum in grammar induction. (Hall et al., 2011) investigates joint optimization of a supervised parsing objective and some extrinsic objectives based on downstream applications. (Agarwal et al., 2011) considers using multiple signals (of varying quality) from online users to train recommendation models. (Eisner and Daum´e III, 2011) trades off speed and accuracy of a parser with reinforcement learning. None of the techniques in NLP use Pareto concepts, however. 6 Opportunities and Limitations We introduce a new approach (PMO) for training MT systems on multiple metrics. Leveraging the diverse perspectives of different evaluation metrics ha"
P12-1001,2009.mtsummit-posters.8,0,0.158214,"and the other is 3/4BLEU+1/4RIBES. This encourages PMO to explore the joint metric space but avoid solutions that sacrifice too much BLEU, and should also outperform Linear Combination that searches only on the (3/4,1/4) direction. 5 How many Pareto points? The number of pareto 7 Related Work Multi-objective optimization for MT is a relatively new area. Linear-combination of BLEU/TER is the most common technique (Zaidan, 2009), sometimes achieving good results in evaluation campaigns (Dyer et al., 2009). As far as we known, the only work that directly proposes a multi-objective technique is (He and Way, 2009), which modifies MERT to optimize a single metric subject to the constraint that it does not degrade others. These approaches all require some setting of constraint strength or combination weights {pk }. Recent work in MT evaluation has examined combining metrics using machine learning for better correlation with human judgments (Liu and Gildea, 2007; Albrecht and Hwa, 2007; Gimnez and M`arquez, 2008) and may give insights for setting {pk }. We view our Pareto-based approach as orthogonal to these efforts. The tunability of metrics is a problem that is gaining recognition (Liu et al., 2011). I"
P12-1001,D11-1125,0,0.225156,"Our approach is based on the theory of Pareto Optimality. It is simple to implement on top of existing single-objective optimization methods (e.g. MERT, PRO) and outperforms ad hoc alternatives based on linear-combination of metrics. We also discuss the issue of metric tunability and show that our Pareto approach is more effective in incorporating new metrics from MT evaluation for MT optimization. 1 Introduction Weight optimization is an important step in building machine translation (MT) systems. Discriminative optimization methods such as MERT (Och, 2003), MIRA (Crammer et al., 2006), PRO (Hopkins and May, 2011), and Downhill-Simplex (Nelder and Mead, 1965) have been influential in improving MT systems in recent years. These methods are effective because they tune the system to maximize an automatic evaluation metric such as BLEU, which serve as surrogate objective for translation quality. However, we know that a single metric such as BLEU is not enough. Ideally, we want to tune towards an automatic metric that has perfect correlation with human judgments of translation quality. ∗ *Now at Nara Institute of Science & Technology (NAIST) While many alternatives have been proposed, such a perfect evaluat"
P12-1001,D10-1092,1,0.861646,"nce & Technology (NAIST) While many alternatives have been proposed, such a perfect evaluation metric remains elusive. As a result, many MT evaluation campaigns now report multiple evaluation metrics (Callison-Burch et al., 2011; Paul, 2010). Different evaluation metrics focus on different aspects of translation quality. For example, while BLEU (Papineni et al., 2002) focuses on word-based n-gram precision, METEOR (Lavie and Agarwal, 2007) allows for stem/synonym matching and incorporates recall. TER (Snover et al., 2006) allows arbitrary chunk movements, while permutation metrics like RIBES (Isozaki et al., 2010; Birch et al., 2010) measure deviation in word order. Syntax (Owczarzak et al., 2007) and semantics (Pado et al., 2009) also help. Arguably, all these metrics correspond to our intuitions on what is a good translation. The current approach of optimizing MT towards a single metric runs the risk of sacrificing other metrics. Can we really claim that a system is good if it has high BLEU, but very low METEOR? Similarly, is a high-METEOR low-BLEU system desirable? Our goal is to propose a multi-objective optimization method that avoids “overfitting to a single metric”. We want to build a MT system"
P12-1001,P07-2045,0,0.0130597,"Missing"
P12-1001,W07-0734,0,0.0337259,"s BLEU is not enough. Ideally, we want to tune towards an automatic metric that has perfect correlation with human judgments of translation quality. ∗ *Now at Nara Institute of Science & Technology (NAIST) While many alternatives have been proposed, such a perfect evaluation metric remains elusive. As a result, many MT evaluation campaigns now report multiple evaluation metrics (Callison-Burch et al., 2011; Paul, 2010). Different evaluation metrics focus on different aspects of translation quality. For example, while BLEU (Papineni et al., 2002) focuses on word-based n-gram precision, METEOR (Lavie and Agarwal, 2007) allows for stem/synonym matching and incorporates recall. TER (Snover et al., 2006) allows arbitrary chunk movements, while permutation metrics like RIBES (Isozaki et al., 2010; Birch et al., 2010) measure deviation in word order. Syntax (Owczarzak et al., 2007) and semantics (Pado et al., 2009) also help. Arguably, all these metrics correspond to our intuitions on what is a good translation. The current approach of optimizing MT towards a single metric runs the risk of sacrificing other metrics. Can we really claim that a system is good if it has high BLEU, but very low METEOR? Similarly, is"
P12-1001,P06-1096,0,0.0438141,"RT4 . Alternatively, by using the largemargin optimizer in (Chiang et al., 2009) and moving it into the for-each loop (lines 4-9), one can get an online algorithm such PMO-MIRA. Virtually all MT optimization algorithms have a place where metric scores feedback into the optimization procedure; the idea of PMO is to replace these raw scores with labels derived from Pareto optimality. 4 Experiments 4.1 Evaluation Methodology We experiment with two datasets: (1) The PubMed task is English-to-Japanese translation of scientific 4 A difference with traditional MERT is the necessity of sentence-BLEU (Liang et al., 2006) in line 6. We use sentenceBLEU for optimization but corpus-BLEU for evaluation here. 5 abstracts. As metrics we use BLEU and RIBES (which demonstrated good human correlation in this language pair (Goto et al., 2011)). (2) The NIST task is Chinese-to-English translation with OpenMT08 training data and MT06 as devset. As metrics we use BLEU and NTER. • BLEU = BP × (Πprecn )1/4 . BP is brevity penality. precn is precision of n-gram matches. 1/4 • RIBES = (τ + 1)/2 × prec1 , with Kendall’s τ computed by measuring permutation between matching words in reference and hypothesis5 . • NTER=max(1−TER,"
P12-1001,N07-1006,0,0.0190011,"w area. Linear-combination of BLEU/TER is the most common technique (Zaidan, 2009), sometimes achieving good results in evaluation campaigns (Dyer et al., 2009). As far as we known, the only work that directly proposes a multi-objective technique is (He and Way, 2009), which modifies MERT to optimize a single metric subject to the constraint that it does not degrade others. These approaches all require some setting of constraint strength or combination weights {pk }. Recent work in MT evaluation has examined combining metrics using machine learning for better correlation with human judgments (Liu and Gildea, 2007; Albrecht and Hwa, 2007; Gimnez and M`arquez, 2008) and may give insights for setting {pk }. We view our Pareto-based approach as orthogonal to these efforts. The tunability of metrics is a problem that is gaining recognition (Liu et al., 2011). If a good evaluation metric could not be used for tuning, it would be a pity. The Tunable Metrics task at WMT2011 concluded that BLEU is still the easiest to tune (Callison-Burch et al., 2011). (Mauser et al., 2008; Cer et al., 2010) report similar observations, in addition citing WER being difficult and BLEU-TER being amenable. One unsolved question"
P12-1001,D11-1035,0,0.262905,"a choice between picking the best weight according to BLEU (BLEU=.265,RIBES=.665) vs. another weight with higher RIBES but poorer BLEU, e.g. (.255,.675). Nevertheless, both the PMO and Linear-Combination with various (p1 , p2 ) samples this joint-objective space broadly. 3. Interestingly, a multi-objective approach can sometimes outperform a single-objective optimizer in its own metric. In Figure 2, singleobjective PRO focusing on optimizing RIBES only achieves 0.68, but PMO-PRO using both BLEU and RIBES outperforms with 0.685. The third observation relates to the issue of metric tunability (Liu et al., 2011). We found that RIBES can be difficult to tune directly. It is an extremely non-smooth objective with many local optima–slight changes in word ordering causes large changes in RIBES. So the best way to improve RIBES is to 6 0.694 0.146 0.148 0.15 0.152 0.154 0.156 0.158 0.16 0.162 0.164 bleu Figure 3: NIST Results not to optimize it directly, but jointly with a more tunable metric BLEU. The learning curve in Figure 4 show that single-objective optimization of RIBES quickly falls into local optimum (at iteration 3) whereas PMO can zigzag and sacrifice RIBES in intermediate iterations (e.g. iter"
P12-1001,D08-1076,0,0.0416377,"f combination weights. Further we observe that multiobjective approaches can be helpful for optimizing difficult-to-tune metrics; this is beneficial for quickly introducing new metrics developed in MT evaluation into MT optimization, especially when good {pk } are not yet known. We conclude by drawing attention to some limitations and opportunities raised by this work: Limitations: (1) The performance of PMO is limited by the size of the Pareto set. Small N-best lists lead to sparsely-sampled Pareto Frontiers, and a much better approach would be to enlarge the hypothesis space using lattices (Macherey et al., 2008). How to compute Pareto points directly from lattices is an interesting open research question. (2) The binary distinction between pareto vs. non-pareto points ignores the fact that 2nd-place non-pareto points may also lead to good practical solutions. A better approach may be to adopt a graded definition of Pareto optimality as done in some multi-objective works (Deb et al., 2002). (3) A robust evaluation methodology that enables significance testing for multi-objective problems is sorely needed. This will make it possible to compare multi-objective methods on more than 2 metrics. We also nee"
P12-1001,mauser-etal-2008-automatic,0,0.138462,"ts {pk }. Recent work in MT evaluation has examined combining metrics using machine learning for better correlation with human judgments (Liu and Gildea, 2007; Albrecht and Hwa, 2007; Gimnez and M`arquez, 2008) and may give insights for setting {pk }. We view our Pareto-based approach as orthogonal to these efforts. The tunability of metrics is a problem that is gaining recognition (Liu et al., 2011). If a good evaluation metric could not be used for tuning, it would be a pity. The Tunable Metrics task at WMT2011 concluded that BLEU is still the easiest to tune (Callison-Burch et al., 2011). (Mauser et al., 2008; Cer et al., 2010) report similar observations, in addition citing WER being difficult and BLEU-TER being amenable. One unsolved question is whether metric tunability is a problem inherent to the metric only, or depends also on the underlying optimization algorithm. Our positive results with PMO suggest that the choice of optimization algorithm can help. Multi-objective ideas are being explored in other NLP areas. (Spitkovsky et al., 2011) describe a technique that alternates between hard and soft EM objectives in order to achieve better local optimum in grammar induction. (Hall et al., 2011)"
P12-1001,P03-1021,0,0.424427,"e diverse aspects to improve overall quality. Our approach is based on the theory of Pareto Optimality. It is simple to implement on top of existing single-objective optimization methods (e.g. MERT, PRO) and outperforms ad hoc alternatives based on linear-combination of metrics. We also discuss the issue of metric tunability and show that our Pareto approach is more effective in incorporating new metrics from MT evaluation for MT optimization. 1 Introduction Weight optimization is an important step in building machine translation (MT) systems. Discriminative optimization methods such as MERT (Och, 2003), MIRA (Crammer et al., 2006), PRO (Hopkins and May, 2011), and Downhill-Simplex (Nelder and Mead, 1965) have been influential in improving MT systems in recent years. These methods are effective because they tune the system to maximize an automatic evaluation metric such as BLEU, which serve as surrogate objective for translation quality. However, we know that a single metric such as BLEU is not enough. Ideally, we want to tune towards an automatic metric that has perfect correlation with human judgments of translation quality. ∗ *Now at Nara Institute of Science & Technology (NAIST) While ma"
P12-1001,W07-0714,0,0.0433798,"Missing"
P12-1001,P02-1040,0,0.0840642,"e for translation quality. However, we know that a single metric such as BLEU is not enough. Ideally, we want to tune towards an automatic metric that has perfect correlation with human judgments of translation quality. ∗ *Now at Nara Institute of Science & Technology (NAIST) While many alternatives have been proposed, such a perfect evaluation metric remains elusive. As a result, many MT evaluation campaigns now report multiple evaluation metrics (Callison-Burch et al., 2011; Paul, 2010). Different evaluation metrics focus on different aspects of translation quality. For example, while BLEU (Papineni et al., 2002) focuses on word-based n-gram precision, METEOR (Lavie and Agarwal, 2007) allows for stem/synonym matching and incorporates recall. TER (Snover et al., 2006) allows arbitrary chunk movements, while permutation metrics like RIBES (Isozaki et al., 2010; Birch et al., 2010) measure deviation in word order. Syntax (Owczarzak et al., 2007) and semantics (Pado et al., 2009) also help. Arguably, all these metrics correspond to our intuitions on what is a good translation. The current approach of optimizing MT towards a single metric runs the risk of sacrificing other metrics. Can we really claim that"
P12-1001,2010.iwslt-evaluation.1,0,0.022396,"ecause they tune the system to maximize an automatic evaluation metric such as BLEU, which serve as surrogate objective for translation quality. However, we know that a single metric such as BLEU is not enough. Ideally, we want to tune towards an automatic metric that has perfect correlation with human judgments of translation quality. ∗ *Now at Nara Institute of Science & Technology (NAIST) While many alternatives have been proposed, such a perfect evaluation metric remains elusive. As a result, many MT evaluation campaigns now report multiple evaluation metrics (Callison-Burch et al., 2011; Paul, 2010). Different evaluation metrics focus on different aspects of translation quality. For example, while BLEU (Papineni et al., 2002) focuses on word-based n-gram precision, METEOR (Lavie and Agarwal, 2007) allows for stem/synonym matching and incorporates recall. TER (Snover et al., 2006) allows arbitrary chunk movements, while permutation metrics like RIBES (Isozaki et al., 2010; Birch et al., 2010) measure deviation in word order. Syntax (Owczarzak et al., 2007) and semantics (Pado et al., 2009) also help. Arguably, all these metrics correspond to our intuitions on what is a good translation. T"
P12-1001,2006.amta-papers.25,0,0.0542521,"ect correlation with human judgments of translation quality. ∗ *Now at Nara Institute of Science & Technology (NAIST) While many alternatives have been proposed, such a perfect evaluation metric remains elusive. As a result, many MT evaluation campaigns now report multiple evaluation metrics (Callison-Burch et al., 2011; Paul, 2010). Different evaluation metrics focus on different aspects of translation quality. For example, while BLEU (Papineni et al., 2002) focuses on word-based n-gram precision, METEOR (Lavie and Agarwal, 2007) allows for stem/synonym matching and incorporates recall. TER (Snover et al., 2006) allows arbitrary chunk movements, while permutation metrics like RIBES (Isozaki et al., 2010; Birch et al., 2010) measure deviation in word order. Syntax (Owczarzak et al., 2007) and semantics (Pado et al., 2009) also help. Arguably, all these metrics correspond to our intuitions on what is a good translation. The current approach of optimizing MT towards a single metric runs the risk of sacrificing other metrics. Can we really claim that a system is good if it has high BLEU, but very low METEOR? Similarly, is a high-METEOR low-BLEU system desirable? Our goal is to propose a multi-objective o"
P12-1001,D11-1117,0,0.0468041,"not be used for tuning, it would be a pity. The Tunable Metrics task at WMT2011 concluded that BLEU is still the easiest to tune (Callison-Burch et al., 2011). (Mauser et al., 2008; Cer et al., 2010) report similar observations, in addition citing WER being difficult and BLEU-TER being amenable. One unsolved question is whether metric tunability is a problem inherent to the metric only, or depends also on the underlying optimization algorithm. Our positive results with PMO suggest that the choice of optimization algorithm can help. Multi-objective ideas are being explored in other NLP areas. (Spitkovsky et al., 2011) describe a technique that alternates between hard and soft EM objectives in order to achieve better local optimum in grammar induction. (Hall et al., 2011) investigates joint optimization of a supervised parsing objective and some extrinsic objectives based on downstream applications. (Agarwal et al., 2011) considers using multiple signals (of varying quality) from online users to train recommendation models. (Eisner and Daum´e III, 2011) trades off speed and accuracy of a parser with reinforcement learning. None of the techniques in NLP use Pareto concepts, however. 6 Opportunities and Limit"
P12-1046,P10-1112,0,0.121454,"as been an increasing interest in tree substitution grammar (TSG) as an alternative to CFG for modeling syntax trees (Post and Gildea, 2009; Tenenbaum et al., 2009; Cohn et al., 2010). TSG is a natural extension of CFG in which nonterminal symbols can be rewritten (substituted) with arbitrarily large tree fragments. These tree fragments have great advantages over tiny CFG rules since they can capture non-local contexts explicitly such as predicate-argument structures, idioms and grammatical agreements (Cohn et al., 2010). Previous work on TSG parsing (Cohn et al., 2010; Post and Gildea, 2009; Bansal and Klein, 2010) has consistently shown that a probabilistic TSG (PTSG) parser is significantly more accurate than a PCFG parser, but is still inferior to state-of-the-art parsers (e.g., the Berkeley parser (Petrov et al., 2006) and the Charniak parser (Charniak and Johnson, 2005)). One major drawback of TSG is that the context freedom assumptions still remain at substitution sites, that is, TSG tree fragments are generated that are conditionally independent of all others given root nonterminal symbols. Furthermore, when a sentence is unparsable with large tree fragments, the PTSG parser usually uses naive CF"
P12-1046,D10-1117,0,0.113633,"nparametric Bayesian model. The PYP produces power-law distributions, which have been shown to be well-suited for such uses as language modeling (Teh, 2006b), and TSG induction (Cohn et al., 2010). One major issue as regards modeling an SR-TSG is that the space of the grammar rules will be very sparse since SR-TSG allows for arbitrarily large tree fragments and also an arbitrarily large set of symbol subcategories. To address the sparseness problem, we employ a hierarchical PYP to encode a backoff scheme from the SRTSG rules to simpler CFG rules, inspired by recent work on dependency parsing (Blunsom and Cohn, 2010). Our model consists of a three-level hierarchy. Table 1 shows an example of the SR-TSG rule and its backoff tree fragments as an illustration of this threelevel hierarchy. The topmost level of our model is a distribution over the SR-TSG rules as follows. e |xk Gxk ∼ Gxk  ∼ PYP dxk , θxk , P sr-tsg (· |xk ) , where xk is a refined root symbol of an elementary tree e, while x is a raw nonterminal symbol in the corpus and k = 0, 1, . . . is an index of the symbol subcategory. Suppose x is NP and its symbol subcategory is 0, then xk is NP0 . The PYP has three parameters: (dxk , θxk , P sr-tsg )."
P12-1046,P05-1022,0,0.838813,"ten (substituted) with arbitrarily large tree fragments. These tree fragments have great advantages over tiny CFG rules since they can capture non-local contexts explicitly such as predicate-argument structures, idioms and grammatical agreements (Cohn et al., 2010). Previous work on TSG parsing (Cohn et al., 2010; Post and Gildea, 2009; Bansal and Klein, 2010) has consistently shown that a probabilistic TSG (PTSG) parser is significantly more accurate than a PCFG parser, but is still inferior to state-of-the-art parsers (e.g., the Berkeley parser (Petrov et al., 2006) and the Charniak parser (Charniak and Johnson, 2005)). One major drawback of TSG is that the context freedom assumptions still remain at substitution sites, that is, TSG tree fragments are generated that are conditionally independent of all others given root nonterminal symbols. Furthermore, when a sentence is unparsable with large tree fragments, the PTSG parser usually uses naive CFG rules derived from its backoff model, which diminishes the benefits obtained from large tree fragments. On the other hand, current state-of-the-art parsers use symbol refinement techniques (Johnson, 1998; Collins, 2003; Matsuzaki et al., 2005). Symbol refinement"
P12-1046,N10-1081,0,0.163802,"However, an adaptor grammar differs from ours in that all its rules are complete: all leaf nodes must be terminal symbols, while our model permits nonterminal symbols as leaf nodes. Furthermore, adaptor grammars have largely been applied to the task of unsupervised structural induction from raw texts such as (a) (b) (c) Figure 1: (a) Example parse tree. (b) Example TSG derivation of (a). (c) Example SR-TSG derivation of (a). The refinement annotation is hyphenated with a nonterminal symbol. morphology analysis, word segmentation (Johnson and Goldwater, 2009), and dependency grammar induction (Cohen et al., 2010), rather than constituent syntax parsing. An all-fragments grammar (Bansal and Klein, 2010) is another variant of TSG that aims to utilize all possible subtrees as rules. It maps a TSG to an implicit representation to make the grammar tractable and practical for large-scale parsing. The manual symbol refinement described in (Klein and Manning, 2003) was applied to an all-fragments grammar and this improved accuracy in the English WSJ parsing task. As mentioned in the introduction, our model focuses on the automatic learning of a TSG and symbol refinement without heuristics. 3 Symbol-Refined Tr"
P12-1046,J03-4003,0,0.043631,"2006) and the Charniak parser (Charniak and Johnson, 2005)). One major drawback of TSG is that the context freedom assumptions still remain at substitution sites, that is, TSG tree fragments are generated that are conditionally independent of all others given root nonterminal symbols. Furthermore, when a sentence is unparsable with large tree fragments, the PTSG parser usually uses naive CFG rules derived from its backoff model, which diminishes the benefits obtained from large tree fragments. On the other hand, current state-of-the-art parsers use symbol refinement techniques (Johnson, 1998; Collins, 2003; Matsuzaki et al., 2005). Symbol refinement is a successful approach for weakening context freedom assumptions by dividing coarse treebank symbols (e.g. NP and VP) into subcategories, rather than extracting large tree fragments. As shown in several studies on TSG parsing (Zuidema, 2007; Bansal and Klein, 2010), large 440 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 440–448, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics tree fragments and symbol refinement work complementarily for syntactic parsing. F"
P12-1046,D09-1076,0,0.042041,"Missing"
P12-1046,N09-2064,0,0.139748,"Missing"
P12-1046,N04-1035,0,0.0974858,"Missing"
P12-1046,P08-1067,0,0.305949,"Missing"
P12-1046,N09-1036,0,0.151665,"ol refinement, and is thus closely related to our SR-TSG model. However, an adaptor grammar differs from ours in that all its rules are complete: all leaf nodes must be terminal symbols, while our model permits nonterminal symbols as leaf nodes. Furthermore, adaptor grammars have largely been applied to the task of unsupervised structural induction from raw texts such as (a) (b) (c) Figure 1: (a) Example parse tree. (b) Example TSG derivation of (a). (c) Example SR-TSG derivation of (a). The refinement annotation is hyphenated with a nonterminal symbol. morphology analysis, word segmentation (Johnson and Goldwater, 2009), and dependency grammar induction (Cohen et al., 2010), rather than constituent syntax parsing. An all-fragments grammar (Bansal and Klein, 2010) is another variant of TSG that aims to utilize all possible subtrees as rules. It maps a TSG to an implicit representation to make the grammar tractable and practical for large-scale parsing. The manual symbol refinement described in (Klein and Manning, 2003) was applied to an all-fragments grammar and this improved accuracy in the English WSJ parsing task. As mentioned in the introduction, our model focuses on the automatic learning of a TSG and sy"
P12-1046,N07-1018,0,0.296046,"a process of forming a parse tree. It starts with a root symbol and rewrites (substi441 p (e |t ) ∝ p (t |e ) p (e) . where p (t |e ) is either equal to 1 (when t and e are consistent) or 0 (otherwise). Therefore, the task of TSG induction from parse trees turns out to consist of modeling the prior distribution p (e). Recent work on TSG induction defines p (e) as a nonparametric Bayesian model such as the Dirichlet Process (Ferguson, 1973) or the Pitman-Yor Process to encourage sparse and compact grammars. Several studies have combined TSG induction and symbol refinement. An adaptor grammar (Johnson et al., 2007a) is a sort of nonparametric Bayesian TSG model with symbol refinement, and is thus closely related to our SR-TSG model. However, an adaptor grammar differs from ours in that all its rules are complete: all leaf nodes must be terminal symbols, while our model permits nonterminal symbols as leaf nodes. Furthermore, adaptor grammars have largely been applied to the task of unsupervised structural induction from raw texts such as (a) (b) (c) Figure 1: (a) Example parse tree. (b) Example TSG derivation of (a). (c) Example SR-TSG derivation of (a). The refinement annotation is hyphenated with a no"
P12-1046,J98-4004,0,0.640761,"Petrov et al., 2006) and the Charniak parser (Charniak and Johnson, 2005)). One major drawback of TSG is that the context freedom assumptions still remain at substitution sites, that is, TSG tree fragments are generated that are conditionally independent of all others given root nonterminal symbols. Furthermore, when a sentence is unparsable with large tree fragments, the PTSG parser usually uses naive CFG rules derived from its backoff model, which diminishes the benefits obtained from large tree fragments. On the other hand, current state-of-the-art parsers use symbol refinement techniques (Johnson, 1998; Collins, 2003; Matsuzaki et al., 2005). Symbol refinement is a successful approach for weakening context freedom assumptions by dividing coarse treebank symbols (e.g. NP and VP) into subcategories, rather than extracting large tree fragments. As shown in several studies on TSG parsing (Zuidema, 2007; Bansal and Klein, 2010), large 440 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 440–448, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics tree fragments and symbol refinement work complementarily for synta"
P12-1046,P03-1054,0,0.254122,"ion Syntactic parsing has played a central role in natural language processing. The resulting syntactic analysis can be used for various applications such as machine translation (Galley et al., 2004; DeNeefe and Knight, 2009), sentence compression (Cohn and Lapata, 2009; Yamangil and Shieber, 2010), and question answering (Wang et al., 2007). Probabilistic context-free grammar (PCFG) underlies many statistical parsers, however, it is well known that the PCFG rules extracted from treebank data via maximum likelihood estimation do not perform well due to unrealistic context freedom assumptions (Klein and Manning, 2003). In recent years, there has been an increasing interest in tree substitution grammar (TSG) as an alternative to CFG for modeling syntax trees (Post and Gildea, 2009; Tenenbaum et al., 2009; Cohn et al., 2010). TSG is a natural extension of CFG in which nonterminal symbols can be rewritten (substituted) with arbitrarily large tree fragments. These tree fragments have great advantages over tiny CFG rules since they can capture non-local contexts explicitly such as predicate-argument structures, idioms and grammatical agreements (Cohn et al., 2010). Previous work on TSG parsing (Cohn et al., 201"
P12-1046,J93-2004,0,0.058271,"ation probabilities according to our model. This heuristics is helpful for finding large tree fragments and learning compact grammars. 4.3 Hyperparameter Estimation We treat hyperparameters {d, θ} as random variables and update their values for every MCMC iteration. We place a prior on the hyperparameters as follows: d ∼ Beta (1, 1), θ ∼ Gamma (1, 1). The values of d and θ are optimized with the auxiliary variable technique (Teh, 2006a). 5 Experiment Model 5.1 Settings CFG 5.1.1 Data Preparation We ran experiments on the Wall Street Journal (WSJ) portion of the English Penn Treebank data set (Marcus et al., 1993), using a standard data split (sections 2–21 for training, 22 for development and 23 for testing). We also used section 2 as a small training set for evaluating the performance of our model under low-resource conditions. Henceforth, we distinguish the small training set (section 2) from the full training set (sections 2-21). The treebank data is right-binarized (Matsuzaki et al., 2005) to construct grammars with only unary and binary productions. We replace lexical words with count ≤ 5 in the training data with one of 50 unknown words using lexical features, following (Petrov et al., 2006). We"
P12-1046,P05-1010,1,0.934163,"harniak parser (Charniak and Johnson, 2005)). One major drawback of TSG is that the context freedom assumptions still remain at substitution sites, that is, TSG tree fragments are generated that are conditionally independent of all others given root nonterminal symbols. Furthermore, when a sentence is unparsable with large tree fragments, the PTSG parser usually uses naive CFG rules derived from its backoff model, which diminishes the benefits obtained from large tree fragments. On the other hand, current state-of-the-art parsers use symbol refinement techniques (Johnson, 1998; Collins, 2003; Matsuzaki et al., 2005). Symbol refinement is a successful approach for weakening context freedom assumptions by dividing coarse treebank symbols (e.g. NP and VP) into subcategories, rather than extracting large tree fragments. As shown in several studies on TSG parsing (Zuidema, 2007; Bansal and Klein, 2010), large 440 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 440–448, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics tree fragments and symbol refinement work complementarily for syntactic parsing. For example, Bansal and Kl"
P12-1046,P06-1055,0,0.730889,"CFG in which nonterminal symbols can be rewritten (substituted) with arbitrarily large tree fragments. These tree fragments have great advantages over tiny CFG rules since they can capture non-local contexts explicitly such as predicate-argument structures, idioms and grammatical agreements (Cohn et al., 2010). Previous work on TSG parsing (Cohn et al., 2010; Post and Gildea, 2009; Bansal and Klein, 2010) has consistently shown that a probabilistic TSG (PTSG) parser is significantly more accurate than a PCFG parser, but is still inferior to state-of-the-art parsers (e.g., the Berkeley parser (Petrov et al., 2006) and the Charniak parser (Charniak and Johnson, 2005)). One major drawback of TSG is that the context freedom assumptions still remain at substitution sites, that is, TSG tree fragments are generated that are conditionally independent of all others given root nonterminal symbols. Furthermore, when a sentence is unparsable with large tree fragments, the PTSG parser usually uses naive CFG rules derived from its backoff model, which diminishes the benefits obtained from large tree fragments. On the other hand, current state-of-the-art parsers use symbol refinement techniques (Johnson, 1998; Colli"
P12-1046,N10-1003,0,0.14792,"Missing"
P12-1046,P09-2012,0,0.2292,"anslation (Galley et al., 2004; DeNeefe and Knight, 2009), sentence compression (Cohn and Lapata, 2009; Yamangil and Shieber, 2010), and question answering (Wang et al., 2007). Probabilistic context-free grammar (PCFG) underlies many statistical parsers, however, it is well known that the PCFG rules extracted from treebank data via maximum likelihood estimation do not perform well due to unrealistic context freedom assumptions (Klein and Manning, 2003). In recent years, there has been an increasing interest in tree substitution grammar (TSG) as an alternative to CFG for modeling syntax trees (Post and Gildea, 2009; Tenenbaum et al., 2009; Cohn et al., 2010). TSG is a natural extension of CFG in which nonterminal symbols can be rewritten (substituted) with arbitrarily large tree fragments. These tree fragments have great advantages over tiny CFG rules since they can capture non-local contexts explicitly such as predicate-argument structures, idioms and grammatical agreements (Cohn et al., 2010). Previous work on TSG parsing (Cohn et al., 2010; Post and Gildea, 2009; Bansal and Klein, 2010) has consistently shown that a probabilistic TSG (PTSG) parser is significantly more accurate than a PCFG parser, bu"
P12-1046,P06-1124,0,0.186641,"R-TSG derivations from a corpus of parse trees in an unsupervised fashion. That is, we wish to infer the symbol subcategories of every node and substitution site (i.e., nodes where substitution occurs) from parse trees. Extracted rules and their probabilities can be used to parse new raw sentences. 442 3.1 Probabilistic Model We define a probabilistic model of an SR-TSG based on the Pitman-Yor Process (PYP) (Pitman and Yor, 1997), namely a sort of nonparametric Bayesian model. The PYP produces power-law distributions, which have been shown to be well-suited for such uses as language modeling (Teh, 2006b), and TSG induction (Cohn et al., 2010). One major issue as regards modeling an SR-TSG is that the space of the grammar rules will be very sparse since SR-TSG allows for arbitrarily large tree fragments and also an arbitrarily large set of symbol subcategories. To address the sparseness problem, we employ a hierarchical PYP to encode a backoff scheme from the SRTSG rules to simpler CFG rules, inspired by recent work on dependency parsing (Blunsom and Cohn, 2010). Our model consists of a three-level hierarchy. Table 1 shows an example of the SR-TSG rule and its backoff tree fragments as an il"
P12-1046,D07-1003,0,0.0199049,"Missing"
P12-1046,P10-1096,0,0.0399252,"Missing"
P12-1046,D09-1161,0,0.157163,"Missing"
P12-1046,D07-1058,0,0.0141628,"urthermore, when a sentence is unparsable with large tree fragments, the PTSG parser usually uses naive CFG rules derived from its backoff model, which diminishes the benefits obtained from large tree fragments. On the other hand, current state-of-the-art parsers use symbol refinement techniques (Johnson, 1998; Collins, 2003; Matsuzaki et al., 2005). Symbol refinement is a successful approach for weakening context freedom assumptions by dividing coarse treebank symbols (e.g. NP and VP) into subcategories, rather than extracting large tree fragments. As shown in several studies on TSG parsing (Zuidema, 2007; Bansal and Klein, 2010), large 440 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 440–448, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics tree fragments and symbol refinement work complementarily for syntactic parsing. For example, Bansal and Klein (2010) have reported that deterministic symbol refinement with heuristics helps improve the accuracy of a TSG parser. In this paper, we propose Symbol-Refined Tree Substitution Grammars (SR-TSGs) for syntactic parsing. SR-TSG is an extension of the conventio"
P12-2020,W06-2920,0,0.0329395,"ee of an example sentence. ‘*’/ ‘+’=syntactic/semantic heads. Arrows in red (upper)= PASs, orange (bottom)=word-level dependencies generated from PASs, blue=newly appended dependencies. both during rule extracting and target dependency language model (LM) training. 2.2 Dependency parsing Graph-based and transition-based are two predominant paradigms for data-driven dependency parsing. The MST parser (McDonald et al., 2005) and the Malt parser (Nivre, 2003) stand for two typical parsers, respectively. Parsing accuracy comparison and error analysis under the CoNLL-X dependency shared task data (Buchholz and Marsi, 2006) have been performed by McDonald and Nivre (2011). Here, we compare them on the SMT tasks through parsing the real-world SMT data. 2.3 PCFG parsing For PCFG parsing, we select the Berkeley parser (Petrov and Klein, 2007). In order to generate wordlevel dependency trees from the PCFG tree, we use the LTH constituent-to-dependency conversion tool3 written by Johansson and Nugues (2007). The head finding rules4 are according to Magerman (1995) and Collins (1997). Similar approach has been originally used by Shen et al. (2008). 2.4 HPSG parsing In the Enju English HPSG grammar (Miyao et al., 2003)"
P12-2020,J07-4004,0,0.0657609,"tsuhito@lab.ntt.co.jp, kevinduh@is.naist.jp,{tsukada.hajime,nagata.masaaki}@lab.ntt.co.jp Abstract tures. For example, using the constituent-todependency conversion approach proposed by Johansson and Nugues (2007), we can easily yield dependency trees from PCFG style trees. A semantic dependency representation of a whole sentence, predicate-argument structures (PASs), are also included in the output trees of (1) a state-of-the-art head-driven phrase structure grammar (HPSG) (Pollard and Sag, 1994; Sag et al., 2003) parser, Enju1 (Miyao and Tsujii, 2008) and (2) a state-of-the-art CCG parser2 (Clark and Curran, 2007). The motivation of this paper is to investigate the impact of these non-isomorphic dependency structures to be used for SMT. That is, we would like to provide a comparative evaluation of these dependencies in a string-to-dependency decoder (Shen et al., 2008). This paper presents a comparative study of target dependency structures yielded by several state-of-the-art linguistic parsers. Our approach is to measure the impact of these nonisomorphic dependency structures to be used for string-to-dependency translation. Besides using traditional dependency parsers, we also use the dependency struc"
P12-2020,P97-1003,0,0.248621,"typical parsers, respectively. Parsing accuracy comparison and error analysis under the CoNLL-X dependency shared task data (Buchholz and Marsi, 2006) have been performed by McDonald and Nivre (2011). Here, we compare them on the SMT tasks through parsing the real-world SMT data. 2.3 PCFG parsing For PCFG parsing, we select the Berkeley parser (Petrov and Klein, 2007). In order to generate wordlevel dependency trees from the PCFG tree, we use the LTH constituent-to-dependency conversion tool3 written by Johansson and Nugues (2007). The head finding rules4 are according to Magerman (1995) and Collins (1997). Similar approach has been originally used by Shen et al. (2008). 2.4 HPSG parsing In the Enju English HPSG grammar (Miyao et al., 2003) used in this paper, the semantic content of 3 4 http://nlp.cs.lth.se/software/treebank converter/ http://www.cs.columbia.edu/ mcollins/papers/heads 101 a sentence/phrase is represented by a PAS. In an HPSG tree, each leaf node generally introduces a predicate, which is represented by the pair made up of the lexical entry feature and predicate type feature. The arguments of a predicate are designated by the arrows from the argument features in a leaf node to"
P12-2020,W07-2416,0,0.0959405,"ng. The MST parser (McDonald et al., 2005) and the Malt parser (Nivre, 2003) stand for two typical parsers, respectively. Parsing accuracy comparison and error analysis under the CoNLL-X dependency shared task data (Buchholz and Marsi, 2006) have been performed by McDonald and Nivre (2011). Here, we compare them on the SMT tasks through parsing the real-world SMT data. 2.3 PCFG parsing For PCFG parsing, we select the Berkeley parser (Petrov and Klein, 2007). In order to generate wordlevel dependency trees from the PCFG tree, we use the LTH constituent-to-dependency conversion tool3 written by Johansson and Nugues (2007). The head finding rules4 are according to Magerman (1995) and Collins (1997). Similar approach has been originally used by Shen et al. (2008). 2.4 HPSG parsing In the Enju English HPSG grammar (Miyao et al., 2003) used in this paper, the semantic content of 3 4 http://nlp.cs.lth.se/software/treebank converter/ http://www.cs.columbia.edu/ mcollins/papers/heads 101 a sentence/phrase is represented by a PAS. In an HPSG tree, each leaf node generally introduces a predicate, which is represented by the pair made up of the lexical entry feature and predicate type feature. The arguments of a predica"
P12-2020,P07-2045,0,0.00633941,"icate types to the gold-standard grammatical relations can be found in Table 13 in (Clark and 102 Curran, 2007). The post-processing is like that described for HPSG parsing, except we greedily use the MST’s sentence root when we can not determine it based on the CCG parser’s PASs. 3 Experiments 3.1 Setup We re-implemented the string-to-dependency decoder described in (Shen et al., 2008). Dependency structures from non-isomorphic syntactic/semantic parsers are separately used to train the transfer rules as well as target dependency LMs. For intuitive comparison, an outside SMT system is Moses (Koehn et al., 2007). For Chinese-to-English translation, we use the parallel data from NIST Open Machine Translation Evaluation tasks. The training data contains 353,796 sentence pairs, 8.7M Chinese words and 10.4M English words. The NIST 2003 and 2005 test data are respectively taken as the development and test set. We performed GIZA++ (Och and Ney, 2003) and the grow-diag-final-and symmetrizing strategy (Koehn et al., 2007) to obtain word alignments. The Berkeley Language Modeling Toolkit, berkeleylm1.0b35 (Pauls and Klein, 2011), was employed to train (1) a five-gram LM on the Xinhua portion of LDC English Gi"
P12-2020,P95-1037,0,0.136107,"2003) stand for two typical parsers, respectively. Parsing accuracy comparison and error analysis under the CoNLL-X dependency shared task data (Buchholz and Marsi, 2006) have been performed by McDonald and Nivre (2011). Here, we compare them on the SMT tasks through parsing the real-world SMT data. 2.3 PCFG parsing For PCFG parsing, we select the Berkeley parser (Petrov and Klein, 2007). In order to generate wordlevel dependency trees from the PCFG tree, we use the LTH constituent-to-dependency conversion tool3 written by Johansson and Nugues (2007). The head finding rules4 are according to Magerman (1995) and Collins (1997). Similar approach has been originally used by Shen et al. (2008). 2.4 HPSG parsing In the Enju English HPSG grammar (Miyao et al., 2003) used in this paper, the semantic content of 3 4 http://nlp.cs.lth.se/software/treebank converter/ http://www.cs.columbia.edu/ mcollins/papers/heads 101 a sentence/phrase is represented by a PAS. In an HPSG tree, each leaf node generally introduces a predicate, which is represented by the pair made up of the lexical entry feature and predicate type feature. The arguments of a predicate are designated by the arrows from the argument features"
P12-2020,H94-1020,0,0.0614407,"Missing"
P12-2020,J11-1007,0,0.0363269,"age side dependency structures have been successfully used in statistical machine translation (SMT) by Shen et al. (2008) and achieved state-of-the-art results as reported in the NIST 2008 Open MT Evaluation workshop and the NTCIR-9 Chinese-to-English patent translation task (Goto et al., 2011; Ma and Matsoukas, 2011). A primary advantage of dependency representations is that they have a natural mechanism for representing discontinuous constructions, which arise due to longdistance dependencies or in languages where grammatical relations are often signaled by morphology instead of word order (McDonald and Nivre, 2011). It is known that dependency-style structures can be transformed from a number of linguistic struc∗ † Now at Baidu Inc. Now at Nara Institute of Science & Technology (NAIST) 2 Gaining Dependency Structures 2.1 Dependency tree We follow the definition of dependency graph and dependency tree as given in (McDonald and Nivre, 2011). A dependency graph G for sentence s is called a dependency tree when it satisfies, (1) the nodes cover all the words in s besides the ROOT; (2) one node can have one and only one head (word) with a determined syntactic role; and (3) the ROOT of the graph is reachable"
P12-2020,P05-1012,0,0.0580344,"_ aux_ verb_ punct_ noun_ arg0 arg1 arg12 arg12 arg1 when the fluid pressure cylinder 31 * c19 c13 c22 c24 c25 t10 t11 t12 aux_ arg12 adj_ arg1 verb_ arg12 is gradually applied . Figure 1: HPSG tree of an example sentence. ‘*’/ ‘+’=syntactic/semantic heads. Arrows in red (upper)= PASs, orange (bottom)=word-level dependencies generated from PASs, blue=newly appended dependencies. both during rule extracting and target dependency language model (LM) training. 2.2 Dependency parsing Graph-based and transition-based are two predominant paradigms for data-driven dependency parsing. The MST parser (McDonald et al., 2005) and the Malt parser (Nivre, 2003) stand for two typical parsers, respectively. Parsing accuracy comparison and error analysis under the CoNLL-X dependency shared task data (Buchholz and Marsi, 2006) have been performed by McDonald and Nivre (2011). Here, we compare them on the SMT tasks through parsing the real-world SMT data. 2.3 PCFG parsing For PCFG parsing, we select the Berkeley parser (Petrov and Klein, 2007). In order to generate wordlevel dependency trees from the PCFG tree, we use the LTH constituent-to-dependency conversion tool3 written by Johansson and Nugues (2007). The head find"
P12-2020,J08-1002,0,0.0193457,"o, Soraku-gun Kyoto 619-0237 Japan wuxianchao@gmail.com,sudoh.katsuhito@lab.ntt.co.jp, kevinduh@is.naist.jp,{tsukada.hajime,nagata.masaaki}@lab.ntt.co.jp Abstract tures. For example, using the constituent-todependency conversion approach proposed by Johansson and Nugues (2007), we can easily yield dependency trees from PCFG style trees. A semantic dependency representation of a whole sentence, predicate-argument structures (PASs), are also included in the output trees of (1) a state-of-the-art head-driven phrase structure grammar (HPSG) (Pollard and Sag, 1994; Sag et al., 2003) parser, Enju1 (Miyao and Tsujii, 2008) and (2) a state-of-the-art CCG parser2 (Clark and Curran, 2007). The motivation of this paper is to investigate the impact of these non-isomorphic dependency structures to be used for SMT. That is, we would like to provide a comparative evaluation of these dependencies in a string-to-dependency decoder (Shen et al., 2008). This paper presents a comparative study of target dependency structures yielded by several state-of-the-art linguistic parsers. Our approach is to measure the impact of these nonisomorphic dependency structures to be used for string-to-dependency translation. Besides using"
P12-2020,W03-3017,0,0.0491047,"g12 arg1 when the fluid pressure cylinder 31 * c19 c13 c22 c24 c25 t10 t11 t12 aux_ arg12 adj_ arg1 verb_ arg12 is gradually applied . Figure 1: HPSG tree of an example sentence. ‘*’/ ‘+’=syntactic/semantic heads. Arrows in red (upper)= PASs, orange (bottom)=word-level dependencies generated from PASs, blue=newly appended dependencies. both during rule extracting and target dependency language model (LM) training. 2.2 Dependency parsing Graph-based and transition-based are two predominant paradigms for data-driven dependency parsing. The MST parser (McDonald et al., 2005) and the Malt parser (Nivre, 2003) stand for two typical parsers, respectively. Parsing accuracy comparison and error analysis under the CoNLL-X dependency shared task data (Buchholz and Marsi, 2006) have been performed by McDonald and Nivre (2011). Here, we compare them on the SMT tasks through parsing the real-world SMT data. 2.3 PCFG parsing For PCFG parsing, we select the Berkeley parser (Petrov and Klein, 2007). In order to generate wordlevel dependency trees from the PCFG tree, we use the LTH constituent-to-dependency conversion tool3 written by Johansson and Nugues (2007). The head finding rules4 are according to Magerm"
P12-2020,J03-1002,0,0.00290707,"pendency decoder described in (Shen et al., 2008). Dependency structures from non-isomorphic syntactic/semantic parsers are separately used to train the transfer rules as well as target dependency LMs. For intuitive comparison, an outside SMT system is Moses (Koehn et al., 2007). For Chinese-to-English translation, we use the parallel data from NIST Open Machine Translation Evaluation tasks. The training data contains 353,796 sentence pairs, 8.7M Chinese words and 10.4M English words. The NIST 2003 and 2005 test data are respectively taken as the development and test set. We performed GIZA++ (Och and Ney, 2003) and the grow-diag-final-and symmetrizing strategy (Koehn et al., 2007) to obtain word alignments. The Berkeley Language Modeling Toolkit, berkeleylm1.0b35 (Pauls and Klein, 2011), was employed to train (1) a five-gram LM on the Xinhua portion of LDC English Gigaword corpus v3 (LDC2007T07) and (2) a tri-gram dependency LM on the English dependency structures of the training data. We report the translation quality using the case-insensitive BLEU-4 metric (Papineni et al., 2002). 3.2 Statistics of dependencies We compare the similarity of the dependencies with each other, as shown in Table 2. Ba"
P12-2020,P02-1040,0,0.0827304,"Missing"
P12-2020,P11-1027,0,0.01549,"as target dependency LMs. For intuitive comparison, an outside SMT system is Moses (Koehn et al., 2007). For Chinese-to-English translation, we use the parallel data from NIST Open Machine Translation Evaluation tasks. The training data contains 353,796 sentence pairs, 8.7M Chinese words and 10.4M English words. The NIST 2003 and 2005 test data are respectively taken as the development and test set. We performed GIZA++ (Och and Ney, 2003) and the grow-diag-final-and symmetrizing strategy (Koehn et al., 2007) to obtain word alignments. The Berkeley Language Modeling Toolkit, berkeleylm1.0b35 (Pauls and Klein, 2011), was employed to train (1) a five-gram LM on the Xinhua portion of LDC English Gigaword corpus v3 (LDC2007T07) and (2) a tri-gram dependency LM on the English dependency structures of the training data. We report the translation quality using the case-insensitive BLEU-4 metric (Papineni et al., 2002). 3.2 Statistics of dependencies We compare the similarity of the dependencies with each other, as shown in Table 2. Basically, we investigate (1) if two dependency graphs of one sentence share the same root word and (2) if the head of one word in one sentence are identical in two dependency graph"
P12-2020,P08-1066,0,0.377419,"le trees. A semantic dependency representation of a whole sentence, predicate-argument structures (PASs), are also included in the output trees of (1) a state-of-the-art head-driven phrase structure grammar (HPSG) (Pollard and Sag, 1994; Sag et al., 2003) parser, Enju1 (Miyao and Tsujii, 2008) and (2) a state-of-the-art CCG parser2 (Clark and Curran, 2007). The motivation of this paper is to investigate the impact of these non-isomorphic dependency structures to be used for SMT. That is, we would like to provide a comparative evaluation of these dependencies in a string-to-dependency decoder (Shen et al., 2008). This paper presents a comparative study of target dependency structures yielded by several state-of-the-art linguistic parsers. Our approach is to measure the impact of these nonisomorphic dependency structures to be used for string-to-dependency translation. Besides using traditional dependency parsers, we also use the dependency structures transformed from PCFG trees and predicate-argument structures (PASs) which are generated by an HPSG parser and a CCG parser. The experiments on Chinese-to-English translation show that the HPSG parser’s PASs achieved the best dependency and translation a"
P12-2020,N07-1051,0,\N,Missing
P13-2004,P08-1068,0,0.0276306,"of most previous studies. We used CoNLL’03 data (Tjong Kim Sang and De Meulder, 2003) for NER, and the Penn Treebank (PTB) III corpus (Marcus et al., 1994) converted to dependency trees for DEPAR (McDonald et al., 2005). 20 4.1 Our decoding models are the Viterbi algorithm on CRF (Lafferty et al., 2001), and the secondorder parsing model proposed by (Carreras, 2007) for NER and DEPAR, respectively. Features are automatically generated according to the predefined feature templates widely-used in the previous studies. We also integrated the cluster features obtained by the method explained in (Koo et al., 2008) as additional features for evaluating our method in the range of the current best systems. Configurations of Our Method Base learning algorithm: The settings of our method in our experiments imitate L1 -regularized learning algorithm since the purpose of our experiments is to investigate the effectiveness against standard L1 -regularized learning algorithms. Then, we have the following two possible settings; DC-ADMM: we leveraged the baseline L1 -regularized learning algorithm to solve Step1, and set λ1 = 0 and λ2 = 0 for Step2. DCwL1ADMM: we leveraged the baseline L2 -regularized learning al"
P13-2004,D07-1101,0,0.0175757,"s 2 through 4 is relatively much smaller than that of Step1. 4 Experiments We conducted experiments on two well-studied NLP tasks, namely named entity recognition (NER) and dependency parsing (DEPAR). Basic settings: We simply reused the settings of most previous studies. We used CoNLL’03 data (Tjong Kim Sang and De Meulder, 2003) for NER, and the Penn Treebank (PTB) III corpus (Marcus et al., 1994) converted to dependency trees for DEPAR (McDonald et al., 2005). 20 4.1 Our decoding models are the Viterbi algorithm on CRF (Lafferty et al., 2001), and the secondorder parsing model proposed by (Carreras, 2007) for NER and DEPAR, respectively. Features are automatically generated according to the predefined feature templates widely-used in the previous studies. We also integrated the cluster features obtained by the method explained in (Koo et al., 2008) as additional features for evaluating our method in the range of the current best systems. Configurations of Our Method Base learning algorithm: The settings of our method in our experiments imitate L1 -regularized learning algorithm since the purpose of our experiments is to investigate the effectiveness against standard L1 -regularized learning al"
P13-2004,P05-1012,0,0.446259,"2011). Note that the total calculation cost of our method does not increase much from original online learning algorithm since the calculation cost of Steps 2 through 4 is relatively much smaller than that of Step1. 4 Experiments We conducted experiments on two well-studied NLP tasks, namely named entity recognition (NER) and dependency parsing (DEPAR). Basic settings: We simply reused the settings of most previous studies. We used CoNLL’03 data (Tjong Kim Sang and De Meulder, 2003) for NER, and the Penn Treebank (PTB) III corpus (Marcus et al., 1994) converted to dependency trees for DEPAR (McDonald et al., 2005). 20 4.1 Our decoding models are the Viterbi algorithm on CRF (Lafferty et al., 2001), and the secondorder parsing model proposed by (Carreras, 2007) for NER and DEPAR, respectively. Features are automatically generated according to the predefined feature templates widely-used in the previous studies. We also integrated the cluster features obtained by the method explained in (Koo et al., 2008) as additional features for evaluating our method in the range of the current best systems. Configurations of Our Method Base learning algorithm: The settings of our method in our experiments imitate L1"
P13-2004,W03-0419,0,0.0592831,"Missing"
P13-2004,P09-1054,0,0.0224915,"gative integers from zero to ζ − 1, that is, Sζ = {m}ζ−1 m=0 . For example, if we set η = 0.1, δ = 0.4, κ = 4, and ζ = 3, then Sη,δ,κ,ζ = {−2.0, −0.8, −0.5, 0, 0.5, 0.8, 2.0}. The intuition of this template is that the distribution of the feature weights in trained model often takes a form a similar to that of the ‘power law’ in the case of the large feature sets. Therefore, using an exponential function with a scale and bias seems to be appropriate for fitting them. 4 RDA provided better results at least in our experiments than L1 -regularized FOBOS (Duchi and Singer, 2009), and its variant (Tsuruoka et al., 2009), which are more familiar to the NLP community. 5 L2PA is also known as a loss augmented variant of onebest MIRA, well-known in DEPAR (McDonald et al., 2005). 21 quantized 89.0 87.0 83.0 DC-ADMM L1CRF (w/ QT) L1CRF L2CRF 81.0 1.0E+00 1.0E+03 1.0E+06 # of degrees of freedom (#DoF) [log-scale] Complete Sentence Accuracy Complete Sentence Accuracy quantized 85.0 Test NER COMP F-sc L2CRF 84.88 89.97 L1CRF 84.85 89.99 (w/ QT ζ = 4) 78.39 85.33 73.40 81.45 (w/ QT ζ = 2) (w/ QT ζ = 1) 65.53 75.87 DC-ADMM (ζ = 4) 84.96 89.92 (ζ = 2) 84.04 89.35 (ζ = 1) 83.06 88.62 Test DEPER COMP UAS L2PA 49.67 93.51"
P13-2004,P07-1104,0,0.199476,"ion variables, which are also interpreted as feature weights. L(w; D) and Ω(w) represent a loss function and a regularization term, respectively. Nowadays, we, in most cases, utilize a supervised learning method expressed as the above optimization problem to estimate the feature weights of many natural language processing (NLP) tasks, such as text classification, POS-tagging, named entity recognition, dependency parsing, and semantic role labeling. In the last decade, the L1 -regularization technique, which incorporates L1 -norm into Ω(w), has become popular and widely-used in many NLP tasks (Gao et al., 2007; Tsuruoka et al., Feature Grouping Concept Going beyond L1 -regularized sparse modeling, the idea of ‘automatic feature grouping’ has recently been developed. Examples are fused lasso (Tibshirani et al., 2005), grouping pursuit (Shen and Huang, 2010), and OSCAR (Bondell and Reich, 2008). The concept of automatic feature grouping is to find accurate models that have fewer degrees of freedom. This is equivalent to enforce every optimization variables to be equal as much as possible. A simple example is ˆ 1 = (0.1, 0.5, 0.1, 0.5, 0.1) is preferred over that w ˆ 2 = (0.1, 0.3, 0.2, 0.5, 0.3) sinc"
P13-2004,J93-2004,0,\N,Missing
P13-2038,P08-1088,0,0.238888,"e proposes latent semantic matching, which embeds objects in both source and target language domains into a shared latent topic space. We demonstrate the effectiveness of our method on cross-language text categorization. The results show that our method outperforms conventional unsupervised object matching methods. 1 Introduction Unsupervised object matching is a method for finding one-to-one correspondences between objects across different domains without knowledge about the relation between the domains. Kernelized sorting (Novi et al., 2010) and canonical correlation analysis based methods (Haghighi et al., 2008; Tripathi et al., 2010) are two such examples of unsupervised object matching, which have been shown to be quite useful for cross-language natural language processing (NLP) tasks. One of the most important properties of the unsupervised object matching is that it does not require any linguistic resources which connects between the languages. This distinguishes it from other crosslanguage NLP methods such as machine transla212 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 212–216, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computat"
P13-2038,D10-1025,0,0.0534617,"Missing"
P13-2038,W05-0802,0,\N,Missing
P14-2052,P13-1020,0,0.132407,"Missing"
P14-2052,P13-1101,1,0.60525,"Missing"
P14-2052,P11-1049,0,0.200251,"Missing"
P14-2052,C10-2105,0,0.017709,"n based on RST use EDUs as extraction textual units. We converted the rhetorical relations 12 between EDUs to the relations between sentences to build the nested tree structure. We could 10 thus take into account both relations between sentences and relations between words. 8 Figure 2: Example of one sentence. Each line corresponds to one EDU. Number of selected sentences from source document tion, their method required large sets of data to calculate the accurate probability. There have been some studies that have used discourse structures locally to optimize the order of selected sentences (Nishikawa et al., 2010; Christensen et al., 2013). 3 Generating summary from nested tree 3.1 Building Nested Tree with RST A document in RST is segmented into EDUs and adjacent EDUs are linked with rhetorical relations Extracting a subtree from the dependency tree of to build an RST-Discourse Tree (RST-DT) that has 4 words is one approach to sentence compression a hierarchical structure of the relations. There are (Tomita et al., 2009; Qian and Liu, 2013; Morita 78 types of rhetorical relations between two spans, 2 et al., 2013; Gillick and Favre, 2009). However, and each span has one of two aspects of a nuthese st"
P14-2052,W01-1605,0,0.409306,"Missing"
P14-2052,D13-1156,0,0.0298168,"ta to calculate the accurate probability. There have been some studies that have used discourse structures locally to optimize the order of selected sentences (Nishikawa et al., 2010; Christensen et al., 2013). 3 Generating summary from nested tree 3.1 Building Nested Tree with RST A document in RST is segmented into EDUs and adjacent EDUs are linked with rhetorical relations Extracting a subtree from the dependency tree of to build an RST-Discourse Tree (RST-DT) that has 4 words is one approach to sentence compression a hierarchical structure of the relations. There are (Tomita et al., 2009; Qian and Liu, 2013; Morita 78 types of rhetorical relations between two spans, 2 et al., 2013; Gillick and Favre, 2009). However, and each span has one of two aspects of a nuthese studies have only extracted rooted subtrees cleus and a satellite. The nucleus is more salient 0 from sentences. We allowed our model to extract to the discourse structure, while the other span, the EDU選択 文選択 information. 参照要約 a subtree that did not include the root word (See提案手法 satellite, represents supporting RSTthe sentence with an asterisk ∗ in Figure 1). The DT is a tree whose terminal nodes correspond method of Filippova and St"
P14-2052,N13-1136,0,0.00537175,"as extraction textual units. We converted the rhetorical relations 12 between EDUs to the relations between sentences to build the nested tree structure. We could 10 thus take into account both relations between sentences and relations between words. 8 Figure 2: Example of one sentence. Each line corresponds to one EDU. Number of selected sentences from source document tion, their method required large sets of data to calculate the accurate probability. There have been some studies that have used discourse structures locally to optimize the order of selected sentences (Nishikawa et al., 2010; Christensen et al., 2013). 3 Generating summary from nested tree 3.1 Building Nested Tree with RST A document in RST is segmented into EDUs and adjacent EDUs are linked with rhetorical relations Extracting a subtree from the dependency tree of to build an RST-Discourse Tree (RST-DT) that has 4 words is one approach to sentence compression a hierarchical structure of the relations. There are (Tomita et al., 2009; Qian and Liu, 2013; Morita 78 types of rhetorical relations between two spans, 2 et al., 2013; Gillick and Favre, 2009). However, and each span has one of two aspects of a nuthese studies have only extracted r"
P14-2052,P02-1057,0,0.0824756,"Missing"
P14-2052,P09-1075,0,0.015587,"Missing"
P14-2052,C04-1057,0,0.0689065,"rtant content in the source document. The results from an empirical evaluation revealed that our method based on the trimming of the nested tree significantly improved the summarization of texts. 1 Introduction Extractive summarization is one well-known approach to text summarization and extractive methods represent a document (or a set of documents) as a set of some textual units (e.g., sentences, clauses, and words) and select their subset as a summary. Formulating extractive summarization as a combinational optimization problem greatly improves the quality of summarization (McDonald, 2007; Filatova and Hatzivassiloglou, 2004; Takamura and Okumura, 2009). There has recently been increasing attention focused on approaches that jointly optimize sentence extraction and sentence compression (Tomita et al., 2009; 315 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 315–320, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics Source document John was running on a track in the park. He looks very tired. Mike said he is training for a race. The race is held next month. John was running on a track in the park. ＊ He looks ve"
P14-2052,W08-1105,0,0.109308,"an and Liu, 2013; Morita 78 types of rhetorical relations between two spans, 2 et al., 2013; Gillick and Favre, 2009). However, and each span has one of two aspects of a nuthese studies have only extracted rooted subtrees cleus and a satellite. The nucleus is more salient 0 from sentences. We allowed our model to extract to the discourse structure, while the other span, the EDU選択 文選択 information. 参照要約 a subtree that did not include the root word (See提案手法 satellite, represents supporting RSTthe sentence with an asterisk ∗ in Figure 1). The DT is a tree whose terminal nodes correspond method of Filippova and Strube (2008) allows the to EDUs and whose nonterminal nodes indicate model to extract non-rooted subtrees in sentence the relations. Hirao et al. converted RST-DTs compression tasks that compress a single sentence into dependency-based discourse trees (DEP-DTs) with a given compression ratio. However, it is not whose nodes corresponded to EDUs and whose trivial to apply their method to text summarizaedges corresponded to the head modifier relationtion because no compression ratio is given to senships of EDUs. See Hirao et al. for details (Hirao tences. None of these methods use the discourse et al., 2013)"
P14-2052,W09-1802,0,0.0293277,"structures locally to optimize the order of selected sentences (Nishikawa et al., 2010; Christensen et al., 2013). 3 Generating summary from nested tree 3.1 Building Nested Tree with RST A document in RST is segmented into EDUs and adjacent EDUs are linked with rhetorical relations Extracting a subtree from the dependency tree of to build an RST-Discourse Tree (RST-DT) that has 4 words is one approach to sentence compression a hierarchical structure of the relations. There are (Tomita et al., 2009; Qian and Liu, 2013; Morita 78 types of rhetorical relations between two spans, 2 et al., 2013; Gillick and Favre, 2009). However, and each span has one of two aspects of a nuthese studies have only extracted rooted subtrees cleus and a satellite. The nucleus is more salient 0 from sentences. We allowed our model to extract to the discourse structure, while the other span, the EDU選択 文選択 information. 参照要約 a subtree that did not include the root word (See提案手法 satellite, represents supporting RSTthe sentence with an asterisk ∗ in Figure 1). The DT is a tree whose terminal nodes correspond method of Filippova and Strube (2008) allows the to EDUs and whose nonterminal nodes indicate model to extract non-rooted subtr"
P14-2052,D13-1158,1,0.609684,"Missing"
P14-2052,W04-1013,0,0.03652,"Missing"
P14-2052,W98-1124,0,0.257936,"ct Qian and Liu, 2013; Morita et al., 2013; Gillick and Favre, 2009; Almeida and Martins, 2013; Berg-Kirkpatrick et al., 2011). We can only extract important content by trimming redundant parts from sentences. However, as these methods did not include the discourse structures of documents, the generated summaries lacked coherence. It is important for generated summaries to have a discourse structure that is similar to that of the source document. Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) is one way of introducing the discourse structure of a document to a summarization task (Marcu, 1998; Daum´e III and Marcu, 2002; Hirao et al., 2013). Hirao et al. recently transformed RST trees into dependency trees and used them for single document summarization (Hirao et al., 2013). They formulated the summarization problem as a tree knapsack problem with constraints represented by the dependency trees. We propose a method of summarizing a single document that utilizes dependency between sentences obtained from rhetorical structures and dependency between words obtained from a dependency parser. We have explained our method with an example in Figure 1. First, we represent a document as a"
P15-2023,W10-1749,0,0.0237746,"r to train such a classifier, we need an oracle label, W or M , for each node. Since we cannot rely on manual label annotation, we define a procedure to obtain oracle labels from word alignments. The principal idea is that we determine an oracle label of each node v(i, p, j) so that it maximizes Kendall’s τ under v(i, p, j). This is intuitively a straightforward idea, because our objective is to find a monotonic order, which indicates maximization of Kendall’s τ . In the context of statistical machine translation, Kendall’s τ is used as an evaluation metric for monotonicity of word orderings (Birch and Osborne, 2010; Isozaki et al., 2010a; Talbot et al., 2011). Given an integer list x = x1 , . . . , xn , τ (x) −c(a(p + 1, j) · a(i, p)), where · indicates a concatenation of vectors. Then, a node that has s(v(i, p, j)) < 0 is assigned W , and a node that has s(v(i, p, j)) > 0 is assigned M . All the nodes scored as s = 0 are excluded from the training data, because they are noisy and ambiguous in terms of binary classification. 2.3 Proof of Independency over Constituency The question then arises: Can oracle labels achieve the best reordering in total? We see this 2 We used median values to approximate this"
P15-2023,P05-1066,0,0.109678,"dering method, and is comparable with, or superior to, state-of-the-art methods that rely on language-specific heuristics. Our contributions are summarized as follows: Introduction Current statistical machine translation systems suffer from major accuracy degradation in distant languages, primarily because they utilize exceptionally dissimilar word orders. One promising solution to this problem is preordering, in which source sentences are reordered to resemble the target language word orders, after which statistical machine translation is applied to reordered sentences (Xia and McCord, 2004; Collins et al., 2005). This is particularly effective for distant language pairs such as English and Japanese (Isozaki et al., 2010b). Among such preordering, one of the simplest and straightforward model is a discriminative preordering model (Li et al., 2007), which classifies whether children of each constituent node should be reordered, given binary trees.1 This simple model has, however, difficulty to find oracle labels. Yang et al. (2012) proposed a method to approximate oracle labels along dependency trees. The present paper proposes a new procedure to find oracle labels. The main idea is simple: we • We def"
P15-2023,W08-0509,0,0.0154767,"BLEU ∆ RIBES ∆ ∆ +3.43 70.22 78.07 +7.85 30.51 34.13 +3.62 68.90 76.76 +7.86 29.99 33.14 +3.15 +2.99 +0.49 +2.84 +3.16 Table 5: Comparison with previous systems in Japanese-to-English translation, of which scores are retrieved from their papers. Boldfaces indicate the highest scores and differences. 8 training sets, used the first 1000 sentences in NTCIR-8 development set, and then fetched both the NTCIR-9 and NTCIR-10 testing sets. The machine translation experiments pipelined Moses 3 (Koehn et al., 2007) with lexicalized reordering, SRILM 1.7.0 (Stolcke et al., 2011) in 6-gram order, MGIZA (Gao and Vogel, 2008), and RIBES (Isozaki et al., 2010a) and BLEU (Papineni et al., 2002) for evaluation. Binary constituent parsing in Japanese used Haruniwa (Fang et al., 2014), Berkeley parser 1.7 (Petrov and Klein, 2007), Comainu 0.7.0 (Kozawa et al., 2014), MeCab 0.996 (Kudo et al., 2004), and Unidic 2.1.2. We explore two types of word alignment data for training our preordering model. The first data (Giza) is created by running an unsupervised aligner Giza (Och and Ney, 2003) on the training data (3 million sentences). The second data (Nile) is developed by training a supervised aligner Nile (Riesa et al., 2"
P15-2023,P12-2061,0,0.705339,"to our oracle labels, hence c(a) and τ (a) of entire sentence.3 Essentially, our decisions on each node are equivalent to sorting a list consists of left and right points, while the order of the points inside of left and right lists are left untouched. We determine oracle labels for a given constituent tree by computing s(v(i, p, j)) for every v(i, p, j) independently. 3 Experiment 3.1 Experimental Settings We perform experiments over the NTCIR patent corpus (Goto et al., 2011) that consists of more than 3 million sentences in English and Japanese. Following conventional literature settings (Goto et al., 2012; Hayashi et al., 2013), we used all 3 million sentences from the NTCIR-7 and NTCIR3 Oracle labels guarantee τ (a) ≥ 0, but not τ (a) = 1, because parsed trees will not correspond to word alignments. 141 test9 BLEU Reordering Methods DL RIBES ∆ Moses Proposed preordering 20 10 69.88 77.97 +8.09 Moses (Hoshino et al., 2013) Preordering (Hoshino et al., 2013) Moses (Goto et al., 2012) Moses-chart (Goto et al., 2012) Postordering (Goto et al., 2012) Moses (Hayashi et al., 2013) Postordering (Hayashi et al., 2013) 20 10 20 68.08 72.37 68.28 70.64 75.48 69.31 76.46 20 0 +4.29 +2.36 +7.20 +7.15 30.1"
P15-2023,D13-1139,1,0.949755,"ls, hence c(a) and τ (a) of entire sentence.3 Essentially, our decisions on each node are equivalent to sorting a list consists of left and right points, while the order of the points inside of left and right lists are left untouched. We determine oracle labels for a given constituent tree by computing s(v(i, p, j)) for every v(i, p, j) independently. 3 Experiment 3.1 Experimental Settings We perform experiments over the NTCIR patent corpus (Goto et al., 2011) that consists of more than 3 million sentences in English and Japanese. Following conventional literature settings (Goto et al., 2012; Hayashi et al., 2013), we used all 3 million sentences from the NTCIR-7 and NTCIR3 Oracle labels guarantee τ (a) ≥ 0, but not τ (a) = 1, because parsed trees will not correspond to word alignments. 141 test9 BLEU Reordering Methods DL RIBES ∆ Moses Proposed preordering 20 10 69.88 77.97 +8.09 Moses (Hoshino et al., 2013) Preordering (Hoshino et al., 2013) Moses (Goto et al., 2012) Moses-chart (Goto et al., 2012) Postordering (Goto et al., 2012) Moses (Hayashi et al., 2013) Postordering (Hayashi et al., 2013) 20 10 20 68.08 72.37 68.28 70.64 75.48 69.31 76.46 20 0 +4.29 +2.36 +7.20 +7.15 30.12 33.55 27.57 30.56 30."
P15-2023,I13-1147,1,0.859223,"ng s(v(i, p, j)) for every v(i, p, j) independently. 3 Experiment 3.1 Experimental Settings We perform experiments over the NTCIR patent corpus (Goto et al., 2011) that consists of more than 3 million sentences in English and Japanese. Following conventional literature settings (Goto et al., 2012; Hayashi et al., 2013), we used all 3 million sentences from the NTCIR-7 and NTCIR3 Oracle labels guarantee τ (a) ≥ 0, but not τ (a) = 1, because parsed trees will not correspond to word alignments. 141 test9 BLEU Reordering Methods DL RIBES ∆ Moses Proposed preordering 20 10 69.88 77.97 +8.09 Moses (Hoshino et al., 2013) Preordering (Hoshino et al., 2013) Moses (Goto et al., 2012) Moses-chart (Goto et al., 2012) Postordering (Goto et al., 2012) Moses (Hayashi et al., 2013) Postordering (Hayashi et al., 2013) 20 10 20 68.08 72.37 68.28 70.64 75.48 69.31 76.46 20 0 +4.29 +2.36 +7.20 +7.15 30.12 33.55 27.57 30.56 30.20 30.69 33.04 29.43 32.59 test10 BLEU ∆ RIBES ∆ ∆ +3.43 70.22 78.07 +7.85 30.51 34.13 +3.62 68.90 76.76 +7.86 29.99 33.14 +3.15 +2.99 +0.49 +2.84 +3.16 Table 5: Comparison with previous systems in Japanese-to-English translation, of which scores are retrieved from their papers. Boldfaces indicate th"
P15-2023,D10-1092,1,0.885359,"Missing"
P15-2023,W10-1736,1,0.961659,"heuristics. Our contributions are summarized as follows: Introduction Current statistical machine translation systems suffer from major accuracy degradation in distant languages, primarily because they utilize exceptionally dissimilar word orders. One promising solution to this problem is preordering, in which source sentences are reordered to resemble the target language word orders, after which statistical machine translation is applied to reordered sentences (Xia and McCord, 2004; Collins et al., 2005). This is particularly effective for distant language pairs such as English and Japanese (Isozaki et al., 2010b). Among such preordering, one of the simplest and straightforward model is a discriminative preordering model (Li et al., 2007), which classifies whether children of each constituent node should be reordered, given binary trees.1 This simple model has, however, difficulty to find oracle labels. Yang et al. (2012) proposed a method to approximate oracle labels along dependency trees. The present paper proposes a new procedure to find oracle labels. The main idea is simple: we • We define a method for obtaining oracle labels in discriminative preordering as the maximization of Kendall’s τ . •"
P15-2023,N07-1051,0,0.0573074,"which scores are retrieved from their papers. Boldfaces indicate the highest scores and differences. 8 training sets, used the first 1000 sentences in NTCIR-8 development set, and then fetched both the NTCIR-9 and NTCIR-10 testing sets. The machine translation experiments pipelined Moses 3 (Koehn et al., 2007) with lexicalized reordering, SRILM 1.7.0 (Stolcke et al., 2011) in 6-gram order, MGIZA (Gao and Vogel, 2008), and RIBES (Isozaki et al., 2010a) and BLEU (Papineni et al., 2002) for evaluation. Binary constituent parsing in Japanese used Haruniwa (Fang et al., 2014), Berkeley parser 1.7 (Petrov and Klein, 2007), Comainu 0.7.0 (Kozawa et al., 2014), MeCab 0.996 (Kudo et al., 2004), and Unidic 2.1.2. We explore two types of word alignment data for training our preordering model. The first data (Giza) is created by running an unsupervised aligner Giza (Och and Ney, 2003) on the training data (3 million sentences). The second data (Nile) is developed by training a supervised aligner Nile (Riesa et al., 2011) with manually annotated 8,000 sentences, then applied the trained alignment model to remaining training data. In the evaluation on manually annotated 1,000 sentences4 , Giza achieved F1 50.1 score,"
P15-2023,D11-1046,0,0.173982,"Missing"
P15-2023,W04-3250,0,0.55929,"Missing"
P15-2023,2011.mtsummit-papers.36,1,0.933737,"Missing"
P15-2023,W11-2102,0,0.013815,"label, W or M , for each node. Since we cannot rely on manual label annotation, we define a procedure to obtain oracle labels from word alignments. The principal idea is that we determine an oracle label of each node v(i, p, j) so that it maximizes Kendall’s τ under v(i, p, j). This is intuitively a straightforward idea, because our objective is to find a monotonic order, which indicates maximization of Kendall’s τ . In the context of statistical machine translation, Kendall’s τ is used as an evaluation metric for monotonicity of word orderings (Birch and Osborne, 2010; Isozaki et al., 2010a; Talbot et al., 2011). Given an integer list x = x1 , . . . , xn , τ (x) −c(a(p + 1, j) · a(i, p)), where · indicates a concatenation of vectors. Then, a node that has s(v(i, p, j)) < 0 is assigned W , and a node that has s(v(i, p, j)) > 0 is assigned M . All the nodes scored as s = 0 are excluded from the training data, because they are noisy and ambiguous in terms of binary classification. 2.3 Proof of Independency over Constituency The question then arises: Can oracle labels achieve the best reordering in total? We see this 2 We used median values to approximate this y-th word in the target sentence for simplic"
P15-2023,W04-3230,0,0.165103,"Missing"
P15-2023,C04-1073,0,0.176348,"rms a rule-based preordering method, and is comparable with, or superior to, state-of-the-art methods that rely on language-specific heuristics. Our contributions are summarized as follows: Introduction Current statistical machine translation systems suffer from major accuracy degradation in distant languages, primarily because they utilize exceptionally dissimilar word orders. One promising solution to this problem is preordering, in which source sentences are reordered to resemble the target language word orders, after which statistical machine translation is applied to reordered sentences (Xia and McCord, 2004; Collins et al., 2005). This is particularly effective for distant language pairs such as English and Japanese (Isozaki et al., 2010b). Among such preordering, one of the simplest and straightforward model is a discriminative preordering model (Li et al., 2007), which classifies whether children of each constituent node should be reordered, given binary trees.1 This simple model has, however, difficulty to find oracle labels. Yang et al. (2012) proposed a method to approximate oracle labels along dependency trees. The present paper proposes a new procedure to find oracle labels. The main idea"
P15-2023,P14-2091,0,0.0664321,"bstantial gain in RIBES, we attained a rather comparable gain in BLEU. The investigation of our translation suggests that insufficient generation of English articles caused a significant degradation in the BLEU score. Previous systems listed in Table 5 incorporated article generation and demonstrated its positive effect (Goto et al., 2012; Hayashi et al., 2013). While we achieved state-ofthe-art accuracy without language-specific techniques, it is also a promising direction to integrate our preordering method with language-specific techniques such as article generation and subject generation (Kudo et al., 2014). 3.2 Result Table 4 shows the performance of our method, which indicates that our preordering significantly improved translation accuracy in both RIBES and BLEU scores, from the baseline result attained by Moses without preordering. In particular, the preordering model trained with the Giza data revealed a substantial improvement, while the use of the Nile data further improves accuracy. This suggests that our method is particularly effective when high-accuracy word alignments are given. In 4 5 We could not find a comparable report using tree-based machine translation systems apart from Moses"
P15-2023,P12-1096,0,0.368513,"sentences are reordered to resemble the target language word orders, after which statistical machine translation is applied to reordered sentences (Xia and McCord, 2004; Collins et al., 2005). This is particularly effective for distant language pairs such as English and Japanese (Isozaki et al., 2010b). Among such preordering, one of the simplest and straightforward model is a discriminative preordering model (Li et al., 2007), which classifies whether children of each constituent node should be reordered, given binary trees.1 This simple model has, however, difficulty to find oracle labels. Yang et al. (2012) proposed a method to approximate oracle labels along dependency trees. The present paper proposes a new procedure to find oracle labels. The main idea is simple: we • We define a method for obtaining oracle labels in discriminative preordering as the maximization of Kendall’s τ . • We give a theoretical background to Kendall’s τ based reordering for binary constituent trees. • We achieve state-of-the-art accuracy in Japanese-to-English translation with a simple method without language-specific heuristics. 1 It is also possible to use n-ary trees (Li et al., 2007; Yang et al., 2012), but we ke"
P15-2023,D13-1049,0,0.0380846,"suke Mori, Toshiaki Nakazawa, Graham Neubig, Hiroshi Noji, and anonymous reviewers for their insightful comments. Li et al. (2007) proposed a simple discriminative preordering model as described in Section 2.1. They employed heuristics that utilize Giza to align their training sentences, then sort source words to resemble target word indices. After that, sorted source sentences without overlaps are used to train the model. They gained BLEU +1.54 improvement in Chinese-to-English evaluation. Our proposal follows their model, while we do not rely on their heuristics for preparing training data. Lerner and Petrov (2013) proposed another discriminative preordering model along dependency trees, which classifies whether the parent of each node should be the head in target language. They reported BLEU +3.7 improvement in English-to-Japanese translation. Hoshino et al. (2013) proposed a similar but rule-based method for Japanese-to-English dependency preordering. Yang et al. (2012) proposed a method to produce oracle reordering in the discriminative preordering model along dependency trees. Their idea behind is to minimize word alignment crossing recursively, which is essentially the same reordering objective as"
P15-2023,P07-1091,0,0.642315,"or accuracy degradation in distant languages, primarily because they utilize exceptionally dissimilar word orders. One promising solution to this problem is preordering, in which source sentences are reordered to resemble the target language word orders, after which statistical machine translation is applied to reordered sentences (Xia and McCord, 2004; Collins et al., 2005). This is particularly effective for distant language pairs such as English and Japanese (Isozaki et al., 2010b). Among such preordering, one of the simplest and straightforward model is a discriminative preordering model (Li et al., 2007), which classifies whether children of each constituent node should be reordered, given binary trees.1 This simple model has, however, difficulty to find oracle labels. Yang et al. (2012) proposed a method to approximate oracle labels along dependency trees. The present paper proposes a new procedure to find oracle labels. The main idea is simple: we • We define a method for obtaining oracle labels in discriminative preordering as the maximization of Kendall’s τ . • We give a theoretical background to Kendall’s τ based reordering for binary constituent trees. • We achieve state-of-the-art accu"
P15-2023,P14-2024,0,0.0127024,"hows the performance of our method, which indicates that our preordering significantly improved translation accuracy in both RIBES and BLEU scores, from the baseline result attained by Moses without preordering. In particular, the preordering model trained with the Giza data revealed a substantial improvement, while the use of the Nile data further improves accuracy. This suggests that our method is particularly effective when high-accuracy word alignments are given. In 4 5 We could not find a comparable report using tree-based machine translation systems apart from Moses-chart; nevertheless, Neubig and Duh (2014) reported that their forestto-string system on the same corpus, which is unfortunately evaluated on the different testing data (test7), showed RIBES +6.19 (75.94) and BLEU +2.93 (33.70) improvements. Although not directly comparable, our method achieves a comparable or superior improvement. This testing data is excluded from latter experiments. 142 4 Related Work Acknowledgments We would like to thank Kevin Duh, Atsushi Fujita, Taku Kudo, Shinsuke Mori, Toshiaki Nakazawa, Graham Neubig, Hiroshi Noji, and anonymous reviewers for their insightful comments. Li et al. (2007) proposed a simple disc"
P15-2023,J03-1002,0,0.00843586,"ments pipelined Moses 3 (Koehn et al., 2007) with lexicalized reordering, SRILM 1.7.0 (Stolcke et al., 2011) in 6-gram order, MGIZA (Gao and Vogel, 2008), and RIBES (Isozaki et al., 2010a) and BLEU (Papineni et al., 2002) for evaluation. Binary constituent parsing in Japanese used Haruniwa (Fang et al., 2014), Berkeley parser 1.7 (Petrov and Klein, 2007), Comainu 0.7.0 (Kozawa et al., 2014), MeCab 0.996 (Kudo et al., 2004), and Unidic 2.1.2. We explore two types of word alignment data for training our preordering model. The first data (Giza) is created by running an unsupervised aligner Giza (Och and Ney, 2003) on the training data (3 million sentences). The second data (Nile) is developed by training a supervised aligner Nile (Riesa et al., 2011) with manually annotated 8,000 sentences, then applied the trained alignment model to remaining training data. In the evaluation on manually annotated 1,000 sentences4 , Giza achieved F1 50.1 score, while Nile achieved F1 86.9 score, for word alignment task. addition, we achieved modest improvements even with DL=0 (no distortion allowed), which indicates the monotonicity of our reordered sentences. Table 5 shows a comparison of the proposed method with a ru"
P15-2023,P02-1040,0,0.092262,"6.76 +7.86 29.99 33.14 +3.15 +2.99 +0.49 +2.84 +3.16 Table 5: Comparison with previous systems in Japanese-to-English translation, of which scores are retrieved from their papers. Boldfaces indicate the highest scores and differences. 8 training sets, used the first 1000 sentences in NTCIR-8 development set, and then fetched both the NTCIR-9 and NTCIR-10 testing sets. The machine translation experiments pipelined Moses 3 (Koehn et al., 2007) with lexicalized reordering, SRILM 1.7.0 (Stolcke et al., 2011) in 6-gram order, MGIZA (Gao and Vogel, 2008), and RIBES (Isozaki et al., 2010a) and BLEU (Papineni et al., 2002) for evaluation. Binary constituent parsing in Japanese used Haruniwa (Fang et al., 2014), Berkeley parser 1.7 (Petrov and Klein, 2007), Comainu 0.7.0 (Kozawa et al., 2014), MeCab 0.996 (Kudo et al., 2004), and Unidic 2.1.2. We explore two types of word alignment data for training our preordering model. The first data (Giza) is created by running an unsupervised aligner Giza (Och and Ney, 2003) on the training data (3 million sentences). The second data (Nile) is developed by training a supervised aligner Nile (Riesa et al., 2011) with manually annotated 8,000 sentences, then applied the train"
P15-2023,P07-2045,0,\N,Missing
P15-2031,N09-1003,0,0.258112,"Missing"
P15-2031,P12-1092,0,0.164755,"Missing"
P15-2031,Q15-1016,0,0.646872,"ity and analogy tasks. Introduction Neural-network-inspired word embedding methods such as Skip-Gram (SkipGram) have been proven to capture high quality syntactic and semantic relationships between words in a vector space (Mikolov et al., 2013a). A similar embedding method, called ‘Global Vector (GloVe)’, was recently proposed. It has demonstrated significant improvements over SkipGram on the widely used ‘Word Analogy’ and ‘Word Similarity’ benchmark datasets (Pennington et al., 2014). Unfortunately, a later deep re-evaluation has revealed that GloVe does not consistently outperform SkipGram (Levy et al., 2015); both methods provided basically the same level of performance, and SkipGram even seems ‘more robust (not yielding very poor results)’ than GloVe. Moreover, some other papers, i.e., (Shi and Liu, 2014), and some researchers in the community have discussed a relationship, and/or which is superior, SkipGram or GloVe. From this background, we revisit the relationship between SkipGram and GloVe from a machine learning viewpoint. We show that it is nat2 SkipGram and GloVe Table 1 shows the notations used in this paper. 2.1 Matrix factorization view of SkipGram SkipGram can be categorized as one of"
P15-2031,W13-3512,0,0.161407,"Missing"
P15-2031,N13-1090,0,0.406305,"learning configurations. The final goal of this paper is to provide a unified learning framework that encompasses the configurations used in SkipGram and GloVe to gain a deeper understanding of the behavior of these embedding methods. We also empirically investigate which learning configuration most clearly elucidates the performance difference often observed in word similarity and analogy tasks. Introduction Neural-network-inspired word embedding methods such as Skip-Gram (SkipGram) have been proven to capture high quality syntactic and semantic relationships between words in a vector space (Mikolov et al., 2013a). A similar embedding method, called ‘Global Vector (GloVe)’, was recently proposed. It has demonstrated significant improvements over SkipGram on the widely used ‘Word Analogy’ and ‘Word Similarity’ benchmark datasets (Pennington et al., 2014). Unfortunately, a later deep re-evaluation has revealed that GloVe does not consistently outperform SkipGram (Levy et al., 2015); both methods provided basically the same level of performance, and SkipGram even seems ‘more robust (not yielding very poor results)’ than GloVe. Moreover, some other papers, i.e., (Shi and Liu, 2014), and some researchers"
P15-2031,D14-1162,0,0.121142,"empirically investigate which learning configuration most clearly elucidates the performance difference often observed in word similarity and analogy tasks. Introduction Neural-network-inspired word embedding methods such as Skip-Gram (SkipGram) have been proven to capture high quality syntactic and semantic relationships between words in a vector space (Mikolov et al., 2013a). A similar embedding method, called ‘Global Vector (GloVe)’, was recently proposed. It has demonstrated significant improvements over SkipGram on the widely used ‘Word Analogy’ and ‘Word Similarity’ benchmark datasets (Pennington et al., 2014). Unfortunately, a later deep re-evaluation has revealed that GloVe does not consistently outperform SkipGram (Levy et al., 2015); both methods provided basically the same level of performance, and SkipGram even seems ‘more robust (not yielding very poor results)’ than GloVe. Moreover, some other papers, i.e., (Shi and Liu, 2014), and some researchers in the community have discussed a relationship, and/or which is superior, SkipGram or GloVe. From this background, we revisit the relationship between SkipGram and GloVe from a machine learning viewpoint. We show that it is nat2 SkipGram and GloV"
P15-2039,W02-2016,0,0.511379,"Missing"
P15-2039,W08-1301,0,0.570359,"Missing"
P15-2039,de-marneffe-etal-2014-universal,0,0.104896,"Missing"
P15-2039,W07-1522,0,0.031105,"rticles に and で “de” are widely used outside the temporal and locative cases. 5 Conclusion Predicate-argument structure We extracted predicate-argument structure information as triplets, which are pairs of predicates and arguments connected by a relation, i.e. (pred , rel , arg), from the dependency parsing results by tracing the paths with the argument and gapped relative clause types. pred in a triplet is a verb or an adjective, arg is a head noun of an argument, and rel is nsubj, dobj or iobj. The gold standard data is built by converting predicate-argument structures in NAIST Text Corpus (Iida et al., 2007) into the above triples. Basically, the cases “ga”, “o” and “ni” in the corpus correspond to “nsubj”, “dobj“ and “iobj”, respectively, however, we should apply the alternative conversion to passive or causative voice, since the annotation is based on active voice. The conversion for case alternation was manually done for We proposed a scheme of Japanese typeddependency parsing for dealing with constituents and capturing the grammatical function as a dependency type that bypasses the traditional limitations of bunsetsu-based dependency parsing. The evaluations demonstrated that a word-based dep"
P15-2039,P11-1081,0,0.0496291,"Missing"
P15-2039,W13-4913,1,0.89034,"Missing"
P15-2039,N06-1023,0,0.338047,"Missing"
P15-2039,uchimoto-den-2008-word,0,0.714465,"Missing"
P15-2039,W04-3230,0,0.229706,"Missing"
P15-2039,den-etal-2008-proper,0,\N,Missing
P15-2039,mori-etal-2014-japanese,0,\N,Missing
P15-2039,maekawa-etal-2000-spontaneous,0,\N,Missing
P16-2016,K15-1029,0,0.171409,"Missing"
P16-2016,W08-2102,0,0.434935,"ituents, provide important information for understanding the semantic structure of sentences. Previous studies attempt empty element recovery by casting it as linear tagging (Dienes and Dubey, 2003), PCFG parsing (Schmid, 2006; Cai et al., 2011) or post-processing of syntactic parsing (Johnson, 2002; Gabbard et al., 2006). To the best of our knowledge, the results reported by (Cai et al., 2011) are the best yet reported, so we pursue a method that uses syntactic parsing to jointly solve the empty element recovery problem. Our proposal uses the spinal Tree Adjoining Grammar (TAG) formalism of (Carreras et al., 2008). The spinal TAG has a set of elementary trees, called spines, each consisting of a lexical anchor with a series of unary projections. Figure 1 displays (a) a head-annotated constituent tree and (b) spines extracted from the tree. This paper presents a transition-based algorithm together with several operations to combine spines for constructing full parse trees with empty elements. Compared with the PCFG parsing approaches, one advantage of our method is its flexible feature representations, which allow the incorporation of constituency-, dependency- and spine-based features. Of particular in"
P16-2016,C02-1126,0,0.0796641,"pes increased to 63. with parsing (Schmid, 2006; Cai et al., 2011). Schmid (2006) annotated a constituent tree with slash features to recover a direct path from a filler node to its trace. Cai et al. (2011) successfully integrated empty element recovery into lattice parsing for latent PCFGs. Compared with PCFG parsing, the spinal TAG parser provides a more flexible feature representation. We used the Wall Street Journal (WSJ) part of the English Penn Treebank: Sections 02–21 were used for training, Section 22 for development, and Section 23 for testing. We annotated trees with heads by treep (Chiang and Bikel, 2002)3 with the application of Collins’s head rules. The 78524 lexical and 115 phrasal empty spine types were obtained from the training data4 . The set of phrasal empty spines covered all phrasal empty spines extracted from the development data. We used the Stanford part-of-speech tagger to tag development and test data. To train the proposed parsing model, we used the violation–fixing Table 1 shows final results on Section 23. To evaluate the accuracy of empty element recovery, we calculated precision, recall and F1 scores for (1) Labeled Empty Bracket (X/t,i,i), (2) Labeled Empty Element (t,i,i)"
P16-2016,P03-1055,0,0.65021,"nglish and Japanese empty element recovery problems. 1 . S. NP . . PRP-H . MD-H . . VP-H . . VP . . . . must . . VB-H . We . . find NP . . . . NP-H SBAR . . DT . NN-H . WHADVP-H . any . way . . . S. 0-H . NP . . *e* . *-H . TO-H . VP-H . . . to. . *e* Introduction . Empty categories, which are used in Penn Treebank style annotations to represent complex syntactic phenomena like constituent movement and discontinuous constituents, provide important information for understanding the semantic structure of sentences. Previous studies attempt empty element recovery by casting it as linear tagging (Dienes and Dubey, 2003), PCFG parsing (Schmid, 2006; Cai et al., 2011) or post-processing of syntactic parsing (Johnson, 2002; Gabbard et al., 2006). To the best of our knowledge, the results reported by (Cai et al., 2011) are the best yet reported, so we pursue a method that uses syntactic parsing to jointly solve the empty element recovery problem. Our proposal uses the spinal Tree Adjoining Grammar (TAG) formalism of (Carreras et al., 2008). The spinal TAG has a set of elementary trees, called spines, each consisting of a lexical anchor with a series of unary projections. Figure 1 displays (a) a head-annotated co"
P16-2016,N12-1015,0,0.0256534,"3 12 2 4 0 43 5 26 78 6 30 Table 2: Result Analysis: M denotes the number of matches of system outputs (O) with the gold. parsing time (secs) 2 0.5 Rule Takeno15 57.4 60.4 50.5 50.6 53.7 55.1 – – – – – – Tagger Lattice Proposed 63.1 64.1 65.3 34.7 52.2 57.6 44.8 57.5 61.2 72.9 73.7 74.3 68.6 70.6 72.8 70.7 72.1 73.6 • Lattice is a method by Cai et al. (2011). We also used blatt5 , which is an extension of the Berkeley parser, to parse word lattices in which the special word *e* is encoded as described in (Cai et al., 2011). 0 20 30 40 50 sentence length All Brackets R F1 perceptron algorithm (Huang et al., 2012). For training and testing, we set beam size to 16 and max count b, introduced in Section 4.2, to 2. For comparison with other systems in our environment, we also implemented two systems: 1 10 P Table 3: Results on the Japanese Keyaki Treebank. H16 Berk C11 Prop 1.5 Typed-empty (t,i,i) P R F1 Gold 60 70 Figure 4: Scatter plot of parsing time against sentence length, comparing with Hayashi16, Berkeley and Cai11 parsers. • Tagger decides whether some empty category is inserted at the front of a word or not, with regularized logistic regression. To simplify point-wise linear tagging, we combined"
P16-2016,P02-1018,0,0.626535,"VB-H . We . . find NP . . . . NP-H SBAR . . DT . NN-H . WHADVP-H . any . way . . . S. 0-H . NP . . *e* . *-H . TO-H . VP-H . . . to. . *e* Introduction . Empty categories, which are used in Penn Treebank style annotations to represent complex syntactic phenomena like constituent movement and discontinuous constituents, provide important information for understanding the semantic structure of sentences. Previous studies attempt empty element recovery by casting it as linear tagging (Dienes and Dubey, 2003), PCFG parsing (Schmid, 2006; Cai et al., 2011) or post-processing of syntactic parsing (Johnson, 2002; Gabbard et al., 2006). To the best of our knowledge, the results reported by (Cai et al., 2011) are the best yet reported, so we pursue a method that uses syntactic parsing to jointly solve the empty element recovery problem. Our proposal uses the spinal Tree Adjoining Grammar (TAG) formalism of (Carreras et al., 2008). The spinal TAG has a set of elementary trees, called spines, each consisting of a lexical anchor with a series of unary projections. Figure 1 displays (a) a head-annotated constituent tree and (b) spines extracted from the tree. This paper presents a transition-based algorith"
P16-2016,P06-1023,0,0.84659,"ery problems. 1 . S. NP . . PRP-H . MD-H . . VP-H . . VP . . . . must . . VB-H . We . . find NP . . . . NP-H SBAR . . DT . NN-H . WHADVP-H . any . way . . . S. 0-H . NP . . *e* . *-H . TO-H . VP-H . . . to. . *e* Introduction . Empty categories, which are used in Penn Treebank style annotations to represent complex syntactic phenomena like constituent movement and discontinuous constituents, provide important information for understanding the semantic structure of sentences. Previous studies attempt empty element recovery by casting it as linear tagging (Dienes and Dubey, 2003), PCFG parsing (Schmid, 2006; Cai et al., 2011) or post-processing of syntactic parsing (Johnson, 2002; Gabbard et al., 2006). To the best of our knowledge, the results reported by (Cai et al., 2011) are the best yet reported, so we pursue a method that uses syntactic parsing to jointly solve the empty element recovery problem. Our proposal uses the spinal Tree Adjoining Grammar (TAG) formalism of (Carreras et al., 2008). The spinal TAG has a set of elementary trees, called spines, each consisting of a lexical anchor with a series of unary projections. Figure 1 displays (a) a head-annotated constituent tree and (b) spine"
P16-2016,D15-1156,1,0.875115,"Missing"
P16-2016,P13-1081,0,0.0170177,". *e* . . NP . S. VP . . DT . NP . PRP . VP . VB . any . NN . WHADVP . We . MD . find . SBAR . way . must . 0. *e* . ADVP . NN-H . *T*-H . NP . S. VP . NP . ADVP . *. VP . VB . NN . *T* . *e* . TO . get . business . *e* . to. Figure 1: (a) an example of a constituent tree with head annotations denoted by -H; (b) spinal elementary trees extracted from the parse tree (a). intuition that features extracted from spines can be expected to be useful for empty element recovery in the same way as constituency-based vertical higher-order conjunctive features are used in recent post-processing methods (Xiang et al., 2013; Takeno et al., 2015). Experiments on English and Japanese datasets empirically show that our system outperforms existing alternatives. 2 Spinal Tree Adjoining Grammars We define here the spinal TAG G = (N, PT, T, LS) where N is a set of nonterminal symbols, PT is a set of pre-terminal symbols (or part-of-speech tags), T is a set of terminal symbols (or words), and LS is a set of lexical spines. Each spine, s, has the form n0 → n1 → · · · → nk−1 → nk (k ∈ N) which satisfies the conditions: • n0 ∈ T and n1 ∈ PT , • ∀i ∈ [2, k], ni ∈ N. The height of spine s is ht(s) = k + 1 and for some positi"
P16-2016,P13-1043,0,0.0352388,"and combine, both of which have left and right types. Figures 2 (c) and (d) show insert left and combine right operations. These operations are similar to sister adjunctions in that the former simply inserts some phrasal empty spine into some node of another spine and the latter also inserts a spine into (σ |s1 , β , A) ⊢ (σ |s′1 , β , A ∪ {s1 .s  ▷ j s}) and a combine right transition of the form ◁ j s1 .s}) (σ |s1 , β , A) ⊢ (σ |s′1 , β , A ∪ {s  where s′1 = (s, j); 11. an idle transition of the form (σ |s1 , β , A) ⊢ (σ |s1 , β , A); Like unary and idle rules in shift-reduce CFG parsing (Zhu et al., 2013), our current system prohibits > b consecutive actions consisting of only insert, combine and idle operations. Given an input sentence with length n, after performing n shift, n − 1 adjunction, b · (2n − 1) {insert, combine or idle} actions, the system triggers the finish action and terminates. For training, we make oracle derivations using the stack-shortest strategy. 5 Related Work To realize empty element recovery, other lexicalized TAG formalisms (Chen and Shanker, 2004; Shen et al., 2008) attach some or all empty elements directly to surface word lexicons. Our framework, however, uses spi"
P16-2016,N06-1024,0,\N,Missing
P16-2016,P10-1110,0,\N,Missing
P16-2066,P07-2045,0,0.0148058,"ed in the training corpus1 . Thus our algorithm does not cause exponential explosion of the computation time with longer phrases. log [c(X, fik ) + 1] i=1 k=1 + |ei | N X X 4 log [c(X, eik ) + 1] , i=1 k=1 Evaluation 4.1 where c(X, fik ) is the number of phrase pairs contained in X that cover fik , the k-th word of the ith source sentence fi . Similarly, c(X, eik ) is the number of phrase pairs that cover eik . Settings We conducted experiments on the ChineseEnglish and Arabic-English datasets used in NIST OpenMT 2012. In each experiment, English was set as the target language. We used Moses (Koehn et al., 2007) as the phrase-based machine translation system. We used the 5-gram Kneser-Ney language model trained separately using the English GigaWord V5 corpus (LDC2011T07), a monolingual corpus distributed at WMT 2012, and Google Web 1T 5-gram data (LDC2006T13). Word alignments are obtained by running giza++ (Och and Ney, 2003) included in the Moses system. As the test data, we used 1378 segments for the Arabic-English dataset and 2190 segments for the Chinese-English dataset, where all test segments have 4 references (LDC2013T07, LDC2013T03). The tuning set consists of about 5000 segments gathered fro"
P16-2066,N10-1134,0,0.211612,"en it satisfies the condition g(X ∪ {x}) − g(X) ≥ g(Y ∪ {x}) − g(Y ) , x∈ΩX X ← X ∪ {x∗ } 5: output X 4: 2Ω , where X, Y ∈ X ⊆ Y , and x ∈ Ω  Y . This condition represents the diminishing return property of a submodular function, i.e., the increase in the value of the function due to the addition of item x to Y is always smaller than that obtained by adding x to any subset X ⊆ Y . We say a submodular function is monotone if g(Y ) ≥ g(X) for any X, Y ∈ 2Ω satisfying X ⊆ Y . Since a submodular function has many useful properties, it appears in a wide range of applications (Kempe et al., 2003; Lin and Bilmes, 2010; Kirchhoff and Bilmes, 2014). The maximization problem of a monotone submodular function under cardinality constraints is formulated as could solve the problems in 24 hours. Moreover, further enhancement can be achieved by applying distributed algorithms (Mirzasoleiman et al., 2013) and stochastic greedy algorithms (Mirzasoleiman et al., 2015). 3 Phrase Table Pruning We first define some notations. Let Ω = {x1 , . . . , xM } be a phrase table that has M phrase pairs. Each phrase pair, xi , consists of a source language phrase, pi , and a target language phrase, qi , and is written as xi = hpi"
P16-2066,N09-1015,0,0.0479695,"Missing"
P16-2066,P11-1052,0,0.0349508,"on of the number of phrase pairs (Chinese-English). methods (Ling et al., 2012; Zens et al., 2012), a significance-based method (Johnson et al., 2007), and our method are self-contained methods. Non self-contained methods exploit usage statistics for phrase pairs (Eck et al., 2007) and additional bilingual corpora (Chen et al., 2009). Since self contained methods require additional resources, it is easy to apply to existing MT systems. Effectiveness of the submodular functions maximization formulation is confirmed in various NLP applications including text summarization (Lin and Bilmes, 2010; Lin and Bilmes, 2011) and training data selection for machine translation (Kirchhoff and Bilmes, 2014). These methods are used for selecting a subset that contains important items but not redundant items. This paper can be seen as applying the subset selection formulation to the phrase table pruning problem. Related Work 6 Previous phrase table pruning methods fall into two groups. Self-contained methods only use resources already used in the MT system, e.g., training corpus and phrase tables. Entropy-based Conclusion We have introduced a method that solves the phrase table pruning problem as a submodular function"
P16-2066,P08-2007,0,0.0293061,"aki,suzuki.jun,nagata.masaaki}@lab.ntt.co.jp Abstract ing method (Ling et al., 2012; Zens et al., 2012) offers the best performance. The entropy-based pruning method uses entropy to measure the redundancy of a phrase pair, where we say a phrase pair is redundant if it can be replaced by other phrase pairs. The entropy-based pruning method runs in time linear to the number of phrase-pairs. Unfortunately, its running time is also exponential to the length of phrases contained in the phrase pairs, since it contains the problem of finding an optimal phrase alignment, which is known to be NP-hard (DeNero and Klein, 2008). Therefore, the method can be impractical if the phrase pairs consist of longer phrases. Phrase table pruning is the act of removing phrase pairs from a phrase table to make it smaller, ideally removing the least useful phrases first. We propose a phrase table pruning method that formulates the task as a submodular function maximization problem, and solves it by using a greedy heuristic algorithm. The proposed method can scale with input size and long phrases, and experiments show that it achieves higher BLEU scores than state-of-the-art pruning methods. 1 Introduction In this paper, we intro"
P16-2066,D12-1088,0,0.0392014,"Missing"
P16-2066,N07-2006,0,0.071812,"Missing"
P16-2066,J03-1002,0,0.0102312,"ith source sentence fi . Similarly, c(X, eik ) is the number of phrase pairs that cover eik . Settings We conducted experiments on the ChineseEnglish and Arabic-English datasets used in NIST OpenMT 2012. In each experiment, English was set as the target language. We used Moses (Koehn et al., 2007) as the phrase-based machine translation system. We used the 5-gram Kneser-Ney language model trained separately using the English GigaWord V5 corpus (LDC2011T07), a monolingual corpus distributed at WMT 2012, and Google Web 1T 5-gram data (LDC2006T13). Word alignments are obtained by running giza++ (Och and Ney, 2003) included in the Moses system. As the test data, we used 1378 segments for the Arabic-English dataset and 2190 segments for the Chinese-English dataset, where all test segments have 4 references (LDC2013T07, LDC2013T03). The tuning set consists of about 5000 segments gathered from MT02 to MT06 evaluation sets (LDC2010T10, LDC2010T11, LDC2010T12, LDC2010T14, LDC2010T17). We set the maximum length of extracted phrases to 7. Table 1 shows the sizes of phrase tables. Following the settings used in (Zens et al., 2012), we reduce the effects of other components by using the same feature weights obta"
P16-2066,P03-1021,0,0.0406272,"ta, we used 1378 segments for the Arabic-English dataset and 2190 segments for the Chinese-English dataset, where all test segments have 4 references (LDC2013T07, LDC2013T03). The tuning set consists of about 5000 segments gathered from MT02 to MT06 evaluation sets (LDC2010T10, LDC2010T11, LDC2010T12, LDC2010T14, LDC2010T17). We set the maximum length of extracted phrases to 7. Table 1 shows the sizes of phrase tables. Following the settings used in (Zens et al., 2012), we reduce the effects of other components by using the same feature weights obtained by running the MERT training algorithm (Och, 2003) on full size phrase tables and tuning data to all pruned tables. We run MERT for 10 times to obtain 10 different feature weights. The BLEU scores reported in the following experiments are the averages of the results obtained by using these different feature weights. We adopt the entropy-based pruning method used in (Ling et al., 2012; Zens et al., 2012) as the baseline method, since it shows best BLEU Example 1. Consider phrase table X holding phrase pairs x1 = h(das Haus), (the house)i, x2 = h(Haus), (house)i, and x3 = h(das Haus), (the building)i. If a corpus consists of a pair of sentences"
P16-2066,D12-1089,0,0.0901783,"nvironments such as mobile phones. Even if a computer has enough resources, the large phrase tables increase turnaround time and prevent the rapid development of MT systems. Phrase table pruning is the technique of removing ineffective phrase pairs from a phrase table to make it smaller while minimizing the performance degradation. Existing phrase table pruning methods use different metrics to rank the phrase pairs contained in the table, and then remove lowranked pairs. Metrics used in previous work are frequency, conditional probability, and Fisher’s exact test score (Johnson et al., 2007). Zens et al. (2012) evaluated many phrase table pruning methods, and concluded that entropy-based prunOne key factor of the proposed method is its carefully designed objective function that evaluates the quality of a given phrase table. In this paper, we use a simple monotone submodular function that evaluates the quality of a given phrase table by its coverage of a training corpus. Our method is simple, parameter free, and does not cause exponential explosion of the computation time with longer phrases. We conduct experiments with two different language pairs, and show that the proposed method shows higher BLEU"
P16-2066,D07-1103,0,\N,Missing
P16-2066,D14-1014,0,\N,Missing
P17-2043,P11-1049,0,0.0707887,"pper bound performance of the paradigm. Experimental results on the DUC dataset showed that ROUGE scores of compressive oracles are significantly higher than those of extractive oracles and state-of-the-art summarization systems. These results reveal that compressive summarization is a promising paradigm and encourage us to continue with the research to produce informative summaries. 1 Introduction Compressive summarization, a joint model integrating sentence extraction and sentence compression within a unified framework, has been attracting attention in recent years (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Almeida and Martins, 2013; Qian and Liu, 2013; Kikuchi et al., 2014; Yao et al., 2015). Since compressive summarization methods can use a sub-sentence as an atomic unit, they can pack more information into summaries than extractive methods, which employ sentences as atomic units. Thus, compressive summarization is essential when we want to produce summaries under tight length constraints. There are two approaches to compress entire document(s) to be grammatical; one is trimming the phrase structure trees (Berg-Kirkpatrick et al., 2011) and the other is trimming the dependency trees obtained"
P17-2043,P13-1020,0,0.0175913,"aradigm. Experimental results on the DUC dataset showed that ROUGE scores of compressive oracles are significantly higher than those of extractive oracles and state-of-the-art summarization systems. These results reveal that compressive summarization is a promising paradigm and encourage us to continue with the research to produce informative summaries. 1 Introduction Compressive summarization, a joint model integrating sentence extraction and sentence compression within a unified framework, has been attracting attention in recent years (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Almeida and Martins, 2013; Qian and Liu, 2013; Kikuchi et al., 2014; Yao et al., 2015). Since compressive summarization methods can use a sub-sentence as an atomic unit, they can pack more information into summaries than extractive methods, which employ sentences as atomic units. Thus, compressive summarization is essential when we want to produce summaries under tight length constraints. There are two approaches to compress entire document(s) to be grammatical; one is trimming the phrase structure trees (Berg-Kirkpatrick et al., 2011) and the other is trimming the dependency trees obtained from the document(s) (Marti"
P17-2043,de-marneffe-etal-2006-generating,0,0.30686,"Missing"
P17-2043,D13-1155,0,0.0615266,"ch topic to leverage our resources. Table 1: ROUGE scores and the number of sentences of extractive and compressive oracle summaries and those obtained from state-of-the-art summarization systems, RegSum and ICSISumm. n= 1 corresponds to ROUGE1 , n=2 corresponds to ROUGE2 , n=1+2 corresponds to ROUGE-SU0. “Sent.” indicates the average number of sentences in the summaries. sentences in the dataset to obtain dependency relations between words, and then we transformed them into trees that represent the dependency relations between chunks by applying Filippova’s rules (Filippova and Strube, 2008; Filippova and Altun, 2013). To solve the ILP problem, we utilized CPLEX version 12.5.1.0. We obtained and evaluated oracle summaries based on three variants of ROUGE, ROUGE1 , ROUGE2 and ROUGE -SU0, with the following conditions3 : (1) ROUGE1 , utilizing unigrams excluding stopwords (2) ROUGE2 , utilizing bigrams with stopwords, and (3) ROUGE -SU0, which is an extension of ROUGEn , utilizing unigram and bigram (excluding skip-bigram) statistics. 4.2 n=1 n=2 n=1 n=2 Results and Discussion Table 1 shows ROUGE scores of compressive and extractive oracle summaries and those of RegSum (Hong and Nenkova, 2014) that achieved"
P17-2043,W08-1105,0,0.12461,"rization is important research topic to leverage our resources. Table 1: ROUGE scores and the number of sentences of extractive and compressive oracle summaries and those obtained from state-of-the-art summarization systems, RegSum and ICSISumm. n= 1 corresponds to ROUGE1 , n=2 corresponds to ROUGE2 , n=1+2 corresponds to ROUGE-SU0. “Sent.” indicates the average number of sentences in the summaries. sentences in the dataset to obtain dependency relations between words, and then we transformed them into trees that represent the dependency relations between chunks by applying Filippova’s rules (Filippova and Strube, 2008; Filippova and Altun, 2013). To solve the ILP problem, we utilized CPLEX version 12.5.1.0. We obtained and evaluated oracle summaries based on three variants of ROUGE, ROUGE1 , ROUGE2 and ROUGE -SU0, with the following conditions3 : (1) ROUGE1 , utilizing unigrams excluding stopwords (2) ROUGE2 , utilizing bigrams with stopwords, and (3) ROUGE -SU0, which is an extension of ROUGEn , utilizing unigram and bigram (excluding skip-bigram) statistics. 4.2 n=1 n=2 n=1 n=2 Results and Discussion Table 1 shows ROUGE scores of compressive and extractive oracle summaries and those of RegSum (Hong and N"
P17-2043,W09-1802,0,0.0385364,"utilized CPLEX version 12.5.1.0. We obtained and evaluated oracle summaries based on three variants of ROUGE, ROUGE1 , ROUGE2 and ROUGE -SU0, with the following conditions3 : (1) ROUGE1 , utilizing unigrams excluding stopwords (2) ROUGE2 , utilizing bigrams with stopwords, and (3) ROUGE -SU0, which is an extension of ROUGEn , utilizing unigram and bigram (excluding skip-bigram) statistics. 4.2 n=1 n=2 n=1 n=2 Results and Discussion Table 1 shows ROUGE scores of compressive and extractive oracle summaries and those of RegSum (Hong and Nenkova, 2014) that achieved the best ROUGE1 and ICSISumm (Gillick and Favre, 2009; Gillick et al., 2009) that achieved the best ROUGE2 on the DUC-2004 dataset, respectively. We compare ROUGE scores of compressive oracle summaries with extractive oracle summaries. The best scores are obtained when we use the same ROUGE variant for both computation and evaluation (see bolded scores in Table 1). There are large differences between the best scores of ex4.3 Readability evaluation We conducted human evaluation to compare readability of extractive oracle summaries to that of compressive oracle summaries. We presented the oracle summaries to five human subjects and asked them to r"
P17-2043,E17-1037,1,0.865656,". As a result, researchers cannot know how much room for further improvement is left. Thus, it is beneficial to reveal the upper bound summary that achieves the maximum ROUGE score and can be produced by the systems. The upper bound summary is known as the oracle summary. To obtain the oracle summary on extractive summarization paradigms, several approaches have been proposed. Sipos et al. (2012) utilized a greedy algorithm, and Kubina et al. (2013) utilized exhaustive search based on heuristics. However, their oracle summaries do not always retain the optimal (maximum) ROUGE score. Recently, Hirao et al. (2017) derived an Integer Linear Programming (ILP) formulation to obtain the optimal oracle summary. Their oracle summary can help researchers to comprehend the strict limitation of the extractive summarization paradigm. However, their method cannot be applied to obtain compressive oracle summaries. To reveal the ultimate limitation of the compressive summarization paradigm, we propose an ILP formulation to obtain a compressive oracle summary that maximizes the ROUGE score. We conThis paper derives an Integer Linear Programming (ILP) formulation to obtain an oracle summary of the compressive summari"
P17-2043,hong-etal-2014-repository,0,0.0155404,"there is no dependency relationship between c3,3 and c3,5 . After solving the ILP problem, we can obtain compressive oracle summaries by collecting chunks according to bi,u =1. 4 Experiments To investigate the potential limitation of the compressive summarization paradigm, we compare ROUGE scores of compressive oracle summaries with those of extractive oracle summaries and those obtained from state-of-the-art summarization systems. Extractive oracle summaries are obtained by solving the ILP formulation proposed by (Hirao et al., 2017). System summaries are extracted from a public repository2 (Hong et al., 2014). We give an example to show how chunks and word sequences are related. When we pack a bigram “live in” in an oracle summary, there are four candidates in the source document (Fig. 1). Word subsequences, w1,6 ,w2,5 ,w2,6 and w3.9 match “live in”. Thus, T (live in) = {(1, 6), (2, 5), (2, 6), (3, 9)}. Here, when we want to pack w2,6 into the oracle summary, we have to pack both chunks c2,2 and c2,4 (b2,2 = b2,4 = 1) because U2 (w2,6 ) = {2, 4}. Then, we have to drop chunk c2,3 (b2,3 = 0) because c2,3 is within the gap between chunks c2,2 and c2,4 (V2 (w2,6 ) = 3). Similarly, when we pack w3,9 in"
P17-2043,E14-1075,0,0.013269,"rube, 2008; Filippova and Altun, 2013). To solve the ILP problem, we utilized CPLEX version 12.5.1.0. We obtained and evaluated oracle summaries based on three variants of ROUGE, ROUGE1 , ROUGE2 and ROUGE -SU0, with the following conditions3 : (1) ROUGE1 , utilizing unigrams excluding stopwords (2) ROUGE2 , utilizing bigrams with stopwords, and (3) ROUGE -SU0, which is an extension of ROUGEn , utilizing unigram and bigram (excluding skip-bigram) statistics. 4.2 n=1 n=2 n=1 n=2 Results and Discussion Table 1 shows ROUGE scores of compressive and extractive oracle summaries and those of RegSum (Hong and Nenkova, 2014) that achieved the best ROUGE1 and ICSISumm (Gillick and Favre, 2009; Gillick et al., 2009) that achieved the best ROUGE2 on the DUC-2004 dataset, respectively. We compare ROUGE scores of compressive oracle summaries with extractive oracle summaries. The best scores are obtained when we use the same ROUGE variant for both computation and evaluation (see bolded scores in Table 1). There are large differences between the best scores of ex4.3 Readability evaluation We conducted human evaluation to compare readability of extractive oracle summaries to that of compressive oracle summaries. We prese"
P17-2043,P14-2052,1,0.885796,"t showed that ROUGE scores of compressive oracles are significantly higher than those of extractive oracles and state-of-the-art summarization systems. These results reveal that compressive summarization is a promising paradigm and encourage us to continue with the research to produce informative summaries. 1 Introduction Compressive summarization, a joint model integrating sentence extraction and sentence compression within a unified framework, has been attracting attention in recent years (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Almeida and Martins, 2013; Qian and Liu, 2013; Kikuchi et al., 2014; Yao et al., 2015). Since compressive summarization methods can use a sub-sentence as an atomic unit, they can pack more information into summaries than extractive methods, which employ sentences as atomic units. Thus, compressive summarization is essential when we want to produce summaries under tight length constraints. There are two approaches to compress entire document(s) to be grammatical; one is trimming the phrase structure trees (Berg-Kirkpatrick et al., 2011) and the other is trimming the dependency trees obtained from the document(s) (Martins and Smith, 2009; Almeida and Martins, 2"
P17-2043,W04-1013,0,0.139099,"Missing"
P17-2043,W09-1801,0,0.0314274,"essential to reveal the upper bound performance of the paradigm. Experimental results on the DUC dataset showed that ROUGE scores of compressive oracles are significantly higher than those of extractive oracles and state-of-the-art summarization systems. These results reveal that compressive summarization is a promising paradigm and encourage us to continue with the research to produce informative summaries. 1 Introduction Compressive summarization, a joint model integrating sentence extraction and sentence compression within a unified framework, has been attracting attention in recent years (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Almeida and Martins, 2013; Qian and Liu, 2013; Kikuchi et al., 2014; Yao et al., 2015). Since compressive summarization methods can use a sub-sentence as an atomic unit, they can pack more information into summaries than extractive methods, which employ sentences as atomic units. Thus, compressive summarization is essential when we want to produce summaries under tight length constraints. There are two approaches to compress entire document(s) to be grammatical; one is trimming the phrase structure trees (Berg-Kirkpatrick et al., 2011) and the other is trimming"
P17-2043,D13-1156,0,0.0213451,"ts on the DUC dataset showed that ROUGE scores of compressive oracles are significantly higher than those of extractive oracles and state-of-the-art summarization systems. These results reveal that compressive summarization is a promising paradigm and encourage us to continue with the research to produce informative summaries. 1 Introduction Compressive summarization, a joint model integrating sentence extraction and sentence compression within a unified framework, has been attracting attention in recent years (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Almeida and Martins, 2013; Qian and Liu, 2013; Kikuchi et al., 2014; Yao et al., 2015). Since compressive summarization methods can use a sub-sentence as an atomic unit, they can pack more information into summaries than extractive methods, which employ sentences as atomic units. Thus, compressive summarization is essential when we want to produce summaries under tight length constraints. There are two approaches to compress entire document(s) to be grammatical; one is trimming the phrase structure trees (Berg-Kirkpatrick et al., 2011) and the other is trimming the dependency trees obtained from the document(s) (Martins and Smith, 2009;"
P17-2043,E12-1023,0,0.0179702,"system summaries cannot achieve ROUGE=1 since summarization systems cannot reproduce reference summaries in most cases. In other words, the maximum ROUGE score that can be achieved by compressive summarization is unclear. As a result, researchers cannot know how much room for further improvement is left. Thus, it is beneficial to reveal the upper bound summary that achieves the maximum ROUGE score and can be produced by the systems. The upper bound summary is known as the oracle summary. To obtain the oracle summary on extractive summarization paradigms, several approaches have been proposed. Sipos et al. (2012) utilized a greedy algorithm, and Kubina et al. (2013) utilized exhaustive search based on heuristics. However, their oracle summaries do not always retain the optimal (maximum) ROUGE score. Recently, Hirao et al. (2017) derived an Integer Linear Programming (ILP) formulation to obtain the optimal oracle summary. Their oracle summary can help researchers to comprehend the strict limitation of the extractive summarization paradigm. However, their method cannot be applied to obtain compressive oracle summaries. To reveal the ultimate limitation of the compressive summarization paradigm, we propo"
P18-2097,D16-1140,0,0.0340332,"Vinyals et al. (2015); Durrett and Klein (2015). To maintain as much reproducibility of our experiments as possible, we simply applied publicly available pre-trained word embeddings, i.e., glove.840B.300d7 , as initial values of the encoder embedding layer. (3) Output length controlling As described in Vinyals et al. (2015), not all the outputs (predicted linearized parse trees) obtained from the Seq2seq parser are valid (well-formed) as a parse tree. Toward guaranteeing that every output is a valid tree, we introduce a simple extension of the method for controlling the Seq2seq output length (Kikuchi et al., 2016). First, we introduce an additional bias term b in the decoder output layer to prevent the selection of certain output words: pj = softmax(oj + b). 200 2 0.3 1.0 1.0 3.6 Model ensemble Ensembling several independently trained models together significantly improves many NLP tasks. In the ensembling process, we predict the output tokens using the arithmetic mean of predicted probabilities computed by each model: (4) pj = If we set a large negative value at the m-th element in b, namely bm ≈ −∞, then the m-th element in pj becomes approximately 0, namely pj,m ≈ 0, regardless of the value of the k"
P18-2097,Q17-1010,0,0.0362725,"ize the unknown embedding as a bias term b of linear layer (W x + b) when obtaining every encoder embeddings for overcoming infrequent word problem. Then, we modify Eq. 2 as follows: X ei = (Exk + u) + (F sk0 + u). (2) Applying subword decomposition has recently become a leading technique in NMT literature (Sennrich et al., 2016; Wu et al., 2016). Its primary advantage is a significant reduction of the serious out-of-vocabulary (OOV) problem. We incorporated subword information as an additional feature of the original input words. A similar usage of subword features was previously proposed in Bojanowski et al. (2017). Formally, the encoder embedding vector at encoder position i, namely, ei , is calculated as follows: X ei = Exk + F sk0 , (1) k0 ∈ψ(wi ) 3 We did not substitute POS-tags for punctuation symbols such as “.”, and “,”. 4 Several recently developed neural-based constituency parsers ignore POS tags since they are not evaluated in the standard evaluation metric of constituency parsing (Bracketing F-measure). 5 Figure in the supplementary material shows the brief sketch of the method explained in the following section. k0 ∈ψ(wi ) Note that if wi is unknown token, then Eq. 2 beP comes ei = 2u + k0 ∈"
P18-2097,D14-1179,0,0.019755,"Missing"
P18-2097,E17-1117,0,0.0940347,"Missing"
P18-2097,Q17-1004,0,0.197093,"Missing"
P18-2097,D16-1257,0,0.248642,"closed brackets. (2) if the number of predicted XX-tags (or POS-tags) is equivalent to that of the words in a given input sentence, then we mask the XX-tags (or all the POS-tags) and all the open brackets. If both conditions (1) and (2) are satisfied, then the decoding process is finished. The additional cost for controlling the mask is to count the number of XX-tags and the open and closed brackets so far generated in the decoding process. 1 XA (a) p , a=1 j A (5) (a) where pj represents the probability distribution at position j predicted by the a-th model. 3.7 Language model (LM) reranking Choe and Charniak (2016) demonstrated that reranking the predicted parser output candidates with an RNN language model (LM) significantly improves performance. We refer to this reranking process as LM-rerank. Following their success, we also trained RNN-LMs on the PTB dataset with their published preprocessing code8 to reproduce the experiments in Choe and Charniak (2016) for our LM-rerank. We selected the current stateof-the-art LM (Yang et al., 2018)9 as our LMreranker, which is a much stronger LM than was used in Choe and Charniak (2016). 7 https://nlp.stanford.edu/projects/glove/ https://github.com/cdg720/emnlp20"
P18-2097,P16-2006,0,0.0315668,"Missing"
P18-2097,D15-1166,0,0.0781473,"sions This section describes several generic techniques that improve Seq2seq performance5 . Table 2 lists the notations used in this paper for a convenient reference. 3.1 : encoder embedding matrix for V (e) , where E ∈ RD×|V E trees (Vinyals et al., 2015). Roughly speaking, a linearized parse tree consists of open, close bracketing and POS-tags that correspond to a given input raw sentence. Since a one-to-one mapping exists between a parse tree and its linearized form (if the linearized form is a valid tree), we can recover parse trees from the predicted linearized parse tree. Vinyals et al. (2015) also introduced the part-of-speech (POS) tag normalization technique. They substituted each POS tag in a linearized parse tree to a single XX-tag3 , which allows Seq2seq models to achieve a more competitive performance range than the current state-ofthe-art parses4 . Table 1 shows an example of a parse tree to which linearization and POS-tag normalization was applied. : dimension of the embeddings : dimension of the hidden states : index of the (token) position in input sentence : index of the (token) position in output linearized format of parse tree : vocabulary of word for input (encoder)"
P18-2097,D16-1001,0,0.0734685,"Missing"
P18-2097,P15-1002,0,0.03537,"Missing"
P18-2097,W17-3203,0,0.0729695,"Missing"
P18-2097,P15-1030,0,0.070847,"e POS-tag normalization are independently and simultaneously estimated as oj and qj , respectively, in the decoder output layer by following equation: oj = W (o) zj , 3.4 and qj = W (q) zj . Table 3: List of model and optimization configurations (hyper-parameters) in our experiments 3.5 Pre-trained word embeddings The pre-trained word embeddings obtained from a large external corpora often boost the final task performance even if they only initialize the input embedding layer. In constituency parsing, several systems also incorporate pre-trained word embeddings, such as Vinyals et al. (2015); Durrett and Klein (2015). To maintain as much reproducibility of our experiments as possible, we simply applied publicly available pre-trained word embeddings, i.e., glove.840B.300d7 , as initial values of the encoder embedding layer. (3) Output length controlling As described in Vinyals et al. (2015), not all the outputs (predicted linearized parse trees) obtained from the Seq2seq parser are valid (well-formed) as a parse tree. Toward guaranteeing that every output is a valid tree, we introduce a simple extension of the method for controlling the Seq2seq output length (Kikuchi et al., 2016). First, we introduce an a"
P18-2097,N16-1024,0,0.163382,"s, especially for natural language generation (NLG) tasks, such as machine translation (MT) (Sutskever et al., 2014; Cho et al., 2014) and abstractive summarization (Rush et al., 2015). Seq2seq models have also been applied to constituency parsing (Vinyals et al., 2015) and provided a fairly good result. However one obvious, intuitive drawback of Seq2seq models when they are applied to constituency parsing is that they have no explicit architecture to model latent nested relationships among the words and phrases in constituency parse trees, Thus, models that directly model them, such as RNNG (Dyer et al., 2016), are an intuitively more promising approach. In fact, RNNG and its extensions (Kuncoro et al., 2017; Fried et al., 2017) provide the current stateof-the-art performance. Sec2seq models are currently considered a simple baseline of neuralbased constituency parsing. After the first proposal of an Seq2seq constituency parser, many task-independent techniques have been developed, mainly in the NLG 2 Constituency Parsing by Seq2seq Our starting point is an RNN-based Seq2seq model with an attention mechanism that was applied to constituency parsing (Vinyals et al., 2015). We omit detailed descripti"
P18-2097,D15-1044,0,0.181955,"rate several techniques that were mainly developed in natural language generation tasks, e.g., machine translation and summarization, and demonstrate that the sequenceto-sequence model achieves the current top-notch parsers’ performance without requiring explicit task-specific knowledge or architecture of constituent parsing. 1 Introduction Sequence-to-sequence (Seq2seq) models have successfully improved many well-studied NLP tasks, especially for natural language generation (NLG) tasks, such as machine translation (MT) (Sutskever et al., 2014; Cho et al., 2014) and abstractive summarization (Rush et al., 2015). Seq2seq models have also been applied to constituency parsing (Vinyals et al., 2015) and provided a fairly good result. However one obvious, intuitive drawback of Seq2seq models when they are applied to constituency parsing is that they have no explicit architecture to model latent nested relationships among the words and phrases in constituency parse trees, Thus, models that directly model them, such as RNNG (Dyer et al., 2016), are an intuitively more promising approach. In fact, RNNG and its extensions (Kuncoro et al., 2017; Fried et al., 2017) provide the current stateof-the-art performa"
P18-2097,P17-2025,0,0.137599,"et al., 2014) and abstractive summarization (Rush et al., 2015). Seq2seq models have also been applied to constituency parsing (Vinyals et al., 2015) and provided a fairly good result. However one obvious, intuitive drawback of Seq2seq models when they are applied to constituency parsing is that they have no explicit architecture to model latent nested relationships among the words and phrases in constituency parse trees, Thus, models that directly model them, such as RNNG (Dyer et al., 2016), are an intuitively more promising approach. In fact, RNNG and its extensions (Kuncoro et al., 2017; Fried et al., 2017) provide the current stateof-the-art performance. Sec2seq models are currently considered a simple baseline of neuralbased constituency parsing. After the first proposal of an Seq2seq constituency parser, many task-independent techniques have been developed, mainly in the NLG 2 Constituency Parsing by Seq2seq Our starting point is an RNN-based Seq2seq model with an attention mechanism that was applied to constituency parsing (Vinyals et al., 2015). We omit detailed descriptions due to space limitations, but note that our model architecture is identical to the one introduced in Luong et al. (20"
P18-2097,P16-1162,0,0.0377396,"coder output scores at decoder position j : output scores of auxiliary task at decoder position j : additional bias term in the decoder output layer for mask : vector format of output probability at decoder position j : number of models for ensembling : number of candidates generating for LM-reranking Table 2: List of notations used in this paper. where k = φ(wi ). Note that the second term of RHS indicates our additional subword features, and the first represents the standard word embedding extraction procedure. Among several choices, we used the byte-pair encoding (BPE) approach proposed in Sennrich et al. (2016) applying 1,000 merge operations6 . Task-independent Extensions This section describes several generic techniques that improve Seq2seq performance5 . Table 2 lists the notations used in this paper for a convenient reference. 3.1 : encoder embedding matrix for V (e) , where E ∈ RD×|V E trees (Vinyals et al., 2015). Roughly speaking, a linearized parse tree consists of open, close bracketing and POS-tags that correspond to a given input raw sentence. Since a one-to-one mapping exists between a parse tree and its linearized form (if the linearized form is a valid tree), we can recover parse trees"
P18-2097,I17-2002,1,0.908979,"Missing"
P18-2097,P12-1046,1,0.886858,"Missing"
P18-2097,P17-1076,0,0.124719,"Missing"
P18-2097,D17-1178,0,0.0767172,"Missing"
P18-2097,P15-1113,0,0.0496068,"Missing"
P19-1225,D15-1075,0,0.0386021,"et al., 2018) is also an explainable multi-hop QA dataset that provides gold evidence sentences. However, it is difficult to compare the performance of the evidence extraction with other studies because its evaluation script and leaderboard do not report the evidence extraction score. Because annotation of the evidence sentence is costly, unsupervised learning of the evidence extraction is another important issue. Wang et al. (2019) tackled unsupervised learning for explainable multi-hop QA, but their model is restricted to the multiple-choice setting. 6.2 Recognizing Textual Entailment RTE (Bowman et al., 2015; Williams et al., 2018) is performed by sentence matching (Rockt¨aschel et al., 2016; Chen et al., 2017b). FEVER (Thorne et al., 2018) has the aim of verification and fact checking for RTE on a large database. FEVER requires three sub tasks: document retrieval, evidence extraction, and answer prediction. In the previous work, the sub tasks are performed using pipelined models (Nie et al., 2019; Yoneda et al., 2018). In contrast, our approach performs evidence extraction and answer prediction simultaneously by regarding FEVER as an explainable multi-hop QA task. 6.3 Summarization A typical app"
P19-1225,P17-1171,0,0.026332,"it is difficult to compare the performance of the evidence extraction with other studies because its evaluation script and leaderboard do not report the evidence extraction score. Because annotation of the evidence sentence is costly, unsupervised learning of the evidence extraction is another important issue. Wang et al. (2019) tackled unsupervised learning for explainable multi-hop QA, but their model is restricted to the multiple-choice setting. 6.2 Recognizing Textual Entailment RTE (Bowman et al., 2015; Williams et al., 2018) is performed by sentence matching (Rockt¨aschel et al., 2016; Chen et al., 2017b). FEVER (Thorne et al., 2018) has the aim of verification and fact checking for RTE on a large database. FEVER requires three sub tasks: document retrieval, evidence extraction, and answer prediction. In the previous work, the sub tasks are performed using pipelined models (Nie et al., 2019; Yoneda et al., 2018). In contrast, our approach performs evidence extraction and answer prediction simultaneously by regarding FEVER as an explainable multi-hop QA task. 6.3 Summarization A typical approach to sentence-level extractive summarization has an encoder-decoder architecture (Cheng and Lapata,"
P19-1225,P17-1152,0,0.0357435,"it is difficult to compare the performance of the evidence extraction with other studies because its evaluation script and leaderboard do not report the evidence extraction score. Because annotation of the evidence sentence is costly, unsupervised learning of the evidence extraction is another important issue. Wang et al. (2019) tackled unsupervised learning for explainable multi-hop QA, but their model is restricted to the multiple-choice setting. 6.2 Recognizing Textual Entailment RTE (Bowman et al., 2015; Williams et al., 2018) is performed by sentence matching (Rockt¨aschel et al., 2016; Chen et al., 2017b). FEVER (Thorne et al., 2018) has the aim of verification and fact checking for RTE on a large database. FEVER requires three sub tasks: document retrieval, evidence extraction, and answer prediction. In the previous work, the sub tasks are performed using pipelined models (Nie et al., 2019; Yoneda et al., 2018). In contrast, our approach performs evidence extraction and answer prediction simultaneously by regarding FEVER as an explainable multi-hop QA task. 6.3 Summarization A typical approach to sentence-level extractive summarization has an encoder-decoder architecture (Cheng and Lapata,"
P19-1225,P18-1063,0,0.123293,"is the query vector considering the current summarization. et is the extracted sentence. xet updates the RNN state. Loss Function: Our model uses multi-task learning with a loss function L = LA +LE , where LA is the loss of the answer and LE is the loss of the evidence. The answer loss LA is the sum of the cross-entropy losses for all probability distributions obtained by the answer layer. The evidence loss LE is defined in subsection 3.3. 3.2 Query Focused Extractor Query Focused Extractor (QFE) is shown as the red box in Figure 2. QFE is an extension of the extractive summarization model of Chen and Bansal (2018), which is not for query-focused settings. Chen and Bansal used an attention mechanism to extract sentences from the source document such that the summary would cover the important information in the source document. To focus on the query, QFE extracts sentences from C with attention on Q such that the evidence covers the important information with respect to Q. Figure 3 shows an overview of QFE. The inputs of QFE are the sentence-level context vectors X ∈ Rls ×2dc and contextual query vectors Y ∈ Rmw ×2dc . We define the timestep to be the operation to extract a sentence. QFE updates the stat"
P19-1225,P16-1046,0,0.0340622,"Chen et al., 2017b). FEVER (Thorne et al., 2018) has the aim of verification and fact checking for RTE on a large database. FEVER requires three sub tasks: document retrieval, evidence extraction, and answer prediction. In the previous work, the sub tasks are performed using pipelined models (Nie et al., 2019; Yoneda et al., 2018). In contrast, our approach performs evidence extraction and answer prediction simultaneously by regarding FEVER as an explainable multi-hop QA task. 6.3 Summarization A typical approach to sentence-level extractive summarization has an encoder-decoder architecture (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2018). Sentence-level extractive summarization is also used for content selection in abstractive summarization (Chen and Bansal, 2018). The model extracts sentences in order of importance and edits them. We have extended this model so that it can be used for evidence extraction because we consider that the evidence must be extracted in order of importance rather than the original order, which the conventional models use. 7 Conclusion We consider that the main contributions of our study are (1) the QFE model that is based on a summarization model for th"
P19-1225,P17-1020,0,0.0863488,"Missing"
P19-1225,P18-1078,0,0.0221114,"and only if the answer type AT is ‘Span’. C consists of ten Wikipedia paragraphs. The evidence E consists of two or more sentences in C. For RTE, we tackle FEVER. In FEVER, the answer candidates are ‘Supports’, ‘Refutes’, and ‘Not Enough Info’. The answer string AS does not exist. C is the Wikipedia database. The evidence E consists of the sentences in C. 3 Proposed Method This section first explains the overall model architecture, which contains our model as a module, and then the details of our QFE. 3.1 Model Architecture Except for the evidence layer, our model is the same as the baseline (Clark and Gardner, 2018) used in HotpotQA (Yang et al., 2018). Figure 2 shows the model architecture. The input of the model is the context C and the query Q. The model has the following layers. The Word Embedding Layer encodes C and Q as sequences of word vectors. A word vector is the concatenation of a pre-trained word embedding and a character-based embedding obtained using a CNN (Kim, 2014). The outputs are C1 ∈ 2336 Rlw ×dw , Q1 ∈ Rmw ×dw , where lw is the length (in words) of C, mw is the length of Q and dw is the size of the word vector. Sentence Vectors  ڭ The Matching Layer encodes C2 , Q2 as matching vec"
P19-1225,N19-1423,0,0.0184757,"the answer selection score and therefore achieved state-of-the-art performance on the joint EM and F1 metrics, which are the main metric on the dataset. QFE outperformed the baseline model in all metrics. Although our model does not use any pre-trained language model such as Answer EM F1 44.4 58.3 52.7 67.3 52.7 67.3 53.7 68.7 53.1 67.9 46.9 63.6 Evidence EM F1 22.0 66.7 38.0 78.4 48.0 77.8 58.8 84.7 58.4 84.3 – – Joint EM F1 11.6 40.9 21.9 54.9 27.6 54.4 35.4 60.6 34.8 59.6 – – Table 4: Performance of our models and the baseline models on the development set in the distractor setting. BERT (Devlin et al., 2019) for encoding, it outperformed the methods that used BERT such as DFGN + BERT and BERT Plus. In particular, the improvement in the evidence EM score was +37.5 points against the baseline and +5.4 points against GRN. In the fullwiki setting, Table 3 shows that QFE outperformed the baseline in all metrics. Compared with the unpublished model at the submission time, Cognitive Graph (Ding et al., 2019) outperformed our model. There is a dataset shift problem (Quionero-Candela et al., 2009) in HotpotQA, where the distribution of the number of gold evidence sentences and the answerability differs be"
P19-1225,P19-1259,0,0.511447,"s. Whereas we use equation (1), they use Pr(i) = sigmoid(w⊤ xi + b), where w ∈ R2dc , b ∈ R are trainable parameters. The evidence loss LE is the sum of binary cross-entropy functions on whether each of the sentences is evidence or not. In the test phase, the sentences with probabilities higher than a threshold are selected. We set the threshold to 0.4 because it gave the highest F1 score on the development set. The remaining parts of the implementations of our and baseline models are the same. The details are in Appendix A.1. We also compared DFGN + BERT (Xiao et al., 2019), Cognitive Graph (Ding et al., 2019), GRN and BERT Plus, which were unpublished at the submission time (4 March 2019). 2338 Baseline BERT Plus DFGN + BERT GRN QFE Answer EM F1 45.6 59.0 56.0 69.9 55.2 68.5 52.9 66.7 53.9 68.1 Evidence EM F1 20.3 64.5 42.3 80.6 49.9 81.1 52.4 84.1 57.8 84.5 Joint EM F1 10.8 40.2 26.9 58.1 31.9 58.2 31.8 58.5 34.6 59.6 Yang et al. (2018) our implementation2 + top 2 extraction QFE without glimpse pipeline model Table 2: Performance of the models on the HotpotQA distractor setting leaderboard1 (4 March 2019). The models except for the baseline were unpublished at the time of submission of this paper"
P19-1225,D16-1264,0,0.071045,"rt evidence extraction score on HotpotQA. Although designed for RC, it also achieves a state-of-the-art evidence extraction score on FEVER, which is a recognizing textual entailment task on a large textual database. 1 Figure 1: Concept of explainable multi-hop QA. Given a question and multiple textual sources, the system extracts evidence sentences from the sources and returns the answer and the evidence. Introduction Reading comprehension (RC) is a task that uses textual sources to answer any question. It has seen significant progress since the publication of numerous datasets such as SQuAD (Rajpurkar et al., 2016). To achieve the goal of RC, systems must be able to reason over disjoint pieces of information in the reference texts. Recently, multi-hop question answering (QA) datasets focusing on this capability, such as QAngaroo (Welbl et al., 2018) and HotpotQA (Yang et al., 2018), have been released. Multi-hop QA faces two challenges. The first is the difficulty of reasoning. It is difficult for the system to find the disjoint pieces of information as evidence and reason using the multiple pieces of such evidence. The second challenge is interpretability. The evidence used to reason is not necessarily"
P19-1225,P17-1099,0,0.0407048,"the green box): X gt = αjt Wg1 yj ∈ R2dc Ave. Max Median Min Context # paragraphs # words 10.0 1162.0 10 3079 10 1142 2 60 Query # words 17.8 59 17 7 Evidence # sentences 2.4 8 2 2 j t α = softmax(at ) ∈ Rmw atj = vg⊤ tanh(Wg1 yj t + Wg2 z ). The initial state of the RNN is the vector obtained via the fully connected layer and the max pooling from X. All parameters W· ∈ R2dc ×2dc and v· ∈ R2dc are trainable. 3.3 Training Phase In the training phase, we use teacher-forcing to make the loss function. The loss of the evidence LE is the negative log likelihood regularized by a coverage mechanism (See et al., 2017): LE = − |E| X log t=1 +  max i∈EE t−1 X Pr(i; E t−1 )  min(cti , αit ). i The max operation in the first term enables the sentence with the highest probability to be extracted. This operation means that QFE extracts the sentences in the predicted importance order. On the other hand, the evidence does not have the ground truth order in which it is to be extracted, so the loss function ignores the order of the evidence sentences. coverage vector ct is defined as Pt−1 The t τ c = τ =1 α . In order to learn the terminal condition of the extraction, QFE adds a dummy sentence, called the EOE sen"
P19-1225,W18-5516,0,0.0738697,"Missing"
P19-1225,P17-1147,0,0.0333518,"Missing"
P19-1225,N18-1023,0,0.12836,"Missing"
P19-1225,D14-1181,0,0.00393342,"n first explains the overall model architecture, which contains our model as a module, and then the details of our QFE. 3.1 Model Architecture Except for the evidence layer, our model is the same as the baseline (Clark and Gardner, 2018) used in HotpotQA (Yang et al., 2018). Figure 2 shows the model architecture. The input of the model is the context C and the query Q. The model has the following layers. The Word Embedding Layer encodes C and Q as sequences of word vectors. A word vector is the concatenation of a pre-trained word embedding and a character-based embedding obtained using a CNN (Kim, 2014). The outputs are C1 ∈ 2336 Rlw ×dw , Q1 ∈ Rmw ×dw , where lw is the length (in words) of C, mw is the length of Q and dw is the size of the word vector. Sentence Vectors  ڭ The Matching Layer encodes C2 , Q2 as matching vectors C3 ∈ Rlw ×dc by using bi-directional attention (Seo et al., 2017), a Bi-RNN, and selfattention (Wang et al., 2017). The Evidence Layer first encodes C3 as − → ← − [C4 ; C4 ] ∈ Rlw ×2dc by a Bi-RNN. Let j1 (i) be the index of the first word of the i-th sentence in C and j2 (i) be the index of the last word. We define the vector of the i-th sentence as: −−→; ← −−−] ∈"
P19-1225,W18-5517,0,0.084346,"Missing"
P19-1225,N18-1158,0,0.0173374,") has the aim of verification and fact checking for RTE on a large database. FEVER requires three sub tasks: document retrieval, evidence extraction, and answer prediction. In the previous work, the sub tasks are performed using pipelined models (Nie et al., 2019; Yoneda et al., 2018). In contrast, our approach performs evidence extraction and answer prediction simultaneously by regarding FEVER as an explainable multi-hop QA task. 6.3 Summarization A typical approach to sentence-level extractive summarization has an encoder-decoder architecture (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2018). Sentence-level extractive summarization is also used for content selection in abstractive summarization (Chen and Bansal, 2018). The model extracts sentences in order of importance and edits them. We have extended this model so that it can be used for evidence extraction because we consider that the evidence must be extracted in order of importance rather than the original order, which the conventional models use. 7 Conclusion We consider that the main contributions of our study are (1) the QFE model that is based on a summarization model for the explainable multihop QA, (2) the dependency a"
P19-1225,K19-1065,0,0.0494,"(1) QAngaroo does not have supervised evidence and (2) the questions in QAngaroo are inherently limited because the dataset is constructed using a knowledge base. MultiRC (Khashabi et al., 2018) is also an explainable multi-hop QA dataset that provides gold evidence sentences. However, it is difficult to compare the performance of the evidence extraction with other studies because its evaluation script and leaderboard do not report the evidence extraction score. Because annotation of the evidence sentence is costly, unsupervised learning of the evidence extraction is another important issue. Wang et al. (2019) tackled unsupervised learning for explainable multi-hop QA, but their model is restricted to the multiple-choice setting. 6.2 Recognizing Textual Entailment RTE (Bowman et al., 2015; Williams et al., 2018) is performed by sentence matching (Rockt¨aschel et al., 2016; Chen et al., 2017b). FEVER (Thorne et al., 2018) has the aim of verification and fact checking for RTE on a large database. FEVER requires three sub tasks: document retrieval, evidence extraction, and answer prediction. In the previous work, the sub tasks are performed using pipelined models (Nie et al., 2019; Yoneda et al., 2018"
P19-1225,P17-1018,0,0.0267189,"is the context C and the query Q. The model has the following layers. The Word Embedding Layer encodes C and Q as sequences of word vectors. A word vector is the concatenation of a pre-trained word embedding and a character-based embedding obtained using a CNN (Kim, 2014). The outputs are C1 ∈ 2336 Rlw ×dw , Q1 ∈ Rmw ×dw , where lw is the length (in words) of C, mw is the length of Q and dw is the size of the word vector. Sentence Vectors  ڭ The Matching Layer encodes C2 , Q2 as matching vectors C3 ∈ Rlw ×dc by using bi-directional attention (Seo et al., 2017), a Bi-RNN, and selfattention (Wang et al., 2017). The Evidence Layer first encodes C3 as − → ← − [C4 ; C4 ] ∈ Rlw ×2dc by a Bi-RNN. Let j1 (i) be the index of the first word of the i-th sentence in C and j2 (i) be the index of the last word. We define the vector of the i-th sentence as: −−→; ← −−−] ∈ R2dc . xi = [ − c4,j c4,j 2 (i) 1 (i) Here, X ∈ Rls ×2dc is the sentence-level context vectors, where ls is the number of sentences of C. QFE, described later, receives sentence-level context vectors X ∈ Rls ×2dc and the contextual query vectors Q2 ∈ Rmw ×2dc as Y. QFE outputs the probability distribution that the i-th sentence is the evidence:"
P19-1225,Q18-1021,0,0.0451536,"ainable multi-hop QA. Given a question and multiple textual sources, the system extracts evidence sentences from the sources and returns the answer and the evidence. Introduction Reading comprehension (RC) is a task that uses textual sources to answer any question. It has seen significant progress since the publication of numerous datasets such as SQuAD (Rajpurkar et al., 2016). To achieve the goal of RC, systems must be able to reason over disjoint pieces of information in the reference texts. Recently, multi-hop question answering (QA) datasets focusing on this capability, such as QAngaroo (Welbl et al., 2018) and HotpotQA (Yang et al., 2018), have been released. Multi-hop QA faces two challenges. The first is the difficulty of reasoning. It is difficult for the system to find the disjoint pieces of information as evidence and reason using the multiple pieces of such evidence. The second challenge is interpretability. The evidence used to reason is not necessarily located close to the answer, so it is difficult for users to verify the answer. Yang et al. (2018) released HotpotQA, an explainable multi-hop QA dataset, as shown in Figure 1. Hotpot QA provides the evidence sentences of the answer for s"
P19-1225,N18-1101,0,0.0273513,"o an explainable multi-hop QA dataset that provides gold evidence sentences. However, it is difficult to compare the performance of the evidence extraction with other studies because its evaluation script and leaderboard do not report the evidence extraction score. Because annotation of the evidence sentence is costly, unsupervised learning of the evidence extraction is another important issue. Wang et al. (2019) tackled unsupervised learning for explainable multi-hop QA, but their model is restricted to the multiple-choice setting. 6.2 Recognizing Textual Entailment RTE (Bowman et al., 2015; Williams et al., 2018) is performed by sentence matching (Rockt¨aschel et al., 2016; Chen et al., 2017b). FEVER (Thorne et al., 2018) has the aim of verification and fact checking for RTE on a large database. FEVER requires three sub tasks: document retrieval, evidence extraction, and answer prediction. In the previous work, the sub tasks are performed using pipelined models (Nie et al., 2019; Yoneda et al., 2018). In contrast, our approach performs evidence extraction and answer prediction simultaneously by regarding FEVER as an explainable multi-hop QA task. 6.3 Summarization A typical approach to sentence-level"
P19-1225,P19-1617,0,0.115524,"n Yang et al. (2018) except as follows. Whereas we use equation (1), they use Pr(i) = sigmoid(w⊤ xi + b), where w ∈ R2dc , b ∈ R are trainable parameters. The evidence loss LE is the sum of binary cross-entropy functions on whether each of the sentences is evidence or not. In the test phase, the sentences with probabilities higher than a threshold are selected. We set the threshold to 0.4 because it gave the highest F1 score on the development set. The remaining parts of the implementations of our and baseline models are the same. The details are in Appendix A.1. We also compared DFGN + BERT (Xiao et al., 2019), Cognitive Graph (Ding et al., 2019), GRN and BERT Plus, which were unpublished at the submission time (4 March 2019). 2338 Baseline BERT Plus DFGN + BERT GRN QFE Answer EM F1 45.6 59.0 56.0 69.9 55.2 68.5 52.9 66.7 53.9 68.1 Evidence EM F1 20.3 64.5 42.3 80.6 49.9 81.1 52.4 84.1 57.8 84.5 Joint EM F1 10.8 40.2 26.9 58.1 31.9 58.2 31.8 58.5 34.6 59.6 Yang et al. (2018) our implementation2 + top 2 extraction QFE without glimpse pipeline model Table 2: Performance of the models on the HotpotQA distractor setting leaderboard1 (4 March 2019). The models except for the baseline were unpublished at"
P19-1225,D18-1259,0,0.114017,"Missing"
P19-1225,W18-5515,0,0.142047,"MN document retriever (Nie et al., 2019) and gave only the topfive paragraphs to our model. Similar to NSMN, in order to capture the semantic and numeric relationships, we used 30-dimensional WordNet features and five-dimensional number embeddings. The WordNet features are binaries reflecting the existence of hypernymy/antonymy words in the input. The number embedding is a real-valued embedding assigned to any unique number. Because the number of samples in the training data is biased on the answer type AT , randomly selected samples were copied in order to equalize the 2341 Nie et al. (2019) Yoneda et al. (2018) who Kudo avonamila hz66pasa aschern QFE Evidence F1 53.0 35.0 37.4 36.8 60.3 71.4 70.4 77.7 Answer Acc. 68.2 67.6 72.1 70.6 71.4 33.3 69.3 69.3 FEVER Nie et al. (2019) Yoneda et al. (2018) Hanselowski et al. (2018) Malon (2018) QFE ensemble (test) QFE single (dev) QFE ensemble (dev) 64.2 62.5 66.6 65.7 65.3 22.0 60.9 61.8 Table 10: Performance of the models on the FEVER leaderboard3 (4 March 2019). The top two rows are the models submitted during the FEVER Shared Task that have higher FEVER scores than ours. The middle three rows are the top-three FEVER models submitted after the Shared Task."
P19-1225,D14-1179,0,\N,Missing
P19-1225,D14-1162,0,\N,Missing
P19-1225,D18-1453,0,\N,Missing
P19-2030,N18-1118,0,0.032865,"p and 7-layer stacked vanilla Transformer ei Layers 4 4 5 5 6 6 7 7 (4) (5) First, MLP attention between the output of the (h) first hop, ai , and the query, Q, is calculated. Attention is considered as the calculation of a relationship between the query and the key/value. Therefore, in the second hop, attention is calculated again by using the output of the first hop, rather than the key/value. Equations 4 and 5 are head gate in Figure 1. The head gate normalizes the attention score of (h) each head to βi , using the softmax function, where h ranges over all heads. In hierarchical attention (Bawden et al., 2018), the softmax function is used to select a single source from multiple sources. Here, the proposed head gate uses the softmax function to select a head from multi3.2 Experimental Setup In our experiments, the baseline was the Transformer (Vaswani et al., 2017) model. We used 1 https://sites.google.com/site/ iwsltevaluation2017/ 2 http://www.statmt.org/wmt17/ translation-task.html 219 (a) All learning curve view (b) Enlarged view (loss 3.9 to 4.4) Figure 2: Validation loss by each epoch for IWSLT2017 de-en - second hop in layer n to 6 fairseq (Gehring et al., 2017) 3 toolkit and the source code"
P19-2030,P17-1055,0,0.0493162,"Missing"
P19-2030,N19-1423,0,0.0350455,"is well known that the Transformer is difficult to train (Popel and Bojar, 2018). As it has a large number of parameters, it takes time to converge and sometimes it does not do so at all without appropriate hyper parameter tuning. Considering the experimental results of our multi-hop attention experiments, and that of the Weight Transformer, an appropriate design of the network to combine multi-head attention could result in faster and more stable convergence of the Transformer. As the Transformer is used as a building block for the recently proposed pre-trained language models such as BERT (Devlin et al., 2019) which takes about a month for training, we think it is worthwhile to pursue this line of research including the proposed multi-hop attention. Universal Transformer (Dehghani et al., 2019) can be thought of variable-depth recurrent attention. It obtained Turing-complete expressive power in exchange for a vast increase in the number of parameters and training time. As shown in Table 4, we have proposed an efficient method to increase the depth of recurrence in terms of the number of parameters and training time. Recently, Voita et al. (2019) and Michel et al. (2019) independently reported that"
P19-2030,P17-2031,0,0.0810122,"Missing"
P19-2030,P16-1162,0,0.0856626,"tention and head gate, as shown in Figure 1 and the following equations. (h) 3 Experiment 3.1 Data (h) (h) = vbT tanh(Wb Q(h) + Ub ai ) (3) We used German-English parallel data obtained from the IWSLT2017 1 and the WMT17 2 shared tasks. The IWSLT2017 training, validation, and test sets contain approximately 160K, 7.3K, and 6.7K sentence pairs, respectively. There are approximately 5.9M sentence pairs in the WMT17 training dataset. For the WMT17 corpus, we used newstest2013 as the validation set and newstest2014 and newstest2017 as the test sets. For tokenization, we used the subword-nmt tool (Sennrich et al., 2016) to set a vocabulary size of 32,000 for both German and English. (h) (h) βi ′(h) ai = = exp(ei ) ∑N (h) n=1 exp(ei ) (h) (h) βi Uc(h) ai IWSLT2017 de→en en→de 40,747K 41,882K 40,763K 41,898K 48,103K 49,238K 48,120K 49,254K 55,459K 56,594K 55,492K 56.627K 62,816K 63,951K 62,833K 63,967K Table 4: Model Parameters Table 3: Difference between 6-layer Transformer with multi-hop and 7-layer stacked vanilla Transformer ei Layers 4 4 5 5 6 6 7 7 (4) (5) First, MLP attention between the output of the (h) first hop, ai , and the query, Q, is calculated. Attention is considered as the calculation of a re"
P19-2056,C04-1051,0,0.113486,"nda bear is eating some bamboo.” and “A panda is eating bamboo.”. Such a sentence pair would receive an unfavourable score in similarity evaluation using token-wise comparison, because every word after “panda” would be considered as a mismatched token. In contrast, the STS score given to the pair is 4.2. Omission of words “bear” and “some” in the latter sentence does not alter the meaning from the first sentence, and thus the pair is considered semantically similar. STS is similar to other semantic comparison tasks such as textual entailment (Dagan et al., 2010) and paraphrase identification (Dolan et al., 2004). One key distinction that STS has from these two tasks is that STS expects the model to output continuous scores with interpretable intermediate values rather than discrete binary values describing whether or not given sentence pairs have certain semantic relationships. 3.2 STS Estimator The STS estimator model rψ consists of two modules. As described in Eq. (6), one is the BERT encoder with pooling layer B and the other is a linear output layer (with weight vector Wψ and bias bψ ) with ReLU activation rψ . B (Y1 , Y2 ) = Pool (BERT (Y1 , Y2 )) , (6) rψ (Y1 , Y2 ) = ReLU (Wψ · B (Y1 , Y2 ) +"
P19-2056,D15-1166,0,0.0450471,"4.2 monsense inference task SWAG (Zellers et al., 2018). STS is one of the tasks included in GLUE. Sentence Pair A man is playing a guitar. A girl is playing a guitar. A panda bear is eating some bamboo. A panda is eating bamboo. 3 Models 3.1 Sentence Generation Model The sentence generation model πθ used for this research is a neural machine translation (NMT) model consisting of a single-layer LSTM encoderdecoder model with attention mechanism and the softmax output layer. The model also incorporates input feeding to make itself aware of the alignment decision in the previous decoding step (Luong et al., 2015). The encoder LSTM is bidirectional while the decoder LSTM is unidirectional. On the other hand, STS scores are tolerant of modifications that do not change the meaning of sentence. This leniency is illustrated by the second sentence pair in Table 1, “A panda bear is eating some bamboo.” and “A panda is eating bamboo.”. Such a sentence pair would receive an unfavourable score in similarity evaluation using token-wise comparison, because every word after “panda” would be considered as a mismatched token. In contrast, the STS score given to the pair is 4.2. Omission of words “bear” and “some” in"
P19-2056,S12-1051,0,0.269421,"o token mismatches. As another example, the sentence pair “He often walked to school.” and “He walked to school often.” would be severely punished by the token misalignment, despite having identical meanings. To tackle the inflexible nature of model evaluation during training, we propose an approach of using semantic similarity between the output sequence and the ground-truth sequence to train the generation model. In the proposed framework, semantic similarity of sentence pairs is estimated by a BERT-based (Devlin et al., 2018) regression model fine-tuned against Semantic Textual Similarity (Agirre et al., 2012) dataset, and the resulting score is passed back to the model using reinforcement learning strategies. Our experiment on translation datasets suggests that the proposed method is better at improving the BLEU score than the traditional cross-entropy learning. However, since the model outputs had limited paraphrastic variations, the results are also inconclusive in supporting the effectiveness of applying the proposed method to sentence generation. Traditional model training for sentence generation employs cross-entropy loss as the loss function. While cross-entropy loss has convenient propertie"
P19-2056,N18-2102,0,0.0246464,"ure known as an Encoder-Decoder model. The decoder model, the portion of Encoder400 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 400–406 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Decoder responsible for generating tokens, is usually an RNN. For an intermediate representation X, output token distribution at time t yˆt for the RNN decoder πθ can be written as st+1 = Φθ (ˆ yt , st , X) (1) yˆt+1 ∼ πθ (yt |yˆt , st , X) (2) generation model against sentence-level metrics (Pasunuru and Bansal, 2018; Ranzato et al., 2015). Sentence-level metrics commonly used in RL settings, such as BLEU, ROUGE and METEOR, are typically not differentiable, and thus are not usable under the regular supervised training. One of the common RL algorithms used in sentence generation is REINFORCE (Williams, 1992). REINFORCE is a relatively simple policy gradient algorithm. In the context of sentence generation, the goal of the agent is to maximize the expectation of the reward provided as the function r as in the following: where st is the hidden state of the decoder at time t, Φθ is the state update function,"
P19-2056,P17-1161,0,0.0289446,"oined by a separation (SEP) token and outputs intermediate representations that are then fed into the linear layer through a pooling layer. The output layer projects the input into scalar values representing the estimated STS scores for input sentence pairs. The model rψ is trained using the mean squared error (MSE) to fit the corresponding real-valued label v as written in Eq. (8). 2.4 BERT Bidirectional Encoder Representations from Transformer (BERT) (Devlin et al., 2018) is a pre-training model based on the transformer Previous premodel (Vaswani et al., 2017). training models such as ELMo (Peters et al., 2017) and OpenAI-GPT (Radford et al., 2018) used unidirectional language models to learn general language representations and this limited their ability to capture token relationships in both directions. Instead, BERT employs a bidirectional self-attention architecture to capture the language representations more thoroughly. Upon its release, BERT broke numerous state-of-the-art records such as those on a general language understanding task GLUE (Wang et al., 2018), question answering task SQuAD v1.1 (Rajpurkar et al., 2016), and grounded comLBERT = |rψ (Y1 , Y2 ) − v|2 . (8) While the use of the B"
P19-2056,2012.eamt-1.60,0,0.00977491,"ion model Ωω is trained using the MSE loss as follows: 2 ) 1 ( ˆ LBSE = rψ Y , Y − Ωω (st ) . (13) 5 3.3 Baseline Estimator The reward predictor does not share its parameter with the NMT model. Following the previous work (Ranzato et al., 2015), the baseline estimator Ωω is defined as follows: Ωω (st ) = σ (Wω · st + bω ) , 4 Experiment 4.1 Dataset (9) The dataset used for fine-tuning the STS estimator is STS-B (Cer et al., 2017). The tokenizer used is a wordpiece tokenizer for BERT. For machine translation, we used De-En parallel corpora from multi30k-dataset (Elliott et al., 2016) and WIT3 (Cettolo et al., 2012). The multi30k-dataset is comprised of textual descriptions of images while the WIT3 consists of transcribed TED talks. Each corpus provides a single validation set and multiple test sets. We chose the best models based on their scores for the validation sets and used the two newest test sets from each corpus for testing. Both corpora are tokenized using the sentencepiece BPE tokenizer with a vocabulary size of 8,000 for each language. All letters are turned to lowercase and any consecutive spaces are turned into a single space before tokenization. The source and target vocabularies are kept s"
P19-2056,D15-1044,0,0.0321925,"ed by the training with crossentropy loss. We use the BERT-based scorer fine-tuned to the Semantic Textual Similarity (STS) task for semantic similarity estimation, and train the model with the estimated scores through reinforcement learning (RL). Our experiments show that reinforcement learning with semantic similarity reward improves the BLEU scores from the baseline LSTM NMT model. 1 Introduction Sentence generation using neural networks has become a vital part of various natural language processing tasks including machine translation (Sutskever et al., 2014) and abstractive summarization (Rush et al., 2015). Most previous work on sentence generation employ crossentropy loss between the model outputs and the ground-truth sentence to guide the maximumlikelihood training on the token-level. Differentiability of cross-entropy loss is useful for computing gradients in supervised learning; however, it lacks flexibility and may penalize the generation model for a slight shift or change in token sequence even if the sequence retains the meaning. For instance, consider the sentence pair, “I watched a movie last night.” and “I saw a film last 2 Related Work 2.1 Sentence Generation Recurrent neural network"
P19-2056,W18-5446,0,0.0354785,"t al., 2018) is a pre-training model based on the transformer Previous premodel (Vaswani et al., 2017). training models such as ELMo (Peters et al., 2017) and OpenAI-GPT (Radford et al., 2018) used unidirectional language models to learn general language representations and this limited their ability to capture token relationships in both directions. Instead, BERT employs a bidirectional self-attention architecture to capture the language representations more thoroughly. Upon its release, BERT broke numerous state-of-the-art records such as those on a general language understanding task GLUE (Wang et al., 2018), question answering task SQuAD v1.1 (Rajpurkar et al., 2016), and grounded comLBERT = |rψ (Y1 , Y2 ) − v|2 . (8) While the use of the BERT-based STS estimator as an evaluation mechanism allows the sentence generation model to train its outputs against sentence-wise evaluation criteria, there is a downside to this framework. The BERT encoder expects the input sentences to be sequences of tokens. As with most sentence generation models, the outputs of the encoderdecoder model described in the previous subsection are sequences of output probability distributions of tokens. 402 LRL in the RL stag"
P19-2056,D18-1009,0,0.0140093,"model training. Reinforcement learning, a framework in which the agent must choose a series of discrete actions to maximize the reward returned from its surrounding environment, is one of such approaches. The advantages of using RL are that the reward for an action does not have to be returned spontaneously and that the reward function does not have to be differentiable by the parameter of the agent model. Because of these advantages, RL has often been used as a means to train sentence 401 Table 1: Examples of STS similarity scores in STS-B dataset. Score 2.8 4.2 monsense inference task SWAG (Zellers et al., 2018). STS is one of the tasks included in GLUE. Sentence Pair A man is playing a guitar. A girl is playing a guitar. A panda bear is eating some bamboo. A panda is eating bamboo. 3 Models 3.1 Sentence Generation Model The sentence generation model πθ used for this research is a neural machine translation (NMT) model consisting of a single-layer LSTM encoderdecoder model with attention mechanism and the softmax output layer. The model also incorporates input feeding to make itself aware of the alignment decision in the previous decoding step (Luong et al., 2015). The encoder LSTM is bidirectional w"
P98-2152,P96-1010,0,0.0216401,"oundaries are placed by white spaces. If the tokenized string is not in the dictionary, it is a nonword. For a non-word, correction candidates are retrieved from the dictionary by approximate string 922 match techniques using context-independent word distance measures such as edit distance (Wagner and Fischer, 1974) and ngram distance (Angell et al., 1983). Recently, statistical language models and featurebased method have been used for context-sensitive spelling correction, where errors are corrected considering the context in which the error occurs (Church and Gale, 1991; Mays et al., 1991; Golding and Schabes, 1996). Similar techniques are used for correcting the output of English OCRs (Tong and Evans, 1996) and English speech recognizers (Ringger and Allen, 1996). There are two problems in Japanese (and Chinese) spelling correction. The first is the word boundary problem. It is impossible to use isolated word error correction techniques because there are no delimiters between words. The second is the short word problem. Word distance measures are useless because the average word length is short (< 2), and the character set is large (&gt; 3000). There are a much larger number of one edit distance neighbors"
P98-2152,C96-2136,1,0.615485,"r correction techniques because there are no delimiters between words. The second is the short word problem. Word distance measures are useless because the average word length is short (< 2), and the character set is large (&gt; 3000). There are a much larger number of one edit distance neighbors for a word, compared with English. Recently, the first problem was solved by selecting the most likely word sequence from all combinations of exactly and approximately matched words using a Viterbi-like word segmentation algorithm and a statistical language model considering unknown words and non-words (Nagata, 1996). However, the second problem is not solved yet, at least elegantly. The solution presented in (Nagata, 1996) which sorts a list of one edit distance words considering the context in which it will be placed is inaccurate because the context itself might include some errors. In this paper, we present a context-independent approximate word match method using character shape similarity. This is suitable for languages with large character sets, such as Japanese and Chinese. We also present a method to build a statistical OCR model by smoothing the character confusion probability using character sh"
P98-2152,W96-0108,0,\N,Missing
P99-1036,J96-2001,0,0.0976728,"corpus whose infrequent words are replaced with their corresponding unknown word tags based on their part of speeches 2 Table 1 shows examples of word bigrams including unknown word tags. Here, a word is represented by a list of surface form, pronunciation, and part of speech, which are delimited by a slash '/'. The first 2 Throughout in this paper, we use the term ""infrequent words"" to represent words that appeared only once in the corpus. They are also called ""hapax legomena"" or ""hapax words"". It is well known that the characteristics of hapax legomena are similar to those of unknown words (Baayen and Sproat, 1996). 278 (4) where k is the length of the character sequence. We call P ( k I &lt; U N K > ) the word length model, and P ( c z . . . ck Ik, &lt; U N K > ) the word spelling model. In order to estimate the entropy of English, (Brown et al., 1992) approximated P ( k I &lt; U N K > ) by a Poisson distribution whose parameter is the average word length A in the training corpus, and P ( c z . . . cklk, &lt; U N K > ) by the product of character zerogram probabilities. This means all characters in the character set are considered to be selected independently and uniformly. )k P(Cl . . . c k I &lt; U N K > ) -~ -~. e"
P99-1036,J92-1002,0,0.0960765,"Missing"
P99-1036,W95-0109,0,0.050074,"Missing"
P99-1036,P97-1030,0,0.116948,"Missing"
P99-1036,J97-3003,0,0.0605112,"Missing"
P99-1036,C96-2202,0,0.183978,"Missing"
P99-1036,C94-1032,1,0.850403,"an unknown word as a word having a special part of speech &lt; U N K > . Then, the unknown word model is formally defined as the joint probability of the character sequence wi = cl .. • ck if it is an unknown word. Without loss of generality, we decompose it into the product of word length probability and word spelling probability given its length, relative frequencies of the corresponding events in the word segmented training corpus, with appropriate smoothing techniques. The maximization search can be efficiently implemented by using the Viterbilike dynamic programming procedure described in (Nagata, 1994). 2.2 Unknown P(&lt;U-t>lwi-1)P(wil&lt;U-t>,wi-a) P(&lt;U-t>[wi_l)P(wil&lt;U-t>) (3) Here, we made an assumption that the spelling of an unknown word solely depends on its part of speech and is independent of the previous word. This is the same assumption made in the hidden Markov model, which is called output independence. The probabilities P ( &lt; U - t > l w i _ l ) can be estimated from the relative frequencies in the training corpus whose infrequent words are replaced with their corresponding unknown word tags based on their part of speeches 2 Table 1 shows examples of word bigrams including unknown wo"
P99-1036,C96-2136,1,0.875878,"y using a word-based language model and the Viterbi-like dynamic programming procedures (Nagata, 1994; Yamamoto, 1996; Takeuchi and Matsumoto, 1997; Haruno and Matsumoto, 1997). About the same accuracy is reported in Chinese by statistical methods (Sproat et al., 1996). But there has been relatively little improvement in recent years because most of the remaining errors are due to unknown words. There are two approaches to solve this problem: to increase the coverage of the dictionary (Fung and Wu, 1994; Chang et al., 1995; Mori and Nagao, 1996) and to design a better model for unknown words (Nagata, 1996; Sproat et al., 1996). We take the latter approach. To improve word segmentation accuracy, (Nagata, 1996) used a single general purpose unknown word model, while (Sproat et al., 1996) used a set of specific word models such as for plurals, personal names, and transliterated foreign words. The goal of our research is to assign a correct part of speech to unknown word as well as identifying it correctly. In this paper, we present a novel statistical model for Japanese unknown words. It consists of a set of word models for each part of speech and word type. We classified Japanese words into nine"
P99-1036,W96-0213,0,0.240127,"Missing"
P99-1036,J96-3004,0,0.0845444,"Missing"
P99-1036,J93-2006,0,0.0635274,"Missing"
P99-1036,W96-0113,0,0.0477968,"Missing"
P99-1036,J95-4004,0,\N,Missing
W01-1413,J93-2003,0,0.00330428,"ngine. We are very interested in combining such ideas as focused crawling (Chakrabarti et al., 1999) and domain-specific Internet portals (McCallum et al., 2000) with the proposed term translation extractor to develop a domain-specific on-line dictionary service. 6 Conclusion We investigated the possibility of using the web as a bilingual dictionary, and reported the preliminary results of an experiment on extracting the English translations of given Japanese technical terms from the web. One interesting approach to extending the current system is to introduce a statistical translation model (Brown et al., 1993) to filter out irrelevant translation candidates and to extract the most appropriate subpart from a long English sequence as the translation by locally aligning the Japanese and English sequences. Unlike ordinary machine translation which generates English sentences from Japanese sentences, this is a recognition-type application which identifies whether or not a Japanese term and an English term are translations of each other. Considering the fact that what the statistical translation model provides is the joint probability of Japanese and English phrases, this could be a more natural and pros"
W01-1413,P00-1062,0,0.0159514,"fees, domains, language pairs, etc. One approach that partially overcomes these limitations is to collect parallel texts from the web (Nie et al., 1999; Resnik, 1999). To provide better coverage with fewer restrictions, we focused on partially bilingual text. Considering the enormous volume of such texts and the variety of fields covered, we believe they are the best resource to mine for MT-related applications that involve English and Asian languages. The current system for extracting the translation of a given term is more similar to the information extraction system for term descriptions (Fujii and Ishikawa, 2000) than any other machine translation systems. In order to collect descriptions for technical term X, such as ‘data mining’, (Fujii and Ishikawa, 2000) collected phrases like “X is Y” and “X is defined as Y”, from the web. As our system used a scoring function based solely on byte distance, introducing this kind of pattern matching might improve its accuracy. Practically speaking, the factor that most influences the accuracy of the term translation extractor is the set of documents returned from the search engine. In order to evaluate the system, we used a test set that guarantees to contain at"
W01-1413,P99-1068,0,0.0133618,"ge information retrieval. 5 Discussion and Related Works Previous studies on bilingual text mainly focused on either parallel texts, non-parallel texts, or comparable texts, in which a pair of texts are written in two different languages (Veronis, 2000). However, except for governmental documents from Canada (English/French) and Hong Kong (Chinese/English), bilingual texts are usually subject to such limitations as licensing conditions, usage fees, domains, language pairs, etc. One approach that partially overcomes these limitations is to collect parallel texts from the web (Nie et al., 1999; Resnik, 1999). To provide better coverage with fewer restrictions, we focused on partially bilingual text. Considering the enormous volume of such texts and the variety of fields covered, we believe they are the best resource to mine for MT-related applications that involve English and Asian languages. The current system for extracting the translation of a given term is more similar to the information extraction system for term descriptions (Fujii and Ishikawa, 2000) than any other machine translation systems. In order to collect descriptions for technical term X, such as ‘data mining’, (Fujii and Ishikawa"
W03-1506,W98-1118,0,0.0886549,"tively. Since N-best word sequence search and statistical language model don’t depend on language, we can apply this analytical engine to all languages. This makes it possible to treat any language if a corpus is available for training the language model. The next section explains the hidden Markov model used for named-entity recognition. 3. Named-entity Recognition Model The named-entity task is to recognize entities such as organizations, personal names, and locations. Several papers have tackled named-entity recognition through the use of Markov model (HMM) [6], maximum entropy method (ME) [7, 8], and support vector machine (SVM) [9]. It is generally said that HMM is inferior to ME and SVM in terms of accuracy, but is superior with regard to training and processing speed. That is, HMM is suitable for applications that require realtime response or have to process large amounts of text such as information retrieval. We extended the original HMM reported by BBN. BBN’s named-entity system is for English and offers high accuracy. Input sentence 東京ディズニーランド Tokyo Disneyland は、東京駅から10km is 10 km away from 離れている the Tokyo station. Word Candidates 東 東京 ディズニーランド は 、 東 東京 東京駅 京 京駅 京駅か 駅 駅か 駅から か"
W03-1506,P00-1042,0,0.0220943,"tively. Since N-best word sequence search and statistical language model don’t depend on language, we can apply this analytical engine to all languages. This makes it possible to treat any language if a corpus is available for training the language model. The next section explains the hidden Markov model used for named-entity recognition. 3. Named-entity Recognition Model The named-entity task is to recognize entities such as organizations, personal names, and locations. Several papers have tackled named-entity recognition through the use of Markov model (HMM) [6], maximum entropy method (ME) [7, 8], and support vector machine (SVM) [9]. It is generally said that HMM is inferior to ME and SVM in terms of accuracy, but is superior with regard to training and processing speed. That is, HMM is suitable for applications that require realtime response or have to process large amounts of text such as information retrieval. We extended the original HMM reported by BBN. BBN’s named-entity system is for English and offers high accuracy. Input sentence 東京ディズニーランド Tokyo Disneyland は、東京駅から10km is 10 km away from 離れている the Tokyo station. Word Candidates 東 東京 ディズニーランド は 、 東 東京 東京駅 京 京駅 京駅か 駅 駅か 駅から か"
W03-1506,J93-2004,0,0.0372531,"sparse data problem. We changed this smoothing to linear interpolation to improve the accuracy, and in addition, we used not only the morpheme frequency of terms but also part of speech frequency. Table 4 shows the linear interpolation scheme used here. Underlined items are added in our model. The weight for each probability was decided from experiments. 4. Experiments To evaluate our system, we prepared original corpora for Japanese, Chinese, Korean and English. The material was mainly taken from newspapers and Web texts. We used the morpheme analysis definition of Pen Tree Bank for English [11], Jtag for Japanese [12], Beijing Univ. for Chinese [13] and MATEC99 for Korean [14]. The named-entity tag definitions were based on MUC [15] for English and IREX [16] for Japanese. We defined Chinese and Korean named-entity tags following the Japanese IREX specifications. Table 5 shows dictionary and corpus size. Dictionary words means the size of the dictionary for Table 4. Linear Interpolation Scheme P( NC i |NC i −1 , wi −1 ) P( wi |NC1 , NCi −1 ) P ( wi |wi −1 , NC1 ) P ( NCi |NCi −1 , posi −1 ) P ( posi |NC1 , NCi −1 ) P( posi |posi −1 , NC1 ) P( NCi |NCi −1 ) P( wi |NC1 ) P( posi |NC1 )"
W03-1506,N01-1025,0,0.0216846,") P(NC ) 1 / number of NC P( posi |NC1 ) morphological analysis. Total words and sentences represent the size of the corpus for named-entity recognition. Named-entity accuracy is expressed in terms of recall and precision. We also use the F-measure to indicate the overall performance. It is calculated as follows; (4) F= 2 × recall × precision recall + precision Table 6 shows the F-measure for all languages. Since we used our original corpora in this evaluation, we cannot compare our results to those of previous works. Accordingly, we also evaluated SVM using our original corpora (see Table 6) [17]. The accuracy of HMM and SVM were approximately equivalent. But the analysis speed of HMM was ten times faster than that of SVM [9]. This means that our system is very fast and has state-of-the-art accuracy in four languages. We noted that the accuracy of SVM is unusually lower than that of HMM for Japanese. We have not yet confirmed the cause of this, but a plausible argument is as follows. First, the word segmentation ambiguity has a worse affect on accuracy than expected. Since current SVM implementations can not handle N-best morpheme candidates and lower-order candidates are not consider"
W03-1506,P03-1010,0,0.0182632,", we improved equation (5) to take into account the difference in a word sequence length and cooccurrence frequency as follows; (6) S (Y |X ) = freq ⋅ match( X ) ⋅ match(Y ) ⋅ P (Y |X ) E (Y |X ) : cooccurrence frequency of X and Y in parallel text match( X ) : ratio of t ( y j |xi ) ≠ 0 in X freq match(Y ) : ratio of t ( y j |xi ) ≠ 0 in Y E (Y |X ) = ε m l ∏ ⋅ ∑ t ( y j |xi ) (l +1) m j =1 i =1 t ( y j |xi ) is the average of t ( y j |xi ) . S (Y |X ) is used as a measure of candidate suitability. We used Japanese-English news article alignment data as parallel texts that is released by CRL [19, 20]. In this data, articles and sentences are aligned automatically. We separated the parallel text into a small set (about 1000 sentences) and a Table 7. List of Bilingual Lexicons North Korea United States International Monetary Fund Soviet Union Middle East North Atlantic Treaty Organization U.S. President Bill Clinton North American Free Trade Agreement European Community Taiwan Strait Clinton administration U.N. General Assembly Tokyo Stock Exchange 北朝鮮 米国 国際通貨基金 ソ連 中東 北大西洋条約機構 クリントン米大統領 北米自由貿易協定 欧州共同体 台湾海峡 クリントン政権 国連総会 東京証券取引所 large set (about 150 thousand sentences). We extracted bilingual"
W03-1506,C02-1054,0,\N,Missing
W03-1506,C94-1032,1,\N,Missing
W03-1506,J93-2003,0,\N,Missing
W03-1506,C96-2110,0,\N,Missing
W03-1506,P98-1068,0,\N,Missing
W03-1506,C98-1065,0,\N,Missing
W03-1506,W99-0612,0,\N,Missing
W04-3255,J93-2003,0,0.0245694,"d into a composition model beforehand. Furthermore, the ambiguity of the composition model is reduced by the statistics of hypotheses while decoding. The experimental results show that the proposed model representation drastically improves the efficiency of decoding compared to the dynamic composition of the submodels, which corresponds to conventional approaches. 1 Introduction Recently, research on statistical machine translation has grown along with the increase in computational power as well as the amount of bilingual corpora. The basic idea of modeling machine translation was proposed by Brown et al. (1993), who assumed that machine translation can be modeled on noisy channels. The source language is encoded from a target language by a noisy channel, and translation is performed as a decoding process from source language to target language. Knight (1999) showed that the translation problem defined by Brown et al. (1993) is NPcomplete. Therefore, with this model it is almost impossible to search for optimal solutions in the decoding process. Several studies have proposed methods for searching suboptimal solutions. Berger et al. (1996) and Och et al. (2001) proposed such depth-first search methods"
W04-3255,H91-1026,0,0.0265323,"crease model errors, but QR =S corrects the errors caused by merging states. Unlike ordinary FSA minimization, states are merged without considering their successor states. If the weight represents probability, thesum of the weights of output transitions may not be 1.0 after merging states, and then thecondition of probability may be destroyed. Since the decoder does not sum up all possible paths but searches for the most appropriate paths, this kind of state merging does not pose a serious problem in practice. In the following experiment, we measured the association between states by &quot; + in Gale and Church (1991). &quot; + is a T + -like statistic that is bounded between 0 and 1. If the &quot; + of two states is higher than the specified threshold, these two states are merged. The definition of &quot; + is U    U   V  V : as W fol-, :   V V V K lows, where , X Y  V  V  Z , and &gt;[ + G Z  K ]X . G +number of hypothesis lists.  V  V  is the  total   (  V V : V ) is the number of hypothesis lists in + (both V : and V appear). which V appears + &gt;   X &quot; +   O^   _^`X   K ^ + &gt;/  Xa^ (&gt;   K K Figure 6: FSA for All Input Permutations Merging the beginning and end states of a transition who"
W04-3255,knight-al-onaizan-1998-translation,0,0.544428,"1) studied WFST models in call-routing tasks, and Kumar and Byrne (2003) modeled phrase-based translation by WFSTs. All of these studies mainly focused on the representation of each submodel used in machine translation. However, few studies have focued on the integration of each WFST submodel to improve the decoding efficiency of machine translation. To this end, we propose a method that expands all of the submodels into a composition model, reducing the ambiguity of the expanded model by the statistics of hypotheses while decoding. First, we explain the translation model (Brown et al., 1993; Knight and Al-Onaizan, 1998) that we used as a base for our decoding research. Second, our proposed method is introduced. Finally, experimental results show that our proposed method drastically improves decoding efficiency. 2 IBM Model For our decoding research, we assume the IBMstyle modeling for translation proposed in Brown et al. (1993). In this model, translation from Japanese to find  . Using attempts to English  the  that maximizes Bayes’ rule,  is rewritten as                 is referred to as a language model and where     is  referred to as a tra"
W04-3255,J99-4005,0,0.0928239,"decoding compared to the dynamic composition of the submodels, which corresponds to conventional approaches. 1 Introduction Recently, research on statistical machine translation has grown along with the increase in computational power as well as the amount of bilingual corpora. The basic idea of modeling machine translation was proposed by Brown et al. (1993), who assumed that machine translation can be modeled on noisy channels. The source language is encoded from a target language by a noisy channel, and translation is performed as a decoding process from source language to target language. Knight (1999) showed that the translation problem defined by Brown et al. (1993) is NPcomplete. Therefore, with this model it is almost impossible to search for optimal solutions in the decoding process. Several studies have proposed methods for searching suboptimal solutions. Berger et al. (1996) and Och et al. (2001) proposed such depth-first search methods as stack decoders. Wand and Waibel (1997) and Tillmann and Ney (2003) proposed breadth-first search methods, i.e. beam search. Germann (2001) and Watanabe and Sumita (2003) proposed greedy type decoding methods. In all of these search algorithms, bett"
W04-3255,N03-1019,0,0.0778281,"otivation is to apply this approach to machine translation. However, WFST optimization operations such as determinization are nearly impossible to apply to WFSTs in machine translation because they are much more ambiguous than speech recognition. To reduce the ambiguity, we propose a WFST optimization method that considers the statistics of hypotheses while decoding. Some approaches have applied WFST to statistical machine translation. Knight and AlOnaizan (1998) proposed the representation of IBM model 3 with WFSTs; Bangalore and Riccardi (2001) studied WFST models in call-routing tasks, and Kumar and Byrne (2003) modeled phrase-based translation by WFSTs. All of these studies mainly focused on the representation of each submodel used in machine translation. However, few studies have focued on the integration of each WFST submodel to improve the decoding efficiency of machine translation. To this end, we propose a method that expands all of the submodels into a composition model, reducing the ambiguity of the expanded model by the statistics of hypotheses while decoding. First, we explain the translation model (Brown et al., 1993; Knight and Al-Onaizan, 1998) that we used as a base for our decoding res"
W04-3255,J03-1002,0,0.00755588,"adopted a simpler method to deal with a negative L loop as described above. 5 Experiments 5.1 Effect of Full Expansion To clarify the effectiveness of a full-expansion approach, we compared the computational costs while using the same decoder with both dynamic composition and static composition, a full-expansion model in other words. In the forward beam-search, 0dcd0de any hypothesis whose probability is lower than of the top of the hypothesis list is pruned. In this experiment, permutation is restricted, and words can be moved 6 positions at most. The translation model was trained by GIZA++ (Och and Ney, 2003), and the trigram was trained by the CMU-Cambridge Statistical Language Modeling Toolkit v2 (Clarkson and Rosenfeld, 1997). For the experiment, we used a Japanese-toEnglish bilingual corpus consisting of example sentences for a rule-based machine translation system. Each language sentence is aligned in the corpus. The total number of sentence pairs is 20,204. We used 17,678 pairs for training and 2,526 pairs for the test. The average length of Japanese sentences was 8.4 words, and that of English sentences was 6.7 words. The Japanese vocabulary consisted of 15,510 words, and the English vocabu"
W04-3255,W01-1408,0,0.015455,"ng machine translation was proposed by Brown et al. (1993), who assumed that machine translation can be modeled on noisy channels. The source language is encoded from a target language by a noisy channel, and translation is performed as a decoding process from source language to target language. Knight (1999) showed that the translation problem defined by Brown et al. (1993) is NPcomplete. Therefore, with this model it is almost impossible to search for optimal solutions in the decoding process. Several studies have proposed methods for searching suboptimal solutions. Berger et al. (1996) and Och et al. (2001) proposed such depth-first search methods as stack decoders. Wand and Waibel (1997) and Tillmann and Ney (2003) proposed breadth-first search methods, i.e. beam search. Germann (2001) and Watanabe and Sumita (2003) proposed greedy type decoding methods. In all of these search algorithms, better representation of the statistical model in systems can improve the search efficiency. For model representation, a search method based Masaaki Nagata NTT Cyber Space Labs. 1-1 Hikari-no-Oka Yokosuka-shi Kanagawa 239-0847 Japan nagata.masaaki@lab.ntt.co.jp on weighted finite-state transducer (WFST) (Mohri"
W04-3255,P02-1040,0,0.0716066,"Each language sentence is aligned in the corpus. The total number of sentence pairs is 20,204. We used 17,678 pairs for training and 2,526 pairs for the test. The average length of Japanese sentences was 8.4 words, and that of English sentences was 6.7 words. The Japanese vocabulary consisted of 15,510 words, and the English vocabulary was 11,806 words. Table 1 shows the size of the WFSTs used in the experiment. In these WFSTs, special symbols that express beginning and end of sentence are added to the WFSTs described in the previous section. The NIST score (Doddington, 2002) and BLEU Score (Papineni et al., 2002) were used to measure translation accuracy. Table 2 shows the experimental results. The fullexpansion model provided translations more than 10 times faster than conventional dynamic composition submodels without degrading accuracy. However, the NIST scores are slightly different. In the course of composition, some paths that do not reach the final states are produced. In the full-expansion model these paths are trimmed. These trimmed paths may cause a slight difference in NIST scores. 5.2 Effect of Ambiguity Reduction To show the effect of ambiguity reduction, we compared the translation resul"
W04-3255,J03-1005,0,0.0175463,"odeled on noisy channels. The source language is encoded from a target language by a noisy channel, and translation is performed as a decoding process from source language to target language. Knight (1999) showed that the translation problem defined by Brown et al. (1993) is NPcomplete. Therefore, with this model it is almost impossible to search for optimal solutions in the decoding process. Several studies have proposed methods for searching suboptimal solutions. Berger et al. (1996) and Och et al. (2001) proposed such depth-first search methods as stack decoders. Wand and Waibel (1997) and Tillmann and Ney (2003) proposed breadth-first search methods, i.e. beam search. Germann (2001) and Watanabe and Sumita (2003) proposed greedy type decoding methods. In all of these search algorithms, better representation of the statistical model in systems can improve the search efficiency. For model representation, a search method based Masaaki Nagata NTT Cyber Space Labs. 1-1 Hikari-no-Oka Yokosuka-shi Kanagawa 239-0847 Japan nagata.masaaki@lab.ntt.co.jp on weighted finite-state transducer (WFST) (Mohri et al., 2002) has achieved great success in the speech recognition field. The basic idea is that each statisti"
W04-3255,P97-1047,0,0.0740554,"Missing"
W04-3255,2003.mtsummit-papers.54,0,0.0259442,"and translation is performed as a decoding process from source language to target language. Knight (1999) showed that the translation problem defined by Brown et al. (1993) is NPcomplete. Therefore, with this model it is almost impossible to search for optimal solutions in the decoding process. Several studies have proposed methods for searching suboptimal solutions. Berger et al. (1996) and Och et al. (2001) proposed such depth-first search methods as stack decoders. Wand and Waibel (1997) and Tillmann and Ney (2003) proposed breadth-first search methods, i.e. beam search. Germann (2001) and Watanabe and Sumita (2003) proposed greedy type decoding methods. In all of these search algorithms, better representation of the statistical model in systems can improve the search efficiency. For model representation, a search method based Masaaki Nagata NTT Cyber Space Labs. 1-1 Hikari-no-Oka Yokosuka-shi Kanagawa 239-0847 Japan nagata.masaaki@lab.ntt.co.jp on weighted finite-state transducer (WFST) (Mohri et al., 2002) has achieved great success in the speech recognition field. The basic idea is that each statistical model is represented by a WFST and they are composed beforehand; the composed model is optimized by"
W04-3255,N01-1018,0,\N,Missing
W10-1757,W06-1615,0,0.0749079,"for multitask learning on partof-speech tagging, chunking, semantic role labeling, etc. They showed that jointly learning these related tasks lead to overall improvements. (Deselaers et al., 2009) applies similar methods for machine transliteration. In information extraction, learning different relation types can be naturally cast as a multitask problem (Jiang, 2009; Carlson et al., 2009). Our work can be seen as following the same philosophy, but applied to N-best lists. In other areas, (Reichart et al., 2008) introduced an active learning strategy for annotating multitask linguistic data. (Blitzer et al., 2006) applies the multitask algorithm of (Ando and Zhang, 2005) to domain adaptation problems in NLP. We expect that more novel applications of multitask learning will appear in NLP as the techniques become scalable and standard. 6 Discussion and Conclusion N-best reranking is a beneficial framework for experimenting with large feature sets, but unfortunately feature sparsity leads to overfitting. We addressed this by re-casting N-best lists as multitask learning data. Our MT experiments show consistent statistically significant improvements. From the Bayesian view, multitask formulation of N-best"
W10-1757,W09-2201,0,0.0321418,"r better features. Multitask learning is currently an active subfield 381 within machine learning. There has already been some applications in NLP: For example, (Collobert and Weston, 2008) uses a deep neural network architecture for multitask learning on partof-speech tagging, chunking, semantic role labeling, etc. They showed that jointly learning these related tasks lead to overall improvements. (Deselaers et al., 2009) applies similar methods for machine transliteration. In information extraction, learning different relation types can be naturally cast as a multitask problem (Jiang, 2009; Carlson et al., 2009). Our work can be seen as following the same philosophy, but applied to N-best lists. In other areas, (Reichart et al., 2008) introduced an active learning strategy for annotating multitask linguistic data. (Blitzer et al., 2006) applies the multitask algorithm of (Ando and Zhang, 2005) to domain adaptation problems in NLP. We expect that more novel applications of multitask learning will appear in NLP as the techniques become scalable and standard. 6 Discussion and Conclusion N-best reranking is a beneficial framework for experimenting with large feature sets, but unfortunately feature sparsi"
W10-1757,N09-1025,0,0.167697,"tanabe et al., 2007) defines feature templates based on bilingual word alignments, which lead to extraction of heavilylexicalized features of the form: 2. The input (f ) has high variability (e.g. large vocabulary size), so that features for different inputs are rarely shared. 3. The N-best list output also exhibits high variability (e.g. many different word reorderings). Larger N may improve reranking performance, but may also increase feature sparsity. When the number of features is too large, even popular reranking algorithms such as SVM (Shen et al., 2004) and MIRA (Watanabe et al., 2007; Chiang et al., 2009) may fail. Our goal here is to address this situation. 3 Proposed Reranking Framework h(e, f ) =  1       0 if foreign word “Monsieur” and English word “Mr.” co-occur in e,f otherwise In the following, we first give an intuitive comparison between single vs. multiple task learning (Section 3.1), before presenting the general metaalgorithm (Section 3.2) and particular instantiations (Section 3.3). (2) One can imagine that such features are sparse because it may only fire for input sentences that contain the word “Monsieur”. For all other input sentences, it is an useless, inactive featur"
W10-1757,J07-2003,0,0.0561179,"Missing"
W10-1757,J05-1003,0,0.264845,"f features. 1. We introduce the idea of viewing N-best reranking as a multitask learning problem. This view is particularly apt to any general reranking problem with sparse feature sets. 2. We propose a simple meta-algorithm that first discovers common feature representations across N-bests (via multitask learning) before training a conventional reranker. Thus it is easily applicable to existing systems. 1 Introduction Many natural language processing applications, such as machine translation (MT), parsing, and language modeling, benefit from the N-best reranking framework (Shen et al., 2004; Collins and Koo, 2005; Roark et al., 2007). The advantage of N-best reranking is that it abstracts away the complexities of first-pass decoding, allowing the researcher to try new features and learning algorithms with fast experimental turnover. In the N-best reranking scenario, the training data consists of sets of hypotheses (i.e. N-best lists) generated by a first-pass system, along with their labels. Given a new N-best list, the goal is to rerank it such that the best hypothesis appears near the top of the list. Existing research have focused on training a single reranker directly on the 3. We demonstrate that"
W10-1757,N09-1068,0,0.0391788,"gularizer to ensure that the learned functions of related tasks are close to each other. The popular ℓ1 /ℓ2 objective can be optimized by various methods, such as boosting (Obozinski et al., 2009) and convex programming (Argyriou et al., 2008). Yet another regularizer is the ℓ1 /ℓ∞ norm (Quattoni et al., 2009), which replaces the 2-norm with a max. One could also define a regularizer to ensure i that each task-specific to some average P wi is close parameter, e.g. i ||w − wavg ||2 . If we interpret wavg as a prior, we begin to see links to Hierarchical Bayesian methods for multitask learning (Finkel and Manning, 2009; Daume, 2009). 2. Shared Subspace: This approach assumes that there is an underlying feature subspace that is common to all tasks. Early works on multitask learning implement this by neural networks, where different tasks have different output layers but share the same hidden layer (Caruana, 1997). Another method is to write the weight vector as two parts w = [u; v] and let the task-specific function be uT · h(e, f ) + vT · Θ · h(e, f ) (Ando and Zhang, 2005). Θ is a D ′ × D matrix that maps the original features to a subspace common to all tasks. The new feature representation is computed by"
W10-1757,W08-0804,0,0.0274275,"res are shared: Wa : » – 4 0 0 4 0 3 3 0 4 4 3 3 → 14 Wb : » – 4 0 3 4 0 3 0 0 4 5 3 0 → 12 2 In MT, evaluation metrics like BLEU do not exactly decompose across sentences, so for some training algorithms this loss is an approximation. [optional] RandomHashing({Hi }) W = MultitaskLearn({(Hi , yi )}) hc = ExtractCommonFeature(W) {Hic } = RemapFeature({Hi }, hc ) wc = ConventionalReranker({(Hic , yi )}) The first step, random hashing, is optional. Random hashing is an effective trick for reducing the dimension of sparse feature sets without suffering losses in fidelity (Weinberger et al., 2009; Ganchev and Dredze, 2008). It works by collapsing random subsets of features. This step can be performed to speed-up multitask learning later. In some cases, the original feature dimension may be so large that hashed representations may be necessary. The next two steps are key. A multitask learning algorithm is run on the N-best lists, and a common feature space shared by all lists is extracted. For example, if one uses the multitask objective of Eq. 5, the result of step 2 is a set of weights W. ExtractCommonFeature(W) then returns the feature id’s (either from original or hashed representation) that receive nonzero"
W10-1757,P09-1114,0,0.0254945,"help discover better features. Multitask learning is currently an active subfield 381 within machine learning. There has already been some applications in NLP: For example, (Collobert and Weston, 2008) uses a deep neural network architecture for multitask learning on partof-speech tagging, chunking, semantic role labeling, etc. They showed that jointly learning these related tasks lead to overall improvements. (Deselaers et al., 2009) applies similar methods for machine transliteration. In information extraction, learning different relation types can be naturally cast as a multitask problem (Jiang, 2009; Carlson et al., 2009). Our work can be seen as following the same philosophy, but applied to N-best lists. In other areas, (Reichart et al., 2008) introduced an active learning strategy for annotating multitask linguistic data. (Blitzer et al., 2006) applies the multitask algorithm of (Ando and Zhang, 2005) to domain adaptation problems in NLP. We expect that more novel applications of multitask learning will appear in NLP as the techniques become scalable and standard. 6 Discussion and Conclusion N-best reranking is a beneficial framework for experimenting with large feature sets, but unfor"
W10-1757,P05-1024,1,0.783566,"engineering in various MT tasks. 2. Designing better training algorithms: Nbest reranking can be seen as a subproblem of structured prediction, so many general structured prediction algorithms (c.f. (Bakir et al., 2007)) can be applied. In fact, some structured prediction algorithms, such as the MIRA algorithm used in dependency parsing (McDonald et al., 2005) and MT (Watanabe et al., 2007) uses iterative sets of N-best lists in its training process. Other training algorithms include perceptron-style algorithms (Liang et al., 2006), MaxEnt (Charniak and Johnson, 2005), and boosting variants (Kudo et al., 2005). The division into two research focuses is convenient, but may be suboptimal if the training algorithm and features do not match well together. Our work can be seen as re-connecting the two focuses, where the training algorithm is explicitly used to help discover better features. Multitask learning is currently an active subfield 381 within machine learning. There has already been some applications in NLP: For example, (Collobert and Weston, 2008) uses a deep neural network architecture for multitask learning on partof-speech tagging, chunking, semantic role labeling, etc. They showed that jo"
W10-1757,N04-1022,0,0.0383443,"Missing"
W10-1757,P06-1096,0,0.0716939,"., 2009) and IWSLT (Paul, 2009) also contains a wealth of information for feature engineering in various MT tasks. 2. Designing better training algorithms: Nbest reranking can be seen as a subproblem of structured prediction, so many general structured prediction algorithms (c.f. (Bakir et al., 2007)) can be applied. In fact, some structured prediction algorithms, such as the MIRA algorithm used in dependency parsing (McDonald et al., 2005) and MT (Watanabe et al., 2007) uses iterative sets of N-best lists in its training process. Other training algorithms include perceptron-style algorithms (Liang et al., 2006), MaxEnt (Charniak and Johnson, 2005), and boosting variants (Kudo et al., 2005). The division into two research focuses is convenient, but may be suboptimal if the training algorithm and features do not match well together. Our work can be seen as re-connecting the two focuses, where the training algorithm is explicitly used to help discover better features. Multitask learning is currently an active subfield 381 within machine learning. There has already been some applications in NLP: For example, (Collobert and Weston, 2008) uses a deep neural network architecture for multitask learning on p"
W10-1757,P05-1012,0,0.149052,", 2009) describes new features for hierarchical phrase-based MT, while (Collins and Koo, 2005) describes features for parsing. Evaluation campaigns like WMT (Callison-Burch et al., 2009) and IWSLT (Paul, 2009) also contains a wealth of information for feature engineering in various MT tasks. 2. Designing better training algorithms: Nbest reranking can be seen as a subproblem of structured prediction, so many general structured prediction algorithms (c.f. (Bakir et al., 2007)) can be applied. In fact, some structured prediction algorithms, such as the MIRA algorithm used in dependency parsing (McDonald et al., 2005) and MT (Watanabe et al., 2007) uses iterative sets of N-best lists in its training process. Other training algorithms include perceptron-style algorithms (Liang et al., 2006), MaxEnt (Charniak and Johnson, 2005), and boosting variants (Kudo et al., 2005). The division into two research focuses is convenient, but may be suboptimal if the training algorithm and features do not match well together. Our work can be seen as re-connecting the two focuses, where the training algorithm is explicitly used to help discover better features. Multitask learning is currently an active subfield 381 within m"
W10-1757,N04-1021,0,0.102794,"Missing"
W10-1757,P02-1040,0,0.0791554,"Missing"
W10-1757,2009.iwslt-evaluation.1,0,0.0128431,"feature sets, with corresponding feature size and train/test BLEU/PER. All multitask features give statistically significant improvements over the baselines (boldfaced), e.g. Shared Subspace: 29.1 BLEU vs Baseline: 28.6 BLEU. Combinations of multitask features with high frequency features also give significant improvements over the high frequency features alone. method. Recent work by (Chiang et al., 2009) describes new features for hierarchical phrase-based MT, while (Collins and Koo, 2005) describes features for parsing. Evaluation campaigns like WMT (Callison-Burch et al., 2009) and IWSLT (Paul, 2009) also contains a wealth of information for feature engineering in various MT tasks. 2. Designing better training algorithms: Nbest reranking can be seen as a subproblem of structured prediction, so many general structured prediction algorithms (c.f. (Bakir et al., 2007)) can be applied. In fact, some structured prediction algorithms, such as the MIRA algorithm used in dependency parsing (McDonald et al., 2005) and MT (Watanabe et al., 2007) uses iterative sets of N-best lists in its training process. Other training algorithms include perceptron-style algorithms (Liang et al., 2006), MaxEnt (Ch"
W10-1757,P08-1098,0,0.0306633,"me applications in NLP: For example, (Collobert and Weston, 2008) uses a deep neural network architecture for multitask learning on partof-speech tagging, chunking, semantic role labeling, etc. They showed that jointly learning these related tasks lead to overall improvements. (Deselaers et al., 2009) applies similar methods for machine transliteration. In information extraction, learning different relation types can be naturally cast as a multitask problem (Jiang, 2009; Carlson et al., 2009). Our work can be seen as following the same philosophy, but applied to N-best lists. In other areas, (Reichart et al., 2008) introduced an active learning strategy for annotating multitask linguistic data. (Blitzer et al., 2006) applies the multitask algorithm of (Ando and Zhang, 2005) to domain adaptation problems in NLP. We expect that more novel applications of multitask learning will appear in NLP as the techniques become scalable and standard. 6 Discussion and Conclusion N-best reranking is a beneficial framework for experimenting with large feature sets, but unfortunately feature sparsity leads to overfitting. We addressed this by re-casting N-best lists as multitask learning data. Our MT experiments show con"
W10-1757,N04-1023,0,0.65709,"nvolving millions of features. 1. We introduce the idea of viewing N-best reranking as a multitask learning problem. This view is particularly apt to any general reranking problem with sparse feature sets. 2. We propose a simple meta-algorithm that first discovers common feature representations across N-bests (via multitask learning) before training a conventional reranker. Thus it is easily applicable to existing systems. 1 Introduction Many natural language processing applications, such as machine translation (MT), parsing, and language modeling, benefit from the N-best reranking framework (Shen et al., 2004; Collins and Koo, 2005; Roark et al., 2007). The advantage of N-best reranking is that it abstracts away the complexities of first-pass decoding, allowing the researcher to try new features and learning algorithms with fast experimental turnover. In the N-best reranking scenario, the training data consists of sets of hypotheses (i.e. N-best lists) generated by a first-pass system, along with their labels. Given a new N-best list, the goal is to rerank it such that the best hypothesis appears near the top of the list. Existing research have focused on training a single reranker directly on the"
W10-1757,P09-1054,0,0.143227,"Missing"
W10-1757,D07-1080,1,0.936069,") is a D-dimensional feature vector, w is the weight vector to be trained, and N (f ) is the set of likely translations of f , i.e. the N-best list. The feature h(e, f ) can be any quantity defined in terms of the sentence pair, such as translation model and language model probabilities. Here we are interested in situations where the feature definitions can be quite sparse. A common methodology in reranking is to first design feature templates based on linguistic intuition and domain knowledge. Then, numerous features are instantiated based on the training data seen. For example, the work of (Watanabe et al., 2007) defines feature templates based on bilingual word alignments, which lead to extraction of heavilylexicalized features of the form: 2. The input (f ) has high variability (e.g. large vocabulary size), so that features for different inputs are rarely shared. 3. The N-best list output also exhibits high variability (e.g. many different word reorderings). Larger N may improve reranking performance, but may also increase feature sparsity. When the number of features is too large, even popular reranking algorithms such as SVM (Shen et al., 2004) and MIRA (Watanabe et al., 2007; Chiang et al., 2009)"
W10-1757,zhang-etal-2004-interpreting,0,0.0226462,"“de”) or special characters (such as numeral symbol and punctuation). These are features that can be expected to be widely applicable, and it is promising that multitask learning is able to recover these from the millions of potential features. 10 3. All three multitask methods obtained features that outperformed the baseline. The BLEU scores are 28.8, 28.9, 29.1 for Unsupervised Feature Selection, Joint Regularization, and Shared Subspace, respectively, which all outperform the 28.6 baseline. All improvements are statistically significant by bootstrap sampling test (1000 samples, p &lt; 0.05) (Zhang et al., 2004). 300 4. Shared Subspace performed the best. We conjecture this is because its feature projection can create new feature combinations that is more expressive than the feature selection used by the two other methods. Bootstrap samples 250 50 0 −0.2 Wabbit 1.2 5 Related Work in NLP Previous reranking work in NLP can be classified into two different research focuses: 1. Engineering better features: In MT, (Och and others, 2004) investigates features extracted from a wide variety of syntactic representations, such as parse tree probability on the outputs. Although their results show that the propo"
W10-1757,W09-0401,0,\N,Missing
W10-1757,P05-1022,0,\N,Missing
W10-1757,W09-0438,0,\N,Missing
W10-1762,J93-2003,0,0.011344,"ated studies on reordering. Section 3 describes the proposed method in detail. Section 4 presents and discusses our experimental results. Finally, we conclude this paper with our thoughts on future studies. 2 Related Work Reordering in SMT can be roughly classiﬁed into two approaches, namely a search in SMT decoding and preprocessing. The former approach is a straightforward way that models reordering in noisy channel translation, and has been studied from the early period of SMT research. Distance-based reordering is a typical approach used in many previous studies related to word-based SMT (Brown et al., 1993) and phrase-based SMT (Koehn et al., 2003). Along with the advances in phrase-based SMT, lexicalized reordering with a block orientation model was proposed (Tillmann, 2004; Koehn et al., 2005). This kind of reordering is suitable and commonly used in phrase-based SMT. On the other hand, a syntax-based SMT naturally includes reordering in its translation model. A lot of research work undertaken in this decade has used syntactic parsing for linguistically-motivated translation. (Yamada and Knight, 2001; Graehl and Knight, 2004; Galley et al., 2004; Liu et al., 2006). Wu (1997) and Chiang (2007)"
W10-1762,J07-2003,0,0.366382,"et al., 1993) and phrase-based SMT (Koehn et al., 2003). Along with the advances in phrase-based SMT, lexicalized reordering with a block orientation model was proposed (Tillmann, 2004; Koehn et al., 2005). This kind of reordering is suitable and commonly used in phrase-based SMT. On the other hand, a syntax-based SMT naturally includes reordering in its translation model. A lot of research work undertaken in this decade has used syntactic parsing for linguistically-motivated translation. (Yamada and Knight, 2001; Graehl and Knight, 2004; Galley et al., 2004; Liu et al., 2006). Wu (1997) and Chiang (2007) focus on formal structures that can be extracted from parallel corpora, instead of a syntactic parser trained using treebanks. These syntactic approaches can theoretically model reordering over an arbitrary length, however, long distance reordering still faces the difﬁculty of searching over an extremely large search space. The preprocessing approach employs deterministic reordering so that the following translation process requires only short distance reordering (or even a monotone). Several previous studies have proposed syntax-driven reordering based on source-side parse trees. Xia and Our"
W10-1762,P05-1066,0,0.134623,"Missing"
W10-1762,P98-1070,0,0.140627,"ur method can be seen as a variant of tree-to-string translation that focuses only on the clause structure in parse trees and independently translates the clauses. Although previous syntax-based methods can theoretically model this kind of derivation, it is practically difﬁcult to decode long multi-clause sentences as described above. Our approach is also related to sentence simpliﬁcation and is intended to obtain simple and short source sentences for better translation. Kim and Ehara (1994) proposed a rule-based method for splitting long Japanese sentences for Japaneseto-English translation; Furuse et al. (1998) used a syntactic structure to split ill-formed inputs in speech translation. Their splitting approach splits a sentence sequentially to obtain short segments, and does not undertake their reordering. Another related ﬁeld is clause identiﬁcation (Tjong et al., 2001). The proposed method is not limited to a speciﬁc clause identiﬁcation method and any method can be employed, if their clause deﬁnition matches the proposed method where clauses are independently translated. 3 Bilingual source Corpus (Training) target parse & clause segmentation Source Sentences (clause-segmented) word alignment Wor"
W10-1762,P02-1040,0,0.0837809,"est sentences are multi-clause sentences. Training Corpus Type Parallel (no-clause-seg.) Parallel (auto-aligned) (oracle-aligned) Dictionary Development Corpus Type Parallel (oracle-aligned) Test Corpus Type Parallel (clause-seg.) E J E J J E J #words 690,536 942,913 135,698 183,043 183,147 263,175 291,455 #words E 34,417 J 46,480 E J #words 34,433 45,975 decoders employed two language models: a word 5-gram language model from the Japanese sentences in the parallel corpus and a word 4-gram language model from the Japanese entries in the dictionary. The feature weights were optimized for BLEU (Papineni et al., 2002) by MERT, using the development sentences. 4.4 Results Table 3 shows the results in BLEU, Translation Edit Rate (TER) (Snover et al., 2006), and Position-independent Word-error Rate (PER) (Och et al., 2001), obtained with Moses and our hierarchical phrase-based SMT, respectively. Bold face results indicate the best scores obtained with the compared methods (excluding oracles). The proposed method consistently outperformed the baseline. The BLEU improvements with the proposed method over the baseline and comparison methods were statistically signiﬁcant according to the bootstrap sampling test ("
W10-1762,N04-1035,0,0.0349554,"many previous studies related to word-based SMT (Brown et al., 1993) and phrase-based SMT (Koehn et al., 2003). Along with the advances in phrase-based SMT, lexicalized reordering with a block orientation model was proposed (Tillmann, 2004; Koehn et al., 2005). This kind of reordering is suitable and commonly used in phrase-based SMT. On the other hand, a syntax-based SMT naturally includes reordering in its translation model. A lot of research work undertaken in this decade has used syntactic parsing for linguistically-motivated translation. (Yamada and Knight, 2001; Graehl and Knight, 2004; Galley et al., 2004; Liu et al., 2006). Wu (1997) and Chiang (2007) focus on formal structures that can be extracted from parallel corpora, instead of a syntactic parser trained using treebanks. These syntactic approaches can theoretically model reordering over an arbitrary length, however, long distance reordering still faces the difﬁculty of searching over an extremely large search space. The preprocessing approach employs deterministic reordering so that the following translation process requires only short distance reordering (or even a monotone). Several previous studies have proposed syntax-driven reorderi"
W10-1762,2006.amta-papers.25,0,0.0712603,"Missing"
W10-1762,N04-1014,0,0.0158515,"typical approach used in many previous studies related to word-based SMT (Brown et al., 1993) and phrase-based SMT (Koehn et al., 2003). Along with the advances in phrase-based SMT, lexicalized reordering with a block orientation model was proposed (Tillmann, 2004; Koehn et al., 2005). This kind of reordering is suitable and commonly used in phrase-based SMT. On the other hand, a syntax-based SMT naturally includes reordering in its translation model. A lot of research work undertaken in this decade has used syntactic parsing for linguistically-motivated translation. (Yamada and Knight, 2001; Graehl and Knight, 2004; Galley et al., 2004; Liu et al., 2006). Wu (1997) and Chiang (2007) focus on formal structures that can be extracted from parallel corpora, instead of a syntactic parser trained using treebanks. These syntactic approaches can theoretically model reordering over an arbitrary length, however, long distance reordering still faces the difﬁculty of searching over an extremely large search space. The preprocessing approach employs deterministic reordering so that the following translation process requires only short distance reordering (or even a monotone). Several previous studies have proposed s"
W10-1762,N04-4026,0,0.0472133,"ur thoughts on future studies. 2 Related Work Reordering in SMT can be roughly classiﬁed into two approaches, namely a search in SMT decoding and preprocessing. The former approach is a straightforward way that models reordering in noisy channel translation, and has been studied from the early period of SMT research. Distance-based reordering is a typical approach used in many previous studies related to word-based SMT (Brown et al., 1993) and phrase-based SMT (Koehn et al., 2003). Along with the advances in phrase-based SMT, lexicalized reordering with a block orientation model was proposed (Tillmann, 2004; Koehn et al., 2005). This kind of reordering is suitable and commonly used in phrase-based SMT. On the other hand, a syntax-based SMT naturally includes reordering in its translation model. A lot of research work undertaken in this decade has used syntactic parsing for linguistically-motivated translation. (Yamada and Knight, 2001; Graehl and Knight, 2004; Galley et al., 2004; Liu et al., 2006). Wu (1997) and Chiang (2007) focus on formal structures that can be extracted from parallel corpora, instead of a syntactic parser trained using treebanks. These syntactic approaches can theoretically"
W10-1762,W01-0708,0,0.0411713,"Missing"
W10-1762,D09-1105,0,0.0200396,"nguistically-motivated clause restructuring rules for German-to-English translation; Li et al. (2007) modeled reordering on parse tree nodes by using a maximum entropy model with surface and syntactic features for Chinese-to-English translation; Katz-Brown and Collins (2008) applied a very simple reverse ordering to Japanese-toEnglish translation, which reversed the word order in Japanese segments separated by a few simple cues; Xu et al. (2009) utilized a dependency parser with several hand-labeled precedence rules for reordering English to subject-object-verb order like Korean and Japanese. Tromble and Eisner (2009) proposed another reordering approach based on a linear ordering problem over source words without a linguistically syntactic structure. These preprocessing methods reorder source words close to the target-side order by employing languagedependent rules or statistical reordering models based on automatic word alignment. Although the use of language-dependent rules is a natural and promising way of bridging gaps between languages with large syntactic differences, the rules are usually unsuitable for other language groups. On the other hand, statistical methods can be applied to any language pai"
W10-1762,J97-3002,0,0.192686,"sed SMT (Brown et al., 1993) and phrase-based SMT (Koehn et al., 2003). Along with the advances in phrase-based SMT, lexicalized reordering with a block orientation model was proposed (Tillmann, 2004; Koehn et al., 2005). This kind of reordering is suitable and commonly used in phrase-based SMT. On the other hand, a syntax-based SMT naturally includes reordering in its translation model. A lot of research work undertaken in this decade has used syntactic parsing for linguistically-motivated translation. (Yamada and Knight, 2001; Graehl and Knight, 2004; Galley et al., 2004; Liu et al., 2006). Wu (1997) and Chiang (2007) focus on formal structures that can be extracted from parallel corpora, instead of a syntactic parser trained using treebanks. These syntactic approaches can theoretically model reordering over an arbitrary length, however, long distance reordering still faces the difﬁculty of searching over an extremely large search space. The preprocessing approach employs deterministic reordering so that the following translation process requires only short distance reordering (or even a monotone). Several previous studies have proposed syntax-driven reordering based on source-side parse"
W10-1762,N03-1017,0,0.0275575,"ribes the proposed method in detail. Section 4 presents and discusses our experimental results. Finally, we conclude this paper with our thoughts on future studies. 2 Related Work Reordering in SMT can be roughly classiﬁed into two approaches, namely a search in SMT decoding and preprocessing. The former approach is a straightforward way that models reordering in noisy channel translation, and has been studied from the early period of SMT research. Distance-based reordering is a typical approach used in many previous studies related to word-based SMT (Brown et al., 1993) and phrase-based SMT (Koehn et al., 2003). Along with the advances in phrase-based SMT, lexicalized reordering with a block orientation model was proposed (Tillmann, 2004; Koehn et al., 2005). This kind of reordering is suitable and commonly used in phrase-based SMT. On the other hand, a syntax-based SMT naturally includes reordering in its translation model. A lot of research work undertaken in this decade has used syntactic parsing for linguistically-motivated translation. (Yamada and Knight, 2001; Graehl and Knight, 2004; Galley et al., 2004; Liu et al., 2006). Wu (1997) and Chiang (2007) focus on formal structures that can be ext"
W10-1762,C04-1073,0,0.0479303,"Missing"
W10-1762,2005.iwslt-1.8,0,0.0209809,"uture studies. 2 Related Work Reordering in SMT can be roughly classiﬁed into two approaches, namely a search in SMT decoding and preprocessing. The former approach is a straightforward way that models reordering in noisy channel translation, and has been studied from the early period of SMT research. Distance-based reordering is a typical approach used in many previous studies related to word-based SMT (Brown et al., 1993) and phrase-based SMT (Koehn et al., 2003). Along with the advances in phrase-based SMT, lexicalized reordering with a block orientation model was proposed (Tillmann, 2004; Koehn et al., 2005). This kind of reordering is suitable and commonly used in phrase-based SMT. On the other hand, a syntax-based SMT naturally includes reordering in its translation model. A lot of research work undertaken in this decade has used syntactic parsing for linguistically-motivated translation. (Yamada and Knight, 2001; Graehl and Knight, 2004; Galley et al., 2004; Liu et al., 2006). Wu (1997) and Chiang (2007) focus on formal structures that can be extracted from parallel corpora, instead of a syntactic parser trained using treebanks. These syntactic approaches can theoretically model reordering ove"
W10-1762,N09-1028,0,0.052652,"Computational Linguistics McCord (2004) extracted reordering rules automatically from bilingual corpora for English-toFrench translation; Collins et al. (2005) used linguistically-motivated clause restructuring rules for German-to-English translation; Li et al. (2007) modeled reordering on parse tree nodes by using a maximum entropy model with surface and syntactic features for Chinese-to-English translation; Katz-Brown and Collins (2008) applied a very simple reverse ordering to Japanese-toEnglish translation, which reversed the word order in Japanese segments separated by a few simple cues; Xu et al. (2009) utilized a dependency parser with several hand-labeled precedence rules for reordering English to subject-object-verb order like Korean and Japanese. Tromble and Eisner (2009) proposed another reordering approach based on a linear ordering problem over source words without a linguistically syntactic structure. These preprocessing methods reorder source words close to the target-side order by employing languagedependent rules or statistical reordering models based on automatic word alignment. Although the use of language-dependent rules is a natural and promising way of bridging gaps between l"
W10-1762,P01-1067,0,0.0503465,"ce-based reordering is a typical approach used in many previous studies related to word-based SMT (Brown et al., 1993) and phrase-based SMT (Koehn et al., 2003). Along with the advances in phrase-based SMT, lexicalized reordering with a block orientation model was proposed (Tillmann, 2004; Koehn et al., 2005). This kind of reordering is suitable and commonly used in phrase-based SMT. On the other hand, a syntax-based SMT naturally includes reordering in its translation model. A lot of research work undertaken in this decade has used syntactic parsing for linguistically-motivated translation. (Yamada and Knight, 2001; Graehl and Knight, 2004; Galley et al., 2004; Liu et al., 2006). Wu (1997) and Chiang (2007) focus on formal structures that can be extracted from parallel corpora, instead of a syntactic parser trained using treebanks. These syntactic approaches can theoretically model reordering over an arbitrary length, however, long distance reordering still faces the difﬁculty of searching over an extremely large search space. The preprocessing approach employs deterministic reordering so that the following translation process requires only short distance reordering (or even a monotone). Several previou"
W10-1762,P07-2045,0,0.00356861,"inal symbol s0 with the second clause and obtain the Japanese sentence: watashi wa tom ga kino susume ta zasshi o kat ta . 4 Experiment We conducted the following experiments on the English-to-Japanese translation of research paper abstracts in the medical domain. Such technical documents are logically and formally written, and sentences are often so long and syntactically complex that their translation needs long distance reordering. We believe that the medical domain is suitable as regards evaluating the proposed method. 4.2 Model and Decoder We used two decoders in the experiments, Moses9 (Koehn et al., 2007) and our inhouse hierarchical phrase-based SMT (almost equivalent to Hiero (Chiang, 2007)). Moses used a phrase table with a maximum phrase length of 7, a lexicalized reordering model with msd-bidirectional-fe, and a distortion limit of 1210 . Our hierarchical phrase-based SMT used a phrase table with a maximum rule length of 7 and a window size (Hiero’s Λ) of 12 11 . Both 4.1 Resources Our bilingual resources were taken from the medical domain. The parallel corpus consisted of research paper abstracts in English taken from PubMed4 and the corresponding Japanese translations. The training port"
W10-1762,zhang-etal-2004-interpreting,0,0.0423294,"Missing"
W10-1762,P07-1091,0,0.327459,"ally follows the Penn Treebank II scheme but also includes SINV, SQ, SBAR. See http://www-tsujii.is.s.u-tokyo.ac.jp/enju/enju-manual/enjuoutput-spec.html#correspondence for details. 418 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 418–427, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics McCord (2004) extracted reordering rules automatically from bilingual corpora for English-toFrench translation; Collins et al. (2005) used linguistically-motivated clause restructuring rules for German-to-English translation; Li et al. (2007) modeled reordering on parse tree nodes by using a maximum entropy model with surface and syntactic features for Chinese-to-English translation; Katz-Brown and Collins (2008) applied a very simple reverse ordering to Japanese-toEnglish translation, which reversed the word order in Japanese segments separated by a few simple cues; Xu et al. (2009) utilized a dependency parser with several hand-labeled precedence rules for reordering English to subject-object-verb order like Korean and Japanese. Tromble and Eisner (2009) proposed another reordering approach based on a linear ordering problem ove"
W10-1762,P06-1077,0,0.0402884,"related to word-based SMT (Brown et al., 1993) and phrase-based SMT (Koehn et al., 2003). Along with the advances in phrase-based SMT, lexicalized reordering with a block orientation model was proposed (Tillmann, 2004; Koehn et al., 2005). This kind of reordering is suitable and commonly used in phrase-based SMT. On the other hand, a syntax-based SMT naturally includes reordering in its translation model. A lot of research work undertaken in this decade has used syntactic parsing for linguistically-motivated translation. (Yamada and Knight, 2001; Graehl and Knight, 2004; Galley et al., 2004; Liu et al., 2006). Wu (1997) and Chiang (2007) focus on formal structures that can be extracted from parallel corpora, instead of a syntactic parser trained using treebanks. These syntactic approaches can theoretically model reordering over an arbitrary length, however, long distance reordering still faces the difﬁculty of searching over an extremely large search space. The preprocessing approach employs deterministic reordering so that the following translation process requires only short distance reordering (or even a monotone). Several previous studies have proposed syntax-driven reordering based on source-"
W10-1762,P06-1004,0,0.0185365,"in our corpora. 421 John lost the book that was borrowed ... clause(1) clause(2) p(that |kara) + p(was |kara ) + ... p(John |john ) + p(lost |john ) + ... john John k|fm ) between each Japanese word fm and English clause k. Theoretically, we can simply output the clause id k ′ for each fm by ﬁnding k ′ = arg maxk t(lm = k|fm ). In practice, this may sometimes lead to Japanese clauses that have too many gaps, so we employ a two-stage procedure to extract clauses that are more contiguous. First, we segment the Japanese sentence into K clauses based on a dynamic programming algorithm proposed by Malioutov and Barzilay (2006). We deﬁne an M × M similarity matrix S = [sij ] with sij = exp(−||li −lj ||) where li is (K + i)-th row vector in the label matrix L. sij represents the similarity between the i-th and j-th Japanese words with respect to their clause alignment score distributions; if the score distributions are similar then sij is large. The details of this algorithm can be found in (Malioutov and Barzilay, 2006). The clause segmentation gives us contiguous Japanese clauses f˜1 , f˜2 , ..., f˜K , thus minimizing inter-segment similarity and maximizing intra-segment similarity. Second, we determine the clause"
W10-1762,J08-1002,0,0.0605287,"Missing"
W10-1762,W01-1408,0,0.038533,"Missing"
W10-1762,C98-1067,0,\N,Missing
W10-1762,J08-3004,0,\N,Missing
W10-3302,bond-etal-2008-boot,0,0.0416792,"Missing"
W10-3302,C92-2082,0,0.0451113,"n of the concept referred to by the title. We applied language dependent lexicosyntactic patterns to the definition sentence to extract the hypernym. The hypernym of the category name is extracted from the definition sentence if it exists. If there is an article whose title is the same as its category, the hypernym of the article is used as that of the category. As for lexico-syntactic patterns, we used almost the same patterns described in previous work related to Japanese such as (Kobayashi et al., 2008; Sumida et al., 2008), which is basically equivalent to work related to English such as (Hearst, 1992). Here are some examples. Category Alignment For each leaf category in Goi-Taikei, we first make a list of junction category candidates. Wikipedia categories satisfying at least one of the following three conditions are extracted as candidates: • The Goi-Taikei category name exactly matches the Wikipedia category name. • One of the instances of the Goi-Taikei category exactly matches the Wikipedia category name. • More than two instances of the Goi-Taikei category exactly match either instances or subcategories of the Wikipedia category. Here, an instance of a Goi-Taikei category refers to wor"
W10-3302,sumida-etal-2008-boosting,0,0.0194643,"category in advance. We regard the first sentence of each article page as the definition of the concept referred to by the title. We applied language dependent lexicosyntactic patterns to the definition sentence to extract the hypernym. The hypernym of the category name is extracted from the definition sentence if it exists. If there is an article whose title is the same as its category, the hypernym of the article is used as that of the category. As for lexico-syntactic patterns, we used almost the same patterns described in previous work related to Japanese such as (Kobayashi et al., 2008; Sumida et al., 2008), which is basically equivalent to work related to English such as (Hearst, 1992). Here are some examples. Category Alignment For each leaf category in Goi-Taikei, we first make a list of junction category candidates. Wikipedia categories satisfying at least one of the following three conditions are extracted as candidates: • The Goi-Taikei category name exactly matches the Wikipedia category name. • One of the instances of the Goi-Taikei category exactly matches the Wikipedia category name. • More than two instances of the Goi-Taikei category exactly match either instances or subcategories of"
W10-3501,sumida-etal-2008-boosting,0,0.0197526,"onductors Structural relation A. The target Wikipedia category label. is-a Category without parent and child Conductors is-a As most category labels and D-hypernyms are common nouns, they are likely to match instances in Goi-Taikei which lists possible semantic categories of words. is-a Composers Announcers Japanese conductors Figure 4: Example of Wikipedia category hierarchy (top) and constructed Wikipedia person category hierarchy (bottom) 5 As for D-hypernym extraction patterns, we used almost the same patterns described in previous works on Japanese sources such as (Kobayashi et al. 2008; Sumida et al., 2008), which are basically equivalent to the works on English sources such as (Hearst, 1992). 4 After the texts located at various structural relations A-F are collected, they are matched to the instances of Goi-Taikei in two different spans: features related to B-Ⅱ, and the relative frequencies used for the feature value related B-Ⅱ are 0, 0.5, 0.5, 0, respectively. In this way, we use 48 relative frequencies calculated from the combinations of structural relation A-F, span Ⅰ and Ⅱ, and semantic type a-d, as the feature vector for the SVM. Span of the text Ⅰ. All character strings of the text Ⅱ. T"
W10-3501,bond-etal-2008-boot,0,0.0313082,"Missing"
W10-3501,C92-2082,0,0.020447,"rent and child Conductors is-a As most category labels and D-hypernyms are common nouns, they are likely to match instances in Goi-Taikei which lists possible semantic categories of words. is-a Composers Announcers Japanese conductors Figure 4: Example of Wikipedia category hierarchy (top) and constructed Wikipedia person category hierarchy (bottom) 5 As for D-hypernym extraction patterns, we used almost the same patterns described in previous works on Japanese sources such as (Kobayashi et al. 2008; Sumida et al., 2008), which are basically equivalent to the works on English sources such as (Hearst, 1992). 4 After the texts located at various structural relations A-F are collected, they are matched to the instances of Goi-Taikei in two different spans: features related to B-Ⅱ, and the relative frequencies used for the feature value related B-Ⅱ are 0, 0.5, 0.5, 0, respectively. In this way, we use 48 relative frequencies calculated from the combinations of structural relation A-F, span Ⅰ and Ⅱ, and semantic type a-d, as the feature vector for the SVM. Span of the text Ⅰ. All character strings of the text Ⅱ. The last word of the text 芸術 Art For the span Ⅱ, the text is segmented into words using"
W11-3506,P00-1031,0,0.04063,"editor by converting erroneous text written in Roman characters into correct text written in kana while leaving foreign words unchanged. Our method consists of three steps: iden2 Related Work Our interest is mainly focused on how to deal with erroneous inputs. Error detection and correction on sentences written in kana with kana character N-gram was proposed in (Shinnou, 1999). Our approach is similar to this, but our target is sentences in Roman characters and has the additional difficulty of language identification. Errortolerant Chinese input methods were introduced in (Zheng et al., 2011; Chen and Lee, 2000). Though Roman-to-kana conversion is similar to pinyin-toChinese conversion, our target differs from them because our motivation is to help Japanese language teachers. Japanese commercial IMs such as Microsoft Office IME1 , ATOK2 , and Google IME3 have a module of spelling correction, but their target is native Japanese speakers. (Ehara and Tanaka-Ishii, 2008) presented a high accuracy language detection system for text input. We perform 1 http://www.microsoft.com/japan/ office/2010/ime/default.mspx 2 http://www.atok.com/ 3 http://www.google.com/intl/ja/ime/ 38 Proceedings of the Workshop on A"
W11-3506,I08-1058,0,0.0138414,"proposed in (Shinnou, 1999). Our approach is similar to this, but our target is sentences in Roman characters and has the additional difficulty of language identification. Errortolerant Chinese input methods were introduced in (Zheng et al., 2011; Chen and Lee, 2000). Though Roman-to-kana conversion is similar to pinyin-toChinese conversion, our target differs from them because our motivation is to help Japanese language teachers. Japanese commercial IMs such as Microsoft Office IME1 , ATOK2 , and Google IME3 have a module of spelling correction, but their target is native Japanese speakers. (Ehara and Tanaka-Ishii, 2008) presented a high accuracy language detection system for text input. We perform 1 http://www.microsoft.com/japan/ office/2010/ime/default.mspx 2 http://www.atok.com/ 3 http://www.google.com/intl/ja/ime/ 38 Proceedings of the Workshop on Advances in Text Input Methods (WTIM 2011), pages 38–42, Chiang Mai, Thailand, November 13, 2011. 5.1 Language Identification error correction in addition to language identification. Correcting Japanese learners’ error is also proposed in (Mizumoto et al., 2011). They try to correct sentences written in kana and kanji mixed, whereas we aim at texts in Roman cha"
W11-3506,I11-1017,1,0.785266,"gle IME3 have a module of spelling correction, but their target is native Japanese speakers. (Ehara and Tanaka-Ishii, 2008) presented a high accuracy language detection system for text input. We perform 1 http://www.microsoft.com/japan/ office/2010/ime/default.mspx 2 http://www.atok.com/ 3 http://www.google.com/intl/ja/ime/ 38 Proceedings of the Workshop on Advances in Text Input Methods (WTIM 2011), pages 38–42, Chiang Mai, Thailand, November 13, 2011. 5.1 Language Identification error correction in addition to language identification. Correcting Japanese learners’ error is also proposed in (Mizumoto et al., 2011). They try to correct sentences written in kana and kanji mixed, whereas we aim at texts in Roman characters. 3 Language identification is done by exact matching input sequences in English with a romanized5 Japanese dictionary. Learners sometimes directly write words in their native language without adapting to Japanese romaji style. Since we are not focusing on implementing full transliteration (Knight and Graehl, 1998), we would like to convert only Japanese words into kana. To achieve this, we use an English word dictionary because most foreign words found in learners’ sentences are English"
W11-3506,C10-1096,0,0.0204255,"mber of candidates using approximate word matching with cosine distance before calculating edit distance (Kukich, 1992). Cosine distance is calculated using character n-gram features. We set n = 1 because it covers most candidates in dictionary and reduces the number of candidates appropriately. For example, when we retrieved the approximate words for packu in our dictionary with cosine distance, the number of candidates is reduced to 163, and examples of retrieved words are kau, pakku, chikau, pachikuri, etc. Approximate word matching with cosine similarity can be performed very efficiently (Okazaki and Tsujii, 2010)9 to get candidates from a large scale word dictionary. kana よろしく おねがい します。 Muscle musical を みたい。 あなた は えいご が わかります か。 Table 2: Examples of successfully corrected word misspelled shuutmatsu do-yoobi packu kana しゅう t まつ どよおび ぱcく correct shuumatsu doyoubi pakku kana しゅうまつ どようび ぱっく some pairs have several possibilities. One of them is a pair of n and following characters. For example, we can read Japanese word kinyuu as “きん ゆう/kin-yuu: finance” and “きにゅう/kinyuu: entry.” The reason why it occurs is that n can be a syllable alone. Solving this kind of ambiguity is out of scope of this paper; and we"
W11-3506,H01-1044,0,0.0967435,"Missing"
W11-3506,J98-4003,0,\N,Missing
W12-4207,W08-0336,0,0.0779511,"Missing"
W12-4207,P05-1066,0,0.323609,"Missing"
W12-4207,C10-1043,0,0.0945607,"oached the reordering problem in multiple ways. The most basic idea is preordering (Xia and McCord, 2004; Collins et al., 2005), that is, to do reordering during preprocessing time, where the source side of the training and development data and sentences from a source language that have to be translated are first reordered to ease the training and the translation, respectively. In (Xu et al., 2009), authors used a dependency parser to introduce manually created preordering rules to reorder English sentences when translating into five different SOV(Subject-ObjectVerb) languages. Other authors (Genzel, 2010; Wu et al., 2011) use automatically generated rules induced from parallel data. Tillmann (2004) used a lexical reordering model, and Galley et al. (2004) followed a syntactic-based model. In this work, however, we are centered in the design of manual rules inspired by the Head Finalization (HF) reordering (Isozaki et al., 2010b). HF reordering is one of the simplest methods for preordering that significantly improves word alignments and leads to a better translation quality. Al59 though the method is limited to translation where the target language is head-final, it requires neither training"
W12-4207,D10-1092,1,0.926453,"Missing"
W12-4207,W10-1736,1,0.0700138,"me Tsukada‡ Masaaki Nagata‡ + The Graduate University For Advanced Studies, Tokyo, Japan ‡ NTT Communication Science Laboratories, NTT Corporation + handan@nii.ac.jp, ∗ wuxianchao@baidu.com, † kevinduh@is.naist.jp ‡ {sudoh.katsuhito, tsukada.hajime, nagata.masaaki}@lab.ntt.co.jp Abstract In Statistical Machine Translation, reordering rules have proved useful in extracting bilingual phrases and in decoding during translation between languages that are structurally different. Linguistically motivated rules have been incorporated into Chineseto-English (Wang et al., 2007) and Englishto-Japanese (Isozaki et al., 2010b) translation with significant gains to the statistical translation system. Here, we carry out a linguistic analysis of the Chinese-to-Japanese translation problem and propose one of the first reordering rules for this language pair. Experimental results show substantially improvements (from 20.70 to 23.17 BLEU) when head-finalization rules based on HPSG parses are used, and further gains (to 24.14 BLEU) were obtained using more refined rules. 1 Introduction In state-of-the-art Statistical Machine Translation (SMT) systems, bilingual phrases are the main building blocks for constructing a tra"
W12-4207,N03-1017,0,0.0605813,"ranslation (SMT) systems, bilingual phrases are the main building blocks for constructing a translation given a sentence from a source language. To extract those bilingual phrases from a parallel corpus, the first step is to discover the implicit wordto-word correspondences between bilingual sentences (Brown et al., 1993). Then, a symmetrization matrix is built (Och and Ney, 2004) by using word-to-word alignments, and a wide variety ∗ Now at Baidu Japan Inc. Now at Nara Institute of Science and Technology (NAIST) † of heuristics can be used to extract the bilingual phrases (Zens et al., 2002; Koehn et al., 2003). This method performs relatively well when the source and the target languages have similar word order, as in the case of French, Spanish, and English. However, when translating between languages with very different structures, as in the case of English and Japanese, or Japanese and Chinese, the quality of extracted bilingual phrases and the overall translation quality diminishes. In the latter scenario, a simple but effective strategy to cope with this problem is to reorder the words of sentences in one language so that it resembles the word order of another language (Wu et al., 2011; Isozak"
W12-4207,P07-2045,0,0.0344953,"Missing"
W12-4207,J08-1002,0,0.110732,"h languages with different phrase structures like English and Japanese. Head Finalization is a successful syntax-based reordering method designed to reorder sentences from a head-initial language to resemble the word order in sentences from a headfinal language (Isozaki et al., 2010b). The essence 58 of this rule is to move the syntactic heads to the end of its dependency by swapping child nodes in a phrase structure tree when the head child appears before the dependent child. Isozaki et al. (2010b) proposed a simple method of Head Finalization, by using an HPSG-based deep parser for English (Miyao and Tsujii, 2008) to obtain phrase structures and head information. The score results from several mainstream evaluation methods indicated that the translation quality had been improved; the scores of Word Error Rate (WER) and Translation Edit Rate (TER) (Snover et al., 2006) had especially been greatly reduced. 2.2 Chinese Deep Parsing Syntax-based reordering methods need parsed sentences as input. Isozaki et al. (2010b) used Enju, an HPSG-based deep parser for English, but they also discussed using other types of parsers, such as word dependency parsers and Penn Treebankstyle parsers. However, to use word de"
W12-4207,J03-1002,0,0.0108902,"reebank. Chinese Enju requires segmented and POS-tagged sentences to do parsing. We used the Stanford Chinese segmenter (Chang et al., 2008) and Stanford POStagger (Toutanova et al., 2003) to obtain the segmentation and POS-tagging of the Chinese side of the training, development, and test sets. The baseline system was trained following the instructions of recent SMT evaluation campaigns (Callison-Burch et al., 2010) by using the MT toolkit Moses (Koehn et al., 2007) in its default configuration. Phrase pairs were extracted from symmetrized word alignments and distortions generated by GIZA++ (Och and Ney, 2003) using the combination of heuristics “grow-diagfinal-and” and “msd-bidirectional-fe”. The language model was a 5-gram language model estimated on the target side of the parallel corpora by using the modified Kneser-Ney smoothing (Chen and Goodman, 1999) implemented in the SRILM (Stolcke, 2002) toolkit. The weights of the log-linear combination of feature functions were estimated by using MERT (Och, 2003) on the development set described in Table 6. The effectiveness of the reorderings proposed in Section 3.3 was assessed by using two precision metrics and two error metrics on translation quali"
W12-4207,J04-4002,0,0.081366,"ally improvements (from 20.70 to 23.17 BLEU) when head-finalization rules based on HPSG parses are used, and further gains (to 24.14 BLEU) were obtained using more refined rules. 1 Introduction In state-of-the-art Statistical Machine Translation (SMT) systems, bilingual phrases are the main building blocks for constructing a translation given a sentence from a source language. To extract those bilingual phrases from a parallel corpus, the first step is to discover the implicit wordto-word correspondences between bilingual sentences (Brown et al., 1993). Then, a symmetrization matrix is built (Och and Ney, 2004) by using word-to-word alignments, and a wide variety ∗ Now at Baidu Japan Inc. Now at Nara Institute of Science and Technology (NAIST) † of heuristics can be used to extract the bilingual phrases (Zens et al., 2002; Koehn et al., 2003). This method performs relatively well when the source and the target languages have similar word order, as in the case of French, Spanish, and English. However, when translating between languages with very different structures, as in the case of English and Japanese, or Japanese and Chinese, the quality of extracted bilingual phrases and the overall translation"
W12-4207,P03-1021,0,0.0679623,"2010) by using the MT toolkit Moses (Koehn et al., 2007) in its default configuration. Phrase pairs were extracted from symmetrized word alignments and distortions generated by GIZA++ (Och and Ney, 2003) using the combination of heuristics “grow-diagfinal-and” and “msd-bidirectional-fe”. The language model was a 5-gram language model estimated on the target side of the parallel corpora by using the modified Kneser-Ney smoothing (Chen and Goodman, 1999) implemented in the SRILM (Stolcke, 2002) toolkit. The weights of the log-linear combination of feature functions were estimated by using MERT (Och, 2003) on the development set described in Table 6. The effectiveness of the reorderings proposed in Section 3.3 was assessed by using two precision metrics and two error metrics on translation quality. The first evaluation metric is BLEU (Papineni et al., 2002), a very common accuracy metric in SMT that measures N -gram precision, with a penalty for too short sentences. The second evaluation metric was RIBES (Isozaki et al., 2010a), a recent precision metric used to evaluate translation quality between structurally different languages. It uses notions on rank correlation coefficients and precision"
W12-4207,P02-1040,0,0.0870379,"w-diagfinal-and” and “msd-bidirectional-fe”. The language model was a 5-gram language model estimated on the target side of the parallel corpora by using the modified Kneser-Ney smoothing (Chen and Goodman, 1999) implemented in the SRILM (Stolcke, 2002) toolkit. The weights of the log-linear combination of feature functions were estimated by using MERT (Och, 2003) on the development set described in Table 6. The effectiveness of the reorderings proposed in Section 3.3 was assessed by using two precision metrics and two error metrics on translation quality. The first evaluation metric is BLEU (Papineni et al., 2002), a very common accuracy metric in SMT that measures N -gram precision, with a penalty for too short sentences. The second evaluation metric was RIBES (Isozaki et al., 2010a), a recent precision metric used to evaluate translation quality between structurally different languages. It uses notions on rank correlation coefficients and precision measures. The third evaluation metric is TER (Snover et al., 2006), another error metric that computes the minimum number of edits required to convert translated sentences into its corresponding references. Possible edits include insertion, deletion, subst"
W12-4207,2006.amta-papers.25,0,0.130258,"(Isozaki et al., 2010b). The essence 58 of this rule is to move the syntactic heads to the end of its dependency by swapping child nodes in a phrase structure tree when the head child appears before the dependent child. Isozaki et al. (2010b) proposed a simple method of Head Finalization, by using an HPSG-based deep parser for English (Miyao and Tsujii, 2008) to obtain phrase structures and head information. The score results from several mainstream evaluation methods indicated that the translation quality had been improved; the scores of Word Error Rate (WER) and Translation Edit Rate (TER) (Snover et al., 2006) had especially been greatly reduced. 2.2 Chinese Deep Parsing Syntax-based reordering methods need parsed sentences as input. Isozaki et al. (2010b) used Enju, an HPSG-based deep parser for English, but they also discussed using other types of parsers, such as word dependency parsers and Penn Treebankstyle parsers. However, to use word dependency parsers, they needed an additional heuristic rule to recover phrase structures, and Penn Treebank-style parsers are problematic because they output flat phrase structures (i.e. a phrase may have multiple dependents, which causes a problem of reorderi"
W12-4207,N04-4026,0,0.0537842,"cCord, 2004; Collins et al., 2005), that is, to do reordering during preprocessing time, where the source side of the training and development data and sentences from a source language that have to be translated are first reordered to ease the training and the translation, respectively. In (Xu et al., 2009), authors used a dependency parser to introduce manually created preordering rules to reorder English sentences when translating into five different SOV(Subject-ObjectVerb) languages. Other authors (Genzel, 2010; Wu et al., 2011) use automatically generated rules induced from parallel data. Tillmann (2004) used a lexical reordering model, and Galley et al. (2004) followed a syntactic-based model. In this work, however, we are centered in the design of manual rules inspired by the Head Finalization (HF) reordering (Isozaki et al., 2010b). HF reordering is one of the simplest methods for preordering that significantly improves word alignments and leads to a better translation quality. Al59 though the method is limited to translation where the target language is head-final, it requires neither training data nor fine-tuning. To our knowledge, HF is the best method to reorder languages when translat"
W12-4207,N03-1033,0,0.00850002,"and extended CWMT Chinese-Japanese corpus. Dev. stands for Development, OoV for “Out of Vocabulary” words, K for thousands of elements, and M for millions of elements. Data statistics were collected after tokenizing. methods. Detailed Corpus statistics can be found in Table 6. To parse Chinese sentences, we used Chinese Enju (Yu et al., 2010), an HPSG-based parser trained with the Chinese HPSG treebank converted from Penn Chinese Treebank. Chinese Enju requires segmented and POS-tagged sentences to do parsing. We used the Stanford Chinese segmenter (Chang et al., 2008) and Stanford POStagger (Toutanova et al., 2003) to obtain the segmentation and POS-tagging of the Chinese side of the training, development, and test sets. The baseline system was trained following the instructions of recent SMT evaluation campaigns (Callison-Burch et al., 2010) by using the MT toolkit Moses (Koehn et al., 2007) in its default configuration. Phrase pairs were extracted from symmetrized word alignments and distortions generated by GIZA++ (Och and Ney, 2003) using the combination of heuristics “grow-diagfinal-and” and “msd-bidirectional-fe”. The language model was a 5-gram language model estimated on the target side of the p"
W12-4207,D07-1077,0,0.101287,"uhito Sudoh‡ Xianchao Wu‡∗ Kevin Duh‡† Hajime Tsukada‡ Masaaki Nagata‡ + The Graduate University For Advanced Studies, Tokyo, Japan ‡ NTT Communication Science Laboratories, NTT Corporation + handan@nii.ac.jp, ∗ wuxianchao@baidu.com, † kevinduh@is.naist.jp ‡ {sudoh.katsuhito, tsukada.hajime, nagata.masaaki}@lab.ntt.co.jp Abstract In Statistical Machine Translation, reordering rules have proved useful in extracting bilingual phrases and in decoding during translation between languages that are structurally different. Linguistically motivated rules have been incorporated into Chineseto-English (Wang et al., 2007) and Englishto-Japanese (Isozaki et al., 2010b) translation with significant gains to the statistical translation system. Here, we carry out a linguistic analysis of the Chinese-to-Japanese translation problem and propose one of the first reordering rules for this language pair. Experimental results show substantially improvements (from 20.70 to 23.17 BLEU) when head-finalization rules based on HPSG parses are used, and further gains (to 24.14 BLEU) were obtained using more refined rules. 1 Introduction In state-of-the-art Statistical Machine Translation (SMT) systems, bilingual phrases are th"
W12-4207,I11-1004,1,0.873372,"02; Koehn et al., 2003). This method performs relatively well when the source and the target languages have similar word order, as in the case of French, Spanish, and English. However, when translating between languages with very different structures, as in the case of English and Japanese, or Japanese and Chinese, the quality of extracted bilingual phrases and the overall translation quality diminishes. In the latter scenario, a simple but effective strategy to cope with this problem is to reorder the words of sentences in one language so that it resembles the word order of another language (Wu et al., 2011; Isozaki et al., 2010b). The advantages of this strategy are two fold. The first advantage is at the decoding stage, since it enables the translation to be constructed almost monotonically. The second advantage is at the training stage, since automatically estimated word-to-word alignments are likely to be more accurate and symmetrization matrices reveal more evident bilingual phrases, leading to the extraction of better quality bilingual phrases and cleaner phrase tables. In this work, we focus on Chinese-to-Japanese translation, motivated by the increasing interaction between these two coun"
W12-4207,C04-1073,0,0.304241,"ute “head” indicates which child node is the syntactic head. In this figure, &lt;head=“c4” id=“c3”> means that the node that has id=“c4” is the syntactic head of the node that has id=“c3”. Figure 1: An XML output for a Chinese sentence from Chinese Enju. For clarity, we only draw information related to the phrase structure and the heads. 2.3 Related Work Reordering is a popular strategy for improving machine translation quality when source and target languages are structurally very different. Researchers have approached the reordering problem in multiple ways. The most basic idea is preordering (Xia and McCord, 2004; Collins et al., 2005), that is, to do reordering during preprocessing time, where the source side of the training and development data and sentences from a source language that have to be translated are first reordered to ease the training and the translation, respectively. In (Xu et al., 2009), authors used a dependency parser to introduce manually created preordering rules to reorder English sentences when translating into five different SOV(Subject-ObjectVerb) languages. Other authors (Genzel, 2010; Wu et al., 2011) use automatically generated rules induced from parallel data. Tillmann (2"
W12-4207,N09-1028,0,0.298949,"the phrase structure and the heads. 2.3 Related Work Reordering is a popular strategy for improving machine translation quality when source and target languages are structurally very different. Researchers have approached the reordering problem in multiple ways. The most basic idea is preordering (Xia and McCord, 2004; Collins et al., 2005), that is, to do reordering during preprocessing time, where the source side of the training and development data and sentences from a source language that have to be translated are first reordered to ease the training and the translation, respectively. In (Xu et al., 2009), authors used a dependency parser to introduce manually created preordering rules to reorder English sentences when translating into five different SOV(Subject-ObjectVerb) languages. Other authors (Genzel, 2010; Wu et al., 2011) use automatically generated rules induced from parallel data. Tillmann (2004) used a lexical reordering model, and Galley et al. (2004) followed a syntactic-based model. In this work, however, we are centered in the design of manual rules inspired by the Head Finalization (HF) reordering (Isozaki et al., 2010b). HF reordering is one of the simplest methods for preorde"
W12-4207,C10-2162,0,0.0590977,"Missing"
W12-4207,W11-2907,0,0.172115,"word dependency parsers and Penn Treebankstyle parsers. However, to use word dependency parsers, they needed an additional heuristic rule to recover phrase structures, and Penn Treebank-style parsers are problematic because they output flat phrase structures (i.e. a phrase may have multiple dependents, which causes a problem of reordering within a phrase). Consequently, compared to different types of parsers, Head-Final English performs the best on the basis of English Enju’s parsing result. In this paper, we follow their observation, and use the HPSG-based parser for Chinese (Chinese Enju) (Yu et al., 2011) for Chinese syntactic parsing. Since Chinese Enju is based on the same parsing model as English Enju, it provides rich syntactic information including phrase structures and syntactic/semantic heads. Figure 1 shows an example of an XML output from Chinese Enju for the sentence “wo (I) qu (go to) dongjing (Tokyo) he (and) jingdu (Kyoto).” The label &lt;cons> and &lt;tok> represent the non-terminal nodes and terminal nodes, respectively. Each node is identified by a unique “id” and has several attributes. The attribute “head” indicates which child node is the syntactic head. In this figure, &lt;head=“c4”"
W12-4207,2002.tmi-tutorials.2,0,0.0338415,"atistical Machine Translation (SMT) systems, bilingual phrases are the main building blocks for constructing a translation given a sentence from a source language. To extract those bilingual phrases from a parallel corpus, the first step is to discover the implicit wordto-word correspondences between bilingual sentences (Brown et al., 1993). Then, a symmetrization matrix is built (Och and Ney, 2004) by using word-to-word alignments, and a wide variety ∗ Now at Baidu Japan Inc. Now at Nara Institute of Science and Technology (NAIST) † of heuristics can be used to extract the bilingual phrases (Zens et al., 2002; Koehn et al., 2003). This method performs relatively well when the source and the target languages have similar word order, as in the case of French, Spanish, and English. However, when translating between languages with very different structures, as in the case of English and Japanese, or Japanese and Chinese, the quality of extracted bilingual phrases and the overall translation quality diminishes. In the latter scenario, a simple but effective strategy to cope with this problem is to reorder the words of sentences in one language so that it resembles the word order of another language (Wu"
W12-4207,J93-2003,0,\N,Missing
W12-4207,D08-1076,0,\N,Missing
W12-4213,W10-1736,1,0.905882,"Missing"
W12-4213,P02-1018,0,0.0219674,"inserts ‘anata wa (you)’ when the predicate is a verb, and ‘sore wa (it)’ when the predicate is a adjective or a copula. These inserted position is the beginning of the sentence. In the case that the sentence is imperative, the system does not solve the zero pronouns (Fig. 3). 4 Experiments 4.1 Experimental Setting devset5 7 500 test 16 489 evaluation of the performances, comparing the system outputs with the English references of test data. Using only BLEU score is not adequate for evaluation of pronoun translation (Hardmeier et al., 2010). We were inspired empty node recovery evaluation by (Johnson, 2002) and defined antecedent Precision (P), Recall (R) and F-measure (F) as follows, P = |G ∩ S| |S| R= |G ∩ S| |G| 2P R P +R Here, S is the set of each pronoun in English translated by decoder, G is the set of the gold standard zero pronoun. We evaluated the effect of performance of every case among completed sentences by human, ones by the baseline system, and the original sentences. F = 4.3 Experimental Result Fig. 4 shows the outline of the procedure of our experiment. We used Moses (Koehn et al., 2007) for the training of the translation and language models, tuning with MERT (Och, 2003) and th"
W12-4213,C88-2159,0,\N,Missing
W12-4213,2007.iwslt-1.1,0,\N,Missing
W12-4213,P02-1040,0,\N,Missing
W12-4213,P07-2045,0,\N,Missing
W12-4213,2010.iwslt-papers.10,0,\N,Missing
W12-4213,W97-0114,0,\N,Missing
W12-4213,D08-1076,0,\N,Missing
W13-2806,P07-1091,0,0.0205674,"reordered, but their positions will be decided relative to their verb. In what follows, we describe in detail how to identify verbal blocks, their objects and the invariable grammatical particles that will play a role in our reordering method. As mentioned earlier, the only information that will be used to perform this task will be the POS tags of the words and their unlabeled dependency structures. Related Work Although there are many works on pre-reordering methods for other languages to English translation or inverse (Xia and McCord, 2004; Xu et al., 2009; Habash, 2007; Wang et al., 2007; Li et al., 2007; Wu et al., 2011), reordering method for Chineseto-Japanese translation, which is a representative of long distance language pairs, has received little attention. The most related work to ours is in (Han et al., 2012), in which the authors introduced a refined reordering approach by importing an existing reordering method for English proposed in (Isozaki et al., 2010b). These reordering strategies are based on Head-driven phrase structure grammars (HPSG) (Pollard and Sag, 1994), in that the reordering decisions are made based on the head of phrases. Specifically, HPSG parsers (Miyao and Tsuji"
W13-2806,W08-0336,0,0.0552701,"Missing"
W13-2806,J08-1002,1,0.869913,"Li et al., 2007; Wu et al., 2011), reordering method for Chineseto-Japanese translation, which is a representative of long distance language pairs, has received little attention. The most related work to ours is in (Han et al., 2012), in which the authors introduced a refined reordering approach by importing an existing reordering method for English proposed in (Isozaki et al., 2010b). These reordering strategies are based on Head-driven phrase structure grammars (HPSG) (Pollard and Sag, 1994), in that the reordering decisions are made based on the head of phrases. Specifically, HPSG parsers (Miyao and Tsujii, 2008; Yu et al., 2011) are used to extract the structure of sentences in the form of binary trees, and head branches are swapped with their dependents according to certain heuristics to resemble the word order of the target language. However, those strategies are sensitive to parsing errors, and the binary structure of their parse trees impose hard constraints in sentences with loose word order. Moreover, as Han et al. (2012) noted, reordering strategies that are derived from the HPSG theory may not perform well when the head definition is inconsistent in the language pair under study. A typical e"
W13-2806,W09-2307,0,0.0211337,"d sentences using a phrasebased SMT system. However, Chinese parsers 25 Proceedings of the Second Workshop on Hybrid Approaches to Translation, pages 25–33, c Sofia, Bulgaria, August 8, 2013. 2013 Association for Computational Linguistics limited the extensibility of their method. Our approach follows the idea of using dependency tree structures and POS tags, but we discard the information on dependency labels since we did not find them informative to guide our reordering strategies in our preliminary experiments, partly due to Chinese showing less dependencies and a larger label variability (Chang et al., 2009). volve either Chinese or Japanese, and explain how our method builds upon them. From a linguistic perspective, we describe in section 3 our observations of reordering issues between Chinese and Japanese and detail how our framework solves those issues. In section 4 we assess to what extent our pre-reordering method succeeds in reordering words in Chinese sentences to resemble the order of Japanese sentences, and measure its impact on translation quality. The last section is dedicated to discuss our findings and point to future directions. 2 3 Methodology In Subject-Verb-Object (SVO) languages"
W13-2806,J03-1002,0,0.00360351,"es are necessary, and linguistic knowledge on structural difference can be encoded in the form of reordering rules. We show significant improvements in translation quality of sentences from news domain, when compared to state-of-the-art reordering methods. 1 Introduction Translation between Chinese and Japanese languages gains interest as their economic and political relationship intensifies. Despite their linguistic influences, these languages have different syntactic structures and phrase-based statistical machine translation (SMT) systems do not perform well. Current word alignment models (Och and Ney, 2003) account for local differences in word order between bilingual sentences, but fail at capturing long distance word alignments. One of the main problems in the search of the best word alignment is the combinatorial explosion of word orders, but linguistically-motivated heuristics can help to guide the search. This work explores syntax-informed prereordering for Chinese; that is, we obtain syntactic structures of Chinese sentences, reorder the words to resemble the Japanese word order, and then translate the reordered sentences using a phrasebased SMT system. However, Chinese parsers 25 Proceedi"
W13-2806,P03-1021,0,0.0334338,"010b), particle seed words were inserted at a preprocessing stage for Refined-HFC and our DPC method. DPC and Refined-HFC pre-reordering strategies were followed in the pipeline by a standard Moses-based baseline system (Koehn et al., 2007), using a default distance reordering model and a lexicalized reordering model “msd-bidirectionalfe”. A 5-gram language model was built using SRILM (Stolcke, 2002) on the target side of the corresponding training corpus. Word alignments were extracted using MGIZA++ (Gao and Vogel, 2008) and the parameters of the log-linear combination were tuned using MERT (Och, 2003). Table 3 summarizes the results of the Baseline system (no pre-reordering nor particle word insertion), the Refined-HFC (Ref-HFC) and our DPC method, using the well-known BLEU score (Papineni et al., 2002) and a word order sensitive metric named RIBES (Isozaki et al., 2010a). 5 Conclusions In the present paper, we have analyzed the differences in word order between Chinese and Japanese sentences. We captured the regularities of ordering differences between Chinese and Japanese sentences, and proposed a framework to reorder Chinese sentences to resemble the word order of Japanese. Our framewor"
W13-2806,W08-0509,0,0.0596627,"nju to obtain HPSG parsing trees. For comparison purposes with the work in (Isozaki et al., 2010b), particle seed words were inserted at a preprocessing stage for Refined-HFC and our DPC method. DPC and Refined-HFC pre-reordering strategies were followed in the pipeline by a standard Moses-based baseline system (Koehn et al., 2007), using a default distance reordering model and a lexicalized reordering model “msd-bidirectionalfe”. A 5-gram language model was built using SRILM (Stolcke, 2002) on the target side of the corresponding training corpus. Word alignments were extracted using MGIZA++ (Gao and Vogel, 2008) and the parameters of the log-linear combination were tuned using MERT (Och, 2003). Table 3 summarizes the results of the Baseline system (no pre-reordering nor particle word insertion), the Refined-HFC (Ref-HFC) and our DPC method, using the well-known BLEU score (Papineni et al., 2002) and a word order sensitive metric named RIBES (Isozaki et al., 2010a). 5 Conclusions In the present paper, we have analyzed the differences in word order between Chinese and Japanese sentences. We captured the regularities of ordering differences between Chinese and Japanese sentences, and proposed a framewor"
W13-2806,P02-1040,0,0.0874926,"ses-based baseline system (Koehn et al., 2007), using a default distance reordering model and a lexicalized reordering model “msd-bidirectionalfe”. A 5-gram language model was built using SRILM (Stolcke, 2002) on the target side of the corresponding training corpus. Word alignments were extracted using MGIZA++ (Gao and Vogel, 2008) and the parameters of the log-linear combination were tuned using MERT (Och, 2003). Table 3 summarizes the results of the Baseline system (no pre-reordering nor particle word insertion), the Refined-HFC (Ref-HFC) and our DPC method, using the well-known BLEU score (Papineni et al., 2002) and a word order sensitive metric named RIBES (Isozaki et al., 2010a). 5 Conclusions In the present paper, we have analyzed the differences in word order between Chinese and Japanese sentences. We captured the regularities of ordering differences between Chinese and Japanese sentences, and proposed a framework to reorder Chinese sentences to resemble the word order of Japanese. Our framework consists in three steps. First, we identify verbal blocks, which consist of Chinese words that will move all together as a block without altering their relative inner order. Second, we identify the right-"
W13-2806,2007.mtsummit-papers.29,0,0.039146,"vicinity of the verb will also be reordered, but their positions will be decided relative to their verb. In what follows, we describe in detail how to identify verbal blocks, their objects and the invariable grammatical particles that will play a role in our reordering method. As mentioned earlier, the only information that will be used to perform this task will be the POS tags of the words and their unlabeled dependency structures. Related Work Although there are many works on pre-reordering methods for other languages to English translation or inverse (Xia and McCord, 2004; Xu et al., 2009; Habash, 2007; Wang et al., 2007; Li et al., 2007; Wu et al., 2011), reordering method for Chineseto-Japanese translation, which is a representative of long distance language pairs, has received little attention. The most related work to ours is in (Han et al., 2012), in which the authors introduced a refined reordering approach by importing an existing reordering method for English proposed in (Isozaki et al., 2010b). These reordering strategies are based on Head-driven phrase structure grammars (HPSG) (Pollard and Sag, 1994), in that the reordering decisions are made based on the head of phrases. Specifi"
W13-2806,P06-1055,0,0.00949413,"and the combination of both corpora were used for training. sets of parallel sentences, namely an in-housecollected Chinese-Japanese news corpus (News), and the News corpus augmented with the CWMT (Zhao et al., 2011) corpus. We extracted disjoint development and test sets from News corpus, containing 1, 000 and 2, 000 sentences respectively. Table 2 shows the corpora statistics. We used MeCab 4 (Kudo and Matsumoto, 2000) and the Stanford Chinese segmenter 5 (Chang et al., 2008) to segment Japanese and Chinese sentences. POS tags of Chinese sentences were obtained using the Berkeley parser 6 (Petrov et al., 2006), while dependency trees were extracted using Corbit 7 (Hatori et al., 2011). Following the work in (Han et al., 2012), we re-implemented the Refined-HFC using the Chinese Enju to obtain HPSG parsing trees. For comparison purposes with the work in (Isozaki et al., 2010b), particle seed words were inserted at a preprocessing stage for Refined-HFC and our DPC method. DPC and Refined-HFC pre-reordering strategies were followed in the pipeline by a standard Moses-based baseline system (Koehn et al., 2007), using a default distance reordering model and a lexicalized reordering model “msd-bidirectio"
W13-2806,W12-4207,1,0.839206,"saaki}@lab.ntt.co.jp Abstract have difficulties in extracting reliable syntactic information, mainly because Chinese has a loose word order and few syntactic clues such as inflection and function words. On one hand, parsers implementing head-driven phrase structure grammars infer a detailed constituent structure, and such a rich syntactic structure can be exploited to design well informed reordering methods. However, inferring abundant syntactic information often implies introducing errors, and reordering methods that heavily rely on detailed information are sensitive to those parsing errors (Han et al., 2012). On the other hand, dependency parsers are committed to the simpler task of finding dependency relations and dependency labels, which can also be useful to guide reordering (Xu et al., 2009). However, reordering methods that rely on those dependency labels will also be prone to errors, specially in the case of Chinese since it has a richer set of dependency labels when compared to other languages. Since improving parsers for Chinese is challenging, we thus aim at reducing the influence of parsing errors in the reordering procedure. We present a hybrid approach that boosts the performance of p"
W13-2806,I11-1136,1,0.782054,"l sentences, namely an in-housecollected Chinese-Japanese news corpus (News), and the News corpus augmented with the CWMT (Zhao et al., 2011) corpus. We extracted disjoint development and test sets from News corpus, containing 1, 000 and 2, 000 sentences respectively. Table 2 shows the corpora statistics. We used MeCab 4 (Kudo and Matsumoto, 2000) and the Stanford Chinese segmenter 5 (Chang et al., 2008) to segment Japanese and Chinese sentences. POS tags of Chinese sentences were obtained using the Berkeley parser 6 (Petrov et al., 2006), while dependency trees were extracted using Corbit 7 (Hatori et al., 2011). Following the work in (Han et al., 2012), we re-implemented the Refined-HFC using the Chinese Enju to obtain HPSG parsing trees. For comparison purposes with the work in (Isozaki et al., 2010b), particle seed words were inserted at a preprocessing stage for Refined-HFC and our DPC method. DPC and Refined-HFC pre-reordering strategies were followed in the pipeline by a standard Moses-based baseline system (Koehn et al., 2007), using a default distance reordering model and a lexicalized reordering model “msd-bidirectionalfe”. A 5-gram language model was built using SRILM (Stolcke, 2002) on the"
W13-2806,D07-1077,0,0.0192007,"e verb will also be reordered, but their positions will be decided relative to their verb. In what follows, we describe in detail how to identify verbal blocks, their objects and the invariable grammatical particles that will play a role in our reordering method. As mentioned earlier, the only information that will be used to perform this task will be the POS tags of the words and their unlabeled dependency structures. Related Work Although there are many works on pre-reordering methods for other languages to English translation or inverse (Xia and McCord, 2004; Xu et al., 2009; Habash, 2007; Wang et al., 2007; Li et al., 2007; Wu et al., 2011), reordering method for Chineseto-Japanese translation, which is a representative of long distance language pairs, has received little attention. The most related work to ours is in (Han et al., 2012), in which the authors introduced a refined reordering approach by importing an existing reordering method for English proposed in (Isozaki et al., 2010b). These reordering strategies are based on Head-driven phrase structure grammars (HPSG) (Pollard and Sag, 1994), in that the reordering decisions are made based on the head of phrases. Specifically, HPSG parsers"
W13-2806,D10-1092,1,0.903219,"Missing"
W13-2806,I11-1004,1,0.842316,"heir positions will be decided relative to their verb. In what follows, we describe in detail how to identify verbal blocks, their objects and the invariable grammatical particles that will play a role in our reordering method. As mentioned earlier, the only information that will be used to perform this task will be the POS tags of the words and their unlabeled dependency structures. Related Work Although there are many works on pre-reordering methods for other languages to English translation or inverse (Xia and McCord, 2004; Xu et al., 2009; Habash, 2007; Wang et al., 2007; Li et al., 2007; Wu et al., 2011), reordering method for Chineseto-Japanese translation, which is a representative of long distance language pairs, has received little attention. The most related work to ours is in (Han et al., 2012), in which the authors introduced a refined reordering approach by importing an existing reordering method for English proposed in (Isozaki et al., 2010b). These reordering strategies are based on Head-driven phrase structure grammars (HPSG) (Pollard and Sag, 1994), in that the reordering decisions are made based on the head of phrases. Specifically, HPSG parsers (Miyao and Tsujii, 2008; Yu et al."
W13-2806,W10-1736,1,0.962597,"their unlabeled dependency structures. Related Work Although there are many works on pre-reordering methods for other languages to English translation or inverse (Xia and McCord, 2004; Xu et al., 2009; Habash, 2007; Wang et al., 2007; Li et al., 2007; Wu et al., 2011), reordering method for Chineseto-Japanese translation, which is a representative of long distance language pairs, has received little attention. The most related work to ours is in (Han et al., 2012), in which the authors introduced a refined reordering approach by importing an existing reordering method for English proposed in (Isozaki et al., 2010b). These reordering strategies are based on Head-driven phrase structure grammars (HPSG) (Pollard and Sag, 1994), in that the reordering decisions are made based on the head of phrases. Specifically, HPSG parsers (Miyao and Tsujii, 2008; Yu et al., 2011) are used to extract the structure of sentences in the form of binary trees, and head branches are swapped with their dependents according to certain heuristics to resemble the word order of the target language. However, those strategies are sensitive to parsing errors, and the binary structure of their parse trees impose hard constraints in s"
W13-2806,C04-1073,0,0.237416,"grammatical particles in the original vicinity of the verb will also be reordered, but their positions will be decided relative to their verb. In what follows, we describe in detail how to identify verbal blocks, their objects and the invariable grammatical particles that will play a role in our reordering method. As mentioned earlier, the only information that will be used to perform this task will be the POS tags of the words and their unlabeled dependency structures. Related Work Although there are many works on pre-reordering methods for other languages to English translation or inverse (Xia and McCord, 2004; Xu et al., 2009; Habash, 2007; Wang et al., 2007; Li et al., 2007; Wu et al., 2011), reordering method for Chineseto-Japanese translation, which is a representative of long distance language pairs, has received little attention. The most related work to ours is in (Han et al., 2012), in which the authors introduced a refined reordering approach by importing an existing reordering method for English proposed in (Isozaki et al., 2010b). These reordering strategies are based on Head-driven phrase structure grammars (HPSG) (Pollard and Sag, 1994), in that the reordering decisions are made based"
W13-2806,P07-2045,0,0.00976568,"Chinese sentences. POS tags of Chinese sentences were obtained using the Berkeley parser 6 (Petrov et al., 2006), while dependency trees were extracted using Corbit 7 (Hatori et al., 2011). Following the work in (Han et al., 2012), we re-implemented the Refined-HFC using the Chinese Enju to obtain HPSG parsing trees. For comparison purposes with the work in (Isozaki et al., 2010b), particle seed words were inserted at a preprocessing stage for Refined-HFC and our DPC method. DPC and Refined-HFC pre-reordering strategies were followed in the pipeline by a standard Moses-based baseline system (Koehn et al., 2007), using a default distance reordering model and a lexicalized reordering model “msd-bidirectionalfe”. A 5-gram language model was built using SRILM (Stolcke, 2002) on the target side of the corresponding training corpus. Word alignments were extracted using MGIZA++ (Gao and Vogel, 2008) and the parameters of the log-linear combination were tuned using MERT (Och, 2003). Table 3 summarizes the results of the Baseline system (no pre-reordering nor particle word insertion), the Refined-HFC (Ref-HFC) and our DPC method, using the well-known BLEU score (Papineni et al., 2002) and a word order sensit"
W13-2806,N09-1028,0,0.465198,"ion words. On one hand, parsers implementing head-driven phrase structure grammars infer a detailed constituent structure, and such a rich syntactic structure can be exploited to design well informed reordering methods. However, inferring abundant syntactic information often implies introducing errors, and reordering methods that heavily rely on detailed information are sensitive to those parsing errors (Han et al., 2012). On the other hand, dependency parsers are committed to the simpler task of finding dependency relations and dependency labels, which can also be useful to guide reordering (Xu et al., 2009). However, reordering methods that rely on those dependency labels will also be prone to errors, specially in the case of Chinese since it has a richer set of dependency labels when compared to other languages. Since improving parsers for Chinese is challenging, we thus aim at reducing the influence of parsing errors in the reordering procedure. We present a hybrid approach that boosts the performance of phrase-based SMT systems by pre-reordering the source language using unlabeled parse trees augmented with constituent information derived from Part-of-Speech tags. Specifically, we propose a f"
W13-2806,W11-2907,1,0.651863,"l., 2011), reordering method for Chineseto-Japanese translation, which is a representative of long distance language pairs, has received little attention. The most related work to ours is in (Han et al., 2012), in which the authors introduced a refined reordering approach by importing an existing reordering method for English proposed in (Isozaki et al., 2010b). These reordering strategies are based on Head-driven phrase structure grammars (HPSG) (Pollard and Sag, 1994), in that the reordering decisions are made based on the head of phrases. Specifically, HPSG parsers (Miyao and Tsujii, 2008; Yu et al., 2011) are used to extract the structure of sentences in the form of binary trees, and head branches are swapped with their dependents according to certain heuristics to resemble the word order of the target language. However, those strategies are sensitive to parsing errors, and the binary structure of their parse trees impose hard constraints in sentences with loose word order. Moreover, as Han et al. (2012) noted, reordering strategies that are derived from the HPSG theory may not perform well when the head definition is inconsistent in the language pair under study. A typical example for the lan"
W13-2806,W00-1303,0,0.708324,"proposed DPC method obtained p-values 0.002 and 0.0, which indicates significant improvements over the phrase-based system. Table 3: Evaluation of translation quality of two test sets when CWMT, News and the combination of both corpora were used for training. sets of parallel sentences, namely an in-housecollected Chinese-Japanese news corpus (News), and the News corpus augmented with the CWMT (Zhao et al., 2011) corpus. We extracted disjoint development and test sets from News corpus, containing 1, 000 and 2, 000 sentences respectively. Table 2 shows the corpora statistics. We used MeCab 4 (Kudo and Matsumoto, 2000) and the Stanford Chinese segmenter 5 (Chang et al., 2008) to segment Japanese and Chinese sentences. POS tags of Chinese sentences were obtained using the Berkeley parser 6 (Petrov et al., 2006), while dependency trees were extracted using Corbit 7 (Hatori et al., 2011). Following the work in (Han et al., 2012), we re-implemented the Refined-HFC using the Chinese Enju to obtain HPSG parsing trees. For comparison purposes with the work in (Isozaki et al., 2010b), particle seed words were inserted at a preprocessing stage for Refined-HFC and our DPC method. DPC and Refined-HFC pre-reordering st"
W13-2806,D08-1076,0,\N,Missing
W13-4913,W04-3224,0,0.0239107,"between syntactic and semantic units cause difficulties integrating semantic analysis with syntactic analysis. Our goal is to construct a practical constituent parser that can deal with appropriate grammatical units and output grammatical functions as semisemantic information, e.g., grammatical or semantic roles of arguments and gapping types of relative clauses. We take an approach to deriving a grammar from manually annotated corpora by training probabilistic models like current statistical constituent parsers of de facto standards (Petrov et al., 2006; Klein et al., 2003 ; Charniak, 2000; Bikel, 2004). We used a constituent-based treebank that Uematsu et al. (2013) converted from an existing bunsetsubased corpus as a base treebank, and retag the nonterminals and transform the tree structures in described in Section 3. We will present the results of evaluations of the parser trained with the treebank in Section 4, and show some analyses in Section 5. 2 Related work The number of researches on Japanese constituentbased parser is quite few compared to that of bunsetsu-dependency-based parser. Most of them have been conducted under lexicalized grammatical formalism. HPSG (Head-driven Phrase St"
W13-4913,A00-2018,0,0.167495,"the discrepancy between syntactic and semantic units cause difficulties integrating semantic analysis with syntactic analysis. Our goal is to construct a practical constituent parser that can deal with appropriate grammatical units and output grammatical functions as semisemantic information, e.g., grammatical or semantic roles of arguments and gapping types of relative clauses. We take an approach to deriving a grammar from manually annotated corpora by training probabilistic models like current statistical constituent parsers of de facto standards (Petrov et al., 2006; Klein et al., 2003 ; Charniak, 2000; Bikel, 2004). We used a constituent-based treebank that Uematsu et al. (2013) converted from an existing bunsetsubased corpus as a base treebank, and retag the nonterminals and transform the tree structures in described in Section 3. We will present the results of evaluations of the parser trained with the treebank in Section 4, and show some analyses in Section 5. 2 Related work The number of researches on Japanese constituentbased parser is quite few compared to that of bunsetsu-dependency-based parser. Most of them have been conducted under lexicalized grammatical formalism. HPSG (Head-dr"
W13-4913,W12-3411,0,0.0237099,"ics, however, generally requires a high computational cost. We aim at constructing a more light-weighted and practical constituent parser, e.g. a PCFG parser, from Penn Treebank style treebank with function labels. Gabbard et al. (2006) introduced function tags by modifying those in Penn Treebank to their parser. Even though Noro et al. (2005) built a Japanese corpus for deriving Japanese CFG, and evaluated its grammar, they did not treat the predicate-argument structure or the distinction of adnominal phrases. This paper is also closely related to the work of Korean treebank transformations (Choi et al., 2012). Most of the Korean corpus was built using grammatical chunks eojeols, which resemble Japanese bunsetsus and consist of content words and morphemes that represent grammatical functions. Choi et al. transformed the eojeol-based structure of Korean treebanks into entity-based to make them more suitable for parser training. We converted an existing bunsetsu-based corpus into a constituent-based one and integrating other information into it for training a parser. 3 Treebank for parser training In this section, we describe the overview of our treebank for training a parser. 3.1 Construction of a b"
W13-4913,N06-1024,0,0.0306115,"nguages. Uematsu et al. (2013) constructed a CCG (Combinatory Categorial Grammar) bank based on the scheme proposed by Bekki (2010), by integrating several corpora including a constituent-based treebank converted from a dependency-base corpus. These approaches above use a unification-based parser, which offers rich information integrating syntax, semantics and pragmatics, however, generally requires a high computational cost. We aim at constructing a more light-weighted and practical constituent parser, e.g. a PCFG parser, from Penn Treebank style treebank with function labels. Gabbard et al. (2006) introduced function tags by modifying those in Penn Treebank to their parser. Even though Noro et al. (2005) built a Japanese corpus for deriving Japanese CFG, and evaluated its grammar, they did not treat the predicate-argument structure or the distinction of adnominal phrases. This paper is also closely related to the work of Korean treebank transformations (Choi et al., 2012). Most of the Korean corpus was built using grammatical chunks eojeols, which resemble Japanese bunsetsus and consist of content words and morphemes that represent grammatical functions. Choi et al. transformed the eoj"
W13-4913,I11-1023,0,0.0133172,"dings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages, pages 108–118, c Seattle, Washington, USA, 18 October 2013. 2013 Association for Computational Linguistics operating syntactically meaningful units in such applications as statistical machine translation, which needs to recognize syntactic units in building a translation model (e.g. tree-to-string and tree-to-tree) and in preordering source language sentences. Semantic analysis, such as predicate-argument structure analysis, is usually done as a pipeline process after syntactic analysis (Iida et al., 2011 ; Hayashibe et al., 2011 ); but in Japanese, the discrepancy between syntactic and semantic units cause difficulties integrating semantic analysis with syntactic analysis. Our goal is to construct a practical constituent parser that can deal with appropriate grammatical units and output grammatical functions as semisemantic information, e.g., grammatical or semantic roles of arguments and gapping types of relative clauses. We take an approach to deriving a grammar from manually annotated corpora by training probabilistic models like current statistical constituent parsers of de facto standards (Petrov et al., 2006; K"
W13-4913,P11-1081,0,0.0602191,"Missing"
W13-4913,N06-1023,0,0.37192,"Missing"
W13-4913,W02-2016,0,0.56148,"Missing"
W13-4913,J93-2004,0,0.0427744,"Missing"
W13-4913,Y03-1034,0,0.0806128,"Missing"
W13-4913,J05-1004,0,0.0411909,"inu-ga “dog-NOM” 猫を neko-o “cat-ACC” 追いかけた oikaketa “chased” (The dog chased the cat.) We annotated predicate arguments by two different schemes (different tag sets) in our treebank: grammatical roles and semantic roles. In using a tag set based on grammatical roles, the arguments are assigned with the suffixes based on their syntactic roles in the sentence, like Penn Treebank: SBJ (subject), OBJ (direct object), and OB2 (indirect object). Figure 2 is annotated by this scheme. Alternatively, the arguments are labeled based on their semantic roles from case frame of predicates, like PropBank (Palmer et al., 2005 ): ARG0, ARG1 and ARG2. These arguments are annotated by converting semantic roles defined in the case frame dictionary Goitaikei into simple labels, the labels are not influenced by case alternation. In both annotation schemes, we also annotated two types of arbitrary arguments with semantic role labels: LOC (locative) and TMP (temporal), which can be assigned consistently and are useful for various applications. 112 Adnominal clauses Clauses modifying noun phrases are divided into two types: (gapping) relative and non-gapping adnominal clauses. Relative clauses are denoted by adding functio"
W13-4913,P06-1055,0,0.381759,"Hayashibe et al., 2011 ); but in Japanese, the discrepancy between syntactic and semantic units cause difficulties integrating semantic analysis with syntactic analysis. Our goal is to construct a practical constituent parser that can deal with appropriate grammatical units and output grammatical functions as semisemantic information, e.g., grammatical or semantic roles of arguments and gapping types of relative clauses. We take an approach to deriving a grammar from manually annotated corpora by training probabilistic models like current statistical constituent parsers of de facto standards (Petrov et al., 2006; Klein et al., 2003 ; Charniak, 2000; Bikel, 2004). We used a constituent-based treebank that Uematsu et al. (2013) converted from an existing bunsetsubased corpus as a base treebank, and retag the nonterminals and transform the tree structures in described in Section 3. We will present the results of evaluations of the parser trained with the treebank in Section 4, and show some analyses in Section 5. 2 Related work The number of researches on Japanese constituentbased parser is quite few compared to that of bunsetsu-dependency-based parser. Most of them have been conducted under lexicalized"
W13-4913,W02-1210,0,0.198001,"Missing"
W13-4913,P13-1103,0,0.101464,"integrating semantic analysis with syntactic analysis. Our goal is to construct a practical constituent parser that can deal with appropriate grammatical units and output grammatical functions as semisemantic information, e.g., grammatical or semantic roles of arguments and gapping types of relative clauses. We take an approach to deriving a grammar from manually annotated corpora by training probabilistic models like current statistical constituent parsers of de facto standards (Petrov et al., 2006; Klein et al., 2003 ; Charniak, 2000; Bikel, 2004). We used a constituent-based treebank that Uematsu et al. (2013) converted from an existing bunsetsubased corpus as a base treebank, and retag the nonterminals and transform the tree structures in described in Section 3. We will present the results of evaluations of the parser trained with the treebank in Section 4, and show some analyses in Section 5. 2 Related work The number of researches on Japanese constituentbased parser is quite few compared to that of bunsetsu-dependency-based parser. Most of them have been conducted under lexicalized grammatical formalism. HPSG (Head-driven Phrase Structure Grammar) (Sag et al., 2003 ) is a representative one. Gun"
W13-4913,W07-1522,0,\N,Missing
W15-5012,W15-5001,0,0.0351006,"ndirect or rhetorical expressions. Due to this aspect, patent documents are good candidates for literal translation, which most machine translation (MT) approaches aim to do. One technical challenge for patent machine translation is the complex syntactic structure of patent documents, which typically have long sentences that complicate MT reordering, especially for the word order in distant languages. Chinese and Japanese have similar word order in noun modifiers but different subject-verb-object order, requiring long distance reordering in translation. In this year’s WAT evaluation campaign (Nakazawa et al., 2015), we tackle long distance reordering by syntactic pre-ordering based on Chinese dependency structures (Sudoh et al., 2014) in a Chinese-to-Japanese patent translation task. Our system basically consists of three components: Chinese syntactic analysis (word segmentation, part-of-speech (POS) tagging, and dependency parsing) adapted to patent documents; dependency-based syntactic pre-ordering with hand-written rules or a learning-to-rank model; and a standard phrase-based statistical MT. This paper describes our system’s details and discusses our evaluation results. 2 System Overview Figure 1 sh"
W15-5012,D09-1058,0,0.0879295,"al., 2012) for better Chinese word segmentation based on the POS tag sequences. The dependency parser produces un95 Proceedings of the 2nd Workshop on Asian Translation (WAT2015), pages 95‒98, Kyoto, Japan, 16th October 2015. 2015 Copyright is held by the author(s). typed dependency trees. The Chinese analysis models were trained using in-house Chinese treebanks in the patent domain (about 35,000 sentences) as well as the standard Penn Chinese Treebank dataset (Sudoh et al., 2014). The training also utilized unlabeled Chinese patent documents (about 100 G bytes) for semi-supervised training (Suzuki et al., 2009; Sudoh et al., 2014). 4 dependency trees and have to consider all the possible permutation over one head word and one or more modifier words. 5 Evaluation 5.1 Setup We trained a word n-gram language model and two different phrase-based translation models by the above different pre-ordering approaches. We used all of the supplied Chinese-Japanese bilingual training corpora of one million sentence pairs (except for long sentences over 64 words) for the MT models: phrase tables, lexicalized reordering tables, and word 5-gram language models using standard Moses and KenLM training parameters. We"
W15-5012,P12-1096,0,0.0602246,"BES and BLEU than the tree-to-string baseline, but the difference may not be significant. The performances of our systems were lower than the tree-to-string baseline in the Human evaluation. With respect to the difference in the pre-ordering approaches, the rule-based system outperformed the data-driven one. Data-driven pre-ordering obtains the most probable reordering of a source language sentence that is monotone with the target language counterpart. It learns rules or models using reordering oracles over word-aligned bilingual corpora. We used a learning-to-rank approach with Ranking SVMs (Yang et al., 2012), which reorders the head word and its modifier words in a dependency tree based on their ranks. The features resemble those by Yang et al. (2012); we did not use label-related ones because our dependency trees do not have labels. The reordering oracles were determined to maximize Kendall’s τ over automatic word alignment in a similar manner to Hoshino et al. (2015). The only difference is the tree structure; Hoshino et al. (2015) used binary trees and just considered monotone or reverse for two child nodes of each tree node. But we use 5.3 Discussion One critical concern is the difference bet"
W15-5012,P11-1084,0,0.0464458,"Missing"
W15-5012,W06-3119,0,0.130237,"Missing"
W15-5012,W14-7001,0,\N,Missing
W15-5012,E14-1026,0,\N,Missing
W16-3616,W08-1301,0,0.159623,"Missing"
W16-3616,egg-redeker-2010-complex,0,0.0153872,"e the more accurate parser does not always lead to better performance of textbased applications. 2 simplify discourse parsing. They also presented a method to automatically derive DDTs from SDR structures. Wolf and Gibson (2005) used a chain-graph for representing discourse structures and annotated 135 articles from the AP Newswire and the Wall Street Journal. The annotated corpus is called the Discourse Graphbank. The graph represents crossed dependency and multiple parentship discourse phenomena, which cannot be represented by tree structures, but whose graph structures become very complex (Egg and Redeker, 2010). The Penn Discourse Treebank (PDTB) (Prasad et al., 2008) is a large-scale corpus of annotated discourse connectives and their arguments. Its connective-argument structure can also represent complex discourse phenomena like multiple parentship, but its objective is to annotate the discourse relations between individual discourse units, not full discourse structures. Unfortunately, to the best of our knowledge, neither the Discourse Graphbank nor the PDTB has been used for any specific NLP applications. Related Work Mann and Thompson (1988)’s Rhetorical Structure Theory (RST), which is one of"
W16-3616,P99-1059,0,0.080509,"he performance with MST’s DDTs closely approached that of the gold DDTs. These results imply that the auto parse trees obtained from Hirao13 have broad and shallow hierarchies because important EDUs, which must be included in a summary, can be easily extracted by TKP. Thus, the DDTs converted by the Hirao13 rule have better tree structures for a single document summarization even though the structures are complex and difficult to parse. This is a significant advantage over Li’s conversion rule. ing since it offers cubic-time dynamic programming algorithms for dependency parsing (Eisner, 1996; Eisner and Satta, 1999; G´omez-Rodrıguez et al., 2008). A higher gap degree means that the dependency trees have more complex nonprojective structures. Both the Hirao13 and MHirao13 methods produce many non-projective dependency edges, but most of the DDTs have at most 1 gap degree and all are well-nested. The well-nested dependency structures of the low gap degree also allow efficient dynamic programming solutions with polynominal time complexity to dependency parsing (G´omez-Rodrıguez et al., 2009). 5.2 Impact on Automatic Parsing Accuracy The conversion methods introduce different complexities in DDTs. This sect"
W16-3616,afantenos-etal-2012-empirical,0,0.0544238,"Missing"
W16-3616,P12-1007,0,0.271197,"corpora for the automatic analysis of text syntax, most notably the RST Discourse Treebank (RST-DTB) (Carlson et al., 2003). 128 Proceedings of the SIGDIAL 2016 Conference, pages 128–136, c Los Angeles, USA, 13-15 September 2016. 2016 Association for Computational Linguistics are more suitable to text summarization? We show from experimental results that even though the Hirao13 DDT format reduces performance, as measured by intrinsic evaluations, it is more useful for text summarization. While researchers developing discourse syntactic parsing (Soricut and Marcu, 2003; Hernault et al., 2010; Feng and Hirst, 2012; Joty et al., 2013; Li et al., 2014) have focused excessively on improving accuracies, our experimental results emphasize the importance of extrinsic evaluations since the more accurate parser does not always lead to better performance of textbased applications. 2 simplify discourse parsing. They also presented a method to automatically derive DDTs from SDR structures. Wolf and Gibson (2005) used a chain-graph for representing discourse structures and annotated 135 articles from the AP Newswire and the Wall Street Journal. The annotated corpus is called the Discourse Graphbank. The graph repr"
W16-3616,W08-1105,0,0.0168151,"s more complex dependency structures. 1 Introduction Recent years have seen an increase in the use of dependency representations throughout various NLP applications. For the discourse analysis of texts, dependency graph representations have also been studied by many researchers (Prasad et al., 2008; Muller et al., 2012; Hirao et al., 2013; Li et al., 2014). In particular, Hirao et al. (2013) proposed a current state-of-the-art text summarization method based on trimming discourse dependency trees (DDTs). Dependency tree representation is the key to the formulation of the tree trimming method (Filippova and Strube, 2008), and dependency-based discourse syntax has further potential to improve the modeling of a wide range of text-based applications. 1 Mann and Thompson (1988)’s Rhetorical Structure Theory (RST), which is one of the most influential text organization frameworks, represents discourse as a (constituent-style) tree structure. RST was developed as the basis of annotated corpora for the automatic analysis of text syntax, most notably the RST Discourse Treebank (RST-DTB) (Carlson et al., 2003). 128 Proceedings of the SIGDIAL 2016 Conference, pages 128–136, c Los Angeles, USA, 13-15 September 2016. 201"
W16-3616,P08-1110,0,0.037315,"Missing"
W16-3616,P98-1044,0,0.664343,"Rhetorical Structure Theory (RST), which is one of the most influential text organization frameworks, represents a discourse structure as a constituent tree. The RST Discourse Treebank (RST-DTB) (Carlson et al., 2003) has played a critical role in automatic discourse analysis (Soricut and Marcu, 2003; Hernault et al., 2010; Feng and Hirst, 2012; Joty et al., 2013), mainly because trees are both easy to formalize and computationally tractable. RST discourse trees (RST-DTs) are also used for modeling many text-based applications, such as text summarization (Marcu, 2000) and anaphora resolution (Cristea et al., 1998). Hirao et al. (2013) and Li et al. (2014) introduced dependency conversion methods from RSTDTs into DDTs in which a full discourse structure is represented by head-dependent binary relations between elementary discourse units. Hirao et al. (2013) also showed that a text summarization method, based on trimming DDTs, achieves significant improvements against Marcu (2000)’s method using RST-DTs. On the other hand, some researchers argue that trees are inadequate to account for a full discourse structure (Wolf and Gibson, 2005; Lee et al., 2006; Danlos and others, 2008; Venant et al., 2013). Segm"
W16-3616,E09-1034,0,0.0424113,"Missing"
W16-3616,prasad-etal-2008-penn,0,0.781536,"formats from a dependency graph theoretic point of view? (2) Which formats are analyzed more accurately by automatic parsers? (3) Which are more suitable for text summarization task? Experimental results showed that Hirao’s conversion rule produces DDTs that are more useful for text summarization, even though it derives more complex dependency structures. 1 Introduction Recent years have seen an increase in the use of dependency representations throughout various NLP applications. For the discourse analysis of texts, dependency graph representations have also been studied by many researchers (Prasad et al., 2008; Muller et al., 2012; Hirao et al., 2013; Li et al., 2014). In particular, Hirao et al. (2013) proposed a current state-of-the-art text summarization method based on trimming discourse dependency trees (DDTs). Dependency tree representation is the key to the formulation of the tree trimming method (Filippova and Strube, 2008), and dependency-based discourse syntax has further potential to improve the modeling of a wide range of text-based applications. 1 Mann and Thompson (1988)’s Rhetorical Structure Theory (RST), which is one of the most influential text organization frameworks, represents"
W16-3616,D13-1158,1,0.798445,"Missing"
W16-3616,W09-3813,0,0.576112,"roduced by Li’s method for RST discourse tree in Figure 1: “Elabo.” is short for “Elaboration”. Li et al. (2014)’s dependency conversion method is based on the idea of assigning each discourse unit in an RST-DT a unique head selected among the unit’s children. Traversing each nonterminal node in a bottom-up manner, the headassignment procedure determines the head from its children in the following manner: the head of the leftmost child node with the Nucleus is the head; if no child node is the Nucleus, the head of the leftmost child node is the head. The procedure was originally introduced by Sagae (2009), and its core idea is identical as the head-assignment rules for Penn Treebankstyle constituent trees (Magerman, 1994; Collins, 1999). Li’s conversion method uses the procedure to assign a head to each non-terminal node of a right-branching binarized RST-DT (Hernault et al., 2010) and transforms the head-annotated binary tree into a DDT. Algorithms 1-3 show the dependency conversion method. For brevity, we describe it in a different form from Li’s original conversion process2 cited above. In Algorithm 1, the main routine iteratively processes every EDU in given RST-DT t to directly find its s"
W16-3616,N03-1030,0,0.452687,"ure. RST was developed as the basis of annotated corpora for the automatic analysis of text syntax, most notably the RST Discourse Treebank (RST-DTB) (Carlson et al., 2003). 128 Proceedings of the SIGDIAL 2016 Conference, pages 128–136, c Los Angeles, USA, 13-15 September 2016. 2016 Association for Computational Linguistics are more suitable to text summarization? We show from experimental results that even though the Hirao13 DDT format reduces performance, as measured by intrinsic evaluations, it is more useful for text summarization. While researchers developing discourse syntactic parsing (Soricut and Marcu, 2003; Hernault et al., 2010; Feng and Hirst, 2012; Joty et al., 2013; Li et al., 2014) have focused excessively on improving accuracies, our experimental results emphasize the importance of extrinsic evaluations since the more accurate parser does not always lead to better performance of textbased applications. 2 simplify discourse parsing. They also presented a method to automatically derive DDTs from SDR structures. Wolf and Gibson (2005) used a chain-graph for representing discourse structures and annotated 135 articles from the AP Newswire and the Wall Street Journal. The annotated corpus is c"
W16-3616,W07-2416,0,0.0368233,"then 8: C←P 9: end if 10: Return C Conversions from RST-DTs to DDTs Next, this paper discusses text-level dependency syntax, which represents grammatical structure by head-dependent binary relations between EDUs. This section introduces two existing automatic conversion methods from RST-DTs to DDTs: the methods of Li et al. (2014) and Hirao et al. (2013). Additionally, this paper presents a simple postediting method to reduce the complexity of DDTs. The heart of these conversions closely resembles that of constituent-to-dependency conversions for English sentences (Yamada and Matsumoto, 2003; Johansson and Nugues, 2007; De Marneffe and Manning, 2008), since RST-DTs can be regarded as Penn Treebank-style constituent trees because EDUs and discourse units respectively correspond to terminal and non-terminal nodes, and a rhetorical relation, like a CFG-rule, forms an edge in the tree. 4.1 Li et al. (2014)’s Method 130 Algorithm 3 find-Head-EDU(P) node in t to which current processed EDU e- j must be assigned as the head in Sagae’s lexicalization manner. Parent(P) and LeftmostNucleusChild(P) are respectively operations that return the parent node of node P and the leftmost child node with the Nucleus of node P3"
W16-3616,P13-1048,0,0.336485,"atic analysis of text syntax, most notably the RST Discourse Treebank (RST-DTB) (Carlson et al., 2003). 128 Proceedings of the SIGDIAL 2016 Conference, pages 128–136, c Los Angeles, USA, 13-15 September 2016. 2016 Association for Computational Linguistics are more suitable to text summarization? We show from experimental results that even though the Hirao13 DDT format reduces performance, as measured by intrinsic evaluations, it is more useful for text summarization. While researchers developing discourse syntactic parsing (Soricut and Marcu, 2003; Hernault et al., 2010; Feng and Hirst, 2012; Joty et al., 2013; Li et al., 2014) have focused excessively on improving accuracies, our experimental results emphasize the importance of extrinsic evaluations since the more accurate parser does not always lead to better performance of textbased applications. 2 simplify discourse parsing. They also presented a method to automatically derive DDTs from SDR structures. Wolf and Gibson (2005) used a chain-graph for representing discourse structures and annotated 135 articles from the AP Newswire and the Wall Street Journal. The annotated corpus is called the Discourse Graphbank. The graph represents crossed depe"
W16-3616,W13-4002,0,0.0593735,"ion (Cristea et al., 1998). Hirao et al. (2013) and Li et al. (2014) introduced dependency conversion methods from RSTDTs into DDTs in which a full discourse structure is represented by head-dependent binary relations between elementary discourse units. Hirao et al. (2013) also showed that a text summarization method, based on trimming DDTs, achieves significant improvements against Marcu (2000)’s method using RST-DTs. On the other hand, some researchers argue that trees are inadequate to account for a full discourse structure (Wolf and Gibson, 2005; Lee et al., 2006; Danlos and others, 2008; Venant et al., 2013). Segmented Discourse Representation Theory (SDRT) (Asher and Lascarides, 2003) represents discourse structures as logical form, and relations function like logical operators on the meaning of their arguments. The annotation in the ANNODIS corpus was conducted based on SDRT (Afantenos et al., 2012). For automatic discourse analysis using the corpus, Muller et al. (2012) adopted dependency tree representation to 3 RST Discourse Tree RST represents a discourse as a tree structure. The leaves of an RST discourse tree (RST-DT) correspond to Elementary Discourse Units (EDUs). Adjacent EDUs are link"
W16-3616,J05-2005,0,0.445795,"reduces performance, as measured by intrinsic evaluations, it is more useful for text summarization. While researchers developing discourse syntactic parsing (Soricut and Marcu, 2003; Hernault et al., 2010; Feng and Hirst, 2012; Joty et al., 2013; Li et al., 2014) have focused excessively on improving accuracies, our experimental results emphasize the importance of extrinsic evaluations since the more accurate parser does not always lead to better performance of textbased applications. 2 simplify discourse parsing. They also presented a method to automatically derive DDTs from SDR structures. Wolf and Gibson (2005) used a chain-graph for representing discourse structures and annotated 135 articles from the AP Newswire and the Wall Street Journal. The annotated corpus is called the Discourse Graphbank. The graph represents crossed dependency and multiple parentship discourse phenomena, which cannot be represented by tree structures, but whose graph structures become very complex (Egg and Redeker, 2010). The Penn Discourse Treebank (PDTB) (Prasad et al., 2008) is a large-scale corpus of annotated discourse connectives and their arguments. Its connective-argument structure can also represent complex discou"
W16-3616,P06-2066,0,0.468614,"ly because the authors described only abstracts of their conversion methods. To clarify their algorithmic differences, this paper provides pseudocodes where the two different methods can be described in a unified form, showing that they analyze multinuclear relations differently on RST-DTs. As we show by example in Section 4, such a slight difference can derive significantly different DDTs. The main purpose of this paper is to experimentally reveal the differences between dependency formats. By investigating the complexity of their structures from the dependency graph theoretic point of view (Kuhlmann and Nivre, 2006), we prove that the Hirao13 method, which keeps the semantic equivalence of multinuclear discourse units in the dependency structures, introduces much more complex DDTs than Li14, while a simple post-editing method greatly reduces the complexity of DDTs. This paper also compares the methods with both intrinsic and extrinsic evaluations: (1) Which dependency structures are analyzed more accurately by automatic parsers? and (2) Which structures Two heuristic rules that transform Rhetorical Structure Theory discourse trees into discourse dependency trees (DDTs) have recently been proposed (Hirao"
W16-3616,W03-3023,0,0.0829739,"hile 7: if isRoot(P) = TRUE then 8: C←P 9: end if 10: Return C Conversions from RST-DTs to DDTs Next, this paper discusses text-level dependency syntax, which represents grammatical structure by head-dependent binary relations between EDUs. This section introduces two existing automatic conversion methods from RST-DTs to DDTs: the methods of Li et al. (2014) and Hirao et al. (2013). Additionally, this paper presents a simple postediting method to reduce the complexity of DDTs. The heart of these conversions closely resembles that of constituent-to-dependency conversions for English sentences (Yamada and Matsumoto, 2003; Johansson and Nugues, 2007; De Marneffe and Manning, 2008), since RST-DTs can be regarded as Penn Treebank-style constituent trees because EDUs and discourse units respectively correspond to terminal and non-terminal nodes, and a rhetorical relation, like a CFG-rule, forms an edge in the tree. 4.1 Li et al. (2014)’s Method 130 Algorithm 3 find-Head-EDU(P) node in t to which current processed EDU e- j must be assigned as the head in Sagae’s lexicalization manner. Parent(P) and LeftmostNucleusChild(P) are respectively operations that return the parent node of node P and the leftmost child node"
W16-3616,D14-1196,1,0.788655,"ee and all are well-nested. The well-nested dependency structures of the low gap degree also allow efficient dynamic programming solutions with polynominal time complexity to dependency parsing (G´omez-Rodrıguez et al., 2009). 5.2 Impact on Automatic Parsing Accuracy The conversion methods introduce different complexities in DDTs. This section investigates which formats are more accurately analyzed by automatic discourse parsers. For evaluation, we implemented a maximum spanning tree algorithm for discourse dependency parsing, which was recently proposed (Muller et al., 2012; Li et al., 2014; Yoshida et al., 2014). To compare discourse dependency parsing with standard RST parsing, we also implemented the HILDA RST parser (Hernault et al., 2010), which achieved 82.6/66.6/54.2 points for a standard set of RST-style evaluation measures, i.e., Span, Nuclearity and Relation (Marcu, 2000). We used a standard split of DDTs automatically converted from RST-DTB: 347 DDTs as the training set and 38 as the test set. Table 3 shows the evaluation results of dependency parsing. The lower the complexity of the DDT format, the higher is the dependency unlabeled attachment score. Post-editing the Hirao13 DDTs improves"
W16-3616,P14-1003,0,0.567173,"t the Hirao13 method, which keeps the semantic equivalence of multinuclear discourse units in the dependency structures, introduces much more complex DDTs than Li14, while a simple post-editing method greatly reduces the complexity of DDTs. This paper also compares the methods with both intrinsic and extrinsic evaluations: (1) Which dependency structures are analyzed more accurately by automatic parsers? and (2) Which structures Two heuristic rules that transform Rhetorical Structure Theory discourse trees into discourse dependency trees (DDTs) have recently been proposed (Hirao et al., 2013; Li et al., 2014), but these rules derive significantly different DDTs because their conversion schemes on multinuclear relations are not identical. This paper reveals the difference among DDT formats with respect to the following questions: (1) How complex are the formats from a dependency graph theoretic point of view? (2) Which formats are analyzed more accurately by automatic parsers? (3) Which are more suitable for text summarization task? Experimental results showed that Hirao’s conversion rule produces DDTs that are more useful for text summarization, even though it derives more complex dependency struc"
W16-3616,C12-1115,0,0.402053,"ency graph theoretic point of view? (2) Which formats are analyzed more accurately by automatic parsers? (3) Which are more suitable for text summarization task? Experimental results showed that Hirao’s conversion rule produces DDTs that are more useful for text summarization, even though it derives more complex dependency structures. 1 Introduction Recent years have seen an increase in the use of dependency representations throughout various NLP applications. For the discourse analysis of texts, dependency graph representations have also been studied by many researchers (Prasad et al., 2008; Muller et al., 2012; Hirao et al., 2013; Li et al., 2014). In particular, Hirao et al. (2013) proposed a current state-of-the-art text summarization method based on trimming discourse dependency trees (DDTs). Dependency tree representation is the key to the formulation of the tree trimming method (Filippova and Strube, 2008), and dependency-based discourse syntax has further potential to improve the modeling of a wide range of text-based applications. 1 Mann and Thompson (1988)’s Rhetorical Structure Theory (RST), which is one of the most influential text organization frameworks, represents discourse as a (const"
W16-3616,W01-1605,0,\N,Missing
W16-3616,C96-1058,0,\N,Missing
W16-3616,C98-1044,0,\N,Missing
W16-4615,D10-1062,0,0.654598,"ightly. 1 Introduction Empty categories are phonetically null elements that are used for representing dropped pronouns (“pro” or “small pro”), controlled elements (“PRO” or “big pro”) and traces of movement (“T” or “trace”). Dropped pronouns are one of the major problems caused on machine translation from the pro-drop language such as Japanese to the non-pro-drop language such as English because it is difficult to produce the correct pronouns on the target side when the pronoun is missing on the source side. The effects of empty categories in machine translation have previously been examined (Chung and Gildea, 2010; Taira et al., 2012; Xiang et al., 2013; Kudo et al., 2014; Wang et al., 2016). In this paper, we address two new problems that were not fully discussed in previous work. The first problem is that, even if empty categories are correctly recovered, it is difficult to automatically obtain the correct word alignment for languages with a completely different word order such as Japanese and English. The second problem is that it is not only difficult to translate non-existent pronouns but also relative clauses because relative pronouns do not exist in Japanese. In theory, we can safely ignore PRO"
W16-4615,2005.iwslt-1.1,0,0.0607067,"Missing"
W16-4615,D13-1095,0,0.0186162,"In the following sections, we first briefly describe related works. We then describe empty category detection method (Takeno et al., 2015) and discriminative preordering method (Hoshino et al., 2015) used in the proposed method. We then report experiment results of Japanese-to-English translation on both spoken (IWSLT dataset) and written (KFTT dataset) languages. 2 Related works Conventional approaches to recover zero pronouns in Japanese are to frame it as zero anaphora resolution, which is a sub-problem of predicate argument structure analysis (Nakaiwa and Ikehara, 1995; Iida et al., 2007; Hangyo et al., 2013). Zero anaphora resolution consists of two procedures: zero pronoun detection and anaphora resolution. It is difficult to integrate zero anaphora resolution (or predicate-argument structure analysis) into SMT for two reasons. The first is that anaphora resolution requires context analysis, which complicates the translation method. The second is that predicate argument structure analysis provides semantic relations, not syntactic structure. This makes it difficult to use the information of recovered zero pronouns in SMT, because there is no position information for the zero pronouns in the word"
W16-4615,W11-2123,0,0.0136321,"gories, as described in the previous section. We achieved this by first tokenizing Japanese sentences by using a CRF-based tokenizing and chunking software (Uchimoto and Den, 2008) to obtain the long unit words required by the Japanese parser (Hayashi et al., 2016). We then achieved word alignment by using short unit words in Japanese obtained by using the MeCab morphological analyzer with the UniDic dictionary2 . For the Japanese-to-English translation experiment, we used a phrase-based translation model (Koehn et al., 2007). For all systems we compared, the language model is a 5-gram KenLM (Heafield, 2011), which uses modified Kneser-Ney smoothing and tuning is performed to maximize the BLEU score using minimum error rate training (Och, 2007). Other configurable setting of all tool use default values unless otherwise stated. We compared three translation methods, each with and without empty category detection. BASELINE is a phrase-based machine translation system (Moses) (Koehn et al., 2007) which consists of training data comprising a bilingual dataset without preordering. REORDERING(H) and REORDERING(C) are described in the previous section. For REORDERING(H), 5,319 sentences with manual word"
W16-4615,P15-2023,1,0.909812,"te machine translation outputs. We solve this problem by integrating empty category detection into preordering-based statistical machine translation. We first insert empty categories into the source sentence, and then reorder them such that the word order is similar to that of the target sentence. We find it is effective to filter out unreliable empty category candidates to improve the accuracy of machine translation. In the following sections, we first briefly describe related works. We then describe empty category detection method (Takeno et al., 2015) and discriminative preordering method (Hoshino et al., 2015) used in the proposed method. We then report experiment results of Japanese-to-English translation on both spoken (IWSLT dataset) and written (KFTT dataset) languages. 2 Related works Conventional approaches to recover zero pronouns in Japanese are to frame it as zero anaphora resolution, which is a sub-problem of predicate argument structure analysis (Nakaiwa and Ikehara, 1995; Iida et al., 2007; Hangyo et al., 2013). Zero anaphora resolution consists of two procedures: zero pronoun detection and anaphora resolution. It is difficult to integrate zero anaphora resolution (or predicate-argument"
W16-4615,D10-1092,0,0.0157439,"al dataset without preordering. REORDERING(H) and REORDERING(C) are described in the previous section. For REORDERING(H), 5,319 sentences with manual word alignment 2 http://taku910.github.io/mecab/unidic 161 Figure 3: Characteristic of machine translation evaluation scores to empty categories filtered for development set of the IWSLT dataset is used. These systems are equivalent to Hoshino et al., (2015)’s method. They are taken from both the spoken language (CSJ) and written (KTC) language corpus. As for evaluation measures, we use the standard BLEU (Papineni et al., 2002) as well as RIBES (Isozaki et al., 2010), which is a rank correlation based metric that has been shown to be highly correlated with human evaluations of machine translation systems between languages with a very different word order such as Japanese and English. Result of filtering empty categories In this experiment, we search for the best threshold value to filter out empty categories in Sec 3. Changing the threshold values θ from 0.50 to 1.0, we measure both BLEU and RIBES, where θ = 1.0 corresponds to the result produces by machine translation systems trained from a dataset without empty categories. When decoding the text into En"
W16-4615,P07-2045,0,0.0084825,"detection method to source Japanese sentences to obtain syntactic trees with empty categories, as described in the previous section. We achieved this by first tokenizing Japanese sentences by using a CRF-based tokenizing and chunking software (Uchimoto and Den, 2008) to obtain the long unit words required by the Japanese parser (Hayashi et al., 2016). We then achieved word alignment by using short unit words in Japanese obtained by using the MeCab morphological analyzer with the UniDic dictionary2 . For the Japanese-to-English translation experiment, we used a phrase-based translation model (Koehn et al., 2007). For all systems we compared, the language model is a 5-gram KenLM (Heafield, 2011), which uses modified Kneser-Ney smoothing and tuning is performed to maximize the BLEU score using minimum error rate training (Och, 2007). Other configurable setting of all tool use default values unless otherwise stated. We compared three translation methods, each with and without empty category detection. BASELINE is a phrase-based machine translation system (Moses) (Koehn et al., 2007) which consists of training data comprising a bilingual dataset without preordering. REORDERING(H) and REORDERING(C) are de"
W16-4615,P14-2091,0,0.535961,"ements that are used for representing dropped pronouns (“pro” or “small pro”), controlled elements (“PRO” or “big pro”) and traces of movement (“T” or “trace”). Dropped pronouns are one of the major problems caused on machine translation from the pro-drop language such as Japanese to the non-pro-drop language such as English because it is difficult to produce the correct pronouns on the target side when the pronoun is missing on the source side. The effects of empty categories in machine translation have previously been examined (Chung and Gildea, 2010; Taira et al., 2012; Xiang et al., 2013; Kudo et al., 2014; Wang et al., 2016). In this paper, we address two new problems that were not fully discussed in previous work. The first problem is that, even if empty categories are correctly recovered, it is difficult to automatically obtain the correct word alignment for languages with a completely different word order such as Japanese and English. The second problem is that it is not only difficult to translate non-existent pronouns but also relative clauses because relative pronouns do not exist in Japanese. In theory, we can safely ignore PRO in control structures for translation because they are abse"
W16-4615,1995.tmi-1.7,0,0.321952,"improve the accuracy of machine translation. In the following sections, we first briefly describe related works. We then describe empty category detection method (Takeno et al., 2015) and discriminative preordering method (Hoshino et al., 2015) used in the proposed method. We then report experiment results of Japanese-to-English translation on both spoken (IWSLT dataset) and written (KFTT dataset) languages. 2 Related works Conventional approaches to recover zero pronouns in Japanese are to frame it as zero anaphora resolution, which is a sub-problem of predicate argument structure analysis (Nakaiwa and Ikehara, 1995; Iida et al., 2007; Hangyo et al., 2013). Zero anaphora resolution consists of two procedures: zero pronoun detection and anaphora resolution. It is difficult to integrate zero anaphora resolution (or predicate-argument structure analysis) into SMT for two reasons. The first is that anaphora resolution requires context analysis, which complicates the translation method. The second is that predicate argument structure analysis provides semantic relations, not syntactic structure. This makes it difficult to use the information of recovered zero pronouns in SMT, because there is no position info"
W16-4615,P02-1040,0,0.0948532,"sts of training data comprising a bilingual dataset without preordering. REORDERING(H) and REORDERING(C) are described in the previous section. For REORDERING(H), 5,319 sentences with manual word alignment 2 http://taku910.github.io/mecab/unidic 161 Figure 3: Characteristic of machine translation evaluation scores to empty categories filtered for development set of the IWSLT dataset is used. These systems are equivalent to Hoshino et al., (2015)’s method. They are taken from both the spoken language (CSJ) and written (KTC) language corpus. As for evaluation measures, we use the standard BLEU (Papineni et al., 2002) as well as RIBES (Isozaki et al., 2010), which is a rank correlation based metric that has been shown to be highly correlated with human evaluations of machine translation systems between languages with a very different word order such as Japanese and English. Result of filtering empty categories In this experiment, we search for the best threshold value to filter out empty categories in Sec 3. Changing the threshold values θ from 0.50 to 1.0, we measure both BLEU and RIBES, where θ = 1.0 corresponds to the result produces by machine translation systems trained from a dataset without empty ca"
W16-4615,W12-4213,1,0.892896,"mpty categories are phonetically null elements that are used for representing dropped pronouns (“pro” or “small pro”), controlled elements (“PRO” or “big pro”) and traces of movement (“T” or “trace”). Dropped pronouns are one of the major problems caused on machine translation from the pro-drop language such as Japanese to the non-pro-drop language such as English because it is difficult to produce the correct pronouns on the target side when the pronoun is missing on the source side. The effects of empty categories in machine translation have previously been examined (Chung and Gildea, 2010; Taira et al., 2012; Xiang et al., 2013; Kudo et al., 2014; Wang et al., 2016). In this paper, we address two new problems that were not fully discussed in previous work. The first problem is that, even if empty categories are correctly recovered, it is difficult to automatically obtain the correct word alignment for languages with a completely different word order such as Japanese and English. The second problem is that it is not only difficult to translate non-existent pronouns but also relative clauses because relative pronouns do not exist in Japanese. In theory, we can safely ignore PRO in control structure"
W16-4615,D15-1156,1,0.756541,"ld worsen automatic word alignment and result in less accurate machine translation outputs. We solve this problem by integrating empty category detection into preordering-based statistical machine translation. We first insert empty categories into the source sentence, and then reorder them such that the word order is similar to that of the target sentence. We find it is effective to filter out unreliable empty category candidates to improve the accuracy of machine translation. In the following sections, we first briefly describe related works. We then describe empty category detection method (Takeno et al., 2015) and discriminative preordering method (Hoshino et al., 2015) used in the proposed method. We then report experiment results of Japanese-to-English translation on both spoken (IWSLT dataset) and written (KFTT dataset) languages. 2 Related works Conventional approaches to recover zero pronouns in Japanese are to frame it as zero anaphora resolution, which is a sub-problem of predicate argument structure analysis (Nakaiwa and Ikehara, 1995; Iida et al., 2007; Hangyo et al., 2013). Zero anaphora resolution consists of two procedures: zero pronoun detection and anaphora resolution. It is difficult"
W16-4615,uchimoto-den-2008-word,0,0.0179019,"ade from the “Japanese-English Bilingual Corpus of Wikipedia’s Kyoto Articles”, which is created by manually translating Japanese Wikipedia articles related to Kyoto City into English. The dataset consists of 440,000, 1,235, and 1,160 sentences for training, tuning, and testing, respectively. We built the preordering model by applying the empty category detection method to source Japanese sentences to obtain syntactic trees with empty categories, as described in the previous section. We achieved this by first tokenizing Japanese sentences by using a CRF-based tokenizing and chunking software (Uchimoto and Den, 2008) to obtain the long unit words required by the Japanese parser (Hayashi et al., 2016). We then achieved word alignment by using short unit words in Japanese obtained by using the MeCab morphological analyzer with the UniDic dictionary2 . For the Japanese-to-English translation experiment, we used a phrase-based translation model (Koehn et al., 2007). For all systems we compared, the language model is a 5-gram KenLM (Heafield, 2011), which uses modified Kneser-Ney smoothing and tuning is performed to maximize the BLEU score using minimum error rate training (Och, 2007). Other configurable setti"
W16-4615,N16-1113,0,0.556293,"d for representing dropped pronouns (“pro” or “small pro”), controlled elements (“PRO” or “big pro”) and traces of movement (“T” or “trace”). Dropped pronouns are one of the major problems caused on machine translation from the pro-drop language such as Japanese to the non-pro-drop language such as English because it is difficult to produce the correct pronouns on the target side when the pronoun is missing on the source side. The effects of empty categories in machine translation have previously been examined (Chung and Gildea, 2010; Taira et al., 2012; Xiang et al., 2013; Kudo et al., 2014; Wang et al., 2016). In this paper, we address two new problems that were not fully discussed in previous work. The first problem is that, even if empty categories are correctly recovered, it is difficult to automatically obtain the correct word alignment for languages with a completely different word order such as Japanese and English. The second problem is that it is not only difficult to translate non-existent pronouns but also relative clauses because relative pronouns do not exist in Japanese. In theory, we can safely ignore PRO in control structures for translation because they are absent from both Japanes"
W16-4615,P13-1081,0,0.622535,"phonetically null elements that are used for representing dropped pronouns (“pro” or “small pro”), controlled elements (“PRO” or “big pro”) and traces of movement (“T” or “trace”). Dropped pronouns are one of the major problems caused on machine translation from the pro-drop language such as Japanese to the non-pro-drop language such as English because it is difficult to produce the correct pronouns on the target side when the pronoun is missing on the source side. The effects of empty categories in machine translation have previously been examined (Chung and Gildea, 2010; Taira et al., 2012; Xiang et al., 2013; Kudo et al., 2014; Wang et al., 2016). In this paper, we address two new problems that were not fully discussed in previous work. The first problem is that, even if empty categories are correctly recovered, it is difficult to automatically obtain the correct word alignment for languages with a completely different word order such as Japanese and English. The second problem is that it is not only difficult to translate non-existent pronouns but also relative clauses because relative pronouns do not exist in Japanese. In theory, we can safely ignore PRO in control structures for translation be"
W16-4615,P03-1021,0,\N,Missing
W16-4621,P15-2023,1,0.730929,"-classification-based model for pre-ordering (Jehl et al., 2014), instead of Ranking SVMs (Yang et al., 2012) that we used the last year. An advantage of pairwise classification is that we can use features defined on every node pair, while we can only use node-wise features with Ranking SVMs. We found that the pairwise-based method gave slightly better pre-ordering performance than the Ranking SVMs in our pilot test, as did Jehl et al. (2014). We also renewed the features for this year’s system. We used span-based features (word and part-ofspeech sequences over dependency sub-structures) like Hoshino et al. (2015), word and part-of-speech n-grams (n=2,3,4) including head word annotations, and those described in Jehl et al. (2014). Since these features are very sparse, we chose those appearing more than twice in the training parallel data. The reordering oracles were determined to maximize Kendall’s τ over automatic word alignment in a similar manner to Hoshino et al. (2015). We used the intersection of bidirectional automatic word alignment (Nakagawa, 2015). The pairwise formulation enables a simple solution to determine the oracles for which we choose a binary decision, obtaining higher Kendall’s τ wi"
W16-4621,E14-1026,0,0.228,"Missing"
W16-4621,D15-1166,0,0.0489628,"ning parameters. We applied modified Kneser-Ney phrase table smoothing with an additional phrase scoring option: --KneserNey. The model weights were optimized by standard Minimum Error Rate Training (MERT), but we compared five independent MERT runs and chose the best weights for the development test set. The distortion limit was 9 for both the baseline and pre-ordering conditions, chosen from 0, 3, 6, and 9 by comparing the results of the MERT runs. 5.3 Neural MT Setup We also tried a recent neural MT for comparison with a phrase-based MT. We used a sequence-tosequence attentional neural MT (Luong et al., 2015) implemented by the Harvard NLP group1 with a vocabulary size of 50,000 and a 2-layer bidirectional LSTM with 500 hidden units on both the encoder/decoder2 . The neural MT, which was word-based with the same tokenizer used in the phrase-based MT setting, did not employ recent subword-based or character-based methods. The training time of the neural MT was about two days (13 epochs with 3.5 hours/epoch) with a NVIDIA Tesla K80 GPU. The decoding employed a beam search with a beam size of five and dictionary-based unknown word mapping with the IBM-4 lexical translation table obtained by MGIZA++."
W16-4621,P15-1021,0,0.100791,"newed the features for this year’s system. We used span-based features (word and part-ofspeech sequences over dependency sub-structures) like Hoshino et al. (2015), word and part-of-speech n-grams (n=2,3,4) including head word annotations, and those described in Jehl et al. (2014). Since these features are very sparse, we chose those appearing more than twice in the training parallel data. The reordering oracles were determined to maximize Kendall’s τ over automatic word alignment in a similar manner to Hoshino et al. (2015). We used the intersection of bidirectional automatic word alignment (Nakagawa, 2015). The pairwise formulation enables a simple solution to determine the oracles for which we choose a binary decision, obtaining higher Kendall’s τ with and without swapping every node pair. 5 Evaluation 5.1 Pre-ordering Setup The pre-ordering model for the data-driven method was trained over the MGIZA++ word alignment used for the phrase tables described later. We trained a logistic-regression-based binary classification model 212 using the reordering oracles over training data with LIBLINEAR (version 2.1). Hyperparameter c was set to 0.01, chosen by the binary classification accuracy on the de"
W16-4621,W15-5012,1,0.80914,"ne translation is the complex syntactic structure of patent documents, which typically have long sentences that complicate MT reordering, especially for word order in distant languages. Chinese and Japanese have similar word order in noun modifiers but different subject-verb-object order, requiring long distance reordering in translation. In the WAT 2016 evaluation campaign (Nakazawa et al., 2016), we participated in a Chinese-to-Japanese patent translation task and tackled long distance reordering by syntactic pre-ordering based on Chinese dependency structures, as in our last year’s system (Sudoh and Nagata, 2015). We also use a recent neural MT as the following MT implementation for comparison with a traditional phrase-based statistical MT. Our system basically consists of three components: Chinese syntactic analysis (word segmentation, part-of-speech (POS) tagging, and dependency parsing) adapted to patent documents; dependency-based syntactic pre-ordering with hand-written rules or a learning-to-rank model; and the following MT component (phrase-based MT or neural MT). This paper describes our system’s details and discusses our evaluation results. 2 System Overview Figure 1 shows a brief workflow of"
W16-4621,D09-1058,0,0.0285827,"inese dependency parsing Dep. models Chinese language resource (patent) Dependency-based syntactic pre-ordeirng Phrase-based or Neural MT Pre-ordering model MT models Supplied parallel text Japanese sentence Figure 1: Brief workflow of our MT system. Gray-colored resource is an in-house one. Chinese analysis models were trained using an in-house Chinese treebank of about 35,000 sentences in the patent domain (Sudoh et al., 2014) as well as the standard Penn Chinese Treebank dataset. The training also utilized unlabeled Chinese patent documents (about 100 G bytes) for semi-supervised training (Suzuki et al., 2009; Sudoh et al., 2014). 4 Syntactic Pre-ordering Data-driven pre-ordering obtains the most probable reordering of a source language sentence that is monotone with the target language counterpart. It learns rules or models using reordering oracles over word-aligned bilingual corpora. We used a pairwise-classification-based model for pre-ordering (Jehl et al., 2014), instead of Ranking SVMs (Yang et al., 2012) that we used the last year. An advantage of pairwise classification is that we can use features defined on every node pair, while we can only use node-wise features with Ranking SVMs. We fo"
W16-4621,P12-1096,0,0.017906,"(Sudoh et al., 2014) as well as the standard Penn Chinese Treebank dataset. The training also utilized unlabeled Chinese patent documents (about 100 G bytes) for semi-supervised training (Suzuki et al., 2009; Sudoh et al., 2014). 4 Syntactic Pre-ordering Data-driven pre-ordering obtains the most probable reordering of a source language sentence that is monotone with the target language counterpart. It learns rules or models using reordering oracles over word-aligned bilingual corpora. We used a pairwise-classification-based model for pre-ordering (Jehl et al., 2014), instead of Ranking SVMs (Yang et al., 2012) that we used the last year. An advantage of pairwise classification is that we can use features defined on every node pair, while we can only use node-wise features with Ranking SVMs. We found that the pairwise-based method gave slightly better pre-ordering performance than the Ranking SVMs in our pilot test, as did Jehl et al. (2014). We also renewed the features for this year’s system. We used span-based features (word and part-ofspeech sequences over dependency sub-structures) like Hoshino et al. (2015), word and part-of-speech n-grams (n=2,3,4) including head word annotations, and those d"
W17-5702,2016.amta-researchers.10,0,0.0592766,"Missing"
W17-5702,P17-2061,0,0.0395071,"Missing"
W17-5702,D10-1062,0,0.0907421,"h as zero pronouns (dropped subject and object) in Japanese and expletives in English (there in thereconstruction, do in interrogative sentence, it in formal subject, etc.). In machine translation, unaligned words in target sentence are problematic because the information required for translation is not explicitly present in the source sentence. There are many works that aim at improving machine translation performance by supplementing unaligned words, but they focus on specific linguistic phenomena such as Japanese case marker (Hisami and Suzuki, 2007), Chinese zero pronoun (empty category) (Chung and Gildea, 2010; 3.3 Domain Adaptation The third example encodes dataset names of a bilingual text for domain adaptation. Here, IWSLT is a travel expression corpus and KFTT is a corpus of Japanese Wikipedia pages on Kyoto and its English translation. 朝食はいくらですか。 → #IWSLT How much is the breakfast ? 妙法蓮華経を根本経典とする。 → #KFTT Its fundamental sutra is lotus sutra . Kobus et al. (2016) proposed a domain adaptation method using side constraints. They used a separate classifier for predicting the domain of a sentence before translation if it is not known. Li et al. (2016) used Speaker IDs of Twitter to 57 Xiang et al."
W17-5702,2005.iwslt-1.1,0,0.0147211,"aiting for you that day . (5) 5 Experiment Our preliminary experiment showed that the scores yielded by Eq. (5) are not reliable for low frequency target words. We therefore use the following equation to filter out low frequency NULLgenerated target words. 5.1 Datasets and Tools 4.3 Prefix Constraints for Unaligned Target Words The experiments used five publicly available Japanese-English parallel corpora, namely IWSLT-2005, KFTT, GVOICES, REUTERS, and TATOEBA, as shown in Table 2. IWSLT-2005 is a dataset for Japanese-English Tasks of the International Workshop on Spoken Language Translation (Eck and Hori, 2005). It is available from ALAGIN2 . KFTT (Kyoto Free Translation Task) is a Japanese-English translation task on Wikipedia articles related to Kyoto3 . Parallel Global Voices is a multilingual corpus created from Global Voices websites which translate social media and blogs (Prokopidis et al., 2016). Tatoeba is a collection of multilingual translated example sentences from Tatoeba website. These last two are available from OPUS (Tiedemann, 2012). Reuters are Japanese-English parallel corpus made by aligning Reuters RCV1 RCV2 multilingual text categorization test collection data set (RCV1 for Engl"
W17-5702,N07-1007,0,0.0160352,"n be caused by specific linguistic phenomena in one language, such as zero pronouns (dropped subject and object) in Japanese and expletives in English (there in thereconstruction, do in interrogative sentence, it in formal subject, etc.). In machine translation, unaligned words in target sentence are problematic because the information required for translation is not explicitly present in the source sentence. There are many works that aim at improving machine translation performance by supplementing unaligned words, but they focus on specific linguistic phenomena such as Japanese case marker (Hisami and Suzuki, 2007), Chinese zero pronoun (empty category) (Chung and Gildea, 2010; 3.3 Domain Adaptation The third example encodes dataset names of a bilingual text for domain adaptation. Here, IWSLT is a travel expression corpus and KFTT is a corpus of Japanese Wikipedia pages on Kyoto and its English translation. 朝食はいくらですか。 → #IWSLT How much is the breakfast ? 妙法蓮華経を根本経典とする。 → #KFTT Its fundamental sutra is lotus sutra . Kobus et al. (2016) proposed a domain adaptation method using side constraints. They used a separate classifier for predicting the domain of a sentence before translation if it is not known."
W17-5702,Q17-1024,0,0.0940404,"Missing"
W17-5702,D16-1140,0,0.0869114,".ntt.co.jp Abstract length (Kikuchi et al., 2016), and target language (Johnson et al., 2016). Meta-textual information include dialogue act (Wen et al., 2015), user personality (Li et al., 2016), topic (Chen et al., 2016), and domain (Kobus et al., 2016) Two approaches can be used to provide additional information to the encoder-decoder model, word-level methods and sentence-level methods. Word-level methods encode the additional information as a vector (embedding) that is input together with a word at each time step of either (or both) encoder and decoder (Wen et al., 2015; Li et al., 2016; Kikuchi et al., 2016). Sentence level methods encode the additional information as special tokens. Side constraints are placed at the end of source sentence (Sennrich et al., 2016; Johnson et al., 2016; Yamagishi et al., 2016), while our proposal, prefix constraints, is placed at the beginning of target sentence. The advantage of sentence-level methods over word-level methods is their simplicity in application. The network structure of the underlying encoder-decoder model does not have to be modified. The problem with side constraints is that, at test time, additional information must be either specified by the us"
W17-5702,P07-2045,0,0.00613964,"age corpora as the IWSLT-2005 dataset is very small. One is the Daionsen parallel sentence database, made by Straightword Inc5 , which is a phrase book for daily conversation. It has 50,709 sentences with 431,258 words in English and 471,677 words in Japanese. The other is the HIT (Harbin Institute of Technology) parallel corpus (Yang et al., 2006) developed for speech translation. It is a collection of 62,727 sentences with 635,809 words in English and 796,200 words in Japanese. We call this dataset IWSLT2005+EXTRA. English sentences are tokenized and lowercased by the scripts used in Moses (Koehn et al., 2007). Japanese sentences are normalized by NFKC (a unicode normalization form) and word segmented by MeCab6 with UniDic. For neural 5 6 Baseline Side Constraints Prefix Constraints None 34.8 33.0 31.7 Oracle 35.4 35.7 Table 3: Comparison between side constraints and prefix constraints on length control As shown in Table 3, Prefix Constraints are comparable to or better than Side Constraints in controlling the length of the target sentence if the correct length is known and provided as an oracle. It is difficult to predict the length of target sentence from source sentence, which lowered the achttp"
W17-5702,P14-2091,0,0.0150655,"lingual text for domain adaptation. Here, IWSLT is a travel expression corpus and KFTT is a corpus of Japanese Wikipedia pages on Kyoto and its English translation. 朝食はいくらですか。 → #IWSLT How much is the breakfast ? 妙法蓮華経を根本経典とする。 → #KFTT Its fundamental sutra is lotus sutra . Kobus et al. (2016) proposed a domain adaptation method using side constraints. They used a separate classifier for predicting the domain of a sentence before translation if it is not known. Li et al. (2016) used Speaker IDs of Twitter to 57 Xiang et al., 2013; Wang et al., 2016), Japanese zero pronoun (Taira et al., 2012; Kudo et al., 2014) and English determiner (Tsvetkov et al., 2013). There are no language independent methods that can cope with unaligned target words. A special token, #GO, is added to delimit the variable length prefix relative to target sentence. In the following examples, words with underline indicate unaligned target words. 赤ワインを頂けますか。 →#i #GO may i have some red wine ? では当日御待ちして居ります。 →#we #you #GO we are waiting for you that day . 4.2 Identifying Unaligned Target Words We first propose a language independent method for automatically identifying unaligned target words. We assume word alignment is given for"
W17-5702,D15-1199,0,0.0663687,"Missing"
W17-5702,N16-1046,0,0.0374182,"Missing"
W17-5702,P16-1007,0,0.210826,"nce. The advantage of sentence-level methods over word-level methods is their simplicity in application. The network structure of the underlying encoder-decoder model does not have to be modified. The problem with side constraints is that, at test time, additional information must be either specified by the user or automatically predicted by some other method. As prefix constraints move the special tokens from source to target, they can be predicted by the network jointly with target sentence. Like side constraints, the user can specify prefix constraints by using prefix-constrained decoding (Wuebker et al., 2016), which can be implemented by a trivial modification of the decoder. The following sections start by describing the framework of prefix constraints. We then show three simple use cases, namely, length control, bidirectional decoding, and domain adaptation. We then show a more sophisticated usage of prefix constraints: unaligned target word generation. We propose prefix constraints, a novel method to enforce constraints on target sentences in neural machine translation. It places a sequence of special tokens at the beginning of target sentence (target prefix), while side constraints (Sennrich e"
W17-5702,2015.iwslt-evaluation.11,0,0.0431552,"length once at the initial state of the decoder. They designed a dedicated network structure for each method. In spirit, our method is similar to the LenInit method, but we don’t have to modify the underlying network structure. Note that we do not tell the network that ’#3’ is the length of the target sentence. The network automatically learns the meaning of the symbol from the regularity of the training data and then calculates its embedding. add personality to a conversational agent. Speaker embeddings are learned jointly with word embeddings and entered into the decoder at each time step. Luong and Manning (2015) proposed a domain adaptation method based on fine tuning in which an out-of-domain model is further trained on in-domain data. Our method can automatically predict domain jointly with target sentence. We don’t have to change the underlying network structure and domain embeddings are jointly learned with word embeddings as a part of target vocabulary. One of the potential benefits of our method is that only one model is made and used for all domains. If multiple domains must be supported, the methods based on fine tuning (Luong and Manning, 2015) have to make a model for each domain. 3.2 Bidir"
W17-5702,P13-1081,0,0.0411866,"Gildea, 2010; 3.3 Domain Adaptation The third example encodes dataset names of a bilingual text for domain adaptation. Here, IWSLT is a travel expression corpus and KFTT is a corpus of Japanese Wikipedia pages on Kyoto and its English translation. 朝食はいくらですか。 → #IWSLT How much is the breakfast ? 妙法蓮華経を根本経典とする。 → #KFTT Its fundamental sutra is lotus sutra . Kobus et al. (2016) proposed a domain adaptation method using side constraints. They used a separate classifier for predicting the domain of a sentence before translation if it is not known. Li et al. (2016) used Speaker IDs of Twitter to 57 Xiang et al., 2013; Wang et al., 2016), Japanese zero pronoun (Taira et al., 2012; Kudo et al., 2014) and English determiner (Tsvetkov et al., 2013). There are no language independent methods that can cope with unaligned target words. A special token, #GO, is added to delimit the variable length prefix relative to target sentence. In the following examples, words with underline indicate unaligned target words. 赤ワインを頂けますか。 →#i #GO may i have some red wine ? では当日御待ちして居ります。 →#we #you #GO we are waiting for you that day . 4.2 Identifying Unaligned Target Words We first propose a language independent method for auto"
W17-5702,D15-1166,0,0.351218,"17. 2017 AFNLP 2 Encoder-Decoder Model with Prefix Constraints 2.3 Prefix Constraints In our proposed method, a sequence of special tokens is placed at the beginning of the target sentence. In other words, they are the prefix to the extended target sentence. Let a sequence of features extracted from a pair of source sentence x and target sentence y be c = c1 . . . ck , and extended target sentence be y ˜ = cy. The baseline encoder-decoder model Eq. (1) is extended as follows. 2.1 Encoder-Decoder Model First, we briefly describe the attention-based encoder-decoder model (Bahdanau et al., 2015; Luong et al., 2015), which is the state-of-the-art neural machine translation method and the baseline of this study. Given input sequence x = x1 . . . xn and model parameters θ, the encoder-decoder model formulates the likelihood of the output sequences y = y1 . . . ym as follows: ∑m log p(y|x) = log p (yj |y<j , x; θ) (1) log p(˜ y |x) = log p(c|x) + log p(y|x, c) log p(c|x) = j=1 log p(y|x, c) = The encoder is a recurrent neural network (RNN) which projects input sequence x into a sequence of hidden states h = h1 . . . hn via non-linear transformation. The decoder is another RNN which predicts target words y s"
W17-5702,W16-4620,0,0.243908,"(Chen et al., 2016), and domain (Kobus et al., 2016) Two approaches can be used to provide additional information to the encoder-decoder model, word-level methods and sentence-level methods. Word-level methods encode the additional information as a vector (embedding) that is input together with a word at each time step of either (or both) encoder and decoder (Wen et al., 2015; Li et al., 2016; Kikuchi et al., 2016). Sentence level methods encode the additional information as special tokens. Side constraints are placed at the end of source sentence (Sennrich et al., 2016; Johnson et al., 2016; Yamagishi et al., 2016), while our proposal, prefix constraints, is placed at the beginning of target sentence. The advantage of sentence-level methods over word-level methods is their simplicity in application. The network structure of the underlying encoder-decoder model does not have to be modified. The problem with side constraints is that, at test time, additional information must be either specified by the user or automatically predicted by some other method. As prefix constraints move the special tokens from source to target, they can be predicted by the network jointly with target sentence. Like side constra"
W17-5702,J03-1002,0,0.00880432,"target words: LEX and COUNT. LEX places a sequence of unaligned target words at the beginning of the target sentence in the same order they appear in the target sentence. The COUNT feature can be thought of a substitute for the fertility of the IBM model (Brown et al., 1993), or the generative model for NULL-generated target words (Schulz et al., 2016). 2 http://alagin.jp/ 3 http://www.phontron.com/kftt/index.html 4 The aligned parallel corpus is available from the homepage of the first author of (Utiyama and Isahara, 2003) Su (w) = p(e = w|f = N U LL) ∗ p(f = N U LL|e = w) (6) We use GIZA++ (Och and Ney, 2003) to obtain word alignment for both translation directions. Word alignment is symmetrized by intersection heuristics, because the word alignment obtained by grow-diag-final-and, is noisy for unaligned words. Table 1 shows the top 50 unaligned target words as determined by Eq. (6) in the IWSLT-2005 Japanese-to-English translation dataset, which is described in the experiment section. We can see that the automatically extracted unaligned target words include zero pronouns (i, you, it), articles (a, the), light verbs (take, get, make), and expletives (do, does). 1 58 i the a you , it to for do ple"
W17-5702,P02-1040,0,0.0983736,"in dev test train dev test train dev test train dev test train dev test Sents. 19,972 506 1,000 440,288 1,235 1,160 43,488 1,000 1,000 54,011 1,000 1,000 185,426 1,000 1,000 753,185 4,741 5,160 len.(ja) 9.9 8.1 8.2 27.0 27.8 24.5 26.3 25.1 28.7 34.3 34.4 34.6 10.1 10.2 11.8 23.3 23.2 22.2 len.(en) 9.4 7.5 7.6 26.3 25.1 23.5 19.8 18.9 21.2 25.2 25.2 25.5 9.14 9.21 9.23 21.2 18.6 17.5 machine translation, we used seq2seq-attn7 , which implements an attention-based encoder-decoder (Luong et al., 2015). We used default settings unless otherwise specified. Translation accuracy is measured by BLEU (Papineni et al., 2002). 5.2 Length Control Table 3 compares side constraints with prefix constraints in terms of length control for IWSLT-2005 dataset. Baseline is a NMT system trained on the parallel corpus without length tag. Side Constraints and Prefix Constraints stand for NMT systems trained on the corpus with length tags placed at the end of source sentence and at the begging of target sentence, respectively. In None, source sentences without length tag are entered into the system at test time. In Oracle, reference length is encoded as length tag and prefix constrained decoding is used in Prefix Constraints."
W17-5702,L16-1144,0,0.0159862,"onstraints for Unaligned Target Words The experiments used five publicly available Japanese-English parallel corpora, namely IWSLT-2005, KFTT, GVOICES, REUTERS, and TATOEBA, as shown in Table 2. IWSLT-2005 is a dataset for Japanese-English Tasks of the International Workshop on Spoken Language Translation (Eck and Hori, 2005). It is available from ALAGIN2 . KFTT (Kyoto Free Translation Task) is a Japanese-English translation task on Wikipedia articles related to Kyoto3 . Parallel Global Voices is a multilingual corpus created from Global Voices websites which translate social media and blogs (Prokopidis et al., 2016). Tatoeba is a collection of multilingual translated example sentences from Tatoeba website. These last two are available from OPUS (Tiedemann, 2012). Reuters are Japanese-English parallel corpus made by aligning Reuters RCV1 RCV2 multilingual text categorization test collection data set (RCV1 for English and RCV2 for other languages) available from NIST (Utiyama and Isahara, 2003)4 . The unaligned target word generation experiments used two additional proprietary spoken We propose here two types of prefix constraints for improving the translation of unaligned target words: LEX and COUNT. LEX"
W17-5702,P16-2028,0,0.0355694,"Missing"
W17-5702,N16-1005,0,0.46565,"), user personality (Li et al., 2016), topic (Chen et al., 2016), and domain (Kobus et al., 2016) Two approaches can be used to provide additional information to the encoder-decoder model, word-level methods and sentence-level methods. Word-level methods encode the additional information as a vector (embedding) that is input together with a word at each time step of either (or both) encoder and decoder (Wen et al., 2015; Li et al., 2016; Kikuchi et al., 2016). Sentence level methods encode the additional information as special tokens. Side constraints are placed at the end of source sentence (Sennrich et al., 2016; Johnson et al., 2016; Yamagishi et al., 2016), while our proposal, prefix constraints, is placed at the beginning of target sentence. The advantage of sentence-level methods over word-level methods is their simplicity in application. The network structure of the underlying encoder-decoder model does not have to be modified. The problem with side constraints is that, at test time, additional information must be either specified by the user or automatically predicted by some other method. As prefix constraints move the special tokens from source to target, they can be predicted by the network"
W17-5702,W12-4213,1,0.874792,"ataset names of a bilingual text for domain adaptation. Here, IWSLT is a travel expression corpus and KFTT is a corpus of Japanese Wikipedia pages on Kyoto and its English translation. 朝食はいくらですか。 → #IWSLT How much is the breakfast ? 妙法蓮華経を根本経典とする。 → #KFTT Its fundamental sutra is lotus sutra . Kobus et al. (2016) proposed a domain adaptation method using side constraints. They used a separate classifier for predicting the domain of a sentence before translation if it is not known. Li et al. (2016) used Speaker IDs of Twitter to 57 Xiang et al., 2013; Wang et al., 2016), Japanese zero pronoun (Taira et al., 2012; Kudo et al., 2014) and English determiner (Tsvetkov et al., 2013). There are no language independent methods that can cope with unaligned target words. A special token, #GO, is added to delimit the variable length prefix relative to target sentence. In the following examples, words with underline indicate unaligned target words. 赤ワインを頂けますか。 →#i #GO may i have some red wine ? では当日御待ちして居ります。 →#we #you #GO we are waiting for you that day . 4.2 Identifying Unaligned Target Words We first propose a language independent method for automatically identifying unaligned target words. We assume word al"
W17-5702,tiedemann-2012-parallel,0,0.0358316,"RS, and TATOEBA, as shown in Table 2. IWSLT-2005 is a dataset for Japanese-English Tasks of the International Workshop on Spoken Language Translation (Eck and Hori, 2005). It is available from ALAGIN2 . KFTT (Kyoto Free Translation Task) is a Japanese-English translation task on Wikipedia articles related to Kyoto3 . Parallel Global Voices is a multilingual corpus created from Global Voices websites which translate social media and blogs (Prokopidis et al., 2016). Tatoeba is a collection of multilingual translated example sentences from Tatoeba website. These last two are available from OPUS (Tiedemann, 2012). Reuters are Japanese-English parallel corpus made by aligning Reuters RCV1 RCV2 multilingual text categorization test collection data set (RCV1 for English and RCV2 for other languages) available from NIST (Utiyama and Isahara, 2003)4 . The unaligned target word generation experiments used two additional proprietary spoken We propose here two types of prefix constraints for improving the translation of unaligned target words: LEX and COUNT. LEX places a sequence of unaligned target words at the beginning of the target sentence in the same order they appear in the target sentence. The COUNT f"
W17-5702,W13-2234,0,0.0154563,"WSLT is a travel expression corpus and KFTT is a corpus of Japanese Wikipedia pages on Kyoto and its English translation. 朝食はいくらですか。 → #IWSLT How much is the breakfast ? 妙法蓮華経を根本経典とする。 → #KFTT Its fundamental sutra is lotus sutra . Kobus et al. (2016) proposed a domain adaptation method using side constraints. They used a separate classifier for predicting the domain of a sentence before translation if it is not known. Li et al. (2016) used Speaker IDs of Twitter to 57 Xiang et al., 2013; Wang et al., 2016), Japanese zero pronoun (Taira et al., 2012; Kudo et al., 2014) and English determiner (Tsvetkov et al., 2013). There are no language independent methods that can cope with unaligned target words. A special token, #GO, is added to delimit the variable length prefix relative to target sentence. In the following examples, words with underline indicate unaligned target words. 赤ワインを頂けますか。 →#i #GO may i have some red wine ? では当日御待ちして居ります。 →#we #you #GO we are waiting for you that day . 4.2 Identifying Unaligned Target Words We first propose a language independent method for automatically identifying unaligned target words. We assume word alignment is given for a bilingual sentence pair, where NULL represen"
W17-5702,P03-1010,0,0.024974,"lation Task) is a Japanese-English translation task on Wikipedia articles related to Kyoto3 . Parallel Global Voices is a multilingual corpus created from Global Voices websites which translate social media and blogs (Prokopidis et al., 2016). Tatoeba is a collection of multilingual translated example sentences from Tatoeba website. These last two are available from OPUS (Tiedemann, 2012). Reuters are Japanese-English parallel corpus made by aligning Reuters RCV1 RCV2 multilingual text categorization test collection data set (RCV1 for English and RCV2 for other languages) available from NIST (Utiyama and Isahara, 2003)4 . The unaligned target word generation experiments used two additional proprietary spoken We propose here two types of prefix constraints for improving the translation of unaligned target words: LEX and COUNT. LEX places a sequence of unaligned target words at the beginning of the target sentence in the same order they appear in the target sentence. The COUNT feature can be thought of a substitute for the fertility of the IBM model (Brown et al., 1993), or the generative model for NULL-generated target words (Schulz et al., 2016). 2 http://alagin.jp/ 3 http://www.phontron.com/kftt/index.html"
W17-5702,N16-1113,0,0.0369129,"main Adaptation The third example encodes dataset names of a bilingual text for domain adaptation. Here, IWSLT is a travel expression corpus and KFTT is a corpus of Japanese Wikipedia pages on Kyoto and its English translation. 朝食はいくらですか。 → #IWSLT How much is the breakfast ? 妙法蓮華経を根本経典とする。 → #KFTT Its fundamental sutra is lotus sutra . Kobus et al. (2016) proposed a domain adaptation method using side constraints. They used a separate classifier for predicting the domain of a sentence before translation if it is not known. Li et al. (2016) used Speaker IDs of Twitter to 57 Xiang et al., 2013; Wang et al., 2016), Japanese zero pronoun (Taira et al., 2012; Kudo et al., 2014) and English determiner (Tsvetkov et al., 2013). There are no language independent methods that can cope with unaligned target words. A special token, #GO, is added to delimit the variable length prefix relative to target sentence. In the following examples, words with underline indicate unaligned target words. 赤ワインを頂けますか。 →#i #GO may i have some red wine ? では当日御待ちして居ります。 →#we #you #GO we are waiting for you that day . 4.2 Identifying Unaligned Target Words We first propose a language independent method for automatically identifyin"
W17-5706,W16-4616,0,0.0209001,"rpus. mance. In an ensembling process, several models are run at each time step and an arithmetic mean of predicted probability is obtained, which is used to determine the next word. In our settings, we trained eight models independently and used them for the ensemble. 2.4 Testing 2.4.1 Length Normalized Re-ranking Naive beam searches with a large beam size may tend to output shorter sentences, leading to a drop in performance (Tu et al., 2017). To reduce this negative effect, we re-ranked the candidate output sentences t by using the following score function once we finished the beam search (Cromieres et al., 2016): } { p(t) ˆ t = arg max , (1) |t| t∈t 3 Task-Specific Settings 3.1 ASPEC 3.1.1 Synthetic Corpus As we mentioned in section 2.2, ASPEC contains some unreliable sentence pairs. For SMT, we can use these sentences as monolingual data to train a language model. However in the current NMT model architecture, the model cannot be trained with monolingual data, so the previous participants with NMT models simply ignored these parts of the data (Neubig, 2016; Eriguchi et al., 2016). In a way similar to that reported by Sennrich et al. Sennrich et al. (2016b), we tried to use the unreliable part of the"
W17-5706,W16-4617,0,0.0185076,"t, we re-ranked the candidate output sentences t by using the following score function once we finished the beam search (Cromieres et al., 2016): } { p(t) ˆ t = arg max , (1) |t| t∈t 3 Task-Specific Settings 3.1 ASPEC 3.1.1 Synthetic Corpus As we mentioned in section 2.2, ASPEC contains some unreliable sentence pairs. For SMT, we can use these sentences as monolingual data to train a language model. However in the current NMT model architecture, the model cannot be trained with monolingual data, so the previous participants with NMT models simply ignored these parts of the data (Neubig, 2016; Eriguchi et al., 2016). In a way similar to that reported by Sennrich et al. Sennrich et al. (2016b), we tried to use the unreliable part of the corpus by making a synthetic corpus. Figure 1 illustrates the overview of how we made the synthetic corpus. First, we made an NMT model with the reliable part of the provided data (in our case, the first 2.0M sentences), then translated the unreliable part of the corpus by using it to make a synthetic corpus. Finally, we made a corpus of 3.0M sentences by concatenating this where p(t) is the predicted log-probability of a candidate output sentence t and |t |is the length o"
W17-5706,2015.iwslt-evaluation.11,0,0.0335328,"I 5.2 Model Fine-tuning 3.2.1 Model Fine-tuning We thought that training with a larger amount of data would enable the model to use more sentences and that this would be beneficial for further training. However, as is clear from Table 3, we couldn’t find any improvements over fine-tuning. We suspect that the parallel corpus used to initialize the model is quite out-of-domain, so the model couldn’t get any benefits from it. We thought the JIJI corpus was too small to train an NMT model, so we tried to train the model with other large parallel corpora and then fine-tune it with the JIJI corpus (Luong and Manning, 2015). In our settings, we first trained the model with ASPEC (2.0M) and Japan Patent Office Patent Corpus (JPC) (1.0M). We learned BPE codes with the JIJI corpus and applied them to ASPEC and JPC. We trained the model with ASPEC and JPC for 20 epochs, then continued training with the JIJI corpus for a further 20 epochs. 4 5.3 JIJI Corpus Quality In the JIJI corpus subtasks, we were only able to see a small correlation between BLEU scores and human evaluation. To find out the reason for this, we manually looked into the JIJI corpus. In doing so, we found that it was too noisy for efficient learning"
W17-5706,D15-1166,0,0.0626273,"l Fine-tuning 3.2.1 Model Fine-tuning We thought that training with a larger amount of data would enable the model to use more sentences and that this would be beneficial for further training. However, as is clear from Table 3, we couldn’t find any improvements over fine-tuning. We suspect that the parallel corpus used to initialize the model is quite out-of-domain, so the model couldn’t get any benefits from it. We thought the JIJI corpus was too small to train an NMT model, so we tried to train the model with other large parallel corpora and then fine-tune it with the JIJI corpus (Luong and Manning, 2015). In our settings, we first trained the model with ASPEC (2.0M) and Japan Patent Office Patent Corpus (JPC) (1.0M). We learned BPE codes with the JIJI corpus and applied them to ASPEC and JPC. We trained the model with ASPEC and JPC for 20 epochs, then continued training with the JIJI corpus for a further 20 epochs. 4 5.3 JIJI Corpus Quality In the JIJI corpus subtasks, we were only able to see a small correlation between BLEU scores and human evaluation. To find out the reason for this, we manually looked into the JIJI corpus. In doing so, we found that it was too noisy for efficient learning"
W17-5706,P16-1009,0,0.49681,"the previous state-of-the-art system. We also tried to make use of the unreliable part of the provided parallel corpus by backtranslating and making a synthetic corpus. Our submitted system achieved the new state-of-the-art performance in terms of the BLEU score, as well as human evaluation. 1 2.2 Data Preprocessing First, we tokenize the provided corpus using KyTea (Neubig et al., 2011) for the Japanese side, and Moses tokenizer2 for the English side. We remove the sentences over 60 words to clean the corpus. Then we further split it into sub-words using joint byte pair encoding (joint-BPE) (Sennrich et al., 2016c) with applying 16,000 merge operations. For ASPEC subtasks, though the provided training data contained over 3.0M sentences, we only used the first 2.0M sentences, in the same way as the previous participants (Neubig, 2014). ASPEC was collected by aligning parallel sentences automatically and sorting them on the basis of the alignment confidence score (Nakazawa et al., 2016). This means that the latter side of the corpus may contain noisy parallel sentences, which would have a negative impact on training. We used the latter 1.0M sentences as a monolingual corpus and made a synthetic corpus ("
W17-5706,P16-1162,0,0.84021,"the previous state-of-the-art system. We also tried to make use of the unreliable part of the provided parallel corpus by backtranslating and making a synthetic corpus. Our submitted system achieved the new state-of-the-art performance in terms of the BLEU score, as well as human evaluation. 1 2.2 Data Preprocessing First, we tokenize the provided corpus using KyTea (Neubig et al., 2011) for the Japanese side, and Moses tokenizer2 for the English side. We remove the sentences over 60 words to clean the corpus. Then we further split it into sub-words using joint byte pair encoding (joint-BPE) (Sennrich et al., 2016c) with applying 16,000 merge operations. For ASPEC subtasks, though the provided training data contained over 3.0M sentences, we only used the first 2.0M sentences, in the same way as the previous participants (Neubig, 2014). ASPEC was collected by aligning parallel sentences automatically and sorting them on the basis of the alignment confidence score (Nakazawa et al., 2016). This means that the latter side of the corpus may contain noisy parallel sentences, which would have a negative impact on training. We used the latter 1.0M sentences as a monolingual corpus and made a synthetic corpus ("
W17-5706,W14-7002,0,0.0491336,"ce in terms of the BLEU score, as well as human evaluation. 1 2.2 Data Preprocessing First, we tokenize the provided corpus using KyTea (Neubig et al., 2011) for the Japanese side, and Moses tokenizer2 for the English side. We remove the sentences over 60 words to clean the corpus. Then we further split it into sub-words using joint byte pair encoding (joint-BPE) (Sennrich et al., 2016c) with applying 16,000 merge operations. For ASPEC subtasks, though the provided training data contained over 3.0M sentences, we only used the first 2.0M sentences, in the same way as the previous participants (Neubig, 2014). ASPEC was collected by aligning parallel sentences automatically and sorting them on the basis of the alignment confidence score (Nakazawa et al., 2016). This means that the latter side of the corpus may contain noisy parallel sentences, which would have a negative impact on training. We used the latter 1.0M sentences as a monolingual corpus and made a synthetic corpus (see section 3.1.1 for details). Introduction In this paper, we describe our systems submitted to this year’s translation shared tasks at WAT 2017 (Nakazawa et al., 2017). For this year, we focused on scientific paper (ASPEC J"
W17-5706,W16-4610,0,0.0139501,"negative effect, we re-ranked the candidate output sentences t by using the following score function once we finished the beam search (Cromieres et al., 2016): } { p(t) ˆ t = arg max , (1) |t| t∈t 3 Task-Specific Settings 3.1 ASPEC 3.1.1 Synthetic Corpus As we mentioned in section 2.2, ASPEC contains some unreliable sentence pairs. For SMT, we can use these sentences as monolingual data to train a language model. However in the current NMT model architecture, the model cannot be trained with monolingual data, so the previous participants with NMT models simply ignored these parts of the data (Neubig, 2016; Eriguchi et al., 2016). In a way similar to that reported by Sennrich et al. Sennrich et al. (2016b), we tried to use the unreliable part of the corpus by making a synthetic corpus. Figure 1 illustrates the overview of how we made the synthetic corpus. First, we made an NMT model with the reliable part of the provided data (in our case, the first 2.0M sentences), then translated the unreliable part of the corpus by using it to make a synthetic corpus. Finally, we made a corpus of 3.0M sentences by concatenating this where p(t) is the predicted log-probability of a candidate output sentence t"
W17-5706,P11-2093,0,0.0500419,"further experiments1 . In this year, we participated in four translation subtasks at WAT 2017. Our model structure is quite simple but we used it with well-tuned hyper-parameters, leading to a significant improvement compared to the previous state-of-the-art system. We also tried to make use of the unreliable part of the provided parallel corpus by backtranslating and making a synthetic corpus. Our submitted system achieved the new state-of-the-art performance in terms of the BLEU score, as well as human evaluation. 1 2.2 Data Preprocessing First, we tokenize the provided corpus using KyTea (Neubig et al., 2011) for the Japanese side, and Moses tokenizer2 for the English side. We remove the sentences over 60 words to clean the corpus. Then we further split it into sub-words using joint byte pair encoding (joint-BPE) (Sennrich et al., 2016c) with applying 16,000 merge operations. For ASPEC subtasks, though the provided training data contained over 3.0M sentences, we only used the first 2.0M sentences, in the same way as the previous participants (Neubig, 2014). ASPEC was collected by aligning parallel sentences automatically and sorting them on the basis of the alignment confidence score (Nakazawa et"
W17-5706,W16-2323,0,0.194381,"the previous state-of-the-art system. We also tried to make use of the unreliable part of the provided parallel corpus by backtranslating and making a synthetic corpus. Our submitted system achieved the new state-of-the-art performance in terms of the BLEU score, as well as human evaluation. 1 2.2 Data Preprocessing First, we tokenize the provided corpus using KyTea (Neubig et al., 2011) for the Japanese side, and Moses tokenizer2 for the English side. We remove the sentences over 60 words to clean the corpus. Then we further split it into sub-words using joint byte pair encoding (joint-BPE) (Sennrich et al., 2016c) with applying 16,000 merge operations. For ASPEC subtasks, though the provided training data contained over 3.0M sentences, we only used the first 2.0M sentences, in the same way as the previous participants (Neubig, 2014). ASPEC was collected by aligning parallel sentences automatically and sorting them on the basis of the alignment confidence score (Nakazawa et al., 2016). This means that the latter side of the corpus may contain noisy parallel sentences, which would have a negative impact on training. We used the latter 1.0M sentences as a monolingual corpus and made a synthetic corpus ("
W17-6308,W06-2932,0,0.010275,"function compound word. The flag becomes 1 only if a function compound word that begins with a target SUW exists in dictionaries, and otherwise is 0. The features are similar to the additional features used for the joint model (Joint+dict) proposed in (Kato et al., 2017) in terms of utilizing lexical knowledge in dictionaries. We chose 12 for the beam width based on trial results. For the pipeline methods, we used Comainu (Coma) (Kozawa et al., 2014) as an LUW chunker that is independent of a syntactic parser. We compared the following three parsers by combining them with Comainu: MST Parser (McDonald et al., 2006), MaltParser (Nivre et al., 2007), and SR joint without LUW-chunking transition (SR single). The LUW-chunking model and the LUWbased dependency parsing models were built with the training division of JP Dep. The SUW-based dependency parsing models were also trained to directly test the parsing of the SUW sequence. The model was trained with LUW-based structures decomposed into SUWs as a structure shown at the bottom right of Figure 1. Freq SR joint SR single UAS LAS UAS LAS case particle に-つい-て about 19 89 と-いう (a bird,) called (swallow) 138 94 conjunctive particle と-し-て by way of (explanation"
W17-6308,P16-1016,0,0.0473459,"Missing"
W17-6308,W08-1301,0,0.110662,"Missing"
W17-6308,P12-1110,0,0.0252265,"yntactic parsing so that the LUW chunking is consistent with the syntactic analysis. The method directly constructs a dependency structure from an SUW sequence, as shown at the bottom right of Figure 1. LUWs consisting of multiple SUWs such as を-もっ-て and 通知-する are represented as a flat structure with a special dependency type luw. We employed an algorithm based on shiftreduce parsing and defined two types of transitions: LUW chunking and dependency parsing. This algorithm is devised by applying a joint analysis method of word segmentation and dependency parsing in Chinese (Zhang et al., 2014; Hatori et al., 2012), or a method which combines lexical and syntactic analysis (Constant and Nivre, 2016). One of features of our algorithm is that a shift transition (ShLUW) assigns a leftmost SUW of an LUW with a POS. We found this obtains better scores than a pop transition (PopLUW) does. Two stacks, σS and σL , are provided for SUWs to be processed and chunked LUWs respectively. The algorithm outputs an LUW sequence and an LUW-based parsed tree to a set of internal dependencies in LUW chunks L, and a set of dependencies A. A parsing status is represented as quintuple (σS , σL , β, A, L), where β is a buffer"
W17-6308,Q13-1012,1,0.844399,"based and do not have complete LUW information2 , we used another typed word dependency treebank in Japanese described in (Tanaka and Nagata, 2015)(JP Dep). JP Dep is annotated with LUW-based dependencies in accordance with 2 They have partly compound word information by annotating dependencies with relation types “mwe”(UD1.2), “fixed”(UD 2.0) and so on. 58 Multiword a scheme that resembles SD, and consists of 20,000 sentences (Table 1) from a newspaper corpus, Kyoto Corpus (Kurohashi and Nagao, 2003). SR joint employs a shift-reduce parser based on dynamic programming (Huang and Sagae, 2010; Hayashi et al., 2013) that is expanded with LUWchunking transitions. We used the features related to LUW and the function compound words, in addition to the original features. Moreover, we employ features with flag where SUW may form an LUW of a function compound word. The flag becomes 1 only if a function compound word that begins with a target SUW exists in dictionaries, and otherwise is 0. The features are similar to the additional features used for the joint model (Joint+dict) proposed in (Kato et al., 2017) in terms of utilizing lexical knowledge in dictionaries. We chose 12 for the beam width based on trial"
W17-6308,P10-1110,0,0.0388767,"apanese corpora are SUWbased and do not have complete LUW information2 , we used another typed word dependency treebank in Japanese described in (Tanaka and Nagata, 2015)(JP Dep). JP Dep is annotated with LUW-based dependencies in accordance with 2 They have partly compound word information by annotating dependencies with relation types “mwe”(UD1.2), “fixed”(UD 2.0) and so on. 58 Multiword a scheme that resembles SD, and consists of 20,000 sentences (Table 1) from a newspaper corpus, Kyoto Corpus (Kurohashi and Nagao, 2003). SR joint employs a shift-reduce parser based on dynamic programming (Huang and Sagae, 2010; Hayashi et al., 2013) that is expanded with LUWchunking transitions. We used the features related to LUW and the function compound words, in addition to the original features. Moreover, we employ features with flag where SUW may form an LUW of a function compound word. The flag becomes 1 only if a function compound word that begins with a target SUW exists in dictionaries, and otherwise is 0. The features are similar to the additional features used for the joint model (Joint+dict) proposed in (Kato et al., 2017) in terms of utilizing lexical knowledge in dictionaries. We chose 12 for the bea"
W17-6308,L16-1261,1,0.847925,"s of parsing methods based on hierarchical word structure (LUW chunking+parsing) by comparing them with methods using single layer word structure (SUW parsing). We also show joint analysis of LUW-chunking and dependency parsing improves the performance of identifying predicate-argument structures, while there is not much difference between overall results of them. 1 Introduction Some research has recently been introducing word-based dependency schemes into Japanese syntactic parsing from a cross-lingual standpoint such as Universal Dependencies (UD) (Nivre et al., 2016; Kanayama et al., 2015; Tanaka et al., 2016), although syntactic structures are traditionally represented as dependencies between chunks called bunsetsus. However, for languages like Japanese where words are not segmented by white spaces in orthography, word-based dependency parsing is problematic due to difficulties in defining a word unit. Actually, in Japanese several word unit standards exists that can be found in corpus annotation schemes or in the outputs of morpho56 Proceedings of the 15th International Conference on Parsing Technologies, pages 56–60, c Pisa, Italy; September 20–22, 2017. 2017 Association for Computational Lingui"
W17-6308,P15-2039,1,0.926364,"redicate argument structures as shown in the top right of the figure. In an LUW-based dependency structure (bottom left), multiword を-もっ-て is considered an LUW with a flat structure, which clearly indicates the relation between main verb 通 知-する notify and argument あなた に you-DAT. The conversion from SUW sequences into LUWs contains ambiguity. For example, sequence を も っ て in the sentence, “彼 は その 本 を もっ て いる”, lit. He has the book., is not just a single LUW but three LUWs with a main verb. The amount of research on Japanese wordbased dependency parsing is much less than bunsetsu-based parsing. Tanaka and Nagata (2015) proposed LUW based analysis using a scheme that resembles Stanford typed dependencies (SD) (de Marneffe and Manning, 2008), however, they do not treat LUW-chunking problems. Kato et al. (2017) explored English dependency parsing models that predict multiword expression (MWE)-aware structure. We treat broader categories of multiword in this paper, e.g. LUWs contain ordinary compound nouns as well as named entities. The test set has 8,291 multiwords (LUWs) in 2,000 sentences, while their corpus has 27,949 MWE instances in 37,015 sentences. Table 1: Corpus statistics. results of hierarchical wor"
W17-6308,P17-2068,0,0.0870237,"dicates the relation between main verb 通 知-する notify and argument あなた に you-DAT. The conversion from SUW sequences into LUWs contains ambiguity. For example, sequence を も っ て in the sentence, “彼 は その 本 を もっ て いる”, lit. He has the book., is not just a single LUW but three LUWs with a main verb. The amount of research on Japanese wordbased dependency parsing is much less than bunsetsu-based parsing. Tanaka and Nagata (2015) proposed LUW based analysis using a scheme that resembles Stanford typed dependencies (SD) (de Marneffe and Manning, 2008), however, they do not treat LUW-chunking problems. Kato et al. (2017) explored English dependency parsing models that predict multiword expression (MWE)-aware structure. We treat broader categories of multiword in this paper, e.g. LUWs contain ordinary compound nouns as well as named entities. The test set has 8,291 multiwords (LUWs) in 2,000 sentences, while their corpus has 27,949 MWE instances in 37,015 sentences. Table 1: Corpus statistics. results of hierarchical word-based parsing (LUWbased) and single layer word-based (SUW-based) parsing in Section 4. 2 Hierarchical Word Dependencies We employed two levels of word unit definitions as described in Section"
W17-6308,P14-1125,0,0.0205594,"h LUW chunking and syntactic parsing so that the LUW chunking is consistent with the syntactic analysis. The method directly constructs a dependency structure from an SUW sequence, as shown at the bottom right of Figure 1. LUWs consisting of multiple SUWs such as を-もっ-て and 通知-する are represented as a flat structure with a special dependency type luw. We employed an algorithm based on shiftreduce parsing and defined two types of transitions: LUW chunking and dependency parsing. This algorithm is devised by applying a joint analysis method of word segmentation and dependency parsing in Chinese (Zhang et al., 2014; Hatori et al., 2012), or a method which combines lexical and syntactic analysis (Constant and Nivre, 2016). One of features of our algorithm is that a shift transition (ShLUW) assigns a leftmost SUW of an LUW with a POS. We found this obtains better scores than a pop transition (PopLUW) does. Two stacks, σS and σL , are provided for SUWs to be processed and chunked LUWs respectively. The algorithm outputs an LUW sequence and an LUW-based parsed tree to a set of internal dependencies in LUW chunks L, and a set of dependencies A. A parsing status is represented as quintuple (σS , σL , β, A, L)"
W18-5410,D14-1179,0,0.0293295,"Missing"
W18-5410,P17-1106,0,0.0248262,"et al., 2014; Bahdanau et al., 2015; Luong et al., 2015) has been an epoch-making development that has led to great progress in many natural language generation tasks, such as machine translation (Bahdanau et al., 2015), dialog generation (Shang et al., 2015), and headline generation (Rush et al., 2015). An enormous number of studies have attempted to enhance the ability of EncDec. Furthermore, several intensive studies have also attempted to analyze and interpret the inside of black-box EncDec models, especially how they translate a given source sentence to the corresponding target sentence (Ding et al., 2017). One typical approach to this is to visualize an attention matrix, which is a collection of attention vectors (Bahdanau et al., 2015; Luong et al., 2015; Tu et al., 2016). ∗ This work is a product of collaborative research program of Tohoku University and NTT Communication Science Laboratories. 1 Our code for reproducing the experiments is available at https://github.com/butsugiri/UAM 74 Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 74–81 c Brussels, Belgium, November 1, 2018. 2018 Association for Computational Linguistics 3.1 sp"
W18-5410,I17-1004,0,0.0451013,"EN Center for Advanced Intelligence Project {kiyono,jun.suzuki,inui}@ecei.tohoku.ac.jp, {takase.sho, nagata.masaaki}@lab.ntt.co.jp, okazaki@c.titech.ac.jp Abstract The assumption behind this interpretation is that the attention matrix has a “soft” token-wise alignment between the source and target sequences, and thus we can use EncDec models to skim which tokens in the source are converted to which tokens in the target. However, recent studies have empirically revealed that an attention model can operate not only for token-wise alignment but also for other functionalities, such as reordering (Ghader and Monz, 2017; Liu et al., 2016). In addition, Luong et al. (2015) reported that the quality of attention matrixbased alignment is quantitatively inferior to that of the Berkeley aligner (Liang et al., 2006). Koehn and Knowles (2017) also reported that attention matrix-based alignment is significantly different from that acquired from an off-the-shelf aligner for English-German language pairs. From these recent findings, the goal of this paper is to provide a method that can offer a better interpretation of how EncDec models translate a given source sentence to the corresponding target sentence. In this pa"
W18-5410,P15-1152,0,0.0223308,"se a method that explicitly models the token-wise alignment between the source and target sequences to provide a better analysis. Experiments show that our method can acquire token-wise alignments that are superior to those of an attention mechanism1 . 1 Introduction The Encoder-Decoder model with an attention mechanism (EncDec) (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Luong et al., 2015) has been an epoch-making development that has led to great progress in many natural language generation tasks, such as machine translation (Bahdanau et al., 2015), dialog generation (Shang et al., 2015), and headline generation (Rush et al., 2015). An enormous number of studies have attempted to enhance the ability of EncDec. Furthermore, several intensive studies have also attempted to analyze and interpret the inside of black-box EncDec models, especially how they translate a given source sentence to the corresponding target sentence (Ding et al., 2017). One typical approach to this is to visualize an attention matrix, which is a collection of attention vectors (Bahdanau et al., 2015; Luong et al., 2015; Tu et al., 2016). ∗ This work is a product of collaborative research program of Tohoku"
W18-5410,D17-1227,0,0.0166013,"ns q˜ with the sum of the source-side ˜ as an objective function `src . tokens x Since oj for each j is a vector representation of the ˆ 0:j−1 , X, θ) over the target probabilities of p(y|y vocabularies yˆ ∈ Vt , we can calculate `trg as (8) `trg (Y , X, θ) = − J+1 X  yj> · log oj . (12) j=1 3.3 Inference of EncDec In the inference step, we use the trained parameters to search for the best target sequence. We use beam search to find the target sequence that maximizes the product of the conditional probabilities as described in Equation 1. From among several stopping criteria for beam search (Huang et al., 2017), we adopt the widely used “shrinking beam” implemented in RNNsearch (Bahdanau et al., 2015)3 . (9) where Ws ∈ RH×2H is a parameter matrix. Finally, zj is fed into the softmax layer. The model generates a target-side token based on the probability distribution oj ∈ RVt as (10) where Wo ∈ RVt ×H is a parameter matrix and bo ∈ RVt is a bias term. 3.2 & ? + (?"":$ ) Next, the source-side information is mixed with the decoder hidden state to derive final hidden state zj . Concretely, the context vector cj is concate~j to form vector uj ∈ R2H . uj is then nated with z fed into a single fully-connect"
W18-5410,E17-2047,1,0.939025,"ecent findings, the goal of this paper is to provide a method that can offer a better interpretation of how EncDec models translate a given source sentence to the corresponding target sentence. In this paper, we focus exclusively on the headline generation task, which is categorized as a lossycompression generation (lossy-gen) task (Nallapati et al., 2016). Compared with a machine translation task, which is categorized as a loss-less generation (lossless-gen) task, the headline generation task additionally requires EncDec models to appropriately select salient ideas in given source sentences (Suzuki and Nagata, 2017). Therefore, the lossy-gen task seems to make modeling by EncDec much harder. In fact, our preliminary experiments revealed that the attention mechanism in EncDec models largely fails to capture token-wise alignments, e.g., less than 10 percent accuracy, even if we use one of the current state-of-the-art EncDec models (Table 3). To obtain a better analysis of how EncDec models translate a given source sentence to the correDeveloping a method for understanding the inner workings of black-box neural methods is an important research endeavor. Conventionally, many studies have used an attention ma"
W18-5410,W17-3204,0,0.0232542,"attention matrix has a “soft” token-wise alignment between the source and target sequences, and thus we can use EncDec models to skim which tokens in the source are converted to which tokens in the target. However, recent studies have empirically revealed that an attention model can operate not only for token-wise alignment but also for other functionalities, such as reordering (Ghader and Monz, 2017; Liu et al., 2016). In addition, Luong et al. (2015) reported that the quality of attention matrixbased alignment is quantitatively inferior to that of the Berkeley aligner (Liang et al., 2006). Koehn and Knowles (2017) also reported that attention matrix-based alignment is significantly different from that acquired from an off-the-shelf aligner for English-German language pairs. From these recent findings, the goal of this paper is to provide a method that can offer a better interpretation of how EncDec models translate a given source sentence to the corresponding target sentence. In this paper, we focus exclusively on the headline generation task, which is categorized as a lossycompression generation (lossy-gen) task (Nallapati et al., 2016). Compared with a machine translation task, which is categorized a"
W18-5410,N06-1014,0,0.0899348,"pretation is that the attention matrix has a “soft” token-wise alignment between the source and target sequences, and thus we can use EncDec models to skim which tokens in the source are converted to which tokens in the target. However, recent studies have empirically revealed that an attention model can operate not only for token-wise alignment but also for other functionalities, such as reordering (Ghader and Monz, 2017; Liu et al., 2016). In addition, Luong et al. (2015) reported that the quality of attention matrixbased alignment is quantitatively inferior to that of the Berkeley aligner (Liang et al., 2006). Koehn and Knowles (2017) also reported that attention matrix-based alignment is significantly different from that acquired from an off-the-shelf aligner for English-German language pairs. From these recent findings, the goal of this paper is to provide a method that can offer a better interpretation of how EncDec models translate a given source sentence to the corresponding target sentence. In this paper, we focus exclusively on the headline generation task, which is categorized as a lossycompression generation (lossy-gen) task (Nallapati et al., 2016). Compared with a machine translation ta"
W18-5410,D16-1033,0,0.017834,"hat used in Rush et al. (2015). The dataset consists of pairs of the first sentence of each article and its headline from the annotated English Gigaword corpus (Napoles et al., 2012). Rush et al. (2015) defined the training, validation and test splits, which contain approximately 3.8M, 200K and 400K source-headline pairs, respectively We used the entire training split for training as in the previous studies. We randomly sampled 8K instances as validation data and 10K instances as test data from the validation split. Moreover, we experimented on the test data provided by Zhou et al. (2017) and Toutanova et al. (2016) for comparison with the reported state-of-the-art performance (Zhou et al., 2017). We refer to those test data sets as Test (Ours), Test (Zhou), and MSRATC respectively. Among these test sets, MSRATC is the only dataset created by a human worker. 5.2 5131 5131 200 400 Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) 2-layer bidirectional-LSTM 2-layer LSTM with attention (Luong et al., 2015) Adam (Kingma and Ba, 2015) 0.001 0.5 for each epoch (after epoch 9) 10 256 (shuffled at each epoch) 5 Max 15 epochs with early stopping Dropout (rate 0.3) Beam size 20 with the length norma"
W18-5410,C16-1291,0,0.0193027,"Intelligence Project {kiyono,jun.suzuki,inui}@ecei.tohoku.ac.jp, {takase.sho, nagata.masaaki}@lab.ntt.co.jp, okazaki@c.titech.ac.jp Abstract The assumption behind this interpretation is that the attention matrix has a “soft” token-wise alignment between the source and target sequences, and thus we can use EncDec models to skim which tokens in the source are converted to which tokens in the target. However, recent studies have empirically revealed that an attention model can operate not only for token-wise alignment but also for other functionalities, such as reordering (Ghader and Monz, 2017; Liu et al., 2016). In addition, Luong et al. (2015) reported that the quality of attention matrixbased alignment is quantitatively inferior to that of the Berkeley aligner (Liang et al., 2006). Koehn and Knowles (2017) also reported that attention matrix-based alignment is significantly different from that acquired from an off-the-shelf aligner for English-German language pairs. From these recent findings, the goal of this paper is to provide a method that can offer a better interpretation of how EncDec models translate a given source sentence to the corresponding target sentence. In this paper, we focus exclu"
W18-5410,P16-1008,0,0.10152,"as machine translation (Bahdanau et al., 2015), dialog generation (Shang et al., 2015), and headline generation (Rush et al., 2015). An enormous number of studies have attempted to enhance the ability of EncDec. Furthermore, several intensive studies have also attempted to analyze and interpret the inside of black-box EncDec models, especially how they translate a given source sentence to the corresponding target sentence (Ding et al., 2017). One typical approach to this is to visualize an attention matrix, which is a collection of attention vectors (Bahdanau et al., 2015; Luong et al., 2015; Tu et al., 2016). ∗ This work is a product of collaborative research program of Tohoku University and NTT Communication Science Laboratories. 1 Our code for reproducing the experiments is available at https://github.com/butsugiri/UAM 74 Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 74–81 c Brussels, Belgium, November 1, 2018. 2018 Association for Computational Linguistics 3.1 sponding target sentence in the headline generation task, this paper introduces the Unsupervised tokenwise Alignment Module (UAM), a novel component that can be plugged into"
W18-5410,D15-1166,0,0.627426,".suzuki,inui}@ecei.tohoku.ac.jp, {takase.sho, nagata.masaaki}@lab.ntt.co.jp, okazaki@c.titech.ac.jp Abstract The assumption behind this interpretation is that the attention matrix has a “soft” token-wise alignment between the source and target sequences, and thus we can use EncDec models to skim which tokens in the source are converted to which tokens in the target. However, recent studies have empirically revealed that an attention model can operate not only for token-wise alignment but also for other functionalities, such as reordering (Ghader and Monz, 2017; Liu et al., 2016). In addition, Luong et al. (2015) reported that the quality of attention matrixbased alignment is quantitatively inferior to that of the Berkeley aligner (Liang et al., 2006). Koehn and Knowles (2017) also reported that attention matrix-based alignment is significantly different from that acquired from an off-the-shelf aligner for English-German language pairs. From these recent findings, the goal of this paper is to provide a method that can offer a better interpretation of how EncDec models translate a given source sentence to the corresponding target sentence. In this paper, we focus exclusively on the headline generation"
W18-5410,P17-1101,0,0.406026,"ments is identical to that used in Rush et al. (2015). The dataset consists of pairs of the first sentence of each article and its headline from the annotated English Gigaword corpus (Napoles et al., 2012). Rush et al. (2015) defined the training, validation and test splits, which contain approximately 3.8M, 200K and 400K source-headline pairs, respectively We used the entire training split for training as in the previous studies. We randomly sampled 8K instances as validation data and 10K instances as test data from the validation split. Moreover, we experimented on the test data provided by Zhou et al. (2017) and Toutanova et al. (2016) for comparison with the reported state-of-the-art performance (Zhou et al., 2017). We refer to those test data sets as Test (Ours), Test (Zhou), and MSRATC respectively. Among these test sets, MSRATC is the only dataset created by a human worker. 5.2 5131 5131 200 400 Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) 2-layer bidirectional-LSTM 2-layer LSTM with attention (Luong et al., 2015) Adam (Kingma and Ba, 2015) 0.001 0.5 for each epoch (after epoch 9) 10 256 (shuffled at each epoch) 5 Max 15 epochs with early stopping Dropout (rate 0.3) Beam s"
W18-5410,K16-1028,0,0.201448,"y inferior to that of the Berkeley aligner (Liang et al., 2006). Koehn and Knowles (2017) also reported that attention matrix-based alignment is significantly different from that acquired from an off-the-shelf aligner for English-German language pairs. From these recent findings, the goal of this paper is to provide a method that can offer a better interpretation of how EncDec models translate a given source sentence to the corresponding target sentence. In this paper, we focus exclusively on the headline generation task, which is categorized as a lossycompression generation (lossy-gen) task (Nallapati et al., 2016). Compared with a machine translation task, which is categorized as a loss-less generation (lossless-gen) task, the headline generation task additionally requires EncDec models to appropriately select salient ideas in given source sentences (Suzuki and Nagata, 2017). Therefore, the lossy-gen task seems to make modeling by EncDec much harder. In fact, our preliminary experiments revealed that the attention mechanism in EncDec models largely fails to capture token-wise alignments, e.g., less than 10 percent accuracy, even if we use one of the current state-of-the-art EncDec models (Table 3). To"
W18-5410,W12-3018,0,0.197412,"Missing"
W18-5410,D15-1044,0,0.46313,"ise alignment between the source and target sequences to provide a better analysis. Experiments show that our method can acquire token-wise alignments that are superior to those of an attention mechanism1 . 1 Introduction The Encoder-Decoder model with an attention mechanism (EncDec) (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Luong et al., 2015) has been an epoch-making development that has led to great progress in many natural language generation tasks, such as machine translation (Bahdanau et al., 2015), dialog generation (Shang et al., 2015), and headline generation (Rush et al., 2015). An enormous number of studies have attempted to enhance the ability of EncDec. Furthermore, several intensive studies have also attempted to analyze and interpret the inside of black-box EncDec models, especially how they translate a given source sentence to the corresponding target sentence (Ding et al., 2017). One typical approach to this is to visualize an attention matrix, which is a collection of attention vectors (Bahdanau et al., 2015; Luong et al., 2015; Tu et al., 2016). ∗ This work is a product of collaborative research program of Tohoku University and NTT Communication Science Lab"
W18-5410,P16-1162,0,0.0510147,"ly for MSR-ATC and a performance comparable to EncDec+sGate in Test (Ours) and Test (Zhou). Considering that the MSR-ATC dataset was created by a human worker, we believe that the improvement in MSR-ATC is the most remarkable result among the three test sets, since it indicates that our model most closely fits the human-generated summary. Implementation Details In the experiment, we selected the hyper-parameter settings commonly used in previous studies e.g., (Rush et al., 2015; Nallapati et al., 2016; Suzuki and Nagata, 2017) We constructed the vocabulary set using byte pair encoding4 (BPE) (Sennrich et al., 2016) to handle low-frequency words, since this is now a common practice in the field of neural machine translation. The BPE merge operations are jointly learned from the source and the target. We set the number of BPE merge operations at 5, 000. 6 Discussion We investigated whether the UAM improves tokenwise alignment between the source and target se5 We restored sub-words to the standard token split for the evaluation. 6 ROUGE script option is: “-n2 -m -w 1.2” 4 https://github.com/rsennrich/ subword-nmt 78 Test (Ours) Test (Zhou) MSR-ATC Model RG-1 RG-2 RG-L RG-1 RG-2 RG-L RG-1 RG-2 RG-L EncDec+s"
W18-6421,W12-3131,0,0.112139,"ora3 and translated News Crawl 2017 to make a synthetic corpus. From here, we discuss these features and experimentally verify each one. 3.1 Synthetic Corpus Noisy Data Filtering This year, ParaCrawl and Common Crawl corpora, which were created by crawling parallel websites, were provided for training. Since these web-based corpora are large but noisy, it seems essential to filter out noisy sentence pairs. Since the ParaCrawl corpus has already been cleaned by Zipporah (Xu and Koehn, 2017), we chose another method for further cleaning1 . To clean the corpus, we selected the qe-clean2 toolkit (Denkowski et al., 2012), which uses a language model to evaluate a sentences naturalness and a word alignment model to check whether the sentence pair has the same meaning. Both models are trained with clean data for scoring possibly noisy parallel sentence pairs and removes sentences with scores below a threshold. For more details, see Denkowski et al. (2012). We used Europarl, News Commentary, and Rapid corpora as clean parallel data for training the word alignment model. We also used News Crawl 2017 as an additional monolingual corpus for language modeling. Since our target is news translation, using a news-relat"
W18-6421,N13-1073,0,0.0592128,"the sentence pair has the same meaning. Both models are trained with clean data for scoring possibly noisy parallel sentence pairs and removes sentences with scores below a threshold. For more details, see Denkowski et al. (2012). We used Europarl, News Commentary, and Rapid corpora as clean parallel data for training the word alignment model. We also used News Crawl 2017 as an additional monolingual corpus for language modeling. Since our target is news translation, using a news-related monolingual corpus is beneficial to train language models. We used KenLM (Heafield, 2011) and fast align (Dyer et al., 2013, 2010) for language modeling and word alignment. To find the appropriate 3.3 Back-translation BLEU-based Filtering for Synthetic Corpus A synthetic corpus might contain noise due to translation errors. Since these noisy sentences might deleteriously affect the training, we filtered them out. In this work, we did back-translation BLEUbased synthetic corpus filtering (Imankulova et al., 2017). We hypothesize that synthetic sentence pairs can be correctly back-translated to the target language unless they contains translation errors. Based on this hypothesis, we found better synthetic sentence p"
W18-6421,C04-1072,0,0.0299765,"monolingual sentences in the target language to the source language by a target-to1 Although the provided ParaCrawl corpus was already filtered by Zipporah (Xu and Koehn, 2017), a cursory glance suggested that it still contains many noisy sentence pairs. 2 https://github.com/cmu-mtlab/qe-clean 3 Europarl + News Commentary + Rapid + a filtered version of Common Crawl and ParaCrawl corpora 462 source translation model. After getting the translation, we back-translated it with the source-totarget model. Then we evaluated how well it restored the original sentences by sentence-level BLEU scores (Lin and Och, 2004), selected the high-scoring sentence pairs, and created a synthetic corpus whose size equals the naturally occurring parallel corpus. all the datasets used in our experiments. Then we split the words into subwords by joint BytePair-Encoding (BPE) (Sennrich et al., 2016b) with 32,000 merge operations. Finally, we discarded from the training data the sentence pairs that exceed 80 subwords either in the source or target sentences. As a development set, we used newstest 2017 (3004 sentences). 3.4 4.2 Right-to-Left Re-ranking Liu et al. (2016) pointed out that RNN-based sequence generation models l"
W18-6421,D15-1166,0,0.160226,"Missing"
W18-6421,W17-5706,1,0.911703,"Missing"
W18-6421,P02-1040,0,0.100892,"training the model. Experimental Results and Discussions Table 1 shows the provided and filtered corpus sizes for training. The Original Common Crawl and ParaCrawl corpora contain around 35.56M sentences. However, since most of the sentence pairs are noisy, we only retained the cleanest 4.01M sentences that were selected by the qe-clean toolkit. For the synthetic corpus, we chose the same size as the filtered parallel corpus based on the back-translation BLEU+1 scores. Table 2 shows the evaluation results of our submission and baseline systems. Here, we report the case-sensitive BLEU scores (Papineni et al., 2002) evaluated by the provided automatic evaluation system8 . In the following, unless specified, we mainly discuss the Transformer model results. 4.3.1 4.3.2 Effect of Corpus Filtering By re-ranking the n-best hypothesis by the R2L model, we saw a gain of 1.5 points for En-De and 0.5 points for De-En (Setting (6)). We submitted these results as our primary submission. R2L n-best re-ranking works well with the RNN-based model, but we confirmed that it also works well with the Transformer model. We suppose both the Transformer and the RNN models lack the ability to decode the end of the sentence, b"
W18-6421,D17-1319,0,0.120434,"ion model and used it as additional parallel data. In our case, we trained a baseline NMT model with a provided parallel corpora3 and translated News Crawl 2017 to make a synthetic corpus. From here, we discuss these features and experimentally verify each one. 3.1 Synthetic Corpus Noisy Data Filtering This year, ParaCrawl and Common Crawl corpora, which were created by crawling parallel websites, were provided for training. Since these web-based corpora are large but noisy, it seems essential to filter out noisy sentence pairs. Since the ParaCrawl corpus has already been cleaned by Zipporah (Xu and Koehn, 2017), we chose another method for further cleaning1 . To clean the corpus, we selected the qe-clean2 toolkit (Denkowski et al., 2012), which uses a language model to evaluate a sentences naturalness and a word alignment model to check whether the sentence pair has the same meaning. Both models are trained with clean data for scoring possibly noisy parallel sentence pairs and removes sentences with scores below a threshold. For more details, see Denkowski et al. (2012). We used Europarl, News Commentary, and Rapid corpora as clean parallel data for training the word alignment model. We also used Ne"
W18-6421,E17-2025,0,0.0283163,"Transformer model. Our hyper-parameters are based on the previously introduced Transformer big setting (Vaswani et al., 2017), and we also referred Popel and Bojar (2018) for tuning hyper-parameters. We used six layers for both the encoder and the decoder. All the sub-layers and the embeddings layers output 1024 dimension vectors, and the inner-layer of the position-wise feed-forward layers has 4096 dimensions. For multi-head attention, we used 16 parallel attention layers. We use the same weights for the encoder/decoder embedding layers and the decoder output layer by three-way-weight-tying (Press and Wolf, 2017). As an optimizer, we used Adam (Kingma and Ba, 2015) with 1 = 0.9 and 2 = 0.997 and set dropout (Srivastava et al., 2014) with a probability of 0.1. We used a learning rate decaying method proposed by (Vaswani et al., 2017) with 16,000 warm-up steps and trained the model for 300,000 steps. Each mini-batch contained roughly 20,000 tokens. We saved a model every hour and averaged the last 16 model parameters for decoding. The training took about three days for both En-De and De-En with eight GTX 1080Ti GPUs. During decoding, we used a beam search with a size of ten and a length normalization te"
W18-6421,P16-1009,0,0.181746,"C Model TRG monolingual Figure 1: Overview of back-translation BLEU-based synthetic corpus filtering back-translation BLEU-based filtering (Section 3.2). weights for each feature, we used newstest 2017 as a development set and fixed the threshold as one standard deviation. • n-best re-ranking by a right-to-left translation model (Section 3.4). 3.2 One drawback of NMT is that it can only be trained with parallel data. Using synthetic corpora, which are pseudo-parallel corpora created by translating monolingual data with an existing NMT model, is one of the ways to make use of monolingual data (Sennrich et al., 2016a). We created a synthetic corpus by translating monolingual sentences with a target-to-source translation model and used it as additional parallel data. In our case, we trained a baseline NMT model with a provided parallel corpora3 and translated News Crawl 2017 to make a synthetic corpus. From here, we discuss these features and experimentally verify each one. 3.1 Synthetic Corpus Noisy Data Filtering This year, ParaCrawl and Common Crawl corpora, which were created by crawling parallel websites, were provided for training. Since these web-based corpora are large but noisy, it seems essentia"
W18-6421,P16-1162,0,0.364195,"C Model TRG monolingual Figure 1: Overview of back-translation BLEU-based synthetic corpus filtering back-translation BLEU-based filtering (Section 3.2). weights for each feature, we used newstest 2017 as a development set and fixed the threshold as one standard deviation. • n-best re-ranking by a right-to-left translation model (Section 3.4). 3.2 One drawback of NMT is that it can only be trained with parallel data. Using synthetic corpora, which are pseudo-parallel corpora created by translating monolingual data with an existing NMT model, is one of the ways to make use of monolingual data (Sennrich et al., 2016a). We created a synthetic corpus by translating monolingual sentences with a target-to-source translation model and used it as additional parallel data. In our case, we trained a baseline NMT model with a provided parallel corpora3 and translated News Crawl 2017 to make a synthetic corpus. From here, we discuss these features and experimentally verify each one. 3.1 Synthetic Corpus Noisy Data Filtering This year, ParaCrawl and Common Crawl corpora, which were created by crawling parallel websites, were provided for training. Since these web-based corpora are large but noisy, it seems essentia"
W19-5365,P18-1007,0,0.0158784,"the Japanese side and not translated into English. We fixed these errors by the script provided by Yamamoto and Takahashi (2016)1 . We use different preprocessing steps for each translation direction. This is because we need to submit tokenized output for En-Ja translation, thus it seems to be better to tokenize the Japanese side in the same way as the submission in the preprocessing steps, whereas we use a relatively simple method for Ja-En direction. For Ja-En, we tokenized the raw text into subwords by simply applying sentencepiece with the vocabulary size of 32,000 for each language side (Kudo, 2018; Kudo and Richardson, 2018). For En-Ja, we tokenized the text by KyTea (Neubig et al., 2011) and the Moses tokenizer (Koehn et al., 2007) for Japanese and English, respectively. We also truecased the English words by the script provided with Moses toolkits2 . Then we further tokenized the words into subwords using joint Byte-Pair-Encoding (BPE) with 16,000 merge operations3 (Sennrich et al., 2016b). provided monolingual corpus, and noisy data filtering for its data. (Section 2.3). • Placeholder mechanism to handle tokens that should be copied from a source-side sentence (Section 2.4). NMT Mod"
W19-5365,D18-2012,0,0.0176228,"side and not translated into English. We fixed these errors by the script provided by Yamamoto and Takahashi (2016)1 . We use different preprocessing steps for each translation direction. This is because we need to submit tokenized output for En-Ja translation, thus it seems to be better to tokenize the Japanese side in the same way as the submission in the preprocessing steps, whereas we use a relatively simple method for Ja-En direction. For Ja-En, we tokenized the raw text into subwords by simply applying sentencepiece with the vocabulary size of 32,000 for each language side (Kudo, 2018; Kudo and Richardson, 2018). For En-Ja, we tokenized the text by KyTea (Neubig et al., 2011) and the Moses tokenizer (Koehn et al., 2007) for Japanese and English, respectively. We also truecased the English words by the script provided with Moses toolkits2 . Then we further tokenized the words into subwords using joint Byte-Pair-Encoding (BPE) with 16,000 merge operations3 (Sennrich et al., 2016b). provided monolingual corpus, and noisy data filtering for its data. (Section 2.3). • Placeholder mechanism to handle tokens that should be copied from a source-side sentence (Section 2.4). NMT Model Neural Machine Translatio"
W19-5365,2012.eamt-1.60,0,0.0291062,"of sentences and words contained in the provided monolingual corpus. As NMT can be trained with only parallel data, utilizing a monolingual corpus for NMT is a key Data Preprocessing For an in-domain corpus, the organizers provided the MTNT (Machine Translation of Noisy Text) parallel corpus (Michel and Neubig, 2018), which is a collection of Reddit discussions and their manual translations. They also provided relatively large out-of-domain parallel corpora, namely KFTT (Kyoto Free Translation Task) (Neubig, 2011), JESC (Japanese-English Subtitle Corpus) (Pryzant et al., 2017), and TED talks (Cettolo et al., 2012). Table 2 shows the number of sentences and words on the English side contained in the provided parallel corpora. 1 https://github.com/kanjirz50/mt_ ialp2016/blob/master/script/ja_prepro.pl 2 https://github.com/moses-smt/ mosesdecoder/blob/master/scripts/ recaser/truecase.perl 3 Normally, Japanese and English do not share any words, thus using joint BPE does not seem effective. However, for this dataset, we found that Japanese sentences often include English words (e.g., named entities), so we use joint BPE even for this language pair. 545 TRG monolingual TRG → SRC Model (2) data cleaning & tr"
W19-5365,W19-5303,0,0.0852541,"created a synthetic corpus from a target-side monolingual corpus with a target-to-source translation model. Lastly, we fine-tuned our translation model with the synthetic and in-domain parallel corpora for domain adaptation. The paper is organized as follows. In Section 2, we present a detailed overview of our systems. Section 3 shows experimental settings and main results, and Section 4 provides an analysis of our systems. Finally, Section 5 draws a brief conclusion of our work for the WMT19 robustness task. Introduction This paper describes NTT’s submission to the WMT 2019 robustness task (Li et al., 2019). This year, we participated in English-to-Japanese (EnJa) and Japanese-to-English (Ja-En) translation tasks with a constrained setting, i.e., we used only the parallel and monolingual corpora provided by the organizers. The task focuses on the robustness of Machine Translation (MT) to noisy text that can be found on social media (e.g., Reddit, Twitter). The task is more challenging than a typical machine translation task like the news translation tasks (Bojar et al., 2018) due to the characteristics of noisy text and the lack of a publicly available parallel corpus (Michel and Neubig, 2018)."
W19-5365,D15-1166,0,0.201069,"Missing"
W19-5365,D18-1050,0,0.150521,"ess task (Li et al., 2019). This year, we participated in English-to-Japanese (EnJa) and Japanese-to-English (Ja-En) translation tasks with a constrained setting, i.e., we used only the parallel and monolingual corpora provided by the organizers. The task focuses on the robustness of Machine Translation (MT) to noisy text that can be found on social media (e.g., Reddit, Twitter). The task is more challenging than a typical machine translation task like the news translation tasks (Bojar et al., 2018) due to the characteristics of noisy text and the lack of a publicly available parallel corpus (Michel and Neubig, 2018). Table 1 shows example comments from Reddit, a discussion website. Text on social media usually contains various noise such as (1) abbreviations, (2) grammatical errors, (3) misspellings, (4) emojis, and (5) emoticons. In addition, most provided parallel corpora are not related to our target domain, ⇤ 2 System Details In this section, we describe the overview and features of our systems: • Data preprocessing techniques for the provided parallel corpora (Section 2.2). • Synthetic corpus, back-translated from the Equal contribution. 544 Proceedings of the Fourth Conference on Machine Translatio"
W19-5365,W12-3131,0,0.0267844,"n using the spaCy4 toolkit. After that, we discarded the sentences that are either longer than 80 tokens or equal to 1 token. Since a synthetic corpus might contain noisy sentence pairs, previous work shows that an additional filtering technique helps to improve accuracy (Morishita et al., 2018). We also apply a filtering technique to the synthetic corpus as illustrated in (3) in Figure 1. For this task, we use the qe-clean5 toolkit, which filtered out the noisy sentences on the basis of a word alignment and language models by estimating how correctly translated and natural the sentences are (Denkowski et al., 2012). We train the word alignment and language models by using KFTT, TED, and MTNT corpora6 . We use fast_align for word alignment and KenLM for language modeling (Dyer et al., 2013; Heafield, 2011). 2.4 Placeholder Noisy text on social media often contains tokens that do not require translation such as emojis, “ , , ”, and emoticons, “m(_ _)m, ( ` · ! · ´ ), 4 https://spacy.io https://github.com/cmu-mtlab/qe-clean 6 Note that the JESC corpus is relatively noisy, thus we decided not to use it for cleaning. 5 546 (ˆoˆ)/”. However, to preserve the meaning of the input sentence that contains emojis"
W19-5365,N13-1073,0,0.0374731,"rs, previous work shows that an additional filtering technique helps to improve accuracy (Morishita et al., 2018). We also apply a filtering technique to the synthetic corpus as illustrated in (3) in Figure 1. For this task, we use the qe-clean5 toolkit, which filtered out the noisy sentences on the basis of a word alignment and language models by estimating how correctly translated and natural the sentences are (Denkowski et al., 2012). We train the word alignment and language models by using KFTT, TED, and MTNT corpora6 . We use fast_align for word alignment and KenLM for language modeling (Dyer et al., 2013; Heafield, 2011). 2.4 Placeholder Noisy text on social media often contains tokens that do not require translation such as emojis, “ , , ”, and emoticons, “m(_ _)m, ( ` · ! · ´ ), 4 https://spacy.io https://github.com/cmu-mtlab/qe-clean 6 Note that the JESC corpus is relatively noisy, thus we decided not to use it for cleaning. 5 546 (ˆoˆ)/”. However, to preserve the meaning of the input sentence that contains emojis or emoticons, such tokens need to be output to the target language side. Therefore, we simply copy the emojis and emoticons from a source language to a target language with a pl"
W19-5365,W18-6421,1,0.776039,"for the sentence i. Finally, if i is higher than a specific threshold, we assume that the sentence i contains an ASCII art and discard it from the monolingual data. We set the threshold to 6.0. Moreover, since the provided monolingual data includes lines with more than one sentence, we first performed the sentence tokenization using the spaCy4 toolkit. After that, we discarded the sentences that are either longer than 80 tokens or equal to 1 token. Since a synthetic corpus might contain noisy sentence pairs, previous work shows that an additional filtering technique helps to improve accuracy (Morishita et al., 2018). We also apply a filtering technique to the synthetic corpus as illustrated in (3) in Figure 1. For this task, we use the qe-clean5 toolkit, which filtered out the noisy sentences on the basis of a word alignment and language models by estimating how correctly translated and natural the sentences are (Denkowski et al., 2012). We train the word alignment and language models by using KFTT, TED, and MTNT corpora6 . We use fast_align for word alignment and KenLM for language modeling (Dyer et al., 2013; Heafield, 2011). 2.4 Placeholder Noisy text on social media often contains tokens that do not"
W19-5365,W11-2123,0,0.0125137,"hows that an additional filtering technique helps to improve accuracy (Morishita et al., 2018). We also apply a filtering technique to the synthetic corpus as illustrated in (3) in Figure 1. For this task, we use the qe-clean5 toolkit, which filtered out the noisy sentences on the basis of a word alignment and language models by estimating how correctly translated and natural the sentences are (Denkowski et al., 2012). We train the word alignment and language models by using KFTT, TED, and MTNT corpora6 . We use fast_align for word alignment and KenLM for language modeling (Dyer et al., 2013; Heafield, 2011). 2.4 Placeholder Noisy text on social media often contains tokens that do not require translation such as emojis, “ , , ”, and emoticons, “m(_ _)m, ( ` · ! · ´ ), 4 https://spacy.io https://github.com/cmu-mtlab/qe-clean 6 Note that the JESC corpus is relatively noisy, thus we decided not to use it for cleaning. 5 546 (ˆoˆ)/”. However, to preserve the meaning of the input sentence that contains emojis or emoticons, such tokens need to be output to the target language side. Therefore, we simply copy the emojis and emoticons from a source language to a target language with a placeholder mechani"
W19-5365,P11-2093,0,0.0184736,"ipt provided by Yamamoto and Takahashi (2016)1 . We use different preprocessing steps for each translation direction. This is because we need to submit tokenized output for En-Ja translation, thus it seems to be better to tokenize the Japanese side in the same way as the submission in the preprocessing steps, whereas we use a relatively simple method for Ja-En direction. For Ja-En, we tokenized the raw text into subwords by simply applying sentencepiece with the vocabulary size of 32,000 for each language side (Kudo, 2018; Kudo and Richardson, 2018). For En-Ja, we tokenized the text by KyTea (Neubig et al., 2011) and the Moses tokenizer (Koehn et al., 2007) for Japanese and English, respectively. We also truecased the English words by the script provided with Moses toolkits2 . Then we further tokenized the words into subwords using joint Byte-Pair-Encoding (BPE) with 16,000 merge operations3 (Sennrich et al., 2016b). provided monolingual corpus, and noisy data filtering for its data. (Section 2.3). • Placeholder mechanism to handle tokens that should be copied from a source-side sentence (Section 2.4). NMT Model Neural Machine Translation (NMT) has been making remarkable progress in the field of MT (B"
W19-5365,N19-4009,0,0.0192537,"that makes use of GPUs more efficiently for faster training (Micikevicius et al., 2018). Each minibatch contains about 8000 tokens (subwords), and we accumulated the gradients of 128 mini-batches for an update (Ott et al., 2018). We trained the model for 20,000 iterations, saved the model parameters each 200 iterations, and took an average of the last eight models9 . Training took about 1.5 days to converge with four NVIDIA V100 GPUs. We compute case-sensitive BLEU scores (Papineni et al., 2002) for evaluating translation quality10 . All our implementations are based on the fairseq11 toolkit (Ott et al., 2019). After training the model with the whole provided parallel corpora, we fine-tuned it with indomain data. During fine-tuning, we used almost the same settings as the initial training setup except we changed the model save interval to every three iterations and continued the learning rate decay schedule. For fine-tuning, we trained the model for 50 iterations, which took less than 10 minutes with four GPUs. When decoding, we used a beam search with the size of six and a length normalization technique with ↵ = 2.0 and = 0.0 (Wu et al., 2016). For the submission, we used an ensemble of three (En-"
W19-5365,W18-6301,0,0.0618061,"layer by three-way-weight-tying (Press and Wolf, 2017). Each layer is connected with a dropout probability of 0.3 (Srivastava et al., 2014). For an optimizer, we used Adam (Kingma and Ba, 2015) with a learning rate of 0.001, 1 = 0.9, 2 = 0.98. We use a root-square decay learning rate schedule with a linear warmup of 4000 steps (Vaswani et al., 2017). We applied mixed precision training that makes use of GPUs more efficiently for faster training (Micikevicius et al., 2018). Each minibatch contains about 8000 tokens (subwords), and we accumulated the gradients of 128 mini-batches for an update (Ott et al., 2018). We trained the model for 20,000 iterations, saved the model parameters each 200 iterations, and took an average of the last eight models9 . Training took about 1.5 days to converge with four NVIDIA V100 GPUs. We compute case-sensitive BLEU scores (Papineni et al., 2002) for evaluating translation quality10 . All our implementations are based on the fairseq11 toolkit (Ott et al., 2019). After training the model with the whole provided parallel corpora, we fine-tuned it with indomain data. During fine-tuning, we used almost the same settings as the initial training setup except we changed the"
W19-5365,P02-1040,0,0.107578,"cay learning rate schedule with a linear warmup of 4000 steps (Vaswani et al., 2017). We applied mixed precision training that makes use of GPUs more efficiently for faster training (Micikevicius et al., 2018). Each minibatch contains about 8000 tokens (subwords), and we accumulated the gradients of 128 mini-batches for an update (Ott et al., 2018). We trained the model for 20,000 iterations, saved the model parameters each 200 iterations, and took an average of the last eight models9 . Training took about 1.5 days to converge with four NVIDIA V100 GPUs. We compute case-sensitive BLEU scores (Papineni et al., 2002) for evaluating translation quality10 . All our implementations are based on the fairseq11 toolkit (Ott et al., 2019). After training the model with the whole provided parallel corpora, we fine-tuned it with indomain data. During fine-tuning, we used almost the same settings as the initial training setup except we changed the model save interval to every three iterations and continued the learning rate decay schedule. For fine-tuning, we trained the model for 50 iterations, which took less than 10 minutes with four GPUs. When decoding, we used a beam search with the size of six and a length no"
W19-5365,E17-2025,0,0.0142778,"etected on the basis of Unicode Emoji Charts7 . We detect emoticons included in both the source- and the target-side sentences with the nagisa8 toolkit, which is a Japanese morphological analyzer that can also be used as an emoticon detector for Japanese and English text. Moreover, we also replace “&gt;” tokens at the beginning of the sentence with the placeholders because “&gt;” is commonly used as a quotation mark in social media posts and emails and does not require translation. 2.5 the parameter of the encoder/decoder word embedding layers and the decoder output layer by three-way-weight-tying (Press and Wolf, 2017). Each layer is connected with a dropout probability of 0.3 (Srivastava et al., 2014). For an optimizer, we used Adam (Kingma and Ba, 2015) with a learning rate of 0.001, 1 = 0.9, 2 = 0.98. We use a root-square decay learning rate schedule with a linear warmup of 4000 steps (Vaswani et al., 2017). We applied mixed precision training that makes use of GPUs more efficiently for faster training (Micikevicius et al., 2018). Each minibatch contains about 8000 tokens (subwords), and we accumulated the gradients of 128 mini-batches for an update (Ott et al., 2018). We trained the model for 20,000 ite"
W19-5365,P16-1009,0,0.14271,"eps, whereas we use a relatively simple method for Ja-En direction. For Ja-En, we tokenized the raw text into subwords by simply applying sentencepiece with the vocabulary size of 32,000 for each language side (Kudo, 2018; Kudo and Richardson, 2018). For En-Ja, we tokenized the text by KyTea (Neubig et al., 2011) and the Moses tokenizer (Koehn et al., 2007) for Japanese and English, respectively. We also truecased the English words by the script provided with Moses toolkits2 . Then we further tokenized the words into subwords using joint Byte-Pair-Encoding (BPE) with 16,000 merge operations3 (Sennrich et al., 2016b). provided monolingual corpus, and noisy data filtering for its data. (Section 2.3). • Placeholder mechanism to handle tokens that should be copied from a source-side sentence (Section 2.4). NMT Model Neural Machine Translation (NMT) has been making remarkable progress in the field of MT (Bahdanau et al., 2015; Luong et al., 2015). However, most existing MT systems still struggle with noisy text and easily make mistranslations (Belinkov and Bisk, 2018), though the Transformer has achieved the state-of-the-art performance in several MT tasks (Vaswani et al., 2017). In our submission system, w"
W19-5365,P16-1162,0,0.229085,"Missing"
W19-5365,W17-4811,0,0.0250148,"anslation quality. In future work, we will explore ways to use monolingual data more effectively, introduce contextual information, and deal with a variety of noisy tokens such as abbreviations, ASCII-arts, and grammar errors. Use of Contextual Information Some sentences need contextual information for them to be precisely translated. The MTNT corpus provides comment IDs as the contextual information to group sentences from the same original comment. We did not use the contextual information in our systems, but we consider that it would help to improve translation quality as in previous work (Tiedemann and Scherrer, 2017; Bawden et al., 2018). For example, in the following two sentences, “Airborne school isn’t a hard school.” and “Get in there with some confidence!”, which can be found in the MTNT corpus and have the same comment ID, we consider that leveraging their contextual information would help to clarify what “there” means in the latter and to translate it more accurately. 5 Acknowledgments We thank two anonymous reviewers for their careful reading and insightful comments and suggestions. References Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning"
W19-5365,W18-6401,0,\N,Missing
W19-5365,W18-6408,0,\N,Missing
W19-5365,W18-6429,0,\N,Missing
W19-5365,N18-1118,0,\N,Missing
W19-6616,W17-3206,0,0.0216436,"1 , x0 ), . . . , y022 (x±5 , x0 ), we select the translation that yields the highest 3 In the evaluation discussed in Section 7.1, forced backtranslation using the 1-to-1 model achieved merely the same BLEU scores as that of the 2-to-1 model. Dublin, Aug. 19-23, 2019 |p. 166 forced back-translation probability B when backtranslating into the source sentence x0 as below:   (i = 0)  B x0 , y011 (x0 )  argmax  0) B x0 , yi22 (xi , x0 ), (i =  i=0,±1,...,±5 y022 (xi , x0 ) Employing the forced back-translation probability differs from existing approaches (Rapp, 2009; Li and Jurafsky, 2016; Goto and Tanaka, 2017; Kimura et al., 2017) that incorporate backtranslation from the translated target sentence to the source sentence. Rapp (2009) employed the BLEU score between the source sentence and source language sentence back-translated from the target translated sentence in an automatic MT evaluation context. Li and Jurafsky (Li and Jurafsky, 2016) proposed to re-rank decoded translations based on mutual information between source and target sentences x and y i.e., the probabilities p(y |x) and p(x |y). Goto and Tanaka (2017) and Kimura et al. (2017) also employed the ratio of forced back-translation pro"
W19-6616,P17-4012,0,0.0260002,"ion 5 when back-translating y022 (xi , x0 ) (i = 0, i.e., translated from x0 with a context sentence by the 2-to-2 model ). 6 In training and development, the encoder rejects input sentences (source sentence concatenated with the context sentence for the 2-to-2 models) with greater than 50 tokens. Average token length of the 10,000 pairs for oracle statistics and evaluation is 7.9 (English) and 6.9 (Japanese). 7 Experimental setup is as follows: Tokenizers are Moses tokenizer (Koehn et al., 2007) for English and MeCab ( http://taku910.github.io/mecab/ ) for Japanese tokenization. OpenNMT-py (Klein et al., 2017) is used for training and testing NMT models. 50,000 vocabulary sizes are employed for both English and Japanese. Embedding sizes are 512. Encoder and decoder are with six layers with batch size as 4,096 and dropout rate as 0.3 and 100,000 steps for training. Adam optimizer (Kingma and Ba, 2015) is used. One NVIDIA Tesla P100 16GB GPU is used. MTEval Toolkit ( https://github.com/odashi/mteval ) is used to measure BLEU, and Moses decoder’s sentence-bleu.cpp is used to measure sentence-BLEU. Dublin, Aug. 19-23, 2019 |p. 167 with the maximum sentence-BLEU score among the candidate translations af"
W19-6616,P07-2045,0,0.00664312,"(x0 ) (i = 0), while we used the 2-to-1 Transformer model (denoted as back-tran21 ) with the setup described in section 5 when back-translating y022 (xi , x0 ) (i = 0, i.e., translated from x0 with a context sentence by the 2-to-2 model ). 6 In training and development, the encoder rejects input sentences (source sentence concatenated with the context sentence for the 2-to-2 models) with greater than 50 tokens. Average token length of the 10,000 pairs for oracle statistics and evaluation is 7.9 (English) and 6.9 (Japanese). 7 Experimental setup is as follows: Tokenizers are Moses tokenizer (Koehn et al., 2007) for English and MeCab ( http://taku910.github.io/mecab/ ) for Japanese tokenization. OpenNMT-py (Klein et al., 2017) is used for training and testing NMT models. 50,000 vocabulary sizes are employed for both English and Japanese. Embedding sizes are 512. Encoder and decoder are with six layers with batch size as 4,096 and dropout rate as 0.3 and 100,000 steps for training. Adam optimizer (Kingma and Ba, 2015) is used. One NVIDIA Tesla P100 16GB GPU is used. MTEval Toolkit ( https://github.com/odashi/mteval ) is used to measure BLEU, and Moses decoder’s sentence-bleu.cpp is used to measure sen"
W19-6616,P09-2034,0,0.0401943,"e translations y011 (x0 ), y022 (x±1 , x0 ), . . . , y022 (x±5 , x0 ), we select the translation that yields the highest 3 In the evaluation discussed in Section 7.1, forced backtranslation using the 1-to-1 model achieved merely the same BLEU scores as that of the 2-to-1 model. Dublin, Aug. 19-23, 2019 |p. 166 forced back-translation probability B when backtranslating into the source sentence x0 as below:   (i = 0)  B x0 , y011 (x0 )  argmax  0) B x0 , yi22 (xi , x0 ), (i =  i=0,±1,...,±5 y022 (xi , x0 ) Employing the forced back-translation probability differs from existing approaches (Rapp, 2009; Li and Jurafsky, 2016; Goto and Tanaka, 2017; Kimura et al., 2017) that incorporate backtranslation from the translated target sentence to the source sentence. Rapp (2009) employed the BLEU score between the source sentence and source language sentence back-translated from the target translated sentence in an automatic MT evaluation context. Li and Jurafsky (Li and Jurafsky, 2016) proposed to re-rank decoded translations based on mutual information between source and target sentences x and y i.e., the probabilities p(y |x) and p(x |y). Goto and Tanaka (2017) and Kimura et al. (2017) also emp"
W19-6616,P15-4020,0,0.0524053,"Missing"
W19-6616,W17-4811,0,0.511817,"ral machine translation (NMT) models (Sutskever et al., 2014; Luong et al., 2015; c 2019 The authors. This article is licensed under a Creative  Commons 4.0 licence, no derivative works, attribution, CCBY-ND. Proceedings of MT Summit XVII, volume 1 Vaswani et al., 2017) have made remarkable progress. Most NMT models are designed to translate a single sentence and do not accept input greater than one sentence, i.e., input sentences that include additional context information. However, recently, several approaches that attempt to translate inputs with more than one sentence have been proposed (Tiedemann and Scherrer, 2017; Libovick´y and Helcl, 2017; Maruf and Haffari, 2018; Miculicich et al., 2018; Bawden et al., 2018; Voita et al., 2018; Tu et al., 2018). These approaches to context-based NMT models can be roughly categorized according to the width of the context considered in those models. A typical approach is to consider the sentence immediately preceding the source sentence to be translated as the context (Tiedemann and Scherrer, 2017; Libovick´y and Helcl, 2017; Bawden et al., 2018; Voita et al., 2018). Context-based NMT models can be further categorized according to whether the source and context sente"
W19-6616,Q18-1029,0,0.287154,"mons 4.0 licence, no derivative works, attribution, CCBY-ND. Proceedings of MT Summit XVII, volume 1 Vaswani et al., 2017) have made remarkable progress. Most NMT models are designed to translate a single sentence and do not accept input greater than one sentence, i.e., input sentences that include additional context information. However, recently, several approaches that attempt to translate inputs with more than one sentence have been proposed (Tiedemann and Scherrer, 2017; Libovick´y and Helcl, 2017; Maruf and Haffari, 2018; Miculicich et al., 2018; Bawden et al., 2018; Voita et al., 2018; Tu et al., 2018). These approaches to context-based NMT models can be roughly categorized according to the width of the context considered in those models. A typical approach is to consider the sentence immediately preceding the source sentence to be translated as the context (Tiedemann and Scherrer, 2017; Libovick´y and Helcl, 2017; Bawden et al., 2018; Voita et al., 2018). Context-based NMT models can be further categorized according to whether the source and context sentences are encoded using a single (Tiedemann and Scherrer, 2017) or multiple encoders (Libovick´y and Helcl, 2017; Bawden et al., 2018; Voi"
W19-6616,P18-1117,0,0.0656541,"der a Creative  Commons 4.0 licence, no derivative works, attribution, CCBY-ND. Proceedings of MT Summit XVII, volume 1 Vaswani et al., 2017) have made remarkable progress. Most NMT models are designed to translate a single sentence and do not accept input greater than one sentence, i.e., input sentences that include additional context information. However, recently, several approaches that attempt to translate inputs with more than one sentence have been proposed (Tiedemann and Scherrer, 2017; Libovick´y and Helcl, 2017; Maruf and Haffari, 2018; Miculicich et al., 2018; Bawden et al., 2018; Voita et al., 2018; Tu et al., 2018). These approaches to context-based NMT models can be roughly categorized according to the width of the context considered in those models. A typical approach is to consider the sentence immediately preceding the source sentence to be translated as the context (Tiedemann and Scherrer, 2017; Libovick´y and Helcl, 2017; Bawden et al., 2018; Voita et al., 2018). Context-based NMT models can be further categorized according to whether the source and context sentences are encoded using a single (Tiedemann and Scherrer, 2017) or multiple encoders (Libovick´y and Helcl, 2017; Bawden"
W19-6616,D17-1018,0,0.0227174,"Missing"
W19-6616,P17-2031,0,0.0699766,"Missing"
W19-6616,L18-1275,0,0.0875507,"Missing"
W19-6616,D15-1166,0,0.121673,"d. Experimental results with Japanese and English parallel sentences from the OpenSubtitles2018 corpus demonstrate that, when the context length of ﬁve preceding and ﬁve subsequent sentences are examined, the proposed approach achieved signiﬁcant improvements of 0.74 (Japanese to English) and 1.14 (English to Japanese) BLEU scores compared to the baseline 2-to-2 model, where the oracle translation achieved upper bounds improvements of 5.88 (Japanese to English) and 9.10 (English to Japanese) BLEU scores. 1 Introduction Recently, neural machine translation (NMT) models (Sutskever et al., 2014; Luong et al., 2015; c 2019 The authors. This article is licensed under a Creative  Commons 4.0 licence, no derivative works, attribution, CCBY-ND. Proceedings of MT Summit XVII, volume 1 Vaswani et al., 2017) have made remarkable progress. Most NMT models are designed to translate a single sentence and do not accept input greater than one sentence, i.e., input sentences that include additional context information. However, recently, several approaches that attempt to translate inputs with more than one sentence have been proposed (Tiedemann and Scherrer, 2017; Libovick´y and Helcl, 2017; Maruf and Haffari, 201"
W19-6616,P18-1118,0,0.287562,"14; Luong et al., 2015; c 2019 The authors. This article is licensed under a Creative  Commons 4.0 licence, no derivative works, attribution, CCBY-ND. Proceedings of MT Summit XVII, volume 1 Vaswani et al., 2017) have made remarkable progress. Most NMT models are designed to translate a single sentence and do not accept input greater than one sentence, i.e., input sentences that include additional context information. However, recently, several approaches that attempt to translate inputs with more than one sentence have been proposed (Tiedemann and Scherrer, 2017; Libovick´y and Helcl, 2017; Maruf and Haffari, 2018; Miculicich et al., 2018; Bawden et al., 2018; Voita et al., 2018; Tu et al., 2018). These approaches to context-based NMT models can be roughly categorized according to the width of the context considered in those models. A typical approach is to consider the sentence immediately preceding the source sentence to be translated as the context (Tiedemann and Scherrer, 2017; Libovick´y and Helcl, 2017; Bawden et al., 2018; Voita et al., 2018). Context-based NMT models can be further categorized according to whether the source and context sentences are encoded using a single (Tiedemann and Scherr"
W19-6616,D18-1325,0,0.155944,"2019 The authors. This article is licensed under a Creative  Commons 4.0 licence, no derivative works, attribution, CCBY-ND. Proceedings of MT Summit XVII, volume 1 Vaswani et al., 2017) have made remarkable progress. Most NMT models are designed to translate a single sentence and do not accept input greater than one sentence, i.e., input sentences that include additional context information. However, recently, several approaches that attempt to translate inputs with more than one sentence have been proposed (Tiedemann and Scherrer, 2017; Libovick´y and Helcl, 2017; Maruf and Haffari, 2018; Miculicich et al., 2018; Bawden et al., 2018; Voita et al., 2018; Tu et al., 2018). These approaches to context-based NMT models can be roughly categorized according to the width of the context considered in those models. A typical approach is to consider the sentence immediately preceding the source sentence to be translated as the context (Tiedemann and Scherrer, 2017; Libovick´y and Helcl, 2017; Bawden et al., 2018; Voita et al., 2018). Context-based NMT models can be further categorized according to whether the source and context sentences are encoded using a single (Tiedemann and Scherrer, 2017) or multiple enc"
W19-7203,N18-1118,0,0.0171848,"n the proposed multi-hop attention mechanism. In evaluation, we compared the performance of the proposed method with Transformer and RNN encoder-decoder using OpenSubtitles 2018 (Lison et al., 2018) and Asian Scientific Paper Excerpt Corpus (ASPEC) (Nakazawa et al., 2016). To test the power of translating long sentences, we also Dublin, Aug. 20, 2019 |p. 24 (a) Baseline RNN-based model (b) Multi-head RNN model (c) Hierarchical attention model (d) Proposed method: Multi-hop attention model Figure 1: Baseline attention and proposed attention made a context-aware translation model, called 2to-2 (Bawden et al., 2018; Tiedemann and Scherrer, 2017) for OpenSubtitles 2018. In the Japaneseto-English translation of the ASPEC corpus, the proposed method achieved a significantly better score than the Transformer for long sentences with more than 120 tokens. In the following sections, we first show previous works on baseline RNN and multi-head RNN encoder-decoders in Section 2. We then describe the proposed multi-hop method in Section 3. We then show the performance for Japanese-toEnglish and English-to-Japanese translation tasks, focusing on long sentences in Section 4. 2 Neural Machine Translation 2.1 RNN base"
W19-7203,P18-1008,0,0.0315345,"ni et al., 2017) is used to calcu(k) late the context vector ci between k-th head of a (k) decoder state si and encoder states H. When the model has two heads (N = 2), the equation (1) and the equation (2) becomes as follows. si (1) = Wa(1) di (3) (2) si = Wa(2) di (4) (1) (1) = sof tmax(si H T )H (5) (2) ci (2) sof tmax(si H T )H (6) ci = Dublin, Aug. 20, 2019 |p. 25 Figure 2: Proposed method detail As shown in the equation (5) and the equation (6), by using multiple parallel attention via the param(k) eters Wa , we expect that each head will attend to a different part of the encoder states. Chen et al. (2018) attempted to incorporate the various mechanisms of the Transformer into RNN encoder-decoder. They used multi-head attention as shown in Figure 1(b) in source-target attention. Our method becomes the same as their method when we use single-hop attention. 3 Multi-Hop Attention RNN 3.1 Multi-Hop Dependent Attention To the best of our knowledge, multi-hop attention is first used in end-to-end memory network (Sukhbaatar et al., 2015) to extend the expressive power of RNN. To introduce multi-hop attention into translation, we refer to hierarchical attention (Libovick´y and Helcl, 2017) in multimoda"
W19-7203,N19-1423,0,0.0143468,"i-hop independent attention) Proposed Method (multi-hop dependent attention) Transformer 5 head 1 2 3 hop 1 1 1 Parameter 68,460,544 70,557,696 72,654,848 2 2 72,654,848 2 2 3 3 4 2 3 2 3 1 75,800,576 81,043,456 79,994,880 87,334,912 81,604,608 Related Works Dehghani et al. (2019) proposed Universal Transformer for solving the problems of Transformer including the weakness for long distance dependency. Although it has a mechanism to repeat updating the states for each word with parameters shared, it requires a larger number of parameters than Transformer. There could be an approach like BERT (Devlin et al., 2019) where the number of parameters is increased significantly to make a more powerful Transformer model. Our approach, on the other hand, improves the strength of RNN with a little increase of parameters as shown in Table 5. Moreover, Iida et al. (2019) also applied the multi-hop attention mechanism to the Transformer and reported that the Transformer augmented with the multi-hop attention mechanism significantly outperformed the Transformer. Among other existing approaches to neural machine translation, it is known that ConvS2S (Gehring et al., 2017) is equipped with multiple decoder layers wher"
W19-7203,W18-6412,0,0.0192606,"the N context vectors ci with the RNN decoder state di to obtain the prediction of the output word distribution p(yi |yi−1 , X) where Wo is a learnable parameter. ′(1) ′(k) ]) (16) p(yi |yi−1 , X) = sof tmax(oi ) (17) oi = tanh(Wo [di ; ci ; ...; ci When the number of heads N is 2, equation (16) becomes the following: ′(1) ′(2) oi = tanh(Wo [di ; ci ; ci ]) (18) 3.2 Multi-Hop Independent Attention In the multi-hop dependent attention described in the previous subsection, we use the information of other heads and share parameters of MLP attention (Wb and vb ) over all heads (equation (7)) to 1 Haddow et al. (2018) evaluated a similar multi-head and multi-hop attention mechanism, although Haddow et al. (2018) employed the vector concatenation over the multiple heads in stead of normalization. Haddow et al. (2018) also reported that the multi-head and multi-hop attention mechanism outperformed the baseline RNN model in the evaluation of the language pairs of CS-EN, EN-CS, ET-EN, EN-ET, FIEN, and EN-FI, where the length of the training sentences is limited to 50 words or less. In this paper, on the other hand, in the evaluation of the language pairs of JA-EN and EN-JA, the proposed multi-head and multi-ho"
W19-7203,P19-2030,1,0.830527,"Works Dehghani et al. (2019) proposed Universal Transformer for solving the problems of Transformer including the weakness for long distance dependency. Although it has a mechanism to repeat updating the states for each word with parameters shared, it requires a larger number of parameters than Transformer. There could be an approach like BERT (Devlin et al., 2019) where the number of parameters is increased significantly to make a more powerful Transformer model. Our approach, on the other hand, improves the strength of RNN with a little increase of parameters as shown in Table 5. Moreover, Iida et al. (2019) also applied the multi-hop attention mechanism to the Transformer and reported that the Transformer augmented with the multi-hop attention mechanism significantly outperformed the Transformer. Among other existing approaches to neural machine translation, it is known that ConvS2S (Gehring et al., 2017) is equipped with multiple decoder layers where each decoder layer has a separate attention module. The attention of each of those multiple layers is computed and is then fed to another layer, which then takes the fed information into account when computing its own attention etc. The way those m"
W19-7203,P07-2045,0,0.0122451,"ed so as to keep the total number of word tokens within each subset as 20,000. We do not set any upper bound of sentence length in training/development/test. This is for the purpose of evaluating the capability of the proposed method against long sentences. For tokenization, we used the SentencePiece tool (Kudo and Richardson, 2018) to set the vocabulary size of 32,000 each for both Japanese and English in order to avoid unknown words. Before splitting into subword units by SentencePiece, tokenization is performed by the morphological analysis tool MeCab2 for Japanese, and by Moses Tokenizer (Koehn et al., 2007) for English3 . 2 3 http://taku910.github.io/mecab/ By performing tokenization before splitting into subword Dublin, Aug. 20, 2019 |p. 28 Table 3: BLEU per sentence length (ASPEC ja→en) sentence length number of sentences RNN baseline multi-hop dependent (head2, hop2) Transformer 0-9 1594 5.94 10-19 1248 18.47 20-29 810 27.10 30-39 579 27.16 40-49 457 25.44 50-59 372 22.97 60-69 315 23.71 70-79 272 21.70 80-89 238 21.34 90-99 214 20.96 100-109 192 23.14 110-119 176 21.18 120-129 162 19.78 130-139 151 18.73 6.40‡ 19.43‡ 27.62 27.78 26.49† 24.08‡ 25.02‡ 22.42 22.74‡ 22.72‡ 23.11 20.62 20.56†† 20"
W19-7203,D18-2012,0,0.0421826,"Missing"
W19-7203,P17-2031,0,0.031579,"Missing"
W19-7203,L18-1275,0,0.0251852,"chanism, those increased number of parameters are well-tuned so that the overall translation accuracy improves, in particular, for long sentences. The proposed multi-hop attention mechanism is based on the hierarchical attention (Libovick´y and Helcl, 2017) for multi-source encoders, although, in the hierarchical attention (Libovick´y and Helcl, 2017), the number of parameters for one input does not increase, unlike in the proposed multi-hop attention mechanism. In evaluation, we compared the performance of the proposed method with Transformer and RNN encoder-decoder using OpenSubtitles 2018 (Lison et al., 2018) and Asian Scientific Paper Excerpt Corpus (ASPEC) (Nakazawa et al., 2016). To test the power of translating long sentences, we also Dublin, Aug. 20, 2019 |p. 24 (a) Baseline RNN-based model (b) Multi-head RNN model (c) Hierarchical attention model (d) Proposed method: Multi-hop attention model Figure 1: Baseline attention and proposed attention made a context-aware translation model, called 2to-2 (Bawden et al., 2018; Tiedemann and Scherrer, 2017) for OpenSubtitles 2018. In the Japaneseto-English translation of the ASPEC corpus, the proposed method achieved a significantly better score than t"
W19-7203,D15-1166,0,0.524119,"op attention model has two heads, where for each head, a context vector is calculated based on the states of the encoder and the decoder. Then, in the second turn of the context vector calculation, those context vectors are updated depending not only on one’s own context vector but also on the context vector of the other head. Experimental results show that the proposed model significantly outperforms the baseline in BLEU score in Japanese-to-English/English-toJapanese machine translation tasks with and without extended context. 1 Introduction RNN encoder-decoder model (Bahdanau et al., 2015; Luong et al., 2015; Sutskever et al., © 2019 The authors. This article is licensed under a Creative Commons 4.0 licence, no derivative works, attribution, CCBY-ND. Proceedings of The 8th Workshop on Patent and Scientific Literature Translation 2014) was the state-of-the-art in machine translation. However, it is outperformed by nonrecursive encoder-decoder models such as Transformer (Vaswani et al., 2017) and Convolutional Sequence-to-Sequence (Gehring et al., 2017) in recent years. However, RNN is not considered to be inferior to Transformer in all respects. For example, according to Tran et al. (2018), it is"
W19-7203,L16-1350,0,0.174966,"e overall translation accuracy improves, in particular, for long sentences. The proposed multi-hop attention mechanism is based on the hierarchical attention (Libovick´y and Helcl, 2017) for multi-source encoders, although, in the hierarchical attention (Libovick´y and Helcl, 2017), the number of parameters for one input does not increase, unlike in the proposed multi-hop attention mechanism. In evaluation, we compared the performance of the proposed method with Transformer and RNN encoder-decoder using OpenSubtitles 2018 (Lison et al., 2018) and Asian Scientific Paper Excerpt Corpus (ASPEC) (Nakazawa et al., 2016). To test the power of translating long sentences, we also Dublin, Aug. 20, 2019 |p. 24 (a) Baseline RNN-based model (b) Multi-head RNN model (c) Hierarchical attention model (d) Proposed method: Multi-hop attention model Figure 1: Baseline attention and proposed attention made a context-aware translation model, called 2to-2 (Bawden et al., 2018; Tiedemann and Scherrer, 2017) for OpenSubtitles 2018. In the Japaneseto-English translation of the ASPEC corpus, the proposed method achieved a significantly better score than the Transformer for long sentences with more than 120 tokens. In the follow"
W19-7203,P02-1040,0,0.104028,"Missing"
W19-7203,W17-4811,0,0.0183823,"hop attention mechanism. In evaluation, we compared the performance of the proposed method with Transformer and RNN encoder-decoder using OpenSubtitles 2018 (Lison et al., 2018) and Asian Scientific Paper Excerpt Corpus (ASPEC) (Nakazawa et al., 2016). To test the power of translating long sentences, we also Dublin, Aug. 20, 2019 |p. 24 (a) Baseline RNN-based model (b) Multi-head RNN model (c) Hierarchical attention model (d) Proposed method: Multi-hop attention model Figure 1: Baseline attention and proposed attention made a context-aware translation model, called 2to-2 (Bawden et al., 2018; Tiedemann and Scherrer, 2017) for OpenSubtitles 2018. In the Japaneseto-English translation of the ASPEC corpus, the proposed method achieved a significantly better score than the Transformer for long sentences with more than 120 tokens. In the following sections, we first show previous works on baseline RNN and multi-head RNN encoder-decoders in Section 2. We then describe the proposed multi-hop method in Section 3. We then show the performance for Japanese-toEnglish and English-to-Japanese translation tasks, focusing on long sentences in Section 4. 2 Neural Machine Translation 2.1 RNN based sequence to sequence NMT Ther"
W19-7203,D18-1503,0,0.0255545,", 2015; Luong et al., 2015; Sutskever et al., © 2019 The authors. This article is licensed under a Creative Commons 4.0 licence, no derivative works, attribution, CCBY-ND. Proceedings of The 8th Workshop on Patent and Scientific Literature Translation 2014) was the state-of-the-art in machine translation. However, it is outperformed by nonrecursive encoder-decoder models such as Transformer (Vaswani et al., 2017) and Convolutional Sequence-to-Sequence (Gehring et al., 2017) in recent years. However, RNN is not considered to be inferior to Transformer in all respects. For example, according to Tran et al. (2018), it is reported that Transformer is not good at decoding sentences whose length is not included in the training data and it is weak to long distance dependency. In other words, it is weak against long sentence translation. It seems that Transformer became more powerful than RNN by increasing the number of parameters, but it became weak to long sentences for the same reason. We propose an RNN based source-to-target attention mechanism where the number of parameters increases by repeating the calculation of multihead attention for a single-source encoder like multi-hop attention in end-to-end m"
W96-0205,A92-1018,0,0.0659678,"Missing"
W96-0205,C94-1101,0,0.179908,"Missing"
W96-0205,C94-1032,1,0.926282,"Missing"
W96-0205,C96-2136,1,0.880707,"Missing"
W96-0205,P94-1010,0,0.023449,"Missing"
W96-0205,W95-0109,0,0.0349653,"Missing"
W96-0205,A88-1019,0,0.205816,"Missing"
W96-0205,J96-3004,0,\N,Missing
W97-0120,P96-1019,0,0.0469835,"Missing"
W97-0120,C94-1032,1,0.882317,"Missing"
W97-0120,C96-2136,1,0.892579,"Missing"
W97-0120,J96-3004,0,0.112481,"Missing"
W97-0120,W96-0113,0,0.315012,"Missing"
W97-0120,W95-0109,0,\N,Missing
Y09-2046,A88-1019,0,0.0487432,"terview on the 5th.” Figure 1: NER vs BaseNP Supersense Tagging The rest of this paper is organized as follows. We describe a proposed definition of Japanese baseNP in Section 2. Next, we describe the Nihongo Goi Taikei and the supersenses defined on it in Section 3. We describe the algorithm for baseNP supersense tagging as a sequential labeling task in Section 4. In Section 5, we show our experiments and results. Our conclusions are provided in Section 6. 2 BaseNP in Japanese The baseNP in English is defined as non-recursive noun phrase, i.e., a noun phrase not containing other noun phrase (Church, 1988) (Ramshaw and Marcus, 1995). We consider Bunsetsu phrases in Japanese excluding predicates (predicate bunsetsu phrases) as a possible candidate of the definition of the baseNP in Japanese. Bunsetsu phrase is a phonological unit of Japanese, containing one content word. However, Bunsetsu phrases often contain functional words and the meanings of more than two phrases sometimes change from that of the base phrase. So, we defined a definition of baseNP in Japanese as below. 1. Word sequence in phrases (Bunsetsu in Japanese) obtained by morphological analysis, excluding functional words that at th"
Y09-2046,W06-1670,0,0.128727,"HMM, and achieve high performance compared to a baseline. Keywords: Supersense, BaseNP, Named Entity, Predicate Argument Structure Analysis, Semantic Role Labeling. 1 Introduction Named entity recognition (NER), has been useful for various natural language processing tasks such as searching for answer candidates in factoid question answering systems. However, if the answer is a common noun, NER cannot recognize the answer candidate. Ciaramita et al. proposed supersense tagging for noun phrases including common nouns and showed that the task has many applications (Ciaramita and Johnson, 2003) (Ciaramita and Altun, 2006). Moreover, predicate argument structure analysis has attracted the attention of researchers recently because this information can increase the precision of text processing tasks, such as machine translation, information extraction (Hirschman et al., 1999), question answering (Shen and Lapata, 2007), and summarization (Melli et al., 2005). In the analysis, it is necessary to determine argument candidates, i.e. argument base noun phrases (baseNPs) before determining the semantic role of the candidates (Pradhan et al., 2004), and high performance noun phrase chunking is expected. Furthermore, su"
Y09-2046,W03-1022,0,0.157319,"the averaged perceptron with HMM, and achieve high performance compared to a baseline. Keywords: Supersense, BaseNP, Named Entity, Predicate Argument Structure Analysis, Semantic Role Labeling. 1 Introduction Named entity recognition (NER), has been useful for various natural language processing tasks such as searching for answer candidates in factoid question answering systems. However, if the answer is a common noun, NER cannot recognize the answer candidate. Ciaramita et al. proposed supersense tagging for noun phrases including common nouns and showed that the task has many applications (Ciaramita and Johnson, 2003) (Ciaramita and Altun, 2006). Moreover, predicate argument structure analysis has attracted the attention of researchers recently because this information can increase the precision of text processing tasks, such as machine translation, information extraction (Hirschman et al., 1999), question answering (Shen and Lapata, 2007), and summarization (Melli et al., 2005). In the analysis, it is necessary to determine argument candidates, i.e. argument base noun phrases (baseNPs) before determining the semantic role of the candidates (Pradhan et al., 2004), and high performance noun phrase chunking"
Y09-2046,isahara-etal-2008-development,0,0.0327746,"outline of Japanese vocabulary), for supersenses. Nihongo Goi Taikei was originally developed for a Japanese-to-English machine translation system, ALT-J/E. It has three different semantic category hierarchies for common nouns, proper nouns, and verbs. Only the common noun category is widely used. The thesaurus consists of a hierarchy of 2,710 semantic classes, defined for over 264,312 nouns, with a maximum depth of twelve (Ikehara et al., 1997). The coverage for nouns are larger comparing with other Japanese thesaurus including Bunrui Goi Hyo (NIJL, 2004)(96,051 words) and Japanese WordNet (Isahara et al., 2008)(85,966 words (Ver.0.9)). We used the semantic classes of the third level as supersenses because the level is similar to semantic roles. The top three levels of the Nihongo Goi Taikei common noun thesaurus are shown in Figure 3. For example, the Japanese word ライター (raitaa), which is derived from two different English words “writer” and “lighter”, but transliterated into the same Japanese string, has two different semantic categories, (353:author) and (915:household appliance). By following the is-a link, we can learn that the former sense refers to a person (4: person) while the latter sense r"
Y09-2046,N06-1023,0,0.0130784,"a, 2007), and summarization (Melli et al., 2005). In the analysis, it is necessary to determine argument candidates, i.e. argument base noun phrases (baseNPs) before determining the semantic role of the candidates (Pradhan et al., 2004), and high performance noun phrase chunking is expected. Furthermore, supersenses annotated for NPs are helpful for predicate argument structure analysis (Taira et al., 2008), because we can use the case frame of verbs with semantic categories such as the NTT pattern pair dictionary (Fujita and Bond, 2008) and the large-scale case frame dictionary from the web (Kawahara and Kurohashi, 2006). Although baseNP chunking is a basic task in English and there are a lot of researches, the concept of baseNP in Japanese has been unclear. We propose a definition of Japanese baseNP in this paper. We show the difference between NER and baseNP supersense tagging in Figure 1. Suppose that the sentence “彼は 5 日に記者会見を開いた。(He held a press interview on the 5th.)” is entered to the system. In this case, while NER only detects the ‘TIME’ phrase as ‘5 日 (the 5th)’, the baseNP supersense tagger can recognize the common noun phrases, ‘彼 (he)’ and ‘記者会見 (press interview)’ as ‘HUMAN’ and ‘HUMAN ACTIVITY’,"
Y09-2046,P03-1004,0,0.0165443,"document d in Document Set do for sentence s in d do predict predict Compute yd,s,1 P... yd,s,max s.t. maximize wt for words in s, xd,s,1 ... xd,s,max using Viterbi algorithm for xd,s,i in s do gold predict if yd,s,i 6= yd,s,i then gold predict ) −Φ ( xd,s,i , yd,s,i wt+1 ← wt + Φ ( xd,s,i , yd,s,i ) end if end for end for end for end forP w = T1 t wt Output w Figure 4: Training algorithm • Dependency ( dep0 ) The combination of functional words in the phrase containing the target word, and the head word in the phrase the target word depends on. The dependency analysis is obtained by Cabocha (Kudo and Matsumoto, 2003), which is a Japanese dependency analyzer, and we fixed the mistaken dependencies by hand. • Next tag ( y+1 ) We also used the predicted supersense of the next word. 4.3 Sequential Tag Format We can understand the baseNP tagging task as a sequential labeling task (Ciaramita and Altun, 2006). There are some different formats for encoding chunks in the sequences (Sang and Veenstra, 1999) (Uchimoto et al., 2000). Kudo et al. indicated that the performance is the highest in a baseNP chunking task (not including supersense tagging) in English with SVM when they used the IOE2 format and the processi"
Y09-2046,N04-1030,0,0.0326006,"that the task has many applications (Ciaramita and Johnson, 2003) (Ciaramita and Altun, 2006). Moreover, predicate argument structure analysis has attracted the attention of researchers recently because this information can increase the precision of text processing tasks, such as machine translation, information extraction (Hirschman et al., 1999), question answering (Shen and Lapata, 2007), and summarization (Melli et al., 2005). In the analysis, it is necessary to determine argument candidates, i.e. argument base noun phrases (baseNPs) before determining the semantic role of the candidates (Pradhan et al., 2004), and high performance noun phrase chunking is expected. Furthermore, supersenses annotated for NPs are helpful for predicate argument structure analysis (Taira et al., 2008), because we can use the case frame of verbs with semantic categories such as the NTT pattern pair dictionary (Fujita and Bond, 2008) and the large-scale case frame dictionary from the web (Kawahara and Kurohashi, 2006). Although baseNP chunking is a basic task in English and there are a lot of researches, the concept of baseNP in Japanese has been unclear. We propose a definition of Japanese baseNP in this paper. We show"
Y09-2046,W95-0107,0,0.0836606,"5th.” Figure 1: NER vs BaseNP Supersense Tagging The rest of this paper is organized as follows. We describe a proposed definition of Japanese baseNP in Section 2. Next, we describe the Nihongo Goi Taikei and the supersenses defined on it in Section 3. We describe the algorithm for baseNP supersense tagging as a sequential labeling task in Section 4. In Section 5, we show our experiments and results. Our conclusions are provided in Section 6. 2 BaseNP in Japanese The baseNP in English is defined as non-recursive noun phrase, i.e., a noun phrase not containing other noun phrase (Church, 1988) (Ramshaw and Marcus, 1995). We consider Bunsetsu phrases in Japanese excluding predicates (predicate bunsetsu phrases) as a possible candidate of the definition of the baseNP in Japanese. Bunsetsu phrase is a phonological unit of Japanese, containing one content word. However, Bunsetsu phrases often contain functional words and the meanings of more than two phrases sometimes change from that of the base phrase. So, we defined a definition of baseNP in Japanese as below. 1. Word sequence in phrases (Bunsetsu in Japanese) obtained by morphological analysis, excluding functional words that at the end of the last Bunsetsu."
Y09-2046,E99-1023,0,0.0476285,"hm • Dependency ( dep0 ) The combination of functional words in the phrase containing the target word, and the head word in the phrase the target word depends on. The dependency analysis is obtained by Cabocha (Kudo and Matsumoto, 2003), which is a Japanese dependency analyzer, and we fixed the mistaken dependencies by hand. • Next tag ( y+1 ) We also used the predicted supersense of the next word. 4.3 Sequential Tag Format We can understand the baseNP tagging task as a sequential labeling task (Ciaramita and Altun, 2006). There are some different formats for encoding chunks in the sequences (Sang and Veenstra, 1999) (Uchimoto et al., 2000). Kudo et al. indicated that the performance is the highest in a baseNP chunking task (not including supersense tagging) in English with SVM when they used the IOE2 format and the processing direction was backwards (Kudo and Matsumoto, 2002). This can probably be attributed to the fact that the head word at the chunk often exists in the end of the chunk. The situation is similar in Japanese, and we adopted the IOE2 format and backward processing. Figure 5 shows an example of the IOE2 tag format and a part of features in our task. 5 5.1 Experiments Experimental Setting W"
Y09-2046,N03-1028,0,0.0143716,"423:existence 2432:system 2443:relationship 2483:property 2507:state 2564:shape 2585:amount 2610:location 2670:time Figure 3: Top 3 levels of the Japanese thesaurus, ‘Nihongo Goi Taikei’ 4 BaseNP Supersense Tagging as Sequential Labeling 4.1 Averaged Perceptron with HMM We used the averaged perceptron algorithm with HMM (Collins, 2002) for sequential tagging. Although a perceptron algorithm generally tends to overfit the training data, it avoids overfitting using a sort of voting method. The performance of the algorithm is reportedly comparable to that of the Conditional Random Fields (CRFs) (Sha and Pereira, 2003) and the calculation is generally faster and more memory efficient than that of CRFs. The training algorithm is shown in Figure 4. Here, d is a document in a document set, s is a sentence in the document d, xd,s,i is the i-th word in the sentence s in the document d, T is gold the number of iterations, and w is a set of weights. yd,s,i is the gold standard tag for xd,s,i , and predicted is the predicted tag by the system for xd,s,i . Φ(x, y) is the feature set for (x, y). The final yd,s,i weight is calculated by averaging the weights after every iteration to avoid overfitting (Collins, 2002)."
Y09-2046,D07-1002,0,0.0197409,"andidates in factoid question answering systems. However, if the answer is a common noun, NER cannot recognize the answer candidate. Ciaramita et al. proposed supersense tagging for noun phrases including common nouns and showed that the task has many applications (Ciaramita and Johnson, 2003) (Ciaramita and Altun, 2006). Moreover, predicate argument structure analysis has attracted the attention of researchers recently because this information can increase the precision of text processing tasks, such as machine translation, information extraction (Hirschman et al., 1999), question answering (Shen and Lapata, 2007), and summarization (Melli et al., 2005). In the analysis, it is necessary to determine argument candidates, i.e. argument base noun phrases (baseNPs) before determining the semantic role of the candidates (Pradhan et al., 2004), and high performance noun phrase chunking is expected. Furthermore, supersenses annotated for NPs are helpful for predicate argument structure analysis (Taira et al., 2008), because we can use the case frame of verbs with semantic categories such as the NTT pattern pair dictionary (Fujita and Bond, 2008) and the large-scale case frame dictionary from the web (Kawahara"
Y09-2046,D08-1055,1,0.837121,"searchers recently because this information can increase the precision of text processing tasks, such as machine translation, information extraction (Hirschman et al., 1999), question answering (Shen and Lapata, 2007), and summarization (Melli et al., 2005). In the analysis, it is necessary to determine argument candidates, i.e. argument base noun phrases (baseNPs) before determining the semantic role of the candidates (Pradhan et al., 2004), and high performance noun phrase chunking is expected. Furthermore, supersenses annotated for NPs are helpful for predicate argument structure analysis (Taira et al., 2008), because we can use the case frame of verbs with semantic categories such as the NTT pattern pair dictionary (Fujita and Bond, 2008) and the large-scale case frame dictionary from the web (Kawahara and Kurohashi, 2006). Although baseNP chunking is a basic task in English and there are a lot of researches, the concept of baseNP in Japanese has been unclear. We propose a definition of Japanese baseNP in this paper. We show the difference between NER and baseNP supersense tagging in Figure 1. Suppose that the sentence “彼は 5 日に記者会見を開いた。(He held a press interview on the 5th.)” is entered to the sy"
Y09-2046,P00-1042,0,0.0330335,"he combination of functional words in the phrase containing the target word, and the head word in the phrase the target word depends on. The dependency analysis is obtained by Cabocha (Kudo and Matsumoto, 2003), which is a Japanese dependency analyzer, and we fixed the mistaken dependencies by hand. • Next tag ( y+1 ) We also used the predicted supersense of the next word. 4.3 Sequential Tag Format We can understand the baseNP tagging task as a sequential labeling task (Ciaramita and Altun, 2006). There are some different formats for encoding chunks in the sequences (Sang and Veenstra, 1999) (Uchimoto et al., 2000). Kudo et al. indicated that the performance is the highest in a baseNP chunking task (not including supersense tagging) in English with SVM when they used the IOE2 format and the processing direction was backwards (Kudo and Matsumoto, 2002). This can probably be attributed to the fact that the head word at the chunk often exists in the end of the chunk. The situation is similar in Japanese, and we adopted the IOE2 format and backward processing. Figure 5 shows an example of the IOE2 tag format and a part of features in our task. 5 5.1 Experiments Experimental Setting We performed our experime"
Y09-2046,W02-1001,0,\N,Missing
Y09-2046,N01-1025,0,\N,Missing
Y09-2052,W06-3006,0,0.0158889,"n the subject of many studies, including work on zero anaphora resolution (Isozaki and Hirao, 2003; Iida et al., 2007a). Those state-of-the-art systems utilize statistical machine learning to achieve good levels of performance. They learn the weights of features extracted from corpora such as annotated newspaper articles to obtain rules for anaphora resolution. Although most such systems target written texts, anaphora resolution is also valuable for spoken text processing, such as speech summarization (Steinbergera et al., 2007) and interactive QA systems (van Schooten and op den Akker, 2005; Fukumoto, 2006). Recently, some projects have produced corpora of anaphora on spoken texts and used them to build anaphora resolution systems based on statistical machine learning. For example, M¨uller (2007) tagged anaphora on the ICSI Meeting Corpus and performed pronoun resolution using an empirical method that utilizes a logistic regression classifier. As regards zero pronouns in Japanese spoken texts, however, there have been few studies that adopt a state-of-the-art statistical machine learning approach with large corpora. Dohsaka (1990) proposed a zero pronoun resolution system with heuristic constrai"
Y09-2052,W07-1522,0,0.122198,"on (anaphor) can be clarified by binding it with an entity in its context. A typical example of an anaphor is a pronoun. In languages where the subject is often omitted, e.g. Japanese, the omitted subject can be regarded as an anaphor, and this is called a zero pronoun. For systems that need deep text processing, such as an automatic text summarization system, it is helpful to resolve such anaphora. The task of identifying the referent of an anaphor is called anaphora resolution and this has been the subject of many studies, including work on zero anaphora resolution (Isozaki and Hirao, 2003; Iida et al., 2007a). Those state-of-the-art systems utilize statistical machine learning to achieve good levels of performance. They learn the weights of features extracted from corpora such as annotated newspaper articles to obtain rules for anaphora resolution. Although most such systems target written texts, anaphora resolution is also valuable for spoken text processing, such as speech summarization (Steinbergera et al., 2007) and interactive QA systems (van Schooten and op den Akker, 2005; Fukumoto, 2006). Recently, some projects have produced corpora of anaphora on spoken texts and used them to build ana"
Y09-2052,W03-1024,0,0.385208,"menon whereby an expression (anaphor) can be clarified by binding it with an entity in its context. A typical example of an anaphor is a pronoun. In languages where the subject is often omitted, e.g. Japanese, the omitted subject can be regarded as an anaphor, and this is called a zero pronoun. For systems that need deep text processing, such as an automatic text summarization system, it is helpful to resolve such anaphora. The task of identifying the referent of an anaphor is called anaphora resolution and this has been the subject of many studies, including work on zero anaphora resolution (Isozaki and Hirao, 2003; Iida et al., 2007a). Those state-of-the-art systems utilize statistical machine learning to achieve good levels of performance. They learn the weights of features extracted from corpora such as annotated newspaper articles to obtain rules for anaphora resolution. Although most such systems target written texts, anaphora resolution is also valuable for spoken text processing, such as speech summarization (Steinbergera et al., 2007) and interactive QA systems (van Schooten and op den Akker, 2005; Fukumoto, 2006). Recently, some projects have produced corpora of anaphora on spoken texts and use"
Y09-2052,maekawa-etal-2000-spontaneous,0,0.0488538,"r is organized as follows: In Section 2 we describe our anaphora data. In Section 3 we present a set of verbal features and show an experimental result that employs it to distinguish first persons from others. In Section 4 we propose our anaphora resolution method and show that the resolution accuracy improves by adding the verbal features. In Section 5 we discuss our method. Section 6 provides our conclusion. 2 Annotation of anaphora in CSJ In this section, we describe how we tagged anaphora on spoken texts. As a base corpus of spoken texts, we adopt the Corpus of Spontaneous Japanese (CSJ) (Maekawa et al., 2000). The main part of CSJ consists of monologues and they are divided into two categories; academic presentations and simulated public speech. Each talk is about 10 - 15 minutes long. For both categories, CSJ has a core collection of monologues, in which morphemes and dependency between bunsetsus (base phrases in Japanese) are manually annotated. We tagged anaphora on twelve talks from the core collection, consisting of six academic presentation transcriptions and six simulated public speech transcriptions. Because the talks are monologues, they include many first-person pronouns and zero pronoun"
Y09-2052,P07-1103,0,0.0442394,"Missing"
Y09-2052,A92-1028,0,0.400893,"spaper articles, where the subjects and objects are rarely first / second persons. On the other hand, in spoken texts such as transcriptions of presentations or meetings, the subjects and objects are often the first person (the speaker) or the second person (the addressee), and they are often omitted in Japanese. Therefore, to distinguish the person of a zero pronoun can be a substantial cue for zero pronoun resolution. In Japanese, promising cues for determining the person of the subject or the object are features related to the verb, such as auxiliary verbs and other functional expressions. Nakaiwa and Ikehara (1992) used semantic attributes of verbs in their rule-based zero pronoun resolution system. Yamamoto et al. (1997) showed that features related to verbs contribute to the accuracy of their person resolution system based on decision trees learned from a dialogue corpus. For these reasons, in this paper we propose a method for utilizing verb related features for zero pronoun resolution based on statistical machine learning from a corpus of spoken texts. The system learns the weights of features from annotation data that have anaphora tags with the attributes of persons. This paper is organized as fol"
Y09-2052,poesio-artstein-2008-anaphoric,0,0.0263121,"purpose of identifying a shorter word within a talk in CSJ, we use a combination of unit IDs on the hierarchy of CSJ’s data structure; the sentence ID, the bunsetsu ID, the longer word ID, and the shorter word ID.2 For example, the ID 23 7 2 1 represents the first shorter word in the second longer word in the seventh bunsetsu in the 23rd sentence. Sometimes an anaphor refers not to a word or a phrase in the preceding utterances, but to a clause or a sentence. In such cases, the range of the referent tends to be ambiguous and specifying it is rather difficult, especially in spontaneous speech (Poesio and Artstein, 2008; M¨uller, 2007). According to the NAIST Text Corpus manual, when an anaphor is regarded as a reference to a clause or a sentence, the referent is not identified but a label is attached simply to indicate that the anaphor refers to a clause or a sentence. We follow this policy with our corpus. Similarly, when the referent is an entity in the real world and does not appear in the text, a label is attached to indicate that it is an exophora. The NAIST Text Corpus manual separates exophoras into three types; first-person, secondperson, and general. In other words, the person of a referent is spec"
Y09-2052,P08-1096,0,0.0161994,"exophora. The NAIST Text Corpus manual separates exophoras into three types; first-person, secondperson, and general. In other words, the person of a referent is specified only when it is an exophora. When a referent is an endophora, information regarding the person is not available. In our annotation design, the label of the person can be attached regardless of whether the referents are exophoras or endophoras. We attached person labels for all first-person endophoras and exophoras as well as other exophoras for our experiment. This approach is a partial adoption of the entity-mention model (Yang et al., 2008). In terms of that model, all first-person zero pronouns are regarded as mentions of one first-person entity, which is actually the presenter of the talk. We also added an attribute label for deixis. Typically, a deictic label is attached when the presenter says kore (this) to point to something such as a graph in a presentation slide. 2.3 Annotation Result We selected six talks from both academic presentations and simulated public speech, and so annotated a total of twelve talks. The numbers of annotations in each type of anaphora are given in Table 1. Table 1: Numbers of annotations type num"
Y13-1026,W10-1736,1,0.929613,"dering local permutations of words might be effective to translate languages with a similar sentence structure, but these methods have a limited performance when translating sentences from languages with different syntactical structures. An effective technique to translate sentences between distant language pairs is pre-reordering, where words in sentences from the source language are re-arranged with the objective to resemble the word order of the target language. Rearranging rules are automatically extracted (Xia and McCord, 2004; Genzel, 2010), or linguistically motivated (Xu et al., 2009; Isozaki et al., 2010; Han et al., 2012; Han et al., 2013). We work following the latter strategy, where the source sentence is parsed to find its syntactical structure, and linguistically-motivated rules are used in combination with the structure of the sentence to guide the word reordering. The language pair under consideration is Chinese-to-Japanese, which despite their common roots, it is a well known language pair for their different sentence structure. However, syntax-based pre-reordering techniques are sensitive to parsing errors, but insight into their relationship has been elusive. The contribution of thi"
Y13-1026,D11-1017,0,0.019774,"pes of errors. One most relevant work to ours is observing the impact of parsing accuracy on a SMT system introduced in Quirk and Corston-Oliver (2006). They showed the general idea that syntax-based SMT models are sensitive to syntactic analysis. However, they did not further analyze concrete parsing error types that affect task accuracy. Green (2011) explored the effects of noun phrase bracketing in dependency parsing in English, and further on English to Czech machine translation. But the work focused on using noun phrase structure to improve a machine translation framework. In the work of Katz-Brown et al. (2011), they proposed a training method to improve a parser’s performance by using reordering quality to examine the parse quality. But they did not study the relationship between reordering quality and parse quality. There are more works on parsing error analysis. For instance, Hara et al. (2009) defined several types of parsing error patterns on predicate argument relation and tested them with a Headdriven phrase structure grammar (HPSG) (Pollard and Sag, 1994) parser (Miyao and Tsujii, 2008). McDonald and Nivre (2007) explored parsing errors for data-driven dependency parsing by 1 In this work, P"
Y13-1026,W08-0509,0,0.0238615,"of CTB-7 which is split according to (Wang et al., 2011). However, we found that sentences in BC and NW are mainly from spoken language, which tend to have faults like repetitions, incomplete sentences, corrections, or incorrect sentence segmentation. Therefore, we randomly selected another 2, 126 unique sentences 269 PACLIC-27 set-1 set-2 Total AL Voc. BN 100 797 897 29.8 5.5K BC 100 100 20.0 690 NM 100 578 678 33.5 5K NS 117 751 868 28.4 5.1K NW 100 100 25.9 972 Total 517 2, 126 2, 643 29.8 9.5K M-reordered Gold-DPC Auto-DPC 0.88 0.95 nese sentences. Word alignments are produced by MGIZA++ (Gao and Vogel, 2008). In both scenarios, we carry out the reordering method DPC (See Section 2.1). Auto-parse trees are generated by an unlabeled Chinese dependency parser, Corbit4 (Hatori et al., 2011). Gold trees5 are converted from CTB-7 parsed text which are created by human annotators. More specifically, we refer to auto-parse tree based reordering system as Auto-DPC and to gold-tree based reordering system as Gold-DPC. Baseline system uses unreordered Chinese sentences. Scenario 1 Preliminary observation about the effects of parsing errors on reordering performance is to compare word order similarities betw"
Y13-1026,W00-1303,0,0.0484713,"PC is limited to dependency structure and POS tags, for analysis on the causes of reordering errors, we examine parsing errors from these two linguistic categories. In this section, the value of Kendall’s tau measures the word order similarity between Gold-DPC and Auto-DPC. Figure 2: The distribution of Kendall’s tau values for 2, 236 bilingual sentences (Chinese-Japanese) in which the Chinese is from three systems of baseline, Auto-DPC, and Gold-DPC. file, ch-ja.A3.final. The comparison implies how monotonically the Chinese sentences have been reordered to align with Japanese. We use MeCab6 (Kudo and Matsumoto, 2000) to segment Japanese sentences and also filter out sentences with more than 64 tokens. There are 2, 236 valid Chinese-Japanese bilingual sentences in total. Figure 2 shows the distribution of Kendall’s tau from three systems in which the baseline is built up by using ordinary Chinese. In Figure 2, baseline system contains a large numbers of non-monotonic aligned sentences, whereas both Auto-DPC and Gold-DPC increased the amount of sentences that achieved high τ values. Reordering based on gold-tree reduced more percentage of low τ sentences than reordering based on automatically parsed trees."
Y13-1026,C10-1043,0,0.040679,"ord correspondence due to the combinatorial complexity. Considering local permutations of words might be effective to translate languages with a similar sentence structure, but these methods have a limited performance when translating sentences from languages with different syntactical structures. An effective technique to translate sentences between distant language pairs is pre-reordering, where words in sentences from the source language are re-arranged with the objective to resemble the word order of the target language. Rearranging rules are automatically extracted (Xia and McCord, 2004; Genzel, 2010), or linguistically motivated (Xu et al., 2009; Isozaki et al., 2010; Han et al., 2012; Han et al., 2013). We work following the latter strategy, where the source sentence is parsed to find its syntactical structure, and linguistically-motivated rules are used in combination with the structure of the sentence to guide the word reordering. The language pair under consideration is Chinese-to-Japanese, which despite their common roots, it is a well known language pair for their different sentence structure. However, syntax-based pre-reordering techniques are sensitive to parsing errors, but insig"
Y13-1026,D07-1013,0,0.0632143,"noun phrase structure to improve a machine translation framework. In the work of Katz-Brown et al. (2011), they proposed a training method to improve a parser’s performance by using reordering quality to examine the parse quality. But they did not study the relationship between reordering quality and parse quality. There are more works on parsing error analysis. For instance, Hara et al. (2009) defined several types of parsing error patterns on predicate argument relation and tested them with a Headdriven phrase structure grammar (HPSG) (Pollard and Sag, 1994) parser (Miyao and Tsujii, 2008). McDonald and Nivre (2007) explored parsing errors for data-driven dependency parsing by 1 In this work, POS tag definitions follow the POS tag guidelines of the Penn Chinese Treebank v3.0. 2 According to (Han et al., 2013), a Vb includes the head of the Vb (Vb-H) and an optional component (Vb-D). 268 PACLIC-27 influence reordering. Meanwhile, the former measurement shows additionally the general figure of the upper bound of the reordering method. However, since it is not only time-consuming but also labor-intensive to set up the benchmark in scenario 1, we use the Japanese reference as the benchmark in scenario 2 and"
Y13-1026,gimenez-marquez-2008-towards,0,0.0609847,"Missing"
Y13-1026,J08-1002,1,0.759309,"he work focused on using noun phrase structure to improve a machine translation framework. In the work of Katz-Brown et al. (2011), they proposed a training method to improve a parser’s performance by using reordering quality to examine the parse quality. But they did not study the relationship between reordering quality and parse quality. There are more works on parsing error analysis. For instance, Hara et al. (2009) defined several types of parsing error patterns on predicate argument relation and tested them with a Headdriven phrase structure grammar (HPSG) (Pollard and Sag, 1994) parser (Miyao and Tsujii, 2008). McDonald and Nivre (2007) explored parsing errors for data-driven dependency parsing by 1 In this work, POS tag definitions follow the POS tag guidelines of the Penn Chinese Treebank v3.0. 2 According to (Han et al., 2013), a Vb includes the head of the Vb (Vb-H) and an optional component (Vb-D). 268 PACLIC-27 influence reordering. Meanwhile, the former measurement shows additionally the general figure of the upper bound of the reordering method. However, since it is not only time-consuming but also labor-intensive to set up the benchmark in scenario 1, we use the Japanese reference as the b"
Y13-1026,P11-3013,0,0.0255294,"ependency parsing as well as its extensibility to other language pairs. 2.2 Related Work Although there are studies on analyzing parsing errors and reordering errors, as far as we know, there is not any work on observing the relationship between these two types of errors. One most relevant work to ours is observing the impact of parsing accuracy on a SMT system introduced in Quirk and Corston-Oliver (2006). They showed the general idea that syntax-based SMT models are sensitive to syntactic analysis. However, they did not further analyze concrete parsing error types that affect task accuracy. Green (2011) explored the effects of noun phrase bracketing in dependency parsing in English, and further on English to Czech machine translation. But the work focused on using noun phrase structure to improve a machine translation framework. In the work of Katz-Brown et al. (2011), they proposed a training method to improve a parser’s performance by using reordering quality to examine the parse quality. But they did not study the relationship between reordering quality and parse quality. There are more works on parsing error analysis. For instance, Hara et al. (2009) defined several types of parsing erro"
Y13-1026,W06-1608,0,0.178502,"of parsing errors on reordering performance. In this analysis, we borrow this state-of-the-art pre-reordering model for our experiments since it is a rule-based pre-reordering method for a distant language pair based on dependency parsing as well as its extensibility to other language pairs. 2.2 Related Work Although there are studies on analyzing parsing errors and reordering errors, as far as we know, there is not any work on observing the relationship between these two types of errors. One most relevant work to ours is observing the impact of parsing accuracy on a SMT system introduced in Quirk and Corston-Oliver (2006). They showed the general idea that syntax-based SMT models are sensitive to syntactic analysis. However, they did not further analyze concrete parsing error types that affect task accuracy. Green (2011) explored the effects of noun phrase bracketing in dependency parsing in English, and further on English to Czech machine translation. But the work focused on using noun phrase structure to improve a machine translation framework. In the work of Katz-Brown et al. (2011), they proposed a training method to improve a parser’s performance by using reordering quality to examine the parse quality. B"
Y13-1026,W12-4207,1,0.931686,"ons of words might be effective to translate languages with a similar sentence structure, but these methods have a limited performance when translating sentences from languages with different syntactical structures. An effective technique to translate sentences between distant language pairs is pre-reordering, where words in sentences from the source language are re-arranged with the objective to resemble the word order of the target language. Rearranging rules are automatically extracted (Xia and McCord, 2004; Genzel, 2010), or linguistically motivated (Xu et al., 2009; Isozaki et al., 2010; Han et al., 2012; Han et al., 2013). We work following the latter strategy, where the source sentence is parsed to find its syntactical structure, and linguistically-motivated rules are used in combination with the structure of the sentence to guide the word reordering. The language pair under consideration is Chinese-to-Japanese, which despite their common roots, it is a well known language pair for their different sentence structure. However, syntax-based pre-reordering techniques are sensitive to parsing errors, but insight into their relationship has been elusive. The contribution of this work is two fold"
Y13-1026,I11-1035,0,0.0178476,"nce separately, we quantify the extent of parsing errors that 4 4.1 Preliminary Experiment Gold Data In order to build up gold parse tree sets for comparison, we used the annotated sentences from Chinese Penn Treebank ver. 7.0 (CTB-7) which is a well known corpus that consists of parsed text in five genres. They are Chinese newswire (NS), magazine news (NM), broadcast news (BN), broadcast conversation programs (BC), and web newsgroups, weblogs (NW). We first randomly selected 517 unique sentences (hereinafter set-1) from all five genres in development set of CTB-7 which is split according to (Wang et al., 2011). However, we found that sentences in BC and NW are mainly from spoken language, which tend to have faults like repetitions, incomplete sentences, corrections, or incorrect sentence segmentation. Therefore, we randomly selected another 2, 126 unique sentences 269 PACLIC-27 set-1 set-2 Total AL Voc. BN 100 797 897 29.8 5.5K BC 100 100 20.0 690 NM 100 578 678 33.5 5K NS 117 751 868 28.4 5.1K NW 100 100 25.9 972 Total 517 2, 126 2, 643 29.8 9.5K M-reordered Gold-DPC Auto-DPC 0.88 0.95 nese sentences. Word alignments are produced by MGIZA++ (Gao and Vogel, 2008). In both scenarios, we carry out th"
Y13-1026,W13-2806,1,0.816096,"Missing"
Y13-1026,C04-1073,0,0.102245,"plore every possible word correspondence due to the combinatorial complexity. Considering local permutations of words might be effective to translate languages with a similar sentence structure, but these methods have a limited performance when translating sentences from languages with different syntactical structures. An effective technique to translate sentences between distant language pairs is pre-reordering, where words in sentences from the source language are re-arranged with the objective to resemble the word order of the target language. Rearranging rules are automatically extracted (Xia and McCord, 2004; Genzel, 2010), or linguistically motivated (Xu et al., 2009; Isozaki et al., 2010; Han et al., 2012; Han et al., 2013). We work following the latter strategy, where the source sentence is parsed to find its syntactical structure, and linguistically-motivated rules are used in combination with the structure of the sentence to guide the word reordering. The language pair under consideration is Chinese-to-Japanese, which despite their common roots, it is a well known language pair for their different sentence structure. However, syntax-based pre-reordering techniques are sensitive to parsing er"
Y13-1026,N09-1028,0,0.0647854,"complexity. Considering local permutations of words might be effective to translate languages with a similar sentence structure, but these methods have a limited performance when translating sentences from languages with different syntactical structures. An effective technique to translate sentences between distant language pairs is pre-reordering, where words in sentences from the source language are re-arranged with the objective to resemble the word order of the target language. Rearranging rules are automatically extracted (Xia and McCord, 2004; Genzel, 2010), or linguistically motivated (Xu et al., 2009; Isozaki et al., 2010; Han et al., 2012; Han et al., 2013). We work following the latter strategy, where the source sentence is parsed to find its syntactical structure, and linguistically-motivated rules are used in combination with the structure of the sentence to guide the word reordering. The language pair under consideration is Chinese-to-Japanese, which despite their common roots, it is a well known language pair for their different sentence structure. However, syntax-based pre-reordering techniques are sensitive to parsing errors, but insight into their relationship has been elusive. T"
Y13-1026,D09-1121,1,0.844566,"sing error types that affect task accuracy. Green (2011) explored the effects of noun phrase bracketing in dependency parsing in English, and further on English to Czech machine translation. But the work focused on using noun phrase structure to improve a machine translation framework. In the work of Katz-Brown et al. (2011), they proposed a training method to improve a parser’s performance by using reordering quality to examine the parse quality. But they did not study the relationship between reordering quality and parse quality. There are more works on parsing error analysis. For instance, Hara et al. (2009) defined several types of parsing error patterns on predicate argument relation and tested them with a Headdriven phrase structure grammar (HPSG) (Pollard and Sag, 1994) parser (Miyao and Tsujii, 2008). McDonald and Nivre (2007) explored parsing errors for data-driven dependency parsing by 1 In this work, POS tag definitions follow the POS tag guidelines of the Penn Chinese Treebank v3.0. 2 According to (Han et al., 2013), a Vb includes the head of the Vb (Vb-H) and an optional component (Vb-D). 268 PACLIC-27 influence reordering. Meanwhile, the former measurement shows additionally the genera"
Y13-1026,W11-2907,1,0.848779,"ubclasses according to the methodology of the reordering method. We then plot the distribution of these parsing errors for various reordering qualities. In Section 5.2, we illustrate these parsing errors with examples. comparing a graph-based parser with a transitionbased parser, which are representing two dominant parsing models. At the same time, Dredze et al. (2007) provided a comparison analysis on differences in annotation guidelines among treebanks which were suspected to be responsible for dependency parsing errors in domain adaptation tasks. Unlike analyzing parsing errors, authors in Yu et al. (2011) focused on the difficulties in Chinese deep parsing by comparing the linguistic properties between Chinese and English. There are also works on reordering error analysis like Han et al. (2012) which examined an existing reordering method and refined it after a detailed linguistic analysis on reordering issues. Although they discovered that parsing errors affect the reordering quality, they did not observe the concrete relationship. On the other hand, Gim´enez and M`arquez (2008) proposed an automatic error analysis method of machine translation output, by compiling a set of metric variants. H"
Y13-1026,I11-1136,1,0.828769,"ncomplete sentences, corrections, or incorrect sentence segmentation. Therefore, we randomly selected another 2, 126 unique sentences 269 PACLIC-27 set-1 set-2 Total AL Voc. BN 100 797 897 29.8 5.5K BC 100 100 20.0 690 NM 100 578 678 33.5 5K NS 117 751 868 28.4 5.1K NW 100 100 25.9 972 Total 517 2, 126 2, 643 29.8 9.5K M-reordered Gold-DPC Auto-DPC 0.88 0.95 nese sentences. Word alignments are produced by MGIZA++ (Gao and Vogel, 2008). In both scenarios, we carry out the reordering method DPC (See Section 2.1). Auto-parse trees are generated by an unlabeled Chinese dependency parser, Corbit4 (Hatori et al., 2011). Gold trees5 are converted from CTB-7 parsed text which are created by human annotators. More specifically, we refer to auto-parse tree based reordering system as Auto-DPC and to gold-tree based reordering system as Gold-DPC. Baseline system uses unreordered Chinese sentences. Scenario 1 Preliminary observation about the effects of parsing errors on reordering performance is to compare word order similarities between manually reordered Chinese sentences and automatically reordered Chinese sentences from set-1. Table 3 shows the average τ value. For baseline system, the average τ value shows h"
Y13-1026,D07-1112,0,\N,Missing
