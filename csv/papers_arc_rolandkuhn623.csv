2020.lrec-1.312,The {N}unavut {H}ansard {I}nuktitut{--}{E}nglish Parallel Corpus 3.0 with Preliminary Machine Translation Results,2020,-1,-1,3,0.9588,13774,eric joanis,Proceedings of the 12th Language Resources and Evaluation Conference,0,"The Inuktitut language, a member of the Inuit-Yupik-Unangan language family, is spoken across Arctic Canada and noted for its morphological complexity. It is an official language of two territories, Nunavut and the Northwest Territories, and has recognition in additional regions. This paper describes a newly released sentence-aligned Inuktitut{--}English corpus based on the proceedings of the Legislative Assembly of Nunavut, covering sessions from April 1999 to June 2017. With approximately 1.3 million aligned sentence pairs, this is, to our knowledge, the largest parallel corpus of a polysynthetic language or an Indigenous language of the Americas released to date. The paper describes the alignment methodology used, the evaluation of the alignments, and preliminary experiments on statistical and neural machine translation (SMT and NMT) between Inuktitut and English, in both directions."
2020.coling-main.516,The Indigenous Languages Technology project at {NRC} {C}anada: An empowerment-oriented approach to developing language software,2020,-1,-1,1,1,17283,roland kuhn,Proceedings of the 28th International Conference on Computational Linguistics,0,"This paper surveys the first, three-year phase of a project at the National Research Council of Canada that is developing software to assist Indigenous communities in Canada in preserving their languages and extending their use. The project aimed to work within the empowerment paradigm, where collaboration with communities and fulfillment of their goals is central. Since many of the technologies we developed were in response to community needs, the project ended up as a collection of diverse subprojects, including the creation of a sophisticated framework for building verb conjugators for highly inflectional polysynthetic languages (such as Kanyen{'}k{\'e}ha, in the Iroquoian language family), release of what is probably the largest available corpus of sentences in a polysynthetic language (Inuktut) aligned with English sentences and experiments with machine translation (MT) systems trained on this corpus, free online services based on automatic speech recognition (ASR) for easing the transcription bottleneck for recordings of speech in Indigenous languages (and other languages), software for implementing text prediction and read-along audiobooks for Indigenous languages, and several other subprojects."
C18-1222,"Indigenous language technologies in {C}anada: Assessment, challenges, and successes",2018,0,3,3,0,12351,patrick littell,Proceedings of the 27th International Conference on Computational Linguistics,0,"In this article, we discuss which text, speech, and image technologies have been developed, and would be feasible to develop, for the approximately 60 Indigenous languages spoken in Canada. In particular, we concentrate on technologies that may be feasible to develop for most or all of these languages, not just those that may be feasible for the few most-resourced of these. We assess past achievements and consider future horizons for Indigenous language transliteration, text prediction, spell-checking, approximate search, machine translation, speech recognition, speaker diarization, speech synthesis, optical character recognition, and computer-aided language learning."
W17-4732,{NRC} Machine Translation System for {WMT} 2017,2017,0,1,7,0.241615,13775,chikiu lo,Proceedings of the Second Conference on Machine Translation,0,None
W16-2317,{NRC} {R}ussian-{E}nglish Machine Translation System for {WMT} 2016,2016,13,4,7,0.241615,13775,chikiu lo,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,"We describe the statistical machine translation system developed at the National Research Council of Canada (NRC) for the Russian-English news translation task of the First Conference on Machine Translation (WMT 2016). Our submission is a phrase-based SMT system that tackles the morphological complexity of Russian through comprehensive use of lemmatization. The core of our lemmatization strategy is to use different views of Russian for different SMT components: word alignment and bilingual neural network language models use lemmas, while sparse features and reordering models use fully inflected forms. Some components, such as the phrase table, use both views of the source. Russian words that remain out-ofvocabulary (OOV) after lemmatization are transliterated into English using a statistical model trained on examples mined from the parallel training corpus. The NRC Russian-English MT system achieved the highest uncased BLEU and the lowest TER scores among the eight participants in WMT 2016."
2016.amta-researchers.8,Bilingual Methods for Adaptive Training Data Selection for Machine Translation,2016,-1,-1,2,0,4084,boxing chen,Conferences of the Association for Machine Translation in the Americas: MT Researchers' Track,0,"In this paper, we propose a new data selection method which uses semi-supervised convolutional neural networks based on bitokens (Bi-SSCNNs) for training machine translation systems from a large bilingual corpus. In earlier work, we devised a data selection method based on semi-supervised convolutional neural networks (SSCNNs). The new method, Bi-SSCNN, is based on bitokens, which use bilingual information. When the new methods are tested on two translation tasks (Chinese-to-English and Arabic-to-English), they significantly outperform the other three data selection methods in the experiments. We also show that the BiSSCNN method is much more effective than other methods in preventing noisy sentence pairs from being chosen for training. More interestingly, this method only needs a tiny amount of in-domain data to train the selection model, which makes fine-grained topic-dependent translation adaptation possible. In the follow-up experiments, we find that neural machine translation (NMT) is more sensitive to noisy data than statistical machine translation (SMT). Therefore, Bi-SSCNN which can effectively screen out noisy sentence pairs, can benefit NMT much more than SMT.We observed a BLEU improvement over 3 points on an English-to-French WMT task when Bi-SSCNNs were used."
W15-3044,Multi-level Evaluation for Machine Translation,2015,13,4,3,0,4084,boxing chen,Proceedings of the Tenth Workshop on Statistical Machine Translation,0,"Translations generated by current statistical systems often have a large variance, in terms of their quality against human references. To cope with such variation, we propose to evaluate translations using a multi-level framework. The method varies the evaluation criteria based on the clusters to which a translation belongs. Our experiments on the WMT metric task data show that the multi-level framework consistently improves the performance of two benchmarking metrics, resulting in better correlation with human judgment."
2014.amta-researchers.3,Coarse {``}split and lump{''} bilingual language models for richer source information in {SMT},2014,-1,-1,2,1,12350,darlene stewart,Proceedings of the 11th Conference of the Association for Machine Translation in the Americas: MT Researchers Track,0,"Recently, there has been interest in automatically generated word classes for improving statistical machine translation (SMT) quality: e.g, (Wuebker et al, 2013). We create new models by replacing words with word classes in features applied during decoding; we call these {``}coarse models{''}. We find that coarse versions of the bilingual language models (biLMs) of (Niehues et al, 2011) yield larger BLEU gains than the original biLMs. BiLMs provide phrase-based systems with rich contextual information from the source sentence; because they have a large number of types, they suffer from data sparsity. Niehues et al (2011) mitigated this problem by replacing source or target words with parts of speech (POSs). We vary their approach in two ways: by clustering words on the source or target side over a range of granularities (word clustering), and by clustering the bilingual units that make up biLMs (bitoken clustering). We find that loglinear combinations of the resulting coarse biLMs with each other and with coarse LMs (LMs based on word classes) yield even higher scores than single coarse models. When we add an appealing {``}generic{''} coarse configuration chosen on English {\textgreater} French devtest data to four language pairs (keeping the structure fixed, but providing language-pair-specific models for each pair), BLEU gains on blind test data against strong baselines averaged over 5 runs are +0.80 for English {\textgreater} French, +0.35 for French {\textgreater} English, +1.0 for Arabic {\textgreater} English, and +0.6 for Chinese {\textgreater} English."
2014.amta-researchers.10,A comparison of mixture and vector space techniques for translation model adaptation,2014,39,4,2,0,4084,boxing chen,Proceedings of the 11th Conference of the Association for Machine Translation in the Americas: MT Researchers Track,0,"In this paper, we propose two extensions to the vector space model (VSM) adaptation technique (Chen et al., 2013b) for statistical machine translation (SMT), both of which result in significant improvements. We also systematically compare the VSM techniques to three mixture model adaptation techniques: linear mixture, log-linear mixture (Foster and Kuhn, 2007), and provenance features (Chiang et al., 2011). Experiments on NIST Chinese-to-English and Arabic-to-English tasks show that all methods achieve significant improvement over a competitive non-adaptive baseline. Except for the original VSM adaptation method, all methods yield improvements in the +1.7-2.0 BLEU range. Combining them gives further significant improvements of up to +2.6-3.3 BLEU over the baseline."
P13-1126,Vector Space Model for Adaptation in Statistical Machine Translation,2013,25,30,2,0.639049,4084,boxing chen,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"This paper proposes a new approach to domain adaptation in statistical machine translation (SMT) based on a vector space model (VSM). The general idea is first to create a vector profile for the in-domain development (xe2x80x9cdevxe2x80x9d) set. This profile might, for instance, be a vector with a dimensionality equal to the number of training subcorpora; each entry in the vector reflects the contribution of a particular subcorpus to all the phrase pairs that can be extracted from the dev set. Then, for each phrase pair extracted from the training data, we create a vector with features defined in the same way, and calculate its similarity score with the vector representing the dev set. Thus, we obtain a decoding feature whose value represents the phrase pairxe2x80x99s closeness to the dev. This is a simple, computationally cheap form of instance weighting for phrase pairs. Experiments on large scale NIST evaluation data show improvements over strong baselines: 1.8 BLEU on Arabic to English and 1.4 BLEU on Chinese to English over a non-adapted baseline, and significant improvements in most circumstances over baselines with linear mixture model adaptation. An informal analysis suggests that VSM adaptation may help in making a good choice among words with the same meaning, on the basis of style and genre."
N13-1114,Adaptation of Reordering Models for Statistical Machine Translation,2013,21,16,3,0.639049,4084,boxing chen,Proceedings of the 2013 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Previous research on domain adaptation (DA) for statistical machine translation (SMT) has mainly focused on the translation model (TM) and the language model (LM). To the best of our knowledge, there is no previous work on reordering model (RM) adaptation for phrasebased SMT. In this paper, we demonstrate that mixture model adaptation of a lexicalized RM can significantly improve SMT performance, even when the system already contains a domain-adapted TM and LM. We find that, surprisingly, different training corpora can vary widely in their reordering characteristics for particular phrase pairs. Furthermore, particular training corpora may be highly suitable for training the TM or the LM, but unsuitable for training the RM, or vice versa, so mixture weights for these models should be estimated separately. An additional contribution of the paper is to propose two improvements to mixture model adaptation: smoothing the in-domain sample, and weighting instances by document frequency. Applied to mixture RMs in our experiments, these techniques (especially smoothing) yield significant performance improvements."
2013.mtsummit-wptp.9,Transferring markup tags in statistical machine translation: a two-stream approach,2013,-1,-1,4,1,13774,eric joanis,Proceedings of the 2nd Workshop on Post-editing Technology and Practice,0,None
2013.mtsummit-papers.23,Simulating Discriminative Training for Linear Mixture Adaptation in Statistical Machine Translation,2013,-1,-1,3,0,3518,george foster,Proceedings of Machine Translation Summit XIV: Papers,0,None
W12-3104,"Improving {AMBER}, an {MT} Evaluation Metric",2012,10,23,2,0.719808,4084,boxing chen,Proceedings of the Seventh Workshop on Statistical Machine Translation,0,"A recent paper described a new machine translation evaluation metric, AMBER. This paper describes two changes to AMBER. The first one is incorporation of a new ordering penalty; the second one is the use of the downhill simplex algorithm to tune the weights for the components of AMBER. We tested the impact of the two changes, using data from the WMT metrics task. Each of the changes by itself improved the performance of AMBER, and the two together yielded even greater improvement, which in some cases was more than additive. The new version of AMBER clearly outperforms BLEU in terms of correlation with human judgment."
P12-1098,{PORT}: a Precision-Order-Recall {MT} Evaluation Metric for Tuning,2012,31,17,2,0.719808,4084,boxing chen,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Many machine translation (MT) evaluation metrics have been shown to correlate better with human judgment than BLEU. In principle, tuning on these metrics should yield better systems than tuning on BLEU. However, due to issues such as speed, requirements for linguistic resources, and optimization difficulty, they have not been widely adopted for tuning. This paper presents PORT, a new MT evaluation metric which combines precision, recall and an ordering metric and which is primarily designed for tuning MT systems. PORT does not require external resources and is quick to compute. It has a better correlation with human judgment than BLEU. We compare PORT-tuned MT systems to BLEU-tuned baselines in five experimental conditions involving four language pairs. PORT tuning achieves consistently better performance than BLEU tuning, according to four automated metrics (including BLEU) and to human evaluation: in comparisons of outputs from 300 source sentences, human judges preferred the PORT-tuned output 45.3% of the time (vs. 32.7% BLEU tuning preferences and 22.0% ties)."
D12-1058,Enlarging Paraphrase Collections through Generalization and Instantiation,2012,36,16,3,0,5049,atsushi fujita,Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,0,"This paper presents a paraphrase acquisition method that uncovers and exploits generalities underlying paraphrases: paraphrase patterns are first induced and then used to collect novel instances. Unlike existing methods, ours uses both bilingual parallel and monolingual corpora. While the former are regarded as a source of high-quality seed paraphrases, the latter are searched for paraphrases that match patterns learned from the seed paraphrases. We show how one can use monolingual corpora, which are far more numerous and larger than bilingual corpora, to obtain paraphrases that rival in quality those derived directly from bilingual corpora. In our experiments, the number of paraphrase pairs obtained in this way from monolingual corpora was a large multiple of the number of seed paraphrases. Human evaluation through a paraphrase substitution test demonstrated that the newly acquired paraphrase pairs are of reasonable quality. Remaining noise can be further reduced by filtering seed paraphrases."
W11-2105,"{AMBER}: A Modified {BLEU}, Enhanced Ranking Metric",2011,19,21,2,0.800202,4084,boxing chen,Proceedings of the Sixth Workshop on Statistical Machine Translation,0,"This paper proposes a new automatic machine translation evaluation metric: AMBER, which is based on the metric BLEU but incorporates recall, extra penalties, and some text processing variants. There is very little linguistic information in AMBER. We evaluate its system-level correlation and sentence-level consistency scores with human rankings from the WMT shared evaluation task; AMBER achieves state-of-the-art performance."
2011.mtsummit-papers.30,Unpacking and Transforming Feature Functions: New Ways to Smooth Phrase Tables,2011,-1,-1,2,0.800202,4084,boxing chen,Proceedings of Machine Translation Summit XIII: Papers,0,None
2011.iwslt-evaluation.19,Semantic smoothing and fabrication of phrase pairs for {SMT},2011,15,4,2,0.800202,4084,boxing chen,Proceedings of the 8th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"In statistical machine translation systems, phrases with similar meanings often have similar but not identical distributions of translations. This paper proposes a new soft clustering method to smooth the conditional translation probabilities for a given phrase with those of semantically similar phrases. We call this semantic smoothing (SS). Moreover, we fabricate new phrase pairs that were not observed in training data, but which may be used for decoding. In learning curve experiments against a strong baseline, we obtain a consistent pattern of modest improvement from semantic smoothing, and further modest improvement from phrase pair fabrication."
W10-1702,Fast Consensus Hypothesis Regeneration for Machine Translation,2010,13,0,3,0.800202,4084,boxing chen,Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and {M}etrics{MATR},0,"This paper presents a fast consensus hypothesis regeneration approach for machine translation. It combines the advantages of feature-based fast consensus decoding and hypothesis regeneration. Our approach is more efficient than previous work on hypothesis regeneration, and it explores a wider search space than consensus decoding, resulting in improved performance. Experimental results show consistent improvements across language pairs, and an improvement of up to 0.72 BLEU is obtained over a competitive single-pass baseline on the Chinese-to-English NIST task."
W10-1717,Lessons from {NRC}{'}s Portage System at {WMT} 2010,2010,9,6,7,1,5046,samuel larkin,Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and {M}etrics{MATR},0,"NRC's Portage system participated in the English-French (E-F) and French-English (F-E) translation tasks of the ACL WMT 2010 evaluation. The most notable improvement over earlier versions of Portage is an efficient implementation of lattice MERT. While Portage has typically performed well in Chinese to English MT evaluations, most recently in the NIST09 evaluation, our participation in WMT 2010 revealed some interesting differences between Chinese-English and E-F/F-E translation, and alerted us to certain weak spots in our system. Most of this paper discusses the problems we found in our system and ways of fixing them. We learned several lessons that we think will be of general interest."
P10-1086,Bilingual Sense Similarity for Statistical Machine Translation,2010,39,15,3,0.800202,4084,boxing chen,Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,1,"This paper proposes new algorithms to compute the sense similarity between two units (words, phrases, rules, etc.) from parallel corpora. The sense similarity scores are computed by using the vector space model. We then apply the algorithms to statistical machine translation by computing the sense similarity between the source and target side of translation rule pairs. Similarity scores are used as additional features of the translation model to improve translation performance. Significant improvements are obtained over a state-of-the-art hierarchical phrase-based machine translation system."
D10-1044,Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation,2010,24,170,3,0,3518,george foster,Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,0,"We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not. This extends previous work on discriminative weighting by using a finer granularity, focusing on the properties of instances rather than corpus components, and using a simpler training procedure. We incorporate instance weighting into a mixture-model framework, and find that it yields consistent improvements over a wide range of baselines."
C10-1069,"Phrase Clustering for Smoothing {TM} Probabilities - or, How to Extract Paraphrases from Phrase Tables",2010,14,12,1,1,17283,roland kuhn,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),0,"This paper describes how to cluster together the phrases of a phrase-based statistical machine translation (SMT) system, using information in the phrase table itself. The clustering is symmetric and recursive: it is applied both to source-language and target-language phrases, and the clustering in one language helps determine the clustering in the other. The phrase clusters have many possible uses. This paper looks at one of these uses: smoothing the conditional translation model (TM) probabilities employed by the SMT system. We incorporated phrase-cluster-derived probability estimates into a baseline loglinear feature combination that included relative frequency and lexically-weighted conditional probability estimates. In Chinese-English (C-E) and French-English (F-E) learning curve experiments, we obtained a gain over the baseline in 29 of 30 tests, with a maximum gain of 0.55 BLEU points (though most gains were fairly small). The largest gains came with medium (200--400K sentence pairs) rather than with small (less than 100K sentence pairs) amounts of training data, contrary to what one would expect from the paraphrasing literature. We have only begun to explore the original smoothing approach described here."
2010.amta-papers.24,Translating Structured Documents,2010,-1,-1,3,0,3518,george foster,Proceedings of the 9th Conference of the Association for Machine Translation in the Americas: Research Papers,0,"Machine Translation traditionally treats documents as sets of independent sentences. In many genres, however, documents are highly structured, and their structure contains information that can be used to improve translation quality. We present a preliminary approach to document translation that uses structural features to modify the behaviour of a language model, at sentence-level granularity. To our knowledge, this is the first attempt to incorporate structural information into statistical MT. In experiments on structured English/French documents from the Hansard corpus, we demonstrate small but statistically significant improvements."
W09-0439,Stabilizing Minimum Error Rate Training,2009,11,43,2,0,3518,george foster,Proceedings of the Fourth Workshop on Statistical Machine Translation,0,"The most commonly used method for training feature weights in statistical machine translation (SMT) systems is Och's minimum error rate training (MERT) procedure. A well-known problem with Och's procedure is that it tends to be sensitive to small changes in the system, particularly when the number of features is large. In this paper, we quantify the stability of Och's procedure by supplying different random seeds to a core component of the procedure (Powell's algorithm). We show that for systems with many features, there is extensive variation in outcomes, both on the development data and on the test data. We analyze the causes of this variation and propose modifications to the MERT procedure that improve stability while helping performance on test data."
2009.mtsummit-plenaries.3,{MT}: the Current Research Landscape,2009,-1,-1,1,1,17283,roland kuhn,Proceedings of Machine Translation Summit XII: Plenaries,0,None
2009.mtsummit-plenaries.15,{P}ortage{L}ive: delivering machine translation technology via virtualization,2009,6,0,5,0,47483,patrick paul,Proceedings of Machine Translation Summit XII: Plenaries,0,None
2009.mtsummit-papers.2,Phrase Translation Model Enhanced with Association based Features,2009,-1,-1,3,0.789534,4084,boxing chen,Proceedings of Machine Translation Summit XII: Papers,0,None
C08-1115,Tighter Integration of Rule-Based and Statistical {MT} in Serial System Combination,2008,11,10,6,0.3803,28492,nicola ueffing,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"Recent papers have described machine translation (MT) based on an automatic post-editing or serial combination strategy whereby the input language is first translated into the target language by a rule-based MT (RBMT) system, then the target language output is automatically post-edited by a phrase-based statistical machine translation (SMT) system. This approach has been shown to improve MT quality over RBMT or SMT alone. In this previous work, there was a very loose coupling between the two systems: the SMT system only had access to the final 1-best translations from RBMT. Furthermore, the previous work involved European language pairs and relatively small training corpora. In this paper, we describe a more tightly integrated serial combination for the Chinese-to-English MT task. We will present experimental evaluation results on the 2008 NIST constrained data track where a significant gain in terms of both automatic and subjective metrics is achieved through the tighter coupling of the two systems."
W07-0703,Integration of an {A}rabic Transliteration Module into a Statistical Machine Translation System,2007,9,10,3,0,49040,mehdi kashani,Proceedings of the Second Workshop on Statistical Machine Translation,0,"We provide an in-depth analysis of the integration of an Arabic-to-English transliteration system into a general-purpose phrase-based statistical machine translation system. We study the integration from different aspects and evaluate the improvement that can be attributed to the integration using the BLEU metric. Our experiments show that a transliteration module can help significantly in the situation where the test data is rich with previously unseen named entities. We obtain 70% and 53% of the theoretical maximum improvement we could achieve, as measured by an oracle on development and test sets respectively for OOV words (out of vocabulary source words not appearing in the phrase table)."
W07-0717,Mixture-Model Adaptation for {SMT},2007,21,201,2,0,3518,george foster,Proceedings of the Second Workshop on Statistical Machine Translation,0,"We describe a mixture-model approach to adapting a Statistical Machine Translation System for new domains, using weights that depend on text distances to mixture components. We investigate a number of variants on this approach, including cross-domain versus dynamic adaptation; linear versus loglinear mixtures; language and translation model adaptation; different methods of assigning weights; and granularity of the source unit being adapted to. The best methods achieve gains of approximately one BLEU percentage point over a state-of-the art non-adapted baseline system."
W07-0728,Rule-Based Translation with Statistical Phrase-Based Post-Editing,2007,5,109,4,0,5047,michel simard,Proceedings of the Second Workshop on Statistical Machine Translation,0,"This article describes a machine translation system based on an automatic post-editing strategy: initially translate the input text into the target-language using a rule-based MT system, then automatically post-edit the output using a statistical phrase-based system. An implementation of this approach based on the SYSTRAN and PORTAGE MT systems was used in the shared task of the Second Workshop on Statistical Machine Translation. Experimental results on the test data of the previous campaign are presented."
D07-1103,Improving Translation Quality by Discarding Most of the Phrasetable,2007,10,171,4,1,43880,howard johnson,Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL}),0,"It is possible to reduce the bulk of phrasetables for Statistical Machine Translation using a technique based on the significance testing of phrase pair co-occurrence in the parallel corpus. The savings can be quite substantial (up to 90%) and cause no reduction in BLEU score. In some cases, an improvement in BLEU is obtained at the same time although the effect is less pronounced if state-of-the-art phrasetable smoothing is employed."
W06-3118,{PORTAGE}: with Smoothed Phrase Tables and Segment Choice Models,2006,7,10,4,1,43880,howard johnson,Proceedings on the Workshop on Statistical Machine Translation,0,Improvements to Portage and its participation in the shared task of NAACL 2006 Workshop on Statistical Machine Translation are described. Promising ideas in phrase table smoothing and global distortion using feature-rich models are discussed as well as numerous improvements in the software base.
W06-1607,Phrasetable Smoothing for Statistical Machine Translation,2006,19,105,2,0,3518,george foster,Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,0,"We discuss different strategies for smoothing the phrasetable in Statistical MT, and give results over a range of translation settings. We show that any type of smoothing is a better idea than the relative-frequency estimates that are often used. The best smoothing techniques yield consistent gains of approximately 1% (absolute) according to the BLEU metric."
N06-1004,Segment Choice Models: Feature-Rich Models for Global Distortion in Statistical Machine Translation,2006,15,10,1,1,17283,roland kuhn,"Proceedings of the Human Language Technology Conference of the {NAACL}, Main Conference",0,"This paper presents a new approach to distortion (phrase reordering) in phrase-based machine translation (MT). Distortion is modeled as a sequence of choices during translation. The approach yields trainable, probabilistic distortion models that are global: they assign a probability to each possible phrase reordering. These segment choice models (SCMs) can be trained on segment-aligned sentence pairs; they can be applied during decoding or rescoring. The approach yields a metric called distortion perplexity (disperp) for comparing SCMs offline on test data, analogous to perplexity for language models. A decision-tree-based SCM is tested on Chinese-to-English translation, and outperforms a baseline distortion penalty approach at the 99% confidence level."
2006.jeptalnrecital-poster.23,Syst{\\`e}me de traduction automatique statistique combinant diff{\\'e}rentes ressources,2006,-1,-1,3,0.961538,5647,fatiha sadat,Actes de la 13{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Posters,0,"Cet article d{\'e}crit une approche combinant diff{\'e}rents mod{\`e}les statistiques pour la traduction automatique bas{\'e}e sur les segments. Pour ce faire, diff{\'e}rentes ressources sont utilis{\'e}es, dont deux corpus parall{\`e}les aux caract{\'e}ristiques diff{\'e}rentes et un dictionnaire de terminologie bilingue et ce, afin d{'}am{\'e}liorer la performance quantitative et qualitative du syst{\`e}me de traduction. Nous {\'e}valuons notre approche sur la paire de langues fran{\c{c}}ais-anglais et montrons comment la combinaison des ressources propos{\'e}es am{\'e}liore de fa{\c{c}}on significative les r{\'e}sultats."
W05-0822,{PORTAGE}: A Phrase-Based Machine Translation System,2005,7,31,5,0.961538,5647,fatiha sadat,Proceedings of the {ACL} Workshop on Building and Using Parallel Texts,0,"This paper describes the participation of the Portage team at NRC Canada in the shared task of ACL 2005 Workshop on Building and Using Parallel Texts. We discuss Portage, a statistical phrase-based machine translation system, and present experimental results on the four language pairs of the shared task. First, we focus on the French-English task using multiple resources and techniques. Then we describe our contribution on the Finnish-English, Spanish-English and German-English language pairs using the provided data for the shared task."
H91-1043,Some Results on Stochastic Language Modelling,1991,12,3,2,0,35963,renato mori,"Speech and Natural Language: Proceedings of a Workshop Held at Pacific Grove, California, {F}ebruary 19-22, 1991",0,"The paper will discuss three issues. The first is the derivation of precise probability scores for partial hypotheses containing islands, in the context of a Stochastic-Context-Free-Grammar (SCFG) for Language Modeling (LM). The second issue is the possibility of adding a cache component to a LM. This component alters the expected probability of words to reflect the speaker's patterns of word use. Finally, the idiosyncratic properties of dialogue are being studied; this work will indicate how knowledge about the discourse state can be incorporated into the LM and into the semantic component."
C88-1071,Speech Recognition and the Frequency of Recently Used Words: A Modified {M}arkov Model for Natural Language,1988,8,48,1,1,17283,roland kuhn,{C}oling {B}udapest 1988 Volume 1: {I}nternational {C}onference on {C}omputational {L}inguistics,0,"Speech recognition systems incorporate a language model which, at each stage of the recognition task, assigns a probability of occurrence to each word in the vocabulary. A class of Markov language models identified by Jelinek has achieved considerable success in this domain. A modification of the Markov approach, which assigns higher probabilities to recently used words, is proposed and tested against a pure Markov model. Parameter calculation and comparison of the two models both involve use of the LOB Corpus of tagged modern English."
