2011.iwslt-evaluation.1,P02-1040,0,0.111082,"4.2. In addition, for development purposes, ASR outputs for the IWSLT 2010 development and test sets were also made available to participants. 3.3. Evaluation Specifications The participants had to provide the result of the translation of the English audio in NIST XML format. The output had to be true-cased and had to contain punctuation. The participants could either use the audio files directly, or use the output— either first best hypotheses in CTM format or word lattices in SLF — of KIT, LIUM, and FBK from the ASR task. The quality of the translations was measured automatically with BLEU [1] by scoring against the human translations created by the TED open translation project, and by human subjective evaluation (paired comparison, Section 7). Since the reference translations from the TED website match the segmentation of the reference transcriptions of the talks, 12 automatic evaluation scores for the MT outputs could be directly computed. The evaluation specifications for the SLT task were defined as case-sensitive with punctuation marks (case+punc). Tokenization scripts were applied automatically to all run submissions prior to evaluation. Moreover, automatic evaluation scores"
2011.iwslt-evaluation.1,W07-0734,0,0.031722,"Missing"
2011.iwslt-evaluation.1,niessen-etal-2000-evaluation,0,0.0557796,"Missing"
2011.iwslt-evaluation.1,2006.amta-papers.25,0,0.148776,"Missing"
2011.iwslt-evaluation.1,2003.mtsummit-papers.51,0,0.142051,"Missing"
2011.iwslt-evaluation.1,2011.iwslt-evaluation.8,0,0.040341,"Missing"
2011.iwslt-evaluation.1,2011.iwslt-evaluation.7,0,0.030857,"Missing"
2011.iwslt-evaluation.1,2011.iwslt-evaluation.10,0,0.0773305,"Missing"
2011.iwslt-evaluation.1,2011.mtsummit-papers.59,1,0.858689,"xception of sentences with less than 5 words, which were excluded from the subjective evaluation. The IWSLT 2011 subjective evaluation focused solely on the Ranking task10 and a number of novelties were introduced with respect to the traditional system ranking evaluation carried out in previous campaigns. Firstly, this year’s evaluation was not carried out by hired expert graders but by relying on crowdsourced data. The feasibility of using crowdsourcing methodologies as an effective way to reduce the costs of MT evaluation without sacrificing quality was investigated in a previous experiment [23], where the ranking evaluation of the IWSLT 2010 Arabic-English BTEC task was replicated by hiring non-experts through Amazon’s Mechanical Turk. The analysis of the collected data showed that agreement rates for non-experts were comparable to those for experts, and that the crowd-based system ranking had a very strong correlation with expert-based ranking. Secondly, the cost reduction obtained by using crowdsourcing allowed us to focus on modifying and extending the ranking methodology in different respects, with the aim of maximizing the overall evaluation reliability. The goal of the Ranking"
2011.iwslt-evaluation.1,W07-0718,0,0.0308834,"uation is to produce a complete ordering of the systems participating in a given task. The ranking task requires human judges to decide whether one system output is better than another for a given source sentence. The judgments collected through these comparisons are used to obtain the ranking scores, which are calculated as the average number of times that a system was judged better than any other system. Traditionally, in the ranking task, the judge was presented with the output of five submissions for a given source sentence and was asked to rank them from best to worst (ties were allowed) [24]. Each evaluation block contained the implicit pairwise comparisons (i.e. each system against the other systems presented in the same block) which constituted the basis of the ranking scores. 10 Last year human evaluation was also carried out for the Fluency and Adequacy metrics. In the following sections we analyze the data that we collected by posting the ranking task on Amazon’s Mechanical Although ranking a number of translated sentences relative to each other is quite intuitive, a 5-fold ranking task is less reliable than a direct comparison between only two translated sentences due to th"
2011.iwslt-evaluation.1,D09-1030,0,0.0421625,"Missing"
2011.iwslt-evaluation.1,2011.iwslt-evaluation.6,0,\N,Missing
2011.iwslt-evaluation.1,2011.iwslt-evaluation.3,0,\N,Missing
2011.iwslt-evaluation.1,2011.iwslt-evaluation.9,0,\N,Missing
2011.iwslt-evaluation.1,2011.iwslt-evaluation.13,0,\N,Missing
2011.iwslt-evaluation.1,zhang-etal-2004-interpreting,0,\N,Missing
2011.iwslt-evaluation.1,2011.iwslt-evaluation.4,0,\N,Missing
2011.iwslt-evaluation.1,2011.iwslt-evaluation.5,0,\N,Missing
2011.iwslt-evaluation.1,2011.iwslt-evaluation.11,0,\N,Missing
2011.mtsummit-papers.59,S10-1007,0,0.0268043,"tasks, including the creation of parallel corpora, the word-level alignment of parallel sentences, the creation of paraphrases of existing reference translations, and the creation of translation lexica for low resource languages (Callison-Burch and Dredze, 2010). Annotated data is also crucial for evaluation purposes. Very recently, crowdsourced data has started being ofﬁcially used in international evaluation campaigns. In the CLEF 2010 Web People Search Clustering Task (Artiles et al., 2010) and the SemEval2010 Task of Noun Compounds Interpretation Using Paraphrasing Verbs and Prepositions (Butnariu et al., 2010), for example, MTurk was used for the annotation of the training/test data sets. For MT evaluation, a number of studies have been carried out with the aim of understanding the feasibility of substituting expert data with non-expert data for different types of human evaluation tasks. In (Callison-Burch, 2009), it is shown that MTurk can be effectively used to collect relative rankings, to perform human-mediated translation edit rate (HTER), and to carry out evaluation through reading comprehension questions. In (Denkowski and Lavie, 2010) MTurk was used to obtain translation adequacy assessment"
2011.mtsummit-papers.59,W10-0701,0,0.0210162,"Vision, and Natural Language Processing tasks such as relation extraction, word sense disambiguation, textual entailment, named entity annotation, and natural language generation. Machine Translation is one of the ﬁelds where research on crowdsourcing is most active. The feasibility of collecting good quality crowdsourced data has been explored for many different MT tasks, including the creation of parallel corpora, the word-level alignment of parallel sentences, the creation of paraphrases of existing reference translations, and the creation of translation lexica for low resource languages (Callison-Burch and Dredze, 2010). Annotated data is also crucial for evaluation purposes. Very recently, crowdsourced data has started being ofﬁcially used in international evaluation campaigns. In the CLEF 2010 Web People Search Clustering Task (Artiles et al., 2010) and the SemEval2010 Task of Noun Compounds Interpretation Using Paraphrasing Verbs and Prepositions (Butnariu et al., 2010), for example, MTurk was used for the annotation of the training/test data sets. For MT evaluation, a number of studies have been carried out with the aim of understanding the feasibility of substituting expert data with non-expert data for"
2011.mtsummit-papers.59,W07-0718,0,0.230666,"a given translation depends on numerous factors like the intended use of the translation, the characteristics of the MT software, and the nature of the translation process. Early attempts tried to manually produce numerical judgements of MT quality with respect to a set of reference translations (White et al., 1994). Recently, human assessment of MT quality has been carried out by either assigning a single grade on a scale of 5 or 7 specifying the ﬂuency or adequacy of a given translation (Przybocki et al., 2008), or by relatively ranking to each other multiple translations of the same input (Callison-Burch et al., 2007). 521 Although human evaluation of MT output provides the most direct and reliable assessment, it is time consuming, costly and subjective, i.e., evaluation results might vary from person to person due to different backgrounds, bilingual experience, and inconsistent judgements caused by the high complexity of the multi-class grading task. These drawbacks to human assessment schemes have encouraged many researchers to seek reliable methods for estimating such measures automatically. Various automatic evaluation measures have been proposed to make the evaluation of MT outputs cheaper and faster."
2011.mtsummit-papers.59,D09-1030,0,0.0704963,"purposes. Very recently, crowdsourced data has started being ofﬁcially used in international evaluation campaigns. In the CLEF 2010 Web People Search Clustering Task (Artiles et al., 2010) and the SemEval2010 Task of Noun Compounds Interpretation Using Paraphrasing Verbs and Prepositions (Butnariu et al., 2010), for example, MTurk was used for the annotation of the training/test data sets. For MT evaluation, a number of studies have been carried out with the aim of understanding the feasibility of substituting expert data with non-expert data for different types of human evaluation tasks. In (Callison-Burch, 2009), it is shown that MTurk can be effectively used to collect relative rankings, to perform human-mediated translation edit rate (HTER), and to carry out evaluation through reading comprehension questions. In (Denkowski and Lavie, 2010) MTurk was used to obtain translation adequacy assessments. Finally, in (Callison-Burch et al., 2010) a subset of the ofﬁcial WMT10 relative ranking evaluation was reproduced with non-expert judges and various methods to improve the quality of the collected data are presented. All these MT evaluation experiments have been conducted using MTurk directly, as have mo"
2011.mtsummit-papers.59,W10-0709,0,0.112325,"ds Interpretation Using Paraphrasing Verbs and Prepositions (Butnariu et al., 2010), for example, MTurk was used for the annotation of the training/test data sets. For MT evaluation, a number of studies have been carried out with the aim of understanding the feasibility of substituting expert data with non-expert data for different types of human evaluation tasks. In (Callison-Burch, 2009), it is shown that MTurk can be effectively used to collect relative rankings, to perform human-mediated translation edit rate (HTER), and to carry out evaluation through reading comprehension questions. In (Denkowski and Lavie, 2010) MTurk was used to obtain translation adequacy assessments. Finally, in (Callison-Burch et al., 2010) a subset of the ofﬁcial WMT10 relative ranking evaluation was reproduced with non-expert judges and various methods to improve the quality of the collected data are presented. All these MT evaluation experiments have been conducted using MTurk directly, as have most of the available studies on the effectiveness of crowdsourcing. Up to now, only a few papers have reported on the use of CF as an interface to MTurk (Wang and Callison-Burch, 2010; Finin et al., 2010; Negri and Mehdad, 2010), none"
2011.mtsummit-papers.59,W10-0713,0,0.0166157,"ehension questions. In (Denkowski and Lavie, 2010) MTurk was used to obtain translation adequacy assessments. Finally, in (Callison-Burch et al., 2010) a subset of the ofﬁcial WMT10 relative ranking evaluation was reproduced with non-expert judges and various methods to improve the quality of the collected data are presented. All these MT evaluation experiments have been conducted using MTurk directly, as have most of the available studies on the effectiveness of crowdsourcing. Up to now, only a few papers have reported on the use of CF as an interface to MTurk (Wang and Callison-Burch, 2010; Finin et al., 2010; Negri and Mehdad, 2010), none of them addressing the task of MT evaluation. 3 IWSLT 2010 BTEC Task Evaluation The International Workshop on Spoken Language Translation (IWSLT) is a yearly, open evaluation campaign for spoken language translation. IWSLT’s evaluations are not competition-oriented; their goal is to foster cooperative work and scientiﬁc exchange. In this respect, IWSLT proposes challenging research tasks and an open experimental infrastructure for the scientiﬁc community working on spoken and written language translation. In the IWSLT 2010 campaign (Paul et al., 2010), three dif"
2011.mtsummit-papers.59,W10-0734,0,0.0133446,"In (Denkowski and Lavie, 2010) MTurk was used to obtain translation adequacy assessments. Finally, in (Callison-Burch et al., 2010) a subset of the ofﬁcial WMT10 relative ranking evaluation was reproduced with non-expert judges and various methods to improve the quality of the collected data are presented. All these MT evaluation experiments have been conducted using MTurk directly, as have most of the available studies on the effectiveness of crowdsourcing. Up to now, only a few papers have reported on the use of CF as an interface to MTurk (Wang and Callison-Burch, 2010; Finin et al., 2010; Negri and Mehdad, 2010), none of them addressing the task of MT evaluation. 3 IWSLT 2010 BTEC Task Evaluation The International Workshop on Spoken Language Translation (IWSLT) is a yearly, open evaluation campaign for spoken language translation. IWSLT’s evaluations are not competition-oriented; their goal is to foster cooperative work and scientiﬁc exchange. In this respect, IWSLT proposes challenging research tasks and an open experimental infrastructure for the scientiﬁc community working on spoken and written language translation. In the IWSLT 2010 campaign (Paul et al., 2010), three different shared tasks addre"
2011.mtsummit-papers.59,J82-2005,0,0.718625,"Missing"
2011.mtsummit-papers.59,W10-0725,0,0.0281807,"valuation through reading comprehension questions. In (Denkowski and Lavie, 2010) MTurk was used to obtain translation adequacy assessments. Finally, in (Callison-Burch et al., 2010) a subset of the ofﬁcial WMT10 relative ranking evaluation was reproduced with non-expert judges and various methods to improve the quality of the collected data are presented. All these MT evaluation experiments have been conducted using MTurk directly, as have most of the available studies on the effectiveness of crowdsourcing. Up to now, only a few papers have reported on the use of CF as an interface to MTurk (Wang and Callison-Burch, 2010; Finin et al., 2010; Negri and Mehdad, 2010), none of them addressing the task of MT evaluation. 3 IWSLT 2010 BTEC Task Evaluation The International Workshop on Spoken Language Translation (IWSLT) is a yearly, open evaluation campaign for spoken language translation. IWSLT’s evaluations are not competition-oriented; their goal is to foster cooperative work and scientiﬁc exchange. In this respect, IWSLT proposes challenging research tasks and an open experimental infrastructure for the scientiﬁc community working on spoken and written language translation. In the IWSLT 2010 campaign (Paul et a"
2011.mtsummit-papers.59,1994.amta-1.25,0,0.69757,"Missing"
2013.iwslt-evaluation.1,2010.iwslt-evaluation.1,1,0.863584,"al of 217 primary runs submitted. All runs were evaluated with objective metrics on a current test set and two progress test sets, in order to compare the progresses against systems of the previous years. In addition, submissions of one of the official machine translation tracks were also evaluated with human post-editing. 1. Introduction This paper overviews the results of the evaluation campaign organized by the International Workshop of Spoken Language Translation. The IWSLT evaluation has been now running for a decade and has offered along these years a variety of speech translation tasks [1, 2, 3, 4, 5, 6, 7, 8, 9]. The 2013 IWSLT evaluation continued along the line set in 2010, by focusing on the translation of TED Talks, a collection of public speeches covering many different topics. As in the previous two years, the evaluation included tracks for all the core technologies involved in the spoken language translation task, namely: • Automatic speech recognition (ASR), i.e. the conversion of a speech signal into a transcript, • Machine translation (MT), i.e. the translation of a polished transcript into another language, • Spoken language translation (SLT), that addressed the conversion and translation"
2013.iwslt-evaluation.1,2012.eamt-1.60,1,0.897031,"on tracks, many other optional translation directions were also offered. Optional SLT directions were from English to Spanish, Portuguese (B), Italian, Chinese, Polish, Slovenian, Arabic, and Persian. Optional MT translation directions were: English from/to Arabic, Spanish, Portuguese (B), Italian, Chinese, Polish, Persian, Slovenian, Turkish, Dutch, Romanian, and Russian. For each official and optional translation direction, training and development data were supplied by the organizers through the workshop’s website. Major parallel collections made available to the participants were the WIT3 [10] corpus of TED talks, all data from the WMT 2013 workshop [11], the MULTIUN corpus [12], and the SETIMES parallel corpus [13]. A list of monolingual resources was provided too, that includes both freely available corpora and corpora available from the LDC. Test data were released at the begin of each test period, requiring participants to return one primary run and optional contrastive runs within one week. The schedule of the evaluation was organized as follows: June 8, release of training data; Sept 2-8, ASR test of period; Sept 9-15, SLT test period; Oct 7-13, MT test period; Oct 7-20, test"
2013.iwslt-evaluation.1,2013.iwslt-evaluation.21,0,0.0238082,"Missing"
2013.iwslt-evaluation.1,2013.iwslt-evaluation.11,0,0.0818098,"Missing"
2013.iwslt-evaluation.1,2013.iwslt-evaluation.5,0,0.0447354,"Missing"
2013.iwslt-evaluation.1,2013.iwslt-evaluation.6,0,0.0540129,"Missing"
2013.iwslt-evaluation.1,2013.iwslt-evaluation.9,0,0.0348889,"Missing"
2013.iwslt-evaluation.1,2013.iwslt-evaluation.23,0,0.0935135,"Missing"
2013.iwslt-evaluation.1,2005.mtsummit-papers.11,0,0.125368,"y, the standard dev2010 and tst2010 development sets have been released as well. Tables 2 and 3 provide statistics on in-domain texts supplied for training, development and evaluation purposes for the official directions. Reference results from baseline MT systems on the development set tst2010 are provided via the WIT3 repository. This helps participants and MT scientists to assess their experimental outcomes. MT baselines were trained from TED data only, i.e. no additional out-of-domain resources were used. The standard tokenization via the tokenizer script released with the Europarl corpus [38] was applied to all languages, with the exception of Chinese and Arabic languages, which were preprocessed by, respectively: the Stanford Chinese Segmenter [39]; either AMIRA [40], in the Arabic-to-English direction, or the QCRI-normalizer,3 in the English-to-Arabic direction. The baselines were developed with the Moses toolkit [41]. Translation and lexicalized reordering models were trained on the parallel training data; 5-gram LMs with improved Kneser-Ney smoothing were estimated on the target side of the training parallel data with the IRSTLM toolkit [42]. The weights of the log-linear inte"
2013.iwslt-evaluation.1,N04-4038,0,0.0999494,"tion purposes for the official directions. Reference results from baseline MT systems on the development set tst2010 are provided via the WIT3 repository. This helps participants and MT scientists to assess their experimental outcomes. MT baselines were trained from TED data only, i.e. no additional out-of-domain resources were used. The standard tokenization via the tokenizer script released with the Europarl corpus [38] was applied to all languages, with the exception of Chinese and Arabic languages, which were preprocessed by, respectively: the Stanford Chinese Segmenter [39]; either AMIRA [40], in the Arabic-to-English direction, or the QCRI-normalizer,3 in the English-to-Arabic direction. The baselines were developed with the Moses toolkit [41]. Translation and lexicalized reordering models were trained on the parallel training data; 5-gram LMs with improved Kneser-Ney smoothing were estimated on the target side of the training parallel data with the IRSTLM toolkit [42]. The weights of the log-linear interpolation model were 3 Specifically developed for IWSLT 2013 by P. Nakov and F. Al-Obaidli at Qatar Computing Research Institute. Table 3: Bilingual resources for official languag"
2013.iwslt-evaluation.1,P07-2045,1,0.010973,"This helps participants and MT scientists to assess their experimental outcomes. MT baselines were trained from TED data only, i.e. no additional out-of-domain resources were used. The standard tokenization via the tokenizer script released with the Europarl corpus [38] was applied to all languages, with the exception of Chinese and Arabic languages, which were preprocessed by, respectively: the Stanford Chinese Segmenter [39]; either AMIRA [40], in the Arabic-to-English direction, or the QCRI-normalizer,3 in the English-to-Arabic direction. The baselines were developed with the Moses toolkit [41]. Translation and lexicalized reordering models were trained on the parallel training data; 5-gram LMs with improved Kneser-Ney smoothing were estimated on the target side of the training parallel data with the IRSTLM toolkit [42]. The weights of the log-linear interpolation model were 3 Specifically developed for IWSLT 2013 by P. Nakov and F. Al-Obaidli at Qatar Computing Research Institute. Table 3: Bilingual resources for official language pairs task MTEnF r MTDeEn MTEnDe data set train dev2010 tst2010 tst2011 tst2012 tst2013 train dev2010 tst2010 dev2012 tst2013 train dev2010 tst2010 tst20"
2013.iwslt-evaluation.1,2012.amta-papers.22,1,0.822673,"Missing"
2013.iwslt-evaluation.1,2006.amta-papers.25,0,0.207086,"Missing"
2013.iwslt-evaluation.1,J93-3001,0,0.516669,"Missing"
2013.iwslt-evaluation.1,W05-0908,0,0.0754408,"Missing"
2013.iwslt-evaluation.1,1993.eamt-1.1,0,0.4595,"Missing"
2014.iwslt-evaluation.1,2010.iwslt-evaluation.1,1,0.792952,"pants were also asked to submit runs on the 2013 test set (progress test set), in order to measure the progress of systems with respect to the previous year. All runs were evaluated with objective metrics, and submissions for two of the official text translation tracks were also evaluated with human post-editing. 1. Introduction This paper overviews the results of the 2014 evaluation campaign organized by the International Workshop of Spoken Language Translation. The IWSLT evaluation has been running now for over a decade and has offered along these years a variety of speech translation tasks [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]. The 2014 IWSLT evaluation continued along the line set in 2010, by focusing on the translation of TED Talks, a collection of public speeches covering many different topics. As in the previous two years, the evaluation included tracks for all the core technologies involved in the spoken language translation task, namely: • Automatic speech recognition (ASR), i.e. the conversion of a speech signal into a transcript, • Spoken language translation (SLT), that addressed the conversion and translation of a speech signal into a transcript in another language, • Machine translation (MT), i.e. the tr"
2014.iwslt-evaluation.1,2014.iwslt-evaluation.15,0,0.0679155,"Missing"
2014.iwslt-evaluation.1,2014.iwslt-evaluation.14,0,0.0614042,"Missing"
2014.iwslt-evaluation.1,2014.iwslt-evaluation.7,1,0.877515,"Missing"
2014.iwslt-evaluation.1,2014.iwslt-evaluation.13,0,0.0364543,"Missing"
2014.iwslt-evaluation.1,2014.iwslt-evaluation.5,1,0.82235,"Missing"
2014.iwslt-evaluation.1,2014.iwslt-evaluation.12,0,0.0879094,"Missing"
2014.iwslt-evaluation.1,2014.iwslt-evaluation.21,0,0.047922,"Missing"
2014.iwslt-evaluation.1,2014.iwslt-evaluation.10,0,0.0535615,"Missing"
2014.iwslt-evaluation.1,1993.eamt-1.1,0,0.523227,"Missing"
2014.iwslt-evaluation.1,2005.mtsummit-papers.11,0,0.0265085,"time that Italian is involved in ASR/SLT tracks, therefore no evaluation set is available for assessing progress. A single TEDx based development set was released for each pair, together with standard TED based development sets dev2010, tst2010, tst2011 and tst2012 sets. Tables 2 and 3 provides statistics on in-domain texts supplied for training, development and evaluation purposes for the official directions. MT baselines were trained from TED data only, i.e. no additional out-of-domain resources were used. The standard tokenization via the tokenizer script released with the Europarl corpus [33] was applied to all languages, with the exception of Chinese and Arabic languages, which were sent 179k 887 1,664 818 1,124 1,026 1,305 172k 887 1,565 1,433 1,700 993 1,305 1,165 1,363 1,414 182k 887 1,529 1,433 1,704 1,402 1,183 1,056 883 tokens talks En Fr 3.63M 3.88M 1415 20,1k 20,2k 8 32,0k 33,9k 11 14,5k 15,6k 8 21,5k 23,5k 11 21,7k 23,3k 16 24,8k 27,5k 15 En De 3.46M 3.24M 1361 20,1k 19,1k 8 32,0k 30,3k 11 26,9k 26,3k 16 30,7k 29,2k 15 20,9k 19,7k 16 24,8k 23,8k 15 21,6k 20,8k 7 23,3k 22,4k 9 28,1k 27,6k 10 En It 3.68M 3.44M 1434 20,1k 17,9k 8 31,0k 28,7k 10 26,9k 24,5k 16 30,7k 28,2k 15"
2014.iwslt-evaluation.1,2012.amta-papers.22,1,0.779032,"amely the MT English-German (EnDe) track and MT English-French (EnF r) track. Following the methodology introduced last year, human evaluation was based on PostEditing, and HTER (Human-mediated Translation Edit Rate) was adopted as the official evaluation metric to rank the systems. Post-Editing, i.e. the manual correction of machine translation output, has long been investigated by the translation industry as a form of machine assistance to reduce the costs of human translation. Nowadays, Computer-aided translation (CAT) tools incorporate post-editing functionalities, and a number of studies [35, 36] demonstrate the usefulness of MT to increase professional translators’ productivity. The MT TED task offered in IWSLT can be seen as an interesting application scenario to test the utility of MT systems in a real subtitling task. 5.4. Results Table 4: BLEU and TER scores of baseline SMT systems on all tst2014 sets. († ) TEDx test set. (⋆ ) Char-level scores. pair En Fr De It Ar Es Fa He Nl Pl Pt Ro Ru Sl Tr Zh direction BLEU 32.07 18.33 27.15 11.13 31.31 11.31 15.91 22.77 9.63 31.25 18.05 11.74 8.46 7.75 ⋆ 16.49 → TER 48.62 62.11 53.19 73.01 48.29 71.20 65.62 58.38 82.81 47.25 65.25 71.99 73."
2014.iwslt-evaluation.1,2006.amta-papers.25,0,0.397152,"paign, our goal was to adopt a human evaluation framework able to maximize the benefit to the research community, both in terms of information about MT systems and data and resources to be reused. With respect to other types of human assessment, such as judgments of translation quality (i.e. adequacy/fluency and ranking tasks), the post-editing task has the double advantage of producing (i) a set of edits pointing to specific translation errors, and (ii) a set of additional reference translations. Both these byproducts are very useful for MT system development and evaluation. Furthermore, HTER[37] - which consists of measuring the minimum edit distance between the machine translation and its manually post-edited version - has been shown to correlate quite well with human judgments of MT quality. First of all, for reference purposes Table 4 shows BLEU and TER scores on the tst2014 evaluation sets of the baseline systems we developed as described in Section 5.1. The results on the official test set for each participant are shown in Appendix A.1. For most languages, we show the case-sensitive and case-insensitive BLEU and TER scores. The human evaluation setup and the collection of posted"
2014.iwslt-evaluation.1,J93-3001,0,0.560866,"Missing"
2015.iwslt-evaluation.1,2005.iwslt-1.19,0,\N,Missing
2015.iwslt-evaluation.1,2007.iwslt-1.1,0,\N,Missing
2015.iwslt-evaluation.1,2005.iwslt-1.1,0,\N,Missing
2015.iwslt-evaluation.1,2004.iwslt-evaluation.1,1,\N,Missing
2015.iwslt-evaluation.1,J93-3001,0,\N,Missing
2015.iwslt-evaluation.1,W05-0908,0,\N,Missing
2015.iwslt-evaluation.1,2005.mtsummit-papers.11,0,\N,Missing
2015.iwslt-evaluation.1,2015.iwslt-evaluation.16,0,\N,Missing
2015.iwslt-evaluation.1,2006.iwslt-evaluation.1,0,\N,Missing
2015.iwslt-evaluation.1,2008.iwslt-evaluation.1,0,\N,Missing
2015.iwslt-evaluation.1,2011.iwslt-evaluation.1,1,\N,Missing
2015.iwslt-evaluation.1,2009.iwslt-evaluation.1,0,\N,Missing
2015.iwslt-evaluation.1,2012.eamt-1.60,1,\N,Missing
2015.iwslt-evaluation.1,2010.iwslt-evaluation.1,1,\N,Missing
2020.acl-main.619,E06-1032,0,0.285666,"Missing"
2020.acl-main.619,W19-3824,0,0.276167,"counting for MT systems’ strengths and weaknesses in the translation of gender shed light on the problem but, at the same time, have limitations. On one hand, the existing evaluations focused on gender bias were largely conducted on challenge datasets, which are controlled 6923 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6923–6933 c July 5 - 10, 2020. 2020 Association for Computational Linguistics artificial benchmarks that provide a limited perspective on the extent of the phenomenon and may force unreliable conclusions (Prates et al., 2018; Cho et al., 2019; Escud´e Font and Costa-juss`a, 2019; Stanovsky et al., 2019). On the other hand, the natural corpora built on conversational language that were used in few studies (Elaraby et al., 2018; Vanmassenhove et al., 2018) include only a restricted quantity of not isolated gender-expressing forms, thus not permitting either extensive or targeted evaluations. Moreover, no attempt has yet been made to assess if and how speech translation (ST) systems are affected by this particular problem. As such, whether ST technologies that leverage audio inputs can retrieve useful clues for translating gender in"
2020.acl-main.619,N19-1202,1,0.898237,"Missing"
2020.acl-main.619,S18-2005,0,0.14022,"ias in speech translation, contributing with: i) the release of a benchmark useful for future studies, and ii) the comparison of different technologies (cascade and end-to-end) on two language directions (English-Italian/French). 1 Introduction With the exponential popularity of deep learning approaches for a great range of natural language processing (NLP) tasks being integrated in our daily life, the need to address the issues of gender fairness1 and gender bias has become a growing interdisciplinary concern. Present-day studies on a variety of NLP-related tasks, such as sentiment analysis (Kiritchenko and Mohammad, 2018) coreference resolution (Rudinger et al., 2018; Webster et al., 2018; Zhao et al., 2018), visual semantic-role labeling (Zhao et al., 2017) or language modeling (Lu ∗ ∗These authors contributed equally. The work by Beatrice Savoldi was carried out during an internship at Fondazione Bruno Kessler. 1 We acknowledge that gender is a multifaceted notion, not necessarily constrained within binary assumptions. However, since speech translation is hindered by the scarcity of available data, we rely on the female/male distinction of gender, as it is linguistically reflected in existing natural data. e"
2020.acl-main.619,L18-1001,0,0.119832,"bias in ST, which, as explained in §4, is used for a targeted gender-sensitive evaluation approach. 3 The MuST-SHE benchmark We built MuST-SHE on naturally occurring data retrieved from MuST-C (Di Gangi et al., 2019a), the largest freely available multilingual corpus for ST, which comprises (audio, transcript, translation) triplets extracted from TED talks data. Besides being multilingual, MuST-C is characterized by highquality speech and a variety of different speakers that adequately represent women, two aspects that determined its selection among other existing corpora (Post et al., 2013; Kocabiyikoglu et al., 2018; Sanabria et al., 2018). As such, MuST-SHE was compiled by targeting in the original dataset linguistic phenomena that entail a gender identification from English into Italian and French, two Romance languages that extensively express gender via feminine or masculine morphological markers on nouns, adjectives, verbs and other functional words (e.g. articles and demonstratives). 3.1 Categorization of gender phenomena MuST-SHE is compiled with segments that require the translation of at least one English genderneutral word into the corresponding masculine or feminine target word(s), where such"
2020.acl-main.619,W19-3807,0,0.171844,"male test sets containing first person singular pronouns. This strategy increases the chances to isolate speakerdependent gendered expressions, but still, the employed BLEU metric does not pointedly grasp the effect of gender translation on the output, as the overall performance is also impacted by other factors. Analogously, Elaraby et al. (2018) design a set of agreement rules to automatically recover 300 gender-affected sentences in their corpus, but the evaluation relies on global BLEU scores computed on a bigger set (1,300 sentences) and does not consider male-female related differences. Moryossef et al. (2019) use a parser to detect morphological realizations of speakers’ gender on a single femalespeaker corpus that does not permit inter-gender comparisons. In light of above, an ideal test set should consist of naturally occurring data exhibiting a diversified assortment of gender phenomena so to avoid forced predictions with over-controlled procedures. Also, a consistent amount of equally distributed feminine and masculine gender realizations need to be identified to disentangle the accuracy of gender translation from the overall model’s performance. Accordingly, in §3 we present MuST-SHE, a multi"
2020.acl-main.619,W19-3821,0,0.0629603,"Missing"
2020.acl-main.619,J86-2003,0,0.123168,"data, we rely on the female/male distinction of gender, as it is linguistically reflected in existing natural data. et al., 2019), attest the existence of a systemic bias that reproduces gender stereotypes discriminating women. In translation-related tasks, gender bias arises from the extent through which each language formally expresses the female or male gender of a referred human entity. Languages with a grammatical system of gender, such as Romance languages, rely on a copious set of morphological (inflection) and syntactic (gender agreement) devices applying to numerous parts of speech (Hockett, 1958). Differently, English is a natural gender language that only reflects distinction of sex via pronouns, inherently gendered words (boy, girl) and exceptionally with marked nouns (actor, actress). For all the other indistinct neutral words, the gender of the referred entity – if available – is inferred from contextual information present in the discourse, e.g. he/she is a friend. Nascent inquiries on machine translation (MT) pointed out that machines tend to reproduce the linguistic asymmetries present in the real-world data they are trained on. In the case of gender inequality, this is made ap"
2020.acl-main.619,P02-1040,0,0.111543,"Missing"
2020.acl-main.619,2013.iwslt-papers.14,0,0.0283928,"stigation of gender bias in ST, which, as explained in §4, is used for a targeted gender-sensitive evaluation approach. 3 The MuST-SHE benchmark We built MuST-SHE on naturally occurring data retrieved from MuST-C (Di Gangi et al., 2019a), the largest freely available multilingual corpus for ST, which comprises (audio, transcript, translation) triplets extracted from TED talks data. Besides being multilingual, MuST-C is characterized by highquality speech and a variety of different speakers that adequately represent women, two aspects that determined its selection among other existing corpora (Post et al., 2013; Kocabiyikoglu et al., 2018; Sanabria et al., 2018). As such, MuST-SHE was compiled by targeting in the original dataset linguistic phenomena that entail a gender identification from English into Italian and French, two Romance languages that extensively express gender via feminine or masculine morphological markers on nouns, adjectives, verbs and other functional words (e.g. articles and demonstratives). 3.1 Categorization of gender phenomena MuST-SHE is compiled with segments that require the translation of at least one English genderneutral word into the corresponding masculine or feminine"
2020.acl-main.619,N18-2002,0,0.0661732,"Missing"
2020.acl-main.619,2006.amta-papers.25,0,0.105408,"ion Testset (Sun et al., 2019), and represents posite gender form (containing feminine-marked the very first of its kind for ST and MT created on words “une”, “grandes” and “innovatrices”). The natural data. result is a new set of references that, compared to the correct ones, are “wrong” only with respect to 4 Experimental Setting the formal expression of gender. 4.1 Evaluation Method The underlying idea is that, as the two reference sets differ only for the swapped gendered forms, MT evaluation metrics like BLEU (Papineni et al., results’ differences for the same set of hypothe2002) or TER (Snover et al., 2006) provide a global ses produced by a given system can measure its score about translation “quality” as a whole. Used as-is, their holistic nature hinders the precise eval- capability to handle gender phenomena. In particuation of systems’ performance on an individual 5 Still the de facto standard in MT evaluation in spite of phenomenon as gender translation, since the vari- constant research efforts towards metrics that better correlate ations of BLEU score are only a coarse and indi- with human judgements. 6927 ular, we argue that higher values on the wrong set can signal a potentially gender-"
2020.acl-main.619,P19-1115,0,0.0259698,"Missing"
2020.acl-main.619,P19-1164,0,0.384908,"e translation of gender shed light on the problem but, at the same time, have limitations. On one hand, the existing evaluations focused on gender bias were largely conducted on challenge datasets, which are controlled 6923 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6923–6933 c July 5 - 10, 2020. 2020 Association for Computational Linguistics artificial benchmarks that provide a limited perspective on the extent of the phenomenon and may force unreliable conclusions (Prates et al., 2018; Cho et al., 2019; Escud´e Font and Costa-juss`a, 2019; Stanovsky et al., 2019). On the other hand, the natural corpora built on conversational language that were used in few studies (Elaraby et al., 2018; Vanmassenhove et al., 2018) include only a restricted quantity of not isolated gender-expressing forms, thus not permitting either extensive or targeted evaluations. Moreover, no attempt has yet been made to assess if and how speech translation (ST) systems are affected by this particular problem. As such, whether ST technologies that leverage audio inputs can retrieve useful clues for translating gender in addition to contextual information present in the discourse, o"
2020.acl-main.619,P19-1159,0,0.193689,"from standard test sets, as it gender agreement. In particular, for each genderis precisely designed to: i) equally distribute genneutral English word in the source utterance (e.g. der references as well as speakers, and ii) allow “one”, “great” and “innovators” in the 4th examfor a sound and focused evaluation on the accuracy ple of Table 1), the correct translation (containing of gender translation. As such, it satisfies the pathe French words with masculine inflection “un”, rameters to be qualified as a GBET, Gender Bias “grands” and “innovateurs”) is swapped into its opEvaluation Testset (Sun et al., 2019), and represents posite gender form (containing feminine-marked the very first of its kind for ST and MT created on words “une”, “grandes” and “innovatrices”). The natural data. result is a new set of references that, compared to the correct ones, are “wrong” only with respect to 4 Experimental Setting the formal expression of gender. 4.1 Evaluation Method The underlying idea is that, as the two reference sets differ only for the swapped gendered forms, MT evaluation metrics like BLEU (Papineni et al., results’ differences for the same set of hypothe2002) or TER (Snover et al., 2006) provide a"
2020.acl-main.619,D18-1334,0,0.366988,"s were largely conducted on challenge datasets, which are controlled 6923 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6923–6933 c July 5 - 10, 2020. 2020 Association for Computational Linguistics artificial benchmarks that provide a limited perspective on the extent of the phenomenon and may force unreliable conclusions (Prates et al., 2018; Cho et al., 2019; Escud´e Font and Costa-juss`a, 2019; Stanovsky et al., 2019). On the other hand, the natural corpora built on conversational language that were used in few studies (Elaraby et al., 2018; Vanmassenhove et al., 2018) include only a restricted quantity of not isolated gender-expressing forms, thus not permitting either extensive or targeted evaluations. Moreover, no attempt has yet been made to assess if and how speech translation (ST) systems are affected by this particular problem. As such, whether ST technologies that leverage audio inputs can retrieve useful clues for translating gender in addition to contextual information present in the discourse, or supply for their lack, remains a largely unexplored question. In the light of above, the contributions of this paper are: (1) We present the first syste"
2020.acl-main.619,Q18-1042,0,0.0272845,"ful for future studies, and ii) the comparison of different technologies (cascade and end-to-end) on two language directions (English-Italian/French). 1 Introduction With the exponential popularity of deep learning approaches for a great range of natural language processing (NLP) tasks being integrated in our daily life, the need to address the issues of gender fairness1 and gender bias has become a growing interdisciplinary concern. Present-day studies on a variety of NLP-related tasks, such as sentiment analysis (Kiritchenko and Mohammad, 2018) coreference resolution (Rudinger et al., 2018; Webster et al., 2018; Zhao et al., 2018), visual semantic-role labeling (Zhao et al., 2017) or language modeling (Lu ∗ ∗These authors contributed equally. The work by Beatrice Savoldi was carried out during an internship at Fondazione Bruno Kessler. 1 We acknowledge that gender is a multifaceted notion, not necessarily constrained within binary assumptions. However, since speech translation is hindered by the scarcity of available data, we rely on the female/male distinction of gender, as it is linguistically reflected in existing natural data. et al., 2019), attest the existence of a systemic bias that reproduce"
2020.acl-main.619,D17-1323,0,0.0216682,"(cascade and end-to-end) on two language directions (English-Italian/French). 1 Introduction With the exponential popularity of deep learning approaches for a great range of natural language processing (NLP) tasks being integrated in our daily life, the need to address the issues of gender fairness1 and gender bias has become a growing interdisciplinary concern. Present-day studies on a variety of NLP-related tasks, such as sentiment analysis (Kiritchenko and Mohammad, 2018) coreference resolution (Rudinger et al., 2018; Webster et al., 2018; Zhao et al., 2018), visual semantic-role labeling (Zhao et al., 2017) or language modeling (Lu ∗ ∗These authors contributed equally. The work by Beatrice Savoldi was carried out during an internship at Fondazione Bruno Kessler. 1 We acknowledge that gender is a multifaceted notion, not necessarily constrained within binary assumptions. However, since speech translation is hindered by the scarcity of available data, we rely on the female/male distinction of gender, as it is linguistically reflected in existing natural data. et al., 2019), attest the existence of a systemic bias that reproduces gender stereotypes discriminating women. In translation-related tasks"
2020.acl-main.619,N18-2003,0,0.135351,", and ii) the comparison of different technologies (cascade and end-to-end) on two language directions (English-Italian/French). 1 Introduction With the exponential popularity of deep learning approaches for a great range of natural language processing (NLP) tasks being integrated in our daily life, the need to address the issues of gender fairness1 and gender bias has become a growing interdisciplinary concern. Present-day studies on a variety of NLP-related tasks, such as sentiment analysis (Kiritchenko and Mohammad, 2018) coreference resolution (Rudinger et al., 2018; Webster et al., 2018; Zhao et al., 2018), visual semantic-role labeling (Zhao et al., 2017) or language modeling (Lu ∗ ∗These authors contributed equally. The work by Beatrice Savoldi was carried out during an internship at Fondazione Bruno Kessler. 1 We acknowledge that gender is a multifaceted notion, not necessarily constrained within binary assumptions. However, since speech translation is hindered by the scarcity of available data, we rely on the female/male distinction of gender, as it is linguistically reflected in existing natural data. et al., 2019), attest the existence of a systemic bias that reproduces gender stereotypes"
2020.coling-main.350,P14-2134,0,0.0272858,"nslation from English into Italian and French. To this aim, we manually annotated large datasets with speakers’ gender information and used them for experiments reflecting different possible real-world scenarios. Our results show that gender-aware direct ST solutions can significantly outperform strong – but gender-unaware – direct ST models. In particular, the translation of gender-marked words can increase up to 30 points in accuracy while preserving overall translation quality. 1 Introduction Language use is intrinsically social and situated as it varies across groups and even individuals (Bamman et al., 2014). As a result, the language data that are collected to build the corpora on which natural language processing models are trained are often far from being homogeneous and rarely offer a fair representation of different demographic groups and their linguistic behaviours (Bender and Friedman, 2018). Consequently, as predictive models learn from the data distribution they have seen, they tend to favor the demographic group most represented in their training data (Hovy and Spruit, 2016; Shah et al., 2020). This brings serious social consequences as well, since the people who are more likely to be u"
2020.coling-main.350,N19-1006,0,0.0279948,"ely on data augmentation and knowledge transfer techniques that were shown to yield competitive models at the IWSLT-2020 evaluation campaign (Ansari et al., 2020; Potapczyk and Przybysz, 2020; Gaido et al., 2020). In particular, we use three data augmentation methods – SpecAugment (Park et al., 2019), time stretch (Nguyen et al., 2020), and synthetic data generation (Jia et al., 2019) – and we transfer knowledge both from ASR and MT through component initialization and knowledge distillation (Hinton et al., 2015). The ST model’s encoder is initialized with the encoder of an English ASR model (Bansal et al., 2019) with a lower number of encoder layers (the missing layers are initialized randomly, as well as the decoder). This ASR model is trained on Librispeech (Panayotov et al., 2015), Mozilla Common Voice,5 How2 (Sanabria et al., 2018), TEDLIUM-v3 (Hernandez et al., 2018), and the utterance-transcript pairs of the ST corpora – Europarl-ST (Iranzo-S´anchez et al., 2020) and MuST-C. These datasets are either gender unbalanced or do not provide speaker’s gender information apart from Librispeech, which is balanced in terms of female/male speakers (Garnerin et al., 2020). However, since these speakers ar"
2020.coling-main.350,Q18-1041,0,0.0116889,"erform strong – but gender-unaware – direct ST models. In particular, the translation of gender-marked words can increase up to 30 points in accuracy while preserving overall translation quality. 1 Introduction Language use is intrinsically social and situated as it varies across groups and even individuals (Bamman et al., 2014). As a result, the language data that are collected to build the corpora on which natural language processing models are trained are often far from being homogeneous and rarely offer a fair representation of different demographic groups and their linguistic behaviours (Bender and Friedman, 2018). Consequently, as predictive models learn from the data distribution they have seen, they tend to favor the demographic group most represented in their training data (Hovy and Spruit, 2016; Shah et al., 2020). This brings serious social consequences as well, since the people who are more likely to be underrepresented within datasets are those whose representation is often less accounted for within our society. A case in point regards the gender data gap.1 In fact, studies on speech taggers (Hovy and Søgaard, 2015) and speech recognition (Tatman, 2017) showed that the underrepresentation of fe"
2020.coling-main.350,2020.acl-main.619,1,0.851509,"Missing"
2020.coling-main.350,2020.acl-main.418,0,0.0734234,"Missing"
2020.coling-main.350,N19-1202,1,0.887446,"Missing"
2020.coling-main.350,W19-6603,1,0.668742,"Missing"
2020.coling-main.350,W19-3821,0,0.0520695,"Missing"
2020.coling-main.350,2020.iwslt-1.8,1,0.829653,"Missing"
2020.coling-main.350,2020.lrec-1.813,0,0.0165881,"he encoder of an English ASR model (Bansal et al., 2019) with a lower number of encoder layers (the missing layers are initialized randomly, as well as the decoder). This ASR model is trained on Librispeech (Panayotov et al., 2015), Mozilla Common Voice,5 How2 (Sanabria et al., 2018), TEDLIUM-v3 (Hernandez et al., 2018), and the utterance-transcript pairs of the ST corpora – Europarl-ST (Iranzo-S´anchez et al., 2020) and MuST-C. These datasets are either gender unbalanced or do not provide speaker’s gender information apart from Librispeech, which is balanced in terms of female/male speakers (Garnerin et al., 2020). However, since these speakers are just book narrators, first-person sentences do not really refer to the speakers themselves. Knowledge distillation (KD) is performed from a teacher MT model by optimizing the cross entropy between the distribution produced by the teacher and by the student ST model being trained (Liu et al., 2019). For both en-it and en-fr, the MT model is trained on the OPUS datasets (Tiedemann, 2016). The ST model is trained in three consecutive steps. In the first step, we use the synthetic data obtained by pairing ASR audio samples with the automatic translations of the"
2020.coling-main.350,J86-2003,0,0.172817,"uages that do not convey such information. Indeed, languages with grammatical gender, such as French and † The authors contributed equally. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. 1 For a comprehensive overview on such societal issue see (Criado-Perez, 2019). License details: http:// 3951 Proceedings of the 28th International Conference on Computational Linguistics, pages 3951–3964 Barcelona, Spain (Online), December 8-13, 2020 Italian, display a complex morphosyntactic and semantic system of gender agreement (Hockett, 1958; Corbett, 1991), relying on feminine/masculine markings that reflect speakers’ gender on numerous parts of speech whenever they are talking about themselves (e.g. En: I’ve never been there – It: Non ci sono mai stata/stato). Differently, English is a natural gender language (Hellinger and Bußman, 2001) that mostly conveys gender via its pronoun system, but only for third-person pronouns (he/she), thus to refer to an entity other than the speaker. As the example shows, in absence of contextual information (e.g As a woman, I have never been there) correctly translating gender can be prohibitive"
2020.coling-main.350,P15-2079,0,0.0126195,"resentation of different demographic groups and their linguistic behaviours (Bender and Friedman, 2018). Consequently, as predictive models learn from the data distribution they have seen, they tend to favor the demographic group most represented in their training data (Hovy and Spruit, 2016; Shah et al., 2020). This brings serious social consequences as well, since the people who are more likely to be underrepresented within datasets are those whose representation is often less accounted for within our society. A case in point regards the gender data gap.1 In fact, studies on speech taggers (Hovy and Søgaard, 2015) and speech recognition (Tatman, 2017) showed that the underrepresentation of female speakers in the training data leads to significantly lower accuracy in modeling that demographic group. The problem of gender-related differences has also been inspected within automatic translation, both from text (Vanmassenhove et al., 2018) and from audio (Bentivogli et al., 2020). These studies – focused on the translation of spoken language – revealed a systemic gender bias whenever systems are required to overtly and formally express speaker’s gender in the target languages, while translating from langua"
2020.coling-main.350,P16-2096,0,0.0643539,". 1 Introduction Language use is intrinsically social and situated as it varies across groups and even individuals (Bamman et al., 2014). As a result, the language data that are collected to build the corpora on which natural language processing models are trained are often far from being homogeneous and rarely offer a fair representation of different demographic groups and their linguistic behaviours (Bender and Friedman, 2018). Consequently, as predictive models learn from the data distribution they have seen, they tend to favor the demographic group most represented in their training data (Hovy and Spruit, 2016; Shah et al., 2020). This brings serious social consequences as well, since the people who are more likely to be underrepresented within datasets are those whose representation is often less accounted for within our society. A case in point regards the gender data gap.1 In fact, studies on speech taggers (Hovy and Søgaard, 2015) and speech recognition (Tatman, 2017) showed that the underrepresentation of female speakers in the training data leads to significantly lower accuracy in modeling that demographic group. The problem of gender-related differences has also been inspected within automat"
2020.coling-main.350,W17-1601,0,0.423292,"ot equally accurate across demographic groups (e.g. women and children are hard to distinguish as their pitch is typically high (Levitan et al., 2016)), manual assignment prevents from incorporating gender misclassifications in our training data. Second, biological essentialist frameworks that categorize gender based on acoustic cues (Zimman, 2020) are especially problematic for transgender individuals, whose gender identity is not aligned with the sex they have been assigned at birth based on designated anatomical/biological criteria (Stryker, 2008). Differently, following the guidelines in (Larson, 2017), we do not want to run the risk of making assumptions about speakers’ gender identity and introducing additional bias within an environment that has been specifically designed to inspect gender bias. By looking at the personal pronouns used by the speakers to describe themselves, our manual assignment instead is meant to account for the gender linguistic forms by which the speakers accept to be referred to in English (GLAAD, 2007), and would want their translations to conform to. We stress that gendered linguistic expressions do not directly map to speakers’ self-determined gender identity (C"
2020.coling-main.350,W19-3807,0,0.218546,"– although unintended – can affect users’ self-esteem (Bourguignon et al., 2015), especially when the linguistic bias is shaped as a perpetuation of stereotypical gender roles and associations (Levesque, 2011). Additionally, as the system does not perform equally well across gender groups, such tools may not be suitable for women, excluding them from benefiting from new technological resources. To date, few attempts have been made towards developing gender-aware translation models, and surprisingly, almost exclusively within the MT community (Vanmassenhove et al., 2018; Elaraby et al., 2018; Moryossef et al., 2019). The only work on gender bias in ST (Bentivogli et al., 2020) proved that direct ST has an advantage when it comes to speaker-dependent gender translation (as in I’ve never been there uttered by a woman), since it can leverage acoustic properties from the audio input (e.g. speaker’s fundamental frequency). However, relying on perceptual markers of speakers’ gender is not the best solution for all kinds of users (e.g. transgenders, children, vocally-impaired people). Moreover, although their conclusions remark that direct ST is nonetheless affected by gender bias, no attempt has yet been made"
2020.coling-main.350,2020.iwslt-1.9,0,0.290027,"on, 2017)). For the sake of simplicity, in our study we use female/male to respectively indicate those speakers whose personal pronouns are she/he. 3954 4.1 Base ST Model We are interested in evaluating and improving gender translation on strong ST models that can be used in real-world contexts. As such, our base, gender-unaware model is trained with the goal of achieving state-of-the-art performance on the ST task. To this aim, we rely on data augmentation and knowledge transfer techniques that were shown to yield competitive models at the IWSLT-2020 evaluation campaign (Ansari et al., 2020; Potapczyk and Przybysz, 2020; Gaido et al., 2020). In particular, we use three data augmentation methods – SpecAugment (Park et al., 2019), time stretch (Nguyen et al., 2020), and synthetic data generation (Jia et al., 2019) – and we transfer knowledge both from ASR and MT through component initialization and knowledge distillation (Hinton et al., 2015). The ST model’s encoder is initialized with the encoder of an English ASR model (Bansal et al., 2019) with a lower number of encoder layers (the missing layers are initialized randomly, as well as the decoder). This ASR model is trained on Librispeech (Panayotov et al., 2"
2020.coling-main.350,2020.acl-main.690,0,0.197261,"inine/masculine gender forms regardless of the perceptual features received from the audio signal, offering a solution for cases where relying on speakers’ vocal characteristics is detrimental to a proper gender translation. 3952 2 Background Besides the abundant work carried out for English monolingual NLP tasks (Sun et al., 2019), a consistent amount of studies have now inspected how MT is affected by the problem of gender bias. Most of them, however, do not focus on speaker-dependent gender agreement. Rather, a number of studies (Stanovsky et al., 2019; Escud´e Font and Costa-juss`a, 2019; Saunders and Byrne, 2020) evaluate whether MT is able to associate prononimal coreference with an occupational noun to produce the correct masculine/feminine forms in the target gender-inflected languages (En: I’ve known her for a long time, my friend is a cook. Es: La conozco desde hace mucho tiempo, mi amiga es cocinera). Notably, few approaches have been employed to make neural MT systems speaker-aware by controlling gender realization in their output. Elaraby et al. (2018) enrich their data with a set of genderagreement rules so to force the system to account for them in the prediction step. In (Vanmassenhove et a"
2020.coling-main.350,2020.acl-main.468,0,0.115074,"ge use is intrinsically social and situated as it varies across groups and even individuals (Bamman et al., 2014). As a result, the language data that are collected to build the corpora on which natural language processing models are trained are often far from being homogeneous and rarely offer a fair representation of different demographic groups and their linguistic behaviours (Bender and Friedman, 2018). Consequently, as predictive models learn from the data distribution they have seen, they tend to favor the demographic group most represented in their training data (Hovy and Spruit, 2016; Shah et al., 2020). This brings serious social consequences as well, since the people who are more likely to be underrepresented within datasets are those whose representation is often less accounted for within our society. A case in point regards the gender data gap.1 In fact, studies on speech taggers (Hovy and Søgaard, 2015) and speech recognition (Tatman, 2017) showed that the underrepresentation of female speakers in the training data leads to significantly lower accuracy in modeling that demographic group. The problem of gender-related differences has also been inspected within automatic translation, both"
2020.coling-main.350,P19-1164,0,0.0595861,"tion quality. Moreover, our best systems learn to produce feminine/masculine gender forms regardless of the perceptual features received from the audio signal, offering a solution for cases where relying on speakers’ vocal characteristics is detrimental to a proper gender translation. 3952 2 Background Besides the abundant work carried out for English monolingual NLP tasks (Sun et al., 2019), a consistent amount of studies have now inspected how MT is affected by the problem of gender bias. Most of them, however, do not focus on speaker-dependent gender agreement. Rather, a number of studies (Stanovsky et al., 2019; Escud´e Font and Costa-juss`a, 2019; Saunders and Byrne, 2020) evaluate whether MT is able to associate prononimal coreference with an occupational noun to produce the correct masculine/feminine forms in the target gender-inflected languages (En: I’ve known her for a long time, my friend is a cook. Es: La conozco desde hace mucho tiempo, mi amiga es cocinera). Notably, few approaches have been employed to make neural MT systems speaker-aware by controlling gender realization in their output. Elaraby et al. (2018) enrich their data with a set of genderagreement rules so to force the system to"
2020.coling-main.350,P19-1159,0,0.129883,"Missing"
2020.coling-main.350,W17-1606,0,0.0231593,"their linguistic behaviours (Bender and Friedman, 2018). Consequently, as predictive models learn from the data distribution they have seen, they tend to favor the demographic group most represented in their training data (Hovy and Spruit, 2016; Shah et al., 2020). This brings serious social consequences as well, since the people who are more likely to be underrepresented within datasets are those whose representation is often less accounted for within our society. A case in point regards the gender data gap.1 In fact, studies on speech taggers (Hovy and Søgaard, 2015) and speech recognition (Tatman, 2017) showed that the underrepresentation of female speakers in the training data leads to significantly lower accuracy in modeling that demographic group. The problem of gender-related differences has also been inspected within automatic translation, both from text (Vanmassenhove et al., 2018) and from audio (Bentivogli et al., 2020). These studies – focused on the translation of spoken language – revealed a systemic gender bias whenever systems are required to overtly and formally express speaker’s gender in the target languages, while translating from languages that do not convey such informatio"
2020.coling-main.350,2016.eamt-2.8,0,0.0451076,"C. These datasets are either gender unbalanced or do not provide speaker’s gender information apart from Librispeech, which is balanced in terms of female/male speakers (Garnerin et al., 2020). However, since these speakers are just book narrators, first-person sentences do not really refer to the speakers themselves. Knowledge distillation (KD) is performed from a teacher MT model by optimizing the cross entropy between the distribution produced by the teacher and by the student ST model being trained (Liu et al., 2019). For both en-it and en-fr, the MT model is trained on the OPUS datasets (Tiedemann, 2016). The ST model is trained in three consecutive steps. In the first step, we use the synthetic data obtained by pairing ASR audio samples with the automatic translations of the corresponding transcripts. In the second step, the model is trained on the ST corpora. In these first two steps, we use the KD loss function. Finally, in the third step, the model is fine-tuned on the same ST corpora using label-smoothed cross entropy (Szegedy et al., 2016). SpecAugment and time stretch are used in all steps. 4.2 Multi-gender Systems The idea of “multi-gender” models, i.e. models informed about the speak"
2020.coling-main.350,D18-1334,0,0.434685,"s serious social consequences as well, since the people who are more likely to be underrepresented within datasets are those whose representation is often less accounted for within our society. A case in point regards the gender data gap.1 In fact, studies on speech taggers (Hovy and Søgaard, 2015) and speech recognition (Tatman, 2017) showed that the underrepresentation of female speakers in the training data leads to significantly lower accuracy in modeling that demographic group. The problem of gender-related differences has also been inspected within automatic translation, both from text (Vanmassenhove et al., 2018) and from audio (Bentivogli et al., 2020). These studies – focused on the translation of spoken language – revealed a systemic gender bias whenever systems are required to overtly and formally express speaker’s gender in the target languages, while translating from languages that do not convey such information. Indeed, languages with grammatical gender, such as French and † The authors contributed equally. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. 1 For a comprehensive overview on such societal issue see (Criado-"
2020.coling-main.350,1991.mtsummit-papers.18,0,0.272103,"Missing"
2021.acl-long.224,D11-1033,0,0.0217744,"Missing"
2021.acl-long.224,2020.iwslt-1.3,0,0.734347,"ith ASR data structures (e.g. ASR n-best, lattices or confusion networks) which are more informative than the 1-best output (Lavie et al., 1996; Matusov et al., 2005; Bertoldi and Federico, 2005; Beck et al., 2019; Sperber et al., 2019), and ii) making the MT robust to ASR errors, for instance by training it on parallel data incorporating real or emulated ASR errors as in (Peitz et al., 2012; Ruiz et al., 2015; Sperber et al., 2017; Cheng et al., 2019; Di Gangi et al., 2019a). Although the former solutions are effective to some extent, state-of-theart cascade architectures (Pham et al., 2019; Bahar et al., 2020) prefer the latter, as they are simpler to implement and maintain. Direct ST. To overcome the limitations of cascade models, B´erard et al. (2016) and Weiss et al. (2017) proposed the first direct solutions bypassing intermediate representations by means of encoderdecoder architectures based on recurrent neural networks. Currently, more effective solutions (Potapczyk and Przybysz, 2020; Bahar et al., 2020; Gaido et al., 2020) rely on ST-oriented adaptations of Transformer (Vaswani et al., 2017) integrating the encoder with: i) convolutional layers to reduce input length, and ii) penalties bias"
2021.acl-long.224,N19-1006,0,0.0282767,"The problem has been mainly tackled with data augmentation and knowledge transfer techniques. Data augmentation consists in producing artificial training corpora by altering existing datasets or by generating (audio, translation) pairs through speech synthesis or MT (Bahar et al., 2019b; Nguyen et al., 2020; Ko et al., 2015; Jia et al., 2019). Knowledge transfer (Gutstein et al., 2008) consists in passing (here to ST) the knowledge learnt by a neural network trained on closely related tasks (here, ASR and MT). Existing ASR models have been used for encoder pre-training (B´erard et al., 2018; Bansal et al., 2019; Bahar et al., 2019a) and multi-task learning (Weiss et al., 2017; Anastasopoulos and Chiang, 2018; Indurthi et al., 2020). Existing neural MT models have been used for decoder pre-training (Bahar et al., 2019a; Inaguma et al., 2020), joint learning (Indurthi et al., 2020; Liu et al., 2020) and knowledge distillation (Liu et al., 2019). Previous comparisons. Most of the works on direct ST also evaluate the proposed solutions against a cascade counterpart. The conclusions, however, are discordant. Looking at recent works, Pino et al. (2019) show similar scores, Indurthi et al. (2020) report hi"
2021.acl-long.224,D19-5304,0,0.0221619,"ffer from well-known problems related to the concatenation of multiple systems. First, they require ad-hoc training and maintenance procedures for the ASR and MT modules; second, they suffer from error propagation and from the loss of speech information (e.g. prosody) that might be useful to improve final translations. Research has focused on mitigating error propagation by: i) feeding the MT system with ASR data structures (e.g. ASR n-best, lattices or confusion networks) which are more informative than the 1-best output (Lavie et al., 1996; Matusov et al., 2005; Bertoldi and Federico, 2005; Beck et al., 2019; Sperber et al., 2019), and ii) making the MT robust to ASR errors, for instance by training it on parallel data incorporating real or emulated ASR errors as in (Peitz et al., 2012; Ruiz et al., 2015; Sperber et al., 2017; Cheng et al., 2019; Di Gangi et al., 2019a). Although the former solutions are effective to some extent, state-of-theart cascade architectures (Pham et al., 2019; Bahar et al., 2020) prefer the latter, as they are simpler to implement and maintain. Direct ST. To overcome the limitations of cascade models, B´erard et al. (2016) and Weiss et al. (2017) proposed the first dire"
2021.acl-long.224,W04-3250,0,0.391428,"ww.cs.umd.edu/˜snover/tercom 6 BLEU8 (Post, 2018) and TER scores computed only on the official MuST-C Common references. C D C es D C it D de HTER 28.65 30.22 29.96 28.19∗ 25.69 26.14 PE Set mTER BLEU 24.41 28.96 25.60 28.46 25.30 34.05∗ 24.02∗ 32.17 23.29 30.04∗ 23.26 28.81 TER 53.23 52.56 50.75 51.08 54.01 54.06 M. Common BLEU TER 28.86 53.93 29.05 52.77∗ 32.93∗ 53.21∗ 31.98 54.00 28.56 56.29 28.56 55.35∗ Table 1: Performance of (C)ascade and (D)irect systems on the PE-sets and MuST-C Common test sets. Statistically significant differences (∗ ) are computed with Paired Bootstrap Resampling (Koehn, 2004). A bird’s-eye view of the results shows that, in more than half of the cases, performance differences between cascade and direct systems are not statistically significant. When they are, the raw count of wins for the two approaches is the same (4), attesting their substantial parity. Looking at our primary metrics (HTER and mTER), systems are on par on en-it and en-de, while for en-es the direct approach significantly outperforms the cascade one. This difference, however, does not emerge with the other metrics. Indeed, BLEU and TER scores computed against the official references are less cohe"
2021.acl-long.224,C96-1075,0,0.130421,"bility across languages and domains. At the same time, however, they suffer from well-known problems related to the concatenation of multiple systems. First, they require ad-hoc training and maintenance procedures for the ASR and MT modules; second, they suffer from error propagation and from the loss of speech information (e.g. prosody) that might be useful to improve final translations. Research has focused on mitigating error propagation by: i) feeding the MT system with ASR data structures (e.g. ASR n-best, lattices or confusion networks) which are more informative than the 1-best output (Lavie et al., 1996; Matusov et al., 2005; Bertoldi and Federico, 2005; Beck et al., 2019; Sperber et al., 2019), and ii) making the MT robust to ASR errors, for instance by training it on parallel data incorporating real or emulated ASR errors as in (Peitz et al., 2012; Ruiz et al., 2015; Sperber et al., 2017; Cheng et al., 2019; Di Gangi et al., 2019a). Although the former solutions are effective to some extent, state-of-theart cascade architectures (Pham et al., 2019; Bahar et al., 2020) prefer the latter, as they are simpler to implement and maintain. Direct ST. To overcome the limitations of cascade models,"
2021.acl-long.224,W18-1818,0,0.0340255,"Missing"
2021.acl-long.224,N19-4009,0,0.01971,"Missing"
2021.acl-long.224,2012.iwslt-papers.18,0,0.0208779,"hey suffer from error propagation and from the loss of speech information (e.g. prosody) that might be useful to improve final translations. Research has focused on mitigating error propagation by: i) feeding the MT system with ASR data structures (e.g. ASR n-best, lattices or confusion networks) which are more informative than the 1-best output (Lavie et al., 1996; Matusov et al., 2005; Bertoldi and Federico, 2005; Beck et al., 2019; Sperber et al., 2019), and ii) making the MT robust to ASR errors, for instance by training it on parallel data incorporating real or emulated ASR errors as in (Peitz et al., 2012; Ruiz et al., 2015; Sperber et al., 2017; Cheng et al., 2019; Di Gangi et al., 2019a). Although the former solutions are effective to some extent, state-of-theart cascade architectures (Pham et al., 2019; Bahar et al., 2020) prefer the latter, as they are simpler to implement and maintain. Direct ST. To overcome the limitations of cascade models, B´erard et al. (2016) and Weiss et al. (2017) proposed the first direct solutions bypassing intermediate representations by means of encoderdecoder architectures based on recurrent neural networks. Currently, more effective solutions (Potapczyk and P"
2021.acl-long.224,2006.amta-papers.25,0,0.267854,", we manually checked the selected samples and kept only those segments for which the audio-transcripttranslation alignment was correct. Each of the three resulting test sets – henceforth PE-sets – is composed of 550 segments, corresponding to about 10,000 English source words. Post-editing. A key element of our multi-faceted analysis is human post-editing (PE), which consists in manually correcting systems’ output according to the input (the source audio in our case). In PEbased evaluation, the original output is compared against its post-edited version using distance-based metrics like TER (Snover et al., 2006). This allows for counting only the true errors made by a system, without penalising differences due to linguistic variation as it happens when exploiting independent references. This makes PE-based evaluation one of the most prominent methodologies used for translation quality assessment (Snover et al., 2006, 2009; Denkowski and Lavie, 2010; Cettolo et al., 2013; Bojar et al., 2015; Graham et al., 2016; Bentivogli et al., 2018b). To collect the post-edits for our study, we strictly followed the methodology of the IWSLT 20132017 evaluation campaigns (Cettolo et al., 2013), which offered us a c"
2021.acl-long.224,W09-0441,0,0.0747003,"Missing"
2021.acl-long.224,P19-1115,0,0.0127146,"n problems related to the concatenation of multiple systems. First, they require ad-hoc training and maintenance procedures for the ASR and MT modules; second, they suffer from error propagation and from the loss of speech information (e.g. prosody) that might be useful to improve final translations. Research has focused on mitigating error propagation by: i) feeding the MT system with ASR data structures (e.g. ASR n-best, lattices or confusion networks) which are more informative than the 1-best output (Lavie et al., 1996; Matusov et al., 2005; Bertoldi and Federico, 2005; Beck et al., 2019; Sperber et al., 2019), and ii) making the MT robust to ASR errors, for instance by training it on parallel data incorporating real or emulated ASR errors as in (Peitz et al., 2012; Ruiz et al., 2015; Sperber et al., 2017; Cheng et al., 2019; Di Gangi et al., 2019a). Although the former solutions are effective to some extent, state-of-theart cascade architectures (Pham et al., 2019; Bahar et al., 2020) prefer the latter, as they are simpler to implement and maintain. Direct ST. To overcome the limitations of cascade models, B´erard et al. (2016) and Weiss et al. (2017) proposed the first direct solutions bypassing"
2021.acl-long.224,W18-6319,0,0.0184861,"based7 metrics: i) human-targeted TER (HTER) computed between the automatic translation and its human post-edited version, and ii) multi-reference TER (mTER) computed against the closest reference among the three available ones (two post-edits and the official reference from MuST-C). The latter metric better accounts for post-editors’ variability, making the evaluation more reliable and informative. For the sake of completeness, in Table 1 we also report Sacre5 www.matecat.com The ad-hoc ST PE guidelines given to translators are included in Appendix B. 7 www.cs.umd.edu/˜snover/tercom 6 BLEU8 (Post, 2018) and TER scores computed only on the official MuST-C Common references. C D C es D C it D de HTER 28.65 30.22 29.96 28.19∗ 25.69 26.14 PE Set mTER BLEU 24.41 28.96 25.60 28.46 25.30 34.05∗ 24.02∗ 32.17 23.29 30.04∗ 23.26 28.81 TER 53.23 52.56 50.75 51.08 54.01 54.06 M. Common BLEU TER 28.86 53.93 29.05 52.77∗ 32.93∗ 53.21∗ 31.98 54.00 28.56 56.29 28.56 55.35∗ Table 1: Performance of (C)ascade and (D)irect systems on the PE-sets and MuST-C Common test sets. Statistically significant differences (∗ ) are computed with Paired Bootstrap Resampling (Koehn, 2004). A bird’s-eye view of the results sh"
2021.acl-long.224,2020.iwslt-1.9,0,0.44219,"tz et al., 2012; Ruiz et al., 2015; Sperber et al., 2017; Cheng et al., 2019; Di Gangi et al., 2019a). Although the former solutions are effective to some extent, state-of-theart cascade architectures (Pham et al., 2019; Bahar et al., 2020) prefer the latter, as they are simpler to implement and maintain. Direct ST. To overcome the limitations of cascade models, B´erard et al. (2016) and Weiss et al. (2017) proposed the first direct solutions bypassing intermediate representations by means of encoderdecoder architectures based on recurrent neural networks. Currently, more effective solutions (Potapczyk and Przybysz, 2020; Bahar et al., 2020; Gaido et al., 2020) rely on ST-oriented adaptations of Transformer (Vaswani et al., 2017) integrating the encoder with: i) convolutional layers to reduce input length, and ii) penalties biasing attention to local context in the encoder self-attention layers (Povey et al., 2018; Sperber et al., 2018; Di Gangi et al., 2019b). Though effective, these architectures have to confront with training data paucity, a critical bottleneck for neural solutions. The problem has been mainly tackled with data augmentation and knowledge transfer techniques. Data augmentation consists in p"
2021.acl-long.224,2020.acl-main.661,0,0.0171621,"ences 5 240 191 45 74 215 234 54 47 231 212 55 52 6.15 6.00 6.68 2.71 5.92 6.28 6.47 3.09 6.03 6.06 6.93 2.96 14.43 14.52 14.31 15.53 12.20 12.09 12.01 13.14 12.31 12.21 11.94 12.68 19.75 18.88 22.07 9.64 19.52 20.39 20.26 10.23 19.41 19.33 21.73 10.33 16.30 44.85 40.74 0 16.09 46.45 40.22 0 14.82 37.65 35.39 0 40.53 17.89 40.18 0 38.76 21.14 40.37 0 36.40 15.80 35.37 0 Table 2: Comparison of (C)ascade and (D)irect performance based on different audio properties. In particular, although suffering from the wellknown scarcity of sizeable training corpora, direct solutions come with the promise (Sperber and Paulik, 2020) of: i) higher robustness to error propagation, and ii) reduced loss of speech information (e.g. prosody). Our next qualitative analysis tries to delve into these aspects by looking at audio understanding and prosody issues. Audio understanding. Errors due to wrong audio understanding are easy to identify for cascade systems – since they are evident in the intermediate ASR transcripts – but harder to spot for direct systems, whose internal representations are by far less accessible. In this case, errors can still be identified in mistranslations corresponding to words which are phonetically si"
2021.acl-long.224,2016.eamt-2.8,0,0.113294,"Missing"
2021.acl-long.224,1991.mtsummit-papers.18,0,0.578059,"ng errors (§6). We finally explore whether, due to latent characteristics overlooked by all previous investigations, the output of cascade and direct systems can be distinguished either by a human or by an automatic classifier (§7). Together with a comparative study attesting the parity of the two paradigms on our test data, another contribution of this paper is the release of the manual post-edits that rendered our investigation possible. The data is available at: https://ict.fbk.eu/mustc-post-edits. 2 Background Cascade ST. By concatenating ASR and MT components (Stentiford and Steer, 1988; Waibel et al., 1991), cascade ST architectures represent an intuitive solution to achieve reasonable performance and high adaptability across languages and domains. At the same time, however, they suffer from well-known problems related to the concatenation of multiple systems. First, they require ad-hoc training and maintenance procedures for the ASR and MT modules; second, they suffer from error propagation and from the loss of speech information (e.g. prosody) that might be useful to improve final translations. Research has focused on mitigating error propagation by: i) feeding the MT system with ASR data stru"
2021.emnlp-main.128,W17-4716,1,0.883528,"Missing"
2021.emnlp-main.128,W19-6603,1,0.877107,"Missing"
2021.emnlp-main.128,P19-1294,0,0.013632,"xisting studies (Ghannay challenges for neural machine translation (NMT) et al., 2018; Caubrière et al., 2020) are all limited models (Sennrich et al., 2016; Koehn and Knowles, to ASR, for which two benchmarks are available 2017). Among rare words, named entities (NEs) (Galibert et al., 2014; Yadav et al., 2020), while and terminology are particularly critical: not only suitable benchmarks do not even exist for ST. The are they important to understand the meaning of situation is similar for terminology: few annotated a sentence (Li et al., 2013), but they are also dif- test sets exist for MT (Dinu et al., 2019; Scansani ficult to handle due to the small number of valid et al., 2019; Bergmanis and Pinnis, 2021), but none translation options. While common words can be for ST, which so far has remained unexplored. rendered in the target language with synonyms or In light of the above, the contribution of this paraphrases, NEs and terminology offer less ex- work is twofold: (1) we present the first investipressive freedom, which is typically limited to one gation on the behavior of state-of-the-art ST sysvalid option. Under these conditions, translation tems in translating NEs and terms, discussing the"
2021.emnlp-main.128,2020.lrec-1.593,0,0.0549842,"Missing"
2021.emnlp-main.128,2020.iwslt-1.8,1,0.842204,"Missing"
2021.emnlp-main.128,galibert-etal-2014-etape,0,0.0540402,"Missing"
2021.emnlp-main.128,N18-2081,0,0.054873,"Missing"
2021.emnlp-main.128,W04-3250,0,0.512284,"Missing"
2021.emnlp-main.128,W17-3204,0,0.025107,"Missing"
2021.emnlp-main.128,P13-1059,0,0.0273085,"anslation of rare words is one of the main regards NEs, the few existing studies (Ghannay challenges for neural machine translation (NMT) et al., 2018; Caubrière et al., 2020) are all limited models (Sennrich et al., 2016; Koehn and Knowles, to ASR, for which two benchmarks are available 2017). Among rare words, named entities (NEs) (Galibert et al., 2014; Yadav et al., 2020), while and terminology are particularly critical: not only suitable benchmarks do not even exist for ST. The are they important to understand the meaning of situation is similar for terminology: few annotated a sentence (Li et al., 2013), but they are also dif- test sets exist for MT (Dinu et al., 2019; Scansani ficult to handle due to the small number of valid et al., 2019; Bergmanis and Pinnis, 2021), but none translation options. While common words can be for ST, which so far has remained unexplored. rendered in the target language with synonyms or In light of the above, the contribution of this paraphrases, NEs and terminology offer less ex- work is twofold: (1) we present the first investipressive freedom, which is typically limited to one gation on the behavior of state-of-the-art ST sysvalid option. Under these conditi"
2021.emnlp-main.128,W18-6319,0,0.0464177,"Missing"
2021.emnlp-main.128,2020.iwslt-1.9,0,0.0354527,"(Wang et al., 2020) ST corpora. ASR outputs are post-processed to add true-casing and punctuation. The MT model is trained on data collected from the OPUS repository,3 amounting to about 19M, 28M, and 45M parallel sentence pairs respectively for en-es, en-fr, and en-it. Our direct model has the same Transformer2 3 http://commonvoice.mozilla.org/en/ http://opus.nlpl.eu based architecture of the ASR component used in the cascade system. It exploits data augmentation and knowledge transfer techniques successfully applied by participants in the IWSLT-2020 evaluation campaign (Ansari et al., 2020; Potapczyk and Przybysz, 2020; Gaido et al., 2020a) and it is trained on MuST-C, Europarl-ST and synthetic data (∼1.5M pairs for each language direction). Systems’ performance is shown in Table 2 and discussed in Section 4. Complete details about their implementation and training procedures are provided in the Appendix. All the related code is available at https://github.com/mgaido91/ FBK-fairseq-ST/tree/emnlp2021. 3 Evaluation Data: NEuRoparl-ST To the best of our knowledge, freely available NE/term-labelled ST benchmarks suitable for our analysis do not exist. The required resource should contain i) the audio correspond"
2021.emnlp-main.128,W95-0107,0,0.10514,"ut, being polysemic, can be technical terms in different contexts (e.g. the word “board” 4 http://docs.deeppavlov.ai/en/master/ features/models/ner.html 5 Note that Dice coefficient has the same value of the F1 measure computed considering either annotator as reference. 6 http://iate.europa.eua 7 Preprocessing made with Spacy: http://spacy.io/ can refer to a tool or to a committee). Checking the presence of the corresponding translation in the target language disambiguates these cases, leading to a more accurate annotation. NE and term annotations were merged into a single test set using BIO (Ramshaw and Marcus, 1995) as span labeling format. Had a word been tagged both as term and NE, the latter was chosen favoring the more reliable manual annotation. Table 1 presents the total number of NEs and terms for the three language pairs, together with their corresponding number of tokens.8 These numbers differ between source and target texts and across pairs due to the peculiarities of the Europarl-ST data. Specifically, i) sometimes translations are not literal and NEs are omitted in the translation (e.g. when a NE is repeated in the source, one of the occurrences may be replaced by a pronoun in the target text"
2021.emnlp-main.128,W19-6608,1,0.879777,"Missing"
2021.emnlp-main.128,P16-1162,0,0.116616,"Missing"
2021.emnlp-main.128,2020.acl-main.661,0,0.0208585,"nology present in an utterance. To this aim, we compare instances of the two main approaches. One is the traditional cascade approach (Stentiford and Steer, 1988; Waibel et al., 1991), which consists of a pipeline where an ASR model produces a transcript of the input audio and an MT model generates its translation. The other is the so-called direct approach (Bérard et al., 2016; Weiss et al., 2017), which relies on a single neural network that maps the audio into target language text bypassing any intermediate symbolic representation. The two approaches have inherent strengths and weaknesses (Sperber and Paulik, 2020). Cascade solutions can exploit sizeable datasets for the ASR and MT subcomponents, but rely on a complex architecture prone to error propagation. Direct models suffer from the paucity of training data, but avoid error propagation and can take advantage of unmediated access to audio information (e.g. prosody) during the translation phase. In recent years, after a long dominance of the cascade paradigm, the initially huge performance gap between the two approaches has gradually closed (Ansari et al., 2020). Our cascade system integrates competitive Transformer-based (Vaswani et al., 2017) ASR a"
2021.emnlp-main.128,1991.mtsummit-papers.18,0,0.514023,"ion for Computational Linguistics riches their textual portions with NE and terminology annotation. Besides being the first benchmark of this type for ST, it can also be used for the evaluation of NE/terminology recognition (ASR) and translation (MT). The dataset is available at: ict.fbk.eu/neuroparl-st/. 2 Speech Translation Models Our goal is to assess the capability of state-of-theart ST systems to properly translate NEs and terminology present in an utterance. To this aim, we compare instances of the two main approaches. One is the traditional cascade approach (Stentiford and Steer, 1988; Waibel et al., 1991), which consists of a pipeline where an ASR model produces a transcript of the input audio and an MT model generates its translation. The other is the so-called direct approach (Bérard et al., 2016; Weiss et al., 2017), which relies on a single neural network that maps the audio into target language text bypassing any intermediate symbolic representation. The two approaches have inherent strengths and weaknesses (Sperber and Paulik, 2020). Cascade solutions can exploit sizeable datasets for the ASR and MT subcomponents, but rely on a complex architecture prone to error propagation. Direct mode"
2021.emnlp-main.128,2020.lrec-1.517,0,0.0203461,"n recent years, after a long dominance of the cascade paradigm, the initially huge performance gap between the two approaches has gradually closed (Ansari et al., 2020). Our cascade system integrates competitive Transformer-based (Vaswani et al., 2017) ASR and MT components built from large training corpora. Specifically, the ASR model is trained on LibriSpeech (Panayotov et al., 2015), TEDLIUM v3 (Hernandez et al., 2018) and Mozilla Common Voice,2 together with (utterance, transcript) pairs extracted from MuST-C (Cattoni et al., 2021), Europarl-ST (Iranzo-Sánchez et al., 2020), and CoVoST 2 (Wang et al., 2020) ST corpora. ASR outputs are post-processed to add true-casing and punctuation. The MT model is trained on data collected from the OPUS repository,3 amounting to about 19M, 28M, and 45M parallel sentence pairs respectively for en-es, en-fr, and en-it. Our direct model has the same Transformer2 3 http://commonvoice.mozilla.org/en/ http://opus.nlpl.eu based architecture of the ASR component used in the cascade system. It exploits data augmentation and knowledge transfer techniques successfully applied by participants in the IWSLT-2020 evaluation campaign (Ansari et al., 2020; Potapczyk and Przyb"
2021.findings-acl.313,P16-2058,0,0.0452662,"Missing"
2021.findings-acl.313,2020.gebnlp-1.3,0,0.0431789,"Missing"
2021.findings-acl.313,W19-6603,1,0.88023,"Missing"
2021.findings-acl.313,W19-4636,0,0.0191748,"utputs has been identified in the training data, which reflect the under-participation of women – e.g. in the media (Madaan et al., 2018), sexist language and gender categories overgeneralization (Devinney et al., 2020). Hence, preventive initiatives concerning data documentation have emerged (Bender and Friedman, 2018), and several mitigating strategies have been proposed by training models on ad-hoc gender-balanced datasets (Saunders and Byrne, 2020; Costa-juss`a and de Jorge, 2020), or by enriching data with additional gender information (Moryossef et al., 2019; Vanmassenhove et al., 2018; Elaraby and Zahran, 2019; Saunders et al., 2020; Stafanoviˇcs et al., 2020). Comparatively, very little work has tried to identify concurring factors to gender bias going beyond data. Among those, Vanmassenhove et al. (2019) ascribes to an algorithmic bias the loss of less frequent feminine forms in both phrase-based and neural MT. Closer to our intent, two recent works pinpoint the impact of models’ components and inner mechanisms. Costa-juss`a et al. (2020b) investigate the role of different architectural de3577 signs in multilingual MT, showing that languagespecific encoder-decoders (Escolano et al., 2019) better"
2021.findings-acl.313,P19-2033,0,0.020981,"Missing"
2021.findings-acl.313,2020.coling-main.350,1,0.720637,"Avg. 21.4 21.8 21.6 21.3 20.7 21.0 21.9 21.7 21.8 21.7 21.4 21.6 22.0 21.5 21.8 Table 2: SacreBLEU scores on MuST-C tst-COMMON (M-C) and MuST-SHE (M-SHE) for en-fr and en-it. 4.2 Evaluation We are interested in measuring both i) the overall translation quality obtained by different segmentation techniques, and ii) the correct generation of gender forms. We evaluate translation quality on both the MuST-C tst-COMMON set (2,574 sentences for en-it and 2,632 for en-fr) and MuST-SHE (§3.2), using SacreBLEU (Post, 2018).10 For fine-grained analysis on gender translation, we rely on gender accuracy (Gaido et al., 2020).11 We differentiate between two categories of phenomena represented in MuST-SHE. Category (1) contains first-person references (e.g. I’m a student) to be translated according to the speakers’ preferred linguistic expression of gender. In this context, ST models can leverage speakers’ vocal characteristics as a gender cue to infer gender translation.12 Gender phenomena of Category (2), instead, shall be translated in concordance with other gender information in the sentence (e.g. she/he is a student). 5 Comparison of Segmentation Methods Table 1: Resulting dictionary sizes. For fair comparison"
2021.findings-acl.313,2020.gebnlp-1.8,0,0.0607853,"Missing"
2021.findings-acl.313,2020.findings-emnlp.180,0,0.0128546,"is indeed an important factor for models’ gender bias. Our experiments consistently show that BPE leads to the highest BLEU scores, while character-based models are the best at translating gender. Preliminary analyses suggests that the isolation of the morphemes encoding gender can be a key factor for gender translation. (3) Finally, we propose a multi-decoder architecture able to combine BPE overall translation quality and the higher ability to translate gender of character-based segmentation. 2 Background Gender bias. Recent years have seen a surge of studies dedicated to gender bias in MT (Gonen and Webster, 2020; Rescigno et al., 2020) and ST (Costa-juss`a et al., 2020a). The primary source of such gender imbalance and adverse outputs has been identified in the training data, which reflect the under-participation of women – e.g. in the media (Madaan et al., 2018), sexist language and gender categories overgeneralization (Devinney et al., 2020). Hence, preventive initiatives concerning data documentation have emerged (Bender and Friedman, 2018), and several mitigating strategies have been proposed by training models on ad-hoc gender-balanced datasets (Saunders and Byrne, 2020; Costa-juss`a and de Jorg"
2021.findings-acl.313,2020.amta-research.13,1,0.729514,"Missing"
2021.findings-acl.313,2020.acl-main.275,0,0.0176651,"chnique proves simple and particularly effective at generalizing over unseen words. On the other hand, the length of the resulting sequences increases the memory footprint, and slows both the training and inference phases. We perform our segmentation by appending “@@ ” to all characters but the last of each word. Statistical Segmentation. This family comprises data-driven algorithms that generate statistically significant subwords units. The most popular one is BPE (Sennrich et al., 2016),6 which proceeds by merging the most frequently co-occurring characters or character sequences. Recently, He et al. (2020) introduced the Dynamic Programming Encoding (DPE) algorithm, which performs 5 Source code available at https://github.com/ mgaido91/FBK-fairseq-ST/tree/acl_2021. 6 We use SentencePiece (Kudo and Richardson, 2018): https://github.com/google/sentencepiece. 3579 competitively and was claimed to accidentally produce more linguistically-plausible subwords with respect to BPE. DPE is obtained by training a mixed character-subword model. As such, the computational cost of a DPE-based ST model is around twice that of a BPE-based one. We trained the DPE segmentation on the transcripts and the target t"
2021.findings-acl.313,2020.acl-main.154,0,0.0152175,"ir Encoding (BPE) (Sennrich et al., 2016) represents the de-facto standard and has recently shown to yield better results compared to character-based segmentation in ST (Di Gangi et al., 2020). But does this hold true for gender translation as well? If not, why? Languages like French and Italian often exhibit comparatively complex feminine forms, derived from the masculine ones by means of an additional suffix (e.g. en: professor, fr: professeur M vs. professeure F). Additionally, women and their referential linguistic expressions of gender are typically under-represented in existing corpora (Hovy et al., 2020). In light of the above, purely statistical segmentation methods could be unfavourable for gender translation, as they can break the morphological structure of words and thus lose relevant linguistic information (Ataman et al., 2017). Indeed, as BPE merges the character sequences that co-occur more frequently, rarer or more complex feminine-marked words may result in less compact sequences of tokens (e.g. en: described, it: des@@critto M vs. des@@crit@@ta F). Due to such typological and distributive conditions, may certain splitting methods render feminine gender less probable and hinder its p"
2021.findings-acl.313,P16-2096,0,0.0303994,"verall translation quality and gender representation: our proposal of a model that combines two segmentation techniques is a step towards this goal. Note that technical mitigation approaches should be integrated with the long-term multidisciplinary commitment (Criado-Perez, 2019; Benjamin, 2019; D’Ignazio and Klein, 2020) necessary to radically address bias in our community. Also, we recognize the limits of working on binary gender, as we further discuss in the ethic section (§8). 1 Introduction The widespread use of language technologies has motivated growing interest on their social impact (Hovy and Spruit, 2016; Blodgett et al., 2020), with gender bias representing a major cause of concern (Costa-juss`a, 2019; Sun et al., 2019). As regards translation tools, focused evaluations have exposed that speech translation (ST) – and machine translation (MT) – models do in fact overproduce masculine references in their outputs (Cho et al., 2019; Bentivogli et al., 2020), except for feminine asso3576 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3576–3589 August 1–6, 2021. ©2021 Association for Computational Linguistics ciations perpetuating traditional gender roles and ste"
2021.findings-acl.313,Q17-1024,0,0.0210177,"Missing"
2021.findings-acl.313,D18-2012,0,0.0156258,"d inference phases. We perform our segmentation by appending “@@ ” to all characters but the last of each word. Statistical Segmentation. This family comprises data-driven algorithms that generate statistically significant subwords units. The most popular one is BPE (Sennrich et al., 2016),6 which proceeds by merging the most frequently co-occurring characters or character sequences. Recently, He et al. (2020) introduced the Dynamic Programming Encoding (DPE) algorithm, which performs 5 Source code available at https://github.com/ mgaido91/FBK-fairseq-ST/tree/acl_2021. 6 We use SentencePiece (Kudo and Richardson, 2018): https://github.com/google/sentencepiece. 3579 competitively and was claimed to accidentally produce more linguistically-plausible subwords with respect to BPE. DPE is obtained by training a mixed character-subword model. As such, the computational cost of a DPE-based ST model is around twice that of a BPE-based one. We trained the DPE segmentation on the transcripts and the target translations of the MuST-C training set, using the same settings of the original paper.7 Morphological Segmentation. A third possibility is linguistically-guided tokenization that follows morpheme boundaries. Among"
2021.findings-acl.313,W19-3807,0,0.0281782,"rimary source of such gender imbalance and adverse outputs has been identified in the training data, which reflect the under-participation of women – e.g. in the media (Madaan et al., 2018), sexist language and gender categories overgeneralization (Devinney et al., 2020). Hence, preventive initiatives concerning data documentation have emerged (Bender and Friedman, 2018), and several mitigating strategies have been proposed by training models on ad-hoc gender-balanced datasets (Saunders and Byrne, 2020; Costa-juss`a and de Jorge, 2020), or by enriching data with additional gender information (Moryossef et al., 2019; Vanmassenhove et al., 2018; Elaraby and Zahran, 2019; Saunders et al., 2020; Stafanoviˇcs et al., 2020). Comparatively, very little work has tried to identify concurring factors to gender bias going beyond data. Among those, Vanmassenhove et al. (2019) ascribes to an algorithmic bias the loss of less frequent feminine forms in both phrase-based and neural MT. Closer to our intent, two recent works pinpoint the impact of models’ components and inner mechanisms. Costa-juss`a et al. (2020b) investigate the role of different architectural de3577 signs in multilingual MT, showing that languagespe"
2021.findings-acl.313,W18-1818,0,0.0449539,"Missing"
2021.findings-acl.313,N19-4009,0,0.0202664,"Missing"
2021.findings-acl.313,J19-3005,0,0.0204504,"Missing"
2021.findings-acl.313,W18-6319,0,0.0144413,"5 24.2 26.9 DPE 29.8 25.3 27.6 25.7 27.7 Morfessor 29.7 LMVR 30.3 26.0 28.2 en-it M-C M-SHE Avg. 21.4 21.8 21.6 21.3 20.7 21.0 21.9 21.7 21.8 21.7 21.4 21.6 22.0 21.5 21.8 Table 2: SacreBLEU scores on MuST-C tst-COMMON (M-C) and MuST-SHE (M-SHE) for en-fr and en-it. 4.2 Evaluation We are interested in measuring both i) the overall translation quality obtained by different segmentation techniques, and ii) the correct generation of gender forms. We evaluate translation quality on both the MuST-C tst-COMMON set (2,574 sentences for en-it and 2,632 for en-fr) and MuST-SHE (§3.2), using SacreBLEU (Post, 2018).10 For fine-grained analysis on gender translation, we rely on gender accuracy (Gaido et al., 2020).11 We differentiate between two categories of phenomena represented in MuST-SHE. Category (1) contains first-person references (e.g. I’m a student) to be translated according to the speakers’ preferred linguistic expression of gender. In this context, ST models can leverage speakers’ vocal characteristics as a gender cue to infer gender translation.12 Gender phenomena of Category (2), instead, shall be translated in concordance with other gender information in the sentence (e.g. she/he is a stu"
2021.findings-acl.313,W17-1601,0,0.043847,"Missing"
2021.findings-acl.313,2020.iwslt-1.9,0,0.0768968,"Missing"
2021.findings-acl.313,2020.amta-impact.4,0,0.0174404,"ctor for models’ gender bias. Our experiments consistently show that BPE leads to the highest BLEU scores, while character-based models are the best at translating gender. Preliminary analyses suggests that the isolation of the morphemes encoding gender can be a key factor for gender translation. (3) Finally, we propose a multi-decoder architecture able to combine BPE overall translation quality and the higher ability to translate gender of character-based segmentation. 2 Background Gender bias. Recent years have seen a surge of studies dedicated to gender bias in MT (Gonen and Webster, 2020; Rescigno et al., 2020) and ST (Costa-juss`a et al., 2020a). The primary source of such gender imbalance and adverse outputs has been identified in the training data, which reflect the under-participation of women – e.g. in the media (Madaan et al., 2018), sexist language and gender categories overgeneralization (Devinney et al., 2020). Hence, preventive initiatives concerning data documentation have emerged (Bender and Friedman, 2018), and several mitigating strategies have been proposed by training models on ad-hoc gender-balanced datasets (Saunders and Byrne, 2020; Costa-juss`a and de Jorge, 2020), or by enrichin"
2021.findings-acl.313,2020.acl-main.690,0,0.0523287,"r feminine asso3576 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3576–3589 August 1–6, 2021. ©2021 Association for Computational Linguistics ciations perpetuating traditional gender roles and stereotypes (Prates et al., 2020; Stanovsky et al., 2019). In this context, most works identified data as the primary source of gender asymmetries. Accordingly, many pointed out the misrepresentation of gender groups in datasets (Garnerin et al., 2019; Vanmassenhove et al., 2018), focusing on the development of data-centred mitigating techniques (Zmigrod et al., 2019; Saunders and Byrne, 2020). Although data are not the only factor contributing to generate bias (Shah et al., 2020; Savoldi et al., 2021), only few inquiries devoted attention to other technical components that exacerbate the problem (Vanmassenhove et al., 2019) or to architectural changes that can contribute to its mitigation (Costa-juss`a et al., 2020b). From an algorithmic perspective, Roberts et al. (2020) additionally expose how “taken-for-granted” approaches may come with high overall translation quality in terms of BLEU scores, but are actually detrimental when it comes to gender bias. Along this line, we focus"
2021.findings-acl.313,2020.gebnlp-1.4,0,0.015649,"in the training data, which reflect the under-participation of women – e.g. in the media (Madaan et al., 2018), sexist language and gender categories overgeneralization (Devinney et al., 2020). Hence, preventive initiatives concerning data documentation have emerged (Bender and Friedman, 2018), and several mitigating strategies have been proposed by training models on ad-hoc gender-balanced datasets (Saunders and Byrne, 2020; Costa-juss`a and de Jorge, 2020), or by enriching data with additional gender information (Moryossef et al., 2019; Vanmassenhove et al., 2018; Elaraby and Zahran, 2019; Saunders et al., 2020; Stafanoviˇcs et al., 2020). Comparatively, very little work has tried to identify concurring factors to gender bias going beyond data. Among those, Vanmassenhove et al. (2019) ascribes to an algorithmic bias the loss of less frequent feminine forms in both phrase-based and neural MT. Closer to our intent, two recent works pinpoint the impact of models’ components and inner mechanisms. Costa-juss`a et al. (2020b) investigate the role of different architectural de3577 signs in multilingual MT, showing that languagespecific encoder-decoders (Escolano et al., 2019) better translate gender than s"
2021.findings-acl.313,E17-2060,0,0.0121917,"taken into account, BPE greedy procedures can be suboptimal. By breaking the surface of words into plausible semantic units, linguistically motivated segmentations (Smit et al., 2014; Ataman et al., 2017) were proven more effective for low-resource and morphologically-rich languages (e.g. agglutinative languages like Turkish), which often have a high level of sparsity in the lexical distribution due to their numerous derivational and inflectional variants. Moreover, fine-grained analyses comparing the grammaticality of character, morpheme and BPE-based models exhibited different capabilities. Sennrich (2017) and Ataman et al. (2019) show the syntactic advantage of BPE in managing several agreement phenomena in German, a language that requires resolving long range dependencies. In contrast, Belinkov et al. (2020) demonstrate that while subword units better capture semantic information, character-level representations perform best at generalizing morphology, thus being more robust in handling unknown and lowfrequency words. Indeed, using different atomic units does affect models’ ability to handle specific linguistic phenomena. However, whether low gender translation accuracy can be to a certain ex"
2021.findings-acl.313,P16-1162,0,0.597551,"., 2021), only few inquiries devoted attention to other technical components that exacerbate the problem (Vanmassenhove et al., 2019) or to architectural changes that can contribute to its mitigation (Costa-juss`a et al., 2020b). From an algorithmic perspective, Roberts et al. (2020) additionally expose how “taken-for-granted” approaches may come with high overall translation quality in terms of BLEU scores, but are actually detrimental when it comes to gender bias. Along this line, we focus on ST systems and inspect a core aspect of neural models: word segmentation. Byte-Pair Encoding (BPE) (Sennrich et al., 2016) represents the de-facto standard and has recently shown to yield better results compared to character-based segmentation in ST (Di Gangi et al., 2020). But does this hold true for gender translation as well? If not, why? Languages like French and Italian often exhibit comparatively complex feminine forms, derived from the masculine ones by means of an additional suffix (e.g. en: professor, fr: professeur M vs. professeure F). Additionally, women and their referential linguistic expressions of gender are typically under-represented in existing corpora (Hovy et al., 2020). In light of the above"
2021.findings-acl.313,2020.acl-main.468,0,0.0187644,"pages 3576–3589 August 1–6, 2021. ©2021 Association for Computational Linguistics ciations perpetuating traditional gender roles and stereotypes (Prates et al., 2020; Stanovsky et al., 2019). In this context, most works identified data as the primary source of gender asymmetries. Accordingly, many pointed out the misrepresentation of gender groups in datasets (Garnerin et al., 2019; Vanmassenhove et al., 2018), focusing on the development of data-centred mitigating techniques (Zmigrod et al., 2019; Saunders and Byrne, 2020). Although data are not the only factor contributing to generate bias (Shah et al., 2020; Savoldi et al., 2021), only few inquiries devoted attention to other technical components that exacerbate the problem (Vanmassenhove et al., 2019) or to architectural changes that can contribute to its mitigation (Costa-juss`a et al., 2020b). From an algorithmic perspective, Roberts et al. (2020) additionally expose how “taken-for-granted” approaches may come with high overall translation quality in terms of BLEU scores, but are actually detrimental when it comes to gender bias. Along this line, we focus on ST systems and inspect a core aspect of neural models: word segmentation. Byte-Pair E"
2021.findings-acl.313,E14-2006,0,0.0640264,"Missing"
2021.findings-acl.313,2020.wmt-1.73,0,0.0348449,"Missing"
2021.findings-acl.313,P19-1164,0,0.0261457,"gender bias representing a major cause of concern (Costa-juss`a, 2019; Sun et al., 2019). As regards translation tools, focused evaluations have exposed that speech translation (ST) – and machine translation (MT) – models do in fact overproduce masculine references in their outputs (Cho et al., 2019; Bentivogli et al., 2020), except for feminine asso3576 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3576–3589 August 1–6, 2021. ©2021 Association for Computational Linguistics ciations perpetuating traditional gender roles and stereotypes (Prates et al., 2020; Stanovsky et al., 2019). In this context, most works identified data as the primary source of gender asymmetries. Accordingly, many pointed out the misrepresentation of gender groups in datasets (Garnerin et al., 2019; Vanmassenhove et al., 2018), focusing on the development of data-centred mitigating techniques (Zmigrod et al., 2019; Saunders and Byrne, 2020). Although data are not the only factor contributing to generate bias (Shah et al., 2020; Savoldi et al., 2021), only few inquiries devoted attention to other technical components that exacerbate the problem (Vanmassenhove et al., 2019) or to architectural chan"
2021.findings-acl.313,P19-1159,0,0.0170259,"a step towards this goal. Note that technical mitigation approaches should be integrated with the long-term multidisciplinary commitment (Criado-Perez, 2019; Benjamin, 2019; D’Ignazio and Klein, 2020) necessary to radically address bias in our community. Also, we recognize the limits of working on binary gender, as we further discuss in the ethic section (§8). 1 Introduction The widespread use of language technologies has motivated growing interest on their social impact (Hovy and Spruit, 2016; Blodgett et al., 2020), with gender bias representing a major cause of concern (Costa-juss`a, 2019; Sun et al., 2019). As regards translation tools, focused evaluations have exposed that speech translation (ST) – and machine translation (MT) – models do in fact overproduce masculine references in their outputs (Cho et al., 2019; Bentivogli et al., 2020), except for feminine asso3576 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3576–3589 August 1–6, 2021. ©2021 Association for Computational Linguistics ciations perpetuating traditional gender roles and stereotypes (Prates et al., 2020; Stanovsky et al., 2019). In this context, most works identified data as the primary sour"
2021.findings-acl.313,D18-1334,0,0.111261,"ls do in fact overproduce masculine references in their outputs (Cho et al., 2019; Bentivogli et al., 2020), except for feminine asso3576 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3576–3589 August 1–6, 2021. ©2021 Association for Computational Linguistics ciations perpetuating traditional gender roles and stereotypes (Prates et al., 2020; Stanovsky et al., 2019). In this context, most works identified data as the primary source of gender asymmetries. Accordingly, many pointed out the misrepresentation of gender groups in datasets (Garnerin et al., 2019; Vanmassenhove et al., 2018), focusing on the development of data-centred mitigating techniques (Zmigrod et al., 2019; Saunders and Byrne, 2020). Although data are not the only factor contributing to generate bias (Shah et al., 2020; Savoldi et al., 2021), only few inquiries devoted attention to other technical components that exacerbate the problem (Vanmassenhove et al., 2019) or to architectural changes that can contribute to its mitigation (Costa-juss`a et al., 2020b). From an algorithmic perspective, Roberts et al. (2020) additionally expose how “taken-for-granted” approaches may come with high overall translation qu"
2021.findings-acl.313,W19-6622,0,0.118919,"otypes (Prates et al., 2020; Stanovsky et al., 2019). In this context, most works identified data as the primary source of gender asymmetries. Accordingly, many pointed out the misrepresentation of gender groups in datasets (Garnerin et al., 2019; Vanmassenhove et al., 2018), focusing on the development of data-centred mitigating techniques (Zmigrod et al., 2019; Saunders and Byrne, 2020). Although data are not the only factor contributing to generate bias (Shah et al., 2020; Savoldi et al., 2021), only few inquiries devoted attention to other technical components that exacerbate the problem (Vanmassenhove et al., 2019) or to architectural changes that can contribute to its mitigation (Costa-juss`a et al., 2020b). From an algorithmic perspective, Roberts et al. (2020) additionally expose how “taken-for-granted” approaches may come with high overall translation quality in terms of BLEU scores, but are actually detrimental when it comes to gender bias. Along this line, we focus on ST systems and inspect a core aspect of neural models: word segmentation. Byte-Pair Encoding (BPE) (Sennrich et al., 2016) represents the de-facto standard and has recently shown to yield better results compared to character-based se"
2021.findings-acl.313,P19-1161,0,0.0281112,"al., 2020), except for feminine asso3576 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3576–3589 August 1–6, 2021. ©2021 Association for Computational Linguistics ciations perpetuating traditional gender roles and stereotypes (Prates et al., 2020; Stanovsky et al., 2019). In this context, most works identified data as the primary source of gender asymmetries. Accordingly, many pointed out the misrepresentation of gender groups in datasets (Garnerin et al., 2019; Vanmassenhove et al., 2018), focusing on the development of data-centred mitigating techniques (Zmigrod et al., 2019; Saunders and Byrne, 2020). Although data are not the only factor contributing to generate bias (Shah et al., 2020; Savoldi et al., 2021), only few inquiries devoted attention to other technical components that exacerbate the problem (Vanmassenhove et al., 2019) or to architectural changes that can contribute to its mitigation (Costa-juss`a et al., 2020b). From an algorithmic perspective, Roberts et al. (2020) additionally expose how “taken-for-granted” approaches may come with high overall translation quality in terms of BLEU scores, but are actually detrimental when it comes to gender bias."
abad-etal-2010-resource,poesio-etal-2008-anawiki,0,\N,Missing
abad-etal-2010-resource,poesio-artstein-2008-anaphoric,0,\N,Missing
abad-etal-2010-resource,J97-1005,0,\N,Missing
abad-etal-2010-resource,W04-0210,0,\N,Missing
abad-etal-2010-resource,hasler-etal-2006-nps,0,\N,Missing
abad-etal-2010-resource,taule-etal-2008-ancora,0,\N,Missing
bentivogli-etal-2010-building,W07-1409,0,\N,Missing
bentivogli-etal-2010-building,W09-2508,0,\N,Missing
bentivogli-etal-2010-building,W09-2507,0,\N,Missing
bentivogli-etal-2010-building,W07-1401,1,\N,Missing
bentivogli-etal-2010-building,P08-1118,0,\N,Missing
bentivogli-etal-2010-building,P07-1058,1,\N,Missing
bentivogli-pianta-2002-opportunistic,mana-corazzari-2002-lexico,0,\N,Missing
bentivogli-pianta-2002-opportunistic,ahrenberg-etal-2000-evaluation,0,\N,Missing
bentivogli-pianta-2002-opportunistic,W00-0801,0,\N,Missing
bentivogli-pianta-2002-opportunistic,W00-0203,0,\N,Missing
bentivogli-pianta-2002-opportunistic,de-yzaguirre-etal-2000-technical,0,\N,Missing
C04-1053,ahrenberg-etal-2000-evaluation,0,0.0670581,"Missing"
C04-1053,bentivogli-pianta-2002-opportunistic,1,0.768299,"per represents our contribution to the research in this field. We present a novel methodology to create a semantically annotated corpus by exploiting information contained in an already annotated corpus, using word alignment as a bridge. The methodology has been applied in the creation of the MultiSemCor corpus. MultiSemCor is an English/Italian parallel corpus which is being created on the basis of the English SemCor corpus and where the texts are aligned at the word level and semantically annotated with a shared inventory of senses. Given the promising results of a pilot study presented in (Bentivogli and Pianta, 2002), the MultiSemCor corpus is now under development. In this paper we focus on a thorough evaluation of the steps involved in the transfer methodology. We evaluate the performance of a new version of the word alignment system and the final quality of the annotations transferred from English to Italian. In Section 2 we lay out the annotation transfer methodology and summarize some related work. In Section 3 we discuss some problematic issues related to the methodology which will be extensively tested and evaluated in Section 4. In Section 5 we report about the state of development of the MultiSem"
C04-1053,P91-1034,0,0.0340013,"with PoS, lemma and word sense, but also an aligned parallel corpus lexically annotated with a shared inventory of word senses. More specifically, the sense inventory used is MultiWordNet (Pianta et al., 2002), a multilingual lexical database in which the Italian component is strictly aligned with the English WordNet. 2.1 Related Work The idea of obtaining linguistic information about a text in one language by exploiting parallel or comparable texts in another language has been explored in the field of Word Sense Disambiguation (WSD) since the early 90’s, the most representative works being (Brown et al., 1991), (Gale et al., 1992), and (Dagan and Itai, 1994). In more recent years, Ide et al. (2002) present a method to identify word meanings starting from a multilingual corpus. A by-product of applying this method is that once a word in one language is word-sense tagged, the translation equivalents in the parallel texts are also automatically annotated. Cross-language tagging is the goal of the work by Diab and Resnik (2002), who present a method for word sense tagging both the source and target texts of parallel bilingual corpora with the WordNet sense inventory. Parallel to the studies regarding t"
C04-1053,J94-4003,0,0.082025,"igned parallel corpus lexically annotated with a shared inventory of word senses. More specifically, the sense inventory used is MultiWordNet (Pianta et al., 2002), a multilingual lexical database in which the Italian component is strictly aligned with the English WordNet. 2.1 Related Work The idea of obtaining linguistic information about a text in one language by exploiting parallel or comparable texts in another language has been explored in the field of Word Sense Disambiguation (WSD) since the early 90’s, the most representative works being (Brown et al., 1991), (Gale et al., 1992), and (Dagan and Itai, 1994). In more recent years, Ide et al. (2002) present a method to identify word meanings starting from a multilingual corpus. A by-product of applying this method is that once a word in one language is word-sense tagged, the translation equivalents in the parallel texts are also automatically annotated. Cross-language tagging is the goal of the work by Diab and Resnik (2002), who present a method for word sense tagging both the source and target texts of parallel bilingual corpora with the WordNet sense inventory. Parallel to the studies regarding the projection of semantic information, more recen"
C04-1053,P02-1033,0,0.0356166,"oiting parallel or comparable texts in another language has been explored in the field of Word Sense Disambiguation (WSD) since the early 90’s, the most representative works being (Brown et al., 1991), (Gale et al., 1992), and (Dagan and Itai, 1994). In more recent years, Ide et al. (2002) present a method to identify word meanings starting from a multilingual corpus. A by-product of applying this method is that once a word in one language is word-sense tagged, the translation equivalents in the parallel texts are also automatically annotated. Cross-language tagging is the goal of the work by Diab and Resnik (2002), who present a method for word sense tagging both the source and target texts of parallel bilingual corpora with the WordNet sense inventory. Parallel to the studies regarding the projection of semantic information, more recently the NLP community has also explored the possibility of exploiting translation to project more syntaxoriented annotations. Yarowsky et al. (2001) describe a successful method consisting of (i) automatic annotation of English texts, (ii) crosslanguage projection of annotations onto target language texts, and (iii) induction of noise-robust taggers for the target langua"
C04-1053,P80-1024,0,0.0772644,"Missing"
C04-1053,W02-0808,0,0.0253939,"h a shared inventory of word senses. More specifically, the sense inventory used is MultiWordNet (Pianta et al., 2002), a multilingual lexical database in which the Italian component is strictly aligned with the English WordNet. 2.1 Related Work The idea of obtaining linguistic information about a text in one language by exploiting parallel or comparable texts in another language has been explored in the field of Word Sense Disambiguation (WSD) since the early 90’s, the most representative works being (Brown et al., 1991), (Gale et al., 1992), and (Dagan and Itai, 1994). In more recent years, Ide et al. (2002) present a method to identify word meanings starting from a multilingual corpus. A by-product of applying this method is that once a word in one language is word-sense tagged, the translation equivalents in the parallel texts are also automatically annotated. Cross-language tagging is the goal of the work by Diab and Resnik (2002), who present a method for word sense tagging both the source and target texts of parallel bilingual corpora with the WordNet sense inventory. Parallel to the studies regarding the projection of semantic information, more recently the NLP community has also explored t"
C04-1053,C04-1156,1,0.830196,"Missing"
C04-1053,C02-1070,0,0.0206203,"inventory of word senses. More specifically, the sense inventory used is MultiWordNet (Pianta et al., 2002), a multilingual lexical database in which the Italian component is strictly aligned with the English WordNet. 2.1 Related Work The idea of obtaining linguistic information about a text in one language by exploiting parallel or comparable texts in another language has been explored in the field of Word Sense Disambiguation (WSD) since the early 90’s, the most representative works being (Brown et al., 1991), (Gale et al., 1992), and (Dagan and Itai, 1994). In more recent years, Ide et al. (2002) present a method to identify word meanings starting from a multilingual corpus. A by-product of applying this method is that once a word in one language is word-sense tagged, the translation equivalents in the parallel texts are also automatically annotated. Cross-language tagging is the goal of the work by Diab and Resnik (2002), who present a method for word sense tagging both the source and target texts of parallel bilingual corpora with the WordNet sense inventory. Parallel to the studies regarding the projection of semantic information, more recently the NLP community has also explored t"
C04-1053,H01-1035,0,0.138175,"Missing"
C04-1053,J03-1002,0,\N,Missing
C04-1156,ahrenberg-etal-2000-evaluation,0,0.0619146,"anguage Processing task that has found various applications in recent years. Word level alignments can be used to build bilingual concordances for human browsing, to feed machine learning-based translation algorithms, or as a basis for sense disambiguation algorithms or for automatic projection of linguistic annotations from one language to another. A number of word alignment algorithms have been presented in the literature, see for instance (Véronis, 2000) and (Melamed, 2001). Shared evaluation procedures have been established, although there are still open issues on some evaluation details (Ahrenberg et al. 2000). Most of the known alignment algorithms are statistics-based and do not exploit external linguistic resources, or use them to a very limited extent. The main attractive of such algorithms is that they are language independent, and only require a parallel corpus of reasonable size to be trained. However, word alignment can be used for different purposes and in different application scenarios; different kinds of alignment strategies produce different kinds of results (for instance in terms of precision/recall) which can be more or less suitable to the goal to be achieved. The requirement of hav"
C04-1156,bentivogli-pianta-2002-opportunistic,1,0.911714,"Missing"
C04-1156,J03-1002,0,0.0163386,"s-based algorithms to achieve their objective, a parallel corpus for the specific domain needs to be available, a requirement that in some cases cannot be met easily. For these reasons, we claim that in some cases algorithms based on external, linguistics resources, if available, can be a useful alternative to statisticsbased algorithms. In the rest of this paper we will compare the results obtained by a statistics-based and a linguistic resource-based algorithm when applied to the EuroCor and MultiSemCor English/Italian corpora. The statistics-based algorithm to be evaluated is described in (Och and Ney, 2003). For its evaluation we used an implementation by the authors themselves, called GIZA++, which is freely available to the scientific community (Och, 2003). The second algorithm to be evaluated is crucially based on a bilingual dictionary and a morphological analyzer. It is called KNOWA (KNowledge intensive Word Aligner) and has been developed at ITC-irst by the authors of this paper. The results of the comparative evaluation show that, given specific application goals, and given the availability of Italian/English resources, KNOWA obtains results that are comparable or better than the results"
C04-1156,P00-1055,0,0.0301475,"t words. The pivot extension of KNOWA has strong similarities with a strategy that is used by various statistics-based algorithms, aiming at selecting at first the translation correspondents that are most probably correct. Once these pivotal correspondences have been established, the remaining alignments are derived using the pivots as fixed points. Given that fact that these algorithms do not exploit bilingual dictionaries, the selection of the pivotal translation correspondent may be based on cognates, or specific frequency configurations. See among others (Simmard and Plamondon, 1998) and (Ribeiro et al., 2000). The results obtained by applying the one-to-one potential correspondence as criterion for selecting pivot words are illustrated further on in Section 5. 1-Sizzling 2-temperatures 3-and 4-hot 5-summer 6-pavements 7-are 8-anything 9-but 10-kind 11-to 12-the 13-feet 1-Il 2-clima 3-torrido 4-e 5-i 6-marciapiedi 7-dell’ 8-estate 9-rovente 10-non 11-sono 12-niente 13-di 14-buono 15-per 16-i 17-piedi Table 3: pivot words involved in one-to-one potential correspondences 1-Sizzling 2-temperatures 3-and 4-hot 5-summer 6-pavements 7-are 8-anything 9-but 10-kind … 1-Il 2-clima 3-torrido 4-e 5-i 6-marcia"
C04-1156,de-yzaguirre-etal-2000-technical,0,0.0667444,"Missing"
C14-2026,aziz-etal-2012-pet,0,0.105813,"Missing"
C14-2026,P11-4010,0,0.0202014,"ly fulfill the requirements illustrated above. Over the years, various annotation tools with different characteristics have been made available for the assessment tasks offered by our toolkit. However, none of them incorporates all the features of MTEQuAl: either the integration in a multi-task platform, or a web-based interface, or the implementation of the error annotation task which is the most needed to support the upcoming research. The most comparable tools to MT-EQuAl are PET (Aziz et al., 2012), COSTA (Chatzitheodorou and Chatzistamatis, 2013), TAUS DQF framework,1 translate5,2 Blast (Stymne, 2011), and Appraise (Federmann, 2012), since they all implement translation error annotation. These tools were created for different purposes and differ in various ways among each other and with respect to MT-EQuAl. All of them except Appraise do not support multiple MT outputs, and PET, COSTA, and Blast are stand-alone tools. From the error analysis point of view, their interfaces show different levels of flexibility. PET and COSTA permit only sentence-level annotation, which is not the suitable granularity for that kind of information. Appraise offers word-level annotation but displays the MT out"
C14-2026,vilar-etal-2006-error,0,0.191191,"Missing"
C14-2026,2012.eamt-1.31,0,\N,Missing
C14-2026,2012.tc-1.5,0,\N,Missing
D11-1062,W10-0733,0,0.0314394,"Missing"
D11-1062,W10-0701,0,0.0128303,"ios of different complexity (monolingual TE, and CLTE between close or distant languages). We believe that, in the same spirit of recent works promoting large-scale annotation efforts around entailment corpora (Sammons et al., 2010; Bentivogli et al., 2010), the proposed approach and the resulting dataset5 will contribute to meeting the strong need for resources to develop and evaluate novel solutions for textual entailment. 2 Related Works Crowdsourcing services, such as Amazon Mechanical Turk6 (MTurk) and CrowdFlower7 , have been recently used with success for a variety of NLP applications (Callison-Burch and Dredze, 2010). The idea is that the acquisition and annotation of large amounts of data needed to train and evaluate NLP tools can be carried out in a cost-effective manner by defining simple Human Intelligence Tasks (HITs) routed to a crowd of non-expert workers (aka “Turkers”) hired through on-line marketplaces. As regards textual entailment, the first work exploring the use of crowdsourcing services for data annotation is described in (Snow et al., 2008), which shows high agreement between non-expert annotations of the RTE-1 dataset and existing gold standard labels assigned by expert labellers. Focusin"
D11-1062,N10-1045,1,0.400414,"ection without sacrificing quality. We show that a complex data creation task, for which even experts usually feature low agreement scores, can be effectively decomposed into simple subtasks assigned to non-expert annotators. The resulting dataset, obtained from a pipeline of different jobs routed to Amazon Mechanical Turk, contains more than 1,600 aligned pairs for each combination of texts-hypotheses in English, Italian and German. 1 Yashar Mehdad FBK-irst and University of Trento Trento, Italy mehdad@fbk.eu Introduction Cross-lingual Textual Entailment (CLTE) has been recently proposed by (Mehdad et al., 2010; Mehdad et al., 2011) as an extension of Textual Entailment (Dagan and Glickman, 2004). The task consists of deciding, given a text (T) and an hypothesis (H) in different languages, if the meaning of H can be inferred from the meaning of T. As in other NLP applications, both for monolingual and cross-lingual TE, 1 http://www.nist.gov/tac/2011/RTE/ http://nlp.uned.es/clef-qa/ave/ 3 http://www.evalita.it/2009/tasks/te 4 For instance, in the first five RTE Challenges, the average effort needed to create 1,000 pairs featuring full agreement among 3 annotators was around 2.5 person-months. Typical"
D11-1062,P11-1134,1,0.520619,"Missing"
D11-1062,P09-2078,0,0.012216,"bs is harder to assess, due to the fact that multiple valid results are acceptable (e.g. the same content can be expressed, translated, or summarized in different ways). In such situations the standard quality control mechanisms are not directly applicable, and the detection of errors requires either costly manual verification at the end of the acquisition process, or more complex and creative solutions integrating HITs for quality check. Most of the approaches to content generation proposed so far rely on post hoc verification to filter out undesired low-quality data (Mrozinski et al., 2008; Mihalcea and Strapparava, 2009; Wang and Callison-Burch, 2010). The few solutions integrating validation HITs address the translation of single sentences, a task that is substantially different from ours (Negri and Mehdad, 2010; Bloodgood and Callison-Burch, 2010). Compared to sentence translation, the task of creating CLTE pairs is both harder to explain without recurring to notions that are difficult to understand to non-experts (e.g. “semantic equivalence”, “unidirectional entailment”), and harder to execute without mastering these notions. To tackle these issues the “divide and conquer” approach described in the next s"
D11-1062,P08-1051,0,0.0228368,"of content generation jobs is harder to assess, due to the fact that multiple valid results are acceptable (e.g. the same content can be expressed, translated, or summarized in different ways). In such situations the standard quality control mechanisms are not directly applicable, and the detection of errors requires either costly manual verification at the end of the acquisition process, or more complex and creative solutions integrating HITs for quality check. Most of the approaches to content generation proposed so far rely on post hoc verification to filter out undesired low-quality data (Mrozinski et al., 2008; Mihalcea and Strapparava, 2009; Wang and Callison-Burch, 2010). The few solutions integrating validation HITs address the translation of single sentences, a task that is substantially different from ours (Negri and Mehdad, 2010; Bloodgood and Callison-Burch, 2010). Compared to sentence translation, the task of creating CLTE pairs is both harder to explain without recurring to notions that are difficult to understand to non-experts (e.g. “semantic equivalence”, “unidirectional entailment”), and harder to execute without mastering these notions. To tackle these issues the “divide and conquer”"
D11-1062,W10-0734,1,0.928539,"ating from scratch aligned CLTE corpora for different language combinations. To this aim, we do not resort to already annotated data, nor languagespecific preprocessing tools. Second, their approach involves qualitative analysis of the collected data only a posteriori, after manual removal of invalid and trivial generated hypotheses. In contrast, our approach integrates quality control mechanisms at all stages of the data collection/annotation process, thus minimizing the recourse to experts to check the quality of the collected material. Related research in the CLTE direction is reported in (Negri and Mehdad, 2010), which describes the creation of an English-Spanish corpus obtained from the RTE-3 dataset by translating the English hypotheses into Spanish. Translations have been crowdsourced adopting a methodology based on translation-validation cycles, defined as separate HITs. Although simplifying the CLTE corpus creation problem, which is recast as the task of translating already available annotated data, this solution is relevant to our work for the idea of combining gold standard units and “validation HITS” as a way to control the quality of the collected data at runtime. 3 Quality Control of Crowds"
D11-1062,P10-1122,0,0.0213975,"ince the core monolingual tasks of the process are carried out by manipulating English texts, we are able to address the very large community of English speaking workers, with a considerable reduction of costs and execution time. Finally, as a by-product of our method, the acquired pairs are fully aligned for all language combinations, thus enabling meaningful comparisons between scenarios of different complexity (monolingual TE, and CLTE between close or distant languages). We believe that, in the same spirit of recent works promoting large-scale annotation efforts around entailment corpora (Sammons et al., 2010; Bentivogli et al., 2010), the proposed approach and the resulting dataset5 will contribute to meeting the strong need for resources to develop and evaluate novel solutions for textual entailment. 2 Related Works Crowdsourcing services, such as Amazon Mechanical Turk6 (MTurk) and CrowdFlower7 , have been recently used with success for a variety of NLP applications (Callison-Burch and Dredze, 2010). The idea is that the acquisition and annotation of large amounts of data needed to train and evaluate NLP tools can be carried out in a cost-effective manner by defining simple Human Intelligence T"
D11-1062,D08-1027,0,0.0603459,"Missing"
D11-1062,W10-0725,0,0.194152,"ounts of data needed to train and evaluate NLP tools can be carried out in a cost-effective manner by defining simple Human Intelligence Tasks (HITs) routed to a crowd of non-expert workers (aka “Turkers”) hired through on-line marketplaces. As regards textual entailment, the first work exploring the use of crowdsourcing services for data annotation is described in (Snow et al., 2008), which shows high agreement between non-expert annotations of the RTE-1 dataset and existing gold standard labels assigned by expert labellers. Focusing on the actual generation of monolingual entailment pairs, (Wang and Callison-Burch, 2010) experiments the use of MTurk to collect facts and counter facts related to texts extracted from an existing RTE corpus annotated with named entities. Taking a step beyond the task of annotating exist5 The CLTE corpora described in this paper will be made freely available for research purposes through the website of the funding EU Project CoSyne (http://www.cosyne.eu/). 6 https://www.mturk.com/ 7 Although MTurk is directly accessible only to US citizens, the CrowdFlower service (http://crowdflower.com/) provides an interface to MTurk for non-US citizens. ing datasets, and showing the feasibili"
D11-1062,bentivogli-etal-2010-building,1,\N,Missing
D14-1172,W05-0909,0,0.0397333,". Our experiments are carried out on different language pairs involving Chinese, Arabic and Russian as target languages. Interesting findings are reported, concerning the impact of different error types both at the level of human perception of quality and with respect to performance results measured with automatic metrics. 1 Introduction The dominant statistical approach to machine translation (MT) is based on learning from large amounts of parallel data and tuning the resulting models on reference-based metrics that can be computed automatically, such as BLEU (Papineni et al., 2001), METEOR (Banerjee and Lavie, 2005), TER (Snover et al., 2006), GTM (Turian et al., 2003). Despite the steady progress in the last two decades, especially for few well resourced translation directions having English as target language, this way to approach the problem is quickly reaching a performance plateau. One reason is that parallel data are a source of reliable information but, alone, limit systems knowledge to observed positive examples (i.e. how a sentence should be translated) without explicitly modelling any notion of error (i.e. how a sentence should not be translated). Another reason is that, as a development and ev"
D14-1172,2011.mtsummit-papers.17,0,0.0134317,"ems, in different target languages and domains, have been used to determine the quality of translations according to the amount of errors encountered (Popovic et al., 2013), to design new automatic metrics that take into consideration human annotations (Popovic, 2012; Bojar et al., 2013), and to train classifiers that can automatic identify fine-grained errors in the MT output (Popovi´c and Ney, 2011). The impact of edit operations on post-editors’ productivity, which implicitly connects the severity of different errors to human activity, has also been studied (Temnikova, 2010; O’Brien, 2011; Blain et al., 2011), but few attempts have been made to explicitly model how fine-grained errors impact on human quality judgements and automatic metrics. Recently, the relation between different error types, their frequency, and human quality judgements has been investigated from a descriptive standpoint in (Lommel et al., 2014; Popovi´c et al., 2014). In both works, however, the underlying assumption that the most frequent error has also the largest impact on quality perception is not verified (in general and, least of all, across language pairs, domains, MT systems and post-editors). Another limitation of the"
D14-1172,2010.eamt-1.12,0,0.183772,"Missing"
D14-1172,C14-2026,1,0.831976,"e, Arabic, and Russian. An international organization provided us a set of English sentences together with their translation produced by two anonymous MT systems. For each evaluation item (source sentence and two MT outputs) three experts were asked to assign quality scores to the MT outputs, and a fourth expert was asked to annotate translation errors. The four experts, who were all professional translators native in the examined target languages, were carefully trained to get acquainted with the evaluation guidelines and the annotation tool specifically developed for these evaluation tasks (Girardi et al., 2014). The annotation process was carried out in parallel by all annotators over one week, resulting in a final dataset composed of 312 evaluation items for the ENZH direction, 393 for ENAR, and 437 for ENRU. 4.1 Quality Judgements Quality judgements were collected by asking the three experts to rate each automatic translation according to a 1-5 Likert scale, where 1 means “incomprehensible translation” and 5 means “perfect translation”. The distribution of the collected annotations with respect to each quality score is shown in Figure 1. As we can see, this distribution reflects different levels o"
D14-1172,N09-1057,0,0.0141677,"total error frequencies and automatic scores (Popovi´c and Ney, 2011; Farr´us et al., 2012). Using two different error taxonomies, both works show that the sum of the errors has a high correlation with BLEU and TER scores. Similar to the aforementioned works addressing the impact of MT errors on human perception, these studies disregard error interactions, and their possible impact on automatic scores. To overcome these issues, we propose a robust statistic analysis framework based on mixedeffects models, which have been successfully applied to several NLP problems such as sentiment analysis (Greene and Resnik, 2009), automatic speech recognition (Goldwater et al., 2010), and spoken language translation (Ruiz and Federico, 2014). Despite their effectiveness, the use of mixed-effects models in the MT field is rather recent and limited to the analysis of human posteditions (Green et al., 2013; L¨aubli et al., 2013). In both studies, the goal was to evaluate the impact of post-editing on the quality and productivity of human translation assuming an ANOVA mixed model for a between-subject design, in which human translators either post-edited or translated the same texts. Our scenario is rather different as we"
D14-1172,2013.mtsummit-wptp.10,0,0.0451853,"Missing"
D14-1172,C04-1072,0,0.0133562,"n the plot. The total number of errors amounts to 16,320 characters for ENZH, 4,926 words for ENAR, and 5,965 words for ENRU. This distribution highlights some differences between languages directions. For example, translations into Arabic and Russian present several morphology errors, while word reordering is the most frequent issue for translations into Chinese. As we will see in §5.1, error frequency does not give a direct indication of their impact on traslation quality judgements. 4.3 Automatic Metrics In our investigation we consider three popular automatic metrics: sentence-level BLEU (Lin and Och, 2004), TER (Snover et al., 2006), and GTM (Turian et al., 2003). We compute all automatic scores by relying on a single reference and by 1647 4000 3500 3000 2500 2000 1500 1000 500 0 LEX MISS MORPH REO ENZH ENAR ENRU Figure 3: Distribution of error types. means of standard packages. In particular, automatic scores on Chinese are computed at the character level. Moreover, as we use metrics as response variables for our regression models, we compute all metrics at the sentence level. The overall mean scores for all systems and languages are reported in Table 2. Differences in systems’ performance can"
D14-1172,2014.eamt-1.38,0,0.54091,"Missing"
D14-1172,2001.mtsummit-papers.68,0,0.0130816,"empirical observations are drawn. Our experiments are carried out on different language pairs involving Chinese, Arabic and Russian as target languages. Interesting findings are reported, concerning the impact of different error types both at the level of human perception of quality and with respect to performance results measured with automatic metrics. 1 Introduction The dominant statistical approach to machine translation (MT) is based on learning from large amounts of parallel data and tuning the resulting models on reference-based metrics that can be computed automatically, such as BLEU (Papineni et al., 2001), METEOR (Banerjee and Lavie, 2005), TER (Snover et al., 2006), GTM (Turian et al., 2003). Despite the steady progress in the last two decades, especially for few well resourced translation directions having English as target language, this way to approach the problem is quickly reaching a performance plateau. One reason is that parallel data are a source of reliable information but, alone, limit systems knowledge to observed positive examples (i.e. how a sentence should be translated) without explicitly modelling any notion of error (i.e. how a sentence should not be translated). Another reas"
D14-1172,J11-4002,0,0.0770266,"Missing"
D14-1172,2013.mtsummit-posters.5,0,0.0997061,"few simpler methods proposed so far. Overall, our study has clear practical implications for MT systems’ development and evaluation. Indeed, the proposed statistical analysis framework represents an ideal instrument to: i) identify translation issues having the highest impact on human perception of quality and ii) choose the most appropriate evaluation metric to measure progress towards their solution. 2 Related Work Error analysis, as a way to identify systems’ weaknesses and define priorities for their improvement, is gaining increasing interest in the MT community (Popovi´c and Ney, 2011; Popovic et al., 2013). Along this direction, the initial efforts to develop error taxonomies covering different levels of granularity (Flanagan, 1994; Vilar et al., 2006; Farr´us Cabeceran et al., 2010; Stymne and Ahrenberg, 2012; Lommel et al., 2014) have been recently complemented by investigations on how to exploit error annotations for diagnostic purposes. Error annotations of sentences produced by different MT systems, in different target languages and domains, have been used to determine the quality of translations according to the amount of errors encountered (Popovic et al., 2013), to design new automatic"
D14-1172,temnikova-2010-cognitive,0,0.0145572,"es produced by different MT systems, in different target languages and domains, have been used to determine the quality of translations according to the amount of errors encountered (Popovic et al., 2013), to design new automatic metrics that take into consideration human annotations (Popovic, 2012; Bojar et al., 2013), and to train classifiers that can automatic identify fine-grained errors in the MT output (Popovi´c and Ney, 2011). The impact of edit operations on post-editors’ productivity, which implicitly connects the severity of different errors to human activity, has also been studied (Temnikova, 2010; O’Brien, 2011; Blain et al., 2011), but few attempts have been made to explicitly model how fine-grained errors impact on human quality judgements and automatic metrics. Recently, the relation between different error types, their frequency, and human quality judgements has been investigated from a descriptive standpoint in (Lommel et al., 2014; Popovi´c et al., 2014). In both works, however, the underlying assumption that the most frequent error has also the largest impact on quality perception is not verified (in general and, least of all, across language pairs, domains, MT systems and post"
D14-1172,2003.mtsummit-papers.51,0,0.431724,"rs involving Chinese, Arabic and Russian as target languages. Interesting findings are reported, concerning the impact of different error types both at the level of human perception of quality and with respect to performance results measured with automatic metrics. 1 Introduction The dominant statistical approach to machine translation (MT) is based on learning from large amounts of parallel data and tuning the resulting models on reference-based metrics that can be computed automatically, such as BLEU (Papineni et al., 2001), METEOR (Banerjee and Lavie, 2005), TER (Snover et al., 2006), GTM (Turian et al., 2003). Despite the steady progress in the last two decades, especially for few well resourced translation directions having English as target language, this way to approach the problem is quickly reaching a performance plateau. One reason is that parallel data are a source of reliable information but, alone, limit systems knowledge to observed positive examples (i.e. how a sentence should be translated) without explicitly modelling any notion of error (i.e. how a sentence should not be translated). Another reason is that, as a development and evaluation criterion, automatic metrics provide a holist"
D14-1172,vilar-etal-2006-error,0,0.851647,"as a development and evaluation criterion, automatic metrics provide a holistic view of systems’ behaviour without identifying the specific issues of a translation. Indeed, the global scores returned by MT evaluation metrics depend on comparisons between translation hypotheses and reference translations, where the causes and the nature of the differences between them are not identified. To cope with these issues and define system improvement priorities, the focus of MT evaluation research is gradually shifting towards profiling systems’ behaviour with respect to various typologies of errors (Vilar et al., 2006; Popovi´c and Ney, 2011; Farr´us et al., 2012, inter alia). This shift has enriched the traditional MT evaluation framework with a new element, that is the actual errors done by a system. Until now, most of the research has focused on the relationship (i.e. the correlation) between two elements of the framework: humans and automatic evaluation metrics. As a new element of the framework, which becomes a sort of “evaluation triangle”, the analysis of error annotations opens interesting research problems related to the relationships between: i) error types and human perception of MT quality and"
D14-1172,W12-3106,0,0.019253,"rror taxonomies covering different levels of granularity (Flanagan, 1994; Vilar et al., 2006; Farr´us Cabeceran et al., 2010; Stymne and Ahrenberg, 2012; Lommel et al., 2014) have been recently complemented by investigations on how to exploit error annotations for diagnostic purposes. Error annotations of sentences produced by different MT systems, in different target languages and domains, have been used to determine the quality of translations according to the amount of errors encountered (Popovic et al., 2013), to design new automatic metrics that take into consideration human annotations (Popovic, 2012; Bojar et al., 2013), and to train classifiers that can automatic identify fine-grained errors in the MT output (Popovi´c and Ney, 2011). The impact of edit operations on post-editors’ productivity, which implicitly connects the severity of different errors to human activity, has also been studied (Temnikova, 2010; O’Brien, 2011; Blain et al., 2011), but few attempts have been made to explicitly model how fine-grained errors impact on human quality judgements and automatic metrics. Recently, the relation between different error types, their frequency, and human quality judgements has been inv"
D14-1172,2014.amta-researchers.20,1,0.775141,"rror taxonomies, both works show that the sum of the errors has a high correlation with BLEU and TER scores. Similar to the aforementioned works addressing the impact of MT errors on human perception, these studies disregard error interactions, and their possible impact on automatic scores. To overcome these issues, we propose a robust statistic analysis framework based on mixedeffects models, which have been successfully applied to several NLP problems such as sentiment analysis (Greene and Resnik, 2009), automatic speech recognition (Goldwater et al., 2010), and spoken language translation (Ruiz and Federico, 2014). Despite their effectiveness, the use of mixed-effects models in the MT field is rather recent and limited to the analysis of human posteditions (Green et al., 2013; L¨aubli et al., 2013). In both studies, the goal was to evaluate the impact of post-editing on the quality and productivity of human translation assuming an ANOVA mixed model for a between-subject design, in which human translators either post-edited or translated the same texts. Our scenario is rather different as we employ mixed models to measure the influence of different MT error types - expressed as continuous fixed effects"
D14-1172,2006.amta-papers.25,0,0.280249,"t on different language pairs involving Chinese, Arabic and Russian as target languages. Interesting findings are reported, concerning the impact of different error types both at the level of human perception of quality and with respect to performance results measured with automatic metrics. 1 Introduction The dominant statistical approach to machine translation (MT) is based on learning from large amounts of parallel data and tuning the resulting models on reference-based metrics that can be computed automatically, such as BLEU (Papineni et al., 2001), METEOR (Banerjee and Lavie, 2005), TER (Snover et al., 2006), GTM (Turian et al., 2003). Despite the steady progress in the last two decades, especially for few well resourced translation directions having English as target language, this way to approach the problem is quickly reaching a performance plateau. One reason is that parallel data are a source of reliable information but, alone, limit systems knowledge to observed positive examples (i.e. how a sentence should be translated) without explicitly modelling any notion of error (i.e. how a sentence should not be translated). Another reason is that, as a development and evaluation criterion, automat"
D14-1172,stymne-ahrenberg-2012-practice,0,0.266291,"al instrument to: i) identify translation issues having the highest impact on human perception of quality and ii) choose the most appropriate evaluation metric to measure progress towards their solution. 2 Related Work Error analysis, as a way to identify systems’ weaknesses and define priorities for their improvement, is gaining increasing interest in the MT community (Popovi´c and Ney, 2011; Popovic et al., 2013). Along this direction, the initial efforts to develop error taxonomies covering different levels of granularity (Flanagan, 1994; Vilar et al., 2006; Farr´us Cabeceran et al., 2010; Stymne and Ahrenberg, 2012; Lommel et al., 2014) have been recently complemented by investigations on how to exploit error annotations for diagnostic purposes. Error annotations of sentences produced by different MT systems, in different target languages and domains, have been used to determine the quality of translations according to the amount of errors encountered (Popovic et al., 2013), to design new automatic metrics that take into consideration human annotations (Popovic, 2012; Bojar et al., 2013), and to train classifiers that can automatic identify fine-grained errors in the MT output (Popovi´c and Ney, 2011)."
D14-1172,P02-1040,0,\N,Missing
D14-1172,W13-2201,0,\N,Missing
D16-1025,W13-2257,1,0.870008,"Missing"
D16-1025,P11-1059,0,0.0237569,"Missing"
D16-1025,2012.eamt-1.60,1,0.116875,"not applicable to NMT, which does not rely on a fixed inventory of translation units extracted from the parallel data. Previous error analyses based on manually postedited translations were presented in (Bojar, 2011; Koponen, 2012; Popovi´c et al., 2013). We are the first to conduct this kind of study on the output of a neural MT system. 3 Experimental Setting We perform a number of analyses on data and results of the IWSLT 2015 MT En-De task, which consists in translating manual transcripts of English TED talks into German. Evaluation data are publicly available through the WIT3 repository (Cettolo et al., 2012).3 3.1 Task Data TED Talks4 are a collection of rather short speeches (max 18 minutes each, roughly equivalent to 2,500 words) covering a wide variety of topics. All talks have captions, which are translated into many languages by volunteers worldwide. Besides representing a popular benchmark for spoken language technology, TED Talks embed interesting research challenges. Translating TED Talks implies dealing with spoken rather than written language, which is hence expected to be structurally less complex, formal and fluent (Ruiz and Federico, 2014). Moreover, as human translations of the talk"
D16-1025,W14-4012,0,0.0937737,"Missing"
D16-1025,D14-1179,0,0.0584623,"Missing"
D16-1025,daems-etal-2014-origin,0,0.0125712,"aspects are better modeled by different MT frameworks. To this end, a detailed and systematic error analysis of NMT vs. PBMT output is required. Translation error analysis, as a way to identify systems’ weaknesses and define priorities for their improvement, has received a fair amount of attention in the MT community. In this work we opt for the automatic detection and classification of translation errors based on manual post-edits of the MT output. We believe this choice provides an optimal trade-off between fully manual error analysis (Farr´us Cabeceran et al., 2010; Popovi´c et al., 2013; Daems et al., 2014; Federico et al., 2014; Neubig et al., 2015), which is very costly and complex, and fully automatic error analysis (Popovi´c and Ney, 2011; Irvine et al., 2013), which is noisy and biased towards one or few arbitrary reference translations. Existing tools for translation error detection are either based on Word Error Rate (WER) and Position-independent word Error Rate (PER) (Popovi´c, 2011) or on output-reference alignment (Zeman et al., 2011). Regarding error classification, Hjerson (Popovi´c, 2011) detects five main types of word-level errors as defined in (Vilar et al., 2006): morphologica"
D16-1025,N13-1073,0,0.0838252,"Missing"
D16-1025,D14-1172,1,0.84939,"modeled by different MT frameworks. To this end, a detailed and systematic error analysis of NMT vs. PBMT output is required. Translation error analysis, as a way to identify systems’ weaknesses and define priorities for their improvement, has received a fair amount of attention in the MT community. In this work we opt for the automatic detection and classification of translation errors based on manual post-edits of the MT output. We believe this choice provides an optimal trade-off between fully manual error analysis (Farr´us Cabeceran et al., 2010; Popovi´c et al., 2013; Daems et al., 2014; Federico et al., 2014; Neubig et al., 2015), which is very costly and complex, and fully automatic error analysis (Popovi´c and Ney, 2011; Irvine et al., 2013), which is noisy and biased towards one or few arbitrary reference translations. Existing tools for translation error detection are either based on Word Error Rate (WER) and Position-independent word Error Rate (PER) (Popovi´c, 2011) or on output-reference alignment (Zeman et al., 2011). Regarding error classification, Hjerson (Popovi´c, 2011) detects five main types of word-level errors as defined in (Vilar et al., 2006): morphological, reordering, missing"
D16-1025,fishel-etal-2012-terra,0,0.038989,"Missing"
D16-1025,1994.amta-1.9,0,0.196451,"s vocabulary and the variety of subject matter in a text. For the first two features we did not find any correlation; on the contrary, we found a moderate Pearson correlation (R=0.7332) between TTR and the mTER gains of NMT over its closest competitor in each talk. This result suggests that NMT is able to cope with lexical diversity better than any other considered approach. 5 Analysis of Translation Errors We now turn to analyze which types of linguistic errors characterize NMT vs. PBMT. In the literature, various error taxonomies covering different levels of granularity have been developed (Flanagan, 1994; Vilar et al., 2006; Farr´us Cabeceran et al., 2010; Stymne and Ahrenberg, 2012; Lommel et al., 2014). We focus on three error categories, namely (i) morphology errors, (ii) lexical errors, and (iii) word order errors. As for lexical errors, a number of existing taxonomies further distinguish among translation errors due to missing words, extra words, or incorrect lexical choice. However, given the proven difficulty of disambiguating between these three subclasses (Popovi´c and Ney, 2011; Fishel et al., 2012), we prefer to rely on a more coarse-grained linguistic error classification where le"
D16-1025,D12-1043,0,0.0167871,"Missing"
D16-1025,2015.iwslt-evaluation.9,0,0.093503,"Missing"
D16-1025,2015.iwslt-evaluation.4,0,0.120054,"rephrasing and reordering is expected than in the translation of written documents. As regards the English-German language pair, the two languages are interesting since, while belonging to the same language family, they have marked differences in levels of inflection, morphological variation, and word order, especially long-range reordering of verbs. 3.2 Evaluation Data Five systems participated in the MT En-De task and were manually evaluated on a representative subset of the official 2015 test set. The Human Evaluation (HE) set includes the first half of each of the 12 test 3 4 System PBSY (Huck and Birch, 2015) HPB (Jehl et al., 2015) SPB (Ha et al., 2015) NMT (Luong & Manning, 2015) Data 175M/ 3.1B 166M/ 854M 117M/ 2.4B 120M/ – Table 1: MT systems’ overview. Data column: size of parallel/monolingual training data for each system in terms of English and German tokens. talks, for a total of 600 sentences and around 10K words. Five professional translators were asked to post-edit the MT output by applying the minimal edits required to transform it into a fluent sentence with the same meaning as the source sentence. Data were prepared so that all translators equally post-edited the five MT outputs, i.e"
D16-1025,Q13-1035,0,0.0418296,"Missing"
D16-1025,P15-1001,0,0.0782404,"tationally costly and resource demanding to compete with state-of-the-art Phrase-Based MT (PBMT)1 , the situation changed in 2015. For the first time, in the latest edition of IWSLT2 (Cettolo et 1 We use the generic term phrase-based MT to cover standard phrase-based, hierarchical and syntax-based SMT approaches. 2 International Workshop on Spoken Language Translation (http://workshop2015.iwslt.org/) This impressive improvement follows the distance reduction previously observed in the WMT 2015 shared translation task (Bojar et al., 2015). Just few months earlier, the NMT systems described in (Jean et al., 2015b) ranked on par with the best phrase-based models on a couple of language pairs. Such rapid progress stems from the improvement of the recurrent neural network encoderdecoder model, originally proposed in (Sutskever et al., 2014; Cho et al., 2014b), with the use of the attention mechanism (Bahdanau et al., 2015). This evolution has several implications. On one side, NMT represents a simplification with respect to previous paradigms. From a management point of view, similar to PBMT, it allows for a more efficient use of human and data resources with respect to rulebased MT. From the architectu"
D16-1025,W15-3014,0,0.0509369,"tationally costly and resource demanding to compete with state-of-the-art Phrase-Based MT (PBMT)1 , the situation changed in 2015. For the first time, in the latest edition of IWSLT2 (Cettolo et 1 We use the generic term phrase-based MT to cover standard phrase-based, hierarchical and syntax-based SMT approaches. 2 International Workshop on Spoken Language Translation (http://workshop2015.iwslt.org/) This impressive improvement follows the distance reduction previously observed in the WMT 2015 shared translation task (Bojar et al., 2015). Just few months earlier, the NMT systems described in (Jean et al., 2015b) ranked on par with the best phrase-based models on a couple of language pairs. Such rapid progress stems from the improvement of the recurrent neural network encoderdecoder model, originally proposed in (Sutskever et al., 2014; Cho et al., 2014b), with the use of the attention mechanism (Bahdanau et al., 2015). This evolution has several implications. On one side, NMT represents a simplification with respect to previous paradigms. From a management point of view, similar to PBMT, it allows for a more efficient use of human and data resources with respect to rulebased MT. From the architectu"
D16-1025,2015.iwslt-evaluation.6,0,0.129399,"s expected than in the translation of written documents. As regards the English-German language pair, the two languages are interesting since, while belonging to the same language family, they have marked differences in levels of inflection, morphological variation, and word order, especially long-range reordering of verbs. 3.2 Evaluation Data Five systems participated in the MT En-De task and were manually evaluated on a representative subset of the official 2015 test set. The Human Evaluation (HE) set includes the first half of each of the 12 test 3 4 System PBSY (Huck and Birch, 2015) HPB (Jehl et al., 2015) SPB (Ha et al., 2015) NMT (Luong & Manning, 2015) Data 175M/ 3.1B 166M/ 854M 117M/ 2.4B 120M/ – Table 1: MT systems’ overview. Data column: size of parallel/monolingual training data for each system in terms of English and German tokens. talks, for a total of 600 sentences and around 10K words. Five professional translators were asked to post-edit the MT output by applying the minimal edits required to transform it into a fluent sentence with the same meaning as the source sentence. Data were prepared so that all translators equally post-edited the five MT outputs, i.e. 120 sentences for each"
D16-1025,W12-3123,0,0.0131782,"(see also Section 3.4). Irvine et al. (2013) propose another word-level error analysis technique specifically focused on lexical choice and aimed at understanding the effects of domain differences on MT. Their error classification is strictly related to model coverage and insensitive to word order differences. The technique requires access to the system’s phrase table and is thus not applicable to NMT, which does not rely on a fixed inventory of translation units extracted from the parallel data. Previous error analyses based on manually postedited translations were presented in (Bojar, 2011; Koponen, 2012; Popovi´c et al., 2013). We are the first to conduct this kind of study on the output of a neural MT system. 3 Experimental Setting We perform a number of analyses on data and results of the IWSLT 2015 MT En-De task, which consists in translating manual transcripts of English TED talks into German. Evaluation data are publicly available through the WIT3 repository (Cettolo et al., 2012).3 3.1 Task Data TED Talks4 are a collection of rather short speeches (max 18 minutes each, roughly equivalent to 2,500 words) covering a wide variety of topics. All talks have captions, which are translated in"
D16-1025,2014.eamt-1.38,0,0.0817661,"Missing"
D16-1025,2015.iwslt-evaluation.11,0,0.0570707,"e. 120 sentences for each evaluated system. The resulting evaluation data consist of five new reference translations for each of the sentences in the HE set. Each one of these references represents the targeted translation of the system output from which it was derived, but the other four additional translations can also be used to evaluate each MT system. We will see in the next sections how we exploited the available post-edits in the more suitable way depending on the kind of analysis carried out. 3.3 MT Systems Our analysis focuses on the first four top-ranking systems, which include NMT (Luong and Manning, 2015) and three different phrase-based approaches: standard phrase-based (Ha et al., 2015), hierarchical (Jehl et al., 2015) and a combination of phrasebased and syntax-based (Huck and Birch, 2015). Table 1 presents an overview of each system, as well as figures about the training data used.5 The phrase+syntax-based (PBSY) system combines the outputs of a string-to-tree decoder, trained with the GHKM algorithm, with those of two stan5 wit3.fbk.eu http://www.ted.com/ Approach Combination: Phrase+Syntax-based GHKM string-to-tree; hierarchical + sparse lexicalized reordering models Hierarchical Phrase"
D16-1025,D15-1166,0,0.088644,"nvestigate how MT systems’ quality varies with specific characteristics of the input, i.e. sentence length and type of content of each talk (Section 4). Then, we focus on differences among MT systems with respect to morphology, lexical, and word order errors (Section 5). Finally, based on the finding that word reordering is the strongest aspect of NMT compared to the other systems, we carry out a finegrained analysis of word order errors (Section 6). 2 Previous Work To date, NMT systems have only been evaluated by BLEU in single-reference setups (Bahdanau et al., 2015; Sutskever et al., 2014; Luong et al., 2015; Jean et al., 2015a; G¨ulc¸ehre et al., 2015). Ad258 ditionally, the Montreal NMT system submitted to WMT 2015 (Jean et al., 2015b) was part of a manual evaluation experiment where a large number of non-professional annotators were asked to rank the outputs of multiple MT systems (Bojar et al., 2015). Results for the Montreal system were very positive – ranked first in English-German, third in GermanEnglish, English-Czech and Czech-English – which confirmed and strengthened the BLEU results published so far. Unfortunately neither BLEU nor manual ranking judgements tell us which translation as"
D16-1025,W15-5003,0,0.016732,"frameworks. To this end, a detailed and systematic error analysis of NMT vs. PBMT output is required. Translation error analysis, as a way to identify systems’ weaknesses and define priorities for their improvement, has received a fair amount of attention in the MT community. In this work we opt for the automatic detection and classification of translation errors based on manual post-edits of the MT output. We believe this choice provides an optimal trade-off between fully manual error analysis (Farr´us Cabeceran et al., 2010; Popovi´c et al., 2013; Daems et al., 2014; Federico et al., 2014; Neubig et al., 2015), which is very costly and complex, and fully automatic error analysis (Popovi´c and Ney, 2011; Irvine et al., 2013), which is noisy and biased towards one or few arbitrary reference translations. Existing tools for translation error detection are either based on Word Error Rate (WER) and Position-independent word Error Rate (PER) (Popovi´c, 2011) or on output-reference alignment (Zeman et al., 2011). Regarding error classification, Hjerson (Popovi´c, 2011) detects five main types of word-level errors as defined in (Vilar et al., 2006): morphological, reordering, missing words, extra words, an"
D16-1025,J11-4002,0,0.200619,"Missing"
D16-1025,2013.mtsummit-posters.5,0,0.0498368,"Missing"
D16-1025,W14-4009,0,0.0473704,"Missing"
D16-1025,2014.eamt-1.39,1,0.718619,"publicly available through the WIT3 repository (Cettolo et al., 2012).3 3.1 Task Data TED Talks4 are a collection of rather short speeches (max 18 minutes each, roughly equivalent to 2,500 words) covering a wide variety of topics. All talks have captions, which are translated into many languages by volunteers worldwide. Besides representing a popular benchmark for spoken language technology, TED Talks embed interesting research challenges. Translating TED Talks implies dealing with spoken rather than written language, which is hence expected to be structurally less complex, formal and fluent (Ruiz and Federico, 2014). Moreover, as human translations of the talks are required to follow the structure and rhythm of the English captions, a lower amount of rephrasing and reordering is expected than in the translation of written documents. As regards the English-German language pair, the two languages are interesting since, while belonging to the same language family, they have marked differences in levels of inflection, morphological variation, and word order, especially long-range reordering of verbs. 3.2 Evaluation Data Five systems participated in the MT En-De task and were manually evaluated on a represent"
D16-1025,stymne-ahrenberg-2012-practice,0,0.0488173,"rst two features we did not find any correlation; on the contrary, we found a moderate Pearson correlation (R=0.7332) between TTR and the mTER gains of NMT over its closest competitor in each talk. This result suggests that NMT is able to cope with lexical diversity better than any other considered approach. 5 Analysis of Translation Errors We now turn to analyze which types of linguistic errors characterize NMT vs. PBMT. In the literature, various error taxonomies covering different levels of granularity have been developed (Flanagan, 1994; Vilar et al., 2006; Farr´us Cabeceran et al., 2010; Stymne and Ahrenberg, 2012; Lommel et al., 2014). We focus on three error categories, namely (i) morphology errors, (ii) lexical errors, and (iii) word order errors. As for lexical errors, a number of existing taxonomies further distinguish among translation errors due to missing words, extra words, or incorrect lexical choice. However, given the proven difficulty of disambiguating between these three subclasses (Popovi´c and Ney, 2011; Fishel et al., 2012), we prefer to rely on a more coarse-grained linguistic error classification where lexical errors include all of them (Farr´us Cabeceran et al., 2010). 6 The type-to"
D16-1025,vilar-etal-2006-error,0,0.431417,"Missing"
D16-1025,2010.iwslt-evaluation.11,0,\N,Missing
D16-1025,W15-3001,0,\N,Missing
D16-1025,R13-1079,0,\N,Missing
D16-1025,2015.iwslt-evaluation.1,1,\N,Missing
D19-1140,balahur-etal-2014-resource,1,0.907896,"Missing"
D19-1140,N19-1423,0,0.028239,"Missing"
D19-1140,W17-3204,0,0.0163088,"fferings provides a typical example of this situation: a variety of affordable high-performance NLP tools can be easily accessed via APIs but often they are available only for a few languages. Translating into one of these high-resource languages gives the possibility to address the downstream task by: i) using existing tools for that language to process the translated text, and ii) projecting their output back to the original language. However, using MT “as is” might not be optimal for different reasons. First, despite the qualitative leap brought by neural networks, MT is still not perfect (Koehn and Knowles, 2017). Second, previous literature shows that MT can alter some of the properties of the source text (Mirkin et al., 2015; Rabinovich et al., 2017; Vanmassenhove et al., 2018). Finally, even in the case of a perfect MT able to preserve all the traits of the source sentence, models are still trained on parallel data, which are created by humans and thus reflect quality criteria relevant for humans. In this work, we posit that these criteria might not be the optimal ones for a machine (i.e. a downstream NLP tool fed with MT output). In this scenario, MT should pursue the objective of preserving and e"
D19-1140,N18-3012,0,0.0133005,"impossible to exhaustively compute the expected reward, which is thus estimated by samˆ ∼ pθ (.|x(s) ) (2) pθ (ˆ y|x(s) )∆(ˆ y), y s=1 Background and Methodology During training, NMT systems based on the encoder-decoder framework (Sutskever et al., 2014; Bahdanau et al., 2015) are optimized with maximum likelihood estimation (MLE), which aims to maximize the log-likelihood of the training data. In doing so, they indirectly model the human-oriented quality criteria (adequacy and fluency) expressed in the training corpus. A different strand of research (Ranzato et al., 2016; Shen et al., 2016; Kreutzer et al., 2018) focuses on optimizing the model parameters by maximizing an objective function that leverages either an evaluation metric like BLEU (Papineni et al., 2002) or an external human feedback. These methods are based on Reinforcement Learning (RL), in which the MT system parameters θ define a policy that chooses an action, i.e. generating the next word in ˆ , and gets a reward ∆(ˆ a translation candidate y y) according to that action. Given S training sentences {x(s) }Ss=1 , the RL training goal is to maximize the expected reward: S X 4: 5: 6: 7: 8: Input: x(s) s-th source sentence in training data"
D19-1140,P17-1138,0,0.0982028,"Missing"
D19-1140,D15-1130,0,0.0233668,"ccessed via APIs but often they are available only for a few languages. Translating into one of these high-resource languages gives the possibility to address the downstream task by: i) using existing tools for that language to process the translated text, and ii) projecting their output back to the original language. However, using MT “as is” might not be optimal for different reasons. First, despite the qualitative leap brought by neural networks, MT is still not perfect (Koehn and Knowles, 2017). Second, previous literature shows that MT can alter some of the properties of the source text (Mirkin et al., 2015; Rabinovich et al., 2017; Vanmassenhove et al., 2018). Finally, even in the case of a perfect MT able to preserve all the traits of the source sentence, models are still trained on parallel data, which are created by humans and thus reflect quality criteria relevant for humans. In this work, we posit that these criteria might not be the optimal ones for a machine (i.e. a downstream NLP tool fed with MT output). In this scenario, MT should pursue the objective of preserving and emphasizing those properties of the source text that are crucial for the downstream task at hand, even at the expense"
D19-1140,S13-2052,0,0.0283728,"increases the probability to sample a “useful” one, possibly by diverting from the initial model behaviour. On the other side, selecting the candidate with the highest reward will push the system towards translations emphasizing input traits that are relevant for the downstream task at hand. In the sentiment classification use case, these are expected to be sentiment-bearing terms that help the classifier to predict the correct class. • German and Italian classifiers on the original, untranslated tweets (Original). 3 Task-specific data. We experiment with a dataset based on Semeval 2013 data (Nakov et al., 2013), which contains polarity-labeled parallel German/Italian–English corpora (Balahur et al., 2014). For each language pair, the development and test sets respectively comprise 583 (197 negative and 386 positive) and 2,173 tweets (601 negative and 1,572 positive). To cope with the skewed data distribution, the negative tweets in the development sets are over-sampled, leading to new balanced sets of 772 tweets. NMT Systems. Our Generic models are based on Transformer (Vaswani et al., 2017), with parameters similar to those used in the original paper. Training data amount to 6.1M (De-En) and 4.56M"
D19-1140,D17-1153,0,0.119131,"Missing"
D19-1140,P02-1040,0,0.105066,"dology During training, NMT systems based on the encoder-decoder framework (Sutskever et al., 2014; Bahdanau et al., 2015) are optimized with maximum likelihood estimation (MLE), which aims to maximize the log-likelihood of the training data. In doing so, they indirectly model the human-oriented quality criteria (adequacy and fluency) expressed in the training corpus. A different strand of research (Ranzato et al., 2016; Shen et al., 2016; Kreutzer et al., 2018) focuses on optimizing the model parameters by maximizing an objective function that leverages either an evaluation metric like BLEU (Papineni et al., 2002) or an external human feedback. These methods are based on Reinforcement Learning (RL), in which the MT system parameters θ define a policy that chooses an action, i.e. generating the next word in ˆ , and gets a reward ∆(ˆ a translation candidate y y) according to that action. Given S training sentences {x(s) }Ss=1 , the RL training goal is to maximize the expected reward: S X 4: 5: 6: 7: 8: Input: x(s) s-th source sentence in training data, l(s) the ground-truth label, K number of sampled candidates ˆ (s) Output: sampled candidate y C=∅ . Candidates set for k = 1,...,K do c ∼ pθ (.|x(s) ) f ="
D19-1140,E17-1101,0,0.0194126,"often they are available only for a few languages. Translating into one of these high-resource languages gives the possibility to address the downstream task by: i) using existing tools for that language to process the translated text, and ii) projecting their output back to the original language. However, using MT “as is” might not be optimal for different reasons. First, despite the qualitative leap brought by neural networks, MT is still not perfect (Koehn and Knowles, 2017). Second, previous literature shows that MT can alter some of the properties of the source text (Mirkin et al., 2015; Rabinovich et al., 2017; Vanmassenhove et al., 2018). Finally, even in the case of a perfect MT able to preserve all the traits of the source sentence, models are still trained on parallel data, which are created by humans and thus reflect quality criteria relevant for humans. In this work, we posit that these criteria might not be the optimal ones for a machine (i.e. a downstream NLP tool fed with MT output). In this scenario, MT should pursue the objective of preserving and emphasizing those properties of the source text that are crucial for the downstream task at hand, even at the expense of human-quality standar"
D19-1140,N15-1078,0,0.0245338,"sification task, in which Twitter data in German and Italian are to be classified according to their polarity by means of an English classifier. In this setting, a shortcoming of previous translation-based approaches (Denecke, 2008; Balahur et al., 2014) is that, similar to other traits, 1368 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1368–1374, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics sentiment is often not preserved by MT (Salameh et al., 2015; Mohammad et al., 2016; Lohar et al., 2017). Although it represents a viable solution to leverage sentiment analysis to a wide number of languages (Araujo et al., 2016), the translationbased approach should hence be supported by advanced technology able to preserve the sentiment traits of the input. Along this direction, our experiments show that machine-oriented MT optimization makes the classifier’s task easier and eventually results in significant classification improvements. Our results outperform those obtained with translations produced by general-purpose NMT models as well as by an NMT"
D19-1140,P16-1162,0,0.0161613,"e) and 2,173 tweets (601 negative and 1,572 positive). To cope with the skewed data distribution, the negative tweets in the development sets are over-sampled, leading to new balanced sets of 772 tweets. NMT Systems. Our Generic models are based on Transformer (Vaswani et al., 2017), with parameters similar to those used in the original paper. Training data amount to 6.1M (De-En) and 4.56M (It-En) parallel sentences from freely-available corpora. The statistics of the parallel corpora are reported in Table 1. For each language pair, all data are merged and tokenized. Then, byte pair encoding (Sennrich et al., 2016) is applied to obtain 32K sub-word units. Experiments Our evaluation is done by feeding an English sentiment classifier with the translations of German and Italian tweets generated by: • A general-purpose NMT system (Generic); • The same system conditioned with REINFORCE (Reinforce); • The same system conditioned with our Machine-Oriented method (MO-Reinforce). As other terms of comparison, we calculate the results of: De-En It-En Europarl JRC Wikipedia ECB TED KDE News11 News 2M 0.7M 2.5M 0.1M 0.1M 0.3M 0.2M 0.2M 2M 0.8M 1M 0.2M 0.2M 0.3M 0.04M 0.02M Total 6.1M 4.56M Table 1: Statistics of th"
D19-1140,P16-1159,0,0.378266,"e, models are still trained on parallel data, which are created by humans and thus reflect quality criteria relevant for humans. In this work, we posit that these criteria might not be the optimal ones for a machine (i.e. a downstream NLP tool fed with MT output). In this scenario, MT should pursue the objective of preserving and emphasizing those properties of the source text that are crucial for the downstream task at hand, even at the expense of human-quality standards. To this end, inspired by previous – human-oriented – MT approaches based on Reinforcement Learning (Ranzato et al., 2016; Shen et al., 2016) and Bandit Learning (Kreutzer et al., 2017; Nguyen et al., 2017), we explore a NMT optimization strategy that exploits the weak feedback from the downstream task to influence system’s behaviour towards the generation of optimal “machine-oriented” output. As a proof of concept, we test our approach on a sentiment classification task, in which Twitter data in German and Italian are to be classified according to their polarity by means of an English classifier. In this setting, a shortcoming of previous translation-based approaches (Denecke, 2008; Balahur et al., 2014) is that, similar to other"
D19-1140,D18-1334,0,0.0119267,"only for a few languages. Translating into one of these high-resource languages gives the possibility to address the downstream task by: i) using existing tools for that language to process the translated text, and ii) projecting their output back to the original language. However, using MT “as is” might not be optimal for different reasons. First, despite the qualitative leap brought by neural networks, MT is still not perfect (Koehn and Knowles, 2017). Second, previous literature shows that MT can alter some of the properties of the source text (Mirkin et al., 2015; Rabinovich et al., 2017; Vanmassenhove et al., 2018). Finally, even in the case of a perfect MT able to preserve all the traits of the source sentence, models are still trained on parallel data, which are created by humans and thus reflect quality criteria relevant for humans. In this work, we posit that these criteria might not be the optimal ones for a machine (i.e. a downstream NLP tool fed with MT output). In this scenario, MT should pursue the objective of preserving and emphasizing those properties of the source text that are crucial for the downstream task at hand, even at the expense of human-quality standards. To this end, inspired by"
D19-1140,D18-1397,0,0.0327418,"of obtaining less fluent and adequate output. However, as will be shown in Section 4, this 1369 type of reward induces highly polarized translations that are best suited for our downstream task. • The English classifier on the gold standard English tweets (English); Sampling Approach. A possible sampling strategy is to exploit beam search (Sutskever et al., 2014) to find, at each decoding step, the candidate with the highest probability. Another solution is to use multinomial sampling (Ranzato et al., 2016) which, at each decoding step, samples tokens over the model’s output distribution. In (Wu et al., 2018), the higher results achieved by multinomial sampling are ascribed to its capability to better explore the probability space by generating more diverse candidates. This finding is particularly relevant in the proposed “MT for machines” scenario, in which the emphasis on final performance in the downstream task admits radical (application-oriented) changes in the behaviour of the MT model, even at the expense of human quality standards. To increase the possibility of such changes, we propose a new sampling strategy. Instead of generating only one candidate token via multinomial sampling, K cand"
federico-etal-2012-iwslt,niessen-etal-2000-evaluation,0,\N,Missing
federico-etal-2012-iwslt,N04-4038,0,\N,Missing
federico-etal-2012-iwslt,P02-1040,0,\N,Missing
federico-etal-2012-iwslt,W07-0734,0,\N,Missing
federico-etal-2012-iwslt,2005.mtsummit-papers.11,0,\N,Missing
federico-etal-2012-iwslt,O07-5005,0,\N,Missing
federico-etal-2012-iwslt,2011.iwslt-evaluation.10,0,\N,Missing
federico-etal-2012-iwslt,I05-3027,0,\N,Missing
federico-etal-2012-iwslt,2011.iwslt-evaluation.1,1,\N,Missing
federico-etal-2012-iwslt,2010.iwslt-evaluation.5,1,\N,Missing
federico-etal-2012-iwslt,2010.iwslt-evaluation.1,1,\N,Missing
L16-1562,J90-2002,0,0.264106,"Missing"
L16-1562,P02-1033,0,0.10194,"Missing"
L16-1562,N13-1073,0,0.251905,"rucial aspect for WA and – more generally – machine translation. WAGS is publicly released under a Creative Commons Attribution license (CC BY 4.0) and is available at: http://hlt-mt.fbk.eu/technologies/wags In addition to the gold standard data, the release includes the annotation guidelines and an evaluation package that allows to compute Alignment Error Rate (AER) on customizable subsets of WAGS links, for example those aligning only OOV words. In the following, we describe the characteristics of WAGS 3535 and provide results from relevant WA state-of-the-art technology, namely fast align (Dyer et al., 2013) – a variant of IBM model 2 – and IBM model 4 as implemented in mgiza++ (Gao and Vogel, 2008). 2. Dataset Description To create WAGS, we used the publicly available Europarl parallel corpus1 (Koehn, 2005), which contains the proceedings of the European Parliament in the various official languages. A Common Test set, made of the texts from the 4th quarter of year 2000, was defined to be used for machine translation evaluation (Koehn et al., 2003). Table 1 shows some statistics about the Italian and English portions of Europarl v7 release. WAGS is a selection from the Common Test set, realized a"
L16-1562,W14-0313,1,0.899664,"ligned. Regarding this latter issue, two main approaches were followed in previous works: full text alignment, where all words in the text are manually aligned, and sample word alignment, where a set of test words are selected and only those words are manually aligned (V´eronis and Langlais, 2000; Merkel, 1999; Ahrenberg et al., 2002). One of the most challenging issues for current state-ofthe-art word aligners is that they show poor generalization capability and are prone to errors when infrequent or unknown words (with respect to the training data) occur in new sentence pairs to be aligned (Farajian et al., 2014). Thus, WA research would highly benefit from gold standard data specifically tailored to assess WA systems on this issue. However, to our knowledge, none of the available WA benchmarks specifically focuses on the problem of out-ofvocabulary (OOV) and rare words. The main contribution of our work is to provide the research community with WAGS (Word Alignment Gold Standard), a novel benchmark which allows extensive evaluation of WA tools on OOV and rare words. WAGS is a subset of the Common Test section of the Europarl English-Italian parallel corpus (Koehn et al., 2003; Koehn, 2005), and is sp"
L16-1562,J07-3002,0,0.0845348,"Missing"
L16-1562,W08-0509,0,0.188606,"under a Creative Commons Attribution license (CC BY 4.0) and is available at: http://hlt-mt.fbk.eu/technologies/wags In addition to the gold standard data, the release includes the annotation guidelines and an evaluation package that allows to compute Alignment Error Rate (AER) on customizable subsets of WAGS links, for example those aligning only OOV words. In the following, we describe the characteristics of WAGS 3535 and provide results from relevant WA state-of-the-art technology, namely fast align (Dyer et al., 2013) – a variant of IBM model 2 – and IBM model 4 as implemented in mgiza++ (Gao and Vogel, 2008). 2. Dataset Description To create WAGS, we used the publicly available Europarl parallel corpus1 (Koehn, 2005), which contains the proceedings of the European Parliament in the various official languages. A Common Test set, made of the texts from the 4th quarter of year 2000, was defined to be used for machine translation evaluation (Koehn et al., 2003). Table 1 shows some statistics about the Italian and English portions of Europarl v7 release. WAGS is a selection from the Common Test set, realized as described below. Training Common Test #seg 1,908,966 42,753 #Ita tokens 54,848,640 1,224,17"
L16-1562,C14-2026,1,0.822664,"adopted in other available word alignment benchmarks to cope with alignment ambiguity, namely all links in disagreement were included in the final reference alignment as P-links. In Example 4, both annotators linked “orfana” (“orphan” in English) and “deprived”, but one labeled it as S-link while the other as P-link. Since the adjudicator did not prefer one solution to the other, that link was labeled as Possible in the gold standard. Example 4: • Ita: ...questa Unione e` stata un po’ orfana. • Eng: ...the Union was somewhat deprived. Annotations were accomplished using the MT-EQuAl toolkit5 (Girardi et al., 2014). In addition to the traditional matrix-based alignment, MT-EQuAl allows a more user friendly text-based alignment procedure, where mouse clicks on words are used directly to establish alignment links. This alignment method was particularly useful in our task, since annotators were presented with long sentences and only few words were to be annotated. A screenshot of the alignment interface is presented in Figure 3, which shows the alignment of Example 1 above. In the figure, the P-link between “you” and “domanderete” has already been created: the words are underlined in the text (light blue)"
L16-1562,graca-etal-2008-building,0,0.0494955,"Missing"
L16-1562,W11-4615,0,0.0489247,"Missing"
L16-1562,N03-1017,0,0.0140844,"ds. In the following, we describe the characteristics of WAGS 3535 and provide results from relevant WA state-of-the-art technology, namely fast align (Dyer et al., 2013) – a variant of IBM model 2 – and IBM model 4 as implemented in mgiza++ (Gao and Vogel, 2008). 2. Dataset Description To create WAGS, we used the publicly available Europarl parallel corpus1 (Koehn, 2005), which contains the proceedings of the European Parliament in the various official languages. A Common Test set, made of the texts from the 4th quarter of year 2000, was defined to be used for machine translation evaluation (Koehn et al., 2003). Table 1 shows some statistics about the Italian and English portions of Europarl v7 release. WAGS is a selection from the Common Test set, realized as described below. Training Common Test #seg 1,908,966 42,753 #Ita tokens 54,848,640 1,224,178 #Eng tokens 55,141,541 1,266, 968 Table 1: Europarl v7 statistics: number of segments and Italian/English tokens in Training and Common Test sets. 2.1. Data selection The length of segments in the Europarl Common Test set ranges from one (single word segments) to more than two hundreds. It is a matter of fact that the automatic WA of either too short o"
L16-1562,2005.mtsummit-papers.11,0,0.0472688,"addition to the gold standard data, the release includes the annotation guidelines and an evaluation package that allows to compute Alignment Error Rate (AER) on customizable subsets of WAGS links, for example those aligning only OOV words. In the following, we describe the characteristics of WAGS 3535 and provide results from relevant WA state-of-the-art technology, namely fast align (Dyer et al., 2013) – a variant of IBM model 2 – and IBM model 4 as implemented in mgiza++ (Gao and Vogel, 2008). 2. Dataset Description To create WAGS, we used the publicly available Europarl parallel corpus1 (Koehn, 2005), which contains the proceedings of the European Parliament in the various official languages. A Common Test set, made of the texts from the 4th quarter of year 2000, was defined to be used for machine translation evaluation (Koehn et al., 2003). Table 1 shows some statistics about the Italian and English portions of Europarl v7 release. WAGS is a selection from the Common Test set, realized as described below. Training Common Test #seg 1,908,966 42,753 #Ita tokens 54,848,640 1,224,178 #Eng tokens 55,141,541 1,266, 968 Table 1: Europarl v7 statistics: number of segments and Italian/English tok"
L16-1562,kruijff-korbayova-etal-2006-annotation,0,0.0671595,"Missing"
L16-1562,P04-1060,0,0.0831318,"Missing"
L16-1562,macken-2010-annotation,0,0.0586513,"Missing"
L16-1562,W05-0809,0,0.0461213,"l Machine Translation (Och and Ney, 2004; Fraser and Marcu, 2007), but also other applications rely on WA, such as extraction of bilingual lexica (Smadja et al., 1996), word sense disambiguation (Diab and Resnik, 2002), projection of linguistic information between languages (Yarowsky and Ngai, 2001; Kuhn, 2004; Bentivogli and Pianta, 2005). WA gold standards represent a crucial resource to evaluate and analyse WA systems’ performance, and nowadays various benchmarks for different language pairs are available (Melamed, 1998; Och and Ney, 2000; Mihalcea and Pedersen, 2003; Lambert et al., 2005; Martin et al., 2005; Kruijff-Korbayov´a et al., 2006; Grac¸a et al., 2008; Macken, 2010; Holmqvist and Ahrenberg, 2011). Besides the languages addressed, existing benchmarks differ in various respects – also depending on the final application to be evaluated – such as the parallel data used, the annotation scheme adopted (and related guidelines), the selection of words to be manually aligned. Regarding this latter issue, two main approaches were followed in previous works: full text alignment, where all words in the text are manually aligned, and sample word alignment, where a set of test words are selected and"
L16-1562,W03-0301,0,0.129144,"Missing"
L16-1562,P00-1056,0,0.752067,"tence pair (Brown et al., 1990). WA is a basic component of Statistical Machine Translation (Och and Ney, 2004; Fraser and Marcu, 2007), but also other applications rely on WA, such as extraction of bilingual lexica (Smadja et al., 1996), word sense disambiguation (Diab and Resnik, 2002), projection of linguistic information between languages (Yarowsky and Ngai, 2001; Kuhn, 2004; Bentivogli and Pianta, 2005). WA gold standards represent a crucial resource to evaluate and analyse WA systems’ performance, and nowadays various benchmarks for different language pairs are available (Melamed, 1998; Och and Ney, 2000; Mihalcea and Pedersen, 2003; Lambert et al., 2005; Martin et al., 2005; Kruijff-Korbayov´a et al., 2006; Grac¸a et al., 2008; Macken, 2010; Holmqvist and Ahrenberg, 2011). Besides the languages addressed, existing benchmarks differ in various respects – also depending on the final application to be evaluated – such as the parallel data used, the annotation scheme adopted (and related guidelines), the selection of words to be manually aligned. Regarding this latter issue, two main approaches were followed in previous works: full text alignment, where all words in the text are manually aligned"
L16-1562,J04-4002,0,0.0713423,"Missing"
L16-1562,J96-1001,0,0.58811,"Missing"
L16-1562,steinberger-etal-2006-jrc,0,0.0981094,"Missing"
L16-1562,N01-1026,0,0.22907,"Missing"
L16-1562,ahrenberg-etal-2002-system,0,\N,Missing
marelli-etal-2014-sick,D11-1129,0,\N,Missing
marelli-etal-2014-sick,D10-1115,1,\N,Missing
marelli-etal-2014-sick,D12-1110,0,\N,Missing
marelli-etal-2014-sick,D08-1027,0,\N,Missing
marelli-etal-2014-sick,W07-1431,0,\N,Missing
marelli-etal-2014-sick,P08-1028,0,\N,Missing
marelli-etal-2014-sick,S12-1051,0,\N,Missing
marelli-etal-2014-sick,S14-2001,1,\N,Missing
marelli-etal-2014-sick,S13-2005,1,\N,Missing
marelli-etal-2014-sick,S12-1053,1,\N,Missing
N19-1202,N19-1006,0,0.601701,"Missing"
N19-1202,C10-2010,0,0.0464972,"Missing"
N19-1202,N18-1008,0,0.232838,"Missing"
N19-1202,L18-1001,0,0.161824,"large size, speaker variety (male/female, native/non-native) and coverage in terms of topics and languages. To achieve these objectives, similar to (Niehues et al., 2018), we started from English TED Talks, in which a variety of speakers discuss topics spanning from business to science and entertainment. Most importantly, the fact that TED talks are often manually transcribed and translated sets ideal conditions for creating an SLT corpus from high-quality text material. Although the initial data are similar to those used to build the IWSLT18 corpus, our methodology is different. Inspired by Kocabiyikoglu et al. (2018), it exploits automatic alignment procedures, first at the text level (between transcriptions and translations) and then with the corresponding audio segments. More in detail, for each target language Li , the (English-Li ) section of MuST-C is created as follows. First, for all the English talks available from the TED website,3 we download the videos and the HTML files containing the manual transcriptions and their translation into Li .4 Then, the plain text transcription and the translation of each talk are split at the sentence level based on strong punctuation marks and aligned using the G"
N19-1202,D15-1166,0,0.0171243,"ture releases of the corpus. 2013 4 Tgt De Es Fr It Nl Pt Ro Ru #Talk 2,093 2,564 2,510 2,374 2,267 2,050 2,216 2,498 #Sent 234K 270K 280K 258K 253K 211K 240K 270K Hours 408 504 492 465 442 385 432 489 src w 4.3M 5.3M 5.2M 4.9M 4.7M 4.0M 4.6M 5.1M tgt w 4.0M 5.1M 5.4M 4.6M 4.3M 3.8M 4.3M 4.3M tional layers that reduce the sequence length. The output of the convolutions is then processed by three stacked LSTMs (Hochreiter and Schmidhuber, 1997). The decoder consists of a two-layered deep transition (Pascanu et al., 2014) LSTM with an attention network based on the general soft attention score (Luong et al., 2015). The final output of the decoder is a function of the concatenation of the LSTM output, the context vector and the previous-character embedding. Table 2: Statistics for each section of MuST-C. aligned audio using the XNMT tool (Neubig et al., 2018).7 Table 2 provides basic statistics for the 8 sections of the MuST-C corpus. Comparing the 4th column with the numbers reported in Table 1, it is worth noting that, in terms of hours of transcribed/translated speech, each section is larger than any existing publicly available SLT resource. 3 Experiments In this section we present two sets of experi"
N19-1202,W18-1818,0,0.0651267,"Missing"
N19-1202,P02-1040,0,0.107942,"on Metrics In our experiments, texts are tokenized and punctuation is normalized. Furthermore, the English texts are lowercased, while the target language texts are split into characters still preserving the word boundaries. For MT, we segment the English words with the BPE algorithm (Sennrich et al., 2015) using a maximum of 30K merge operations. The output generation of all models is performed using beam search with a beam size of 5. ASR performance is measured with word error rate (WER) computed on lower-cased, tokenized texts without punctuation. MT and SLT results are computed with BLEU (Papineni et al., 2002). 3.3 Experiment 1: Corpus Quality As observed in Section 2, each section of MuSTC is larger than any other existing publicly available SLT corpus. The usefulness of a resource, however, is not only a matter of size but also of quality (in this case, the quality of the audio-transcription-translation alignments). For an empirical verification of this aspect, we experimented with two comparable datasets. One is 8 github.com/neulab/xnmt 2014 www.modernmt.eu the TED-derived English-German IWSLT18 corpus (Niehues et al., 2018), which is built following a pipeline that performs segment extraction a"
N19-1202,2013.iwslt-papers.14,0,0.532837,"Missing"
N19-1202,shimizu-etal-2014-collection,0,0.0820902,"Missing"
N19-1202,stuker-etal-2012-kit,0,0.127078,"Missing"
N19-1202,W17-4608,0,0.0512685,"Missing"
negri-etal-2012-chinese,P02-1040,0,\N,Missing
negri-etal-2012-chinese,D11-1062,1,\N,Missing
negri-etal-2012-chinese,P01-1008,0,\N,Missing
negri-etal-2012-chinese,P08-1077,0,\N,Missing
negri-etal-2012-chinese,P11-1134,1,\N,Missing
negri-etal-2012-chinese,P08-1004,0,\N,Missing
negri-etal-2012-chinese,N06-1058,0,\N,Missing
negri-etal-2012-chinese,P05-1074,0,\N,Missing
negri-etal-2012-chinese,N06-1003,0,\N,Missing
negri-etal-2012-chinese,N10-1045,1,\N,Missing
negri-etal-2012-chinese,P11-1020,0,\N,Missing
negri-etal-2012-chinese,P02-1006,0,\N,Missing
negri-etal-2012-chinese,W10-0734,1,\N,Missing
negri-etal-2012-chinese,W04-3206,0,\N,Missing
S12-1053,W05-0909,0,0.0301332,"Missing"
S12-1053,S12-1065,0,0.0529572,"Missing"
S12-1053,S12-1102,0,0.0711613,"glish pairs. Separate lexical mapping scores are calculated (from T1 to T2 and vice-versa) considering different types of information and similarity metrics. Binary entailment de7 http://www.microsofttranslator.com/ 405 cisions are then heuristically combined into single decisions. Sagan [pivoting, multi-class] (Castillo and Cardenas, 2012) adopts a pivoting method using Google Translate, and trains a monolingual system based on a SVM multi-class classifier. A CLTE corpus derived from the RTE-3 dataset is also used as a source of additional training material. SoftCard [pivoting, multi-class] (Jimenez et al., 2012) after automatic translation with Google Translate, uses SVMs to learn entailment decisions based on information about the cardinality of: T1, T2, their intersection and their union. Cardinalities are computed in different ways, considering tokens in T1 and T2, their IDF, and their similarity (computed with edit-distance) UAlacant [pivoting, multi-class] (Espl`a-Gomis et al., 2012) exploits translations obtained from Google Translate, Microsoft Bing translator, and the Apertium open-source MT platform (Forcada et al., 2011).8 Then, a multi-class SVM classifier is used to take entailment decisi"
S12-1053,P10-4008,1,0.237359,"classification using SVMs. HDU [hybrid, compositional] (W¨aschle and Fendrich, 2012) uses a combination of binary classifiers for each entailment direction. The classifiers use both monolingual alignment features based on METEOR (Banerjee and Lavie, 2005) alignments (translations obtained from Google Translate), and cross-lingual alignment features based on GIZA++ (Och and Ney, 2000) (word alignments learned on Europarl). ICT [pivoting, compositional] (Meng et al., 2012) adopts a pivoting method (using Google Translate and an in-house hierarchical MT system), and the open source EDITS system (Kouylekov and Negri, 2010) to calculate similarity scores between monolingual English pairs. Separate unidirectional entailment judgments obtained from binary classifier are combined to return one of the four valid CLTE judgments. 3 http://translate.google.com/ http://extensions.services.openoffice. org/en/taxonomy/term/233 4 404 5 6 http://www.freedict.com/ http://www.wordreference.com/ SP-EN System name BUAP spa-eng run2 celi spa-eng run2 DirRelCond3 spa-eng run4 FBK spa-eng run3 HDU spa-eng run2 ICT spa-eng run1 JU-CSE-NLP spa-eng run1 Sagan spa-eng run3 SoftCard spa-eng run1 UAlacant spa-eng run1 LATE AVG. P 0,337"
S12-1053,S12-1104,0,0.0962891,"ignment tools, part-of-speech taggers, NP chunkers, named entity recognizers, stemmers, stopwords lists, and Wikipedia as an external multilingual corpus. More in detail: BUAP [pivoting, compositional] (Vilari˜no et al., 2012) adopts a pivoting method based on translating T1 into the language of T2 and vice versa (Google Translate3 and the OpenOffice Thesaurus4 ). Similarity measures (e.g. Jaccard index) and rules are respectively used to annotate the two resulting sentence pairs with entailment judgments and combine them in a single decision. CELI [cross lingual, compositional & multiclass] (Kouylekov, 2012) uses dictionaries for word matching, and a multilingual corpus extracted from Wikipedia for term weighting. Word overlap and similarity measures are then used in different approaches to the task. In one run (Run 1), they are used to train a classifier that assigns separate entailment judgments for each direction. Such judgments are finally composed into a single one for each pair. In the other runs, the same features are used for multi-class classification. DirRelCond3 [cross lingual, compositional] (Perini, 2012) uses bilingual dictionaries (Freedict5 and WordReference6 ) to translate conten"
S12-1053,N10-1045,1,0.464787,"erence over texts written in different languages, targeting at the same time a real application scenario. Participants were presented with datasets for different language pairs, where multi-directional entailment relations (“forward”, “backward”, “bidirectional”, “no entailment”) had to be identified. We report on the training and test data used for evaluation, the process of their creation, the participating systems (10 teams, 92 runs), the approaches adopted and the results achieved. 1 Yashar Mehdad FBK-irst Trento, Italy mehdad@fbk.eu Introduction The cross-lingual textual entailment task (Mehdad et al., 2010) addresses textual entailment (TE) recognition (Dagan and Glickman, 2004) under the new dimension of cross-linguality, and within the new challenging application scenario of content synchronization. Cross-linguality represents a dimension of the TE recognition problem that has been so far only partially investigated. The great potential for integrating monolingual TE recognition components into NLP architectures has been reported in several areas, including question answering, information retrieval, information extraction, and document summarization. However, mainly due to the absence of cross"
S12-1053,P11-1134,1,0.480581,"Missing"
S12-1053,S12-1105,1,0.885932,"Missing"
S12-1053,P12-2024,1,0.450645,"Missing"
S12-1053,W12-3122,1,0.873098,"Missing"
S12-1053,S12-1108,0,0.0711563,"Missing"
S12-1053,D11-1062,1,0.47997,"is task, both T1 and T2 are assumed to be true statements. Although contradiction is relevant from an application-oriented perspective, contradictory pairs are not present in the dataset created for the first round of the task. 3 Dataset description Four CLTE corpora have been created for the following language combinations: Spanish/English (SP-EN), Italian/English (IT-EN), French/English (FR-EN), German/English (DE-EN). The datasets are released in the XML format shown in Figure 1. 3.1 Data collection and annotation The dataset was created following the crowdsourcing methodology proposed in (Negri et al., 2011), which consists of the following steps: 1. First, English sentences were manually extracted from copyright-free sources (Wikipedia and Wikinews). The selected sentences represent one of the elements (T1) of each entailment pair; 2. Next, each T1 was modified through crowdsourcing in various ways in order to obtain a corresponding T2 (e.g. introducing meaning-preserving lexical and syntactic changes, adding and removing portions of text); 3. Each T2 was then paired to the original T1, and the resulting pairs were annotated with one of the four entailment judgments. In order to reduce the corre"
S12-1053,S12-1103,0,0.0191844,"1 0,076 0,201 0,568 0,332 F1 0,235 0,397 0,469 0,644 0,521 0,379 0,247 0,625 0,440 No entailment P R F1 0,344 0,688 0,459 0,339 0,312 0,325 0,367 0,320 0,342 0,540 0,488 0,513 0,390 0,512 0,443 0,315 0,560 0,403 0,405 0,600 0,484 0,521 0,488 0,504 0,403 0,496 0,434 Bidirectional P R F1 0,364 0,288 0,321 0,319 0,288 0,303 0,298 0,312 0,305 0,524 0,520 0,522 0,439 0,552 0,489 0,233 0,080 0,119 0,443 0,344 0,387 0,496 0,504 0,500 0,390 0,361 0,368 Table 4: precision, recall and F1 scores, calculated for each team’s best run for all the language combinations. JU-CSE-NLP [pivoting, compositional] (Neogi et al., 2012) uses Microsoft Bing translator7 to produce monolingual English pairs. Separate lexical mapping scores are calculated (from T1 to T2 and vice-versa) considering different types of information and similarity metrics. Binary entailment de7 http://www.microsofttranslator.com/ 405 cisions are then heuristically combined into single decisions. Sagan [pivoting, multi-class] (Castillo and Cardenas, 2012) adopts a pivoting method using Google Translate, and trains a monolingual system based on a SVM multi-class classifier. A CLTE corpus derived from the RTE-3 dataset is also used as a source of additi"
S12-1053,P00-1056,0,0.151501,"Missing"
S12-1053,S12-1107,0,0.0689288,"Missing"
S12-1053,S12-1106,0,0.0276864,"Missing"
S12-1053,S12-1064,0,0.250345,"Missing"
S13-2005,S13-2024,0,0.0219644,"Missing"
S13-2005,S13-2006,0,0.030069,"ta-classifier. Such classifier combines binary decisions (“YES”/“NO”) taken separately for each of the four CLTE judgements. ECNUCS [pivoting, multi-class] (Jiang and Man, 2013) uses Google Translate to obtain the English translation of each T1. After a pre-processing step aimed at maximizing the commonalities between the two sentences (e.g. abbreviation replacement), a number of features is extracted to train a multi-class SVM classifier. Such features consider information about sentence length, text similarity/difference measures, and syntactic information. SoftCard [pivoting, multi-class] (Jimenez et al., 2013) after automatic translation with Google Translate, uses SVMs to learn entailment decisions based on information about the cardinality of: T1, T2, their intersection and their union. Cardinalities are computed in different ways, considering tokens in T1 and T2, their IDF, and their similarity. Umelb [cross-lingual, pivoting, compositional] (Graham et al., 2013) adopts both pivoting and cross-lingual approaches. For the latter, GIZA++ was used to compute word alignments between the input sentences. Word alignment features are used to train binary SVM classifiers whose decisions are eventually c"
S13-2005,P10-4008,1,0.522467,"using Google Translate5 ). Similarity measures (e.g. Jaccard index) and features based on n-gram overlap, computed at the level of words and part of speech categories, are used (either alone or in combination) by different classification strategies including: multi-class, a meta-classifier (i.e. combining the output of 2/3/4-class classifiers), and majority voting. CELI [cross-lingual, meta-classifier] (Kouylekov, 2013) uses dictionaries for word matching, and a multilingual corpus extracted from Wikipedia for term weighting. A variety of distance measures implemented in the RTE system EDITS (Kouylekov and Negri, 2010; Negri et al., 2009) are used to extract features to train a meta-classifier. Such classifier combines binary decisions (“YES”/“NO”) taken separately for each of the four CLTE judgements. ECNUCS [pivoting, multi-class] (Jiang and Man, 2013) uses Google Translate to obtain the English translation of each T1. After a pre-processing step aimed at maximizing the commonalities between the two sentences (e.g. abbreviation replacement), a number of features is extracted to train a multi-class SVM classifier. Such features consider information about sentence length, text similarity/difference measure"
S13-2005,S13-2099,0,0.0105521,"ned into final CLTE decisions. BUAP [pivoting, multi-class and metaclassifier] (Vilari˜no et al., 2013) adopts a pivoting method based on translating T1 into the language of T2 and vice versa (using Google Translate5 ). Similarity measures (e.g. Jaccard index) and features based on n-gram overlap, computed at the level of words and part of speech categories, are used (either alone or in combination) by different classification strategies including: multi-class, a meta-classifier (i.e. combining the output of 2/3/4-class classifiers), and majority voting. CELI [cross-lingual, meta-classifier] (Kouylekov, 2013) uses dictionaries for word matching, and a multilingual corpus extracted from Wikipedia for term weighting. A variety of distance measures implemented in the RTE system EDITS (Kouylekov and Negri, 2010; Negri et al., 2009) are used to extract features to train a meta-classifier. Such classifier combines binary decisions (“YES”/“NO”) taken separately for each of the four CLTE judgements. ECNUCS [pivoting, multi-class] (Jiang and Man, 2013) uses Google Translate to obtain the English translation of each T1. After a pre-processing step aimed at maximizing the commonalities between the two senten"
S13-2005,N10-1045,1,0.669554,"nce over texts written in different languages, targeting at the same time a real application scenario. Participants were presented with datasets for different language pairs, where multi-directional entailment relations (“forward”, “backward”, “bidirectional”, “no entailment”) had to be identified. We report on the training and test data used for evaluation, the process of their creation, the participating systems (six teams, 61 runs), the approaches adopted and the results achieved. 1 Yashar Mehdad UBC Vancouver, Canada mehdad@cs.ubc.ca Introduction The cross-lingual textual entailment task (Mehdad et al., 2010) addresses textual entailment (TE) recognition (Dagan and Glickman, 2004) under the new dimension of cross-linguality, and within the new challenging application scenario of content synchronization. Given two texts in different languages, the cross-lingual textual entailment (CLTE) task consists of deciding if the meaning of one text can be inferred from the meaning of the other text. Crosslinguality represents an interesting direction for research on recognizing textual entailment (RTE), especially due to its possible application in a variety of tasks. Among others (e.g. question answering, i"
S13-2005,P11-1134,1,0.635365,"about semantic equivalence and novelty depend on the possibility to fully or partially translate a text fragment into the other. The recent advances on monolingual TE on the one hand, and the methodologies used in Statistical Machine Translation (SMT) on the other, offer promising solutions to approach the CLTE task. In line with a number of systems that model the RTE task as a similarity problem (i.e. handling similarity scores between T and H as features contributing to the entailment decision), the standard sentence and word alignment programs used in SMT offer a strong baseline for CLTE (Mehdad et al., 2011; 25 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic c Evaluation (SemEval 2013), pages 25–33, Atlanta, Georgia, June 14-15, 2013. 2013 Association for Computational Linguistics fragments entail each other (semantic equivalence); • forward (T1→T2 & T16←T2): unidirectional entailment from T1 to T2; • backward (T16→T2 & T1←T2): unidirectional entailment from T2 to T1; • no entailment (T16→T2 & T16←T2): there is no entailment between T1 and T2 in either direction; Figure 1: Example of SP-EN CLTE pairs. Mehdad et al., 2012"
S13-2005,P12-2024,1,0.83509,"Mehdad et al., 2011; 25 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic c Evaluation (SemEval 2013), pages 25–33, Atlanta, Georgia, June 14-15, 2013. 2013 Association for Computational Linguistics fragments entail each other (semantic equivalence); • forward (T1→T2 & T16←T2): unidirectional entailment from T1 to T2; • backward (T16→T2 & T1←T2): unidirectional entailment from T2 to T1; • no entailment (T16→T2 & T16←T2): there is no entailment between T1 and T2 in either direction; Figure 1: Example of SP-EN CLTE pairs. Mehdad et al., 2012). However, although representing a solid starting point to approach the problem, similarity-based techniques are just approximations, open to significant improvements coming from semantic inference at the multilingual level (e.g. cross-lingual entailment rules such as “perro”→“animal”). Taken in isolation, similaritybased techniques clearly fall short of providing an effective solution to the problem of assigning directions to the entailment relations (especially in the complex CLTE scenario, where entailment relations are multi-directional). Thanks to the contiguity between CLTE, TE and SMT,"
S13-2005,W10-0734,1,0.383734,"Missing"
S13-2005,D11-1062,1,0.7367,"n/English (DE-EN). Each corpus consists of 1,500 sentence pairs (1,000 for training and 500 for test), balanced across the four entailment judgements. In this year’s evaluation, as training set we used the CLTE-2012 corpus1 that was created for the SemEval-2012 evaluation exercise2 (including both training and test sets). The CLTE-2013 test set was created from scratch, following the methodology described in the next section. 3.1 To collect the entailment pairs for the 2013 test set we adopted a slightly modified version of the crowdsourcing methodology followed to create the CLTE2012 corpus (Negri et al., 2011). The main difference with last year’s procedure is that we did not take advantage of crowdsourcing for the whole data collection process, but only for part of it. As for CLTE-2012, the collection and annotation process consists of the following steps: 1. First, English sentences were manually extracted from Wikipedia and Wikinews. The selected sentences represent one of the elements (T1) of each entailment pair; 1 • bidirectional (T1→T2 & T1←T2): the two 26 Data collection and annotation 2 http://www.celct.it/resources.php?id page=CLTE http://www.cs.york.ac.uk/semeval-2012/task8/ 2. Next, eac"
S13-2005,S12-1053,1,0.323391,"Missing"
S13-2005,J03-1002,0,0.00871819,"a-classification strategies have been proposed. 31 Besides the recourse to MT tools (e.g. Google Translate), other tools and resources used by participants include: WordNet, word alignment tools (e.g. Giza++), part-of-speech taggers (e.g. Stanford POS Tagger), stemmers (e.g. Snowball), machine learning libraries (e.g. Weka, SVMlight), parallel corpora (e.g. Europarl), and stopword lists. More in detail: ALTN [cross-lingual, compositional] (Turchi and Negri, 2013) adopts a supervised learning method based on features that consider word alignments between the two sentences obtained with GIZA++ (Och et al., 2003). Binary entailment judgements are taken separately, and combined into final CLTE decisions. BUAP [pivoting, multi-class and metaclassifier] (Vilari˜no et al., 2013) adopts a pivoting method based on translating T1 into the language of T2 and vice versa (using Google Translate5 ). Similarity measures (e.g. Jaccard index) and features based on n-gram overlap, computed at the level of words and part of speech categories, are used (either alone or in combination) by different classification strategies including: multi-class, a meta-classifier (i.e. combining the output of 2/3/4-class classifiers)"
S13-2005,S13-2023,1,0.84219,"ls. Regarding the latter dimension, in addition to compositional and multi-class strategies, also alternative solutions that leverage more sophisticated meta-classification strategies have been proposed. 31 Besides the recourse to MT tools (e.g. Google Translate), other tools and resources used by participants include: WordNet, word alignment tools (e.g. Giza++), part-of-speech taggers (e.g. Stanford POS Tagger), stemmers (e.g. Snowball), machine learning libraries (e.g. Weka, SVMlight), parallel corpora (e.g. Europarl), and stopword lists. More in detail: ALTN [cross-lingual, compositional] (Turchi and Negri, 2013) adopts a supervised learning method based on features that consider word alignments between the two sentences obtained with GIZA++ (Och et al., 2003). Binary entailment judgements are taken separately, and combined into final CLTE decisions. BUAP [pivoting, multi-class and metaclassifier] (Vilari˜no et al., 2013) adopts a pivoting method based on translating T1 into the language of T2 and vice versa (using Google Translate5 ). Similarity measures (e.g. Jaccard index) and features based on n-gram overlap, computed at the level of words and part of speech categories, are used (either alone or i"
S13-2005,S13-2022,0,0.0210187,"Missing"
S13-2005,negri-etal-2012-chinese,1,\N,Missing
S13-2045,S12-1059,0,0.00894071,"Missing"
S13-2045,P10-4003,1,0.705578,"in, 2013), error detection and correction (Leacock et al., 2010), and classification of texts by grade level (Petersen and Ostendorf, 2009; Sheehan et al., 2010; Nelson et al., 2012). In these applications, NLP methods based on shallow features and supervised learning are often highly effective. However, for the assessment of responses to short-answer questions (Leacock and Chodorow, 2003; Pulman and Sukkarieh, 2005; Nielsen et al., 2008a; Mohler et al., 2011) and in tutorial dialog systems (Graesser et al., 1999; Glass, 2000; Pon-Barry et al., 2004; Jordan et al., 2006; VanLehn et al., 2007; Dzikovska et al., 2010) deeper semantic processing is likely to be appropriate. We present the results of the Joint Student Response Analysis and 8th Recognizing Textual Entailment Challenge, aiming to bring together researchers in educational NLP technology and textual entailment. The task of giving feedback on student answers requires semantic inference and therefore is related to recognizing textual entailment. Thus, we offered to the community a 5-way student response labeling task, as well as 3-way and 2way RTE-style tasks on educational data. In addition, a partial entailment task was piloted. We present and c"
S13-2045,N12-1021,1,0.552195,"ing textual entailment. Thus, we offered to the community a 5-way student response labeling task, as well as 3-way and 2way RTE-style tasks on educational data. In addition, a partial entailment task was piloted. We present and compare results from 9 participating teams, and discuss future directions. 1 Introduction One of the tasks in educational NLP systems is providing feedback to students in the context of exam questions, homework or intelligent tutoring. Much previous work has been devoted to the automated Since the task of making and testing a full educational dialog system is daunting, Dzikovska et al. (2012) identified a key subtask and proposed it as a new shared task for the NLP community. Student response analysis (henceforth SRA) is the task of labeling student answers with categories that could 263 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic c Evaluation (SemEval 2013), pages 263–274, Atlanta, Georgia, June 14-15, 2013. 2013 Association for Computational Linguistics Example 1 Q UESTION R EF. A NS . S TUD . A NS . Example 2 Q UESTION R EF. A NS . S TUD . A NS . You used several methods to separate and identify the"
S13-2045,P11-1076,0,0.189172,"ar-Ilan University Israel dagan@cs.biu.ac.il Hoa Trang Dang NIST hoa.dang@nist.gov Abstract scoring of essays (Attali and Burstein, 2006; Shermis and Burstein, 2013), error detection and correction (Leacock et al., 2010), and classification of texts by grade level (Petersen and Ostendorf, 2009; Sheehan et al., 2010; Nelson et al., 2012). In these applications, NLP methods based on shallow features and supervised learning are often highly effective. However, for the assessment of responses to short-answer questions (Leacock and Chodorow, 2003; Pulman and Sukkarieh, 2005; Nielsen et al., 2008a; Mohler et al., 2011) and in tutorial dialog systems (Graesser et al., 1999; Glass, 2000; Pon-Barry et al., 2004; Jordan et al., 2006; VanLehn et al., 2007; Dzikovska et al., 2010) deeper semantic processing is likely to be appropriate. We present the results of the Joint Student Response Analysis and 8th Recognizing Textual Entailment Challenge, aiming to bring together researchers in educational NLP technology and textual entailment. The task of giving feedback on student answers requires semantic inference and therefore is related to recognizing textual entailment. Thus, we offered to the community a 5-way stud"
S13-2045,nielsen-etal-2008-annotating,1,0.393893,"@vulcan.com Ido Dagan Bar-Ilan University Israel dagan@cs.biu.ac.il Hoa Trang Dang NIST hoa.dang@nist.gov Abstract scoring of essays (Attali and Burstein, 2006; Shermis and Burstein, 2013), error detection and correction (Leacock et al., 2010), and classification of texts by grade level (Petersen and Ostendorf, 2009; Sheehan et al., 2010; Nelson et al., 2012). In these applications, NLP methods based on shallow features and supervised learning are often highly effective. However, for the assessment of responses to short-answer questions (Leacock and Chodorow, 2003; Pulman and Sukkarieh, 2005; Nielsen et al., 2008a; Mohler et al., 2011) and in tutorial dialog systems (Graesser et al., 1999; Glass, 2000; Pon-Barry et al., 2004; Jordan et al., 2006; VanLehn et al., 2007; Dzikovska et al., 2010) deeper semantic processing is likely to be appropriate. We present the results of the Joint Student Response Analysis and 8th Recognizing Textual Entailment Challenge, aiming to bring together researchers in educational NLP technology and textual entailment. The task of giving feedback on student answers requires semantic inference and therefore is related to recognizing textual entailment. Thus, we offered to the"
S13-2045,W05-0202,0,0.185532,"Clark Vulcan Inc. USA peterc@vulcan.com Ido Dagan Bar-Ilan University Israel dagan@cs.biu.ac.il Hoa Trang Dang NIST hoa.dang@nist.gov Abstract scoring of essays (Attali and Burstein, 2006; Shermis and Burstein, 2013), error detection and correction (Leacock et al., 2010), and classification of texts by grade level (Petersen and Ostendorf, 2009; Sheehan et al., 2010; Nelson et al., 2012). In these applications, NLP methods based on shallow features and supervised learning are often highly effective. However, for the assessment of responses to short-answer questions (Leacock and Chodorow, 2003; Pulman and Sukkarieh, 2005; Nielsen et al., 2008a; Mohler et al., 2011) and in tutorial dialog systems (Graesser et al., 1999; Glass, 2000; Pon-Barry et al., 2004; Jordan et al., 2006; VanLehn et al., 2007; Dzikovska et al., 2010) deeper semantic processing is likely to be appropriate. We present the results of the Joint Student Response Analysis and 8th Recognizing Textual Entailment Challenge, aiming to bring together researchers in educational NLP technology and textual entailment. The task of giving feedback on student answers requires semantic inference and therefore is related to recognizing textual entailment. T"
S13-2045,R11-1063,1,0.693982,"ighted Average Precision, Recall and F1 , and computed as described in Section 4.2. We used only a majority class baseline, which labeled all facets as ‘Unaddressed’. Its performance is presented in Section 5.4 jointly with the system results. 5.4 Participants and results Only one participant, UKP-BIU, participated in the Partial Entailment Pilot task. The UKP-BIU system is a hybrid of two semantic relationship approaches, namely (i) computing semantic textual similarity by combining multiple content similarity measures (B¨ar et al., 2012), and (ii) recognizing textual entailment with BIUTEE (Stern and Dagan, 2011). The two approaches are combined by generating indicative features from each one and then applying standard supervised machine learning techniques to train a classifier. The system used several lexicalsemantic resources as part of the BIUTEE entailment system, together with S CI E NTS BANK dependency parses and ESA semantic relatedness indexes from Wikipedia. The team submitted the maximum allowed of 3 runs. Table 7 shows Weighted Average and Macro Average F1 scores respectively, also for the majority baseline. The system outperformed the majority baseline on both metrics. The best performanc"
S13-2045,C00-2137,0,0.0467226,"m one another, with performance varying from being the top rank to nearly the lowest. Hence, it seemed more appropriate to report two separate runs.3 In the rest of the discussion system is used to refer to a row in the tables as just described. Systems with performance that was not statistically different from the best results for a given TS are all shown in bold (significance was not calculated for the TS mean). Systems with performance statistically better than the lexical baseline are displayed in italics. Statistical significance tests were conducted using approximate randomization test (Yeh, 2000) with 10,000 iterations; p ≤ 0.05 was considered statistically significant. 4.4 Dataset: B EETLE 5way Run UA UQ CELI1 0.315 0.300 CNGL2 0.431 0.382 CoMeT1 0.569 0.300 EHUALM2 0.526 0.3703 ETS1 0.444 0.461 ETS2 0.619 0.552 LIMSIILES1 0.327 0.280 SoftCardinality1 0.455 0.436 UKP-BIU1 0.423 0.285 Median 0.444 0.370 Baselines: Lexical 0.424 0.414 Majority 0.114 0.118 Five-way Task The results for the five-way task are shown in Tables 2 and 3. Comparison to baselines All of the systems performed substantially better than the majority class baseline (“correct” for both B EETLE and S CI E NTS BANK),"
S13-2045,W07-1401,1,\N,Missing
S14-2001,D10-1115,1,0.672361,"ss and (ii) entailment. The task attracted 21 teams, most of which participated in both subtasks. We received 17 submissions in the relatedness subtask (for a total of 66 runs) and 18 in the entailment subtask (65 runs). 1 Introduction Distributional Semantic Models (DSMs) approximate the meaning of words with vectors summarizing their patterns of co-occurrence in corpora. Recently, several compositional extensions of DSMs (CDSMs) have been proposed, with the purpose of representing the meaning of phrases and sentences by composing the distributional representations of the words they contain (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Mitchell and Lapata, 2010; Socher et al., 2012). Despite the ever increasing interest in the field, the development of adequate benchmarks for CDSMs, especially at the sentence level, is still lagging. Existing data sets, such as those introduced by Mitchell and Lapata (2008) and Grefenstette and Sadrzadeh (2011), are limited to a few hundred instances of very short sentences with a fixed structure. In the last ten years, several large 2 The Task The Task involved two subtasks. (i) Relatedness: predicting the degree of semantic similarity between two sentenc"
S14-2001,S12-1051,0,0.383512,"Missing"
S14-2001,S14-2013,0,0.0692411,"Missing"
S14-2001,S14-2141,0,0.0602478,"78.5 S UIO-Lien run1 77.1 77.0 FBK-TR run3 P 75.4 StanfordNLP run5 S 74.5 UTexas run1 P/S 73.2 Yamraj run1 70.7 asjai run5 S 69.8 haLF run2 S 69.4 RTM-DCU run1 UANLPCourse run2 67.2 S 48.7 Table 6: Primary run results for the entailment subtask. The table also shows whether a system exploits composition information at either the phrase (P) or sentence (S) level. Approaches A summary of the approaches used by the systems to address the task is presented in Table 8. In the table, systems in bold are those for which the authors submitted a paper (Ferrone and Zanzotto, 2014; Bjerva et al., 2014; Beltagy et al., 2014; Lai and Hockenmaier, 2014; Alves et al., 2014; Le´on et al., 2014; Bestgen, 2014; Zhao et al., 2014; Vo et al., 2014; Bic¸ici and Way, 2014; Lien and Kouylekov, 2014; Jimenez et al., 2014; Proisl and Evert, 2014; Gupta et al., 2014). For the others, we used the brief description sent with the system’s results, double-checking the information with the authors. In the table, “E” and “R” refer to the entailment and relatedness task respectively, and “B” to both. Almost all systems combine several kinds of features. To highlight the role played by composition, we draw a distinction between compo"
S14-2001,S14-2125,0,0.0284327,"NLPCourse run2 67.2 S 48.7 Table 6: Primary run results for the entailment subtask. The table also shows whether a system exploits composition information at either the phrase (P) or sentence (S) level. Approaches A summary of the approaches used by the systems to address the task is presented in Table 8. In the table, systems in bold are those for which the authors submitted a paper (Ferrone and Zanzotto, 2014; Bjerva et al., 2014; Beltagy et al., 2014; Lai and Hockenmaier, 2014; Alves et al., 2014; Le´on et al., 2014; Bestgen, 2014; Zhao et al., 2014; Vo et al., 2014; Bic¸ici and Way, 2014; Lien and Kouylekov, 2014; Jimenez et al., 2014; Proisl and Evert, 2014; Gupta et al., 2014). For the others, we used the brief description sent with the system’s results, double-checking the information with the authors. In the table, “E” and “R” refer to the entailment and relatedness task respectively, and “B” to both. Almost all systems combine several kinds of features. To highlight the role played by composition, we draw a distinction between compositional and non-compositional features, and divide the former into ‘fully compositional’ (systems that compositionally computed the meaning of the full sentences, tho"
S14-2001,S14-2024,0,0.0394666,"S 73.2 Yamraj run1 70.7 asjai run5 S 69.8 haLF run2 S 69.4 RTM-DCU run1 UANLPCourse run2 67.2 S 48.7 Table 6: Primary run results for the entailment subtask. The table also shows whether a system exploits composition information at either the phrase (P) or sentence (S) level. Approaches A summary of the approaches used by the systems to address the task is presented in Table 8. In the table, systems in bold are those for which the authors submitted a paper (Ferrone and Zanzotto, 2014; Bjerva et al., 2014; Beltagy et al., 2014; Lai and Hockenmaier, 2014; Alves et al., 2014; Le´on et al., 2014; Bestgen, 2014; Zhao et al., 2014; Vo et al., 2014; Bic¸ici and Way, 2014; Lien and Kouylekov, 2014; Jimenez et al., 2014; Proisl and Evert, 2014; Gupta et al., 2014). For the others, we used the brief description sent with the system’s results, double-checking the information with the authors. In the table, “E” and “R” refer to the entailment and relatedness task respectively, and “B” to both. Almost all systems combine several kinds of features. To highlight the role played by composition, we draw a distinction between compositional and non-compositional features, and divide the former into ‘fully composi"
S14-2001,marelli-etal-2014-sick,1,0.808802,"re also welcome. Besides being of intrinsic interest, the latter systems’ performance will serve to situate CDSM performance within the broader landscape of computational semantics. 3 Data Set Creation The SICK Data Set The SICK data set, consisting of about 10,000 English sentence pairs annotated for relatedness in meaning and entailment, was used to evaluate the systems participating in the task. The data set creation methodology is outlined in the following subsections, while all the details about data generation and annotation, quality control, and interannotator agreement can be found in Marelli et al. (2014). 1 http://nlp.cs.illinois.edu/HockenmaierGroup/data.html http://www.cs.york.ac.uk/semeval2012/task6/index.php?id=data 2 2 Relatedness score Example 1.6 A: “A man is jumping into an empty pool” B: “There is no biker jumping in the air” 2.9 A: “Two children are lying in the snow and are making snow angels” B: “Two angels are making snow on the lying children” 3.6 A: “The young boys are playing outdoors and the man is smiling nearby” B: “There is no boy playing outdoors and there is no man smiling” 4.9 A: “A person in a black jacket is doing tricks on a motorbike” B: “A man in a black jacket is"
S14-2001,S14-2085,0,0.0831811,"Missing"
S14-2001,P08-1028,0,0.15047,"ectors summarizing their patterns of co-occurrence in corpora. Recently, several compositional extensions of DSMs (CDSMs) have been proposed, with the purpose of representing the meaning of phrases and sentences by composing the distributional representations of the words they contain (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Mitchell and Lapata, 2010; Socher et al., 2012). Despite the ever increasing interest in the field, the development of adequate benchmarks for CDSMs, especially at the sentence level, is still lagging. Existing data sets, such as those introduced by Mitchell and Lapata (2008) and Grefenstette and Sadrzadeh (2011), are limited to a few hundred instances of very short sentences with a fixed structure. In the last ten years, several large 2 The Task The Task involved two subtasks. (i) Relatedness: predicting the degree of semantic similarity between two sentences, and (ii) Entailment: detecting the entailment relation holding between them This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 1 Proceedings of the"
S14-2001,S14-2114,0,0.327729,"Missing"
S14-2001,S14-2093,0,0.0223856,"results for the entailment subtask. The table also shows whether a system exploits composition information at either the phrase (P) or sentence (S) level. Approaches A summary of the approaches used by the systems to address the task is presented in Table 8. In the table, systems in bold are those for which the authors submitted a paper (Ferrone and Zanzotto, 2014; Bjerva et al., 2014; Beltagy et al., 2014; Lai and Hockenmaier, 2014; Alves et al., 2014; Le´on et al., 2014; Bestgen, 2014; Zhao et al., 2014; Vo et al., 2014; Bic¸ici and Way, 2014; Lien and Kouylekov, 2014; Jimenez et al., 2014; Proisl and Evert, 2014; Gupta et al., 2014). For the others, we used the brief description sent with the system’s results, double-checking the information with the authors. In the table, “E” and “R” refer to the entailment and relatedness task respectively, and “B” to both. Almost all systems combine several kinds of features. To highlight the role played by composition, we draw a distinction between compositional and non-compositional features, and divide the former into ‘fully compositional’ (systems that compositionally computed the meaning of the full sentences, though not necessarily by assigning meanings to i"
S14-2001,D12-1110,0,0.111915,"btasks. We received 17 submissions in the relatedness subtask (for a total of 66 runs) and 18 in the entailment subtask (65 runs). 1 Introduction Distributional Semantic Models (DSMs) approximate the meaning of words with vectors summarizing their patterns of co-occurrence in corpora. Recently, several compositional extensions of DSMs (CDSMs) have been proposed, with the purpose of representing the meaning of phrases and sentences by composing the distributional representations of the words they contain (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Mitchell and Lapata, 2010; Socher et al., 2012). Despite the ever increasing interest in the field, the development of adequate benchmarks for CDSMs, especially at the sentence level, is still lagging. Existing data sets, such as those introduced by Mitchell and Lapata (2008) and Grefenstette and Sadrzadeh (2011), are limited to a few hundred instances of very short sentences with a fixed structure. In the last ten years, several large 2 The Task The Task involved two subtasks. (i) Relatedness: predicting the degree of semantic similarity between two sentences, and (ii) Entailment: detecting the entailment relation holding between them Thi"
S14-2001,S14-2047,0,0.0241507,"S 69.8 haLF run2 S 69.4 RTM-DCU run1 UANLPCourse run2 67.2 S 48.7 Table 6: Primary run results for the entailment subtask. The table also shows whether a system exploits composition information at either the phrase (P) or sentence (S) level. Approaches A summary of the approaches used by the systems to address the task is presented in Table 8. In the table, systems in bold are those for which the authors submitted a paper (Ferrone and Zanzotto, 2014; Bjerva et al., 2014; Beltagy et al., 2014; Lai and Hockenmaier, 2014; Alves et al., 2014; Le´on et al., 2014; Bestgen, 2014; Zhao et al., 2014; Vo et al., 2014; Bic¸ici and Way, 2014; Lien and Kouylekov, 2014; Jimenez et al., 2014; Proisl and Evert, 2014; Gupta et al., 2014). For the others, we used the brief description sent with the system’s results, double-checking the information with the authors. In the table, “E” and “R” refer to the entailment and relatedness task respectively, and “B” to both. Almost all systems combine several kinds of features. To highlight the role played by composition, we draw a distinction between compositional and non-compositional features, and divide the former into ‘fully compositional’ (systems that compositionall"
S14-2001,D11-1129,0,0.0154758,"Missing"
S14-2001,S14-2044,0,0.216165,"Missing"
S14-2001,S14-2139,0,0.0534159,"Missing"
S14-2001,S14-2131,0,0.156692,"Table 6: Primary run results for the entailment subtask. The table also shows whether a system exploits composition information at either the phrase (P) or sentence (S) level. Approaches A summary of the approaches used by the systems to address the task is presented in Table 8. In the table, systems in bold are those for which the authors submitted a paper (Ferrone and Zanzotto, 2014; Bjerva et al., 2014; Beltagy et al., 2014; Lai and Hockenmaier, 2014; Alves et al., 2014; Le´on et al., 2014; Bestgen, 2014; Zhao et al., 2014; Vo et al., 2014; Bic¸ici and Way, 2014; Lien and Kouylekov, 2014; Jimenez et al., 2014; Proisl and Evert, 2014; Gupta et al., 2014). For the others, we used the brief description sent with the system’s results, double-checking the information with the authors. In the table, “E” and “R” refer to the entailment and relatedness task respectively, and “B” to both. Almost all systems combine several kinds of features. To highlight the role played by composition, we draw a distinction between compositional and non-compositional features, and divide the former into ‘fully compositional’ (systems that compositionally computed the meaning of the full sentences, though not necessarily by"
S14-2001,S14-2055,0,0.250575,"77.1 77.0 FBK-TR run3 P 75.4 StanfordNLP run5 S 74.5 UTexas run1 P/S 73.2 Yamraj run1 70.7 asjai run5 S 69.8 haLF run2 S 69.4 RTM-DCU run1 UANLPCourse run2 67.2 S 48.7 Table 6: Primary run results for the entailment subtask. The table also shows whether a system exploits composition information at either the phrase (P) or sentence (S) level. Approaches A summary of the approaches used by the systems to address the task is presented in Table 8. In the table, systems in bold are those for which the authors submitted a paper (Ferrone and Zanzotto, 2014; Bjerva et al., 2014; Beltagy et al., 2014; Lai and Hockenmaier, 2014; Alves et al., 2014; Le´on et al., 2014; Bestgen, 2014; Zhao et al., 2014; Vo et al., 2014; Bic¸ici and Way, 2014; Lien and Kouylekov, 2014; Jimenez et al., 2014; Proisl and Evert, 2014; Gupta et al., 2014). For the others, we used the brief description sent with the system’s results, double-checking the information with the authors. In the table, “E” and “R” refer to the entailment and relatedness task respectively, and “B” to both. Almost all systems combine several kinds of features. To highlight the role played by composition, we draw a distinction between compositional and non-compositio"
S14-2001,S14-2021,0,0.0355515,"Missing"
S14-2001,W07-1401,0,\N,Missing
W04-2214,magnini-cavaglia-2000-integrating,1,0.421647,"nd Amsler, 1986). As regards the usage of Domain hierarchies in the field of multilingual lexicography, an example is given by the EuroWordNet Domain-ontology, a language independent domain hierarchy to which interlingual concepts (ILI-records) can be assigned (Vossen, 1998). In the same line, see also the SIMPLE domain hierarchy (SIMPLE, 2000). Large domain hierarchies are also available on the Internet, mainly meant for classifying web documents. See for instance the Google and Yahoo directories. A large-scale application of a domain hierarchy to a lexicon is represented by WORDNET DOMAINS (Magnini and Cavaglià, 2000). WORDNET DOMAINS is a lexical resource developed at ITCirst where each WordNet synset (Fellbaum, 1998) is annotated with one or more domain labels selected from a domain hierarchy which was specifically created to this purpose. As the WORDNET DOMAINS Hierarchy (WDH) is language-independent, it has been possible to exploit it in the framework of MultiWordNet (Pianta et al., 2002), a multilingual lexical database developed at ITC-irst in which the Italian component is strictly aligned with the English WordNet. In MultiWordNet, the domain information has been automatically transferred from Engli"
W04-2214,W02-1304,1,0.736051,"from English to Italian, resulting in a Italian version of WORDNET DOMAINS. For instance, as the English synset {court, tribunal, judicature} was annotated with the domain LAW, also the Italian synset {corte, tribunale}, which is aligned with the corresponding English synset, results automatically annotated with the LAW domain. This procedure can be applied to any other WordNet (or part of it) aligned with Princeton WordNet (see for instance the Spanish WordNet). It is worth noticing that two of the main ongoing projects addressing the construction of multilingual resources, that is MEANING (Rigau et al. 2002) and BALKANET (see web site), make use of WORDNET DOMAINS. Finally, WORDNET DOMAINS is being profitably used by the NLP community mainly for Word Sense Disambiguation tasks in various languages. Another application of domain hierarchies can be found in the field of corpus creation. In many existing corpora (see for instance the BNC, the ANC, the Brown and LOB Corpora) domain is one of the most used criteria for text selection and/or classification. Given that a domain hierarchy is language independent, if the same domain hierarchy is used to build reference corpora for different languages, the"
W04-2214,J98-1004,0,0.00723395,"e the domain of a text is its broad topic. In this work we will assume that also these two points of view on domains are strictly intertwined. By their nature, domains can be organized in hierarchies based on a relation of specificity. For instance we can say that TENNIS is a more specific domain than SPORT, or that ARCHITECTURE is more general than TOWN PLANNING. Domain hierarchies can be usefully integrated into other linguistic resources and are also profitably used in many Natural Language Processing (NLP) tasks such as Word Sense Disambiguation (Magnini et al. 2002), Text Categorization (Schutze, 1998), Information Retrieval (Walker and Amsler, 1986). As regards the usage of Domain hierarchies in the field of multilingual lexicography, an example is given by the EuroWordNet Domain-ontology, a language independent domain hierarchy to which interlingual concepts (ILI-records) can be assigned (Vossen, 1998). In the same line, see also the SIMPLE domain hierarchy (SIMPLE, 2000). Large domain hierarchies are also available on the Internet, mainly meant for classifying web documents. See for instance the Google and Yahoo directories. A large-scale application of a domain hierarchy to a lexicon is"
W06-2713,ide-romary-2002-standards,0,\N,Missing
W10-3503,D07-1074,0,0.248598,"ight also be useful for other IE tasks, which exploit semantic knowledge. In the following we start by providing a brief overview of the existing corpora annotated with links to Wikipedia. In Section 3 we describe some characteristics of the English ACE 2005 corpus, which are relevant to the creation of the extension. Next, we describe the general annotation principles and the procedure adopted to carry out the annotation. In Section 4 we present some analyses of the annotation and statistics about InterAnnotator Agreement. 2 Related work Recent approaches to linking terms to Wikipedia pages (Cucerzan, 2007; Csomai and Mihalcea, 2008; Milne and Witten, 2008; Kulkarni et al., 2009) have used two kinds of corpora for evaluation of accuracy: (i) sets of Wikipedia pages and (ii) manually annotated corpora. In Wikipedia pages links are added to terms “only where they are relevant to the context”4 . Therefore, Wikipedia pages do not contain the full annotation of all entity mentions. This observation applies equally to the corpus used by (Milne and Witten, 2008), which includes 50 documents from the AQUAINT corpus annotated following the same strategy5 . The corpus created by (Cucerzan, 2007) contains"
W10-3503,N07-1011,0,0.0348426,"Missing"
W10-3503,P02-1014,0,0.0262631,"English ACE 2005 Corpus Annotation with Ground-truth Links to Wikipedia Luisa Bentivogli FBK-Irst bentivo@fbk.eu Pamela Forner CELCT forner@celct.it Claudio Giuliano FBK-Irst giuliano@fbk.eu Alessandro Marchetti CELCT amarchetti@celct.it Emanuele Pianta FBK-Irst pianta@fbk.eu Kateryna Tymoshenko FBK-Irst tymoshenko@fbk.eu Abstract the semantic similarity of mentions (Ponzetto and Strube, 2006) or their semantic classes (Ng, 2007; Soon et al., 2001). Up to now, WordNet has been one of the most frequently used sources of semantic knowledge for the coreference resolution task (Soon et al., 2001; Ng and Cardie, 2002). Researchers have shown, however, that WordNet has some limits. On one hand, although WordNet has a big coverage of the English language in terms of common nouns, it still has a limited coverage of proper nouns (e.g. Barack Obama is not available in the on-line version) and entity descriptions (e.g. president of India). On the other hand WordNet sense inventory is considered too finegrained (Ponzetto and Strube, 2006; Mihalcea and Moldovan, 2001). In alternative, it has been recently shown that Wikipedia can be a promising source of semantic knowledge for coreference resolution between nomina"
W10-3503,P07-1068,0,0.0285714,"Missing"
W10-3503,N06-1025,0,0.0375842,"lasses (Ng, 2007; Soon et al., 2001). Up to now, WordNet has been one of the most frequently used sources of semantic knowledge for the coreference resolution task (Soon et al., 2001; Ng and Cardie, 2002). Researchers have shown, however, that WordNet has some limits. On one hand, although WordNet has a big coverage of the English language in terms of common nouns, it still has a limited coverage of proper nouns (e.g. Barack Obama is not available in the on-line version) and entity descriptions (e.g. president of India). On the other hand WordNet sense inventory is considered too finegrained (Ponzetto and Strube, 2006; Mihalcea and Moldovan, 2001). In alternative, it has been recently shown that Wikipedia can be a promising source of semantic knowledge for coreference resolution between nominals (Ponzetto and Strube, 2006). Consider some possible uses of Wikipedia. For example, knowing that the entity mention “Obama” is described on the Wikipedia page Barack_Obama2 , one can benefit from the Wikipedia category structure. Categories assigned to the Barack_Obama page can be used as semantic classes, e.g. “21st-century presidents of the United States”. Another example of a useful Wikipedia feature are the lin"
W10-3503,J01-4004,0,0.0957087,"Missing"
W10-3503,P08-4003,0,0.027345,"Missing"
W19-6608,2014.amta-researchers.5,0,0.0269449,"uin, 2014; Q. Zadeh and Handschuh, 2014; Astrakhantsev et al., 2015). The situation is much less favourable for terminology translation evaluation. Indeed, the majority of works addressing domain adaptation for MT evaluate systems only in terms of overall performance on a domain-specific test set, while very few studies specifically focus on the engines’ ability to translate domain-specific terminology, and thus resort to test sets in which terms are annotated. To the best of our knowledge, only the following manually annotated resources are made available to the community. The BitterCorpus3 (Arcan et al., 2014a) is a collection of parallel English–Italian documents in the information technology domain in which technical terms in both the source and target sides of the bi-texts are manually marked and aligned. TermTraGS4 (Farajian et al., 2018) is a sentence-aligned version of the BitterCorpus, which also includes a large training set. On a different aspect of MT quality evaluation, most of the works comparing NMT with previous paradigms treat correct or wrong lexical choices as one of the main quality indicators (Bentivogli et al., 2016; Bentivogli et al., 2018; Toral and S´anchez-Cartagena, 2017;"
W19-6608,J08-4004,0,0.0309199,"Missing"
W19-6608,D16-1025,1,0.924236,"y from different domains, with disciplinary terms, e.g. Hydrosilylation, Fotoredox catalysis, for a course on chemistry, appearing together with educational ones - e.g. ECTS, module. The potential and challenges mentioned so far make course catalogues an interesting test bed for 1 https://cordis.europa.eu/project/rcn/ 191739/factsheet/en 2 http://tramooc.eu/content/ scientific-publications Dublin, Aug. 19-23, 2019 |p. 78 neural MT (NMT) (Sutskever et al., 2014; Bahdanau et al., 2014). Indeed, in the last few years NMT has delivered considerable improvements in output quality in many respects (Bentivogli et al., 2016), yet not showing clear-cut progresses when it comes to lexis-related issues, e.g. lexical choices, omissions or mistranslations (Castilho et al., 2018). These issues are especially critical for texts rich in domain-specific terminology, or texts containing terms belonging to different domains. Testing an MT engine on course catalogues can provide interesting information on domain-specific terminology handling and on results achievable with a relatively small amount of in-domain resources used to perform domain-adaptation of a neural model. Whilst assessing systems’ ability to correctly transl"
W19-6608,2011.eamt-1.6,0,0.131332,"Missing"
W19-6608,C14-2026,1,0.835869,"1 21 85 16 196 37 233 130 Total 3,916 3,601 7,517 5,132 Table 2: Statistics of the terms annotated in the MAGMATic data sets. Terms in the three domain categories - Disciplinary, Education, Education-equipment (here Equip.) - are further split into the Sure and Possible (Poss.) subcategories. For either of these subcategories, the number of SWs and MWs, and the total number of terms are provided. In the two bottom rows, the total number of terms and the vocabulary (i.e. the number of distinct terms) are given for each category. The annotation was carried out using the MTEQuAL annotation tool (Girardi et al., 2014). For each English sentence, the MT-EQuAL interface displays the source sentence and the disciplinary domain retrieved from the name of the university course catalogue. Furthermore, the tool allows the annotators to perform the two annotation steps simultaneously: they mark each term and annotate it (sure/possible distinction and domain category) in a single go. This makes the annotation task efficient and less demanding in terms of effort. 3.3 Annotation statistics Details regarding the number of terms annotated in the data set are provided in Table 2. In 101 sentences out of 2,157 (see Table"
W19-6608,P16-4009,0,0.0280406,"Missing"
W19-6608,W14-4807,0,0.0541657,"Missing"
W19-6608,E17-1100,0,0.0429838,"Missing"
W19-6608,L18-1600,0,0.0277106,"Missing"
W19-6608,P02-1040,0,0.104847,"9 Proceedings of MT Summit XVII, volume 1 Domain-adaptation It En Sent.pairs 40,361 Tokens 632,223 601,236 Vocabulary 55,458 48,126 Table 4: Size of the domain-adaptation data set: number of sentences, number of tokens (i.e. running words) and vocabulary (i.e. number of distinct word types). 4.3 Evaluation metrics The MT systems were evaluated both in terms of overall performance and specifically targeting their ability to translate domain terminology. The bigger picture of the quality achieved with the setup described so far is provided through an automatic evaluation in terms of BLEU score (Papineni et al., 2002). The evaluation focused on terminology translation is based on the Term Hit Rate (THR) metric (Farajian et al., 2018). THR takes in a list of annotated terms in each reference sentence and looks for their occurrence in the MT output. Then it computes the proportion of terms in the reference that are correctly translated by the MT system. An upper bound of 1 match for each reference term is applied in order not to reward over-generated terms in the MT output. Similarly to the approach adopted for interannotator agreement (see Sect. 3.4), two THR types are computed: perfect THR – where a match"
W19-6711,1985.tmi-1.4,0,0.744869,"Missing"
W19-6711,2006.amta-papers.25,0,0.118986,"words in total. Participants worked in MateCat. A project – including a termbase – was assigned to each of them. A week before, students were given basic information about the experiment.8 After reading the instructions, students started working autonomously. In the instructions they were invited to work as they normally would. They were asked to deliver a target text of publishable quality, but encouraged to use the provided target text as much as possible and not to over-edit. Researchers were present in the lab throughout. 3.4 Evaluation methods Productivity was measured in terms of HTER (Snover et al., 2006) between the original text and the participants’ edited version, and in terms of words per second (WPS). The latter was obtained by converting MateCat time measurements on a segment level into seconds and dividing them by the number of words in the target text. Two separate linear mixed models were built, one for each dependent variable, i.e. HTER and WPS. In both cases, the independent variables (or fixed effects) are categorical, i.e. translation method (MT/HT), and translation correctness (correct/incorrect). We included in the model an interaction of the two, with participant and segment a"
W19-6711,E17-1100,0,0.0364589,"Missing"
W19-6711,W18-1803,0,0.0383905,"Missing"
W19-6711,W15-4910,0,0.0479265,"Missing"
