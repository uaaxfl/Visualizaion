2021.jeptalnrecital-taln.13,D{\\'e}finition et d{\\'e}tection des incoh{\\'e}rences du syst{\\`e}me dans les dialogues orient{\\'e}s t{\\^a}che. (We present experiments on automatically detecting inconsistent behavior of task-oriented dialogue systems from the context),2021,-1,-1,5,0,5611,leonpaul schaub,Actes de la 28e Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Volume 1 : conf{\\'e}rence principale,0,"D{\'e}finition et d{\'e}tection des incoh{\'e}rences du syst{\`e}me dans les dialogues orient{\'e}s t{\^a}che. Nous pr{\'e}sentons des exp{\'e}riences sur la d{\'e}tection automatique des comportements incoh{\'e}rents des syst{\`e}mes de dialogues orient{\'e}s t{\^a}che {\`a} partir du contexte. Nous enrichissons les donn{\'e}es bAbI/DSTC2 (Bordes et al., 2017) avec une annotation automatique des incoh{\'e}rences de dialogue, et nous d{\'e}montrons que les incoh{\'e}rences sont en corr{\'e}lation avec les dialogues rat{\'e}s. Nous supposons que l{'}utilisation d{'}un historique de dialogue limit{\'e} et la pr{\'e}diction du prochain tour de l{'}utilisateur peuvent am{\'e}liorer la classification des incoh{\'e}rences. Si les deux hypoth{\`e}ses sont confirm{\'e}es pour un mod{\`e}le de dialogue bas{\'e} sur les r{\'e}seaux de m{\'e}moire, elles ne le sont pas pour un entra{\^\i}nement bas{\'e} sur le mod{\`e}le de langage GPT-2, qui b{\'e}n{\'e}ficie le plus de l{'}utilisation de l{'}historique complet du dialogue et obtient un score de pr{\'e}cision de 0,99."
2021.fnp-1.2,A sequence to sequence transformer data logic experiment,2021,-1,-1,5,0,6307,danxin cui,Proceedings of the 3rd Financial Narrative Processing Workshop,0,None
2021.fnp-1.11,Annotation model and corpus for opinionated economy and finance narrative detection,2021,-1,-1,2,0,6327,jiahui hu,Proceedings of the 3rd Financial Narrative Processing Workshop,0,None
2021.eval4nlp-1.1,Differential Evaluation: a Qualitative Analysis of Natural Language Processing System Behavior Based Upon Data Resistance to Processing,2021,-1,-1,5,0,8588,lucie gianola,Proceedings of the 2nd Workshop on Evaluation and Comparison of NLP Systems,0,None
2021.econlp-1.1,A Fine-Grained Annotated Corpus for Target-Based Opinion Analysis of Economic and Financial Narratives,2021,-1,-1,2,0,6327,jiahui hu,Proceedings of the Third Workshop on Economics and Natural Language Processing,0,"In this paper about aspect-based sentiment analysis (ABSA), we present the first version of a fine-grained annotated corpus for target-based opinion analysis (TBOA) to analyze economic activities or financial markets. We have annotated, at an intra-sentential level, a corpus of sentences extracted from documents representative of financial analysts{'} most-read materials by considering how financial actors communicate about the evolution of event trends and analyze related publications (news, official communications, etc.). Since we focus on identifying the expressions of opinions related to the economy and financial markets, we annotated the sentences that contain at least one subjective expression about a domain-specific term. Candidate sentences for annotations were randomly chosen from texts of specialized press and professional information channels over a period ranging from 1986 to 2021. Our annotation scheme relies on various linguistic markers like domain-specific vocabulary, syntactic structures, and rhetorical relations to explicitly describe the author{'}s subjective stance. We investigated and evaluated the recourse to automatic pre-annotation with existing natural language processing technologies to alleviate the annotation workload. Our aim is to propose a corpus usable on the one hand as training material for the automatic detection of the opinions expressed on an extensive range of domain-specific aspects and on the other hand as a gold standard for evaluation TBOA. In this paper, we present our pre-annotation models and evaluations of their performance, introduce our annotation scheme and report on the main characteristics of our corpus."
2020.lrec-1.275,{NLP} Analytics in Finance with {D}o{R}e: A {F}rench 250{M} Tokens Corpus of Corporate Annual Reports,2020,-1,-1,2,0,17204,corentin masson,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Recent advances in neural computing and word embeddings for semantic processing open many new applications areas which had been left unaddressed so far because of inadequate language understanding capacity. But this new kind of approaches rely even more on training data to be operational. Corpora for financial applications exists, but most of them concern stock market prediction and are in English. To address this need for the French language and regulation oriented applications which require a deeper understanding of the text content, we hereby present {``}DoRe{''}, a French and dialectal French Corpus for NLP analytics in Finance, Regulation and Investment. This corpus is composed of: (a) 1769 Annual Reports from 336 companies among the most capitalized companies in: France (Euronext Paris) {\&} Belgium (Euronext Brussels), covering a time frame from 2009 to 2019, and (b) related MetaData containing information for each company about its ISIN code, capitalization and sector. This corpus is designed to be as modular as possible in order to allow for maximum reuse in different tasks pertaining to Economics, Finance and Regulation. After presenting existing resources, we relate the construction of the DoRe corpus and the rationale behind our choices, concluding on the spectrum of possible uses of this new resource for NLP applications."
2020.eamt-1.57,The Multilingual Anonymisation Toolkit for Public Administrations ({MAPA}) Project,2020,-1,-1,17,0,17549,eriks ajausks,Proceedings of the 22nd Annual Conference of the European Association for Machine Translation,0,"We describe the MAPA project, funded under the Connecting Europe Facility programme, whose goal is the development of an open-source de-identification toolkit for all official European Union languages. It will be developed since January 2020 until December 2021."
2020.bionlp-1.5,{D}e{S}pin: a prototype system for detecting spin in biomedical publications,2020,-1,-1,4,1,22236,anna koroleva,Proceedings of the 19th SIGBioMed Workshop on Biomedical Language Processing,0,"Improving the quality of medical research reporting is crucial to reduce avoidable waste in research and to improve the quality of health care. Despite various initiatives aiming at improving research reporting {--} guidelines, checklists, authoring aids, peer review procedures, etc. {--} overinterpretation of research results, also known as spin, is still a serious issue in research reporting. In this paper, we propose a Natural Language Processing (NLP) system for detecting several types of spin in biomedical articles reporting randomized controlled trials (RCTs). We use a combination of rule-based and machine learning approaches to extract important information on trial design and to detect potential spin. The proposed spin detection system includes algorithms for text structure analysis, sentence classification, entity and relation extraction, semantic similarity assessment. Our algorithms achieved operational performance for the these tasks, F-measure ranging from 79,42 to 97.86{\%} for different tasks. The most difficult task is extracting reported outcomes. Our tool is intended to be used as a semi-automated aid tool for assisting both authors and peer reviewers to detect potential spin. The tool incorporates a simple interface that allows to run the algorithms and visualize their output. It can also be used for manual annotation and correction of the errors in the outputs. The proposed tool is the first tool for spin detection. The tool and the annotated dataset are freely available."
W19-5038,Extracting relations between outcomes and significance levels in Randomized Controlled Trials ({RCT}s) publications,2019,0,0,2,1,22236,anna koroleva,Proceedings of the 18th BioNLP Workshop and Shared Task,0,"Randomized controlled trials assess the effects of an experimental intervention by comparing it to a control intervention with regard to some variables - trial outcomes. Statistical hypothesis testing is used to test if the experimental intervention is superior to the control. Statistical significance is typically reported for the measured outcomes and is an important characteristic of the results. We propose a machine learning approach to automatically extract reported outcomes, significance levels and the relation between them. We annotated a corpus of 663 sentences with 2,552 outcome - significance level relations (1,372 positive and 1,180 negative relations). We compared several classifiers, using a manually crafted feature set, and a number of deep learning models. The best performance (F-measure of 94{\%}) was shown by the BioBERT fine-tuned model."
L18-1080,Annotating Spin in Biomedical Scientific Publications : the case of Random Controlled Trials ({RCT}s),2018,0,0,2,1,22236,anna koroleva,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1297,Measuring Innovation in Speech and Language Processing Publications.,2018,0,0,3,1,29836,joseph mariani,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
2018.jeptalnrecital-deft.1,{DEFT}2018 : recherche d{'}information et analyse de sentiments dans des tweets concernant les transports en {{\\^I}}le de {F}rance ({DEFT}2018 : Information Retrieval and Sentiment Analysis in Tweets about Public Transportation in {{\\^I}}le de {F}rance Region ),2018,-1,-1,1,1,5615,patrick paroubek,"Actes de la Conf{\\'e}rence TALN. Volume 2 - D{\\'e}monstrations, articles des Rencontres Jeunes Chercheurs, ateliers DeFT",0,"Cet article pr{\'e}sente l{'}{\'e}dition 2018 de la campagne d{'}{\'e}valuation DEFT (D{\'e}fi Fouille de Textes). A partir d{'}un corpus de tweets, quatre t{\^a}ches ont {\'e}t{\'e} propos{\'e}es : identifier les tweets sur la th{\'e}matique des transports, puis parmi ces derniers, identifier la polarit{\'e} (n{\'e}gatif, neutre, positif, mixte), identifier les marqueurs de sentiment et la cible, et enfin, annoter compl{\`e}tement chaque tweet en source et cible des sentiments exprim{\'e}s. Douze {\'e}quipes ont particip{\'e}, majoritairement sur les deux premi{\`e}res t{\^a}ches. Sur l{'}identification de la th{\'e}matique des transports, la micro F-mesure varie de 0,827 {\`a} 0,908. Sur l{'}identification de la polarit{\'e} globale, la micro F-mesure varie de 0,381 {\`a} 0,823."
W16-4711,Providing and Analyzing {NLP} Terms for our Community,2016,-1,-1,3,0.495633,29837,gil francopoulo,Proceedings of the 5th International Workshop on Computational Terminology (Computerm2016),0,"By its own nature, the Natural Language Processing (NLP) community is a priori the best equipped to study the evolution of its own publications, but works in this direction are rare and only recently have we seen a few attempts at charting the field. In this paper, we use the algorithms, resources, standards, tools and common practices of the NLP field to build a list of terms characteristic of ongoing research, by mining a large corpus of scientific publications, aiming at the largest possible exhaustivity and covering the largest possible time span. Study of the evolution of this term list through time reveals interesting insights on the dynamics of field and the availability of the term database and of the corpus (for a large part) make possible many further comparative studies in addition to providing a test field for a new graphic interface designed to perform visual time analytics of large sized thesauri."
W16-1509,A Study of Reuse and Plagiarism in Speech and Natural Language Processing papers,2016,0,1,3,1,29836,joseph mariani,Proceedings of the Joint Workshop on Bibliometric-enhanced Information Retrieval and Natural Language Processing for Digital Libraries ({BIRNDL}),0,None
L16-1052,Predictive Modeling: Guessing the {NLP} Terms of Tomorrow,2016,9,1,3,0.495633,29837,gil francopoulo,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Predictive modeling, often called {``}predictive analytics{''} in a commercial context, encompasses a variety of statistical techniques that analyze historical and present facts to make predictions about unknown events. Often the unknown events are in the future, but prediction can be applied to any type of unknown whether it be in the past or future. In our case, we present some experiments applying predictive modeling to the usage of technical terms within the NLP domain."
L16-1298,A Study of Reuse and Plagiarism in {LREC} papers,2016,6,2,3,0.495633,29837,gil francopoulo,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"The aim of this experiment is to present an easy way to compare fragments of texts in order to detect (supposed) results of copy {\&} paste operations between articles in the domain of Natural Language Processing (NLP). The search space of the comparisons is a corpus labeled as NLP4NLP gathering a large part of the NLP field. The study is centered on LREC papers in both directions, first with an LREC paper borrowing a fragment of text from the collection, and secondly in the reverse direction with fragments of LREC documents borrowed and inserted in the collection."
2016.jeptalnrecital-demo.3,"{A}pp{FM}, une plate-forme de gestion de modules de {TAL} ({A}pp{FM}, a tool for managing {NLP} modules)",2016,0,0,3,0,36081,paul buiquang,Actes de la conf{\\'e}rence conjointe JEP-TALN-RECITAL 2016. volume 5 : D{\\'e}monstrations,0,AppFM 1 est un outil {\`a} mi-chemin entre un environnement de cr{\'e}ation de cha{\^\i}nes modulaires de TAL et un gestionnaire de services syst{\`e}mes. Il permet l{'}int{\'e}gration d{'}applications ayant des d{\'e}pendances complexes en des cha{\^\i}nes de traitements r{\'e}utilisables facilement par le biais de multiples interfaces.
2015.jeptalnrecital-long.24,Utiliser les interjections pour d{\\'e}tecter les {\\'e}motions,2015,-1,-1,2,1,18550,amel fraisse,Actes de la 22e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Bien que les interjections soient un ph{\'e}nom{\`e}ne linguistique connu, elles ont {\'e}t{\'e} peu {\'e}tudi{\'e}es et cela continue d{'}{\^e}tre le cas pour les travaux sur les microblogs. Des travaux en analyse de sentiments ont montr{\'e} l{'}int{\'e}r{\^e}t des {\'e}motic{\^o}nes et r{\'e}cemment des mots-di{\`e}ses, qui s{'}av{\`e}rent {\^e}tre tr{\`e}s utiles pour la classification en polarit{\'e}. Mais malgr{\'e} leur statut grammatical et leur richesse s{\'e}mantique, les interjections sont rest{\'e}es marginalis{\'e}es par les syst{\`e}mes d{'}analyse de sentiments. Nous montrons dans cet article l{'}apport majeur des interjections pour la d{\'e}tection des {\'e}motions. Nous d{\'e}taillons la production automatique, bas{\'e}e sur les interjections, d{'}un corpus {\'e}tiquet{\'e} avec les {\'e}motions. Nous expliquons ensuite comment nous avons utilis{\'e} ce corpus pour en d{\'e}duire, automatiquement, un lexique affectif pour le fran{\c{c}}ais. Ce lexique a {\'e}t{\'e} {\'e}valu{\'e} sur une t{\^a}che de d{\'e}tection des {\'e}motions, qui a montr{\'e} un gain en mesure F1 allant, selon les {\'e}motions, de +0,04 {\`a} +0,21."
W14-6301,Automatic Analysis of Scientific and Literary Texts. Presentation and Results of the {DEFT}2014 Text Mining Challenge (Analyse automatique de textes litt{\\'e}raires et scientifiques : pr{\\'e}sentation et r{\\'e}sultats du d{\\'e}fi fouille de texte {DEFT}2014) [in {F}rench],2014,-1,-1,3,0,18582,thierry hamon,TALN-RECITAL 2014 Workshop DEFT 2014 : D{\\'E}fi Fouille de Textes (DEFT 2014 Workshop: Text Mining Challenge),0,None
fraisse-paroubek-2014-toward,"Toward a unifying model for Opinion, Sentiment and Emotion information extraction",2014,14,4,2,1,18550,amel fraisse,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"This paper presents a logical formalization of a set 20 semantic categories related to opinion, emotion and sentiment. Our formalization is based on the BDI model (Belief, Desire and Intetion) and constitues a first step toward a unifying model for subjective information extraction. The separability of the subjective classes that we propose was assessed both formally and on two subjective reference corpora."
mariani-etal-2014-rediscovering,Rediscovering 15 Years of Discoveries in Language Resources and Evaluation: The {LREC} Anthology Analysis,2014,-1,-1,2,1,29836,joseph mariani,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"This paper aims at analyzing the content of the LREC conferences contained in the ELRA Anthology over the past 15 years (1998-2013). It follows similar exercises that have been conducted, such as the survey on the IEEE ICASSP conference series from 1976 to 1990, which served in the launching of the ESCA Eurospeech conference, a survey of the Association of Computational Linguistics (ACL) over 50 years of existence, which was presented at the ACL conference in 2012, or a survey over the 25 years (1987-2012) of the conferences contained in the ISCA Archive, presented at Interspeech 2013. It contains first an analysis of the evolution of the number of papers and authors over time, including the study of their gender, nationality and affiliation, and of the collaboration among authors. It then studies the funding sources of the research investigations that are reported in the papers. It conducts an analysis of the evolution of the research topics within the community over time. It finally looks at reuse and plagiarism in the papers. The survey shows the present trends in the conference series and in the Language Resources and Evaluation scientific community. Conducting this survey also demonstrated the importance of a clear and unique identification of authors, papers and other sources to facilitate the analysis. This survey is preliminary, as many other aspects also deserve attention. But we hope it will help better understanding and forging our community in the global village."
mariani-etal-2014-facing,Facing the Identification Problem in Language-Related Scientific Data Analysis.,2014,5,4,4,1,29836,joseph mariani,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"This paper describes the problems that must be addressed when studying large amounts of data over time which require entity normalization applied not to the usual genres of news or political speech, but to the genre of academic discourse about language resources, technologies and sciences. It reports on the normalization processes that had to be applied to produce data usable for computing statistics in three past studies on the LRE Map, the ISCA Archive and the LDC Bibliography. It shows the need for human expertise during normalization and the necessity to adapt the work to the study objectives. It investigates possible improvements for reducing the workload necessary to produce comparable results. Through this paper, we show the necessity to define and agree on international persistent and unique identifiers."
asadullah-etal-2014-bidirectionnal,"Bidirectionnal converter between syntactic annotations : from {F}rench Treebank Dependencies to {PASSAGE} annotations, and back",2014,16,0,2,0,23742,munshi asadullah,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"We present here part of a bidirectional converter between the French Tree-bank Dependency (FTB - DEP) annotations into the PASSAGE format. FTB - DEP is the representation used by several freely available parsers and the PASSAGE annotation was used to hand-annotate a relatively large sized corpus, used as gold-standard in the PASSAGE evaluation campaigns. Our converter will give the means to evaluate these parsers on the PASSAGE corpus. We shall illustrate the mapping of important syntactic phenomena using the corpus made of the examples of the FTB - DEP annotation guidelines, which we have hand-annotated with PASSAGE annotations and used to compute quantitative performance measures on the FTB - DEP guidelines.n this paper we will briefly introduce the two annotation formats. Then, we detail the two converters, and the rules which have been written. The last part will detail the results we obtained on the phenomenon we mostly study, the passive forms. We evaluate the converters by a double conversion, from PASSAGE to CoN LL and back to PASSAGE. We will detailed in this paper the linguistic phenomenon we detail here, the passive form."
F13-2011,Improving Minor Opinion Polarity Classification with Named Entity Analysis (L{'}apport des Entit{\\'e}s Nomm{\\'e}es pour la classification des opinions minoritaires) [in {F}rench],2013,0,0,2,1,18550,amel fraisse,Proceedings of TALN 2013 (Volume 2: Short Papers),0,None
F13-2022,Converting dependencies for syntactic analysis of {F}rench into {PASSAGE} functional relations (Convertir des analyses syntaxiques en d{\\'e}pendances vers les relations fonctionnelles {PASSAGE}) [in {F}rench],2013,-1,-1,1,1,5615,patrick paroubek,Proceedings of TALN 2013 (Volume 2: Short Papers),0,None
W12-1101,Indexation libre et contr{\\^o}l{\\'e}e d{'}articles scientifiques. Pr{\\'e}sentation et r{\\'e}sultats du d{\\'e}fi fouille de textes {DEFT}2012 (Controlled and free indexing of scientific papers. Presentation and results of the {DEFT}2012 text-mining challenge) [in {F}rench],2012,-1,-1,1,1,5615,patrick paroubek,"JEP-TALN-RECITAL 2012, Workshop DEFT 2012: D{\\'E}fi Fouille de Textes (DEFT 2012 Workshop: Text Mining Challenge)",0,None
paroubek-tannier-2012-rough,A Rough Set Formalization of Quantitative Evaluation with Ambiguity,2012,22,0,1,1,5615,patrick paroubek,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"In this paper, we present the founding elements of a formal model of the evaluation paradigm in natural language processing. We propose an abstract model of objective quantitative evaluation based on rough sets, as well as the notion of potential performance space for describing the performance variations corresponding to the ambiguity present in hypothesis data produced by a computer program, when comparing it to the reference data created by humans. A formal model of the evaluation paradigm will be useful for comparing evaluations protocols, investigating evaluation constraint relaxation and getting a better understanding of the evaluation paradigm, provided it is general enough to be able to represent any natural language processing task."
2011.jeptalnrecital-long.29,Classification en polarit{\\'e} de sentiments avec une repr{\\'e}sentation textuelle {\\`a} base de sous-graphes d{'}arbres de d{\\'e}pendances (Sentiment polarity classification using a textual representation based on subgraphs of dependency trees),2011,-1,-1,2,1,44926,alexander pak,Actes de la 18e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Les approches classiques {\`a} base de n-grammes en analyse supervis{\'e}e de sentiments ne peuvent pas correctement identifier les expressions complexes de sentiments {\`a} cause de la perte d{'}information induite par l{'}approche Â« sac de mots Â» utilis{\'e}e pour repr{\'e}senter les textes. Dans notre approche, nous avons recours {\`a} des sous-graphes extraits des graphes de d{\'e}pendances syntaxiques comme traits pour la classification de sentiments. Nous repr{\'e}sentons un texte par un vecteur compos{\'e} de ces sous-graphes syntaxiques et nous employons un classifieurs SVM {\'e}tat-de-l{'}art pour identifier la polarit{\'e} d{'}un texte. Nos {\'e}valuations exp{\'e}rimentales sur des critiques de jeux vid{\'e}o montrent que notre approche {\`a} base de sous-graphes est meilleure que les approches standard {\`a} mod{\`e}les Â« sac de mots Â» et n-grammes. Dans cet article nous avons travaill{\'e} sur le fran{\c{c}}ais, mais notre approche peut facilement {\^e}tre adapt{\'e}e {\`a} d{'}autres langues."
S10-1097,{T}witter Based System: Using {T}witter for Disambiguating Sentiment Ambiguous Adjectives,2010,7,74,2,1,44926,alexander pak,Proceedings of the 5th International Workshop on Semantic Evaluation,0,"In this paper, we describe our system which participated in the SemEval 2010 task of disambiguating sentiment ambiguous adjectives for Chinese. Our system uses text messages from Twitter, a popular microblogging platform, for building a dataset of emotional texts. Using the built dataset, the system classifies the meaning of adjectives into positive or negative sentiment polarity according to the given context. Our approach is fully automatic. It does not require any additional hand-built language resources and it is language independent."
pak-paroubek-2010-twitter,{T}witter as a Corpus for Sentiment Analysis and Opinion Mining,2010,15,1628,2,1,44926,alexander pak,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"Microblogging today has become a very popular communication tool among Internet users. Millions of users share opinions on different aspects of life everyday. Therefore microblogging web-sites are rich sources of data for opinion mining and sentiment analysis. Because microblogging has appeared relatively recently, there are a few research works that were devoted to this topic. In our paper, we focus on using Twitter, the most popular microblogging platform, for the task of sentiment analysis. We show how to automatically collect a corpus for sentiment analysis and opinion mining purposes. We perform linguistic analysis of the collected corpus and explain discovered phenomena. Using the corpus, we build a sentiment classifier, that is able to determine positive, negative and neutral sentiments for a document. Experimental evaluations show that our proposed techniques are efficient and performs better than previously proposed methods. In our research, we worked with English, however, the proposed technique can be used with any other language."
paroubek-etal-2010-annotations,Annotations for Opinion Mining Evaluation in the Industrial Context of the {DOXA} project,2010,16,12,1,1,5615,patrick paroubek,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"After presenting opinion and sentiment analysis state of the art and the DOXA project, we review the few evaluation campaigns that have dealt in the past with opinion mining. Then we present the two level opinion and sentiment model that we will use for evaluation in the DOXA project and the annotation interface we use for hand annotating a reference corpus. We then present the corpus which will be used on DOXA and report on the hand-annotation task on a corpus of comments on video games and the solution adopted to obtain a sufficient level of inter-annotator agreement."
paroubek-etal-2010-second,The Second Evaluation Campaign of {PASSAGE} on Parsing of {F}rench,2010,-1,-1,1,1,5615,patrick paroubek,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,None
vilnat-etal-2010-passage,{PASSAGE} Syntactic Representation: a Minimal Common Ground for Evaluation,2010,14,9,2,0.806658,15243,anne vilnat,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"The current PASSAGE syntactic representation is the result of 9 years of constant evolution with the aim of providing a common ground for evaluating parsers of French whatever their type and supporting theory. In this paper we present the latest developments concerning the formalism and show first through a review of basic linguistic phenomena that it is a plausible minimal common ground for representing French syntax in the context of generic black box quantitative objective evaluation. For the phenomena reviewed, which include: the notion of syntactic head, apposition, control and coordination, we explain how PASSAGE representation relates to other syntactic representation schemes for French and English, slightly extending the annotation to address English when needed. Second, we describe the XML format chosen for PASSAGE and show that it is compliant with the latest propositions in terms of linguistic annotation standard. We conclude discussing the influence that corpus-based evaluation has on the characteristics of syntactic representation when willing to assess the performance of any kind of parser."
2010.jeptalnrecital-court.26,Construction d{'}un lexique affectif pour le fran{\\c{c}}ais {\\`a} partir de {T}witter,2010,-1,-1,2,1,44926,alexander pak,Actes de la 17e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles courts,0,"Un lexique affectif est un outil utile pour l{'}{\'e}tude des {\'e}motions ainsi que pour la fouille d{'}opinion et l{'}analyse des sentiments. Un tel lexique contient des listes de mots annot{\'e}s avec leurs {\'e}valuations {\'e}motionnelles. Il existe un certain nombre de lexiques affectifs pour la langue anglaise, espagnole, allemande, mais tr{\`e}s peu pour le fran{\c{c}}ais. Un travail de longue haleine est n{\'e}cessaire pour construire et enrichir un lexique affectif. Nous proposons d{'}utiliser Twitter, la plateforme la plus populaire de microblogging de nos jours, pour recueillir un corpus de textes {\'e}motionnels en fran{\c{c}}ais. En utilisant l{'}ensemble des donn{\'e}es recueillies, nous avons estim{\'e} les normes affectives de chaque mot. Nous utilisons les donn{\'e}es de la Norme Affective desMots Anglais (ANEW, Affective Norms of EnglishWords) que nous avons traduite en fran{\c{c}}ais afin de valider nos r{\'e}sultats. Les valeurs du coefficient tau de Kendall et du coefficient de corr{\'e}lation de rang de Spearman montrent que nos scores estim{\'e}s sont en accord avec les scores ANEW."
W08-1306,Large Scale Production of Syntactic Annotations to Move Forward,2008,19,1,5,1,15243,anne vilnat,Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation,0,"This article presents the methodology of the PASSAGE project, aiming at syntactically annotating large corpora by composing annotations. It introduces the annotation format and the syntactic annotation specifications. It describes an important component of the methodolgy, namely an WEB-based evaluation service, deployed in the context of the first PASSAGE parser evaluation campaign."
W08-1204,Human Judgement as a Parameter in Evaluation Campaigns,2008,7,1,4,0,47756,jeanbaptiste berthelin,Coling 2008: Proceedings of the workshop on Human Judgements in Computational Linguistics,0,"The relevance of human judgment in an evaluation campaign is illustrated here through the DEFT text mining campaigns.n n In a first step, testing a topic for a campaign among a limited number of human evaluators informs us about the feasibility of a task. This information comes from the results obtained by the judges, as well as from their personal impressions after passing the test.n n In a second step, results from individual judges, as well as their pairwise matching, are used in order to adjust the task (choice of a marking scale for DEFT'07 and selection of topical categories for DEFT'08).n n Finally, the mutual comparison of competitors' results, at the end of the evaluation campaign, confirms the choices we made at its starting point, and provides means to redefine the task when we shall launch a future campaign based on the same topic."
adda-decker-etal-2008-annotation,Annotation and analysis of overlapping speech in political interviews,2008,11,16,4,0,14731,martine addadecker,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"Looking for a better understanding of spontaneous speech-related phenomena and to improve automatic speech recognition (ASR), we present here a study on the relationship between the occurrence of overlapping speech segments and disfluencies (filled pauses, repetitions, revisions) in political interviews. First we present our data, and our overlap annotation scheme. We detail our choice of overlapping tags and our definition of disfluencies; the observed ratios of the different overlapping tags are examined, as well as their correlation with of the speaker role and propose two measures to characterise speakersÂ interacting attitude: the attack/resist ratio and the attack density. We then study the relationship between the overlapping speech segments and the disfluencies in our corpus, before concluding on the perspectives that our experiments offer."
paroubek-etal-2008-easy,"{EASY}, Evaluation of Parsers of {F}rench: what are the Results?",2008,10,9,1,1,5615,patrick paroubek,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"This paper presents EASY, which has been the first campaign evaluating syntactic parsers on all the common syntactic phenomena and a large set of dependency relations. The language analyzed was French. During this campaign, an annotation scheme has been elaborated with the different actors: participants and corpus providers; then a corpus made of several syntactic materials has been built and annotated: it reflects a great variety of linguistic styles (from literature to oral transcriptions, and from newspapers to medical texts). Both corpus and annotation scheme are here briefly presented. Moreover, evaluation measures are explained and detailed results are given. The results of the 15 parsers coming from 12 teams are analyzed. To conclude, a first experiment aiming to combine the outputs of the different systems is shown."
villemonte-de-la-clergerie-etal-2008-passage,{PASSAGE}: from {F}rench Parser Evaluation to Large Sized Treebank,2008,11,23,5,0,17825,eric clergerie,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"In this paper we present the PASSAGE project which aims at building automatically a French Treebank of large size by combining the output of several parsers, using the EASY annotation scheme. We present also the results of the of the first evaluation campaign of the project and the preliminary results we have obtained with our ROVER procedure for combining parsers automatically."
2007.jeptalnrecital-poster.24,Les r{\\'e}sultats de la campagne {EASY} d{'}{\\'e}valuation des analyseurs syntaxiques du fran{\\c{c}}ais,2007,-1,-1,1,1,5615,patrick paroubek,Actes de la 14{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Posters,0,"Dans cet article, nous pr{\'e}sentons les r{\'e}sultats de la campagne d{'}{\'e}valuation EASY des analyseurs syntaxiques du fran{\c{c}}ais. EASY a {\'e}t{\'e} la toute premi{\`e}re campagne d{'}{\'e}valuation comparative des analyseurs syntaxiques du fran{\c{c}}ais en mode bo{\^\i}te noire utilisant des mesures objectives quantitatives. EASY fait partie du programme TECHNOLANGUE du Minist{\`e}re d{\'e}l{\'e}gu{\'e} {\`a} la Recherche et {\`a} l{'}{\'E}ducation, avec le soutien du minist{\`e}re de d{\'e}l{\'e}gu{\'e} {\`a} l{'}industrie et du minist{\`e}re de la culture et de la communication. Nous exposons tout d{'}abord la position de la campagne par rapport aux autres projets d{'}{\'e}valuation en analyse syntaxique, puis nous pr{\'e}sentos son d{\'e}roulement, et donnons les r{\'e}sultats des 15 analyseurs participants en fonction des diff{\'e}rents types de corpus et des diff{\'e}rentes annotations (constituants et relations). Nous proposons ensuite un ensemble de le{\c{c}}ons {\`a} tirer de cette campagne, en particulier {\`a} propos du protocole d{'}{\'e}valuation, de la d{\'e}finition de la segmentation en unit{\'e}s linguistiques, du formalisme et des activit{\'e}s d{'}annotation, des crit{\`e}res de qualit{\'e} des donn{\'e}es, des annotations et des r{\'e}sultats, et finalement de la notion de r{\'e}f{\'e}rence en analyse syntaxique. Nous concluons en pr{\'e}sentant comment les r{\'e}sultats d{'}EASY se prolongent dans le projet PASSAGE (ANR-06-MDCA-013) qui vient de d{\'e}buter et dont l{'}objectif est d{'}{\'e}tiqueter un grand corpus par plusieurs analyseurs en les combinant selon des param{\`e}tres issus de l{'}{\'e}valuation."
paroubek-etal-2006-data,"Data, Annotations and Measures in {EASY} the Evaluation Campaign for Parsers of {F}rench.",2006,15,31,1,1,5615,patrick paroubek,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"This paper presents the protocol of EASY the evaluation campaign for syntactic parsers of French in the EVALDA project of the TECHNOLANGUE program. We describe the participants, the corpus and its genre partitioning, the annotation scheme, which allows for the annotation of both constituents and relations, the evaluation methodology and, as an illustration, the results obtained by one participant on half of the corpus."
vilnat-etal-2004-ongoing,The Ongoing Evaluation Campaign of Syntactic Parsing of {F}rench: {EASY},2004,5,10,2,1,15243,anne vilnat,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"This paper presents EASY (Evaluation of Analyzers of SYntax), an ongoing evaluation campaign of syntactic parsing of French, a subproject of EVALDA in the French TECHNOLANGUE program. After presenting the elaboration of the annotation formalism, we describe the corpus building steps, the annotation tools, the evaluation measures and finally, plans to produce a validated large linguistic resource, syntactically annotated"
devillers-etal-2004-french,The {F}rench {MEDIA}/{EVALDA} Project: the Evaluation of the Understanding Capability of Spoken Language Dialogue Systems,2004,9,30,4,1,39952,laurence devillers,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,The aim of the MEDIA project is to design and test a methodology for the evaluat ion of context-dependent and independent spoken dialogue systems. We propose an evaluation paradigm based on the use of test suites from real-world corpora and a common semantic representation and common metrics. This paradigm should allow us to diagnose the context-sensitive understanding capability of dialogue system s. This paradigm will be used within an evaluation campaign involving several si tes all of which will carry out the task of querying information from a database .
2004.jeptalnrecital-poster.17,Apprentissage collectif et lexique,2004,-1,-1,2,0,52459,julien poudade,Actes de la 11{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Posters,0,"Cet article pr{\'e}sente l{'}influence de la zone de travail que poss{\`e}de une entit{\'e} logicielle pour lui permettre de pr{\'e}dire l{'}{\'e}tat futur de son environnement, sur la constitution d{'}un lexique partag{\'e} par les diff{\'e}rents membres d{'}une population, dans le cadre d{'}une variante {``}du jeu de d{\'e}signation{''} (naming game)."
2004.jeptalnrecital-long.32,Annoter en constituants pour {\\'e}valuer des analyseurs syntaxiques,2004,-1,-1,3,1,15243,anne vilnat,Actes de la 11{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Cet article pr{\'e}sente l{'}annotation en constituants men{\'e}e dans le cadre d{'}un protocole d{'}{\'e}valuation des analyseurs syntaxiques (mis au point dans le pr{\'e}-projet PEAS, puis dans le projet EASY). Le choix des constituants est d{\'e}crit en d{\'e}tail et une premi{\`e}re {\'e}valuation effectu{\'e}e {\`a} partir des r{\'e}sultats de deux analyseurs est donn{\'e}e."
W03-2802,The {PEACE} {SLDS} understanding evaluation paradigm of the {F}rench {MEDIA} campaign,2003,17,4,3,1,39952,laurence devillers,"Proceedings of the {EACL} 2003 Workshop on Evaluation Initiatives in Natural Language Processing: are evaluation methods, metrics and resources reusable?",0,"This paper presents a paradigm for evaluating the context-sensitive understanding capability of any spoken language dialog system: PEACE (French acronym for Paradigme d'Evaluation Automatique de la Comprehension hors et En-contexte). This paradigm will be the basis of the French Technolangue MEDIA project, in which dialog systems from various academic and industrial sites will be tested in an evaluation campaign coordinated by ELRA/ELDA (over the next two years). Despite previous efforts such as Eagles, Disc, Aupelf Arcb2 or the ongoing American Darpa Communicator project, the spoken dialog community still lacks common reference tasks and widely agreed upon methods for comparing and diagnosing systems and techniques. Automatic solutions are nowadays being sought both to make possible the comparison of different approaches by means of reliable indicators with generic evaluation methodologies and also to reduce system development costs. However achieving independence from both the dialog system and the task performed seems to be more and more a utopia. Most of the evaluations have up to now either tackled the system as a whole, or based the measurements on dialog-context-free information. The PEACE proposal aims at bypassing some of these shortcomings by extracting, from real dialog corpora, test sets that synthesize contextual information."
E03-1085,"{PEAS}, the first instantiation of a comparative framework for evaluating parsers of {F}rench",2003,9,11,5,1,52094,veronique gendner,10th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"This paper presents PEAS, the first comparative evaluation framework for parsers of French whose annotation formalism allows the annotation of both constituents and functional relations. A test corpus containing an assortment of different text types has been built and part of it has been manually annotated. Precision/Recall and crossing brackets metrics will be adapted to our formalism and applied to the parses produced by one parser from academia and another one from industry in order to validate the framework."
gendner-etal-2002-protocol,A Protocol for Evaluating Analyzers of Syntax ({PEAS}),2002,17,3,5,1,52094,veronique gendner,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"Providing a comparative framework for parsers is a task that has already been tried in the past, e.g. (Abeille, 1991), (Atwell and Sutcliffe, 1997), (Black et al., 1991), and studied in the literature (Black, 1993), (Black, 1994), (Carroll et al., 1998), (Gaizauskas et al., 1998), (WEPS-98, ), (Mengel and Lezius, 2000), but mainly for English. In this paper, we present PEAS: a Protocol for Evaluating Analyzers of Syntax (in French: Protocole dxe2x80x99Evaluation pour les Analyseurs Syntaxiques), based on an ongoing experiment at LIMSI which aims at developing and testing a generic quantitative black-box evaluation protocol for parsers of French. Two fully operational parsers will be used to test the evaluation protocol; they are: the parser (Giguet and Vergne, 1997) developed at GREYC (Caen University) and the latest version of the parser developed at Rank Xerox Research Center in Grenoble (Ait-Mokhtar and Chanod, 1997)"
W01-0901,Introduction,2001,0,2,1,1,5615,patrick paroubek,Proceedings of the {ACL} 2001 Workshop on Evaluation Methodologies for Language and Dialogue Systems,0,None
paroubek-2000-language,Language Resources as by-Product of Evaluation: The {MULTITAG} Example,2000,12,14,1,1,5615,patrick paroubek,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,"In this paper, we show how the paradigm of evaluation can function as language resource producer for high quality and low cost validated language resources. First the paradigm of evaluation is presented, the main points of its history are recalled, from the first deployment that took place in the USA during the DARPA/NIST evaluation campaigns, up to latest efforts in Europe (SENSEVAL2/ROMANSEVAL2, CLEF, CLASS etc.). Then the principle behind the method used to produce high-quality validated language at low cost from the byproducts of an evaluation campaign is exposed. It was inspired by the experiments (Recognizer Output Voting Error Recognition) performed during speech recognition evaluation campaigns in the USA and consists of combining the outputs of the participating systems with a simple voting strategy to obtain higher performance results. Here we make a link with the existing strategies for system combination studied in machine learning. As an illustration we describe how the MULTITAG project funded by CNRS has built from the by-products of the GRACE evaluation campaign (French Part-Of-Speech tagging system evaluation campaign) a corpus of around 1 million words, annotated with a fine grained tagset derived from the EAGLES and MULTEXT projects. A brief presentation of the state of the art in Part-Of-Speech (POS) tagging and of the problem posed by its evaluation is given at the beginning, then the corpus itself is presented along with the procedure used to produce and validate it. In particular, the cost reduction brought by using this method instead of more classical methods is presented and its generalization to other control task is discussed in the conclusion. 1. The paradigm of Evaluation Comparative evaluation in language engineering has been used as a basic paradigm in the USA DARPA program on human language technology since 1984. Activities similar in kind, have been pursued in Europe, both at national and at European level, but on a smaller scale and over a limited time (Mariani and Paroubek, 1999). The latest efforts concerning evaluation in Europe are CLEF (Cross Language Text Retrieval System Evaluation in collaboration with NIST and TREC conference), SENSEVAL2/ROMANSEVAL-2 (Kilgarriff, 1998) and CLASS (evaluation across FP5 project clusters). Comparative evaluation is a paradigm in which a set of participants compare the results of their systems using the same or similar control tasks (Bernsen et al., 1999) and related data with metrics that are agreed upon. More precisely, Comparative evaluation consists in (1) choosing or creating a control task, (2) in gathering system or component developers and integrators who are interested in testing their systems against those of others, (3) in organizing an evaluation campaign which necessarily involves distributing linguistic data for training and testing the systems, and (4) in defining the protocol and the metrics which will be used in the results assessment. A control task is the function that the participating systems have to perform during an evaluation together with the conditions under which this function must be performed (e.g. for parser evaluation, a control task can be the bracketing of the constituents). Every deployment of the paradigm of evaluation in the field of Language Engineering entails the production of linguistic data: the organizers build the reference and test data sets and the participants apply their systems on test data to produce evaluation data. When a quantitative black-box (Sparck-Jones and Galliers, 1995) evaluation methodology is applied the data produced is generally abundant and could easily be re-used as training material if the cost of filtering out the errors was not so high. 2. Combining to Improve In machine learning, it is well known that ensemble methods or committees of learning machines can often improve the performance of a system in comparison to a single learning machine. A very promising algorithm based on this principle now under investigation is Ada boost (Schwenk, 1999). In the same field, people have applied for a long time xe2x80x9cwinner take allxe2x80x9d strategies to combine, inside the same system, the output of several basic processing units (Simpson, 1990). In the course of its evaluation program on speech recognition (S., 1998), NIST developed the ROVER (Recognizer Output Voting Voting Error Reduction) (Fiscus, 1997), to produce a composite Automatic Speech Recognition system output from the output of several ASR systems. Such composite system has an error rate inferior to the one of any of its components. In the ROVER, the output of several ASR systems is first combined into a single transition network using a modified version of the dynamic programming alignment technique used by NIST to score ASR systems1. This network is then explored and a simple voting strategy (highest number of votes) is used to select the best scoring word at each decision point. In (Fiscus, 1997), NIST reports a incremental 5.6% Word Error Rate reduction (12.5% relative) using voting by frequency of occurrence and maximum confidence (the output of the ASR systems was annotated with confidence measures (Chase, 1997)). Concerning, POS tagging, the principle of combination has been used in the past by (Marquez and Padro, 1998) who combines two taggers to annotate a corpus and by (Tufis, 1999) who uses several versions of same tagger but trained on different data. The SCLITE tool is freely available from http://www.itl.nist.gov/iaui/894.01/software.htm 3. The GRACE POS tagging evaluation campaign and its data. GRACE(Adda et al., 1999) was the first large scale evaluation campaign for Part-Of-Speech tagging for the French language. It was part of the French program CCIIL (Cognition, Intelligent Communication and Language Engineering), jointlypromoted by the Engineering Sciences and Human Sciences departments of the CNRS. The call for tenders was published in November 1995 and the first year has been devoted to bootstrapping the program by defining and installing the different organization committees. From the participants point of view, GRACE was made of 3 phases: training, dry-run and test. The first one was used by the participant to calibrate their systems on untagged data, the two others were complete runs of the evaluation protocol were the participants had to tag a large amount of text and to provide a mapping between their tagset and the reference tagset which had been derived with their collaboration from the EAGLES format (Leech and Wilson, 1995). The training corpus was distributed globally to all the participants in January 1996, while the dry run corpus was distributed individually to each participant in an encrypted form during the fall of 1996. The results were discussed during a workshop restricted to the participants, a satellite event to the Journees Scientifiques et Techniques du Reseau FRANCIL, in April 1997 (Adda. et al., 1997). The test corpus was distributed in the same manner as for the dry run, at the end of December 1997. The preliminary results of the tests were discussed with the participants in a workshop in May 1998. The final results were disclosed on the WEB2 during fall of 1998 as soon as they had been validated by the organizers (cross validation with two different processing chains based on different algorithms and developed at two different sites) and the participants. At the beginning there were 18 participants from 5 different countries (CA, USA, D, CH, FR), from both public research and industry, and 3 evaluators EPFL, INaLF-CNRS and Limsi-CNRS.The 2 corpus providers were Limsi and INaLF. Out of the 21 initial participants, 17 only took part in the dry run and only 13 completed the tests. The size of the training corpus was around 10 million words of untagged texts, evenly distributed between literary works and newspaper articles. For the dry run, the participants tagged a corpus of roughly 450,000 words with a similar genre distributionand the performance measure was computed over 20,000 words to which a reference description had been manually assigned. For the tests, the participants had to mark a corpus of 650,000 words and the measure was taken over 40,000 words. GRACE used the quantitative black box metrics: Decision and Precision, which were derived especially for GRACE from the metrics used in Information Retrieval (Precision and Recall). Precision measures the ability of a POS tagger to assign a correct tag to a given word form, and Decision measures the capacity of a POS tagger to restrict for a given word form the number of candidate tags with respect to a given tagset. One of the lessons to draw from the GRACE experience, is that ideally, results should be cross-validated with two different processing chains, based on different algorithms http://www.limsi.fr/TLP/grace (when this is possible) and developed at two different sites in order to ensure their accuracy and quality. The evaluation toolkit of GRACE has been packaged as a demonstration by the ELSE project and is freely available3. GRACE proved to be a success; its results are: a better knowledge of the existing systems in each domain and of their state of development; precise evaluation metrics defined in collaboration with the participants; an evaluation toolkit freely available, a new product on the market (one participant decided to add a tagger to his catalogue as a result of his participation); the creation of a community of actors interested in evaluation; and last of all, the initial data to build the new linguistic resource described here."
A92-1030,{XTAG} - A Graphical Workbench for Developing {T}ree-{A}djoining {G}rammars,1992,23,27,1,1,5615,patrick paroubek,Third Conference on Applied Natural Language Processing,0,"We describe a workbench (XTAG) for the development of tree-adjoining grammars and their parsers, and discuss some issues that arise in the design of the graphical interface.Contrary to string rewriting grammars generating trees, the elementary objects manipulated by a tree-adjoining grammar are extended trees (i.e. trees of depth one or more) which capture syntactic information of lexical items. The unique characteristics of tree-adjoining grammars, its elementary objects found in the lexicon (extended trees) and the derivational history of derived trees (also a tree), require a specially crafted interface in which the perspective has shifted from a string-based to a tree-based system. XTAG provides such a graphical interface in which the elementary objects are trees (or tree sets) and not symbols (or strings).The kernel of XTAG is a predictive left to right parser for unification-based tree-adjoining grammar [Schabes, 1991]. XTAG includes a graphical editor for trees, a graphical tree printer, utilities for manipulating and displaying feature structures for unification-based tree-adjoining grammar, facilities for keeping track of the derivational history of TAG trees combined with adjoining and substitution, a parser for unification based tree-adjoining grammars, utilities for defining grammars and lexicons for tree-adjoining grammars, a morphological recognizer for English (75 000 stems deriving 280 000 inflected forms) and a tree-adjoining grammar for English that covers a large range of linguistic phenomena.Considerations of portability, efficiency, homogeneity and ease of maintenance, lead us to the use of Common Lisp without its object language addition and to the use of the X Window interface to Common Lisp (CLX) for the implementation of XTAG.XTAG without the large morphological and syntactic lexicons is public domain software. The large morphological and syntactic lexicons can be obtained through an agreement with ACL's Data Collection Initiative.XTAG runs under Common Lisp and X Window (CLX)."
