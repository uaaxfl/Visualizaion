2005.iwslt-1.8,P05-1066,1,0.16829,"inal (default) 48.8 40.4 33.9 28.9 15.4 final-and 48.5 39.9 35.7 32.4 9.6 grow-diag 49.9 39.0 27.7 31.7 8.1 grow 39.9 39.1 13.5 32.8 15.4 intersect 47.5 45.1 35.4 34.6 15.2 Table 2: BLEU scores for systems trained using different alignment methods We also carried out experiments to optimise GIZA++ parameters, but this did not yield any significant improvements. We would like to re-visit these experiments at some future time, since we did not have sufficient time for a thorough treatment at this time. We also tried to deal with language-specific problems, as previously done for German–English (Collins et al., 2005). We created hand-written rules that move the Japanese verb from the end of the sentence to the beginning. However, we could not consistently achieve improvements using these rules. Since we did not have a part-of-speech tagger for Japanese, we had to rely on the assumption that the last word of a Japanese sentence is the verb. We did not apply these rules in our official submission. 3.1 Optimising Word Alignment Our experience with GIZA++ alignments has been that IBM Model training performs poorly for source words that occur only once in the training corpus. These words are often incorrectly"
2005.iwslt-1.8,P04-1083,0,0.0145603,"s. 1 Introduction The statistical machine translation group at the University of Edinburgh has been focused on open domain text translation, so we welcomed the challenge to work on the IWSLT 2005 limited domain speech translation task. We participated in the transcription translation tasks for all five language pairs, using only the supplied corpora. Our MT system was originally developed for translation of European parliament texts from German to English (Koehn et al., 2003). We extended the system while working on the DARPA challenges to translate Chinese and Arabic news texts into English (Koehn, 2004a; Koehn et al., 2005). Now, we were faced with the challenge of speech data in mostly Asian languages. The translation of transcribed speech differs in many ways from our traditional translation scenario: Much less training data is available, the domain is more limited, and the text style is very different — short questions and statements. In some respect, the task is easier, since smaller training corpora result in faster training times for the system. But it also meant that we had to re-examine various components of our system. In this paper we present an overview of our current out-of-the-"
2005.iwslt-1.8,koen-2004-pharaoh,0,0.0508616,"s. 1 Introduction The statistical machine translation group at the University of Edinburgh has been focused on open domain text translation, so we welcomed the challenge to work on the IWSLT 2005 limited domain speech translation task. We participated in the transcription translation tasks for all five language pairs, using only the supplied corpora. Our MT system was originally developed for translation of European parliament texts from German to English (Koehn et al., 2003). We extended the system while working on the DARPA challenges to translate Chinese and Arabic news texts into English (Koehn, 2004a; Koehn et al., 2005). Now, we were faced with the challenge of speech data in mostly Asian languages. The translation of transcribed speech differs in many ways from our traditional translation scenario: Much less training data is available, the domain is more limited, and the text style is very different — short questions and statements. In some respect, the task is easier, since smaller training corpora result in faster training times for the system. But it also meant that we had to re-examine various components of our system. In this paper we present an overview of our current out-of-the-"
2005.iwslt-1.8,N03-1017,1,0.060637,"ranscription track. We achieved the highest BLEU score in 2 out of 5 language pairs and had competitive results for the other language pairs. 1 Introduction The statistical machine translation group at the University of Edinburgh has been focused on open domain text translation, so we welcomed the challenge to work on the IWSLT 2005 limited domain speech translation task. We participated in the transcription translation tasks for all five language pairs, using only the supplied corpora. Our MT system was originally developed for translation of European parliament texts from German to English (Koehn et al., 2003). We extended the system while working on the DARPA challenges to translate Chinese and Arabic news texts into English (Koehn, 2004a; Koehn et al., 2005). Now, we were faced with the challenge of speech data in mostly Asian languages. The translation of transcribed speech differs in many ways from our traditional translation scenario: Much less training data is available, the domain is more limited, and the text style is very different — short questions and statements. In some respect, the task is easier, since smaller training corpora result in faster training times for the system. But it als"
2005.iwslt-1.8,W02-1018,0,0.0130598,"code of the grow-diag-final method to symmetrise word alignments. See Section 2.3 for variations of this method. See Figure 5 for an illustration. Note that unaligned words may be included within and at the border of extracted phrase pairs (third example in Figure 5). Each phrase pair, however, must include at least one alignment point. Using word-level alignments to induce phrasebased translation models is common practise in the statistical machine translation community. It has been adopted by most groups participating in the NIST MT Evaluation (Lee and Przybocki, 2005). In contrast to this, Marcu and Wong (2002) have defined a method for directly estimating phrasal translation models from parallel corpora, rather than using heuristic methods to induce phrase alignments from word alignments. Their joint probability phrase-based model is computationally demanding, and as such has not been applied to large data sets. Our group has been implementing a scalable version of the joint probability model (Mayne, 2005), and we hope to submit it as a contrastive system in next year’s IWSLT. 2.5 GIZA++ alignments. Three neighbouring points are added. The alignment point between did and a is added in the grow(-dia"
2005.iwslt-1.8,P03-1021,0,0.0327751,"ght exists • discontinuous: no alignment points to the top left or top right Given these counts, we can learn probability distributions of the form: pr (orientation|¯ e, f¯) (5) For the estimation of the probability distribution, we smooth the collected counts. This lexicalised reordering model is motivated by similar work by Tillmann (2004). Discriminative Training Recall that the components of our machine translation system are combined in a log-linear way. The weight of the feature functions, or model components, is set by minimum error rate training. We reimplemented a method suggested by Och (2003). In short, we optimise the value of the parameter weights λm by iteratively: (a) running the decoder with a currently best weight setting, (b) extracting an n-best list of possible translations, and (c) finding a better weight setting that re-ranks the n-best-list, so that a better translation score is obtained. To score translation quality, we employ the BLEU score (Papineni et al., 2002). The search for the best weight setting is a line search for each λm , which is repeated until no improvement can be achieved. We thank David Chiang of the University of Maryland for providing us with a fas"
2005.iwslt-1.8,J03-1002,0,0.0170359,"in the corpus, extracting phrase pairs that are consistent with the word alignment, and then assigning probabilities (or scores) to the obtained phrase translations. 2.3 Figure 2: Obtaining a high precision, low recall word alignment by intersecting two GIZA++ alignments Word Alignment Word alignments are obtained by first using the GIZA++ toolkit in both translation directions and then symmetrising the two alignments. Since the IBM Models implemented in GIZA++ are not able to map one target (English) word to multiple source (foreign) words, the method of symmetrising — called refined method (Och and Ney, 2003) — effectively overcomes this deficiency. Figure 2 shows the first step in the symmetrisation process: The intersection of the two GIZA++ alignments is taken. Only word alignment points that occur in both alignments are preserved. This is the intersection alignment. In a second step, additional alignment points are added. Only alignment points that are in either of the two GIZA++ alignments (or, in the union of these alignments) are considered. In the growing step, potential alignment points that connect currently unaligned words and that neighbour established alignment points are added. Neigh"
2005.iwslt-1.8,P02-1040,0,0.110837,"the components of our machine translation system are combined in a log-linear way. The weight of the feature functions, or model components, is set by minimum error rate training. We reimplemented a method suggested by Och (2003). In short, we optimise the value of the parameter weights λm by iteratively: (a) running the decoder with a currently best weight setting, (b) extracting an n-best list of possible translations, and (c) finding a better weight setting that re-ranks the n-best-list, so that a better translation score is obtained. To score translation quality, we employ the BLEU score (Papineni et al., 2002). The search for the best weight setting is a line search for each λm , which is repeated until no improvement can be achieved. We thank David Chiang of the University of Maryland for providing us with a faster version of our implementation. 3 Adaptations to IWSLT’05 Task In a period of one month, we optimised our system to the IWSLT’05 task. We chose to only participate in the transcription task using the supplied data, since we did not have adequate additional resources or tools for these language pairs, and also had not enough time to investigate these. The advantage of limiting ourselves t"
2005.iwslt-1.8,N04-4026,0,0.382449,"nected to the previous phrase at all (discontinuous). See Figure 6 for an illustration. When collecting phrase pairs, can classify them into these three categories based on: • monotone: a word alignment point to the top left exists • swap: an alignment point to the top right exists • discontinuous: no alignment points to the top left or top right Given these counts, we can learn probability distributions of the form: pr (orientation|¯ e, f¯) (5) For the estimation of the probability distribution, we smooth the collected counts. This lexicalised reordering model is motivated by similar work by Tillmann (2004). Discriminative Training Recall that the components of our machine translation system are combined in a log-linear way. The weight of the feature functions, or model components, is set by minimum error rate training. We reimplemented a method suggested by Och (2003). In short, we optimise the value of the parameter weights λm by iteratively: (a) running the decoder with a currently best weight setting, (b) extracting an n-best list of possible translations, and (c) finding a better weight setting that re-ranks the n-best-list, so that a better translation score is obtained. To score translati"
2005.mtsummit-papers.11,J93-1004,0,0.123086,"Missing"
2005.mtsummit-papers.11,koen-2004-pharaoh,0,0.115417,"release will include data up to early 2005 and will have better tokenisation. For more details, please check the website. 3 Most systems are largely language-independent, and building a SMT system for a new language pair is mostly a matter of availability of parallel texts. Our efforts to explore open-domain German– English SMT led us to collecting data from the European Parliament. Incidentally, the existence of translations in 11 languages now enabled us to build translation systems for all 110 language pairs. 110 SMT Systems Our SMT system [Koehn et al., 2003] includes the decoder Pharaoh [Koehn, 2004], which is freely available for research purposes3 . Training 110 systems took about 3 weeks on a 16-node Linux cluster. We evaluated the quality of the system with the widely used BLEU metric [Papineni et al., 2002], which measures overlap with a reference translation. The prevailing methodology in statistical machine translation (SMT) has progressed from the initial word-based IBM Models [Brown et al., 1993] to current phrase-based models [Koehn et al., 2003]. To describe the latter quickly: When translating a sentence, source language phrases (any sequences of words) are mapped into phrase"
2005.mtsummit-papers.11,W05-0820,1,0.132328,"aid in scoring alternative translations. Translation speed in our case is a few seconds per sentence. Fuelled by annual competitions and an active research community, we can observe rapid progress We tested on a 2000 sentences held-out test set, which is drawn from text from sessions that took part the last quarter of the year 2000. These sentences are aligned across all 11 languages, so when translation the, say, French sentences into Danish, we can compare the output against the Danish set of sentences. The same test set was used in a shared task at the 2005 ACL Workshop on Parallel Texts [Koehn, 2005]. The scores for the 110 systems are displayed in Table 2. According to these numbers, the easiest translation direction is Spanish to French (BLEU score of 40.2), the hardest Dutch to Finnish (10.3). 3 Available online at licensed-sw/pharaoh/ 82 http://www.isi.edu/ Source Language da de el en es fr fi it nl pt sv da 22.3 22.7 25.2 24.1 23.7 20.0 21.4 20.5 23.2 30.3 de 18.4 17.4 17.6 18.2 18.5 14.5 16.9 18.3 18.2 18.9 el 21.1 20.7 23.2 28.3 26.1 18.2 24.8 17.4 26.4 22.8 en 28.5 25.3 27.2 30.5 30.0 21.8 27.8 23.0 30.1 30.2 Target Language es fr fi 26.4 28.7 14.2 25.4 27.7 11.8 31.2 32.1 11.4 3"
2005.mtsummit-papers.11,P02-1040,0,0.144463,"Missing"
2005.mtsummit-papers.11,N03-1017,1,0.213124,"0 million words per language. A forthcoming third release will include data up to early 2005 and will have better tokenisation. For more details, please check the website. 3 Most systems are largely language-independent, and building a SMT system for a new language pair is mostly a matter of availability of parallel texts. Our efforts to explore open-domain German– English SMT led us to collecting data from the European Parliament. Incidentally, the existence of translations in 11 languages now enabled us to build translation systems for all 110 language pairs. 110 SMT Systems Our SMT system [Koehn et al., 2003] includes the decoder Pharaoh [Koehn, 2004], which is freely available for research purposes3 . Training 110 systems took about 3 weeks on a 16-node Linux cluster. We evaluated the quality of the system with the widely used BLEU metric [Papineni et al., 2002], which measures overlap with a reference translation. The prevailing methodology in statistical machine translation (SMT) has progressed from the initial word-based IBM Models [Brown et al., 1993] to current phrase-based models [Koehn et al., 2003]. To describe the latter quickly: When translating a sentence, source language phrases (any"
2005.mtsummit-papers.11,P99-1068,0,0.121494,"Missing"
2005.mtsummit-papers.11,J99-1003,0,0.100669,"Missing"
2005.mtsummit-papers.11,A97-1004,0,0.0916588,"rom the document aligned files is stripped out. In the current release, the size of the sentence aligned corpus is roughly 30 million words in one million sentences per language. Fore more detailed statistics, see Table 1. word following a period (“ca. three thousand men”), which indicates an abbreviation and not an end of a sentence. There has been extensive work on empirical methods to learn sentence breaking. See for instance the work on SATZ [Palmer and Hearst, 1997]. Various machine learning methods can be applied to this problem, such as decision trees [Riley, 1989] and maximum entropy [Reynar and Ratnaparkhi, 1997]. Issues with tokenisation include the English merging of words such as in “can’t” (which we transform to “can not”), or the separation of possessive markers (“the man’s” becomes “the man ’s”). We do not perform any specialised treatment for other languages than English at this point. In future, we would like to employ a tokenisation scheme that matches the Penn treebank standard. Currently, our provided scripts allow external tokenisation methods. For training a statistical machine translation system, usually all words are lowercased to eliminate the differences between different spelling of"
2005.mtsummit-papers.11,H89-2048,0,0.122442,"ppings of each other. The markup from the document aligned files is stripped out. In the current release, the size of the sentence aligned corpus is roughly 30 million words in one million sentences per language. Fore more detailed statistics, see Table 1. word following a period (“ca. three thousand men”), which indicates an abbreviation and not an end of a sentence. There has been extensive work on empirical methods to learn sentence breaking. See for instance the work on SATZ [Palmer and Hearst, 1997]. Various machine learning methods can be applied to this problem, such as decision trees [Riley, 1989] and maximum entropy [Reynar and Ratnaparkhi, 1997]. Issues with tokenisation include the English merging of words such as in “can’t” (which we transform to “can not”), or the separation of possessive markers (“the man’s” becomes “the man ’s”). We do not perform any specialised treatment for other languages than English at this point. In future, we would like to employ a tokenisation scheme that matches the Penn treebank standard. Currently, our provided scripts allow external tokenisation methods. For training a statistical machine translation system, usually all words are lowercased to elim"
2005.mtsummit-papers.11,J97-2002,0,\N,Missing
2007.iwslt-1.6,P07-2045,1,0.0262484,"nput sentence f: score(e, f) = exp X λi hi (e, f) (1) i The weights of the components λi are set by minimum error rate training on held-out development data[4]. The basic components used in our experiments are: • two phrase translation probabilities (both p(e|f ) and p(f |e)) • two word translation probabilities (both p(e|f ) and p(f |e)) • phrase count • output word count • language model • distance-based reordering model • lexicalised reordering model For a more detailed description of this model, please refer to [5]. 2.1. The Moses MT system 2.2. Lexicalised reordering The open source Moses[2] MT system was originally developed at the University of Edinburgh. It received a major boost through a 2006 Johns Hopkins workshop, and is now used at several academic institutions as the basic infrastructure for statistical machine translation research. There are various models for reordering words to match the target language’s word order. A simplistic method is distance based reordering, which uses a factor δ n to penalise movements over n words. Moses also implements a more sophisticated method called lexicalised reordering[5] which pays on-Burch, H. Hoang, P. Koehn, M. Osborne, D. Talbot"
2007.iwslt-1.6,N03-1017,1,0.0158882,"summary of the phrasebased statistical machine translation system used for this shared task. We go on to discuss the data sources we used to train the system, and show the results of our analysis of the domains of the development test data sets compared to the corpora used for training. Having explained the framework and data used, in Section 5 we present the results of our experiments in crossdomain adaptation. In Section 6 we describe the experiments we conducted with ASR inputs to our system. 2. Framework The Moses system is an implementation of the phrasebased machine translation approach[3]. In this approach, an input sentence is first split into text chunks (so-called phrases), which are then mapped one-to-one to target phrases using a large phrase translation table. Phrases may be reordered, but typically a maximum movement reordering limit is used. Phrase translation probabilities, reordering probabilities and language model probabilities are combined to give each possible sentence translation a score. The best-scoring translation is searched for by the decoding algorithm and outputted by the system as the best translation. The different system components hi (phrase translati"
2007.iwslt-1.6,P03-1021,0,0.00479999,"ation probabilities, reordering probabilities and language model probabilities are combined to give each possible sentence translation a score. The best-scoring translation is searched for by the decoding algorithm and outputted by the system as the best translation. The different system components hi (phrase translation probabilities, language model, etc.) are combined in a log-linear model to obtain the score for the translation e of an input sentence f: score(e, f) = exp X λi hi (e, f) (1) i The weights of the components λi are set by minimum error rate training on held-out development data[4]. The basic components used in our experiments are: • two phrase translation probabilities (both p(e|f ) and p(f |e)) • two word translation probabilities (both p(e|f ) and p(f |e)) • phrase count • output word count • language model • distance-based reordering model • lexicalised reordering model For a more detailed description of this model, please refer to [5]. 2.1. The Moses MT system 2.2. Lexicalised reordering The open source Moses[2] MT system was originally developed at the University of Edinburgh. It received a major boost through a 2006 Johns Hopkins workshop, and is now used at seve"
2007.iwslt-1.6,2005.iwslt-1.8,1,0.805687,"ombined in a log-linear model to obtain the score for the translation e of an input sentence f: score(e, f) = exp X λi hi (e, f) (1) i The weights of the components λi are set by minimum error rate training on held-out development data[4]. The basic components used in our experiments are: • two phrase translation probabilities (both p(e|f ) and p(f |e)) • two word translation probabilities (both p(e|f ) and p(f |e)) • phrase count • output word count • language model • distance-based reordering model • lexicalised reordering model For a more detailed description of this model, please refer to [5]. 2.1. The Moses MT system 2.2. Lexicalised reordering The open source Moses[2] MT system was originally developed at the University of Edinburgh. It received a major boost through a 2006 Johns Hopkins workshop, and is now used at several academic institutions as the basic infrastructure for statistical machine translation research. There are various models for reordering words to match the target language’s word order. A simplistic method is distance based reordering, which uses a factor δ n to penalise movements over n words. Moses also implements a more sophisticated method called lexicalis"
2007.iwslt-1.6,W07-0717,0,0.0182884,"at the official test set for the evaluation would be in a domain not ideally covered by either corpus, we attempted to make all the data from each separate corpus available to the decoder. This can be looked at as a form of mixture modelling or cross-domain adaptation. There has been extensive research in the area of adapting SMT systems to new domains. Recent work has distinguished between cross-domain adaptation, where the domain of the test data is known ahead of time, and dynamic adaptation, where the system must adapt to the test domain on-the-fly without the ability to tune ahead of time[8]. In both cases, models from each domainspecific corpus are trained separately, and weighted relative to their fit with the test domain. In this case, the domain of the test set (SITAL) is known ahead of time. Given that there was only one set of development test data in the domain of the final test set, we chose to split devset5b into two equal halves, using one half for tuning and one half for testing. We refer to these sets below as devset5b-tune and devset5b-test. To minimise vocabulary shifts between individual dialogue sessions and speakers, we shuffled the sentences before splitting. 5."
2007.iwslt-1.6,2006.iwslt-evaluation.6,0,0.0381423,"Missing"
2007.iwslt-1.6,W07-0702,1,0.807001,"ata in devset5b-tune. 5.2. Corpus combination The simplest way to combine two corpora is to append one to the other and train the system as if the data was from one corIn line with recent approaches to cross-domain adaptation, we take the components created from training each corpus separately and combine them at decoding time within one translation model. Using Moses’ architecture, we can add a second language model and reordering model as additional, separate features in the model. In addition, we can use the multiple alternative decoding paths functionality to utilise multiple phrase tables[10]. We use two decoding paths (one for each corpus), each consisting of only one translation step. Like our previous approach to domain adaptation[11], we maintain two separate phrase tables and language models. Unlike our previous work, we allow for two reordering tables, instead of one combined table as used in the previous system. While duplicating our previous best setup for separate language models and phrase tables, we tested using a BTEC lexicalised reordering table, a Europarl table, a combined table generated during the corpus combination experiment with one copy of each corpus, and fin"
2007.iwslt-1.6,W07-0733,1,0.581293,"ata was from one corIn line with recent approaches to cross-domain adaptation, we take the components created from training each corpus separately and combine them at decoding time within one translation model. Using Moses’ architecture, we can add a second language model and reordering model as additional, separate features in the model. In addition, we can use the multiple alternative decoding paths functionality to utilise multiple phrase tables[10]. We use two decoding paths (one for each corpus), each consisting of only one translation step. Like our previous approach to domain adaptation[11], we maintain two separate phrase tables and language models. Unlike our previous work, we allow for two reordering tables, instead of one combined table as used in the previous system. While duplicating our previous best setup for separate language models and phrase tables, we tested using a BTEC lexicalised reordering table, a Europarl table, a combined table generated during the corpus combination experiment with one copy of each corpus, and finally, two separate lexicalised reordering tables. 5.4. Cross-domain adaptation results The results of each of the approaches described above are pre"
2007.iwslt-1.6,2006.iwslt-evaluation.8,0,\N,Missing
2007.mtsummit-papers.3,W02-1001,0,0.606368,"of the art phrase-based SMT systems use around a dozen features. Recent work has tried to address this problem, with both Liang et al. (2006) and Tillmann and Zhang (2006) presenting online perceptron/perceptron-like training schemes to learn parameters for models with millions of features. Typically, discriminative training algorithms come in two flavours, firstly likelihood-based methods which require feature expectations and secondly, margin-based methods which require either an N-best list of best outputs or a marginal distribution across the graphical structure. The perceptron algorithm (Collins, 2002) is a much simpler alternative as it only requires an arg max computation, which is precisely what the decoder is set out to do. While Liang et al. (2006) employ the standard perceptron algorithm in their work, Tillmann and Zhang (2006) present a variant in which the perceptron update is weighted with a factor dependent on the difference in translation score between the reference and the model guess and the difference in their loss. In this paper, we compare the perceptron algorithm to the Margin Infused Relaxed Algorithm(MIRA) (Crammer and Singer, 2003), a large-margin online algorithm that h"
2007.mtsummit-papers.3,H05-1085,0,0.0145101,"tences each for development and test were translated once into Czech and then back into English by five diferent translators. We decided to use this dataset for 2 main reasons : (a) it is a small corpus therefore allowing us to train several models in a short time and (b) it is richly annotated with linguistic infomation which we would like to incorporate as features of our model in future work. However, one of the major drawbacks of using the PCEDT is that since Czech is a morphologically very rich language and the training set is small, the corpus suffers from severe data sparseness issues (Goldwater and McClosky, 2005). We obtained word alignments by using the GIZA++ toolkit (Och and Ney, 2003) on the training corpus in both translation directions. The two sets of alignments were then symmetrised using the grow-diag-final method previously described in (Koehn et al., 2005) and phrase-pairs consistent with the alignments were extracted. Note that both the perceptron and MIRA are error-driven algorithms in that parameter updates are performed only when the learner is unable to classify a training instance properly. Given that the same training data is being used for phrase extraction and for parameter estimat"
2007.mtsummit-papers.3,N03-1017,1,0.050123,"Missing"
2007.mtsummit-papers.3,2005.iwslt-1.8,1,0.791991,"(b) it is richly annotated with linguistic infomation which we would like to incorporate as features of our model in future work. However, one of the major drawbacks of using the PCEDT is that since Czech is a morphologically very rich language and the training set is small, the corpus suffers from severe data sparseness issues (Goldwater and McClosky, 2005). We obtained word alignments by using the GIZA++ toolkit (Och and Ney, 2003) on the training corpus in both translation directions. The two sets of alignments were then symmetrised using the grow-diag-final method previously described in (Koehn et al., 2005) and phrase-pairs consistent with the alignments were extracted. Note that both the perceptron and MIRA are error-driven algorithms in that parameter updates are performed only when the learner is unable to classify a training instance properly. Given that the same training data is being used for phrase extraction and for parameter estimation, overfitting is possible. We would instead like to make the training enviroment as hard as the testing one. Since Koehn et al. (2003) show that there is not much performance gain in using phrases of length longer than 3 words, we limit our phrase table to"
2007.mtsummit-papers.3,P06-1096,0,0.27379,"Missing"
2007.mtsummit-papers.3,C04-1072,0,0.144804,"able by the model. When the reference is reachable by the model, there might be multiple derivations to go from source to reference string. In this scenario in our current implementation, we pick the highest scoring derivation given the current model as the truth. When the reference is not reachable, we investigate two alternate strategies for selecting the surrogate reference. Both strategies rely on selecting a reachable translation which is closest to the reference as measured by a loss function. 4.1. Loss function As loss function, we have implemented sentence level smoothed BLEU (sBLEU) (Lin and Och, 2004) . Note that since we run our learning algorithm on the parallel training corpus, we only have one reference translation per source sentence. It is unclear how reliable a sentence level BLEU with respect to one reference is. Also while BLEU computes the brevity penalty by aggregating the length of the whole document, the sentence level BLEU computes the brevity penalty purely at the sentence level. The MIRA update rule requires a loss function L(yt , y 0 ) which indicates the penalty to be incurred when the guessed output y0 is proposed instead of the correct output yt . This, in our work is g"
2007.mtsummit-papers.3,P05-1012,0,0.103483,"oder is set out to do. While Liang et al. (2006) employ the standard perceptron algorithm in their work, Tillmann and Zhang (2006) present a variant in which the perceptron update is weighted with a factor dependent on the difference in translation score between the reference and the model guess and the difference in their loss. In this paper, we compare the perceptron algorithm to the Margin Infused Relaxed Algorithm(MIRA) (Crammer and Singer, 2003), a large-margin online algorithm that has given state-of-the-art results in other structured prediction tasks in NLP such as dependency parsing (McDonald et al., 2005). Discriminative learning usually requires access to the gold standard, e.g. in discriminative dependency parsing (McDonald et al., 2005), the learning algorithm updates model parameters towards gold parse features. In the case of phrase based SMT, this is a problematic issue. Recall that phrases are extracted through heuristics from word alignments after symmetrisation (Koehn et al., 2003). Only phrases that are consistent with the alignments are extracted, therefore phrase-pairs that are required to reproduce the reference might end up not being extracted. Also, since multiple phrases are ex"
2007.mtsummit-papers.3,J03-1002,0,0.00419401,"English by five diferent translators. We decided to use this dataset for 2 main reasons : (a) it is a small corpus therefore allowing us to train several models in a short time and (b) it is richly annotated with linguistic infomation which we would like to incorporate as features of our model in future work. However, one of the major drawbacks of using the PCEDT is that since Czech is a morphologically very rich language and the training set is small, the corpus suffers from severe data sparseness issues (Goldwater and McClosky, 2005). We obtained word alignments by using the GIZA++ toolkit (Och and Ney, 2003) on the training corpus in both translation directions. The two sets of alignments were then symmetrised using the grow-diag-final method previously described in (Koehn et al., 2005) and phrase-pairs consistent with the alignments were extracted. Note that both the perceptron and MIRA are error-driven algorithms in that parameter updates are performed only when the learner is unable to classify a training instance properly. Given that the same training data is being used for phrase extraction and for parameter estimation, overfitting is possible. We would instead like to make the training envi"
2007.mtsummit-papers.3,P03-1021,0,0.103966,"learning algorithms achieve similar results, with the perceptron converging more rapidly, the aggressive update strategy performs significantly worse than the more conservative strategy corroborating Liang et al. (2006)’s findings. 1. Introduction The direct maximum entropy model proposed by Och and Ney (2001) described a generalisation of the generative noisy-channel model of Brown et al. (1993) allowing the incorporation of additional knowledge sources in form of features. The standard way of training such a discriminative model in the community is to use minimum error rate training (MERT) (Och, 2003). As it name implies, such a training scheme tries to directly minimise the error on training data where the ’error’ is evaluated using a loss function of one’s choice (usually BLEU). A major shortcoming of MERT is that it can only be used to train a model with a small number of features. Typically, state of the art phrase-based SMT systems use around a dozen features. Recent work has tried to address this problem, with both Liang et al. (2006) and Tillmann and Zhang (2006) presenting online perceptron/perceptron-like training schemes to learn parameters for models with millions of features. T"
2007.mtsummit-papers.3,P06-1091,0,0.703773,"orm of features. The standard way of training such a discriminative model in the community is to use minimum error rate training (MERT) (Och, 2003). As it name implies, such a training scheme tries to directly minimise the error on training data where the ’error’ is evaluated using a loss function of one’s choice (usually BLEU). A major shortcoming of MERT is that it can only be used to train a model with a small number of features. Typically, state of the art phrase-based SMT systems use around a dozen features. Recent work has tried to address this problem, with both Liang et al. (2006) and Tillmann and Zhang (2006) presenting online perceptron/perceptron-like training schemes to learn parameters for models with millions of features. Typically, discriminative training algorithms come in two flavours, firstly likelihood-based methods which require feature expectations and secondly, margin-based methods which require either an N-best list of best outputs or a marginal distribution across the graphical structure. The perceptron algorithm (Collins, 2002) is a much simpler alternative as it only requires an arg max computation, which is precisely what the decoder is set out to do. While Liang et al. (2006) em"
2007.mtsummit-tutorials.1,P01-1067,1,\N,Missing
2009.iwslt-papers.4,J07-2003,0,0.736251,"ervice of Google that follows this approach.1 2.2. Hierarchical Phrase-Based Models However, phrase-based methods fail to capture to capture the essence of many language pairs [5]. One of the reasons is that reordering cannot always be reduced to the reordering of atom phrase units. Consider the mapping of the following sentence pair fragment: - 152 - 1 http://translate.google.com/ Proceedings of IWSLT 2009, Tokyo - Japan take the proposal into account ber¨ucksichtigt den Vorschlag The English phrasal verb take into account wraps around its object the proposal. Hierarchical phrasebased models [6] extend the notion of phrase mapping to allow rules such as take X1 into account k ber¨ucksichtigt X1 must explain X1 k muss X1 erkl¨aren either X1 or X2 k entweder X1 oder X2 Such translation rules may be formalized as a synchronous context free grammar, where the non-terminal X matches any constituent, and nonterminals with the same coindexes (e.g. X1 ) are recursively translated by a single rule. Such a formalism reflects one of the major insights of linguistics: Language is recursive and all modern theories of language use recursive structures. The research questions evolve around how to s"
2009.iwslt-papers.4,P06-1121,0,0.124726,"ce. • Store all extracted phrase pairs and rules for scoring. To provide one empirical fact to support this argument: The adaptation of the originally purely phrasebased training process in Moses to hierarchical and syntaxbased models took less than one month of work. Many syntax-based models relax the requirement that phrases have to correspond to syntactic constituents. For instance, in one of the best-performing models translation units may correspond to syntactic treelets (tree fragments), permitting reordering at a scope larger than that of a single constituent and its immediate children [7]. Also, spans that only match a sequence of constituents or incomplete constituents may be labeled with complex tags such as DET + ADJ or NP/N [8]. Note that these are manipulations of the syntax trees that do not change in any way the rule extraction method. There are many refinements to the the rule extraction method. Limits may be imposed to span sizes as well as number of words and non-terminals. Fractional counts may be for rules extracted from the same spans. Only minimal rules may be extracted to explain a sentence pair. Smoothing counts may be done using Good Turing discounting or othe"
2009.iwslt-papers.4,W06-3119,0,0.067609,"ally purely phrasebased training process in Moses to hierarchical and syntaxbased models took less than one month of work. Many syntax-based models relax the requirement that phrases have to correspond to syntactic constituents. For instance, in one of the best-performing models translation units may correspond to syntactic treelets (tree fragments), permitting reordering at a scope larger than that of a single constituent and its immediate children [7]. Also, spans that only match a sequence of constituents or incomplete constituents may be labeled with complex tags such as DET + ADJ or NP/N [8]. Note that these are manipulations of the syntax trees that do not change in any way the rule extraction method. There are many refinements to the the rule extraction method. Limits may be imposed to span sizes as well as number of words and non-terminals. Fractional counts may be for rules extracted from the same spans. Only minimal rules may be extracted to explain a sentence pair. Smoothing counts may be done using Good Turing discounting or other methods. - 153 - Proceedings of IWSLT 2009, Tokyo - Japan PHRASE - BASED HIERARCHICAL SYNTAX - BASED raw translated text raw translated text raw"
2009.iwslt-papers.4,W01-1812,0,0.0359255,"ons may also fan in to a node due to dynamic programming. A hypothesis, or state in the search graph, points back to its highest-probable path, but also alternative paths with lower probability. In practice, we store with each state information such as which foreign words have been covered so far, the partial translation constructed so far, and the model scores along with all underlying component scores. But this information may also be obtained by walking back the best possible path. In chart decoding the transitions may originate from multiple hypotheses. This can visualized as a hypergraph [9, 10], a generalization of a graph in which an edge (called a hyperedge) may originate from multiple nodes (called tail nodes). The nodes of the hypergraph - 155 - Proceedings of IWSLT 2009, Tokyo - Japan correspond to hypotheses, while the hyperedges correspond to rule applications. Just as in the graph case, we can extract a best hyperpath that corresponds to a single set of rule applications. Note that this is simply an extension of the case for phrase-based models, and indeed the graph generated by a phrase-based model is simply the special case of a hypergraph in which each hyperedge has only"
2009.iwslt-papers.4,P07-1019,0,0.587451,"ons may also fan in to a node due to dynamic programming. A hypothesis, or state in the search graph, points back to its highest-probable path, but also alternative paths with lower probability. In practice, we store with each state information such as which foreign words have been covered so far, the partial translation constructed so far, and the model scores along with all underlying component scores. But this information may also be obtained by walking back the best possible path. In chart decoding the transitions may originate from multiple hypotheses. This can visualized as a hypergraph [9, 10], a generalization of a graph in which an edge (called a hyperedge) may originate from multiple nodes (called tail nodes). The nodes of the hypergraph - 155 - Proceedings of IWSLT 2009, Tokyo - Japan correspond to hypotheses, while the hyperedges correspond to rule applications. Just as in the graph case, we can extract a best hyperpath that corresponds to a single set of rule applications. Note that this is simply an extension of the case for phrase-based models, and indeed the graph generated by a phrase-based model is simply the special case of a hypergraph in which each hyperedge has only"
2009.iwslt-papers.4,E09-1061,1,0.86948,"h in which each hyperedge has only one tail node. The virtue of the hypergraph view is that, even though our models have superficially quite different structures, their search spaces can all be represented in the same way, making them amenable to a variety of hypergraph algorithms [11]. These algorithms generalize familiar graph algorithms, which are simply special cases of their hypergraph generalizations. With this in mind, most statistical translation systems can be viewed as implementations of a very small number of generic algorithms, in which the main difference is a modelspecific logic [12]. 4.5. Stacks Viewing decoding as the task of finding the most probable path in a search graph or hypergraph, is one visualization of the problem. However, this graph is too large to efficiently construct even for relatively short sentences. We need to focus on the most promising part of the graph. To this end, we first group together comparable hypotheses in stacks, and then prune out the weaker ones. There are many ways to define the stacks. In sequential decoding, we group together hypotheses that cover the same number of input words. In chart decoding, we group together hypotheses that cov"
2009.iwslt-papers.4,J03-1005,0,0.0102483,"raph is too large to efficiently construct even for relatively short sentences. We need to focus on the most promising part of the graph. To this end, we first group together comparable hypotheses in stacks, and then prune out the weaker ones. There are many ways to define the stacks. In sequential decoding, we group together hypotheses that cover the same number of input words. In chart decoding, we group together hypotheses that cover the same input span. More fine-grained groupings are possible: in sequential decoding we could distinguish between hypotheses that cover different input words [13], and in chart decoding for models with target side syntax, we may keep different stacks for different target-side labels. However, we want to avoid having too many stacks, and such additional distinctions may also be enforced by diversity requirements during pruning [14]. We prune bad hypotheses based on their incremental score so far. When comparing hypotheses that cover different input words, we also include a future cost estimate for the remaining words. 4.6. Search Strategy The final decision of the decoding algorithm is: In which order do we generate the hypotheses? The incremental scori"
2009.iwslt-papers.4,J04-4002,0,0.0402213,"andide project [1]. However, occasionally words have to be inserted and deleted without clear lexical correspondence on the other side, and words do not always map one-to-one. As a consequence, the wordbased models proposed by IBM were burden with additional complexities such as word fertilities and NULL word generation. 2.1. Phrase-Based Models Over the last decade, word-based models have been all but abandoned (they still live on in word alignment methods), and replaced by an even simpler view of language. Phrase-based models view translation of small text chunks, again with some reordering [2, 3]. The complexities of many-to-many translation, insertion and deletion are hidden within the phrasal translation table. To give examples, phrase-based models may include rules such as assumes k geht davon aus, dass with regard to k bez¨uglich ¨ translation system k Ubersetzungssystem Implementations of such phrase-based models of translation have been shown to outperform all existing translation systems for some language pairs [4]. Currently most prominent is the online translation service of Google that follows this approach.1 2.2. Hierarchical Phrase-Based Models However, phrase-based method"
2009.iwslt-papers.4,2008.iwslt-papers.8,0,0.0120533,"e the stacks. In sequential decoding, we group together hypotheses that cover the same number of input words. In chart decoding, we group together hypotheses that cover the same input span. More fine-grained groupings are possible: in sequential decoding we could distinguish between hypotheses that cover different input words [13], and in chart decoding for models with target side syntax, we may keep different stacks for different target-side labels. However, we want to avoid having too many stacks, and such additional distinctions may also be enforced by diversity requirements during pruning [14]. We prune bad hypotheses based on their incremental score so far. When comparing hypotheses that cover different input words, we also include a future cost estimate for the remaining words. 4.6. Search Strategy The final decision of the decoding algorithm is: In which order do we generate the hypotheses? The incremental scoring allows us to already compute fairly indicative scores for partial translation, so we broadly pursue a bottom-up decoding strategy, where we generate hypotheses of increasing input word coverage. This also allows efficient dynamic programming, since we generate all hypo"
2009.iwslt-papers.4,N03-1017,1,0.0173795,"andide project [1]. However, occasionally words have to be inserted and deleted without clear lexical correspondence on the other side, and words do not always map one-to-one. As a consequence, the wordbased models proposed by IBM were burden with additional complexities such as word fertilities and NULL word generation. 2.1. Phrase-Based Models Over the last decade, word-based models have been all but abandoned (they still live on in word alignment methods), and replaced by an even simpler view of language. Phrase-based models view translation of small text chunks, again with some reordering [2, 3]. The complexities of many-to-many translation, insertion and deletion are hidden within the phrasal translation table. To give examples, phrase-based models may include rules such as assumes k geht davon aus, dass with regard to k bez¨uglich ¨ translation system k Ubersetzungssystem Implementations of such phrase-based models of translation have been shown to outperform all existing translation systems for some language pairs [4]. Currently most prominent is the online translation service of Google that follows this approach.1 2.2. Hierarchical Phrase-Based Models However, phrase-based method"
2009.iwslt-papers.4,W09-1114,1,0.818025,"ision Rule Finally, we have to pick one of the hypotheses that cover the entire input sentence to output a translation. Most commonly, this is the hypothesis with the best score, but that is not the only choice. There may be multiple ways to produce the same output. If our goal is to find the most probable translation given the input, then we should find all possible paths through the search graph that result in the same output and sum up their scores. Then, we output the translation with the highest score over all derivation. This is called max-translation decoding vs. maxderivation decoding [15]. But what if the best translation is an outlier? Given the uncertainty in all our models, we may prefer instead a different high-scoring translation that is most similar to the other high-scoring translations. This is the motivation for minimum Bayes risk decoding [16], which has been shown to often lead to better results. - 156 - Proceedings of IWSLT 2009, Tokyo - Japan Model phrase-based hierarchical target-syntax 5. Implementation Based on our observations about the deep similarities between many popular translation models, we have substantially extended the functionality of the Moses tool"
2009.iwslt-papers.4,W09-0401,1,0.68025,"n word alignment methods), and replaced by an even simpler view of language. Phrase-based models view translation of small text chunks, again with some reordering [2, 3]. The complexities of many-to-many translation, insertion and deletion are hidden within the phrasal translation table. To give examples, phrase-based models may include rules such as assumes k geht davon aus, dass with regard to k bez¨uglich ¨ translation system k Ubersetzungssystem Implementations of such phrase-based models of translation have been shown to outperform all existing translation systems for some language pairs [4]. Currently most prominent is the online translation service of Google that follows this approach.1 2.2. Hierarchical Phrase-Based Models However, phrase-based methods fail to capture to capture the essence of many language pairs [5]. One of the reasons is that reordering cannot always be reduced to the reordering of atom phrase units. Consider the mapping of the following sentence pair fragment: - 152 - 1 http://translate.google.com/ Proceedings of IWSLT 2009, Tokyo - Japan take the proposal into account ber¨ucksichtigt den Vorschlag The English phrasal verb take into account wraps around its"
2009.iwslt-papers.4,N04-1022,0,0.0211555,"our goal is to find the most probable translation given the input, then we should find all possible paths through the search graph that result in the same output and sum up their scores. Then, we output the translation with the highest score over all derivation. This is called max-translation decoding vs. maxderivation decoding [15]. But what if the best translation is an outlier? Given the uncertainty in all our models, we may prefer instead a different high-scoring translation that is most similar to the other high-scoring translations. This is the motivation for minimum Bayes risk decoding [16], which has been shown to often lead to better results. - 156 - Proceedings of IWSLT 2009, Tokyo - Japan Model phrase-based hierarchical target-syntax 5. Implementation Based on our observations about the deep similarities between many popular translation models, we have substantially extended the functionality of the Moses toolkit [17], which previously supported only phrase-based models. In particular, our implementation includes a chart decoder that can handle general synchronous contextfree grammars, including both hierarchical and syntaxbased grammars. Both phrase-based and hierarchical d"
2009.iwslt-papers.4,D07-1079,0,0.0381358,"ave substantially extended the functionality of the Moses toolkit [17], which previously supported only phrase-based models. In particular, our implementation includes a chart decoder that can handle general synchronous contextfree grammars, including both hierarchical and syntaxbased grammars. Both phrase-based and hierarchical decoders implement cube pruning [6, 10] and minimum Bayes risk decoding [16]. Our training implementation also includes rule extraction for hierarchical [6] and syntax-based translation. The syntax-based rule extractor produces rules similar to the “composed rules” of [18]. The source code is freely available.2 This allows us to take advantage of the mature Moses infrastructure by retaining much of the existing components. Also, the development of a hierarchical system alongside a phrase-based system allows us to more easily and fairly compare and contrast the models. Re-using and extending the existing Moses decoder reduces the amount of development required. As an illustration, the phrase-based decoder 24,000 lines of code. The more complex hierarchical and syntax extension added 10,000 lines to the codebase. Some components in a phrase-based and hierarchical"
2009.iwslt-papers.4,C04-1024,0,0.0499602,"cal, and a syntax-based model that uses syntax on the target side. We trained systems using the News Commentary training set that was released by WMT 20093 for English to German translation. See Table 1 for statistics on rule table sizes and BLEU scores for the news-dev2009b test set. Decoding for all the three models took about the same time, roughly 0.3 seconds per word. Decoding for hierarchical and syntax-based models is more complex, and we expect to achieve better results by tuning the search algorithm and using larger beam sizes. The syntax-based model uses the BitPar parser for German [19]. Note that recent work has shown that state-of-the-art performance requires improvements to word alignment [20] and data preparation, which were not done for these experiments. 3 http://mosesdecoder.svn.sourceforge.net - 157 - http://www.statmt.org/wmt09/ Proceedings of IWSLT 2009, Tokyo - Japan 7. Conclusions and Outlook ing success in machine translation,” in Proc. of EMNLP, 2008. Our experiments illustrate that the hierarchical and syntactic models in Moses achieve similar quality to the phrase-based model, even though their implementation is less mature. We expect that their performance w"
2009.iwslt-papers.4,W08-0306,0,0.0156471,"training set that was released by WMT 20093 for English to German translation. See Table 1 for statistics on rule table sizes and BLEU scores for the news-dev2009b test set. Decoding for all the three models took about the same time, roughly 0.3 seconds per word. Decoding for hierarchical and syntax-based models is more complex, and we expect to achieve better results by tuning the search algorithm and using larger beam sizes. The syntax-based model uses the BitPar parser for German [19]. Note that recent work has shown that state-of-the-art performance requires improvements to word alignment [20] and data preparation, which were not done for these experiments. 3 http://mosesdecoder.svn.sourceforge.net - 157 - http://www.statmt.org/wmt09/ Proceedings of IWSLT 2009, Tokyo - Japan 7. Conclusions and Outlook ing success in machine translation,” in Proc. of EMNLP, 2008. Our experiments illustrate that the hierarchical and syntactic models in Moses achieve similar quality to the phrase-based model, even though their implementation is less mature. We expect that their performance will continue to be improved by drawing on the substantial body of research in syntactic translation modeling ove"
2009.iwslt-papers.4,C08-1144,0,0.0345081,"Missing"
2009.iwslt-papers.4,D07-1078,0,0.0412003,"Missing"
2009.iwslt-papers.4,J93-2003,0,\N,Missing
2009.iwslt-papers.4,P07-2045,1,\N,Missing
2009.iwslt-papers.4,D08-1078,1,\N,Missing
2009.mtsummit-papers.7,W08-0309,1,0.714167,"cal machine 1 2 http://eur-lex.europa.eu/ http://europa.eu/eurovoc/ translation system, and very little additional processing is needed. It is hard to quantify how much training data is needed to achieve a minimum level of performance. This depends on the expansiveness of the domain and the language pair. Typically, tens of millions of words give decent performance: For instance, systems trained on the 30–40 million word Europarl corpus are competitive with commercial systems, typically better on this domain and even close in performance when translating related material such as news stories (Callison-Burch et al., 2008). The JRC-Acquis corpus is large enough to expect decent translation performance within its domain, but on the other hand, the domain is also very specific. Translation models trained on such legal texts do not necessarily perform well on other domains. 3.2 Tuning and Test Sets Since we develop machine translation systems for 462 language pairs, we wanted to have a common tuning and testing environment. Hence, we extracted from part of the corpus subset where sentences are aligned one-to-one across all languages. First, we identified all documents that exist for all languages. This is a set of"
2009.mtsummit-papers.7,D08-1089,0,0.0147934,"present when choosing candidate translation phrases. We have also included corpus size as a factor as the amount of Acquis data per language pair can vary by a factor of four. The following characteristic form part of our analysis: 5 Reordering We measure word order differences between languages by assuming that reordering is a binary process between two blocks that are adjacent in the source and whose order is reversed in the target. Word alignments are extracted using GIZA++ and then merged using the grow-finaldiag algorithm. Reorderings are then extracted using the shift-reduce algorithm (Galley and Manning, 2008). These reorderings are used to extract a sentence level metric, RQuantity (Birch et al., 2008), which is the sum of the widths of all the reorderings on the source side, normalized by the length of the source sentence. This measure is averaged over a random sample of 2000 training sentences to get the corpus RQuantity. Analysis The Acquis corpus comprises of a very large number and variety of language pairs. The breadth of data conditions make this corpus ideal for performing experiments which investigate language pair characteristics and the effect they have on translation. This allows us to"
2009.mtsummit-papers.7,W09-0431,0,0.0369666,"tion performance more often than not. This is not the case for other languages. See Table 7 for summary statistics for English and French as pivot. When using English as pivot, we find not much difference (BLEU diverges by up to 2 points) for about a third of language pairs, for another third there are significant gains (2-5 points) and for another third even larger gains (5-10 points). However, using French as pivot generally decreases performance, only for a sixth of language pairs there is not much difference. English as pivot has also shown to be beneficial for Arabic–Chinese translation (Habash and Hu, 2009). We find it hard to claim that this is due to linguistic reasons, but rather an artifact of the data set we are using. It is likely that most of the text was originally authored in English. 6.2 Multi-Pivot Translation While pivoting through any language but English does generally lead to worse translations, it does constitute an alternative translation path. A recent trend in statistical machine translation is to combine the output of different MT systems in form of a consensus translation. In multi-pivot translation, we combine the direct translation system with several pivot systems, a nove"
2009.mtsummit-papers.7,P07-2045,1,0.0121715,"age. This gave us a set of 12,322 sentences aligned across all 22 languages of the corpus. We split this set into three parts, a tuning set for parameter optimization, a development test set for experimentation and a final test set to report translation performance. Since these sets contain many short and a few very long sentences, we reduced the tuning set further, by requiring that all sentences are between 8 and 60 words long. This left us with a tuning set of 1944 sentences per language. 3.3 Training For the development of the translation system, we used the defaults of the Moses toolkit (Koehn et al., 2007) with the following additional settings: maximum sentence length 80 words, bi-directional msd reordering model, 5-gram language model. 4 Performance A thorough evaluation of the translation quality of translation systems for 462 different language pairs would be a daunting task, so we rely on automatic metrics. The most commonly used metric in statistical machine translation is the BLEU score (Papineni et al., 2002). Table 3 shows the scores for all the 462 translation systems. Performance varies widely for the different language pairs. For instance, French–English translation (64.0) is better"
2009.mtsummit-papers.7,P02-1040,0,0.101453,"nd 60 words long. This left us with a tuning set of 1944 sentences per language. 3.3 Training For the development of the translation system, we used the defaults of the Moses toolkit (Koehn et al., 2007) with the following additional settings: maximum sentence length 80 words, bi-directional msd reordering model, 5-gram language model. 4 Performance A thorough evaluation of the translation quality of translation systems for 462 different language pairs would be a daunting task, so we rely on automatic metrics. The most commonly used metric in statistical machine translation is the BLEU score (Papineni et al., 2002). Table 3 shows the scores for all the 462 translation systems. Performance varies widely for the different language pairs. For instance, French–English translation (64.0) is better than Bulgarian–Hungarian (24.7). French Input LE CONSEIL DE LA COMMUNAUTE´ ´ ´ ECONOMIQUE EUROPEENNE, consid´erant que l’instauration d’une politique commune des transports comporte entre autres l’´etablissement de r`egles communes applicables aux transports internationaux de marchandises par route, ex´ecut´es au d´epart ou a` destination du territoire d’un e´ tat membre, ou traversant le territoire d’un ou plusieu"
2009.mtsummit-papers.7,N07-1029,0,0.0423619,"Missing"
2009.mtsummit-papers.7,D08-1078,1,0.937108,"s the amount of Acquis data per language pair can vary by a factor of four. The following characteristic form part of our analysis: 5 Reordering We measure word order differences between languages by assuming that reordering is a binary process between two blocks that are adjacent in the source and whose order is reversed in the target. Word alignments are extracted using GIZA++ and then merged using the grow-finaldiag algorithm. Reorderings are then extracted using the shift-reduce algorithm (Galley and Manning, 2008). These reorderings are used to extract a sentence level metric, RQuantity (Birch et al., 2008), which is the sum of the widths of all the reorderings on the source side, normalized by the length of the source sentence. This measure is averaged over a random sample of 2000 training sentences to get the corpus RQuantity. Analysis The Acquis corpus comprises of a very large number and variety of language pairs. The breadth of data conditions make this corpus ideal for performing experiments which investigate language pair characteristics and the effect they have on translation. This allows us to provide a wide perspective on the challenges facing machine translation and provide strong mot"
2009.mtsummit-papers.7,steinberger-etal-2006-jrc,1,0.296887,"Missing"
2009.mtsummit-papers.7,P07-1108,0,\N,Missing
2009.mtsummit-papers.8,J09-1002,0,0.0894987,"Missing"
2009.mtsummit-papers.8,W00-0507,0,0.122889,"anslation output. A user study validates the types of assistance and provides insight into the human translation process. 1 Introduction While machine translation has made tremendous progress over the last years, this progress has made little inroads into tools for human translators. Although it has become frequent practice in the industry to provide human translators with machine translation output for post-editing, typically no deeper integration of machine translation and human translation is found in translation agencies. An interesting new approach was pioneered by the TransType project (Langlais et al., 2000). The machine translation system makes sentence completion predictions in an interactive machine translation setting. The users may accept them or override them by typing in their own translations, which triggers new suggestions by the tool (Barrachina et al., 2009). But also other information of the machine translation system may be useful for the human translator, such as alternative translations for the input words and phrases. We developed the web-based translation tool Caitra that offers various types of assistance and carried out a study involving ten human translators, whose interaction"
2009.mtsummit-papers.8,macklovitch-2006-transtype2,0,0.305779,"creasing interest in the process studies of translation (Fraser, 1996). Such studies of user activity data focused on key strokes and considered statistics such as revision ratios (Buchweitz and Alves, 2006). Carl et al. (2008) presents a study that also uses eye trackers. StudBarry Haddow School of Informatics University of Edinburgh bhaddow@inf.ed.ac.uk ies may also make use of think aloud protocols (J¨aa¨ skel¨ainen, 2001) in which the translator narrates the thought process behind her actions. The interactive machine translation assistance (that we present in Section 2.1) was evaluated by Macklovitch (2006) with an emphasis on the user experience. Our user study is an extension of this prior work. It extends to different and novel types of assistance. We use a relatively large corpus and a large number of test subjects. 2 Types of Assistance Caitra is implemented as a web-based clientserver architecture, using Ajax Web 2.0 technologies. The machine translation back-end is powered by the Moses decoder. The tool is delivered over the web to allow for easier user studies, but also to expose it to a wider community to gather additional feedback. You can find the tool online at http://www.caitra.org/"
2009.mtsummit-posters.6,E06-1032,1,0.866806,"Missing"
2009.mtsummit-posters.6,W07-0718,1,0.820804,"translation Given a certain quality of dictionary, we now face the question of how much translation quality benefits from these dictionaries. We evaluate translation quality with an automatic metric (Papineni et al., 2002) and human judgement. Although the BLEU metric has been shown to be unreliable (CallisonBurch et al., 2006) for comparing systems of so different architecture as rule-based and statistical systems, this does not discard its use for comparing two versions of a given system. As far as human judgement is concerned, in accordance with the findings of recent evaluation campaigns (Callison-Burch et al., 2007), we choose to rely on a ranking of the overall quality of competing outputs. In addition to evaluation, we also perform a human error analysis on a random sample of a hundred sentences. This task consists of comparing the translation output when adding all the extracted rules with the baseline translation and trying to identify reasons for possible deteriorations or improvements. 6 Experiments and results We describe here experiments for both the dictionary extraction and the translation aspects. 6.1 Dictionary extraction Our basic dictionary extraction configuration follows the pipeline desc"
2009.mtsummit-posters.6,C94-1084,0,0.246147,"Missing"
2009.mtsummit-posters.6,W07-0732,1,0.79859,"ng the first to describe a pipeline for extracting dictionaries of noun compounds. Koehn (2003) gives a thorough investigation of the topic of noun phrase translation and extraction of noun phrase lexicons in particular. As far as extraction is concerned, one variation between the different approaches lies in the choice of either extracting all monolingual terms to then find alignments or align chunks of raw text (typically, extract a phrase table) which is then filtered to keep only the syntactically meaningful ones. Daille (1994) and Kumano (1994) belong to the first category, while Itagaki (2007) belongs to the second category. Another variation is the choice of confidence measures to evaluate the quality of a candidate entry. As far as application of such dictionaries is concerned, Melamed (1997) uses mutual information as the ob2 This time, the word ”phrase” is understood as a syntactic constituent. jective function maximized during the learning process. Font Llitjos et al. (2007) describes a semiautomatic procedure to extract new dictionary entries in a rule-based system. This however deals with a small number of entries and necessitates a manual review. Itagaki (2007) presents a f"
2009.mtsummit-posters.6,W08-0328,0,0.0190113,"first explain what motivates a need for bilingual phrase dictionaries. This motivation is twofold. Previous experiments indicated that most of the improvements of a statistical phrase-based layer in a combination with a rule-based system came from lexical changes, most of them phrasal expressions. Then, the distribution of bilingual phrases in terms of phrase length and the nature and availability of manually written phrasal bilingual dictionaries for a given domain is an incentive for corpus extraction. The first argument is illustrated both by a combination of different rule-based systems (Eisele et al., 2008) and a statistical post-editing layer over a rulebased output (Simard et al., 2007), along with some qualitative analysis (Dugast et al., 2007). As for the second argument, in the past few years, statistical machine translation moved from wordbased models to phrase-based1 models. In a more linguistic approach, Bannard (2006) discusses the 1 Here, the word ”phrase” simply denotes any sequence of words, not necessarily a constituent. English big park private bank left bank fig leaf fraud scandal freight traffic to let off steam 1 2 3 4 5 6 7 French grand parc banque priv´ee rive gauche feuille d"
2009.mtsummit-posters.6,P03-1057,0,0.0616347,"Missing"
2009.mtsummit-posters.6,N03-1017,1,0.0156547,"Missing"
2009.mtsummit-posters.6,C94-1009,0,0.0770438,"Missing"
2009.mtsummit-posters.6,P93-1003,0,0.0425527,"rule-based system. A first reason lies in the ability to capture local context to disambiguate the translation (as in examples 1-2 of Table 1). Then, there are phrases that cannot be translated word-for-word, such as examples 4 and 5. And finally, some strong collocations may reduce the syntactic ambiguity of the source sentence (examples 6 and 7). We however probably do not need entries such as big park in Table 1, for its translation into French is compositional, little ambiguous and the English phrase big park could be easily modified into very big park, big natural park. 1.2 Related work Kupiec (1993) is among the first to describe a pipeline for extracting dictionaries of noun compounds. Koehn (2003) gives a thorough investigation of the topic of noun phrase translation and extraction of noun phrase lexicons in particular. As far as extraction is concerned, one variation between the different approaches lies in the choice of either extracting all monolingual terms to then find alignments or align chunks of raw text (typically, extract a phrase table) which is then filtered to keep only the syntactically meaningful ones. Daille (1994) and Kumano (1994) belong to the first category, while I"
2009.mtsummit-posters.6,W07-0410,0,0.0268455,"Missing"
2009.mtsummit-posters.6,W97-0311,0,0.0366792,"lexicons in particular. As far as extraction is concerned, one variation between the different approaches lies in the choice of either extracting all monolingual terms to then find alignments or align chunks of raw text (typically, extract a phrase table) which is then filtered to keep only the syntactically meaningful ones. Daille (1994) and Kumano (1994) belong to the first category, while Itagaki (2007) belongs to the second category. Another variation is the choice of confidence measures to evaluate the quality of a candidate entry. As far as application of such dictionaries is concerned, Melamed (1997) uses mutual information as the ob2 This time, the word ”phrase” is understood as a syntactic constituent. jective function maximized during the learning process. Font Llitjos et al. (2007) describes a semiautomatic procedure to extract new dictionary entries in a rule-based system. This however deals with a small number of entries and necessitates a manual review. Itagaki (2007) presents a filtering method for candidate entries using a Gaussian Mixture Model classifier trained on human judgements of the quality of a dictionary entry, but does not provide evaluation of final translation qualit"
2009.mtsummit-posters.6,P02-1040,0,0.0914537,"added (contiguous) phrasal rules may disable original rules and/or hurt the dependency analysis. Thus, such a verbal expression as make good progress that may have been correctly translated would then be mistranslated once the phrasal entry good progress is added to the rules’ base. A noun phrase such as ”rapid and sound progress” may also get mistranslated from adding sound progress as a contiguous noun phrase, as illustrated on figure 2. Therefore the problem consists of building the optimal subset from the set of candidate entries, according to a translation evaluation metric (here, BLEU (Papineni et al., 2002)), while being constrained by the deterministic firing of these rules. As an approximate (suboptimal) response to this problem, we test each extracted entry individually, starting from the lower n-grams to the longer (source) chunks, following Algorithm 1. For each sentence pair where the entry (of source span N) fires, the translation score (sentence level BLEU) when adding this rule is compared with the baseline translation. Rules showing only a single improved for n=1 to NgramMax do map all n-gram (length of the source phrase) entries to parallel sentences translate training corpus with cur"
2009.mtsummit-posters.6,P05-1034,0,0.0767182,"Missing"
2009.mtsummit-posters.6,2003.mtsummit-papers.46,1,0.731028,"uality of a dictionary entry, but does not provide evaluation of final translation quality. The closest work from what we describe in the present paper might be the one by Imamura (2003), in which examplebased pattern rules are filtered using an automatic evaluation of the final translation output. In the work presented here, we describe two independent training steps that first extract dictionary candidates and then automatically validate them directly within the RBMT system. 2 Dictionary extraction 2.1 Manual coding of entries The SYSTRAN rule-based system provides a dictionary coding tool (Senellart et al., 2003) that allows the manual task of coding entries to be partially automated thanks to the use of monolingual dictionaries (Table 2), morphological guess rules and probabilistic context-free local grammars (Table 3). For example, the second rule illustrated in the latter table simply describes how an English noun phrase may be composed of a adjective+noun sequence. The general rule has a phrase to inherit inflection and semantic (we won’t mention this aspect here, as it is of little significance) features from the headword. The coding tool also allows the user to fine-tune it by correcting the aut"
2009.mtsummit-posters.6,N07-1064,0,0.0215162,"ng the first to describe a pipeline for extracting dictionaries of noun compounds. Koehn (2003) gives a thorough investigation of the topic of noun phrase translation and extraction of noun phrase lexicons in particular. As far as extraction is concerned, one variation between the different approaches lies in the choice of either extracting all monolingual terms to then find alignments or align chunks of raw text (typically, extract a phrase table) which is then filtered to keep only the syntactically meaningful ones. Daille (1994) and Kumano (1994) belong to the first category, while Itagaki (2007) belongs to the second category. Another variation is the choice of confidence measures to evaluate the quality of a candidate entry. As far as application of such dictionaries is concerned, Melamed (1997) uses mutual information as the ob2 This time, the word ”phrase” is understood as a syntactic constituent. jective function maximized during the learning process. Font Llitjos et al. (2007) describes a semiautomatic procedure to extract new dictionary entries in a rule-based system. This however deals with a small number of entries and necessitates a manual review. Itagaki (2007) presents a f"
2010.amta-papers.2,P05-1032,0,0.0738012,"Missing"
2010.amta-papers.2,2009.mtsummit-papers.7,1,0.809711,"Missing"
2010.amta-papers.2,D07-1104,0,0.0123509,"ring methods using a database. They report speeds of ”8 seconds to compare 419 query sentences against 1497 reference sentences”, and ”1.5 seconds per query sentence” with a larger corpus, which is a few orders of magnitude slower than our method. Suffix arrays have been applied to a related problem in machine translation, namely looking up phrases in a word-aligned parallel corpus to compute phrase translation probabilities. Work by CallisonBurch et al. (2005); Zhang and Vogel (2005); McNamee and Mayfield (2006) was extended to socalled hierarchical phrases, essentially phrases with gaps, by Lopez (2007). 4 Suffix Arrays Our method uses n-gram matches between the input segment (the pattern) and the translation memory (the corpus) to identify potential candidate corpus segments. We store the corpus in a suffix array to enable quick lookup. The data structure uses an index of starting positions of all suffixes in the corpus, which is sorted alphabetically (see Figure 2). This allows us to use binary search to find a particular suffix in the corpus. We sort the index using quick sort, which is O(n log n). Our implementation takes a few seconds even for corpora with tens of millions of words. We"
2010.amta-papers.2,2002.tmi-tutorials.1,0,0.0159293,"ltering stage. There are various ways to process the pattern, for instance splitting it into sub-patterns for which exact matching is performed, or compiling it into a finite state machine. Our method utilizes exact matches of sub-patterns. The dynamic programming techniques can be improved in many ways. To give an example, in the canonical algorithm, we do not need to compute the entire matrix but can focus on the alignment points with the lowest cost. The only description of a method addressing the approximate string matching problem in translation memories, that we are aware of, is work by Mandreoli et al. (2002), which uses simple filtering methods using a database. They report speeds of ”8 seconds to compare 419 query sentences against 1497 reference sentences”, and ”1.5 seconds per query sentence” with a larger corpus, which is a few orders of magnitude slower than our method. Suffix arrays have been applied to a related problem in machine translation, namely looking up phrases in a word-aligned parallel corpus to compute phrase translation probabilities. Work by CallisonBurch et al. (2005); Zhang and Vogel (2005); McNamee and Mayfield (2006) was extended to socalled hierarchical phrases, essential"
2010.amta-papers.2,2006.amta-papers.12,0,0.0167637,"em in translation memories, that we are aware of, is work by Mandreoli et al. (2002), which uses simple filtering methods using a database. They report speeds of ”8 seconds to compare 419 query sentences against 1497 reference sentences”, and ”1.5 seconds per query sentence” with a larger corpus, which is a few orders of magnitude slower than our method. Suffix arrays have been applied to a related problem in machine translation, namely looking up phrases in a word-aligned parallel corpus to compute phrase translation probabilities. Work by CallisonBurch et al. (2005); Zhang and Vogel (2005); McNamee and Mayfield (2006) was extended to socalled hierarchical phrases, essentially phrases with gaps, by Lopez (2007). 4 Suffix Arrays Our method uses n-gram matches between the input segment (the pattern) and the translation memory (the corpus) to identify potential candidate corpus segments. We store the corpus in a suffix array to enable quick lookup. The data structure uses an index of starting positions of all suffixes in the corpus, which is sorted alphabetically (see Figure 2). This allows us to use binary search to find a particular suffix in the corpus. We sort the index using quick sort, which is O(n log n"
2010.amta-papers.2,steinberger-etal-2006-jrc,0,0.116149,"Missing"
2010.amta-papers.2,2005.eamt-1.39,0,0.0243755,"te string matching problem in translation memories, that we are aware of, is work by Mandreoli et al. (2002), which uses simple filtering methods using a database. They report speeds of ”8 seconds to compare 419 query sentences against 1497 reference sentences”, and ”1.5 seconds per query sentence” with a larger corpus, which is a few orders of magnitude slower than our method. Suffix arrays have been applied to a related problem in machine translation, namely looking up phrases in a word-aligned parallel corpus to compute phrase translation probabilities. Work by CallisonBurch et al. (2005); Zhang and Vogel (2005); McNamee and Mayfield (2006) was extended to socalled hierarchical phrases, essentially phrases with gaps, by Lopez (2007). 4 Suffix Arrays Our method uses n-gram matches between the input segment (the pattern) and the translation memory (the corpus) to identify potential candidate corpus segments. We store the corpus in a suffix array to enable quick lookup. The data structure uses an index of starting positions of all suffixes in the corpus, which is sorted alphabetically (see Figure 2). This allows us to use binary search to find a particular suffix in the corpus. We sort the index using q"
2010.jec-1.4,J07-2003,0,0.165786,"Missing"
2010.jec-1.4,W07-0732,1,0.857108,"y automatically analyzing translated text and learning the rules. SMT has been embraced by the academic and commercial research communities as the new dominant paradigm in machine translation. Almost all recently published papers on machine translation are published on new SMT techniques. The methodology has left the research labs and become the basis of successful companies such as Language Weaver and the highly visible Google and Microsoft web translation services. Even traditional rule-based companies such as Systran have embraced statistical methods and integrated them into their systems (Dugast et al., 2007). The two technologies have not touched much in the past not only because of the different development communities (software suppliers to translation agencies vs. mostly academic research labs). Another factor is that TM and SMT have recently addressed different translation challenges. While TM have addressed the need of translation agencies to produce high-quality translations of often repetitive material, SMT has set itself the challenge of open domain translations such as news stories and is mostly satisfied with translation quality that is good enough for gisting, i.e., transmitting the me"
2010.jec-1.4,2009.iwslt-papers.4,1,0.28367,"airs: • • • • • • 28 ( the ; les ) ( the big ; les gros ) ( the big fish ; les gros poissons ) ( big ; gros ) ( big fish ; gros poissons ) ( fish ; poissons ) ( the X fish ; les X poissons ) The symbol X is called a non-terminal, since the translation rule is viewed as a synchronous contextfree grammar rule. In essence it is a place-holder for recursively nested sub-phrases. Hierarchical rules require a different decoding algorithm that is typically drawn from syntactic parsing methods, but otherwise use a very similar training, tuning, and testing pipeline as traditional phrase-based models (Hoang et al., 2009). 4.2 TM Matches as Very Large Rules How does that relate to our XML frames? Recall the XML frame that we constructed in Section 2.1: <A` l’ article&gt; 21 <, le texte du deuxi´eme alin´ea est supprim´e .&gt; Instead of replacing the source sentence The second paragraph of Article 21 is deleted . with the XML frame, we can rewrite this frame as a hierarchical phrase rule and provide it to a hierarchical decoder: ( The second paragraph of Article X is deleted . ; A` l’ article X , le texte du deuxi´eme alin´ea est supprim´e . ) In practice, hierarchical models do not use such large rules (and keep in"
2010.jec-1.4,2009.mtsummit-papers.7,1,0.85313,"all mismatched source words are inserted, all TM target words aligned to mismatched TM source words are removed, if the alignment to the target words fails, go to previous word and follow its alignment. Acquis segments English words French words Product segments English words French words Corpus 1,165,867 24,069,452 25,533,259 Test 4,107 129,261 135,224 Corpus 83,461 1,038,762 1,110,284 Test 2,000 24,643 26,248 Table 1: Statistics of the corpus used in experiments (Product) and the English–French part of the publicly available JRC-Acquis corpus1 (Acquis), for which we use the same test set as Koehn et al. (2009). See Table 1 for basic corpus statistics. The Acquis corpus is a collection of laws and regulations that apply to all member countries of the European Union. It has more repetitive content than the parallel corpora that are more commonly used in machine translation research. Still, the commercial 1 24 http://wt.jrc.it/lt/Acquis/ (Steinberger et al., 2006) Product corpus is more representative of the type of data used in TM systems. It is much smaller (around a million words), with shorter segments (average 12 words per segments). We are especially interested in the performance of the methods"
2010.jec-1.4,J10-4005,0,0.0110158,"uals, or several drafts of legislation), being able to find existing translations of segments of the source language text, alleviates the need to carry out redundant translation. In addition, finding close matches (so-called fuzzy matches), may dramatically reduce the translation workload. Various commercial vendors offer TM software and the technology is in wide use by translation agencies. Instead of building machine translation systems by manually writing translation rules, SMT sysJean Senellart Systran La Grande Arche 1, Parvis de la D´efense 92044 Paris, France senellart@systran.fr tems (Koehn, 2010) are built by fully automatically analyzing translated text and learning the rules. SMT has been embraced by the academic and commercial research communities as the new dominant paradigm in machine translation. Almost all recently published papers on machine translation are published on new SMT techniques. The methodology has left the research labs and become the basis of successful companies such as Language Weaver and the highly visible Google and Microsoft web translation services. Even traditional rule-based companies such as Systran have embraced statistical methods and integrated them in"
2010.jec-1.4,D07-1104,0,0.0188109,"cond paragraph of Article 21 is deleted . with the XML frame, we can rewrite this frame as a hierarchical phrase rule and provide it to a hierarchical decoder: ( The second paragraph of Article X is deleted . ; A` l’ article X , le texte du deuxi´eme alin´ea est supprim´e . ) In practice, hierarchical models do not use such large rules (and keep in mind, this particular rule is drawn from a relatively short sentence, thus containing few words and only one non-terminal). But this is purely due to scaling issues and concerns about the size of the rule table. The issues can be resolved. In fact, Lopez (2007) presented a method to compute very large translation rules on the fly for hierarchical models. While these rules were limited to two non-terminals, they could contain any number of words — a very similar situation to our XML frames. November 4th , 2010 Philipp Koehn and Jean Senellart Acquis Figure 5: TM matches as very large rules (VLR): Encoding TM match frames as very large hierarchical grammar rules (VLR) outperforms all previous methods. 4.3 Results We train a hierarchical phrase-based model with Moses, which has very similar performance as the phrase-based model used in the previous exp"
2010.jec-1.4,W99-0604,0,0.0182397,"Missing"
2010.jec-1.4,2009.mtsummit-papers.14,0,0.207381,"Missing"
2010.jec-1.4,P10-1063,0,0.0126719,"Missing"
2010.jec-1.4,2009.mtsummit-papers.16,0,0.0278386,"Missing"
2010.jec-1.4,W10-3806,0,0.116613,"Missing"
2010.jec-1.4,C10-2043,0,\N,Missing
2010.jec-1.4,steinberger-etal-2006-jrc,0,\N,Missing
2010.jec-1.4,P07-2045,1,\N,Missing
2011.iwslt-evaluation.24,2009.iwslt-papers.4,1,0.871245,"nslation. While most features can be summed over grammar rules that comprise a constituent, language models examine cross-constituent N -grams. A straightforward dynamic programming algorithm [1] accounts for these N grams by combining hypotheses only if their first N − 1 and last N − 1 words are the same. This algorithm takes O(V 2N −2 ) time and space per constituent, where V is the vocabulary size. That is too expensive, so practical decoders implement approximate search by estimating the probability of sentence fragments for purposes of pruning and prioritization. We focus on the decoders [2, 3, 4] that build translations bottom-up by recursively concatenating sentence fragments, estimating their score after each rule application. Cube pruning [5] is a commonly-implemented method to prioritize and prune grammar rule applications. Within a hypergraph node, each non-terminal has a set of possible values. Applying a rule consists of choosing a value for each non-terminal and scoring. Cube pruning estimates that the score under rule application will be the product (or sum in log space) of the scores of the rule itself and of each value. It then uses these estimates to prioritize rule applic"
2011.iwslt-evaluation.24,P10-4002,0,0.0502427,"nslation. While most features can be summed over grammar rules that comprise a constituent, language models examine cross-constituent N -grams. A straightforward dynamic programming algorithm [1] accounts for these N grams by combining hypotheses only if their first N − 1 and last N − 1 words are the same. This algorithm takes O(V 2N −2 ) time and space per constituent, where V is the vocabulary size. That is too expensive, so practical decoders implement approximate search by estimating the probability of sentence fragments for purposes of pruning and prioritization. We focus on the decoders [2, 3, 4] that build translations bottom-up by recursively concatenating sentence fragments, estimating their score after each rule application. Cube pruning [5] is a commonly-implemented method to prioritize and prune grammar rule applications. Within a hypergraph node, each non-terminal has a set of possible values. Applying a rule consists of choosing a value for each non-terminal and scoring. Cube pruning estimates that the score under rule application will be the product (or sum in log space) of the scores of the rule itself and of each value. It then uses these estimates to prioritize rule applic"
2011.iwslt-evaluation.24,W09-0424,0,0.0335352,"nslation. While most features can be summed over grammar rules that comprise a constituent, language models examine cross-constituent N -grams. A straightforward dynamic programming algorithm [1] accounts for these N grams by combining hypotheses only if their first N − 1 and last N − 1 words are the same. This algorithm takes O(V 2N −2 ) time and space per constituent, where V is the vocabulary size. That is too expensive, so practical decoders implement approximate search by estimating the probability of sentence fragments for purposes of pruning and prioritization. We focus on the decoders [2, 3, 4] that build translations bottom-up by recursively concatenating sentence fragments, estimating their score after each rule application. Cube pruning [5] is a commonly-implemented method to prioritize and prune grammar rule applications. Within a hypergraph node, each non-terminal has a set of possible values. Applying a rule consists of choosing a value for each non-terminal and scoring. Cube pruning estimates that the score under rule application will be the product (or sum in log space) of the scores of the rule itself and of each value. It then uses these estimates to prioritize rule applic"
2011.iwslt-evaluation.24,J07-2003,0,0.117216,"ard dynamic programming algorithm [1] accounts for these N grams by combining hypotheses only if their first N − 1 and last N − 1 words are the same. This algorithm takes O(V 2N −2 ) time and space per constituent, where V is the vocabulary size. That is too expensive, so practical decoders implement approximate search by estimating the probability of sentence fragments for purposes of pruning and prioritization. We focus on the decoders [2, 3, 4] that build translations bottom-up by recursively concatenating sentence fragments, estimating their score after each rule application. Cube pruning [5] is a commonly-implemented method to prioritize and prune grammar rule applications. Within a hypergraph node, each non-terminal has a set of possible values. Applying a rule consists of choosing a value for each non-terminal and scoring. Cube pruning estimates that the score under rule application will be the product (or sum in log space) of the scores of the rule itself and of each value. It then uses these estimates to prioritize rule applications, starting with the highest estimated score. This process continues until the pop limit is reached, which acts as a hard limit on the number of ru"
2011.iwslt-evaluation.24,P06-1098,0,0.251231,"tate, the decoder recombines them, thus efficiently reasoning over many sentence fragments via dynamic programming. To increase recombination, it is desirable to encode less than 2N –2 words where possible. In this paper, we make three improvements related to state and concatenation: 1. Minimizing the number of words encoded by left state, enabling more recombination. 2. Encoding left state using pointers into the language model’s data structure, making concatenation faster. 3. Avoiding queries that will not impact the estimated score, speeding concatenation. 183 2. Related Work Some decoders [6, 7] avoid left state entirely by building translations left-to-right. Hypotheses may therefore recombine, for purposes of language modeling, when their right states are equal. Typically, these decoders use beam search, where the beam consists of approximately comparable hypotheses, such as those of equal length. These decoders have the advantage that more recombinations do happen, although they risk repeating work because constituents are evaluated in multiple different contexts. The purpose of our work here is not to decide whether one search algorithm or approximation is better, but simply to i"
2011.iwslt-evaluation.24,D10-1027,0,0.256726,"tate, the decoder recombines them, thus efficiently reasoning over many sentence fragments via dynamic programming. To increase recombination, it is desirable to encode less than 2N –2 words where possible. In this paper, we make three improvements related to state and concatenation: 1. Minimizing the number of words encoded by left state, enabling more recombination. 2. Encoding left state using pointers into the language model’s data structure, making concatenation faster. 3. Avoiding queries that will not impact the estimated score, speeding concatenation. 183 2. Related Work Some decoders [6, 7] avoid left state entirely by building translations left-to-right. Hypotheses may therefore recombine, for purposes of language modeling, when their right states are equal. Typically, these decoders use beam search, where the beam consists of approximately comparable hypotheses, such as those of equal length. These decoders have the advantage that more recombinations do happen, although they risk repeating work because constituents are evaluated in multiple different contexts. The purpose of our work here is not to decide whether one search algorithm or approximation is better, but simply to i"
2011.iwslt-evaluation.24,P07-1019,0,0.165067,"s of language modeling, when their right states are equal. Typically, these decoders use beam search, where the beam consists of approximately comparable hypotheses, such as those of equal length. These decoders have the advantage that more recombinations do happen, although they risk repeating work because constituents are evaluated in multiple different contexts. The purpose of our work here is not to decide whether one search algorithm or approximation is better, but simply to improve the commonlyimplemented bottom-up strategy. A faster alternative to bottom-up cube pruning is cube growing [8] that lazily generates hypotheses for each constituent instead of generating a fixed number. Like cube pruning, cube growing generates sentence fragments, recombines them into hypotheses, and ranks hypotheses according to estimated language model probabilities. The improvements we discuss here are therefore complementary, since the effect of our work is to improve recombination and ranking within each constituent. Prior work [9] described and implemented algorithms to minimize left and right state in the context of a bottom-up chart decoder. For hypotheses shorter than N –1 words, they store t"
2011.iwslt-evaluation.24,W08-0402,0,0.491083,"algorithm or approximation is better, but simply to improve the commonlyimplemented bottom-up strategy. A faster alternative to bottom-up cube pruning is cube growing [8] that lazily generates hypotheses for each constituent instead of generating a fixed number. Like cube pruning, cube growing generates sentence fragments, recombines them into hypotheses, and ranks hypotheses according to estimated language model probabilities. The improvements we discuss here are therefore complementary, since the effect of our work is to improve recombination and ranking within each constituent. Prior work [9] described and implemented algorithms to minimize left and right state in the context of a bottom-up chart decoder. For hypotheses shorter than N –1 words, they store the entire hypothesis in state. In our work, we apply state minimization to all hypotheses, including those shorter than N –1 words. For hypotheses longer than N –1 words, we minimize state in the same way that [9] does. While [9] described an “inefficient implementation of the prefix- and suffix-lookup”, we store the additional information with each n-gram entry, incurring minimal overhead and reusing lookups already performed i"
2011.iwslt-evaluation.24,W11-2123,1,0.837556,"e n-gram that it matched with each query. Decoders can use this information to store at most n words in left or right state, depending on the position of the words being queried. However, this does not fully minimize state, as w1n may be matched by the model, but w1n v may not be in the model for any word v. In this case w1 may be safely omitted from right state but this information is hidden from the decoder. Similarly, were the toolkit to indicate that vw1n does not appear for any word v (i.e. w1n does not extend left), then wn could be omitted from left state. In this work, we extend KenLM [12] to store and expose the necessary information. It implements two data structures, probing and trie. The probing data structure is a hash table from n-grams to probability and backoff and is byte-aligned for speed. The trie data structure is a reverse trie similar to SRILM and IRSTLM but with bit-level packing (i.e. it uses 31 bits to store probability since the sign bit is always negative). Since entries that do not extend right have zero backoff, a special backoff value flags n-grams that do not extend right; this information is provided to the decoder, as is the length of n-gram matched. We"
2011.iwslt-evaluation.24,P02-1040,0,0.0855548,"Missing"
2011.iwslt-evaluation.24,2005.mtsummit-papers.11,1,0.0814087,"Missing"
2011.iwslt-evaluation.24,W11-2103,1,0.794863,"Missing"
2011.iwslt-evaluation.24,J03-4003,0,\N,Missing
2012.amta-papers.9,W10-1705,0,0.0209992,"ut back off to the decomposed model for unknown word forms. Interpolated backoff models combine surface and factored translation models, relying more heavily on the surface models for frequent words, and more heavily on the factored models for the rare words. We show that using interpolated backoff improves translation quality, especially of rare nouns and adjectives. 2 Related Work Factored translation models (Koehn and Hoang, 2007) were introduced to overcome data sparsity in morphologically rich languages. Positive results have been reported for languages such as Czech, Turkish, or German (Bojar and Kos, 2010; Yeniterzi and Oflazer, 2010; Koehn et al., 2010). The idea of pooling the evidence of morphologically related words is similar to the automatic clustering of phrases (Kuhn et al., 2010). The popular Arabic–English language pair has received attention in the context of source language morphology reduction. Most work in this area involves splitting off affixes from complex Arabic words that translate into English words of their own (Sadat and Habash, 2006; Popovi´c and Ney, 2004). A concentrated effort on reducing out-ofvocabulary words in Arabic is reported by Habash (2008), which includes th"
2012.amta-papers.9,W11-2103,1,0.923519,"ntrated effort on reducing out-ofvocabulary words in Arabic is reported by Habash (2008), which includes the application of stemming, as we do here. However, in our work, we also address the translation of rare words and use a more complex factored decomposed model for the handling of unknown words. Backoff to stemmed models was explored by Yang and Kirchhoff (2006). Corpus Sentences Europarl News Comm. News Test 11 1,739,154 136,227 3,003 Words English German 48,446,385 45,974,070 3,373,154 3,443,348 75,762 73,726 Table 1: Size of corpora used in experiments. Data from WMT 2011 shared tasks (Callison-Burch et al., 2011). The idea of interpolated backoff stems from language modelling, where it is used in smoothing methods such as Witten-Bell (Witten and Bell, 1991) and Kneser-Ney (Kneser and Ney, 1995). See Chen and Goodman (1998) for an overview. Smoothing methods were previously used by Foster et al. (2006) to discount rare translations, but not in combination with backoff methods. 3 Anatomy of Lexical Sparsity Before we dive into the details of our method, let us first gather some empirical insights into the problem we address. Our work is motivated by overcoming lexical sparsity in corpora of morphologica"
2012.amta-papers.9,P05-1066,1,0.830553,"See Figure 5 for an example of this process. we subtract a fixed number D from each count when deriving probabilities for observed translations e We carry out all our experiments on the German– English language pair, relying on data made available for the 2011 Workshop for Statistical Machine Translation (Callison-Burch et al., 2011). Training data is from European Parliament proceedings and collected news commentaries. The test set consists of a collection of news stories. As is common for this language set, we perform compound splitting (Koehn and Knight, 2003) and syntactic prereordering (Collins et al., 2005). We annotate input words and output words with all three factors (surface, lemma, morphology). This allows us to use 5-gram lemma and 7-gram morphology sequence models to support language modeling. The lexicalized reordering model is based on lemmata, so we can avoid inconsistencies between its use for translations from the joint and decomposed factored translation models. Word alignment is also performed on lemmata instead of surface forms. Phrase length is limited to four words, otherwise default Moses parameters are used. The fullyfactored phrase-based model outperforms a pure surface form"
2012.amta-papers.9,W06-1607,0,0.0248538,"ackoff to stemmed models was explored by Yang and Kirchhoff (2006). Corpus Sentences Europarl News Comm. News Test 11 1,739,154 136,227 3,003 Words English German 48,446,385 45,974,070 3,373,154 3,443,348 75,762 73,726 Table 1: Size of corpora used in experiments. Data from WMT 2011 shared tasks (Callison-Burch et al., 2011). The idea of interpolated backoff stems from language modelling, where it is used in smoothing methods such as Witten-Bell (Witten and Bell, 1991) and Kneser-Ney (Kneser and Ney, 1995). See Chen and Goodman (1998) for an overview. Smoothing methods were previously used by Foster et al. (2006) to discount rare translations, but not in combination with backoff methods. 3 Anatomy of Lexical Sparsity Before we dive into the details of our method, let us first gather some empirical insights into the problem we address. Our work is motivated by overcoming lexical sparsity in corpora of morphologically rich languages. But how big is the portion of rare words in the test set and do we translate them significantly worse? We examined these questions on the German–English language pair, given the News Commentary and Europarl training corpora and the WMT 2011 test set (corpus sizes are given"
2012.amta-papers.9,P08-2015,0,0.0373354,", or German (Bojar and Kos, 2010; Yeniterzi and Oflazer, 2010; Koehn et al., 2010). The idea of pooling the evidence of morphologically related words is similar to the automatic clustering of phrases (Kuhn et al., 2010). The popular Arabic–English language pair has received attention in the context of source language morphology reduction. Most work in this area involves splitting off affixes from complex Arabic words that translate into English words of their own (Sadat and Habash, 2006; Popovi´c and Ney, 2004). A concentrated effort on reducing out-ofvocabulary words in Arabic is reported by Habash (2008), which includes the application of stemming, as we do here. However, in our work, we also address the translation of rare words and use a more complex factored decomposed model for the handling of unknown words. Backoff to stemmed models was explored by Yang and Kirchhoff (2006). Corpus Sentences Europarl News Comm. News Test 11 1,739,154 136,227 3,003 Words English German 48,446,385 45,974,070 3,373,154 3,443,348 75,762 73,726 Table 1: Size of corpora used in experiments. Data from WMT 2011 shared tasks (Callison-Burch et al., 2011). The idea of interpolated backoff stems from language model"
2012.amta-papers.9,W11-2145,0,0.0435195,"Missing"
2012.amta-papers.9,D11-1125,0,0.0692783,"Missing"
2012.amta-papers.9,W10-1715,1,0.823527,"rd forms. Interpolated backoff models combine surface and factored translation models, relying more heavily on the surface models for frequent words, and more heavily on the factored models for the rare words. We show that using interpolated backoff improves translation quality, especially of rare nouns and adjectives. 2 Related Work Factored translation models (Koehn and Hoang, 2007) were introduced to overcome data sparsity in morphologically rich languages. Positive results have been reported for languages such as Czech, Turkish, or German (Bojar and Kos, 2010; Yeniterzi and Oflazer, 2010; Koehn et al., 2010). The idea of pooling the evidence of morphologically related words is similar to the automatic clustering of phrases (Kuhn et al., 2010). The popular Arabic–English language pair has received attention in the context of source language morphology reduction. Most work in this area involves splitting off affixes from complex Arabic words that translate into English words of their own (Sadat and Habash, 2006; Popovi´c and Ney, 2004). A concentrated effort on reducing out-ofvocabulary words in Arabic is reported by Habash (2008), which includes the application of stemming, as we do here. However,"
2012.amta-papers.9,D07-1091,1,0.966644,"BLEU (German–English) over phrase-based models, due to the better translation of rare nouns and adjectives. 1 Introduction Morphologically rich languages pose a special challenge to statistical machine translation. One aspect of the problem is the generative process yielding many surface forms from a single lemma, causing sparse data problems in model estimation, affecting both the translation model and the language model. Another aspect is the prediction of the correct morphological features which may require larger syntactic or even semantic context to resolve. Factored translation models (Koehn and Hoang, 2007) were proposed as a formalism to address these challenges. This modeling framework allows for arbitrary decomposition and enrichment of phrasebased translation models. For morphologically rich languages, one application of this framework is the decomposition of phrase translation into two translation steps, one for lemmata and one for morphological properties, and a generation step to produce the target surface form. While such factored translation models increase robustness by basing statistics on the more frequent lemmata instead of the sparser surface forms, they do make strong independence"
2012.amta-papers.9,P07-2045,1,0.0174468,"thods. 3 Anatomy of Lexical Sparsity Before we dive into the details of our method, let us first gather some empirical insights into the problem we address. Our work is motivated by overcoming lexical sparsity in corpora of morphologically rich languages. But how big is the portion of rare words in the test set and do we translate them significantly worse? We examined these questions on the German–English language pair, given the News Commentary and Europarl training corpora and the WMT 2011 test set (corpus sizes are given in Table 1). We trained a phrase-based translation model using Moses (Koehn et al., 2007) with mostly default parameters (for more details, please check the experimental section). 3.1 Computation of Source Word Translation Precision The question, if a (potentially rare) input word has been translated correctly, does unfortunately not have a straight-forward answer: while target words can be compared against a reference translation, source words need to first tracked to their target word translations (if any), which then in turn can be compared against a reference. We proceed as follows (see Figure 1). We record the word alignment within the phrase mappings, to closely track which"
2012.amta-papers.9,E03-1076,1,0.802171,"he surface generation probability is almost always 1. See Figure 5 for an example of this process. we subtract a fixed number D from each count when deriving probabilities for observed translations e We carry out all our experiments on the German– English language pair, relying on data made available for the 2011 Workshop for Statistical Machine Translation (Callison-Burch et al., 2011). Training data is from European Parliament proceedings and collected news commentaries. The test set consists of a collection of news stories. As is common for this language set, we perform compound splitting (Koehn and Knight, 2003) and syntactic prereordering (Collins et al., 2005). We annotate input words and output words with all three factors (surface, lemma, morphology). This allows us to use 5-gram lemma and 7-gram morphology sequence models to support language modeling. The lexicalized reordering model is based on lemmata, so we can avoid inconsistencies between its use for translations from the joint and decomposed factored translation models. Word alignment is also performed on lemmata instead of surface forms. Phrase length is limited to four words, otherwise default Moses parameters are used. The fullyfactored"
2012.amta-papers.9,N03-1017,1,0.0484067,"ords increases, but not at the same rate as the corpus increase. There are still significant number of rare nouns left — roughly a third occur less than 32 times. It is worthwhile to point out that nouns carry a substantial amount of meaning and their mistranslation is typically more serious than a dropped determiner or punctuation token. Translating them well is important. INPUT OUTPUT word word lemma lemma morphology morphology Figure 3: Factored translation model: Phrase translation is decomposed into a number of mapping steps. 4 Method Our method involves a traditional phrase-based model (Koehn et al., 2003) and a factored translation model (Koehn and Hoang, 2007). The traditional phrase based model is estimated using statistics on phrase mappings found in an automatically word-aligned parallel corpus. 4.1 Decomposed Factored Model The factored translation model decomposes the translation of a phrase into a number of mapping steps. See Figure 3 for an illustration. The decomposition involves two translation steps (between lemmata and between morphologically features) and two generation steps (from lemma to morphologically features and for the generation of the surface from both). Formally, we int"
2012.amta-papers.9,C10-1069,0,0.0490081,"Missing"
2012.amta-papers.9,popovic-ney-2004-towards,0,0.196254,"Missing"
2012.amta-papers.9,P06-1001,0,0.0284577,"overcome data sparsity in morphologically rich languages. Positive results have been reported for languages such as Czech, Turkish, or German (Bojar and Kos, 2010; Yeniterzi and Oflazer, 2010; Koehn et al., 2010). The idea of pooling the evidence of morphologically related words is similar to the automatic clustering of phrases (Kuhn et al., 2010). The popular Arabic–English language pair has received attention in the context of source language morphology reduction. Most work in this area involves splitting off affixes from complex Arabic words that translate into English words of their own (Sadat and Habash, 2006; Popovi´c and Ney, 2004). A concentrated effort on reducing out-ofvocabulary words in Arabic is reported by Habash (2008), which includes the application of stemming, as we do here. However, in our work, we also address the translation of rare words and use a more complex factored decomposed model for the handling of unknown words. Backoff to stemmed models was explored by Yang and Kirchhoff (2006). Corpus Sentences Europarl News Comm. News Test 11 1,739,154 136,227 3,003 Words English German 48,446,385 45,974,070 3,373,154 3,443,348 75,762 73,726 Table 1: Size of corpora used in experiments."
2012.amta-papers.9,C00-2105,0,0.0392933,"Missing"
2012.amta-papers.9,E06-1006,0,0.0157478,"ceived attention in the context of source language morphology reduction. Most work in this area involves splitting off affixes from complex Arabic words that translate into English words of their own (Sadat and Habash, 2006; Popovi´c and Ney, 2004). A concentrated effort on reducing out-ofvocabulary words in Arabic is reported by Habash (2008), which includes the application of stemming, as we do here. However, in our work, we also address the translation of rare words and use a more complex factored decomposed model for the handling of unknown words. Backoff to stemmed models was explored by Yang and Kirchhoff (2006). Corpus Sentences Europarl News Comm. News Test 11 1,739,154 136,227 3,003 Words English German 48,446,385 45,974,070 3,373,154 3,443,348 75,762 73,726 Table 1: Size of corpora used in experiments. Data from WMT 2011 shared tasks (Callison-Burch et al., 2011). The idea of interpolated backoff stems from language modelling, where it is used in smoothing methods such as Witten-Bell (Witten and Bell, 1991) and Kneser-Ney (Kneser and Ney, 1995). See Chen and Goodman (1998) for an overview. Smoothing methods were previously used by Foster et al. (2006) to discount rare translations, but not in com"
2012.amta-papers.9,P10-1047,0,0.0135194,"composed model for unknown word forms. Interpolated backoff models combine surface and factored translation models, relying more heavily on the surface models for frequent words, and more heavily on the factored models for the rare words. We show that using interpolated backoff improves translation quality, especially of rare nouns and adjectives. 2 Related Work Factored translation models (Koehn and Hoang, 2007) were introduced to overcome data sparsity in morphologically rich languages. Positive results have been reported for languages such as Czech, Turkish, or German (Bojar and Kos, 2010; Yeniterzi and Oflazer, 2010; Koehn et al., 2010). The idea of pooling the evidence of morphologically related words is similar to the automatic clustering of phrases (Kuhn et al., 2010). The popular Arabic–English language pair has received attention in the context of source language morphology reduction. Most work in this area involves splitting off affixes from complex Arabic words that translate into English words of their own (Sadat and Habash, 2006; Popovi´c and Ney, 2004). A concentrated effort on reducing out-ofvocabulary words in Arabic is reported by Habash (2008), which includes the application of stemming, as"
2012.iwslt-evaluation.4,2012.iwslt-evaluation.1,0,0.0377376,"Abstract 2.1. Acoustic modelling This paper describes the University of Edinburgh (UEDIN) systems for the IWSLT 2012 Evaluation. We participated in the ASR (English), MT (English-French, German-English) and SLT (English-French) tracks. 1. Introduction We report on experiments carried out for the development of automatic speech recognition (ASR), machine translation (MT) and spoken language translation (SLT) systems on the datasets of the International Workshop on Spoken Language Translation (IWSLT) 2012. Details about the evaluation campaign and the different evaluation tracks can be found in [1]. For the ASR track, we focused on the use of adaptive tandem features derived from deep neural networks, trained on both in-domain data from TED talks [2], and out-of-domain data from a corpus of meetings. Our experiments for the MT track compare approaches to data ﬁltering and phrase table adaptation and focus on adaptation by adding sparse lexicalised features. We explore different tuning setups on in-domain and mixed-domain systems. For the SLT track, we carried out experiments with a punctuation insertion system as an intermediate step between speech recognition and machine translation, f"
2012.iwslt-evaluation.4,2012.eamt-1.60,0,0.0411397,"(English), MT (English-French, German-English) and SLT (English-French) tracks. 1. Introduction We report on experiments carried out for the development of automatic speech recognition (ASR), machine translation (MT) and spoken language translation (SLT) systems on the datasets of the International Workshop on Spoken Language Translation (IWSLT) 2012. Details about the evaluation campaign and the different evaluation tracks can be found in [1]. For the ASR track, we focused on the use of adaptive tandem features derived from deep neural networks, trained on both in-domain data from TED talks [2], and out-of-domain data from a corpus of meetings. Our experiments for the MT track compare approaches to data ﬁltering and phrase table adaptation and focus on adaptation by adding sparse lexicalised features. We explore different tuning setups on in-domain and mixed-domain systems. For the SLT track, we carried out experiments with a punctuation insertion system as an intermediate step between speech recognition and machine translation, focussing on pre- and post-processing steps and comparing different tuning sets. 2. Automatic Speech Recognition (ASR) In this section we describe the 2012"
2012.iwslt-evaluation.4,P05-1066,1,0.826144,"Missing"
2012.iwslt-evaluation.4,2012.iwslt-papers.17,1,0.866553,"Missing"
2012.iwslt-evaluation.4,D11-1033,0,0.0820468,"Missing"
2012.iwslt-evaluation.4,W12-3154,1,0.896939,"Missing"
2012.iwslt-evaluation.4,2011.iwslt-evaluation.14,0,0.0925714,"Missing"
2012.iwslt-evaluation.4,E03-1076,1,0.840572,"Missing"
2012.iwslt-evaluation.4,P07-2045,1,\N,Missing
2012.iwslt-evaluation.4,W10-1711,0,\N,Missing
2012.iwslt-evaluation.4,2010.iwslt-evaluation.22,0,\N,Missing
2012.iwslt-evaluation.4,2011.iwslt-evaluation.18,0,\N,Missing
2012.iwslt-papers.17,D10-1044,0,0.0509215,"Missing"
2012.iwslt-papers.17,P07-2045,1,0.0231833,"Missing"
2012.iwslt-papers.17,D09-1074,0,0.171381,"Missing"
2012.iwslt-papers.17,D09-1022,0,0.0663516,"Missing"
2012.iwslt-papers.17,P03-1021,0,0.052503,"Missing"
2012.iwslt-papers.17,P02-1040,0,0.101407,"Missing"
2012.iwslt-papers.17,P12-1002,0,0.0240912,"Missing"
2012.iwslt-papers.17,P12-1048,0,0.0682546,"Missing"
2012.iwslt-papers.17,D07-1080,0,0.0207495,"Missing"
2012.iwslt-papers.17,eisele-chen-2010-multiun,0,\N,Missing
2012.iwslt-papers.17,D11-1033,0,\N,Missing
2012.iwslt-papers.17,P11-2080,0,\N,Missing
2012.iwslt-papers.17,N09-1025,0,\N,Missing
2012.iwslt-papers.17,P12-2023,0,\N,Missing
2012.iwslt-papers.17,W07-0717,0,\N,Missing
2012.iwslt-papers.17,2012.eamt-1.60,0,\N,Missing
2012.iwslt-papers.5,W12-3102,1,0.837756,"campaigns where research labs use the latest prototype of their system to translate a ﬁxed test set, which is then ranked by human judges. Given the nature of the translation problem, where everybody seems to disagree on what the right translation of a sentence is, it comes of no surprise that the methods used to obtain human judgments and rank different systems against each other is also under constant debate. This paper presents a Monte Carlo simulation that closely follows the current practice in the evaluation campaigns carried out for the Workshop on Statistical Machine Translation (WMT [1]), the International Workshop on Spoken Language Translation (IWSLT [2]), and to a lesser degree, since it mostly relies on automatic metrics, the Open Machine Translation Evaluation organized by NIST (OpenMT1 ). The main questions we answer are: How many judgments do we need to collect to reach a reasonably deﬁnitive statement about the relative quality of submitted systems? Are we ranking systems the right way? How do we obtain proper conﬁdence bounds for the rankings? 2. Related Work While manual evaluation of machine translation systems has a rich history, most recent evaluation campaigns"
2012.iwslt-papers.5,2010.iwslt-evaluation.1,0,0.0270231,"to translate a ﬁxed test set, which is then ranked by human judges. Given the nature of the translation problem, where everybody seems to disagree on what the right translation of a sentence is, it comes of no surprise that the methods used to obtain human judgments and rank different systems against each other is also under constant debate. This paper presents a Monte Carlo simulation that closely follows the current practice in the evaluation campaigns carried out for the Workshop on Statistical Machine Translation (WMT [1]), the International Workshop on Spoken Language Translation (IWSLT [2]), and to a lesser degree, since it mostly relies on automatic metrics, the Open Machine Translation Evaluation organized by NIST (OpenMT1 ). The main questions we answer are: How many judgments do we need to collect to reach a reasonably deﬁnitive statement about the relative quality of submitted systems? Are we ranking systems the right way? How do we obtain proper conﬁdence bounds for the rankings? 2. Related Work While manual evaluation of machine translation systems has a rich history, most recent evaluation campaigns and labinternal manual evaluations restrict themselves to a ranking tas"
2012.iwslt-papers.5,W11-2101,0,0.280632,"conﬁdence bounds for the rankings? 2. Related Work While manual evaluation of machine translation systems has a rich history, most recent evaluation campaigns and labinternal manual evaluations restrict themselves to a ranking task. A human judge is asked, if, for a given input sentence, she prefers output from system A over output from system B. While this is a straight-forward procedure, the question how to convert these pairwise rankings into an overall rank1 http://www.nist.gov/itl/iad/mig/openmt.cfm ing of several machine translation systems has recently received attention. Bojar et al. [3] critiqued the ongoing practice in the WMT evaluation campaigns, which was subsequently changed. Lopez [4] proposed an alternative method to rank systems. We will discuss these methods in more detail below. An intriguing new development in human involvement in the evaluation of machine translation output is HyTER [5]. Automatic metrics suffer from the fact that a handful of human reference translations cannot expected to be matched by other human or machine translators, even if the latter are perfectly ﬁne translations. The idea behind HyTER is to list all possible correct translations in the"
2012.iwslt-papers.5,W12-3101,0,0.121782,"has a rich history, most recent evaluation campaigns and labinternal manual evaluations restrict themselves to a ranking task. A human judge is asked, if, for a given input sentence, she prefers output from system A over output from system B. While this is a straight-forward procedure, the question how to convert these pairwise rankings into an overall rank1 http://www.nist.gov/itl/iad/mig/openmt.cfm ing of several machine translation systems has recently received attention. Bojar et al. [3] critiqued the ongoing practice in the WMT evaluation campaigns, which was subsequently changed. Lopez [4] proposed an alternative method to rank systems. We will discuss these methods in more detail below. An intriguing new development in human involvement in the evaluation of machine translation output is HyTER [5]. Automatic metrics suffer from the fact that a handful of human reference translations cannot expected to be matched by other human or machine translators, even if the latter are perfectly ﬁne translations. The idea behind HyTER is to list all possible correct translations in the compact format of a recursive transition network (RTN). These networks are constructed by a human annotato"
2012.iwslt-papers.5,N12-1017,0,0.0149902,"em A over output from system B. While this is a straight-forward procedure, the question how to convert these pairwise rankings into an overall rank1 http://www.nist.gov/itl/iad/mig/openmt.cfm ing of several machine translation systems has recently received attention. Bojar et al. [3] critiqued the ongoing practice in the WMT evaluation campaigns, which was subsequently changed. Lopez [4] proposed an alternative method to rank systems. We will discuss these methods in more detail below. An intriguing new development in human involvement in the evaluation of machine translation output is HyTER [5]. Automatic metrics suffer from the fact that a handful of human reference translations cannot expected to be matched by other human or machine translators, even if the latter are perfectly ﬁne translations. The idea behind HyTER is to list all possible correct translations in the compact format of a recursive transition network (RTN). These networks are constructed by a human annotator who has access to the source sentence. Machine translation output is then matched against this network using string edit distance, and the number of edits is used as a metric. Construction of the networks takes"
2012.iwslt-papers.5,1993.eamt-1.1,0,0.3813,"Missing"
2012.iwslt-papers.5,S13-1034,0,\N,Missing
2012.iwslt-papers.5,de-marneffe-etal-2006-generating,0,\N,Missing
2012.iwslt-papers.5,W13-2249,0,\N,Missing
2012.iwslt-papers.5,N04-1013,0,\N,Missing
2012.iwslt-papers.5,W02-1001,0,\N,Missing
2012.iwslt-papers.5,W09-0441,0,\N,Missing
2012.iwslt-papers.5,N03-1031,0,\N,Missing
2012.iwslt-papers.5,W13-2221,1,\N,Missing
2012.iwslt-papers.5,W13-2206,0,\N,Missing
2012.iwslt-papers.5,W13-2216,0,\N,Missing
2012.iwslt-papers.5,P12-3024,0,\N,Missing
2012.iwslt-papers.5,W13-2227,0,\N,Missing
2012.iwslt-papers.5,P13-2135,0,\N,Missing
2012.iwslt-papers.5,W09-0401,1,\N,Missing
2012.iwslt-papers.5,W13-2244,0,\N,Missing
2012.iwslt-papers.5,W13-2222,0,\N,Missing
2012.iwslt-papers.5,W13-2242,0,\N,Missing
2012.iwslt-papers.5,W13-2253,0,\N,Missing
2012.iwslt-papers.5,N06-1058,0,\N,Missing
2012.iwslt-papers.5,W13-2208,0,\N,Missing
2012.iwslt-papers.5,W10-1711,0,\N,Missing
2012.iwslt-papers.5,2010.iwslt-evaluation.22,0,\N,Missing
2012.iwslt-papers.5,W13-2228,0,\N,Missing
2012.iwslt-papers.5,W13-2229,0,\N,Missing
2012.iwslt-papers.5,W13-2241,0,\N,Missing
2012.iwslt-papers.5,W11-2104,0,\N,Missing
2012.iwslt-papers.5,W06-3114,1,\N,Missing
2012.iwslt-papers.5,W13-2248,0,\N,Missing
2012.iwslt-papers.5,W13-2209,0,\N,Missing
2012.iwslt-papers.5,W10-1703,1,\N,Missing
2012.iwslt-papers.5,W13-2220,0,\N,Missing
2012.iwslt-papers.5,P13-4014,0,\N,Missing
2012.iwslt-papers.5,W13-2219,0,\N,Missing
2012.iwslt-papers.5,P10-2016,0,\N,Missing
2012.iwslt-papers.5,W08-0309,1,\N,Missing
2012.iwslt-papers.5,P13-1135,1,\N,Missing
2012.iwslt-papers.5,W13-2230,0,\N,Missing
2012.iwslt-papers.5,W13-2218,0,\N,Missing
2012.iwslt-papers.5,W13-2240,0,\N,Missing
2012.iwslt-papers.5,W13-2226,0,\N,Missing
2012.iwslt-papers.5,W13-2250,0,\N,Missing
2012.iwslt-papers.5,W13-2207,0,\N,Missing
2012.iwslt-papers.5,W12-3113,0,\N,Missing
2012.iwslt-papers.5,W13-2212,1,\N,Missing
2012.iwslt-papers.5,P13-1004,0,\N,Missing
2012.iwslt-papers.5,C12-1008,0,\N,Missing
2012.iwslt-papers.5,W13-2247,0,\N,Missing
2012.iwslt-papers.5,W13-2210,0,\N,Missing
2012.iwslt-papers.5,W13-2243,0,\N,Missing
2012.iwslt-papers.5,W12-3118,0,\N,Missing
2012.iwslt-papers.5,2013.mtsummit-papers.9,0,\N,Missing
2012.iwslt-papers.5,2012.amta-papers.13,0,\N,Missing
2013.iwslt-evaluation.16,2012.eamt-1.60,1,0.8976,"neous speech and heterogeneous topics and styles. The 1 http://www.eu-bridge.eu task is open domain, with a wide range of heavily dissimilar subjects and jargons across talks. IWSLT subdivides the task and separately evaluates automatic transcription of talks from audio to text, speech translation of talks from audio, and text translation of talks as three different tracks [3, 4]. The training data is constrained to the corpora specified by the organizers. The supplied list of corpora comprises a large amount of publicly available monolingual and parallel training data, though, including WIT3 [5], Europarl [6], MultiUN [7], the English and French Gigaword corpora as provided by the Linguistic Data Consortium [8], and the News Crawl, 109 and News Commentary corpora from the WMT shared task training data [9]. For the two “official” language pairs [1] for translation at IWSLT 2013, English→French and German→English, these resources allow for building of systems with state-of-the-art performance by participants. The EU-BRIDGE project is funded by the European Union under the Seventh Framework Programme (FP7) [10] and brings together several project partners who have each previously been v"
2013.iwslt-evaluation.16,2005.mtsummit-papers.11,1,0.078384,"nd heterogeneous topics and styles. The 1 http://www.eu-bridge.eu task is open domain, with a wide range of heavily dissimilar subjects and jargons across talks. IWSLT subdivides the task and separately evaluates automatic transcription of talks from audio to text, speech translation of talks from audio, and text translation of talks as three different tracks [3, 4]. The training data is constrained to the corpora specified by the organizers. The supplied list of corpora comprises a large amount of publicly available monolingual and parallel training data, though, including WIT3 [5], Europarl [6], MultiUN [7], the English and French Gigaword corpora as provided by the Linguistic Data Consortium [8], and the News Crawl, 109 and News Commentary corpora from the WMT shared task training data [9]. For the two “official” language pairs [1] for translation at IWSLT 2013, English→French and German→English, these resources allow for building of systems with state-of-the-art performance by participants. The EU-BRIDGE project is funded by the European Union under the Seventh Framework Programme (FP7) [10] and brings together several project partners who have each previously been very successful"
2013.iwslt-evaluation.16,eisele-chen-2010-multiun,0,0.0452925,"ous topics and styles. The 1 http://www.eu-bridge.eu task is open domain, with a wide range of heavily dissimilar subjects and jargons across talks. IWSLT subdivides the task and separately evaluates automatic transcription of talks from audio to text, speech translation of talks from audio, and text translation of talks as three different tracks [3, 4]. The training data is constrained to the corpora specified by the organizers. The supplied list of corpora comprises a large amount of publicly available monolingual and parallel training data, though, including WIT3 [5], Europarl [6], MultiUN [7], the English and French Gigaword corpora as provided by the Linguistic Data Consortium [8], and the News Crawl, 109 and News Commentary corpora from the WMT shared task training data [9]. For the two “official” language pairs [1] for translation at IWSLT 2013, English→French and German→English, these resources allow for building of systems with state-of-the-art performance by participants. The EU-BRIDGE project is funded by the European Union under the Seventh Framework Programme (FP7) [10] and brings together several project partners who have each previously been very successful in contribut"
2013.iwslt-evaluation.16,E06-1005,1,0.921596,"e-scale evaluation campaigns like IWSLT and WMT in recent years, thereby demonstrating their ability to continuously enhance their systems and promoting progress in machine translation. Machine translation research within EU-BRIDGE has a strong focus on translation of spoken language. The IWSLT TED talks task constitutes an interesting framework for empirical testing of some of the systems for spoken language translation which are developed as part of the project. The work described here is an attempt to attain translation quality beyond strong single system performance via system combination [11]. Similar cooperative approaches based on system combination have proven to be valuable for machine translation in other projects, e.g. in the Quaero programme [12, 13]. Within EU-BRIDGE, we built combined system setups for text translation of talks from English to French as well as from German to English. We found that the combined translation engines of RWTH, UEDIN, KIT, and FBK systems are very effective. In the rest of the paper we will give some insight into the technology behind the combined engines which have been used to produce the joint EU-BRIDGE submission to the IWSLT 2013 MT track"
2013.iwslt-evaluation.16,P02-1040,0,0.0892795,"-BRIDGE submission to the IWSLT 2013 MT track. The remainder of the paper is structured as follows: We first describe the individual English→French and German→English systems by RWTH Aachen University (Section 2), the University of Edinburgh (Section 3), Karlsruhe Institute of Technology (Section 4), and Fondazione Bruno Kessler (Section 5), respectively. We then present the techniques for machine translation system combination which have been employed to obtain consensus translations from the outputs of the individual systems of the project partners (Section 6). Experimental results in B LEU [14] and T ER [15] are given in Section 7. A brief error analysis on selected examples from the test data has been conducted which we discuss in Section 8. We finally conclude the paper with Section 9. 2. RWTH Aachen University RWTH applied both the phrase-based (RWTH scss) and the hierarchical (RWTH hiero) decoder implemented in RWTH’s publicly available translation toolkit Jane [16, 17, 18, 19]. The model weights of all systems were tuned with standard Minimum Error Rate Training [20] on the provided dev2010 set. RWTH used B LEU as optimization objective. Language models were created with the SR"
2013.iwslt-evaluation.16,2006.amta-papers.25,0,0.15922,"sion to the IWSLT 2013 MT track. The remainder of the paper is structured as follows: We first describe the individual English→French and German→English systems by RWTH Aachen University (Section 2), the University of Edinburgh (Section 3), Karlsruhe Institute of Technology (Section 4), and Fondazione Bruno Kessler (Section 5), respectively. We then present the techniques for machine translation system combination which have been employed to obtain consensus translations from the outputs of the individual systems of the project partners (Section 6). Experimental results in B LEU [14] and T ER [15] are given in Section 7. A brief error analysis on selected examples from the test data has been conducted which we discuss in Section 8. We finally conclude the paper with Section 9. 2. RWTH Aachen University RWTH applied both the phrase-based (RWTH scss) and the hierarchical (RWTH hiero) decoder implemented in RWTH’s publicly available translation toolkit Jane [16, 17, 18, 19]. The model weights of all systems were tuned with standard Minimum Error Rate Training [20] on the provided dev2010 set. RWTH used B LEU as optimization objective. Language models were created with the SRILM toolkit [2"
2013.iwslt-evaluation.16,W10-1738,1,0.880309,"iques for machine translation system combination which have been employed to obtain consensus translations from the outputs of the individual systems of the project partners (Section 6). Experimental results in B LEU [14] and T ER [15] are given in Section 7. A brief error analysis on selected examples from the test data has been conducted which we discuss in Section 8. We finally conclude the paper with Section 9. 2. RWTH Aachen University RWTH applied both the phrase-based (RWTH scss) and the hierarchical (RWTH hiero) decoder implemented in RWTH’s publicly available translation toolkit Jane [16, 17, 18, 19]. The model weights of all systems were tuned with standard Minimum Error Rate Training [20] on the provided dev2010 set. RWTH used B LEU as optimization objective. Language models were created with the SRILM toolkit [21]. All RWTH systems include the standard set of models provided by Jane. For English→French, the final setups for RWTH scss and RWTH hiero differ in the amount of training data and in the choice of models. For the English→French hierarchical setup the bilingual data was limited to the in-domain WIT3 data, News Commentary, Europarl, and Common Crawl corpora. The word alignment w"
2013.iwslt-evaluation.16,popovic-ney-2006-pos,1,0.929216,"ll available parallel data and trained a word alignment with GIZA++ [24]. The same language model as in the hierarchical setup was used. RWTH applied the following supplementary features for the phrase-based system: a lexicalized reordering model [25], a discriminative word lexicon [26], a 7-gram word class language model [27], a continuous space language model [28], and a second translation model from the WIT3 portion of the training data only. For German→English, RWTH decompounded the German source in a preprocessing step [29] and applied partof-speech-based long-range verb reordering rules [30]. Both systems RWTH scss and RWTH hiero rest upon all available bilingual data and word alignment obtained with GIZA++. A language model was trained on the target side of all available bilingual data plus 12 of the Shuffled News corpus and 1 4 of the English Gigaword v3 corpus, resulting in a total of 1.7 billion running words. In both German→English systems, RWTH applied a more sophisticated discriminative phrase training method. Similar to [31], a gradient-based method is used to optimize a maximum expected B LEU objective, for which we define B LEU on the sentence level with smoothed 3-gram"
2013.iwslt-evaluation.16,P03-1021,0,0.129032,"ns from the outputs of the individual systems of the project partners (Section 6). Experimental results in B LEU [14] and T ER [15] are given in Section 7. A brief error analysis on selected examples from the test data has been conducted which we discuss in Section 8. We finally conclude the paper with Section 9. 2. RWTH Aachen University RWTH applied both the phrase-based (RWTH scss) and the hierarchical (RWTH hiero) decoder implemented in RWTH’s publicly available translation toolkit Jane [16, 17, 18, 19]. The model weights of all systems were tuned with standard Minimum Error Rate Training [20] on the provided dev2010 set. RWTH used B LEU as optimization objective. Language models were created with the SRILM toolkit [21]. All RWTH systems include the standard set of models provided by Jane. For English→French, the final setups for RWTH scss and RWTH hiero differ in the amount of training data and in the choice of models. For the English→French hierarchical setup the bilingual data was limited to the in-domain WIT3 data, News Commentary, Europarl, and Common Crawl corpora. The word alignment was created with fast align [22]. A language model was trained on the target side of all avai"
2013.iwslt-evaluation.16,P12-1031,0,0.10415,"For German→English, RWTH decompounded the German source in a preprocessing step [29] and applied partof-speech-based long-range verb reordering rules [30]. Both systems RWTH scss and RWTH hiero rest upon all available bilingual data and word alignment obtained with GIZA++. A language model was trained on the target side of all available bilingual data plus 12 of the Shuffled News corpus and 1 4 of the English Gigaword v3 corpus, resulting in a total of 1.7 billion running words. In both German→English systems, RWTH applied a more sophisticated discriminative phrase training method. Similar to [31], a gradient-based method is used to optimize a maximum expected B LEU objective, for which we define B LEU on the sentence level with smoothed 3-gram and 4gram precisions. RWTH performed discriminative training on the WIT3 portion of the training data. The German→English phrase-based system was furthermore improved by a lexicalized reordering model and 7-gram word class language model. RWTH finally applied domain adaptation by adding a second translation model to the decoder which was trained on the WIT3 portion of the data only. This second translation model was likewise improved with discri"
2013.iwslt-evaluation.16,P10-2041,0,0.0805135,"setups for RWTH scss and RWTH hiero differ in the amount of training data and in the choice of models. For the English→French hierarchical setup the bilingual data was limited to the in-domain WIT3 data, News Commentary, Europarl, and Common Crawl corpora. The word alignment was created with fast align [22]. A language model was trained on the target side of all available bilingual data plus 12 of the Shuffled News corpus and 41 of the French Gigaword Second Edition corpus. The monolingual data selection for using only parts of the corpora is based on cross-entropy difference as described in [23]. The hierarchical system was extended with a second translation model. The additional translation model was trained on the WIT3 portion of the training data only. For the English→French phrase-based setup, RWTH utilized all available parallel data and trained a word alignment with GIZA++ [24]. The same language model as in the hierarchical setup was used. RWTH applied the following supplementary features for the phrase-based system: a lexicalized reordering model [25], a discriminative word lexicon [26], a 7-gram word class language model [27], a continuous space language model [28], and a se"
2013.iwslt-evaluation.16,D08-1089,0,0.117877,"orpus. The monolingual data selection for using only parts of the corpora is based on cross-entropy difference as described in [23]. The hierarchical system was extended with a second translation model. The additional translation model was trained on the WIT3 portion of the training data only. For the English→French phrase-based setup, RWTH utilized all available parallel data and trained a word alignment with GIZA++ [24]. The same language model as in the hierarchical setup was used. RWTH applied the following supplementary features for the phrase-based system: a lexicalized reordering model [25], a discriminative word lexicon [26], a 7-gram word class language model [27], a continuous space language model [28], and a second translation model from the WIT3 portion of the training data only. For German→English, RWTH decompounded the German source in a preprocessing step [29] and applied partof-speech-based long-range verb reordering rules [30]. Both systems RWTH scss and RWTH hiero rest upon all available bilingual data and word alignment obtained with GIZA++. A language model was trained on the target side of all available bilingual data plus 12 of the Shuffled News corpus and 1 4 of"
2013.iwslt-evaluation.16,P07-2045,1,0.0125349,"EU on the sentence level with smoothed 3-gram and 4gram precisions. RWTH performed discriminative training on the WIT3 portion of the training data. The German→English phrase-based system was furthermore improved by a lexicalized reordering model and 7-gram word class language model. RWTH finally applied domain adaptation by adding a second translation model to the decoder which was trained on the WIT3 portion of the data only. This second translation model was likewise improved with discriminative phrase training. 3. University of Edinburgh UEDIN’s systems were trained using the Moses system [32], replicating the settings described in [33] developed for the 2013 Workshop on Statistical Machine Translation. The characteristics of the system include: a maximum sentence length of 80, grow-diag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM [34] used at runtime, a lexically-driven 5-gram operation sequence model [35] with four additional supportive features (two gap-based penalties, one distance-based feature and one deletion penalty), msdbidirectional-fe lexicalized reordering, sparse lexical and domain features [36], a"
2013.iwslt-evaluation.16,W13-2212,1,0.868834,"m and 4gram precisions. RWTH performed discriminative training on the WIT3 portion of the training data. The German→English phrase-based system was furthermore improved by a lexicalized reordering model and 7-gram word class language model. RWTH finally applied domain adaptation by adding a second translation model to the decoder which was trained on the WIT3 portion of the data only. This second translation model was likewise improved with discriminative phrase training. 3. University of Edinburgh UEDIN’s systems were trained using the Moses system [32], replicating the settings described in [33] developed for the 2013 Workshop on Statistical Machine Translation. The characteristics of the system include: a maximum sentence length of 80, grow-diag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM [34] used at runtime, a lexically-driven 5-gram operation sequence model [35] with four additional supportive features (two gap-based penalties, one distance-based feature and one deletion penalty), msdbidirectional-fe lexicalized reordering, sparse lexical and domain features [36], a distortion limit of 6, 100-best translation"
2013.iwslt-evaluation.16,W11-2123,0,0.0545914,"tion by adding a second translation model to the decoder which was trained on the WIT3 portion of the data only. This second translation model was likewise improved with discriminative phrase training. 3. University of Edinburgh UEDIN’s systems were trained using the Moses system [32], replicating the settings described in [33] developed for the 2013 Workshop on Statistical Machine Translation. The characteristics of the system include: a maximum sentence length of 80, grow-diag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM [34] used at runtime, a lexically-driven 5-gram operation sequence model [35] with four additional supportive features (two gap-based penalties, one distance-based feature and one deletion penalty), msdbidirectional-fe lexicalized reordering, sparse lexical and domain features [36], a distortion limit of 6, 100-best translation options, minimum Bayes risk decoding [37], cube pruning [38] with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-over-punctuation heuristic. UEDIN used the compact phrase table representation by [39]. For English→German, UEDIN used a sequen"
2013.iwslt-evaluation.16,P11-1105,1,0.916011,"d on the WIT3 portion of the data only. This second translation model was likewise improved with discriminative phrase training. 3. University of Edinburgh UEDIN’s systems were trained using the Moses system [32], replicating the settings described in [33] developed for the 2013 Workshop on Statistical Machine Translation. The characteristics of the system include: a maximum sentence length of 80, grow-diag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM [34] used at runtime, a lexically-driven 5-gram operation sequence model [35] with four additional supportive features (two gap-based penalties, one distance-based feature and one deletion penalty), msdbidirectional-fe lexicalized reordering, sparse lexical and domain features [36], a distortion limit of 6, 100-best translation options, minimum Bayes risk decoding [37], cube pruning [38] with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-over-punctuation heuristic. UEDIN used the compact phrase table representation by [39]. For English→German, UEDIN used a sequence model over morphological tags. The UEDIN systems were tuned on the dev"
2013.iwslt-evaluation.16,D09-1022,1,0.892821,"n for using only parts of the corpora is based on cross-entropy difference as described in [23]. The hierarchical system was extended with a second translation model. The additional translation model was trained on the WIT3 portion of the training data only. For the English→French phrase-based setup, RWTH utilized all available parallel data and trained a word alignment with GIZA++ [24]. The same language model as in the hierarchical setup was used. RWTH applied the following supplementary features for the phrase-based system: a lexicalized reordering model [25], a discriminative word lexicon [26], a 7-gram word class language model [27], a continuous space language model [28], and a second translation model from the WIT3 portion of the training data only. For German→English, RWTH decompounded the German source in a preprocessing step [29] and applied partof-speech-based long-range verb reordering rules [30]. Both systems RWTH scss and RWTH hiero rest upon all available bilingual data and word alignment obtained with GIZA++. A language model was trained on the target side of all available bilingual data plus 12 of the Shuffled News corpus and 1 4 of the English Gigaword v3 corpus, resu"
2013.iwslt-evaluation.16,2012.iwslt-papers.17,1,0.860668,"em [32], replicating the settings described in [33] developed for the 2013 Workshop on Statistical Machine Translation. The characteristics of the system include: a maximum sentence length of 80, grow-diag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM [34] used at runtime, a lexically-driven 5-gram operation sequence model [35] with four additional supportive features (two gap-based penalties, one distance-based feature and one deletion penalty), msdbidirectional-fe lexicalized reordering, sparse lexical and domain features [36], a distortion limit of 6, 100-best translation options, minimum Bayes risk decoding [37], cube pruning [38] with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-over-punctuation heuristic. UEDIN used the compact phrase table representation by [39]. For English→German, UEDIN used a sequence model over morphological tags. The UEDIN systems were tuned on the dev2010 set made available for the IWSLT 2013 workshop. Tuning was performed using the k-best batch MIRA algorithm [40] with a maximum number of iterations of 25. B LEU was used as the metric to evaluate resu"
2013.iwslt-evaluation.16,D13-1138,1,0.815085,"based on cross-entropy difference as described in [23]. The hierarchical system was extended with a second translation model. The additional translation model was trained on the WIT3 portion of the training data only. For the English→French phrase-based setup, RWTH utilized all available parallel data and trained a word alignment with GIZA++ [24]. The same language model as in the hierarchical setup was used. RWTH applied the following supplementary features for the phrase-based system: a lexicalized reordering model [25], a discriminative word lexicon [26], a 7-gram word class language model [27], a continuous space language model [28], and a second translation model from the WIT3 portion of the training data only. For German→English, RWTH decompounded the German source in a preprocessing step [29] and applied partof-speech-based long-range verb reordering rules [30]. Both systems RWTH scss and RWTH hiero rest upon all available bilingual data and word alignment obtained with GIZA++. A language model was trained on the target side of all available bilingual data plus 12 of the Shuffled News corpus and 1 4 of the English Gigaword v3 corpus, resulting in a total of 1.7 billion running w"
2013.iwslt-evaluation.16,N04-1022,0,0.487773,"atistical Machine Translation. The characteristics of the system include: a maximum sentence length of 80, grow-diag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM [34] used at runtime, a lexically-driven 5-gram operation sequence model [35] with four additional supportive features (two gap-based penalties, one distance-based feature and one deletion penalty), msdbidirectional-fe lexicalized reordering, sparse lexical and domain features [36], a distortion limit of 6, 100-best translation options, minimum Bayes risk decoding [37], cube pruning [38] with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-over-punctuation heuristic. UEDIN used the compact phrase table representation by [39]. For English→German, UEDIN used a sequence model over morphological tags. The UEDIN systems were tuned on the dev2010 set made available for the IWSLT 2013 workshop. Tuning was performed using the k-best batch MIRA algorithm [40] with a maximum number of iterations of 25. B LEU was used as the metric to evaluate results. While UEDIN’s main submission also includes sequence models and operation sequence m"
2013.iwslt-evaluation.16,W12-2702,0,0.051709,"cribed in [23]. The hierarchical system was extended with a second translation model. The additional translation model was trained on the WIT3 portion of the training data only. For the English→French phrase-based setup, RWTH utilized all available parallel data and trained a word alignment with GIZA++ [24]. The same language model as in the hierarchical setup was used. RWTH applied the following supplementary features for the phrase-based system: a lexicalized reordering model [25], a discriminative word lexicon [26], a 7-gram word class language model [27], a continuous space language model [28], and a second translation model from the WIT3 portion of the training data only. For German→English, RWTH decompounded the German source in a preprocessing step [29] and applied partof-speech-based long-range verb reordering rules [30]. Both systems RWTH scss and RWTH hiero rest upon all available bilingual data and word alignment obtained with GIZA++. A language model was trained on the target side of all available bilingual data plus 12 of the Shuffled News corpus and 1 4 of the English Gigaword v3 corpus, resulting in a total of 1.7 billion running words. In both German→English systems, RW"
2013.iwslt-evaluation.16,P07-1019,0,0.222647,"ranslation. The characteristics of the system include: a maximum sentence length of 80, grow-diag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM [34] used at runtime, a lexically-driven 5-gram operation sequence model [35] with four additional supportive features (two gap-based penalties, one distance-based feature and one deletion penalty), msdbidirectional-fe lexicalized reordering, sparse lexical and domain features [36], a distortion limit of 6, 100-best translation options, minimum Bayes risk decoding [37], cube pruning [38] with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-over-punctuation heuristic. UEDIN used the compact phrase table representation by [39]. For English→German, UEDIN used a sequence model over morphological tags. The UEDIN systems were tuned on the dev2010 set made available for the IWSLT 2013 workshop. Tuning was performed using the k-best batch MIRA algorithm [40] with a maximum number of iterations of 25. B LEU was used as the metric to evaluate results. While UEDIN’s main submission also includes sequence models and operation sequence models over Brown wo"
2013.iwslt-evaluation.16,E03-1076,1,0.900834,"data only. For the English→French phrase-based setup, RWTH utilized all available parallel data and trained a word alignment with GIZA++ [24]. The same language model as in the hierarchical setup was used. RWTH applied the following supplementary features for the phrase-based system: a lexicalized reordering model [25], a discriminative word lexicon [26], a 7-gram word class language model [27], a continuous space language model [28], and a second translation model from the WIT3 portion of the training data only. For German→English, RWTH decompounded the German source in a preprocessing step [29] and applied partof-speech-based long-range verb reordering rules [30]. Both systems RWTH scss and RWTH hiero rest upon all available bilingual data and word alignment obtained with GIZA++. A language model was trained on the target side of all available bilingual data plus 12 of the Shuffled News corpus and 1 4 of the English Gigaword v3 corpus, resulting in a total of 1.7 billion running words. In both German→English systems, RWTH applied a more sophisticated discriminative phrase training method. Similar to [31], a gradient-based method is used to optimize a maximum expected B LEU objective"
2013.iwslt-evaluation.16,N12-1047,0,0.148125,"penalty), msdbidirectional-fe lexicalized reordering, sparse lexical and domain features [36], a distortion limit of 6, 100-best translation options, minimum Bayes risk decoding [37], cube pruning [38] with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-over-punctuation heuristic. UEDIN used the compact phrase table representation by [39]. For English→German, UEDIN used a sequence model over morphological tags. The UEDIN systems were tuned on the dev2010 set made available for the IWSLT 2013 workshop. Tuning was performed using the k-best batch MIRA algorithm [40] with a maximum number of iterations of 25. B LEU was used as the metric to evaluate results. While UEDIN’s main submission also includes sequence models and operation sequence models over Brown word clusters, these setups were not finished in time for the contribution to the EU-BRIDGE system combination. language models trained on WIT3 , Europarl, News Commentary, 109 , and Common Crawl by minimizing the perplexity on the development data. For the class-based language model, KIT utilized in-domain WIT3 data with 4grams and 50 clusters. In addition, a 9-gram POS-based language model derived fr"
2013.iwslt-evaluation.16,2011.iwslt-evaluation.9,1,0.925869,"ge model derived from LIA POS tags [55] on all monolingual data was applied. KIT optimized the log-linear combination of all these models on the provided development data using Minimum Error Rate Training [20]. 4. Karlsruhe Institute of Technology The KIT translations have been generated by an in-house phrase-based translations system [41]. The models were trained on the Europarl, News Commentary, WIT3 , Common Crawl corpora for both directions and WMT 109 for English→French and the additional monolingual training data. The big noisy 109 and Crawl corpora were filtered using an SVM classifier [42]. In addition to the standard preprocessing, KIT used compound splitting [29] for the German text. In both translation directions, KIT performed reordering using two models. KIT encoded different reorderings of the source sentences in a word lattice. For the English→French system, only short-range rules [43] were used to generate these lattices. For German→English, long-range rules [44] and tree-based reordering rules [45] were used as well. The part-of-speech (POS) tags needed for these rules were generated by the TreeTagger [46] and the parse trees by the Stanford Parser [47]. In addition, K"
2013.iwslt-evaluation.16,2007.tmi-papers.21,0,0.422618,"e-based translations system [41]. The models were trained on the Europarl, News Commentary, WIT3 , Common Crawl corpora for both directions and WMT 109 for English→French and the additional monolingual training data. The big noisy 109 and Crawl corpora were filtered using an SVM classifier [42]. In addition to the standard preprocessing, KIT used compound splitting [29] for the German text. In both translation directions, KIT performed reordering using two models. KIT encoded different reorderings of the source sentences in a word lattice. For the English→French system, only short-range rules [43] were used to generate these lattices. For German→English, long-range rules [44] and tree-based reordering rules [45] were used as well. The part-of-speech (POS) tags needed for these rules were generated by the TreeTagger [46] and the parse trees by the Stanford Parser [47]. In addition, KIT scored the different reorderings of both language pairs using a lexicalized reordering model [48]. The phrase tables of the systems were trained using GIZA++ alignment for the English→French task and using a discriminative alignment [49] for the German→English task. KIT adapted the phrase table to the TED"
2013.iwslt-evaluation.16,W09-0435,1,0.918776,"Commentary, WIT3 , Common Crawl corpora for both directions and WMT 109 for English→French and the additional monolingual training data. The big noisy 109 and Crawl corpora were filtered using an SVM classifier [42]. In addition to the standard preprocessing, KIT used compound splitting [29] for the German text. In both translation directions, KIT performed reordering using two models. KIT encoded different reorderings of the source sentences in a word lattice. For the English→French system, only short-range rules [43] were used to generate these lattices. For German→English, long-range rules [44] and tree-based reordering rules [45] were used as well. The part-of-speech (POS) tags needed for these rules were generated by the TreeTagger [46] and the parse trees by the Stanford Parser [47]. In addition, KIT scored the different reorderings of both language pairs using a lexicalized reordering model [48]. The phrase tables of the systems were trained using GIZA++ alignment for the English→French task and using a discriminative alignment [49] for the German→English task. KIT adapted the phrase table to the TED domain using the back off approach and by also adapting the candidate selection"
2013.iwslt-evaluation.16,W13-0805,1,0.889592,"ra for both directions and WMT 109 for English→French and the additional monolingual training data. The big noisy 109 and Crawl corpora were filtered using an SVM classifier [42]. In addition to the standard preprocessing, KIT used compound splitting [29] for the German text. In both translation directions, KIT performed reordering using two models. KIT encoded different reorderings of the source sentences in a word lattice. For the English→French system, only short-range rules [43] were used to generate these lattices. For German→English, long-range rules [44] and tree-based reordering rules [45] were used as well. The part-of-speech (POS) tags needed for these rules were generated by the TreeTagger [46] and the parse trees by the Stanford Parser [47]. In addition, KIT scored the different reorderings of both language pairs using a lexicalized reordering model [48]. The phrase tables of the systems were trained using GIZA++ alignment for the English→French task and using a discriminative alignment [49] for the German→English task. KIT adapted the phrase table to the TED domain using the back off approach and by also adapting the candidate selection [50]. In addition to the phrase tabl"
2013.iwslt-evaluation.16,W08-1006,0,0.169177,"SVM classifier [42]. In addition to the standard preprocessing, KIT used compound splitting [29] for the German text. In both translation directions, KIT performed reordering using two models. KIT encoded different reorderings of the source sentences in a word lattice. For the English→French system, only short-range rules [43] were used to generate these lattices. For German→English, long-range rules [44] and tree-based reordering rules [45] were used as well. The part-of-speech (POS) tags needed for these rules were generated by the TreeTagger [46] and the parse trees by the Stanford Parser [47]. In addition, KIT scored the different reorderings of both language pairs using a lexicalized reordering model [48]. The phrase tables of the systems were trained using GIZA++ alignment for the English→French task and using a discriminative alignment [49] for the German→English task. KIT adapted the phrase table to the TED domain using the back off approach and by also adapting the candidate selection [50]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [51] and a discriminative word lexicon [52]. For the German→English task, a"
2013.iwslt-evaluation.16,2005.iwslt-1.8,1,0.888473,"t. In both translation directions, KIT performed reordering using two models. KIT encoded different reorderings of the source sentences in a word lattice. For the English→French system, only short-range rules [43] were used to generate these lattices. For German→English, long-range rules [44] and tree-based reordering rules [45] were used as well. The part-of-speech (POS) tags needed for these rules were generated by the TreeTagger [46] and the parse trees by the Stanford Parser [47]. In addition, KIT scored the different reorderings of both language pairs using a lexicalized reordering model [48]. The phrase tables of the systems were trained using GIZA++ alignment for the English→French task and using a discriminative alignment [49] for the German→English task. KIT adapted the phrase table to the TED domain using the back off approach and by also adapting the candidate selection [50]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [51] and a discriminative word lexicon [52]. For the German→English task, a discriminative word lexicon with source and target context features was applied, while only the source context feat"
2013.iwslt-evaluation.16,W08-0303,1,0.796238,"word lattice. For the English→French system, only short-range rules [43] were used to generate these lattices. For German→English, long-range rules [44] and tree-based reordering rules [45] were used as well. The part-of-speech (POS) tags needed for these rules were generated by the TreeTagger [46] and the parse trees by the Stanford Parser [47]. In addition, KIT scored the different reorderings of both language pairs using a lexicalized reordering model [48]. The phrase tables of the systems were trained using GIZA++ alignment for the English→French task and using a discriminative alignment [49] for the German→English task. KIT adapted the phrase table to the TED domain using the back off approach and by also adapting the candidate selection [50]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [51] and a discriminative word lexicon [52]. For the German→English task, a discriminative word lexicon with source and target context features was applied, while only the source context features were employed for the English→French task. During decoding, KIT used several language models to adapt the system to the task and to bet"
2013.iwslt-evaluation.16,2012.amta-papers.19,1,0.890564,"and tree-based reordering rules [45] were used as well. The part-of-speech (POS) tags needed for these rules were generated by the TreeTagger [46] and the parse trees by the Stanford Parser [47]. In addition, KIT scored the different reorderings of both language pairs using a lexicalized reordering model [48]. The phrase tables of the systems were trained using GIZA++ alignment for the English→French task and using a discriminative alignment [49] for the German→English task. KIT adapted the phrase table to the TED domain using the back off approach and by also adapting the candidate selection [50]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [51] and a discriminative word lexicon [52]. For the German→English task, a discriminative word lexicon with source and target context features was applied, while only the source context features were employed for the English→French task. During decoding, KIT used several language models to adapt the system to the task and to better model the sentence structure by means of class-based n-grams. For the German→English task, KIT used one language model trained on all data, an in-doma"
2013.iwslt-evaluation.16,W11-2124,1,0.885306,"ated by the TreeTagger [46] and the parse trees by the Stanford Parser [47]. In addition, KIT scored the different reorderings of both language pairs using a lexicalized reordering model [48]. The phrase tables of the systems were trained using GIZA++ alignment for the English→French task and using a discriminative alignment [49] for the German→English task. KIT adapted the phrase table to the TED domain using the back off approach and by also adapting the candidate selection [50]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [51] and a discriminative word lexicon [52]. For the German→English task, a discriminative word lexicon with source and target context features was applied, while only the source context features were employed for the English→French task. During decoding, KIT used several language models to adapt the system to the task and to better model the sentence structure by means of class-based n-grams. For the German→English task, KIT used one language model trained on all data, an in-domain language model trained only on the WIT3 corpus and one language model trained on 5 M sentences selected using cross-"
2013.iwslt-evaluation.16,W13-2264,1,0.887808,"se trees by the Stanford Parser [47]. In addition, KIT scored the different reorderings of both language pairs using a lexicalized reordering model [48]. The phrase tables of the systems were trained using GIZA++ alignment for the English→French task and using a discriminative alignment [49] for the German→English task. KIT adapted the phrase table to the TED domain using the back off approach and by also adapting the candidate selection [50]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [51] and a discriminative word lexicon [52]. For the German→English task, a discriminative word lexicon with source and target context features was applied, while only the source context features were employed for the English→French task. During decoding, KIT used several language models to adapt the system to the task and to better model the sentence structure by means of class-based n-grams. For the German→English task, KIT used one language model trained on all data, an in-domain language model trained only on the WIT3 corpus and one language model trained on 5 M sentences selected using cross-entropy difference [23]. Furthermore, K"
2013.iwslt-evaluation.16,2012.iwslt-papers.3,1,0.886049,"criminative word lexicon with source and target context features was applied, while only the source context features were employed for the English→French task. During decoding, KIT used several language models to adapt the system to the task and to better model the sentence structure by means of class-based n-grams. For the German→English task, KIT used one language model trained on all data, an in-domain language model trained only on the WIT3 corpus and one language model trained on 5 M sentences selected using cross-entropy difference [23]. Furthermore, KIT used an RBM-based language model [53] trained on the WIT3 corpus. Finally, KIT also used a classbased language model, trained on the WIT3 corpus using the MKCLS [54] algorithm to cluster the words. For the English→French translation task, KIT linearly combined the 5. Fondazione Bruno Kessler The FBK component of the system combination corresponds to the “contrastive 1” system of the official FBK submission. The FBK system was built upon a standard phrasebased system using the Moses toolkit [32], and exploited the huge amount of parallel English→French and monolingual French training data, provided by the organizers. It featured a"
2013.iwslt-evaluation.16,E99-1010,0,0.0594124,"ed for the English→French task. During decoding, KIT used several language models to adapt the system to the task and to better model the sentence structure by means of class-based n-grams. For the German→English task, KIT used one language model trained on all data, an in-domain language model trained only on the WIT3 corpus and one language model trained on 5 M sentences selected using cross-entropy difference [23]. Furthermore, KIT used an RBM-based language model [53] trained on the WIT3 corpus. Finally, KIT also used a classbased language model, trained on the WIT3 corpus using the MKCLS [54] algorithm to cluster the words. For the English→French translation task, KIT linearly combined the 5. Fondazione Bruno Kessler The FBK component of the system combination corresponds to the “contrastive 1” system of the official FBK submission. The FBK system was built upon a standard phrasebased system using the Moses toolkit [32], and exploited the huge amount of parallel English→French and monolingual French training data, provided by the organizers. It featured a statistical log-linear model including a filled-up phrase translation model [56] and lexicalized reordering models (RMs), two F"
2013.iwslt-evaluation.16,2011.iwslt-evaluation.18,1,0.928081,"el, trained on the WIT3 corpus using the MKCLS [54] algorithm to cluster the words. For the English→French translation task, KIT linearly combined the 5. Fondazione Bruno Kessler The FBK component of the system combination corresponds to the “contrastive 1” system of the official FBK submission. The FBK system was built upon a standard phrasebased system using the Moses toolkit [32], and exploited the huge amount of parallel English→French and monolingual French training data, provided by the organizers. It featured a statistical log-linear model including a filled-up phrase translation model [56] and lexicalized reordering models (RMs), two French language models (LMs), as well as distortion, word, and phrase penalties. In order to focus it on TED specific domain and genre, and to reduce the size of the system, data selection by means of IRSTLM toolkit [57] was performed on the whole parallel English→French corpus, using the WIT3 training data as in-domain data. Different amount of data are selected from each available corpora but the WIT3 data, for a total of 66 M English running words. Two TMs and two RMs were trained on WIT3 and selected data, separately, and combined using the fil"
2013.iwslt-evaluation.16,W05-0909,0,0.0593782,"es which are outputs of different translation engines. The consensus translations can be better in terms of translation quality than any of the individual hypotheses. To combine the engines of the project partners for the EU-BRIDGE joint setups, we applied a system combination implementation that has been developed at RWTH Aachen University. The basic concept of RWTH’s approach to machine translation system combination has been described by Matusov et al. [60]. This approach includes an enhanced alignment and reordering framework. Alignments between the system outputs are learned using METEOR [61]. A confusion network is then built using one of the hypotheses as “primary” hypothesis. We do not make a hard decision on which of the hypotheses to use for that, but instead combine all possible confusion networks into a single lattice. Majority voting on the generated lattice is performed using the prior probabilities for each system as well as other statistical models, e.g. a special n-gram language model which is learned on the input hypotheses. Scaling factors of the models are optimized using the Minimum Error Rate Training algorithm. The translation with the best total score within the"
2013.iwslt-evaluation.16,W12-3140,1,\N,Missing
2013.iwslt-evaluation.16,J03-1002,1,\N,Missing
2013.iwslt-evaluation.16,C12-3061,1,\N,Missing
2013.iwslt-evaluation.16,federico-etal-2012-iwslt,1,\N,Missing
2013.iwslt-evaluation.16,2011.iwslt-evaluation.1,1,\N,Missing
2013.iwslt-evaluation.16,W13-2223,1,\N,Missing
2013.iwslt-evaluation.16,N13-1073,0,\N,Missing
2013.iwslt-evaluation.3,W13-2212,1,0.917473,"ersity of Edinburgh Scotland, United Kingdom a.birch@ed.ac.uk {dnadir,pkoehn}@inf.ed.ac.uk Abstract This paper gives a description of the University of Edinburgh’s (UEDIN) systems for IWSLT 2013. We participated in all the MT tracks and the German-to-English and Englishto-French SLT tracks. Our SLT submissions experimented with including ASR uncertainty into the decoding process via confusion networks, and looked at different ways of punctuating ASR output. Our MT submissions are mainly based on a system used in the recent evaluation campaign at the Workshop on Statistical Machine Translation [1]. We additionally explored the use of generalized representations (Brown clusters, POS and morphological tags) translating out of English into European languages. 1. Spoken Language Translation We submit two systems to the Spoken Language Translation track: English-French and German-English. These systems were built to take maximum advantage of Edinburgh’s English [2] and German [3] 2013 IWSLT speech recognition systems. We explored different strategies for minimizing the mismatch between unpunctuated ASR output and SMT models, which are typically trained on punctuated text. We wanted to exami"
2013.iwslt-evaluation.3,2013.iwslt-evaluation.11,0,0.22948,"Missing"
2013.iwslt-evaluation.3,2006.iwslt-papers.1,0,0.157669,"Missing"
2013.iwslt-evaluation.3,P07-2045,1,0.0218946,"ilities over the 3000 context-dependent states of a HMM. Language modelling was done with a 4-gram LM which was trained on approximately 30 million words, selected from a text corpus of 994 million words, according to maximal cross-entropy with the TED domain. The lexicon was restricted to 300,000 words, striking a balance between adequate word coverage and low perplexity on the TED domain. The lattices were first generated with a heavily pruned version of this LM, and then rescored with the full model. For details, see [3]. 1.2. Experimental design We trained a phrase-based model using Moses [8] on the parallel corpora described in Table 1. These are large parallel corpora, with only TED talks [9] consisting of in-domain data. Europarl v7 [10], News Commentary corpus and Multi United Nations corpus [11], Gigaword corpus (French Gigaword Second Edition, English Gigaword Fifth Edition) and Common Crawl [12] consist of parallel data which contain some noise, and a large number of examples which are likely irrelevant for the target TED domain. We therefore used a domain filtering technique [13] which was applied successfully in last year’s Edinburgh submission [14]. This uses bilingual c"
2013.iwslt-evaluation.3,2012.eamt-1.60,0,0.140449,"Missing"
2013.iwslt-evaluation.3,W12-3102,1,0.854746,"Missing"
2013.iwslt-evaluation.3,P13-1135,1,0.795193,"Missing"
2013.iwslt-evaluation.3,D11-1033,0,0.0649187,"Missing"
2013.iwslt-evaluation.3,2012.iwslt-papers.17,1,0.710035,"Missing"
2013.iwslt-evaluation.3,E03-1076,1,0.788801,"Missing"
2013.iwslt-evaluation.3,P05-1066,1,0.842596,"Missing"
2013.iwslt-evaluation.3,N03-1017,1,0.0193068,"e submissions did slightly better. 2. Machine Translation Systems Our machine translation systems are based on our setup [1] that has been proven successful at the recent evaluation campaign at the Workshop on Statistical Machine Translation [20]. Language Arabic Chinese Dutch Farsi French German Italian Polish Portuguese Romanian Russian Slovenian Spanish Turkish Into English 24.8 11.8 32.8 14.5 33.3 30.5 29.7 17.7 36.0 31.7 19.1 24.7 39.5 13.5 From English 7.6 9.8 26.5 8.0 33.2 22.9 23.7 9.7 30.8 21.1 13.1 18.0 33.9 7.2 2.1. Baseline The system uses the baseline Moses [8] phrase-based model [21] (as given in the example files for the experimental management system), with the following additions: • limitation of phrase length to 5 • sparse domain indicator, lexical, phrase length, and count bin features [22] • factored models for German–English and English– German • source-side German compound splitting [23] • cube pruning with pop limit 1000 for tuning, 5000 for testing [24] • operation sequence model (OSM) with 4 additional supportive features: 2 gap based penalties, 1 distance based feature and 1 deletion penalty [25] • batch k-best MIRA tuning [26] • interpolated 5-gram KenLM lang"
2013.iwslt-evaluation.3,N09-1025,0,0.063499,"tical Machine Translation [20]. Language Arabic Chinese Dutch Farsi French German Italian Polish Portuguese Romanian Russian Slovenian Spanish Turkish Into English 24.8 11.8 32.8 14.5 33.3 30.5 29.7 17.7 36.0 31.7 19.1 24.7 39.5 13.5 From English 7.6 9.8 26.5 8.0 33.2 22.9 23.7 9.7 30.8 21.1 13.1 18.0 33.9 7.2 2.1. Baseline The system uses the baseline Moses [8] phrase-based model [21] (as given in the example files for the experimental management system), with the following additions: • limitation of phrase length to 5 • sparse domain indicator, lexical, phrase length, and count bin features [22] • factored models for German–English and English– German • source-side German compound splitting [23] • cube pruning with pop limit 1000 for tuning, 5000 for testing [24] • operation sequence model (OSM) with 4 additional supportive features: 2 gap based penalties, 1 distance based feature and 1 deletion penalty [25] • batch k-best MIRA tuning [26] • interpolated 5-gram KenLM language models [27] • minimum Bayes risk decoding [28] • no-reordering-over-punctuation heuristic [29] In the IWSLT systems, we also used: • compact phrase tables [30] • filter out phrase translations with conditional p"
2013.iwslt-evaluation.3,P07-1019,0,0.235981,".8 32.8 14.5 33.3 30.5 29.7 17.7 36.0 31.7 19.1 24.7 39.5 13.5 From English 7.6 9.8 26.5 8.0 33.2 22.9 23.7 9.7 30.8 21.1 13.1 18.0 33.9 7.2 2.1. Baseline The system uses the baseline Moses [8] phrase-based model [21] (as given in the example files for the experimental management system), with the following additions: • limitation of phrase length to 5 • sparse domain indicator, lexical, phrase length, and count bin features [22] • factored models for German–English and English– German • source-side German compound splitting [23] • cube pruning with pop limit 1000 for tuning, 5000 for testing [24] • operation sequence model (OSM) with 4 additional supportive features: 2 gap based penalties, 1 distance based feature and 1 deletion penalty [25] • batch k-best MIRA tuning [26] • interpolated 5-gram KenLM language models [27] • minimum Bayes risk decoding [28] • no-reordering-over-punctuation heuristic [29] In the IWSLT systems, we also used: • compact phrase tables [30] • filter out phrase translations with conditional probability of less than 0.0001 • hierarchical lexicalized reordering (mslr) [31] • MADA tokenizer for source-side Arabic [32] • Stanford Chinese segmenter [33] We also tri"
2013.iwslt-evaluation.3,P11-1105,1,0.799042,"seline The system uses the baseline Moses [8] phrase-based model [21] (as given in the example files for the experimental management system), with the following additions: • limitation of phrase length to 5 • sparse domain indicator, lexical, phrase length, and count bin features [22] • factored models for German–English and English– German • source-side German compound splitting [23] • cube pruning with pop limit 1000 for tuning, 5000 for testing [24] • operation sequence model (OSM) with 4 additional supportive features: 2 gap based penalties, 1 distance based feature and 1 deletion penalty [25] • batch k-best MIRA tuning [26] • interpolated 5-gram KenLM language models [27] • minimum Bayes risk decoding [28] • no-reordering-over-punctuation heuristic [29] In the IWSLT systems, we also used: • compact phrase tables [30] • filter out phrase translations with conditional probability of less than 0.0001 • hierarchical lexicalized reordering (mslr) [31] • MADA tokenizer for source-side Arabic [32] • Stanford Chinese segmenter [33] We also tried hierarchical phrase-based models for Chinese, but did not achieve better results. In addition to the data provided directly from the IWSLT organi"
2013.iwslt-evaluation.3,N12-1047,0,0.0421976,"ine Moses [8] phrase-based model [21] (as given in the example files for the experimental management system), with the following additions: • limitation of phrase length to 5 • sparse domain indicator, lexical, phrase length, and count bin features [22] • factored models for German–English and English– German • source-side German compound splitting [23] • cube pruning with pop limit 1000 for tuning, 5000 for testing [24] • operation sequence model (OSM) with 4 additional supportive features: 2 gap based penalties, 1 distance based feature and 1 deletion penalty [25] • batch k-best MIRA tuning [26] • interpolated 5-gram KenLM language models [27] • minimum Bayes risk decoding [28] • no-reordering-over-punctuation heuristic [29] In the IWSLT systems, we also used: • compact phrase tables [30] • filter out phrase translations with conditional probability of less than 0.0001 • hierarchical lexicalized reordering (mslr) [31] • MADA tokenizer for source-side Arabic [32] • Stanford Chinese segmenter [33] We also tried hierarchical phrase-based models for Chinese, but did not achieve better results. In addition to the data provided directly from the IWSLT organizers, we also included whenever"
2013.iwslt-evaluation.3,W11-2123,0,0.0555168,"n the example files for the experimental management system), with the following additions: • limitation of phrase length to 5 • sparse domain indicator, lexical, phrase length, and count bin features [22] • factored models for German–English and English– German • source-side German compound splitting [23] • cube pruning with pop limit 1000 for tuning, 5000 for testing [24] • operation sequence model (OSM) with 4 additional supportive features: 2 gap based penalties, 1 distance based feature and 1 deletion penalty [25] • batch k-best MIRA tuning [26] • interpolated 5-gram KenLM language models [27] • minimum Bayes risk decoding [28] • no-reordering-over-punctuation heuristic [29] In the IWSLT systems, we also used: • compact phrase tables [30] • filter out phrase translations with conditional probability of less than 0.0001 • hierarchical lexicalized reordering (mslr) [31] • MADA tokenizer for source-side Arabic [32] • Stanford Chinese segmenter [33] We also tried hierarchical phrase-based models for Chinese, but did not achieve better results. In addition to the data provided directly from the IWSLT organizers, we also included whenever applicable: • Common Crawl parallel corpus, as pr"
2013.iwslt-evaluation.3,N04-1022,0,0.439255,"ental management system), with the following additions: • limitation of phrase length to 5 • sparse domain indicator, lexical, phrase length, and count bin features [22] • factored models for German–English and English– German • source-side German compound splitting [23] • cube pruning with pop limit 1000 for tuning, 5000 for testing [24] • operation sequence model (OSM) with 4 additional supportive features: 2 gap based penalties, 1 distance based feature and 1 deletion penalty [25] • batch k-best MIRA tuning [26] • interpolated 5-gram KenLM language models [27] • minimum Bayes risk decoding [28] • no-reordering-over-punctuation heuristic [29] In the IWSLT systems, we also used: • compact phrase tables [30] • filter out phrase translations with conditional probability of less than 0.0001 • hierarchical lexicalized reordering (mslr) [31] • MADA tokenizer for source-side Arabic [32] • Stanford Chinese segmenter [33] We also tried hierarchical phrase-based models for Chinese, but did not achieve better results. In addition to the data provided directly from the IWSLT organizers, we also included whenever applicable: • Common Crawl parallel corpus, as provided by WMT 2013 [34] • Europarl"
2013.iwslt-evaluation.3,W09-0429,1,0.841827,"itions: • limitation of phrase length to 5 • sparse domain indicator, lexical, phrase length, and count bin features [22] • factored models for German–English and English– German • source-side German compound splitting [23] • cube pruning with pop limit 1000 for tuning, 5000 for testing [24] • operation sequence model (OSM) with 4 additional supportive features: 2 gap based penalties, 1 distance based feature and 1 deletion penalty [25] • batch k-best MIRA tuning [26] • interpolated 5-gram KenLM language models [27] • minimum Bayes risk decoding [28] • no-reordering-over-punctuation heuristic [29] In the IWSLT systems, we also used: • compact phrase tables [30] • filter out phrase translations with conditional probability of less than 0.0001 • hierarchical lexicalized reordering (mslr) [31] • MADA tokenizer for source-side Arabic [32] • Stanford Chinese segmenter [33] We also tried hierarchical phrase-based models for Chinese, but did not achieve better results. In addition to the data provided directly from the IWSLT organizers, we also included whenever applicable: • Common Crawl parallel corpus, as provided by WMT 2013 [34] • Europarl version 7 parallel corpus1 [35] • news commentar"
2013.iwslt-evaluation.3,D08-1089,0,0.1348,"an compound splitting [23] • cube pruning with pop limit 1000 for tuning, 5000 for testing [24] • operation sequence model (OSM) with 4 additional supportive features: 2 gap based penalties, 1 distance based feature and 1 deletion penalty [25] • batch k-best MIRA tuning [26] • interpolated 5-gram KenLM language models [27] • minimum Bayes risk decoding [28] • no-reordering-over-punctuation heuristic [29] In the IWSLT systems, we also used: • compact phrase tables [30] • filter out phrase translations with conditional probability of less than 0.0001 • hierarchical lexicalized reordering (mslr) [31] • MADA tokenizer for source-side Arabic [32] • Stanford Chinese segmenter [33] We also tried hierarchical phrase-based models for Chinese, but did not achieve better results. In addition to the data provided directly from the IWSLT organizers, we also included whenever applicable: • Common Crawl parallel corpus, as provided by WMT 2013 [34] • Europarl version 7 parallel corpus1 [35] • news commentary parallel corpus, as provided by WMT 2013 1 http://www.statmt.org/europarl/ Table 7: Baseline system performance for machine translation systems (Section 2.1): Cased BLEU scores on test2010 using"
2013.iwslt-evaluation.3,N06-2013,0,0.0781791,"h pop limit 1000 for tuning, 5000 for testing [24] • operation sequence model (OSM) with 4 additional supportive features: 2 gap based penalties, 1 distance based feature and 1 deletion penalty [25] • batch k-best MIRA tuning [26] • interpolated 5-gram KenLM language models [27] • minimum Bayes risk decoding [28] • no-reordering-over-punctuation heuristic [29] In the IWSLT systems, we also used: • compact phrase tables [30] • filter out phrase translations with conditional probability of less than 0.0001 • hierarchical lexicalized reordering (mslr) [31] • MADA tokenizer for source-side Arabic [32] • Stanford Chinese segmenter [33] We also tried hierarchical phrase-based models for Chinese, but did not achieve better results. In addition to the data provided directly from the IWSLT organizers, we also included whenever applicable: • Common Crawl parallel corpus, as provided by WMT 2013 [34] • Europarl version 7 parallel corpus1 [35] • news commentary parallel corpus, as provided by WMT 2013 1 http://www.statmt.org/europarl/ Table 7: Baseline system performance for machine translation systems (Section 2.1): Cased BLEU scores on test2010 using NIST’s mteval-v13a. Test on tune for Slovenia"
2013.iwslt-evaluation.3,W08-0336,0,0.0381555,"for testing [24] • operation sequence model (OSM) with 4 additional supportive features: 2 gap based penalties, 1 distance based feature and 1 deletion penalty [25] • batch k-best MIRA tuning [26] • interpolated 5-gram KenLM language models [27] • minimum Bayes risk decoding [28] • no-reordering-over-punctuation heuristic [29] In the IWSLT systems, we also used: • compact phrase tables [30] • filter out phrase translations with conditional probability of less than 0.0001 • hierarchical lexicalized reordering (mslr) [31] • MADA tokenizer for source-side Arabic [32] • Stanford Chinese segmenter [33] We also tried hierarchical phrase-based models for Chinese, but did not achieve better results. In addition to the data provided directly from the IWSLT organizers, we also included whenever applicable: • Common Crawl parallel corpus, as provided by WMT 2013 [34] • Europarl version 7 parallel corpus1 [35] • news commentary parallel corpus, as provided by WMT 2013 1 http://www.statmt.org/europarl/ Table 7: Baseline system performance for machine translation systems (Section 2.1): Cased BLEU scores on test2010 using NIST’s mteval-v13a. Test on tune for Slovenian. Moses multi-bleu.perl for Chine"
2013.iwslt-evaluation.3,2005.mtsummit-papers.11,1,0.0568235,"uation heuristic [29] In the IWSLT systems, we also used: • compact phrase tables [30] • filter out phrase translations with conditional probability of less than 0.0001 • hierarchical lexicalized reordering (mslr) [31] • MADA tokenizer for source-side Arabic [32] • Stanford Chinese segmenter [33] We also tried hierarchical phrase-based models for Chinese, but did not achieve better results. In addition to the data provided directly from the IWSLT organizers, we also included whenever applicable: • Common Crawl parallel corpus, as provided by WMT 2013 [34] • Europarl version 7 parallel corpus1 [35] • news commentary parallel corpus, as provided by WMT 2013 1 http://www.statmt.org/europarl/ Table 7: Baseline system performance for machine translation systems (Section 2.1): Cased BLEU scores on test2010 using NIST’s mteval-v13a. Test on tune for Slovenian. Moses multi-bleu.perl for Chinese target. • news language model data provided by WMT 2013 • LDC Gigaword for French, Spanish, and English as output language We built systems for all language pairs of the IWSLT evaluation campaign. The quality scores (BLEU) of the resulting systems as measured on the development test set is given in Tabl"
2013.iwslt-evaluation.3,W13-2205,0,0.0250289,"013 1 http://www.statmt.org/europarl/ Table 7: Baseline system performance for machine translation systems (Section 2.1): Cased BLEU scores on test2010 using NIST’s mteval-v13a. Test on tune for Slovenian. Moses multi-bleu.perl for Chinese target. • news language model data provided by WMT 2013 • LDC Gigaword for French, Spanish, and English as output language We built systems for all language pairs of the IWSLT evaluation campaign. The quality scores (BLEU) of the resulting systems as measured on the development test set is given in Table 7. 2.2. Brown Cluster Language Models As suggested by [36], we explored the use of Brown clusters [37]. We computed the clusters with GIZA++’s mkcls [38] on the target side of the parallel training corpus. Brown clusters are word classes that are optimized to reduce n-gram perplexity. By generating the Brown cluster identifier for each output word, we are able to add an n-gram model over these identifiers as an additional scoring function. The inclusion of such an additional factor is trivial given the factored model implementation [39] of Moses. The n-gram model is trained on the target side of the TED corpus made available by the IWSLT organizers."
2013.iwslt-evaluation.3,J92-4003,0,0.213359,"7: Baseline system performance for machine translation systems (Section 2.1): Cased BLEU scores on test2010 using NIST’s mteval-v13a. Test on tune for Slovenian. Moses multi-bleu.perl for Chinese target. • news language model data provided by WMT 2013 • LDC Gigaword for French, Spanish, and English as output language We built systems for all language pairs of the IWSLT evaluation campaign. The quality scores (BLEU) of the resulting systems as measured on the development test set is given in Table 7. 2.2. Brown Cluster Language Models As suggested by [36], we explored the use of Brown clusters [37]. We computed the clusters with GIZA++’s mkcls [38] on the target side of the parallel training corpus. Brown clusters are word classes that are optimized to reduce n-gram perplexity. By generating the Brown cluster identifier for each output word, we are able to add an n-gram model over these identifiers as an additional scoring function. The inclusion of such an additional factor is trivial given the factored model implementation [39] of Moses. The n-gram model is trained on the target side of the TED corpus made available by the IWSLT organizers. The motivation for using Brown clusters stem"
2013.iwslt-evaluation.3,E99-1010,0,0.159661,"ion systems (Section 2.1): Cased BLEU scores on test2010 using NIST’s mteval-v13a. Test on tune for Slovenian. Moses multi-bleu.perl for Chinese target. • news language model data provided by WMT 2013 • LDC Gigaword for French, Spanish, and English as output language We built systems for all language pairs of the IWSLT evaluation campaign. The quality scores (BLEU) of the resulting systems as measured on the development test set is given in Table 7. 2.2. Brown Cluster Language Models As suggested by [36], we explored the use of Brown clusters [37]. We computed the clusters with GIZA++’s mkcls [38] on the target side of the parallel training corpus. Brown clusters are word classes that are optimized to reduce n-gram perplexity. By generating the Brown cluster identifier for each output word, we are able to add an n-gram model over these identifiers as an additional scoring function. The inclusion of such an additional factor is trivial given the factored model implementation [39] of Moses. The n-gram model is trained on the target side of the TED corpus made available by the IWSLT organizers. The motivation for using Brown clusters stems from the success of using n-gram models over part"
2013.iwslt-evaluation.3,D07-1091,1,0.895243,"ems as measured on the development test set is given in Table 7. 2.2. Brown Cluster Language Models As suggested by [36], we explored the use of Brown clusters [37]. We computed the clusters with GIZA++’s mkcls [38] on the target side of the parallel training corpus. Brown clusters are word classes that are optimized to reduce n-gram perplexity. By generating the Brown cluster identifier for each output word, we are able to add an n-gram model over these identifiers as an additional scoring function. The inclusion of such an additional factor is trivial given the factored model implementation [39] of Moses. The n-gram model is trained on the target side of the TED corpus made available by the IWSLT organizers. The motivation for using Brown clusters stems from the success of using n-gram models over part-of-speech and morphological tags and the lack of the required taggers and analyzers for many language pairs. Brown clustering induces word classes that are similar to part-of-speech tags (for instance, placing adjectives with the same inflection into one class), with some additional semantic grouping (for instance, grouping all color adjectives). Results are shown in Table 8. While the"
2013.iwslt-evaluation.3,N13-1001,1,0.70383,"del alone did not give much improvement. We also tried using OSM models over different numbers of clusters simultaneously for English-to-{French, Spanish and Dutch} pairs. Small gain was observed in the case of English-to-Spanish as the best system improved from 34.7 to 35.0. No further gains were observed in the case of other two pairs. For each system, our official submission is the system with the best performance on the development test set. 2.3. Operation Sequence Models over Generalized Representations 2.3.2. POS and Morph Tags The integration of the OSM model into phrase-based decoding [40, 41] addresses the problem of phrasal independence assumption since the model considers context beyond phrasal boundaries. However, due to data sparsity the model often falls back to very small context sizes. We investigated the use of generalized representations (pos, morphological analysis and word clusters) in the OSM model. The expectation is that given the sparse training data for many of the language pairs, defining this model over the more general word classes would lead to a model that is able to consider wider context and learn richer lexical and reordering patterns. We also tried using t"
2013.iwslt-evaluation.3,P13-2071,1,0.726323,"del alone did not give much improvement. We also tried using OSM models over different numbers of clusters simultaneously for English-to-{French, Spanish and Dutch} pairs. Small gain was observed in the case of English-to-Spanish as the best system improved from 34.7 to 35.0. No further gains were observed in the case of other two pairs. For each system, our official submission is the system with the best performance on the development test set. 2.3. Operation Sequence Models over Generalized Representations 2.3.2. POS and Morph Tags The integration of the OSM model into phrase-based decoding [40, 41] addresses the problem of phrasal independence assumption since the model considers context beyond phrasal boundaries. However, due to data sparsity the model often falls back to very small context sizes. We investigated the use of generalized representations (pos, morphological analysis and word clusters) in the OSM model. The expectation is that given the sparse training data for many of the language pairs, defining this model over the more general word classes would lead to a model that is able to consider wider context and learn richer lexical and reordering patterns. We also tried using t"
2013.iwslt-evaluation.3,2012.iwslt-evaluation.4,1,\N,Missing
2013.iwslt-evaluation.3,N03-1031,0,\N,Missing
2013.iwslt-evaluation.3,2013.iwslt-evaluation.22,1,\N,Missing
2013.iwslt-evaluation.3,W13-2201,1,\N,Missing
2013.iwslt-evaluation.3,P13-3000,0,\N,Missing
2013.iwslt-evaluation.3,P13-4000,0,\N,Missing
2013.iwslt-evaluation.3,P13-5000,0,\N,Missing
2013.iwslt-evaluation.3,P13-1000,0,\N,Missing
2013.mtsummit-wptp.7,2005.mtsummit-papers.19,1,0.838299,"Missing"
2013.mtsummit-wptp.7,2011.eamt-1.2,0,0.108581,"Missing"
2013.mtsummit-wptp.7,D08-1051,1,0.880745,"Missing"
2013.mtsummit-wptp.7,2011.eamt-1.7,0,0.022484,"Missing"
2013.mtsummit-wptp.7,2012.eamt-1.5,1,0.870892,"Missing"
2013.mtsummit-wptp.7,J09-1002,1,0.927174,"Missing"
2013.mtsummit-wptp.7,J93-2003,0,0.0316225,"user with intelligent search and replace functions for fast text revision. Our workbench features a straightforward function to run search and replacement rules on the fly. Whenever a new replacement rule is created, it is automatically populated to the forthcoming predictions made by the system, so that the Javascript ............................................ PHP HTTP GUI web server Figure 5: Visualization of Word Alignment HTTP web socket user only needs to specify them once. Word Alignment Information Alignment of source and target words is an important part of the translation process (Brown et al., 1993). In order to display the correspondences between both the source and target words, this feature was implemented in a way that every time the user places the mouse (yellow) or the text cursor (cyan) on a word, the alignments made by the system are highlighted. See Figure 5 for a screenshot. Prediction Rejection With the purpose of easing user interaction, our workbench also supports a one-click rejection feature (Sanchis-Trilles et al., 2008). This invalidates the current prediction for the sentence that is being translated, and provides the user with an alternate one, in which the first new w"
2013.mtsummit-wptp.7,2012.amta-papers.22,0,0.266531,"Missing"
2013.mtsummit-wptp.7,2010.eamt-1.18,1,0.875303,"Missing"
2013.mtsummit-wptp.7,N10-1078,1,0.886215,"Missing"
2013.mtsummit-wptp.7,P07-2045,1,0.00695691,"swapped out workbench may be use partially, for instance in the following fashion: • As part of a larger localization workflow with existing editing facilities, only the capabilities of the CASMACAT CAT server and CAS MACAT MT server are used. A legacy editing tool is extended to make calls to the CAT server and thus benefit from additional functionality. • If an existing customized MT translation solution is already in place, then the CASMACAT front-end and CAT server can connect to it. Already, the currently implemented CASMACAT workbench supports two different MT server components, Moses (Koehn et al., 2007) and Thot (Ortiz-Mart´ınez et al., 2005). 3.1 CAT Server The CAT server is implemented in Python with the Tornadio library. It uses socket.io to keep a web socket connection with the Javascript GUI. Keep in mind that especially interactive translation prediction requires very quick responses from the server. Establishing an HTTP connection through an Ajax call every time the user presses a key would cause significant overhead. A typical session with interactive translation prediction takes place as follows: • The user moves to a new segment in the GUI. • The GUI sends a startSession request to"
2013.mtsummit-wptp.7,W00-0507,0,0.528068,"Missing"
2014.amta-researchers.11,D11-1033,0,0.0494963,"Overlap) with their gain over the respective baseline (bottom of each block). The best system on the mixed test set is marked in bold.. **: p ≤ 0.01, *: p ≤ 0.05 mark significantly better scores compared to the respective baseline. is equal or better than the best model in Table 4, the performance on T ED falls short of that model by ∼0.6 B LEU. This is likely due to the fact that adding Europarl data is particularly harmful for translating T ED documents. Therefore, in future work we will look at combining the adaptation approaches studied here with data selection methods such as the work of Axelrod et al. (2011). 7.4 Qualitative evaluation In this section, we analyse some concrete output examples that visualise the differences in the translations produced by the different models for training condition 1. Figure 2 shows two input and reference sentences with their translations under the unadapted baseline, the domainadapted model and the model with both domain-adapted and topic-adapted features9 . In the first example, the baseline system does not translate the source verb remontent appropriately. This is fixed by the domain-adapted model and in addition, the topic-adapted model finds a contextually b"
2014.amta-researchers.11,W11-1014,0,0.0591169,"Missing"
2014.amta-researchers.11,2010.amta-papers.16,0,0.111511,"n and language models (Foster and Kuhn, 2007; Matsoukas et al., 2009; Foster et al., 2010; Sennrich, 2012) to specific target domains that are usually known in advance, for example the news domain. An extension of the standard domain adaptation task is multi-domain adaptation where a translation system is adapted to several known target domains (see for example Cui et al. (2013)). In cases where the target domains are not assumed to be known, dedicated domain classifiers can be trained and used to automatically predict the target domain and choose an appropriate model based on the prediction (Banerjee et al., 2010). Recently, there has been increased work on the application of topic modelling to translation model adaptation (Gong et al., 2011; Su et al., 2012; Eidelman et al., 2012; Hewavitharana et al., 2013; Hasler et al., 2014a) in an attempt to move away from the notion of a domain as the source of a corpus. Topic adaptation techniques build on the assumption that the origin of sentences and documents is unknown at test time, and the unsupervised nature of topic models is useful for detecting structure across corpus boundaries in training sets to adapt to diverse test sets. While domain adaptation t"
2014.amta-researchers.11,2012.eamt-1.60,0,0.061737,"Missing"
2014.amta-researchers.11,D13-1107,0,0.0134723,"e stronger of two training conditions. 1 Introduction Domain adaptation is a very active area of research in statistical machine translation (SMT) and there is a large and growing body of work on different techniques to adapt translation and language models (Foster and Kuhn, 2007; Matsoukas et al., 2009; Foster et al., 2010; Sennrich, 2012) to specific target domains that are usually known in advance, for example the news domain. An extension of the standard domain adaptation task is multi-domain adaptation where a translation system is adapted to several known target domains (see for example Cui et al. (2013)). In cases where the target domains are not assumed to be known, dedicated domain classifiers can be trained and used to automatically predict the target domain and choose an appropriate model based on the prediction (Banerjee et al., 2010). Recently, there has been increased work on the application of topic modelling to translation model adaptation (Gong et al., 2011; Su et al., 2012; Eidelman et al., 2012; Hewavitharana et al., 2013; Hasler et al., 2014a) in an attempt to move away from the notion of a domain as the source of a corpus. Topic adaptation techniques build on the assumption tha"
2014.amta-researchers.11,P12-2023,0,0.0699077,"Missing"
2014.amta-researchers.11,D10-1044,0,0.0607935,"and topic adaptation approaches can be beneficial and that topic representations can be used to predict the domain of a test document. Our best combined model yields gains of up to 0.82 B LEU over a domain-adapted translation system and up to 1.67 B LEU over an unadapted system, measured on the stronger of two training conditions. 1 Introduction Domain adaptation is a very active area of research in statistical machine translation (SMT) and there is a large and growing body of work on different techniques to adapt translation and language models (Foster and Kuhn, 2007; Matsoukas et al., 2009; Foster et al., 2010; Sennrich, 2012) to specific target domains that are usually known in advance, for example the news domain. An extension of the standard domain adaptation task is multi-domain adaptation where a translation system is adapted to several known target domains (see for example Cui et al. (2013)). In cases where the target domains are not assumed to be known, dedicated domain classifiers can be trained and used to automatically predict the target domain and choose an appropriate model based on the prediction (Banerjee et al., 2010). Recently, there has been increased work on the application of top"
2014.amta-researchers.11,W07-0717,0,0.210003,"ata. We show empirically that combining domain and topic adaptation approaches can be beneficial and that topic representations can be used to predict the domain of a test document. Our best combined model yields gains of up to 0.82 B LEU over a domain-adapted translation system and up to 1.67 B LEU over an unadapted system, measured on the stronger of two training conditions. 1 Introduction Domain adaptation is a very active area of research in statistical machine translation (SMT) and there is a large and growing body of work on different techniques to adapt translation and language models (Foster and Kuhn, 2007; Matsoukas et al., 2009; Foster et al., 2010; Sennrich, 2012) to specific target domains that are usually known in advance, for example the news domain. An extension of the standard domain adaptation task is multi-domain adaptation where a translation system is adapted to several known target domains (see for example Cui et al. (2013)). In cases where the target domains are not assumed to be known, dedicated domain classifiers can be trained and used to automatically predict the target domain and choose an appropriate model based on the prediction (Banerjee et al., 2010). Recently, there has"
2014.amta-researchers.11,D11-1084,0,0.0243366,"hat are usually known in advance, for example the news domain. An extension of the standard domain adaptation task is multi-domain adaptation where a translation system is adapted to several known target domains (see for example Cui et al. (2013)). In cases where the target domains are not assumed to be known, dedicated domain classifiers can be trained and used to automatically predict the target domain and choose an appropriate model based on the prediction (Banerjee et al., 2010). Recently, there has been increased work on the application of topic modelling to translation model adaptation (Gong et al., 2011; Su et al., 2012; Eidelman et al., 2012; Hewavitharana et al., 2013; Hasler et al., 2014a) in an attempt to move away from the notion of a domain as the source of a corpus. Topic adaptation techniques build on the assumption that the origin of sentences and documents is unknown at test time, and the unsupervised nature of topic models is useful for detecting structure across corpus boundaries in training sets to adapt to diverse test sets. While domain adaptation techniques rely on a given, hard clustering of the data, topic adaptation aims to induce a soft clustering that is more suited to t"
2014.amta-researchers.11,E14-1035,1,0.831418,"dard domain adaptation task is multi-domain adaptation where a translation system is adapted to several known target domains (see for example Cui et al. (2013)). In cases where the target domains are not assumed to be known, dedicated domain classifiers can be trained and used to automatically predict the target domain and choose an appropriate model based on the prediction (Banerjee et al., 2010). Recently, there has been increased work on the application of topic modelling to translation model adaptation (Gong et al., 2011; Su et al., 2012; Eidelman et al., 2012; Hewavitharana et al., 2013; Hasler et al., 2014a) in an attempt to move away from the notion of a domain as the source of a corpus. Topic adaptation techniques build on the assumption that the origin of sentences and documents is unknown at test time, and the unsupervised nature of topic models is useful for detecting structure across corpus boundaries in training sets to adapt to diverse test sets. While domain adaptation techniques rely on a given, hard clustering of the data, topic adaptation aims to induce a soft clustering that is more suited to the task. While topic models are very useful for detecting and grouping the semantic diffe"
2014.amta-researchers.11,W14-3358,1,0.942461,"dard domain adaptation task is multi-domain adaptation where a translation system is adapted to several known target domains (see for example Cui et al. (2013)). In cases where the target domains are not assumed to be known, dedicated domain classifiers can be trained and used to automatically predict the target domain and choose an appropriate model based on the prediction (Banerjee et al., 2010). Recently, there has been increased work on the application of topic modelling to translation model adaptation (Gong et al., 2011; Su et al., 2012; Eidelman et al., 2012; Hewavitharana et al., 2013; Hasler et al., 2014a) in an attempt to move away from the notion of a domain as the source of a corpus. Topic adaptation techniques build on the assumption that the origin of sentences and documents is unknown at test time, and the unsupervised nature of topic models is useful for detecting structure across corpus boundaries in training sets to adapt to diverse test sets. While domain adaptation techniques rely on a given, hard clustering of the data, topic adaptation aims to induce a soft clustering that is more suited to the task. While topic models are very useful for detecting and grouping the semantic diffe"
2014.amta-researchers.11,P13-2122,0,0.0778447,"in. An extension of the standard domain adaptation task is multi-domain adaptation where a translation system is adapted to several known target domains (see for example Cui et al. (2013)). In cases where the target domains are not assumed to be known, dedicated domain classifiers can be trained and used to automatically predict the target domain and choose an appropriate model based on the prediction (Banerjee et al., 2010). Recently, there has been increased work on the application of topic modelling to translation model adaptation (Gong et al., 2011; Su et al., 2012; Eidelman et al., 2012; Hewavitharana et al., 2013; Hasler et al., 2014a) in an attempt to move away from the notion of a domain as the source of a corpus. Topic adaptation techniques build on the assumption that the origin of sentences and documents is unknown at test time, and the unsupervised nature of topic models is useful for detecting structure across corpus boundaries in training sets to adapt to diverse test sets. While domain adaptation techniques rely on a given, hard clustering of the data, topic adaptation aims to induce a soft clustering that is more suited to the task. While topic models are very useful for detecting and groupi"
2014.amta-researchers.11,D11-1125,0,0.0338854,"perimental setup All of the test corpora contain document boundaries which allows us to consider document context during translation and switch translation and language models at document boundaries. While the domain-adapted baselines use gold domain labels, we use automatically predicted domains when combining domain-adapted and topic-adapted models7 . We use a tuning set containing data from all three test domains and tune a single set of feature weights for all portions of the test set. Translation quality is evaluated using the average feature weights of three optimisation runs with P RO (Hopkins and May, 2011). We use the mteval-v13a.pl script to compute case-insensitive B LEU scores and use bootstrap resampling (Koehn, 2004) to measure significance of the B LEU scores on the mixed test set. 6 This 7 Note trend was observed by Banchs and Costa-juss`a (2011) for vectors derived from Latent Semantic Indexing. that topic adaptation does not rely on domain labels. Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 144 6.1 Unadapted baseline system Our baseline is a phrase-based French-English system trained on the concatenation of all parallel data f"
2014.amta-researchers.11,W04-3250,1,0.621116,"tion and switch translation and language models at document boundaries. While the domain-adapted baselines use gold domain labels, we use automatically predicted domains when combining domain-adapted and topic-adapted models7 . We use a tuning set containing data from all three test domains and tune a single set of feature weights for all portions of the test set. Translation quality is evaluated using the average feature weights of three optimisation runs with P RO (Hopkins and May, 2011). We use the mteval-v13a.pl script to compute case-insensitive B LEU scores and use bootstrap resampling (Koehn, 2004) to measure significance of the B LEU scores on the mixed test set. 6 This 7 Note trend was observed by Banchs and Costa-juss`a (2011) for vectors derived from Latent Semantic Indexing. that topic adaptation does not rely on domain labels. Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 144 6.1 Unadapted baseline system Our baseline is a phrase-based French-English system trained on the concatenation of all parallel data for condition 1 and 2, respectively. It was built with the Moses toolkit (Koehn et al., 2007) using the 14 standard cor"
2014.amta-researchers.11,D09-1074,0,0.0770157,"y that combining domain and topic adaptation approaches can be beneficial and that topic representations can be used to predict the domain of a test document. Our best combined model yields gains of up to 0.82 B LEU over a domain-adapted translation system and up to 1.67 B LEU over an unadapted system, measured on the stronger of two training conditions. 1 Introduction Domain adaptation is a very active area of research in statistical machine translation (SMT) and there is a large and growing body of work on different techniques to adapt translation and language models (Foster and Kuhn, 2007; Matsoukas et al., 2009; Foster et al., 2010; Sennrich, 2012) to specific target domains that are usually known in advance, for example the news domain. An extension of the standard domain adaptation task is multi-domain adaptation where a translation system is adapted to several known target domains (see for example Cui et al. (2013)). In cases where the target domains are not assumed to be known, dedicated domain classifiers can be trained and used to automatically predict the target domain and choose an appropriate model based on the prediction (Banerjee et al., 2010). Recently, there has been increased work on t"
2014.amta-researchers.11,2012.iwslt-papers.14,0,0.0137319,"c mixture for the test context. The second feature, Joint-conditional, estimates the joint probability of a target phrase and a test context given a source phrase. It is factorised as the (baseline) probability of a target phrase given a source phrase and the probability of the test context given the source and target phrase. The latter is approximated by the probility of the test context topic mixture given the phrase pair topic mixture, which is further approximated by the cosine similarity between the two topic mixtures. The Target-unigrams feature is inspired by the lazy MDI adaptation of Ruiz and Federico (2012) and measures the probability ratio of a word under the document topic mixture versus under the baseline model1 . We include an additional term to measure the topical relevance of a word by comparing against its probability under the asymmetric topic 0 of the PPT model2 . Sim-phrasePair measures the cosine similarity of a phrase pair topic vector and the topic vector of a test context. Sim-targetPhrase is similar but uses an average topic vector over all phrase pairs with the same target phrase. Sim-targetWord instead replaces the phrase pair topic vector with the word topic vector of the word"
2014.amta-researchers.11,E12-1055,0,0.197487,"approaches can be beneficial and that topic representations can be used to predict the domain of a test document. Our best combined model yields gains of up to 0.82 B LEU over a domain-adapted translation system and up to 1.67 B LEU over an unadapted system, measured on the stronger of two training conditions. 1 Introduction Domain adaptation is a very active area of research in statistical machine translation (SMT) and there is a large and growing body of work on different techniques to adapt translation and language models (Foster and Kuhn, 2007; Matsoukas et al., 2009; Foster et al., 2010; Sennrich, 2012) to specific target domains that are usually known in advance, for example the news domain. An extension of the standard domain adaptation task is multi-domain adaptation where a translation system is adapted to several known target domains (see for example Cui et al. (2013)). In cases where the target domains are not assumed to be known, dedicated domain classifiers can be trained and used to automatically predict the target domain and choose an appropriate model based on the prediction (Banerjee et al., 2010). Recently, there has been increased work on the application of topic modelling to t"
2014.amta-researchers.11,P13-1082,0,0.0149742,"hers in recent years. Xu et al. (2007) tune domain-specific features weights and build domain-specific language models. They use the perplexity of in-domain language models to classify test documents and select the appropriate weights and models per document. Banerjee et al. (2010) train domain-specific translation models and use SVMs to detect the domain of an input sentence to route it to a domain-specific model. Wang et al. (2012) follow a slightly different approach by re-using the same translation model for all domains and tuning domain-specific features weights with modified objectives. Sennrich et al. (2013) adapt the four standard translation model features to unsupervised clusters of the development data obtained by k-means clustering. Another line of research aims to improve topic modelling by encoding domain information via a Dirichlet Forest Prior (Andrzejewski et al., 2009). By specifying Must-Link and CannotLink relations between words, topic modelling is guided to either separate words into different topics or merge them into the same topic. While the idea of combining domain and topic adaptation within the same model is appealing, the model requires manually constructed lists of words an"
2014.amta-researchers.11,P12-1048,0,0.01972,"wn in advance, for example the news domain. An extension of the standard domain adaptation task is multi-domain adaptation where a translation system is adapted to several known target domains (see for example Cui et al. (2013)). In cases where the target domains are not assumed to be known, dedicated domain classifiers can be trained and used to automatically predict the target domain and choose an appropriate model based on the prediction (Banerjee et al., 2010). Recently, there has been increased work on the application of topic modelling to translation model adaptation (Gong et al., 2011; Su et al., 2012; Eidelman et al., 2012; Hewavitharana et al., 2013; Hasler et al., 2014a) in an attempt to move away from the notion of a domain as the source of a corpus. Topic adaptation techniques build on the assumption that the origin of sentences and documents is unknown at test time, and the unsupervised nature of topic models is useful for detecting structure across corpus boundaries in training sets to adapt to diverse test sets. While domain adaptation techniques rely on a given, hard clustering of the data, topic adaptation aims to induce a soft clustering that is more suited to the task. While to"
2014.amta-researchers.11,2012.amta-papers.18,0,0.0131325,"thors 139 efficient architecture that combines online and offline computation. 2 Related work Domain classification for multi-domain adaptation has been the focus of several researchers in recent years. Xu et al. (2007) tune domain-specific features weights and build domain-specific language models. They use the perplexity of in-domain language models to classify test documents and select the appropriate weights and models per document. Banerjee et al. (2010) train domain-specific translation models and use SVMs to detect the domain of an input sentence to route it to a domain-specific model. Wang et al. (2012) follow a slightly different approach by re-using the same translation model for all domains and tuning domain-specific features weights with modified objectives. Sennrich et al. (2013) adapt the four standard translation model features to unsupervised clusters of the development data obtained by k-means clustering. Another line of research aims to improve topic modelling by encoding domain information via a Dirichlet Forest Prior (Andrzejewski et al., 2009). By specifying Must-Link and CannotLink relations between words, topic modelling is guided to either separate words into different topics"
2014.amta-researchers.11,2007.mtsummit-papers.68,0,0.0319857,"ting to topics. By predicting the domain label of test documents, we can combine both approaches to translate unlabelled documents from different genres and topics. We show that domain and topic adaptation can be complementary and that finding the right balance between the two could lead to a more Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 139 efficient architecture that combines online and offline computation. 2 Related work Domain classification for multi-domain adaptation has been the focus of several researchers in recent years. Xu et al. (2007) tune domain-specific features weights and build domain-specific language models. They use the perplexity of in-domain language models to classify test documents and select the appropriate weights and models per document. Banerjee et al. (2010) train domain-specific translation models and use SVMs to detect the domain of an input sentence to route it to a domain-specific model. Wang et al. (2012) follow a slightly different approach by re-using the same translation model for all domains and tuning domain-specific features weights with modified objectives. Sennrich et al. (2013) adapt the four"
2014.amta-researchers.11,W13-2201,1,\N,Missing
2014.eamt-1.17,P02-1051,0,0.050085,"t differs in the following aspects: i) their translation models are based on 11/1-N translation links, we do not put any restriction on the alignments ii) their transliteration system is built from hand-crafted rules, our approach is unsupervised and language independent and iii) we additionally integrate pivoting method along with transliteration and demonstrate the usefulness of the synthesized Hindi data. The idea to integrate transliteration module inside of decoder was earlier used by Hermjakob et al. (2008) for the task of disambiguation in Arabic-English machine translation. Much work (Al-Onaizan and Knight, 2002; Zhao et al., 2007) has been done on transliterating named entities and OOVs. Most previous approaches however train a supervised transliteration system separately outside of an MT pipeline, and na¨ıvely replace the OOV words with their 1best transliterations in the post/pre-processing step Related Work There has been a considerable amount of work on synthesizing parallel data and on using triangulation and transliteration to improve machine translation quality. de Gispert and Mariˆno (2006) induced an English-Catalan parallel corpus by automatically translating Spanish part of EnglishSpanish"
2014.eamt-1.17,baker-etal-2002-emille,0,0.0210797,"n options and hypothesize the same translation, in which case they do not have to compete. To avoid such a scenario, and to reward the common translation options, we modify the baseline phrase-table (created from the Hindi-Urdu parallel data) and edit the probabilities for the translation options that exist in the synthesized triangulated and transliterated phrase-tables. For each phrase (ui , hi ) that exists in the triangulated or transliterated phrasetable we modify its estimates as the following: 4 4.1 Evaluation Data Our baseline Urdu-to-Hindi system is built using a small EMILLE corpus (Baker et al., 2002) which contain roughly 12000 sentences of Hindi and Urdu sentences which are not exactly parallel. After sentence alignment, we were able to extract a little more than 7000 sentence pairs. The model for Urdu-English data was build using Urdu-English segment of the Indic5 multi-parallel corpus (Post et al., 2012) which contain roughly 87K sentences. The Hindi-English systems were trained using Hindi-English parallel data (Bojar et al., 2014) composed by compiling several sources including the Hindi-English segment of the Indic parallel corpus. It contains roughly 273K parallel sentences. The tu"
2014.eamt-1.17,2008.iwslt-papers.1,0,0.0426607,"Missing"
2014.eamt-1.17,bojar-etal-2014-hindencorp,0,0.0570766,"Missing"
2014.eamt-1.17,N06-1003,1,0.833944,"Missing"
2014.eamt-1.17,N12-1047,0,0.0160865,"indi (43.4M Sentences) monolingual data made available for the 9th Workshop of Statistical Machine Translation. 4.2 Baseline System Urdu-to-Hindi: We trained a phrase-based Moses system with the following settings: A maximum sentence length of 80, GDFA symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011) used at runtime, 100-best translation options, MBR decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007), using a stack size of 1000 during tuning and 5000 during test. We tuned with the k-best batch MIRA (Cherry and Foster, 2012). Because Hindi and Urdu have same grammatical structure, we used a distortion limit of 0 and no reordering.6 Hindi-English: We carried out an extrinsic evaluation to measure the quality of our Urdu-toHindi translation systems. We translated the Urdu part of the Urdu-English parallel data using the Urdu-to-Hindi SMT systems described above. We then used the translated corpus to from a synthetic Hindi-English parallel corpus and evaluated its performance by adding it to the baseline Hindi-English systems and in isolation. Table 2 shows the results on adding the synthesized Hindi-English paralle"
2014.eamt-1.17,P07-1092,0,0.0464706,"Missing"
2014.eamt-1.17,P10-1048,1,0.924674,"or to the sentence selection strategy. We will use the phrase-translation strategy to improve the Urdu-to-Hindi translation and sentence selection method to improve HindiEnglish translation, although we only use 1-best pivot translation. A second group of previous research that is related to our work is using transliteration to improve translation for closely related languages. Transliteration has been shown to be useful for more than just translating out-of-vocabulary words and named-entities. Nakov and Tiedemann (2012) built character-based model to improve BulgarianMacedonian translation. Durrani et al. (2010) integrated transliteration inside a word-based decoder for Hindi-to-Urdu machine translation. Our work is similar to them, but differs in the following aspects: i) their translation models are based on 11/1-N translation links, we do not put any restriction on the alignments ii) their transliteration system is built from hand-crafted rules, our approach is unsupervised and language independent and iii) we additionally integrate pivoting method along with transliteration and demonstrate the usefulness of the synthesized Hindi data. The idea to integrate transliteration module inside of decoder"
2014.eamt-1.17,P13-2071,1,0.875401,"MILLE corpus (B and using synthesized phrase-tables (Tg Tr ) sep¯ h,e DB arately. The results in row B shows u,h that the data synthesized from the baseline Urdu¯ u,h ) is harmful in both the HindiHindi system (B English tasks. In comparison the data synthesized from triangulated and transliterated Urdu-to-Hindi system still showed an average improvement of +0.65 in English-to-Hindi task. No gains were observed in the other direction. Doing an error analHindi-English: For Hindi-English systems, we additionally used hierarchical lexicalized reordering (Galley and Manning, 2008), a 5-gram OSM, (Durrani et al., 2013), sparse lexical and domain features, (Hasler et al., 2012), class-based models (Durrani et al., 2014b), a distortion limit of 6, and the no-reordering-over-punctuation heuristic. 4.3 Experiments Urdu-to-Hindi: In the initial experiments, we evaluated the effect of integrating the synthesized phrase-tables into Urdu-to-Hindi machine translation. Table 1 shows results on our Urdu-to-Hindi ¯ u,h ) to the baseline system. Our modification (B phrase-table (Bu,h ) to reward the translation options common between the phrase-tables improve the performance of the baseline system slightly (+0.15). Both"
2014.eamt-1.17,J04-4002,0,0.0835923,"ods such as Kneser-Ney assign significant probability mass to unseen events, thus giving high probability to such unknown transliterations. The LM-OOV feature acts as a prior to penalize such hypotheses. The feature is tuned along with the regular features. Therefore if such transliterations are useful, the optimizer can assign positive weight to this feature. But we noticed that optimizer assigned a high negative weight to this feature, thus heavily penalizing the unknown words. We simply integrate the synthesized phrase-tables into phrase-based decoder using the standard loglinear approach (Och and Ney, 2004). We search for a Hindi string H which maximizes a linear combination of feature functions:   J X  ˆ = arg max H λj hj (U, H)  H  j=1 where λj is the weight associated with the feature hj (U, H). Overlapping Translation Options: In the default baseline formulation, the three phrase-tables compete with each other during decoding and create a separate decoding path. In some cases however, phrase-tables might agree on the translation options and hypothesize the same translation, in which case they do not have to compete. To avoid such a scenario, and to reward the common translation options"
2014.eamt-1.17,W14-3309,1,0.895772,"gly we were able to mine additional 21K transliteration pairs from a list of 95K word pairs.2 Transliteration Mining: In order to create a transliteration phrase-table, we require a transliteration system and to build such a system we need training data, a list of transliteration pairs for Hindi-Urdu. Such data is not readily available. Instead of creating manually hand-crafted mapping rules for Urdu-to-Hindi transliteration as done in Durrani et al. (2010), we induce a transliteration corpus that we can use to train a character-based SMT system. We induced unsupervised transliteration model (Durrani et al., 2014c) adapting the approach of unsupervised transliteration mining described in (Sajjad et al., 2011; Sajjad et al., 2012) for the task of machine translation. The algorithm is based on EM. It takes a list of word pairs and extracts transliteration corpus from it. The mining model is a mixture of two components, a transliteration and a non-transliteration sub-model. The overall idea is that the transliteration model (ptr (h, u)) tries to maximize the probability of the transliteration pairs in the word-list and the nontransliteration (pntr (h, u)) component tries to fit the rest of the data. For"
2014.eamt-1.17,P02-1040,0,0.0913009,", and the no-reordering-over-punctuation heuristic. 4.3 Experiments Urdu-to-Hindi: In the initial experiments, we evaluated the effect of integrating the synthesized phrase-tables into Urdu-to-Hindi machine translation. Table 1 shows results on our Urdu-to-Hindi ¯ u,h ) to the baseline system. Our modification (B phrase-table (Bu,h ) to reward the translation options common between the phrase-tables improve the performance of the baseline system slightly (+0.15). Both triangulated (Tg ) and transliterated (Tr ) phrase-tables show value when used in isolation, although their performance (BLEU (Papineni et al., 2002)) is a lot worse in comparison to the baseline. Some of this difference in perfor6 Results do not improve with reordering enabled. 76 System Hindi-to-English new-dev14 news-test14 English-to-Hindi new-dev14 news-test14 Baseline (Bh,e ) Bh,e DB ¯ u,h Tg Tr DB ¯ u,h Tg Tr 17.11 17.60 ∆+0.49 13.13 ∆-3.98 15.77 15.97 ∆+0.20 10.96 ∆-4.79 11.74 12.83 ∆+1.09 11.14 ∆-0.60 11.57 12.47 ∆+0.90 10.51 ∆-1.06 Bh,e DB ¯ u,h Bh,e DTg Tr 16.91 ∆-0.20 17.15 ∆+0.04 15.39 ∆-0.36 15.84 ∆+0.10 10.63 ∆-1.11 12.47 ∆+0.73 9.87 ∆-1.7 12.13 ∆+0.56 Table 2: Evaluating Synthesized Hindi-English Parallel Data on Standard T"
2014.eamt-1.17,C14-1041,1,0.914748,"gly we were able to mine additional 21K transliteration pairs from a list of 95K word pairs.2 Transliteration Mining: In order to create a transliteration phrase-table, we require a transliteration system and to build such a system we need training data, a list of transliteration pairs for Hindi-Urdu. Such data is not readily available. Instead of creating manually hand-crafted mapping rules for Urdu-to-Hindi transliteration as done in Durrani et al. (2010), we induce a transliteration corpus that we can use to train a character-based SMT system. We induced unsupervised transliteration model (Durrani et al., 2014c) adapting the approach of unsupervised transliteration mining described in (Sajjad et al., 2011; Sajjad et al., 2012) for the task of machine translation. The algorithm is based on EM. It takes a list of word pairs and extracts transliteration corpus from it. The mining model is a mixture of two components, a transliteration and a non-transliteration sub-model. The overall idea is that the transliteration model (ptr (h, u)) tries to maximize the probability of the transliteration pairs in the word-list and the nontransliteration (pntr (h, u)) component tries to fit the rest of the data. For"
2014.eamt-1.17,N09-2056,0,0.0431043,"Missing"
2014.eamt-1.17,E14-4029,1,0.918398,"gly we were able to mine additional 21K transliteration pairs from a list of 95K word pairs.2 Transliteration Mining: In order to create a transliteration phrase-table, we require a transliteration system and to build such a system we need training data, a list of transliteration pairs for Hindi-Urdu. Such data is not readily available. Instead of creating manually hand-crafted mapping rules for Urdu-to-Hindi transliteration as done in Durrani et al. (2010), we induce a transliteration corpus that we can use to train a character-based SMT system. We induced unsupervised transliteration model (Durrani et al., 2014c) adapting the approach of unsupervised transliteration mining described in (Sajjad et al., 2011; Sajjad et al., 2012) for the task of machine translation. The algorithm is based on EM. It takes a list of word pairs and extracts transliteration corpus from it. The mining model is a mixture of two components, a transliteration and a non-transliteration sub-model. The overall idea is that the transliteration model (ptr (h, u)) tries to maximize the probability of the transliteration pairs in the word-list and the nontransliteration (pntr (h, u)) component tries to fit the rest of the data. For"
2014.eamt-1.17,popovic-ney-2004-towards,0,0.0979173,"Missing"
2014.eamt-1.17,D08-1089,0,0.0703684,"eline sys¯ u,h ) tem only trained on the EMILLE corpus (B and using synthesized phrase-tables (Tg Tr ) sep¯ h,e DB arately. The results in row B shows u,h that the data synthesized from the baseline Urdu¯ u,h ) is harmful in both the HindiHindi system (B English tasks. In comparison the data synthesized from triangulated and transliterated Urdu-to-Hindi system still showed an average improvement of +0.65 in English-to-Hindi task. No gains were observed in the other direction. Doing an error analHindi-English: For Hindi-English systems, we additionally used hierarchical lexicalized reordering (Galley and Manning, 2008), a 5-gram OSM, (Durrani et al., 2013), sparse lexical and domain features, (Hasler et al., 2012), class-based models (Durrani et al., 2014b), a distortion limit of 6, and the no-reordering-over-punctuation heuristic. 4.3 Experiments Urdu-to-Hindi: In the initial experiments, we evaluated the effect of integrating the synthesized phrase-tables into Urdu-to-Hindi machine translation. Table 1 shows results on our Urdu-to-Hindi ¯ u,h ) to the baseline system. Our modification (B phrase-table (Bu,h ) to reward the translation options common between the phrase-tables improve the performance of the"
2014.eamt-1.17,W12-3152,0,0.0669578,"e synthesized triangulated and transliterated phrase-tables. For each phrase (ui , hi ) that exists in the triangulated or transliterated phrasetable we modify its estimates as the following: 4 4.1 Evaluation Data Our baseline Urdu-to-Hindi system is built using a small EMILLE corpus (Baker et al., 2002) which contain roughly 12000 sentences of Hindi and Urdu sentences which are not exactly parallel. After sentence alignment, we were able to extract a little more than 7000 sentence pairs. The model for Urdu-English data was build using Urdu-English segment of the Indic5 multi-parallel corpus (Post et al., 2012) which contain roughly 87K sentences. The Hindi-English systems were trained using Hindi-English parallel data (Bojar et al., 2014) composed by compiling several sources including the Hindi-English segment of the Indic parallel corpus. It contains roughly 273K parallel sentences. The tune and test sets for Hindi-Urdu task were created by randomly selecting 1800 sentences from the EMILLE corpus which were then removed from the training data to avoid overfitting. We use half of the selected sentences for tuning and other half for test. The dev and test sets for Hindi-English translation task are"
2014.eamt-1.17,J03-3002,0,0.106117,"Missing"
2014.eamt-1.17,I11-1015,1,0.768127,"Missing"
2014.eamt-1.17,2012.iwslt-papers.17,1,0.845679,"sep¯ h,e DB arately. The results in row B shows u,h that the data synthesized from the baseline Urdu¯ u,h ) is harmful in both the HindiHindi system (B English tasks. In comparison the data synthesized from triangulated and transliterated Urdu-to-Hindi system still showed an average improvement of +0.65 in English-to-Hindi task. No gains were observed in the other direction. Doing an error analHindi-English: For Hindi-English systems, we additionally used hierarchical lexicalized reordering (Galley and Manning, 2008), a 5-gram OSM, (Durrani et al., 2013), sparse lexical and domain features, (Hasler et al., 2012), class-based models (Durrani et al., 2014b), a distortion limit of 6, and the no-reordering-over-punctuation heuristic. 4.3 Experiments Urdu-to-Hindi: In the initial experiments, we evaluated the effect of integrating the synthesized phrase-tables into Urdu-to-Hindi machine translation. Table 1 shows results on our Urdu-to-Hindi ¯ u,h ) to the baseline system. Our modification (B phrase-table (Bu,h ) to reward the translation options common between the phrase-tables improve the performance of the baseline system slightly (+0.15). Both triangulated (Tg ) and transliterated (Tr ) phrase-tables"
2014.eamt-1.17,P12-1049,0,0.174262,"Missing"
2014.eamt-1.17,W11-2123,0,0.0291137,"uning weights, the tune set is concatenated with Hindi-English dev-set (1400 Sentences) made available with the Hindi-English segment of the Indic parallel corpus. We trained the language model using all the English (287.3M Sentences) and Hindi (43.4M Sentences) monolingual data made available for the 9th Workshop of Statistical Machine Translation. 4.2 Baseline System Urdu-to-Hindi: We trained a phrase-based Moses system with the following settings: A maximum sentence length of 80, GDFA symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011) used at runtime, 100-best translation options, MBR decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007), using a stack size of 1000 during tuning and 5000 during test. We tuned with the k-best batch MIRA (Cherry and Foster, 2012). Because Hindi and Urdu have same grammatical structure, we used a distortion limit of 0 and no reordering.6 Hindi-English: We carried out an extrinsic evaluation to measure the quality of our Urdu-toHindi translation systems. We translated the Urdu part of the Urdu-English parallel data using the Urdu-to-Hindi SMT systems described above. We then"
2014.eamt-1.17,P03-1010,0,0.0882425,"Missing"
2014.eamt-1.17,P08-1045,0,0.288309,"Missing"
2014.eamt-1.17,N07-1061,0,0.646569,"motivation to build an MT system to create Hindi and Urdu resources by translating one into another. In this paper, we exploit the relatedness of the two languages and bring together the ideas of triangulation and transliteration to effectively improve Urdu-to-Hindi machine translation. We make use of a tiny Hindi-Urdu parallel corpus, to build a Urdu-to-Hindi translation system. We then improve this system by synthesizing phrase-tables through triangulation and transliteration. We create a triangulated phrase-table using English as a pivot language following the well-known convolution model (Utiyama and Isahara, 2007; Wu and Wang, 2007). The new phrase-table is synthesized using Hindi-English and Urdu-English phrase-tables. We then use the interpolated phrasetable to also synthesize a transliteration phraseIn this paper we improve Urdu→HindiEnglish machine translation through triangulation and transliteration. First we built an Urdu→Hindi SMT system by inducing triangulated and transliterated phrase-tables from Urdu–English and Hindi–English phrase translation models. We then use it to translate the Urdu part of the Urdu-English parallel data into Hindi, thus creating an artificial Hindi-English parallel"
2014.eamt-1.17,P07-1019,0,0.0883615,"di-English segment of the Indic parallel corpus. We trained the language model using all the English (287.3M Sentences) and Hindi (43.4M Sentences) monolingual data made available for the 9th Workshop of Statistical Machine Translation. 4.2 Baseline System Urdu-to-Hindi: We trained a phrase-based Moses system with the following settings: A maximum sentence length of 80, GDFA symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011) used at runtime, 100-best translation options, MBR decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007), using a stack size of 1000 during tuning and 5000 during test. We tuned with the k-best batch MIRA (Cherry and Foster, 2012). Because Hindi and Urdu have same grammatical structure, we used a distortion limit of 0 and no reordering.6 Hindi-English: We carried out an extrinsic evaluation to measure the quality of our Urdu-toHindi translation systems. We translated the Urdu part of the Urdu-English parallel data using the Urdu-to-Hindi SMT systems described above. We then used the translated corpus to from a synthetic Hindi-English parallel corpus and evaluated its performance by adding it to"
2014.eamt-1.17,P07-1108,0,0.104499,"ystem to create Hindi and Urdu resources by translating one into another. In this paper, we exploit the relatedness of the two languages and bring together the ideas of triangulation and transliteration to effectively improve Urdu-to-Hindi machine translation. We make use of a tiny Hindi-Urdu parallel corpus, to build a Urdu-to-Hindi translation system. We then improve this system by synthesizing phrase-tables through triangulation and transliteration. We create a triangulated phrase-table using English as a pivot language following the well-known convolution model (Utiyama and Isahara, 2007; Wu and Wang, 2007). The new phrase-table is synthesized using Hindi-English and Urdu-English phrase-tables. We then use the interpolated phrasetable to also synthesize a transliteration phraseIn this paper we improve Urdu→HindiEnglish machine translation through triangulation and transliteration. First we built an Urdu→Hindi SMT system by inducing triangulated and transliterated phrase-tables from Urdu–English and Hindi–English phrase translation models. We then use it to translate the Urdu part of the Urdu-English parallel data into Hindi, thus creating an artificial Hindi-English parallel data. Our phrase-tr"
2014.eamt-1.17,N04-1022,0,0.0449775,"Sentences) made available with the Hindi-English segment of the Indic parallel corpus. We trained the language model using all the English (287.3M Sentences) and Hindi (43.4M Sentences) monolingual data made available for the 9th Workshop of Statistical Machine Translation. 4.2 Baseline System Urdu-to-Hindi: We trained a phrase-based Moses system with the following settings: A maximum sentence length of 80, GDFA symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011) used at runtime, 100-best translation options, MBR decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007), using a stack size of 1000 during tuning and 5000 during test. We tuned with the k-best batch MIRA (Cherry and Foster, 2012). Because Hindi and Urdu have same grammatical structure, we used a distortion limit of 0 and no reordering.6 Hindi-English: We carried out an extrinsic evaluation to measure the quality of our Urdu-toHindi translation systems. We translated the Urdu part of the Urdu-English parallel data using the Urdu-to-Hindi SMT systems described above. We then used the translated corpus to from a synthetic Hindi-English parallel corpus and eva"
2014.eamt-1.17,N07-1046,0,0.462217,"pects: i) their translation models are based on 11/1-N translation links, we do not put any restriction on the alignments ii) their transliteration system is built from hand-crafted rules, our approach is unsupervised and language independent and iii) we additionally integrate pivoting method along with transliteration and demonstrate the usefulness of the synthesized Hindi data. The idea to integrate transliteration module inside of decoder was earlier used by Hermjakob et al. (2008) for the task of disambiguation in Arabic-English machine translation. Much work (Al-Onaizan and Knight, 2002; Zhao et al., 2007) has been done on transliterating named entities and OOVs. Most previous approaches however train a supervised transliteration system separately outside of an MT pipeline, and na¨ıvely replace the OOV words with their 1best transliterations in the post/pre-processing step Related Work There has been a considerable amount of work on synthesizing parallel data and on using triangulation and transliteration to improve machine translation quality. de Gispert and Mariˆno (2006) induced an English-Catalan parallel corpus by automatically translating Spanish part of EnglishSpanish parallel data into"
2014.eamt-1.17,P12-2059,0,0.152915,"oving Machine Translation via Triangulation and Transliteration Nadir Durrani School of Informatics University of Edinburgh dnadir@inf.ed.ac.uk Philipp Koehn School of Informatics University of Edinburgh pkoehn@inf.ed.ac.uk Abstract to overcome sparsity (Utiyama and Isahara, 2003; Resnik and Smith, 2003). Techniques like triangulation (Cohn and Lapata, 2007; Wu and Wang, 2007) and paraphrasing (Callison-Burch et al., 2006) have also been used to address the problem of data sparsity. Transliteration is shown to be useful when the languages in question are closely related (Durrani et al., 2010; Nakov and Tiedemann, 2012). Our work falls in this second category of generating additional/data and models. Hindi and Urdu are widely spoken yet low resourced languages. Hindi descends from Sanskrit and is written in Devanagri script, where as Urdu inherits its vocabulary and language phenomenon from several languages (Arabic, Farsi and Turkish and Sanskrit) and is written in Arabic script. They are a closely related language pair that share grammatical structure and have a high vocabulary overlap.1 This provides a motivation to build an MT system to create Hindi and Urdu resources by translating one into another. In"
2014.eamt-1.17,J04-2003,0,\N,Missing
2014.iwslt-evaluation.6,P07-2045,1,0.0218346,"English→French, Arabic↔English, Farsi→English, Hebrew→English, Spanish↔English, and Portuguese-Brazil↔English tasks. For our SLT submissions, we experimented with comparing operation sequence models with bilingual neural network language models. For our MT submissions, we explored using unsupervised transliteration for languages which have a different script than English, in particular for Arabic, Farsi, and Hebrew. We also investigated syntax-based translation and system combination. 1. Introduction The University of Edinburgh’s translation engines are based on the open source Moses toolkit [1]. We set up phrase-based systems [2] for all SLT and MT tasks covered in this paper, and additionally a string-to-tree syntax-based system [3, 4] for the English→German MT task. The setups for our phrase-based systems have evolved from the configurations of the engines we built for last year’s IWSLT [5] and for this year’s Workshop on Statistical Machine Translation (WMT) [6]. The notable features of these systems are: • Phrase translation scores in both directions, smoothed with Good-Turing discounting • Lexical translation scores in both directions • Word and phrase penalties • Six simple co"
2014.iwslt-evaluation.6,N03-1017,1,0.0403051,"si→English, Hebrew→English, Spanish↔English, and Portuguese-Brazil↔English tasks. For our SLT submissions, we experimented with comparing operation sequence models with bilingual neural network language models. For our MT submissions, we explored using unsupervised transliteration for languages which have a different script than English, in particular for Arabic, Farsi, and Hebrew. We also investigated syntax-based translation and system combination. 1. Introduction The University of Edinburgh’s translation engines are based on the open source Moses toolkit [1]. We set up phrase-based systems [2] for all SLT and MT tasks covered in this paper, and additionally a string-to-tree syntax-based system [3, 4] for the English→German MT task. The setups for our phrase-based systems have evolved from the configurations of the engines we built for last year’s IWSLT [5] and for this year’s Workshop on Statistical Machine Translation (WMT) [6]. The notable features of these systems are: • Phrase translation scores in both directions, smoothed with Good-Turing discounting • Lexical translation scores in both directions • Word and phrase penalties • Six simple count-based binary features • Phrase l"
2014.iwslt-evaluation.6,N04-1035,0,0.251325,"we experimented with comparing operation sequence models with bilingual neural network language models. For our MT submissions, we explored using unsupervised transliteration for languages which have a different script than English, in particular for Arabic, Farsi, and Hebrew. We also investigated syntax-based translation and system combination. 1. Introduction The University of Edinburgh’s translation engines are based on the open source Moses toolkit [1]. We set up phrase-based systems [2] for all SLT and MT tasks covered in this paper, and additionally a string-to-tree syntax-based system [3, 4] for the English→German MT task. The setups for our phrase-based systems have evolved from the configurations of the engines we built for last year’s IWSLT [5] and for this year’s Workshop on Statistical Machine Translation (WMT) [6]. The notable features of these systems are: • Phrase translation scores in both directions, smoothed with Good-Turing discounting • Lexical translation scores in both directions • Word and phrase penalties • Six simple count-based binary features • Phrase length features • Distance-based distortion cost • A hierarchical lexicalized reordering model [7] • Sparse le"
2014.iwslt-evaluation.6,D08-1089,0,0.662228,"d system [3, 4] for the English→German MT task. The setups for our phrase-based systems have evolved from the configurations of the engines we built for last year’s IWSLT [5] and for this year’s Workshop on Statistical Machine Translation (WMT) [6]. The notable features of these systems are: • Phrase translation scores in both directions, smoothed with Good-Turing discounting • Lexical translation scores in both directions • Word and phrase penalties • Six simple count-based binary features • Phrase length features • Distance-based distortion cost • A hierarchical lexicalized reordering model [7] • Sparse lexical and domain indicator features [8] • Operation sequence models (OSMs) over different word representations [9, 10] • A 5-gram language model (LM) over words We typically train factored phrase-based translation models [11, 12] and also incorporate higher order n-gram LMs over word representations given by the factors. Factors can for instance be lemma, part-of-speech (POS) tag, morphological tag, or automatically learnt word classes in the manner of Brown clusters [13]. Edinburgh’s syntax-based systems have recently yielded state-of-the-art performance on English→German news tra"
2014.iwslt-evaluation.6,2012.iwslt-papers.17,1,0.922891,"setups for our phrase-based systems have evolved from the configurations of the engines we built for last year’s IWSLT [5] and for this year’s Workshop on Statistical Machine Translation (WMT) [6]. The notable features of these systems are: • Phrase translation scores in both directions, smoothed with Good-Turing discounting • Lexical translation scores in both directions • Word and phrase penalties • Six simple count-based binary features • Phrase length features • Distance-based distortion cost • A hierarchical lexicalized reordering model [7] • Sparse lexical and domain indicator features [8] • Operation sequence models (OSMs) over different word representations [9, 10] • A 5-gram language model (LM) over words We typically train factored phrase-based translation models [11, 12] and also incorporate higher order n-gram LMs over word representations given by the factors. Factors can for instance be lemma, part-of-speech (POS) tag, morphological tag, or automatically learnt word classes in the manner of Brown clusters [13]. Edinburgh’s syntax-based systems have recently yielded state-of-the-art performance on English→German news translation tasks [14, 15] but have not been applied i"
2014.iwslt-evaluation.6,P11-1105,1,0.920037,"f the engines we built for last year’s IWSLT [5] and for this year’s Workshop on Statistical Machine Translation (WMT) [6]. The notable features of these systems are: • Phrase translation scores in both directions, smoothed with Good-Turing discounting • Lexical translation scores in both directions • Word and phrase penalties • Six simple count-based binary features • Phrase length features • Distance-based distortion cost • A hierarchical lexicalized reordering model [7] • Sparse lexical and domain indicator features [8] • Operation sequence models (OSMs) over different word representations [9, 10] • A 5-gram language model (LM) over words We typically train factored phrase-based translation models [11, 12] and also incorporate higher order n-gram LMs over word representations given by the factors. Factors can for instance be lemma, part-of-speech (POS) tag, morphological tag, or automatically learnt word classes in the manner of Brown clusters [13]. Edinburgh’s syntax-based systems have recently yielded state-of-the-art performance on English→German news translation tasks [14, 15] but have not been applied in an IWSLT-style setting before. Standard features of our stringto-tree syntax-"
2014.iwslt-evaluation.6,C14-1041,1,0.826647,"f the engines we built for last year’s IWSLT [5] and for this year’s Workshop on Statistical Machine Translation (WMT) [6]. The notable features of these systems are: • Phrase translation scores in both directions, smoothed with Good-Turing discounting • Lexical translation scores in both directions • Word and phrase penalties • Six simple count-based binary features • Phrase length features • Distance-based distortion cost • A hierarchical lexicalized reordering model [7] • Sparse lexical and domain indicator features [8] • Operation sequence models (OSMs) over different word representations [9, 10] • A 5-gram language model (LM) over words We typically train factored phrase-based translation models [11, 12] and also incorporate higher order n-gram LMs over word representations given by the factors. Factors can for instance be lemma, part-of-speech (POS) tag, morphological tag, or automatically learnt word classes in the manner of Brown clusters [13]. Edinburgh’s syntax-based systems have recently yielded state-of-the-art performance on English→German news translation tasks [14, 15] but have not been applied in an IWSLT-style setting before. Standard features of our stringto-tree syntax-"
2014.iwslt-evaluation.6,D07-1091,1,0.88178,"on (WMT) [6]. The notable features of these systems are: • Phrase translation scores in both directions, smoothed with Good-Turing discounting • Lexical translation scores in both directions • Word and phrase penalties • Six simple count-based binary features • Phrase length features • Distance-based distortion cost • A hierarchical lexicalized reordering model [7] • Sparse lexical and domain indicator features [8] • Operation sequence models (OSMs) over different word representations [9, 10] • A 5-gram language model (LM) over words We typically train factored phrase-based translation models [11, 12] and also incorporate higher order n-gram LMs over word representations given by the factors. Factors can for instance be lemma, part-of-speech (POS) tag, morphological tag, or automatically learnt word classes in the manner of Brown clusters [13]. Edinburgh’s syntax-based systems have recently yielded state-of-the-art performance on English→German news translation tasks [14, 15] but have not been applied in an IWSLT-style setting before. Standard features of our stringto-tree syntax-based systems are: • Rule translation scores in both directions, smoothed with Good-Turing discounting • Lexica"
2014.iwslt-evaluation.6,2012.amta-papers.9,1,0.923939,"on (WMT) [6]. The notable features of these systems are: • Phrase translation scores in both directions, smoothed with Good-Turing discounting • Lexical translation scores in both directions • Word and phrase penalties • Six simple count-based binary features • Phrase length features • Distance-based distortion cost • A hierarchical lexicalized reordering model [7] • Sparse lexical and domain indicator features [8] • Operation sequence models (OSMs) over different word representations [9, 10] • A 5-gram language model (LM) over words We typically train factored phrase-based translation models [11, 12] and also incorporate higher order n-gram LMs over word representations given by the factors. Factors can for instance be lemma, part-of-speech (POS) tag, morphological tag, or automatically learnt word classes in the manner of Brown clusters [13]. Edinburgh’s syntax-based systems have recently yielded state-of-the-art performance on English→German news translation tasks [14, 15] but have not been applied in an IWSLT-style setting before. Standard features of our stringto-tree syntax-based systems are: • Rule translation scores in both directions, smoothed with Good-Turing discounting • Lexica"
2014.iwslt-evaluation.6,E99-1010,0,0.726319,"features • Phrase length features • Distance-based distortion cost • A hierarchical lexicalized reordering model [7] • Sparse lexical and domain indicator features [8] • Operation sequence models (OSMs) over different word representations [9, 10] • A 5-gram language model (LM) over words We typically train factored phrase-based translation models [11, 12] and also incorporate higher order n-gram LMs over word representations given by the factors. Factors can for instance be lemma, part-of-speech (POS) tag, morphological tag, or automatically learnt word classes in the manner of Brown clusters [13]. Edinburgh’s syntax-based systems have recently yielded state-of-the-art performance on English→German news translation tasks [14, 15] but have not been applied in an IWSLT-style setting before. Standard features of our stringto-tree syntax-based systems are: • Rule translation scores in both directions, smoothed with Good-Turing discounting • Lexical translation scores in both directions • Word and rule penalties • A rule rareness penalty • The monolingual PCFG probability of the tree fragment from which the rule was extracted • A 5-gram LM over words For our Spanish↔English and PortugueseBr"
2014.iwslt-evaluation.6,W12-3150,1,0.921133,"we experimented with comparing operation sequence models with bilingual neural network language models. For our MT submissions, we explored using unsupervised transliteration for languages which have a different script than English, in particular for Arabic, Farsi, and Hebrew. We also investigated syntax-based translation and system combination. 1. Introduction The University of Edinburgh’s translation engines are based on the open source Moses toolkit [1]. We set up phrase-based systems [2] for all SLT and MT tasks covered in this paper, and additionally a string-to-tree syntax-based system [3, 4] for the English→German MT task. The setups for our phrase-based systems have evolved from the configurations of the engines we built for last year’s IWSLT [5] and for this year’s Workshop on Statistical Machine Translation (WMT) [6]. The notable features of these systems are: • Phrase translation scores in both directions, smoothed with Good-Turing discounting • Lexical translation scores in both directions • Word and phrase penalties • Six simple count-based binary features • Phrase length features • Distance-based distortion cost • A hierarchical lexicalized reordering model [7] • Sparse le"
2014.iwslt-evaluation.6,W13-2221,1,0.91182,"al and domain indicator features [8] • Operation sequence models (OSMs) over different word representations [9, 10] • A 5-gram language model (LM) over words We typically train factored phrase-based translation models [11, 12] and also incorporate higher order n-gram LMs over word representations given by the factors. Factors can for instance be lemma, part-of-speech (POS) tag, morphological tag, or automatically learnt word classes in the manner of Brown clusters [13]. Edinburgh’s syntax-based systems have recently yielded state-of-the-art performance on English→German news translation tasks [14, 15] but have not been applied in an IWSLT-style setting before. Standard features of our stringto-tree syntax-based systems are: • Rule translation scores in both directions, smoothed with Good-Turing discounting • Lexical translation scores in both directions • Word and rule penalties • A rule rareness penalty • The monolingual PCFG probability of the tree fragment from which the rule was extracted • A 5-gram LM over words For our Spanish↔English and PortugueseBrazil↔English submissions, we ran the engines as described in last year’s system description paper [5]. In the following, we focus on de"
2014.iwslt-evaluation.6,2013.iwslt-evaluation.3,1,0.887168,"ansliteration for languages which have a different script than English, in particular for Arabic, Farsi, and Hebrew. We also investigated syntax-based translation and system combination. 1. Introduction The University of Edinburgh’s translation engines are based on the open source Moses toolkit [1]. We set up phrase-based systems [2] for all SLT and MT tasks covered in this paper, and additionally a string-to-tree syntax-based system [3, 4] for the English→German MT task. The setups for our phrase-based systems have evolved from the configurations of the engines we built for last year’s IWSLT [5] and for this year’s Workshop on Statistical Machine Translation (WMT) [6]. The notable features of these systems are: • Phrase translation scores in both directions, smoothed with Good-Turing discounting • Lexical translation scores in both directions • Word and phrase penalties • Six simple count-based binary features • Phrase length features • Distance-based distortion cost • A hierarchical lexicalized reordering model [7] • Sparse lexical and domain indicator features [8] • Operation sequence models (OSMs) over different word representations [9, 10] • A 5-gram language model (LM) over word"
2014.iwslt-evaluation.6,W14-3324,1,0.869051,"al and domain indicator features [8] • Operation sequence models (OSMs) over different word representations [9, 10] • A 5-gram language model (LM) over words We typically train factored phrase-based translation models [11, 12] and also incorporate higher order n-gram LMs over word representations given by the factors. Factors can for instance be lemma, part-of-speech (POS) tag, morphological tag, or automatically learnt word classes in the manner of Brown clusters [13]. Edinburgh’s syntax-based systems have recently yielded state-of-the-art performance on English→German news translation tasks [14, 15] but have not been applied in an IWSLT-style setting before. Standard features of our stringto-tree syntax-based systems are: • Rule translation scores in both directions, smoothed with Good-Turing discounting • Lexical translation scores in both directions • Word and rule penalties • A rule rareness penalty • The monolingual PCFG probability of the tree fragment from which the rule was extracted • A 5-gram LM over words For our Spanish↔English and PortugueseBrazil↔English submissions, we ran the engines as described in last year’s system description paper [5]. In the following, we focus on de"
2014.iwslt-evaluation.6,W14-3309,1,0.886786,"particular for Arabic, Farsi, and Hebrew. We also investigated syntax-based translation and system combination. 1. Introduction The University of Edinburgh’s translation engines are based on the open source Moses toolkit [1]. We set up phrase-based systems [2] for all SLT and MT tasks covered in this paper, and additionally a string-to-tree syntax-based system [3, 4] for the English→German MT task. The setups for our phrase-based systems have evolved from the configurations of the engines we built for last year’s IWSLT [5] and for this year’s Workshop on Statistical Machine Translation (WMT) [6]. The notable features of these systems are: • Phrase translation scores in both directions, smoothed with Good-Turing discounting • Lexical translation scores in both directions • Word and phrase penalties • Six simple count-based binary features • Phrase length features • Distance-based distortion cost • A hierarchical lexicalized reordering model [7] • Sparse lexical and domain indicator features [8] • Operation sequence models (OSMs) over different word representations [9, 10] • A 5-gram language model (LM) over words We typically train factored phrase-based translation models [11, 12] and"
2014.iwslt-evaluation.6,2012.eamt-1.60,0,0.0588662,"rections, smoothed with Good-Turing discounting • Lexical translation scores in both directions • Word and rule penalties • A rule rareness penalty • The monolingual PCFG probability of the tree fragment from which the rule was extracted • A 5-gram LM over words For our Spanish↔English and PortugueseBrazil↔English submissions, we ran the engines as described in last year’s system description paper [5]. In the following, we focus on describing the new systems which were developed for the rest of the tasks. Our this year’s IWSLT systems were trained using monolingual and parallel data from WIT3 [16], Europarl [17], MultiUN [18], the Gigaword corpora as provided by the Linguistic Data Consortium [19], the German Political Speeches Corpus [20], and the corpora provided for the WMT shared translation task [21]. Word alignments for the MT track systems were created by aligning the data in both directions with MGIZA++ [22] and symmetrizing the two alignments with the grow-diagfinal-and heuristic [23, 2]. Word alignments for the SLT track systems were created using fast align [24]. The SRILM toolkit [25] was employed to train 5-gram language models (LMs) with modified Kneser-Ney smoothing [26]"
2014.iwslt-evaluation.6,2005.mtsummit-papers.11,1,0.143449,"hed with Good-Turing discounting • Lexical translation scores in both directions • Word and rule penalties • A rule rareness penalty • The monolingual PCFG probability of the tree fragment from which the rule was extracted • A 5-gram LM over words For our Spanish↔English and PortugueseBrazil↔English submissions, we ran the engines as described in last year’s system description paper [5]. In the following, we focus on describing the new systems which were developed for the rest of the tasks. Our this year’s IWSLT systems were trained using monolingual and parallel data from WIT3 [16], Europarl [17], MultiUN [18], the Gigaword corpora as provided by the Linguistic Data Consortium [19], the German Political Speeches Corpus [20], and the corpora provided for the WMT shared translation task [21]. Word alignments for the MT track systems were created by aligning the data in both directions with MGIZA++ [22] and symmetrizing the two alignments with the grow-diagfinal-and heuristic [23, 2]. Word alignments for the SLT track systems were created using fast align [24]. The SRILM toolkit [25] was employed to train 5-gram language models (LMs) with modified Kneser-Ney smoothing [26]. We trained in"
2014.iwslt-evaluation.6,eisele-chen-2010-multiun,0,0.268558,"Turing discounting • Lexical translation scores in both directions • Word and rule penalties • A rule rareness penalty • The monolingual PCFG probability of the tree fragment from which the rule was extracted • A 5-gram LM over words For our Spanish↔English and PortugueseBrazil↔English submissions, we ran the engines as described in last year’s system description paper [5]. In the following, we focus on describing the new systems which were developed for the rest of the tasks. Our this year’s IWSLT systems were trained using monolingual and parallel data from WIT3 [16], Europarl [17], MultiUN [18], the Gigaword corpora as provided by the Linguistic Data Consortium [19], the German Political Speeches Corpus [20], and the corpora provided for the WMT shared translation task [21]. Word alignments for the MT track systems were created by aligning the data in both directions with MGIZA++ [22] and symmetrizing the two alignments with the grow-diagfinal-and heuristic [23, 2]. Word alignments for the SLT track systems were created using fast align [24]. The SRILM toolkit [25] was employed to train 5-gram language models (LMs) with modified Kneser-Ney smoothing [26]. We trained individual LMs o"
2014.iwslt-evaluation.6,W08-0509,0,0.134751,"ns, we ran the engines as described in last year’s system description paper [5]. In the following, we focus on describing the new systems which were developed for the rest of the tasks. Our this year’s IWSLT systems were trained using monolingual and parallel data from WIT3 [16], Europarl [17], MultiUN [18], the Gigaword corpora as provided by the Linguistic Data Consortium [19], the German Political Speeches Corpus [20], and the corpora provided for the WMT shared translation task [21]. Word alignments for the MT track systems were created by aligning the data in both directions with MGIZA++ [22] and symmetrizing the two alignments with the grow-diagfinal-and heuristic [23, 2]. Word alignments for the SLT track systems were created using fast align [24]. The SRILM toolkit [25] was employed to train 5-gram language models (LMs) with modified Kneser-Ney smoothing [26]. We trained individual LMs on each corpus and then interpolated them using weights tuned to minimize perplexity on a development set. KenLM [27] was utilized for LM scoring during decoding. Model weights for the log-linear 49 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December"
2014.iwslt-evaluation.6,N13-1073,0,0.135113,"or the rest of the tasks. Our this year’s IWSLT systems were trained using monolingual and parallel data from WIT3 [16], Europarl [17], MultiUN [18], the Gigaword corpora as provided by the Linguistic Data Consortium [19], the German Political Speeches Corpus [20], and the corpora provided for the WMT shared translation task [21]. Word alignments for the MT track systems were created by aligning the data in both directions with MGIZA++ [22] and symmetrizing the two alignments with the grow-diagfinal-and heuristic [23, 2]. Word alignments for the SLT track systems were created using fast align [24]. The SRILM toolkit [25] was employed to train 5-gram language models (LMs) with modified Kneser-Ney smoothing [26]. We trained individual LMs on each corpus and then interpolated them using weights tuned to minimize perplexity on a development set. KenLM [27] was utilized for LM scoring during decoding. Model weights for the log-linear 49 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 model combination [28] were optimized with batch k-best MIRA [29] to maximize B LEU [30]. Where not otherwise stated, the systems were tuned"
2014.iwslt-evaluation.6,W11-2123,0,0.137447,"[20], and the corpora provided for the WMT shared translation task [21]. Word alignments for the MT track systems were created by aligning the data in both directions with MGIZA++ [22] and symmetrizing the two alignments with the grow-diagfinal-and heuristic [23, 2]. Word alignments for the SLT track systems were created using fast align [24]. The SRILM toolkit [25] was employed to train 5-gram language models (LMs) with modified Kneser-Ney smoothing [26]. We trained individual LMs on each corpus and then interpolated them using weights tuned to minimize perplexity on a development set. KenLM [27] was utilized for LM scoring during decoding. Model weights for the log-linear 49 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 model combination [28] were optimized with batch k-best MIRA [29] to maximize B LEU [30]. Where not otherwise stated, the systems were tuned on dev2010. Besides participating in the evaluation campaign with our individual engines, we also collaborated with partners from the EU-BRIDGE project to produce additional joint submissions. The combined systems of the University of Edinburgh, RWTH Aachen Un"
2014.iwslt-evaluation.6,P02-1038,0,0.434172,"with the grow-diagfinal-and heuristic [23, 2]. Word alignments for the SLT track systems were created using fast align [24]. The SRILM toolkit [25] was employed to train 5-gram language models (LMs) with modified Kneser-Ney smoothing [26]. We trained individual LMs on each corpus and then interpolated them using weights tuned to minimize perplexity on a development set. KenLM [27] was utilized for LM scoring during decoding. Model weights for the log-linear 49 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 model combination [28] were optimized with batch k-best MIRA [29] to maximize B LEU [30]. Where not otherwise stated, the systems were tuned on dev2010. Besides participating in the evaluation campaign with our individual engines, we also collaborated with partners from the EU-BRIDGE project to produce additional joint submissions. The combined systems of the University of Edinburgh, RWTH Aachen University, Karlsruhe Institute of Technology, and Fondazione Bruno Kessler are described in [31]. 2. Spoken Language Translation Edinburgh’s spoken language translation system experiments set out to compare two recent stra"
2014.iwslt-evaluation.6,N12-1047,0,0.264311,"2]. Word alignments for the SLT track systems were created using fast align [24]. The SRILM toolkit [25] was employed to train 5-gram language models (LMs) with modified Kneser-Ney smoothing [26]. We trained individual LMs on each corpus and then interpolated them using weights tuned to minimize perplexity on a development set. KenLM [27] was utilized for LM scoring during decoding. Model weights for the log-linear 49 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 model combination [28] were optimized with batch k-best MIRA [29] to maximize B LEU [30]. Where not otherwise stated, the systems were tuned on dev2010. Besides participating in the evaluation campaign with our individual engines, we also collaborated with partners from the EU-BRIDGE project to produce additional joint submissions. The combined systems of the University of Edinburgh, RWTH Aachen University, Karlsruhe Institute of Technology, and Fondazione Bruno Kessler are described in [31]. 2. Spoken Language Translation Edinburgh’s spoken language translation system experiments set out to compare two recent strands of research in terms of their performan"
2014.iwslt-evaluation.6,P02-1040,0,0.0952689,"the SLT track systems were created using fast align [24]. The SRILM toolkit [25] was employed to train 5-gram language models (LMs) with modified Kneser-Ney smoothing [26]. We trained individual LMs on each corpus and then interpolated them using weights tuned to minimize perplexity on a development set. KenLM [27] was utilized for LM scoring during decoding. Model weights for the log-linear 49 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 model combination [28] were optimized with batch k-best MIRA [29] to maximize B LEU [30]. Where not otherwise stated, the systems were tuned on dev2010. Besides participating in the evaluation campaign with our individual engines, we also collaborated with partners from the EU-BRIDGE project to produce additional joint submissions. The combined systems of the University of Edinburgh, RWTH Aachen University, Karlsruhe Institute of Technology, and Fondazione Bruno Kessler are described in [31]. 2. Spoken Language Translation Edinburgh’s spoken language translation system experiments set out to compare two recent strands of research in terms of their performance and their properties"
2014.iwslt-evaluation.6,2014.iwslt-evaluation.7,1,0.877033,"Missing"
2014.iwslt-evaluation.6,P14-1129,0,0.129793,"rlsruhe Institute of Technology, and Fondazione Bruno Kessler are described in [31]. 2. Spoken Language Translation Edinburgh’s spoken language translation system experiments set out to compare two recent strands of research in terms of their performance and their properties in order to understand the contributions of each. The first strand of research is bilingual neural network langauge models. There has recently been a great deal of interest bilingual neural network language models as they have shown strong gains in performance for Arabic→English, and to a lesser extent for Chinese→English [32]. It is still not clear what the exact contribution of the bilinugal language model is, and there is reason to believe that its contribution may be that it allows the SMT model to overcome strong phrase pair independence assumptions. The second strand of research is operation sequence modelling [33, 34]. The integration of the OSM model into phrase-based decoding directly addresses the problem of the phrasal independence assumption by modelling the context of phrase pair translations. We aim to compare these two different approaches and combining them. As we see, combining OSM and the bilingua"
2014.iwslt-evaluation.6,P13-2071,1,0.864533,"he contributions of each. The first strand of research is bilingual neural network langauge models. There has recently been a great deal of interest bilingual neural network language models as they have shown strong gains in performance for Arabic→English, and to a lesser extent for Chinese→English [32]. It is still not clear what the exact contribution of the bilinugal language model is, and there is reason to believe that its contribution may be that it allows the SMT model to overcome strong phrase pair independence assumptions. The second strand of research is operation sequence modelling [33, 34]. The integration of the OSM model into phrase-based decoding directly addresses the problem of the phrasal independence assumption by modelling the context of phrase pair translations. We aim to compare these two different approaches and combining them. As we see, combining OSM and the bilingual NN language model slightly outperforms all other models, including the state-of-the-art OSM model, but only for English→French and only very slightly. 2.1. Baseline For the SLT track, we trained phrase-based models using Moses with mostly default settings. We further included basic sparse features [35"
2014.iwslt-evaluation.6,N13-1001,1,0.914464,"he contributions of each. The first strand of research is bilingual neural network langauge models. There has recently been a great deal of interest bilingual neural network language models as they have shown strong gains in performance for Arabic→English, and to a lesser extent for Chinese→English [32]. It is still not clear what the exact contribution of the bilinugal language model is, and there is reason to believe that its contribution may be that it allows the SMT model to overcome strong phrase pair independence assumptions. The second strand of research is operation sequence modelling [33, 34]. The integration of the OSM model into phrase-based decoding directly addresses the problem of the phrasal independence assumption by modelling the context of phrase pair translations. We aim to compare these two different approaches and combining them. As we see, combining OSM and the bilingual NN language model slightly outperforms all other models, including the state-of-the-art OSM model, but only for English→French and only very slightly. 2.1. Baseline For the SLT track, we trained phrase-based models using Moses with mostly default settings. We further included basic sparse features [35"
2014.iwslt-evaluation.6,N09-1025,0,0.0844804,"34]. The integration of the OSM model into phrase-based decoding directly addresses the problem of the phrasal independence assumption by modelling the context of phrase pair translations. We aim to compare these two different approaches and combining them. As we see, combining OSM and the bilingual NN language model slightly outperforms all other models, including the state-of-the-art OSM model, but only for English→French and only very slightly. 2.1. Baseline For the SLT track, we trained phrase-based models using Moses with mostly default settings. We further included basic sparse features [35] and we used factors. For German→English we used POS tags, morphological tags and lemmas as factors in decoding [11], and for English→German we used POS tags and morphological tags on the target side. Table 1 lists the factors used for the translation model, and the factors over which we trained OSM models. The SLT and the MT systems were trained in a similar fashion, with the main difference being that for SLT no prereordering was performed for German→English as this relies on grammatically correct test sentences, and automatic speech recognition (ASR) output, especially for German, is diffic"
2014.iwslt-evaluation.6,2006.iwslt-papers.1,0,0.165301,"ma l, pos p, morphology m) and the size of the parallel and monolingual training data in millions of words. for WMT, and the LDC Gigaword for French and English. The number of words of training data can be seen in Table 1. 2.2. Monolingual Punctuation Models One of the main challenges of spoken language translation is to overcome the mismatch in the style of data that the speech recognition systems output, and the written text that is used to train the translation model. ASR system output lacks punctuation and capitalisation and this is one of the main stylistic differences. Previous research [36, 5] suggests that it is preferrable to punctuate the text before translation, which is what we did by training a monolingual translation system for our two source languages: German and English. The “source language” of the punctuation model has punctuation and capitalisation stripped, and the “target language” is the full original written text. Our handling of punctuation uses a phrase-based translation model with no distortion or reordering, and we tuned the model to the ASR input text (dev2010 for English, and dev2012 for German) using batch MIRA and the B LEU score. After running ASR output th"
2014.iwslt-evaluation.6,D13-1106,0,0.0838934,"help even more when sequence models are applied over more general factors such as POS tags and GIZA++’s mkcls clusters [5]. For this experiment we applied the best OSM settings from last year’s IWSLT experiments which included models over words, lemmas, POS tags, and clusters depending on the language pair. See Table 1 for details. 50 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 2.4. Bilingual Neural Network Language Model There has recently been a great deal of interest in including neural networks in machine translation [37, 38]. There is hope that neural networks provide a way to relax some of the more egregious independence assuptions made in translation models. The challenge with neural networks however, is that they are computationally very expensive, and getting them to operate at scale requires sophisticated efficiency techniques. A recent paper which was able to fully integrate a neural network which includes both source side and target side context in decoding [32], and they managed to show big improvements for a small Arabic→English task, and smaller improvements for a Chinese→English task. We implemented a"
2014.iwslt-evaluation.6,P14-1066,0,0.0858383,"help even more when sequence models are applied over more general factors such as POS tags and GIZA++’s mkcls clusters [5]. For this experiment we applied the best OSM settings from last year’s IWSLT experiments which included models over words, lemmas, POS tags, and clusters depending on the language pair. See Table 1 for details. 50 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 2.4. Bilingual Neural Network Language Model There has recently been a great deal of interest in including neural networks in machine translation [37, 38]. There is hope that neural networks provide a way to relax some of the more egregious independence assuptions made in translation models. The challenge with neural networks however, is that they are computationally very expensive, and getting them to operate at scale requires sophisticated efficiency techniques. A recent paper which was able to fully integrate a neural network which includes both source side and target side context in decoding [32], and they managed to show big improvements for a small Arabic→English task, and smaller improvements for a Chinese→English task. We implemented a"
2014.iwslt-evaluation.6,D13-1140,0,0.162286,"-the-art translation models. We implemented a BiNNLM as a feature function inside Moses, following closely the implementation outlined in [32]. The main focus of our design is to make the Moses specific code flexible and independent of the neural network language model that would be used for scoring. As a result any NNLM could implement the interface and be used by Moses during decoding. Some features such as backoff to POS tag in case of unknown word or use of special < null > token to pad an incomplete parse in the chart decoder are made optional. Currently the implemented backends are NPLM [39] and OxLM [40]. Implementation is available for both phrase based and hierarchical Moses. For our experiments we chose NPLM to be our NNLM backend. We chose it, because it features noise contrastive estimation (NCE) which allows us to avoid having to apply softmax to normalize the outputs, as it is infeasible to do so with large vocabularies. Another benefit of NPLM is that when using NCE and a neural network with one hidden layer we can precompute the values for the first hidden layer of all vocabulary terms, similarly to what [32] do. We also modified the NPLM code a bit and used Magma enabl"
2014.iwslt-evaluation.6,E14-4029,1,0.892491,"Missing"
2014.iwslt-evaluation.6,P12-1049,0,0.0479377,"Missing"
2014.iwslt-evaluation.6,W14-3362,1,0.88321,"cal tags (the latter trained on WIT3 only). Model weights of the phrase-based in-domain system were optimized on dev2010. Syntax-based System. The contrastive 1 system is a string-to-tree translation system with similar features as the ones described in [15]. The target-side data was parsed with BitPar [48], and right binarization was applied to the parse trees. The system was adapted to the TED domain by extracting separate rule tables (from the WIT3 corpus and from the rest of the parallel data) and merging them with a fill-up technique [49]. Augmenting the system with non-syntactic phrases [50] and adding soft source syntactic constraints [51] yielded further improvements. Model weights of the syntaxbased system were optimized on a concatenation of dev2010 and dev2012. System Combination. We combined the outputs of the phrase-based primary system, the auxiliary phrase-based indomain system, and the string-to-tree syntax-based system with the MT system combination approach implemented in the Jane toolkit [52]. The parameters of the system combination were optimized on tst2012. The consensus translation produced by the system combination (syscom) was submitted as contrastive 2. 53 Pro"
2014.iwslt-evaluation.6,W14-4018,1,0.890764,"weights of the phrase-based in-domain system were optimized on dev2010. Syntax-based System. The contrastive 1 system is a string-to-tree translation system with similar features as the ones described in [15]. The target-side data was parsed with BitPar [48], and right binarization was applied to the parse trees. The system was adapted to the TED domain by extracting separate rule tables (from the WIT3 corpus and from the rest of the parallel data) and merging them with a fill-up technique [49]. Augmenting the system with non-syntactic phrases [50] and adding soft source syntactic constraints [51] yielded further improvements. Model weights of the syntaxbased system were optimized on a concatenation of dev2010 and dev2012. System Combination. We combined the outputs of the phrase-based primary system, the auxiliary phrase-based indomain system, and the string-to-tree syntax-based system with the MT system combination approach implemented in the Jane toolkit [52]. The parameters of the system combination were optimized on tst2012. The consensus translation produced by the system combination (syscom) was submitted as contrastive 2. 53 Proceedings of the 11th International Workshop on Spo"
2014.iwslt-evaluation.6,E14-2008,1,0.863115,"parate rule tables (from the WIT3 corpus and from the rest of the parallel data) and merging them with a fill-up technique [49]. Augmenting the system with non-syntactic phrases [50] and adding soft source syntactic constraints [51] yielded further improvements. Model weights of the syntaxbased system were optimized on a concatenation of dev2010 and dev2012. System Combination. We combined the outputs of the phrase-based primary system, the auxiliary phrase-based indomain system, and the string-to-tree syntax-based system with the MT system combination approach implemented in the Jane toolkit [52]. The parameters of the system combination were optimized on tst2012. The consensus translation produced by the system combination (syscom) was submitted as contrastive 2. 53 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 4. Summary The Edinburgh submissions for IWSLT cover many language pairs and research techniques. We have implemented a bilingual neural network language model feature in Moses and have demonstrated that it can lead to state-of-the-art results for English→French. BiNNLM seems less beneficial for German→Engl"
2014.iwslt-evaluation.6,N06-2013,0,0.101422,"luster-ids. This result contradicts our findings in last year IWSLT paper [5] where we reported significant gains using class-based models on many European language pairs with English as source language. Monolingual Arabic Data. Unlike parallel data, adding Gigaword and UN monolingual data in English→Arabic translation task gave significant improvements. The gains are shown in Table 5. 3.3. German→English MT 3.2. Arabic-English MT We carried out a number of experiments for the ArabicEnglish language pair which we now discuss briefly. Tokenization. We used MADA tokenizer for source-side Arabic [43] and tried different segmentation schemes including D*, S2 and ATB. The ATB segmentation consistently outperformed other schemes. For the German→English MT task system, prereordering [46] and compound splitting [47] were applied to the German source language side in a preprocessing step. A factored translation model was employed. Source side factors are word, lemma, POS tag, and morphological tag. Target side factors are word, lemma, and POS tag. Supplementary to the features listed in Section 6, we 52 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, De"
2014.iwslt-evaluation.6,D11-1033,0,0.239047,"Missing"
2014.iwslt-evaluation.6,P05-1066,1,0.888458,"source language. Monolingual Arabic Data. Unlike parallel data, adding Gigaword and UN monolingual data in English→Arabic translation task gave significant improvements. The gains are shown in Table 5. 3.3. German→English MT 3.2. Arabic-English MT We carried out a number of experiments for the ArabicEnglish language pair which we now discuss briefly. Tokenization. We used MADA tokenizer for source-side Arabic [43] and tried different segmentation schemes including D*, S2 and ATB. The ATB segmentation consistently outperformed other schemes. For the German→English MT task system, prereordering [46] and compound splitting [47] were applied to the German source language side in a preprocessing step. A factored translation model was employed. Source side factors are word, lemma, POS tag, and morphological tag. Target side factors are word, lemma, and POS tag. Supplementary to the features listed in Section 6, we 52 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 incorporated two additional LMs into the German→English MT system: a 7-gram LM over POS tags and a 7-gram LM over lemmas (both trained on WIT3 only). Model weight"
2014.iwslt-evaluation.6,E03-1076,1,0.855843,"Arabic Data. Unlike parallel data, adding Gigaword and UN monolingual data in English→Arabic translation task gave significant improvements. The gains are shown in Table 5. 3.3. German→English MT 3.2. Arabic-English MT We carried out a number of experiments for the ArabicEnglish language pair which we now discuss briefly. Tokenization. We used MADA tokenizer for source-side Arabic [43] and tried different segmentation schemes including D*, S2 and ATB. The ATB segmentation consistently outperformed other schemes. For the German→English MT task system, prereordering [46] and compound splitting [47] were applied to the German source language side in a preprocessing step. A factored translation model was employed. Source side factors are word, lemma, POS tag, and morphological tag. Target side factors are word, lemma, and POS tag. Supplementary to the features listed in Section 6, we 52 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 incorporated two additional LMs into the German→English MT system: a 7-gram LM over POS tags and a 7-gram LM over lemmas (both trained on WIT3 only). Model weights were optimized on a concat"
2014.iwslt-evaluation.6,C04-1024,0,0.157242,"t2012 24.9 24.1 24.8 26.0 27.8 26.7 26.5 27.8 23.4 22.2 23.1 24.5 Table 8: Results for the English→German MT task (casesensitive B LEU scores). The contrastive 2 submission is a system combination of three systems which was tuned on tst2012. LM over Brown clusters and a 7-gram LM over morphological tags (the latter trained on WIT3 only). Model weights of the phrase-based in-domain system were optimized on dev2010. Syntax-based System. The contrastive 1 system is a string-to-tree translation system with similar features as the ones described in [15]. The target-side data was parsed with BitPar [48], and right binarization was applied to the parse trees. The system was adapted to the TED domain by extracting separate rule tables (from the WIT3 corpus and from the rest of the parallel data) and merging them with a fill-up technique [49]. Augmenting the system with non-syntactic phrases [50] and adding soft source syntactic constraints [51] yielded further improvements. Model weights of the syntaxbased system were optimized on a concatenation of dev2010 and dev2012. System Combination. We combined the outputs of the phrase-based primary system, the auxiliary phrase-based indomain system, a"
2014.iwslt-evaluation.6,J03-1002,0,\N,Missing
2014.iwslt-evaluation.6,2011.iwslt-evaluation.18,0,\N,Missing
2014.iwslt-evaluation.7,D14-1003,1,0.921764,"slation of spoken language. The IWSLT TED talks task constitutes an interesting framework for empirical testing of some of the systems for spoken language translation which are developed as part of the project. In this work, we describe the EU-BRIDGE submissions to the 2014 IWSLT translation task. This year, we combined several single systems of RWTH, UEDIN, KIT, and FBK for the German→English SLT, German→English MT, English→German MT, and English→French MT tasks. Additionally to the standard system combination pipeline presented in [1, 2], we applied a recurrent neural network rescoring step [3] for the English→French MT task. Similar cooperative approaches based on system combination have proven to be valuable for machine translation in previous joint submissions, e.g. [4, 5]. 2. RWTH Aachen University RWTH applied the identical training pipeline and models on both language pairs: The state-of-the-art phrase-based baseline systems were augmented with a hierarchical reordering model, several additional language models (LMs) and maximum expected B LEU training for phrasal, lexical and reordering models. Further, RWTH employed rescoring with novel recurrent neural language and translat"
2014.iwslt-evaluation.7,W10-1738,1,0.885248,"and maximum expected B LEU training for phrasal, lexical and reordering models. Further, RWTH employed rescoring with novel recurrent neural language and translation models. The same systems were used for the SLT track, where RWTH ad57 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 ditionally performed punctuation prediction on the automatic transcriptions employing hierarchical phrase-based translation. Both the phrase-based and the hierarchical decoder are implemented in RWTH’s publicly available translation toolkit Jane [6, 7]. The model weights of all systems were tuned with standard Minimum Error Rate Training [8] on the provided dev2012 set. RWTH used B LEU as optimization objective. For the German→English translation direction, in a preprocessing step the German source was decompounded [9] and part-of-speech-based long-range verb reordering rules [10] were applied. RWTH’s translation systems are described in more detail in [11]. Backoff Language Models Each translation system used three backoff LMs that were estimated with the KenLM toolkit [12]: A large general domain 5-gram LM, an in-domain 5-gram LM and a 7-"
2014.iwslt-evaluation.7,P03-1021,0,0.488353,"employed rescoring with novel recurrent neural language and translation models. The same systems were used for the SLT track, where RWTH ad57 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 ditionally performed punctuation prediction on the automatic transcriptions employing hierarchical phrase-based translation. Both the phrase-based and the hierarchical decoder are implemented in RWTH’s publicly available translation toolkit Jane [6, 7]. The model weights of all systems were tuned with standard Minimum Error Rate Training [8] on the provided dev2012 set. RWTH used B LEU as optimization objective. For the German→English translation direction, in a preprocessing step the German source was decompounded [9] and part-of-speech-based long-range verb reordering rules [10] were applied. RWTH’s translation systems are described in more detail in [11]. Backoff Language Models Each translation system used three backoff LMs that were estimated with the KenLM toolkit [12]: A large general domain 5-gram LM, an in-domain 5-gram LM and a 7-gram word class language model (wcLM). All of them used interpolated Kneser-Ney smoothing."
2014.iwslt-evaluation.7,popovic-ney-2006-pos,1,0.798687,"th and 5th, 2014 ditionally performed punctuation prediction on the automatic transcriptions employing hierarchical phrase-based translation. Both the phrase-based and the hierarchical decoder are implemented in RWTH’s publicly available translation toolkit Jane [6, 7]. The model weights of all systems were tuned with standard Minimum Error Rate Training [8] on the provided dev2012 set. RWTH used B LEU as optimization objective. For the German→English translation direction, in a preprocessing step the German source was decompounded [9] and part-of-speech-based long-range verb reordering rules [10] were applied. RWTH’s translation systems are described in more detail in [11]. Backoff Language Models Each translation system used three backoff LMs that were estimated with the KenLM toolkit [12]: A large general domain 5-gram LM, an in-domain 5-gram LM and a 7-gram word class language model (wcLM). All of them used interpolated Kneser-Ney smoothing. For the general domain LM, RWTH first selected 12 of the English Shuffled News, and 41 of the French Shuffled News as well as both the English and French Gigaword corpora by the cross-entropy difference criterion described in [13]. The selectio"
2014.iwslt-evaluation.7,P13-2121,1,0.819366,"mplemented in RWTH’s publicly available translation toolkit Jane [6, 7]. The model weights of all systems were tuned with standard Minimum Error Rate Training [8] on the provided dev2012 set. RWTH used B LEU as optimization objective. For the German→English translation direction, in a preprocessing step the German source was decompounded [9] and part-of-speech-based long-range verb reordering rules [10] were applied. RWTH’s translation systems are described in more detail in [11]. Backoff Language Models Each translation system used three backoff LMs that were estimated with the KenLM toolkit [12]: A large general domain 5-gram LM, an in-domain 5-gram LM and a 7-gram word class language model (wcLM). All of them used interpolated Kneser-Ney smoothing. For the general domain LM, RWTH first selected 12 of the English Shuffled News, and 41 of the French Shuffled News as well as both the English and French Gigaword corpora by the cross-entropy difference criterion described in [13]. The selection was then concatenated with all available remaining monolingual data and used to build and unpruned LM. The in-domain language models were estimated on the TED data only. For the word class LM, RWT"
2014.iwslt-evaluation.7,P10-2041,0,0.0916594,"rdering rules [10] were applied. RWTH’s translation systems are described in more detail in [11]. Backoff Language Models Each translation system used three backoff LMs that were estimated with the KenLM toolkit [12]: A large general domain 5-gram LM, an in-domain 5-gram LM and a 7-gram word class language model (wcLM). All of them used interpolated Kneser-Ney smoothing. For the general domain LM, RWTH first selected 12 of the English Shuffled News, and 41 of the French Shuffled News as well as both the English and French Gigaword corpora by the cross-entropy difference criterion described in [13]. The selection was then concatenated with all available remaining monolingual data and used to build and unpruned LM. The in-domain language models were estimated on the TED data only. For the word class LM, RWTH trained 200 classes on the target side of the bilingual training data using an in-house tool similar to mkcls [14]. With these class definitions, RWTH applied the technique shown in [15] to compute the wcLM on the same data as the general-domain LM. Maximum Expected B LEU Training RWTH applied discriminative training, learning three types of features under a maximum expected B LEU ob"
2014.iwslt-evaluation.7,E99-1010,0,0.0737032,"them used interpolated Kneser-Ney smoothing. For the general domain LM, RWTH first selected 12 of the English Shuffled News, and 41 of the French Shuffled News as well as both the English and French Gigaword corpora by the cross-entropy difference criterion described in [13]. The selection was then concatenated with all available remaining monolingual data and used to build and unpruned LM. The in-domain language models were estimated on the TED data only. For the word class LM, RWTH trained 200 classes on the target side of the bilingual training data using an in-house tool similar to mkcls [14]. With these class definitions, RWTH applied the technique shown in [15] to compute the wcLM on the same data as the general-domain LM. Maximum Expected B LEU Training RWTH applied discriminative training, learning three types of features under a maximum expected B LEU objective [16]. It was performed on the TED portion of the data, which is high quality in-domain data of reasonable size. This makes training feasible while at the same time providing an implicit domain adaptation effect. Similar to [16], RWTH generated 100-best lists on the training data which were used as training samples for"
2014.iwslt-evaluation.7,D13-1138,1,0.85854,"RWTH first selected 12 of the English Shuffled News, and 41 of the French Shuffled News as well as both the English and French Gigaword corpora by the cross-entropy difference criterion described in [13]. The selection was then concatenated with all available remaining monolingual data and used to build and unpruned LM. The in-domain language models were estimated on the TED data only. For the word class LM, RWTH trained 200 classes on the target side of the bilingual training data using an in-house tool similar to mkcls [14]. With these class definitions, RWTH applied the technique shown in [15] to compute the wcLM on the same data as the general-domain LM. Maximum Expected B LEU Training RWTH applied discriminative training, learning three types of features under a maximum expected B LEU objective [16]. It was performed on the TED portion of the data, which is high quality in-domain data of reasonable size. This makes training feasible while at the same time providing an implicit domain adaptation effect. Similar to [16], RWTH generated 100-best lists on the training data which were used as training samples for a gradient based update method. Leave-oneout [17] was applied to circumv"
2014.iwslt-evaluation.7,P12-1031,0,0.0125863,"lection was then concatenated with all available remaining monolingual data and used to build and unpruned LM. The in-domain language models were estimated on the TED data only. For the word class LM, RWTH trained 200 classes on the target side of the bilingual training data using an in-house tool similar to mkcls [14]. With these class definitions, RWTH applied the technique shown in [15] to compute the wcLM on the same data as the general-domain LM. Maximum Expected B LEU Training RWTH applied discriminative training, learning three types of features under a maximum expected B LEU objective [16]. It was performed on the TED portion of the data, which is high quality in-domain data of reasonable size. This makes training feasible while at the same time providing an implicit domain adaptation effect. Similar to [16], RWTH generated 100-best lists on the training data which were used as training samples for a gradient based update method. Leave-oneout [17] was applied to circumvent over-fitting. Here, RWTH followed an approach similar to [18], where each feature type was condensed into a single feature for the log-linear model combination. In the first pass, RWTH trained phrase pair and"
2014.iwslt-evaluation.7,P10-1049,1,0.833909,"the technique shown in [15] to compute the wcLM on the same data as the general-domain LM. Maximum Expected B LEU Training RWTH applied discriminative training, learning three types of features under a maximum expected B LEU objective [16]. It was performed on the TED portion of the data, which is high quality in-domain data of reasonable size. This makes training feasible while at the same time providing an implicit domain adaptation effect. Similar to [16], RWTH generated 100-best lists on the training data which were used as training samples for a gradient based update method. Leave-oneout [17] was applied to circumvent over-fitting. Here, RWTH followed an approach similar to [18], where each feature type was condensed into a single feature for the log-linear model combination. In the first pass, RWTH trained phrase pair and phrase-internal word pair features, and in the second pass a hierarchical reordering model, resulting altogether in an additional eight models for log-linear combination. Recurrent Neural Network Models All systems applied rescoring on 1000-best lists using recurrent language and translation models. The recurrency was handled with the long short-term memory (LST"
2014.iwslt-evaluation.7,D14-1132,0,0.157332,"M. Maximum Expected B LEU Training RWTH applied discriminative training, learning three types of features under a maximum expected B LEU objective [16]. It was performed on the TED portion of the data, which is high quality in-domain data of reasonable size. This makes training feasible while at the same time providing an implicit domain adaptation effect. Similar to [16], RWTH generated 100-best lists on the training data which were used as training samples for a gradient based update method. Leave-oneout [17] was applied to circumvent over-fitting. Here, RWTH followed an approach similar to [18], where each feature type was condensed into a single feature for the log-linear model combination. In the first pass, RWTH trained phrase pair and phrase-internal word pair features, and in the second pass a hierarchical reordering model, resulting altogether in an additional eight models for log-linear combination. Recurrent Neural Network Models All systems applied rescoring on 1000-best lists using recurrent language and translation models. The recurrency was handled with the long short-term memory (LSTM) architecture [19] and RWTH used a class-factored output layer for increased efficienc"
2014.iwslt-evaluation.7,2011.iwslt-papers.7,1,0.944851,"ort-term memory (LSTM) architecture [19] and RWTH used a class-factored output layer for increased efficiency as described in [20]. All neural networks were trained on the TED portion of the data with 2000 word classes. In addition to the recurrent language model (RNN-LM), RWTH applied the deep bidirectional word-based translation model (RNN-BTM) described in [3], which is capable of taking the full source context into account for each translation decision. Spoken Language Translation For the SLT task, RWTH reintroduced punctuation and case information before the actual translation similar to [21]. However, RWTH employed a hierarchical phrase-based system with a maximum of one nonterminal symbol per rule in place of a phrase-based system. A punctuation prediction system based on hierarchical translation is able to capture long-range dependencies between words and punctuation marks and is more robust for unseen word sequences. The model weights are tuned with standard MERT on 100best lists. As optimization criterion RWTH used F2 -Score rather than B LEU or W ER. More details can be found in [22]. Since punctuation predicting and recasing were applied before the actual translation, the f"
2014.iwslt-evaluation.7,2014.iwslt-papers.17,1,0.734908,"RWTH reintroduced punctuation and case information before the actual translation similar to [21]. However, RWTH employed a hierarchical phrase-based system with a maximum of one nonterminal symbol per rule in place of a phrase-based system. A punctuation prediction system based on hierarchical translation is able to capture long-range dependencies between words and punctuation marks and is more robust for unseen word sequences. The model weights are tuned with standard MERT on 100best lists. As optimization criterion RWTH used F2 -Score rather than B LEU or W ER. More details can be found in [22]. Since punctuation predicting and recasing were applied before the actual translation, the final translation systems from the MT track could be kept completely unchanged. 3. University of Edinburgh The UEDIN translation engines [23] are based on the open source Moses toolkit [24]. UEDIN set up phrase-based systems for all SLT and MT tasks covered in this paper, and additionally a string-to-tree syntax-based system [25] for the English→German MT task. The systems were trained using monolingual and parallel data from WIT3 , Europarl, MultiUN, the English and French Gigaword corpora as provided"
2014.iwslt-evaluation.7,P07-2045,1,0.0190208,"n hierarchical translation is able to capture long-range dependencies between words and punctuation marks and is more robust for unseen word sequences. The model weights are tuned with standard MERT on 100best lists. As optimization criterion RWTH used F2 -Score rather than B LEU or W ER. More details can be found in [22]. Since punctuation predicting and recasing were applied before the actual translation, the final translation systems from the MT track could be kept completely unchanged. 3. University of Edinburgh The UEDIN translation engines [23] are based on the open source Moses toolkit [24]. UEDIN set up phrase-based systems for all SLT and MT tasks covered in this paper, and additionally a string-to-tree syntax-based system [25] for the English→German MT task. The systems were trained using monolingual and parallel data from WIT3 , Europarl, MultiUN, the English and French Gigaword corpora as provided by the Linguistic Data Consortium, the German Political Speeches Corpus, and the Common Crawl, 109 , and News Commentary corpora from the WMT shared task training data. Word alignments for the MT track systems were created by aligning the data in both directions with MGIZA++ [26]"
2014.iwslt-evaluation.7,N04-1035,0,0.0565459,"equences. The model weights are tuned with standard MERT on 100best lists. As optimization criterion RWTH used F2 -Score rather than B LEU or W ER. More details can be found in [22]. Since punctuation predicting and recasing were applied before the actual translation, the final translation systems from the MT track could be kept completely unchanged. 3. University of Edinburgh The UEDIN translation engines [23] are based on the open source Moses toolkit [24]. UEDIN set up phrase-based systems for all SLT and MT tasks covered in this paper, and additionally a string-to-tree syntax-based system [25] for the English→German MT task. The systems were trained using monolingual and parallel data from WIT3 , Europarl, MultiUN, the English and French Gigaword corpora as provided by the Linguistic Data Consortium, the German Political Speeches Corpus, and the Common Crawl, 109 , and News Commentary corpora from the WMT shared task training data. Word alignments for the MT track systems were created by aligning the data in both directions with MGIZA++ [26] and symmetrizing the two trained alignments. Word alignments for the SLT track system were created using fast align [27]. The SRILM toolkit [2"
2014.iwslt-evaluation.7,W08-0509,0,0.192359,"[24]. UEDIN set up phrase-based systems for all SLT and MT tasks covered in this paper, and additionally a string-to-tree syntax-based system [25] for the English→German MT task. The systems were trained using monolingual and parallel data from WIT3 , Europarl, MultiUN, the English and French Gigaword corpora as provided by the Linguistic Data Consortium, the German Political Speeches Corpus, and the Common Crawl, 109 , and News Commentary corpora from the WMT shared task training data. Word alignments for the MT track systems were created by aligning the data in both directions with MGIZA++ [26] and symmetrizing the two trained alignments. Word alignments for the SLT track system were created using fast align [27]. The SRILM toolkit [28] was employed to train 5-gram LMs with modified Kneser-Ney smoothing [29]. UEDIN trained individual LMs on each corpus and then interpolated them using weights tuned to minimize perplexity on a development set. Common features included in the UEDIN phrase-based systems are the language model, phrase translation scores in both directions smoothed with Good-Turing discounting, lexical translation scores in both directions, word and phrase penalties, six"
2014.iwslt-evaluation.7,N13-1073,0,0.0453396,"e syntax-based system [25] for the English→German MT task. The systems were trained using monolingual and parallel data from WIT3 , Europarl, MultiUN, the English and French Gigaword corpora as provided by the Linguistic Data Consortium, the German Political Speeches Corpus, and the Common Crawl, 109 , and News Commentary corpora from the WMT shared task training data. Word alignments for the MT track systems were created by aligning the data in both directions with MGIZA++ [26] and symmetrizing the two trained alignments. Word alignments for the SLT track system were created using fast align [27]. The SRILM toolkit [28] was employed to train 5-gram LMs with modified Kneser-Ney smoothing [29]. UEDIN trained individual LMs on each corpus and then interpolated them using weights tuned to minimize perplexity on a development set. Common features included in the UEDIN phrase-based systems are the language model, phrase translation scores in both directions smoothed with Good-Turing discounting, lexical translation scores in both directions, word and phrase penalties, six simple count-based binary features, distancebased distortion costs, a hierarchical lexicalized reordering model [30], sp"
2014.iwslt-evaluation.7,C14-1041,1,0.839592,"ndividual LMs on each corpus and then interpolated them using weights tuned to minimize perplexity on a development set. Common features included in the UEDIN phrase-based systems are the language model, phrase translation scores in both directions smoothed with Good-Turing discounting, lexical translation scores in both directions, word and phrase penalties, six simple count-based binary features, distancebased distortion costs, a hierarchical lexicalized reordering model [30], sparse lexical and domain indicator features [31] and operation sequence models over different word representations [32]. Model weights were optimized with batch MIRA [33] to maximize B LEU [34]. Spoken Language Translation One of the main challenges of spoken language translation is to overcome the mismatch in the style of data that the 58 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 speech recognition systems output, and the written text that is used to train the translation model. ASR system output lacks punctuation and capitalization, which is the main stylistic differences. Previous research [35, 21, 36] suggests that it is preferrable"
2014.iwslt-evaluation.7,N12-1047,0,0.0681194,"them using weights tuned to minimize perplexity on a development set. Common features included in the UEDIN phrase-based systems are the language model, phrase translation scores in both directions smoothed with Good-Turing discounting, lexical translation scores in both directions, word and phrase penalties, six simple count-based binary features, distancebased distortion costs, a hierarchical lexicalized reordering model [30], sparse lexical and domain indicator features [31] and operation sequence models over different word representations [32]. Model weights were optimized with batch MIRA [33] to maximize B LEU [34]. Spoken Language Translation One of the main challenges of spoken language translation is to overcome the mismatch in the style of data that the 58 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 speech recognition systems output, and the written text that is used to train the translation model. ASR system output lacks punctuation and capitalization, which is the main stylistic differences. Previous research [35, 21, 36] suggests that it is preferrable to punctuate the text before translation, which is"
2014.iwslt-evaluation.7,P02-1040,0,0.0918061,"d to minimize perplexity on a development set. Common features included in the UEDIN phrase-based systems are the language model, phrase translation scores in both directions smoothed with Good-Turing discounting, lexical translation scores in both directions, word and phrase penalties, six simple count-based binary features, distancebased distortion costs, a hierarchical lexicalized reordering model [30], sparse lexical and domain indicator features [31] and operation sequence models over different word representations [32]. Model weights were optimized with batch MIRA [33] to maximize B LEU [34]. Spoken Language Translation One of the main challenges of spoken language translation is to overcome the mismatch in the style of data that the 58 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 speech recognition systems output, and the written text that is used to train the translation model. ASR system output lacks punctuation and capitalization, which is the main stylistic differences. Previous research [35, 21, 36] suggests that it is preferrable to punctuate the text before translation, which is what UEDIN did by trai"
2014.iwslt-evaluation.7,2006.iwslt-papers.1,1,0.862433,"els over different word representations [32]. Model weights were optimized with batch MIRA [33] to maximize B LEU [34]. Spoken Language Translation One of the main challenges of spoken language translation is to overcome the mismatch in the style of data that the 58 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 speech recognition systems output, and the written text that is used to train the translation model. ASR system output lacks punctuation and capitalization, which is the main stylistic differences. Previous research [35, 21, 36] suggests that it is preferrable to punctuate the text before translation, which is what UEDIN did by training a translation system on the German side of the parallel data. The “source language” of the system had punctuation and capitalization stripped, and the “target language” was the standard German parallel text. The handling of punctuation is similar to the other groups in this paper, however UEDIN used a phrase-based model with no distortion or reordering, and tuned the model to the ASR input text using batch MIRA and the B LEU score. German→English MT For the UEDIN German→English MT tas"
2014.iwslt-evaluation.7,2012.iwslt-papers.15,1,0.927241,"els over different word representations [32]. Model weights were optimized with batch MIRA [33] to maximize B LEU [34]. Spoken Language Translation One of the main challenges of spoken language translation is to overcome the mismatch in the style of data that the 58 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 speech recognition systems output, and the written text that is used to train the translation model. ASR system output lacks punctuation and capitalization, which is the main stylistic differences. Previous research [35, 21, 36] suggests that it is preferrable to punctuate the text before translation, which is what UEDIN did by training a translation system on the German side of the parallel data. The “source language” of the system had punctuation and capitalization stripped, and the “target language” was the standard German parallel text. The handling of punctuation is similar to the other groups in this paper, however UEDIN used a phrase-based model with no distortion or reordering, and tuned the model to the ASR input text using batch MIRA and the B LEU score. German→English MT For the UEDIN German→English MT tas"
2014.iwslt-evaluation.7,P05-1066,1,0.733044,"ferrable to punctuate the text before translation, which is what UEDIN did by training a translation system on the German side of the parallel data. The “source language” of the system had punctuation and capitalization stripped, and the “target language” was the standard German parallel text. The handling of punctuation is similar to the other groups in this paper, however UEDIN used a phrase-based model with no distortion or reordering, and tuned the model to the ASR input text using batch MIRA and the B LEU score. German→English MT For the UEDIN German→English MT task system, prereordering [37] and compound splitting [38] were applied to the German source language side in a preprocessing step. A factored translation model [39] was employed. Source side factors are word, lemma, part-of-speech (POS) tag, and morphological tag. Target side factors are word, lemma, and POS tag. UEDIN incorporated two additional LMs into the German→English MT system: a 7-gram LM over POS tags (trained on WIT3 only) and a 7-gram LM over lemmas (trained on WIT3 only). Model weights were optimized on a concatenation of dev2010 and dev2012. English→French MT UEDIN contributed two phrase-based systems for the"
2014.iwslt-evaluation.7,E03-1076,1,0.858704,"xt before translation, which is what UEDIN did by training a translation system on the German side of the parallel data. The “source language” of the system had punctuation and capitalization stripped, and the “target language” was the standard German parallel text. The handling of punctuation is similar to the other groups in this paper, however UEDIN used a phrase-based model with no distortion or reordering, and tuned the model to the ASR input text using batch MIRA and the B LEU score. German→English MT For the UEDIN German→English MT task system, prereordering [37] and compound splitting [38] were applied to the German source language side in a preprocessing step. A factored translation model [39] was employed. Source side factors are word, lemma, part-of-speech (POS) tag, and morphological tag. Target side factors are word, lemma, and POS tag. UEDIN incorporated two additional LMs into the German→English MT system: a 7-gram LM over POS tags (trained on WIT3 only) and a 7-gram LM over lemmas (trained on WIT3 only). Model weights were optimized on a concatenation of dev2010 and dev2012. English→French MT UEDIN contributed two phrase-based systems for the English→French EU-BRIDGE sy"
2014.iwslt-evaluation.7,2012.amta-papers.9,1,0.84942,"arallel data. The “source language” of the system had punctuation and capitalization stripped, and the “target language” was the standard German parallel text. The handling of punctuation is similar to the other groups in this paper, however UEDIN used a phrase-based model with no distortion or reordering, and tuned the model to the ASR input text using batch MIRA and the B LEU score. German→English MT For the UEDIN German→English MT task system, prereordering [37] and compound splitting [38] were applied to the German source language side in a preprocessing step. A factored translation model [39] was employed. Source side factors are word, lemma, part-of-speech (POS) tag, and morphological tag. Target side factors are word, lemma, and POS tag. UEDIN incorporated two additional LMs into the German→English MT system: a 7-gram LM over POS tags (trained on WIT3 only) and a 7-gram LM over lemmas (trained on WIT3 only). Model weights were optimized on a concatenation of dev2010 and dev2012. English→French MT UEDIN contributed two phrase-based systems for the English→French EU-BRIDGE system combination. Both comprise Brown clusters with 200 classes as additional factors on source and target"
2014.iwslt-evaluation.7,D08-1089,0,0.176922,"ign [27]. The SRILM toolkit [28] was employed to train 5-gram LMs with modified Kneser-Ney smoothing [29]. UEDIN trained individual LMs on each corpus and then interpolated them using weights tuned to minimize perplexity on a development set. Common features included in the UEDIN phrase-based systems are the language model, phrase translation scores in both directions smoothed with Good-Turing discounting, lexical translation scores in both directions, word and phrase penalties, six simple count-based binary features, distancebased distortion costs, a hierarchical lexicalized reordering model [30], sparse lexical and domain indicator features [31] and operation sequence models over different word representations [32]. Model weights were optimized with batch MIRA [33] to maximize B LEU [34]. Spoken Language Translation One of the main challenges of spoken language translation is to overcome the mismatch in the style of data that the 58 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 speech recognition systems output, and the written text that is used to train the translation model. ASR system output lacks punctuation a"
2014.iwslt-evaluation.7,W14-3324,1,0.784121,"ical tag. UEDIN-A was trained with all corpora, whereas for UEDIN-B the parallel training data was restricted to the indomain WIT3 corpus. Additional features of the systems are: a 5-gram LM over Brown clusters, a 7-gram LM over morphological tags (UEDIN-A: trained on all data, UEDIN-B: trained on WIT3 only), and a 7-gram LM over POS tags (UEDIN-A, not UEDIN-B). Model weights of UEDIN-B were optimized on dev2010, model weights of UEDIN-A on a concatenation of dev2010 and dev2012. Syntax-based system. UEDIN-C is a string-to-tree translation system with similar features as the ones described in [40]. The target-side data was parsed with BitPar [41], and right binarization was applied to the parse trees. The system was adapted to the TED domain by extracting separate rule tables (from the WIT3 corpus and from the rest of the parallel data) and merging them with a fill-up technique [42]. Augmenting the system with non-syntactic phrases [43] and adding soft source syntactic constraints [44] yielded further improvements. Model weights of UEDIN-C were optimized on a concatenation of dev2010 and dev2012. 4. Karlsruhe Institute of Technology The KIT translations were generated by an in-house ph"
2014.iwslt-evaluation.7,2012.iwslt-papers.17,1,0.881764,"ain 5-gram LMs with modified Kneser-Ney smoothing [29]. UEDIN trained individual LMs on each corpus and then interpolated them using weights tuned to minimize perplexity on a development set. Common features included in the UEDIN phrase-based systems are the language model, phrase translation scores in both directions smoothed with Good-Turing discounting, lexical translation scores in both directions, word and phrase penalties, six simple count-based binary features, distancebased distortion costs, a hierarchical lexicalized reordering model [30], sparse lexical and domain indicator features [31] and operation sequence models over different word representations [32]. Model weights were optimized with batch MIRA [33] to maximize B LEU [34]. Spoken Language Translation One of the main challenges of spoken language translation is to overcome the mismatch in the style of data that the 58 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 speech recognition systems output, and the written text that is used to train the translation model. ASR system output lacks punctuation and capitalization, which is the main stylistic diff"
2014.iwslt-evaluation.7,C04-1024,0,0.0400394,"ereas for UEDIN-B the parallel training data was restricted to the indomain WIT3 corpus. Additional features of the systems are: a 5-gram LM over Brown clusters, a 7-gram LM over morphological tags (UEDIN-A: trained on all data, UEDIN-B: trained on WIT3 only), and a 7-gram LM over POS tags (UEDIN-A, not UEDIN-B). Model weights of UEDIN-B were optimized on dev2010, model weights of UEDIN-A on a concatenation of dev2010 and dev2012. Syntax-based system. UEDIN-C is a string-to-tree translation system with similar features as the ones described in [40]. The target-side data was parsed with BitPar [41], and right binarization was applied to the parse trees. The system was adapted to the TED domain by extracting separate rule tables (from the WIT3 corpus and from the rest of the parallel data) and merging them with a fill-up technique [42]. Augmenting the system with non-syntactic phrases [43] and adding soft source syntactic constraints [44] yielded further improvements. Model weights of UEDIN-C were optimized on a concatenation of dev2010 and dev2012. 4. Karlsruhe Institute of Technology The KIT translations were generated by an in-house phrasebased translations system [45]. The models wer"
2014.iwslt-evaluation.7,2011.iwslt-evaluation.18,1,0.873679,"ined on WIT3 only), and a 7-gram LM over POS tags (UEDIN-A, not UEDIN-B). Model weights of UEDIN-B were optimized on dev2010, model weights of UEDIN-A on a concatenation of dev2010 and dev2012. Syntax-based system. UEDIN-C is a string-to-tree translation system with similar features as the ones described in [40]. The target-side data was parsed with BitPar [41], and right binarization was applied to the parse trees. The system was adapted to the TED domain by extracting separate rule tables (from the WIT3 corpus and from the rest of the parallel data) and merging them with a fill-up technique [42]. Augmenting the system with non-syntactic phrases [43] and adding soft source syntactic constraints [44] yielded further improvements. Model weights of UEDIN-C were optimized on a concatenation of dev2010 and dev2012. 4. Karlsruhe Institute of Technology The KIT translations were generated by an in-house phrasebased translations system [45]. The models were trained on the Europarl, News Commentary, WIT3 , Common Crawl corpora for all directions, as well as on the additional monolingual training data. The noisy Crawl corpora were filtered using an SVM classifier [46]. In addition to the standa"
2014.iwslt-evaluation.7,W14-3362,1,0.610881,"N-A, not UEDIN-B). Model weights of UEDIN-B were optimized on dev2010, model weights of UEDIN-A on a concatenation of dev2010 and dev2012. Syntax-based system. UEDIN-C is a string-to-tree translation system with similar features as the ones described in [40]. The target-side data was parsed with BitPar [41], and right binarization was applied to the parse trees. The system was adapted to the TED domain by extracting separate rule tables (from the WIT3 corpus and from the rest of the parallel data) and merging them with a fill-up technique [42]. Augmenting the system with non-syntactic phrases [43] and adding soft source syntactic constraints [44] yielded further improvements. Model weights of UEDIN-C were optimized on a concatenation of dev2010 and dev2012. 4. Karlsruhe Institute of Technology The KIT translations were generated by an in-house phrasebased translations system [45]. The models were trained on the Europarl, News Commentary, WIT3 , Common Crawl corpora for all directions, as well as on the additional monolingual training data. The noisy Crawl corpora were filtered using an SVM classifier [46]. In addition to the standard preprocessing, KIT used compound splitting [38] for"
2014.iwslt-evaluation.7,W14-4018,1,0.774295,"ptimized on dev2010, model weights of UEDIN-A on a concatenation of dev2010 and dev2012. Syntax-based system. UEDIN-C is a string-to-tree translation system with similar features as the ones described in [40]. The target-side data was parsed with BitPar [41], and right binarization was applied to the parse trees. The system was adapted to the TED domain by extracting separate rule tables (from the WIT3 corpus and from the rest of the parallel data) and merging them with a fill-up technique [42]. Augmenting the system with non-syntactic phrases [43] and adding soft source syntactic constraints [44] yielded further improvements. Model weights of UEDIN-C were optimized on a concatenation of dev2010 and dev2012. 4. Karlsruhe Institute of Technology The KIT translations were generated by an in-house phrasebased translations system [45]. The models were trained on the Europarl, News Commentary, WIT3 , Common Crawl corpora for all directions, as well as on the additional monolingual training data. The noisy Crawl corpora were filtered using an SVM classifier [46]. In addition to the standard preprocessing, KIT used compound splitting [38] for the German text when translating from German. In t"
2014.iwslt-evaluation.7,2011.iwslt-evaluation.9,1,0.861968,"m with a fill-up technique [42]. Augmenting the system with non-syntactic phrases [43] and adding soft source syntactic constraints [44] yielded further improvements. Model weights of UEDIN-C were optimized on a concatenation of dev2010 and dev2012. 4. Karlsruhe Institute of Technology The KIT translations were generated by an in-house phrasebased translations system [45]. The models were trained on the Europarl, News Commentary, WIT3 , Common Crawl corpora for all directions, as well as on the additional monolingual training data. The noisy Crawl corpora were filtered using an SVM classifier [46]. In addition to the standard preprocessing, KIT used compound splitting [38] for the German text when translating from German. In the SLT task, KIT first recased the input and added punctuation marks to the ASR hypotheses. This was done with a monolingual translation system as shown in [36]. In all translation directions, KIT used a pre-reordering approach. Different reorderings of the source sentences were encoded in a word lattice. For the English→French system, only short-range rules were used to generate these lattices [47]. Long-range rules [48] and tree-based reordering rules [49] were"
2014.iwslt-evaluation.7,2007.tmi-papers.21,0,0.0614729,"ta. The noisy Crawl corpora were filtered using an SVM classifier [46]. In addition to the standard preprocessing, KIT used compound splitting [38] for the German text when translating from German. In the SLT task, KIT first recased the input and added punctuation marks to the ASR hypotheses. This was done with a monolingual translation system as shown in [36]. In all translation directions, KIT used a pre-reordering approach. Different reorderings of the source sentences were encoded in a word lattice. For the English→French system, only short-range rules were used to generate these lattices [47]. Long-range rules [48] and tree-based reordering rules [49] were used for German→English. The POS tags needed for these rules were generated by the TreeTagger [50] and the parse trees by the Stanford Parser [51]. In addition, for the language pairs involving German KIT applied the different reorderings of both language pairs using a lexicalized reordering model. The phrase tables of the systems were trained using GIZA++ alignment [52]. KIT adapted the phrase table to the TED domain using the backoff approach and by means of candidate selection [53]. In addition to the phrase table probabiliti"
2014.iwslt-evaluation.7,W09-0413,1,0.842557,"pora were filtered using an SVM classifier [46]. In addition to the standard preprocessing, KIT used compound splitting [38] for the German text when translating from German. In the SLT task, KIT first recased the input and added punctuation marks to the ASR hypotheses. This was done with a monolingual translation system as shown in [36]. In all translation directions, KIT used a pre-reordering approach. Different reorderings of the source sentences were encoded in a word lattice. For the English→French system, only short-range rules were used to generate these lattices [47]. Long-range rules [48] and tree-based reordering rules [49] were used for German→English. The POS tags needed for these rules were generated by the TreeTagger [50] and the parse trees by the Stanford Parser [51]. In addition, for the language pairs involving German KIT applied the different reorderings of both language pairs using a lexicalized reordering model. The phrase tables of the systems were trained using GIZA++ alignment [52]. KIT adapted the phrase table to the TED domain using the backoff approach and by means of candidate selection [53]. In addition to the phrase table probabilities, KIT modeled the tra"
2014.iwslt-evaluation.7,W13-0805,1,0.85195,"ifier [46]. In addition to the standard preprocessing, KIT used compound splitting [38] for the German text when translating from German. In the SLT task, KIT first recased the input and added punctuation marks to the ASR hypotheses. This was done with a monolingual translation system as shown in [36]. In all translation directions, KIT used a pre-reordering approach. Different reorderings of the source sentences were encoded in a word lattice. For the English→French system, only short-range rules were used to generate these lattices [47]. Long-range rules [48] and tree-based reordering rules [49] were used for German→English. The POS tags needed for these rules were generated by the TreeTagger [50] and the parse trees by the Stanford Parser [51]. In addition, for the language pairs involving German KIT applied the different reorderings of both language pairs using a lexicalized reordering model. The phrase tables of the systems were trained using GIZA++ alignment [52]. KIT adapted the phrase table to the TED domain using the backoff approach and by means of candidate selection [53]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual langu"
2014.iwslt-evaluation.7,W08-1006,0,0.0150981,"k, KIT first recased the input and added punctuation marks to the ASR hypotheses. This was done with a monolingual translation system as shown in [36]. In all translation directions, KIT used a pre-reordering approach. Different reorderings of the source sentences were encoded in a word lattice. For the English→French system, only short-range rules were used to generate these lattices [47]. Long-range rules [48] and tree-based reordering rules [49] were used for German→English. The POS tags needed for these rules were generated by the TreeTagger [50] and the parse trees by the Stanford Parser [51]. In addition, for the language pairs involving German KIT applied the different reorderings of both language pairs using a lexicalized reordering model. The phrase tables of the systems were trained using GIZA++ alignment [52]. KIT adapted the phrase table to the TED domain using the backoff approach and by means of candidate selection [53]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [54] and a discriminative word lexicon using source context features [55]. During decoding, KIT used several LMs to adapt the system to the ta"
2014.iwslt-evaluation.7,2012.amta-papers.19,1,0.839901,"e rules were used to generate these lattices [47]. Long-range rules [48] and tree-based reordering rules [49] were used for German→English. The POS tags needed for these rules were generated by the TreeTagger [50] and the parse trees by the Stanford Parser [51]. In addition, for the language pairs involving German KIT applied the different reorderings of both language pairs using a lexicalized reordering model. The phrase tables of the systems were trained using GIZA++ alignment [52]. KIT adapted the phrase table to the TED domain using the backoff approach and by means of candidate selection [53]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [54] and a discriminative word lexicon using source context features [55]. During decoding, KIT used several LMs to adapt the system to the task and to better model the sentence structure using a class-based LM. For the German→English task, KIT used one LM trained on all data, an in-domain LM trained only on the WIT3 corpus, and one LM trained on 5M sentences selected using cross-entropy difference [13]. As classes KIT used the clusters obtained using the mkcls algorithm on the WI"
2014.iwslt-evaluation.7,W11-2124,1,0.902739,"for German→English. The POS tags needed for these rules were generated by the TreeTagger [50] and the parse trees by the Stanford Parser [51]. In addition, for the language pairs involving German KIT applied the different reorderings of both language pairs using a lexicalized reordering model. The phrase tables of the systems were trained using GIZA++ alignment [52]. KIT adapted the phrase table to the TED domain using the backoff approach and by means of candidate selection [53]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [54] and a discriminative word lexicon using source context features [55]. During decoding, KIT used several LMs to adapt the system to the task and to better model the sentence structure using a class-based LM. For the German→English task, KIT used one LM trained on all data, an in-domain LM trained only on the WIT3 corpus, and one LM trained on 5M sentences selected using cross-entropy difference [13]. As classes KIT used the clusters obtained using the mkcls algorithm on the WIT3 corpus. For German↔ English, KIT used a 9-gram LM with 100 or 1000 clusters and for the English→French MT task, a cl"
2014.iwslt-evaluation.7,W13-2264,1,0.835602,"ed by the TreeTagger [50] and the parse trees by the Stanford Parser [51]. In addition, for the language pairs involving German KIT applied the different reorderings of both language pairs using a lexicalized reordering model. The phrase tables of the systems were trained using GIZA++ alignment [52]. KIT adapted the phrase table to the TED domain using the backoff approach and by means of candidate selection [53]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [54] and a discriminative word lexicon using source context features [55]. During decoding, KIT used several LMs to adapt the system to the task and to better model the sentence structure using a class-based LM. For the German→English task, KIT used one LM trained on all data, an in-domain LM trained only on the WIT3 corpus, and one LM trained on 5M sentences selected using cross-entropy difference [13]. As classes KIT used the clusters obtained using the mkcls algorithm on the WIT3 corpus. For German↔ English, KIT used a 9-gram LM with 100 or 1000 clusters and for the English→French MT task, a cluster-based 4-gram LM was trained on 500 clusters. For English→German"
2014.iwslt-evaluation.7,2012.eamt-1.60,1,0.892622,"Missing"
2014.iwslt-evaluation.7,D11-1033,0,0.167316,"Missing"
2014.iwslt-evaluation.7,W05-0909,0,0.085167,"m multiple hypotheses which are outputs of different translation engines. The consensus translations can be better in terms of translation quality than any of the individual hypotheses. To combine the engines of the project partners for the EU-BRIDGE joint setups, we applied a system combination implementation that has been developed at RWTH Aachen University [1]. In Fig. 1 an overview is illustrated. We first address the generation of a confusion network (CN) from I input translations. For that we need a pairwise alignment between all input hypotheses. This alignment is calculated via METEOR [60]. The hypotheses are then reordered to match the word order of a selected skeleton hypothesis. Instead of using only one of the input hypothesis as skeleton, we generate I different CNs, each having one of the input systems as skeleton. The final lattice is the union of all I previous generated CNs. In Fig. 2 an example confusion network of I = 4 input translations with one skeleton translation is illustrated. Between two adjacent nodes, we always have a choice between the I different system output words. The confusion network decoding step involves determining the shortest path through the ne"
2014.iwslt-evaluation.7,2006.amta-papers.25,0,0.0356913,"andard set of models is a word penalty, a 3-gram language model trained on the input hypotheses, and for each system one binary voting feature. During decoding the binary voting feature for system i (1 ≤ i ≤ I) is 1 iff the word is from system i, otherwise 0. The M different model weights λm are trained with MERT [8]. the red cab the a a red blue green train car car Figure 2: System A: the red cab ; System B: the red train ; System C: a blue car ; System D: a green car ; Reference: the blue car . 7. Results In this section, we present our experimental results. All reported B LEU [34] and T ER [61] scores are case-sensitive with one reference. All system combination results have been generated with RWTH’s open source system combination implementation Jane [1]. German→English SLT For the German→English SLT task, we combined three different individual systems generated by UEDIN, KIT, and RWTH. Experimental results are given in Table 1. The final system combination yields improvements of 1.5 points in B LEU and 1.2 points in T ER compared to the best single system (KIT). All single systems as well as the system combination parameters were tuned on dev2012. For this year’s IWSLT SLT track,"
2014.iwslt-evaluation.7,E06-1005,1,\N,Missing
2014.iwslt-evaluation.7,P11-1105,1,\N,Missing
2014.iwslt-evaluation.7,W10-1711,1,\N,Missing
2014.iwslt-evaluation.7,2010.iwslt-evaluation.22,1,\N,Missing
2014.iwslt-evaluation.7,E14-2008,1,\N,Missing
2014.iwslt-evaluation.7,2014.iwslt-evaluation.6,1,\N,Missing
2014.iwslt-evaluation.7,J03-1002,1,\N,Missing
2014.iwslt-evaluation.7,C12-3061,1,\N,Missing
2014.iwslt-evaluation.7,2013.iwslt-evaluation.16,1,\N,Missing
2014.iwslt-evaluation.7,W14-3310,1,\N,Missing
2016.amta-researchers.13,P02-1040,0,\N,Missing
2016.amta-researchers.13,W09-0429,1,\N,Missing
2016.amta-researchers.13,E14-4029,1,\N,Missing
2016.amta-researchers.13,P07-2045,1,\N,Missing
2016.amta-researchers.13,N13-1090,0,\N,Missing
2016.amta-researchers.13,P06-4018,0,\N,Missing
2016.amta-researchers.13,P13-1140,0,\N,Missing
2016.amta-researchers.13,P08-2015,0,\N,Missing
2016.amta-researchers.13,J03-1002,0,\N,Missing
2016.amta-researchers.13,2012.eamt-1.41,0,\N,Missing
2016.amta-researchers.13,N13-1056,0,\N,Missing
2016.amta-researchers.13,buck-etal-2014-n,0,\N,Missing
2016.amta-researchers.13,W11-3603,0,\N,Missing
2016.amta-researchers.13,N15-1176,0,\N,Missing
2016.amta-researchers.13,E14-1049,0,\N,Missing
2016.amta-researchers.13,N15-1058,0,\N,Missing
2016.amta-researchers.2,P13-2071,1,0.846761,"istics (ACL). Among the data they freely provide are training and test data sets. As part of their 8th Workshop on Statistical Machine Translation (WMT 2013)3 , the organizers released a test set comprised of 3,000 source English sentences and their corresponding Spanish human reference translations. We trained nine MT systems with training data from the European Parliament proceedings, News Commentary, Common Crawl, and United Nations. The systems are phrase-based Moses systems (Koehn et al., 2007) with hierarchical lexicalized reordering (Galley and Manning, 2008), operation sequence model (Durrani et al., 2013), and sparse lexical features, using all available language model data (target side of the full parallel corpus, plus the provided monolingual news corpus and LDC Gigaword). The best system reaches comparable quality to the best system participating in the WMT 2013 evaluation campaign. We iteratively halved the parallel training corpus to obtain systems of inferior quality. The quality of the MT systems was measured with case-sensitive BLEU (Papineni et al., 2002) on the ofﬁcial WMT 2013 test set. Table 3 summarizes MT systems’ quality and training corpus size. System MT1 MT2 MT3 MT4 MT5 MT6 M"
2016.amta-researchers.2,2009.mtsummit-commercial.5,0,0.106204,"Missing"
2016.amta-researchers.2,D08-1089,0,0.0404296,"n campaign of the Association for Computational Linguistics (ACL). Among the data they freely provide are training and test data sets. As part of their 8th Workshop on Statistical Machine Translation (WMT 2013)3 , the organizers released a test set comprised of 3,000 source English sentences and their corresponding Spanish human reference translations. We trained nine MT systems with training data from the European Parliament proceedings, News Commentary, Common Crawl, and United Nations. The systems are phrase-based Moses systems (Koehn et al., 2007) with hierarchical lexicalized reordering (Galley and Manning, 2008), operation sequence model (Durrani et al., 2013), and sparse lexical features, using all available language model data (target side of the full parallel corpus, plus the provided monolingual news corpus and LDC Gigaword). The best system reaches comparable quality to the best system participating in the WMT 2013 evaluation campaign. We iteratively halved the parallel training corpus to obtain systems of inferior quality. The quality of the MT systems was measured with case-sensitive BLEU (Papineni et al., 2002) on the ofﬁcial WMT 2013 test set. Table 3 summarizes MT systems’ quality and train"
2016.amta-researchers.2,2009.mtsummit-commercial.6,0,0.0295689,"Missing"
2016.amta-researchers.2,W14-0307,1,0.834467,"scribed as the biggest online translation workplace. Selected participants were offered a ﬁxed compensation based on the standard, general, English–Spanish translation rates displayed on the website. Each translator post-edited four news texts of about 650 words each. Texts had similar complexity levels and were presented to translators in randomized order to dilute possible familiarization or fatigue effects. All four texts were translated into Spanish with nine MT systems (cf. Section 4.1). The output of all nine systems was assigned randomly to participants. As in Cettolo et al. (2013) and Koehn and Germann (2014), to deal with between-participant variability, the following restrictions were implemented: • All translators post-edited all source sentences. • No translator post-edited the same source sentence twice. • All translators were exposed to roughly the same amount of output of all MT systems. Translators were asked to post-edit to full, human-like quality. They worked remotely on the open-source, web-based, computer aided translation (CAT) tool CASMACAT (Cognitive Analysis and Statistical Methods for Advanced Computer Aided Translation)2 (Alabau et al., 2013). 3 Translator Proﬁle All participant"
2016.amta-researchers.2,P07-2045,1,0.010882,"oration and efforts. One of these efforts is the WMT evaluation campaign of the Association for Computational Linguistics (ACL). Among the data they freely provide are training and test data sets. As part of their 8th Workshop on Statistical Machine Translation (WMT 2013)3 , the organizers released a test set comprised of 3,000 source English sentences and their corresponding Spanish human reference translations. We trained nine MT systems with training data from the European Parliament proceedings, News Commentary, Common Crawl, and United Nations. The systems are phrase-based Moses systems (Koehn et al., 2007) with hierarchical lexicalized reordering (Galley and Manning, 2008), operation sequence model (Durrani et al., 2013), and sparse lexical features, using all available language model data (target side of the full parallel corpus, plus the provided monolingual news corpus and LDC Gigaword). The best system reaches comparable quality to the best system participating in the WMT 2013 evaluation campaign. We iteratively halved the parallel training corpus to obtain systems of inferior quality. The quality of the MT systems was measured with case-sensitive BLEU (Papineni et al., 2002) on the ofﬁcial"
2016.amta-researchers.2,2013.mtsummit-wptp.10,0,0.0710119,"Missing"
2016.amta-researchers.2,P02-1040,0,0.0983289,"ed Moses systems (Koehn et al., 2007) with hierarchical lexicalized reordering (Galley and Manning, 2008), operation sequence model (Durrani et al., 2013), and sparse lexical features, using all available language model data (target side of the full parallel corpus, plus the provided monolingual news corpus and LDC Gigaword). The best system reaches comparable quality to the best system participating in the WMT 2013 evaluation campaign. We iteratively halved the parallel training corpus to obtain systems of inferior quality. The quality of the MT systems was measured with case-sensitive BLEU (Papineni et al., 2002) on the ofﬁcial WMT 2013 test set. Table 3 summarizes MT systems’ quality and training corpus size. System MT1 MT2 MT3 MT4 MT5 MT6 MT7 MT8 MT9 BLEU 30.37 30.08 29.60 29.16 28.61 27.89 26.93 26.14 24.85 Training sentences 14,700k 7,350k 3,675k 1,837k 918k 459k 230k 115k 57k Training words (English) 385M 192M 96M 48M 24M 12M 6.0M 3.0M 1.5M Table 3: MT Systems’ quality and training corpus size 3 http://www.statmt.org/wmt13/ 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S 4.2 Human-mediated Translation Edit Rate Human-mediated Translation Edit Rate ("
2016.amta-researchers.2,2011.eamt-1.7,0,0.0350191,"Missing"
2016.amta-researchers.2,2006.amta-papers.25,0,0.212039,"Missing"
2016.amta-researchers.2,2009.mtsummit-posters.20,0,0.374544,"Missing"
2016.amta-researchers.9,P11-2068,0,0.0186749,"e, we use a statistical machine translation system for interactive translation prediction that closely follows Koehn (2009) and Koehn et al. (2014). The preﬁx could be matched by constraint decoding, however, at a much higher computational cost. Initial work on interactive translation prediction can be found in the TransType and TransType2 projects (Langlais et al., 2000; Foster et al., 2002; Bender et al., 2005; Barrachina et al., 2009). Our current work focuses on how to produce suggestions; for various approaches to interaction modalities, see Sanchis-Trilles et al. (2008) (mouse actions), Alabau et al. (2011) (hand-writing) and Cubel et al. (2009) (speech). 2 Neural Machine Translation The use of neural network methods in machine translation has followed their recent success in computer vision and automatic speech recognition. Motivations for their use include better generalization of the statistical evidence (such as the use of word embeddings that have similar representations for related words), and more powerful non-linear inference. The current state-of-the-art neural machine translation approach (Bahdanau et al., 2015) consists of: • an encoder stage where the input sentence is processed by t"
2016.amta-researchers.9,J09-1002,0,0.888884,"e translation (“auto-complete”), and the translator either accepts suggested words or writes in their own translation. When the suggestion is rejected, the machine translation system recomputes its prediction for how to complete the sentence from the given preﬁx and presents the corrected version to the translator. In prior work, phrase-based machine translation systems have been used for interactive translation prediction, and suggestions were made either by re-decoding constrained by the preﬁx (Green et al., 2014) or by searching for the preﬁx in the original search graph (Och et al., 2003; Barrachina et al., 2009). Recently, neural translation models have been proposed and in some cases have shown superior performance over phrase-based models (Jean et al., 2015; Sennrich et al., 2016). We propose to use such models for interactive translation prediction. Parallel to this work, Wuebker et al. (2016) also explore a similar approach to using neural MT for interactive translation prediction. The decoding mechanism for neural models provides a natural way of doing interactive translation prediction. We show that neural translation models can provide better translation prediction quality and improved recover"
2016.amta-researchers.9,2005.eamt-1.6,0,0.0217693,"to match the partial translator input (called the preﬁx) to the search graph, using approximate string matching techniques (minimal string edit distance) when an exact match cannot be found. As a baseline, we use a statistical machine translation system for interactive translation prediction that closely follows Koehn (2009) and Koehn et al. (2014). The preﬁx could be matched by constraint decoding, however, at a much higher computational cost. Initial work on interactive translation prediction can be found in the TransType and TransType2 projects (Langlais et al., 2000; Foster et al., 2002; Bender et al., 2005; Barrachina et al., 2009). Our current work focuses on how to produce suggestions; for various approaches to interaction modalities, see Sanchis-Trilles et al. (2008) (mouse actions), Alabau et al. (2011) (hand-writing) and Cubel et al. (2009) (speech). 2 Neural Machine Translation The use of neural network methods in machine translation has followed their recent success in computer vision and automatic speech recognition. Motivations for their use include better generalization of the statistical evidence (such as the use of word embeddings that have similar representations for related words)"
2016.amta-researchers.9,W16-2310,1,0.822955,"anslating part of the monolingual news data into German (Sennrich et al., 2015a). It uses byte pair encoding (Sennrich et al., 2015b) for a vocabulary of 90,000 words. Training this model on GPU takes about three weeks. We use the publicly available model6 that matches the training settings of Edinburgh’s WMT submission (Sennrich et al., 2016). Phrase-Based Model The phrase based model, which we use as a baseline to produce search graph based predictions, uses all available parallel and monolingual training data. The system matches Johns Hopkins University’s submission to the WMT shared task (Ding et al., 2016). This version does not use the byte pair encodings used by the neural models. System Quality Since we are concerned with the translation speed, we consider a few simpliﬁcations of the neural translation model. We do not use ensemble decoding (”ensemble”) or a reranking stage (”r2l reranking”)7 . Each of these simpliﬁcations makes decoding several times faster at a cost to quality of 1-2 BLEU points. See Table 1 for a comparison of quality scores for the different settings. Without beam search, the neural system used has the same BLEU score as the phrase-based system. This has the nice advanta"
2016.amta-researchers.9,W02-1020,0,0.315356,". The system attempts to match the partial translator input (called the preﬁx) to the search graph, using approximate string matching techniques (minimal string edit distance) when an exact match cannot be found. As a baseline, we use a statistical machine translation system for interactive translation prediction that closely follows Koehn (2009) and Koehn et al. (2014). The preﬁx could be matched by constraint decoding, however, at a much higher computational cost. Initial work on interactive translation prediction can be found in the TransType and TransType2 projects (Langlais et al., 2000; Foster et al., 2002; Bender et al., 2005; Barrachina et al., 2009). Our current work focuses on how to produce suggestions; for various approaches to interaction modalities, see Sanchis-Trilles et al. (2008) (mouse actions), Alabau et al. (2011) (hand-writing) and Cubel et al. (2009) (speech). 2 Neural Machine Translation The use of neural network methods in machine translation has followed their recent success in computer vision and automatic speech recognition. Motivations for their use include better generalization of the statistical evidence (such as the use of word embeddings that have similar representatio"
2016.amta-researchers.9,D14-1130,0,0.139859,"Missing"
2016.amta-researchers.9,W15-3014,0,0.0465125,"Missing"
2016.amta-researchers.9,P14-2094,1,0.843482,"(Koehn, 2009). The goal of interactive translation prediction is to offer suggestions that the translator will accept. Existing approaches with statistical machine translation use the static search graph produced by the machine translation system. The system attempts to match the partial translator input (called the preﬁx) to the search graph, using approximate string matching techniques (minimal string edit distance) when an exact match cannot be found. As a baseline, we use a statistical machine translation system for interactive translation prediction that closely follows Koehn (2009) and Koehn et al. (2014). The preﬁx could be matched by constraint decoding, however, at a much higher computational cost. Initial work on interactive translation prediction can be found in the TransType and TransType2 projects (Langlais et al., 2000; Foster et al., 2002; Bender et al., 2005; Barrachina et al., 2009). Our current work focuses on how to produce suggestions; for various approaches to interaction modalities, see Sanchis-Trilles et al. (2008) (mouse actions), Alabau et al. (2011) (hand-writing) and Cubel et al. (2009) (speech). 2 Neural Machine Translation The use of neural network methods in machine tra"
2016.amta-researchers.9,W00-0507,0,0.138164,"hine translation system. The system attempts to match the partial translator input (called the preﬁx) to the search graph, using approximate string matching techniques (minimal string edit distance) when an exact match cannot be found. As a baseline, we use a statistical machine translation system for interactive translation prediction that closely follows Koehn (2009) and Koehn et al. (2014). The preﬁx could be matched by constraint decoding, however, at a much higher computational cost. Initial work on interactive translation prediction can be found in the TransType and TransType2 projects (Langlais et al., 2000; Foster et al., 2002; Bender et al., 2005; Barrachina et al., 2009). Our current work focuses on how to produce suggestions; for various approaches to interaction modalities, see Sanchis-Trilles et al. (2008) (mouse actions), Alabau et al. (2011) (hand-writing) and Cubel et al. (2009) (speech). 2 Neural Machine Translation The use of neural network methods in machine translation has followed their recent success in computer vision and automatic speech recognition. Motivations for their use include better generalization of the statistical evidence (such as the use of word embeddings that have"
2016.amta-researchers.9,E03-1032,0,0.084074,"Missing"
2016.amta-researchers.9,D08-1051,0,0.0473657,"Missing"
2016.amta-researchers.9,W16-2323,0,0.201155,"system recomputes its prediction for how to complete the sentence from the given preﬁx and presents the corrected version to the translator. In prior work, phrase-based machine translation systems have been used for interactive translation prediction, and suggestions were made either by re-decoding constrained by the preﬁx (Green et al., 2014) or by searching for the preﬁx in the original search graph (Och et al., 2003; Barrachina et al., 2009). Recently, neural translation models have been proposed and in some cases have shown superior performance over phrase-based models (Jean et al., 2015; Sennrich et al., 2016). We propose to use such models for interactive translation prediction. Parallel to this work, Wuebker et al. (2016) also explore a similar approach to using neural MT for interactive translation prediction. The decoding mechanism for neural models provides a natural way of doing interactive translation prediction. We show that neural translation models can provide better translation prediction quality and improved recovery from rejected suggestions. We also develop efﬁcient methods that enable neural models to meet the speed requirements of live interactive translation prediction systems. 1 I"
2016.amta-researchers.9,P16-1007,0,0.328327,"Missing"
2016.amta-researchers.9,P94-1019,0,\N,Missing
2016.amta-researchers.9,P16-1162,0,\N,Missing
2016.amta-researchers.9,P16-1009,0,\N,Missing
2020.aacl-main.58,D18-1337,0,0.0383681,"University phi@jhu.edu 2 Introduction Simultaneous speech translation (SimulST) generates a translation from an input speech utterance before the end of the utterance has been heard. SimulST systems aim at generating translations with maximum quality and minimum latency, targeting applications such as video caption translations and real-time language interpreter. While great progress has recently been achieved on both end-to-end speech translation (Ansari et al., 2020) and simultaneous text translation (SimulMT) (Grissom II et al., 2014; Gu et al., 2017; Luo et al., 2017; Lawson et al., 2018; Alinejad et al., 2018; Zheng et al., 2019b,a; Ma et al., 2020; Arivazhagan et al., 2019, 2020), little work has combined the two tasks together (Ren et al., 2020). End-to-end SimulST models feature a smaller model size, greater inference speed and fewer compounding errors compared to their cascade counterpart, which perform streaming speech recognition followed by simultaneous machine translation. In addition, it has been demonstrated that end-to-end SimulST systems can have lower latency than cascade systems (Ren et al., 2020). 1 The code is available at https://github.com/ pytorch/fairseq Task formalization A Si"
2020.aacl-main.58,P19-1126,0,0.21791,"Missing"
2020.aacl-main.58,2020.iwslt-1.27,0,0.330728,"Missing"
2020.aacl-main.58,N19-1202,0,0.0769409,"Missing"
2020.aacl-main.58,W19-6603,0,0.0302379,"Missing"
2020.aacl-main.58,D14-1140,0,0.17381,"Missing"
2020.aacl-main.58,E17-1099,0,0.223207,"d from Average Lagging. 1 1 Philipp Koehn Johns Hopkins University phi@jhu.edu 2 Introduction Simultaneous speech translation (SimulST) generates a translation from an input speech utterance before the end of the utterance has been heard. SimulST systems aim at generating translations with maximum quality and minimum latency, targeting applications such as video caption translations and real-time language interpreter. While great progress has recently been achieved on both end-to-end speech translation (Ansari et al., 2020) and simultaneous text translation (SimulMT) (Grissom II et al., 2014; Gu et al., 2017; Luo et al., 2017; Lawson et al., 2018; Alinejad et al., 2018; Zheng et al., 2019b,a; Ma et al., 2020; Arivazhagan et al., 2019, 2020), little work has combined the two tasks together (Ren et al., 2020). End-to-end SimulST models feature a smaller model size, greater inference speed and fewer compounding errors compared to their cascade counterpart, which perform streaming speech recognition followed by simultaneous machine translation. In addition, it has been demonstrated that end-to-end SimulST systems can have lower latency than cascade systems (Ren et al., 2020). 1 The code is available"
2020.aacl-main.58,D18-2012,0,0.0286576,"honeme). A(hi ) represents the token that hi aligns to. The trigger probability can then be defined in Eq. (3): ( 0 if A(hj ) = A(hj−1 ) ptr (j) = (3) 1 Otherwise. 4 Experiments We conduct experiments on the English-German portion of the MuST-C dataset (Di Gangi et al., 2019a), where source audio, source transcript and target translation are available. We train on 408 hours of speech and 234k sentences of text data. We use Kaldi (Povey et al., 2011) to extract 80 dimensional log-mel filter bank features, computed with a 25ms window size and a 10ms window shift. For text, we use SentencePiece (Kudo and Richardson, 2018) to generate a unigram vocabulary of size 10,000. We use Gentle2 to generate the alignment between source text and speech as the label to generate the oracle flexible predecision module. Translation quality is evaluated with case-sensitive detokenized BLEU with S ACRE BLEU (Post, 2018). The latency is evaluated with our proposed modification of AL (Ma et al., 2019). All results are reported on the MuSTC dev set. All speech translation models are first pretrained on the ASR task where the target vocabulary is character-based, in order to initialize the 2 encoder. We follow the same hyperparamet"
2020.aacl-main.58,P02-1040,0,0.109618,"e code is available at https://github.com/ pytorch/fairseq Task formalization A SimulST model takes as input a sequence of acoustic features X = [x1 , ...x|X |] extracted from speech samples every Ts ms, and generates a sequence of text tokens Y = [y1 , ..., y|Y |] in a target language. Additionally, it is able to generate yi with only partial input X1:n(yi ) = [x1 , ...xn(yi ) ], where n(yi ) ≤ |X |is the number of frames needed to generate the i-th target token yi . Note that n is a monotonic function, i.e. n(yi−1 ) ≤ n(yi ). A SimulST model is evaluated with respect to quality, using BLEU (Papineni et al., 2002), and latency. We introduce two latency evaluation methods for SimulST that are adapted from SimulMT. We first define two types of delays to generate the word yi , a computation-aware (CA) and a non computation-aware (NCA) delay. The CA delay of yi , dCA (yi ), is defined as the time that elapses (speech duration) from the beginning of the process to the prediction of yi , while the NCA delay for yi dCA (yi ) is defined by dNCA (yi ) = Ts · n(yi ). Note that dNCA is an ideal case for dCA where the computational time for the model is ignored. Both delays are measured in milliseconds. Two types"
2020.aacl-main.58,W18-6319,0,0.0118891,"urce transcript and target translation are available. We train on 408 hours of speech and 234k sentences of text data. We use Kaldi (Povey et al., 2011) to extract 80 dimensional log-mel filter bank features, computed with a 25ms window size and a 10ms window shift. For text, we use SentencePiece (Kudo and Richardson, 2018) to generate a unigram vocabulary of size 10,000. We use Gentle2 to generate the alignment between source text and speech as the label to generate the oracle flexible predecision module. Translation quality is evaluated with case-sensitive detokenized BLEU with S ACRE BLEU (Post, 2018). The latency is evaluated with our proposed modification of AL (Ma et al., 2019). All results are reported on the MuSTC dev set. All speech translation models are first pretrained on the ASR task where the target vocabulary is character-based, in order to initialize the 2 encoder. We follow the same hyperparameter settings from (Di Gangi et al., 2019b). We follow the latency regularization method introduced by (Ma et al., 2020; Arivazhagan et al., 2019), The objective function to optimize is L = −log (P (Y |X)) + λmax (C(D), 0) (4) Where C is a latency metric (AL in this case) and D is descri"
2020.aacl-main.58,2020.acl-main.350,0,0.290478,"end of the utterance has been heard. SimulST systems aim at generating translations with maximum quality and minimum latency, targeting applications such as video caption translations and real-time language interpreter. While great progress has recently been achieved on both end-to-end speech translation (Ansari et al., 2020) and simultaneous text translation (SimulMT) (Grissom II et al., 2014; Gu et al., 2017; Luo et al., 2017; Lawson et al., 2018; Alinejad et al., 2018; Zheng et al., 2019b,a; Ma et al., 2020; Arivazhagan et al., 2019, 2020), little work has combined the two tasks together (Ren et al., 2020). End-to-end SimulST models feature a smaller model size, greater inference speed and fewer compounding errors compared to their cascade counterpart, which perform streaming speech recognition followed by simultaneous machine translation. In addition, it has been demonstrated that end-to-end SimulST systems can have lower latency than cascade systems (Ren et al., 2020). 1 The code is available at https://github.com/ pytorch/fairseq Task formalization A SimulST model takes as input a sequence of acoustic features X = [x1 , ...x|X |] extracted from speech samples every Ts ms, and generates a seq"
2020.aacl-main.58,D19-1137,0,0.291859,"2 Introduction Simultaneous speech translation (SimulST) generates a translation from an input speech utterance before the end of the utterance has been heard. SimulST systems aim at generating translations with maximum quality and minimum latency, targeting applications such as video caption translations and real-time language interpreter. While great progress has recently been achieved on both end-to-end speech translation (Ansari et al., 2020) and simultaneous text translation (SimulMT) (Grissom II et al., 2014; Gu et al., 2017; Luo et al., 2017; Lawson et al., 2018; Alinejad et al., 2018; Zheng et al., 2019b,a; Ma et al., 2020; Arivazhagan et al., 2019, 2020), little work has combined the two tasks together (Ren et al., 2020). End-to-end SimulST models feature a smaller model size, greater inference speed and fewer compounding errors compared to their cascade counterpart, which perform streaming speech recognition followed by simultaneous machine translation. In addition, it has been demonstrated that end-to-end SimulST systems can have lower latency than cascade systems (Ren et al., 2020). 1 The code is available at https://github.com/ pytorch/fairseq Task formalization A SimulST model takes as"
2020.aacl-main.58,P19-1582,0,0.11644,"2 Introduction Simultaneous speech translation (SimulST) generates a translation from an input speech utterance before the end of the utterance has been heard. SimulST systems aim at generating translations with maximum quality and minimum latency, targeting applications such as video caption translations and real-time language interpreter. While great progress has recently been achieved on both end-to-end speech translation (Ansari et al., 2020) and simultaneous text translation (SimulMT) (Grissom II et al., 2014; Gu et al., 2017; Luo et al., 2017; Lawson et al., 2018; Alinejad et al., 2018; Zheng et al., 2019b,a; Ma et al., 2020; Arivazhagan et al., 2019, 2020), little work has combined the two tasks together (Ren et al., 2020). End-to-end SimulST models feature a smaller model size, greater inference speed and fewer compounding errors compared to their cascade counterpart, which perform streaming speech recognition followed by simultaneous machine translation. In addition, it has been demonstrated that end-to-end SimulST systems can have lower latency than cascade systems (Ren et al., 2020). 1 The code is available at https://github.com/ pytorch/fairseq Task formalization A SimulST model takes as"
2020.acl-main.417,bojar-etal-2012-joy,0,0.0615774,"Missing"
2020.acl-main.417,P91-1022,0,0.752852,"ctors may be recentered around the mean vector for a web domain (Germann, 2016) Document alignment quality can be improved with additional features such ratio of shared links, similarity of link URLs, ratio of shared images, binary feature indicating if the documents are linked, DOM structure similarity (Espl`a-Gomis et al., 2016), same numbers (Papavassiliou et al., 2016), or same named entities (Lohar et al., 2016). Guo et al. (2019) introduce the use of document embeddings, constructed from sentence embeddings, to the document alignment task. 2.3 Sentence Alignment Early sentence aligners (Brown et al., 1991; Gale and Church, 1993) use scoring functions based only on the number of words or characters in each sentence and alignment algorithms based on dynamic programming. Europarl, for example, used metadata to align paragraphs, typically consisting of 2-5 sentences, and using Gale and Church (1993)’s method to align sentences within corresponding paragraphs. Later work added lexical features and heuristics to speed up search, such as limiting the search space to be near the diagonal (Moore, 2002; Varga et al., 2005). More recent work introduced scoring methods that use MT to get both documents in"
2020.acl-main.417,buck-etal-2014-n,1,0.887657,"Missing"
2020.acl-main.417,W16-2347,1,0.931151,"set of patterns for language marking or simple Levenshtein distance (Le et al., 2016). Content matching requires crossing the language barrier at some point, typically by using bilingual dictionaries or translating one of the documents into the other document’s language (Uszkoreit et al., 2010). Documents may be represented by vectors over word frequencies, typically td-idf-weighted. Vectors may also be constructed over bigrams (Dara and Lin, 2016) or even higher order n-grams 4 http://opus.lingfil.uu.se/ (Uszkoreit et al., 2010). The vectors are then typically matched with cosine similarity (Buck and Koehn, 2016a). The raw vectors may be recentered around the mean vector for a web domain (Germann, 2016) Document alignment quality can be improved with additional features such ratio of shared links, similarity of link URLs, ratio of shared images, binary feature indicating if the documents are linked, DOM structure similarity (Espl`a-Gomis et al., 2016), same numbers (Papavassiliou et al., 2016), or same named entities (Lohar et al., 2016). Guo et al. (2019) introduce the use of document embeddings, constructed from sentence embeddings, to the document alignment task. 2.3 Sentence Alignment Early sente"
2020.acl-main.417,W11-1218,0,0.0324271,"Missing"
2020.acl-main.417,W16-2365,1,0.928849,"set of patterns for language marking or simple Levenshtein distance (Le et al., 2016). Content matching requires crossing the language barrier at some point, typically by using bilingual dictionaries or translating one of the documents into the other document’s language (Uszkoreit et al., 2010). Documents may be represented by vectors over word frequencies, typically td-idf-weighted. Vectors may also be constructed over bigrams (Dara and Lin, 2016) or even higher order n-grams 4 http://opus.lingfil.uu.se/ (Uszkoreit et al., 2010). The vectors are then typically matched with cosine similarity (Buck and Koehn, 2016a). The raw vectors may be recentered around the mean vector for a web domain (Germann, 2016) Document alignment quality can be improved with additional features such ratio of shared links, similarity of link URLs, ratio of shared images, binary feature indicating if the documents are linked, DOM structure similarity (Espl`a-Gomis et al., 2016), same numbers (Papavassiliou et al., 2016), or same named entities (Lohar et al., 2016). Guo et al. (2019) introduce the use of document embeddings, constructed from sentence embeddings, to the document alignment task. 2.3 Sentence Alignment Early sente"
2020.acl-main.417,2012.eamt-1.60,0,0.0178318,"2.1 Acquisition Efforts Most publicly available parallel corpora are the result of targeted efforts to extract the translations from a specific source. The French–English Canadian Hansards3 were used in the earliest work on statistical machine translation. A similar popular corpus is Europarl (Koehn, 2005), used throughout the WMT evaluation campaign. Multi-lingual web sites are attractive targets. Rafalovitch and Dale (2009); Ziemski et al. (2015) extract data from the United Nations, T¨ager (2011) from European Patents, Lison and Tiedemann (2016) from a collection of TV and movie subtitles. Cettolo et al. (2012) explain the creation of a multilingual parallel corpus of subtitles from the TED Talks website which is popular due to its use in the IWSLT evaluation campaign. 3 https://www.isi.edu/natural-language/ download/hansard/ 4555 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4555–4567 c July 5 - 10, 2020. 2020 Association for Computational Linguistics There are also various efforts targeted at a single language pair. Martin et al. (2003) build a parallel corpus for Inuktitut–English. Utiyama and Isahara (2003); Fukushima et al. (2006) worked on creat"
2020.acl-main.417,P05-1074,0,0.166175,"s Translation. We report on methods to create the largest publicly available parallel corpora by crawling the web, using open source software. We empirically compare alternative methods and publish benchmark data sets for sentence alignment and sentence pair filtering. We also describe the parallel corpora released and evaluate their quality and their usefulness to create machine translation systems. 1 2 Introduction Parallel corpora are essential for building highquality machine translation systems and have found uses in many other natural language applications, such as learning paraphrases (Bannard and Callison-Burch, 2005; Hu et al., 2019) or cross-lingual projection of language tools (Yarowsky et al., 2001). We report on work to create the largest publicly available parallel corpora by crawling hundreds of thousands of web sites, using open source tools. The processing pipeline consists of the steps: crawling, text extraction, document alignment, sentence alignment, and sentence pair filtering. We describe these steps in detail in Sections 4–8. For some of these steps we evaluate several methods empirically in terms of their impact on machine translation quality. We provide the data resources used in these ev"
2020.acl-main.417,W19-5435,1,0.894745,"Missing"
2020.acl-main.417,2005.mtsummit-papers.11,1,0.214853,"modular pipeline that allows harvesting parallel corpora from multilingual websites or from preexisting or historical web crawls such as the one available as part of the Internet Archive.2 1 2 https://github.com/bitextor/bitextor https://archive.org/ Related Work While the idea of mining the web for parallel data has been already pursued in the 20th century (Resnik, 1999), the most serious efforts have been limited to large companies such as Google (Uszkoreit et al., 2010) and Microsoft (Rarrick et al., 2011), or targeted efforts on specific domains such as the Canadian Hansards and Europarl (Koehn, 2005). The book Bitext Alignment (Tiedemann, 2011) describes some of the challenges in greater detail. 2.1 Acquisition Efforts Most publicly available parallel corpora are the result of targeted efforts to extract the translations from a specific source. The French–English Canadian Hansards3 were used in the earliest work on statistical machine translation. A similar popular corpus is Europarl (Koehn, 2005), used throughout the WMT evaluation campaign. Multi-lingual web sites are attractive targets. Rafalovitch and Dale (2009); Ziemski et al. (2015) extract data from the United Nations, T¨ager (201"
2020.acl-main.417,W19-5404,1,0.891381,"Missing"
2020.acl-main.417,W18-6453,1,0.89611,"Missing"
2020.acl-main.417,W19-5438,0,0.038302,"Missing"
2020.acl-main.417,W16-2371,0,0.0501587,"Missing"
2020.acl-main.417,I08-2120,0,0.0456513,"age/ download/hansard/ 4555 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4555–4567 c July 5 - 10, 2020. 2020 Association for Computational Linguistics There are also various efforts targeted at a single language pair. Martin et al. (2003) build a parallel corpus for Inuktitut–English. Utiyama and Isahara (2003); Fukushima et al. (2006) worked on creating Japanese–English corpora. Uchiyama and Isahara (2007) report on the efforts to build a Japanese–English patent corpus and Macken et al. (2007) on efforts on a broad-based Dutch–English corpus. Li and Liu (2008) mine the web for a Chinese–English corpus. A large Czech–English corpus from various sources was collected (Bojar et al., 2010), linguistically annotated (Bojar et al., 2012), and has been continuously extended to over 300 million words (Bojar et al., 2016). All these efforts rely on methods and implementations that are quite specific for each use case, not documented in great detail, and not publicly available. A discussion of the pitfalls during the construction of parallel corpora is given by Kaalep and Veskis (2007). A large collection of corpora is maintained at the OPUS web site4 (Tiede"
2020.acl-main.417,L16-1147,0,0.0317824,"t (Tiedemann, 2011) describes some of the challenges in greater detail. 2.1 Acquisition Efforts Most publicly available parallel corpora are the result of targeted efforts to extract the translations from a specific source. The French–English Canadian Hansards3 were used in the earliest work on statistical machine translation. A similar popular corpus is Europarl (Koehn, 2005), used throughout the WMT evaluation campaign. Multi-lingual web sites are attractive targets. Rafalovitch and Dale (2009); Ziemski et al. (2015) extract data from the United Nations, T¨ager (2011) from European Patents, Lison and Tiedemann (2016) from a collection of TV and movie subtitles. Cettolo et al. (2012) explain the creation of a multilingual parallel corpus of subtitles from the TED Talks website which is popular due to its use in the IWSLT evaluation campaign. 3 https://www.isi.edu/natural-language/ download/hansard/ 4555 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4555–4567 c July 5 - 10, 2020. 2020 Association for Computational Linguistics There are also various efforts targeted at a single language pair. Martin et al. (2003) build a parallel corpus for Inuktitut–English."
2020.acl-main.417,W16-2372,0,0.0140892,"ra and Lin, 2016) or even higher order n-grams 4 http://opus.lingfil.uu.se/ (Uszkoreit et al., 2010). The vectors are then typically matched with cosine similarity (Buck and Koehn, 2016a). The raw vectors may be recentered around the mean vector for a web domain (Germann, 2016) Document alignment quality can be improved with additional features such ratio of shared links, similarity of link URLs, ratio of shared images, binary feature indicating if the documents are linked, DOM structure similarity (Espl`a-Gomis et al., 2016), same numbers (Papavassiliou et al., 2016), or same named entities (Lohar et al., 2016). Guo et al. (2019) introduce the use of document embeddings, constructed from sentence embeddings, to the document alignment task. 2.3 Sentence Alignment Early sentence aligners (Brown et al., 1991; Gale and Church, 1993) use scoring functions based only on the number of words or characters in each sentence and alignment algorithms based on dynamic programming. Europarl, for example, used metadata to align paragraphs, typically consisting of 2-5 sentences, and using Gale and Church (1993)’s method to align sentences within corresponding paragraphs. Later work added lexical features and heuris"
2020.acl-main.417,2007.mtsummit-papers.42,0,0.0236436,"e in the IWSLT evaluation campaign. 3 https://www.isi.edu/natural-language/ download/hansard/ 4555 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4555–4567 c July 5 - 10, 2020. 2020 Association for Computational Linguistics There are also various efforts targeted at a single language pair. Martin et al. (2003) build a parallel corpus for Inuktitut–English. Utiyama and Isahara (2003); Fukushima et al. (2006) worked on creating Japanese–English corpora. Uchiyama and Isahara (2007) report on the efforts to build a Japanese–English patent corpus and Macken et al. (2007) on efforts on a broad-based Dutch–English corpus. Li and Liu (2008) mine the web for a Chinese–English corpus. A large Czech–English corpus from various sources was collected (Bojar et al., 2010), linguistically annotated (Bojar et al., 2012), and has been continuously extended to over 300 million words (Bojar et al., 2016). All these efforts rely on methods and implementations that are quite specific for each use case, not documented in great detail, and not publicly available. A discussion of the pitfalls during the construction of parallel corpora is given by Kaalep and Veskis (2007). A la"
2020.acl-main.417,W03-0320,0,0.154143,"ons, T¨ager (2011) from European Patents, Lison and Tiedemann (2016) from a collection of TV and movie subtitles. Cettolo et al. (2012) explain the creation of a multilingual parallel corpus of subtitles from the TED Talks website which is popular due to its use in the IWSLT evaluation campaign. 3 https://www.isi.edu/natural-language/ download/hansard/ 4555 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4555–4567 c July 5 - 10, 2020. 2020 Association for Computational Linguistics There are also various efforts targeted at a single language pair. Martin et al. (2003) build a parallel corpus for Inuktitut–English. Utiyama and Isahara (2003); Fukushima et al. (2006) worked on creating Japanese–English corpora. Uchiyama and Isahara (2007) report on the efforts to build a Japanese–English patent corpus and Macken et al. (2007) on efforts on a broad-based Dutch–English corpus. Li and Liu (2008) mine the web for a Chinese–English corpus. A large Czech–English corpus from various sources was collected (Bojar et al., 2010), linguistically annotated (Bojar et al., 2012), and has been continuously extended to over 300 million words (Bojar et al., 2016). All these e"
2020.acl-main.417,moore-2002-fast,0,0.25649,"nce embeddings, to the document alignment task. 2.3 Sentence Alignment Early sentence aligners (Brown et al., 1991; Gale and Church, 1993) use scoring functions based only on the number of words or characters in each sentence and alignment algorithms based on dynamic programming. Europarl, for example, used metadata to align paragraphs, typically consisting of 2-5 sentences, and using Gale and Church (1993)’s method to align sentences within corresponding paragraphs. Later work added lexical features and heuristics to speed up search, such as limiting the search space to be near the diagonal (Moore, 2002; Varga et al., 2005). More recent work introduced scoring methods that use MT to get both documents into the same language (Sennrich and Volk, 2010) or use pruned phrase tables from a statistical MT system (Gomes and Lopes, 2016). Both methods “anchor” highprobability 1–1 alignments in the search space and then fill in and refine alignments. They later propose an extension (Sennrich and Volk, 2011) in which an SMT system is bootstrapped from an initial alignment and then used in Bleualign. Vecalign (Thompson and Koehn, 2019) is a sentence alignment method that relies on bilingual sentence emb"
2020.acl-main.417,J05-4003,0,0.157471,"Our work exploits web sites that provide roughly the same content in multiple languages, leading us to the assumption to find pairs of web pages which are translations of each other, with translated sentences following the same order. This assumption does not hold in less consistently translated web content such as Wikipedia, or accidental parallel sentence found in news stories about the same subject matter written in multiple languages. There have been increasing efforts to mine sentence pairs from large pools of multi-lingual text, which are treated as unstructured bags of sen4557 tences. Munteanu and Marcu (2005) use document retrieval and a maximum entropy classifier to identify parallel sentence pairs in a multi-lingual collection of news stories. Bilingual sentence embeddings (Guo et al., 2018) and multilingual sentence embeddings (Artetxe and Schwenk, 2018) were tested on their ability to reconstruct parallel corpora. This lead to work to construct WikiMatrix, a large corpus of parallel sentences from Wikipedia (Schwenk et al., 2019) based on cosine distance of their crosslingual sentence embeddings. 3 Identifying Multi-Lingual Web Sites Since the start of the collection effort in 2015, we identif"
2020.amta-user.3,W17-3204,1,0.543953,"Missing"
2020.amta-user.3,2020.eamt-1.1,0,0.0332059,"Missing"
2020.emnlp-main.480,E09-1003,0,0.0758881,"vitch et al., 2009; Ziemski et al., 2016) or European Parliament parallel corpus (Koehn, 2005). These parallel corpora were curated from specific, homogeneous sources by examining the content and deriving domainspecific rules for aligning documents. Other approaches have identified parallel documents in unstructured web corpora by relying on metadata (Nie et al., 1999; Espla-Gomis and Forcada, 2010). Some of these methods have focused on publication date and other temporal heuristics to aid in identifying parallel documents (Munteanu and Marcu, 2005, 2006; Udupa et al., 2009; Do et al., 2009; Abdul-Rauf and Schwenk, 2009). However, temporal features can be sparse, noisy, and unreliable. A different class of alignment methods rely on document structure (Resnik and Smith, 2003; Chen and Nie, 2000). In the WMT-2016 bilingual document alignment shared task (Buck and Koehn, 2016a), many techniques applied retrieval and matching on translated 5-grams (Dara and Lin, 2016) to query, retrieve, and align documents. Similar methods for generating candidates by retrieving matches based on the least frequent bi-lingual 5grams have been proposed (Gomes and Lopes, 2016) with the insight that rare snippets are more informativ"
2020.emnlp-main.480,Q19-1038,0,0.0447604,"st mapping according to φ(ds , dt ). In the remainder of this section, we introduce our proposed baseline document pair similarity functions and a simple matching algorithm that aligns source and target documents. 5.2 Embedding-Based Document Similarity To guide the alignment algorithm, a notion of cross-lingual document similarity is necessary. This score should capture the fact that two documents are semantically similar despite having some or all of their content in different languages. We describe three simple language-agnostic document embedding methods. These embeddings leverage LASER1 (Artetxe and Schwenk, 2019), a multilingual sentence representation that uses byte-pair encoding to share the same vocabulary among all languages and trained on parallel sentences pulled from Europarl, United Nations, OpenSubtitles2018, Global Voices, Tanzil and Tatoeba corpus, covering 93 languages. Direct Embedding The first baseline, Direct Embedding (DE) uses a standard cross-lingual encoder to directly embed each document. Each document d has its dense vector representation 5963 1 https://github.com/facebookresearch/LASER vd computed by applying the open-source crosslingual LASER encoder to its full textual content"
2020.emnlp-main.480,buck-etal-2014-n,0,0.1104,"Missing"
2020.emnlp-main.480,W16-2347,1,0.957917,"have identified parallel documents in unstructured web corpora by relying on metadata (Nie et al., 1999; Espla-Gomis and Forcada, 2010). Some of these methods have focused on publication date and other temporal heuristics to aid in identifying parallel documents (Munteanu and Marcu, 2005, 2006; Udupa et al., 2009; Do et al., 2009; Abdul-Rauf and Schwenk, 2009). However, temporal features can be sparse, noisy, and unreliable. A different class of alignment methods rely on document structure (Resnik and Smith, 2003; Chen and Nie, 2000). In the WMT-2016 bilingual document alignment shared task (Buck and Koehn, 2016a), many techniques applied retrieval and matching on translated 5-grams (Dara and Lin, 2016) to query, retrieve, and align documents. Similar methods for generating candidates by retrieving matches based on the least frequent bi-lingual 5grams have been proposed (Gomes and Lopes, 2016) with the insight that rare snippets are more informative. Both of these candidates rely on high-quality translation systems to translate either the source or the target. Such models may not exist, especially for low-resource language directions. The application of alignment to a variety of languages was not exp"
2020.emnlp-main.480,W16-2365,1,0.931521,"have identified parallel documents in unstructured web corpora by relying on metadata (Nie et al., 1999; Espla-Gomis and Forcada, 2010). Some of these methods have focused on publication date and other temporal heuristics to aid in identifying parallel documents (Munteanu and Marcu, 2005, 2006; Udupa et al., 2009; Do et al., 2009; Abdul-Rauf and Schwenk, 2009). However, temporal features can be sparse, noisy, and unreliable. A different class of alignment methods rely on document structure (Resnik and Smith, 2003; Chen and Nie, 2000). In the WMT-2016 bilingual document alignment shared task (Buck and Koehn, 2016a), many techniques applied retrieval and matching on translated 5-grams (Dara and Lin, 2016) to query, retrieve, and align documents. Similar methods for generating candidates by retrieving matches based on the least frequent bi-lingual 5grams have been proposed (Gomes and Lopes, 2016) with the insight that rare snippets are more informative. Both of these candidates rely on high-quality translation systems to translate either the source or the target. Such models may not exist, especially for low-resource language directions. The application of alignment to a variety of languages was not exp"
2020.emnlp-main.480,W19-5435,1,0.862614,", we segment each document solely on new lines. Given each document pair’s decomposition into sentences, we seek 2 https://paracrawl.eu/ to align sentences within each pair of documents. We then aggregate the parallel sentences across all document pairs to form a parallel sentences dataset suitable for training machine translation models. We use the open-source LASER toolkit (Schwenk, 2018) with the marginbased filtering criterion to mine sentences, as this method has been shown to accurately align sentences for across a variety of low, mid, and high-resource directions (Schwenk et al., 2019; Chaudhary et al., 2019). We use the extracted bitexts for training our NMT systems. Experimental setup First the data is processed to induce a 5000 subword vocabulary using SentencePiece (Kudo and Richardson, 2018). The model used is a transformer model from fairseq (Ott et al., 2019) with embeddings shared in the encoder and decoder, 5 encoder and decoder layers with dimensionality 512 are used, encoder and decoder FFN with 2 attention heads each with an embedding dimension of 2048 are used along with encoder and decoder normalization. Dropout of 0.4, attention dropout of 0.2 and relu dropout of 0.2 are applied. Th"
2020.emnlp-main.480,W16-2366,0,0.0275662,"al., 1999; Espla-Gomis and Forcada, 2010). Some of these methods have focused on publication date and other temporal heuristics to aid in identifying parallel documents (Munteanu and Marcu, 2005, 2006; Udupa et al., 2009; Do et al., 2009; Abdul-Rauf and Schwenk, 2009). However, temporal features can be sparse, noisy, and unreliable. A different class of alignment methods rely on document structure (Resnik and Smith, 2003; Chen and Nie, 2000). In the WMT-2016 bilingual document alignment shared task (Buck and Koehn, 2016a), many techniques applied retrieval and matching on translated 5-grams (Dara and Lin, 2016) to query, retrieve, and align documents. Similar methods for generating candidates by retrieving matches based on the least frequent bi-lingual 5grams have been proposed (Gomes and Lopes, 2016) with the insight that rare snippets are more informative. Both of these candidates rely on high-quality translation systems to translate either the source or the target. Such models may not exist, especially for low-resource language directions. The application of alignment to a variety of languages was not explored in WMT-2016 Recently, the use of neural embedding methods has been explored for bilingu"
2020.emnlp-main.480,N19-1423,0,0.0162251,"ages. 1 Introduction Cross-lingual document alignment aims to pair documents such that they are translations or near translations of each other. There are a variety of tasks in natural language processing that consume such parallel cross-lingual data. Traditionally, machine translation approaches have leveraged parallel sentences as training data for use with sequence-to-sequence models. Other tasks include cross-lingual information retrieval and cross-lingual document classification. Additionally, cross-lingual data facilitates training crosslingual representations such as multilingual BERT (Devlin et al., 2019) and XLM (Lample and Conneau, 2019) which are used in many NLP tasks. The availability of high-quality datasets is necessary to both train and evaluate models across these many tasks. While it is possible to manually label aligned documents across languages, the process is costly and time consuming due to the quadratic search space for document pairs. Additionally, for low-resource languages, identifying these crosslingual document pairs is more difficult due to their relative scarcity. Furthermore, lack of access to qualified human annotators makes it necessary to have additional quality cont"
2020.emnlp-main.480,W09-0430,0,0.024085,"solutions (Rafalovitch et al., 2009; Ziemski et al., 2016) or European Parliament parallel corpus (Koehn, 2005). These parallel corpora were curated from specific, homogeneous sources by examining the content and deriving domainspecific rules for aligning documents. Other approaches have identified parallel documents in unstructured web corpora by relying on metadata (Nie et al., 1999; Espla-Gomis and Forcada, 2010). Some of these methods have focused on publication date and other temporal heuristics to aid in identifying parallel documents (Munteanu and Marcu, 2005, 2006; Udupa et al., 2009; Do et al., 2009; Abdul-Rauf and Schwenk, 2009). However, temporal features can be sparse, noisy, and unreliable. A different class of alignment methods rely on document structure (Resnik and Smith, 2003; Chen and Nie, 2000). In the WMT-2016 bilingual document alignment shared task (Buck and Koehn, 2016a), many techniques applied retrieval and matching on translated 5-grams (Dara and Lin, 2016) to query, retrieve, and align documents. Similar methods for generating candidates by retrieving matches based on the least frequent bi-lingual 5grams have been proposed (Gomes and Lopes, 2016) with the insight that ra"
2020.emnlp-main.480,W19-6721,0,0.0811263,"Missing"
2020.emnlp-main.480,W16-2369,0,0.0484401,"2005, 2006; Udupa et al., 2009; Do et al., 2009; Abdul-Rauf and Schwenk, 2009). However, temporal features can be sparse, noisy, and unreliable. A different class of alignment methods rely on document structure (Resnik and Smith, 2003; Chen and Nie, 2000). In the WMT-2016 bilingual document alignment shared task (Buck and Koehn, 2016a), many techniques applied retrieval and matching on translated 5-grams (Dara and Lin, 2016) to query, retrieve, and align documents. Similar methods for generating candidates by retrieving matches based on the least frequent bi-lingual 5grams have been proposed (Gomes and Lopes, 2016) with the insight that rare snippets are more informative. Both of these candidates rely on high-quality translation systems to translate either the source or the target. Such models may not exist, especially for low-resource language directions. The application of alignment to a variety of languages was not explored in WMT-2016 Recently, the use of neural embedding methods has been explored for bilingual alignment of text at the sentence and document level. Guo et al. (2019) propose using hierarchical document embeddings, constructed from sentence embeddings, for bilingual document alignment."
2020.emnlp-main.480,W19-5207,0,0.0135317,"for generating candidates by retrieving matches based on the least frequent bi-lingual 5grams have been proposed (Gomes and Lopes, 2016) with the insight that rare snippets are more informative. Both of these candidates rely on high-quality translation systems to translate either the source or the target. Such models may not exist, especially for low-resource language directions. The application of alignment to a variety of languages was not explored in WMT-2016 Recently, the use of neural embedding methods has been explored for bilingual alignment of text at the sentence and document level. Guo et al. (2019) propose using hierarchical document embeddings, constructed from sentence embeddings, for bilingual document alignment. Dataset Creation and Description The Common Crawl corpus is a publicly available crawl of the web. With a new snapshot uploaded each month, and over 2 billion pages released in each snapshot, this data is a vast resource with content across a large number of domains and languages. Previous works have leveraged the data from Common Crawl for mining ngram counts to perform language modeling (Buck et al., 2014). Other works (Smith et al., 2013) have mined Common Crawl for bitex"
2020.emnlp-main.480,D19-1632,1,0.863754,"019) which are used in many NLP tasks. The availability of high-quality datasets is necessary to both train and evaluate models across these many tasks. While it is possible to manually label aligned documents across languages, the process is costly and time consuming due to the quadratic search space for document pairs. Additionally, for low-resource languages, identifying these crosslingual document pairs is more difficult due to their relative scarcity. Furthermore, lack of access to qualified human annotators makes it necessary to have additional quality control in low-resource scenarios (Guzmán et al., 2019). In this paper, we investigate whether we can rely on weak supervision to generate labels for document pairs. In particular, we focus on the weak signals embedded in the URLs of web documents, that can be used to identify the different translations of a single document across many languages. We preprocess, filter, and apply a set of high-precision hand-crafted rules to automatically sift through a massive collection of 169 billion web documents and identify over a 392 million cross-lingual parallel documents in 8144 language pairs. Of these aligned documents, 292 million are non-English docum"
2020.emnlp-main.480,E17-2068,0,0.121936,"Missing"
2020.emnlp-main.480,2005.mtsummit-papers.11,1,0.249144,"llel corpus for mining parallel sentences and as supervision for a variety of cross-lingual tasks. which only considered English to French document alignment – a high-resource direction. 2 3 Related Works The concept of crawling and mining the web to identify sources of parallel data has been previously explored (Resnik, 1999). A large body of this work has focused on identifying parallel text from multilingual data obtained from a single source: for example the United Nations General Assembly Resolutions (Rafalovitch et al., 2009; Ziemski et al., 2016) or European Parliament parallel corpus (Koehn, 2005). These parallel corpora were curated from specific, homogeneous sources by examining the content and deriving domainspecific rules for aligning documents. Other approaches have identified parallel documents in unstructured web corpora by relying on metadata (Nie et al., 1999; Espla-Gomis and Forcada, 2010). Some of these methods have focused on publication date and other temporal heuristics to aid in identifying parallel documents (Munteanu and Marcu, 2005, 2006; Udupa et al., 2009; Do et al., 2009; Abdul-Rauf and Schwenk, 2009). However, temporal features can be sparse, noisy, and unreliable"
2020.emnlp-main.480,W19-5404,1,0.908456,"Missing"
2020.emnlp-main.480,2009.mtsummit-posters.15,0,0.468976,"Missing"
2020.emnlp-main.480,D18-2012,0,0.0296119,"e then aggregate the parallel sentences across all document pairs to form a parallel sentences dataset suitable for training machine translation models. We use the open-source LASER toolkit (Schwenk, 2018) with the marginbased filtering criterion to mine sentences, as this method has been shown to accurately align sentences for across a variety of low, mid, and high-resource directions (Schwenk et al., 2019; Chaudhary et al., 2019). We use the extracted bitexts for training our NMT systems. Experimental setup First the data is processed to induce a 5000 subword vocabulary using SentencePiece (Kudo and Richardson, 2018). The model used is a transformer model from fairseq (Ott et al., 2019) with embeddings shared in the encoder and decoder, 5 encoder and decoder layers with dimensionality 512 are used, encoder and decoder FFN with 2 attention heads each with an embedding dimension of 2048 are used along with encoder and decoder normalization. Dropout of 0.4, attention dropout of 0.2 and relu dropout of 0.2 are applied. The adam optimizer is used to train the model for up to 20 epochs by optimizing a smoothed-cross entropy with 0.2 label smoothing. We decompose the 100-million parallel documents corresponding"
2020.emnlp-main.480,P99-1068,0,0.788966,"n task. We release the dataset consisting of pairs of translated documents represented by URLs extracted from a massive collection web crawls. We hope that the size, diversity, and quality of this dataset spurs its use not only as a benchmark for document alignment, but also as a parallel corpus for mining parallel sentences and as supervision for a variety of cross-lingual tasks. which only considered English to French document alignment – a high-resource direction. 2 3 Related Works The concept of crawling and mining the web to identify sources of parallel data has been previously explored (Resnik, 1999). A large body of this work has focused on identifying parallel text from multilingual data obtained from a single source: for example the United Nations General Assembly Resolutions (Rafalovitch et al., 2009; Ziemski et al., 2016) or European Parliament parallel corpus (Koehn, 2005). These parallel corpora were curated from specific, homogeneous sources by examining the content and deriving domainspecific rules for aligning documents. Other approaches have identified parallel documents in unstructured web corpora by relying on metadata (Nie et al., 1999; Espla-Gomis and Forcada, 2010). Some o"
2020.emnlp-main.480,J03-3002,0,0.410945,"ces by examining the content and deriving domainspecific rules for aligning documents. Other approaches have identified parallel documents in unstructured web corpora by relying on metadata (Nie et al., 1999; Espla-Gomis and Forcada, 2010). Some of these methods have focused on publication date and other temporal heuristics to aid in identifying parallel documents (Munteanu and Marcu, 2005, 2006; Udupa et al., 2009; Do et al., 2009; Abdul-Rauf and Schwenk, 2009). However, temporal features can be sparse, noisy, and unreliable. A different class of alignment methods rely on document structure (Resnik and Smith, 2003; Chen and Nie, 2000). In the WMT-2016 bilingual document alignment shared task (Buck and Koehn, 2016a), many techniques applied retrieval and matching on translated 5-grams (Dara and Lin, 2016) to query, retrieve, and align documents. Similar methods for generating candidates by retrieving matches based on the least frequent bi-lingual 5grams have been proposed (Gomes and Lopes, 2016) with the insight that rare snippets are more informative. Both of these candidates rely on high-quality translation systems to translate either the source or the target. Such models may not exist, especially for"
2020.emnlp-main.480,J05-4003,0,0.293492,": for example the United Nations General Assembly Resolutions (Rafalovitch et al., 2009; Ziemski et al., 2016) or European Parliament parallel corpus (Koehn, 2005). These parallel corpora were curated from specific, homogeneous sources by examining the content and deriving domainspecific rules for aligning documents. Other approaches have identified parallel documents in unstructured web corpora by relying on metadata (Nie et al., 1999; Espla-Gomis and Forcada, 2010). Some of these methods have focused on publication date and other temporal heuristics to aid in identifying parallel documents (Munteanu and Marcu, 2005, 2006; Udupa et al., 2009; Do et al., 2009; Abdul-Rauf and Schwenk, 2009). However, temporal features can be sparse, noisy, and unreliable. A different class of alignment methods rely on document structure (Resnik and Smith, 2003; Chen and Nie, 2000). In the WMT-2016 bilingual document alignment shared task (Buck and Koehn, 2016a), many techniques applied retrieval and matching on translated 5-grams (Dara and Lin, 2016) to query, retrieve, and align documents. Similar methods for generating candidates by retrieving matches based on the least frequent bi-lingual 5grams have been proposed (Gome"
2020.emnlp-main.480,P18-2037,0,0.0544963,"et al., 2019) and ParaCrawl2 (EsplàGomis et al., 2019), reliable sources of comparable documents. Sentence mining The first step is to decompose and mine the aligned document corpus for parallel sentences. For simplicity, we segment each document solely on new lines. Given each document pair’s decomposition into sentences, we seek 2 https://paracrawl.eu/ to align sentences within each pair of documents. We then aggregate the parallel sentences across all document pairs to form a parallel sentences dataset suitable for training machine translation models. We use the open-source LASER toolkit (Schwenk, 2018) with the marginbased filtering criterion to mine sentences, as this method has been shown to accurately align sentences for across a variety of low, mid, and high-resource directions (Schwenk et al., 2019; Chaudhary et al., 2019). We use the extracted bitexts for training our NMT systems. Experimental setup First the data is processed to induce a 5000 subword vocabulary using SentencePiece (Kudo and Richardson, 2018). The model used is a transformer model from fairseq (Ott et al., 2019) with embeddings shared in the encoder and decoder, 5 encoder and decoder layers with dimensionality 512 are"
2020.emnlp-main.480,P06-1011,0,0.176617,"Missing"
2020.emnlp-main.480,N19-4009,0,0.0409614,"llel sentences dataset suitable for training machine translation models. We use the open-source LASER toolkit (Schwenk, 2018) with the marginbased filtering criterion to mine sentences, as this method has been shown to accurately align sentences for across a variety of low, mid, and high-resource directions (Schwenk et al., 2019; Chaudhary et al., 2019). We use the extracted bitexts for training our NMT systems. Experimental setup First the data is processed to induce a 5000 subword vocabulary using SentencePiece (Kudo and Richardson, 2018). The model used is a transformer model from fairseq (Ott et al., 2019) with embeddings shared in the encoder and decoder, 5 encoder and decoder layers with dimensionality 512 are used, encoder and decoder FFN with 2 attention heads each with an embedding dimension of 2048 are used along with encoder and decoder normalization. Dropout of 0.4, attention dropout of 0.2 and relu dropout of 0.2 are applied. The adam optimizer is used to train the model for up to 20 epochs by optimizing a smoothed-cross entropy with 0.2 label smoothing. We decompose the 100-million parallel documents corresponding to the 137 language pairs that include English and mine over 1B unique"
2020.emnlp-main.480,N18-2084,0,0.0321842,"with encoder and decoder normalization. Dropout of 0.4, attention dropout of 0.2 and relu dropout of 0.2 are applied. The adam optimizer is used to train the model for up to 20 epochs by optimizing a smoothed-cross entropy with 0.2 label smoothing. We decompose the 100-million parallel documents corresponding to the 137 language pairs that include English and mine over 1B unique parallel sentences after filtering. After training models for each direction, we then evaluate the quality of the learned NMT models on a publicly available data set consisting of transcribed and translated TED talks (Qi et al., 2018). Since the development and 5966 Danish Croatian Slovenian Slovak Lithuanian Estonian Language En–x x–En En–x x–En En–x x–En En–x x–En En–x x–En En–x x–En WikiMatrix ParaCrawl CCAligned 30.9 37.3 37.1 32.9 39.8 38.2 18.8 23.0 23.5 22.4 29.0 29.3 16.5 20.4 19.6 17.3 22.7 21.7 13.8 20.4 20.4 16.9 24.3 24.2 16.5 16.7 22.5 21.8 15.6 15.6 19.4 20.0 Table 4: BLEU scores of NMT models trained on bitext data mined from various web-sources including Wikipedia, ParaCrawl, and our CCAligned document set evaluated on TED Talk test sets. test sets were already tokenized, we first detokenize them using the"
2020.emnlp-main.480,P13-1135,1,0.846277,"at the sentence and document level. Guo et al. (2019) propose using hierarchical document embeddings, constructed from sentence embeddings, for bilingual document alignment. Dataset Creation and Description The Common Crawl corpus is a publicly available crawl of the web. With a new snapshot uploaded each month, and over 2 billion pages released in each snapshot, this data is a vast resource with content across a large number of domains and languages. Previous works have leveraged the data from Common Crawl for mining ngram counts to perform language modeling (Buck et al., 2014). Other works (Smith et al., 2013) have mined Common Crawl for bitexts for machine translation. However, this mining was performed on a small scale. For our dataset, we use 68 snapshots published from 2013 to 2020 which is vastly larger than previous works. 3.1 Preprocessing The first step in preprocessing the data is deduplication. While investigating combining many Common Crawl snapshots, we found duplicate URLs both within an individual snapshot and almost always across snapshots. As our data curation method relies on unique URLs for each web document, we apply a heuristic to ensure each URL appears once within the final cl"
2020.emnlp-main.480,E09-1091,0,0.045472,"General Assembly Resolutions (Rafalovitch et al., 2009; Ziemski et al., 2016) or European Parliament parallel corpus (Koehn, 2005). These parallel corpora were curated from specific, homogeneous sources by examining the content and deriving domainspecific rules for aligning documents. Other approaches have identified parallel documents in unstructured web corpora by relying on metadata (Nie et al., 1999; Espla-Gomis and Forcada, 2010). Some of these methods have focused on publication date and other temporal heuristics to aid in identifying parallel documents (Munteanu and Marcu, 2005, 2006; Udupa et al., 2009; Do et al., 2009; Abdul-Rauf and Schwenk, 2009). However, temporal features can be sparse, noisy, and unreliable. A different class of alignment methods rely on document structure (Resnik and Smith, 2003; Chen and Nie, 2000). In the WMT-2016 bilingual document alignment shared task (Buck and Koehn, 2016a), many techniques applied retrieval and matching on translated 5-grams (Dara and Lin, 2016) to query, retrieve, and align documents. Similar methods for generating candidates by retrieving matches based on the least frequent bi-lingual 5grams have been proposed (Gomes and Lopes, 2016) with th"
2020.emnlp-main.480,L16-1561,0,0.0518399,"only as a benchmark for document alignment, but also as a parallel corpus for mining parallel sentences and as supervision for a variety of cross-lingual tasks. which only considered English to French document alignment – a high-resource direction. 2 3 Related Works The concept of crawling and mining the web to identify sources of parallel data has been previously explored (Resnik, 1999). A large body of this work has focused on identifying parallel text from multilingual data obtained from a single source: for example the United Nations General Assembly Resolutions (Rafalovitch et al., 2009; Ziemski et al., 2016) or European Parliament parallel corpus (Koehn, 2005). These parallel corpora were curated from specific, homogeneous sources by examining the content and deriving domainspecific rules for aligning documents. Other approaches have identified parallel documents in unstructured web corpora by relying on metadata (Nie et al., 1999; Espla-Gomis and Forcada, 2010). Some of these methods have focused on publication date and other temporal heuristics to aid in identifying parallel documents (Munteanu and Marcu, 2005, 2006; Udupa et al., 2009; Do et al., 2009; Abdul-Rauf and Schwenk, 2009). However, t"
2020.emnlp-main.483,W16-2365,1,0.936329,"able can take. We select J=16 and γ=20 to produce windows that look reasonable to the authors (see Appendix B). We do not sweep J or γ, as we are concerned about overfitting given our small development set (see Table 1). 2.1.3 Boilerplate Down-weighting Many ‘sentences’ in web-crawled data are not true sentences, but boilerplate text such as text of navigational buttons, headers, or pull-down menus. We explore three methods for down-weighting such boilerplate text: 1. Scaling by the inverse of the log of number of the documents containing a given sentence, inspired by IDF (Sparck Jones, 1988; Buck and Koehn, 2016b) 2. A more aggressive variant of IDF which scales sentences by the inverse of the (linear, as opposed to log) number of documents containing a given sentence, which we denote ‘LIDF’ 3. Scaling each sentence by its length, in characters, as boilerplate lines are often very short (Kohlsch¨utter et al., 2010). We find that all three boilerplate methods improve candidate generation performance, but select LIDF as it resulted in the best recall performance on our development set in preliminary experiments. 2.2 Candidate Re-scoring To re-score a document pair proposed by candidate generation, we p"
2020.emnlp-main.483,D11-1084,0,0.0948889,"Missing"
2020.emnlp-main.483,W19-5435,1,0.818719,"ultilingual embeddings (LASER), this allows us to determine whether using document-level information (i.e., performing document alignment and then sentence alignment) provides better data than simply treating the data as comparable corpora and searching for sentence pairs. We refer to this method ‘LASER-cc.’ For a fair comparison with our document alignment method, we search for sentence pairs within each webdomain. For each method of finding parallel sentences, evaluation is the same: Since the true amount of parallel data is unknown, we rank the data from highest to lowest quality following Chaudhary et al. (2019) and train systems on a number of different data amounts, as measured by the number of English words. We train NMT systems following the WMT19 sentence filtering shared task (Koehn et al., 2019). Following Thompson and Koehn (2019), we train 5 systems per setting and report both mean and standard deviation BLEU scores. We report BLEU scores using sacreBLEU (Post, 2018). 6000 Method BLEU Buck + Hunalign Buck + Vecalign LASER-cc 8.74 +/- 0.20 10.46 +/- 0.13 10.40 +/- 0.15 This Work + Vecalign 11.62 +/- 0.09 Table 3: Downstream BLEU (+/- standard deviation for 5 runs) for the three document align"
2020.emnlp-main.483,P06-1062,0,0.0481003,"Missing"
2020.emnlp-main.483,D19-1136,1,0.828343,"y candidate generation, we perform sentence alignment and 5998 score the quality of the resulting sentence alignment in order to judge whether the proposed document pair appears to be a good translation pair. Our goal is to filter out documents pairs that may contain similar information, but where the order of that information is not consistent between the two documents, indicating they are not parallel. Our proposed document pair scoring function is: S(E, F ) = X 1 sim(e, f )p(LE |e)p(LF |f ) (2) |a(E, F )| e,f ∈a(E,F ) 2.2.1 Sentence Alignment To perform sentence alignment, we use Vecalign (Thompson and Koehn, 2019).4 Vecalign uses multilingual sentence embeddings to judge sentence similarity, in conjunction with a dynamic programming approximation based on fast dynamic time warping (Salvador and Chan, 2007) to approximate a search over the full space of possible sentence alignments in linear time complexity with respect to document length. We follow Thompson and Koehn (2019) and again use LASER embeddings, except we project all embeddings down to size 128. 2.2.2 Language ID One artifact of using multilingual sentence embeddings is that they give perfect alignment scores to exact, un-translated sentence"
2020.emnlp-main.483,1999.mtsummit-1.79,0,0.196589,"used for document-level MT training, but also results in higher quality sentence pairs. 4 Related Work There is a large amount of prior work in document alignment. One of the simplest methods is URL similarity (Resnik, 1998; Chen and Nie, 2000), although this has been shown to be brittle (Tiedemann, 2011). HTML structure (Resnik and Smith, 2003; Shi et al., 2006) or metadata such as publication date (Munteanu and Marcu, 2005) is often similar between parallel websites. However, most more recent work has focused on content similarity via bag-of-words or bag-of-ngrams, using bilingual lexicon (Ma and Liberman, 1999; Fung and Cheung, 2004; Ion et al., 2011; Espl`aGomis et al., 2016; Etchegoyhen and Azpeitia, 2016; Azpeitia and Etchegoyhen, 2019), machine translation (Uszkoreit et al., 2010), or phrase tables (Gomes and Pereira Lopes, 2016). Some work has considered high-level order as a filtering step after using a unordered representation to generate candidates: Ma and Liberman (1999) and Le et al. (2016) discard n-gram pairs outside a fixed window, while Uszkoreit et al. (2010) filters out documents that have high edit distance between sequences of corresponding n-gram pairs. Utiyama and Isahara (2003)"
2020.emnlp-main.483,C10-1124,0,0.0399465,"simplest methods is URL similarity (Resnik, 1998; Chen and Nie, 2000), although this has been shown to be brittle (Tiedemann, 2011). HTML structure (Resnik and Smith, 2003; Shi et al., 2006) or metadata such as publication date (Munteanu and Marcu, 2005) is often similar between parallel websites. However, most more recent work has focused on content similarity via bag-of-words or bag-of-ngrams, using bilingual lexicon (Ma and Liberman, 1999; Fung and Cheung, 2004; Ion et al., 2011; Espl`aGomis et al., 2016; Etchegoyhen and Azpeitia, 2016; Azpeitia and Etchegoyhen, 2019), machine translation (Uszkoreit et al., 2010), or phrase tables (Gomes and Pereira Lopes, 2016). Some work has considered high-level order as a filtering step after using a unordered representation to generate candidates: Ma and Liberman (1999) and Le et al. (2016) discard n-gram pairs outside a fixed window, while Uszkoreit et al. (2010) filters out documents that have high edit distance between sequences of corresponding n-gram pairs. Utiyama and Isahara (2003) and Zhang et al. (2006) use sentence similarity and/or number of aligned sentences after performing sentence alignment to score candidate documents. Guo et al. (2018) score docu"
2020.emnlp-main.483,P03-1010,0,0.181797,"con (Ma and Liberman, 1999; Fung and Cheung, 2004; Ion et al., 2011; Espl`aGomis et al., 2016; Etchegoyhen and Azpeitia, 2016; Azpeitia and Etchegoyhen, 2019), machine translation (Uszkoreit et al., 2010), or phrase tables (Gomes and Pereira Lopes, 2016). Some work has considered high-level order as a filtering step after using a unordered representation to generate candidates: Ma and Liberman (1999) and Le et al. (2016) discard n-gram pairs outside a fixed window, while Uszkoreit et al. (2010) filters out documents that have high edit distance between sequences of corresponding n-gram pairs. Utiyama and Isahara (2003) and Zhang et al. (2006) use sentence similarity and/or number of aligned sentences after performing sentence alignment to score candidate documents. Guo et al. (2018) score document pairs using the sentence-level nearest neighbor as well as the absolute difference in sentence position between sentence pairs. In contrast to these methods, our work considers high-level order in both candidate generation and re-scoring. Guo et al. (2019) demonstrated neural document embeddings are effective representations for document alignment. They trained on millions of document pairs in each specific langua"
2020.emnlp-main.483,P19-1116,0,0.0482718,"Missing"
2020.emnlp-main.483,volk-etal-2010-challenges,0,0.0804372,"Missing"
2020.emnlp-main.483,resnik-1998-parallel,0,\N,Missing
2020.emnlp-main.483,W11-1217,0,\N,Missing
2020.emnlp-main.483,J03-3002,0,\N,Missing
2020.emnlp-main.483,J05-4003,0,\N,Missing
2020.emnlp-main.483,W16-2371,0,\N,Missing
2020.emnlp-main.483,W16-2368,0,\N,Missing
2020.emnlp-main.483,2010.amta-papers.14,0,\N,Missing
2020.emnlp-main.483,D19-1632,1,\N,Missing
2020.emnlp-main.483,E17-2068,0,\N,Missing
2020.emnlp-main.483,W19-5404,1,\N,Missing
2020.emnlp-main.483,W16-2367,0,\N,Missing
2020.emnlp-main.483,W16-3412,0,\N,Missing
2020.emnlp-main.6,W07-0718,1,0.764038,". The use of reverse-created test sets was not the only concern raised by Läubli et al. (2018) and Toral et al. (2018). Both used more context than the original sentence-level evaluation in Hassan et al. (2018), Läubli et al. (2018) now asking human judges to assess entire documents, and Toral et al. (2018) involving assessment of MT output sentences in the order that they appeared in original documents. Furthermore, in contrast to the use of Direct Assessment (Graham et al., 2016) by Hassan et al. (2018), both reassessments used relative ranking, a method formerly used in WMT for evaluation (Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016), but now abandoned, partly due to low inter-annotator agreement. Therefore, although both re-evaluations improved the methodology employed in two respects, by eliminating reverse-created test data and including more context, both potentially include other sources of inaccuracy, such as lack of reliability of human judges when human evaluation takes the form of relative ranking. Furthermore, Toral et al. (2018) employ Trueskill to reach the conclusion that the MT system in question has not achieved human performance, and alth"
2020.emnlp-main.6,W19-5301,1,0.910427,"Missing"
2020.emnlp-main.6,W08-0309,1,0.862244,"Missing"
2020.emnlp-main.6,W10-1703,1,0.865007,"Missing"
2020.emnlp-main.6,W12-3102,1,0.905186,"Missing"
2020.emnlp-main.6,W11-2103,1,0.87397,"Missing"
2020.emnlp-main.6,W19-5204,0,0.0608855,"18) and Toral et al. (2018) used only test data that originated in the source language. Inspired by this work, other authors considered the effect of the 50/50 set-up on evaluation using WMT data. Edunov et al. (2019) questioned whether improvements in performance due to backtranslation were just an artifact of the test set construction. They found that, whilst back-translation had a disproportionately large positive effect on BLEU for reverse-created test sets, human evaluation showed that back-translation did indeed provide robust improvements to MT for forwardcreated text. Related to this, Freitag et al. (2019) also showed BLEU to be misleading on the reversecreated part of the test sets, when analysing why their automatic post-editing (APE) method produced improved translations according to human evaluation, but not according to BLEU. Given the concern in the community about using reversecreated test sets, the organisers of the WMT19 news translation task used only forward-created sentences in all their test sets (Barrault et al., 2019). In this current paper we provide detailed evidence to justify this decision. We note that Zhang and Toral (2019) also provide analysis of the effect of reverse-cre"
2020.emnlp-main.6,J12-4004,0,0.0612623,"Missing"
2020.emnlp-main.6,D18-1512,0,0.115616,"Missing"
2020.emnlp-main.6,P02-1040,0,0.115043,"ems participating in WMT-15 to WMT-18, as well as differences in human DA scores for systems participating in WMT-17 to WMT-18. The absence of systems in the upper-left and lower-right quadrants reassuringly shows that although extreme changes in BLEU and human scores do occur when test set creation direction is altered, the changes are at least somewhat systematic in the sense that when a difference in scores occurs (a drop or increase BLEU Besides human evaluation, the performance of MT systems is often measured using automatic metrics, the most common of which remains to be the BLEU score (Papineni et al., 2002). Figure 2 shows a box plot of absolute differences in BLEU scores for systems (reverse BLEU − forward BLEU) participating in WMT news translation tasks from 2015 to 2018. Counter expectation there is a clear mix of positive and negative BLEU score differences for several language pairs. Comparison of BLEU scores is not as straightforward as human evaluation however, and there are further consideration to be made before drawing conclusions from the mix of positive and negative absolute BLEU score differences described above. For example, the fact that splitting the test 76 Kendall 1.0 1.0 1.0"
2020.emnlp-main.6,W18-6312,0,0.155882,"of a human evaluation previously criticized for including reverse-created test data that claimed human parity of Chinese to English MT. We reveal insights into additional potential sources of inaccuracy of conclusions beyond the presence of translationese with the aim of preventing future inaccuracies. 2 spect to human evaluation, without considering its differing effect on automatic evaluation. Also, they do not consider the problem of statistical power in human evaluation, which we raise below. The use of reverse-created test sets was not the only concern raised by Läubli et al. (2018) and Toral et al. (2018). Both used more context than the original sentence-level evaluation in Hassan et al. (2018), Läubli et al. (2018) now asking human judges to assess entire documents, and Toral et al. (2018) involving assessment of MT output sentences in the order that they appeared in original documents. Furthermore, in contrast to the use of Direct Assessment (Graham et al., 2016) by Hassan et al. (2018), both reassessments used relative ranking, a method formerly used in WMT for evaluation (Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016), but now abandoned, p"
2020.emnlp-main.6,W19-5208,0,0.182034,"nts to MT for forwardcreated text. Related to this, Freitag et al. (2019) also showed BLEU to be misleading on the reversecreated part of the test sets, when analysing why their automatic post-editing (APE) method produced improved translations according to human evaluation, but not according to BLEU. Given the concern in the community about using reversecreated test sets, the organisers of the WMT19 news translation task used only forward-created sentences in all their test sets (Barrault et al., 2019). In this current paper we provide detailed evidence to justify this decision. We note that Zhang and Toral (2019) also provide analysis of the effect of reverse-created test sets on WMT evaluation campaigns. However they focus only on the effect of translationese with re73 REV FWD tion of statistical power. In most MT human evaluations, it is not feasible to evaluate the full test set of sentences for all systems and it is common to instead evaluate a sample of translations, usually drawn at random from the test data. In current WMT evaluations, for example, translations of all test sentences produced by all participating systems are pooled and a random sample is humanevaluated. This method ensures that"
2020.emnlp-main.7,D19-1632,1,0.901696,"Missing"
2020.emnlp-main.7,N06-1003,1,0.659051,"Missing"
2020.emnlp-main.7,K19-1005,1,0.83875,"Missing"
2020.emnlp-main.7,W18-2705,1,0.913357,"Missing"
2020.emnlp-main.7,N12-1017,0,0.0337366,"was Figure 1: Some possible . . . . . . . . . .paraphrases . . . . . . . . . . . of the original reference, ‘The tortoise beat the hare,’ for the Dutch source sentence, ‘De schildpad versloeg de haas.’ A sampled path and some of the other tokens also considered in the training objective are highlighted. We demonstrate the effectiveness of our method on two corpora from the low-resource MATERIAL program, and on bitext from GlobalVoices. We release data & code: data.statmt.org/smrt Introduction Variability and expressiveness are core features of language, and they extend to translation as well. Dreyer and Marcu (2012) showed that naturally occurring sentences have billions of valid translations. Despite this variety, machine translation (MT) models are optimized toward a single translation of each sentence in the training corpus. Training a high resource MT model on millions of sentence pairs likely exposes it to similar sentences translated different ways, but training a low-resource MT model with a single translation for each sentence (out of potentially billions) exacerbates data sparsity. We hypothesize that the discrepancy between linguistic diversity and standard single-reference training hinders MT"
2020.emnlp-main.7,D16-1139,0,0.0503468,"ing (top100 sampling). Greedy search tends to work best: see Table 4. It improves over the baseline for the 10 Global Voices datasets, but not for the two MATERIAL ones. Overall, our proposed method is more effective than this contrastive method. We hypothesize this is due to the wider variety of paraprhases SMRT introduces by sampling and training toward the full distribution from the paraphraser. 5.4 6 Sequence-Level Paraphrastic Data Augmentation Related Work Knowledge Distillation Our proposed objective is similarly structured to word-level knowledge distillation (KD; Hinton et al., 2015; Kim and Rush, 2016), where a student model is trained to match the output distribution of a teacher model. Paraphrasing as preprocessed data augmentation, as discussed in § 5.4, is similarly analogous to sequencelevel knowledge distillation (Kim and Rush, 2016). As a contrastive experiment, we use the paraphraser to generate additional target-side data for use in data augmentation. For each target sentence (y) in 8 All use settings from § 3.2: we use the original reference with LNLL with 1 − p = 0.5 probability, and when sampling we sample from the top w = 100 tokens. 9 This is equivalent to LNLL using a paraphr"
2020.emnlp-main.7,P17-2090,0,0.0597272,"Missing"
2020.emnlp-main.7,P18-1082,0,0.0253537,"beddings, and 2 encoder and decoder attention heads. We regularize with 0.2 label smoothing and 0.4 dropout. We optimize using Adam with a learning rate of 10−3 . We train for 200 epochs, and select the best checkpoint based on validation set perplexity. We translate with a beam size of 5. For our method we use the proposed objective LSMRT with probability p = 0.5 and standard LNLL on the original reference with probability 1 − p. We sample from only the 100 highest probability vocabulary items at a given time step when sampling from the paraphraser distribution to avoid very unlikely tokens (Fan et al., 2018). Using our English paraphraser, we aim to demonstrate improvements in low-resource settings, since these remain a challenge in NMT (Koehn and Knowles, 2017; Sennrich and Zhang, 2019). We use Tagalog (tl) to English and Swahili (sw) to English bitext from the MATERIAL low-resource program (Rubino, 2018). We also report results on MT bitext from GlobalVoices, a non-profit news site that publishes in 53 languages.3 We evaluate on the 10 lowest-resource settings that have at least 10,000 lines of parallel text with English: Hungarian (hu), Indonesian (id), Czech (cs), Serbian (sr), Catalan (ca),"
2020.emnlp-main.7,W04-3250,1,0.636066,"Missing"
2020.emnlp-main.7,W17-3204,1,0.816531,"te of 10−3 . We train for 200 epochs, and select the best checkpoint based on validation set perplexity. We translate with a beam size of 5. For our method we use the proposed objective LSMRT with probability p = 0.5 and standard LNLL on the original reference with probability 1 − p. We sample from only the 100 highest probability vocabulary items at a given time step when sampling from the paraphraser distribution to avoid very unlikely tokens (Fan et al., 2018). Using our English paraphraser, we aim to demonstrate improvements in low-resource settings, since these remain a challenge in NMT (Koehn and Knowles, 2017; Sennrich and Zhang, 2019). We use Tagalog (tl) to English and Swahili (sw) to English bitext from the MATERIAL low-resource program (Rubino, 2018). We also report results on MT bitext from GlobalVoices, a non-profit news site that publishes in 53 languages.3 We evaluate on the 10 lowest-resource settings that have at least 10,000 lines of parallel text with English: Hungarian (hu), Indonesian (id), Czech (cs), Serbian (sr), Catalan (ca), Swahili (sw),4 Dutch (nl), Polish (pl), Macedonian (mk), Arabic (ar). We use 2,000 lines each for a validation set for model selection from checkpoints and"
2020.emnlp-main.7,W18-1902,0,0.0261518,"use the proposed objective LSMRT with probability p = 0.5 and standard LNLL on the original reference with probability 1 − p. We sample from only the 100 highest probability vocabulary items at a given time step when sampling from the paraphraser distribution to avoid very unlikely tokens (Fan et al., 2018). Using our English paraphraser, we aim to demonstrate improvements in low-resource settings, since these remain a challenge in NMT (Koehn and Knowles, 2017; Sennrich and Zhang, 2019). We use Tagalog (tl) to English and Swahili (sw) to English bitext from the MATERIAL low-resource program (Rubino, 2018). We also report results on MT bitext from GlobalVoices, a non-profit news site that publishes in 53 languages.3 We evaluate on the 10 lowest-resource settings that have at least 10,000 lines of parallel text with English: Hungarian (hu), Indonesian (id), Czech (cs), Serbian (sr), Catalan (ca), Swahili (sw),4 Dutch (nl), Polish (pl), Macedonian (mk), Arabic (ar). We use 2,000 lines each for a validation set for model selection from checkpoints and a test set for reporting results. The approximate number of lines of training data is in the top of Table 1. We train an English SentencePiece model"
2020.emnlp-main.7,D18-2012,0,0.125663,"Missing"
2020.emnlp-main.7,P16-1009,0,0.102396,"Missing"
2020.emnlp-main.7,D18-1421,0,0.0608042,"6; Marton et al., 2009), and generation of additional references for tuning (Madnani et al., 2007, 2008). Label Smoothing Label smoothing (which we use when training with LNLL ) spreads probability mass over all non-reference tokens equally (Szegedy et al., 2016); LSMRT places higher probability on semantically plausible tokens. 7 Conclusion We present Simulated Multiple Reference Training (SMRT), which significantly improves performance in low-resource settings—by 1.2 to 7.0 BLEU—and is complementary to back-translation. Neural paraphrasers are rapidly improving (Wieting et al., 2017, 2019b; Li et al., 2018; Wieting and Gimpel, 2018; Hu et al., 2019a,b,c), and the concurrently released P RISM multi-lingual paraphraser Thompson and Post (2020a,b) has coverage of 39 languages and outperforms prior work in English paraphrasing. As paraphrasing continues to improve and cover more languages, we are optimistic SMRT will provide larger improvements across the board—including for higher-resource MT and for additional target languages beyond English. Data Augmentation in NMT Back-translation (BT) translates target-language monolingual text to create synthetic source sentences (Sennrich et al., 2016). BT"
2020.emnlp-main.7,P19-1021,0,0.0255847,"200 epochs, and select the best checkpoint based on validation set perplexity. We translate with a beam size of 5. For our method we use the proposed objective LSMRT with probability p = 0.5 and standard LNLL on the original reference with probability 1 − p. We sample from only the 100 highest probability vocabulary items at a given time step when sampling from the paraphraser distribution to avoid very unlikely tokens (Fan et al., 2018). Using our English paraphraser, we aim to demonstrate improvements in low-resource settings, since these remain a challenge in NMT (Koehn and Knowles, 2017; Sennrich and Zhang, 2019). We use Tagalog (tl) to English and Swahili (sw) to English bitext from the MATERIAL low-resource program (Rubino, 2018). We also report results on MT bitext from GlobalVoices, a non-profit news site that publishes in 53 languages.3 We evaluate on the 10 lowest-resource settings that have at least 10,000 lines of parallel text with English: Hungarian (hu), Indonesian (id), Czech (cs), Serbian (sr), Catalan (ca), Swahili (sw),4 Dutch (nl), Polish (pl), Macedonian (mk), Arabic (ar). We use 2,000 lines each for a validation set for model selection from checkpoints and a test set for reporting re"
2020.emnlp-main.7,W07-0716,0,0.141273,"Missing"
2020.emnlp-main.7,P16-1159,0,0.0874042,"Missing"
2020.emnlp-main.7,2008.amta-papers.13,0,0.201312,"Missing"
2020.emnlp-main.7,D09-1040,0,0.030861,"Missing"
2020.emnlp-main.7,2020.emnlp-main.8,1,0.856459,"Missing"
2020.emnlp-main.7,2020.wmt-1.67,1,0.852344,"Missing"
2020.emnlp-main.7,N19-4009,0,0.0735833,"Missing"
2020.emnlp-main.7,tiedemann-2012-parallel,0,0.0364539,"get token for both the MT model and paraphraser. For a visualization see Figure 1, which shows possible . . . . . . . . .paraphrases . . . . . . . . . . . . of the reference, ‘The tortoise beat the hare.’ The paraphraser and MT model condition on the paraphrase (y 0 ) as the previous output. The paraphrase (y 0 ) and the rest of the tokens in the paraphraser’s distribution make up pPARA , which is used to compute LSMRT . 3 3.1 NMT models Experimental Setup Paraphraser 2 Hu et al. released a trained S OCKEYE paraphraser but we implement our method in FAIRSEQ. 3 We use v2017q3 released on Opus (Tiedemann, 2012, opus.nlpl.eu/GlobalVoices.php). 4 Swahili is in both. MATERIAL data is not widely available, so we separate them to keep GlobalVoices reproducible. For use as an English paraphraser, we train a Transformer model (Vaswani et al., 2017) in FAIRSEQ (Ott et al., 2019) with an 8-layer encoder and decoder, 1024 dimensional embeddings, 16 encoder 83 dataset GlobalVoices MATERIAL * → en train lines hu 8k id 8k cs 11k sr 14k ca 15k sw 24k nl 32k pl 40k mk 44k ar 47k sw 19k tl 46k baseline this work 2.3 5.4 5.3 12.3 3.4 6.6 11.8 16.1 16.0 20.0 17.9 20.5 22.2 24.8 16.0 18.0 27.0 28.2 12.7 14.9 37.8 39."
2020.emnlp-main.7,D18-1100,0,0.0460032,"Missing"
2020.emnlp-main.7,P19-1427,0,0.0478073,"Missing"
2020.emnlp-main.7,P18-1042,0,0.0580104,"2009), and generation of additional references for tuning (Madnani et al., 2007, 2008). Label Smoothing Label smoothing (which we use when training with LNLL ) spreads probability mass over all non-reference tokens equally (Szegedy et al., 2016); LSMRT places higher probability on semantically plausible tokens. 7 Conclusion We present Simulated Multiple Reference Training (SMRT), which significantly improves performance in low-resource settings—by 1.2 to 7.0 BLEU—and is complementary to back-translation. Neural paraphrasers are rapidly improving (Wieting et al., 2017, 2019b; Li et al., 2018; Wieting and Gimpel, 2018; Hu et al., 2019a,b,c), and the concurrently released P RISM multi-lingual paraphraser Thompson and Post (2020a,b) has coverage of 39 languages and outperforms prior work in English paraphrasing. As paraphrasing continues to improve and cover more languages, we are optimistic SMRT will provide larger improvements across the board—including for higher-resource MT and for additional target languages beyond English. Data Augmentation in NMT Back-translation (BT) translates target-language monolingual text to create synthetic source sentences (Sennrich et al., 2016). BT needs a reverse translatio"
2020.emnlp-main.7,P19-1453,0,0.177904,"Missing"
2020.emnlp-main.7,D17-1026,0,0.0879872,"n (Callison-Burch et al., 2006; Marton et al., 2009), and generation of additional references for tuning (Madnani et al., 2007, 2008). Label Smoothing Label smoothing (which we use when training with LNLL ) spreads probability mass over all non-reference tokens equally (Szegedy et al., 2016); LSMRT places higher probability on semantically plausible tokens. 7 Conclusion We present Simulated Multiple Reference Training (SMRT), which significantly improves performance in low-resource settings—by 1.2 to 7.0 BLEU—and is complementary to back-translation. Neural paraphrasers are rapidly improving (Wieting et al., 2017, 2019b; Li et al., 2018; Wieting and Gimpel, 2018; Hu et al., 2019a,b,c), and the concurrently released P RISM multi-lingual paraphraser Thompson and Post (2020a,b) has coverage of 39 languages and outperforms prior work in English paraphrasing. As paraphrasing continues to improve and cover more languages, we are optimistic SMRT will provide larger improvements across the board—including for higher-resource MT and for additional target languages beyond English. Data Augmentation in NMT Back-translation (BT) translates target-language monolingual text to create synthetic source sentences (Sen"
2020.emnlp-main.7,D19-1143,0,0.0478443,"Missing"
2020.emnlp-main.7,N19-1090,1,\N,Missing
2020.wmt-1.1,2020.nlpcovid19-2.5,1,0.796341,"tails of the evaluation. 4.1.1 Covid Test Suite TICO-19 The TICO-19 test suite was developed to evaluate how well can MT systems handle the newlyemerged topic of COVID-19. Accurate automatic translation can play an important role in facilitating communication in order to protect at-risk populations and combat the infodemic of misinformation, as described by the World Health Organization. The test suite has no corresponding paper so its authors provided an analysis of the outcomes directly here. The submitted systems were evaluated using the test set from the recently-released TICO-19 dataset (Anastasopoulos et al., 2020). The dataset provides manually created translations of COVID19 related data. The test set consists of PubMed articles (678 sentences from 5 scientific articles), patient-medical professional conversations (104 sentences), as well as related Wikipedia articles (411 sentences), announcements (98 sentences from Wikisource), and news items (67 sentences from Wikinews), for a total of 2100 sentences. Table 15 outlines the BLEU scores by each submitted system in the English-to-X directions, also breaking down the results per domain. The analysis shows that some systems are significantly more prepar"
2020.wmt-1.1,2020.wmt-1.6,0,0.0647231,"AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) SJTU-NICT (Li et al., 2020) Samsung Research Poland (Krubi´nski et al., 2020) TALP UPC (Escolano et al., 2020) Tencent Translation (Wu et al., 2020b) NLP Lab at Tsinghua Universit"
2020.wmt-1.1,2020.wmt-1.38,0,0.0746945,"Missing"
2020.wmt-1.1,2020.wmt-1.54,1,0.802974,"Missing"
2020.wmt-1.1,W07-0718,1,0.671054,"Missing"
2020.wmt-1.1,W08-0309,1,0.762341,"Missing"
2020.wmt-1.1,W12-3102,1,0.500805,"Missing"
2020.wmt-1.1,2020.lrec-1.461,0,0.0795779,"Missing"
2020.wmt-1.1,2012.eamt-1.60,0,0.124643,"tted systems, for those where the authors provided such details. The training corpus for Inuktitut↔English is the recently released Nunavut Hansard Inuktitut– English Parallel Corpus 3.0 (Joanis et al., 2020). For the Japanese↔English tasks, we added several freely available parallel corpora to the training data. It includes JParaCrawl v2.0 (Morishita et al., 2020), a large web-based parallel corpus, Japanese-English Subtitle Corpus (JESC) (Pryzant et al., 2017), the Kyoto Free Translation Task (KFTT) corpus (Neubig, 2011), constructed from the Kyoto-related Wikipedia articles, and TED Talks (Cettolo et al., 2012). The monolingual data we provided was similar to last year’s, with a 2019 news crawl added to all the news corpora. In addition, we provided versions of the news corpora for Czech, English and German, with both the document and paragraph structure retained. In other words, we did not apply sentence splitting to these corpora, and we retained the document boundaries and text ordering of the originals. Training, development, and test data for Pashto↔English and Khmer↔English are shared with the Parallel Corpus Filtering Shared Task (Koehn et al., 2020). The training data mostly comes from OPUS"
2020.wmt-1.1,2020.wmt-1.3,0,0.0731913,"on Machine Translation (WMT20)1 was held online with EMNLP 2020 and hosted a number of shared tasks on various aspects of machine translation. This conference built on 14 previous editions of WMT as workshops and conferences (Koehn and Monz, 2006; CallisonBurch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016, 2017, 2018; Barrault et al., 2019). This year we conducted several official tasks. We report in this paper on the news and similar translation tasks. Additional shared tasks are described in separate papers in these proceedings: • automatic post-editing (Chatterjee et al., 2020) • biomedical translation (Bawden et al., 2020b) • chat translation (Farajian et al., 2020) • lifelong learning (Barrault et al., 2020) 1 Makoto Morishita NTT Santanu Pal WIPRO AI Abstract 1 Philipp Koehn JHU http://www.statmt.org/wmt20/ 1 Proceedings of the 5th Conference on Machine Translation (WMT), pages 1–55 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics as “direct assessment”) that we explored in the previous years with convincing results in terms of the trade-off between annotation effort and reliable distinctions between systems. The primary objectives o"
2020.wmt-1.1,2020.wmt-1.8,0,0.0898111,"D D I D I -NLP DONG - NMT ENMT E T RANSLATION FACEBOOK AI G RONINGEN GTCOM H ELSINKI NLP H UAWEI TSC IIE M ICROSOFT STC I NDIA NICT-K YOTO NICT-RUI N IU T RANS NRC OPPO PROMT SJTU-NICT SRPOL TALP UPC T ENCENT T RANSLATION THUNLP T ILDE T OHOKU -AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Know"
2020.wmt-1.1,2009.freeopmt-1.3,0,0.088081,"ve (CONTRASTIVE) or primary (PRIMARY), and the BLEU, RIBES and TER results. The scores are sorted by BLEU. In general, primary systems tend to be better than contrastive systems, as expected, but there are some exceptions. This year we recived major number of participants for the case of Indo-Aryan language group NUST-FJWU NUST-FJWU system is an extension of state-of-the-art Transformer model with hierarchical attention networks to incorporate contextual information. During training the model used back-translation. Prompsit This team is participating with a rulebased system based on Apertium (Forcada et al., 2009-11). Apertium is a free/open-source platform for developing rule-based machine translation systems and language technology that was first released in 2005. Apertium is hosted in Github where both language data and code are licensed under the GNU GPL. It is a research and business platform with a very active community that loves small languages. Language pairs are at a very different level of development and output quality in the platform, depending on two main variables: how much funded or in-kind effort has 32 5.4 i.e. Hindi–Marathi (in both directions). We received 22 submissions from 14 te"
2020.wmt-1.1,2020.wmt-1.80,0,0.0933589,"airs, to be evaluated on test sets consisting mainly of news stories. The task was also opened up to additional test suites to probe specific aspects of translation. In the similar language translation task, participants built machine translation systems for translating between closely related pairs of languages. Chi-kiu Lo NRC Masaaki Nagata NTT Marcos Zampieri Rochester Institute of Technology metrics (Mathur et al., 2020) parallel corpus filtering (Koehn et al., 2020) quality estimation (Specia et al., 2020a) robustness (Specia et al., 2020b) unsupervised and very low-resource translation (Fraser, 2020) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (“constrained” condition). We included 22 translation directions this year, with translation between English and each of Chinese, Czech, German and Russian, as well as French to and from German being repeated from last year, and English to and from Inuktitut, Japanese, Polish and Tamil being new for this year. Furthermore, English to and from Khmer and Pashto were included, using the same test sets as in the corpus filtering task. Th"
2020.wmt-1.1,W19-5204,0,0.0543621,"Missing"
2020.wmt-1.1,2020.emnlp-main.5,0,0.0410594,"luation of out-ofEnglish translations, HITs were generated using the same method as described for the SR+DC evaluation of into-English translations in Section 3.2.1 with minor modifications. Source-based DA allows to include human references in the evaluation as another system to provide an estimate of human performance. Human references were added to the pull of system outputs prior to sampling documents for tasks generation. If multiple references are available, which is the case for English→German (3 alternative reference translations, including 1 generated using the paraphrasing method of Freitag et al. (2020)) and English→Chinese (2 translations), each reference is assessed individually. Since the annotations are made by researchers and professional translators who ensure a betTable 11: Amount of data collected in the WMT20 manual document- and segment-level evaluation campaigns for bilingual/source-based evaluation out of English and nonEnglish pairs. et al., 2020; Laubli et al., 2020). It differs from SR+DC DA introduced in WMT19 (Bojar et al., 2019), and still used in into-English human evaluation this year, where a single segment from a document is provided on a screen at a time, followed by s"
2020.wmt-1.1,W18-3931,1,0.874637,"ese improvements to increased capacity (which allows increased sensitivity to long-range relations) of the models. 5 Similar Language Translation Most shared tasks at WMT (e.g. News, Biomedical) have historically dealt with translating texts from and to English. In recent years, we observed a growing interest in training systems to translate between languages other than English. This includes a number of papers applying MT to translate between pairs of closely-related languages, national language varieties, and dialects 27 of the same language (Zhang, 1998; Marujo et al., 2011; Hassani, 2017; Costa-jussà et al., 2018; Popovi´c et al., 2020). To address this topic, the first Similar Language Translation (SLT) shared task at WMT 2019 has been organized. It featured data from three pairs of closely-related languages from different language families: Spanish - Portuguese (Romance languages), Czech - Polish (Slavic languages), and Hindi - Nepali (IndoAryan languages). Following the success of the first SLT shared task at WMT 2019 and the interest of the community in this topic, we organize, for the second time at WMT, this shared task to evaluate the performance of state-of-the-art translation systems on trans"
2020.wmt-1.1,2020.wmt-1.18,0,0.0913945,"2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) SJTU-NICT (Li et al., 2020) Samsung Research Poland (Krubi´nski et al., 2020) TALP UPC (Escolano et al., 2020) Tencent Translation (Wu et al., 2020b) NLP Lab at Tsinghua University (no associated paper) Tilde (Krišlauks and Pinnis, 2020) Tohoku-AIP-NTT (Kiyono et al., 2020) Ubiqus (Hernandez and Nguyen, 2020) University of Edinburgh (Bawden et al., 2020a; Germann, 2020) University of Edinburgh and Charles University (Germann et al., 2020) Université du Québec à Montréal (no associated paper) ByteDance AI Lab (Wu et al., 2020a) WeChat (Meng et al., 2020) Baseline System from Biomedical Task (Bawden et al., 2020b) American University of Beirut (no associated paper) Zoho Corporation (no associated paper) Table 6: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion"
2020.wmt-1.1,2020.wmt-1.43,0,0.0835067,"Missing"
2020.wmt-1.1,2020.wmt-1.9,0,0.0939415,"Missing"
2020.wmt-1.1,2020.wmt-1.19,0,0.0674131,"Missing"
2020.wmt-1.1,2009.mtsummit-btm.6,0,0.103443,"Missing"
2020.wmt-1.1,W13-2305,1,0.929934,"work which can be well applied to different translation. directions. Techniques used in the submitted systems include optional multilingual pre-training (mRASP) for low resource languages, very deep Transformer or dynamic convolution models up to 50 encoder layers, iterative backtranslation, knowledge distillation, model ensemble and development set fine-tuning. The key ingredient of the process seems the strong focus on diversification of the (synthetic) training data, using multiple scalings of the Transformer model 3.1 Direct Assessment Since running a comparison of direct assessments (DA, Graham et al., 2013, 2014, 2016) and relative ranking in 2016 (Bojar et al., 2016) and verifying a high correlation of system rankings for the two methods, as well as the advantages of DA, such as quality controlled crowd-sourcing and linear growth relative to numbers of submissions, we have employed DA as the primary mechanism for evaluating systems. With DA human evaluation, 15 human assessors are asked to rate a given translation by how adequately it expresses the meaning of the corresponding reference translation or source language input on an analogue scale, which corresponds to an underlying absolute 0–100"
2020.wmt-1.1,E14-1047,1,0.888167,"Missing"
2020.wmt-1.1,2020.lrec-1.312,1,0.804196,"A screenshot of OCELoT is shown in Figure 5. For presentation of the results, systems are treated as either constrained or unconstrained, depending on whether their models were trained only on the provided data. Since we do not know how they were built, the online systems are treated as unconstrained during the automatic and human evaluations. In the rest of this section, we provide brief details of the submitted systems, for those where the authors provided such details. The training corpus for Inuktitut↔English is the recently released Nunavut Hansard Inuktitut– English Parallel Corpus 3.0 (Joanis et al., 2020). For the Japanese↔English tasks, we added several freely available parallel corpora to the training data. It includes JParaCrawl v2.0 (Morishita et al., 2020), a large web-based parallel corpus, Japanese-English Subtitle Corpus (JESC) (Pryzant et al., 2017), the Kyoto Free Translation Task (KFTT) corpus (Neubig, 2011), constructed from the Kyoto-related Wikipedia articles, and TED Talks (Cettolo et al., 2012). The monolingual data we provided was similar to last year’s, with a 2019 news crawl added to all the news corpora. In addition, we provided versions of the news corpora for Czech, Engli"
2020.wmt-1.1,2020.emnlp-main.6,1,0.839606,"shown in Table 3, where the first and second are simple merges or splits, whereas the third is a rare case of more complex reordering. We leave a detailed analysis of the translators’ treatment of paragraph-split data for future work. development set is provided, it is a mixture of both “source-original” and “target-original” texts, in order to maximise its size, although the original language is always marked in the sgm file, except for Inuktitut↔English. The consequences of directionality in test sets has been discussed recently in the literature (Freitag et al., 2019; Laubli et al., 2020; Graham et al., 2020), and the conclusion is that it can have an effect on detrimental effect on the accuracy of system evaluation. We use “source-original” parallel sentences wherever possible, on the basis that it is the more realistic scenario for practical MT usage. Exception: the test sets for the two Inuktitut↔English translation directions contain the same data, without regard to original direction. For most news text in the test and development sets, English was the original language and Inuktitut the translation, while the parliamentary data mixes the two directions. The origins of the news test documents"
2020.wmt-1.1,2020.wmt-1.11,0,0.0940191,"N GTCOM H ELSINKI NLP H UAWEI TSC IIE M ICROSOFT STC I NDIA NICT-K YOTO NICT-RUI N IU T RANS NRC OPPO PROMT SJTU-NICT SRPOL TALP UPC T ENCENT T RANSLATION THUNLP T ILDE T OHOKU -AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) S"
2020.wmt-1.1,D19-1632,1,0.881933,"ent and paragraph structure retained. In other words, we did not apply sentence splitting to these corpora, and we retained the document boundaries and text ordering of the originals. Training, development, and test data for Pashto↔English and Khmer↔English are shared with the Parallel Corpus Filtering Shared Task (Koehn et al., 2020). The training data mostly comes from OPUS (software localization, Tatoeba, Global Voices), the Bible, and specialprepared corpora from TED Talks and the Jehova Witness web site (JW300). The development and test sets were created as part of the Flores initiative (Guzmán et al., 2019) by professional translation of Wikipedia content with careful vetting of the translations. Please refer the to the Parallel Corpus Filtering Shared Task overview paper for details on these corpora. 2.3.1 AFRL (Gwinnup and Anderson, 2020) AFRL - SYSCOMB 20 is a system combination consisting of two Marian transformer ensembles, one OpenNMT transformer system and a Moses phrase-based system. AFRL - FINETUNE is an OpenNMT transformer system fine-tuned on newstest2014-2017. 2.3.2 (Xv, 2020) ARIEL XV is a Transformer base model trained with the Sockeye sequence modeling toolkit usSome statistics ab"
2020.wmt-1.1,2020.wmt-1.12,1,0.754946,"Missing"
2020.wmt-1.1,2020.wmt-1.13,0,0.0737827,"2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) SJTU-NICT (Li et al., 2020) Samsung Research Poland (Krubi´nski et al., 2020) TALP UPC (Escolano et al., 2020) Tencent Translation (Wu et al., 2020b) NLP Lab at Tsinghua University (no associated paper) Tilde (Krišlauks and Pinnis, 2020) Tohoku-AIP-NTT (Kiyono et al., 2020) Ubiqus (Hernandez and Nguyen, 2020) University of Edinburgh (Bawden et al., 2020a; Germann, 2020) University of Edinburgh and Charles University (Germann et al., 2020) Université du Québec à Montréal (no associated paper) ByteDance AI Lab (Wu et al., 2020a) WeChat (Meng et al"
2020.wmt-1.1,2020.wmt-1.20,0,0.057602,"Missing"
2020.wmt-1.1,2020.wmt-1.14,1,0.820019,"set, and the origlang tag indicates the original source language. 9 Team AFRL ARIEL XV CUNI DCU D EEP M IND D I D I -NLP DONG - NMT ENMT E T RANSLATION FACEBOOK AI G RONINGEN GTCOM H ELSINKI NLP H UAWEI TSC IIE M ICROSOFT STC I NDIA NICT-K YOTO NICT-RUI N IU T RANS NRC OPPO PROMT SJTU-NICT SRPOL TALP UPC T ENCENT T RANSLATION THUNLP T ILDE T OHOKU -AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Mari"
2020.wmt-1.1,2020.wmt-1.39,1,0.812433,"kables was collected in the first phase of the annotation, which amounted to 4k assessments across the systems. The second annotation phase with 6.5k assessments compared markable translations, always checking outputs of all the 13 competing MT systems but still considering the document-level context of each of them. Among other things, the observations indicate that the better the system, the lower the variance in manual scores. Markables annotation then confirms that frequent errors like bad translation of a term need not be the most severe and conversely, 4.1.3 Gender Coreference and Bias (Kocmi et al., 2020) The test suite by Kocmi et al. (2020) focuses on the gender bias in professions (e.g. physician, teacher, secretary) for the translation from English into Czech, German, Polish and Russian. These nouns are ambiguous with respect to gender in English but exhibit gender in the examined target languages. The test suite is based on the fact that a pronoun referring to the ambiguous noun can reveal the gender of the noun in the English source sentence. Once disambiguated, the gender needs to be preserved in translation. To correctly translate the given noun, the translation system thus has to corr"
2020.wmt-1.1,2020.wmt-1.53,0,0.089538,"Missing"
2020.wmt-1.1,2020.wmt-1.78,1,0.815194,"rence on Machine Translation (WMT) 2020. In the news task, participants were asked to build machine translation systems for any of 11 language pairs, to be evaluated on test sets consisting mainly of news stories. The task was also opened up to additional test suites to probe specific aspects of translation. In the similar language translation task, participants built machine translation systems for translating between closely related pairs of languages. Chi-kiu Lo NRC Masaaki Nagata NTT Marcos Zampieri Rochester Institute of Technology metrics (Mathur et al., 2020) parallel corpus filtering (Koehn et al., 2020) quality estimation (Specia et al., 2020a) robustness (Specia et al., 2020b) unsupervised and very low-resource translation (Fraser, 2020) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (“constrained” condition). We included 22 translation directions this year, with translation between English and each of Chinese, Czech, German and Russian, as well as French to and from German being repeated from last year, and English to and from Inuktitut, Japanese, Polish and Tamil being new fo"
2020.wmt-1.1,W17-1208,0,0.0524248,"ttribute all these improvements to increased capacity (which allows increased sensitivity to long-range relations) of the models. 5 Similar Language Translation Most shared tasks at WMT (e.g. News, Biomedical) have historically dealt with translating texts from and to English. In recent years, we observed a growing interest in training systems to translate between languages other than English. This includes a number of papers applying MT to translate between pairs of closely-related languages, national language varieties, and dialects 27 of the same language (Zhang, 1998; Marujo et al., 2011; Hassani, 2017; Costa-jussà et al., 2018; Popovi´c et al., 2020). To address this topic, the first Similar Language Translation (SLT) shared task at WMT 2019 has been organized. It featured data from three pairs of closely-related languages from different language families: Spanish - Portuguese (Romance languages), Czech - Polish (Slavic languages), and Hindi - Nepali (IndoAryan languages). Following the success of the first SLT shared task at WMT 2019 and the interest of the community in this topic, we organize, for the second time at WMT, this shared task to evaluate the performance of state-of-the-art tr"
2020.wmt-1.1,2020.wmt-1.21,0,0.0791885,"Missing"
2020.wmt-1.1,2020.wmt-1.23,0,0.0607352,"020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) SJTU-NICT (Li et al., 2020) Samsung Research Poland (Krubi´nski et al., 2020) TALP UPC (Escolano et al., 2020) Tencent Translation (Wu et al., 2020b) NLP Lab at Tsinghua University (no associated paper) Tilde (Krišlauks and Pinnis, 2020) Tohoku-AIP-NTT (Kiyono et al., 2020) Ubiqus (Hernandez and Nguyen, 2020) University of Edinburgh (Bawden et al., 2020a; Germann, 2020) University of Edinburgh and Charles University (Germann et al., 2"
2020.wmt-1.1,2020.wmt-1.77,1,0.84299,"slation task, both organised alongside the Conference on Machine Translation (WMT) 2020. In the news task, participants were asked to build machine translation systems for any of 11 language pairs, to be evaluated on test sets consisting mainly of news stories. The task was also opened up to additional test suites to probe specific aspects of translation. In the similar language translation task, participants built machine translation systems for translating between closely related pairs of languages. Chi-kiu Lo NRC Masaaki Nagata NTT Marcos Zampieri Rochester Institute of Technology metrics (Mathur et al., 2020) parallel corpus filtering (Koehn et al., 2020) quality estimation (Specia et al., 2020a) robustness (Specia et al., 2020b) unsupervised and very low-resource translation (Fraser, 2020) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (“constrained” condition). We included 22 translation directions this year, with translation between English and each of Chinese, Czech, German and Russian, as well as French to and from German being repeated from last year, and English to and from Inu"
2020.wmt-1.1,2020.wmt-1.24,0,0.0435945,"Missing"
2020.wmt-1.1,D18-1512,0,0.05415,"Missing"
2020.wmt-1.1,2020.wmt-1.47,1,0.740067,"Missing"
2020.wmt-1.1,W18-3601,1,0.891679,", we have employed DA as the primary mechanism for evaluating systems. With DA human evaluation, 15 human assessors are asked to rate a given translation by how adequately it expresses the meaning of the corresponding reference translation or source language input on an analogue scale, which corresponds to an underlying absolute 0–100 rating scale.5 No sentence or document length restriction is applied during manual evaluation. Direct Assessment is also employed for evaluation of video captioning systems at TRECvid (Graham et al., 2018; Awad et al., 2019) and multilingual surface realisation (Mille et al., 2018, 2019). 3.1.1 tion 2, most of our test sets do not include reversecreated sentence pairs, except when there were resource constraints on the creation of the test sets. 3.1.3 Prior to WMT19, the issue of including document context was raised within the community (Läubli et al., 2018; Toral et al., 2018) and at WMT19 a range of DA styles were subsequently tested that included document context. In WMT19, two options were run, firstly, an evaluation that included the document context “+DC” (with document context), and secondly, a variation that omitted document context “−DC” (without document con"
2020.wmt-1.1,2020.wmt-1.27,0,0.247738,"he original source language. 9 Team AFRL ARIEL XV CUNI DCU D EEP M IND D I D I -NLP DONG - NMT ENMT E T RANSLATION FACEBOOK AI G RONINGEN GTCOM H ELSINKI NLP H UAWEI TSC IIE M ICROSOFT STC I NDIA NICT-K YOTO NICT-RUI N IU T RANS NRC OPPO PROMT SJTU-NICT SRPOL TALP UPC T ENCENT T RANSLATION THUNLP T ILDE T OHOKU -AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans"
2020.wmt-1.1,D19-6301,1,0.888512,"Missing"
2020.wmt-1.1,W18-6424,0,0.0431841,"(Kocmi, 2020) combines transfer learning from a high-resource language pair Czech–English into the low-resource Inuktitut-English with an additional backtranslation step. Surprising behaviour is noticed when using synthetic data, which can be possibly attributed to a narrow domain of training and test data. The system is the Transformer model in a constrained submission. 2.3.3 Charles University (CUNI) CUNI-D OC T RANSFORMER (Popel, 2020) is similar to the sentence-level version (CUNI-T2T2018, CUBBITT), but trained on sequences with multiple sentences of up to 3000 characters. CUNI-T2T-2018 (Popel, 2018), also called CUBBITT, is exactly the same system as in WMT2018. It is the Transformer model trained according to Popel and Bojar (2018) plus a novel concat-regime backtranslation with checkpoint averaging (Popel et al., 2020), tuned separately for CZ-domain and non CZ-domain articles, possibly handling also translation-direction (“translationese”) issues. For cs→en also a coreference preprocessing was used adding the female-gender CUNI-T RANSFORMER (Popel, 2020) is similar to the WMT2018 version of CUBBITT, but with 12 encoder layers instead of 6 and trained on CzEng 2.0 instead of CzEng 1.7."
2020.wmt-1.1,2020.wmt-1.25,0,0.094349,"Missing"
2020.wmt-1.1,2020.wmt-1.28,0,0.0792624,"cument in the test set, and the origlang tag indicates the original source language. 9 Team AFRL ARIEL XV CUNI DCU D EEP M IND D I D I -NLP DONG - NMT ENMT E T RANSLATION FACEBOOK AI G RONINGEN GTCOM H ELSINKI NLP H UAWEI TSC IIE M ICROSOFT STC I NDIA NICT-K YOTO NICT-RUI N IU T RANS NRC OPPO PROMT SJTU-NICT SRPOL TALP UPC T ENCENT T RANSLATION THUNLP T ILDE T OHOKU -AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 20"
2020.wmt-1.1,2020.lrec-1.443,1,0.79707,"er their models were trained only on the provided data. Since we do not know how they were built, the online systems are treated as unconstrained during the automatic and human evaluations. In the rest of this section, we provide brief details of the submitted systems, for those where the authors provided such details. The training corpus for Inuktitut↔English is the recently released Nunavut Hansard Inuktitut– English Parallel Corpus 3.0 (Joanis et al., 2020). For the Japanese↔English tasks, we added several freely available parallel corpora to the training data. It includes JParaCrawl v2.0 (Morishita et al., 2020), a large web-based parallel corpus, Japanese-English Subtitle Corpus (JESC) (Pryzant et al., 2017), the Kyoto Free Translation Task (KFTT) corpus (Neubig, 2011), constructed from the Kyoto-related Wikipedia articles, and TED Talks (Cettolo et al., 2012). The monolingual data we provided was similar to last year’s, with a 2019 news crawl added to all the news corpora. In addition, we provided versions of the news corpora for Czech, English and German, with both the document and paragraph structure retained. In other words, we did not apply sentence splitting to these corpora, and we retained t"
2020.wmt-1.1,2020.wmt-1.48,0,0.090424,"Missing"
2020.wmt-1.1,2020.wmt-1.49,0,0.0519886,"Missing"
2020.wmt-1.1,W19-6712,0,0.0573476,"tly crawled multilingual parallel corpora from Indian government websites (Haddow and Kirefu, 2020; Siripragada et al., 2020), the Tanzil corpus (Tiedemann, 2009), the Pavlick dicParagraph-split Test Sets For the language pairs English↔Czech, English↔German and English→Chinese, we provided the translators with paragraph-split texts, instead of sentence-split texts. We did this in order to provide the translators with greater freedom and, hopefully, to improve the quality of the translation. Allowing translators to merge and split sentences removes one of the “translation shifts” identified by Popovic (2019), which can make translations create solely for MT evaluation different from translations produced for other purposes. We first show some descriptive statistics of the source texts, for Czech, English and German, in 3 Europarl Parallel Corpus Czech ↔ English German ↔ English Polish↔ English German ↔ French Sentences 645,241 1,825,745 632,435 1,801,076 Words 14,948,900 17,380,340 48,125,573 50,506,059 14,691,199 16,995,232 47,517,102 55,366,136 Distinct words 172,452 63,289 371,748 113,960 170,271 62,694 368,585 134,762 News Commentary Parallel Corpus Czech ↔ English 248,927 5,570,734 6,156,063"
2020.wmt-1.1,2020.wmt-1.26,0,0.0845151,"Missing"
2020.wmt-1.1,2020.vardial-1.10,0,0.0933804,"Missing"
2020.wmt-1.1,W18-6301,0,0.038239,"Missing"
2020.wmt-1.1,2020.wmt-1.51,0,0.0917045,"Missing"
2020.wmt-1.1,2020.wmt-1.50,1,0.78567,"Missing"
2020.wmt-1.1,2020.wmt-1.52,0,0.0485419,"Missing"
2020.wmt-1.1,P19-1164,0,0.0581517,"26 26.37 25.51 24.82 28.33 23.33 21.13 21.96 20.43 22.90 22.58 21.90 22.17 22.17 20.53 19.40 20.01 40.44 32.39 30.39 37.04 32.27 27.54 25.97 26.09 46.38 37.30 36.05 35.96 33.76 33.07 27.20 27.07 Table 15: TICO-19 test suite results on the English-to-X WMT20 translation directions. 26 4.1.5 antecedent (a less common direction of information flow), and then correctly express the noun in the target language. The success of the MT system in this test can be established automatically, whenever the gender of the target word can be automatically identified. Kocmi et al. (2020) build upon the WinoMT (Stanovsky et al., 2019) test set, which provides exactly the necessary type of sentences containing an ambiguous profession noun and a personal pronoun which unambiguously (for the human eye) refers to it based the situation described. When extending WinMT with Czech and Polish, Stanovsky et al. have to disregard some test patterns but the principle remains. The results indicate that all MT systems fail in this test, following gender bias (stereotypical patterns attributing the masculine gender to some professions and feminine gender to others) rather than the coreference link. Word Sense Disambiguation (Scherrer et"
2020.wmt-1.1,2020.wmt-1.31,0,0.0881792,"morphological segmentation of the polysynthetic Inuktitut, testing rule-based, supervised, semi-supervised as well as unsupervised word segmentation methods, (2) whether or not adding data from a related language (Greenlandic) helps, and (3) whether contextual word embeddings (XLM) improve translation. G RONINGEN - ENIU use Transformer implemented in Marian with the default setting, improving the performance also with tagged backtranslation, domain-specific data, ensembling and finetuning. 2.3.7 DONG - NMT (no associated paper) No description provided. 2.3.8 ENMT (Kim et al., 2020) Kim et al. (2020) base their approach on transferring knowledge of domain and linguistic characteristics by pre-training the encoder-decoder model with large amount of in-domain monolingual data through unsupervised and supervised prediction task. The model is then fine-tuned with parallel data and in-domain synthetic data, generated with iterative back-translation. For additional gain, final results are generated with an ensemble model and re-ranked with averaged models and language models. G RONINGEN - ENTAM (Dhar et al., 2020) study the effects of various techniques such as linguistically motivated segmenta"
2020.wmt-1.1,2020.wmt-1.32,0,0.0839317,"- NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) SJTU-NICT (Li et al., 2020) Samsung Research Poland (Krubi´nski et al., 2020) TALP UPC (Escolano et al., 2020) Tencent Translation (Wu et al., 2020b) NLP Lab at Tsinghua University (no associated paper) Tilde (Krišlauks and Pinnis, 2020) Tohoku-AIP-NTT (Kiyono et al., 2020) Ub"
2020.wmt-1.1,2020.wmt-1.33,0,0.080166,"Missing"
2020.wmt-1.1,2020.wmt-1.34,0,0.0803867,"Missing"
2020.wmt-1.1,2020.wmt-1.35,0,0.0951745,"ss web site (JW300). The development and test sets were created as part of the Flores initiative (Guzmán et al., 2019) by professional translation of Wikipedia content with careful vetting of the translations. Please refer the to the Parallel Corpus Filtering Shared Task overview paper for details on these corpora. 2.3.1 AFRL (Gwinnup and Anderson, 2020) AFRL - SYSCOMB 20 is a system combination consisting of two Marian transformer ensembles, one OpenNMT transformer system and a Moses phrase-based system. AFRL - FINETUNE is an OpenNMT transformer system fine-tuned on newstest2014-2017. 2.3.2 (Xv, 2020) ARIEL XV is a Transformer base model trained with the Sockeye sequence modeling toolkit usSome statistics about the training and test materials are given in Figures 1, 2, 3 and 4. 4 8 ARIEL XV https://github.com/AppraiseDev/OCELoT English I English II English III Chinese Czech German Inuktitut Japanese Polish Russian Tamil ABC News (2), All Africa (5), Brisbane Times (1), CBS LA (1), CBS News (1), CNBC (3), CNN (2), Daily Express (1), Daily Mail (2), Fox News (1), Gateway (1), Guardian (3), Huffington Post (2), London Evening Standard (2), Metro (2), NDTV (7), RTE (7), Reuters (4), STV (2), S"
2020.wmt-1.1,2020.wmt-1.55,0,0.0877526,"Missing"
2020.wmt-1.1,P17-4012,0,0.0273892,"o SJTU-NICT using large XLM model to improve NMT but the exact relation is unclear. 2.3.14 H UAWEI TSC (Wei et al., 2020a) H UAWEI TSC use Transformer-big with a further increased model size, focussing on standard techniques of careful pre-processing and filtering, back-translation and forward translation, including self-training, i.e. translating one of the sides of the original parallel data. Ensembling of individual training runs is used in the forward as well as backward translation, and single models are created from the ensembles using knowledge distillation. The submission uses THUNMT (Zhang et al., 2017) open-source engine. 2.3.19 N IU T RANS (Zhang et al., 2020) N IU T RANS gain their performance from focussed attention to six areas: (1) careful data preprocessing and filtering, (2) iterative back-translation to generate additional training data, (3) using different model architectures, such as wider and/or deeper models, relative position representation and relative length, to enhance the diversity of translations, (4) iterative knowledge distillation by in-domain monolingual data, (5) iterative finetuning for domain adaptation using small training batches, (6) rule-based post-processing of"
2020.wmt-1.1,P98-2238,0,0.590812,"and punctuation, and we tend to attribute all these improvements to increased capacity (which allows increased sensitivity to long-range relations) of the models. 5 Similar Language Translation Most shared tasks at WMT (e.g. News, Biomedical) have historically dealt with translating texts from and to English. In recent years, we observed a growing interest in training systems to translate between languages other than English. This includes a number of papers applying MT to translate between pairs of closely-related languages, national language varieties, and dialects 27 of the same language (Zhang, 1998; Marujo et al., 2011; Hassani, 2017; Costa-jussà et al., 2018; Popovi´c et al., 2020). To address this topic, the first Similar Language Translation (SLT) shared task at WMT 2019 has been organized. It featured data from three pairs of closely-related languages from different language families: Spanish - Portuguese (Romance languages), Czech - Polish (Slavic languages), and Hindi - Nepali (IndoAryan languages). Following the success of the first SLT shared task at WMT 2019 and the interest of the community in this topic, we organize, for the second time at WMT, this shared task to evaluate th"
2020.wmt-1.1,2020.wmt-1.41,1,0.814291,"Missing"
2020.wmt-1.108,W19-5433,0,0.0190866,"e decided not to use it in our final submission. We also ran the same experiments using fasttext generated embeddings (Bojanowski et al., 2017) but that had poor results as well. 5 Monolingual Language Models The motivation behind using Monolingual Language Models is that we want to learn about sentence pairs that have a high likelihood of coming up in the test data. Ideally we would want both the sides of the corpus to have a high probability of coming up but we also realize it will often not be the case. Thus, we make these language models independent of each other. We take inspiration from Axelrod et al. (2019) and modify the work of Junczys-Dowmunt (2018) to come up with a language model filter. Junczys-Dowmunt (2018) achieved the highest ranking score in WMT’18 and to do so they define HM (.|.) as the wordnormalized conditional cross-entropy of the probability distribution PM (.|.) for a model M: –arch transformer lm –dropout 0.1 –optimizer adam –adam-betas ’(0.9, 0.98)’ –weight-decay 0.01 –clip-norm 0.0 –lr 0.0005 –lr-scheduler inverse sqrt –warmup-updates 4000 –warmup-init-lr 1e-07 –patience 30 Figure 2: Language Model: Transformer architecture –arch transformer lm wiki103 –max-lr 1.0 –t-mult 2"
2020.wmt-1.108,Q17-1010,0,0.00548544,"any explicitly stated rule. • Being more aggressive with the number of loops led to a drastic decrease in BLEU scores. We finally ran experiments where we gave a preference to only large norms and an experiment where we gave a preference to both very large norms and to very small norms. In both the cases we had really bad scores leading to the conclusion that the norm of LASER embeddings is not a good 961 filter. Because of this poor performance when relied on aggressively, we decided not to use it in our final submission. We also ran the same experiments using fasttext generated embeddings (Bojanowski et al., 2017) but that had poor results as well. 5 Monolingual Language Models The motivation behind using Monolingual Language Models is that we want to learn about sentence pairs that have a high likelihood of coming up in the test data. Ideally we would want both the sides of the corpus to have a high probability of coming up but we also realize it will often not be the case. Thus, we make these language models independent of each other. We take inspiration from Axelrod et al. (2019) and modify the work of Junczys-Dowmunt (2018) to come up with a language model filter. Junczys-Dowmunt (2018) achieved th"
2020.wmt-1.108,W19-5435,1,0.910595,"Missing"
2020.wmt-1.108,W18-6478,0,0.0805722,"on. We also ran the same experiments using fasttext generated embeddings (Bojanowski et al., 2017) but that had poor results as well. 5 Monolingual Language Models The motivation behind using Monolingual Language Models is that we want to learn about sentence pairs that have a high likelihood of coming up in the test data. Ideally we would want both the sides of the corpus to have a high probability of coming up but we also realize it will often not be the case. Thus, we make these language models independent of each other. We take inspiration from Axelrod et al. (2019) and modify the work of Junczys-Dowmunt (2018) to come up with a language model filter. Junczys-Dowmunt (2018) achieved the highest ranking score in WMT’18 and to do so they define HM (.|.) as the wordnormalized conditional cross-entropy of the probability distribution PM (.|.) for a model M: –arch transformer lm –dropout 0.1 –optimizer adam –adam-betas ’(0.9, 0.98)’ –weight-decay 0.01 –clip-norm 0.0 –lr 0.0005 –lr-scheduler inverse sqrt –warmup-updates 4000 –warmup-init-lr 1e-07 –patience 30 Figure 2: Language Model: Transformer architecture –arch transformer lm wiki103 –max-lr 1.0 –t-mult 2 –lr-scheduler cosine –lr-shrink 0.75 –warmup-i"
2020.wmt-1.108,W18-2709,1,0.823671,"s for both Pashto-English and Khmer-English systems combining multiple techniques like monolingual language model scores, length based filters, language ID filters with confidence and norm of embedings. 1 Introduction For this task the participants were provided with a corpus of parallel data in Pashto-English (ps-en) and Khmer-English (km-en). Additional parallel and monolingual datasets were also provided. The task organizers built neural machine translation (NMT) systems from the scores produced, based on parallel training sets of 5 million words. These systems are sensitive towards noise (Khayrallah and Koehn, 2018) and thus, it becomes important to separate the useful data from the noise. We view the task as data that passes through a pipeline of filters in order to give us the best possible selection of 5 million words in the end. We determined that language ID filtering is a very strong pre-processing filter and inducting confidence scores is not needed. We also determined that monolingual Language models can help us in selecting sentences even if both the source and target language models are independent of each other. Finally, using using the length of a sentence as a filter helps us create a better"
2020.wmt-1.108,W19-5404,1,0.895659,"Missing"
2020.wmt-1.108,D18-2012,0,0.0238026,"he parallel corpus. This method does not leverage the simple fact that we have more data for one of the languages. This method also assumes a close relationship between the frequencies of letters which might not always be the case. Finally it does not leverage the expressive power of neural language models. In order to improve on this, we propose Neural Language Models that are completely independent of each other. 5.1 to simulate right-to-left prediction of words. Thus we have 8 possible tokenizations for every possible sentence. Pre-processing We first tokenize our data using Sentencepiece (Kudo and Richardson, 2018). We set an upper limit of 5,000 on the vocabulary size for Pashto and Khmer, and an upper limit of 50,000 for the English vocabulary. This is done at both the character and word level and also both with and without splitting at whitespace. We also reverse the data 962 5.2 Language Model Architecture For our Language Models, we use fairseq (Ott et al., 2019) to implement the architecture given in Figure 2. We also create a Language Model using the architecture given by Baevski and Auli (2019) with parameters as given in Figure 3. We use 2 different models because while it is tempting to use de"
2020.wmt-1.108,2020.acl-main.41,0,0.0519745,"Missing"
2020.wmt-1.108,N19-4009,0,0.031481,"Missing"
2020.wmt-1.109,W19-5435,1,0.900378,"Missing"
2020.wmt-1.109,W18-6478,0,0.0351992,"or training. Neural machine translation models in particular have been found to both require more data (Koehn and Knowles, 2017), and be more sensitive to noise in training data (Khayrallah and Koehn, 2018) than statistical machine translation models. While these data can be acquired from online sources, the resulting crawled texts are often noisy and require filtering to produce large amounts of sufficiently clean training data. 2 Rule-based Filtering The most successful scoring method in the WMT18 Shared Task on Parallel Corpus Filtering was Dual Conditional Cross Entropy Filtering (dccef) (Junczys-Dowmunt, 2018). This method trains an NMT model in both translation directions, uses these to calculate the cross-entropy for each sentence, and finally produces a score based on their agreement. As this year’s task deals with lowresource languages (contrary to WMT18, which was En-De), we explore a method to bootstrap the available clean data, thus producing more training data for the intermediate NMT models required for the method (described in more detail subsection 5.2). 2.3 Related Work We refer readers to (Koehn et al., 2019) for a more detailed overview of methods for parallel corpus filtering, here w"
2020.wmt-1.109,W17-3204,1,0.839587,"lexity scores from language models, and DCCEF DUP, dual conditional cross entropy scores combined with a duplication penalty. We improve slightly on the LASER similarity score and find that the provided clean data can successfully be supplemented with a subsampled set of the noisy data, effectively increasing the training data for the models used for dual conditional cross entropy scoring. 1 2.2 Introduction Machine translation systems require large amounts of high quality parallel corpora for training. Neural machine translation models in particular have been found to both require more data (Koehn and Knowles, 2017), and be more sensitive to noise in training data (Khayrallah and Koehn, 2018) than statistical machine translation models. While these data can be acquired from online sources, the resulting crawled texts are often noisy and require filtering to produce large amounts of sufficiently clean training data. 2 Rule-based Filtering The most successful scoring method in the WMT18 Shared Task on Parallel Corpus Filtering was Dual Conditional Cross Entropy Filtering (dccef) (Junczys-Dowmunt, 2018). This method trains an NMT model in both translation directions, uses these to calculate the cross-entrop"
2020.wmt-1.109,2020.tacl-1.47,0,0.0364079,"each of the sentence pairs in the provided noisy 58.3 million-word (English token count) Khmer-English corpus and 11.6 million-word Pashto-English corpus. These scores are used to subsample sentence pairs amounting to 5 million English words. The resulting subset is evaluated by the quality of an NMT system (fairseq (Ott et al., 2019)) trained on this data. Participants were given the scripts to either train the evaluation system from scratch, or use the data to fine-tune a provided pretrained MBART model. The MBART model was trained on monolingual data, the details of which are described in (Liu et al., 2020). The performance of the NMT system is measured by BLEU score on a held-out test set of Wikipedia translations. Participants may also provide re-alignments of the source and target sentences. The organizers provide clean parallel and monolingual data for both of the language pairs, as well as LASER similarity scores, a previously successful method in low-resource conditions (Chaudhary et al., 2019), (Koehn et al., 2019). We participated in the Pashto-English track only, after finding that the model-based methods we explored did not produce meaningful scores for Khmer-English. We did not submit"
2020.wmt-1.109,W18-2709,1,0.851669,"ntropy scores combined with a duplication penalty. We improve slightly on the LASER similarity score and find that the provided clean data can successfully be supplemented with a subsampled set of the noisy data, effectively increasing the training data for the models used for dual conditional cross entropy scoring. 1 2.2 Introduction Machine translation systems require large amounts of high quality parallel corpora for training. Neural machine translation models in particular have been found to both require more data (Koehn and Knowles, 2017), and be more sensitive to noise in training data (Khayrallah and Koehn, 2018) than statistical machine translation models. While these data can be acquired from online sources, the resulting crawled texts are often noisy and require filtering to produce large amounts of sufficiently clean training data. 2 Rule-based Filtering The most successful scoring method in the WMT18 Shared Task on Parallel Corpus Filtering was Dual Conditional Cross Entropy Filtering (dccef) (Junczys-Dowmunt, 2018). This method trains an NMT model in both translation directions, uses these to calculate the cross-entropy for each sentence, and finally produces a score based on their agreement. As"
2020.wmt-1.109,N19-4009,0,0.15899,"s work attempts to augment LASER similarity scores with language model scores (described in more detail in subsection 4). 3 Shared Task For this year’s shared task on Parallel Corpus Filtering and Alignment for Low-Resource conditions, participants are asked to produce scores for each of the sentence pairs in the provided noisy 58.3 million-word (English token count) Khmer-English corpus and 11.6 million-word Pashto-English corpus. These scores are used to subsample sentence pairs amounting to 5 million English words. The resulting subset is evaluated by the quality of an NMT system (fairseq (Ott et al., 2019)) trained on this data. Participants were given the scripts to either train the evaluation system from scratch, or use the data to fine-tune a provided pretrained MBART model. The MBART model was trained on monolingual data, the details of which are described in (Liu et al., 2020). The performance of the NMT system is measured by BLEU score on a held-out test set of Wikipedia translations. Participants may also provide re-alignments of the source and target sentences. The organizers provide clean parallel and monolingual data for both of the language pairs, as well as LASER similarity scores,"
2020.wmt-1.109,W18-6487,0,0.029845,"Missing"
2020.wmt-1.109,W18-6488,0,0.0560554,"Missing"
2020.wmt-1.4,abdelali-etal-2014-amara,1,0.827823,"ne translated all comments using an in-house transformer-based model into Japanese and German. The goal of that was to be able to examine potential differences in source and (one example of) translation segments.3 We then pre-processed and automatically annotated all 17K segments with the following soft labels for catastrophic errors: Development Data The task specified the following data to help participants evaluate their system’s performance on unseen and multiple domains. • English-German: participants can use the development data from the News translation task, development data from QED (Abdelali et al., 2014) corpus, development data from WMT19 Medical translation task, and development data from the WMT16 IT translation task. 1. Introduction of toxicity: we checked both source and machine translation for toxic words (using in-house lists) and labelled as positive (i.e. potentially containing errors) cases where the source does not contain such words, but the translation does (at least one). • English-Japanese: participants can use the development data from the News translation task, and development data from the MTNT dataset, which contains noisy social media texts and their clean translations. 3."
2020.wmt-1.4,D17-1158,0,0.0130333,"Another approach trains a system on multiple domains at the same time, while adding domain-specific tags to the input examples (Kobus et al., 2016). Both these approaches were employed by participants of the first shared task on MT robustness (Li et al., 2019). Other methods for domain adaptation of MT systems include instance weighting (Wang et al., 2017b), incorporating a domain classifier (Chen et al., 2017; Britz et al., 2017), and data selection (Wang et al., 2017a). Some make use of monolingual data available either in the target domain—for example by training the decoder on such data (Domhan and Hieber, 2017) or by backtranslating it (Sennrich et al., 2016)—or in the source domain, via similar techniques (Zhang and Zong, 2016). Chu and Wang (2018) provide a broad survey of domain adaptation for neural MT, which demonstrates that the predominant setup assumes knowledge of the target domain and availability of target domain data at training time. In light of this prior work, the shared task proposed a relatively underexplored scenario, where examples in the target domain are either unavailable or relatively few. Other aspects of robustness are robustness to adversarial examples or noisy inputs. The"
2020.wmt-1.4,N19-1311,0,0.0437239,"Missing"
2020.wmt-1.4,P18-1128,0,0.014933,"Naver Labs and LIMSI made specific efforts towards the task of Robustness. Both of them used lightweight domain adaptors proposed by Bapna and Firat (2019). Both teams UEDIN: Team UEDIN also mainly trained their system towards News translation task, but added Gumbel noise to the output layer of the systems submitted to the Robustness task. They followed standard NMT training pipeline and boasted their systems with additional data filtered from the paracrawl corpus. The data was carefully selected using dual cross-entropy (Junczys-Dowmunt, 2018) and length-normalized cross-entropy. 81 script6 (Dror et al., 2018) with p <0.05. The result of significance test in likert score is used for the human judgement ranking. Interestingly, the correlation in the system rankings between human judgments and BLEU is not strong. In other words, the best performing systems in BLEU do not rank high according human judgement, sometimes even rank the lowest. For example, in Ja→En (set2), the online-B system ranks first in BLEU but last in likert score. OPPO outperforms all systems in both directions on set2, and is overall the best system among the constrained, zero-shot submissions. To get insight on the proportion of"
2020.wmt-1.4,2020.lrec-1.520,0,0.0937737,"raped, filtered for noisy comments and translated by professional translators. This year, data was collected for two translation directions: English→Japanese and Japanese→English. For English, comments were collected from the /r/all feed, which encompasses all communities, and filtered for English. Since Japanese is a minority language on Reddit, comments were scraped from a selection of japanese-speaking communities, detailed in Michel and Neubig (2018). Common Voices Test Set (set3): This data was obtained from from the CoVoST corpus (Wang et al., 2020). CoVoST is derived from Common Voice (Ardila et al., 2020), a crowdsourced speech recognition corpus with an open CC0 license. Transcripts were sent to professional translators and the quality of translations was controlled by automatic and manual checks (Guzm´an et al., 2019). For this task, we used the German→English test set with source German sentences deduplicated. 1 Bad: translation errors are so severe that they cause the target text to be incomprehensible. This may be mainly due to major grammatical, typographical, or lexical errors, or omission of critical or important salient information. 2 Poor: the target text contains translation errors,"
2020.wmt-1.4,N19-1154,1,0.82504,"luation and the results discussed in Section 5. We hope that this task leads to more efforts from the community in building robust MT models. Introduction In recent years, Machine Translation (MT) systems have seen great progress, with neural models becoming the de-facto methods and even approaching human quality in news domain (Hassan et al., 2018). However, like other deep learning models, neural machine translation (NMT) models are found to be sensitive to synthetic and natural noise in input, distributional shift, and adversarial examples (Koehn and Knowles, 2017; Belinkov and Bisk, 2018; Durrani et al., 2019; Anastasopoulos et al., 2019; Michel et al., 2019). From an application perspective, MT systems need to deal with non-standard, noisy text of the kind which is ubiquitous on social media and the internet, yet has different distributional signatures from corpora in common benchmark datasets. Following the first shared task on Machine Translation (MT) Robustness, we now propose a second edition, which aims at testing MT systems’ robustness towards domain diversity. Specifically, this year’s task aims to evaluate a general MT system’s performance in the following two scenarios: 2 Related Work Do"
2020.wmt-1.4,D19-1165,0,0.282315,"n a critical way. Critical errors are those that lead to misleading translations which may carry religious, health, safety, legal or financial implications, or introduce toxicity. The set of critical errors used for the guidelines (which also included examples of these errors) includes – but is not limited to – the cases below: Naver Labs (NLE): They participated in Chat and Biomedical tasks along with the Robustness task. They trained a general big-transformer model using FairSeq toolkit (Ott et al., 2019) and adapted it towards different tasks using lightweight adapter layers for each task (Bapna and Firat, 2019). They compared results against the more traditional finetuning method (Luong and Manning, 2015) to show that the former provides a viable alternative, while significantly reducing the amount of parameters per task. They also explored using embedding from pre-trained language models in their NMT system of which they tried two MLM variants: i) using NMT encoder’s setting, using Roberta (Liu et al., 2019). The latter was found more robust to novel domains and noise. The authors found that initializing with first 8 layers instead of the entire model to 80 OPPO: Team OPPO also trained their system"
2020.wmt-1.4,D19-1632,1,0.894086,"Missing"
2020.wmt-1.4,W17-4712,0,0.0183942,"ne-tune on datasets increasingly similar to the target domain (Sajjad et al., 2017), or to dynamically change the balance of data towards the target domain (van der Wees et al., 2017). Another approach trains a system on multiple domains at the same time, while adding domain-specific tags to the input examples (Kobus et al., 2016). Both these approaches were employed by participants of the first shared task on MT robustness (Li et al., 2019). Other methods for domain adaptation of MT systems include instance weighting (Wang et al., 2017b), incorporating a domain classifier (Chen et al., 2017; Britz et al., 2017), and data selection (Wang et al., 2017a). Some make use of monolingual data available either in the target domain—for example by training the decoder on such data (Domhan and Hieber, 2017) or by backtranslating it (Sennrich et al., 2016)—or in the source domain, via similar techniques (Zhang and Zong, 2016). Chu and Wang (2018) provide a broad survey of domain adaptation for neural MT, which demonstrates that the predominant setup assumes knowledge of the target domain and availability of target domain data at training time. In light of this prior work, the shared task proposed a relatively u"
2020.wmt-1.4,W17-3205,0,0.0203483,"to continuously fine-tune on datasets increasingly similar to the target domain (Sajjad et al., 2017), or to dynamically change the balance of data towards the target domain (van der Wees et al., 2017). Another approach trains a system on multiple domains at the same time, while adding domain-specific tags to the input examples (Kobus et al., 2016). Both these approaches were employed by participants of the first shared task on MT robustness (Li et al., 2019). Other methods for domain adaptation of MT systems include instance weighting (Wang et al., 2017b), incorporating a domain classifier (Chen et al., 2017; Britz et al., 2017), and data selection (Wang et al., 2017a). Some make use of monolingual data available either in the target domain—for example by training the decoder on such data (Domhan and Hieber, 2017) or by backtranslating it (Sennrich et al., 2016)—or in the source domain, via similar techniques (Zhang and Zong, 2016). Chu and Wang (2018) provide a broad survey of domain adaptation for neural MT, which demonstrates that the predominant setup assumes knowledge of the target domain and availability of target domain data at training time. In light of this prior work, the shared task pr"
2020.wmt-1.4,P17-2061,0,0.0561223,"Missing"
2020.wmt-1.4,C18-1111,0,0.0137039,"6). Both these approaches were employed by participants of the first shared task on MT robustness (Li et al., 2019). Other methods for domain adaptation of MT systems include instance weighting (Wang et al., 2017b), incorporating a domain classifier (Chen et al., 2017; Britz et al., 2017), and data selection (Wang et al., 2017a). Some make use of monolingual data available either in the target domain—for example by training the decoder on such data (Domhan and Hieber, 2017) or by backtranslating it (Sennrich et al., 2016)—or in the source domain, via similar techniques (Zhang and Zong, 2016). Chu and Wang (2018) provide a broad survey of domain adaptation for neural MT, which demonstrates that the predominant setup assumes knowledge of the target domain and availability of target domain data at training time. In light of this prior work, the shared task proposed a relatively underexplored scenario, where examples in the target domain are either unavailable or relatively few. Other aspects of robustness are robustness to adversarial examples or noisy inputs. The fragility of neural MT models has been previously demonstrated in various settings (Belinkov and Bisk, 2018; Heigold et al., 2017; Anastasopo"
2020.wmt-1.4,W18-6478,0,0.0263186,"different models to obtain further improvements. Only two teams, namely Naver Labs and LIMSI made specific efforts towards the task of Robustness. Both of them used lightweight domain adaptors proposed by Bapna and Firat (2019). Both teams UEDIN: Team UEDIN also mainly trained their system towards News translation task, but added Gumbel noise to the output layer of the systems submitted to the Robustness task. They followed standard NMT training pipeline and boasted their systems with additional data filtered from the paracrawl corpus. The data was carefully selected using dual cross-entropy (Junczys-Dowmunt, 2018) and length-normalized cross-entropy. 81 script6 (Dror et al., 2018) with p <0.05. The result of significance test in likert score is used for the human judgement ranking. Interestingly, the correlation in the system rankings between human judgments and BLEU is not strong. In other words, the best performing systems in BLEU do not rank high according human judgement, sometimes even rank the lowest. For example, in Ja→En (set2), the online-B system ranks first in BLEU but last in likert score. OPPO outperforms all systems in both directions on set2, and is overall the best system among the cons"
2020.wmt-1.4,P17-4012,0,0.0234194,"the decoder. This allows the test sets from known domains to use adapter layers and for novel domains to use the generic system. They created a noisy domain by adding synthetic noise to source data. The idea is that residual adapter layer learned from such data learns how to deal with noisy domain and is also able to preserve performance on the cleaner domains. However this did not work as well. The residual adapter fine-tuned using the ParaCrawl corpus gave better performance. PROMPT: Team PROMPT also participated mainly in the News translation task. Their systems were trained using OpenNMT (Klein et al., 2017) toolkit. They applied several stages of data preprocessing including length-based filtering, removing duplications, and using in-house classifier based on Hunalign aligner to identify and discard non-parallel sentences. They used two types of synthetic data to improve their systems: i) randomly selecting subset of Wikipedia equal to the size of news data and generating parallel corpus through back-translation, ii) creating synthetic data with unknown words using the procedure described in (Pinnis et al., 2017). Systems were trained with tags to differentiate between original data and syntheti"
2020.wmt-1.4,P02-1040,0,0.114721,"l can bias the selection to cases that are challenging for this particular model. In future work following this methodology, we recommend that multiple MT models be used. 4 https://cloud.google. com/natural-language/docs/ analyzing-sentiment 5 https://github.com/carpedm20/emoji/) 2 www.kaggle.com/c/ jigsaw-toxic-comment-classification- challenge 78 3.5 5. Presence of idioms: we checked if the source contains idiomatic expressions, using an inhouse list of idioms built from various sources, and labelled those cases as positive. Evaluation protocol Automatic evaluation: We first computed BLEU (Papineni et al., 2002) for each system using SacreBLEU (Post, 2018). For all language pairs except En→Ja, we used the original reference and SacreBLEU with the default options. In the case of En→Ja, we used the reference tokenized with KyTea and the option --tokenize none. We note that the automatic labelling using our various pre-processing techniques may have introduced errors, but we believe that basing the selection on such heuristics will still lead to higher chances of selecting very challenging source segments than arbitrarily sampling the data. Human evaluation: The system outputs were evaluated by professi"
2020.wmt-1.4,W17-3204,1,0.837046,"e evaluated both automatically and via a human evaluation and the results discussed in Section 5. We hope that this task leads to more efforts from the community in building robust MT models. Introduction In recent years, Machine Translation (MT) systems have seen great progress, with neural models becoming the de-facto methods and even approaching human quality in news domain (Hassan et al., 2018). However, like other deep learning models, neural machine translation (NMT) models are found to be sensitive to synthetic and natural noise in input, distributional shift, and adversarial examples (Koehn and Knowles, 2017; Belinkov and Bisk, 2018; Durrani et al., 2019; Anastasopoulos et al., 2019; Michel et al., 2019). From an application perspective, MT systems need to deal with non-standard, noisy text of the kind which is ubiquitous on social media and the internet, yet has different distributional signatures from corpora in common benchmark datasets. Following the first shared task on Machine Translation (MT) Robustness, we now propose a second edition, which aims at testing MT systems’ robustness towards domain diversity. Specifically, this year’s task aims to evaluate a general MT system’s performance in"
2020.wmt-1.4,W18-6459,0,0.0133196,"rvey of domain adaptation for neural MT, which demonstrates that the predominant setup assumes knowledge of the target domain and availability of target domain data at training time. In light of this prior work, the shared task proposed a relatively underexplored scenario, where examples in the target domain are either unavailable or relatively few. Other aspects of robustness are robustness to adversarial examples or noisy inputs. The fragility of neural MT models has been previously demonstrated in various settings (Belinkov and Bisk, 2018; Heigold et al., 2017; Anastasopoulos et al., 2019; Lee et al., 2018). Michel and Neubig (2018) proposed a new dataset (MTNT) to test MT models for robustness to the types of noise encountered in the Internet. The previous iteration of the shared task focused on robustness of MT systems to such noise (Li et al., 2019). We refer to that report for more details. 3 participants to explore novel training and modeling approaches so that models have more robust performance at test time on multiple domains, including unseen and diversified domains. We offer two language pairs: English-German (En→De) and English-Japanese (En→Ja), with different test sets focusing on on"
2020.wmt-1.4,W19-5303,1,0.901081,"e 5th Conference on Machine Translation (WMT), pages 76–91 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics et al., 2017), to continuously fine-tune on datasets increasingly similar to the target domain (Sajjad et al., 2017), or to dynamically change the balance of data towards the target domain (van der Wees et al., 2017). Another approach trains a system on multiple domains at the same time, while adding domain-specific tags to the input examples (Kobus et al., 2016). Both these approaches were employed by participants of the first shared task on MT robustness (Li et al., 2019). Other methods for domain adaptation of MT systems include instance weighting (Wang et al., 2017b), incorporating a domain classifier (Chen et al., 2017; Britz et al., 2017), and data selection (Wang et al., 2017a). Some make use of monolingual data available either in the target domain—for example by training the decoder on such data (Domhan and Hieber, 2017) or by backtranslating it (Sennrich et al., 2016)—or in the source domain, via similar techniques (Zhang and Zong, 2016). Chu and Wang (2018) provide a broad survey of domain adaptation for neural MT, which demonstrates that the predomin"
2020.wmt-1.4,W18-6319,0,0.0156248,"or this particular model. In future work following this methodology, we recommend that multiple MT models be used. 4 https://cloud.google. com/natural-language/docs/ analyzing-sentiment 5 https://github.com/carpedm20/emoji/) 2 www.kaggle.com/c/ jigsaw-toxic-comment-classification- challenge 78 3.5 5. Presence of idioms: we checked if the source contains idiomatic expressions, using an inhouse list of idioms built from various sources, and labelled those cases as positive. Evaluation protocol Automatic evaluation: We first computed BLEU (Papineni et al., 2002) for each system using SacreBLEU (Post, 2018). For all language pairs except En→Ja, we used the original reference and SacreBLEU with the default options. In the case of En→Ja, we used the reference tokenized with KyTea and the option --tokenize none. We note that the automatic labelling using our various pre-processing techniques may have introduced errors, but we believe that basing the selection on such heuristics will still lead to higher chances of selecting very challenging source segments than arbitrarily sampling the data. Human evaluation: The system outputs were evaluated by professional translators. The translators were presen"
2020.wmt-1.4,E17-2045,0,0.0383395,"o domain shift assume the existence of significant amounts of parallel data in both the source and target domain. In this scenario, a common approach is to first train an MT system on a (generic) source domain and then to fine-tune it on a (specific) target domain (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016; Servan et al., 2016; Chu 76 Proceedings of the 5th Conference on Machine Translation (WMT), pages 76–91 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics et al., 2017), to continuously fine-tune on datasets increasingly similar to the target domain (Sajjad et al., 2017), or to dynamically change the balance of data towards the target domain (van der Wees et al., 2017). Another approach trains a system on multiple domains at the same time, while adding domain-specific tags to the input examples (Kobus et al., 2016). Both these approaches were employed by participants of the first shared task on MT robustness (Li et al., 2019). Other methods for domain adaptation of MT systems include instance weighting (Wang et al., 2017b), incorporating a domain classifier (Chen et al., 2017; Britz et al., 2017), and data selection (Wang et al., 2017a). Some make use of mono"
2020.wmt-1.4,2021.ccl-1.108,0,0.105338,"Missing"
2020.wmt-1.4,P16-1162,0,0.0120732,"ns at the same time, while adding domain-specific tags to the input examples (Kobus et al., 2016). Both these approaches were employed by participants of the first shared task on MT robustness (Li et al., 2019). Other methods for domain adaptation of MT systems include instance weighting (Wang et al., 2017b), incorporating a domain classifier (Chen et al., 2017; Britz et al., 2017), and data selection (Wang et al., 2017a). Some make use of monolingual data available either in the target domain—for example by training the decoder on such data (Domhan and Hieber, 2017) or by backtranslating it (Sennrich et al., 2016)—or in the source domain, via similar techniques (Zhang and Zong, 2016). Chu and Wang (2018) provide a broad survey of domain adaptation for neural MT, which demonstrates that the predominant setup assumes knowledge of the target domain and availability of target domain data at training time. In light of this prior work, the shared task proposed a relatively underexplored scenario, where examples in the target domain are either unavailable or relatively few. Other aspects of robustness are robustness to adversarial examples or noisy inputs. The fragility of neural MT models has been previously"
2020.wmt-1.4,2015.iwslt-evaluation.11,0,0.64331,"ims at testing MT systems’ robustness towards domain diversity. Specifically, this year’s task aims to evaluate a general MT system’s performance in the following two scenarios: 2 Related Work Domain mismatch is a key challenge in machine translation (Koehn and Knowles, 2017). Most approaches for improving robustness of MT systems to domain shift assume the existence of significant amounts of parallel data in both the source and target domain. In this scenario, a common approach is to first train an MT system on a (generic) source domain and then to fine-tune it on a (specific) target domain (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016; Servan et al., 2016; Chu 76 Proceedings of the 5th Conference on Machine Translation (WMT), pages 76–91 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics et al., 2017), to continuously fine-tune on datasets increasingly similar to the target domain (Sajjad et al., 2017), or to dynamically change the balance of data towards the target domain (van der Wees et al., 2017). Another approach trains a system on multiple domains at the same time, while adding domain-specific tags to the input examples (Kobus et al., 2016). Both these approac"
2020.wmt-1.4,N19-1314,1,0.844424,"hope that this task leads to more efforts from the community in building robust MT models. Introduction In recent years, Machine Translation (MT) systems have seen great progress, with neural models becoming the de-facto methods and even approaching human quality in news domain (Hassan et al., 2018). However, like other deep learning models, neural machine translation (NMT) models are found to be sensitive to synthetic and natural noise in input, distributional shift, and adversarial examples (Koehn and Knowles, 2017; Belinkov and Bisk, 2018; Durrani et al., 2019; Anastasopoulos et al., 2019; Michel et al., 2019). From an application perspective, MT systems need to deal with non-standard, noisy text of the kind which is ubiquitous on social media and the internet, yet has different distributional signatures from corpora in common benchmark datasets. Following the first shared task on Machine Translation (MT) Robustness, we now propose a second edition, which aims at testing MT systems’ robustness towards domain diversity. Specifically, this year’s task aims to evaluate a general MT system’s performance in the following two scenarios: 2 Related Work Domain mismatch is a key challenge in machine transla"
2020.wmt-1.4,D18-1050,1,0.940705,"tation for neural MT, which demonstrates that the predominant setup assumes knowledge of the target domain and availability of target domain data at training time. In light of this prior work, the shared task proposed a relatively underexplored scenario, where examples in the target domain are either unavailable or relatively few. Other aspects of robustness are robustness to adversarial examples or noisy inputs. The fragility of neural MT models has been previously demonstrated in various settings (Belinkov and Bisk, 2018; Heigold et al., 2017; Anastasopoulos et al., 2019; Lee et al., 2018). Michel and Neubig (2018) proposed a new dataset (MTNT) to test MT models for robustness to the types of noise encountered in the Internet. The previous iteration of the shared task focused on robustness of MT systems to such noise (Li et al., 2019). We refer to that report for more details. 3 participants to explore novel training and modeling approaches so that models have more robust performance at test time on multiple domains, including unseen and diversified domains. We offer two language pairs: English-German (En→De) and English-Japanese (En→Ja), with different test sets focusing on one or both these language p"
2020.wmt-1.4,2020.lrec-1.517,1,0.814412,"comments from the social media website reddit.com were scraped, filtered for noisy comments and translated by professional translators. This year, data was collected for two translation directions: English→Japanese and Japanese→English. For English, comments were collected from the /r/all feed, which encompasses all communities, and filtered for English. Since Japanese is a minority language on Reddit, comments were scraped from a selection of japanese-speaking communities, detailed in Michel and Neubig (2018). Common Voices Test Set (set3): This data was obtained from from the CoVoST corpus (Wang et al., 2020). CoVoST is derived from Common Voice (Ardila et al., 2020), a crowdsourced speech recognition corpus with an open CC0 license. Transcripts were sent to professional translators and the quality of translations was controlled by automatic and manual checks (Guzm´an et al., 2019). For this task, we used the German→English test set with source German sentences deduplicated. 1 Bad: translation errors are so severe that they cause the target text to be incomprehensible. This may be mainly due to major grammatical, typographical, or lexical errors, or omission of critical or important salient inform"
2020.wmt-1.4,N19-4007,1,0.808507,"as well as an analysis of catastrophic errors (Section 5.2). 5.1 General Quality Overall, the correlation between human judgments and BLEU is not strong. For En→De (set1), the Pearson’s correlation coefficient is 0.97, while for the other four tasks the coefficients are lower, with 0.78, 0.65, 0.52, 0.79 for En→De (set1), Ja→En (set2), En→Ja (set2), and De→En (set3) respectively. Automatic Evaluation The automatic evaluation (BLEU) results of the Shared Task are summarized in Table 2, where we also include the three online translation systems. We performed significance test using compare-mt (Neubig et al., 2019) where systems are considered as significantly different at p <0.05. The result of significance test is used for the automatic evaluation ranking. Overall, the unconstrained online-B system provides strong results and outperforms most systems in the five language pairs, except the De→En (set3) and En→Ja (set1). Among the participating teams, the best zeroshot systems were OPPO, which outperforms other zero-shot systems in En→De (set1), Ja→En (set2), and En→Ja (set2) tasks, and NLE, which outperforms other systems in the other two tasks. Only Naver Labs participated in the few-shot stage (NLE-f"
2020.wmt-1.4,P17-2089,0,0.0494679,"Missing"
2020.wmt-1.4,D17-1155,0,0.0161114,"Association for Computational Linguistics et al., 2017), to continuously fine-tune on datasets increasingly similar to the target domain (Sajjad et al., 2017), or to dynamically change the balance of data towards the target domain (van der Wees et al., 2017). Another approach trains a system on multiple domains at the same time, while adding domain-specific tags to the input examples (Kobus et al., 2016). Both these approaches were employed by participants of the first shared task on MT robustness (Li et al., 2019). Other methods for domain adaptation of MT systems include instance weighting (Wang et al., 2017b), incorporating a domain classifier (Chen et al., 2017; Britz et al., 2017), and data selection (Wang et al., 2017a). Some make use of monolingual data available either in the target domain—for example by training the decoder on such data (Domhan and Hieber, 2017) or by backtranslating it (Sennrich et al., 2016)—or in the source domain, via similar techniques (Zhang and Zong, 2016). Chu and Wang (2018) provide a broad survey of domain adaptation for neural MT, which demonstrates that the predominant setup assumes knowledge of the target domain and availability of target domain data at traini"
2020.wmt-1.4,D17-1147,0,0.0351203,"Missing"
2020.wmt-1.4,D16-1160,0,0.025774,"ples (Kobus et al., 2016). Both these approaches were employed by participants of the first shared task on MT robustness (Li et al., 2019). Other methods for domain adaptation of MT systems include instance weighting (Wang et al., 2017b), incorporating a domain classifier (Chen et al., 2017; Britz et al., 2017), and data selection (Wang et al., 2017a). Some make use of monolingual data available either in the target domain—for example by training the decoder on such data (Domhan and Hieber, 2017) or by backtranslating it (Sennrich et al., 2016)—or in the source domain, via similar techniques (Zhang and Zong, 2016). Chu and Wang (2018) provide a broad survey of domain adaptation for neural MT, which demonstrates that the predominant setup assumes knowledge of the target domain and availability of target domain data at training time. In light of this prior work, the shared task proposed a relatively underexplored scenario, where examples in the target domain are either unavailable or relatively few. Other aspects of robustness are robustness to adversarial examples or noisy inputs. The fragility of neural MT models has been previously demonstrated in various settings (Belinkov and Bisk, 2018; Heigold et"
2020.wmt-1.68,D18-1214,0,0.112511,"Missing"
2020.wmt-1.68,P18-1073,0,0.0618534,"tion Machine translation (MT) has progressed rapidly since the advent of neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2015; Sutskever et al., 2014) and is better than ever for languages for which ample high-quality bitext exists. Conversely, MT for lowresource languages remains a great challenge due to a dearth of parallel training corpora and poor quality bitext from esoteric domains. To address this, several authors have proposed unsupervised MT techniques, which rely only on monolingual text for training (e.g., Ravi and Knight, 2011; Yang et al., 2018; Artetxe et al., 2018c; Hoshen and Wolf, 2018; Lample et al., 2018a,b; Artetxe et al., 2018b, 2019). Recent unsupervised MT results appear promising, but they primarily report results for the highresource languages for which traditional MT already works well. The limits of these methods are so far under-explored. For unsupervised MT to be a viable path for low-resource machine translation, the field must determine (1) if it works outside highly-controlled environments, and (2) how to effectively evaluate newly-proposed training paradigms to pursue those which are promising for real-world low-resource scenarios. Un"
2020.wmt-1.68,D18-1399,0,0.0621742,"tion Machine translation (MT) has progressed rapidly since the advent of neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2015; Sutskever et al., 2014) and is better than ever for languages for which ample high-quality bitext exists. Conversely, MT for lowresource languages remains a great challenge due to a dearth of parallel training corpora and poor quality bitext from esoteric domains. To address this, several authors have proposed unsupervised MT techniques, which rely only on monolingual text for training (e.g., Ravi and Knight, 2011; Yang et al., 2018; Artetxe et al., 2018c; Hoshen and Wolf, 2018; Lample et al., 2018a,b; Artetxe et al., 2018b, 2019). Recent unsupervised MT results appear promising, but they primarily report results for the highresource languages for which traditional MT already works well. The limits of these methods are so far under-explored. For unsupervised MT to be a viable path for low-resource machine translation, the field must determine (1) if it works outside highly-controlled environments, and (2) how to effectively evaluate newly-proposed training paradigms to pursue those which are promising for real-world low-resource scenarios. Un"
2020.wmt-1.68,P19-1019,0,0.323171,"e crosslingual language model pretraining (Conneau and Lample, 2019), masked sequence-to-sequence pretraining (Song et al., 2019), and multilingual denoising pretraining (Liu et al., 2020), and have shown promise. For instance, Liu et al. (2020) record the first good results on the low-resource Sinhala-English and Nepali-English pairs. While pretraining and multilingual methods are not the subject of this work, they warrant future evaluation. Figure 1 depicts the basic training process. It is the publicly-available SMT setup of Artetxe et al. (2018b)2 , plus the “NMT hybridization” steps from Artetxe et al. (2019).3 572 2 3 https://github.com/artetxem/monoses Shared with us by Mikel Artetxe. √ Figure 1: The unsupervised MT architecture used in this work. This model is a replication of Artetxe et al. (2018b) [steps before NMT] and Artetxe et al. (2019) [NMT component]. Mz is sorted (they find that using the square root works better empirically), and length-normalized, mean-centered, and length-normalized again. For √ each row√i in sorted( Mx ), they find the row j of sorted( Mz ) that is its nearest neighbor, and assign Xi = Zj in the initial translation dictionary D. A cell Dij = 1 if words Xi , and Zj"
2020.wmt-1.68,J82-2005,0,0.761934,"Missing"
2020.wmt-1.68,W95-0114,0,0.212136,"ystems. 571 Proceedings of the 5th Conference on Machine Translation (WMT), pages 571–583 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics and 6. Section 7 presents our findings, and Section 8 discusses the results. We conclude in Section 9. 2 Related Work Bilingual Lexicon Induction Unsupervised MT methods can be thought of as an end-to-end extension of work inducing bilingual lexicons from monolingual corpora. Bilingual lexicon induction (BLI) using non-parallel data has a rich history, beginning with corpus statistic and decipherment methods (e.g., Rapp, 1995; Fung, 1995; Koehn and Knight, 2000, 2002; Haghighi et al., 2008), continuing to modern neural methods to create crosslingual word embeddings (e.g. Mikolov et al., 2013a; Conneau et al., 2018, see Ruder et al. (2019) for a survey) which form a critical component of stateof-the-art unsupervised MT systems. Evaluation of Embedding Spaces Søgaard et al. (2018) determine that monolingual embedding spaces of similar languages are not typically isomorphic as was previously believed, and that bilingual dictionary induction “depends heavily on... the language pair, the comparability of the monolingual corpora, a"
2020.wmt-1.68,P19-1070,0,0.0656921,"Missing"
2020.wmt-1.68,D19-1632,1,0.90559,"Missing"
2020.wmt-1.68,P08-1088,0,0.0684869,"nce on Machine Translation (WMT), pages 571–583 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics and 6. Section 7 presents our findings, and Section 8 discusses the results. We conclude in Section 9. 2 Related Work Bilingual Lexicon Induction Unsupervised MT methods can be thought of as an end-to-end extension of work inducing bilingual lexicons from monolingual corpora. Bilingual lexicon induction (BLI) using non-parallel data has a rich history, beginning with corpus statistic and decipherment methods (e.g., Rapp, 1995; Fung, 1995; Koehn and Knight, 2000, 2002; Haghighi et al., 2008), continuing to modern neural methods to create crosslingual word embeddings (e.g. Mikolov et al., 2013a; Conneau et al., 2018, see Ruder et al. (2019) for a survey) which form a critical component of stateof-the-art unsupervised MT systems. Evaluation of Embedding Spaces Søgaard et al. (2018) determine that monolingual embedding spaces of similar languages are not typically isomorphic as was previously believed, and that bilingual dictionary induction “depends heavily on... the language pair, the comparability of the monolingual corpora, and the parameters of the word embedding algorithms.” V"
2020.wmt-1.68,W11-2123,0,0.0189269,"the goal is to find the linear transformations Wx and Wz which maximize the cosine similarity of the words that are translations of one another as defined by the dictionary D, over the entire dictionary: arg max Wx ,Wz Training begins with two monolingual corpora which are not necessarily related in any way (i.e. they are not assumed to be parallel nor comparable text). First, word embeddings are trained independently for each corpus, resulting in a source and a target embedding space. Specifically, after preprocessing, Artetxe et al. (2018b) train two statistical language models using KenLM (Heafield, 2011), one for the source language and one for the target. They use phrase2vec4 (Artetxe et al., 2018b), an extension of Mikolov et al. (2013b)’s skip-gram model,5 to generate phrase embeddings for 200,000 unigrams, 400,000 bigrams, and 400,000 trigrams. Next, source and target word embeddings are aligned into a common cross-lingual embedding space. They run VecMap6 (Artetxe et al., 2018a) which calculates a linear mapping of one space to another based on the intuition that phrases with similar meaning should have similar neighbors regardless of language. Given a matrix of source word embeddings X"
2020.wmt-1.68,D18-1043,0,0.0289312,"n (MT) has progressed rapidly since the advent of neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2015; Sutskever et al., 2014) and is better than ever for languages for which ample high-quality bitext exists. Conversely, MT for lowresource languages remains a great challenge due to a dearth of parallel training corpora and poor quality bitext from esoteric domains. To address this, several authors have proposed unsupervised MT techniques, which rely only on monolingual text for training (e.g., Ravi and Knight, 2011; Yang et al., 2018; Artetxe et al., 2018c; Hoshen and Wolf, 2018; Lample et al., 2018a,b; Artetxe et al., 2018b, 2019). Recent unsupervised MT results appear promising, but they primarily report results for the highresource languages for which traditional MT already works well. The limits of these methods are so far under-explored. For unsupervised MT to be a viable path for low-resource machine translation, the field must determine (1) if it works outside highly-controlled environments, and (2) how to effectively evaluate newly-proposed training paradigms to pursue those which are promising for real-world low-resource scenarios. Unsupervised MT methods mu"
2020.wmt-1.68,D13-1176,0,0.0503125,"asets. We find that performance rapidly deteriorates when source and target corpora are from different domains, and that stochasticity during embedding training can dramatically affect downstream results. We advocate for extensive empirical evaluation of unsupervised MT systems to highlight failure points and encourage continued research on the most promising paradigms. Towards this goal, we release our preprocessed dataset to stress-test systems under multiple data conditions. 1 Introduction Machine translation (MT) has progressed rapidly since the advent of neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2015; Sutskever et al., 2014) and is better than ever for languages for which ample high-quality bitext exists. Conversely, MT for lowresource languages remains a great challenge due to a dearth of parallel training corpora and poor quality bitext from esoteric domains. To address this, several authors have proposed unsupervised MT techniques, which rely only on monolingual text for training (e.g., Ravi and Knight, 2011; Yang et al., 2018; Artetxe et al., 2018c; Hoshen and Wolf, 2018; Lample et al., 2018a,b; Artetxe et al., 2018b, 2019). Recent unsupervised MT results appear"
2020.wmt-1.68,2020.eamt-1.5,0,0.200863,"Missing"
2020.wmt-1.68,P07-2045,1,0.03092,"cts an initial phrase-table for use in a SMT system. They use the softmax over the cosine similarity of the 100 nearest-neighbors of each source phrase embedding as the phrase translation probabilities. This is done in both directions: e(cos(e,f )/τ ) (f |e) = P (cos(e,f 0 )/τ ) f0 e For the target embedding with the highest cosine similarity, the phrases are aligned, and unigram translation probabilities are multiplied to become the lexical weighting. Combining the preliminary phrase table with a distortion penalty and language model produces the initial unsupervised phrase-based SMT system (Koehn et al., 2007). The SMT model weights 573 are tuned using a variant of MERT (Och, 2003) designed for unsupervised scenarios, which uses 10,000 parallel sentences generated via backtranslation (Sennrich et al., 2016a). The SMT model then undergoes three rounds of iterative backtranslation. Artetxe et al. (2019) extend their 2018 work by adding a critical “NMT hybridization” final step, which achieves significant gains over SMT alone.7 An NMT system is trained using backtranslated output from SMT for one epoch. On the next epoch, a small number of sentences are backtranslated with the newly-trained NMT system"
2020.wmt-1.68,W02-0902,1,0.670726,"Missing"
2020.wmt-1.68,P02-1040,0,0.114773,"w. Namely, we: To elucidate the performance gap due to the unsupervised architecture, we build a standard supervised NMT system using the same neural architecture described above. We train until performance on the development set ceases to improve for 10 epochs. To parallel the unsupervised setup, we translate the test set using an ensemble of 6 models; We perform ensemble selection by performance on a validation set, selecting the best-performing checkpoint along with 5 previous checkpoints. 7 Readers are directed to Artetxe et al. (2019) for additional changes that resulted in sizable BLEU (Papineni et al., 2002) gains before the NMT phase. 574 1. Choose 2 language pairs, at least one of which where the source and target languages utilize different scripts. 2. Choose 3 datasets of different domains, at least one of which is parallel bitext. 3. Perform at least one experiment for each language pair under each of the following data conditions: • Originally parallel • Not originally parallel • Different domain for source and target. 4. Choose 2 true low-resource language pairs. Condition Repro Fr-En En-Fr Supervised Fr-En Ru-En Parallel Fr-En Ru-En Disjoint Fr-En Ru-En Diff. Dom. Fr-En Ru-En News Fr-En R"
2020.wmt-1.68,P19-1018,0,0.0304513,"highlight the importance of evaluating unsupervised MT under varying realistic data conditions. Our evaluation is a step towards this goal, and identifies multiple areas for improvement. A critical step in state-of-the-art unsupervised MT is methods for creating CLEs. Several authors have pointed out that “mapping” methods like VecMap assume that monolingual vector spaces are structurally similar, but that this “approximate isomorphism assumption” is increasingly tenuous as languages and domains diverge (Søgaard et al., 2018; Ormazabal et al., 2019; Glavaˇs et al., 2019; Vuli´c et al., 2019; Patra et al., 2019). Patra et al. (2019) find this for Fr-En and Ru-En specifically, 579 the languages examined in this work. Nakashole and Flauger (2018) argue that while linearity may hold within local “neighborhoods” of the vector space, the global mapping is non-linear. Søgaard et al. (2018) use their eigenvector similarity metric to show a strong correlation between vector space similarity and BLI performance. Analysis of the CLEs from our experiments demonstrate a relationship between BLI performance and downstream BLEU on the translation task. Coupled with our empirical evidence, the works cited in this s"
2020.wmt-1.68,2020.tacl-1.47,0,0.148042,"ith supervised baselines were inequitable. While a modest body of literature has examined the quality of cross-lingual word embeddings (CLEs) by measuring performance on BLI, Glavaˇs et al. (2019) evaluate on downstream natural language tasks, underlining the importance of fullsystem evaluation. The authors conclude that “the quality of CLE models is largely task-dependent and that overfitting the models to the BLI task can result in deteriorated performance in downstream tasks.” Similarly, Doval et al. (2019) investigate cross-lingual natural language inference. Evaluation of Unsupervised MT Liu et al. (2020) helpfully re-define unsupervised machine translation into three distinct categories: (1) no bitext whatsoever, (2) the target language pair is linked through bitext via a pivot language, and (3) no linkage through a pivot language, but bitexts exists for *some* language and the target language. The authors analyze their multilingual pretraining method with respect to other similar training paradigms (Conneau and Lample, 2019; Song et al., 2019) and evaluate unsupervised MT performance when using backtranslation (Definition 1) or language transfer after finetuning on related bitext (Definition"
2020.wmt-1.68,P18-2036,0,0.0501533,"rds this goal, and identifies multiple areas for improvement. A critical step in state-of-the-art unsupervised MT is methods for creating CLEs. Several authors have pointed out that “mapping” methods like VecMap assume that monolingual vector spaces are structurally similar, but that this “approximate isomorphism assumption” is increasingly tenuous as languages and domains diverge (Søgaard et al., 2018; Ormazabal et al., 2019; Glavaˇs et al., 2019; Vuli´c et al., 2019; Patra et al., 2019). Patra et al. (2019) find this for Fr-En and Ru-En specifically, 579 the languages examined in this work. Nakashole and Flauger (2018) argue that while linearity may hold within local “neighborhoods” of the vector space, the global mapping is non-linear. Søgaard et al. (2018) use their eigenvector similarity metric to show a strong correlation between vector space similarity and BLI performance. Analysis of the CLEs from our experiments demonstrate a relationship between BLI performance and downstream BLEU on the translation task. Coupled with our empirical evidence, the works cited in this section suggest that nonisometric vector spaces lead to poor quality translation. Factors observed in our experiments that lead to lower"
2020.wmt-1.68,P03-1021,0,0.0706128,"cosine similarity of the 100 nearest-neighbors of each source phrase embedding as the phrase translation probabilities. This is done in both directions: e(cos(e,f )/τ ) (f |e) = P (cos(e,f 0 )/τ ) f0 e For the target embedding with the highest cosine similarity, the phrases are aligned, and unigram translation probabilities are multiplied to become the lexical weighting. Combining the preliminary phrase table with a distortion penalty and language model produces the initial unsupervised phrase-based SMT system (Koehn et al., 2007). The SMT model weights 573 are tuned using a variant of MERT (Och, 2003) designed for unsupervised scenarios, which uses 10,000 parallel sentences generated via backtranslation (Sennrich et al., 2016a). The SMT model then undergoes three rounds of iterative backtranslation. Artetxe et al. (2019) extend their 2018 work by adding a critical “NMT hybridization” final step, which achieves significant gains over SMT alone.7 An NMT system is trained using backtranslated output from SMT for one epoch. On the next epoch, a small number of sentences are backtranslated with the newly-trained NMT system and concatenated with a slightly smaller fraction of SMT-generated bitex"
2020.wmt-1.68,P19-1492,0,0.0313061,"erform on the use cases for which they are needed. These challenges highlight the importance of evaluating unsupervised MT under varying realistic data conditions. Our evaluation is a step towards this goal, and identifies multiple areas for improvement. A critical step in state-of-the-art unsupervised MT is methods for creating CLEs. Several authors have pointed out that “mapping” methods like VecMap assume that monolingual vector spaces are structurally similar, but that this “approximate isomorphism assumption” is increasingly tenuous as languages and domains diverge (Søgaard et al., 2018; Ormazabal et al., 2019; Glavaˇs et al., 2019; Vuli´c et al., 2019; Patra et al., 2019). Patra et al. (2019) find this for Fr-En and Ru-En specifically, 579 the languages examined in this work. Nakashole and Flauger (2018) argue that while linearity may hold within local “neighborhoods” of the vector space, the global mapping is non-linear. Søgaard et al. (2018) use their eigenvector similarity metric to show a strong correlation between vector space similarity and BLI performance. Analysis of the CLEs from our experiments demonstrate a relationship between BLI performance and downstream BLEU on the translation task"
2020.wmt-1.68,P95-1050,0,0.163797,"testing of systems. 571 Proceedings of the 5th Conference on Machine Translation (WMT), pages 571–583 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics and 6. Section 7 presents our findings, and Section 8 discusses the results. We conclude in Section 9. 2 Related Work Bilingual Lexicon Induction Unsupervised MT methods can be thought of as an end-to-end extension of work inducing bilingual lexicons from monolingual corpora. Bilingual lexicon induction (BLI) using non-parallel data has a rich history, beginning with corpus statistic and decipherment methods (e.g., Rapp, 1995; Fung, 1995; Koehn and Knight, 2000, 2002; Haghighi et al., 2008), continuing to modern neural methods to create crosslingual word embeddings (e.g. Mikolov et al., 2013a; Conneau et al., 2018, see Ruder et al. (2019) for a survey) which form a critical component of stateof-the-art unsupervised MT systems. Evaluation of Embedding Spaces Søgaard et al. (2018) determine that monolingual embedding spaces of similar languages are not typically isomorphic as was previously believed, and that bilingual dictionary induction “depends heavily on... the language pair, the comparability of the monolingua"
2020.wmt-1.68,P11-1002,0,0.133892,"under multiple data conditions. 1 Introduction Machine translation (MT) has progressed rapidly since the advent of neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2015; Sutskever et al., 2014) and is better than ever for languages for which ample high-quality bitext exists. Conversely, MT for lowresource languages remains a great challenge due to a dearth of parallel training corpora and poor quality bitext from esoteric domains. To address this, several authors have proposed unsupervised MT techniques, which rely only on monolingual text for training (e.g., Ravi and Knight, 2011; Yang et al., 2018; Artetxe et al., 2018c; Hoshen and Wolf, 2018; Lample et al., 2018a,b; Artetxe et al., 2018b, 2019). Recent unsupervised MT results appear promising, but they primarily report results for the highresource languages for which traditional MT already works well. The limits of these methods are so far under-explored. For unsupervised MT to be a viable path for low-resource machine translation, the field must determine (1) if it works outside highly-controlled environments, and (2) how to effectively evaluate newly-proposed training paradigms to pursue those which are promising"
2020.wmt-1.68,P16-1009,0,0.438638,"ies. This is done in both directions: e(cos(e,f )/τ ) (f |e) = P (cos(e,f 0 )/τ ) f0 e For the target embedding with the highest cosine similarity, the phrases are aligned, and unigram translation probabilities are multiplied to become the lexical weighting. Combining the preliminary phrase table with a distortion penalty and language model produces the initial unsupervised phrase-based SMT system (Koehn et al., 2007). The SMT model weights 573 are tuned using a variant of MERT (Och, 2003) designed for unsupervised scenarios, which uses 10,000 parallel sentences generated via backtranslation (Sennrich et al., 2016a). The SMT model then undergoes three rounds of iterative backtranslation. Artetxe et al. (2019) extend their 2018 work by adding a critical “NMT hybridization” final step, which achieves significant gains over SMT alone.7 An NMT system is trained using backtranslated output from SMT for one epoch. On the next epoch, a small number of sentences are backtranslated with the newly-trained NMT system and concatenated with a slightly smaller fraction of SMT-generated bitext. The procedure continues for 30 epochs, gradually increasing the percentage of synthetic training data created by the NMT sys"
2020.wmt-1.68,P16-1162,0,0.261177,"ies. This is done in both directions: e(cos(e,f )/τ ) (f |e) = P (cos(e,f 0 )/τ ) f0 e For the target embedding with the highest cosine similarity, the phrases are aligned, and unigram translation probabilities are multiplied to become the lexical weighting. Combining the preliminary phrase table with a distortion penalty and language model produces the initial unsupervised phrase-based SMT system (Koehn et al., 2007). The SMT model weights 573 are tuned using a variant of MERT (Och, 2003) designed for unsupervised scenarios, which uses 10,000 parallel sentences generated via backtranslation (Sennrich et al., 2016a). The SMT model then undergoes three rounds of iterative backtranslation. Artetxe et al. (2019) extend their 2018 work by adding a critical “NMT hybridization” final step, which achieves significant gains over SMT alone.7 An NMT system is trained using backtranslated output from SMT for one epoch. On the next epoch, a small number of sentences are backtranslated with the newly-trained NMT system and concatenated with a slightly smaller fraction of SMT-generated bitext. The procedure continues for 30 epochs, gradually increasing the percentage of synthetic training data created by the NMT sys"
2020.wmt-1.68,P18-1072,0,0.0450552,"Missing"
2020.wmt-1.68,tiedemann-2012-parallel,0,0.047503,"Missing"
2020.wmt-1.68,D19-1449,0,0.0718782,"Missing"
2020.wmt-1.68,P18-1005,0,0.0425578,"ditions. 1 Introduction Machine translation (MT) has progressed rapidly since the advent of neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2015; Sutskever et al., 2014) and is better than ever for languages for which ample high-quality bitext exists. Conversely, MT for lowresource languages remains a great challenge due to a dearth of parallel training corpora and poor quality bitext from esoteric domains. To address this, several authors have proposed unsupervised MT techniques, which rely only on monolingual text for training (e.g., Ravi and Knight, 2011; Yang et al., 2018; Artetxe et al., 2018c; Hoshen and Wolf, 2018; Lample et al., 2018a,b; Artetxe et al., 2018b, 2019). Recent unsupervised MT results appear promising, but they primarily report results for the highresource languages for which traditional MT already works well. The limits of these methods are so far under-explored. For unsupervised MT to be a viable path for low-resource machine translation, the field must determine (1) if it works outside highly-controlled environments, and (2) how to effectively evaluate newly-proposed training paradigms to pursue those which are promising for real-world low-"
2020.wmt-1.68,L16-1561,0,0.0315403,"is in BPE tokens. All others are token counts for SMT (pre-BPE). Datasets Training datasets used in our reinvestigation of the unsupervised MT system presented in Artetxe et al. (2019) are shown in Table 1. We focus on RussianEnglish (Ru-En) and French-English (Fr-En) tasks and include as reference Sinhala-English (Si-En) and Nepali-English (Ne-En) as well. Following Section 5, we evaluate the same system under various ablated data setups: Corpus News News UN: A UN: A UN: A UN: A UN: A / B UN: A / B UN: A / CC UN: A / CC News News CC CC United Nations The United Nations Parallel Corpus (UN) (Ziemski et al., 2016) contains official United Nations documents from 1990-2014, human-translated into six languages. The first 10,000 lines of each dataset are held-out. The remaining lines are partitioned into training sets A & B. Training set A on the source side and A on the target side are paired to form the Parallel training set; Training set A on the source side and B on the target side are paired to form the Disjoint training set. 6.2 News Crawl News crawl (News) consists of monolingual data crawled from news websites. Data for each year has been shuffled. Following Artetxe et al. (2018b), we concatenate N"
2020.wmt-1.68,D18-1549,0,\N,Missing
2020.wmt-1.78,N19-1388,0,0.0216725,"election methods developed for statistical machine translation are less effective for neural machine translation. This is different from our goals of handling noise since those methods tend to discard perfectly fine sentence pairs that are just not relevant for the targeted domain. Our task is focused on data quality that is relevant for all domains. 2.4 2.6 By now, neural machine translation systems are rarely trained only on the parallel corpus of the desired language pair. Common foundations are pretrained models trained on multiple language pairs which share the source or target language (Aharoni et al., 2019; Fan et al., 2020) or monolingual pre-training methods (Liu et al., 2020). Often, the models are also improved by a second stage of training that uses back-translated synthetic parallel data that was generated from first stage model — a process that may be iterated (Hoang et al., 2018). To reflect such a more realistic training setup, we provided pre-trained models that were trained on monolingual data using a denoising autoencoder method called mBART (Liu et al., 2020). Here, monolingual data is converted into input and output pairs by (a) masking out words in the input, forcing the model to"
2020.wmt-1.78,W11-1218,0,0.0186235,"epali–English and Sinhala–English. For these languages much less clean parallel data was available and hence many of the methods developed for high-resource languages are less reliable. The best-performing submission that year (Chaudhary et al., 2019) also considered dual crossentropy but found that matching multilingual sentence embeddings (Schwenk, 2018) gave better results. are translations that have been created by machine translation. Venugopal et al. (2011) propose a method to watermark the output of machine translation systems to aid this distinction, with a negligible loss of quality. Antonova and Misyurev (2011) report that rule-based machine translation output can be detected due to certain word choices, and statistical machine translation output can be detected due to lack of reordering. It is notable that none of the participants in our shared task have tried to detect machine translation. There is a rich literature on data selection which aims at sub-sampling parallel data relevant for a task-specific machine translation system (Axelrod et al., 2011). Van der Wees et al. (2017) find that the existing data selection methods developed for statistical machine translation are less effective for neura"
2020.wmt-1.78,D11-1033,0,0.0508684,"l. (2011) propose a method to watermark the output of machine translation systems to aid this distinction, with a negligible loss of quality. Antonova and Misyurev (2011) report that rule-based machine translation output can be detected due to certain word choices, and statistical machine translation output can be detected due to lack of reordering. It is notable that none of the participants in our shared task have tried to detect machine translation. There is a rich literature on data selection which aims at sub-sampling parallel data relevant for a task-specific machine translation system (Axelrod et al., 2011). Van der Wees et al. (2017) find that the existing data selection methods developed for statistical machine translation are less effective for neural machine translation. This is different from our goals of handling noise since those methods tend to discard perfectly fine sentence pairs that are just not relevant for the targeted domain. Our task is focused on data quality that is relevant for all domains. 2.4 2.6 By now, neural machine translation systems are rarely trained only on the parallel corpus of the desired language pair. Common foundations are pretrained models trained on multiple"
2020.wmt-1.78,P13-2061,0,0.027165,"ng Noisy Parallel Corpora In 2016, a shared task on sentence pair filtering4 was organized, albeit in the context of cleaning translation memories which tend to be cleaner than the data at the end of a pipeline that starts with web crawls. There is a robust body of work on filtering out noise in parallel data. For example: Taghipour et al. (2011) use an outlier detection algorithm to filter a parallel corpus; Xu and Koehn (2017) generate synthetic noisy data (inadequate and nonfluent translations) and use this data to train a classifier to identify good sentence pairs from a noisy corpus; and Cui et al. (2013) use a graph-based random walk algorithm and extract phrase pair scores to weight the phrase translation probabilities to bias towards more trustworthy ones. Most of this work was done in the context of statistical machine translation, but more recent work targets neural models. Carpuat et al. (2017) focus on identifying semantic differences in translation pairs using cross-lingual textual entailment and additional length-based features, and demonstrate that removing such sentences improves neural machine translation performance. As Rarrick et al. (2011) point out, one type of noise in paralle"
2020.wmt-1.78,2020.emnlp-main.480,1,0.817813,"Missing"
2020.wmt-1.78,2020.aacl-main.62,1,0.86925,"Missing"
2020.wmt-1.78,C10-2010,0,0.0316759,"icipating systems and provides analysis on additional subset sizes, the average sentence length of sub-selected data, and overlap between the submissions. 2 Sentence alignment has been a very active field of research since the early days of statistical machine translation. An influential early method is based on sentence length, measured in words (Gale and Church, 1993). Several researchers proposed including lexical information (Chen, 1993; Moore, 2002) with the emergence of tools that use provided bilingual dictionaries (Varga et al., 2005) or acquire them during in an unsupervised fashion (Braune and Fraser, 2010). Later work introduced scoring methods that use MT to get both documents into the same language (Sennrich and Volk, 2010) or use pruned phrase tables from a statistical MT system (Gomes and Pereira Lopes, 2016). Both methods anchor high-probability 1–1 alignments in the search space and then fill in and refine alignments. More recently, Thompson and Koehn (2019) introduced the use of sentence embeddings and a coarse-to-fine search method to the task (Vecalign). Related Work Although the idea of crawling the web indiscriminately for parallel data goes back to the 20th century (Resnik, 1999), w"
2020.wmt-1.78,W19-5436,0,0.0174017,"RT-style Transformer. A relatively small training corpus is used (2,000 or 10,000 sentence pairs) with 10x over-sampled negatives. 5.2.2 JHU-Koerner (Koerner and Koehn, 2020) employ a linear combination of LASER scores, monolingual language model scores, dual cross entropy, and use a sentence duplication penalty. JHU-Kejriwal (Kejriwal and Koehn, 2020) use LASER scores with some novel transformation of score ranges, language ID confidence scores, monolingual language models trained on words and characters, and length-based filters. Individual Submissions AFRL use their corpus-building method (Erdmann and Gwinnup, 2019) but with a bidirectional quality metric that nearly eliminates pre-filtering (used only for the limit on training line length). The coverage metric encourages the addition of a sentence that improves corpus-level bilingual vocabulary frequencies. The new quality metric is the average of sentence-level NMT scores (“loglikelihoods”) in both directions. Microsoft (Nokrashy et al., 2020) focus on the LASER scores, using both the provided LASER scores, custom LASER scores using a model trained on the provided clean parallel data (which are better for Pashto but worse for Khmer), and a classifier b"
2020.wmt-1.78,W16-2347,1,0.84143,"llel documents and parallel sentences were sourced from the CCAligned2 dataset (ElKishky et al., 2020a), a massive collection of cross-lingual web documents covering over 8k language pairs aligned from 68 Common Crawl snapshots. Additional parallel data was sourced from the Paracrawl project – a large-scale effort to crawl text from the web3 (Ba˜no´ n et al., 2020). Acquiring parallel corpora from the web (ElKishky et al., 2020b) is an active area of research that typically involves identifying web sites with parallel text, downloading the documents from the web site, aligning document pairs (Buck and Koehn, 2016; Thompson and Koehn, 2020; ElKishky and Guzm´an, 2020), and aligning sentence pairs. A final stage of the processing pipeline filters out non-parallel sentence pairs. Such noise exists either because the original web site did not have any actual parallel data (garbage in, garbage out), only partially-parallel data, or due to failures of processing steps. 1 http://opus.nlpl.eu http://statmt.org/cc-aligned 3 http://www.paracrawl.eu/ 2 4 NLP4TM 2016: Shared task http://rgcl.wlv.ac.uk/nlp4tm2016/shared-task/ 727 translation directions to score sentence pairs with dual cross-entropy. Last year, we"
2020.wmt-1.78,2020.wmt-1.107,0,0.0624061,"Missing"
2020.wmt-1.78,W17-3209,0,0.0262772,"parallel data. For example: Taghipour et al. (2011) use an outlier detection algorithm to filter a parallel corpus; Xu and Koehn (2017) generate synthetic noisy data (inadequate and nonfluent translations) and use this data to train a classifier to identify good sentence pairs from a noisy corpus; and Cui et al. (2013) use a graph-based random walk algorithm and extract phrase pair scores to weight the phrase translation probabilities to bias towards more trustworthy ones. Most of this work was done in the context of statistical machine translation, but more recent work targets neural models. Carpuat et al. (2017) focus on identifying semantic differences in translation pairs using cross-lingual textual entailment and additional length-based features, and demonstrate that removing such sentences improves neural machine translation performance. As Rarrick et al. (2011) point out, one type of noise in parallel corpora extracted from the web Parallel Corpus Acquisition Noisy parallel documents and parallel sentences were sourced from the CCAligned2 dataset (ElKishky et al., 2020a), a massive collection of cross-lingual web documents covering over 8k language pairs aligned from 68 Common Crawl snapshots. A"
2020.wmt-1.78,W19-5435,1,0.888005,"Missing"
2020.wmt-1.78,J93-1004,0,0.881688,"ain neural machine translation systems on these subsets, and measure their quality with the BLEU score on a test set of multi-domain Wikipedia content (Guzm´an et al., 2019). This paper gives an overview of the task, presents the results for the participating systems and provides analysis on additional subset sizes, the average sentence length of sub-selected data, and overlap between the submissions. 2 Sentence alignment has been a very active field of research since the early days of statistical machine translation. An influential early method is based on sentence length, measured in words (Gale and Church, 1993). Several researchers proposed including lexical information (Chen, 1993; Moore, 2002) with the emergence of tools that use provided bilingual dictionaries (Varga et al., 2005) or acquire them during in an unsupervised fashion (Braune and Fraser, 2010). Later work introduced scoring methods that use MT to get both documents into the same language (Sennrich and Volk, 2010) or use pruned phrase tables from a statistical MT system (Gomes and Pereira Lopes, 2016). Both methods anchor high-probability 1–1 alignments in the search space and then fill in and refine alignments. More recently, Thompson"
2020.wmt-1.78,P93-1002,0,0.703239,"ith the BLEU score on a test set of multi-domain Wikipedia content (Guzm´an et al., 2019). This paper gives an overview of the task, presents the results for the participating systems and provides analysis on additional subset sizes, the average sentence length of sub-selected data, and overlap between the submissions. 2 Sentence alignment has been a very active field of research since the early days of statistical machine translation. An influential early method is based on sentence length, measured in words (Gale and Church, 1993). Several researchers proposed including lexical information (Chen, 1993; Moore, 2002) with the emergence of tools that use provided bilingual dictionaries (Varga et al., 2005) or acquire them during in an unsupervised fashion (Braune and Fraser, 2010). Later work introduced scoring methods that use MT to get both documents into the same language (Sennrich and Volk, 2010) or use pruned phrase tables from a statistical MT system (Gomes and Pereira Lopes, 2016). Both methods anchor high-probability 1–1 alignments in the search space and then fill in and refine alignments. More recently, Thompson and Koehn (2019) introduced the use of sentence embeddings and a coarse"
2020.wmt-1.78,W16-2369,0,0.0558247,"Missing"
2020.wmt-1.78,2020.acl-main.747,1,0.71089,"Missing"
2020.wmt-1.78,D19-1632,1,0.909569,"Missing"
2020.wmt-1.78,N10-1000,0,0.18974,"Missing"
2020.wmt-1.78,W18-2703,1,0.83705,"ask is focused on data quality that is relevant for all domains. 2.4 2.6 By now, neural machine translation systems are rarely trained only on the parallel corpus of the desired language pair. Common foundations are pretrained models trained on multiple language pairs which share the source or target language (Aharoni et al., 2019; Fan et al., 2020) or monolingual pre-training methods (Liu et al., 2020). Often, the models are also improved by a second stage of training that uses back-translated synthetic parallel data that was generated from first stage model — a process that may be iterated (Hoang et al., 2018). To reflect such a more realistic training setup, we provided pre-trained models that were trained on monolingual data using a denoising autoencoder method called mBART (Liu et al., 2020). Here, monolingual data is converted into input and output pairs by (a) masking out words in the input, forcing the model to learn the correct word or word sequence from the context, and (b) shuffling the order of a few concatenated sentence pairs. Impact of Noise on Neural Machine Translation Belinkov and Bisk (2017) investigate the impact of noise on neural machine translation. They focus on creating syste"
2020.wmt-1.78,J82-2005,0,0.715747,"Missing"
2020.wmt-1.78,W18-6478,0,0.0178193,"he shared task tackled the problem of filtering parallel corpora. Given a noisy parallel corpus (crawled from the web), participants developed methods to align sentences in document pairs and to filter it to a smaller size of high quality sentence pairs. Findings of Previous Shared Tasks We organized versions of this shared task in the previous two years. In 2018, we started with a high-resource language pair (German–English) and a very large web-crawled parallel corpus, a subset of the Paracrawl corpus consisting of 1 billion English words (Koehn et al., 2018). The bestperforming submission (Junczys-Dowmunt, 2018) used neural machine translation systems in both 3.1 Filtering For the filtering-only task, we provided a very noisy 58.3 million word corpus for Khmer– English (English token count) and a 11.6 million word corpus for Pashto–English, crawled from the 728 web (see Section 4.3 for details). We asked participants to generate sentence-level quality scores that allow selecting subsets of sentence pairs that amount to 5 million words, counted on the English side. This amount was chosen based on preliminary experiments (we report below on additional subset sizes). Participants in the shared task subm"
2020.wmt-1.78,2020.tacl-1.47,1,0.90024,"today’s neural machine translation models, perform poorly on low-resource language pairs, for which clean, high-quality training data is lacking (Koehn and Knowles, 2017). Improving performance on low resource language pairs has high impact considering that these languages are spoken by a large fraction of the world population. This is a particular challenge for industrial machine translation systems that need to support hundreds of languages in order to provide adequate services to their multilingual user base. While there have been advances in using monolingual corpora (Lample et al., 2018; Liu et al., 2020) and parallel corpora in multiple language 726 Proceedings of the 5th Conference on Machine Translation (WMT), pages 726–742 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics 2.2 a fixed size (5 million English words), train neural machine translation systems on these subsets, and measure their quality with the BLEU score on a test set of multi-domain Wikipedia content (Guzm´an et al., 2019). This paper gives an overview of the task, presents the results for the participating systems and provides analysis on additional subset sizes, the average sentence length of s"
2020.wmt-1.78,2020.wmt-1.108,1,0.92033,"2,378 2,309 2,320 40,436 44,471 40,341 Table 7: Statistics for the flores test sets used to evaluate the machine translation systems trained on the subsampled data sets. Word counts are obtained with wc on tokenized text. Short Name Participant and System Description Citation AFRL Alibaba Bytedance Edinburgh Huawei JHU-Kejriwal JHU-Koerner Microsoft NRC UA-Prompsit LASER Air Force Research Lab, USA Alibaba, China (Lu et al., 2020) Bytedance, China (Xu et al., 2020) University of Edinburgh, Scotland Huawei, Turkey/China (Ac¸arc¸ic¸ek et al., 2020) Ankur Kejriwal, Johns Hopkins University, USA (Kejriwal and Koehn, 2020) Felicia Koerner, Johns Hopkins University, USA (Koerner and Koehn, 2020) Microsoft, Egypt Development Center, Egypt (Nokrashy et al., 2020) National Research Council, Canada (Lo and Joanis, 2020) University of Alicante and Prompsit, Spain (Espl`a-Gomis et al., 2020) Officially provided baseline Table 8: Participants in the shared task. Dual cross entropy Neural machine translation systems trained on the provided clean parallel data can be used by feeding in the English sentence and computing the probability of the foreign sentence according to the model, and vice versa. JunczysDowmunt (2018)"
2020.wmt-1.78,W19-5358,0,0.0278491,"ges pairs, a combination of them performs best. Alibaba (Lu et al., 2020) use a number of features that are combined linearly: a bilingual GPT2 model trained on source-target language pairs as well as monolingual GPT-2 model each of the languages, dual cross entropy from neural machine translation models trained in both directions and statistical word translation model scores. They report that they experimented with classifiers to weight features but found this to be not beneficial. NRC (Lo and Joanis, 2020) tackle both filtering and alignment. Their filtering score is mainly based on Yisi-2 (Lo, 2019), a language model trained on the target side, and representations obtained with XLM-RoBERTa (Conneau et al., 2020) pre-trained for Pashto, Khmer, and English. Sentence alignment is based on the approach by Moore (2002), first applied to align paragraphs and then sentences. Bytedance (Xu et al., 2020) tackle only the combined alignment/filtering task. The sentence alignment methods draws on statistical lexical translation scores, as used in YiSi-2. They iteratively improve the lexical model by adding high-quality mined sentence pair to its training data. Their filtering method is a classifier"
2020.wmt-1.78,W18-2709,1,0.918844,"ltimore, Maryland, United States † Facebook AI, Menlo Park, California, United States Abstract pairs (Aharoni et al., 2019; Fan et al., 2020), the best training data for machine translation are still parallel corpora in the targeted language pair and domain. Parallel corpora are typically gathered from any available source without much guarantees about quality. This is especially the case for parallel corpora that are extracted from the web without much control over which web sites are mined. Since noisy training data has been recognized as a challenge for neural machine translation training (Khayrallah and Koehn, 2018), an essential step in using such data is filtering or discounting noisy sentence pairs. Recently, there is increased interest in the filtering of noisy parallel corpora to improve the data that can be used to train translation systems. The Shared Task on Parallel Corpus Filtering and Alignment at the Conference for Machine Translation (WMT 2020) was organized to promote research to make learning from noisy data more viable for low-resource languages. It is similar to the previous year’s task but tackles different languages (Pashto and Khmer instead of Nepali and Sinhala) and also included the"
2020.wmt-1.78,2005.mtsummit-papers.11,1,0.195972,"2016). Both methods anchor high-probability 1–1 alignments in the search space and then fill in and refine alignments. More recently, Thompson and Koehn (2019) introduced the use of sentence embeddings and a coarse-to-fine search method to the task (Vecalign). Related Work Although the idea of crawling the web indiscriminately for parallel data goes back to the 20th century (Resnik, 1999), work in the academic community on extraction of parallel corpora from the web has so far mostly focused on large stashes of multilingual content in homogeneous form, such as the Canadian Hansards, Europarl (Koehn, 2005), the United Nations (Rafalovitch and Dale, 2009; Ziemski et al., 2015), or European Patents (T¨ager, 2011). A nice collection of the products of these efforts is the OPUS web site1 (Tiedemann, 2012). 2.1 Sentence Alignment 2.3 Filtering Noisy Parallel Corpora In 2016, a shared task on sentence pair filtering4 was organized, albeit in the context of cleaning translation memories which tend to be cleaner than the data at the end of a pipeline that starts with web crawls. There is a robust body of work on filtering out noise in parallel data. For example: Taghipour et al. (2011) use an outlier d"
2020.wmt-1.78,W19-5404,1,0.7475,"Missing"
2020.wmt-1.78,2020.wmt-1.111,0,0.315983,"e an overview of methods and then give a short sum731 Pashto dev dev test test Khmer Sentence Pairs English Words Sentence Pairs English Words 3,162 2,698 2,719 55,439 46,175 47,695 2,378 2,309 2,320 40,436 44,471 40,341 Table 7: Statistics for the flores test sets used to evaluate the machine translation systems trained on the subsampled data sets. Word counts are obtained with wc on tokenized text. Short Name Participant and System Description Citation AFRL Alibaba Bytedance Edinburgh Huawei JHU-Kejriwal JHU-Koerner Microsoft NRC UA-Prompsit LASER Air Force Research Lab, USA Alibaba, China (Lu et al., 2020) Bytedance, China (Xu et al., 2020) University of Edinburgh, Scotland Huawei, Turkey/China (Ac¸arc¸ic¸ek et al., 2020) Ankur Kejriwal, Johns Hopkins University, USA (Kejriwal and Koehn, 2020) Felicia Koerner, Johns Hopkins University, USA (Koerner and Koehn, 2020) Microsoft, Egypt Development Center, Egypt (Nokrashy et al., 2020) National Research Council, Canada (Lo and Joanis, 2020) University of Alicante and Prompsit, Spain (Espl`a-Gomis et al., 2020) Officially provided baseline Table 8: Participants in the shared task. Dual cross entropy Neural machine translation systems trained on the p"
2020.wmt-1.78,moore-2002-fast,0,0.519033,"score on a test set of multi-domain Wikipedia content (Guzm´an et al., 2019). This paper gives an overview of the task, presents the results for the participating systems and provides analysis on additional subset sizes, the average sentence length of sub-selected data, and overlap between the submissions. 2 Sentence alignment has been a very active field of research since the early days of statistical machine translation. An influential early method is based on sentence length, measured in words (Gale and Church, 1993). Several researchers proposed including lexical information (Chen, 1993; Moore, 2002) with the emergence of tools that use provided bilingual dictionaries (Varga et al., 2005) or acquire them during in an unsupervised fashion (Braune and Fraser, 2010). Later work introduced scoring methods that use MT to get both documents into the same language (Sennrich and Volk, 2010) or use pruned phrase tables from a statistical MT system (Gomes and Pereira Lopes, 2016). Both methods anchor high-probability 1–1 alignments in the search space and then fill in and refine alignments. More recently, Thompson and Koehn (2019) introduced the use of sentence embeddings and a coarse-to-fine searc"
2020.wmt-1.78,W18-6453,1,0.818016,"d interest in the filtering of noisy parallel corpora to improve the data that can be used to train translation systems. The Shared Task on Parallel Corpus Filtering and Alignment at the Conference for Machine Translation (WMT 2020) was organized to promote research to make learning from noisy data more viable for low-resource languages. It is similar to the previous year’s task but tackles different languages (Pashto and Khmer instead of Nepali and Sinhala) and also included the challenge to extract sentence pairs from document pairs. The shared task is organized similarly to previous years (Koehn et al., 2018, 2019). We provide about 11.6 million word noisy parallel data for Pashto-English and 58.3 million word noisy parallel data for Khmer-English. We also provide small amounts of clean parallel data of varying quality and monolingual data from Wikipedia and CommonCrawl. Participants developed methods to assign a quality score for each sentence pair. These scores are used to filter the web crawled corpora down to Following two preceding WMT Shared Tasks on Parallel Corpus Filtering (Koehn et al., 2018, 2019), we posed again the challenge of assigning sentence-level quality scores for very noisy c"
2020.wmt-1.78,W17-3204,1,0.80612,"10 participants from companies, national research labs, and universities participated in this task. 1 Introduction The field of Machine Translation has experienced significant advances in recent years thanks to improvements in neural modeling (Bahdanau et al., 2015; Gehring et al., 2016; Vaswani et al., 2017), as well as the availability of large parallel corpora for training (Tiedemann, 2012; Smith et al., 2013; Bojar et al., 2017). Unfortunately, today’s neural machine translation models, perform poorly on low-resource language pairs, for which clean, high-quality training data is lacking (Koehn and Knowles, 2017). Improving performance on low resource language pairs has high impact considering that these languages are spoken by a large fraction of the world population. This is a particular challenge for industrial machine translation systems that need to support hundreds of languages in order to provide adequate services to their multilingual user base. While there have been advances in using monolingual corpora (Lample et al., 2018; Liu et al., 2020) and parallel corpora in multiple language 726 Proceedings of the 5th Conference on Machine Translation (WMT), pages 726–742 c Online, November 19–20, 20"
2020.wmt-1.78,N19-4009,0,0.152457,"provided detailed instructions on how to use these tools to replicate the official testing environment. Alignment 4 Data We provided three types of data for this shared task: (1) clean parallel and monolingual data, including related language data in Hindi, to train models that aid with the filtering task, (2) the noisy parallel data crawled from the web which participants have to score for filtering, and (3) development and test sets that are used to evaluate translation systems trained on filtered data. Evaluation The submissions were scored by building a neural machine translation system (Ott et al., 2019) trained on this data, and then measuring their BLEU score on the flores Wikipedia test sets (Guzm´an et al., 2019). The neural machine translation model was either randomly initialized or initialized by monolingual pre-training (mBART). For development purposes, we released configuration files and scripts that mirror the official testing procedure with a development test set. The development pack consists of: • A script to subsample corpora based on quality scores. • fairseq scripts to train and test a neural machine translation system. • A pre-trained mBART model for continued training. • Th"
2020.wmt-1.78,2020.wmt-1.109,1,0.92417,"test sets used to evaluate the machine translation systems trained on the subsampled data sets. Word counts are obtained with wc on tokenized text. Short Name Participant and System Description Citation AFRL Alibaba Bytedance Edinburgh Huawei JHU-Kejriwal JHU-Koerner Microsoft NRC UA-Prompsit LASER Air Force Research Lab, USA Alibaba, China (Lu et al., 2020) Bytedance, China (Xu et al., 2020) University of Edinburgh, Scotland Huawei, Turkey/China (Ac¸arc¸ic¸ek et al., 2020) Ankur Kejriwal, Johns Hopkins University, USA (Kejriwal and Koehn, 2020) Felicia Koerner, Johns Hopkins University, USA (Koerner and Koehn, 2020) Microsoft, Egypt Development Center, Egypt (Nokrashy et al., 2020) National Research Council, Canada (Lo and Joanis, 2020) University of Alicante and Prompsit, Spain (Espl`a-Gomis et al., 2020) Officially provided baseline Table 8: Participants in the shared task. Dual cross entropy Neural machine translation systems trained on the provided clean parallel data can be used by feeding in the English sentence and computing the probability of the foreign sentence according to the model, and vice versa. JunczysDowmunt (2018) proposed a metric that uses not only the individual computed cross entrop"
2020.wmt-1.78,2009.mtsummit-posters.15,0,0.077943,"obability 1–1 alignments in the search space and then fill in and refine alignments. More recently, Thompson and Koehn (2019) introduced the use of sentence embeddings and a coarse-to-fine search method to the task (Vecalign). Related Work Although the idea of crawling the web indiscriminately for parallel data goes back to the 20th century (Resnik, 1999), work in the academic community on extraction of parallel corpora from the web has so far mostly focused on large stashes of multilingual content in homogeneous form, such as the Canadian Hansards, Europarl (Koehn, 2005), the United Nations (Rafalovitch and Dale, 2009; Ziemski et al., 2015), or European Patents (T¨ager, 2011). A nice collection of the products of these efforts is the OPUS web site1 (Tiedemann, 2012). 2.1 Sentence Alignment 2.3 Filtering Noisy Parallel Corpora In 2016, a shared task on sentence pair filtering4 was organized, albeit in the context of cleaning translation memories which tend to be cleaner than the data at the end of a pipeline that starts with web crawls. There is a robust body of work on filtering out noise in parallel data. For example: Taghipour et al. (2011) use an outlier detection algorithm to filter a parallel corpus;"
2020.wmt-1.78,2011.mtsummit-papers.48,0,0.0272057,"ood sentence pairs from a noisy corpus; and Cui et al. (2013) use a graph-based random walk algorithm and extract phrase pair scores to weight the phrase translation probabilities to bias towards more trustworthy ones. Most of this work was done in the context of statistical machine translation, but more recent work targets neural models. Carpuat et al. (2017) focus on identifying semantic differences in translation pairs using cross-lingual textual entailment and additional length-based features, and demonstrate that removing such sentences improves neural machine translation performance. As Rarrick et al. (2011) point out, one type of noise in parallel corpora extracted from the web Parallel Corpus Acquisition Noisy parallel documents and parallel sentences were sourced from the CCAligned2 dataset (ElKishky et al., 2020a), a massive collection of cross-lingual web documents covering over 8k language pairs aligned from 68 Common Crawl snapshots. Additional parallel data was sourced from the Paracrawl project – a large-scale effort to crawl text from the web3 (Ba˜no´ n et al., 2020). Acquiring parallel corpora from the web (ElKishky et al., 2020b) is an active area of research that typically involves i"
2020.wmt-1.78,P99-1068,0,0.107062,"nd Fraser, 2010). Later work introduced scoring methods that use MT to get both documents into the same language (Sennrich and Volk, 2010) or use pruned phrase tables from a statistical MT system (Gomes and Pereira Lopes, 2016). Both methods anchor high-probability 1–1 alignments in the search space and then fill in and refine alignments. More recently, Thompson and Koehn (2019) introduced the use of sentence embeddings and a coarse-to-fine search method to the task (Vecalign). Related Work Although the idea of crawling the web indiscriminately for parallel data goes back to the 20th century (Resnik, 1999), work in the academic community on extraction of parallel corpora from the web has so far mostly focused on large stashes of multilingual content in homogeneous form, such as the Canadian Hansards, Europarl (Koehn, 2005), the United Nations (Rafalovitch and Dale, 2009; Ziemski et al., 2015), or European Patents (T¨ager, 2011). A nice collection of the products of these efforts is the OPUS web site1 (Tiedemann, 2012). 2.1 Sentence Alignment 2.3 Filtering Noisy Parallel Corpora In 2016, a shared task on sentence pair filtering4 was organized, albeit in the context of cleaning translation memori"
2020.wmt-1.78,P18-2037,0,0.0212053,"ac.uk/nlp4tm2016/shared-task/ 727 translation directions to score sentence pairs with dual cross-entropy. Last year, we moved the focus to low resource languages (Koehn et al., 2019) with smaller noisy parallel corpora, comprising 50-60 million words for Nepali–English and Sinhala–English. For these languages much less clean parallel data was available and hence many of the methods developed for high-resource languages are less reliable. The best-performing submission that year (Chaudhary et al., 2019) also considered dual crossentropy but found that matching multilingual sentence embeddings (Schwenk, 2018) gave better results. are translations that have been created by machine translation. Venugopal et al. (2011) propose a method to watermark the output of machine translation systems to aid this distinction, with a negligible loss of quality. Antonova and Misyurev (2011) report that rule-based machine translation output can be detected due to certain word choices, and statistical machine translation output can be detected due to lack of reordering. It is notable that none of the participants in our shared task have tried to detect machine translation. There is a rich literature on data selectio"
2020.wmt-1.78,2010.amta-papers.14,0,0.0352622,"overlap between the submissions. 2 Sentence alignment has been a very active field of research since the early days of statistical machine translation. An influential early method is based on sentence length, measured in words (Gale and Church, 1993). Several researchers proposed including lexical information (Chen, 1993; Moore, 2002) with the emergence of tools that use provided bilingual dictionaries (Varga et al., 2005) or acquire them during in an unsupervised fashion (Braune and Fraser, 2010). Later work introduced scoring methods that use MT to get both documents into the same language (Sennrich and Volk, 2010) or use pruned phrase tables from a statistical MT system (Gomes and Pereira Lopes, 2016). Both methods anchor high-probability 1–1 alignments in the search space and then fill in and refine alignments. More recently, Thompson and Koehn (2019) introduced the use of sentence embeddings and a coarse-to-fine search method to the task (Vecalign). Related Work Although the idea of crawling the web indiscriminately for parallel data goes back to the 20th century (Resnik, 1999), work in the academic community on extraction of parallel corpora from the web has so far mostly focused on large stashes of"
2020.wmt-1.78,D11-1126,0,0.0249695,"ropy. Last year, we moved the focus to low resource languages (Koehn et al., 2019) with smaller noisy parallel corpora, comprising 50-60 million words for Nepali–English and Sinhala–English. For these languages much less clean parallel data was available and hence many of the methods developed for high-resource languages are less reliable. The best-performing submission that year (Chaudhary et al., 2019) also considered dual crossentropy but found that matching multilingual sentence embeddings (Schwenk, 2018) gave better results. are translations that have been created by machine translation. Venugopal et al. (2011) propose a method to watermark the output of machine translation systems to aid this distinction, with a negligible loss of quality. Antonova and Misyurev (2011) report that rule-based machine translation output can be detected due to certain word choices, and statistical machine translation output can be detected due to lack of reordering. It is notable that none of the participants in our shared task have tried to detect machine translation. There is a rich literature on data selection which aims at sub-sampling parallel data relevant for a task-specific machine translation system (Axelrod e"
2020.wmt-1.78,P13-1135,1,0.8158,"train machine translation systems. This year, the task tackled the low resource condition of Pashto– English and Khmer–English and also included the challenge of sentence alignment from document pairs. 10 participants from companies, national research labs, and universities participated in this task. 1 Introduction The field of Machine Translation has experienced significant advances in recent years thanks to improvements in neural modeling (Bahdanau et al., 2015; Gehring et al., 2016; Vaswani et al., 2017), as well as the availability of large parallel corpora for training (Tiedemann, 2012; Smith et al., 2013; Bojar et al., 2017). Unfortunately, today’s neural machine translation models, perform poorly on low-resource language pairs, for which clean, high-quality training data is lacking (Koehn and Knowles, 2017). Improving performance on low resource language pairs has high impact considering that these languages are spoken by a large fraction of the world population. This is a particular challenge for industrial machine translation systems that need to support hundreds of languages in order to provide adequate services to their multilingual user base. While there have been advances in using mono"
2020.wmt-1.78,W18-6314,0,0.0200536,"ms that can translate the kinds of orthographic errors (typos, misspellings, etc.) that humans can comprehend. In contrast, Khayrallah and Koehn (2018) examine noisy training data and focus on types of noise occurring in web-crawled corpora. They carried out a study about how noise that occurs in crawled parallel text impacts statistical and neural machine translation. Neural machine translation model training may combine data selection and model training, taking advantage of the increasing quality of the model to better detect noisy data or to increasingly focus on cleaner parts of the data (Wang et al., 2018; Kumar et al., 2019). 2.5 Monolingual Pre-Training 3 Shared Task Definition The shared task tackled the problem of filtering parallel corpora. Given a noisy parallel corpus (crawled from the web), participants developed methods to align sentences in document pairs and to filter it to a smaller size of high quality sentence pairs. Findings of Previous Shared Tasks We organized versions of this shared task in the previous two years. In 2018, we started with a high-resource language pair (German–English) and a very large web-crawled parallel corpus, a subset of the Paracrawl corpus consisting of"
2020.wmt-1.78,2011.eamt-1.25,0,0.0731606,"Missing"
2020.wmt-1.78,2011.mtsummit-papers.47,0,0.0601538,"anadian Hansards, Europarl (Koehn, 2005), the United Nations (Rafalovitch and Dale, 2009; Ziemski et al., 2015), or European Patents (T¨ager, 2011). A nice collection of the products of these efforts is the OPUS web site1 (Tiedemann, 2012). 2.1 Sentence Alignment 2.3 Filtering Noisy Parallel Corpora In 2016, a shared task on sentence pair filtering4 was organized, albeit in the context of cleaning translation memories which tend to be cleaner than the data at the end of a pipeline that starts with web crawls. There is a robust body of work on filtering out noise in parallel data. For example: Taghipour et al. (2011) use an outlier detection algorithm to filter a parallel corpus; Xu and Koehn (2017) generate synthetic noisy data (inadequate and nonfluent translations) and use this data to train a classifier to identify good sentence pairs from a noisy corpus; and Cui et al. (2013) use a graph-based random walk algorithm and extract phrase pair scores to weight the phrase translation probabilities to bias towards more trustworthy ones. Most of this work was done in the context of statistical machine translation, but more recent work targets neural models. Carpuat et al. (2017) focus on identifying semantic"
2020.wmt-1.78,D17-1147,0,0.0510981,"Missing"
2020.wmt-1.78,D19-1136,1,0.902758,"h, 1993). Several researchers proposed including lexical information (Chen, 1993; Moore, 2002) with the emergence of tools that use provided bilingual dictionaries (Varga et al., 2005) or acquire them during in an unsupervised fashion (Braune and Fraser, 2010). Later work introduced scoring methods that use MT to get both documents into the same language (Sennrich and Volk, 2010) or use pruned phrase tables from a statistical MT system (Gomes and Pereira Lopes, 2016). Both methods anchor high-probability 1–1 alignments in the search space and then fill in and refine alignments. More recently, Thompson and Koehn (2019) introduced the use of sentence embeddings and a coarse-to-fine search method to the task (Vecalign). Related Work Although the idea of crawling the web indiscriminately for parallel data goes back to the 20th century (Resnik, 1999), work in the academic community on extraction of parallel corpora from the web has so far mostly focused on large stashes of multilingual content in homogeneous form, such as the Canadian Hansards, Europarl (Koehn, 2005), the United Nations (Rafalovitch and Dale, 2009; Ziemski et al., 2015), or European Patents (T¨ager, 2011). A nice collection of the products of t"
2020.wmt-1.78,D17-1319,1,0.792925,"; Ziemski et al., 2015), or European Patents (T¨ager, 2011). A nice collection of the products of these efforts is the OPUS web site1 (Tiedemann, 2012). 2.1 Sentence Alignment 2.3 Filtering Noisy Parallel Corpora In 2016, a shared task on sentence pair filtering4 was organized, albeit in the context of cleaning translation memories which tend to be cleaner than the data at the end of a pipeline that starts with web crawls. There is a robust body of work on filtering out noise in parallel data. For example: Taghipour et al. (2011) use an outlier detection algorithm to filter a parallel corpus; Xu and Koehn (2017) generate synthetic noisy data (inadequate and nonfluent translations) and use this data to train a classifier to identify good sentence pairs from a noisy corpus; and Cui et al. (2013) use a graph-based random walk algorithm and extract phrase pair scores to weight the phrase translation probabilities to bias towards more trustworthy ones. Most of this work was done in the context of statistical machine translation, but more recent work targets neural models. Carpuat et al. (2017) focus on identifying semantic differences in translation pairs using cross-lingual textual entailment and additio"
2020.wmt-1.78,2020.wmt-1.112,0,0.179267,"ive a short sum731 Pashto dev dev test test Khmer Sentence Pairs English Words Sentence Pairs English Words 3,162 2,698 2,719 55,439 46,175 47,695 2,378 2,309 2,320 40,436 44,471 40,341 Table 7: Statistics for the flores test sets used to evaluate the machine translation systems trained on the subsampled data sets. Word counts are obtained with wc on tokenized text. Short Name Participant and System Description Citation AFRL Alibaba Bytedance Edinburgh Huawei JHU-Kejriwal JHU-Koerner Microsoft NRC UA-Prompsit LASER Air Force Research Lab, USA Alibaba, China (Lu et al., 2020) Bytedance, China (Xu et al., 2020) University of Edinburgh, Scotland Huawei, Turkey/China (Ac¸arc¸ic¸ek et al., 2020) Ankur Kejriwal, Johns Hopkins University, USA (Kejriwal and Koehn, 2020) Felicia Koerner, Johns Hopkins University, USA (Koerner and Koehn, 2020) Microsoft, Egypt Development Center, Egypt (Nokrashy et al., 2020) National Research Council, Canada (Lo and Joanis, 2020) University of Alicante and Prompsit, Spain (Espl`a-Gomis et al., 2020) Officially provided baseline Table 8: Participants in the shared task. Dual cross entropy Neural machine translation systems trained on the provided clean parallel data can be"
2020.wmt-1.78,2020.emnlp-main.483,1,0.833336,"allel sentences were sourced from the CCAligned2 dataset (ElKishky et al., 2020a), a massive collection of cross-lingual web documents covering over 8k language pairs aligned from 68 Common Crawl snapshots. Additional parallel data was sourced from the Paracrawl project – a large-scale effort to crawl text from the web3 (Ba˜no´ n et al., 2020). Acquiring parallel corpora from the web (ElKishky et al., 2020b) is an active area of research that typically involves identifying web sites with parallel text, downloading the documents from the web site, aligning document pairs (Buck and Koehn, 2016; Thompson and Koehn, 2020; ElKishky and Guzm´an, 2020), and aligning sentence pairs. A final stage of the processing pipeline filters out non-parallel sentence pairs. Such noise exists either because the original web site did not have any actual parallel data (garbage in, garbage out), only partially-parallel data, or due to failures of processing steps. 1 http://opus.nlpl.eu http://statmt.org/cc-aligned 3 http://www.paracrawl.eu/ 2 4 NLP4TM 2016: Shared task http://rgcl.wlv.ac.uk/nlp4tm2016/shared-task/ 727 translation directions to score sentence pairs with dual cross-entropy. Last year, we moved the focus to low re"
2021.acl-long.66,N19-1121,0,0.0215805,"optimizes four tasks to perform low-resource translation: (1) denoising autoencoder (2) adversarial training (3) high-resource translation and (4) low-resource backtranslation. We test our proposed method and demonstrate its effectiveness in improving low-resource translation from three distinct families: (1) Iberian languages, (2) Indic languages, and (3) Semitic languages, specifically Arabic dialects. We make our code and resources publicly available.2 2 Related Work Zero-shot translation Our work is closely related to that of zero-shot translation (Johnson et al., 2017; Chen et al., 2017; Al-Shedivat and Parikh, 2019). However, while zero-shot translation translates between a language pair with no parallel data, there is an assumption that both languages in the target pair have some parallel data with other languages. As such, the system can learn to process both languages. In one work, Currey and Heafield (2019) improved zero-shot translation using monolingual data on the pivot language. However, in our scenario, there is no parallel data between the low-resource language and any other language. In other work, Arivazhagan et al. (2019) showed that adding adversarial training to the encoder output could he"
2021.acl-long.66,D18-1549,0,0.0199215,"age. However, in our scenario, there is no parallel data between the low-resource language and any other language. In other work, Arivazhagan et al. (2019) showed that adding adversarial training to the encoder output could help zero shot training. We adopt a similar philosophy in our multi-task training to ensure our low-resource target is in the same latent space as the higher-resource language. Unsupervised translation A related set of work is the family of unsupervised translation techniques; these approaches translate between language pairs with no parallel corpus of any kind. In work by Artetxe et al. (2018); Lample et al. (2018a), unsupervised translation is performed by training denoising autoencoding and backtranslation tasks concurrently. In these approaches, multiple pretraining methods were proposed to better initialize the model (Lample et al., 2018b; Lample and Conneau, 2019; Liu et al., 2020; Song et al., 2019). 2 https://github.com/wjko2/NMT-Adapt Different approaches were proposed that used parallel data between X-Y to improve unsupervised translation between X-Z (Garcia et al., 2020a; Li et al., 2020; Wang et al., 2020). This scenario differs from our setting as it does not assume tha"
2021.acl-long.66,2020.findings-emnlp.283,0,0.0611464,"es; these approaches translate between language pairs with no parallel corpus of any kind. In work by Artetxe et al. (2018); Lample et al. (2018a), unsupervised translation is performed by training denoising autoencoding and backtranslation tasks concurrently. In these approaches, multiple pretraining methods were proposed to better initialize the model (Lample et al., 2018b; Lample and Conneau, 2019; Liu et al., 2020; Song et al., 2019). 2 https://github.com/wjko2/NMT-Adapt Different approaches were proposed that used parallel data between X-Y to improve unsupervised translation between X-Z (Garcia et al., 2020a; Li et al., 2020; Wang et al., 2020). This scenario differs from our setting as it does not assume that Y and Z are similar languages. These approaches leverage a cross-translation method on a multilingual NMT model where for a parallel data pair (Sx ,Sy ), they translate Sx into language Z with the current model to get Sz0 . Then use (Sy ,Sz0 ) as an additional synthesized data pair to further improve the model. Garcia et al. (2020b) experiment using multilingual cross-translation on low-resource languages with some success. While these approaches view the parallel data as auxiliary, to sup"
2021.acl-long.66,P19-1121,0,0.0235806,"?? ??? ??? ??? ???? Figure 1: Illustration of the training tasks for translating from English into a low-resource language (LRL) and from an LRL to English. English to low-resource backtranslation data. The aim of this task is to capture a language-modeling effect in the low-resource language. We describe how we obtain this data using the high-resource translation model to bootstrap backtranslation in Section 3.3. The objective used is, 0 Lbt = LCE (D(ZEn , [LRL]), XLRL ) (discriminators). The critics are recurrent networks to ensure that they can handle variable-length text input. Similar to Gu et al. (2019b), the adversarial component is trained using a Wasserstein loss, which is the difference of expectations between the two types of data. This loss minimizes the earth mover’s distance between the distributions of different languages. We compute the loss function as follows: (3) 0 , where ZEn = E(YEn , [En]). (YEn , XLRL ) is an English to low-resource backtranslation pair. Ladv1 = E[Disc(ZHRL )] − E[Disc(ZLRL )] (4) Task 4: Adversarial Training The final task aims to make the encoder output language-agnostic features. The representation is language agnostic to the noised high and low-resource"
2021.acl-long.66,2020.findings-emnlp.371,0,0.015344,"ranslate between language pairs with no parallel corpus of any kind. In work by Artetxe et al. (2018); Lample et al. (2018a), unsupervised translation is performed by training denoising autoencoding and backtranslation tasks concurrently. In these approaches, multiple pretraining methods were proposed to better initialize the model (Lample et al., 2018b; Lample and Conneau, 2019; Liu et al., 2020; Song et al., 2019). 2 https://github.com/wjko2/NMT-Adapt Different approaches were proposed that used parallel data between X-Y to improve unsupervised translation between X-Z (Garcia et al., 2020a; Li et al., 2020; Wang et al., 2020). This scenario differs from our setting as it does not assume that Y and Z are similar languages. These approaches leverage a cross-translation method on a multilingual NMT model where for a parallel data pair (Sx ,Sy ), they translate Sx into language Z with the current model to get Sz0 . Then use (Sy ,Sz0 ) as an additional synthesized data pair to further improve the model. Garcia et al. (2020b) experiment using multilingual cross-translation on low-resource languages with some success. While these approaches view the parallel data as auxiliary, to supplement unsupervis"
2021.acl-long.66,2020.tacl-1.47,1,0.896063,"ng to ensure our low-resource target is in the same latent space as the higher-resource language. Unsupervised translation A related set of work is the family of unsupervised translation techniques; these approaches translate between language pairs with no parallel corpus of any kind. In work by Artetxe et al. (2018); Lample et al. (2018a), unsupervised translation is performed by training denoising autoencoding and backtranslation tasks concurrently. In these approaches, multiple pretraining methods were proposed to better initialize the model (Lample et al., 2018b; Lample and Conneau, 2019; Liu et al., 2020; Song et al., 2019). 2 https://github.com/wjko2/NMT-Adapt Different approaches were proposed that used parallel data between X-Y to improve unsupervised translation between X-Z (Garcia et al., 2020a; Li et al., 2020; Wang et al., 2020). This scenario differs from our setting as it does not assume that Y and Z are similar languages. These approaches leverage a cross-translation method on a multilingual NMT model where for a parallel data pair (Sx ,Sy ), they translate Sx into language Z with the current model to get Sz0 . Then use (Sy ,Sz0 ) as an additional synthesized data pair to further im"
2021.acl-long.66,D19-1632,1,0.901445,"Missing"
2021.acl-long.66,D18-1103,0,0.0196006,"to further improve the model. Garcia et al. (2020b) experiment using multilingual cross-translation on low-resource languages with some success. While these approaches view the parallel data as auxiliary, to supplement unsupervised NMT, our work looks at the problem from a domain adaptation perspective. We attempt to use monolingual data in Z to make the supervised model trained on X-Y generalize to Z. Leveraging High-resource Languages to Improve Low-resource Translation Several works have leveraged data in high-resource languages to improve the translation of similar low-resource languages. Neubig and Hu (2018) showed that it is beneficial to mix the limited parallel data pairs of low-resource languages with high-resource language data. Lakew et al. (2019) proposed selecting high-resource language data with lower perplexity in the low-resource language model. Xia et al. (2019) created synthetic sentence pairs by unsupervised machine translation, using the high-resource language as a pivot. However these previous approaches emphasize translating from the low-resource language to English, while the opposite direction is either unconsidered or shows poor translation performance. Siddhant et al. (2020)"
2021.acl-long.66,2013.iwslt-papers.2,1,0.837467,"Missing"
2021.acl-long.66,L18-1548,0,0.0979839,"Missing"
2021.acl-long.66,W18-6316,0,0.0238497,"n language could be mapped word by word into the dialect vocabulary, and they calculate the corresponding word for substitution using 803 localized projection. This approach differs from our work in that it relies on the existence of a seed bilingual lexicon to the dialect/similar language. Additionally, the approach only considers translating from a dialect to English and not the reverse direction. Other work trains a massively multilingual many-to-many model and demonstrates that high-resource training data improves related lowresource language translation (Fan et al., 2020). In other work, Lakew et al. (2018) compared ways to model translations of different language varieties, in the setting that parallel data for both varieties is available, the variety for some pairs may not be labeled. Another line of work focus on translating between similar languages. In one such work, Pourdamghani and Knight (2017) learned a character-based cipher model. In other work, Wan et al. (2020) improved unsupervised translation between the main language and the dialect by separating the token embeddings into pivot and private parts while performing layer coordination. 3 Method We describe the NMT-Adapt approach to t"
2021.acl-long.66,D17-1266,0,0.0184878,". Additionally, the approach only considers translating from a dialect to English and not the reverse direction. Other work trains a massively multilingual many-to-many model and demonstrates that high-resource training data improves related lowresource language translation (Fan et al., 2020). In other work, Lakew et al. (2018) compared ways to model translations of different language varieties, in the setting that parallel data for both varieties is available, the variety for some pairs may not be labeled. Another line of work focus on translating between similar languages. In one such work, Pourdamghani and Knight (2017) learned a character-based cipher model. In other work, Wan et al. (2020) improved unsupervised translation between the main language and the dialect by separating the token embeddings into pivot and private parts while performing layer coordination. 3 Method We describe the NMT-Adapt approach to translating a low-resource language into and out of English without utilizing any low-resource language parallel data. In Section 3.1, we describe how NMT-Adapt leverages a novel multi-task domain adaptation approach to translating English into a low-resource language. In Section 3.2, we then describe"
2021.acl-long.66,N18-2084,0,0.063027,"Missing"
2021.acl-long.66,2020.semeval-1.271,0,0.033269,"Missing"
2021.acl-long.66,P16-1009,0,0.165315,"Missing"
2021.acl-long.66,2020.acl-main.252,0,0.0202162,"s. Neubig and Hu (2018) showed that it is beneficial to mix the limited parallel data pairs of low-resource languages with high-resource language data. Lakew et al. (2019) proposed selecting high-resource language data with lower perplexity in the low-resource language model. Xia et al. (2019) created synthetic sentence pairs by unsupervised machine translation, using the high-resource language as a pivot. However these previous approaches emphasize translating from the low-resource language to English, while the opposite direction is either unconsidered or shows poor translation performance. Siddhant et al. (2020) trained multilingual translation and denoising simultaneously, and showed that the model could translate languages without parallel data into English near the performance of supervised multilingual NMT. Similar language translation Similar to our work, there have been methods proposed that leverage similar languages to improve translation. Hassan et al. (2017) generated synthetic English-dialect parallel data from English-main language corpus. However, this method assumes that the vocabulary in the main language could be mapped word by word into the dialect vocabulary, and they calculate the"
2021.acl-long.66,tiedemann-2012-parallel,0,0.262167,"Missing"
2021.acl-long.66,2020.lrec-1.494,1,0.836882,"Missing"
2021.acl-long.66,P19-1579,0,0.0175642,"domain adaptation perspective. We attempt to use monolingual data in Z to make the supervised model trained on X-Y generalize to Z. Leveraging High-resource Languages to Improve Low-resource Translation Several works have leveraged data in high-resource languages to improve the translation of similar low-resource languages. Neubig and Hu (2018) showed that it is beneficial to mix the limited parallel data pairs of low-resource languages with high-resource language data. Lakew et al. (2019) proposed selecting high-resource language data with lower perplexity in the low-resource language model. Xia et al. (2019) created synthetic sentence pairs by unsupervised machine translation, using the high-resource language as a pivot. However these previous approaches emphasize translating from the low-resource language to English, while the opposite direction is either unconsidered or shows poor translation performance. Siddhant et al. (2020) trained multilingual translation and denoising simultaneously, and showed that the model could translate languages without parallel data into English near the performance of supervised multilingual NMT. Similar language translation Similar to our work, there have been me"
2021.acl-long.66,P11-2007,0,0.0962119,"Missing"
2021.adaptnlp-1.21,Q17-1010,0,0.287427,"2019), and dependency parsing (Schuster et al., 2019), where dependency paring is scoped out in this paper. Compared with the delexicalized parsers (McDonald et al., 2011), multilingual word embeddings have been demonstrated to significantly improve the performance of zero-shot dependency parsing by bridging the lexical feature gap (Guo et al., 2015). With the remarkable development of monolingual contextual pre-trained models (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2019), which dramatically outperform static word embeddings (Mikolov et al., 2013a; Pennington et al., 2014; Bojanowski et al., 2017) in broad NLP applications, increasing number of researchers have started focusing on contextual representation alignment for cross-lingual dependency parsing (Schuster et al., 2019; Wang et al., 2019). Moreover, with the appearance of multilingual pre-trained models, such as Multilingual BERT (mBERT) (Devlin et al., 2019), zero-shot dependency parsing becomes easier by utilizing the large vocabulary of the multilingual models (Kondratyuk and Straka, 2019). Our approach is most similar to Schuster et al. (2019), which maps a target language space into a source language space through a linear t"
2021.adaptnlp-1.21,J93-2003,0,0.142279,"Missing"
2021.adaptnlp-1.21,N19-1423,0,0.00708272,"ctively facilitated by shared semantic spaces in NLP tasks, e.g., named entity recognition (Xie et al., 2018), part-of-speech tagging (Hsu et al., 2019), and dependency parsing (Schuster et al., 2019), where dependency paring is scoped out in this paper. Compared with the delexicalized parsers (McDonald et al., 2011), multilingual word embeddings have been demonstrated to significantly improve the performance of zero-shot dependency parsing by bridging the lexical feature gap (Guo et al., 2015). With the remarkable development of monolingual contextual pre-trained models (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2019), which dramatically outperform static word embeddings (Mikolov et al., 2013a; Pennington et al., 2014; Bojanowski et al., 2017) in broad NLP applications, increasing number of researchers have started focusing on contextual representation alignment for cross-lingual dependency parsing (Schuster et al., 2019; Wang et al., 2019). Moreover, with the appearance of multilingual pre-trained models, such as Multilingual BERT (mBERT) (Devlin et al., 2019), zero-shot dependency parsing becomes easier by utilizing the large vocabulary of the multilingual models (Kondratyuk and St"
2021.adaptnlp-1.21,P81-1022,0,0.541865,"Missing"
2021.adaptnlp-1.21,N13-1073,0,0.0172481,"mproved with the orthogonal restriction, i.e, W T W = I. Thus, the problem can be solved by Procrustes approach (Sch¨onemann, 1966): ˆ = arg min kW X − Y kF = U V T W W ∈Od×d s.t. U ΣV (2) T T = svd(Y X ) where Od×d is the set of orthogonal matrices. 3 3.1 Method Contextual Embedding Transformation An unsupervised bidirectional word alignment algorithm based on IBM Model 2 (Brown et al., 1 Code is available at: https://github.com/ fe1ixxu/ZeroShot-CrossLing-Parsing. 2 Different from usual settings, we use x-related symbols for target data and y-related ones for source data. 1993), Fast Align (Dyer et al., 2013), is first applied to a parallel corpus to derive silver aligned token pairs. We then respectively feed the parallel corpus to the BERTs of the target and the source languages and extract the outputs as contextual embeddings. As shown in Figure 1, Fast Align bridges “links” between silver token pairs, and between the embeddings of the token pairs as well. Thus, for each target type, a collection of its contextual embeddings can be obtained, as well as a collection of contextual embeddings of its aligned source tokens. Vectors are normalized to satisfy the orthogonal condition. Motivated by the"
2021.adaptnlp-1.21,D19-1006,0,0.0184842,"l embedding alignment, while we explore a sense-level embedding alignment method to map bilingual spaces more precisely, where sense-level representations are split from multi-sense word-level embeddings. Furthermore, our mapping approach is dictionary-free which utilizes the silver token pairs from parallel corpora and eliminates the necessity of gold dictionaries. The experimental results of zero-shot dependency parsing demonstrate that two parser evaluation scores (UAS and LAS) of sense-level mapping are always better than of word-level one. Moreover, we also notice the anisotropy problem (Ethayarajh, 2019) (defined in Section 3.2) in contextual embeddings, which potentially deteriorate the performance of the zero-shot transfer task. We significantly mitigate this drawback by leveraging a prepossessing step, iterative normalization (IN) (Zhang et al., 2019), which is originally used for improving the performance of static embedding mapping on the bilingual dictionary induction task. Zero-shot dependency parsing experiments are conducted on Universal Dependencies treebank 204 Proceedings of the Second Workshop on Domain Adaptation for NLP, pages 204–213 April 20, 2021. ©2021 Association for Compu"
2021.adaptnlp-1.21,P15-1119,0,0.0306718,"015; Conneau et al., 2018) recently has been attracted a lot of attention because cross-lingual model transfer is effectively facilitated by shared semantic spaces in NLP tasks, e.g., named entity recognition (Xie et al., 2018), part-of-speech tagging (Hsu et al., 2019), and dependency parsing (Schuster et al., 2019), where dependency paring is scoped out in this paper. Compared with the delexicalized parsers (McDonald et al., 2011), multilingual word embeddings have been demonstrated to significantly improve the performance of zero-shot dependency parsing by bridging the lexical feature gap (Guo et al., 2015). With the remarkable development of monolingual contextual pre-trained models (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2019), which dramatically outperform static word embeddings (Mikolov et al., 2013a; Pennington et al., 2014; Bojanowski et al., 2017) in broad NLP applications, increasing number of researchers have started focusing on contextual representation alignment for cross-lingual dependency parsing (Schuster et al., 2019; Wang et al., 2019). Moreover, with the appearance of multilingual pre-trained models, such as Multilingual BERT (mBERT) (Devlin et al., 2019), zer"
2021.adaptnlp-1.21,D19-1607,0,0.0234874,"nisotropy problem and its solution. Experiments on zero-shot dependency parsing through the concept-shared space built by our embedding transformation substantially outperform state-of-the-art methods using multilingual embeddings. 1 Introduction Cross-lingual embedding space alignment (Mikolov et al., 2013b; Artetxe et al., 2016; Xing et al., 2015; Conneau et al., 2018) recently has been attracted a lot of attention because cross-lingual model transfer is effectively facilitated by shared semantic spaces in NLP tasks, e.g., named entity recognition (Xie et al., 2018), part-of-speech tagging (Hsu et al., 2019), and dependency parsing (Schuster et al., 2019), where dependency paring is scoped out in this paper. Compared with the delexicalized parsers (McDonald et al., 2011), multilingual word embeddings have been demonstrated to significantly improve the performance of zero-shot dependency parsing by bridging the lexical feature gap (Guo et al., 2015). With the remarkable development of monolingual contextual pre-trained models (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2019), which dramatically outperform static word embeddings (Mikolov et al., 2013a; Pennington et al., 2014; Bojano"
2021.adaptnlp-1.21,P19-1356,0,0.0133316,"sically decrease to 0 at the first iteration and converge afterwards. mBERT: We compare our approach with both uncased and cased version of mBERT. Outputs of mBERT are directly used for the embedding layer. 4.2 sense-level embeddings only if the token occurs more than 100 times. Otherwise, the representation for the token is the basic word-level embedding, i.e., the mean vector of its vector collection. Experiments of word-level embedding alignment are also conducted to compare with sense-level results. Settings Following the analysis that top layers of BERT contain more semantic information (Jawahar et al., 2019), our contextual representation are normalized mean vector of the last 4 layers of BERT. The parallel corpora used to extract contextual embeddings are obtained from ParaCrawl v6.0 3 . For each language pairs, we select 1M parallel sentences whose length is shorter than 150. Since some noisy alignments are produced during Fast Align, we only take one-to-one token alignment into consideration. The dataset used for cross-lingual dependency parsing is the Universal Dependencies treebank v2.6 4 (Zeman et al., 2020). We store up to 10K contextual vectors extracted from BERT for non-OOV tokens 5 . V"
2021.adaptnlp-1.21,D18-1330,0,0.0361356,"Missing"
2021.adaptnlp-1.21,N18-1202,0,0.0234905,"odel transfer is effectively facilitated by shared semantic spaces in NLP tasks, e.g., named entity recognition (Xie et al., 2018), part-of-speech tagging (Hsu et al., 2019), and dependency parsing (Schuster et al., 2019), where dependency paring is scoped out in this paper. Compared with the delexicalized parsers (McDonald et al., 2011), multilingual word embeddings have been demonstrated to significantly improve the performance of zero-shot dependency parsing by bridging the lexical feature gap (Guo et al., 2015). With the remarkable development of monolingual contextual pre-trained models (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2019), which dramatically outperform static word embeddings (Mikolov et al., 2013a; Pennington et al., 2014; Bojanowski et al., 2017) in broad NLP applications, increasing number of researchers have started focusing on contextual representation alignment for cross-lingual dependency parsing (Schuster et al., 2019; Wang et al., 2019). Moreover, with the appearance of multilingual pre-trained models, such as Multilingual BERT (mBERT) (Devlin et al., 2019), zero-shot dependency parsing becomes easier by utilizing the large vocabulary of the multilingual mode"
2021.adaptnlp-1.21,D14-1162,0,0.0879081,"eech tagging (Hsu et al., 2019), and dependency parsing (Schuster et al., 2019), where dependency paring is scoped out in this paper. Compared with the delexicalized parsers (McDonald et al., 2011), multilingual word embeddings have been demonstrated to significantly improve the performance of zero-shot dependency parsing by bridging the lexical feature gap (Guo et al., 2015). With the remarkable development of monolingual contextual pre-trained models (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2019), which dramatically outperform static word embeddings (Mikolov et al., 2013a; Pennington et al., 2014; Bojanowski et al., 2017) in broad NLP applications, increasing number of researchers have started focusing on contextual representation alignment for cross-lingual dependency parsing (Schuster et al., 2019; Wang et al., 2019). Moreover, with the appearance of multilingual pre-trained models, such as Multilingual BERT (mBERT) (Devlin et al., 2019), zero-shot dependency parsing becomes easier by utilizing the large vocabulary of the multilingual models (Kondratyuk and Straka, 2019). Our approach is most similar to Schuster et al. (2019), which maps a target language space into a source languag"
2021.emnlp-main.539,2020.acl-main.747,0,0.030344,"975). The state-of-the-art approach to this task is based on the Predictor-Estimator (PredEst) architecture (Kim et al., 2017; Kepler et al., 2019). At a very high level, the predictor training uses a crosslingual masked language model (MLM) objective, which trains the model to predict a word in the target sentence given the source and both the left and right context on the target side. An estimator is then finetuned from the predictor model to predict word-level QE tags. In recent years, the top-ranking systems also incorporate large-scale pre-trained crosslingual encoder such as XLMRoBERTa (Conneau et al., 2020), glass-box features (Moura et al., 2020) and pseudo post-editing data augmentation (Wei et al., 2020; Lee, 2020). The Levenshtein Transformer (LevT, Gu et al., 2019) is a neural network architecture that can iteratively generate sequences in a non-autoregressive manner. Unlike normal autoregressive sequence models that have only one prediction head Aw to predict the next output words, LevT has two extra prediction heads Adel and Ains that predicts deletion and insertion operations based on the output sequence from the previous iteration. For translation generation, at the k-th iteration durin"
2021.emnlp-main.539,2020.wmt-1.117,0,0.0681255,"Missing"
2021.emnlp-main.814,C18-1139,0,0.0227933,"multilingual web corpora (ElKishky et al., 2020b). In particular, we select three mined web corpora 1) CCAligned (El-Kishky et al., 2020a), 2) WikiMatrix (Schwenk et al., 2019a), and 3) CCMatrix (Schwenk et al., 2019b)) due to the wide diversity of language pairs available in these mined corpora. We select language pairs of the form English-Target and tag each English sentence with named entity tags (Ramshaw and Marcus, 1999) using a pretrained NER tagger provided in the Stanza NLP toolkit3 (Qi et al., 2020). This NER model adopts a contextualized string representationbased tagger proposed by Akbik et al. (2018) and utilizes a forward and backward character-level LSTM language model. At tagging time, the representation at the end of each word position from both language models with word embeddings is fed into a standard Bi-LSTM sequence tagger with a conditional-random-field decoder. Model 2 (Brown et al., 1993) and symmetrize alignments using the grow-diagonal-final-and (GDFA) heuristic. FastAlign performs unsupervised word alignment over the full collection of mined bitexts using an expectation maximization based algorithm. While FastAlign is state-of-the-art in word alignment, due to its reliance"
2021.emnlp-main.814,Q19-1038,0,0.0185475,"with word embeddings is fed into a standard Bi-LSTM sequence tagger with a conditional-random-field decoder. Model 2 (Brown et al., 1993) and symmetrize alignments using the grow-diagonal-final-and (GDFA) heuristic. FastAlign performs unsupervised word alignment over the full collection of mined bitexts using an expectation maximization based algorithm. While FastAlign is state-of-the-art in word alignment, due to its reliance on lexical co-occurences, it may misalign low-frequency entities. 3.2.2 Semantic Alignment We leverage multilingual representations (embeddings) from the LASER toolkit (Artetxe and Schwenk, 2019) to align words that are semantically close. We propose a simple greedy word alignment algorithm guided by a distance function between words: v ? · v? ???(? ? , ? ? ) = 1 − (1) ||v? |v? || Algorithm 1: Distance Word Alignment Input: ? = {(? ? , ? ? ) |? ? ∈ ? ? , ? ? ∈ ? ? } Output: ? 0 = {(? ?,? , ? ? ,? ), ...} ⊂ ? 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 ????−???? ? ← {( ?, ???? ( ?)) for ? ∈ ?} ?????? ← ???? (????−???? ?) in ascending order aligned, ? ? , ?? ← ∅, ∅, ∅ ? ??? ← ||? ? |− |? ? || for ? ? , ? ? ∈ sorted do if ? ? ∉ ? ? ∧ ? ? ∉ ? ? then ??????? ← ??????? ∪ {(? ? , ? ? )}"
2021.emnlp-main.814,W15-3902,0,0.0263025,"lexicon created by (Pan et al., 2017) that leverages eight named parallel entity corpora4. We select nine languages from a diverse set of resource availability, language families, and scripts for evaluation. Evaluation Protocol We evaluated the performance of the methods using the commonly used fuzzy-f1 score (Tsai and Roth, 2018) which is defined as the harmonic mean of the fuzzy precision and fuzzy recall scores. This metric is based on the longest common subsequence between a gold and mined entity, and has been used for several years in the NEWS transliteration workshops (Li et al., 2009; Banchs et al., 2015). The fuzzy precision and recall between a predicted string ? and the correct string ? is computed as follows: fuzzy−precision( ?, ?) = fuzzy−recall( ?, ?) = |LCS( ?,?) |/ |? |, |LCS( ?,?) |/|? |, where ???( ·, · ) is the longest common subsequence between two strings. 4.1 Cross-lingual Entity Extraction We take a small sample of parallel sentences for each language, mine entity pairs using each projection technique, and compute Fuzzy-F1 using the gold-standard as a reference. As seen in Table 1, while lexical alignment outperforms semantic alignment, it displays similar performance to phoneti"
2021.emnlp-main.814,J93-2003,0,0.0909015,"t language pairs of the form English-Target and tag each English sentence with named entity tags (Ramshaw and Marcus, 1999) using a pretrained NER tagger provided in the Stanza NLP toolkit3 (Qi et al., 2020). This NER model adopts a contextualized string representationbased tagger proposed by Akbik et al. (2018) and utilizes a forward and backward character-level LSTM language model. At tagging time, the representation at the end of each word position from both language models with word embeddings is fed into a standard Bi-LSTM sequence tagger with a conditional-random-field decoder. Model 2 (Brown et al., 1993) and symmetrize alignments using the grow-diagonal-final-and (GDFA) heuristic. FastAlign performs unsupervised word alignment over the full collection of mined bitexts using an expectation maximization based algorithm. While FastAlign is state-of-the-art in word alignment, due to its reliance on lexical co-occurences, it may misalign low-frequency entities. 3.2.2 Semantic Alignment We leverage multilingual representations (embeddings) from the LASER toolkit (Artetxe and Schwenk, 2019) to align words that are semantically close. We propose a simple greedy word alignment algorithm guided by a di"
2021.emnlp-main.814,I17-2016,0,0.0121682,"nd multilingual natural language processing (NLP) (Sekine and Ranchhod, 2009). As such, cross-lingual named entity lexica can be invaluable resources towards making tasks such as entity linking, named entity recognition (Ren et al., 2016b,a), and information and knowledge base construction (Tao et al., 2014) inherently multilingual. However, the coverage of many such multilingual entity lexica (e.g., Wikipedia titles) is less complete for lower-resource languages and approaches to automatically generate them under-perform due to the poor performance of low-resource taggers (Feng et al., 2018; Cotterell and Duh, 2017). To perform low-resource NER, previous efforts 1http://data.statmt.org/xlent/ have applied word alignment techniques to project 2https://opus.nlpl.eu/XLEnt-v1.1.php 10424 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10424–10430 c November 7–11, 2021. 2021 Association for Computational Linguistics 2 Preliminaries We formally define an entity collection as a collection of extracted text spans tied to named entity mentions. We denote these named entity mentions ? , where ?? is the ? named entity as ? = {?? ? }?=1 ? ?ℎ in the mention collection ? a"
2021.emnlp-main.814,P11-1061,0,0.00994268,"onstrate LSP-Align outperforms baselines at extracting cross-lingual entity pairs and mine 164 million entity pairs from 120 different languages aligned with English. We release these cross-lingual entity pairs along with the massively multilingual tagged named entity corpus as a resource to the NLP community. 1 Introduction Figure 1: Identify entity pairs by projecting English entities onto lower-resource languages via word-alignment. available labels to other languages. Kim et al. (2010) applies heuristic approaches with alignment correction using an alignment dictionary of entity mentions. Das and Petrov (2011) introduced a novel label propagation technique that creates a tag lexicon for the target language, while Wang and Manning (2014) instead projected model expectation rather than labels thus transferring word boundary uncertainty. Additional work jointly performs word alignment while training bilingual name tagging (Wang et al., 2013); however this method assumes the availability of named entity taggers in both languages. Other methods have leveraged bilingual embeddings for projection (Ni et al., 2017; Xie et al., 2018). In this work, we propose using named-entity projection to automatically c"
2021.emnlp-main.814,N13-1073,0,0.0803429,"Missing"
2021.emnlp-main.814,2020.emnlp-main.480,1,0.72757,"llections should be generated such that for each entity mention in ?? ? ∈ ?1 in the source language, there is a corresponding named entity ?? ? ∈ ?2 in the target language such that ?? ? and ?? ? refer to the same named entity in their respective language. 3 Mining Cross-lingual Entities We introduce our approach to automatically extract cross-lingual entity pairs from large mined corpora. 3.1 High-Resource NER We begin with large collections of comparable bitexts mined from large multilingual web corpora (ElKishky et al., 2020b). In particular, we select three mined web corpora 1) CCAligned (El-Kishky et al., 2020a), 2) WikiMatrix (Schwenk et al., 2019a), and 3) CCMatrix (Schwenk et al., 2019b)) due to the wide diversity of language pairs available in these mined corpora. We select language pairs of the form English-Target and tag each English sentence with named entity tags (Ramshaw and Marcus, 1999) using a pretrained NER tagger provided in the Stanza NLP toolkit3 (Qi et al., 2020). This NER model adopts a contextualized string representationbased tagger proposed by Akbik et al. (2018) and utilizes a forward and backward character-level LSTM language model. At tagging time, the representation at the"
2021.emnlp-main.814,C10-1064,0,0.0104531,"antic-Phonetic Align (LSP-Align), a technique to automatically mine cross-lingual entity lexica from mined web data. We demonstrate LSP-Align outperforms baselines at extracting cross-lingual entity pairs and mine 164 million entity pairs from 120 different languages aligned with English. We release these cross-lingual entity pairs along with the massively multilingual tagged named entity corpus as a resource to the NLP community. 1 Introduction Figure 1: Identify entity pairs by projecting English entities onto lower-resource languages via word-alignment. available labels to other languages. Kim et al. (2010) applies heuristic approaches with alignment correction using an alignment dictionary of entity mentions. Das and Petrov (2011) introduced a novel label propagation technique that creates a tag lexicon for the target language, while Wang and Manning (2014) instead projected model expectation rather than labels thus transferring word boundary uncertainty. Additional work jointly performs word alignment while training bilingual name tagging (Wang et al., 2013); however this method assumes the availability of named entity taggers in both languages. Other methods have leveraged bilingual embedding"
2021.emnlp-main.814,W09-3501,0,0.0477237,"andard evaluation lexicon created by (Pan et al., 2017) that leverages eight named parallel entity corpora4. We select nine languages from a diverse set of resource availability, language families, and scripts for evaluation. Evaluation Protocol We evaluated the performance of the methods using the commonly used fuzzy-f1 score (Tsai and Roth, 2018) which is defined as the harmonic mean of the fuzzy precision and fuzzy recall scores. This metric is based on the longest common subsequence between a gold and mined entity, and has been used for several years in the NEWS transliteration workshops (Li et al., 2009; Banchs et al., 2015). The fuzzy precision and recall between a predicted string ? and the correct string ? is computed as follows: fuzzy−precision( ?, ?) = fuzzy−recall( ?, ?) = |LCS( ?,?) |/ |? |, |LCS( ?,?) |/|? |, where ???( ·, · ) is the longest common subsequence between two strings. 4.1 Cross-lingual Entity Extraction We take a small sample of parallel sentences for each language, mine entity pairs using each projection technique, and compute Fuzzy-F1 using the gold-standard as a reference. As seen in Table 1, while lexical alignment outperforms semantic alignment, it displays similar"
2021.emnlp-main.814,P17-1135,0,0.0450091,"Missing"
2021.emnlp-main.814,P17-1178,0,0.0178312,"xical Semantic Phonetic LSP-Align Russian Chinese Turkish 3.2M 5.2M 2.5M 40.4K 28.4K 27.4K 0.84 0.85 0.88 0.81 0.78 0.89 0.83 0.73 0.87 0.86 0.85 0.90 Arabic Hindi Romanian 4.9M 1.2M 2.1M 26.4K 7.60K 26.2K 0.88 0.89 0.93 0.80 0.73 0.94 0.81 0.87 0.92 0.88 0.90 0.94 Estonian Armenian Tamil 1.3M 52K 45K 15.2K 2.30K 2.50K 0.87 0.78 0.67 0.89 0.44 0.50 0.87 0.83 0.71 0.89 0.81 0.72 Avg - - 0.84 0.75 0.83 0.86 Table 1: Fuzzy-F1 scores of mined cross-lingual entity pairs evaluated against gold-standard pairs. 4 Experiments & Results Datasets We utilize a gold standard evaluation lexicon created by (Pan et al., 2017) that leverages eight named parallel entity corpora4. We select nine languages from a diverse set of resource availability, language families, and scripts for evaluation. Evaluation Protocol We evaluated the performance of the methods using the commonly used fuzzy-f1 score (Tsai and Roth, 2018) which is defined as the harmonic mean of the fuzzy precision and fuzzy recall scores. This metric is based on the longest common subsequence between a gold and mined entity, and has been used for several years in the NEWS transliteration workshops (Li et al., 2009; Banchs et al., 2015). The fuzzy precis"
2021.emnlp-main.814,2020.acl-demos.14,0,0.0123405,"corpora. 3.1 High-Resource NER We begin with large collections of comparable bitexts mined from large multilingual web corpora (ElKishky et al., 2020b). In particular, we select three mined web corpora 1) CCAligned (El-Kishky et al., 2020a), 2) WikiMatrix (Schwenk et al., 2019a), and 3) CCMatrix (Schwenk et al., 2019b)) due to the wide diversity of language pairs available in these mined corpora. We select language pairs of the form English-Target and tag each English sentence with named entity tags (Ramshaw and Marcus, 1999) using a pretrained NER tagger provided in the Stanza NLP toolkit3 (Qi et al., 2020). This NER model adopts a contextualized string representationbased tagger proposed by Akbik et al. (2018) and utilizes a forward and backward character-level LSTM language model. At tagging time, the representation at the end of each word position from both language models with word embeddings is fed into a standard Bi-LSTM sequence tagger with a conditional-random-field decoder. Model 2 (Brown et al., 1993) and symmetrize alignments using the grow-diagonal-final-and (GDFA) heuristic. FastAlign performs unsupervised word alignment over the full collection of mined bitexts using an expectation"
2021.emnlp-main.814,P13-1106,0,0.0208148,"dentify entity pairs by projecting English entities onto lower-resource languages via word-alignment. available labels to other languages. Kim et al. (2010) applies heuristic approaches with alignment correction using an alignment dictionary of entity mentions. Das and Petrov (2011) introduced a novel label propagation technique that creates a tag lexicon for the target language, while Wang and Manning (2014) instead projected model expectation rather than labels thus transferring word boundary uncertainty. Additional work jointly performs word alignment while training bilingual name tagging (Wang et al., 2013); however this method assumes the availability of named entity taggers in both languages. Other methods have leveraged bilingual embeddings for projection (Ni et al., 2017; Xie et al., 2018). In this work, we propose using named-entity projection to automatically curate a large crosslingual entity lexicon for many language pairs. As shown Figure 1, we construct this resource by performing NER in a higher-resource language, then projecting the entities onto text in a lowerresource language using word-alignment models. Our main contribution is the construction and release of a large web-mined cr"
2021.emnlp-main.814,Q14-1005,0,0.0190939,"fferent languages aligned with English. We release these cross-lingual entity pairs along with the massively multilingual tagged named entity corpus as a resource to the NLP community. 1 Introduction Figure 1: Identify entity pairs by projecting English entities onto lower-resource languages via word-alignment. available labels to other languages. Kim et al. (2010) applies heuristic approaches with alignment correction using an alignment dictionary of entity mentions. Das and Petrov (2011) introduced a novel label propagation technique that creates a tag lexicon for the target language, while Wang and Manning (2014) instead projected model expectation rather than labels thus transferring word boundary uncertainty. Additional work jointly performs word alignment while training bilingual name tagging (Wang et al., 2013); however this method assumes the availability of named entity taggers in both languages. Other methods have leveraged bilingual embeddings for projection (Ni et al., 2017; Xie et al., 2018). In this work, we propose using named-entity projection to automatically curate a large crosslingual entity lexicon for many language pairs. As shown Figure 1, we construct this resource by performing NE"
2021.emnlp-main.814,D18-1034,0,0.0116825,"alignment correction using an alignment dictionary of entity mentions. Das and Petrov (2011) introduced a novel label propagation technique that creates a tag lexicon for the target language, while Wang and Manning (2014) instead projected model expectation rather than labels thus transferring word boundary uncertainty. Additional work jointly performs word alignment while training bilingual name tagging (Wang et al., 2013); however this method assumes the availability of named entity taggers in both languages. Other methods have leveraged bilingual embeddings for projection (Ni et al., 2017; Xie et al., 2018). In this work, we propose using named-entity projection to automatically curate a large crosslingual entity lexicon for many language pairs. As shown Figure 1, we construct this resource by performing NER in a higher-resource language, then projecting the entities onto text in a lowerresource language using word-alignment models. Our main contribution is the construction and release of a large web-mined cross-lingual entity dataset that will be beneficial to the NLP community. Our proposed alignment model, LSP-Align, principally combines the lexical, semantic, and phonetic signals to extract"
2021.findings-emnlp.64,D18-1214,0,0.0169699,"ions that co-occur frequently in another. Diab and Finch (2000) extend this by measuring similarity between words based on co-occurrence vectors and matching words across language by preserving these similarities. Mikolov et al. (2013) are the first to perform BLI over word embeddings, estimating the transformation matrix using stochastic gradient descent. Most recent work solves a variation of the generalized Procrustes problem (e.g., Conneau et al., 2018; Artetxe et al., 2016b, 2017; Patra et al., 2019; Artetxe et al., 2018b; Doval et al., 2018; Joulin et al., 2018; Jawanpuria et al., 2019; Alvarez-Melis and Jaakkola, 2018). Zhang et al. (2020) learn a mapping that overfits to training pairs thus enforcing “hard-seeding"", while Ruder et al. (2018) enforce a one-to-one constraint on the output for BLI. Some BLI work uses graph based methods implicitly or explicitly. Artetxe et al. (2018a) form an initial solution with similarity matrices and refine with iterative Procrustes. Grave et al. (2019b) optimize “Procrustes in Wasserstein Distance"", employing a quadratic assignment formulation and the Frank-Wolfe method. Ren et al. (2020) form CSLS 8 Related Work similarity matrices, iteratively extract cliques, and Matc"
2021.findings-emnlp.64,Q17-1010,0,0.089593,"Missing"
2021.findings-emnlp.64,W95-0114,0,0.638172,"paces, and for well-trained embeddings in mismatched domains (e.g., Marchisio et al., 2020). In these cases, SGM might benefit from a different distance metric. A detailed analysis should be performed when data is many-to-many, as translation is naturally a many-to-many task. One might revisit word vectors based on co-occurrence statistics. The size of training and test sets should be increased, as the presence of more synonyms/antonyms and other “distractor"" words may elicit different behavior. There are computational considerations as we scale-up, particularly for SGM. tics. Rapp (1995) and Fung (1995) induce bilingual lexica based on the principle that words that frequently co-occur in one language have translations that co-occur frequently in another. Diab and Finch (2000) extend this by measuring similarity between words based on co-occurrence vectors and matching words across language by preserving these similarities. Mikolov et al. (2013) are the first to perform BLI over word embeddings, estimating the transformation matrix using stochastic gradient descent. Most recent work solves a variation of the generalized Procrustes problem (e.g., Conneau et al., 2018; Artetxe et al., 2016b, 20"
2021.findings-emnlp.64,P19-1070,0,0.033764,"Missing"
2021.findings-emnlp.64,P19-1143,0,0.062845,"etween points are unchanged by the transform and a closed form solution can be computed by singular value decomposition (Schönemann, 1966). Once languages are mapped to the same space by W, nearest neighbor search finds additional translation pairs. If W is known, one can find translations by optimizing over permutations Π: min ||XW − PY||2F P∈Π (2) P ∈ {0, 1}n×n is permutation matrix that shuffles the rows of Y. If we enforce the 1-to-1 correspondence, this is linear assignment problem that is solvable in polynomial time, e.g. with the Hungarian algorithm (Kuhn, 1955) or Wasserstein methods (Grave et al., 2019a). In the NLP literature, a large number of methods are based on the same underlying idea of linear transform followed by correspondence search/matching (see Related Work). To extract lexicons, one performs nearest neighbor search on the transformed embeddings. To mitigate the hubness problem (where some words are close to too many others) (Radovanovic et al., 2010; Suzuki et al., 2013), Conneau et al. (2018) modifies the similarity using cross-domain similarity local scaling (CSLS) to penalize hubs. For x, y in embedding space V : CSLS(x, y) = 2 cos(x, y) − avg(x, k) − avg(y, k) avg(v, k) ="
2021.findings-emnlp.64,P08-1088,0,0.0348171,"rsus transform that maps the spaces. The graph-based graph-based approaches to BLI under differview works with graphs for each language and ing data conditions and show that they comdirectly performs matching on edge pairs based plement each other when combined. We reon neighborhood information. This view is exemlease our code at https://github.com/ plified by graph matching methods that solve the kellymarchisio/euc-v-graph-bli. quadratic assignment problem from the combina1 Introduction torial optimization literature. Ruder et al. (2018) Bilingual lexicons are useful in many natural lan- and Haghighi et al. (2008) incorporate related techguage processing tasks including constrained de- niques for bilingual lexicon induction. We use coding in machine translation, cross-lingual infor- Seeded Graph Matching (SGM; Fishkind et al., mation retrieval, and unsupervised machine trans- 2019) as representative of this class of approach. lation. There is a large literature inducing bilin- Figure 1 illustrates the differences between the gual lexicons from cross-lingual spaces. “Map- framings; while they both exploit the idea that words with similar neighbors (in Euclidean or ping"" methods based on solving the orth"
2021.findings-emnlp.64,Q19-1007,0,0.0167937,"ne language have translations that co-occur frequently in another. Diab and Finch (2000) extend this by measuring similarity between words based on co-occurrence vectors and matching words across language by preserving these similarities. Mikolov et al. (2013) are the first to perform BLI over word embeddings, estimating the transformation matrix using stochastic gradient descent. Most recent work solves a variation of the generalized Procrustes problem (e.g., Conneau et al., 2018; Artetxe et al., 2016b, 2017; Patra et al., 2019; Artetxe et al., 2018b; Doval et al., 2018; Joulin et al., 2018; Jawanpuria et al., 2019; Alvarez-Melis and Jaakkola, 2018). Zhang et al. (2020) learn a mapping that overfits to training pairs thus enforcing “hard-seeding"", while Ruder et al. (2018) enforce a one-to-one constraint on the output for BLI. Some BLI work uses graph based methods implicitly or explicitly. Artetxe et al. (2018a) form an initial solution with similarity matrices and refine with iterative Procrustes. Grave et al. (2019b) optimize “Procrustes in Wasserstein Distance"", employing a quadratic assignment formulation and the Frank-Wolfe method. Ren et al. (2020) form CSLS 8 Related Work similarity matrices, it"
2021.findings-emnlp.64,D18-1330,0,0.0123565,"quently co-occur in one language have translations that co-occur frequently in another. Diab and Finch (2000) extend this by measuring similarity between words based on co-occurrence vectors and matching words across language by preserving these similarities. Mikolov et al. (2013) are the first to perform BLI over word embeddings, estimating the transformation matrix using stochastic gradient descent. Most recent work solves a variation of the generalized Procrustes problem (e.g., Conneau et al., 2018; Artetxe et al., 2016b, 2017; Patra et al., 2019; Artetxe et al., 2018b; Doval et al., 2018; Joulin et al., 2018; Jawanpuria et al., 2019; Alvarez-Melis and Jaakkola, 2018). Zhang et al. (2020) learn a mapping that overfits to training pairs thus enforcing “hard-seeding"", while Ruder et al. (2018) enforce a one-to-one constraint on the output for BLI. Some BLI work uses graph based methods implicitly or explicitly. Artetxe et al. (2018a) form an initial solution with similarity matrices and refine with iterative Procrustes. Grave et al. (2019b) optimize “Procrustes in Wasserstein Distance"", employing a quadratic assignment formulation and the Frank-Wolfe method. Ren et al. (2020) form CSLS 8 Related Wor"
2021.findings-emnlp.64,2020.wmt-1.68,1,0.865857,"with or construct two graphs and try to match vertices such that neighborhood structure is preserved. Intuitively, the motivation of preserving neighborhood structure is the same as Procrustes methods, but the absence of linear transform W is an important distinction that potentially makes graph matching more flexible. Indeed, some recent BLI work argue against linear transforms (Mohiuddin et al., 2020) and discuss the failure modes due to lack of isometry (Søgaard et al., 2018; Nakashole and Flauger, 2018; Ormazabal et al., 2019; Glavaš et al., 2019; Vuli´c et al., 2019; Patra et al., 2019; Marchisio et al., 2020). For BLI, we may build the graphs as Gx = XXT and Gy = YYT . For standard graph matching objectives, we restrict the vocabularies of X and Y to equal size, thus Gx , Gy ∈ Rn×n . We find the optimal relabeling of nodes such that: min ||Gx − PGy PT ||2F P∈Π (3) This is an instance of the quadratic assignment problem and is much harder than Eq. 2. It is NPHard (Sahni and Gonzalez, 1976) but various approximation methods exist. Vogelstein et al. (2015) use the Frank-Wolfe method (Frank et al., 1956) to find an approximate doubly-stochastic solution, then project onto the space of permutation matr"
2021.findings-emnlp.64,2020.emnlp-main.215,0,0.0121005,"tion, network science, and computer vision, there exist a large body of related work termed “graph matching."" Rather than assuming the existence of a linear transform between the embedding spaces, these methods start with or construct two graphs and try to match vertices such that neighborhood structure is preserved. Intuitively, the motivation of preserving neighborhood structure is the same as Procrustes methods, but the absence of linear transform W is an important distinction that potentially makes graph matching more flexible. Indeed, some recent BLI work argue against linear transforms (Mohiuddin et al., 2020) and discuss the failure modes due to lack of isometry (Søgaard et al., 2018; Nakashole and Flauger, 2018; Ormazabal et al., 2019; Glavaš et al., 2019; Vuli´c et al., 2019; Patra et al., 2019; Marchisio et al., 2020). For BLI, we may build the graphs as Gx = XXT and Gy = YYT . For standard graph matching objectives, we restrict the vocabularies of X and Y to equal size, thus Gx , Gy ∈ Rn×n . We find the optimal relabeling of nodes such that: min ||Gx − PGy PT ||2F P∈Π (3) This is an instance of the quadratic assignment problem and is much harder than Eq. 2. It is NPHard (Sahni and Gonzalez, 19"
2021.findings-emnlp.64,P18-2036,0,0.0214469,"tching."" Rather than assuming the existence of a linear transform between the embedding spaces, these methods start with or construct two graphs and try to match vertices such that neighborhood structure is preserved. Intuitively, the motivation of preserving neighborhood structure is the same as Procrustes methods, but the absence of linear transform W is an important distinction that potentially makes graph matching more flexible. Indeed, some recent BLI work argue against linear transforms (Mohiuddin et al., 2020) and discuss the failure modes due to lack of isometry (Søgaard et al., 2018; Nakashole and Flauger, 2018; Ormazabal et al., 2019; Glavaš et al., 2019; Vuli´c et al., 2019; Patra et al., 2019; Marchisio et al., 2020). For BLI, we may build the graphs as Gx = XXT and Gy = YYT . For standard graph matching objectives, we restrict the vocabularies of X and Y to equal size, thus Gx , Gy ∈ Rn×n . We find the optimal relabeling of nodes such that: min ||Gx − PGy PT ||2F P∈Π (3) This is an instance of the quadratic assignment problem and is much harder than Eq. 2. It is NPHard (Sahni and Gonzalez, 1976) but various approximation methods exist. Vogelstein et al. (2015) use the Frank-Wolfe method (Frank e"
2021.findings-emnlp.64,P19-1492,0,0.0191284,"the existence of a linear transform between the embedding spaces, these methods start with or construct two graphs and try to match vertices such that neighborhood structure is preserved. Intuitively, the motivation of preserving neighborhood structure is the same as Procrustes methods, but the absence of linear transform W is an important distinction that potentially makes graph matching more flexible. Indeed, some recent BLI work argue against linear transforms (Mohiuddin et al., 2020) and discuss the failure modes due to lack of isometry (Søgaard et al., 2018; Nakashole and Flauger, 2018; Ormazabal et al., 2019; Glavaš et al., 2019; Vuli´c et al., 2019; Patra et al., 2019; Marchisio et al., 2020). For BLI, we may build the graphs as Gx = XXT and Gy = YYT . For standard graph matching objectives, we restrict the vocabularies of X and Y to equal size, thus Gx , Gy ∈ Rn×n . We find the optimal relabeling of nodes such that: min ||Gx − PGy PT ||2F P∈Π (3) This is an instance of the quadratic assignment problem and is much harder than Eq. 2. It is NPHard (Sahni and Gonzalez, 1976) but various approximation methods exist. Vogelstein et al. (2015) use the Frank-Wolfe method (Frank et al., 1956) to find an"
2021.findings-emnlp.64,P19-1018,0,0.177662,"arately trained on monolingual data in language Y. We assume seeds {(x1 , y1 ), (x2 , y2 ), ...(xs , ys )} are given, which are supervised labels indicating translation correspondence between vocabulary items in the languages. We sort the corresponding submatrices of X and Y so each row of X ∈ Rs×d and Y ∈ Rs×d corresponds to the seeds. Usually, s is strictly smaller than both n and m and the goal is to find translation correspondences in the remaining words. Procrustes and linear transforms: The popular Procrustes-based methods for BLI (e.g. Artetxe et al., 2016a, 2019; Conneau et al., 2018; Patra et al., 2019) match seeds by calculating a linear transformation W by a variant of the below: min W∈Rd×d ||XW − Y||2F (1) If W is required to be orthogonal, then distances between points are unchanged by the transform and a closed form solution can be computed by singular value decomposition (Schönemann, 1966). Once languages are mapped to the same space by W, nearest neighbor search finds additional translation pairs. If W is known, one can find translations by optimizing over permutations Π: min ||XW − PY||2F P∈Π (2) P ∈ {0, 1}n×n is permutation matrix that shuffles the rows of Y. If we enforce the 1-to-"
2021.findings-emnlp.64,D19-1449,0,0.0381571,"Missing"
2021.findings-emnlp.64,P95-1050,0,0.386726,"word embedding spaces, and for well-trained embeddings in mismatched domains (e.g., Marchisio et al., 2020). In these cases, SGM might benefit from a different distance metric. A detailed analysis should be performed when data is many-to-many, as translation is naturally a many-to-many task. One might revisit word vectors based on co-occurrence statistics. The size of training and test sets should be increased, as the presence of more synonyms/antonyms and other “distractor"" words may elicit different behavior. There are computational considerations as we scale-up, particularly for SGM. tics. Rapp (1995) and Fung (1995) induce bilingual lexica based on the principle that words that frequently co-occur in one language have translations that co-occur frequently in another. Diab and Finch (2000) extend this by measuring similarity between words based on co-occurrence vectors and matching words across language by preserving these similarities. Mikolov et al. (2013) are the first to perform BLI over word embeddings, estimating the transformation matrix using stochastic gradient descent. Most recent work solves a variation of the generalized Procrustes problem (e.g., Conneau et al., 2018; Artetxe e"
2021.findings-emnlp.64,2020.emnlp-main.482,0,0.0264729,"Missing"
2021.findings-emnlp.64,2020.acl-main.318,0,0.0276985,"; Doval et al., 2018; Joulin et al., 2018; Jawanpuria et al., 2019; Alvarez-Melis and Jaakkola, 2018). Zhang et al. (2020) learn a mapping that overfits to training pairs thus enforcing “hard-seeding"", while Ruder et al. (2018) enforce a one-to-one constraint on the output for BLI. Some BLI work uses graph based methods implicitly or explicitly. Artetxe et al. (2018a) form an initial solution with similarity matrices and refine with iterative Procrustes. Grave et al. (2019b) optimize “Procrustes in Wasserstein Distance"", employing a quadratic assignment formulation and the Frank-Wolfe method. Ren et al. (2020) form CSLS 8 Related Work similarity matrices, iteratively extract cliques, and Matching words using vector representations be- map with Procrustes. Gutierrez-Vasques and Mijangan with vectors based on co-occurrence statis- gos (2017) create a weighted graph of translation 745 candidates then create word vectors with Node2Vec (Grover and Leskovec, 2016). Wushouer et al. (2013) use graphs for a source, target, and pivot language to iteratively extract translation pairs based on heuristics. Our active learning approach is inspired by Yuan et al. (2020). Mikel Artetxe, Gorka Labaka, and Eneko Agi"
2021.findings-emnlp.64,D18-1042,0,0.084923,"and assumes the existence of a linear we study the behavior of Euclidean versus transform that maps the spaces. The graph-based graph-based approaches to BLI under differview works with graphs for each language and ing data conditions and show that they comdirectly performs matching on edge pairs based plement each other when combined. We reon neighborhood information. This view is exemlease our code at https://github.com/ plified by graph matching methods that solve the kellymarchisio/euc-v-graph-bli. quadratic assignment problem from the combina1 Introduction torial optimization literature. Ruder et al. (2018) Bilingual lexicons are useful in many natural lan- and Haghighi et al. (2008) incorporate related techguage processing tasks including constrained de- niques for bilingual lexicon induction. We use coding in machine translation, cross-lingual infor- Seeded Graph Matching (SGM; Fishkind et al., mation retrieval, and unsupervised machine trans- 2019) as representative of this class of approach. lation. There is a large literature inducing bilin- Figure 1 illustrates the differences between the gual lexicons from cross-lingual spaces. “Map- framings; while they both exploit the idea that words w"
2021.findings-emnlp.64,2020.acl-main.201,0,0.0200584,"other. Diab and Finch (2000) extend this by measuring similarity between words based on co-occurrence vectors and matching words across language by preserving these similarities. Mikolov et al. (2013) are the first to perform BLI over word embeddings, estimating the transformation matrix using stochastic gradient descent. Most recent work solves a variation of the generalized Procrustes problem (e.g., Conneau et al., 2018; Artetxe et al., 2016b, 2017; Patra et al., 2019; Artetxe et al., 2018b; Doval et al., 2018; Joulin et al., 2018; Jawanpuria et al., 2019; Alvarez-Melis and Jaakkola, 2018). Zhang et al. (2020) learn a mapping that overfits to training pairs thus enforcing “hard-seeding"", while Ruder et al. (2018) enforce a one-to-one constraint on the output for BLI. Some BLI work uses graph based methods implicitly or explicitly. Artetxe et al. (2018a) form an initial solution with similarity matrices and refine with iterative Procrustes. Grave et al. (2019b) optimize “Procrustes in Wasserstein Distance"", employing a quadratic assignment formulation and the Frank-Wolfe method. Ren et al. (2020) form CSLS 8 Related Work similarity matrices, iteratively extract cliques, and Matching words using vect"
2021.findings-emnlp.64,P19-1307,0,0.0119624,"ays appear in the solution. This is ideal when one is confident about the quality of the seeds, but means that SGM is not robust to errors in the seed set. 3 Experimental Setup Because there are three methods and results sections, we detail the experimental setup first. We evaluate on English→German (En-De) and Russian→English (Ru-En). Monolingual Word Embeddings We use 300dimensional monolingual word embeddings trained on Wikipedia using fastText (Bojanowski et al., 2017).1 We normalize to unit length, mean-center, and renormalize, following Artetxe et al. (2018a) (“iterative normalization"", Zhang et al. (2019)). Data & Software Bilingual dictionaries from MUSE2 are many-to-many lexicons of the 5000 most-frequent words from the source language, paired with one or more target-side translations. We filter each lexicon to be one-to-one for simplicity of analysis. For source words with multiple target words, we keep the first occurrence. This is equivalent to randomly sampling a target sense for polysemous source words because target words are in arbitrary order. En-De originally contains 14667 pairs, and 4903 remain after filtering. Ru-En has 7452 pairs, reduced to 4084. We use 100-4000 pairs as seeds,"
2021.findings-emnlp.64,P18-1072,0,0.0463968,"Missing"
2021.findings-emnlp.64,D13-1058,0,0.0327119,"that shuffles the rows of Y. If we enforce the 1-to-1 correspondence, this is linear assignment problem that is solvable in polynomial time, e.g. with the Hungarian algorithm (Kuhn, 1955) or Wasserstein methods (Grave et al., 2019a). In the NLP literature, a large number of methods are based on the same underlying idea of linear transform followed by correspondence search/matching (see Related Work). To extract lexicons, one performs nearest neighbor search on the transformed embeddings. To mitigate the hubness problem (where some words are close to too many others) (Radovanovic et al., 2010; Suzuki et al., 2013), Conneau et al. (2018) modifies the similarity using cross-domain similarity local scaling (CSLS) to penalize hubs. For x, y in embedding space V : CSLS(x, y) = 2 cos(x, y) − avg(x, k) − avg(y, k) avg(v, k) = 1 k X cos(vn , v) vn ∈Nk (v,V ) Nk (v, V ) returns the k-nearest-neighbors to v ∈ V by cosine similarity (typically k = 10). 739 Graph matching: In fields such as pattern recognition, network science, and computer vision, there exist a large body of related work termed “graph matching."" Rather than assuming the existence of a linear transform between the embedding spaces, these methods s"
2021.mtsummit-research.1,N19-1388,0,0.0193293,"rs. Attempts to learn a curriculum jointly with the NMT training setup (Kumar et al., 2019) can suffer from observation sparsity, where a single training run does not provide enough training samples for an external agent to learn a good curriculum policy. Our NMT task of choice in this paper is low-resource multi-lingual NMT (MNMT). While standard NMT systems typically deal with a language pair, the source and the target, an MNMT model may have multiple languages as source and/or target. Most large-scale MNMT models are trained using some form of model parameter sharing (Johnson et al., 2017; Aharoni et al., 2019; Arivazhagan et al., 2019; Bapna and Firat, 2019). The notion of how input data should be presented to the MNMT system during training only finds prominence in the case of low-resource MNMT. A typical low-resource task will try to leverage a high-resource language pair to aid the training of an NMT system for a low-resource (very small or no parallel data available) and related language-pair of interest. Typical approaches for low resource MNMT involve pivoting and zero-shot training (Lakew et al., 2018; Johnson et al., 2017) and transfer learning via fine-tuning (Zoph et al., 2016; Dabre et"
2021.mtsummit-research.1,D19-1165,0,0.0164252,"the NMT training setup (Kumar et al., 2019) can suffer from observation sparsity, where a single training run does not provide enough training samples for an external agent to learn a good curriculum policy. Our NMT task of choice in this paper is low-resource multi-lingual NMT (MNMT). While standard NMT systems typically deal with a language pair, the source and the target, an MNMT model may have multiple languages as source and/or target. Most large-scale MNMT models are trained using some form of model parameter sharing (Johnson et al., 2017; Aharoni et al., 2019; Arivazhagan et al., 2019; Bapna and Firat, 2019). The notion of how input data should be presented to the MNMT system during training only finds prominence in the case of low-resource MNMT. A typical low-resource task will try to leverage a high-resource language pair to aid the training of an NMT system for a low-resource (very small or no parallel data available) and related language-pair of interest. Typical approaches for low resource MNMT involve pivoting and zero-shot training (Lakew et al., 2018; Johnson et al., 2017) and transfer learning via fine-tuning (Zoph et al., 2016; Dabre et al., 2019). Finn et al. (2017) attempt to meta-lea"
2021.mtsummit-research.1,D19-1146,0,0.0180657,"al., 2019; Arivazhagan et al., 2019; Bapna and Firat, 2019). The notion of how input data should be presented to the MNMT system during training only finds prominence in the case of low-resource MNMT. A typical low-resource task will try to leverage a high-resource language pair to aid the training of an NMT system for a low-resource (very small or no parallel data available) and related language-pair of interest. Typical approaches for low resource MNMT involve pivoting and zero-shot training (Lakew et al., 2018; Johnson et al., 2017) and transfer learning via fine-tuning (Zoph et al., 2016; Dabre et al., 2019). Finn et al. (2017) attempt to meta-learn parameter initialization for child models using trained-high resource parent models for this task. Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 1 Figure 1: The multi-arm bandit agents’ (MAB) interface with the NMT system. In this paper, we build upon the framework for learning curricula introduced in Kumar et al. (2019) and attempt to alleviate the problem of observation sparsity by learning more robust policies from multiple training runs. We use contextual multi-arm b"
2021.mtsummit-research.1,P13-2119,0,0.0253054,"learned curricula can provide better starting points for fine tuning and improve overall performance of the translation system. 1 Introduction Curriculum learning (Bengio et al., 2009; Elman, 1993; Rohde and Plaut, 1994) hypothesizes that presenting training samples in a meaningful order to machine learners during training may help improve model quality and convergence speed. In the field of Neural Machine Translation (NMT) most curricula are hand designed e.g., fine-tuning (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016) and data selection (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Durrani et al., 2016). Another common curriculum is one based on ordering samples from easy to hard using linguistic features and auxiliary model scores (Zhang et al., 2018, 2019) but these are hard to tune, relying to extensive trial and error to find the right hyperparameters. Attempts to learn a curriculum jointly with the NMT training setup (Kumar et al., 2019) can suffer from observation sparsity, where a single training run does not provide enough training samples for an external agent to learn a good curriculum policy. Our NMT task of choice in this paper is low-resource multi-lingual"
2021.mtsummit-research.1,C16-1299,0,0.0215611,"can provide better starting points for fine tuning and improve overall performance of the translation system. 1 Introduction Curriculum learning (Bengio et al., 2009; Elman, 1993; Rohde and Plaut, 1994) hypothesizes that presenting training samples in a meaningful order to machine learners during training may help improve model quality and convergence speed. In the field of Neural Machine Translation (NMT) most curricula are hand designed e.g., fine-tuning (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016) and data selection (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Durrani et al., 2016). Another common curriculum is one based on ordering samples from easy to hard using linguistic features and auxiliary model scores (Zhang et al., 2018, 2019) but these are hard to tune, relying to extensive trial and error to find the right hyperparameters. Attempts to learn a curriculum jointly with the NMT training setup (Kumar et al., 2019) can suffer from observation sparsity, where a single training run does not provide enough training samples for an external agent to learn a good curriculum policy. Our NMT task of choice in this paper is low-resource multi-lingual NMT (MNMT). While stan"
2021.mtsummit-research.1,D19-1632,1,0.896888,"Missing"
2021.mtsummit-research.1,Q17-1024,0,0.0154309,"he right hyperparameters. Attempts to learn a curriculum jointly with the NMT training setup (Kumar et al., 2019) can suffer from observation sparsity, where a single training run does not provide enough training samples for an external agent to learn a good curriculum policy. Our NMT task of choice in this paper is low-resource multi-lingual NMT (MNMT). While standard NMT systems typically deal with a language pair, the source and the target, an MNMT model may have multiple languages as source and/or target. Most large-scale MNMT models are trained using some form of model parameter sharing (Johnson et al., 2017; Aharoni et al., 2019; Arivazhagan et al., 2019; Bapna and Firat, 2019). The notion of how input data should be presented to the MNMT system during training only finds prominence in the case of low-resource MNMT. A typical low-resource task will try to leverage a high-resource language pair to aid the training of an NMT system for a low-resource (very small or no parallel data available) and related language-pair of interest. Typical approaches for low resource MNMT involve pivoting and zero-shot training (Lakew et al., 2018; Johnson et al., 2017) and transfer learning via fine-tuning (Zoph e"
2021.mtsummit-research.1,N19-1208,1,0.914862,"gence speed. In the field of Neural Machine Translation (NMT) most curricula are hand designed e.g., fine-tuning (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016) and data selection (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Durrani et al., 2016). Another common curriculum is one based on ordering samples from easy to hard using linguistic features and auxiliary model scores (Zhang et al., 2018, 2019) but these are hard to tune, relying to extensive trial and error to find the right hyperparameters. Attempts to learn a curriculum jointly with the NMT training setup (Kumar et al., 2019) can suffer from observation sparsity, where a single training run does not provide enough training samples for an external agent to learn a good curriculum policy. Our NMT task of choice in this paper is low-resource multi-lingual NMT (MNMT). While standard NMT systems typically deal with a language pair, the source and the target, an MNMT model may have multiple languages as source and/or target. Most large-scale MNMT models are trained using some form of model parameter sharing (Johnson et al., 2017; Aharoni et al., 2019; Arivazhagan et al., 2019; Bapna and Firat, 2019). The notion of how i"
2021.mtsummit-research.1,2015.iwslt-evaluation.11,0,0.049697,"ranslation system using contextual multi-arm bandits. We show on the FLORES low-resource translation dataset that these learned curricula can provide better starting points for fine tuning and improve overall performance of the translation system. 1 Introduction Curriculum learning (Bengio et al., 2009; Elman, 1993; Rohde and Plaut, 1994) hypothesizes that presenting training samples in a meaningful order to machine learners during training may help improve model quality and convergence speed. In the field of Neural Machine Translation (NMT) most curricula are hand designed e.g., fine-tuning (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016) and data selection (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Durrani et al., 2016). Another common curriculum is one based on ordering samples from easy to hard using linguistic features and auxiliary model scores (Zhang et al., 2018, 2019) but these are hard to tune, relying to extensive trial and error to find the right hyperparameters. Attempts to learn a curriculum jointly with the NMT training setup (Kumar et al., 2019) can suffer from observation sparsity, where a single training run does not provide enough training samples for an ext"
2021.mtsummit-research.1,P10-2041,0,0.0675284,"low-resource translation dataset that these learned curricula can provide better starting points for fine tuning and improve overall performance of the translation system. 1 Introduction Curriculum learning (Bengio et al., 2009; Elman, 1993; Rohde and Plaut, 1994) hypothesizes that presenting training samples in a meaningful order to machine learners during training may help improve model quality and convergence speed. In the field of Neural Machine Translation (NMT) most curricula are hand designed e.g., fine-tuning (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016) and data selection (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Durrani et al., 2016). Another common curriculum is one based on ordering samples from easy to hard using linguistic features and auxiliary model scores (Zhang et al., 2018, 2019) but these are hard to tune, relying to extensive trial and error to find the right hyperparameters. Attempts to learn a curriculum jointly with the NMT training setup (Kumar et al., 2019) can suffer from observation sparsity, where a single training run does not provide enough training samples for an external agent to learn a good curriculum policy. Our NMT task of choice in"
2021.mtsummit-research.1,N19-4009,0,0.0281775,"mini-batch. The reward obtained for this action is the delta-validation perplexity post update, the improvement in perplexity on the validation set in a finite window. The exploration strategy is the linearly-decaying epsilon-greedy strategy (Kuleshov and Precup, 2014). The contextual MABs are implemented as simple feed-forward neural networks which take the observation vector as input and produce a distribution over two states representing the bins. If we choose to exploit this learned policy, the bin with maximum probability mass is selected for sampling. 3 Experiment Setup We use Fairseq (Ott et al., 2019) for all our NMT experiments and the our NMT systems are configured to replicate the setup described in Guzmán et al. (2019). The grid search experiments search over the the range [0, 1] for sampling in increments of 0.1. The pruned tree-search uses a beam width of 1. The phase duration for tree-search is set to one epoch of NMT training. We use either 5 or 10 concurrent contextual MABs which are implemented as two 256-dimensional feed forward neural networks trained using RMSProp with a learning rate of 0.00025 and a decay of 0.95. Rewards for the agent (validation delta-perplexity) are provi"
2021.mtsummit-research.1,N19-1189,1,0.838007,"Missing"
2021.mtsummit-research.1,D16-1163,0,0.0203494,", 2017; Aharoni et al., 2019; Arivazhagan et al., 2019; Bapna and Firat, 2019). The notion of how input data should be presented to the MNMT system during training only finds prominence in the case of low-resource MNMT. A typical low-resource task will try to leverage a high-resource language pair to aid the training of an NMT system for a low-resource (very small or no parallel data available) and related language-pair of interest. Typical approaches for low resource MNMT involve pivoting and zero-shot training (Lakew et al., 2018; Johnson et al., 2017) and transfer learning via fine-tuning (Zoph et al., 2016; Dabre et al., 2019). Finn et al. (2017) attempt to meta-learn parameter initialization for child models using trained-high resource parent models for this task. Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 1 Figure 1: The multi-arm bandit agents’ (MAB) interface with the NMT system. In this paper, we build upon the framework for learning curricula introduced in Kumar et al. (2019) and attempt to alleviate the problem of observation sparsity by learning more robust policies from multiple training runs. We use c"
2021.mtsummit-research.24,D18-1214,0,0.0349599,"Missing"
2021.mtsummit-research.24,D16-1250,0,0.0233676,"has been a popular task in natural language processing for decades, beginning with statistical decipherment (e.g., Rapp, 1995; Fung, 1995; Koehn and Knight, 2000, 2002; Haghighi et al., 2008). With the advent of the ability to create large monolingual vector spaces from abundant monolingual text, the focus has shifted to finding an optimal linear transformation between such monolingual embedding spaces from which a seed lexicon can be extracted using nearest neighbors search. Practically, this often involves solving variations of the generalized Procrustes problem (e.g., Conneau et al., 2018; Artetxe et al., 2016, 2017; Patra et al., 2019; Artetxe et al., 2018b; Doval et al., 2018; Joulin et al., 2018; Jawanpuria et al., 2019; AlvarezMelis and Jaakkola, 2018). Differing metrics and heuristics can be used to extract the seed lexicon once the mapping is found. Cross-domain similarity local scaling (CSLS) to mitigate the hubness is popular and effective (Conneau et al., 2018). While the orthogonal variant of the Procrustes problem has a simple closed-form solution, one must know in advance the pairings of words one wants to be closest after the transformation (i.e., you already know the translations). To"
2021.mtsummit-research.24,P17-1042,0,0.20971,"ducing bilingual lexicons from small amounts of parallel data. Particularly when words occur frequently in the corpus, statistical models easily recover the translation. At the same time, however, the number of seed translation pairs possible to extract is limited by the vocabulary of the parallel corpus. We address a more realistic scenario: there is ample monolingual data and a small parallel corpus. We combine the strengths of statistical alignment and unsupervised mapping methods and achieve state-of-the-art results on 3 of 4 languages in the challenging VecMap dataset (Dinu et al., 2015; Artetxe et al., 2017, 2018a), trailing by only 0.1 in the 4th language pair. 2 Related Work Automatic BLI has been a popular task in natural language processing for decades, beginning with statistical decipherment (e.g., Rapp, 1995; Fung, 1995; Koehn and Knight, 2000, 2002; Haghighi et al., 2008). With the advent of the ability to create large monolingual vector spaces from abundant monolingual text, the focus has shifted to finding an optimal linear transformation between such monolingual embedding spaces from which a seed lexicon can be extracted using nearest neighbors search. Practically, this often involves"
2021.mtsummit-research.24,P18-1073,0,0.0828004,"essing for decades, beginning with statistical decipherment (e.g., Rapp, 1995; Fung, 1995; Koehn and Knight, 2000, 2002; Haghighi et al., 2008). With the advent of the ability to create large monolingual vector spaces from abundant monolingual text, the focus has shifted to finding an optimal linear transformation between such monolingual embedding spaces from which a seed lexicon can be extracted using nearest neighbors search. Practically, this often involves solving variations of the generalized Procrustes problem (e.g., Conneau et al., 2018; Artetxe et al., 2016, 2017; Patra et al., 2019; Artetxe et al., 2018b; Doval et al., 2018; Joulin et al., 2018; Jawanpuria et al., 2019; AlvarezMelis and Jaakkola, 2018). Differing metrics and heuristics can be used to extract the seed lexicon once the mapping is found. Cross-domain similarity local scaling (CSLS) to mitigate the hubness is popular and effective (Conneau et al., 2018). While the orthogonal variant of the Procrustes problem has a simple closed-form solution, one must know in advance the pairings of words one wants to be closest after the transformation (i.e., you already know the translations). To adapt to the unsupervised or semi-supervised sc"
2021.mtsummit-research.24,P19-1019,0,0.0203656,"expansion via embedding space mapping Using the induced translations from the previous step as seeds, we map the monolingual embedding spaces using the public implementation of VecMap1 in supervised mode (Artetxe et al., 2018a). In this method, word embeddings are length-normalized, mean-centered, and lengthnormalized again. A whitening transformation is performed, and then VecMap solves the orthogonal Procrustes problem over the known seeds, and the resulting spaces are reweighted and dewhitened. We extract a phrase table from the resulting mapped monolingual embedding spaces using Monoses2 (Artetxe et al., 2019). For a mapped source word e, let its k nearest neighbors in the mapped target embedding space be N (x, k). Here, k=100. We calculate the translation probability for x and each of its k nearest neighbors using the softmax of the cosine similarity. Let f ∈ N (x, k). Then, p(f |e) = exp(cos(e, f )/τ ) P exp(cos(e, f 0 )/τ ) f 0 ∈N (x,k) See Artetxe et al. (2019) for further details. 1 https://github.com/artetxem/vecmap 2 https://github.com/artetxem/monoses Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 296 We extrac"
2021.mtsummit-research.24,D18-1027,0,0.0222793,"inning with statistical decipherment (e.g., Rapp, 1995; Fung, 1995; Koehn and Knight, 2000, 2002; Haghighi et al., 2008). With the advent of the ability to create large monolingual vector spaces from abundant monolingual text, the focus has shifted to finding an optimal linear transformation between such monolingual embedding spaces from which a seed lexicon can be extracted using nearest neighbors search. Practically, this often involves solving variations of the generalized Procrustes problem (e.g., Conneau et al., 2018; Artetxe et al., 2016, 2017; Patra et al., 2019; Artetxe et al., 2018b; Doval et al., 2018; Joulin et al., 2018; Jawanpuria et al., 2019; AlvarezMelis and Jaakkola, 2018). Differing metrics and heuristics can be used to extract the seed lexicon once the mapping is found. Cross-domain similarity local scaling (CSLS) to mitigate the hubness is popular and effective (Conneau et al., 2018). While the orthogonal variant of the Procrustes problem has a simple closed-form solution, one must know in advance the pairings of words one wants to be closest after the transformation (i.e., you already know the translations). To adapt to the unsupervised or semi-supervised scenario, such mapping-"
2021.mtsummit-research.24,E14-1049,0,0.0763778,"Missing"
2021.mtsummit-research.24,W95-0114,0,0.666168,"possible to extract is limited by the vocabulary of the parallel corpus. We address a more realistic scenario: there is ample monolingual data and a small parallel corpus. We combine the strengths of statistical alignment and unsupervised mapping methods and achieve state-of-the-art results on 3 of 4 languages in the challenging VecMap dataset (Dinu et al., 2015; Artetxe et al., 2017, 2018a), trailing by only 0.1 in the 4th language pair. 2 Related Work Automatic BLI has been a popular task in natural language processing for decades, beginning with statistical decipherment (e.g., Rapp, 1995; Fung, 1995; Koehn and Knight, 2000, 2002; Haghighi et al., 2008). With the advent of the ability to create large monolingual vector spaces from abundant monolingual text, the focus has shifted to finding an optimal linear transformation between such monolingual embedding spaces from which a seed lexicon can be extracted using nearest neighbors search. Practically, this often involves solving variations of the generalized Procrustes problem (e.g., Conneau et al., 2018; Artetxe et al., 2016, 2017; Patra et al., 2019; Artetxe et al., 2018b; Doval et al., 2018; Joulin et al., 2018; Jawanpuria et al., 2019;"
2021.mtsummit-research.24,P08-1088,0,0.0862913,"abulary of the parallel corpus. We address a more realistic scenario: there is ample monolingual data and a small parallel corpus. We combine the strengths of statistical alignment and unsupervised mapping methods and achieve state-of-the-art results on 3 of 4 languages in the challenging VecMap dataset (Dinu et al., 2015; Artetxe et al., 2017, 2018a), trailing by only 0.1 in the 4th language pair. 2 Related Work Automatic BLI has been a popular task in natural language processing for decades, beginning with statistical decipherment (e.g., Rapp, 1995; Fung, 1995; Koehn and Knight, 2000, 2002; Haghighi et al., 2008). With the advent of the ability to create large monolingual vector spaces from abundant monolingual text, the focus has shifted to finding an optimal linear transformation between such monolingual embedding spaces from which a seed lexicon can be extracted using nearest neighbors search. Practically, this often involves solving variations of the generalized Procrustes problem (e.g., Conneau et al., 2018; Artetxe et al., 2016, 2017; Patra et al., 2019; Artetxe et al., 2018b; Doval et al., 2018; Joulin et al., 2018; Jawanpuria et al., 2019; AlvarezMelis and Jaakkola, 2018). Differing metrics an"
2021.mtsummit-research.24,1983.tc-1.13,0,0.684283,"Missing"
2021.mtsummit-research.24,D18-1330,0,0.0392841,"Missing"
2021.mtsummit-research.24,2005.mtsummit-papers.11,1,0.403408,"obability p(f |e) assigned to a given word pair in the translation table is iteratively refined according to the occurrence of f and e in the corpus. While this procedure can capture alignment and translation likelihoods of common words in a large bilingual corpus accurately, the probability can become inaccurate for rare words (not to mention those absent from the corpus). The risk of such inaccuracies of low-frequency words increases as corpus size shrinks. There are 10,673 unique source tokens in the first 10,000 lowercased lines of the Englishside of the Europarl v7 German-English corpus (Koehn, 2005), used later in this work. Of those, 4015 tokens occur just once. Only 5214 — less than half of the vocabulary — occur more than twice. Such a large percentage of rare words is explained by the well-known Zipf’s law (Zipf, 1935, 1949; Mandelbrot, 1953, 1961), whereby the kth most common word tends to occur with a frequency approaching the below, where α ∼ 1 and β ∼ 2.7 (Piantadosi, 2014). f req(w) ∝ 1 (rank(w) + β)α (1) Embedding space mapping can take advantage of large amounts of monolingual data. Just as statistical methods for word translation are more accurate for common words, inducing t"
2021.mtsummit-research.24,P07-2045,1,0.0139087,"r each language pair. We create a development set for EnglishGerman and English-Finnish using the last 2,000 lines of the training seeds provided by Dinu et al. (2015); Artetxe et al. (2017, 2018a), which are disjoint from the test set. We use Europarl v7 as our parallel bitext, which is a corpus of European Parliamentary proceedings available in 11 languages (Koehn, 2005). We normalize punctuation, tokenize, and clean the corpus to remove sentences with more than 100 tokens or with a source-to-target length ratio above 9. Each of these steps uses scripts from the Moses statistical MT system (Koehn et al., 2007). We then lowercase all bitext. For subsequent experiments varying the data size of the input corpus, we use the first N lines of the bitext, where N ranges from 500 to 50,000. We stop at 50,000 because our focus is on very small corpora. We use the NLTK3 (Bird et al., 2009) implementation of IBM Model 2, and the public implementation of VecMap. 6.3 Hyperparameter Settings For the IBM Model 2 step detailed in 5.1, we use N=3000, M=2, and minimum confidence threshold is set to 0.1. Final translations for the test set are retrieved by choosing the nearest neighbor in the target-side mapped space"
2021.mtsummit-research.24,W02-0902,1,0.66997,"Missing"
2021.mtsummit-research.24,2020.lrec-1.352,0,0.0177391,"r a traditional supervised setup where no parallel bitext exists, such as for MT or cross-lingual information retrieval. A starting lexicon is induced in an unsupervised manner, and then serves as initial training data to the supervised model. Practically, however, one struggles to identify a scenario where one would truly fail to have any parallel text whatesoever from which to gain some supervision. The Christian Bible, for instance, is translated into over 1600 world languages, providing multi-way parallel data for many of the world’s languages that are typically considered “low-resource” (McCarthy et al., 2020). Human translators can also create a small translation corpus or seed dictionary. The practical necessity of fully unsupervised scenarios for BLI or MT therefore becomes hard to imagine. Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 293 Statistical translation/alignment models are very proficient at inducing bilingual lexicons from small amounts of parallel data. Particularly when words occur frequently in the corpus, statistical models easily recover the translation. At the same time, however, the number of see"
2021.mtsummit-research.24,2020.emnlp-main.215,0,0.032574,"Missing"
2021.mtsummit-research.24,2020.cl-2.2,0,0.0292212,"Missing"
2021.mtsummit-research.24,P95-1050,0,0.598425,"lation pairs possible to extract is limited by the vocabulary of the parallel corpus. We address a more realistic scenario: there is ample monolingual data and a small parallel corpus. We combine the strengths of statistical alignment and unsupervised mapping methods and achieve state-of-the-art results on 3 of 4 languages in the challenging VecMap dataset (Dinu et al., 2015; Artetxe et al., 2017, 2018a), trailing by only 0.1 in the 4th language pair. 2 Related Work Automatic BLI has been a popular task in natural language processing for decades, beginning with statistical decipherment (e.g., Rapp, 1995; Fung, 1995; Koehn and Knight, 2000, 2002; Haghighi et al., 2008). With the advent of the ability to create large monolingual vector spaces from abundant monolingual text, the focus has shifted to finding an optimal linear transformation between such monolingual embedding spaces from which a seed lexicon can be extracted using nearest neighbors search. Practically, this often involves solving variations of the generalized Procrustes problem (e.g., Conneau et al., 2018; Artetxe et al., 2016, 2017; Patra et al., 2019; Artetxe et al., 2018b; Doval et al., 2018; Joulin et al., 2018; Jawanpuria et"
2021.mtsummit-research.24,2021.acl-long.67,0,0.0366943,"st after the transformation (i.e., you already know the translations). To adapt to the unsupervised or semi-supervised scenario, such mapping-based BLI procedures must make a “guess” of some correct translation pairs. The solution can then iteratively refined through self-learning. The initial “guess” can come in the form of direct supervision using a bilingual training dictionary, or in an unsupervised manner, such as by identifying the nearest neighbors in a similarity matrix (e.g., Artetxe et al., 2018b) or via adversarial training (e.g., Conneau et al., 2018; Patra et al., 2019). Like us, Shi et al. (2021) also use statistical alignment within a pipeline for BLI, but unlike our work, they do not use the induced alignments as seeds for monolingual embedding mapping. 3 3.1 Background The Orthogonal Procrustes Problem Let A and B be matrices in Rm×n . Let Q be a matrix in Rn×n . The goal of the orthogonal Procrustes problem is to find Q such that: arg minkAQ − BkF QQT =I The solution to the orthogonal Procrustes problem is Q = V U T , where U ΣV is the singular value decomposition of B T A (Sch¨onemann, 1966). 3.2 IBM Model 2 IBM Model 2 (Brown et al., 1993) is designed to be a noisy channel model"
2021.mtsummit-research.24,tiedemann-2012-parallel,0,0.0441476,"3a). Table 1 details the parallel text used to train the embeddings. We conduct experiments on all four available language pairs (English-German, English-Spanish, English-Italian, English-Finnish). Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 297 6.2 Data We use the popular and challenging VecMap data set, which is the original English-Italian data set of Dinu et al. (2015) with the subsequent extensions by Artetxe et al. (2017, 2018a). The dataset was obtained via alignment of the Europarl corpus (Koehn, 2005; Tiedemann, 2012). Test sets contain approximately 1500 source words and 2000 word pairs total. The source words are sampled evenly from frequency bins in the Europarl lexicon: one-fifth from each of frequency ranks [1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066"
2021.naacl-main.399,J93-2004,0,0.0779994,"ynthetic dataset and one natural dataset. Both synthetic datasets ensure there is only one cue and one attractor for each test instance, while for natural datasets, there are often more than one. For number agreement, our synthetic dataset is constructed from selected sections of Syneval, a targeted language model evaluation dataset from Marvin and Linzen (2018), where the verbs and the subjects could be easily induced with heuristics. We only use the most challenging sections where strongly interceding attractors are involved. Our natural dataset for this task is filtered from Penn Treebank (Marcus et al., 1993, PTB), including training, development, and test. We choose PTB because it offers not only human-annotated POStags necessary for benchmark construction but also dependent subjects of verbs for further analysis. For gender agreement, our synthetic dataset comes from the unambiguous Winobias coreference resolution dataset used in Jumelet et al. (2019), and we only use the 1000-example subset where there is respectively one male and one female antecedent. Because this dataset is intentionally designed such that most humans will find pronouns of either gender equally likely to follow the prefix,"
2021.naacl-main.399,D18-1151,0,0.0165565,"k, as exemplified in Table 2. Two of the datasets are concerned with number agreement of a verb with its subject. The other two are concerned with gender agreement of a pronoun with its anteceding entity mentions. For each lexical agreement type, we have one synthetic dataset and one natural dataset. Both synthetic datasets ensure there is only one cue and one attractor for each test instance, while for natural datasets, there are often more than one. For number agreement, our synthetic dataset is constructed from selected sections of Syneval, a targeted language model evaluation dataset from Marvin and Linzen (2018), where the verbs and the subjects could be easily induced with heuristics. We only use the most challenging sections where strongly interceding attractors are involved. Our natural dataset for this task is filtered from Penn Treebank (Marcus et al., 1993, PTB), including training, development, and test. We choose PTB because it offers not only human-annotated POStags necessary for benchmark construction but also dependent subjects of verbs for further analysis. For gender agreement, our synthetic dataset comes from the unambiguous Winobias coreference resolution dataset used in Jumelet et al."
2021.naacl-main.399,P18-1176,0,0.0224731,"While neural network models for Natural Language Processing (NLP) have recently become popular, a general complaint is that their internal decision mechanisms are hard to understand. To alleviate this problem, recent work has deployed interpretation methods on top of the neural network models. Among them, there is a category of interpretation In this paper, we address this problem by buildmethods called saliency method that is especially ing a comprehensive quantitative benchmark to widely adopted (Li et al., 2016a,b; Arras et al., evaluate saliency methods. Our benchmark focuses 2016, 2017; Mudrakarta et al., 2018; Ding et al., on a fundamental category of NLP models: neural 2019). At a very high level, these methods assign language models. Following the concepts proposed an importance score to each feature in the input fea- by Jacovi and Goldberg (2020), our benchmark ture set F , regarding a specific prediction y made evaluates the credibility of saliency interpretations by a neural network model M . Such feature impor- from two aspects: plausibility and faithfulness. In tance scores can hopefully shed light on the neural short, plausibility measures how much these internetwork models’ internal decis"
2021.naacl-main.399,N19-4009,0,0.014427,"ty benchmark result. Each number is the fraction of cases the interpretation passes the benchmark test, while the numbers in brackets for each architecture are the fraction of times these scenarios occur for predictions generated by the corresponding model. Results from the best interpretation method for each architecture are boldfaced. The exp. and alt. columns are breakdown of evaluation results into expected scenarios and alternative scenarios as defined in Section 3. V, SG, IG stands for the vanilla saliency, SmoothGrad, and Integrated Gradients, respectively. mentation in fairseq tookit (Ott et al., 2019). We follow the setup for DistillBERT (Sanh et al., 2019) for the distillation process involved during For all the task-specific “probes”, the fine-tuning is performed on examples extracted from Wiki- the model consistency test, which reduces the depth of models but not the width. For our LSTM (3 layText-2 training data. A tuning example consists ers) and QRNN model (4 layers), the M 0 we distill of an input prefix and a gold tag for the lexical agreement in both cases. For number agreement, is one layer shallower than the original model M . we first run Stanford POS Tagger (Toutanova et al.,"
2021.naacl-main.399,D16-1159,0,0.0255809,"e.g. sing. To minimize such discrepancy and constrain the Model Consistency Test To measure model conscope of agreements used to make predictions, we sistency, we propose to measure the consistency draw inspiration from the previous work on repbetween feature importance ΨM (F ) and ΨM 0 (F ), resentation probing and make adjustment to the which is respectively generated from the original model M we are evaluating on (Tenney et al., model M and a smaller model M 0 that is trained 2019a,b; Kim et al., 2019; Conneau et al., 2018; by distilling knowledge from M . In this way, alAdi et al., 2017; Shi et al., 2016). The idea is to take though M and M 0 have different architectures, M 0 a language model that is trained to predict words is trained to mimic the behavior of M to the ex(e.g., vote in the example above) and substitute the tent possible, and thus having similar underlying original final linear layer with a new linear layer decision mechanisms. (which we refer to as a probe) fine-tuned to predict a binary lexical agreement tag (e.g., PLURAL) Input Consistency Test To measure input consistency, we perform substitutions in the input and corresponding to the word choice. By making this measure the"
C14-1041,C14-1181,0,0.0290178,"tion over all possible classes when calculating the n-gram probabilities. 422 Figure 1: Operation Sequence Model – Training Sentence with Generation and Test Sentences distributions and better generalizations has been a widely known and applied technique in natural language processing. Training based on word classes has been previously explored by various researchers. Cherry (2013) addressed data sparsity in lexicalized reordering models by using sparse features based on word classes. Other parallel attempts on using word-class models include Wuebker et al. (2013), Chahuneau et al. (2013) and Bisazza and Monz (2014). More recent research has started to set apart from the conventional maximum likelihood estimates toward neural network-based models that use continuous space representation (Schwenk, 2012; Le et al., 2012; Hu et al., 2014; Gao et al., 2014). Although these methods have achieved impressive improvements, traditional models continue to dominate the field due to their simplicity and low computational complexity. How much of the improvement will be retained when scaling these models to all available data instead of a limited amount will be interesting. 3 Operation Sequence Model The Operation Seq"
C14-1041,D13-1174,0,0.344604,"o improve mid-range reorderings. Our work is similar to them except that OSM model is substantially different from the TSM model as it integrates both the translation and reordering mechanisms into a combined model. Therefore both translation and reordering decisions can benefit from richer generalized representations. A second group of work addresses the problem of translating into morphologically richer languages. The idea of translating to stems and then inflecting the stems in a separate step has been studied by Toutanova et al. (2008), de Gispert and Mariˆno (2008), Fraser et al. (2012), Chahuneau et al. (2013) and others. Koehn and Hoang (2007) proposed to integrate different levels of linguistic information as factors into the phrase-based translation model. Yeniterzi and Oflazer (2010) used source syntactic structures as additional complex tag factors for English-to-Turkish phrase-based machine translation. Green and DeNero (2012) proposed a target-side, class-based agreement model to handle morpho-syntactic agreement errors when translating from English-to-Arabic. El Kholy and Habash (2012) tested three models to find out which features are best handled by modeling them as a part of translation,"
C14-1041,N12-1047,0,0.00920044,"urrani et al., 2014) to transliterate OOV words when translating into Russian. Tuning and Test: The systems were tuned on the dev2010 dataset and evaluated on the test2010-2013 datasets made available for the IWSLT-13 workshop. We performed a secondary set of experiments for German-English pairs using tuning and test sets made available for the WMT-13 workshop. We concatenated the news-test sets 2008 and 2009 to obtain a large dev-set of 4576 sentences. Evaluation was performed on the news-test set 2013 which contains 3000 sentences. Tuning was performed using the k-best batch MIRA algorithm (Cherry and Foster, 2012) with at most 25 iterations. We use BLEU (Papineni et al., 2002) as a metric to evaluate our results. Results I – Using Linguistic Annotation: We trained 5-gram OSM models over different representations and added these to the baseline system. First we evaluated Modeland (Mand ) which uses a MIRA tuned linear combination of different OSM models versus Modelor (Mor ) which computes only one OSM model but allows the generator to switch between different OSM models built on various generalized forms. Table 2 shows results from running experiments on German-English pairs. We found that the simpler"
C14-1041,N13-1003,0,0.251845,"t al., 2010; Wuebker and Ney, 2012). Automatically clustering the training data into word classes in order to obtain smoother 1 We are referring to hard clustering here. Soft clustering is intractable as it requires a marginalization over all possible classes when calculating the n-gram probabilities. 422 Figure 1: Operation Sequence Model – Training Sentence with Generation and Test Sentences distributions and better generalizations has been a widely known and applied technique in natural language processing. Training based on word classes has been previously explored by various researchers. Cherry (2013) addressed data sparsity in lexicalized reordering models by using sparse features based on word classes. Other parallel attempts on using word-class models include Wuebker et al. (2013), Chahuneau et al. (2013) and Bisazza and Monz (2014). More recent research has started to set apart from the conventional maximum likelihood estimates toward neural network-based models that use continuous space representation (Schwenk, 2012; Le et al., 2012; Hu et al., 2014; Gao et al., 2014). Although these methods have achieved impressive improvements, traditional models continue to dominate the field due t"
C14-1041,J07-2003,0,0.0625223,"order. Xia and McCord (2004) proposed a method to automatically learn rewrite rules to preorder source sentences. Collins et al. (2005) and Popovi´c and Ney (2006) proposed methods for reordering the source using a small set of handcrafted rules. Crego and Mari˜no (2007) use syntactic trees to derive rewrite rules. Hoang and Koehn (2009) used POS tags to create templates for surface word translation to create longer phrase translation. A whole new paradigm of using syntactic annotation to address long range reorderings has emerged following Galley et al. (2006), Zollmann and Venugopal (2006), Chiang (2007) etc. Crego and Yvon (2010) and Niehues et al. (2011) used a Tuple Sequence Model (TSM) over POS tags in an N-gram-based search to improve mid-range reorderings. Our work is similar to them except that OSM model is substantially different from the TSM model as it integrates both the translation and reordering mechanisms into a combined model. Therefore both translation and reordering decisions can benefit from richer generalized representations. A second group of work addresses the problem of translating into morphologically richer languages. The idea of translating to stems and then inflectin"
C14-1041,P05-1066,1,0.078446,"nts the experimental setup and the results. Section 5 concludes the paper. 2 Related Work Previous work on integrating linguistic knowledge into SMT models can be broken into two groups. The first group focuses on using linguistic knowledge to improve reordering between syntactically different languages. A second group focuses on translating into morphologically rich languages. Initial efforts to use linguistic annotation focused on rearranging source sentences to be in the target order. Xia and McCord (2004) proposed a method to automatically learn rewrite rules to preorder source sentences. Collins et al. (2005) and Popovi´c and Ney (2006) proposed methods for reordering the source using a small set of handcrafted rules. Crego and Mari˜no (2007) use syntactic trees to derive rewrite rules. Hoang and Koehn (2009) used POS tags to create templates for surface word translation to create longer phrase translation. A whole new paradigm of using syntactic annotation to address long range reorderings has emerged following Galley et al. (2006), Zollmann and Venugopal (2006), Chiang (2007) etc. Crego and Yvon (2010) and Niehues et al. (2011) used a Tuple Sequence Model (TSM) over POS tags in an N-gram-based s"
C14-1041,2007.mtsummit-papers.16,0,0.201558,"Missing"
C14-1041,C10-2023,0,0.179282,"rd (2004) proposed a method to automatically learn rewrite rules to preorder source sentences. Collins et al. (2005) and Popovi´c and Ney (2006) proposed methods for reordering the source using a small set of handcrafted rules. Crego and Mari˜no (2007) use syntactic trees to derive rewrite rules. Hoang and Koehn (2009) used POS tags to create templates for surface word translation to create longer phrase translation. A whole new paradigm of using syntactic annotation to address long range reorderings has emerged following Galley et al. (2006), Zollmann and Venugopal (2006), Chiang (2007) etc. Crego and Yvon (2010) and Niehues et al. (2011) used a Tuple Sequence Model (TSM) over POS tags in an N-gram-based search to improve mid-range reorderings. Our work is similar to them except that OSM model is substantially different from the TSM model as it integrates both the translation and reordering mechanisms into a combined model. Therefore both translation and reordering decisions can benefit from richer generalized representations. A second group of work addresses the problem of translating into morphologically richer languages. The idea of translating to stems and then inflecting the stems in a separate s"
C14-1041,P11-1105,1,0.479753,"research has started to set apart from the conventional maximum likelihood estimates toward neural network-based models that use continuous space representation (Schwenk, 2012; Le et al., 2012; Hu et al., 2014; Gao et al., 2014). Although these methods have achieved impressive improvements, traditional models continue to dominate the field due to their simplicity and low computational complexity. How much of the improvement will be retained when scaling these models to all available data instead of a limited amount will be interesting. 3 Operation Sequence Model The Operation Sequence Model (Durrani et al., 2011) is an instance of the N-gram based SMT framework (Casacuberta and Vidal, 2004; Mari˜no et al., 2006). It represents the translation process through a sequence of operations. An operation can be to simultaneously generate source or target words or to perform reordering. Reordering is carried out through jump and gap operations. The model is different from its ancestors in that it strongly integrates translation and reordering into a single generative story in which translation decisions can influence and get impacted by the reordering decisions and vice versa. Given a bilingual sentence pair &lt;"
C14-1041,N13-1001,1,0.226932,"gically rich or syntactically divergent languages. The former becomes challenging due to lexical sparsity and the latter suffers from sparsity in learning underlying reordering patterns. The last decade of research in Statistical Machine Translation has witnessed many attempts to integrate linguistic analysis into SMT models, to address the challenges of (i) translating into morphologically rich language languages, (ii) modeling syntactic divergence across languages for better generalization in sparse data conditions. The integration of the Operation Sequence Model into phrase-based paradigm (Durrani et al., 2013a; Durrani et al., 2013b) improved the reordering capability and addressed the problem of the phrasal independence assumption in the phrase-based models. The OSM model integrates translation and reordering into a single generative story. By jointly considering translation and reordering context across phrasal boundaries, the OSM model considers much richer conditioning than phrasal translation and lexicalized reordering models. However, due to data sparsity the model often falls back to very small context sizes. We address this problem by learning operation sequences over generalized represent"
C14-1041,P13-2071,1,0.567963,"gically rich or syntactically divergent languages. The former becomes challenging due to lexical sparsity and the latter suffers from sparsity in learning underlying reordering patterns. The last decade of research in Statistical Machine Translation has witnessed many attempts to integrate linguistic analysis into SMT models, to address the challenges of (i) translating into morphologically rich language languages, (ii) modeling syntactic divergence across languages for better generalization in sparse data conditions. The integration of the Operation Sequence Model into phrase-based paradigm (Durrani et al., 2013a; Durrani et al., 2013b) improved the reordering capability and addressed the problem of the phrasal independence assumption in the phrase-based models. The OSM model integrates translation and reordering into a single generative story. By jointly considering translation and reordering context across phrasal boundaries, the OSM model considers much richer conditioning than phrasal translation and lexicalized reordering models. However, due to data sparsity the model often falls back to very small context sizes. We address this problem by learning operation sequences over generalized represent"
C14-1041,E14-4029,1,0.417624,"ack-size of 1000 during tuning and 5000 during test and the no-reordering-over-punctuation heuristic. We used the compact phrase table representation by JunczysDowmunt (2012). For our German-to-English experiments, we used compound splitting (Koehn and Knight, 2003). German-to-English and English-to-German baseline systems also used POS and morphological target sequence models built on the in-domain subset of the parallel corpus using Kneser-Ney smoothed 7-gram models and as additional factors in phrase translation models (Koehn and Hoang, 2007). We used an unsupervised transliteration model (Durrani et al., 2014) to transliterate OOV words when translating into Russian. Tuning and Test: The systems were tuned on the dev2010 dataset and evaluated on the test2010-2013 datasets made available for the IWSLT-13 workshop. We performed a secondary set of experiments for German-English pairs using tuning and test sets made available for the WMT-13 workshop. We concatenated the news-test sets 2008 and 2009 to obtain a large dev-set of 4576 sentences. Evaluation was performed on the news-test set 2013 which contains 3000 sentences. Tuning was performed using the k-best batch MIRA algorithm (Cherry and Foster, 2"
C14-1041,P08-1115,0,0.0148386,"used source syntactic structures as additional complex tag factors for English-to-Turkish phrase-based machine translation. Green and DeNero (2012) proposed a target-side, class-based agreement model to handle morpho-syntactic agreement errors when translating from English-to-Arabic. El Kholy and Habash (2012) tested three models to find out which features are best handled by modeling them as a part of translation, and which ones are better predicted through generation, also in the English-to-Arabic task. Several researchers attempted to use word lattices to handle generalized representation (Dyer et al., 2008; Hardmeier et al., 2010; Wuebker and Ney, 2012). Automatically clustering the training data into word classes in order to obtain smoother 1 We are referring to hard clustering here. Soft clustering is intractable as it requires a marginalization over all possible classes when calculating the n-gram probabilities. 422 Figure 1: Operation Sequence Model – Training Sentence with Generation and Test Sentences distributions and better generalizations has been a widely known and applied technique in natural language processing. Training based on word classes has been previously explored by various"
C14-1041,2012.eamt-1.6,0,0.0235645,"Missing"
C14-1041,E12-1068,1,0.40458,"N-gram-based search to improve mid-range reorderings. Our work is similar to them except that OSM model is substantially different from the TSM model as it integrates both the translation and reordering mechanisms into a combined model. Therefore both translation and reordering decisions can benefit from richer generalized representations. A second group of work addresses the problem of translating into morphologically richer languages. The idea of translating to stems and then inflecting the stems in a separate step has been studied by Toutanova et al. (2008), de Gispert and Mariˆno (2008), Fraser et al. (2012), Chahuneau et al. (2013) and others. Koehn and Hoang (2007) proposed to integrate different levels of linguistic information as factors into the phrase-based translation model. Yeniterzi and Oflazer (2010) used source syntactic structures as additional complex tag factors for English-to-Turkish phrase-based machine translation. Green and DeNero (2012) proposed a target-side, class-based agreement model to handle morpho-syntactic agreement errors when translating from English-to-Arabic. El Kholy and Habash (2012) tested three models to find out which features are best handled by modeling them"
C14-1041,P06-1121,0,0.00965802,"on rearranging source sentences to be in the target order. Xia and McCord (2004) proposed a method to automatically learn rewrite rules to preorder source sentences. Collins et al. (2005) and Popovi´c and Ney (2006) proposed methods for reordering the source using a small set of handcrafted rules. Crego and Mari˜no (2007) use syntactic trees to derive rewrite rules. Hoang and Koehn (2009) used POS tags to create templates for surface word translation to create longer phrase translation. A whole new paradigm of using syntactic annotation to address long range reorderings has emerged following Galley et al. (2006), Zollmann and Venugopal (2006), Chiang (2007) etc. Crego and Yvon (2010) and Niehues et al. (2011) used a Tuple Sequence Model (TSM) over POS tags in an N-gram-based search to improve mid-range reorderings. Our work is similar to them except that OSM model is substantially different from the TSM model as it integrates both the translation and reordering mechanisms into a combined model. Therefore both translation and reordering decisions can benefit from richer generalized representations. A second group of work addresses the problem of translating into morphologically richer languages. The i"
C14-1041,P14-1066,0,0.0440211,"hnique in natural language processing. Training based on word classes has been previously explored by various researchers. Cherry (2013) addressed data sparsity in lexicalized reordering models by using sparse features based on word classes. Other parallel attempts on using word-class models include Wuebker et al. (2013), Chahuneau et al. (2013) and Bisazza and Monz (2014). More recent research has started to set apart from the conventional maximum likelihood estimates toward neural network-based models that use continuous space representation (Schwenk, 2012; Le et al., 2012; Hu et al., 2014; Gao et al., 2014). Although these methods have achieved impressive improvements, traditional models continue to dominate the field due to their simplicity and low computational complexity. How much of the improvement will be retained when scaling these models to all available data instead of a limited amount will be interesting. 3 Operation Sequence Model The Operation Sequence Model (Durrani et al., 2011) is an instance of the N-gram based SMT framework (Casacuberta and Vidal, 2004; Mari˜no et al., 2006). It represents the translation process through a sequence of operations. An operation can be to simultaneo"
C14-1041,P12-1016,0,0.00479192,"group of work addresses the problem of translating into morphologically richer languages. The idea of translating to stems and then inflecting the stems in a separate step has been studied by Toutanova et al. (2008), de Gispert and Mariˆno (2008), Fraser et al. (2012), Chahuneau et al. (2013) and others. Koehn and Hoang (2007) proposed to integrate different levels of linguistic information as factors into the phrase-based translation model. Yeniterzi and Oflazer (2010) used source syntactic structures as additional complex tag factors for English-to-Turkish phrase-based machine translation. Green and DeNero (2012) proposed a target-side, class-based agreement model to handle morpho-syntactic agreement errors when translating from English-to-Arabic. El Kholy and Habash (2012) tested three models to find out which features are best handled by modeling them as a part of translation, and which ones are better predicted through generation, also in the English-to-Arabic task. Several researchers attempted to use word lattices to handle generalized representation (Dyer et al., 2008; Hardmeier et al., 2010; Wuebker and Ney, 2012). Automatically clustering the training data into word classes in order to obtain"
C14-1041,W10-1710,0,0.04755,"ic structures as additional complex tag factors for English-to-Turkish phrase-based machine translation. Green and DeNero (2012) proposed a target-side, class-based agreement model to handle morpho-syntactic agreement errors when translating from English-to-Arabic. El Kholy and Habash (2012) tested three models to find out which features are best handled by modeling them as a part of translation, and which ones are better predicted through generation, also in the English-to-Arabic task. Several researchers attempted to use word lattices to handle generalized representation (Dyer et al., 2008; Hardmeier et al., 2010; Wuebker and Ney, 2012). Automatically clustering the training data into word classes in order to obtain smoother 1 We are referring to hard clustering here. Soft clustering is intractable as it requires a marginalization over all possible classes when calculating the n-gram probabilities. 422 Figure 1: Operation Sequence Model – Training Sentence with Generation and Test Sentences distributions and better generalizations has been a widely known and applied technique in natural language processing. Training based on word classes has been previously explored by various researchers. Cherry (201"
C14-1041,2012.iwslt-papers.17,1,0.832647,"et al., 2007), replicating the settings described in (Birch et al., 2013) developed for the 2013 Workshop on Spoken Language Translation. The features included: a maximum sentence length of 80, grow-diag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011) used at runtime, a lexically-driven 5-gram operation sequence model (Durrani et al., 2013b) with 4 additional supportive features: 2 gap-based penalties, 1 distance-based feature and 1 deletion penalty, lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), a distortion limit of 6, 100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the no-reordering-over-punctuation heuristic. We used the compact phrase table representation by JunczysDowmunt (2012). For our German-to-English experiments, we used compound splitting (Koehn and Knight, 2003). German-to-English and English-to-German baseline systems also used POS and morphological target sequence models built on the in-domain subset of the parallel corpus using Kne"
C14-1041,W11-2123,0,0.00711102,"∆+0.54 27.71 ∆+0.44 31.55 ∆+0.09 27.32 ∆+0.05 31.58 ∆+0.12 27.20 ∆-0.07 31.40 ∆-0.06 27.15 ∆-0.12 Table 2: Evaluating Generalized OSM Models for German-English pairs – Bold: Statistically Significant (Koehn, 2004) w.r.t Baseline Baseline System: We trained a Moses system (Koehn et al., 2007), replicating the settings described in (Birch et al., 2013) developed for the 2013 Workshop on Spoken Language Translation. The features included: a maximum sentence length of 80, grow-diag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011) used at runtime, a lexically-driven 5-gram operation sequence model (Durrani et al., 2013b) with 4 additional supportive features: 2 gap-based penalties, 1 distance-based feature and 1 deletion penalty, lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), a distortion limit of 6, 100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the no-reordering-over-punctuation heuristic. We used the compact phrase table representation by Juncz"
C14-1041,E09-1043,1,0.844957,"focuses on using linguistic knowledge to improve reordering between syntactically different languages. A second group focuses on translating into morphologically rich languages. Initial efforts to use linguistic annotation focused on rearranging source sentences to be in the target order. Xia and McCord (2004) proposed a method to automatically learn rewrite rules to preorder source sentences. Collins et al. (2005) and Popovi´c and Ney (2006) proposed methods for reordering the source using a small set of handcrafted rules. Crego and Mari˜no (2007) use syntactic trees to derive rewrite rules. Hoang and Koehn (2009) used POS tags to create templates for surface word translation to create longer phrase translation. A whole new paradigm of using syntactic annotation to address long range reorderings has emerged following Galley et al. (2006), Zollmann and Venugopal (2006), Chiang (2007) etc. Crego and Yvon (2010) and Niehues et al. (2011) used a Tuple Sequence Model (TSM) over POS tags in an N-gram-based search to improve mid-range reorderings. Our work is similar to them except that OSM model is substantially different from the TSM model as it integrates both the translation and reordering mechanisms into"
C14-1041,E14-1003,0,0.045332,"n and applied technique in natural language processing. Training based on word classes has been previously explored by various researchers. Cherry (2013) addressed data sparsity in lexicalized reordering models by using sparse features based on word classes. Other parallel attempts on using word-class models include Wuebker et al. (2013), Chahuneau et al. (2013) and Bisazza and Monz (2014). More recent research has started to set apart from the conventional maximum likelihood estimates toward neural network-based models that use continuous space representation (Schwenk, 2012; Le et al., 2012; Hu et al., 2014; Gao et al., 2014). Although these methods have achieved impressive improvements, traditional models continue to dominate the field due to their simplicity and low computational complexity. How much of the improvement will be retained when scaling these models to all available data instead of a limited amount will be interesting. 3 Operation Sequence Model The Operation Sequence Model (Durrani et al., 2011) is an instance of the N-gram based SMT framework (Casacuberta and Vidal, 2004; Mari˜no et al., 2006). It represents the translation process through a sequence of operations. An operation c"
C14-1041,P07-1019,0,0.0167512,"atures included: a maximum sentence length of 80, grow-diag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011) used at runtime, a lexically-driven 5-gram operation sequence model (Durrani et al., 2013b) with 4 additional supportive features: 2 gap-based penalties, 1 distance-based feature and 1 deletion penalty, lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), a distortion limit of 6, 100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the no-reordering-over-punctuation heuristic. We used the compact phrase table representation by JunczysDowmunt (2012). For our German-to-English experiments, we used compound splitting (Koehn and Knight, 2003). German-to-English and English-to-German baseline systems also used POS and morphological target sequence models built on the in-domain subset of the parallel corpus using Kneser-Ney smoothed 7-gram models and as additional factors in phrase translation models (Koehn and Hoang, 2007). We used an unsupervised transliterat"
C14-1041,D07-1091,1,0.330247,"can generalize better in sparse data conditions. The model benefits from wider contextual information as we show empirically in our results. We investigate two methods to combine generalized OSM models with the lexically driven OSM model and experimented on German-English translation tasks. Our best system that uses a linear combination of different OSM models gives significant improvements over a competitive baseline system. An improvement of up to +1.35 was observed on the English-to-German and up to +0.63 BLEU points on the German-to-English task over a factored augmented baseline system (Koehn and Hoang, 2007). POS taggers and morphological analyzers, however, are not available for many resource poor languages. In the second half of the paper we investigate whether annotating the data with automatic word This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 421 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 421–432, Dublin, Ireland, August 23-29 2014. clusters helps improve t"
C14-1041,E03-1076,1,0.545896,"2013b) with 4 additional supportive features: 2 gap-based penalties, 1 distance-based feature and 1 deletion penalty, lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), a distortion limit of 6, 100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the no-reordering-over-punctuation heuristic. We used the compact phrase table representation by JunczysDowmunt (2012). For our German-to-English experiments, we used compound splitting (Koehn and Knight, 2003). German-to-English and English-to-German baseline systems also used POS and morphological target sequence models built on the in-domain subset of the parallel corpus using Kneser-Ney smoothed 7-gram models and as additional factors in phrase translation models (Koehn and Hoang, 2007). We used an unsupervised transliteration model (Durrani et al., 2014) to transliterate OOV words when translating into Russian. Tuning and Test: The systems were tuned on the dev2010 dataset and evaluated on the test2010-2013 datasets made available for the IWSLT-13 workshop. We performed a secondary set of exper"
C14-1041,P07-2045,1,0.0212808,"Missing"
C14-1041,W04-3250,1,0.141116,"Missing"
C14-1041,N04-1022,0,0.0715901,"on Spoken Language Translation. The features included: a maximum sentence length of 80, grow-diag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011) used at runtime, a lexically-driven 5-gram operation sequence model (Durrani et al., 2013b) with 4 additional supportive features: 2 gap-based penalties, 1 distance-based feature and 1 deletion penalty, lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), a distortion limit of 6, 100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the no-reordering-over-punctuation heuristic. We used the compact phrase table representation by JunczysDowmunt (2012). For our German-to-English experiments, we used compound splitting (Koehn and Knight, 2003). German-to-English and English-to-German baseline systems also used POS and morphological target sequence models built on the in-domain subset of the parallel corpus using Kneser-Ney smoothed 7-gram models and as additional factors in phrase translation models (Koehn and Hoang, 2007"
C14-1041,N12-1005,0,0.0378999,"een a widely known and applied technique in natural language processing. Training based on word classes has been previously explored by various researchers. Cherry (2013) addressed data sparsity in lexicalized reordering models by using sparse features based on word classes. Other parallel attempts on using word-class models include Wuebker et al. (2013), Chahuneau et al. (2013) and Bisazza and Monz (2014). More recent research has started to set apart from the conventional maximum likelihood estimates toward neural network-based models that use continuous space representation (Schwenk, 2012; Le et al., 2012; Hu et al., 2014; Gao et al., 2014). Although these methods have achieved impressive improvements, traditional models continue to dominate the field due to their simplicity and low computational complexity. How much of the improvement will be retained when scaling these models to all available data instead of a limited amount will be interesting. 3 Operation Sequence Model The Operation Sequence Model (Durrani et al., 2011) is an instance of the N-gram based SMT framework (Casacuberta and Vidal, 2004; Mari˜no et al., 2006). It represents the translation process through a sequence of operation"
C14-1041,J06-4004,0,0.140693,"Missing"
C14-1041,W11-2124,0,0.0222468,"d to automatically learn rewrite rules to preorder source sentences. Collins et al. (2005) and Popovi´c and Ney (2006) proposed methods for reordering the source using a small set of handcrafted rules. Crego and Mari˜no (2007) use syntactic trees to derive rewrite rules. Hoang and Koehn (2009) used POS tags to create templates for surface word translation to create longer phrase translation. A whole new paradigm of using syntactic annotation to address long range reorderings has emerged following Galley et al. (2006), Zollmann and Venugopal (2006), Chiang (2007) etc. Crego and Yvon (2010) and Niehues et al. (2011) used a Tuple Sequence Model (TSM) over POS tags in an N-gram-based search to improve mid-range reorderings. Our work is similar to them except that OSM model is substantially different from the TSM model as it integrates both the translation and reordering mechanisms into a combined model. Therefore both translation and reordering decisions can benefit from richer generalized representations. A second group of work addresses the problem of translating into morphologically richer languages. The idea of translating to stems and then inflecting the stems in a separate step has been studied by To"
C14-1041,J03-1002,0,0.0202923,"421–432, Dublin, Ireland, August 23-29 2014. clusters helps improve the performance. Word clustering is similar to POS-tagging/Morphological annotation except that it also captures interesting syntactic and lexical semantics, for example countries and languages are grouped in separate clusters, animate objects are differentiated from inanimate objects, colors are grouped in a separate cluster etc. Word clusters, however, deterministically map each word type to a unique1 cluster, unlike POS/Morph tagging, and therefore might be less useful for disambiguation. We use the mkcls utility in GIZA (Och and Ney, 2003) to cluster source and target vocabularies into classes and will therefore refer to automatic classes as Och clusters/classes in this paper. We first use Och classes as an additional factor in phrase-based translation model, along with a target LM model over cluster-ids to improve the baseline system. We then additionally use the OSM model over cluster-ids. Our experiments include translation from English to Dutch, French, Italian, Polish, Portuguese, Russian, Spanish, Slovenian and Turkish on IWSLT shared task data. Our results show an average improvement of +0.80, ranging from +0.41 to +2.02"
C14-1041,E99-1010,0,0.442864,"or the Germanto-English pair, giving a statistically significant gain of +0.63 on iwslt10 and +0.35 on wmt13 . Using both the models together did not give any further significant improvements. The results changed by +0.10 and -0.09 on the wmt13 and iwslt10 test-sets respectively. Results-II – Using Och Classes: In our secondary experiments we tested the effect of using Och clusters. The overall goal was to study whether using unsupervised word classes can serve the same purpose as POS tags and to compare the two methods of annotating the data. We obtained Och clusters using the mkcls utility (Och, 1999) in GIZA++ (Och and Ney, 2003). This is generally run during the alignment process where data is divided into 50 classes to estimate IBM Model-4. Chahuneau et al. (2013) found mapping data to 600 Och clusters useful, so we used this as well. We additionally experimented with using 200 and 1000 classes. We integrated Och clusters as additional factors4 when training the phrase-translation models and used a monolingual n-gram model over cluster-ids built on the target-side of the in-domain corpus. Then we added a 5-gram OSM model over cluster-ids. We replace surface forms with their cluster-ids"
C14-1041,P02-1040,0,0.100863,"nto Russian. Tuning and Test: The systems were tuned on the dev2010 dataset and evaluated on the test2010-2013 datasets made available for the IWSLT-13 workshop. We performed a secondary set of experiments for German-English pairs using tuning and test sets made available for the WMT-13 workshop. We concatenated the news-test sets 2008 and 2009 to obtain a large dev-set of 4576 sentences. Evaluation was performed on the news-test set 2013 which contains 3000 sentences. Tuning was performed using the k-best batch MIRA algorithm (Cherry and Foster, 2012) with at most 25 iterations. We use BLEU (Papineni et al., 2002) as a metric to evaluate our results. Results I – Using Linguistic Annotation: We trained 5-gram OSM models over different representations and added these to the baseline system. First we evaluated Modeland (Mand ) which uses a MIRA tuned linear combination of different OSM models versus Modelor (Mor ) which computes only one OSM model but allows the generator to switch between different OSM models built on various generalized forms. Table 2 shows results from running experiments on German-English pairs. We found that the simpler model Modeland outperforms Modelor in all the experiments. Model"
C14-1041,popovic-ney-2006-pos,0,0.201708,"Missing"
C14-1041,C12-2104,0,0.0162602,"lizations has been a widely known and applied technique in natural language processing. Training based on word classes has been previously explored by various researchers. Cherry (2013) addressed data sparsity in lexicalized reordering models by using sparse features based on word classes. Other parallel attempts on using word-class models include Wuebker et al. (2013), Chahuneau et al. (2013) and Bisazza and Monz (2014). More recent research has started to set apart from the conventional maximum likelihood estimates toward neural network-based models that use continuous space representation (Schwenk, 2012; Le et al., 2012; Hu et al., 2014; Gao et al., 2014). Although these methods have achieved impressive improvements, traditional models continue to dominate the field due to their simplicity and low computational complexity. How much of the improvement will be retained when scaling these models to all available data instead of a limited amount will be interesting. 3 Operation Sequence Model The Operation Sequence Model (Durrani et al., 2011) is an instance of the N-gram based SMT framework (Casacuberta and Vidal, 2004; Mari˜no et al., 2006). It represents the translation process through a sequ"
C14-1041,P08-1059,0,0.0128728,"1) used a Tuple Sequence Model (TSM) over POS tags in an N-gram-based search to improve mid-range reorderings. Our work is similar to them except that OSM model is substantially different from the TSM model as it integrates both the translation and reordering mechanisms into a combined model. Therefore both translation and reordering decisions can benefit from richer generalized representations. A second group of work addresses the problem of translating into morphologically richer languages. The idea of translating to stems and then inflecting the stems in a separate step has been studied by Toutanova et al. (2008), de Gispert and Mariˆno (2008), Fraser et al. (2012), Chahuneau et al. (2013) and others. Koehn and Hoang (2007) proposed to integrate different levels of linguistic information as factors into the phrase-based translation model. Yeniterzi and Oflazer (2010) used source syntactic structures as additional complex tag factors for English-to-Turkish phrase-based machine translation. Green and DeNero (2012) proposed a target-side, class-based agreement model to handle morpho-syntactic agreement errors when translating from English-to-Arabic. El Kholy and Habash (2012) tested three models to find"
C14-1041,W12-3157,0,0.0126006,"nal complex tag factors for English-to-Turkish phrase-based machine translation. Green and DeNero (2012) proposed a target-side, class-based agreement model to handle morpho-syntactic agreement errors when translating from English-to-Arabic. El Kholy and Habash (2012) tested three models to find out which features are best handled by modeling them as a part of translation, and which ones are better predicted through generation, also in the English-to-Arabic task. Several researchers attempted to use word lattices to handle generalized representation (Dyer et al., 2008; Hardmeier et al., 2010; Wuebker and Ney, 2012). Automatically clustering the training data into word classes in order to obtain smoother 1 We are referring to hard clustering here. Soft clustering is intractable as it requires a marginalization over all possible classes when calculating the n-gram probabilities. 422 Figure 1: Operation Sequence Model – Training Sentence with Generation and Test Sentences distributions and better generalizations has been a widely known and applied technique in natural language processing. Training based on word classes has been previously explored by various researchers. Cherry (2013) addressed data sparsi"
C14-1041,D13-1138,0,0.365066,"stering is intractable as it requires a marginalization over all possible classes when calculating the n-gram probabilities. 422 Figure 1: Operation Sequence Model – Training Sentence with Generation and Test Sentences distributions and better generalizations has been a widely known and applied technique in natural language processing. Training based on word classes has been previously explored by various researchers. Cherry (2013) addressed data sparsity in lexicalized reordering models by using sparse features based on word classes. Other parallel attempts on using word-class models include Wuebker et al. (2013), Chahuneau et al. (2013) and Bisazza and Monz (2014). More recent research has started to set apart from the conventional maximum likelihood estimates toward neural network-based models that use continuous space representation (Schwenk, 2012; Le et al., 2012; Hu et al., 2014; Gao et al., 2014). Although these methods have achieved impressive improvements, traditional models continue to dominate the field due to their simplicity and low computational complexity. How much of the improvement will be retained when scaling these models to all available data instead of a limited amount will be inte"
C14-1041,C04-1073,0,0.0421089,"Section 2 gives an account on related work. Section 3 discusses the factor-based OSM model. Section 4 presents the experimental setup and the results. Section 5 concludes the paper. 2 Related Work Previous work on integrating linguistic knowledge into SMT models can be broken into two groups. The first group focuses on using linguistic knowledge to improve reordering between syntactically different languages. A second group focuses on translating into morphologically rich languages. Initial efforts to use linguistic annotation focused on rearranging source sentences to be in the target order. Xia and McCord (2004) proposed a method to automatically learn rewrite rules to preorder source sentences. Collins et al. (2005) and Popovi´c and Ney (2006) proposed methods for reordering the source using a small set of handcrafted rules. Crego and Mari˜no (2007) use syntactic trees to derive rewrite rules. Hoang and Koehn (2009) used POS tags to create templates for surface word translation to create longer phrase translation. A whole new paradigm of using syntactic annotation to address long range reorderings has emerged following Galley et al. (2006), Zollmann and Venugopal (2006), Chiang (2007) etc. Crego and"
C14-1041,P10-1047,0,0.0375461,"ering mechanisms into a combined model. Therefore both translation and reordering decisions can benefit from richer generalized representations. A second group of work addresses the problem of translating into morphologically richer languages. The idea of translating to stems and then inflecting the stems in a separate step has been studied by Toutanova et al. (2008), de Gispert and Mariˆno (2008), Fraser et al. (2012), Chahuneau et al. (2013) and others. Koehn and Hoang (2007) proposed to integrate different levels of linguistic information as factors into the phrase-based translation model. Yeniterzi and Oflazer (2010) used source syntactic structures as additional complex tag factors for English-to-Turkish phrase-based machine translation. Green and DeNero (2012) proposed a target-side, class-based agreement model to handle morpho-syntactic agreement errors when translating from English-to-Arabic. El Kholy and Habash (2012) tested three models to find out which features are best handled by modeling them as a part of translation, and which ones are better predicted through generation, also in the English-to-Arabic task. Several researchers attempted to use word lattices to handle generalized representation"
C14-1041,W06-3119,0,0.0289361,"sentences to be in the target order. Xia and McCord (2004) proposed a method to automatically learn rewrite rules to preorder source sentences. Collins et al. (2005) and Popovi´c and Ney (2006) proposed methods for reordering the source using a small set of handcrafted rules. Crego and Mari˜no (2007) use syntactic trees to derive rewrite rules. Hoang and Koehn (2009) used POS tags to create templates for surface word translation to create longer phrase translation. A whole new paradigm of using syntactic annotation to address long range reorderings has emerged following Galley et al. (2006), Zollmann and Venugopal (2006), Chiang (2007) etc. Crego and Yvon (2010) and Niehues et al. (2011) used a Tuple Sequence Model (TSM) over POS tags in an N-gram-based search to improve mid-range reorderings. Our work is similar to them except that OSM model is substantially different from the TSM model as it integrates both the translation and reordering mechanisms into a combined model. Therefore both translation and reordering decisions can benefit from richer generalized representations. A second group of work addresses the problem of translating into morphologically richer languages. The idea of translating to stems and"
C14-1041,J04-2004,0,\N,Missing
C14-1041,2013.iwslt-evaluation.16,1,\N,Missing
C14-1041,W13-2201,1,\N,Missing
C14-1041,2013.iwslt-evaluation.3,1,\N,Missing
C14-2028,2013.mtsummit-papers.5,1,0.880457,"Missing"
C14-2028,2013.mtsummit-wptp.13,1,0.900099,"Missing"
C14-2028,2012.amta-papers.22,1,0.865102,"Missing"
C14-2028,2013.mtsummit-wptp.10,0,0.135035,"Missing"
C14-2028,W13-2231,1,0.678754,"Missing"
C14-2028,P14-1067,1,0.806908,"Missing"
C14-2028,2013.mtsummit-wptp.7,1,\N,Missing
D07-1077,C04-1090,0,0.0253863,". (2006) build a full parse tree in the target language, again effectively allowing hierarchical reordering based on synchronous grammars. It is worth noting that none of these approaches to reordering make use of explicit syntactic information in the source language—for example, none of the methods make use of an existing source-language parser (the systems of Yamada and Knight (2001) and Marcu et al. (2006) make use of a parser in the target language, i.e., English). Finally, note that a number of statistical MT systems make use of source language syntax in transducer-style approaches; see (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006). In contrast to the preprocessing approach, they attempt to incorporate syntax directly into the decoding stage. 3 Chinese Syntactic Reordering Rules We used the Penn Chinese Treebank guidelines (Xue et al., 2005) in searching for a suitable set of reordering rules. We examined all phrase types in the Treebank; potentially phrases of any type could be candidates for reordering rules. Table 1 provides a list of Treebank phrase tags for easy reference. We ruled out several phrase types as not requiring reordering"
D07-1077,P06-1077,0,0.0336674,"e, again effectively allowing hierarchical reordering based on synchronous grammars. It is worth noting that none of these approaches to reordering make use of explicit syntactic information in the source language—for example, none of the methods make use of an existing source-language parser (the systems of Yamada and Knight (2001) and Marcu et al. (2006) make use of a parser in the target language, i.e., English). Finally, note that a number of statistical MT systems make use of source language syntax in transducer-style approaches; see (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006). In contrast to the preprocessing approach, they attempt to incorporate syntax directly into the decoding stage. 3 Chinese Syntactic Reordering Rules We used the Penn Chinese Treebank guidelines (Xue et al., 2005) in searching for a suitable set of reordering rules. We examined all phrase types in the Treebank; potentially phrases of any type could be candidates for reordering rules. Table 1 provides a list of Treebank phrase tags for easy reference. We ruled out several phrase types as not requiring reordering 739 ADJP ADVP CLP CP DNP DP DVP FRAG IP LCP LST NP PP PRN QP"
D07-1077,J96-1002,0,0.00714724,"hese parsers are typically of relatively low accuracy, particularly given that Chinese requires a word-segmentation step that is not required in languages such as English. Our results show that Chinese parses are useful in SMT in spite of this problem. We report results showing the precision of the reordering rules—essentially testing how often the Chinese sentences are correctly reordered— to give more insight into this issue. We also report experiments which assess the impact of each type of reordering rule on translation accuracy. 2 Related Work A number of researchers (Brown et al., 1992; Berger et al., 1996; Niessen and Ney, 2004; Xia and McCord, 2004; Collins et al., 2005) have described approaches that preprocess the source language input in SMT systems. We are not, however, aware of work on this topic for translation from Chinese to English. Brown et al. (1992) describe an analysis component for French which moves phrases around (in addition to other transformations) so the source and target sentences are closer to each other in word order. Berger et al. (1996) describe an approach for French that reorders phrases of the form NOUN1 de NOUN2 . Xia and McCord (2004) describe an approach for Fre"
D07-1077,1992.tmi-1.8,0,0.438727,"38 parsers is that these parsers are typically of relatively low accuracy, particularly given that Chinese requires a word-segmentation step that is not required in languages such as English. Our results show that Chinese parses are useful in SMT in spite of this problem. We report results showing the precision of the reordering rules—essentially testing how often the Chinese sentences are correctly reordered— to give more insight into this issue. We also report experiments which assess the impact of each type of reordering rule on translation accuracy. 2 Related Work A number of researchers (Brown et al., 1992; Berger et al., 1996; Niessen and Ney, 2004; Xia and McCord, 2004; Collins et al., 2005) have described approaches that preprocess the source language input in SMT systems. We are not, however, aware of work on this topic for translation from Chinese to English. Brown et al. (1992) describe an analysis component for French which moves phrases around (in addition to other transformations) so the source and target sentences are closer to each other in word order. Berger et al. (1996) describe an approach for French that reorders phrases of the form NOUN1 de NOUN2 . Xia and McCord (2004) describ"
D07-1077,W06-1606,0,0.216949,"at the Winter Olympics” in English. ÁG $¬þ¤1 this is best accomplishment DEC French delegation achieve at on Winter Olympics This reordering is relatively easy to express using syntactic transformations—for example, it is simple to move the entire relative clause “French delegation at Winter Olympics on achieve DEC” to a position that is after the noun phrase it modifies, namely “best accomplishment.” Phrase-based systems are quite limited in their ability to perform transformations of this type. More recently developed hierarchical systems (e.g., (Yamada and Knight, 2001; Chiang, 2005; Marcu et al., 2006)) may be better equipped to deal with reordering of this type; however, in this example they would effectively have to first identify the span of the relative clause, and then move it into the correct position, without any explicit representation of the source language syntax. In this paper, we describe a set of syntactic reordering rules that exploit systematic differences between Chinese and English word order. The resulting system is used as a preprocessor for both training and test sentences, transforming Chinese sentences to be much closer to English. We report results for the method on t"
D07-1077,J04-2003,0,0.0335531,"cally of relatively low accuracy, particularly given that Chinese requires a word-segmentation step that is not required in languages such as English. Our results show that Chinese parses are useful in SMT in spite of this problem. We report results showing the precision of the reordering rules—essentially testing how often the Chinese sentences are correctly reordered— to give more insight into this issue. We also report experiments which assess the impact of each type of reordering rule on translation accuracy. 2 Related Work A number of researchers (Brown et al., 1992; Berger et al., 1996; Niessen and Ney, 2004; Xia and McCord, 2004; Collins et al., 2005) have described approaches that preprocess the source language input in SMT systems. We are not, however, aware of work on this topic for translation from Chinese to English. Brown et al. (1992) describe an analysis component for French which moves phrases around (in addition to other transformations) so the source and target sentences are closer to each other in word order. Berger et al. (1996) describe an approach for French that reorders phrases of the form NOUN1 de NOUN2 . Xia and McCord (2004) describe an approach for French, where reordering r"
D07-1077,P05-1033,0,0.766039,"ation achieved at the Winter Olympics” in English. ÁG $¬þ¤1 this is best accomplishment DEC French delegation achieve at on Winter Olympics This reordering is relatively easy to express using syntactic transformations—for example, it is simple to move the entire relative clause “French delegation at Winter Olympics on achieve DEC” to a position that is after the noun phrase it modifies, namely “best accomplishment.” Phrase-based systems are quite limited in their ability to perform transformations of this type. More recently developed hierarchical systems (e.g., (Yamada and Knight, 2001; Chiang, 2005; Marcu et al., 2006)) may be better equipped to deal with reordering of this type; however, in this example they would effectively have to first identify the span of the relative clause, and then move it into the correct position, without any explicit representation of the source language syntax. In this paper, we describe a set of syntactic reordering rules that exploit systematic differences between Chinese and English word order. The resulting system is used as a preprocessor for both training and test sentences, transforming Chinese sentences to be much closer to English. We report result"
D07-1077,P02-1040,0,0.114258,"r training, including LDC2002E18, LDC2003E07, LDC2003E14, LDC2005E83, LDC2005T06, LDC2006E26, LDC2006E8, and LDC2006G05. 742 Dev 31.57 32.86 +1.29 Nist06 28.52 30.86 +2.34 Table 2: BLEU score of the baseline and reordered systems. presented in Collins (1997). We then applied the reordering rules described in the previous section to the parse tree of each input. The reordered sentence is then re-tokenized to be consistent with the baseline system, which uses a different tokenization scheme that is more friendly to the MT system.3 We use BLEU scores as the performance measure in our evaluation (Papineni et al., 2002). Table 2 gives results for the baseline and reordered systems on both the development and test sets. As shown in the table, the reordering method is able to improve the BLEU scores by 1.29 points on the development set, and by 2.34 on the NIST 2006 set. 4.1 Frequency and Accuracy of Reordering Rules We collected statistics to evaluate how often and accurately the reordering rules are applied in the data. The accuracy is measured in terms of the percentage of rule applications that correctly reorder sentences. The vast majority of reordering errors are due to parsing mistakes. Table 3 summariz"
D07-1077,P05-1066,1,0.685642,"Missing"
D07-1077,P05-1034,0,0.120971,"n the target language, again effectively allowing hierarchical reordering based on synchronous grammars. It is worth noting that none of these approaches to reordering make use of explicit syntactic information in the source language—for example, none of the methods make use of an existing source-language parser (the systems of Yamada and Knight (2001) and Marcu et al. (2006) make use of a parser in the target language, i.e., English). Finally, note that a number of statistical MT systems make use of source language syntax in transducer-style approaches; see (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006). In contrast to the preprocessing approach, they attempt to incorporate syntax directly into the decoding stage. 3 Chinese Syntactic Reordering Rules We used the Penn Chinese Treebank guidelines (Xue et al., 2005) in searching for a suitable set of reordering rules. We examined all phrase types in the Treebank; potentially phrases of any type could be candidates for reordering rules. Table 1 provides a list of Treebank phrase tags for easy reference. We ruled out several phrase types as not requiring reordering 739 ADJP ADVP CLP CP DNP DP DVP FRAG IP LCP"
D07-1077,P97-1003,1,0.1369,"be applied, which include segmentation, part-of-speech tagging, and parsing. We trained a Chinese Treebank-style tokenizer and partof-speech tagger, both using a tagging model based on a perceptron learning algorithm (Collins, 2002). We used the Chinese parser described by Sun and Jurafsky (2004), which was adapted from the parser 2 We used 8 corpora for training, including LDC2002E18, LDC2003E07, LDC2003E14, LDC2005E83, LDC2005T06, LDC2006E26, LDC2006E8, and LDC2006G05. 742 Dev 31.57 32.86 +1.29 Nist06 28.52 30.86 +2.34 Table 2: BLEU score of the baseline and reordered systems. presented in Collins (1997). We then applied the reordering rules described in the previous section to the parse tree of each input. The reordered sentence is then re-tokenized to be consistent with the baseline system, which uses a different tokenization scheme that is more friendly to the MT system.3 We use BLEU scores as the performance measure in our evaluation (Papineni et al., 2002). Table 2 gives results for the baseline and reordered systems on both the development and test sets. As shown in the table, the reordering method is able to improve the BLEU scores by 1.29 points on the development set, and by 2.34 on"
D07-1077,W02-1001,1,0.0378144,"lit into two sets of roughly equal sizes: a tuning set of 2347 sentences is used for optimizing various parameters using minimum error training (also using the MOSES toolkit), and a development set of 2320 sentences is used for various analysis experiments. We report results on the NIST 2006 evaluation data. A series of processing steps are needed before the reordering rules can be applied, which include segmentation, part-of-speech tagging, and parsing. We trained a Chinese Treebank-style tokenizer and partof-speech tagger, both using a tagging model based on a perceptron learning algorithm (Collins, 2002). We used the Chinese parser described by Sun and Jurafsky (2004), which was adapted from the parser 2 We used 8 corpora for training, including LDC2002E18, LDC2003E07, LDC2003E14, LDC2005E83, LDC2005T06, LDC2006E26, LDC2006E8, and LDC2006G05. 742 Dev 31.57 32.86 +1.29 Nist06 28.52 30.86 +2.34 Table 2: BLEU score of the baseline and reordered systems. presented in Collins (1997). We then applied the reordering rules described in the previous section to the parse tree of each input. The reordered sentence is then re-tokenized to be consistent with the baseline system, which uses a different tok"
D07-1077,N04-1032,0,0.038956,"f 2347 sentences is used for optimizing various parameters using minimum error training (also using the MOSES toolkit), and a development set of 2320 sentences is used for various analysis experiments. We report results on the NIST 2006 evaluation data. A series of processing steps are needed before the reordering rules can be applied, which include segmentation, part-of-speech tagging, and parsing. We trained a Chinese Treebank-style tokenizer and partof-speech tagger, both using a tagging model based on a perceptron learning algorithm (Collins, 2002). We used the Chinese parser described by Sun and Jurafsky (2004), which was adapted from the parser 2 We used 8 corpora for training, including LDC2002E18, LDC2003E07, LDC2003E14, LDC2005E83, LDC2005T06, LDC2006E26, LDC2006E8, and LDC2006G05. 742 Dev 31.57 32.86 +1.29 Nist06 28.52 30.86 +2.34 Table 2: BLEU score of the baseline and reordered systems. presented in Collins (1997). We then applied the reordering rules described in the previous section to the parse tree of each input. The reordered sentence is then re-tokenized to be consistent with the baseline system, which uses a different tokenization scheme that is more friendly to the MT system.3 We use"
D07-1077,N04-4026,0,0.436662,"al. (2005) also describe an approach for German, concentrating on reordering German clauses, which have quite different word order from clauses in English. Our approach is most similar to that of Collins et al. (2005). Most SMT systems employ some mechanism that allows reordering of the source language during translation (i.e., non-monotonic decoding). The MOSES phrase-based system that we use has a relatively simple reordering model which has a fixed penalty for reordering moves in the decoder. More sophisticated models include reordering parameters that are sensitive to lexical information (Tillmann, 2004; Kumar and Byrne, 2005; Koehn et al., 2005). The model of Chiang (2005) employs a synchronous context-free grammar to allow hierarchical approaches to reordering. The syntaxbased models of Yamada and Knight (2001) and Marcu et al. (2006) build a full parse tree in the target language, again effectively allowing hierarchical reordering based on synchronous grammars. It is worth noting that none of these approaches to reordering make use of explicit syntactic information in the source language—for example, none of the methods make use of an existing source-language parser (the systems of Yamada"
D07-1077,P05-1067,0,0.039507,"ild a full parse tree in the target language, again effectively allowing hierarchical reordering based on synchronous grammars. It is worth noting that none of these approaches to reordering make use of explicit syntactic information in the source language—for example, none of the methods make use of an existing source-language parser (the systems of Yamada and Knight (2001) and Marcu et al. (2006) make use of a parser in the target language, i.e., English). Finally, note that a number of statistical MT systems make use of source language syntax in transducer-style approaches; see (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006). In contrast to the preprocessing approach, they attempt to incorporate syntax directly into the decoding stage. 3 Chinese Syntactic Reordering Rules We used the Penn Chinese Treebank guidelines (Xue et al., 2005) in searching for a suitable set of reordering rules. We examined all phrase types in the Treebank; potentially phrases of any type could be candidates for reordering rules. Table 1 provides a list of Treebank phrase tags for easy reference. We ruled out several phrase types as not requiring reordering 739 ADJP ADVP CLP CP DN"
D07-1077,C04-1073,0,0.855106,"362 32 Vassar Street, Room G-484 2 Buccleuch Place, 5BP 2L2 Cambridge, MA 02139, USA Cambridge, MA 02139, USA Edinburgh, EH8 9LW, UK wangc@csail.mit.edu mcollins@csail.mit.edu pkoehn@inf.ed.ac.uk Abstract is then applied to the resulting parse tree, with the goal of transforming the source language sentence into a word order that is closer to that of the target language. The reordering process is used to preprocess both the training and test data used within an existing SMT system. Reordering approaches have given significant improvements in performance for translation from French to English (Xia and McCord, 2004) and from German to English (Collins et al., 2005). This paper describes a syntactic reordering approach for translation from Chinese to English. Figure 1 gives an example illustrating some of the differences in word order between the two languages. The example shows a Chinese sentence whose literal translation in English is: Syntactic reordering approaches are an effective method for handling word-order differences between source and target languages in statistical machine translation (SMT) systems. This paper introduces a reordering approach for translation from Chinese to English. We descri"
D07-1077,2006.amta-papers.8,0,0.0614087,"ly allowing hierarchical reordering based on synchronous grammars. It is worth noting that none of these approaches to reordering make use of explicit syntactic information in the source language—for example, none of the methods make use of an existing source-language parser (the systems of Yamada and Knight (2001) and Marcu et al. (2006) make use of a parser in the target language, i.e., English). Finally, note that a number of statistical MT systems make use of source language syntax in transducer-style approaches; see (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006). In contrast to the preprocessing approach, they attempt to incorporate syntax directly into the decoding stage. 3 Chinese Syntactic Reordering Rules We used the Penn Chinese Treebank guidelines (Xue et al., 2005) in searching for a suitable set of reordering rules. We examined all phrase types in the Treebank; potentially phrases of any type could be candidates for reordering rules. Table 1 provides a list of Treebank phrase tags for easy reference. We ruled out several phrase types as not requiring reordering 739 ADJP ADVP CLP CP DNP DP DVP FRAG IP LCP LST NP PP PRN QP UCP VP adjective phra"
D07-1077,P01-1067,0,0.8385,"ent that the French delegation achieved at the Winter Olympics” in English. ÁG $¬þ¤1 this is best accomplishment DEC French delegation achieve at on Winter Olympics This reordering is relatively easy to express using syntactic transformations—for example, it is simple to move the entire relative clause “French delegation at Winter Olympics on achieve DEC” to a position that is after the noun phrase it modifies, namely “best accomplishment.” Phrase-based systems are quite limited in their ability to perform transformations of this type. More recently developed hierarchical systems (e.g., (Yamada and Knight, 2001; Chiang, 2005; Marcu et al., 2006)) may be better equipped to deal with reordering of this type; however, in this example they would effectively have to first identify the span of the relative clause, and then move it into the correct position, without any explicit representation of the source language syntax. In this paper, we describe a set of syntactic reordering rules that exploit systematic differences between Chinese and English word order. The resulting system is used as a preprocessor for both training and test sentences, transforming Chinese sentences to be much closer to English. We"
D07-1077,P07-2045,1,0.131262,"fective method for handling word-order differences between source and target languages in statistical machine translation (SMT) systems. This paper introduces a reordering approach for translation from Chinese to English. We describe a set of syntactic reordering rules that exploit systematic differences between Chinese and English word order. The resulting system is used as a preprocessor for both training and test sentences, transforming Chinese sentences to be much closer to English in terms of their word order. We evaluated the reordering approach within the MOSES phrase-based SMT system (Koehn et al., 2007). The reordering approach improved the BLEU score for the MOSES system from 28.52 to 30.86 on the NIST 2006 evaluation data. We also conducted a series of experiments to analyze the accuracy and impact of different types of reordering rules. this is French delegation at Winter Olympics on achieve DEC best accomplishment and where a natural translation would be this is the best accomplishment that the French delegation achieved at the Winter Olympics 1 Introduction Syntactic reordering approaches are an effective method for handling systematic differences in word order between source and target"
D07-1077,H05-1021,0,0.02585,"describe an approach for German, concentrating on reordering German clauses, which have quite different word order from clauses in English. Our approach is most similar to that of Collins et al. (2005). Most SMT systems employ some mechanism that allows reordering of the source language during translation (i.e., non-monotonic decoding). The MOSES phrase-based system that we use has a relatively simple reordering model which has a fixed penalty for reordering moves in the decoder. More sophisticated models include reordering parameters that are sensitive to lexical information (Tillmann, 2004; Kumar and Byrne, 2005; Koehn et al., 2005). The model of Chiang (2005) employs a synchronous context-free grammar to allow hierarchical approaches to reordering. The syntaxbased models of Yamada and Knight (2001) and Marcu et al. (2006) build a full parse tree in the target language, again effectively allowing hierarchical reordering based on synchronous grammars. It is worth noting that none of these approaches to reordering make use of explicit syntactic information in the source language—for example, none of the methods make use of an existing source-language parser (the systems of Yamada and Knight (2001) and"
D07-1077,2005.iwslt-1.8,1,\N,Missing
D07-1091,P98-1006,0,0.0268535,"ow some success in re-ranking noun phrases for German-English. In their approaches, first, an n-best list with the best translations is generated for each input sentence. Then, the n-best list is enriched with additional features, for instance by syntactically parsing each candidate translation and adding a parse score. The additional features are used to rescore the n-best list, resulting possibly in a better best translation for the sentence. The goal of integrating syntactic information into the translation model has prompted many researchers to pursue tree-based transfer models (Wu, 1997; Alshawi et al., 1998; Yamada and Knight, 2001; Melamed, 2004; Menezes and Quirk, 2005; Galley et al., 2006), with increasingly encouraging results. Our goal is complementary to these efforts: we are less interested in recursive syntactic structure, but in richer annotation at the word level. In future work, these approaches may be combined. 869 word lemma part-of-speech morphology Output word lemma part-of-speech morphology Figure 2: Example factored model: morphological analysis and generation, decomposed into three mapping steps (translation of lemmas, translation of part-of-speech and morphological information"
D07-1091,W07-0702,1,0.606771,"Missing"
D07-1091,W07-0735,0,0.602083,"Missing"
D07-1091,J95-4004,0,0.0939471,"Missing"
D07-1091,P05-1066,1,0.181886,"g and Kirchhoff, 2006; Talbot and Osborne, 2006) by keeping a more complex representation throughout the translation process. Rich morphology often poses a challenge to statistical machine translation, since a multitude of word forms derived from the same lemma fragment the data and lead to sparse data problems. If the input language is morphologically richer than the output language, it helps to stem or segment the input in a pre-processing step, before passing it on to the translation system (Lee, 2004; Sadat and Habash, 2006). Structural problems have also been addressed by pre-processing: Collins et al. (2005) reorder the input to a statistical system to closer match the word order of the output language. On the other end of the translation pipeline, additional information has been used in post-processing. Och et al. (2004) report minor improvements with linguistic features on a Chinese-English task, Koehn and Knight (2003) show some success in re-ranking noun phrases for German-English. In their approaches, first, an n-best list with the best translations is generated for each input sentence. Then, the n-best list is enriched with additional features, for instance by syntactically parsing each can"
D07-1091,P06-1121,0,0.284207,"st, an n-best list with the best translations is generated for each input sentence. Then, the n-best list is enriched with additional features, for instance by syntactically parsing each candidate translation and adding a parse score. The additional features are used to rescore the n-best list, resulting possibly in a better best translation for the sentence. The goal of integrating syntactic information into the translation model has prompted many researchers to pursue tree-based transfer models (Wu, 1997; Alshawi et al., 1998; Yamada and Knight, 2001; Melamed, 2004; Menezes and Quirk, 2005; Galley et al., 2006), with increasingly encouraging results. Our goal is complementary to these efforts: we are less interested in recursive syntactic structure, but in richer annotation at the word level. In future work, these approaches may be combined. 869 word lemma part-of-speech morphology Output word lemma part-of-speech morphology Figure 2: Example factored model: morphological analysis and generation, decomposed into three mapping steps (translation of lemmas, translation of part-of-speech and morphological information, generation of surface forms). 3 Motivating Example: Morphology One example to illustr"
D07-1091,P07-2045,1,0.0474901,"Missing"
D07-1091,W06-3114,1,0.370675,"Missing"
D07-1091,N03-1017,1,0.0598862,"ure 3: Example sentence translation by a standard phrase model. Factored models extend this approach. The three mapping steps in our morphological analysis and generation model may provide the following applicable mappings: 1. Translation: Mapping lemmas • haus → house, home, building, shell 2. Translation: Mapping morphology • NN|plural-nominative-neutral → NN|plural, NN|singular 1. Translate input lemmas into output lemmas 2. Translate morphological and POS factors 3. Generate surface forms given the lemma and linguistic factors Factored translation models build on the phrasebased approach (Koehn et al., 2003) that breaks up the translation of a sentence into the translation of small text chunks (so-called phrases). This approach implicitly defines a segmentation of the input and output sentences into phrases. See an example in Figure 3. Our current implementation of factored translation models follows strictly the phrase-based approach, with the additional decomposition of phrase translation into a sequence of mapping steps. Translation steps map factors in input phrases to factors in output phrases. Generation steps map output factors within individual output words. To reiterate: all translation"
D07-1091,W07-0733,1,0.265962,"Missing"
D07-1091,N04-4015,0,0.0474861,"nt work on models that back off to representations with richer statistics (Nießen and Ney, 2001; Yang and Kirchhoff, 2006; Talbot and Osborne, 2006) by keeping a more complex representation throughout the translation process. Rich morphology often poses a challenge to statistical machine translation, since a multitude of word forms derived from the same lemma fragment the data and lead to sparse data problems. If the input language is morphologically richer than the output language, it helps to stem or segment the input in a pre-processing step, before passing it on to the translation system (Lee, 2004; Sadat and Habash, 2006). Structural problems have also been addressed by pre-processing: Collins et al. (2005) reorder the input to a statistical system to closer match the word order of the output language. On the other end of the translation pipeline, additional information has been used in post-processing. Och et al. (2004) report minor improvements with linguistic features on a Chinese-English task, Koehn and Knight (2003) show some success in re-ranking noun phrases for German-English. In their approaches, first, an n-best list with the best translations is generated for each input sent"
D07-1091,P04-1083,0,0.0041468,"German-English. In their approaches, first, an n-best list with the best translations is generated for each input sentence. Then, the n-best list is enriched with additional features, for instance by syntactically parsing each candidate translation and adding a parse score. The additional features are used to rescore the n-best list, resulting possibly in a better best translation for the sentence. The goal of integrating syntactic information into the translation model has prompted many researchers to pursue tree-based transfer models (Wu, 1997; Alshawi et al., 1998; Yamada and Knight, 2001; Melamed, 2004; Menezes and Quirk, 2005; Galley et al., 2006), with increasingly encouraging results. Our goal is complementary to these efforts: we are less interested in recursive syntactic structure, but in richer annotation at the word level. In future work, these approaches may be combined. 869 word lemma part-of-speech morphology Output word lemma part-of-speech morphology Figure 2: Example factored model: morphological analysis and generation, decomposed into three mapping steps (translation of lemmas, translation of part-of-speech and morphological information, generation of surface forms). 3 Motiva"
D07-1091,2005.iwslt-1.12,0,0.0157201,"In their approaches, first, an n-best list with the best translations is generated for each input sentence. Then, the n-best list is enriched with additional features, for instance by syntactically parsing each candidate translation and adding a parse score. The additional features are used to rescore the n-best list, resulting possibly in a better best translation for the sentence. The goal of integrating syntactic information into the translation model has prompted many researchers to pursue tree-based transfer models (Wu, 1997; Alshawi et al., 1998; Yamada and Knight, 2001; Melamed, 2004; Menezes and Quirk, 2005; Galley et al., 2006), with increasingly encouraging results. Our goal is complementary to these efforts: we are less interested in recursive syntactic structure, but in richer annotation at the word level. In future work, these approaches may be combined. 869 word lemma part-of-speech morphology Output word lemma part-of-speech morphology Figure 2: Example factored model: morphological analysis and generation, decomposed into three mapping steps (translation of lemmas, translation of part-of-speech and morphological information, generation of surface forms). 3 Motivating Example: Morphology"
D07-1091,W01-1407,0,0.0250878,"ming papers. 868 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 868–876, Prague, June 2007. 2007 Association for Computational Linguistics 2 Related Work Input Many attempts have been made to add richer information to statistical machine translation models. Most of these focus on the pre-processing of the input to the statistical system, or the post-processing of its output. Our framework is more general and goes beyond recent work on models that back off to representations with richer statistics (Nießen and Ney, 2001; Yang and Kirchhoff, 2006; Talbot and Osborne, 2006) by keeping a more complex representation throughout the translation process. Rich morphology often poses a challenge to statistical machine translation, since a multitude of word forms derived from the same lemma fragment the data and lead to sparse data problems. If the input language is morphologically richer than the output language, it helps to stem or segment the input in a pre-processing step, before passing it on to the translation system (Lee, 2004; Sadat and Habash, 2006). Structural problems have also been addressed by pre-process"
D07-1091,P03-1021,0,0.15016,"ach feature function hG given a scoring function γ is defined over the output words ek only: X hG (e, f ) = γ(ek ) (4) k The feature functions follow from the scoring functions (τ , γ) acquired during the training of translation and generation tables. For instance, recall our earlier example: a scoring function for a generation model component that is a conditional probability distribution between input and output factors, e.g., γ(fish,NN,singular) = p(NN|fish). The feature weights λi in the log-linear model are determined using a minimum error rate training method, typically Powell’s method (Och, 2003). 5.3 Output Efficient Decoding Compared to phrase-based models, the decomposition of phrase translation into several mapping steps creates additional computational complexity. Instead of a simple table look-up to obtain the possible translations for an input phrase, now multiple tables have to be consulted and their content combined. In phrase-based models it is easy to identify the entries in the phrase table that may be used for a specific input sentence. These are called translation options. We usually limit ourselves to the top 20 translation options for each input phrase. The beam search"
D07-1091,N04-1021,0,0.0827071,"word forms derived from the same lemma fragment the data and lead to sparse data problems. If the input language is morphologically richer than the output language, it helps to stem or segment the input in a pre-processing step, before passing it on to the translation system (Lee, 2004; Sadat and Habash, 2006). Structural problems have also been addressed by pre-processing: Collins et al. (2005) reorder the input to a statistical system to closer match the word order of the output language. On the other end of the translation pipeline, additional information has been used in post-processing. Och et al. (2004) report minor improvements with linguistic features on a Chinese-English task, Koehn and Knight (2003) show some success in re-ranking noun phrases for German-English. In their approaches, first, an n-best list with the best translations is generated for each input sentence. Then, the n-best list is enriched with additional features, for instance by syntactically parsing each candidate translation and adding a parse score. The additional features are used to rescore the n-best list, resulting possibly in a better best translation for the sentence. The goal of integrating syntactic information"
D07-1091,P06-1001,0,0.0441499,"models that back off to representations with richer statistics (Nießen and Ney, 2001; Yang and Kirchhoff, 2006; Talbot and Osborne, 2006) by keeping a more complex representation throughout the translation process. Rich morphology often poses a challenge to statistical machine translation, since a multitude of word forms derived from the same lemma fragment the data and lead to sparse data problems. If the input language is morphologically richer than the output language, it helps to stem or segment the input in a pre-processing step, before passing it on to the translation system (Lee, 2004; Sadat and Habash, 2006). Structural problems have also been addressed by pre-processing: Collins et al. (2005) reorder the input to a statistical system to closer match the word order of the output language. On the other end of the translation pipeline, additional information has been used in post-processing. Och et al. (2004) report minor improvements with linguistic features on a Chinese-English task, Koehn and Knight (2003) show some success in re-ranking noun phrases for German-English. In their approaches, first, an n-best list with the best translations is generated for each input sentence. Then, the n-best li"
D07-1091,C00-2105,0,0.00775863,"Missing"
D07-1091,P06-1122,0,0.0310074,"onference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 868–876, Prague, June 2007. 2007 Association for Computational Linguistics 2 Related Work Input Many attempts have been made to add richer information to statistical machine translation models. Most of these focus on the pre-processing of the input to the statistical system, or the post-processing of its output. Our framework is more general and goes beyond recent work on models that back off to representations with richer statistics (Nießen and Ney, 2001; Yang and Kirchhoff, 2006; Talbot and Osborne, 2006) by keeping a more complex representation throughout the translation process. Rich morphology often poses a challenge to statistical machine translation, since a multitude of word forms derived from the same lemma fragment the data and lead to sparse data problems. If the input language is morphologically richer than the output language, it helps to stem or segment the input in a pre-processing step, before passing it on to the translation system (Lee, 2004; Sadat and Habash, 2006). Structural problems have also been addressed by pre-processing: Collins et al. (2005) reorder the input to a sta"
D07-1091,N06-1001,0,0.0592394,"Missing"
D07-1091,J97-3002,0,0.279949,"(2003) show some success in re-ranking noun phrases for German-English. In their approaches, first, an n-best list with the best translations is generated for each input sentence. Then, the n-best list is enriched with additional features, for instance by syntactically parsing each candidate translation and adding a parse score. The additional features are used to rescore the n-best list, resulting possibly in a better best translation for the sentence. The goal of integrating syntactic information into the translation model has prompted many researchers to pursue tree-based transfer models (Wu, 1997; Alshawi et al., 1998; Yamada and Knight, 2001; Melamed, 2004; Menezes and Quirk, 2005; Galley et al., 2006), with increasingly encouraging results. Our goal is complementary to these efforts: we are less interested in recursive syntactic structure, but in richer annotation at the word level. In future work, these approaches may be combined. 869 word lemma part-of-speech morphology Output word lemma part-of-speech morphology Figure 2: Example factored model: morphological analysis and generation, decomposed into three mapping steps (translation of lemmas, translation of part-of-speech and mor"
D07-1091,P01-1067,0,0.131337,"ranking noun phrases for German-English. In their approaches, first, an n-best list with the best translations is generated for each input sentence. Then, the n-best list is enriched with additional features, for instance by syntactically parsing each candidate translation and adding a parse score. The additional features are used to rescore the n-best list, resulting possibly in a better best translation for the sentence. The goal of integrating syntactic information into the translation model has prompted many researchers to pursue tree-based transfer models (Wu, 1997; Alshawi et al., 1998; Yamada and Knight, 2001; Melamed, 2004; Menezes and Quirk, 2005; Galley et al., 2006), with increasingly encouraging results. Our goal is complementary to these efforts: we are less interested in recursive syntactic structure, but in richer annotation at the word level. In future work, these approaches may be combined. 869 word lemma part-of-speech morphology Output word lemma part-of-speech morphology Figure 2: Example factored model: morphological analysis and generation, decomposed into three mapping steps (translation of lemmas, translation of part-of-speech and morphological information, generation of surface f"
D07-1091,E06-1006,0,0.0565291,"edings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 868–876, Prague, June 2007. 2007 Association for Computational Linguistics 2 Related Work Input Many attempts have been made to add richer information to statistical machine translation models. Most of these focus on the pre-processing of the input to the statistical system, or the post-processing of its output. Our framework is more general and goes beyond recent work on models that back off to representations with richer statistics (Nießen and Ney, 2001; Yang and Kirchhoff, 2006; Talbot and Osborne, 2006) by keeping a more complex representation throughout the translation process. Rich morphology often poses a challenge to statistical machine translation, since a multitude of word forms derived from the same lemma fragment the data and lead to sparse data problems. If the input language is morphologically richer than the output language, it helps to stem or segment the input in a pre-processing step, before passing it on to the translation system (Lee, 2004; Sadat and Habash, 2006). Structural problems have also been addressed by pre-processing: Collins et al. (2005)"
D07-1091,P03-1040,1,\N,Missing
D07-1091,C98-1006,0,\N,Missing
D07-1091,P03-1020,0,\N,Missing
D07-1091,D08-1076,0,\N,Missing
D07-1091,2006.iwslt-evaluation.8,0,\N,Missing
D08-1078,J93-2003,0,0.00596267,"ge pairs, translation quality is still low. Certain systematic differences between languages can be used to predict this. Many researchers have speculated on the reasons why machine translation is hard. However, there has never been, to our knowledge, an analysis of what the actual contribution of different aspects of language pairs is to translation performance. This understanding of where the difficulties lie will allow researchers to know where to most gainfully direct their efforts to improving the current models of machine translation. Many of the challenges of SMT were first outlined by Brown et al. (1993). The original IBM Models were broken down into separate translation and distortion models, recognizing the importance of word order differences in modeling translation. Brown et al. also highlighted the importance of modeling morphology, both for reducing sparse counts and improving parameter estimation and for the correct production of translated forms. We see these two factors, reordering and morphology, as fundamental to the quality of machine translation output, and we would like to quantify their impact on system performance. It is not sufficient, however, to analyze the morphological co"
D08-1078,W08-0309,1,0.276713,"ance levels of coefficients and R2 are also 751 = 0.16 pt da es = 0.24 fr nl fi = 0.32 de = 0.4 Target Languages Figure 7. System performance - the width of the squares indicates the system performance in terms of the B LEU score. Explanatory Variable Target Vocab. Size Language Similarity Reordering Amount Target Vocab. Size2 Language Similarity2 Interaction: Reord/Sim We used the phrase-based model Moses (Koehn et al., 2007) for the experiments with all the standard settings, including a lexicalized reordering model, and a 5-gram language model. Tests were run on the ACL WSMT 2008 test set (Callison-Burch et al., 2008). 6.1 sv en el Coefficient -3.885 3.274 -1.883 1.017 -1.858 -1.4536 *** *** *** *** ** *** Table 3. The impact of the various explanatory features on the B LEU score via their coefficients in the minimal adequate model. given where * means p &lt; 0.05, ** means p &lt; 0.01, and *** means p &lt; 0.001. 7 7.1 Results Combined Model The first question we are interested in answering is which factors contribute most and how they interact. We fit a multiple regression model to the data. The source vocabulary size has no significant effect on the outcome. All explanatory variable vectors were normalized to be"
D08-1078,W07-0729,0,0.0116329,"rms of the same lemma as completely independent of one another. This can result in sparse statistics and poorly estimated models. Furthermore, different variations of the lemma may result in crucial differences in meaning that affect the quality of the translation. Work on improving MT systems’ treatment of morphology has focussed on either reducing word forms to lemmas to reduce sparsity (Goldwater and McClosky, 2005; Talbot and Osborne, 2006) or including morphological information in decod746 en fr it es pt el nl sv da de fi Language Figure 1. Average vocabulary size for each language. ing (Dyer, 2007). Although there is a significant amount of research into improving the treatment of morphology, in this paper we aim to discover the effect that different levels of morphology have on translation. We measure the amount of morphological complexity that exists in both languages and then relate this to translation performance. Some languages seem to be intuitively more complex than others, for instance Finnish appears more complex than English. There is, however, no obvious way of measuring this complexity. One method of measuring complexity is by choosing a number of hand-picked, intuitive prop"
D08-1078,H05-1085,0,0.0139242,"lexity of the language pairs involved in translation is widely recognized as one of the factors influencing translation performance. However, most statistical translation systems treat different inflected forms of the same lemma as completely independent of one another. This can result in sparse statistics and poorly estimated models. Furthermore, different variations of the lemma may result in crucial differences in meaning that affect the quality of the translation. Work on improving MT systems’ treatment of morphology has focussed on either reducing word forms to lemmas to reduce sparsity (Goldwater and McClosky, 2005; Talbot and Osborne, 2006) or including morphological information in decod746 en fr it es pt el nl sv da de fi Language Figure 1. Average vocabulary size for each language. ing (Dyer, 2007). Although there is a significant amount of research into improving the treatment of morphology, in this paper we aim to discover the effect that different levels of morphology have on translation. We measure the amount of morphological complexity that exists in both languages and then relate this to translation performance. Some languages seem to be intuitively more complex than others, for instance Finnis"
D08-1078,N03-1017,1,0.0249735,"nce. 749 Experimental Design We select the German-English language pair because it has a reasonably high level of reordering. A manually aligned German-English corpus was provided by Chris Callison-Burch and consists of the first 220 sentences of test data from the 2006 ACL Workshop on Machine Translation (WMT06) test set. This test set is from a held out portion of the Europarl corpus. The automatic alignments were extracted by appending the manually aligned sentences on to the respective Europarl v3 corpora and aligning them using GIZA++ (Och and Ney, 2003) and the growfinal-diag algorithm (Koehn et al., 2003). 5.3.2 P Automatic Alignments Results In order to use automatic alignments to extract reordering statistics, we need to show that reorderings from automatic alignments are comparable to those from manual alignments. We first look at global reordering statistics and then we look in more detail at the reordering distribution of the corpora. Table 2 shows the amount of reordering in the WMT06 test corpora, with both manual and automatic alignments, and in the automatically aligned Europarl DE-EN parallel corpus. fi 0.8 nl Source Languages 0.4 0.6 ACL Test Manual ACL Test Automatic Euromatrix 0.2"
D08-1078,P07-2045,1,0.0191775,"etermine whether the coefficients for the independent variables are reliably different from zero. We also test how well the model explains the data using an R2 test. The two-tailed significance levels of coefficients and R2 are also 751 = 0.16 pt da es = 0.24 fr nl fi = 0.32 de = 0.4 Target Languages Figure 7. System performance - the width of the squares indicates the system performance in terms of the B LEU score. Explanatory Variable Target Vocab. Size Language Similarity Reordering Amount Target Vocab. Size2 Language Similarity2 Interaction: Reord/Sim We used the phrase-based model Moses (Koehn et al., 2007) for the experiments with all the standard settings, including a lexicalized reordering model, and a 5-gram language model. Tests were run on the ACL WSMT 2008 test set (Callison-Burch et al., 2008). 6.1 sv en el Coefficient -3.885 3.274 -1.883 1.017 -1.858 -1.4536 *** *** *** *** ** *** Table 3. The impact of the various explanatory features on the B LEU score via their coefficients in the minimal adequate model. given where * means p &lt; 0.05, ** means p &lt; 0.01, and *** means p &lt; 0.001. 7 7.1 Results Combined Model The first question we are interested in answering is which factors contribute m"
D08-1078,2005.mtsummit-papers.11,1,0.302052,"ance. It is not sufficient, however, to analyze the morphological complexity of the source and target languages. It is also very important to know how similar the morphology is between the two languages, as two languages which are morphologically complex in very similar ways, could be relatively easy to translate. Therefore, we also include a measure of the family relatedness of languages in our analysis. The impact of these factors on translation is measured by using linear regression models. We perform the analysis with data from 110 different language pairs drawn from the Europarl project (Koehn, 2005). This contains parallel data for the 11 official language pairs of the European Union, providing a rich variety of different language characteristics for our experiments. Many research papers report results on only one or two languages pairs. By analyzing so many language pairs, we are able to provide a much wider perspective on the challenges facing machine translation. This analysis is important as it provides very strong motivation for further research. The findings of this paper are as follows: (1) each of the main effects, reordering, target language complexity and language relatedness,"
D08-1078,J03-1002,0,0.00182469,"ce side, normalized by the length of the source sentence. 749 Experimental Design We select the German-English language pair because it has a reasonably high level of reordering. A manually aligned German-English corpus was provided by Chris Callison-Burch and consists of the first 220 sentences of test data from the 2006 ACL Workshop on Machine Translation (WMT06) test set. This test set is from a held out portion of the Europarl corpus. The automatic alignments were extracted by appending the manually aligned sentences on to the respective Europarl v3 corpora and aligning them using GIZA++ (Och and Ney, 2003) and the growfinal-diag algorithm (Koehn et al., 2003). 5.3.2 P Automatic Alignments Results In order to use automatic alignments to extract reordering statistics, we need to show that reorderings from automatic alignments are comparable to those from manual alignments. We first look at global reordering statistics and then we look in more detail at the reordering distribution of the corpora. Table 2 shows the amount of reordering in the WMT06 test corpora, with both manual and automatic alignments, and in the automatically aligned Europarl DE-EN parallel corpus. fi 0.8 nl Source Languages 0.4"
D08-1078,P02-1040,0,0.10138,"Missing"
D08-1078,2006.amta-papers.25,0,0.0889646,"Missing"
D08-1078,P06-1122,1,0.768304,"nvolved in translation is widely recognized as one of the factors influencing translation performance. However, most statistical translation systems treat different inflected forms of the same lemma as completely independent of one another. This can result in sparse statistics and poorly estimated models. Furthermore, different variations of the lemma may result in crucial differences in meaning that affect the quality of the translation. Work on improving MT systems’ treatment of morphology has focussed on either reducing word forms to lemmas to reduce sparsity (Goldwater and McClosky, 2005; Talbot and Osborne, 2006) or including morphological information in decod746 en fr it es pt el nl sv da de fi Language Figure 1. Average vocabulary size for each language. ing (Dyer, 2007). Although there is a significant amount of research into improving the treatment of morphology, in this paper we aim to discover the effect that different levels of morphology have on translation. We measure the amount of morphological complexity that exists in both languages and then relate this to translation performance. Some languages seem to be intuitively more complex than others, for instance Finnish appears more complex than"
D08-1078,J97-3002,0,0.271232,"largely driven by syntactic differences between languages and can involve complex rearrangements between nodes in synchronous trees. Modeling reordering exactly would require a synchronous tree-substitution grammar. This representation would be sparse and heterogeneous, limiting its usefulness as a basis for analysis. We make an important simplifying assumption in order for the detection and extraction of reordering data to be tractable and useful. We assume that reordering is a binary process occurring between two blocks that are adjacent in the source. This is similar to the ITG constraint (Wu, 1997), however our reorderings are not dependent on a synchronous grammar or a derivation which covers the sentences. There are also similarities with the Human-Targeted Transla748 A consistent block means that between Atmin and Atmax there are no target word positions aligned to source words outside of the block’s source span As . A reordering is consistent if the block projected from rAsmin to rBsmax is consistent. The following algorithm detects reorderings and determines the dimensions of the blocks involved. We step through all the source words, and if a word is reordered in the target with re"
D11-1079,2009.mtsummit-papers.1,0,0.616093,"s are quite similar and not shown here. Translation outputs are in lower case. tures the boxed area as a whole and uses Rule 10 to perform the right global reordering. X → (dui1 X2 gandao3 manyi4 .5 , expressed3 satisfaction4 with1 X2 .5 ) 5 (10) Related Work In recent years, there has been a growing body of research on using dependency for statistical machine translation. Some directly encodes dependency in the translation model (Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Mi and Liu, 2010), while others use dependency as a soft constraint (Cherry, 2008; Bach et al., 2009a,b; Chang et al., 2009). Among them, Shen et al. (2008) report that just filtering the phrase table by the socalled well-formed target dependency structure does not help, yet adding a target dependency language model improves performance significantly. Our intuitive interpretation is that the target dependency language model capitalizes on two characteristics of the dependency structure: it is based on words and it directly connects head and child. Therefore, the target dependency language model makes good use of the dependency representation as well as the target side training data. We follo"
D11-1079,N09-2001,0,0.254564,"s are quite similar and not shown here. Translation outputs are in lower case. tures the boxed area as a whole and uses Rule 10 to perform the right global reordering. X → (dui1 X2 gandao3 manyi4 .5 , expressed3 satisfaction4 with1 X2 .5 ) 5 (10) Related Work In recent years, there has been a growing body of research on using dependency for statistical machine translation. Some directly encodes dependency in the translation model (Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Mi and Liu, 2010), while others use dependency as a soft constraint (Cherry, 2008; Bach et al., 2009a,b; Chang et al., 2009). Among them, Shen et al. (2008) report that just filtering the phrase table by the socalled well-formed target dependency structure does not help, yet adding a target dependency language model improves performance significantly. Our intuitive interpretation is that the target dependency language model capitalizes on two characteristics of the dependency structure: it is based on words and it directly connects head and child. Therefore, the target dependency language model makes good use of the dependency representation as well as the target side training data. We follo"
D11-1079,W09-0434,1,0.743819,"distance reodering. 1 Introduction Reordering, especially movement over longer distances, continues to be a hard problem in statistical machine translation. It motivates much of the recent work on tree-based translation models, such as the hierarchical phrase-based model (Chiang, 2007) which extends the phrase-based model (Koehn et al., 2003) by allowing the so-called hierarchical phrases containing subphrases. The hierarchical phrase-based model captures the recursiveness of language without relying on syntactic annotation, and promises better reordering than the phrase-based model. However, Birch et al. (2009) find that although the hierarchical phrasebased model outperforms the phrase-based model in 857 terms of medium-range reordering, it does equally poorly in long-distance reordering due to constraints to guarantee efficiency. Syntax-based models that use phrase structure constituent labels as non-terminals in their transfer rules, exemplified by that of Galley et al. (2004), produce smarter and syntactically motivated reordering. However, when working with off-the-shelf tools for parsing and alignment, this approach may impose harsh limits on rule extraction and requires serious efforts of opt"
D11-1079,W10-1749,1,0.625285,"nts, with the “bin-2” setting problem of overfitting sets in when we use 3 bins (with slightly higher tuning BLEU, not shown here). We also studied the effect of adding features incrementally onto the baseline with the “bin-2” setting, as shown in Table 3. On average, all three features seem to have similar contributions. 3.3 Using LRscore as the Tuning Metric Since our features are proposed to address the reordering problem and BLEU is not sensitive enough to reordering (especially in long-distance cases), we have also tried tuning with a metric that highlights reordering, i.e., the LRscore (Birch and Osborne, 2010). LRscore is a linear interpolation of a lexical metric and a reordering metric. We interpolated BLEU (as the lexical metric) with the Kendall’s tau permutation distance (as the reordering metric). The Kendall’s tau permutation distance measures the relative word order difference between the transla862 tion output and the reference(s) and is particularly sensitive to long-distance reordering. Testing results in terms of BLEU, LRscore and TER (Snover et al., 2006) are shown in Table 4. Tuned with the LRscore, our feature-augmented model achieves further average improvements (compare “bin-2” and"
D11-1079,P11-1103,1,0.527866,"gs in the translation, and recall as the number of reproduced reorderings divided by the number of reorder3 One of our reviewers points out that according to the inductive learning theory, it is counter-intuitive to improve on BLEU and TER if we optimize by the LRscore. Yet we do observe some other papers reporting increased TER or other metric scores when BLEU is used for tuning (Carpuat and Wu, 2007; Shen et al., 2008), suggesting that MT evaluation might be too complicated to be characterized just with inductive learning. Similar results based on extensive experiments can also be found in (Birch and Osborne, 2011). Setting baseline bin-2 baseline-lr bin-2-lr MT02 37.5 36.8 37.0 37.7 MT05 36.2 35.9 35.6 36.7 MT08 33.2 31.8 32.2 33.2 0.5 0.4 Average 35.6 34.8 (-0.8) 34.9 (-0.7) 35.9 (+0.3) 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Reordering Widths 0.6 Table 6: Overall recall for the test sets. 0.1 0.2 Recall 0.4 0.5 baseline bin−2 bin−2−lr 0.0 ings in the reference. Then we average the precision and recall over all four reference translations. Details of measuring reproduced reordering can be found in Birch et al. (2008). An important difference in this work is in handling many-to-one and one-to-many alignments"
D11-1079,D08-1078,1,0.886194,"ductive learning. Similar results based on extensive experiments can also be found in (Birch and Osborne, 2011). Setting baseline bin-2 baseline-lr bin-2-lr MT02 37.5 36.8 37.0 37.7 MT05 36.2 35.9 35.6 36.7 MT08 33.2 31.8 32.2 33.2 0.5 0.4 Average 35.6 34.8 (-0.8) 34.9 (-0.7) 35.9 (+0.3) 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Reordering Widths 0.6 Table 6: Overall recall for the test sets. 0.1 0.2 Recall 0.4 0.5 baseline bin−2 bin−2−lr 0.0 ings in the reference. Then we average the precision and recall over all four reference translations. Details of measuring reproduced reordering can be found in Birch et al. (2008). An important difference in this work is in handling many-to-one and one-to-many alignments, as we only retain the first word alignment for any source or target word which has multiple alignments. This is consistent with our treatment in dependency orientation classification, and results in more reorderings being extracted. From Table 5 we can see that our features improve precision by an average of 4.7 absolute points when BLEU is used for tuning (“bin-2”). Switching from BLEU to the LRscore (“bin-2-lr”), we gain 2.2 points more and have a total improvement of 6.9 absolute points on average."
D11-1079,D07-1007,0,0.0204947,"ecall for reordering is to investigate whether reorderings in the references are reproduced in the translations. We calculate precision as the number of reproduced reorderings divided by the total number of reorderings in the translation, and recall as the number of reproduced reorderings divided by the number of reorder3 One of our reviewers points out that according to the inductive learning theory, it is counter-intuitive to improve on BLEU and TER if we optimize by the LRscore. Yet we do observe some other papers reporting increased TER or other metric scores when BLEU is used for tuning (Carpuat and Wu, 2007; Shen et al., 2008), suggesting that MT evaluation might be too complicated to be characterized just with inductive learning. Similar results based on extensive experiments can also be found in (Birch and Osborne, 2011). Setting baseline bin-2 baseline-lr bin-2-lr MT02 37.5 36.8 37.0 37.7 MT05 36.2 35.9 35.6 36.7 MT08 33.2 31.8 32.2 33.2 0.5 0.4 Average 35.6 34.8 (-0.8) 34.9 (-0.7) 35.9 (+0.3) 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Reordering Widths 0.6 Table 6: Overall recall for the test sets. 0.1 0.2 Recall 0.4 0.5 baseline bin−2 bin−2−lr 0.0 ings in the reference. Then we average the precision"
D11-1079,W09-2307,0,0.0661964,"d not shown here. Translation outputs are in lower case. tures the boxed area as a whole and uses Rule 10 to perform the right global reordering. X → (dui1 X2 gandao3 manyi4 .5 , expressed3 satisfaction4 with1 X2 .5 ) 5 (10) Related Work In recent years, there has been a growing body of research on using dependency for statistical machine translation. Some directly encodes dependency in the translation model (Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Mi and Liu, 2010), while others use dependency as a soft constraint (Cherry, 2008; Bach et al., 2009a,b; Chang et al., 2009). Among them, Shen et al. (2008) report that just filtering the phrase table by the socalled well-formed target dependency structure does not help, yet adding a target dependency language model improves performance significantly. Our intuitive interpretation is that the target dependency language model capitalizes on two characteristics of the dependency structure: it is based on words and it directly connects head and child. Therefore, the target dependency language model makes good use of the dependency representation as well as the target side training data. We follow the second line of res"
D11-1079,P08-1009,0,0.228185,"version outputs are quite similar and not shown here. Translation outputs are in lower case. tures the boxed area as a whole and uses Rule 10 to perform the right global reordering. X → (dui1 X2 gandao3 manyi4 .5 , expressed3 satisfaction4 with1 X2 .5 ) 5 (10) Related Work In recent years, there has been a growing body of research on using dependency for statistical machine translation. Some directly encodes dependency in the translation model (Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Mi and Liu, 2010), while others use dependency as a soft constraint (Cherry, 2008; Bach et al., 2009a,b; Chang et al., 2009). Among them, Shen et al. (2008) report that just filtering the phrase table by the socalled well-formed target dependency structure does not help, yet adding a target dependency language model improves performance significantly. Our intuitive interpretation is that the target dependency language model capitalizes on two characteristics of the dependency structure: it is based on words and it directly connects head and child. Therefore, the target dependency language model makes good use of the dependency representation as well as the target side trai"
D11-1079,P05-1033,0,0.824256,"will also be favored by our cohesion penalty feature. However, ignorant of the syntactic structure, the 860 glue rule penalty may penalize a reasonably cohesive derivation such as Derivation 5 and at the same time promote a less cohesive hierarchical translation, such as Derivation 6. Compared with constituency constraints based on the phrase structure, our cohesion penalty derived from the binary dependency parsing has two different characteristics. First, our cohesion penalty is by nature more tolerant to some meaningful noncontituent translations. For example, constituency constraints in (Chiang, 2005; Marton and Resnik, 2008; Chiang et al., 2009) would penalize Rule 7 below which is useful for German–English translation (Koehn et al., 2003), and Rule 8 which can be applied to the Figure 1 sentence. Fuzzy constituency constraints can solve this problem with a combination of product categories and slash categories (Chiang, 2010). Yet our cohesion penalty by nature admits these translations as cohesive (with no extra cost from es and Aozhou since both are locally resolved). Admittedly, our current implementation of the cohesion penalty is blind to some other meaningful nonconstituent colloca"
D11-1079,J07-2003,0,0.897572,"witch the tuning function from BLEU to the LRscore which promotes reordering, we observe total improvements of 1.21 BLEU, 1.30 LRscore and 3.36 TER over the baseline. On average our approach improves reordering precision and recall by 6.9 and 0.3 absolute points, respectively, and is found to be especially effective for long-distance reodering. 1 Introduction Reordering, especially movement over longer distances, continues to be a hard problem in statistical machine translation. It motivates much of the recent work on tree-based translation models, such as the hierarchical phrase-based model (Chiang, 2007) which extends the phrase-based model (Koehn et al., 2003) by allowing the so-called hierarchical phrases containing subphrases. The hierarchical phrase-based model captures the recursiveness of language without relying on syntactic annotation, and promises better reordering than the phrase-based model. However, Birch et al. (2009) find that although the hierarchical phrasebased model outperforms the phrase-based model in 857 terms of medium-range reordering, it does equally poorly in long-distance reordering due to constraints to guarantee efficiency. Syntax-based models that use phrase struc"
D11-1079,P10-1146,0,0.0779101,"on the phrase structure, our cohesion penalty derived from the binary dependency parsing has two different characteristics. First, our cohesion penalty is by nature more tolerant to some meaningful noncontituent translations. For example, constituency constraints in (Chiang, 2005; Marton and Resnik, 2008; Chiang et al., 2009) would penalize Rule 7 below which is useful for German–English translation (Koehn et al., 2003), and Rule 8 which can be applied to the Figure 1 sentence. Fuzzy constituency constraints can solve this problem with a combination of product categories and slash categories (Chiang, 2010). Yet our cohesion penalty by nature admits these translations as cohesive (with no extra cost from es and Aozhou since both are locally resolved). Admittedly, our current implementation of the cohesion penalty is blind to some other meaningful nonconstituent collocations, such as neighbouring siblings of a common uncovered head (regulated as the “floating structure” in (Shen et al., 2008)). A concrete example is Rule 9 which is useful for the Figure 1 sentence. To address this problem, another feature can be defined in the same manner to capture how each head word is translated with its child"
D11-1079,N09-1025,0,0.368393,"enalty feature. However, ignorant of the syntactic structure, the 860 glue rule penalty may penalize a reasonably cohesive derivation such as Derivation 5 and at the same time promote a less cohesive hierarchical translation, such as Derivation 6. Compared with constituency constraints based on the phrase structure, our cohesion penalty derived from the binary dependency parsing has two different characteristics. First, our cohesion penalty is by nature more tolerant to some meaningful noncontituent translations. For example, constituency constraints in (Chiang, 2005; Marton and Resnik, 2008; Chiang et al., 2009) would penalize Rule 7 below which is useful for German–English translation (Koehn et al., 2003), and Rule 8 which can be applied to the Figure 1 sentence. Fuzzy constituency constraints can solve this problem with a combination of product categories and slash categories (Chiang, 2010). Yet our cohesion penalty by nature admits these translations as cohesive (with no extra cost from es and Aozhou since both are locally resolved). Admittedly, our current implementation of the cohesion penalty is blind to some other meaningful nonconstituent collocations, such as neighbouring siblings of a commo"
D11-1079,D08-1024,0,0.0396653,"countries2 ) (9) Second, our cohesion penalty can be by nature more discriminative. Compared with the constituency constraints, the cohesion penalty is integer-valued, and can be made sensitive to the depth of each word in the dependency hierarchy (see Section 2.4). Inspired by (Marton and Resnik, 2008; Chiang et al., 2009), the cohesion penalty could also be made sensitive to the dependency relation of each word. However, this drastically increases the number of features and requires a tuning algorithm which scales better to high-dimensional model spaces, such as MIRA (Watanabe et al., 2007; Chiang et al., 2008). pobj 是 Depth 1 top Depth 2 澳洲 . 之一 有 prep 与 Depth 4 pobj Depth 5 nummod 少数 国家 cpm dobj Bin 1 3 Bin 2 3.1 nn rcmod Depth 3 the tree levels spread out. punct attr 邦交 的 北韩 Figure 3: Using 2 bins for the dependency parse tree of the Figure 1 sentence. 2.3 Unaligned Penalty The dependency orientation and cohesion penalty cannot be applied to unaligned source words. This may lead to search error, such as dropping (i.e., unaligning) key content words that are important for lexical translation and reordering. The problem is mitigated by an unaligned penalty applicable to all words in the dependency"
D11-1079,P05-1066,1,0.750058,"slations as well as some nonconstituent translations, if not all of them (as discussed in Section 2.2). Our dependency orientation feature is similar to the order model within dependency treelet translation (Quirk et al., 2005). Yet instead of a head-relative position number for each modifier word, we simply predict the head-dependent orientation which is either monotone or reversed. Our coarser-grained approach is more robust from a machine learning perspective, yet still captures prominent and long-distance reordering patterns observed in Chinese–English (Wang et al., 2007), German–English (Collins et al., 2005), Japanese– English (Katz-Brown and Collins, 2008) and translation from English to a group of SOV languages (Xu et al., 2009). Not committed to specific language pairs, we learn orientation classification from the word-aligned parallel data through maximum entropy training as Zens and Ney (2006) and Chang et al. (2009) for phrase-based translation and Xiong et al. (2006) for the BTG model (Wu, 1996). While Chang et al. (2009) also make use of source dependency, their orientation classification concerns two subsequent phrase pairs in the leftto-right phrase-based decoding (as apposed to each de"
D11-1079,de-marneffe-etal-2006-generating,0,0.00377784,"Missing"
D11-1079,P05-1067,0,0.0590493,"[and the european union ( eu )]]]] .] Figure 5: Example translations from the NIST MT08 set, output by the baseline model and “bin-2” model. The “-lr” version outputs are quite similar and not shown here. Translation outputs are in lower case. tures the boxed area as a whole and uses Rule 10 to perform the right global reordering. X → (dui1 X2 gandao3 manyi4 .5 , expressed3 satisfaction4 with1 X2 .5 ) 5 (10) Related Work In recent years, there has been a growing body of research on using dependency for statistical machine translation. Some directly encodes dependency in the translation model (Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Mi and Liu, 2010), while others use dependency as a soft constraint (Cherry, 2008; Bach et al., 2009a,b; Chang et al., 2009). Among them, Shen et al. (2008) report that just filtering the phrase table by the socalled well-formed target dependency structure does not help, yet adding a target dependency language model improves performance significantly. Our intuitive interpretation is that the target dependency language model capitalizes on two characteristics of the dependency structure: it is based on words and it directly connects h"
D11-1079,W02-1039,0,0.169076,"rch, and derive three word-based soft constraints from the source dependency parsing. Note that although we reuse the word “cohesion” to name one of the constraints, our work is different from (Cherry, 2008; Bach et al., 2009a,b) which have successfully defined another cohesion constraint from the source depen864 dency structure, with the aim of improving reordering in phrase-based MT. To take a glance, Cherry (2008) and Bach et al. (2009b) define cohesion as translating a source dependency subtree contiguously into the target side without interruption (span or subtree overlapping), following Fox (2002). This span-based cohesion constraint has a different criterion from our wordbased cohesion penalty and often leads to opposite conclusions. Bach et al. (2009a) also use cohesion to correlate with the lexicalized reordering model (Tillman, 2004; Koehn et al., 2005), whereas we define an orthogonal dependency orientation feature to explicitly model head-dependent reordering. The fundamental difference, however, is rooted in the translation model. Their span-based cohesion constraint is implemented as an “interruption check” to encourage finishing a subtree before translating something else. Thi"
D11-1079,N04-1035,0,0.105467,"archical phrases containing subphrases. The hierarchical phrase-based model captures the recursiveness of language without relying on syntactic annotation, and promises better reordering than the phrase-based model. However, Birch et al. (2009) find that although the hierarchical phrasebased model outperforms the phrase-based model in 857 terms of medium-range reordering, it does equally poorly in long-distance reordering due to constraints to guarantee efficiency. Syntax-based models that use phrase structure constituent labels as non-terminals in their transfer rules, exemplified by that of Galley et al. (2004), produce smarter and syntactically motivated reordering. However, when working with off-the-shelf tools for parsing and alignment, this approach may impose harsh limits on rule extraction and requires serious efforts of optimization (Wang et al., 2010). An alternative approach is to augment the general hierarchical phrase-based model with soft syntactic constraints. Here, we derive three word-based, complementary constraints from the source dependency parsing, including: • A dependency orientation feature, trained with maximum entropy on the word-aligned parallel data, which directly models t"
D11-1079,C10-1050,0,0.0351318,"you (English have), showing that yu is a prepositional modifier of you. We use the Stanford Parser1 to generate dependency parsing, which automatically extracts dependency relations from phrase structure parsing (de Marneffe et al., 2006). 2.1 Dependency Orientation Based on the assumption that constituents generally move as a whole (Quirk et al., 2005), we decompose the sentence reordering probability into the reordering probability for each aligned source word with respect to its head, excluding the root word at the top of the dependency hierarchy which does not have a head word. Similarly, Hayashi et al. (2010) also take a word-based reordering approach for HPBMT, but they model all possible pairwise orientation from the source side as a general linear ordering problem (Tromble and Eisner, 2009). To be more specific, we have a maximum entropy orientation classifier that predicts the probability of a source word being translated in a monotone or reversed manner with respect to its head. For example, 1 (b) S1 http://nlp.stanford.edu/software/lex-parser.shtml 858 S2 S3 S1 T1 ihead T1 idep T2 idep T2 ihead T3 T3 T4 T4 S2 S3 Figure 2: Word alignments to illustrate orientation classification. In (a), mono"
D11-1079,2009.iwslt-papers.4,1,0.828278,"re2 ) for dependency orientation classification. We trained three 5-gram language models with modified Kneser-Ney smoothing (Kneser and Ney, 1995): one on the English half of the parallel corpus, one on the Xinhua part of the Gigaword corpus, one on the AFP part, and interpolated them for best fit to the tuning set (Schwenk and Koehn, 2008). We used NIST MT06 evaluation data (1664 lines) as our tuning set, and tested on NIST MT02 (878 lines), MT05 (1082 lines) and MT08 (1357 lines). Our baseline system was the Moses implementation of the hierarchical phrase-based model with standard settings (Hoang et al., 2009). When only 1 bin was used, 3 additional features were added to the baseline, one each from the soft dependency constraints. When we used 2 or 3 bins, the additional feature counts doubled or tripled. We preserved terminal alignment alongside nonterminal alignment during the rule extraction and output word alignments together with translated strings. Since the features we currently define are based entirely on the source side, we used preprocessing to speed up decoding of our feature-augmented model. All experiments were tuned with MERT (Och, 2003). 3.2 Using BLEU as the Tuning Metric As a sta"
D11-1079,D10-1014,0,0.0673881,"e span matches the source constituent as defined by phrase structure parsing. Finer-grained constituency constraints significantly improve hierarchical phrase-based MT when applied on the source side (Marton and Resnik, 2008; Chiang et al., 2009), or on the target side in a more tolerant fashion (Zollmann and Venugopal, 2006). Using both source and target syntax, but relaxing on rule extraction and substitution enables HPBMT to produce more well-formed and syntactically richer derivations (Chiang, 2010). Softening constituency matching with latent syntactic distributions proves to be helpful (Huang et al., 2010). Compared to constituency-based approaches, our cohesion penalty based on the dependency structure naturally supports constituent translations as well as some nonconstituent translations, if not all of them (as discussed in Section 2.2). Our dependency orientation feature is similar to the order model within dependency treelet translation (Quirk et al., 2005). Yet instead of a head-relative position number for each modifier word, we simply predict the head-dependent orientation which is either monotone or reversed. Our coarser-grained approach is more robust from a machine learning perspectiv"
D11-1079,W04-3250,1,0.716511,".82 BLEU / LRscore / TER MT05 MT08 32.23 / 40.50 / 68.15 28.09 / 37.17 / 66.82 33.18 / 41.62 / 65.59 28.63 / 38.12 / 65.36 32.28 / 40.61 / 67.61 27.99 / 37.27 / 66.98 33.44 / 41.80 / 64.88 29.10 / 38.38 / 64.14 Average 31.44 / 39.84 / 67.97 32.28 / 40.94 / 65.51 31.50 / 39.98 / 67.56 32.65 / 41.14 / 64.61 Table 4: Results for the baseline model and the complete feature-augmented model with 2 bins (“bin-2”), using BLEU and LRscore (“-lr”) as the tuning function. The BLEU scores of “bin-2” and “bin-2-lr” are significantly better than baseline (p &lt; 0.05), computed by paired bootstrap resampling (Koehn, 2004). Setting baseline bin-1 bin-2 bin-3 MT02 34.01 34.20 35.04 34.35 MT05 32.23 32.13 33.18 32.79 BLEU MT08 28.09 28.41 28.63 28.37 Average 31.44 31.58(+.14) 32.28(+.84) 31.84(+.40) Table 2: Results of the baseline model as well as our complete feature-augmented model with 1, 2 and 3 bins. BLEU is the tuning function. Setting MT02 baseline 34.01 dep 34.26 dep+coP 34.47 dep+coP+unP 35.04 MT05 32.23 32.58 32.81 33.18 BLEU MT08 28.09 28.07 28.61 28.63 Average 31.44 31.64(+.20) 31.96(+.52) 32.28(+.84) Table 3: Contributions of the three soft dependency constraints, with the “bin-2” setting problem of"
D11-1079,2005.iwslt-1.8,1,0.161694,"fined another cohesion constraint from the source depen864 dency structure, with the aim of improving reordering in phrase-based MT. To take a glance, Cherry (2008) and Bach et al. (2009b) define cohesion as translating a source dependency subtree contiguously into the target side without interruption (span or subtree overlapping), following Fox (2002). This span-based cohesion constraint has a different criterion from our wordbased cohesion penalty and often leads to opposite conclusions. Bach et al. (2009a) also use cohesion to correlate with the lexicalized reordering model (Tillman, 2004; Koehn et al., 2005), whereas we define an orthogonal dependency orientation feature to explicitly model head-dependent reordering. The fundamental difference, however, is rooted in the translation model. Their span-based cohesion constraint is implemented as an “interruption check” to encourage finishing a subtree before translating something else. This check is very effective for phrase-based decoding which searches over an entire space within the distortion limit in order to advance a hypothesis. In fact, it constrains reordering for the phrase-based model, as Cherry finds that the cohesion constraint is used"
D11-1079,N03-1017,1,0.0284058,"hich promotes reordering, we observe total improvements of 1.21 BLEU, 1.30 LRscore and 3.36 TER over the baseline. On average our approach improves reordering precision and recall by 6.9 and 0.3 absolute points, respectively, and is found to be especially effective for long-distance reodering. 1 Introduction Reordering, especially movement over longer distances, continues to be a hard problem in statistical machine translation. It motivates much of the recent work on tree-based translation models, such as the hierarchical phrase-based model (Chiang, 2007) which extends the phrase-based model (Koehn et al., 2003) by allowing the so-called hierarchical phrases containing subphrases. The hierarchical phrase-based model captures the recursiveness of language without relying on syntactic annotation, and promises better reordering than the phrase-based model. However, Birch et al. (2009) find that although the hierarchical phrasebased model outperforms the phrase-based model in 857 terms of medium-range reordering, it does equally poorly in long-distance reordering due to constraints to guarantee efficiency. Syntax-based models that use phrase structure constituent labels as non-terminals in their transfer"
D11-1079,P08-1114,0,0.588206,"favored by our cohesion penalty feature. However, ignorant of the syntactic structure, the 860 glue rule penalty may penalize a reasonably cohesive derivation such as Derivation 5 and at the same time promote a less cohesive hierarchical translation, such as Derivation 6. Compared with constituency constraints based on the phrase structure, our cohesion penalty derived from the binary dependency parsing has two different characteristics. First, our cohesion penalty is by nature more tolerant to some meaningful noncontituent translations. For example, constituency constraints in (Chiang, 2005; Marton and Resnik, 2008; Chiang et al., 2009) would penalize Rule 7 below which is useful for German–English translation (Koehn et al., 2003), and Rule 8 which can be applied to the Figure 1 sentence. Fuzzy constituency constraints can solve this problem with a combination of product categories and slash categories (Chiang, 2010). Yet our cohesion penalty by nature admits these translations as cohesive (with no extra cost from es and Aozhou since both are locally resolved). Admittedly, our current implementation of the cohesion penalty is blind to some other meaningful nonconstituent collocations, such as neighbouri"
D11-1079,P10-1145,0,0.0127649,"MT08 set, output by the baseline model and “bin-2” model. The “-lr” version outputs are quite similar and not shown here. Translation outputs are in lower case. tures the boxed area as a whole and uses Rule 10 to perform the right global reordering. X → (dui1 X2 gandao3 manyi4 .5 , expressed3 satisfaction4 with1 X2 .5 ) 5 (10) Related Work In recent years, there has been a growing body of research on using dependency for statistical machine translation. Some directly encodes dependency in the translation model (Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Mi and Liu, 2010), while others use dependency as a soft constraint (Cherry, 2008; Bach et al., 2009a,b; Chang et al., 2009). Among them, Shen et al. (2008) report that just filtering the phrase table by the socalled well-formed target dependency structure does not help, yet adding a target dependency language model improves performance significantly. Our intuitive interpretation is that the target dependency language model capitalizes on two characteristics of the dependency structure: it is based on words and it directly connects head and child. Therefore, the target dependency language model makes good use"
D11-1079,P03-1021,0,0.0373922,"-based model with standard settings (Hoang et al., 2009). When only 1 bin was used, 3 additional features were added to the baseline, one each from the soft dependency constraints. When we used 2 or 3 bins, the additional feature counts doubled or tripled. We preserved terminal alignment alongside nonterminal alignment during the rule extraction and output word alignments together with translated strings. Since the features we currently define are based entirely on the source side, we used preprocessing to speed up decoding of our feature-augmented model. All experiments were tuned with MERT (Och, 2003). 3.2 Using BLEU as the Tuning Metric As a standard practice, we first used BLEU (Papineni et al., 2002) as the objective function for tuning. Table 2 shows the results of the baseline model as well as our complete feature-augmented model with different bin numbers. With the “bin-2” setting, we get substantial improvement of up to 1.03 BLEU points (on MT02 data), and 0.84 BLEU points on average. Using more than one bin (i.e., differentiating tree depths) is generally beneficial, although the 2 http://www.umiacs.umd.edu/∼hal/megam/index.html Setting baseline bin-2 baseline-lr bin-2-lr MT02 34.0"
D11-1079,P02-1040,0,0.118839,"onal features were added to the baseline, one each from the soft dependency constraints. When we used 2 or 3 bins, the additional feature counts doubled or tripled. We preserved terminal alignment alongside nonterminal alignment during the rule extraction and output word alignments together with translated strings. Since the features we currently define are based entirely on the source side, we used preprocessing to speed up decoding of our feature-augmented model. All experiments were tuned with MERT (Och, 2003). 3.2 Using BLEU as the Tuning Metric As a standard practice, we first used BLEU (Papineni et al., 2002) as the objective function for tuning. Table 2 shows the results of the baseline model as well as our complete feature-augmented model with different bin numbers. With the “bin-2” setting, we get substantial improvement of up to 1.03 BLEU points (on MT02 data), and 0.84 BLEU points on average. Using more than one bin (i.e., differentiating tree depths) is generally beneficial, although the 2 http://www.umiacs.umd.edu/∼hal/megam/index.html Setting baseline bin-2 baseline-lr bin-2-lr MT02 34.01 / 41.85 / 68.93 35.04 / 43.07 / 65.58 34.23 / 42.06 / 68.08 35.42 / 43.25 / 64.82 BLEU / LRscore / TER"
D11-1079,P05-1034,0,0.535922,"s shown in Figure 1. The basic unit of dependency parsing is a triple consisting of the dependent word, the head word and the dependency relation that connects them. For example, in Figure 1, an arrow labelled prep goes from the word yu (English with) to the word you (English have), showing that yu is a prepositional modifier of you. We use the Stanford Parser1 to generate dependency parsing, which automatically extracts dependency relations from phrase structure parsing (de Marneffe et al., 2006). 2.1 Dependency Orientation Based on the assumption that constituents generally move as a whole (Quirk et al., 2005), we decompose the sentence reordering probability into the reordering probability for each aligned source word with respect to its head, excluding the root word at the top of the dependency hierarchy which does not have a head word. Similarly, Hayashi et al. (2010) also take a word-based reordering approach for HPBMT, but they model all possible pairwise orientation from the source side as a general linear ordering problem (Tromble and Eisner, 2009). To be more specific, we have a maximum entropy orientation classifier that predicts the probability of a source word being translated in a monot"
D11-1079,I08-2089,1,0.587986,"rallel training corpus with 2.1 million Chinese–English sentence pairs, aligned by GIZA++. The Chinese side was parsed by the Stanford Parser. Then we extracted 33.8 million examples from the parsed Chinese side to discriminatively train 1.1 million features (using the MegaM software2 ) for dependency orientation classification. We trained three 5-gram language models with modified Kneser-Ney smoothing (Kneser and Ney, 1995): one on the English half of the parallel corpus, one on the Xinhua part of the Gigaword corpus, one on the AFP part, and interpolated them for best fit to the tuning set (Schwenk and Koehn, 2008). We used NIST MT06 evaluation data (1664 lines) as our tuning set, and tested on NIST MT02 (878 lines), MT05 (1082 lines) and MT08 (1357 lines). Our baseline system was the Moses implementation of the hierarchical phrase-based model with standard settings (Hoang et al., 2009). When only 1 bin was used, 3 additional features were added to the baseline, one each from the soft dependency constraints. When we used 2 or 3 bins, the additional feature counts doubled or tripled. We preserved terminal alignment alongside nonterminal alignment during the rule extraction and output word alignments toge"
D11-1079,P09-1037,0,0.0590482,"tive to GIZA++, we would like to experiment with syntactically informed aligners that better handle function words which often exhibit high alignment ambiguity due to low cross-lingual correspondence. Finally, since our soft dependency constraints promote reordering without increasing model complexity, further gains can be achieved when combining our approach with orthogonal studies to improve the quantity and quality of hierarchical (reordering) rules, such as relaxing hierarchical rule extraction constraints (Setiawan and Resnik, 2010) and selectively lexicalizing rules with function words (Setiawan et al., 2009). Acknowledgments We would like to thank Miles Osborne, Adam Lopez, Barry Haddow, Hieu Hoang, Philip Williams and Michael Auli in the Edinburgh SMT group as well as Kevin Knight, David Chiang and Andrew Dai for inspiring discussions. We appreciate Pichuan Chang, Huihsin Tseng, Richard Zens, Matthew Snover and Nguyen Bach for helping us understand their brilliant work. Many thanks to the anonymous reviewers for their insightful comments and suggestions. This work was supported in part by the EuroMatrixPlus project funded by the European Commission (7th Framework Programme) and in part under the"
D11-1079,N10-1052,0,0.0192205,"Rscore which uses word alignment to compute the permutation distance. As an alternative to GIZA++, we would like to experiment with syntactically informed aligners that better handle function words which often exhibit high alignment ambiguity due to low cross-lingual correspondence. Finally, since our soft dependency constraints promote reordering without increasing model complexity, further gains can be achieved when combining our approach with orthogonal studies to improve the quantity and quality of hierarchical (reordering) rules, such as relaxing hierarchical rule extraction constraints (Setiawan and Resnik, 2010) and selectively lexicalizing rules with function words (Setiawan et al., 2009). Acknowledgments We would like to thank Miles Osborne, Adam Lopez, Barry Haddow, Hieu Hoang, Philip Williams and Michael Auli in the Edinburgh SMT group as well as Kevin Knight, David Chiang and Andrew Dai for inspiring discussions. We appreciate Pichuan Chang, Huihsin Tseng, Richard Zens, Matthew Snover and Nguyen Bach for helping us understand their brilliant work. Many thanks to the anonymous reviewers for their insightful comments and suggestions. This work was supported in part by the EuroMatrixPlus project fu"
D11-1079,P08-1066,0,0.558226,"ish translation (Koehn et al., 2003), and Rule 8 which can be applied to the Figure 1 sentence. Fuzzy constituency constraints can solve this problem with a combination of product categories and slash categories (Chiang, 2010). Yet our cohesion penalty by nature admits these translations as cohesive (with no extra cost from es and Aozhou since both are locally resolved). Admittedly, our current implementation of the cohesion penalty is blind to some other meaningful nonconstituent collocations, such as neighbouring siblings of a common uncovered head (regulated as the “floating structure” in (Shen et al., 2008)). A concrete example is Rule 9 which is useful for the Figure 1 sentence. To address this problem, another feature can be defined in the same manner to capture how each head word is translated with its children. X → (es1 gibt2 , there1 is2 ) (7) X → (Aozhou1 shi2 , Australia1 is2 ) (8) X → (shaoshu1 guojia2 , few1 countries2 ) (9) Second, our cohesion penalty can be by nature more discriminative. Compared with the constituency constraints, the cohesion penalty is integer-valued, and can be made sensitive to the depth of each word in the dependency hierarchy (see Section 2.4). Inspired by (Mar"
D11-1079,2006.amta-papers.25,0,0.0604956,"ring (especially in long-distance cases), we have also tried tuning with a metric that highlights reordering, i.e., the LRscore (Birch and Osborne, 2010). LRscore is a linear interpolation of a lexical metric and a reordering metric. We interpolated BLEU (as the lexical metric) with the Kendall’s tau permutation distance (as the reordering metric). The Kendall’s tau permutation distance measures the relative word order difference between the transla862 tion output and the reference(s) and is particularly sensitive to long-distance reordering. Testing results in terms of BLEU, LRscore and TER (Snover et al., 2006) are shown in Table 4. Tuned with the LRscore, our feature-augmented model achieves further average improvements (compare “bin-2” and “bin-2-lr”) of 0.20 LRscore as well as 0.37 BLEU and 0.90 TER. Note that while the BLEU increase can largely be seen as a projection of the LRscore increase back into its lexical component, the consistent TER drop confirms that our improvement is not metric-specific3 . Altogether the final improvement is 1.21 BLEU, 1.30 LRscore and 3.36 TER on average over the baseline. However, an important question is how our features affect short, medium and long-distance reo"
D11-1079,N04-4026,0,0.103896,"successfully defined another cohesion constraint from the source depen864 dency structure, with the aim of improving reordering in phrase-based MT. To take a glance, Cherry (2008) and Bach et al. (2009b) define cohesion as translating a source dependency subtree contiguously into the target side without interruption (span or subtree overlapping), following Fox (2002). This span-based cohesion constraint has a different criterion from our wordbased cohesion penalty and often leads to opposite conclusions. Bach et al. (2009a) also use cohesion to correlate with the lexicalized reordering model (Tillman, 2004; Koehn et al., 2005), whereas we define an orthogonal dependency orientation feature to explicitly model head-dependent reordering. The fundamental difference, however, is rooted in the translation model. Their span-based cohesion constraint is implemented as an “interruption check” to encourage finishing a subtree before translating something else. This check is very effective for phrase-based decoding which searches over an entire space within the distortion limit in order to advance a hypothesis. In fact, it constrains reordering for the phrase-based model, as Cherry finds that the cohesio"
D11-1079,D09-1105,0,0.0521927,"om phrase structure parsing (de Marneffe et al., 2006). 2.1 Dependency Orientation Based on the assumption that constituents generally move as a whole (Quirk et al., 2005), we decompose the sentence reordering probability into the reordering probability for each aligned source word with respect to its head, excluding the root word at the top of the dependency hierarchy which does not have a head word. Similarly, Hayashi et al. (2010) also take a word-based reordering approach for HPBMT, but they model all possible pairwise orientation from the source side as a general linear ordering problem (Tromble and Eisner, 2009). To be more specific, we have a maximum entropy orientation classifier that predicts the probability of a source word being translated in a monotone or reversed manner with respect to its head. For example, 1 (b) S1 http://nlp.stanford.edu/software/lex-parser.shtml 858 S2 S3 S1 T1 ihead T1 idep T2 idep T2 ihead T3 T3 T4 T4 S2 S3 Figure 2: Word alignments to illustrate orientation classification. In (a), monotone (M); in (b), reversed (R). given the alignment in Figure 2(a), with the alignment points (idep , jdep ) for the source dependent word and (ihead , jhead ) for the source head word, we"
D11-1079,D07-1077,1,0.872369,"naturally supports constituent translations as well as some nonconstituent translations, if not all of them (as discussed in Section 2.2). Our dependency orientation feature is similar to the order model within dependency treelet translation (Quirk et al., 2005). Yet instead of a head-relative position number for each modifier word, we simply predict the head-dependent orientation which is either monotone or reversed. Our coarser-grained approach is more robust from a machine learning perspective, yet still captures prominent and long-distance reordering patterns observed in Chinese–English (Wang et al., 2007), German–English (Collins et al., 2005), Japanese– English (Katz-Brown and Collins, 2008) and translation from English to a group of SOV languages (Xu et al., 2009). Not committed to specific language pairs, we learn orientation classification from the word-aligned parallel data through maximum entropy training as Zens and Ney (2006) and Chang et al. (2009) for phrase-based translation and Xiong et al. (2006) for the BTG model (Wu, 1996). While Chang et al. (2009) also make use of source dependency, their orientation classification concerns two subsequent phrase pairs in the leftto-right phras"
D11-1079,J10-2004,0,0.039313,"although the hierarchical phrasebased model outperforms the phrase-based model in 857 terms of medium-range reordering, it does equally poorly in long-distance reordering due to constraints to guarantee efficiency. Syntax-based models that use phrase structure constituent labels as non-terminals in their transfer rules, exemplified by that of Galley et al. (2004), produce smarter and syntactically motivated reordering. However, when working with off-the-shelf tools for parsing and alignment, this approach may impose harsh limits on rule extraction and requires serious efforts of optimization (Wang et al., 2010). An alternative approach is to augment the general hierarchical phrase-based model with soft syntactic constraints. Here, we derive three word-based, complementary constraints from the source dependency parsing, including: • A dependency orientation feature, trained with maximum entropy on the word-aligned parallel data, which directly models the headdependent orientation for source words; • An integer-valued cohesion penalty that complements the dependency orientation feature, and fires when a word is not translated with its head. It measures derivation well-formedness and is used to indirec"
D11-1079,D07-1080,0,0.0219106,"haoshu1 guojia2 , few1 countries2 ) (9) Second, our cohesion penalty can be by nature more discriminative. Compared with the constituency constraints, the cohesion penalty is integer-valued, and can be made sensitive to the depth of each word in the dependency hierarchy (see Section 2.4). Inspired by (Marton and Resnik, 2008; Chiang et al., 2009), the cohesion penalty could also be made sensitive to the dependency relation of each word. However, this drastically increases the number of features and requires a tuning algorithm which scales better to high-dimensional model spaces, such as MIRA (Watanabe et al., 2007; Chiang et al., 2008). pobj 是 Depth 1 top Depth 2 澳洲 . 之一 有 prep 与 Depth 4 pobj Depth 5 nummod 少数 国家 cpm dobj Bin 1 3 Bin 2 3.1 nn rcmod Depth 3 the tree levels spread out. punct attr 邦交 的 北韩 Figure 3: Using 2 bins for the dependency parse tree of the Figure 1 sentence. 2.3 Unaligned Penalty The dependency orientation and cohesion penalty cannot be applied to unaligned source words. This may lead to search error, such as dropping (i.e., unaligning) key content words that are important for lexical translation and reordering. The problem is mitigated by an unaligned penalty applicable to all wo"
D11-1079,P96-1021,0,0.0524512,"h is more robust from a machine learning perspective, yet still captures prominent and long-distance reordering patterns observed in Chinese–English (Wang et al., 2007), German–English (Collins et al., 2005), Japanese– English (Katz-Brown and Collins, 2008) and translation from English to a group of SOV languages (Xu et al., 2009). Not committed to specific language pairs, we learn orientation classification from the word-aligned parallel data through maximum entropy training as Zens and Ney (2006) and Chang et al. (2009) for phrase-based translation and Xiong et al. (2006) for the BTG model (Wu, 1996). While Chang et al. (2009) also make use of source dependency, their orientation classification concerns two subsequent phrase pairs in the leftto-right phrase-based decoding (as apposed to each dependent word and its head) and is therefore less linguistically-motivated. 865 6 Conclusion We have derived three novel features from the source dependency structure for hierarchical phrase-based MT. They work as a whole to capitalize on two characteristics of the dependency representation: it is directly based on words and it directly connects head and child. The effectiveness of our approach has b"
D11-1079,P06-1066,0,0.0927272,"r reversed. Our coarser-grained approach is more robust from a machine learning perspective, yet still captures prominent and long-distance reordering patterns observed in Chinese–English (Wang et al., 2007), German–English (Collins et al., 2005), Japanese– English (Katz-Brown and Collins, 2008) and translation from English to a group of SOV languages (Xu et al., 2009). Not committed to specific language pairs, we learn orientation classification from the word-aligned parallel data through maximum entropy training as Zens and Ney (2006) and Chang et al. (2009) for phrase-based translation and Xiong et al. (2006) for the BTG model (Wu, 1996). While Chang et al. (2009) also make use of source dependency, their orientation classification concerns two subsequent phrase pairs in the leftto-right phrase-based decoding (as apposed to each dependent word and its head) and is therefore less linguistically-motivated. 865 6 Conclusion We have derived three novel features from the source dependency structure for hierarchical phrase-based MT. They work as a whole to capitalize on two characteristics of the dependency representation: it is directly based on words and it directly connects head and child. The effect"
D11-1079,W07-0706,0,0.0439119,"e 5: Example translations from the NIST MT08 set, output by the baseline model and “bin-2” model. The “-lr” version outputs are quite similar and not shown here. Translation outputs are in lower case. tures the boxed area as a whole and uses Rule 10 to perform the right global reordering. X → (dui1 X2 gandao3 manyi4 .5 , expressed3 satisfaction4 with1 X2 .5 ) 5 (10) Related Work In recent years, there has been a growing body of research on using dependency for statistical machine translation. Some directly encodes dependency in the translation model (Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Mi and Liu, 2010), while others use dependency as a soft constraint (Cherry, 2008; Bach et al., 2009a,b; Chang et al., 2009). Among them, Shen et al. (2008) report that just filtering the phrase table by the socalled well-formed target dependency structure does not help, yet adding a target dependency language model improves performance significantly. Our intuitive interpretation is that the target dependency language model capitalizes on two characteristics of the dependency structure: it is based on words and it directly connects head and child. Therefore, the target dep"
D11-1079,N09-1028,0,0.0828397,"feature is similar to the order model within dependency treelet translation (Quirk et al., 2005). Yet instead of a head-relative position number for each modifier word, we simply predict the head-dependent orientation which is either monotone or reversed. Our coarser-grained approach is more robust from a machine learning perspective, yet still captures prominent and long-distance reordering patterns observed in Chinese–English (Wang et al., 2007), German–English (Collins et al., 2005), Japanese– English (Katz-Brown and Collins, 2008) and translation from English to a group of SOV languages (Xu et al., 2009). Not committed to specific language pairs, we learn orientation classification from the word-aligned parallel data through maximum entropy training as Zens and Ney (2006) and Chang et al. (2009) for phrase-based translation and Xiong et al. (2006) for the BTG model (Wu, 1996). While Chang et al. (2009) also make use of source dependency, their orientation classification concerns two subsequent phrase pairs in the leftto-right phrase-based decoding (as apposed to each dependent word and its head) and is therefore less linguistically-motivated. 865 6 Conclusion We have derived three novel featu"
D11-1079,W06-3108,0,0.0648542,", we simply predict the head-dependent orientation which is either monotone or reversed. Our coarser-grained approach is more robust from a machine learning perspective, yet still captures prominent and long-distance reordering patterns observed in Chinese–English (Wang et al., 2007), German–English (Collins et al., 2005), Japanese– English (Katz-Brown and Collins, 2008) and translation from English to a group of SOV languages (Xu et al., 2009). Not committed to specific language pairs, we learn orientation classification from the word-aligned parallel data through maximum entropy training as Zens and Ney (2006) and Chang et al. (2009) for phrase-based translation and Xiong et al. (2006) for the BTG model (Wu, 1996). While Chang et al. (2009) also make use of source dependency, their orientation classification concerns two subsequent phrase pairs in the leftto-right phrase-based decoding (as apposed to each dependent word and its head) and is therefore less linguistically-motivated. 865 6 Conclusion We have derived three novel features from the source dependency structure for hierarchical phrase-based MT. They work as a whole to capitalize on two characteristics of the dependency representation: it i"
D11-1079,W06-3119,0,0.174907,"t al., 2009a,b) (They do have a non-empty intersection, but neither subsumes the other). Therefore, our cohesion penalty is better suited for the hierarchical phrase-based model. To discourage nonconstituent translation, Chiang (2005) has proposed a constituency feature to examine whether a source rule span matches the source constituent as defined by phrase structure parsing. Finer-grained constituency constraints significantly improve hierarchical phrase-based MT when applied on the source side (Marton and Resnik, 2008; Chiang et al., 2009), or on the target side in a more tolerant fashion (Zollmann and Venugopal, 2006). Using both source and target syntax, but relaxing on rule extraction and substitution enables HPBMT to produce more well-formed and syntactically richer derivations (Chiang, 2010). Softening constituency matching with latent syntactic distributions proves to be helpful (Huang et al., 2010). Compared to constituency-based approaches, our cohesion penalty based on the dependency structure naturally supports constituent translations as well as some nonconstituent translations, if not all of them (as discussed in Section 2.2). Our dependency orientation feature is similar to the order model with"
D11-1079,D08-1076,0,\N,Missing
D12-1107,D07-1090,0,0.0471064,"o and Bertoldi, 2006) uses less bits to store each numerical value at the expense of model quality, including scores of full sentences, and is compatible with our approach. In fact, the lower-order probabilities might be quantized further than normal since these are used solely for rest cost purposes. Our compression technique reduces storage from two values, probability and backoff, to one value, theoretically halving the bits per value (except N -grams which all have backoff 1). This makes the storage requirement for higher-quality modified Kneser-Ney smoothing comparable to stupid backoff (Brants et al., 2007). Whether to use one smoothing technique or the other then becomes largely an issue of training costs and quality after quantization. 3 Contribution 3.1 Better Rest Costs As alluded to in the introduction, the first few words of a sentence fragment are typically scored using lower-order entries from an N -gram language model. However, Kneser-Ney smoothing (Kneser and Ney, 1995) conditions lower-order probabilities on backing off. Specifically, lower-order counts are adjusted to represent the number of unique extensions an n-gram has: ( |{w0 : c(w0n ) &gt; 0} |if n < N a(w1n ) = c(w1n ) if n = N w"
D12-1107,W11-2103,1,0.847704,"k Y k−1 k−1 k rules) and the order in which rules are tried during p(w1 )p(wk |w1 ) b(wj ) cube pruning. j=f 1173 3.3 Combined Scheme Our two language model modifications can be trivially combined by using lower-order probabilities on the left of a fragment and by charging all backoff penalties on the right of a fragment. The net result is a language model that uses the same memory as the baseline but has better rest cost estimates. 4 Experiments To measure the impact of different rest costs, we use the Moses chart decoder (Koehn et al., 2007) for the WMT 2011 German-English translation task (Callison-Burch et al., 2011). Using the Moses pipeline, we trained two syntactic German-English systems, one with target-side syntax and the other hierarchical with unlabeled grammar rules (Chiang, 2007). Grammar rules were extracted from Europarl (Koehn, 2005) using the Collins parser (Collins, 1999) for syntax on the English side. The language model interpolates, on the WMT 2010 test set, separate models built on Europarl, news commentary, and the WMT news data for each year. Models were built and interpolated using SRILM (Stolcke, 2002) with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998)"
D12-1107,J07-2003,0,0.528898,"ability and backoff. We will show that the probability and backoff values in a language model can be collapsed into a single value for each n-gram without changing sentence probability. This transformation saves memory by halving the number of values stored per entry, but it makes rest cost estimates worse. Specifically, the rest cost pessimistically assumes that the model will back off to unigrams immediately following the sentence fragment. The two modifications can be used independently or simultaneously. To measure the impact of their different rest costs, we experiment with cube pruning (Chiang, 2007) in syntactic machine transla2 Other smoothing techniques, including Witten-Bell (Witten and Bell, 1991), do not make this assumption. 1170 tion. Cube pruning’s goal is to find high-scoring sentence fragments for the root non-terminal in the parse tree. It does so by going bottom-up in the parse tree, searching for high-scoring sentence fragments for each non-terminal. Within each non-terminal, it generates a fixed number of high-scoring sentence fragments; this is known as the pop limit. Increasing the pop limit therefore makes search more accurate but costs more time. By moderating the pop l"
D12-1107,P10-4002,0,0.0701019,"Missing"
D12-1107,W06-3113,0,0.0321054,"rnal to the sentence fragment, namely output that the decoder may generate in the future. Our rest costs examine words internal to the sentence fragment, namely the first and last few words. We also differ by focusing on syntactic translation. A wide variety of work has been done on language model compression. While data structure compression (Raj and Whittaker, 2003; Heafield, 2011) and randomized data structures (Talbot and Osborne, 2007; Guthrie and Hepple, 2010) are useful, here we are concerned solely with the values stored by these data structures. Quantization (Whittaker and Raj, 2001; Federico and Bertoldi, 2006) uses less bits to store each numerical value at the expense of model quality, including scores of full sentences, and is compatible with our approach. In fact, the lower-order probabilities might be quantized further than normal since these are used solely for rest cost purposes. Our compression technique reduces storage from two values, probability and backoff, to one value, theoretically halving the bits per value (except N -grams which all have backoff 1). This makes the storage requirement for higher-quality modified Kneser-Ney smoothing comparable to stupid backoff (Brants et al., 2007)."
D12-1107,D10-1026,0,0.0733215,"h passes, so our work is largely orthogonal. Zens and Ney (2008) present rest costs for phrasebased translation. These rest costs are based on factors external to the sentence fragment, namely output that the decoder may generate in the future. Our rest costs examine words internal to the sentence fragment, namely the first and last few words. We also differ by focusing on syntactic translation. A wide variety of work has been done on language model compression. While data structure compression (Raj and Whittaker, 2003; Heafield, 2011) and randomized data structures (Talbot and Osborne, 2007; Guthrie and Hepple, 2010) are useful, here we are concerned solely with the values stored by these data structures. Quantization (Whittaker and Raj, 2001; Federico and Bertoldi, 2006) uses less bits to store each numerical value at the expense of model quality, including scores of full sentences, and is compatible with our approach. In fact, the lower-order probabilities might be quantized further than normal since these are used solely for rest cost purposes. Our compression technique reduces storage from two values, probability and backoff, to one value, theoretically halving the bits per value (except N -grams whic"
D12-1107,2011.iwslt-evaluation.24,1,0.682728,"are typically scored using lower-order entries from an N -gram language model. However, Kneser-Ney smoothing (Kneser and Ney, 1995) conditions lower-order probabilities on backing off. Specifically, lower-order counts are adjusted to represent the number of unique extensions an n-gram has: ( |{w0 : c(w0n ) &gt; 0} |if n < N a(w1n ) = c(w1n ) if n = N ways back off to w1n−1 or fewer words4 . This criterion is the same as used to minimize the length of left language model state (Li and Khudanpur, 2008) and can be retrieved for each n-gram without using additional memory in common data structures (Heafield et al., 2011). Where it is unknown if the model will back off, we use a language model of the same order to produce a rest cost. Specifically, there are N language models, one of each order from 1 to N . The models are trained on the same corpus with the same smoothing parameters to the extent that they apply. We then compile these into one data structure where each n-gram record has three values: 1. Probability pn from the n-gram language model 2. Probability pN from the N -gram language model 3. Backoff b from the N -gram language model where c(w1n ) is the number of times w1n appears in the training dat"
D12-1107,W11-2123,1,0.86198,"options. The rest cost estimates we describe here could be applied in both passes, so our work is largely orthogonal. Zens and Ney (2008) present rest costs for phrasebased translation. These rest costs are based on factors external to the sentence fragment, namely output that the decoder may generate in the future. Our rest costs examine words internal to the sentence fragment, namely the first and last few words. We also differ by focusing on syntactic translation. A wide variety of work has been done on language model compression. While data structure compression (Raj and Whittaker, 2003; Heafield, 2011) and randomized data structures (Talbot and Osborne, 2007; Guthrie and Hepple, 2010) are useful, here we are concerned solely with the values stored by these data structures. Quantization (Whittaker and Raj, 2001; Federico and Bertoldi, 2006) uses less bits to store each numerical value at the expense of model quality, including scores of full sentences, and is compatible with our approach. In fact, the lower-order probabilities might be quantized further than normal since these are used solely for rest cost purposes. Our compression technique reduces storage from two values, probability and b"
D12-1107,P07-1019,0,0.0445105,"oring sentence fragments for the root non-terminal in the parse tree. It does so by going bottom-up in the parse tree, searching for high-scoring sentence fragments for each non-terminal. Within each non-terminal, it generates a fixed number of high-scoring sentence fragments; this is known as the pop limit. Increasing the pop limit therefore makes search more accurate but costs more time. By moderating the pop limit, improved accuracy can be interpreted as a reduction in CPU time and vice-versa. 2 Related Work Vilar and Ney (2011) study several modifications to cube pruning and cube growing (Huang and Chiang, 2007). Most relevant is their use of a class-based language model for the first of two decoding passes. This first pass is cheaper because translation alternatives are likely to fall into the same class. Entries are scored with the maximum probability over class members (thereby making them no longer normalized). Thus, paths that score highly in this first pass may contain high-scoring paths under the lexicalized language model, so the second pass more fully explores these options. The rest cost estimates we describe here could be applied in both passes, so our work is largely orthogonal. Zens and"
D12-1107,P07-2045,1,0.0206737,"ete sentence is held constant. We first show how to improve rest cost quality over standard practice by using additional space. Then, conversely, we show how to compress the language model by making a pessimistic rest cost assumption1 . Language models are designed to assign probability to sentences. However, approximate search algorithms use estimates for sentence fragments. If the language model has order N (an N -gram model), then the first N − 1 words of the fragment have incomplete context and the last N − 1 words have not been completely used as context. Our baseline is common practice (Koehn et al., 2007; Dyer et al., 2010; Li et al., 2009) that uses lower-order entries from the language model for the first words in the fragment and no rest cost adjustment for the last few words. Formally, the baseline estimate for sentence fragment w1k is Approximate search algorithms, such as cube pruning in syntactic machine translation, rely on the language model to estimate probabilities of sentence fragments. We contribute two changes that trade between accuracy of these estimates and memory, holding sentence-level scores constant. Common practice uses lowerorder entries in an N -gram model to score the"
D12-1107,2005.mtsummit-papers.11,1,0.0635177,"fragment and by charging all backoff penalties on the right of a fragment. The net result is a language model that uses the same memory as the baseline but has better rest cost estimates. 4 Experiments To measure the impact of different rest costs, we use the Moses chart decoder (Koehn et al., 2007) for the WMT 2011 German-English translation task (Callison-Burch et al., 2011). Using the Moses pipeline, we trained two syntactic German-English systems, one with target-side syntax and the other hierarchical with unlabeled grammar rules (Chiang, 2007). Grammar rules were extracted from Europarl (Koehn, 2005) using the Collins parser (Collins, 1999) for syntax on the English side. The language model interpolates, on the WMT 2010 test set, separate models built on Europarl, news commentary, and the WMT news data for each year. Models were built and interpolated using SRILM (Stolcke, 2002) with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998) and the default pruning settings. In all scenarios, the primary language model has order 5. For lower-order rest costs, we also built models with orders 1 through 4 then used the n-gram model to score n-grams in the 5-gram model. Fea"
D12-1107,W08-0402,0,0.841283,"tion. 3 Contribution 3.1 Better Rest Costs As alluded to in the introduction, the first few words of a sentence fragment are typically scored using lower-order entries from an N -gram language model. However, Kneser-Ney smoothing (Kneser and Ney, 1995) conditions lower-order probabilities on backing off. Specifically, lower-order counts are adjusted to represent the number of unique extensions an n-gram has: ( |{w0 : c(w0n ) &gt; 0} |if n < N a(w1n ) = c(w1n ) if n = N ways back off to w1n−1 or fewer words4 . This criterion is the same as used to minimize the length of left language model state (Li and Khudanpur, 2008) and can be retrieved for each n-gram without using additional memory in common data structures (Heafield et al., 2011). Where it is unknown if the model will back off, we use a language model of the same order to produce a rest cost. Specifically, there are N language models, one of each order from 1 to N . The models are trained on the same corpus with the same smoothing parameters to the extent that they apply. We then compile these into one data structure where each n-gram record has three values: 1. Probability pn from the n-gram language model 2. Probability pN from the N -gram language"
D12-1107,W09-0424,0,0.0571311,"Missing"
D12-1107,P03-1021,0,0.00735297,"999) for syntax on the English side. The language model interpolates, on the WMT 2010 test set, separate models built on Europarl, news commentary, and the WMT news data for each year. Models were built and interpolated using SRILM (Stolcke, 2002) with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998) and the default pruning settings. In all scenarios, the primary language model has order 5. For lower-order rest costs, we also built models with orders 1 through 4 then used the n-gram model to score n-grams in the 5-gram model. Feature weights were trained with MERT (Och, 2003) on the baseline using a pop limit of 1000 and 100-best output. Since final feature values are unchanged, we did not re-run MERT in each condition. Measurements were collected by running the decoder on the 3003-sentence test set. 4.1 Rest Costs as Prediction Scoring the first few words of a sentence fragment is a prediction task. The goal is to predict what the probability will be when more context becomes known. In order to measure performance on this task, we ran the decoder on the hierarchical system with a pop limit of 1000. Every time more context became known, we logged5 the prediction e"
D12-1107,P02-1040,0,0.0836066,"cause lower variance means cube pruning’s relative rankings are more accurate. Our lower-order rest costs are better across the board in terms of absolute bias, mean squared error, and variance. 4.2 Pop Limit Trade-Offs The cube pruning pop limit is a trade-off between search accuracy and CPU time. Here, we measure how our rest costs improve (or degrade) that trade-off. Search accuracy is measured by the average model score of single-best translations. Model scores are scale-invariant and include a large constant factor; higher is better. We also measure overall performance with uncased BLEU (Papineni et al., 2002). CPU time is the sum of user and system time used by Moses divided by the number of sentences (3003). Timing includes time to load, though files were forced into the disk cache in advance. Our test machine has 64 GB of RAM and 32 cores. Results are shown in Figures 3 and 4. Lower-order rest costs perform better in both systems, reaching plateau model scores and BLEU with less CPU time. The gain is much larger for tarPop 2 10 50 500 700 Baseline Model BLEU -105.56 20.45 -104.74 21.13 -104.31 21.36 -104.25 21.33 -104.25 21.34 CPU 3.29 5.21 23.30 54.61 64.08 Lower Order CPU Model BLEU 3.68 -105."
D12-1107,P07-1065,0,0.0615394,"re could be applied in both passes, so our work is largely orthogonal. Zens and Ney (2008) present rest costs for phrasebased translation. These rest costs are based on factors external to the sentence fragment, namely output that the decoder may generate in the future. Our rest costs examine words internal to the sentence fragment, namely the first and last few words. We also differ by focusing on syntactic translation. A wide variety of work has been done on language model compression. While data structure compression (Raj and Whittaker, 2003; Heafield, 2011) and randomized data structures (Talbot and Osborne, 2007; Guthrie and Hepple, 2010) are useful, here we are concerned solely with the values stored by these data structures. Quantization (Whittaker and Raj, 2001; Federico and Bertoldi, 2006) uses less bits to store each numerical value at the expense of model quality, including scores of full sentences, and is compatible with our approach. In fact, the lower-order probabilities might be quantized further than normal since these are used solely for rest cost purposes. Our compression technique reduces storage from two values, probability and backoff, to one value, theoretically halving the bits per"
D12-1107,2008.iwslt-papers.8,0,0.0205576,"ng, 2007). Most relevant is their use of a class-based language model for the first of two decoding passes. This first pass is cheaper because translation alternatives are likely to fall into the same class. Entries are scored with the maximum probability over class members (thereby making them no longer normalized). Thus, paths that score highly in this first pass may contain high-scoring paths under the lexicalized language model, so the second pass more fully explores these options. The rest cost estimates we describe here could be applied in both passes, so our work is largely orthogonal. Zens and Ney (2008) present rest costs for phrasebased translation. These rest costs are based on factors external to the sentence fragment, namely output that the decoder may generate in the future. Our rest costs examine words internal to the sentence fragment, namely the first and last few words. We also differ by focusing on syntactic translation. A wide variety of work has been done on language model compression. While data structure compression (Raj and Whittaker, 2003; Heafield, 2011) and randomized data structures (Talbot and Osborne, 2007; Guthrie and Hepple, 2010) are useful, here we are concerned sole"
D12-1107,J03-4003,0,\N,Missing
D17-1319,P13-2119,0,0.0432227,") data from the data pool; both have the objective of improving translation performances. For practical purposes, it is highly desirable to perform data selection in a very fast and scalable manner. In this paper we introduce Zipporah1 , a fast and scalable system which can select an arbitrary size of good data from a large noisy data pool to be used in SMT model training. 1 https://github.com/hainan-xv/zipporah Prior Work Many researchers have studied the data cleaning/selection problem. For data selection, there have been a lot of work on selecting a subset of data based on domain-matching. Duh et al. (2013) used a neural network based language model trained on a small in-domain corpus to select from a larger data pool. Moore and Lewis (2010) computed cross-entropy between indomain and out-of-domain language models to select data for training language models. XenC (Rousseau, 2013), an open-source tool, also selects data based on cross-entropy scores on language models. Axelrod et al. (2015) utilized partof-speech tags and used a class-based n-gram language model for selecting in-domain data. There are a few works that utilize other metrics. L¨u et al. (2007) redistributed different weights for se"
D17-1319,P13-2061,0,0.205846,"defined sub-models. Shah and Specia (2014) described experiments on quality estimation which, given a source sentence, select the best translation among several options. The qeclean system (Denkowski et al., 2012; Dyer et al., 2010; Heafield, 2011) uses word alignments and language models to select sentence pairs that are likely to be good translations of one another. For data cleaning, a lot of researchers worked on getting rid of noising data. Taghipour et al. (2011) proposed an outlier detection algorithm which leads to an improved translation quality when trimming a small portion of data. Cui et al. (2013) used a graph-based random walk algorithm to do bilingual data cleaning. BiTextor (Espl´aGomis and Forcada, 2009) utilizes sentence alignment scores and source URL information to filter out bad URL pairs and selects good sentence pairs. In this paper we propose a novel way to evaluate the quality of a sentence pair which runs efficiently. We do not make a clear distinction 2945 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2945–2950 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics between data selection"
D17-1319,2009.mtsummit-btm.6,0,0.0476473,"Missing"
D17-1319,W11-2123,0,0.392922,"models. XenC (Rousseau, 2013), an open-source tool, also selects data based on cross-entropy scores on language models. Axelrod et al. (2015) utilized partof-speech tags and used a class-based n-gram language model for selecting in-domain data. There are a few works that utilize other metrics. L¨u et al. (2007) redistributed different weights for sentence pairs/predefined sub-models. Shah and Specia (2014) described experiments on quality estimation which, given a source sentence, select the best translation among several options. The qeclean system (Denkowski et al., 2012; Dyer et al., 2010; Heafield, 2011) uses word alignments and language models to select sentence pairs that are likely to be good translations of one another. For data cleaning, a lot of researchers worked on getting rid of noising data. Taghipour et al. (2011) proposed an outlier detection algorithm which leads to an improved translation quality when trimming a small portion of data. Cui et al. (2013) used a graph-based random walk algorithm to do bilingual data cleaning. BiTextor (Espl´aGomis and Forcada, 2009) utilizes sentence alignment scores and source URL information to filter out bad URL pairs and selects good sentence p"
D17-1319,2005.mtsummit-papers.11,1,0.0548201,"e feature because we want the scoring function to be monotonic both w.r.t x and y, which could break if we allow multiple higher-order mappings of the same feature and they end up with weights with different signs. Evaluation We evaluate Zipporah on 3 language pairs, French-English, German-English and SpanishEnglish. The noisy web-crawled data comes from an early version of http://statmt.org/ paracrawl. The number of words are (in millions) 340, 487 and 70 respectively. To generate the dictionaries for computing the adequacy scores, we use fast align (Dyer et al., 2013) to align the Europarl (Koehn, 2005) corpus and generate probabilistic dictionaries from the alignments. We set the n-gram order to be 5 and use SRILM (Stolcke et al., 2011) to train language models on the Europarl corpus and generate the n-gram scores. For each language pair, we use scikit-learn (Pedregosa et al., 2011) to train a logistic regression model to classify between the original and the synthetic noisy corpus of newstest09, and the trained model is used to score all sentence pairs in the data pool. We keep selecting the best ones until the desired number of words is reached. To evaluate the quality, we train a Moses ("
D17-1319,P07-2045,1,0.0188475,"corpus and generate probabilistic dictionaries from the alignments. We set the n-gram order to be 5 and use SRILM (Stolcke et al., 2011) to train language models on the Europarl corpus and generate the n-gram scores. For each language pair, we use scikit-learn (Pedregosa et al., 2011) to train a logistic regression model to classify between the original and the synthetic noisy corpus of newstest09, and the trained model is used to score all sentence pairs in the data pool. We keep selecting the best ones until the desired number of words is reached. To evaluate the quality, we train a Moses (Koehn et al., 2007) SMT system on selected data, and evaluate each trained SMT system on 3 test corpora: newstest2011 which contains 3003 sentence pairs, and a random subset of the TED-talks corpus and the movie-subtitle corpus from OPUS (Tiedemann, 2012), each of which contains 3000 sentence pairs. Tables 2, 3 and 4 show the BLEU performance of the selected subsets of the Zipporah system compared to the baseline, which selects sentence pairs at random; for comparison, we also give the BLEU performance of systems trained on Europarl. The Zipporah system gives consistently better performance across multiple datas"
D17-1319,D07-1036,0,0.0313319,"Missing"
D17-1319,P10-2041,0,0.0849079,"able to perform data selection in a very fast and scalable manner. In this paper we introduce Zipporah1 , a fast and scalable system which can select an arbitrary size of good data from a large noisy data pool to be used in SMT model training. 1 https://github.com/hainan-xv/zipporah Prior Work Many researchers have studied the data cleaning/selection problem. For data selection, there have been a lot of work on selecting a subset of data based on domain-matching. Duh et al. (2013) used a neural network based language model trained on a small in-domain corpus to select from a larger data pool. Moore and Lewis (2010) computed cross-entropy between indomain and out-of-domain language models to select data for training language models. XenC (Rousseau, 2013), an open-source tool, also selects data based on cross-entropy scores on language models. Axelrod et al. (2015) utilized partof-speech tags and used a class-based n-gram language model for selecting in-domain data. There are a few works that utilize other metrics. L¨u et al. (2007) redistributed different weights for sentence pairs/predefined sub-models. Shah and Specia (2014) described experiments on quality estimation which, given a source sentence, se"
D17-1319,2014.eamt-1.22,0,0.0826862,"e model trained on a small in-domain corpus to select from a larger data pool. Moore and Lewis (2010) computed cross-entropy between indomain and out-of-domain language models to select data for training language models. XenC (Rousseau, 2013), an open-source tool, also selects data based on cross-entropy scores on language models. Axelrod et al. (2015) utilized partof-speech tags and used a class-based n-gram language model for selecting in-domain data. There are a few works that utilize other metrics. L¨u et al. (2007) redistributed different weights for sentence pairs/predefined sub-models. Shah and Specia (2014) described experiments on quality estimation which, given a source sentence, select the best translation among several options. The qeclean system (Denkowski et al., 2012; Dyer et al., 2010; Heafield, 2011) uses word alignments and language models to select sentence pairs that are likely to be good translations of one another. For data cleaning, a lot of researchers worked on getting rid of noising data. Taghipour et al. (2011) proposed an outlier detection algorithm which leads to an improved translation quality when trimming a small portion of data. Cui et al. (2013) used a graph-based rando"
D17-1319,2014.amta-researchers.6,0,0.224008,"a and synthetic noisy data in the proposed feature space. The trained model is used to score parallel sentences in the data pool for selection. As shown in experiments, Zipporah selects a high-quality parallel corpus from a large, mixed quality data pool. In particular, for one noisy dataset, Zipporah achieves a 2.1 BLEU score improvement with using 1/5 of the data over using the entire corpus. 1 Introduction Statistical machine translation (SMT) systems require the use of parallel corpora for training the internal model parameters. Data quality is vital for the performance of the SMT system (Simard, 2014). To acquire a massive parallel corpus, many researchers have been using the Internet as a resource, but the quality of data acquired from the Internet usually has no guarantee, and data cleaning/data selection is needed before the data is used in actual systems. Usually data cleaning refers to getting rid of a small amount of very noisy data from a large data pool, and data selection refers to selecting a small subset of clean (or in-domain) data from the data pool; both have the objective of improving translation performances. For practical purposes, it is highly desirable to perform data se"
D17-1319,2011.mtsummit-papers.47,0,0.525236,"r selecting in-domain data. There are a few works that utilize other metrics. L¨u et al. (2007) redistributed different weights for sentence pairs/predefined sub-models. Shah and Specia (2014) described experiments on quality estimation which, given a source sentence, select the best translation among several options. The qeclean system (Denkowski et al., 2012; Dyer et al., 2010; Heafield, 2011) uses word alignments and language models to select sentence pairs that are likely to be good translations of one another. For data cleaning, a lot of researchers worked on getting rid of noising data. Taghipour et al. (2011) proposed an outlier detection algorithm which leads to an improved translation quality when trimming a small portion of data. Cui et al. (2013) used a graph-based random walk algorithm to do bilingual data cleaning. BiTextor (Espl´aGomis and Forcada, 2009) utilizes sentence alignment scores and source URL information to filter out bad URL pairs and selects good sentence pairs. In this paper we propose a novel way to evaluate the quality of a sentence pair which runs efficiently. We do not make a clear distinction 2945 Proceedings of the 2017 Conference on Empirical Methods in Natural Language"
D17-1319,tiedemann-2012-parallel,0,0.0219621,"r, we use scikit-learn (Pedregosa et al., 2011) to train a logistic regression model to classify between the original and the synthetic noisy corpus of newstest09, and the trained model is used to score all sentence pairs in the data pool. We keep selecting the best ones until the desired number of words is reached. To evaluate the quality, we train a Moses (Koehn et al., 2007) SMT system on selected data, and evaluate each trained SMT system on 3 test corpora: newstest2011 which contains 3003 sentence pairs, and a random subset of the TED-talks corpus and the movie-subtitle corpus from OPUS (Tiedemann, 2012), each of which contains 3000 sentence pairs. Tables 2, 3 and 4 show the BLEU performance of the selected subsets of the Zipporah system compared to the baseline, which selects sentence pairs at random; for comparison, we also give the BLEU performance of systems trained on Europarl. The Zipporah system gives consistently better performance across multiple datasets and multiple languages than the baseline.3 2947 3 We also point out that the performance of the selected BLEU newstest11 ted-talk subtitle num-words rand zipp rand zipp rand zipp 10 million 20 million 50 million 100 million 200 mill"
D17-1319,W12-3131,0,\N,Missing
D18-1339,D16-1162,0,0.0342803,"aps the most interesting cases are those where words appear to be substituted with a fluent but not adequate alternative. Many substitutions occur when the rare word is inserted next to a word that often forms a collocation (like “United States” – in sentences that include “in the [NNP] States” the translation sometimes defaults to a translation of “United States” regardless of the actual NNP inserted in place of “United”). Others have a less common NNP swapped for one that belongs to a similar semantic category (e.g. the place name Dublin being generated instead of the less common Halle – as Arthur et al. (2016) and others observed). For novel-copy words labeled as other, three quarters are substitutions and one quarter exhibit small changes. The reverse is true for novel-non-copy words: the majority exhibit small changes while almost thirty percent are substitutions. 5.3 Properties of Copied Words Certain words exhibit properties that make them more likely to be copied, regardless of context. At first glance, it seems unintuitive that the rate of copying of novel-copy words and novel-noncopy words differs (Fig. 1) – the model has never observed any of these words, and they are being presented in ide"
D18-1339,I17-1001,0,0.050419,"Missing"
D18-1339,W17-4715,0,0.0220399,"that we seek to answer: to what extent does byte-pair encoding1 solve the copying problem (without requiring modifications to the network structure)? 1 A type of subword vocabulary (Sennrich et al., 2016b). 2 Related Work Prior work on copying in neural machine translation has typically focused on rare or unknown words. Luong et al. (2015) augment data with word alignments to train a neural machine translation system (without attention) that emits both a translation and source word positions for any outof-vocabulary (OOV) tokens emitted. They postprocess OOVs with a dictionary or by copying. Currey et al. (2017) augment training data with monolingual target language text as bitext and find that it improves copying in low-resource settings. Ott et al. (2018) and Khayrallah and Koehn (2018) examine negative effects of source copying. Both Gu et al. (2016) and Gulcehre et al. (2016) modify neural sequence to sequence models to explicitly perform copying. Gu et al. (2016) focus on monolingual tasks (dialogue systems and summarization), proposing a model that can both generate and copy text. Gulcehre et al. (2016) perform experiments on neural machine translation (with attention), using whole-word vocabul"
D18-1339,W17-3204,1,0.84652,"l machine translation systems, which often use limited or subword vocabularies and soft attention rather than strict alignment. This has resulted in a variety of approaches to copying, which make use of pre-/postprocessing and/or network modifications (e.g. explicit switching between generation and copying). Neural machine translation models that use subword vocabularies to perform open-vocabulary translation have been observed to correctly translate unknown words or copy words (one subword at a time, if need be) even when the full word to be translated or copied was not observed in training. Koehn and Knowles (2017) found that neural machine translation systems using subword vocabularies outperformed phrase-based statistical machine translation systems on the translation of unknown words. This raises the questions that we seek to answer: to what extent does byte-pair encoding1 solve the copying problem (without requiring modifications to the network structure)? 1 A type of subword vocabulary (Sennrich et al., 2016b). 2 Related Work Prior work on copying in neural machine translation has typically focused on rare or unknown words. Luong et al. (2015) augment data with word alignments to train a neural mac"
D18-1339,P15-1002,0,0.0246081,"translated or copied was not observed in training. Koehn and Knowles (2017) found that neural machine translation systems using subword vocabularies outperformed phrase-based statistical machine translation systems on the translation of unknown words. This raises the questions that we seek to answer: to what extent does byte-pair encoding1 solve the copying problem (without requiring modifications to the network structure)? 1 A type of subword vocabulary (Sennrich et al., 2016b). 2 Related Work Prior work on copying in neural machine translation has typically focused on rare or unknown words. Luong et al. (2015) augment data with word alignments to train a neural machine translation system (without attention) that emits both a translation and source word positions for any outof-vocabulary (OOV) tokens emitted. They postprocess OOVs with a dictionary or by copying. Currey et al. (2017) augment training data with monolingual target language text as bitext and find that it improves copying in low-resource settings. Ott et al. (2018) and Khayrallah and Koehn (2018) examine negative effects of source copying. Both Gu et al. (2016) and Gulcehre et al. (2016) modify neural sequence to sequence models to exp"
D18-1339,W16-2323,0,0.0272165,"ion have been observed to correctly translate unknown words or copy words (one subword at a time, if need be) even when the full word to be translated or copied was not observed in training. Koehn and Knowles (2017) found that neural machine translation systems using subword vocabularies outperformed phrase-based statistical machine translation systems on the translation of unknown words. This raises the questions that we seek to answer: to what extent does byte-pair encoding1 solve the copying problem (without requiring modifications to the network structure)? 1 A type of subword vocabulary (Sennrich et al., 2016b). 2 Related Work Prior work on copying in neural machine translation has typically focused on rare or unknown words. Luong et al. (2015) augment data with word alignments to train a neural machine translation system (without attention) that emits both a translation and source word positions for any outof-vocabulary (OOV) tokens emitted. They postprocess OOVs with a dictionary or by copying. Currey et al. (2017) augment training data with monolingual target language text as bitext and find that it improves copying in low-resource settings. Ott et al. (2018) and Khayrallah and Koehn (2018) exa"
D18-1339,P16-1154,0,0.0303612,"ural machine translation has typically focused on rare or unknown words. Luong et al. (2015) augment data with word alignments to train a neural machine translation system (without attention) that emits both a translation and source word positions for any outof-vocabulary (OOV) tokens emitted. They postprocess OOVs with a dictionary or by copying. Currey et al. (2017) augment training data with monolingual target language text as bitext and find that it improves copying in low-resource settings. Ott et al. (2018) and Khayrallah and Koehn (2018) examine negative effects of source copying. Both Gu et al. (2016) and Gulcehre et al. (2016) modify neural sequence to sequence models to explicitly perform copying. Gu et al. (2016) focus on monolingual tasks (dialogue systems and summarization), proposing a model that can both generate and copy text. Gulcehre et al. (2016) perform experiments on neural machine translation (with attention), using whole-word vocabularies (and an UNK token to represent unknown words). Their model incorporates a switching variable that determines whether to copy or generate a translation. In this work, we focus on subword vocabularies for neural machine translation, using byt"
D18-1339,P16-1162,0,0.0632942,"ion have been observed to correctly translate unknown words or copy words (one subword at a time, if need be) even when the full word to be translated or copied was not observed in training. Koehn and Knowles (2017) found that neural machine translation systems using subword vocabularies outperformed phrase-based statistical machine translation systems on the translation of unknown words. This raises the questions that we seek to answer: to what extent does byte-pair encoding1 solve the copying problem (without requiring modifications to the network structure)? 1 A type of subword vocabulary (Sennrich et al., 2016b). 2 Related Work Prior work on copying in neural machine translation has typically focused on rare or unknown words. Luong et al. (2015) augment data with word alignments to train a neural machine translation system (without attention) that emits both a translation and source word positions for any outof-vocabulary (OOV) tokens emitted. They postprocess OOVs with a dictionary or by copying. Currey et al. (2017) augment training data with monolingual target language text as bitext and find that it improves copying in low-resource settings. Ott et al. (2018) and Khayrallah and Koehn (2018) exa"
D18-1339,P16-1014,0,0.0203876,"ion has typically focused on rare or unknown words. Luong et al. (2015) augment data with word alignments to train a neural machine translation system (without attention) that emits both a translation and source word positions for any outof-vocabulary (OOV) tokens emitted. They postprocess OOVs with a dictionary or by copying. Currey et al. (2017) augment training data with monolingual target language text as bitext and find that it improves copying in low-resource settings. Ott et al. (2018) and Khayrallah and Koehn (2018) examine negative effects of source copying. Both Gu et al. (2016) and Gulcehre et al. (2016) modify neural sequence to sequence models to explicitly perform copying. Gu et al. (2016) focus on monolingual tasks (dialogue systems and summarization), proposing a model that can both generate and copy text. Gulcehre et al. (2016) perform experiments on neural machine translation (with attention), using whole-word vocabularies (and an UNK token to represent unknown words). Their model incorporates a switching variable that determines whether to copy or generate a translation. In this work, we focus on subword vocabularies for neural machine translation, using byte-pair en3034 Proceedings o"
D18-1339,N03-1033,0,0.0269449,"and NOUN. Punctuation would rank highly if we included short tokens. 5 Experiments and Analysis We address two main questions: (1) Do certain contexts encourage copying? (2) Do certain words exhibit features that make them more likely to be copied (regardless of context)? 5.1 Contexts Working from the intuition that certain contexts indicate that copying should occur – for example, a name following a title like “Ms” or “Frau” 7 The two full training sets differ due to the synthetic backtranslated data; the rest of the corpora are identical. 8 POS tags are generated by the Stanford POS tagger (Toutanova et al., 2003). For English: english-left3wordsdistsim.tagger. For German: german-ud.tagger. 3035 S: Therefore, Mrs Ashton, your role in this is invaluable. R: Darum, Frau Ashton, ist Ihre Aufgabe in diesem Zusammenhang von unsch¨atzbarem Wert. T: Therefore, Mrs [NNP], your role in this is invaluable. E1: Therefore, Mrs BBC, your role in this is invaluable. D1: Deshalb, Frau BBC, ist Ihre Rolle hierbei von [...] E2: Therefore, Mrs June, your role in this is invaluable. D2: Deshalb, Frau June, ist Ihre Rolle dabei von [...] E3: Therefore, Mrs Lutreo, your role in this is invaluable. D3: Daher, Frau Lutreo, i"
D18-1339,P18-4020,0,0.0489928,"Missing"
D18-1339,W18-2709,1,0.744277,"cabulary (Sennrich et al., 2016b). 2 Related Work Prior work on copying in neural machine translation has typically focused on rare or unknown words. Luong et al. (2015) augment data with word alignments to train a neural machine translation system (without attention) that emits both a translation and source word positions for any outof-vocabulary (OOV) tokens emitted. They postprocess OOVs with a dictionary or by copying. Currey et al. (2017) augment training data with monolingual target language text as bitext and find that it improves copying in low-resource settings. Ott et al. (2018) and Khayrallah and Koehn (2018) examine negative effects of source copying. Both Gu et al. (2016) and Gulcehre et al. (2016) modify neural sequence to sequence models to explicitly perform copying. Gu et al. (2016) focus on monolingual tasks (dialogue systems and summarization), proposing a model that can both generate and copy text. Gulcehre et al. (2016) perform experiments on neural machine translation (with attention), using whole-word vocabularies (and an UNK token to represent unknown words). Their model incorporates a switching variable that determines whether to copy or generate a translation. In this work, we focus"
D19-1136,P19-1309,0,0.0443444,"to get both documents into the same language (Bleualign; Sennrich and Volk, 2010) or use pruned phrase tables from a statistical MT system (CoverageBased; Gomes and Lopes, 2016). Both methods “anchor” high-probability 1–1 alignments in the search space and then fill in and refine alignments. Locating anchors is O(N M ) time complexity. 3 Cosine similarity is an obvious choice for comparing embeddings but has been noted to be globally inconsistent due to “hubness” (Radovanovi´c et al., 2010; Lazaridou et al., 2015). Guo et al. (2018) proposed a supervised training approach for calibration, and Artetxe and Schwenk (2019) proposed normalization using nearest neighbors. We propose normalizing instead with randomly selected embeddings as it has linear complexity. Sentence alignment seeks minimal parallel units, but we find that DP with cosine similarity favors many-to-many alignments (e.g. reporting a 3–3 alignment when it should report three 1–1 alignments). To remedy this issue, we scale the cost by the number of source and target sentences being considered in a given alignment. Our resulting scoring cost function is: c(x,y) = Method (1 − cos(x,y)) nSents(x) nSents(y) S S P P 1 − cos(x,ys ) + 1 − cos(xs ,y) s="
D19-1136,C10-2010,0,0.174987,"sults 4.1 Text+Berg Alignment Accuracy We evaluate sentence alignment accuracy using the development/test split released with Bleualign, consisting of manually aligned yearbook articles published in both German and French by the Swiss Alpine Club from the Text+Berg corpus (Volk et al., 2010). Hyperparameters were chosen to optimize F1 on the development set. We consider alignments of up to 6 total sentences; that is we allow alignments of size Q–R where Q + R ≤ 6. We compare to Gale and Church (1993), Moore (2002), Hunalign (Varga et al., 2007), Bleualign (Sennrich and Volk, 2010), Gargantua (Braune and Fraser, 2010), and Coverage-Based (Gomes and Lopes, 2016). We run Hunalign in both bootstrapping mode as well as using a publically available De–Fr lexicon from OPUS (Tiedemann, 2012)5 created from Europarl (Koehn, 2005). Since Bleualign depends on the quality of MT output, we re-run it with a modern NMT system.6 Our proposed method outperforms the next best method (Coverage-Based) by 5 F1 points: see Table 1. Gargantua and bootstrapped Hunalign have both been reported to perform well (Abdul-Rauf et al., 2012); this dataset may be too small to bootstrap good lexical features.7 Bleualign improves by 3 F1 po"
D19-1136,P91-1022,0,0.88156,"ning our implementation.1 Our method outperforms previous state-of-theart, which has quadratic complexity, indicating that our proposed score function outperforms prior work and the approximations we make in alignment are sufficiently accurate. 1 https://github.com/thompsonb/vecalign 1342 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1342–1348, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2 Related Work 3.2 Early sentence aligners (Brown et al., 1991; Gale and Church, 1993) use scoring functions based only on the number of words or characters in each sentence and alignment algorithms based on dynamic programming (DP; Bellman, 1953). DP is O(N M ) time complexity, where N and M are the number of sentences in the source and target documents. Later work added lexical features and heuristics to speed up search, such as limiting the search space to be near the diagonal (Moore, 2002; Varga et al., 2007). More recent work introduced scoring methods that use MT to get both documents into the same language (Bleualign; Sennrich and Volk, 2010) or u"
D19-1136,W16-2347,1,0.891293,"ing within an alignment. Sentence-aligned bitext is used to train nearly all machine translation (MT) systems. Alignment errors have been noted to have a small effect on statistical MT performance (Goutte et al., 2012). However, misaligned sentences have been shown to be much more detrimental to neural MT (NMT) (Khayrallah and Koehn, 2018). Sentence alignment was a popular research topic in the early days of statistical MT, but received less attention once standard sentencealigned parallel corpora became available. Interest in low-resource MT has led to a resurgence in data gathering methods (Buck and Koehn, 2016; Zweigenbaum et al., 2018; Koehn et al., 2019), but we find limited recent work on bilingual sentence alignment. Automatic sentence alignment can be roughly decomposed into two parts: 1. A score function which takes one or more adjacent source sentences and one or more adjacent target sentences and returns a score indicating the likelihood that they are translations of each other; 2. An alignment algorithm which, using the score function above, takes in two documents and returns a hypothesis alignment. We improve both parts, presenting (1) a novel scoring function based on normalized cosine d"
D19-1136,W19-5435,1,0.893968,"e similarity of blocks of sentences does not depend on the number of sentences being compared. We show empirically (see § 4.2) that average embeddings for blocks of sentences are sufficient to produce approximate alignments, even in low-resource languages. This enables us to approximate DP in O(N + M ) in time and space. 3.1 Scoring Function Bilingual Sentence Embeddings We propose to use the similarity between sentence embeddings as the scoring function for sentence alignment. Sentence embedding similarity has been shown effective at filtering out non-parallel sentences (Hassan et al., 2018; Chaudhary et al., 2019) and locating parallel sentences in comparable corpora (Guo et al., 2018). We use the publicly available LASER multilingual sentence embedding method (Artetxe and Schwenk, 2018) and model, which is pretrained on 93 languages. However, our method is not specific to LASER. s=1 where x, y denote one or more sequential sentences from the source/target document; cos(x,y) is the cosine similarity between embeddings2 of x, y; nSents(x), nSents(y) denote the number of sentences in x, y; and x1 ,...,xS , y1 ,...,yS are sampled uniformly from the given document. Following standard practice, we model ins"
D19-1136,J93-1004,0,0.963533,"ion.1 Our method outperforms previous state-of-theart, which has quadratic complexity, indicating that our proposed score function outperforms prior work and the approximations we make in alignment are sufficiently accurate. 1 https://github.com/thompsonb/vecalign 1342 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1342–1348, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2 Related Work 3.2 Early sentence aligners (Brown et al., 1991; Gale and Church, 1993) use scoring functions based only on the number of words or characters in each sentence and alignment algorithms based on dynamic programming (DP; Bellman, 1953). DP is O(N M ) time complexity, where N and M are the number of sentences in the source and target documents. Later work added lexical features and heuristics to speed up search, such as limiting the search space to be near the diagonal (Moore, 2002; Varga et al., 2007). More recent work introduced scoring methods that use MT to get both documents into the same language (Bleualign; Sennrich and Volk, 2010) or use pruned phrase tables"
D19-1136,L16-1354,0,0.057709,"er of words or characters in each sentence and alignment algorithms based on dynamic programming (DP; Bellman, 1953). DP is O(N M ) time complexity, where N and M are the number of sentences in the source and target documents. Later work added lexical features and heuristics to speed up search, such as limiting the search space to be near the diagonal (Moore, 2002; Varga et al., 2007). More recent work introduced scoring methods that use MT to get both documents into the same language (Bleualign; Sennrich and Volk, 2010) or use pruned phrase tables from a statistical MT system (CoverageBased; Gomes and Lopes, 2016). Both methods “anchor” high-probability 1–1 alignments in the search space and then fill in and refine alignments. Locating anchors is O(N M ) time complexity. 3 Cosine similarity is an obvious choice for comparing embeddings but has been noted to be globally inconsistent due to “hubness” (Radovanovi´c et al., 2010; Lazaridou et al., 2015). Guo et al. (2018) proposed a supervised training approach for calibration, and Artetxe and Schwenk (2019) proposed normalization using nearest neighbors. We propose normalizing instead with randomly selected embeddings as it has linear complexity. Sentence"
D19-1136,2012.amta-papers.7,0,0.0501153,"in this case (e1 )(f1 , f2 ), (e2 )-(f3 ), (e3 ,e4 )-(f4 ), and (e5 )-(f6 ). Introduction Sentence alignment is the task of taking parallel documents, which have been split into sentences, and finding a bipartite graph which matches minimal groups of sentences that are translations of each other (see Figure 1). Following prior work, we assume non-crossing alignments but allow local sentence reordering within an alignment. Sentence-aligned bitext is used to train nearly all machine translation (MT) systems. Alignment errors have been noted to have a small effect on statistical MT performance (Goutte et al., 2012). However, misaligned sentences have been shown to be much more detrimental to neural MT (NMT) (Khayrallah and Koehn, 2018). Sentence alignment was a popular research topic in the early days of statistical MT, but received less attention once standard sentencealigned parallel corpora became available. Interest in low-resource MT has led to a resurgence in data gathering methods (Buck and Koehn, 2016; Zweigenbaum et al., 2018; Koehn et al., 2019), but we find limited recent work on bilingual sentence alignment. Automatic sentence alignment can be roughly decomposed into two parts: 1. A score fu"
D19-1136,W18-6317,0,0.162469,"; Varga et al., 2007). More recent work introduced scoring methods that use MT to get both documents into the same language (Bleualign; Sennrich and Volk, 2010) or use pruned phrase tables from a statistical MT system (CoverageBased; Gomes and Lopes, 2016). Both methods “anchor” high-probability 1–1 alignments in the search space and then fill in and refine alignments. Locating anchors is O(N M ) time complexity. 3 Cosine similarity is an obvious choice for comparing embeddings but has been noted to be globally inconsistent due to “hubness” (Radovanovi´c et al., 2010; Lazaridou et al., 2015). Guo et al. (2018) proposed a supervised training approach for calibration, and Artetxe and Schwenk (2019) proposed normalization using nearest neighbors. We propose normalizing instead with randomly selected embeddings as it has linear complexity. Sentence alignment seeks minimal parallel units, but we find that DP with cosine similarity favors many-to-many alignments (e.g. reporting a 3–3 alignment when it should report three 1–1 alignments). To remedy this issue, we scale the cost by the number of source and target sentences being considered in a given alignment. Our resulting scoring cost function is: c(x,y"
D19-1136,W18-2709,1,0.752334,"task of taking parallel documents, which have been split into sentences, and finding a bipartite graph which matches minimal groups of sentences that are translations of each other (see Figure 1). Following prior work, we assume non-crossing alignments but allow local sentence reordering within an alignment. Sentence-aligned bitext is used to train nearly all machine translation (MT) systems. Alignment errors have been noted to have a small effect on statistical MT performance (Goutte et al., 2012). However, misaligned sentences have been shown to be much more detrimental to neural MT (NMT) (Khayrallah and Koehn, 2018). Sentence alignment was a popular research topic in the early days of statistical MT, but received less attention once standard sentencealigned parallel corpora became available. Interest in low-resource MT has led to a resurgence in data gathering methods (Buck and Koehn, 2016; Zweigenbaum et al., 2018; Koehn et al., 2019), but we find limited recent work on bilingual sentence alignment. Automatic sentence alignment can be roughly decomposed into two parts: 1. A score function which takes one or more adjacent source sentences and one or more adjacent target sentences and returns a score indi"
D19-1136,2005.mtsummit-papers.11,1,0.145268,"ch by the Swiss Alpine Club from the Text+Berg corpus (Volk et al., 2010). Hyperparameters were chosen to optimize F1 on the development set. We consider alignments of up to 6 total sentences; that is we allow alignments of size Q–R where Q + R ≤ 6. We compare to Gale and Church (1993), Moore (2002), Hunalign (Varga et al., 2007), Bleualign (Sennrich and Volk, 2010), Gargantua (Braune and Fraser, 2010), and Coverage-Based (Gomes and Lopes, 2016). We run Hunalign in both bootstrapping mode as well as using a publically available De–Fr lexicon from OPUS (Tiedemann, 2012)5 created from Europarl (Koehn, 2005). Since Bleualign depends on the quality of MT output, we re-run it with a modern NMT system.6 Our proposed method outperforms the next best method (Coverage-Based) by 5 F1 points: see Table 1. Gargantua and bootstrapped Hunalign have both been reported to perform well (Abdul-Rauf et al., 2012); this dataset may be too small to bootstrap good lexical features.7 Bleualign improves by 3 F1 points by using an NMT system. 5 https://object.pouta.csc.fi/ OPUS-Europarl/v7/dic/de-fr.dic.gz 6 https://docs.microsoft.com/en-us/ azure/cognitive-services/translator/ 7 We run only on the test/development ar"
D19-1136,W19-5404,1,0.913974,"is used to train nearly all machine translation (MT) systems. Alignment errors have been noted to have a small effect on statistical MT performance (Goutte et al., 2012). However, misaligned sentences have been shown to be much more detrimental to neural MT (NMT) (Khayrallah and Koehn, 2018). Sentence alignment was a popular research topic in the early days of statistical MT, but received less attention once standard sentencealigned parallel corpora became available. Interest in low-resource MT has led to a resurgence in data gathering methods (Buck and Koehn, 2016; Zweigenbaum et al., 2018; Koehn et al., 2019), but we find limited recent work on bilingual sentence alignment. Automatic sentence alignment can be roughly decomposed into two parts: 1. A score function which takes one or more adjacent source sentences and one or more adjacent target sentences and returns a score indicating the likelihood that they are translations of each other; 2. An alignment algorithm which, using the score function above, takes in two documents and returns a hypothesis alignment. We improve both parts, presenting (1) a novel scoring function based on normalized cosine distance between multilingual sentence embedding"
D19-1136,P15-1027,0,0.0208837,"the diagonal (Moore, 2002; Varga et al., 2007). More recent work introduced scoring methods that use MT to get both documents into the same language (Bleualign; Sennrich and Volk, 2010) or use pruned phrase tables from a statistical MT system (CoverageBased; Gomes and Lopes, 2016). Both methods “anchor” high-probability 1–1 alignments in the search space and then fill in and refine alignments. Locating anchors is O(N M ) time complexity. 3 Cosine similarity is an obvious choice for comparing embeddings but has been noted to be globally inconsistent due to “hubness” (Radovanovi´c et al., 2010; Lazaridou et al., 2015). Guo et al. (2018) proposed a supervised training approach for calibration, and Artetxe and Schwenk (2019) proposed normalization using nearest neighbors. We propose normalizing instead with randomly selected embeddings as it has linear complexity. Sentence alignment seeks minimal parallel units, but we find that DP with cosine similarity favors many-to-many alignments (e.g. reporting a 3–3 alignment when it should report three 1–1 alignments). To remedy this issue, we scale the cost by the number of source and target sentences being considered in a given alignment. Our resulting scoring cost"
D19-1136,moore-2002-fast,0,0.805006,"ocessing, pages 1342–1348, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2 Related Work 3.2 Early sentence aligners (Brown et al., 1991; Gale and Church, 1993) use scoring functions based only on the number of words or characters in each sentence and alignment algorithms based on dynamic programming (DP; Bellman, 1953). DP is O(N M ) time complexity, where N and M are the number of sentences in the source and target documents. Later work added lexical features and heuristics to speed up search, such as limiting the search space to be near the diagonal (Moore, 2002; Varga et al., 2007). More recent work introduced scoring methods that use MT to get both documents into the same language (Bleualign; Sennrich and Volk, 2010) or use pruned phrase tables from a statistical MT system (CoverageBased; Gomes and Lopes, 2016). Both methods “anchor” high-probability 1–1 alignments in the search space and then fill in and refine alignments. Locating anchors is O(N M ) time complexity. 3 Cosine similarity is an obvious choice for comparing embeddings but has been noted to be globally inconsistent due to “hubness” (Radovanovi´c et al., 2010; Lazaridou et al., 2015)."
D19-1136,2010.amta-papers.14,0,0.637134,"aligners (Brown et al., 1991; Gale and Church, 1993) use scoring functions based only on the number of words or characters in each sentence and alignment algorithms based on dynamic programming (DP; Bellman, 1953). DP is O(N M ) time complexity, where N and M are the number of sentences in the source and target documents. Later work added lexical features and heuristics to speed up search, such as limiting the search space to be near the diagonal (Moore, 2002; Varga et al., 2007). More recent work introduced scoring methods that use MT to get both documents into the same language (Bleualign; Sennrich and Volk, 2010) or use pruned phrase tables from a statistical MT system (CoverageBased; Gomes and Lopes, 2016). Both methods “anchor” high-probability 1–1 alignments in the search space and then fill in and refine alignments. Locating anchors is O(N M ) time complexity. 3 Cosine similarity is an obvious choice for comparing embeddings but has been noted to be globally inconsistent due to “hubness” (Radovanovi´c et al., 2010; Lazaridou et al., 2015). Guo et al. (2018) proposed a supervised training approach for calibration, and Artetxe and Schwenk (2019) proposed normalization using nearest neighbors. We pro"
D19-1136,tiedemann-2012-parallel,0,0.0656234,"rticles published in both German and French by the Swiss Alpine Club from the Text+Berg corpus (Volk et al., 2010). Hyperparameters were chosen to optimize F1 on the development set. We consider alignments of up to 6 total sentences; that is we allow alignments of size Q–R where Q + R ≤ 6. We compare to Gale and Church (1993), Moore (2002), Hunalign (Varga et al., 2007), Bleualign (Sennrich and Volk, 2010), Gargantua (Braune and Fraser, 2010), and Coverage-Based (Gomes and Lopes, 2016). We run Hunalign in both bootstrapping mode as well as using a publically available De–Fr lexicon from OPUS (Tiedemann, 2012)5 created from Europarl (Koehn, 2005). Since Bleualign depends on the quality of MT output, we re-run it with a modern NMT system.6 Our proposed method outperforms the next best method (Coverage-Based) by 5 F1 points: see Table 1. Gargantua and bootstrapped Hunalign have both been reported to perform well (Abdul-Rauf et al., 2012); this dataset may be too small to bootstrap good lexical features.7 Bleualign improves by 3 F1 points by using an NMT system. 5 https://object.pouta.csc.fi/ OPUS-Europarl/v7/dic/de-fr.dic.gz 6 https://docs.microsoft.com/en-us/ azure/cognitive-services/translator/ 7 W"
D19-1136,volk-etal-2010-challenges,0,0.156579,"pace complexity is linear. 3 We use w = 10 for all experiments in this work. 4 In practice, we compute the full DP alignment once the down sampled sizes are below an acceptably small constant. We also find vectors for large blocks of sentences become correlated with each other, so we center them around ~0. 4 Experiments & Results 4.1 Text+Berg Alignment Accuracy We evaluate sentence alignment accuracy using the development/test split released with Bleualign, consisting of manually aligned yearbook articles published in both German and French by the Swiss Alpine Club from the Text+Berg corpus (Volk et al., 2010). Hyperparameters were chosen to optimize F1 on the development set. We consider alignments of up to 6 total sentences; that is we allow alignments of size Q–R where Q + R ≤ 6. We compare to Gale and Church (1993), Moore (2002), Hunalign (Varga et al., 2007), Bleualign (Sennrich and Volk, 2010), Gargantua (Braune and Fraser, 2010), and Coverage-Based (Gomes and Lopes, 2016). We run Hunalign in both bootstrapping mode as well as using a publically available De–Fr lexicon from OPUS (Tiedemann, 2012)5 created from Europarl (Koehn, 2005). Since Bleualign depends on the quality of MT output, we re-"
D19-1142,D16-1162,0,0.0233046,"isier lexicon that more closely mimics real-world lexicons (e.g. by adding irrelevant entries or relevant morphological variants, lemmatizing entries, or subsampling). At a high level, our lexicons are created by a twostep process: (1) identifying interesting words on the source side of the test and development sets, and (2) human annotators correcting or validating automatic alignments of the identified words. Incorporation at Training Time Zhang and Zong (2016) and Fadaee et al. (2017) both propose using bilingual lexicons to create synthetic bitext to augment training data for NMT systems. Arthur et al. (2016) use translation probabilities from a lexicon (like SMT phrase tables) in conjunction with NMT probabilities. 2.2 Incorporation at Decode Time Kothur et al. (2018) perform fine-grained continued training adaptation on very small, documentspecific bilingual lexicons of novel words.3 A popular inference-time approach is constrained decoding (Anderson et al., 2016; Hokamp and Liu, 2017; Chatterjee et al., 2017; Hasler et al., 2018; Post and Vilar, 2018), which modifies beam search to require that user-specified words or phrases to be present in the output hypotheses. Constrained decoding can be u"
D19-1142,2012.eamt-1.60,0,0.0263817,"e general domain model and adapting to WIPO. We apply the Moses tokenizer (Koehn et al., 2007), lowercasing, and bytepair encoding (BPE; Sennrich et al., 2016) with a vocabulary size of 30k. BPE is trained on the general domain corpus only, then applied to all data. 4.1 them to be particularly challenging for MT, but given time and resources there is nothing to prevent the application of the annotation protocol to other terms. 6 For strong initial alignments, GIZA++ (Och and Ney, 2003) is trained on train, development, and test for all data described in section 4 as well as a TED talk corpus (Cettolo et al., 2012). 7 While we want the dictionary to match the reference, we did not want to train on large phrases from the reference. Evaluation Metrics We evaluate lexicon incorporation approaches using two main metrics: BLEU and recall. For each annotated instance of a source-side lexical entry, we can check whether the system output contains the correct aligned target-side translation. Recall is computed as the percentage of the time that the system produces the correct output, averaged over all annotations. Note that this does not guarantee that the words are placed in a sensible location in the sentence"
D19-1142,W17-4716,0,0.0166719,"words. Incorporation at Training Time Zhang and Zong (2016) and Fadaee et al. (2017) both propose using bilingual lexicons to create synthetic bitext to augment training data for NMT systems. Arthur et al. (2016) use translation probabilities from a lexicon (like SMT phrase tables) in conjunction with NMT probabilities. 2.2 Incorporation at Decode Time Kothur et al. (2018) perform fine-grained continued training adaptation on very small, documentspecific bilingual lexicons of novel words.3 A popular inference-time approach is constrained decoding (Anderson et al., 2016; Hokamp and Liu, 2017; Chatterjee et al., 2017; Hasler et al., 2018; Post and Vilar, 2018), which modifies beam search to require that user-specified words or phrases to be present in the output hypotheses. Constrained decoding can be used to ensure that target entries from a bilingual lexicon be present in the MT output whenever their corresponding source entries are present in the input. Constrained decoding with multiple target options (e.g. when a source word can be translated into one of several target words) is addressed in Chatterjee et al. (2017) and Hasler et al. (2018). 2.3 Datasets Used in Prior Studies Prior work has used eith"
D19-1142,P17-2090,0,0.216085,"than would be found in most realworld scenarios. However, it could also be used as a standardized starting point to produce a noisier lexicon that more closely mimics real-world lexicons (e.g. by adding irrelevant entries or relevant morphological variants, lemmatizing entries, or subsampling). At a high level, our lexicons are created by a twostep process: (1) identifying interesting words on the source side of the test and development sets, and (2) human annotators correcting or validating automatic alignments of the identified words. Incorporation at Training Time Zhang and Zong (2016) and Fadaee et al. (2017) both propose using bilingual lexicons to create synthetic bitext to augment training data for NMT systems. Arthur et al. (2016) use translation probabilities from a lexicon (like SMT phrase tables) in conjunction with NMT probabilities. 2.2 Incorporation at Decode Time Kothur et al. (2018) perform fine-grained continued training adaptation on very small, documentspecific bilingual lexicons of novel words.3 A popular inference-time approach is constrained decoding (Anderson et al., 2016; Hokamp and Liu, 2017; Chatterjee et al., 2017; Hasler et al., 2018; Post and Vilar, 2018), which modifies b"
D19-1142,grimes-etal-2012-automatic,0,0.0487356,"Missing"
D19-1142,N18-2081,0,0.0304261,"Missing"
D19-1142,E17-3017,0,0.0316391,"ible effect on BLEU score results. To build strong baseline systems, we first train models on general domain data. As general domain data, we use the OpenSubtitles18 (Lison et al., 2018) corpora for both Ru→En and Ko→En. For Ru→En and Zh→En we also use the parallel portion of the WMT17 news translation task (Bojar et al., 2017). We then fine-tune these generaldomain models on WIPO training data (Luong and Manning, 2015), using the dev set for validation. These domain-adapted models are then used as the initial systems for our lexicon incorporation experiments. We build the systems in Sockeye (Hieber et al., 2017), using a two-layer LSTM network with hidden unit size 512. We use an initial learning rate of 3e-4 both for training the general domain model and adapting to WIPO. We apply the Moses tokenizer (Koehn et al., 2007), lowercasing, and bytepair encoding (BPE; Sennrich et al., 2016) with a vocabulary size of 30k. BPE is trained on the general domain corpus only, then applied to all data. 4.1 them to be particularly challenging for MT, but given time and resources there is nothing to prevent the application of the annotation protocol to other terms. 6 For strong initial alignments, GIZA++ (Och and"
D19-1142,P17-1141,0,0.0221676,"ents of the identified words. Incorporation at Training Time Zhang and Zong (2016) and Fadaee et al. (2017) both propose using bilingual lexicons to create synthetic bitext to augment training data for NMT systems. Arthur et al. (2016) use translation probabilities from a lexicon (like SMT phrase tables) in conjunction with NMT probabilities. 2.2 Incorporation at Decode Time Kothur et al. (2018) perform fine-grained continued training adaptation on very small, documentspecific bilingual lexicons of novel words.3 A popular inference-time approach is constrained decoding (Anderson et al., 2016; Hokamp and Liu, 2017; Chatterjee et al., 2017; Hasler et al., 2018; Post and Vilar, 2018), which modifies beam search to require that user-specified words or phrases to be present in the output hypotheses. Constrained decoding can be used to ensure that target entries from a bilingual lexicon be present in the MT output whenever their corresponding source entries are present in the input. Constrained decoding with multiple target options (e.g. when a source word can be translated into one of several target words) is addressed in Chatterjee et al. (2017) and Hasler et al. (2018). 2.3 Datasets Used in Prior Studies"
D19-1142,P07-2045,1,0.0239386,"icon integration into neural machine translation. Our data consists of humangenerated alignments of words and phrases in machine translation test sets in three language pairs (Russian→English, Chinese→English, and Korean→English), resulting in clean bilingual lexicons which are well matched to the reference. We also present two simple baselines—constrained decoding and continued training—and an improvement to continued training to address overfitting. 1 Lexical Entry Target Neural machine translation (NMT) is the current state-of-the-art. In contrast with statistical machine translation (SMT; Koehn et al., 2007), where there are several established methods of incorporating external knowledge,1 recent work is still examining how best to incorporate bilingual lexicons into NMT systems. Bilingual lexicon integration is desirable in a number of scenarios: highly technical vocabulary (which might be rare, or require translations of a domain-specific sense), lowerresource settings (where bilingual lexicons might be a significant portion of the available parallel data), translation settings where a client requires particular terms to be used (e.g. brand names), or for improving rare word translation. At pre"
D19-1142,N19-1209,1,0.842185,"ensure high quality, (2) derived from the development and test references so the best-case-scenario impact on translation performance can be directly measured, (3) covering 3 language pairs, and (4) focused on challenging words. We perform exploratory work on our development set, showing two representative baselines to compare incorporating the lexicon at training time (continued training) vs. at decoding time (constrained decoding). We examine the tradeoffs in terms of BLEU, recall, and speed. We also present a novel application of Elastic Weight Consolidation (EWC; Kirkpatrick et al., 2017; Thompson et al., 2019) which significantly improves performance by preventing overfitting during continued training on the bilingual lexicon. 2 Related Work We first review prior work on incorporation of bilingual lexicons into NMT and then discuss the datasets used and explain how our new dataset addresses some shortcomings. Recent work on the incorporation of bilingual lexicons into NMT systems can be loosely clus2 http://www.cs.jhu.edu/~kevinduh/a/ hablex2019/ 1382 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language"
D19-1142,W18-2708,1,0.821566,"ng). At a high level, our lexicons are created by a twostep process: (1) identifying interesting words on the source side of the test and development sets, and (2) human annotators correcting or validating automatic alignments of the identified words. Incorporation at Training Time Zhang and Zong (2016) and Fadaee et al. (2017) both propose using bilingual lexicons to create synthetic bitext to augment training data for NMT systems. Arthur et al. (2016) use translation probabilities from a lexicon (like SMT phrase tables) in conjunction with NMT probabilities. 2.2 Incorporation at Decode Time Kothur et al. (2018) perform fine-grained continued training adaptation on very small, documentspecific bilingual lexicons of novel words.3 A popular inference-time approach is constrained decoding (Anderson et al., 2016; Hokamp and Liu, 2017; Chatterjee et al., 2017; Hasler et al., 2018; Post and Vilar, 2018), which modifies beam search to require that user-specified words or phrases to be present in the output hypotheses. Constrained decoding can be used to ensure that target entries from a bilingual lexicon be present in the MT output whenever their corresponding source entries are present in the input. Constr"
D19-1142,L18-1275,0,0.0173633,"alignment, adding or removing source or target side words as needed to complete a valid alignment. Since annotation is cumbersome with very long sentences, we omit sentences of length 100 or more tokens. Filtering out numerical entries and phrases longer than 3 words7 results in bilingual lexicons with sizes shown in Table 2. The data contains a small number of discontiguous alignments; these are so infrequent as to have a negligible effect on BLEU score results. To build strong baseline systems, we first train models on general domain data. As general domain data, we use the OpenSubtitles18 (Lison et al., 2018) corpora for both Ru→En and Ko→En. For Ru→En and Zh→En we also use the parallel portion of the WMT17 news translation task (Bojar et al., 2017). We then fine-tune these generaldomain models on WIPO training data (Luong and Manning, 2015), using the dev set for validation. These domain-adapted models are then used as the initial systems for our lexicon incorporation experiments. We build the systems in Sockeye (Hieber et al., 2017), using a two-layer LSTM network with hidden unit size 512. We use an initial learning rate of 3e-4 both for training the general domain model and adapting to WIPO. W"
D19-1142,2015.iwslt-evaluation.11,0,0.0790031,"ies and phrases longer than 3 words7 results in bilingual lexicons with sizes shown in Table 2. The data contains a small number of discontiguous alignments; these are so infrequent as to have a negligible effect on BLEU score results. To build strong baseline systems, we first train models on general domain data. As general domain data, we use the OpenSubtitles18 (Lison et al., 2018) corpora for both Ru→En and Ko→En. For Ru→En and Zh→En we also use the parallel portion of the WMT17 news translation task (Bojar et al., 2017). We then fine-tune these generaldomain models on WIPO training data (Luong and Manning, 2015), using the dev set for validation. These domain-adapted models are then used as the initial systems for our lexicon incorporation experiments. We build the systems in Sockeye (Hieber et al., 2017), using a two-layer LSTM network with hidden unit size 512. We use an initial learning rate of 3e-4 both for training the general domain model and adapting to WIPO. We apply the Moses tokenizer (Koehn et al., 2007), lowercasing, and bytepair encoding (BPE; Sennrich et al., 2016) with a vocabulary size of 30k. BPE is trained on the general domain corpus only, then applied to all data. 4.1 them to be p"
D19-1142,J03-1002,0,0.0247052,"., 2017), using a two-layer LSTM network with hidden unit size 512. We use an initial learning rate of 3e-4 both for training the general domain model and adapting to WIPO. We apply the Moses tokenizer (Koehn et al., 2007), lowercasing, and bytepair encoding (BPE; Sennrich et al., 2016) with a vocabulary size of 30k. BPE is trained on the general domain corpus only, then applied to all data. 4.1 them to be particularly challenging for MT, but given time and resources there is nothing to prevent the application of the annotation protocol to other terms. 6 For strong initial alignments, GIZA++ (Och and Ney, 2003) is trained on train, development, and test for all data described in section 4 as well as a TED talk corpus (Cettolo et al., 2012). 7 While we want the dictionary to match the reference, we did not want to train on large phrases from the reference. Evaluation Metrics We evaluate lexicon incorporation approaches using two main metrics: BLEU and recall. For each annotated instance of a source-side lexical entry, we can check whether the system output contains the correct aligned target-side translation. Recall is computed as the percentage of the time that the system produces the correct output"
D19-1142,N18-1119,0,0.0605426,"nd Zong (2016) and Fadaee et al. (2017) both propose using bilingual lexicons to create synthetic bitext to augment training data for NMT systems. Arthur et al. (2016) use translation probabilities from a lexicon (like SMT phrase tables) in conjunction with NMT probabilities. 2.2 Incorporation at Decode Time Kothur et al. (2018) perform fine-grained continued training adaptation on very small, documentspecific bilingual lexicons of novel words.3 A popular inference-time approach is constrained decoding (Anderson et al., 2016; Hokamp and Liu, 2017; Chatterjee et al., 2017; Hasler et al., 2018; Post and Vilar, 2018), which modifies beam search to require that user-specified words or phrases to be present in the output hypotheses. Constrained decoding can be used to ensure that target entries from a bilingual lexicon be present in the MT output whenever their corresponding source entries are present in the input. Constrained decoding with multiple target options (e.g. when a source word can be translated into one of several target words) is addressed in Chatterjee et al. (2017) and Hasler et al. (2018). 2.3 Datasets Used in Prior Studies Prior work has used either human-generated general purpose bilingual"
D19-1142,P16-1162,0,0.0667478,"the WMT17 news translation task (Bojar et al., 2017). We then fine-tune these generaldomain models on WIPO training data (Luong and Manning, 2015), using the dev set for validation. These domain-adapted models are then used as the initial systems for our lexicon incorporation experiments. We build the systems in Sockeye (Hieber et al., 2017), using a two-layer LSTM network with hidden unit size 512. We use an initial learning rate of 3e-4 both for training the general domain model and adapting to WIPO. We apply the Moses tokenizer (Koehn et al., 2007), lowercasing, and bytepair encoding (BPE; Sennrich et al., 2016) with a vocabulary size of 30k. BPE is trained on the general domain corpus only, then applied to all data. 4.1 them to be particularly challenging for MT, but given time and resources there is nothing to prevent the application of the annotation protocol to other terms. 6 For strong initial alignments, GIZA++ (Och and Ney, 2003) is trained on train, development, and test for all data described in section 4 as well as a TED talk corpus (Cettolo et al., 2012). 7 While we want the dictionary to match the reference, we did not want to train on large phrases from the reference. Evaluation Metrics"
D19-1632,buck-etal-2014-n,0,0.109306,"Missing"
D19-1632,W19-5435,1,0.90476,"data are available for Sinhala–English and Nepali–English. Statistics can be found in Table 2. This data comes from different sources. Open Subtitles and GNOME/KDE/Ubuntu come from the OPUS repository7 . Global Voices is an updated version (2018q4) of a data set originally created for the CASMACAT project8 . Bible translations come from the bible-corpus9 . The Paracrawl corpus comes from the Paracrawl project10 . The filtered version (Clean Paracrawl) was generated using the LASER model (Artetxe and Schwenk, 2018) to get the best sentence pairs having 1 million English tokens as specified in Chaudhary et al. (2019). We also contrast this filtered version with a randomly filtered version (Random Paracrawl) with the same number of English tokens. Finally, our multilingual experiments in Nepali use Hindi monolingual (about 5 million sentences) and EnglishHindi parallel data (about 1.5 million parallel sentences) from the IIT Bombay corpus11 . 4.2 Training Settings We evaluate models in four training settings. First, we consider a fully supervised training setting using the parallel data listed in Table 2. Second, we consider a fully unsupervised setting, whereby only monolingual data on both the source and"
D19-1632,D18-1045,1,0.858046,"only in the weakly-supervised experiments since alignments are noisy. (back-translated) source sentences with the original target sentences and add them as additional parallel data for training source-to-target MT system. Since monolingual data is available for both languages, we train backward MT systems in both directions and repeat the back-translation process iteratively (He et al., 2016; Lample et al., 2018a). We consider up to two back-translation iterations. At each iteration we generate back-translations using beam search, which has been shown to perform well in low-resource settings (Edunov et al., 2018); we use a beam width of 5 and individually tune the length-penalty on the dev set. Finally, we consider a weakly supervised setting by using a baseline system to filter out Paracrawl data using LASER (Artetxe and Schwenk, 2018) by following the approach similar to Chaudhary et al. (2019), in order to augment the original training set with a possibly larger but noisier set of parallel sentences. 6102 For Nepali only, we also consider training using Hindi data, both in a joint supervised and semi-supervised setting. For instance, at each iteration of the joint semi-supervised setting, we use mo"
D19-1632,W17-3204,1,0.850788,"l., 2017), as well as the availability of large parallel corpora for training (Tiedemann, 2012; Smith et al., ♥ Equal contribution. 2013; Bojar et al., 2017). Indeed, modern neural MT systems can achieve near human-level translation performance on language pairs for which sufficient parallel training resources exist (e.g., Chinese–English translation (Hassan et al., 2018) and English–French translation (Gehring et al., 2016; Ott et al., 2018a). Unfortunately, MT systems, and in particular neural models, perform poorly on low-resource language pairs, for which parallel training data is scarce (Koehn and Knowles, 2017). Improving translation performance on low-resource language pairs could be very impactful considering that these languages are spoken by a large fraction of the world population. Technically, there are several challenges to solve in order to improve translation for lowresource languages. First, in face of the scarcity of clean parallel data, MT systems should be able to use any source of data available, namely monolingual resources, noisy comparable data, as well as parallel data in related languages. Second, we need reliable public evaluation benchmarks to track progress in translation quali"
D19-1632,N19-4009,1,0.823346,"upervised task (in both directions). 4.3 Models & Architectures We consider both phrase-based statistical machine translation (PBSMT) and neural machine translation (NMT) systems in our experiments. All hyper-parameters have been cross-validated using the dev set. The PBSMT systems use Moses (Koehn et al., 2007), with state-of-theart settings (5-gram language model, hierarchical lexicalized reordering model, operation sequence model) but no additional monolingual data to train the language model. The NMT systems use the Transformer (Vaswani et al., 2017) implementation in the Fairseq toolkit (Ott et al., 2019); preliminary experiments showed these to perform better than LSTM-based NMT models. More specifically, in the supervised setting, we use a Transformer architecture with 5 encoder and 5 decoder layers, where the number of attention heads, embedding dimension and inner-layer dimension are 2, 512 and 2048, respectively. In the semi-supervised setting, where we augment our small parallel training data with millions of back-translated sentence pairs, we use a larger Transformer architecture with 6 encoder and 6 decoder layers, where the number of attention heads, embedding dimension and inner-laye"
D19-1632,W18-6301,1,0.93604,"nces in recent years thanks to improvements in modeling, and in particular neural models (Sutskever et al., 2014; Bahdanau et al., 2015; Gehring et al., 2016; Vaswani et al., 2017), as well as the availability of large parallel corpora for training (Tiedemann, 2012; Smith et al., ♥ Equal contribution. 2013; Bojar et al., 2017). Indeed, modern neural MT systems can achieve near human-level translation performance on language pairs for which sufficient parallel training resources exist (e.g., Chinese–English translation (Hassan et al., 2018) and English–French translation (Gehring et al., 2016; Ott et al., 2018a). Unfortunately, MT systems, and in particular neural models, perform poorly on low-resource language pairs, for which parallel training data is scarce (Koehn and Knowles, 2017). Improving translation performance on low-resource language pairs could be very impactful considering that these languages are spoken by a large fraction of the world population. Technically, there are several challenges to solve in order to improve translation for lowresource languages. First, in face of the scarcity of clean parallel data, MT systems should be able to use any source of data available, namely monoli"
D19-1632,P02-1040,0,0.108923,"extracted from Wikipedia articles in each language and translated by professional translators. The datasets we release to the community are composed of a tune set of 2559 and 2898 sentences, a development set of 2835 and 2766 sentences, and a test set of 2924 and 2905 sentences for Nepali–English and Sinhala–English respectively. In §3, we describe the methodology we used to collect the data as well as to check the quality of translations. The experiments reported in §4 demonstrate that these benchmarks are very challenging for current state-of-the-art methods, yielding very low BLEU scores (Papineni et al., 2002) even using all available parallel data as well as monolingual data or Paracrawl1 filtered data. This suggests that these languages and evaluation benchmarks can constitute a useful test-bed for developing and comparing MT systems for lowresource language pairs. 2 Related Work There is ample literature on low-resource MT. From the modeling side, one possibility is to design methods that make more effective use of monolingual data. This is a research avenue that has seen a recent surge of interest, starting with semisupervised methods relying on backtranslation (Sennrich et al., 2015), integrat"
D19-1632,W18-6319,0,0.195431,"ze Nepali and Sinhala using the Indic NLP Library.12 For the PBSMT system, we tokenize English sentences using the Moses tokenization scripts. For NMT systems, we instead use a vocabulary of 5K symbols based on a joint source and target Byte-Pair Encoding (BPE; Sennrich et al., 2015) learned using the sentencepiece library13 over the parallel training data. We learn the joint BPE for each language pair over the raw English sentences and tokenized Nepali or Sinhala sentences. We then remove training sentence pairs with more than 250 source or target BPE tokens. We report detokenized SacreBLEU (Post, 2018) when translating into English, and tokenized BLEU (Papineni et al., 2002) when translating from English into Nepali or Sinhala. 4.5 Results In the supervised setting, PBSMT performed quite worse than NMT, achieving BLEU scores of 2.5, 4.4, 1.6 and 5.0 on English–Nepali, Nepali– English, English–Sinhala and Sinhala–English, respectively. Table 3 reports results using NMT in all the other learning configurations described in §4.2. There are several observations we can make. First, these language pairs are very difficult, as even supervised NMT baselines achieve BLEU scores less than 8. Second a"
D19-1632,W12-3152,0,0.0351591,", 2016) have collected translations on several lowresource languages like English–Tagalog. Unfortunately, the data is only made available to the program’s participants. More recently, the Asian Language Treebank project (Riza et al., 2016) has introduced parallel datasets for several low-resource language pairs, but these are sampled from text originating in English and thus may not generalize to text sampled from low-resource languages. In the past, there has been work on extracting high quality translations from crowd-sourced workers using automatic methods (Zaidan and Callison-Burch, 2011; Post et al., 2012). However, crowd-sourced translations have generally lower quality than professional translations. In contrast, in this work we explore the quality checks that are required to filter professional translations of lowresource languages in order to build a high quality benchmark set. In practice, there are very few publicly available datasets for low-resource language pairs, and often times, researchers simulate learning on lowresource languages by using a high-resource language pair like English–French, and merely limiting how much labeled data they use for training (Johnson et al., 2016; Lample"
D19-1632,P11-1122,0,0.102719,"Missing"
D19-1632,P07-2045,1,\N,Missing
D19-1632,Q17-1010,0,\N,Missing
D19-1632,Q17-1024,0,\N,Missing
D19-1632,L16-1521,0,\N,Missing
D19-1632,P17-1042,0,\N,Missing
D19-1632,W17-4717,1,\N,Missing
D19-1632,D17-1319,1,\N,Missing
D19-1632,D18-1103,0,\N,Missing
D19-1632,D18-1399,0,\N,Missing
D19-1632,W18-6321,0,\N,Missing
D19-1632,P16-1009,0,\N,Missing
D19-1632,tiedemann-2012-parallel,0,\N,Missing
D19-1632,P13-1135,1,\N,Missing
D19-1632,W13-2305,0,\N,Missing
D19-1632,L18-1275,0,\N,Missing
D19-1679,Q17-1010,0,0.0557769,"in this paper, we simply took = 1. Spelling-Aware Extension ˜ · w+ E ˜ X n ˜ n ·w E ˜n 1 1· w ˜n (6) ˜ is the full-word embedding matrix and w where E ˜ is a one-hot vector associated with the word type ˜ n is a character n-gram embedding matrix w, E and w ˜ n is a multi-hot vector associated with all the character n-grams for the word type w. For each n, the summand gives the average embedding of all n-grams in w (where 1· w ˜ n counts these n-grams). We set n to range from 3 to 4 (see Appendix B). This formulation is similar to previous sub-word based embedding models (Wieting et al., 2016; Bojanowski et al., 2017). Similarly, the embedding of an L2 word w is parameterized as ˜ · w+ F ˜ X n ˜ n ·w F ˜n 1 1· w ˜n (7) ˜ n to µE ˜ n (where Crucially, we initialize F µ &gt; 0) so that L2 words can inherit part of their initial embedding from similarly spelled L1 words: ˜ 4 Afri := µE ˜ 4 Afri .2 But we allow F ˜ n to diverge F over time in case an n-gram functions differently in the two languages. In the same way, we initialize ˜ to the corresponding row of µ · E, ˜ each row of F if any, and otherwise to 0. Our experiments set µ = 0.2 (see Appendix B). We refer to this spelling-aware extension to GSM as sGSM."
D19-1679,P82-1020,0,0.747387,"Missing"
D19-1679,K16-1013,1,0.852077,"such as word order and fixed phrases, in this paper we only consider simple lexical substitutions. Our hope is that such a system can augment traditional foreign language instruction. As an example, consider a native speaker of English (learning German) presented with the following sentence: Der Nile is a Fluss in Afrika. With a little effort, one would hope the student could infer the meaning of the German words because there is sufficient contextual information and spelling information for the cognate Afrika. In our previous papers on foreign language teaching (Renduchintala et al., 2016b; Knowles et al., 2016; Renduchintala et al., 2017), we focused on fitting detailed models of students’ learning when the instructional stimuli (macaronic or otherwise) were chosen by a simple random or heuristic teaching policy. In the present paper, we flip the emphasis to choosing good instructional stimuli—machine teaching. This still requires a model of student learning. We employ a reasonable model that is not trained on any human students at all, but only on text that a generic student is presumed to have read. Thus, our model is not personalized, although it may be specialized to the domain of L1 text that"
D19-1679,P14-1053,0,0.0284583,"a recent workshop (Renduchintala et al., 2019): it experimented with three variants of the generic student model, using an artificial L2 language. In this paper, we extend the best of those models to consider an L2 word’s spelling (along with its context) when guessing its embeddings. We therefore conduct our experiments on real L2 languages (Spanish and German). 2 Related Work Our motivation is similar to that of commercially available prior systems such as Swych (2015) and OneThirdStories (2018) that also incorporate incidental learning within foreign language instruction. Other prior work (Labutov and Lipson, 2014; Renduchintala et al., 2016b) relied on building a model of the student’s incidental learning capabilities, using supervised data that was painfully collected by asking students to react to the actions of an initially untrained machine teacher. Our method, by contrast, constructs a generic student model from unannotated L1 text alone. This makes it possible for us to quickly create macaronic documents in any domain covered by that text corpus. 3 Method Our machine teacher can be viewed as a search algorithm that tries to find the (approximately) best macaronic configuration for the next sente"
D19-1679,L18-1008,0,0.0397456,"Missing"
D19-1679,E17-1096,0,0.0160009,"andidate macaronic configuration: Der Nile ist ein Fluss in Africa.1 Understanding may arise from inference on this sentence as well as whatever the student has learned about these words from previous sentences. The teacher makes this assessment by presenting this sentence to a generic student model (§§3.1–3.3). It uses a L2 embedding scoring scheme (§3.4) to guide a greedy search for the best macaronic configuration (§3.5). 3.1 Generic Student Model Our model of a “generic student” (GSM) is equipped with a cloze language model that uses a bidirectional LSTM to predict L1 words in L1 context (Mousa and Schuller, 2017; Hochreiter and Schmidhuber, 1997). Given a sentence x = [x1 ,...,xt ,...,xT ], the cloze model defines p(xt |hf t ,hb t ) 8t 2 {1,...,T }, where: hf t = LSTMf ([x1 ,...,xt 1 ];✓ f ) 2 RD (1) hb t = LSTMb ([xT ,...,xt+1 ];✓ b ) 2 RD (2) p(· |hf ,hb ) = softmax(E h([hf ;hb ]; ✓ h )) (3) are hidden states of forward and backward LSTM encoders parameterized by ✓ f and ✓ b respectively. The model assumes a fixed L1 vocabulary of size V , and the vectors xt above are embeddings of these word types, which correspond to the rows of an embedding matrix E 2 RV ⇥D . The cloze distribution at each posit"
D19-1679,E17-2025,0,0.0333356,".,xt+1 ];✓ b ) 2 RD (2) p(· |hf ,hb ) = softmax(E h([hf ;hb ]; ✓ h )) (3) are hidden states of forward and backward LSTM encoders parameterized by ✓ f and ✓ b respectively. The model assumes a fixed L1 vocabulary of size V , and the vectors xt above are embeddings of these word types, which correspond to the rows of an embedding matrix E 2 RV ⇥D . The cloze distribution at each position t in the sentence is obtained using where h(·;✓ h ) is a projection function that reduces the dimension of the concatenated hidden states from 2D to D. We “tie” the input embeddings and output embeddings as in Press and Wolf (2017). We train the parameters ✓ = [✓ f ; ✓ b ; ✓ h ; E] using P Adam (Kingma and Ba, 2014) to maximize x L(x), where the summation is over sentences x in a large L1 training corpus, and X L(x) = log p(xt |hf t ,hb t ) (4) t We set the dimensionality of word embeddings and LSTM hidden units to 300. We use the WikiText-103 corpus (Merity et al., 2016) as the L1 training corpus. We apply dropout (p = 0.2) between the word embeddings and LSTM layers, and between the LSTM and projection layers (Srivastava et al., 2014). We assume that the resulting model represents the entirety of the student’s L1 know"
D19-1679,P16-4023,1,0.909362,"rrounding context and spelling (Krashen, 1989). An initial “rough” understanding of a novel word might suffice for the reader to continue reading, with subsequent exposures refining their understanding of the novel word. Our goal is to design a machine teacher that uses a human reader’s incidental learning ability to teach foreign language (L2) vocabulary. The machine teacher’s modus operandi is to replace L1 words with their L2 glosses, which results in a macaronic document that mixes two languages in an effort to ease the human reader into understanding the L2. While some of our prior work (Renduchintala et al., 2016b,a) considered incorporating other features of the L2 such as word order and fixed phrases, in this paper we only consider simple lexical substitutions. Our hope is that such a system can augment traditional foreign language instruction. As an example, consider a native speaker of English (learning German) presented with the following sentence: Der Nile is a Fluss in Afrika. With a little effort, one would hope the student could infer the meaning of the German words because there is sufficient contextual information and spelling information for the cognate Afrika. In our previous papers on fo"
D19-1679,P16-1175,1,0.874373,"rrounding context and spelling (Krashen, 1989). An initial “rough” understanding of a novel word might suffice for the reader to continue reading, with subsequent exposures refining their understanding of the novel word. Our goal is to design a machine teacher that uses a human reader’s incidental learning ability to teach foreign language (L2) vocabulary. The machine teacher’s modus operandi is to replace L1 words with their L2 glosses, which results in a macaronic document that mixes two languages in an effort to ease the human reader into understanding the L2. While some of our prior work (Renduchintala et al., 2016b,a) considered incorporating other features of the L2 such as word order and fixed phrases, in this paper we only consider simple lexical substitutions. Our hope is that such a system can augment traditional foreign language instruction. As an example, consider a native speaker of English (learning German) presented with the following sentence: Der Nile is a Fluss in Afrika. With a little effort, one would hope the student could infer the meaning of the German words because there is sufficient contextual information and spelling information for the cognate Afrika. In our previous papers on fo"
D19-1679,K17-1025,1,0.862213,"d fixed phrases, in this paper we only consider simple lexical substitutions. Our hope is that such a system can augment traditional foreign language instruction. As an example, consider a native speaker of English (learning German) presented with the following sentence: Der Nile is a Fluss in Afrika. With a little effort, one would hope the student could infer the meaning of the German words because there is sufficient contextual information and spelling information for the cognate Afrika. In our previous papers on foreign language teaching (Renduchintala et al., 2016b; Knowles et al., 2016; Renduchintala et al., 2017), we focused on fitting detailed models of students’ learning when the instructional stimuli (macaronic or otherwise) were chosen by a simple random or heuristic teaching policy. In the present paper, we flip the emphasis to choosing good instructional stimuli—machine teaching. This still requires a model of student learning. We employ a reasonable model that is not trained on any human students at all, but only on text that a generic student is presumed to have read. Thus, our model is not personalized, although it may be specialized to the domain of L1 text that it was initially trained on."
D19-1679,W19-4439,1,0.537773,"Nile Nile Nil ist is ist a a ein river Fluss river in in in Africa Africa Africa Table 1: An example English (L1) sentence with German (L2) glosses. Using the glosses, many possible macaronic configurations are possible. Note that the gloss sequence is not a fluent L2 sentence. macaronic sentences shown to the student. Our teacher does not yet attempt to monitor the human student’s actual learning. Still, we show that it is useful to a beginner student and far less frustrating than a random (or heuristic based) alternative. A “pilot” version of the present paper appeared at a recent workshop (Renduchintala et al., 2019): it experimented with three variants of the generic student model, using an artificial L2 language. In this paper, we extend the best of those models to consider an L2 word’s spelling (along with its context) when guessing its embeddings. We therefore conduct our experiments on real L2 languages (Spanish and German). 2 Related Work Our motivation is similar to that of commercially available prior systems such as Swych (2015) and OneThirdStories (2018) that also incorporate incidental learning within foreign language instruction. Other prior work (Labutov and Lipson, 2014; Renduchintala et al."
D19-1679,P16-1174,0,0.0470856,"Missing"
D19-1679,D16-1157,0,0.0274375,"riments. In practice, in this paper, we simply took = 1. Spelling-Aware Extension ˜ · w+ E ˜ X n ˜ n ·w E ˜n 1 1· w ˜n (6) ˜ is the full-word embedding matrix and w where E ˜ is a one-hot vector associated with the word type ˜ n is a character n-gram embedding matrix w, E and w ˜ n is a multi-hot vector associated with all the character n-grams for the word type w. For each n, the summand gives the average embedding of all n-grams in w (where 1· w ˜ n counts these n-grams). We set n to range from 3 to 4 (see Appendix B). This formulation is similar to previous sub-word based embedding models (Wieting et al., 2016; Bojanowski et al., 2017). Similarly, the embedding of an L2 word w is parameterized as ˜ · w+ F ˜ X n ˜ n ·w F ˜n 1 1· w ˜n (7) ˜ n to µE ˜ n (where Crucially, we initialize F µ &gt; 0) so that L2 words can inherit part of their initial embedding from similarly spelled L1 words: ˜ 4 Afri := µE ˜ 4 Afri .2 But we allow F ˜ n to diverge F over time in case an n-gram functions differently in the two languages. In the same way, we initialize ˜ to the corresponding row of µ · E, ˜ each row of F if any, and otherwise to 0. Our experiments set µ = 0.2 (see Appendix B). We refer to this spelling-aware"
E03-1076,A00-1031,0,0.00604947,"ences, and therefore taken as evidence for the existence of a translation for den. Another example for this is the word Voraussetzung (English: condition), which is split into vor and aussetzung. The word vor translates to many different prepositions, which frequently occur in English. To exclude these mistakes, we use information about the parts-of-speech of words. We do not want to break up a compound into parts that are prepositions or determiners, but only content words: nouns, adverbs, adjectives, and verbs. To accomplish this, we tag the German corpus with POS tags using the TnT tagger [Brants, 2000]. We then obtain statistics on the parts-ofspeech of words in the corpus. This allows us to exclude words based on their POS as possible parts of compounds. We limit possible parts of compounds to words that occur most of the time as one of following POS: ADJA, ADJD, ADV, NN, NE, PTKNEG, VVFIN, VVIMP, VVINF, VVIZU, VVPP, VAFIN, VAIMP, VAINF, VAPP, VMFIN, VMINF, VMPP. 7 Evaluation The training set for the experiments is a corpus of 650,000 noun phrases and prepositional phrases (NP/PP). For each German NP/PP, we have a English translation. This data was extracted from the Europarl corpus [Koeh"
E03-1076,J90-2002,0,0.292355,"not 191 Method raw eager frequency based using parallel using parallel and POS BLEU 0.291 0.222 0.317 0.294 0.306 Table 2: Evaluation of the methods with a word based statistical machine translation system (IBM Model 4). Frequency based splitting is best, the methods using splitting knowledge from a parallel corpus also improve over unsplit (raw) data. systems. Hence, we use the splitting methods to prepare training and testing data to optimize the performance of such systems. First, we measured the impact on a word based statistical machine translation system, the widely studied IBM Model 4 [Brown et al., 1990], for which training tools [Al-Onaizan et al., 19991 and decoders [Germann et al., 2001] are freely available. We trained the system on the 650,000 NP/PPs with the Giza toolkit, and evaluated the translation quality on the same 1000 NP/PP test set as in the previous section. Training and testing data was split consistently in the same way. The translation accuracy is measured against reference translations using the BLEU score [Papineni et al., 2002]. Table 2 displays the results. Somewhat surprisingly, the frequency based method leads to better translation quality than the more accurate meth"
E03-1076,2002.tmi-papers.3,0,0.291402,"Missing"
E03-1076,W02-1018,0,0.0104247,"o that a one-to-one correspondence to English can be established. Note that we are looking for a one-to-one correspondence to English content words: Say, the preferred translation of Aktionsplan is plan for action. The lack of correspondence for the English word for does not detract from the definition of the task: We would still like to break up the German compound into the two parts Aktion and Plan. The insertion of function words is not our concern. Ultimately, the purpose of this work is to improve the quality of machine translation systems. For instance, phrase-based translation systems [Marcu and Wong, 2002] may recover more easily from splitting regimes that do not create a one-to-one translation correspondence. One splitting method may mistakenly break up the word Aktionsplan into the three words Akt, Ion, and Plan. But if we consistently break up the word Aktion into Akt and Ion in our training data, such a 187 system will likely learn the translation of the word pair Akt Ion into the single English word action. These considerations lead us to three different objectives and therefore three different evaluation metrics for the task of compound splitting: • One-to-One correspondence • Translati"
E03-1076,P02-1040,0,0.103409,"e performance of such systems. First, we measured the impact on a word based statistical machine translation system, the widely studied IBM Model 4 [Brown et al., 1990], for which training tools [Al-Onaizan et al., 19991 and decoders [Germann et al., 2001] are freely available. We trained the system on the 650,000 NP/PPs with the Giza toolkit, and evaluated the translation quality on the same 1000 NP/PP test set as in the previous section. Training and testing data was split consistently in the same way. The translation accuracy is measured against reference translations using the BLEU score [Papineni et al., 2002]. Table 2 displays the results. Somewhat surprisingly, the frequency based method leads to better translation quality than the more accurate methods that take advantage from knowledge from the parallel corpus. One reason for this is that the system recovers more easily from words that are split too much than from words that are not split up sufficiently. Of course, this has limitations: Eager splitting into as many parts as possible fares abysmally. 7.3 Translation Quality with Phrase Based Machine Translation Compound words violate the bias for one-to-one word correspondences of word based S"
E03-1076,P01-1030,1,0.26186,"291 0.222 0.317 0.294 0.306 Table 2: Evaluation of the methods with a word based statistical machine translation system (IBM Model 4). Frequency based splitting is best, the methods using splitting knowledge from a parallel corpus also improve over unsplit (raw) data. systems. Hence, we use the splitting methods to prepare training and testing data to optimize the performance of such systems. First, we measured the impact on a word based statistical machine translation system, the widely studied IBM Model 4 [Brown et al., 1990], for which training tools [Al-Onaizan et al., 19991 and decoders [Germann et al., 2001] are freely available. We trained the system on the 650,000 NP/PPs with the Giza toolkit, and evaluated the translation quality on the same 1000 NP/PP test set as in the previous section. Training and testing data was split consistently in the same way. The translation accuracy is measured against reference translations using the BLEU score [Papineni et al., 2002]. Table 2 displays the results. Somewhat surprisingly, the frequency based method leads to better translation quality than the more accurate methods that take advantage from knowledge from the parallel corpus. One reason for this is"
E03-1076,W01-0504,1,0.432753,"akt—ion—plan We arrive at these splitting options, since all the parts — aktionsplan, aktions, aktion, akt, ion, and plan — have been observed as whole words in the training corpus. These splitting options are the basis of our work. In the following we discuss methods that pick one of them as the correct splitting of the compound. 4 Frequency Based Metric 5 Guidance from a Parallel Corpus The more frequent a word occurs in a training corpus, the bigger the statistical basis to estimate translation probabilities, and the more likely the correct translation probability distribution is learned [Koehn and Knight, 20011. This insight leads us to define a splitting metric based on word frequency. Given the count of words in the corpus, we pick the split S with the highest geometric mean of word frequencies of its parts pi (n being the number of parts): As stated earlier, one of our objectives is the splitting of compounds into parts that have one-to-one correspondence to English. One source of information about word correspondence is a parallel corpus: text in a foreign language, accompanied by translations into English. Usually, such a corpus is provided in form of sentence translation pairs. Going through"
E06-1032,P04-1079,0,0.00865044,"rominent factors contribute to Bleu’s crudeness: • Synonyms and paraphrases are only handled if they are in the set of multiple reference translations. • The scores for words are equally weighted so missing out on content-bearing material brings no additional penalty. was being led to the |calm as he was | would take |carry him |seemed quite | when |taken would receive the same Bleu score as the hypothesis translation from Table 1, even though human judges would assign it a much lower score. This problem is made worse by the fact that Bleu equally weights all items in the reference sentences (Babych and Hartley, 2004). Therefore omitting content-bearing lexical items does • The brevity penalty is a stop-gap measure to compensate for the fairly serious problem of not being able to calculate recall. Each of these failures contributes to an increased amount of inappropriately indistinguishable translations in the analysis presented above. Given that Bleu can theoretically assign equal scoring to translations of obvious different quality, it is logical that a higher Bleu score may not 252 Fluency How do you judge the fluency of this translation? 5 = Flawless English 4 = Good English 3 = Non-native English 2 ="
E06-1032,W05-0909,0,0.108748,"to inappropriate phrase movement by matching part-of-speech tag sequences against reference translations in addition to Bleu’s n-gram matches. Babych and Hartley (2004) extend Bleu by adding frequency weighting to lexical items through TF/IDF as a way of placing greater emphasis on content-bearing words and phrases. Two alternative automatic translation evaluation metrics do a much better job at incorporating recall than Bleu does. Melamed et al. (2003) formulate a metric which measures translation accuracy in terms of precision and recall directly rather than precision and a brevity penalty. Banerjee and Lavie (2005) introduce the Meteor metric, which also incorporates recall on the unigram level and further provides facilities incorporating stemming, and WordNet synonyms as a more flexible match. Lin and Hovy (2003) as well as Soricut and Brill (2004) present ways of extending the notion of ngram co-occurrence statistics over multiple references, such as those used in Bleu, to other natural language generation tasks such as summarization. Both these approaches potentially suffer from the same weaknesses that Bleu has in machine translation evaluation. Conclusions In this paper we have shown theoretical a"
E06-1032,P05-1048,0,0.0875749,"Missing"
E06-1032,2003.mtsummit-papers.6,0,0.0277887,"Missing"
E06-1032,2003.mtsummit-papers.9,0,0.335611,"ric for the purposes of evaluating incremental system changes and optimizing systems through minimum error rate training (Och, 2003). Conference papers routinely claim improvements in translation quality by reporting improved Bleu scores, while neglecting to show any actual example translations. Workshops commonly compare systems using Bleu scores, often without confirming these rankings through manual evaluation. All these uses of Bleu are predicated on the assumption that it correlates with human judgments of translation quality, which has been shown to hold in many cases (Doddington, 2002; Coughlin, 2003). However, there is a question as to whether minimizing the error rate with respect to Bleu does indeed guarantee genuine translation improvements. If Bleu’s correlation with human judgments has been overestimated, then the field needs to ask itself whether it should continue to be driven by Bleu to the extent that it currently is. In this paper we give a number of counterexamples for Bleu’s correlation with human judgments. We show that under some circumstances an improvement in Bleu is not sufficient to reflect a genuine improvement in translation quality, and in other circumstances that it"
E06-1032,koen-2004-pharaoh,0,0.026394,"and seeing whether Bleu cor0.4 0.42 0.44 0.46 0.48 0.5 0.52 Bleu Score Figure 3: Bleu scores plotted against human judgments of fluency, with R2 = 0.002 when the outlier entry is included rectly ranked the systems. We used Systran for the rule-based system, and used the French-English portion of the Europarl corpus (Koehn, 2005) to train the SMT systems and to evaluate all three systems. We built the first phrase-based SMT system with the complete set of Europarl data (1415 million words per language), and optimized its feature functions using minimum error rate training in the standard way (Koehn, 2004). We evaluated it and the Systran system with Bleu using a set of 2,000 held out sentence pairs, using the same normalization and tokenization schemes on both systems’ output. We then built a number of SMT systems with various portions of the training 1 corpus, and selected one that was trained with 64 of the data, which had a Bleu score that was close to, but still higher than that for the rule-based system. We then performed a manual evaluation where we had three judges assign fluency and adequacy ratings for the English translations of 300 French sentences for each of the three systems. The"
E06-1032,2005.mtsummit-papers.11,1,0.0613044,"tput which has differing characteristics, and might end up in different regions of the human scores / Bleu score graph. We investigated this by performing a manual evaluation comparing the output of two statistical machine translation systems with a rule-based machine translation, and seeing whether Bleu cor0.4 0.42 0.44 0.46 0.48 0.5 0.52 Bleu Score Figure 3: Bleu scores plotted against human judgments of fluency, with R2 = 0.002 when the outlier entry is included rectly ranked the systems. We used Systran for the rule-based system, and used the French-English portion of the Europarl corpus (Koehn, 2005) to train the SMT systems and to evaluate all three systems. We built the first phrase-based SMT system with the complete set of Europarl data (1415 million words per language), and optimized its feature functions using minimum error rate training in the standard way (Koehn, 2004). We evaluated it and the Systran system with Bleu using a set of 2,000 held out sentence pairs, using the same normalization and tokenization schemes on both systems’ output. We then built a number of SMT systems with various portions of the training 1 corpus, and selected one that was trained with 64 of the data, wh"
E06-1032,N03-1020,0,0.24596,"to lexical items through TF/IDF as a way of placing greater emphasis on content-bearing words and phrases. Two alternative automatic translation evaluation metrics do a much better job at incorporating recall than Bleu does. Melamed et al. (2003) formulate a metric which measures translation accuracy in terms of precision and recall directly rather than precision and a brevity penalty. Banerjee and Lavie (2005) introduce the Meteor metric, which also incorporates recall on the unigram level and further provides facilities incorporating stemming, and WordNet synonyms as a more flexible match. Lin and Hovy (2003) as well as Soricut and Brill (2004) present ways of extending the notion of ngram co-occurrence statistics over multiple references, such as those used in Bleu, to other natural language generation tasks such as summarization. Both these approaches potentially suffer from the same weaknesses that Bleu has in machine translation evaluation. Conclusions In this paper we have shown theoretical and practical evidence that Bleu may not correlate with human judgment to the degree that it is currently believed to do. We have shown that Bleu’s rather coarse model of allowable variation in translation"
E06-1032,N03-2021,0,0.459458,"s to an arithmetic average, and calculating the brevity penalty in a slightly different manner. Hovy and Ravichandra (2003) suggested increasing Bleu’s sensitivity to inappropriate phrase movement by matching part-of-speech tag sequences against reference translations in addition to Bleu’s n-gram matches. Babych and Hartley (2004) extend Bleu by adding frequency weighting to lexical items through TF/IDF as a way of placing greater emphasis on content-bearing words and phrases. Two alternative automatic translation evaluation metrics do a much better job at incorporating recall than Bleu does. Melamed et al. (2003) formulate a metric which measures translation accuracy in terms of precision and recall directly rather than precision and a brevity penalty. Banerjee and Lavie (2005) introduce the Meteor metric, which also incorporates recall on the unigram level and further provides facilities incorporating stemming, and WordNet synonyms as a more flexible match. Lin and Hovy (2003) as well as Soricut and Brill (2004) present ways of extending the notion of ngram co-occurrence statistics over multiple references, such as those used in Bleu, to other natural language generation tasks such as summarization."
E06-1032,P02-1038,0,0.108484,"n that it was not fully automatic machine translation; instead the entry was aided by monolingual English speakers selecting among alternative automatic translations of phrases in the Arabic source sentences and post-editing the result (Callison-Burch, 2005). The remaining six entries were all fully automatic machine translation systems; in fact, they were all phrase-based statistical machine translation system that had been trained on the same parallel corpus and most used Bleubased minimum error rate training (Och, 2003) to optimize the weights of their log linear models’ feature functions (Och and Ney, 2002). This opens the possibility that in order for Bleu to be valid only sufficiently similar systems should be compared with one another. For instance, when measuring correlation using Pearson’s we get a very low correlation of R2 = 0.14 when the outlier in Figure 2 is included, but a strong R2 = 0.87 when it is excluded. Similarly Figure 3 goes from R2 = 0.002 to a much stronger R2 = 0.742. Systems which explore different areas of translation space may produce output which has differing characteristics, and might end up in different regions of the human scores / Bleu score graph. We investigated"
E06-1032,P03-1021,0,0.33672,"s of quality. This offers new potential for research which was previously deemed unpromising by an inability to improve upon Bleu scores. 1 Introduction Over the past five years progress in machine translation, and to a lesser extent progress in natural language generation tasks such as summarization, has been driven by optimizing against n-grambased evaluation metrics such as Bleu (Papineni et al., 2002). The statistical machine translation community relies on the Bleu metric for the purposes of evaluating incremental system changes and optimizing systems through minimum error rate training (Och, 2003). Conference papers routinely claim improvements in translation quality by reporting improved Bleu scores, while neglecting to show any actual example translations. Workshops commonly compare systems using Bleu scores, often without confirming these rankings through manual evaluation. All these uses of Bleu are predicated on the assumption that it correlates with human judgments of translation quality, which has been shown to hold in many cases (Doddington, 2002; Coughlin, 2003). However, there is a question as to whether minimizing the error rate with respect to Bleu does indeed guarantee gen"
E06-1032,P02-1040,0,0.129827,"We show that an improved Bleu score is neither necessary nor sufficient for achieving an actual improvement in translation quality, and give two significant counterexamples to Bleu’s correlation with human judgments of quality. This offers new potential for research which was previously deemed unpromising by an inability to improve upon Bleu scores. 1 Introduction Over the past five years progress in machine translation, and to a lesser extent progress in natural language generation tasks such as summarization, has been driven by optimizing against n-grambased evaluation metrics such as Bleu (Papineni et al., 2002). The statistical machine translation community relies on the Bleu metric for the purposes of evaluating incremental system changes and optimizing systems through minimum error rate training (Och, 2003). Conference papers routinely claim improvements in translation quality by reporting improved Bleu scores, while neglecting to show any actual example translations. Workshops commonly compare systems using Bleu scores, often without confirming these rankings through manual evaluation. All these uses of Bleu are predicated on the assumption that it correlates with human judgments of translation q"
E06-1032,P04-1078,0,0.0207213,"F as a way of placing greater emphasis on content-bearing words and phrases. Two alternative automatic translation evaluation metrics do a much better job at incorporating recall than Bleu does. Melamed et al. (2003) formulate a metric which measures translation accuracy in terms of precision and recall directly rather than precision and a brevity penalty. Banerjee and Lavie (2005) introduce the Meteor metric, which also incorporates recall on the unigram level and further provides facilities incorporating stemming, and WordNet synonyms as a more flexible match. Lin and Hovy (2003) as well as Soricut and Brill (2004) present ways of extending the notion of ngram co-occurrence statistics over multiple references, such as those used in Bleu, to other natural language generation tasks such as summarization. Both these approaches potentially suffer from the same weaknesses that Bleu has in machine translation evaluation. Conclusions In this paper we have shown theoretical and practical evidence that Bleu may not correlate with human judgment to the degree that it is currently believed to do. We have shown that Bleu’s rather coarse model of allowable variation in translation can mean that an improved Bleu scor"
E06-1032,N04-1021,0,\N,Missing
E09-1043,W06-1609,0,0.0509279,"Missing"
E09-1043,2005.mtsummit-papers.11,1,0.0740764,"Missing"
E09-1043,D07-1091,1,0.910719,"ttice to the translation stage. The ATS models do provide an integrated approach, but their lexical translation is → ADJ CONJ ADJ NOUN is typically observed many times in the training corpus. The alignment information in the training corpus shows exactly how the individual words in this phrase should be distorted, along with the POS tag of the target words. The challenge addressed by this paper is to integrate POS tag phrase translations and alignment information into a phrasebased decoder in order to improve reordering. 2.4 Related Work Factor Model Decomposition Factored translation models (Koehn and Hoang, 2007) extend the phrase-based model by integrating word level factors into the decoding process. Words are represented by vectors of factors, not simple tokens. Factors are user-definable and do not have any specific meaning within the model. Typically, factors are obtained from linguistic tools such as taggers and parsers. The factored decoding process can be decomposed into multiple steps to fully translate the input. Formally, this decomposes Equation 4 further into sub-component models (also called translation steps) X ¯ TM (t¯, s¯) = ¯ i (t¯, s¯) h h (5) TM i ¯ i for each with an translation f"
E09-1043,P07-2045,1,0.0244934,"Missing"
E09-1043,N03-1017,1,0.0221309,"the language model backs off to lower order n-grams, thus further reducing the context window. (2) where h0m (t, s) is the feature function for component m and λm is the weight given to component m. Z is a normalization factor which is ignored in practice. Components are translation model scoring functions, language model, reordering models and other features. The problem is typically presented in log-space, which simplifies computations, but otherwise does not change the problem due to the monotonicity of the log function (hm = log h0m ) X log p(t|s) = λm hm (t, s) (3) m Phrase-based models (Koehn et al., 2003) are limited to the mapping of small contiguous chunks of text. In these models, the source sentence s is segmented into a number of phrases s¯k , which are translated one-to-one into target phrases t¯k . The translation feature functions hTM (t, s) are computed as sum of phrase translation feature func¯ TM (t¯k , s¯k ): tions h X ¯ TM (t¯k , s¯k ) hTM (t, s) = h (4) 2.3 This paper will look at the use of POS tags to condition reordering of phrases which are closely positioned in the source and target, such as intraclausal reordering, however, we do not explicit segment along clausal boundarie"
E09-1043,J03-1002,0,0.00451222,"ags for both source and target languages were augmented to the training corpus and used in the decoding and an additional trigram language 3 in-domain 18.2 18.8 18.8 Table 2: French–English results We performed our experiments on the news commentary corpus2 which contains 60,000 parallel sentences for German–English and 43,000 sentences for French–English. Tuning was done on a 2000 sentence subset of the Europarl corpus (Koehn, 2005) and tested on a 2000 sentence Europarl subset for out-of-domain, and a 1064 news commentary sentences for in-domain. The training corpus is aligned using Giza++ (Och and Ney, 2003). To create POS tag translation models, the surface forms on both source and target language training data are replaced with POS tags before phrases are extracted. The taggers used were the Brill Tagger (Brill, 1995) for English, the Treetagger for French (Schmid, 1994), and the LoPar Tagger (Schmidt and Schulte im Walde, 2000) for German. The training script supplied with the Moses toolkit (Koehn et al., 2007) was used, extended to enable alignment information of each phrase pair. The vanilla Moses MERT tuning script was used throughout. Results are also presented for models trained on the la"
E09-1043,2007.tmi-papers.20,0,0.0363963,"ce and target sentences to be augmented with factors, while 374 where J is the set of positions, jJ , that I is aligned to in the other language. Phrase pairs for each translation model are used only if they can satisfy condition 9 for each position of every source word covered. limited to the word level. In contrast to prior work, we present a integrated approach that allows POS-based reordering and phrase translation. It is also open to the use of any other factors, such as driving reordering with automatic word classes. Our proposed solution is similar to structural templates described in Phillips (2007) which was applied to an example-based MT system. 4 ∀a, b  T ∀p : Jap Jbp 6= ∅ where Jap is the alignment information for translation model, a, at word position, p and T is the set of translation models. Translation Using Templates of Factors A major motivation for the introduction of factors into machine translation is to generalize phrase translation over longer segments using less sparse factors than is possible with surface forms. (Koehn and Hoang, 2007) describes various strategies for the decomposition of the decoding into multiple translation models using the Moses decoder. We shall fo"
E09-1043,C00-2105,0,0.0575949,"Missing"
E09-1043,N04-4026,0,0.634099,"eordering in Phrase Models Phrase-based systems implicitly perform shortrange reordering by translating multi-word phrases where the component words may be reordered relative to each other. However, multiword phrases have to have been seen and learnt from the training corpus. This works better when the parallel corpus is large and the training corpus and input are from the same domain. Otherwise, the ability to apply multi-word phrases is lessened due to data sparsity, and therefore most used phrases are only 1 or 2 words long. A popular model for phrasal reordering is lexicalized reordering (Tillmann, 2004) which introduces a probability distribution for each phrase pair that indicates the likelihood of being translated monotone, swapped, or placed discontinuous to its previous phrase. However, whether a Union Europ´eenne → European Union However, phrase-based models may not reorder even these small two-word phrases if the phrase is not in the training data or involves rare words. This situation worsens for longer phrases where the likelihood of the phrase being previously un373 at the same time controlling data sparsity. However, decomposition also implies certain independence assumptions which"
E09-1043,C04-1073,0,0.131273,"n this particular phrase, it contains many similar phrases with the same underlying POS tags. For example, the correct translation of the corresponding POS tags of the above translation 3 NOUN ADJ CONJ ADJ Efforts have been made to integrate syntactic information into the decoding process to improve reordering. Collins et al. (2005) reorder the source sentence using a sequence of six manually-crafted rules, given the syntactic parse tree of the source sentence. While the transformation rules are specific to the German parser that was used, they could be adapted to other languages and parsers. Xia and McCord (2004) automatically create rewrite rules which reorder the source sentence. Zhang and Zens (2007) take a slightly different approach by using chunk level tags to reorder the source sentence, creating a confusion network to represent the possible reorderings of the source sentence. All these approaches seek to improve reordering by making the ordering of the source sentence similar to the target sentence. Costa-juss`a and Fonollosa (2006) use a two stage process to reorder translation in an n-gram based decoder. The first stage uses word classes of source words to reorder the source sentence into a"
E09-1043,2007.iwslt-1.3,0,0.0442125,". For example, the correct translation of the corresponding POS tags of the above translation 3 NOUN ADJ CONJ ADJ Efforts have been made to integrate syntactic information into the decoding process to improve reordering. Collins et al. (2005) reorder the source sentence using a sequence of six manually-crafted rules, given the syntactic parse tree of the source sentence. While the transformation rules are specific to the German parser that was used, they could be adapted to other languages and parsers. Xia and McCord (2004) automatically create rewrite rules which reorder the source sentence. Zhang and Zens (2007) take a slightly different approach by using chunk level tags to reorder the source sentence, creating a confusion network to represent the possible reorderings of the source sentence. All these approaches seek to improve reordering by making the ordering of the source sentence similar to the target sentence. Costa-juss`a and Fonollosa (2006) use a two stage process to reorder translation in an n-gram based decoder. The first stage uses word classes of source words to reorder the source sentence into a string of word classes which can be translated monotonically to the target sentences in the"
E09-1043,J95-4004,0,0.0372016,"Missing"
E09-1043,P05-1033,0,0.0776525,"istical machine translation is reordering due to systematic wordordering differences between languages. Often reordering is best explained by linguistic categories, such as part-of-speech tags. In fact, prior work has examined the use of part-of-speech tags in pre-reordering schemes, Tomas and Casacuberta (2003). Re-ordering can also be viewed as composing of a number of related problems which can be explained or solved by a variety of linguistic phenomena. Firstly, differences between phrase ordering account for much of the long-range reordering. Syntax-based and hierarchical models such as (Chiang, 2005) attempts to address this problem. Shorter range re-ordering, such as intraphrasal word re-ordering, can often be predicted from the underlying property of the words and its context, the most obvious property being POS tags. In this paper, we tackle the issue of shorterrange re-ordering in phrase-based decoding by presenting an extension of the factored translation which directly models the translation of nonsurface factors such as POS tags. We shall call this 2 Background Let us first provide some background on phrasebased and factored translation, as well as the use of part-of-speech tags in"
E09-1043,P05-1066,1,0.848304,"r. The following example has a source POS pattern NOUN ADJECTIVE CONJUNCTION ADJECTIVE but is incorrectly ordered as the surface phrase does not occur in training, difficult´es e´ conomiques et socials → economic and social difficulties However, even if the training data does not contain this particular phrase, it contains many similar phrases with the same underlying POS tags. For example, the correct translation of the corresponding POS tags of the above translation 3 NOUN ADJ CONJ ADJ Efforts have been made to integrate syntactic information into the decoding process to improve reordering. Collins et al. (2005) reorder the source sentence using a sequence of six manually-crafted rules, given the syntactic parse tree of the source sentence. While the transformation rules are specific to the German parser that was used, they could be adapted to other languages and parsers. Xia and McCord (2004) automatically create rewrite rules which reorder the source sentence. Zhang and Zens (2007) take a slightly different approach by using chunk level tags to reorder the source sentence, creating a confusion network to represent the possible reorderings of the source sentence. All these approaches seek to improve"
E09-1082,E06-1005,0,0.300582,"nnot make good use of the full diversity of the translations. In this paper we present M AX scores as a baseline for output selection, and approximate an oracle using the B LEU metric as an upper bound for the output selection technique. 2.2  big insert dog dog – barked barked shift very  delete loudly loudly – Table 1: Example minimum TER edit script. 0 the a 1 ε big 2 dog 3 barked 4 very ε 5 loudly 6 Figure 1: Conversion of TER script from Table 1 to a confusion network. improvements in translation quality by performing multi-source translation using generic system combination techniques (Matusov et al., 2006; Paulik et al., 2007). One class of approaches to consensus decoding focuses on construction of a confusion network or lattice1 from translation outputs, from which new sentences can be created using different reorderings or combinations of translation fragments (e.g., Bangalore et al. (2001); Rosti et al. (2007b)). These methods differ in the types of lattices used, their means of creation, and scoring method used to extract the best consensus output from the lattice. The system used in this paper is a variant of the one proposed in Rosti et al. (2007a), which we now describe in detail. The"
E09-1082,2001.mtsummit-papers.46,0,0.790458,"m {jschroe1, tcohn, pkoehn}@inf.ed.ac.uk Abstract In this paper, we present three models of multisource translation, with increasing degrees of sophistication, which we compare empirically on a number of different corpora. We generalize the definition of multi-source translation to include any translation case with multiple inputs and a single output, allowing for, e.g., multiple paraphrased inputs in a single language. Our methods include simple output selection, which treats the multisource translation task as many independent translation steps followed by selection of one of their outputs (Och and Ney, 2001), and output combination, which uses consensus decoding to construct a string from n-gram fragments of the translation outputs (Bangalore et al., 2001). We also present a novel method, input combination, in which we compile the input texts into a compact lattice, over which we perform a single decoding pass. We show that as we add additional inputs, the simplest output selection method performs quite poorly relative to a single input translation system, while the latter two methods are able to make better use of the additional inputs. Multi-source statistical machine translation is the process"
E09-1082,N03-1003,0,0.0516823,"h. The model parameters are λ1 , . . . , λn , ν, µ, ξ, which are trained using Powell’s search to maximise the B LEU score for the highest scoring path, arg maxP w(P). 2.3 1 ε Figure 2: Structure of a lattice of confusion networks for consensus decoding. d∈P watch it thief purse ε Input Combination Loosely defined, input combination refers to finding a compact single representation of N translation inputs. The hope is that the new input preserves as many of the salient differences between the inputs as possible, while eliminating redundant information. Lattices are well suited to this task. 2 Barzilay and Lee (2003) construct lattices over paraphrases using an iterative pairwise multiple sequence alignment (MSA) algorithm. Unlike our approach, MSA does not allow reordering of inputs. 721 ¿ podría darnos las cifras correspondientes a españa y grecia ? bination. We take a similar approach for multilingual lattice generation. Our process consists of four steps: (i) Align words for each of the N (N − 1) pairs of inputs; (ii) choose an input (or many inputs) to be the lattice skeleton; (iii) extract all minimal consistent alignments between the skeleton and the other inputs; and (iv) add links to the lattice"
E09-1082,J03-1002,0,0.00882982,"Missing"
E09-1082,W07-0727,0,0.0449446,"Missing"
E09-1082,D08-1078,1,0.687833,"The oracle selection order differs from the order of the best performing systems, which could be due to the high scoring systems having very similar output while lower scoring systems exhibit greater diversity. Interestingly, the order of the languages chosen iterates between the Roman and Germanic language families and includes Greek early on. This supports our claim that diversity is important. Note though that Finnish, which is also in a separate language family, is selected last, most likely due to difficulties in word alignment and translation stemming from its morphological complexity (Birch et al., 2008). This finding might also carry over to phrase-table triangulation (Cohn and Lapata, 2007), where multi-parallel data is used in training to augment a standard translation Lattice Inputs As described in Section 2.3, lattices can be used to provide a compact format for translating multilingual inputs to a multi-source translation system. We trim all non-skeleton node paths to a maximum length of four to reduce complexity when decoding. Such long paths are mostly a result of errors in the original word alignments, and therefore pruning these links is largely innocuous. We train on the Europarl c"
E09-1082,N06-1002,0,0.0352643,"Missing"
E09-1082,P07-1092,1,0.554816,"h could be due to the high scoring systems having very similar output while lower scoring systems exhibit greater diversity. Interestingly, the order of the languages chosen iterates between the Roman and Germanic language families and includes Greek early on. This supports our claim that diversity is important. Note though that Finnish, which is also in a separate language family, is selected last, most likely due to difficulties in word alignment and translation stemming from its morphological complexity (Birch et al., 2008). This finding might also carry over to phrase-table triangulation (Cohn and Lapata, 2007), where multi-parallel data is used in training to augment a standard translation Lattice Inputs As described in Section 2.3, lattices can be used to provide a compact format for translating multilingual inputs to a multi-source translation system. We trim all non-skeleton node paths to a maximum length of four to reduce complexity when decoding. Such long paths are mostly a result of errors in the original word alignments, and therefore pruning these links is largely innocuous. We train on the Europarl corpus and use the 725 Approach test2006 test2007 French Only 29.72 30.21 French + Swedish"
E09-1082,P07-1040,0,0.0911201,"able 1: Example minimum TER edit script. 0 the a 1 ε big 2 dog 3 barked 4 very ε 5 loudly 6 Figure 1: Conversion of TER script from Table 1 to a confusion network. improvements in translation quality by performing multi-source translation using generic system combination techniques (Matusov et al., 2006; Paulik et al., 2007). One class of approaches to consensus decoding focuses on construction of a confusion network or lattice1 from translation outputs, from which new sentences can be created using different reorderings or combinations of translation fragments (e.g., Bangalore et al. (2001); Rosti et al. (2007b)). These methods differ in the types of lattices used, their means of creation, and scoring method used to extract the best consensus output from the lattice. The system used in this paper is a variant of the one proposed in Rosti et al. (2007a), which we now describe in detail. The first step in forming a lattice is to align the inputs. Consensus decoding systems often use the script of edit operations that minimises the translation edit rate (TER; Snover et al. (2006)). TER is a word-based measure of edit distance which also allows n-gram shifts when calculating the best match between a hy"
E09-1082,W07-0720,0,0.0373821,"Missing"
E09-1082,N07-1029,0,0.120249,"able 1: Example minimum TER edit script. 0 the a 1 ε big 2 dog 3 barked 4 very ε 5 loudly 6 Figure 1: Conversion of TER script from Table 1 to a confusion network. improvements in translation quality by performing multi-source translation using generic system combination techniques (Matusov et al., 2006; Paulik et al., 2007). One class of approaches to consensus decoding focuses on construction of a confusion network or lattice1 from translation outputs, from which new sentences can be created using different reorderings or combinations of translation fragments (e.g., Bangalore et al. (2001); Rosti et al. (2007b)). These methods differ in the types of lattices used, their means of creation, and scoring method used to extract the best consensus output from the lattice. The system used in this paper is a variant of the one proposed in Rosti et al. (2007a), which we now describe in detail. The first step in forming a lattice is to align the inputs. Consensus decoding systems often use the script of edit operations that minimises the translation edit rate (TER; Snover et al. (2006)). TER is a word-based measure of edit distance which also allows n-gram shifts when calculating the best match between a hy"
E09-1082,P08-1115,0,0.183992,"a language model. Formally, the path score is given by: When translating speech recognition output, previous work has shown that representing the ambiguity in the recognized text via confusion networks leads to better translations than simply translating the single best hypothesis of the speech recognition system (Bertoldi et al., 2007). The application of input lattices to other forms of input ambiguity has been limited to encoding input reorderings, word segmentation, or morphological segmentation, all showing improvements in translation quality (Costa-juss`a et al., 2007; Xu et al., 2005; Dyer et al., 2008). However, these applications encode the ambiguity arising from a single input, while in this work we combine distinct inputs into a more compact and expressive single input format. When given many monolingual inputs, we can apply TER and construct a confusion network as in Section 2.2.2 In this application of confusion networks, arc weights are calculated by summing votes from each input for a given word, and normalizing all arcs leaving a node to sum to 1. Figure 3 shows an example of a TER-derived input from IWSLT data. Because the decoder will handle reordering, we select the input with th"
E09-1082,2008.amta-srw.6,0,0.165453,"N 1 = t1 , . . . , tN . Och and Ney present two approaches for selecting a single target from these outputs. The first, P ROD, finds Q the maximiser of the product, arg maxt∈tN p(t) N n=1 pn (sn |t), where 1 p(t) is the language model probability. For reasons of tractability, the maximisation is performed only over targets generated by the translation systems, tN 1 , not the full space of all translations. The P ROD method requires each model to provide a model score for each tn generated by the other models. However, this is often impossible due to the models’ highly divergent output spaces (Schwartz, 2008), and therefore the technique cannot be easily applied. The second approach, M AX, solves arg maxt∈tN maxN p(t)p (s |t), which is n n n=1 1 much easier to calculate. As with P ROD, the translation models’ outputs are used for the candidate translations. While different models may have different score ranges, Och and Ney (2001) state that there is little benefit in weighting these scores to normalise the output range. In their experiments, they show that M AX used on pairs or triples of language inputs can outperform a model with single language input, but that performance degrades as more lang"
E09-1082,2006.amta-papers.25,0,0.0501545,"sentences can be created using different reorderings or combinations of translation fragments (e.g., Bangalore et al. (2001); Rosti et al. (2007b)). These methods differ in the types of lattices used, their means of creation, and scoring method used to extract the best consensus output from the lattice. The system used in this paper is a variant of the one proposed in Rosti et al. (2007a), which we now describe in detail. The first step in forming a lattice is to align the inputs. Consensus decoding systems often use the script of edit operations that minimises the translation edit rate (TER; Snover et al. (2006)). TER is a word-based measure of edit distance which also allows n-gram shifts when calculating the best match between a hypothesis and reference. Because TER describes the correspondence between the hypothesis and reference as a sequence of insertions, substitutions, deletions, and shifts, the edit script it produces can be used to create a confusion network. Consider a reference of “The dog barked very loudly” and a hypothesis “A big dog loudly barked.” The TER alignment is shown in Table 1, along with the edit operations. Note that the matching “barked” tokens are labelled shift, as one ne"
E09-1082,2005.mtsummit-papers.11,1,0.108174,"Missing"
E09-1082,takezawa-etal-2002-toward,0,0.0630672,"11 a pour l' 12 13 espagne españa spanien la y 14 et och 15 17 grèce grecia 18 ? grekland ? ? 19 16 Figure 5: A multi-lingual lattice input for French, Spanish, and Swedish from Europarl dev2006. Data sets for this condition are readily available in the form of test sets created for machine translation evaluation, which contains multiple target references for each source sentence. By flipping these test sets around, we create multiple monolingual inputs (the original references) and a single reference output (the original source text). We examine two datasets: the BTEC Italian-English corpus (Takezawa et al., 2002), and the Multiple Translation Chinese to English (MTC) corpora,3 as used in past years’ NIST MT evaluations. All of our translation experiments use the Moses decoder (Koehn et al., 2007), and are evaluated using B LEU-4. Moses is a phrase-based decoder with features for lexicalized reordering, distance-based reordering, phrase and word translation probabilities, phrase and word counts, and an n-gram language model. 3.1 B EST O RACLE M AX S YS C OMB CN I NPUT We tune our translation models on devset1, system combination on devset2 and report results on devset3 for each condition. When tuning t"
E09-1082,D07-1105,0,0.0205643,"tational complexity of dealing with so many models, we train on only the first 100,000 sentences of each parallel corpus. Single system baseline scores for each language are shown in Table 5. verse is true for S YS C OMB. Given the robust performance of M AX when translation scores originated from the same translation model in English to Italian, it is not surprising that it favors the case where all the outputs are scored by the same model (“All tuned”). On the other hand, diversity amongst the system outputs has been shown to be important to the performance of system combination techniques (Macherey and Och, 2007). This may give an indication as to why the “Self tuned” data produced higher scores in consensus decoding – the outputs will be more highly divergent due to their different tuning conditions. Besides comparing the different multi-source translation methods discussed above, in this task we also want to examine what happens when we use different numbers of input languages. To determine the best order to add languages, we performed a greedy search over oracle B LEU scores for test set test2005. We started with the best scoring single system, French to English, and in each iteration picked one ad"
E09-1082,2005.iwslt-1.18,0,0.00944384,"the assistance of a language model. Formally, the path score is given by: When translating speech recognition output, previous work has shown that representing the ambiguity in the recognized text via confusion networks leads to better translations than simply translating the single best hypothesis of the speech recognition system (Bertoldi et al., 2007). The application of input lattices to other forms of input ambiguity has been limited to encoding input reorderings, word segmentation, or morphological segmentation, all showing improvements in translation quality (Costa-juss`a et al., 2007; Xu et al., 2005; Dyer et al., 2008). However, these applications encode the ambiguity arising from a single input, while in this work we combine distinct inputs into a more compact and expressive single input format. When given many monolingual inputs, we can apply TER and construct a confusion network as in Section 2.2.2 In this application of confusion networks, arc weights are calculated by summing votes from each input for a given word, and normalizing all arcs leaving a node to sum to 1. Figure 3 shows an example of a TER-derived input from IWSLT data. Because the decoder will handle reordering, we sele"
E09-1082,D07-1103,0,\N,Missing
E09-1082,J93-1004,0,\N,Missing
E09-1082,E06-1032,1,\N,Missing
E09-1082,A94-1016,0,\N,Missing
E09-1082,W02-1021,0,\N,Missing
E09-1082,W05-0828,0,\N,Missing
E09-1082,C96-2141,0,\N,Missing
E09-1082,C08-1005,0,\N,Missing
E09-1082,E99-1010,0,\N,Missing
E09-1082,J90-2002,0,\N,Missing
E09-1082,D07-1005,0,\N,Missing
E09-1082,2007.iwslt-1.1,0,\N,Missing
E09-1082,P02-1040,0,\N,Missing
E09-1082,W05-0820,1,\N,Missing
E09-1082,P01-1008,0,\N,Missing
E09-1082,W09-0401,1,\N,Missing
E09-1082,W08-0329,0,\N,Missing
E09-1082,N03-1024,0,\N,Missing
E09-1082,J04-4002,0,\N,Missing
E09-1082,2007.iwslt-1.6,1,\N,Missing
E09-1082,P07-2045,1,\N,Missing
E09-1082,N04-1022,0,\N,Missing
E09-1082,W07-0733,1,\N,Missing
E09-1082,D08-1064,0,\N,Missing
E09-1082,J06-4004,0,\N,Missing
E09-1082,W07-0718,1,\N,Missing
E09-1082,P05-1033,0,\N,Missing
E09-1082,W06-3114,1,\N,Missing
E09-1082,N03-1017,1,\N,Missing
E09-1082,P02-1038,0,\N,Missing
E09-1082,P05-3026,0,\N,Missing
E09-1082,2005.eamt-1.20,0,\N,Missing
E09-1082,2008.amta-srw.3,0,\N,Missing
E09-1082,W08-0309,1,\N,Missing
E09-1082,J08-4005,1,\N,Missing
E09-1082,D08-1011,0,\N,Missing
E09-1082,2006.iwslt-evaluation.1,0,\N,Missing
E09-1082,N04-1021,0,\N,Missing
E09-1082,W99-0602,0,\N,Missing
E09-1082,W09-0409,0,\N,Missing
E09-1082,W09-0407,0,\N,Missing
E09-1082,D08-1065,0,\N,Missing
E09-1082,D08-1076,0,\N,Missing
E09-1082,P03-1021,0,\N,Missing
E14-1035,P12-2023,0,0.700385,"er translations within the same data set. If these distributions overlap, then we expect topic adaptation to help separate them and yield better translations than an unadapted system. Topics can be of varying granularity and are therefore a flexible means to structure data that is not uniform enough to be modelled in its entirety. In recent years there have been several attempts to integrating topical information into SMT either by learning better word alignments (Zhao and Xing, 2006), by adapting translation features cross-domain (Su et al., 2012), or by dynamically adapting lexical weights (Eidelman et al., 2012) or adding sparse topic features (Hasler et al., 2012). Translating text from diverse sources poses a challenge to current machine translation systems which are rarely adapted to structure beyond corpus level. We explore topic adaptation on a diverse data set and present a new bilingual variant of Latent Dirichlet Allocation to compute topic-adapted, probabilistic phrase translation features. We dynamically infer document-specific translation probabilities for test sets of unknown origin, thereby capturing the effects of document context on phrase translations. We show gains of up to 1.26 BLEU"
E14-1035,W07-0717,0,0.0868419,"gin, thereby capturing the effects of document context on phrase translations. We show gains of up to 1.26 BLEU over the baseline and 1.04 over a domain adaptation benchmark. We further provide an analysis of the domain-specific data and show additive gains of our model in combination with other types of topic-adapted features. 1 Introduction In statistical machine translation (SMT), there has been a lot of interest in trying to incorporate information about the provenance of training examples in order to improve translations for specific target domains. A popular approach are mixture models (Foster and Kuhn, 2007) where each component contains data from a specific genre or domain. Mixture models can be trained for crossdomain adaption when the target domain is known or for dynamic adaptation when the target domain is inferred from the source text under translation. More recent domain adaptation methods employ corpus or instance weights to promote relevant training examples (Matsoukas et al., 2009; Foster et al., 2010) or do more radical data selection based on language model perplexity (Axelrod et al., 2011). In this work, we are interested in the dynamic adaptation case, which is challenging because w"
E14-1035,D11-1033,0,0.0183881,"er to improve translations for specific target domains. A popular approach are mixture models (Foster and Kuhn, 2007) where each component contains data from a specific genre or domain. Mixture models can be trained for crossdomain adaption when the target domain is known or for dynamic adaptation when the target domain is inferred from the source text under translation. More recent domain adaptation methods employ corpus or instance weights to promote relevant training examples (Matsoukas et al., 2009; Foster et al., 2010) or do more radical data selection based on language model perplexity (Axelrod et al., 2011). In this work, we are interested in the dynamic adaptation case, which is challenging because we cannot tune our model towards any specific domain. In previous literature, domains have often been loosely defined in terms of corpora, for example, news texts would be defined as belonging to We take a new approach to topic adaptation by estimating probabilistic phrase translation features in a completely Bayesian fashion. The motivation is that automatically identifying topics in the training data can help to select the appropriate translation of a source phrase in the context of a document. By"
E14-1035,2012.iwslt-papers.17,1,0.830437,"ibutions overlap, then we expect topic adaptation to help separate them and yield better translations than an unadapted system. Topics can be of varying granularity and are therefore a flexible means to structure data that is not uniform enough to be modelled in its entirety. In recent years there have been several attempts to integrating topical information into SMT either by learning better word alignments (Zhao and Xing, 2006), by adapting translation features cross-domain (Su et al., 2012), or by dynamically adapting lexical weights (Eidelman et al., 2012) or adding sparse topic features (Hasler et al., 2012). Translating text from diverse sources poses a challenge to current machine translation systems which are rarely adapted to structure beyond corpus level. We explore topic adaptation on a diverse data set and present a new bilingual variant of Latent Dirichlet Allocation to compute topic-adapted, probabilistic phrase translation features. We dynamically infer document-specific translation probabilities for test sets of unknown origin, thereby capturing the effects of document context on phrase translations. We show gains of up to 1.26 BLEU over the baseline and 1.04 over a domain adaptation b"
E14-1035,W11-1014,0,0.331415,"r baseline counterparts after tuning, replacing them yielded better results in combination with the other adapted features. We believe the reason could be that fewer phrase table features in total are easier to optimise. |t| 2 f (x) = , 1 + 1x CC 110K 818 1892 Table 1: Number of sentence pairs and documents (in brackets) in the French-English data sets. The training data has 2.7M English words per domain. More topic-adapted features lazy MDI Mixed 354K (6450) 2453 (39) 5664 (112) relevance (4) (5) The third feature is a document similarity feature, similar to the semantic feature described by Banchs and Costa-jussà (2011): docSimt = max(1 − JSD(vtrain doci , vtest doc )) (6) i where vtrain_doci and vtest_doc are document topic vector of training and test documents. Because topic 0 captures phrase pairs that are common to many documents, we exclude it from the topic vectors before computing similarities. 4.1 Feature combination We tried integrating the four topic-adapted features separately and in all possible combinations. As we will see in the results section, while all features improve over the baseline in isolation, the adapted translation feature P(t|s,d) is the strongest feature. For the features that hav"
E14-1035,D11-1125,0,0.0087326,"containing data from all three domains and train one language model on the concatenation of (equally sized) target sides of the training data. Word alignments are trained on the concatenation of all training data and fixed for all models. Our baseline (ALL) is a phrase-based FrenchEnglish system trained on the concatenation of all parallel data. It was built with the Moses toolkit (Koehn et al., 2007) using the 14 standard core features including a 5gram language model. Translation quality is evaluated on a large test set, using the average feature weights of three optimisation runs with PRO (Hopkins and May, 2011). We use the mteval-v13a.pl script to compute caseinsensitive BLEU. As domain-aware benchmark systems, we use the phrase table fill-up method (FILLUP) of Bisazza et al. (2011) which preserves the translation scores of phrases from the IN model and the linear mixture models (LINTM) of Sennrich (2012b) (both available in the Moses toolkit). For both systems, we build separate phrase tables for each domain and use a wrapper to decode tuning and test sets with domainspecific tables. Both benchmarks have an advanModel IN ALL Mixed 26.77 26.86 CC 18.76 19.61 NC 29.56 29.42 TED 32.47 31.88 Model Ted-"
E14-1035,2011.iwslt-evaluation.18,0,0.0695227,"e concatenation of all training data and fixed for all models. Our baseline (ALL) is a phrase-based FrenchEnglish system trained on the concatenation of all parallel data. It was built with the Moses toolkit (Koehn et al., 2007) using the 14 standard core features including a 5gram language model. Translation quality is evaluated on a large test set, using the average feature weights of three optimisation runs with PRO (Hopkins and May, 2011). We use the mteval-v13a.pl script to compute caseinsensitive BLEU. As domain-aware benchmark systems, we use the phrase table fill-up method (FILLUP) of Bisazza et al. (2011) which preserves the translation scores of phrases from the IN model and the linear mixture models (LINTM) of Sennrich (2012b) (both available in the Moses toolkit). For both systems, we build separate phrase tables for each domain and use a wrapper to decode tuning and test sets with domainspecific tables. Both benchmarks have an advanModel IN ALL Mixed 26.77 26.86 CC 18.76 19.61 NC 29.56 29.42 TED 32.47 31.88 Model Ted-half vs Ted-full CC-half vs CC-full NC-half vs NC-full Table 2: BLEU of in-domain and baseline models. Model Ted-IN vs ALL CC-IN vs ALL NC-IN vs ALL Avg JSD 0.15 0.17 0.13 Avg"
E14-1035,W04-3250,1,0.221354,"The botton row of the table indicates the relative improvement of the best topic-adapted model per domain over the ALL model. Using all four topic-adapted features yields an improvement of 0.81 BLEU on the mixed test set. The highest improvement on a given domain is achieved for TED with an increase of 1.26 BLEU. The smallest improvement is measured on the NC domain. This is in line with the observation that distributions in the NC in-domain table are most similar to the ALL table, therefore we would expect the smallest improvement for domain or topic adaptation. We used bootstrap resampling (Koehn, 2004) to measure significance on the mixed test set and marked all statistically significant results compared to the respective baselines with asterisk (*: p ≤ 0.01). There is quite a clear horizontal separation between documents of different domains, for example, topics 6, 8, 19 occur mostly in Ted, NC and CC documents respectively. The overall structure is very similar between training (top) and test (bottom) documents, which shows that test inference was successful in carrying over the information learned on training documents. There is also some degree of topic sharing across domains, for examp"
E14-1035,D10-1005,0,0.00711491,"en a conversation and training documents in an additional feature. This is similar to the work of Banchs and Costa-jussà (2011), both of which inspired our document similarity feature. Also related is the work of Sennrich (2012a) who explore mixturemodelling on unsupervised clusters for domain adaptation and Chen et al. (2013) who compute phrase pair features from vector space representations that capture domain similarity to a development set. Both are cross-domain adaptation approaches, though. Instances of multilingual topic models outside the field of MT include BoydGraber and Blei (2009; Boyd-Graber and Resnik (2010) who learn cross-lingual topic correspondences (but do not learn conditional distributions like our model does). In terms of model structure, our model is similar to BiTAM (Zhao and Xing, 2006) which is an LDA-style model to learn topicbased word alignments. The work of Carpuat and Wu (2007) is similar to ours in spirit, but they predict the most probable translation in a context at the token level while our adaptation operates at the type level of a document. many IT-related documents). The last example, démon, has three frequent translations in English: devil, demon and daemon. The last tran"
E14-1035,2012.iwslt-papers.14,0,0.052881,"atures make use of the topic mixtures learned by our bilingual topic model. The first feature is an adapted lexical weight, similar to the features in the work of Eidelman et al. (2012). Our feature is different in that we marginalise over topics to produce a single adapted feature where v[k] is the kth element of a document topic vector for document d and w(t|s,k) is a topic-dependent word translation probability: lex(t¯|s, ¯ d) = 5 5.1 1 ∏ { j|(i, j) ∈ a} ∑ ∑ w(t|s, k) · v[k] (3) i ∀(i, j)∈a k | {z } w(t|s) The second feature is a target unigram feature similar to the lazy MDI adaptation of Ruiz and Federico (2012). It includes an additional term that measures the relevance of a target word wi by comparing its document-specific probability Pdoc to its probability under the asymmetric topic 0: |t| Pdoc (wi ) Pdoc (wi ) )· f ( ) trgUnigramst = ∏ f ( (wi ) Ptopic0 (wi ) i=1 |Pbaseline {z } | {z } x>0 NC 103K 817 1878 TED 140K 818 1894 the log-linear model. We found that while adding the features worked well and yielded close to zero weights for their baseline counterparts after tuning, replacing them yielded better results in combination with the other adapted features. We believe the reason could be that"
E14-1035,2007.tmi-papers.6,0,0.032213,"ation and Chen et al. (2013) who compute phrase pair features from vector space representations that capture domain similarity to a development set. Both are cross-domain adaptation approaches, though. Instances of multilingual topic models outside the field of MT include BoydGraber and Blei (2009; Boyd-Graber and Resnik (2010) who learn cross-lingual topic correspondences (but do not learn conditional distributions like our model does). In terms of model structure, our model is similar to BiTAM (Zhao and Xing, 2006) which is an LDA-style model to learn topicbased word alignments. The work of Carpuat and Wu (2007) is similar to ours in spirit, but they predict the most probable translation in a context at the token level while our adaptation operates at the type level of a document. many IT-related documents). The last example, démon, has three frequent translations in English: devil, demon and daemon. The last translation refers to a computer process and would occur in an IT context. The topic-phrase probabilities reveal that its mostly likely translation as daemon occurs under topic 19 which clusters IT-related phrase pairs and is frequent in the CC corpus. These examples show that our model can disa"
E14-1035,2012.eamt-1.60,0,0.0433237,"opic vectors before computing similarities. 4.1 Feature combination We tried integrating the four topic-adapted features separately and in all possible combinations. As we will see in the results section, while all features improve over the baseline in isolation, the adapted translation feature P(t|s,d) is the strongest feature. For the features that have a counterpart in the baseline model (p(t|s,d) and lex(t|s,d)), we experimented with either adding or replacing them in 331 Experimental setup Data and baselines Our experiments were carried out on a mixed data set, containing the TED corpus (Cettolo et al., 2012), parts of the News Commentary corpus (NC) and parts of the Commoncrawl corpus (CC) from the WMT13 shared task (Bojar et al., 2013) as described in Table 1. We were guided by two constraints in chosing our data set. 1) the data has document boundaries and the content of each document is assumed to be topically related, 2) there is some degree of topical variation within each data set. In order to compare to domain adaptation approaches, we chose a setup with data from different corpora. We want to abstract away from adaptation effects that concern tuning of length penalties and language models"
E14-1035,P06-2124,0,0.183172,"We view topic adaptation as fine-grained domain adaptation with the implicit assumption that there can be multiple distributions over translations within the same data set. If these distributions overlap, then we expect topic adaptation to help separate them and yield better translations than an unadapted system. Topics can be of varying granularity and are therefore a flexible means to structure data that is not uniform enough to be modelled in its entirety. In recent years there have been several attempts to integrating topical information into SMT either by learning better word alignments (Zhao and Xing, 2006), by adapting translation features cross-domain (Su et al., 2012), or by dynamically adapting lexical weights (Eidelman et al., 2012) or adding sparse topic features (Hasler et al., 2012). Translating text from diverse sources poses a challenge to current machine translation systems which are rarely adapted to structure beyond corpus level. We explore topic adaptation on a diverse data set and present a new bilingual variant of Latent Dirichlet Allocation to compute topic-adapted, probabilistic phrase translation features. We dynamically infer document-specific translation probabilities for te"
E14-1035,D10-1044,0,\N,Missing
E14-1035,D09-1074,0,\N,Missing
E14-1035,E12-1055,0,\N,Missing
E14-1035,P13-2122,0,\N,Missing
E14-2007,J93-2003,0,0.0320766,"Missing"
E14-2007,2010.eamt-1.18,1,0.868591,"Missing"
E14-2007,2005.mtsummit-papers.19,1,0.793548,"Missing"
E14-2007,D08-1051,1,0.931323,"Missing"
E14-2007,J09-1002,1,\N,Missing
E14-2007,P07-2045,1,\N,Missing
E14-2007,2012.eamt-1.5,1,\N,Missing
E14-4029,P13-2071,1,0.630639,"Missing"
E14-4029,P02-1051,0,0.0983573,"s to integrate transliterations, we observed improvements from 0.23-0.75 (∆ 0.41) BLEU points across 7 language pairs. We also show that our mined transliteration corpora provide better rule coverage and translation quality compared to the gold standard transliteration corpora. 1 Introduction All machine translation (MT) systems suffer from the existence of out-of-vocabulary (OOV) words, irrespective of the amount of data available for training. OOV words are mostly named entities, technical terms or foreign words that can be translated to the target language using transliteration. Much work (Al-Onaizan and Knight, 2002; Zhao et al., 2007; Kashani et al., 2007; Habash, 2009) has been done on transliterating named entities and OOVs, and transliteration has been shown to improve MT quality. Transliteration has also shown to be useful for translating closely related language pairs (Durrani et al., 2010; Nakov and Tiedemann, 2012), and for disambiguation (Hermjakob et al., 2008; Azab et al., 2013). However, despite its utility, a transliteration module does not exist in the commonly used MT toolkits, such as Moses (Koehn et al., 2007). One of the main reasons is that the training data, a corpus of transliteratio"
E14-4029,W13-2212,1,0.618295,"Missing"
E14-4029,P08-2014,0,0.0181782,"by tightening the mining threshold probability. However, our end goal is to improve end-to-end MT and not the transliteration system. We observed that recall is more important than precision for overall MT quality. We provide an empirical justification for this when discussing the final experiments. 3 Method 3 is desirable in cases where the decoder can translate or transliterate a word. For example Hindi word can be translated to “Border” and also transliterated to name “Seema”. Identifying such candidates that can be translated or transliterated is a challenge. Machine learning techniques (Goldwasser and Roth, 2008; Kirschenbaum and Wintner, 2009) and named entity recognizers (Klementiev and Roth, 2006; Hermjakob et al., 2008) have been used for this purpose. Though, we only focus on OOV words, method 3 can be used if such a classifier/NE tagger is available. 4 Arabic and Urdu are segmented using MADA (Habash and Sadat, 2006) and UWS (Durrani and Hussain, 2010). 5 Retuning the transliteration features was not helpful, default weights are used. 6 M-N/1-N alignments are less likely to be transliterations. 150 MT Experiments: Table 3 gives a comprehensive evaluation of the three methods of integration disc"
E14-4029,N06-2013,0,0.0489938,"irable in cases where the decoder can translate or transliterate a word. For example Hindi word can be translated to “Border” and also transliterated to name “Seema”. Identifying such candidates that can be translated or transliterated is a challenge. Machine learning techniques (Goldwasser and Roth, 2008; Kirschenbaum and Wintner, 2009) and named entity recognizers (Klementiev and Roth, 2006; Hermjakob et al., 2008) have been used for this purpose. Though, we only focus on OOV words, method 3 can be used if such a classifier/NE tagger is available. 4 Arabic and Urdu are segmented using MADA (Habash and Sadat, 2006) and UWS (Durrani and Hussain, 2010). 5 Retuning the transliteration features was not helpful, default weights are used. 6 M-N/1-N alignments are less likely to be transliterations. 150 MT Experiments: Table 3 gives a comprehensive evaluation of the three methods of integration discussed in Section 4 along with the number7 of OOV words (types) in different tests. We report BLEU gains (Papineni et al., 2002) obtained by each method. Method 1 (M1 ), that replaces OOV words with 1-best transliteration gave an average improvement of +0.13. This result can be attributed to the low precision of the"
E14-4029,2012.eamt-1.60,0,0.0120164,"ase tables and back-off models. As in method 2, we also use the LM-OOV feature in method 3.3 5 ized reordering, sparse lexical and domain features (Hasler et al., 2012), a distortion limit of 6, 100-best translation options, MBR decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007), and the no-reordering-overpunctuation heuristic. We tuned with the k-best batch MIRA (Cherry and Foster, 2012).5 Evaluation Data: We experimented with 7 language pairs, namely: Arabic, Bengali, Farsi, Hindi, Russian, Telugu and Urdu-into-English. For Arabic4 and Farsi, we used the TED talks data (Cettolo et al., 2012) made available for IWSLT-13, and we used the dev2010 set for tuning and the test2011 and test2012 sets for evaluation. For Indian languages we used the Indic multi-parallel corpus (Post et al., 2012), and we used the dev and test sets provided with the parallel corpus. For Russian, we used WMT-13 data (Bojar et al., 2013), and we used half of the news-test2012 for tuning and other half for testing. We also evaluated on the newstest2013 set. For all, we trained the language model using the monolingual WMT-13 data. See Table 1 for data statistics. Lang AR BN FA HI RU TE UR Transliteration Miner"
E14-4029,2012.iwslt-papers.17,1,0.77685,"sing step. The transliteration model learns character alignment using expectation maximization (EM). See Sajjad et al. (2012) for more details. 1 Mining algorithm also makes this assumption. Tuning data is subtracted from the training corpus while tuning to avoid over-fitting. After the weights are tuned, we add it back, retrain GIZA, and estimate new models. 2 149 use the decoding-graph-backoff option in Moses, that allows multiple translation phrase tables and back-off models. As in method 2, we also use the LM-OOV feature in method 3.3 5 ized reordering, sparse lexical and domain features (Hasler et al., 2012), a distortion limit of 6, 100-best translation options, MBR decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007), and the no-reordering-overpunctuation heuristic. We tuned with the k-best batch MIRA (Cherry and Foster, 2012).5 Evaluation Data: We experimented with 7 language pairs, namely: Arabic, Bengali, Farsi, Hindi, Russian, Telugu and Urdu-into-English. For Arabic4 and Farsi, we used the TED talks data (Cettolo et al., 2012) made available for IWSLT-13, and we used the dev2010 set for tuning and the test2011 and test2012 sets for evaluation. For Indian languages we use"
E14-4029,N12-1047,0,0.0529277,"s while tuning to avoid over-fitting. After the weights are tuned, we add it back, retrain GIZA, and estimate new models. 2 149 use the decoding-graph-backoff option in Moses, that allows multiple translation phrase tables and back-off models. As in method 2, we also use the LM-OOV feature in method 3.3 5 ized reordering, sparse lexical and domain features (Hasler et al., 2012), a distortion limit of 6, 100-best translation options, MBR decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007), and the no-reordering-overpunctuation heuristic. We tuned with the k-best batch MIRA (Cherry and Foster, 2012).5 Evaluation Data: We experimented with 7 language pairs, namely: Arabic, Bengali, Farsi, Hindi, Russian, Telugu and Urdu-into-English. For Arabic4 and Farsi, we used the TED talks data (Cettolo et al., 2012) made available for IWSLT-13, and we used the dev2010 set for tuning and the test2011 and test2012 sets for evaluation. For Indian languages we used the Indic multi-parallel corpus (Post et al., 2012), and we used the dev and test sets provided with the parallel corpus. For Russian, we used WMT-13 data (Bojar et al., 2013), and we used half of the news-test2012 for tuning and other half f"
E14-4029,W11-2123,0,0.0165735,"Missing"
E14-4029,W10-2407,0,0.0431795,"53, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics 2 Transliteration Mining The main bottleneck in building a transliteration system is the lack of availability of transliteration training pairs. It is, however, fair to assume that any parallel data would contain a reasonable number of transliterated word pairs. Transliteration mining can be used to extract such word pairs from the parallel corpus. Most previous techniques on transliteration mining generally use supervised and semi-supervised methods (Sherif and Kondrak, 2007; Jiampojamarn et al., 2010; Darwish, 2010; Kahki et al., 2012). This constrains the mining solution to language pairs for which training data (seed data) is available. A few researchers proposed unsupervised approaches to mine transliterations (Lee and Choi, 1998; Sajjad et al., 2011; Lin et al., 2011). We adapted the work of Sajjad et al. (2012) as summarized below. i=1 pE (ei ) Integration to Machine Translation Method 1: involves replacing OOVs in the output with the 1-best transliteration. The success of Method 1 is solely contingent on the accuracy of the transliteration model. Also, it ignores context which may lead to incorrec"
E14-4029,P08-1045,0,0.0826147,"Missing"
E14-4029,P07-1019,0,0.0661469,"more details. 1 Mining algorithm also makes this assumption. Tuning data is subtracted from the training corpus while tuning to avoid over-fitting. After the weights are tuned, we add it back, retrain GIZA, and estimate new models. 2 149 use the decoding-graph-backoff option in Moses, that allows multiple translation phrase tables and back-off models. As in method 2, we also use the LM-OOV feature in method 3.3 5 ized reordering, sparse lexical and domain features (Hasler et al., 2012), a distortion limit of 6, 100-best translation options, MBR decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007), and the no-reordering-overpunctuation heuristic. We tuned with the k-best batch MIRA (Cherry and Foster, 2012).5 Evaluation Data: We experimented with 7 language pairs, namely: Arabic, Bengali, Farsi, Hindi, Russian, Telugu and Urdu-into-English. For Arabic4 and Farsi, we used the TED talks data (Cettolo et al., 2012) made available for IWSLT-13, and we used the dev2010 set for tuning and the test2011 and test2012 sets for evaluation. For Indian languages we used the Indic multi-parallel corpus (Post et al., 2012), and we used the dev and test sets provided with the parallel corpus. For Russ"
E14-4029,P10-1048,1,0.84313,"n All machine translation (MT) systems suffer from the existence of out-of-vocabulary (OOV) words, irrespective of the amount of data available for training. OOV words are mostly named entities, technical terms or foreign words that can be translated to the target language using transliteration. Much work (Al-Onaizan and Knight, 2002; Zhao et al., 2007; Kashani et al., 2007; Habash, 2009) has been done on transliterating named entities and OOVs, and transliteration has been shown to improve MT quality. Transliteration has also shown to be useful for translating closely related language pairs (Durrani et al., 2010; Nakov and Tiedemann, 2012), and for disambiguation (Hermjakob et al., 2008; Azab et al., 2013). However, despite its utility, a transliteration module does not exist in the commonly used MT toolkits, such as Moses (Koehn et al., 2007). One of the main reasons is that the training data, a corpus of transliteration pairs, required to build a transliteration system, is not readily available for many language pairs. Even if such a training data is available, mechanisms to integrate transliterated words 148 Proceedings of the 14th Conference of the European Chapter of the Association for Computat"
E14-4029,W10-2405,0,0.109169,"Missing"
E14-4029,J03-1002,0,0.0188912,"Missing"
E14-4029,W07-0703,0,0.103886,"rovements from 0.23-0.75 (∆ 0.41) BLEU points across 7 language pairs. We also show that our mined transliteration corpora provide better rule coverage and translation quality compared to the gold standard transliteration corpora. 1 Introduction All machine translation (MT) systems suffer from the existence of out-of-vocabulary (OOV) words, irrespective of the amount of data available for training. OOV words are mostly named entities, technical terms or foreign words that can be translated to the target language using transliteration. Much work (Al-Onaizan and Knight, 2002; Zhao et al., 2007; Kashani et al., 2007; Habash, 2009) has been done on transliterating named entities and OOVs, and transliteration has been shown to improve MT quality. Transliteration has also shown to be useful for translating closely related language pairs (Durrani et al., 2010; Nakov and Tiedemann, 2012), and for disambiguation (Hermjakob et al., 2008; Azab et al., 2013). However, despite its utility, a transliteration module does not exist in the commonly used MT toolkits, such as Moses (Koehn et al., 2007). One of the main reasons is that the training data, a corpus of transliteration pairs, required to build a transliterat"
E14-4029,P02-1040,0,0.100942,"l., 2008) have been used for this purpose. Though, we only focus on OOV words, method 3 can be used if such a classifier/NE tagger is available. 4 Arabic and Urdu are segmented using MADA (Habash and Sadat, 2006) and UWS (Durrani and Hussain, 2010). 5 Retuning the transliteration features was not helpful, default weights are used. 6 M-N/1-N alignments are less likely to be transliterations. 150 MT Experiments: Table 3 gives a comprehensive evaluation of the three methods of integration discussed in Section 4 along with the number7 of OOV words (types) in different tests. We report BLEU gains (Papineni et al., 2002) obtained by each method. Method 1 (M1 ), that replaces OOV words with 1-best transliteration gave an average improvement of +0.13. This result can be attributed to the low precision of the transliteration system (Table 2). Method 2 (M2 ), that transliterates OOVs in second pass monotonic decoding, gave an average improvement of +0.39. Slightly higher gains were obtained using Method 3 (M3 ), that integrates transliteration phrase-table inside decoder on the fly. However, the efficacy of M3 in comparison to M2 is not as apparent, as M2 produced better results than M3 in half of the cases. both"
E14-4029,E09-1050,0,0.0166804,"hreshold probability. However, our end goal is to improve end-to-end MT and not the transliteration system. We observed that recall is more important than precision for overall MT quality. We provide an empirical justification for this when discussing the final experiments. 3 Method 3 is desirable in cases where the decoder can translate or transliterate a word. For example Hindi word can be translated to “Border” and also transliterated to name “Seema”. Identifying such candidates that can be translated or transliterated is a challenge. Machine learning techniques (Goldwasser and Roth, 2008; Kirschenbaum and Wintner, 2009) and named entity recognizers (Klementiev and Roth, 2006; Hermjakob et al., 2008) have been used for this purpose. Though, we only focus on OOV words, method 3 can be used if such a classifier/NE tagger is available. 4 Arabic and Urdu are segmented using MADA (Habash and Sadat, 2006) and UWS (Durrani and Hussain, 2010). 5 Retuning the transliteration features was not helpful, default weights are used. 6 M-N/1-N alignments are less likely to be transliterations. 150 MT Experiments: Table 3 gives a comprehensive evaluation of the three methods of integration discussed in Section 4 along with the"
E14-4029,W12-3152,0,0.0194846,"translation options, MBR decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007), and the no-reordering-overpunctuation heuristic. We tuned with the k-best batch MIRA (Cherry and Foster, 2012).5 Evaluation Data: We experimented with 7 language pairs, namely: Arabic, Bengali, Farsi, Hindi, Russian, Telugu and Urdu-into-English. For Arabic4 and Farsi, we used the TED talks data (Cettolo et al., 2012) made available for IWSLT-13, and we used the dev2010 set for tuning and the test2011 and test2012 sets for evaluation. For Indian languages we used the Indic multi-parallel corpus (Post et al., 2012), and we used the dev and test sets provided with the parallel corpus. For Russian, we used WMT-13 data (Bojar et al., 2013), and we used half of the news-test2012 for tuning and other half for testing. We also evaluated on the newstest2013 set. For all, we trained the language model using the monolingual WMT-13 data. See Table 1 for data statistics. Lang AR BN FA HI RU TE UR Transliteration Miner: The miner extracts transliterations from a word-aligned parallel corpus. We only used word pairs with 1-to-1 alignments.6 Before feeding the list into the miner, we cleaned it by removing digits, sy"
E14-4029,N06-1011,0,0.03029,"to-end MT and not the transliteration system. We observed that recall is more important than precision for overall MT quality. We provide an empirical justification for this when discussing the final experiments. 3 Method 3 is desirable in cases where the decoder can translate or transliterate a word. For example Hindi word can be translated to “Border” and also transliterated to name “Seema”. Identifying such candidates that can be translated or transliterated is a challenge. Machine learning techniques (Goldwasser and Roth, 2008; Kirschenbaum and Wintner, 2009) and named entity recognizers (Klementiev and Roth, 2006; Hermjakob et al., 2008) have been used for this purpose. Though, we only focus on OOV words, method 3 can be used if such a classifier/NE tagger is available. 4 Arabic and Urdu are segmented using MADA (Habash and Sadat, 2006) and UWS (Durrani and Hussain, 2010). 5 Retuning the transliteration features was not helpful, default weights are used. 6 M-N/1-N alignments are less likely to be transliterations. 150 MT Experiments: Table 3 gives a comprehensive evaluation of the three methods of integration discussed in Section 4 along with the number7 of OOV words (types) in different tests. We rep"
E14-4029,P11-1044,1,0.785964,"t is, however, fair to assume that any parallel data would contain a reasonable number of transliterated word pairs. Transliteration mining can be used to extract such word pairs from the parallel corpus. Most previous techniques on transliteration mining generally use supervised and semi-supervised methods (Sherif and Kondrak, 2007; Jiampojamarn et al., 2010; Darwish, 2010; Kahki et al., 2012). This constrains the mining solution to language pairs for which training data (seed data) is available. A few researchers proposed unsupervised approaches to mine transliterations (Lee and Choi, 1998; Sajjad et al., 2011; Lin et al., 2011). We adapted the work of Sajjad et al. (2012) as summarized below. i=1 pE (ei ) Integration to Machine Translation Method 1: involves replacing OOVs in the output with the 1-best transliteration. The success of Method 1 is solely contingent on the accuracy of the transliteration model. Also, it ignores context which may lead to incorrect transliteration. For example, the Arabic word transliterates to “Bill” when followed by “Clinton” and “Bell” if preceded by “Alexander Graham”. Method 2: provides n-best transliterations to a monotonic decoder that uses a monolingual languag"
E14-4029,P12-1049,1,0.900033,"ain a reasonable number of transliterated word pairs. Transliteration mining can be used to extract such word pairs from the parallel corpus. Most previous techniques on transliteration mining generally use supervised and semi-supervised methods (Sherif and Kondrak, 2007; Jiampojamarn et al., 2010; Darwish, 2010; Kahki et al., 2012). This constrains the mining solution to language pairs for which training data (seed data) is available. A few researchers proposed unsupervised approaches to mine transliterations (Lee and Choi, 1998; Sajjad et al., 2011; Lin et al., 2011). We adapted the work of Sajjad et al. (2012) as summarized below. i=1 pE (ei ) Integration to Machine Translation Method 1: involves replacing OOVs in the output with the 1-best transliteration. The success of Method 1 is solely contingent on the accuracy of the transliteration model. Also, it ignores context which may lead to incorrect transliteration. For example, the Arabic word transliterates to “Bill” when followed by “Clinton” and “Bell” if preceded by “Alexander Graham”. Method 2: provides n-best transliterations to a monotonic decoder that uses a monolingual language model and a transliteration phrasetranslation table to rescore"
E14-4029,P07-1109,0,0.0454649,"ssociation for Computational Linguistics, pages 148–153, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics 2 Transliteration Mining The main bottleneck in building a transliteration system is the lack of availability of transliteration training pairs. It is, however, fair to assume that any parallel data would contain a reasonable number of transliterated word pairs. Transliteration mining can be used to extract such word pairs from the parallel corpus. Most previous techniques on transliteration mining generally use supervised and semi-supervised methods (Sherif and Kondrak, 2007; Jiampojamarn et al., 2010; Darwish, 2010; Kahki et al., 2012). This constrains the mining solution to language pairs for which training data (seed data) is available. A few researchers proposed unsupervised approaches to mine transliterations (Lee and Choi, 1998; Sajjad et al., 2011; Lin et al., 2011). We adapted the work of Sajjad et al. (2012) as summarized below. i=1 pE (ei ) Integration to Machine Translation Method 1: involves replacing OOVs in the output with the 1-best transliteration. The success of Method 1 is solely contingent on the accuracy of the transliteration model. Also, it"
E14-4029,N04-1022,0,0.0441673,"ion (EM). See Sajjad et al. (2012) for more details. 1 Mining algorithm also makes this assumption. Tuning data is subtracted from the training corpus while tuning to avoid over-fitting. After the weights are tuned, we add it back, retrain GIZA, and estimate new models. 2 149 use the decoding-graph-backoff option in Moses, that allows multiple translation phrase tables and back-off models. As in method 2, we also use the LM-OOV feature in method 3.3 5 ized reordering, sparse lexical and domain features (Hasler et al., 2012), a distortion limit of 6, 100-best translation options, MBR decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007), and the no-reordering-overpunctuation heuristic. We tuned with the k-best batch MIRA (Cherry and Foster, 2012).5 Evaluation Data: We experimented with 7 language pairs, namely: Arabic, Bengali, Farsi, Hindi, Russian, Telugu and Urdu-into-English. For Arabic4 and Farsi, we used the TED talks data (Cettolo et al., 2012) made available for IWSLT-13, and we used the dev2010 set for tuning and the test2011 and test2012 sets for evaluation. For Indian languages we used the Indic multi-parallel corpus (Post et al., 2012), and we used the dev and test sets prov"
E14-4029,N07-1046,0,0.130438,"ns, we observed improvements from 0.23-0.75 (∆ 0.41) BLEU points across 7 language pairs. We also show that our mined transliteration corpora provide better rule coverage and translation quality compared to the gold standard transliteration corpora. 1 Introduction All machine translation (MT) systems suffer from the existence of out-of-vocabulary (OOV) words, irrespective of the amount of data available for training. OOV words are mostly named entities, technical terms or foreign words that can be translated to the target language using transliteration. Much work (Al-Onaizan and Knight, 2002; Zhao et al., 2007; Kashani et al., 2007; Habash, 2009) has been done on transliterating named entities and OOVs, and transliteration has been shown to improve MT quality. Transliteration has also shown to be useful for translating closely related language pairs (Durrani et al., 2010; Nakov and Tiedemann, 2012), and for disambiguation (Hermjakob et al., 2008; Azab et al., 2013). However, despite its utility, a transliteration module does not exist in the commonly used MT toolkits, such as Moses (Koehn et al., 2007). One of the main reasons is that the training data, a corpus of transliteration pairs, required t"
E14-4029,W10-2404,0,0.0966661,"Missing"
E14-4029,W11-2206,0,0.0183811,"o assume that any parallel data would contain a reasonable number of transliterated word pairs. Transliteration mining can be used to extract such word pairs from the parallel corpus. Most previous techniques on transliteration mining generally use supervised and semi-supervised methods (Sherif and Kondrak, 2007; Jiampojamarn et al., 2010; Darwish, 2010; Kahki et al., 2012). This constrains the mining solution to language pairs for which training data (seed data) is available. A few researchers proposed unsupervised approaches to mine transliterations (Lee and Choi, 1998; Sajjad et al., 2011; Lin et al., 2011). We adapted the work of Sajjad et al. (2012) as summarized below. i=1 pE (ei ) Integration to Machine Translation Method 1: involves replacing OOVs in the output with the 1-best transliteration. The success of Method 1 is solely contingent on the accuracy of the transliteration model. Also, it ignores context which may lead to incorrect transliteration. For example, the Arabic word transliterates to “Bill” when followed by “Clinton” and “Bell” if preceded by “Alexander Graham”. Method 2: provides n-best transliterations to a monotonic decoder that uses a monolingual language model and a trans"
E14-4029,N10-1077,1,\N,Missing
E14-4029,N12-1025,0,\N,Missing
E14-4029,P12-2059,0,\N,Missing
E14-4029,P07-2045,1,\N,Missing
E14-4029,N13-1046,0,\N,Missing
E14-4029,W13-2201,1,\N,Missing
E14-4029,2013.iwslt-evaluation.3,1,\N,Missing
I08-2089,D07-1090,0,0.0600348,"creasingly large LMs for MT has been recognized in recent years. The effect of doubling LM size has been powerfully demonstrated by Google’s submissions to the NIST evaluation campaigns. The use of billions of words of LM training data has become standard in large-scale SMT systems, and even trillion word LMs have been demonstrated. Since lookup of LM scores is one of the fundamental functions in SMT decoding, efficient storage and access of the model becomes increasingly difficult. A recent trend is to store the LM in a distributed cluster of machines, which are queried via network requests (Brants et al., 2007; Emami et al., 2007). It is easier, however, to use such large LMs in reranking (Zhang et al., 2006). Since the use of clusters of machines is not always practical (or affordable) for SMT applications, an alternative strategy is to find more efficient ways to store the LM in the working memory of a single machine, for instance by using efficient prefix trees and fewer bits to store the LM probability (Federico and Bertoldi, 2006). Also the use of lossy data structures based on Bloom filters has been demonstrated to be effective for LMs (Talbot and Osborne, 2007a; Talbot and Osborne, 2007b). T"
I08-2089,W07-0718,1,0.802514,"ssible translations, we may miss the translation that we would have found with a LM integrated into the decoding process. 6 Experiments In our experiments we are looking for answers to the open questions on the use of LMs for SMT: Do perplexity and B LEU score performance correlate when interpolating LMs? Should LMs be combined by interpolation or be used as separate feature functions in the log-linear machine translation model? Is the use of LMs in re-ranking sufficient to increase machine translation performance? 6.1 29 Dev Test Interpolation In the WMT 2007 shared task evaluation campaign (Callison-Burch et al., 2007) domain adaptation was a special challenge. Two training corpora were provided: a small in-domain corpus (News Commentary) and the about 30 times bigger out-of-domain Europarl corpus (see Table 1). One method for domain adaptation is to bias the LM towards the indomain data. We train two LMs and interpolate them to optimize performance on in-domain data. In our experiments, the translation model is first trained on the combined corpus without weighting. We use the Moses decoder (Koehn et al., 2007) with default settings. The 5-gram LM was trained using the SRILM toolkit. We only run minimum er"
I08-2089,2007.mtsummit-papers.18,1,0.852093,"Missing"
I08-2089,W06-3113,0,0.0419437,"torage and access of the model becomes increasingly difficult. A recent trend is to store the LM in a distributed cluster of machines, which are queried via network requests (Brants et al., 2007; Emami et al., 2007). It is easier, however, to use such large LMs in reranking (Zhang et al., 2006). Since the use of clusters of machines is not always practical (or affordable) for SMT applications, an alternative strategy is to find more efficient ways to store the LM in the working memory of a single machine, for instance by using efficient prefix trees and fewer bits to store the LM probability (Federico and Bertoldi, 2006). Also the use of lossy data structures based on Bloom filters has been demonstrated to be effective for LMs (Talbot and Osborne, 2007a; Talbot and Osborne, 2007b). This allows the use of much larger LMs, but increases the risk of errors. 3 Combination of Language Models LM training data may be any text in the output language. Typically, however, we are interested in building a MT system for a particular domain. If text resources come from a diversity of domains, some may be more suitable than others. A common stratNeural Network input output layer probability estimation wj−n+1 projection laye"
I08-2089,W06-3114,1,0.787443,"Missing"
I08-2089,P07-2045,1,0.0103622,"Missing"
I08-2089,2005.mtsummit-papers.11,1,0.0176338,"Missing"
I08-2089,2006.iwslt-papers.2,1,0.886116,"Missing"
I08-2089,P07-1065,0,0.0131391,"ch are queried via network requests (Brants et al., 2007; Emami et al., 2007). It is easier, however, to use such large LMs in reranking (Zhang et al., 2006). Since the use of clusters of machines is not always practical (or affordable) for SMT applications, an alternative strategy is to find more efficient ways to store the LM in the working memory of a single machine, for instance by using efficient prefix trees and fewer bits to store the LM probability (Federico and Bertoldi, 2006). Also the use of lossy data structures based on Bloom filters has been demonstrated to be effective for LMs (Talbot and Osborne, 2007a; Talbot and Osborne, 2007b). This allows the use of much larger LMs, but increases the risk of errors. 3 Combination of Language Models LM training data may be any text in the output language. Typically, however, we are interested in building a MT system for a particular domain. If text resources come from a diversity of domains, some may be more suitable than others. A common stratNeural Network input output layer probability estimation wj−n+1 projection layer p1 = P (wj =1|hj ) hidden layer M cl wj−n+2 oi dj pi = P (wj =i|hj ) V shared projections H wj−1 P N discrete continuous representat"
I08-2089,D07-1049,0,0.0136637,"ch are queried via network requests (Brants et al., 2007; Emami et al., 2007). It is easier, however, to use such large LMs in reranking (Zhang et al., 2006). Since the use of clusters of machines is not always practical (or affordable) for SMT applications, an alternative strategy is to find more efficient ways to store the LM in the working memory of a single machine, for instance by using efficient prefix trees and fewer bits to store the LM probability (Federico and Bertoldi, 2006). Also the use of lossy data structures based on Bloom filters has been demonstrated to be effective for LMs (Talbot and Osborne, 2007a; Talbot and Osborne, 2007b). This allows the use of much larger LMs, but increases the risk of errors. 3 Combination of Language Models LM training data may be any text in the output language. Typically, however, we are interested in building a MT system for a particular domain. If text resources come from a diversity of domains, some may be more suitable than others. A common stratNeural Network input output layer probability estimation wj−n+1 projection layer p1 = P (wj =1|hj ) hidden layer M cl wj−n+2 oi dj pi = P (wj =i|hj ) V shared projections H wj−1 P N discrete continuous representat"
I08-2089,N07-1062,0,0.0194608,"Missing"
I08-2089,W06-1626,0,0.0109876,"een powerfully demonstrated by Google’s submissions to the NIST evaluation campaigns. The use of billions of words of LM training data has become standard in large-scale SMT systems, and even trillion word LMs have been demonstrated. Since lookup of LM scores is one of the fundamental functions in SMT decoding, efficient storage and access of the model becomes increasingly difficult. A recent trend is to store the LM in a distributed cluster of machines, which are queried via network requests (Brants et al., 2007; Emami et al., 2007). It is easier, however, to use such large LMs in reranking (Zhang et al., 2006). Since the use of clusters of machines is not always practical (or affordable) for SMT applications, an alternative strategy is to find more efficient ways to store the LM in the working memory of a single machine, for instance by using efficient prefix trees and fewer bits to store the LM probability (Federico and Bertoldi, 2006). Also the use of lossy data structures based on Bloom filters has been demonstrated to be effective for LMs (Talbot and Osborne, 2007a; Talbot and Osborne, 2007b). This allows the use of much larger LMs, but increases the risk of errors. 3 Combination of Language Mo"
I17-2004,D16-1162,0,0.0293866,"sults compared to NMT decoding with standard beam search. Introduction Domain adaptation is a major challenge for neural machine translation (NMT). Although impressive improvements have been achieved in recent years (c.f. Bojar et al. (2016)), NMT systems require a large amount of training data and thus perform poorly relative to phrase-based machine translation (PBMT) systems in low resource and domain adaptation scenarios (Koehn and Knowles, 2017). In such situations, neural systems often produce fluent output that unfortunately contains words not licensed by the unfamiliar source sentence (Arthur et al., 2016; Tu et al., 2016). Phrase-based systems, in contrast, explicitly model the translation of all source words via coverage vectors, and tend to produce translations that are adequate but less fluent. This situation is depicted in Table 1, which contains examples of PBMT and NMT systems trained on WMT training sets which are then applied to IT texts. We present an approach that combines the best of both worlds by using the lattice output of PBMT to constrain the search space available to an NMT decoder, thereby bringing together the adequacy Search Graph Source sentence PBMT NMT Lattice Search Ta"
I17-2004,D13-1106,0,0.0627969,"Missing"
I17-2004,P17-2061,0,0.0264912,"on? While the answer depends on the amount of in-domain and out-of-domain data, we find that PBMTin × NMTin and PBMTin × NMTout perform the best. This supports previous findings (Koehn and Knowles, 2017) that PBMTin is robust when training data is insufficient. In conclusion, we recommend using lattice search with search graphs from PBMTin , and NMT models can be trained on either in-domain or out-of-domain corpora. Related Work Previous work on domain adaptation in NMT focuses on training methods such as transfer learning or fine-tuning (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016; Chu et al., 2017). This strategy begins with a strong model trained on a large out-of-domain corpus and then continuesx training on an in-domain corpus. Our approach is orthogonal in that we focus on search. Conceivably, advances in training methods might be incorporated to improve our individual NMTin models. Our lattice search algorithm is related to previous work in hybrid NMT/PBMT systems, which can be visualized on a spectrum depending on how tightly integrated the two systems are. On one end, NMT can easily be used to rerank N best lists output by PBMT; on the other, NMT can be incorporated as features i"
I17-2004,D11-1103,0,0.0778349,"Missing"
I17-2004,P14-1129,0,0.0327377,"combine in (b). This figure does not demonstrate pruning, descendants of items that fall off the beam would not be explored. 2 This is similar to PBMT stack decoding. However, in PBMT stack decoding, stacks are grouped by the number of translated source words, which is not possible in NMT, since the translation of individual source words is not tracked. 21 Corpus Medical IT Koran Subtitles WMT Words 14,301,472 3,041,677 9,848,539 114,371,754 113,165,079 Sentences 1,104,752 337,817 480,421 13,873,398 4,562,102 W/S 13 9 21 8 25 include German specific processing and Neural Network Joint Models (Devlin et al., 2014), replicating Ding et al. (2016). The PBMTin models are Moses models with standard settings, replicating Koehn and Knowles (2017). The NMT models are trained with Nematus (Sennrich et al., 2017). The NMTout models replicate Sennrich et al. (2016);4 the NMTin models replicate Koehn and Knowles (2017). We use Marian (Junczys-Dowmunt et al., 2016a) to rescore N -best lists. The search graphs are pre-processed by converting them to the OpenFST format (Allauzen et al., 2007) and applying operations to remove epsilon arcs, determinize, minimize and topsort. Since the search graphs may be prohibitive"
I17-2004,E17-3017,0,0.0437165,"Missing"
I17-2004,W16-2323,0,0.0301409,"ed source words, which is not possible in NMT, since the translation of individual source words is not tracked. 21 Corpus Medical IT Koran Subtitles WMT Words 14,301,472 3,041,677 9,848,539 114,371,754 113,165,079 Sentences 1,104,752 337,817 480,421 13,873,398 4,562,102 W/S 13 9 21 8 25 include German specific processing and Neural Network Joint Models (Devlin et al., 2014), replicating Ding et al. (2016). The PBMTin models are Moses models with standard settings, replicating Koehn and Knowles (2017). The NMT models are trained with Nematus (Sennrich et al., 2017). The NMTout models replicate Sennrich et al. (2016);4 the NMTin models replicate Koehn and Knowles (2017). We use Marian (Junczys-Dowmunt et al., 2016a) to rescore N -best lists. The search graphs are pre-processed by converting them to the OpenFST format (Allauzen et al., 2007) and applying operations to remove epsilon arcs, determinize, minimize and topsort. Since the search graphs may be prohibitively large in size, we prune them with a threshold.5 We perform 5fold cross-validation over pruning thresholds (.1, .25, .5) and lattice search beamsizes (1, 10, 100). Very aggressive pruning with a small beam limits the search to be very similar t"
I17-2004,W16-2316,0,0.0613373,"ds is not tracked. 21 Corpus Medical IT Koran Subtitles WMT Words 14,301,472 3,041,677 9,848,539 114,371,754 113,165,079 Sentences 1,104,752 337,817 480,421 13,873,398 4,562,102 W/S 13 9 21 8 25 include German specific processing and Neural Network Joint Models (Devlin et al., 2014), replicating Ding et al. (2016). The PBMTin models are Moses models with standard settings, replicating Koehn and Knowles (2017). The NMT models are trained with Nematus (Sennrich et al., 2017). The NMTout models replicate Sennrich et al. (2016);4 the NMTin models replicate Koehn and Knowles (2017). We use Marian (Junczys-Dowmunt et al., 2016a) to rescore N -best lists. The search graphs are pre-processed by converting them to the OpenFST format (Allauzen et al., 2007) and applying operations to remove epsilon arcs, determinize, minimize and topsort. Since the search graphs may be prohibitively large in size, we prune them with a threshold.5 We perform 5fold cross-validation over pruning thresholds (.1, .25, .5) and lattice search beamsizes (1, 10, 100). Very aggressive pruning with a small beam limits the search to be very similar to the PBMT output. In contrast, a very deep lattice with a large beam begins to approach the uncons"
I17-2004,E17-2058,0,0.0705903,"Missing"
I17-2004,P07-2045,1,0.0125337,"in terms of domain, but the training data is not large (relative to WMT). 3. PBMTin × NMTout : PBMT is trained on small in-domain data while NMT is trained on larger out-of-domain data. 4. PBMTout × NMTin : NMT is trained on small in-domain data while PBMT is trained on larger out-of-domain data. For each training configuration, we are interested in seeing how our proposed NMT lattice search compares to standard NMT beam search. Additionally, we compare the results of PBMT 1best decoding and PBMT N -best lists rescoring (N=500) using the same NMT model. The PBMT models are trained with Moses (Koehn et al., 2007). The PBMTout models 3 Results 4 github.com/rsennrich/wmt16-scripts Pruning removes arcs that do not appear on a lattice path whose score is within than t ⊗ w, where w is the weight of the FST’s shortest path, and t is the pruning threshold. 5 opensubtitles.org 22 Test Domain IT Medical Koran Subtitle Training Configuration PBMTout × NMTout PBMTin × NMTin PBMTin × NMTout PBMTout × NMTin PBMTout × NMTout PBMTin × NMTin PBMTin × NMTout PBMTout × NMTin PBMTout × NMTout PBMTin × NMTin PBMTin × NMTout PBMTout × NMTin PBMTout × NMTout PBMTin × NMTin PBMTin × NMTout PBMTout × NMTin PBMT 1-best 25.1 ("
I17-2004,P16-2049,0,0.037377,"us on search. Conceivably, advances in training methods might be incorporated to improve our individual NMTin models. Our lattice search algorithm is related to previous work in hybrid NMT/PBMT systems, which can be visualized on a spectrum depending on how tightly integrated the two systems are. On one end, NMT can easily be used to rerank N best lists output by PBMT; on the other, NMT can be incorporated as features in PBMT (JunczysDowmunt et al., 2016b). In the middle of the spectrum is NMT search (or re-scoring) based on constraints from PBMT. Our algorithm is conceptually very similar to Stahlberg et al. (2016), who rescore a WFSA reformulation of the Hiero formalism. Their 23 References algorithm is a breadth-first search over all the nodes of the lattice, capped by a beam. Other hybrid methods include: constraining the output vocabulary of NMT on a per-sentence basis, using bilingual information provided by PBMT (Mi et al., 2016), Minimum Bayes Risk decoding with PBMT n-gram posteriors (Stahlberg et al., 2017), and incorporating PBMT hypotheses as additional input in a modified NMT architecture (Wang et al., 2017). Related works in lattice search/re-scoring with RNNs (without NMT encoder-decoders)"
I17-2004,W17-3204,1,0.933362,"le stack-based lattice search algorithm for NMT,1 and (2) a set of domain adaptation experiments showing that PBMT lattice constraints are effective in achieving robust results compared to NMT decoding with standard beam search. Introduction Domain adaptation is a major challenge for neural machine translation (NMT). Although impressive improvements have been achieved in recent years (c.f. Bojar et al. (2016)), NMT systems require a large amount of training data and thus perform poorly relative to phrase-based machine translation (PBMT) systems in low resource and domain adaptation scenarios (Koehn and Knowles, 2017). In such situations, neural systems often produce fluent output that unfortunately contains words not licensed by the unfamiliar source sentence (Arthur et al., 2016; Tu et al., 2016). Phrase-based systems, in contrast, explicitly model the translation of all source words via coverage vectors, and tend to produce translations that are adequate but less fluent. This situation is depicted in Table 1, which contains examples of PBMT and NMT systems trained on WMT training sets which are then applied to IT texts. We present an approach that combines the best of both worlds by using the lattice ou"
I17-2004,tiedemann-2012-parallel,0,0.123673,"Missing"
I17-2004,P16-1008,0,0.025807,"decoding with standard beam search. Introduction Domain adaptation is a major challenge for neural machine translation (NMT). Although impressive improvements have been achieved in recent years (c.f. Bojar et al. (2016)), NMT systems require a large amount of training data and thus perform poorly relative to phrase-based machine translation (PBMT) systems in low resource and domain adaptation scenarios (Koehn and Knowles, 2017). In such situations, neural systems often produce fluent output that unfortunately contains words not licensed by the unfamiliar source sentence (Arthur et al., 2016; Tu et al., 2016). Phrase-based systems, in contrast, explicitly model the translation of all source words via coverage vectors, and tend to produce translations that are adequate but less fluent. This situation is depicted in Table 1, which contains examples of PBMT and NMT systems trained on WMT training sets which are then applied to IT texts. We present an approach that combines the best of both worlds by using the lattice output of PBMT to constrain the search space available to an NMT decoder, thereby bringing together the adequacy Search Graph Source sentence PBMT NMT Lattice Search Target translation F"
I17-2004,2015.iwslt-evaluation.11,0,0.0598293,"hich training configuration is best for domain adaptation? While the answer depends on the amount of in-domain and out-of-domain data, we find that PBMTin × NMTin and PBMTin × NMTout perform the best. This supports previous findings (Koehn and Knowles, 2017) that PBMTin is robust when training data is insufficient. In conclusion, we recommend using lattice search with search graphs from PBMTin , and NMT models can be trained on either in-domain or out-of-domain corpora. Related Work Previous work on domain adaptation in NMT focuses on training methods such as transfer learning or fine-tuning (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016; Chu et al., 2017). This strategy begins with a strong model trained on a large out-of-domain corpus and then continuesx training on an in-domain corpus. Our approach is orthogonal in that we focus on search. Conceivably, advances in training methods might be incorporated to improve our individual NMTin models. Our lattice search algorithm is related to previous work in hybrid NMT/PBMT systems, which can be visualized on a spectrum depending on how tightly integrated the two systems are. On one end, NMT can easily be used to rerank N best lists output by PBMT; on"
I17-2004,P16-2021,0,0.0231845,"o rerank N best lists output by PBMT; on the other, NMT can be incorporated as features in PBMT (JunczysDowmunt et al., 2016b). In the middle of the spectrum is NMT search (or re-scoring) based on constraints from PBMT. Our algorithm is conceptually very similar to Stahlberg et al. (2016), who rescore a WFSA reformulation of the Hiero formalism. Their 23 References algorithm is a breadth-first search over all the nodes of the lattice, capped by a beam. Other hybrid methods include: constraining the output vocabulary of NMT on a per-sentence basis, using bilingual information provided by PBMT (Mi et al., 2016), Minimum Bayes Risk decoding with PBMT n-gram posteriors (Stahlberg et al., 2017), and incorporating PBMT hypotheses as additional input in a modified NMT architecture (Wang et al., 2017). Related works in lattice search/re-scoring with RNNs (without NMT encoder-decoders) (Ladhak et al., 2016; Deoras et al., 2011; Hori et al., 2014) may serve as other interesting comparisons. Specifically, Auli et al. (2013) and Liu et al. (2016) provide alternatives to our approach to the problem of recombination. The former work allows the splitting of previously recombined decoder states (thresholded) whil"
I17-3002,E17-2114,1,0.840887,"Missing"
I17-3002,P07-2045,1,0.0098071,"Missing"
J15-2001,W13-2257,0,0.0166003,"ight words lets the translation accuracy drop because of search errors (Koehn et al. 2005). The use of a hard limit is undesirable for German–English and similar language pairs with significantly different syntactic structures. Several researchers have tried to address this problem. Moore and Quirk (2007) proposed improved future cost estimation to enable higher distortion limits in phrasal MT. Green, Galley, and Manning (2010) additionally proposed discriminative distortion models to achieve better translation accuracy than the baseline phrase-based system for a distortion limit of 15 words. Bisazza and Federico (2013) recently proposed a novel method to dynamically select which longrange reorderings to consider during the hypothesis extension process in a phrasebased decoder and showed an improvement in a German–English task by increasing the distortion limit to 18. Spurious Phrasal Segmentation. A problem with the phrase-based model is that there is no unique correct phrasal segmentation of a sentence. Therefore, all possible ways of segmenting a bilingual sentence consistent with the word alignment are learned and used. This leads to two problems: (i) phrase frequencies are obtained by counting all possi"
J15-2001,J93-2003,0,0.090203,"tics Volume 41, Number 2 better than state-of-the-art phrase-based systems (Moses and Phrasal) and N-gram-based systems (Ncode) on standard translation tasks. We compare the reordering component of the OSM to the Moses lexical reordering model by integrating it into Moses. Our results show that OSM outperforms lexicalized reordering on all translation tasks. The translation quality is shown to be improved further by learning generalized representations with a POS-based OSM. 1. Introduction Statistical Machine Translation (SMT) advanced near the beginning of the century from word-based models (Brown et al. 1993) towards more advanced models that take contextual information into account. Phrase-based (Koehn, Och, and Marcu 2003; Och and Ney 2004) and N-gram-based (Casacuberta and Vidal 2004; Marino ˜ et al. 2006) models are two instances of such frameworks. Although the two models have some common properties, they are substantially different. The present work is a step towards combining the benefits and remedying the flaws of these two frameworks. Phrase-based systems have a simple but effective mechanism that learns larger chunks of translation called bilingual phrases.1 Memorizing larger units enabl"
J15-2001,N10-2003,0,0.0347044,"Missing"
J15-2001,N13-1003,0,0.0149619,"ccount how previous words were translated and reordered. Although such an independence assumption is useful to reduce sparsity, it is overly generalizing and does not help to disambiguate good reorderings from the bad ones. Moreover, a vast majority of extracted phrases are singletons and the corresponding probability of orientation given phrase-pair estimates are based on a single observation. Due to sparsity, the model falls back to use one-word phrases instead, the orientation of which is ambiguous and can only be judged based on context that is ignored. This drawback has been addressed by Cherry (2013) by using sparse features for reordering models. Hard Distortion Limit. The lexicalized reordering model fails to filter out bad largescale reorderings effectively (Koehn 2010). A hard distortion limit is therefore required during decoding in order to produce good translations. A distortion limit beyond eight words lets the translation accuracy drop because of search errors (Koehn et al. 2005). The use of a hard limit is undesirable for German–English and similar language pairs with significantly different syntactic structures. Several researchers have tried to address this problem. Moore and"
J15-2001,J07-2003,0,0.0395574,"mework. We also use four supportive features: the Gap, Open Gap, Gap-distance, and Deletion counts, as described earlier (see Section 3.6.1). 6.1 Baseline Our Moses (Koehn et al. 2007) baseline systems are based on the setup described in Durrani et al. (2013b). We trained our systems with the following settings: maximum sentence length 80, grow-diag-final and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield 2011) used at runtime, distortion limit of 6, minimum Bayes-risk decoding (Kumar and Byrne 2004), cube pruning (Huang and Chiang 2007), and the no-reordering-over-punctuation heuristic. We used factored models (Koehn and Hoang 2007), for German–English and English–German. We trained the lexicalized reordering model (Koehn et al. 2005) with msd-bidirectional-fe settings. 6.2 Results Table 5 shows that the OSM results in higher gains than the lexicalized reordering model on top of a plain phrase-based baseline (Pb). The average improvement obtained using the lexicalized reordering model (Pblex ) over the baseline (Pb) is 0.50. In comparison, the average improvement obtained by using the OSM (Pbosm ) over the baseline (Pb) is 0"
J15-2001,N07-2035,0,0.0710623,"Missing"
J15-2001,2005.iwslt-1.25,0,0.0712731,"taining content words (which are less frequent than functional words). For example, the alignment link hinunterschuttete ¨ – ‘down’ is deleted and only the link hinunterschuttete ¨ – ‘poured’ is retained because ‘down’ occurs more frequently than ‘poured’. Crego and Yvon (2009) used split tokens to deal with this phenomenon. For MTU-based decoding we also need to deal with unaligned target words. For each unaligned target word, we determine the (left or right) neighbor that it appears more frequently with and align it with the same source word as this neighbor. Crego, de Gispert, and Marino ˜ (2005) and Marino ˜ et al. (2006) instead used lexical probabilities p( f |e) obtained from IBM Model 1 (Brown et al. 1993) to decide whether to attach left or right. A more sophisticated strategy based on part-of-speech entropy was proposed by Gispert and Marino ˜ (2006). 4.2 Initial Evaluation We evaluated our systems on German-to-English, French-to-English, and Spanish-toEnglish news translation for the purpose of development and evaluation. We used data from the eighth version of the Europarl Corpus and the News Commentary made available for the translation task of the Eighth Workshop on Statist"
J15-2001,2007.mtsummit-papers.16,0,0.0557062,"ce 164 Durrani et al. Operation Sequence Model versa. Notice how the reordering decision is triggered by the translation decision in the example. The probability of a gap insertion operation after the generation of the auxiliaries wurden ¨ – ‘would’ will be high because reordering is necessary in order to move the second part of the German verb complex (stimmen) to its correct position at the end of the clause. Complex reorderings can be achieved by inserting multiple gaps and/or recursively inserting a gap within a gap. Consider the generation of the example in Figure 3 (borrowed from Chiang [2007]). The generation of this bilingual sentence pair proceeds as follows: Generate(Aozhou, Australia) Generate(shi, is) Insert Gap Generate(zhiyi, one of ) At this point, the (partial) Chinese and English sentences look like this: Aozhou shi zhiyi ↓ Australia is one of The translator now jumps back and recursively inserts a gap inside of the gap before continuing translation: Jump Back (1) Insert Gap Generate(shaoshu, the few) Generate(guojia, countries) Aozhou shi shaoshu guojia ↓ zhiyi Australia is one of the few countries The rest of the sentence pair is generated as follows: Jump Back (1) Ins"
J15-2001,2009.eamt-1.10,0,0.0196584,"aligned and discontinuous targets. If a source word is aligned with multiple target words that are not consecutive, first the link to the least frequent target word is identified, and the group (consecutive adjacent words) of links containing this word is retained while the others are deleted. The intuition here is to keep the alignments containing content words (which are less frequent than functional words). For example, the alignment link hinunterschuttete ¨ – ‘down’ is deleted and only the link hinunterschuttete ¨ – ‘poured’ is retained because ‘down’ occurs more frequently than ‘poured’. Crego and Yvon (2009) used split tokens to deal with this phenomenon. For MTU-based decoding we also need to deal with unaligned target words. For each unaligned target word, we determine the (left or right) neighbor that it appears more frequently with and align it with the same source word as this neighbor. Crego, de Gispert, and Marino ˜ (2005) and Marino ˜ et al. (2006) instead used lexical probabilities p( f |e) obtained from IBM Model 1 (Brown et al. 1993) to decide whether to attach left or right. A more sophisticated strategy based on part-of-speech entropy was proposed by Gispert and Marino ˜ (2006). 4.2"
J15-2001,C10-2023,0,0.0480813,"Missing"
J15-2001,N13-1001,1,0.781265,"the subsequent parts of discontinuous target cepts to appear after the first word of the cept. During decoding we use phrase-internal alignments to hypothesize such a linearization. This is done only for the estimation of the OSM, and the target for all other purposes is generated in its original order. This heuristic allows us to deal with target discontinuities without extending the operation sequence model in complicated ways. It results in better BLEU accuracy in comparison with the post-editing of the alignments method described in Section 4.1. For details and empirical results refer to Durrani et al. (2013a) (see Table 2 therein, compare Rows 4 and 5). Note that the OSM, like the discontinuous phrase-based model (Galley and Manning 2010), allows all possible geometries as shown in Figure 7. However, because our decoder only uses continuous phrases, we cannot hypothesize (ii) and (iii) unless they appear inside of a phrase. But our model could be integrated into a discontinuous phrase-based system to overcome this limitation. 6. Further Comparative Experiments Our model, like the reordering models (Tillmann and Zhang 2005; Galley and Manning 2008) used in phrase-based decoders, is lexicalized. H"
J15-2001,P13-2071,1,0.853331,"the subsequent parts of discontinuous target cepts to appear after the first word of the cept. During decoding we use phrase-internal alignments to hypothesize such a linearization. This is done only for the estimation of the OSM, and the target for all other purposes is generated in its original order. This heuristic allows us to deal with target discontinuities without extending the operation sequence model in complicated ways. It results in better BLEU accuracy in comparison with the post-editing of the alignments method described in Section 4.1. For details and empirical results refer to Durrani et al. (2013a) (see Table 2 therein, compare Rows 4 and 5). Note that the OSM, like the discontinuous phrase-based model (Galley and Manning 2010), allows all possible geometries as shown in Figure 7. However, because our decoder only uses continuous phrases, we cannot hypothesize (ii) and (iii) unless they appear inside of a phrase. But our model could be integrated into a discontinuous phrase-based system to overcome this limitation. 6. Further Comparative Experiments Our model, like the reordering models (Tillmann and Zhang 2005; Galley and Manning 2008) used in phrase-based decoders, is lexicalized. H"
J15-2001,W13-2212,1,0.943997,"the subsequent parts of discontinuous target cepts to appear after the first word of the cept. During decoding we use phrase-internal alignments to hypothesize such a linearization. This is done only for the estimation of the OSM, and the target for all other purposes is generated in its original order. This heuristic allows us to deal with target discontinuities without extending the operation sequence model in complicated ways. It results in better BLEU accuracy in comparison with the post-editing of the alignments method described in Section 4.1. For details and empirical results refer to Durrani et al. (2013a) (see Table 2 therein, compare Rows 4 and 5). Note that the OSM, like the discontinuous phrase-based model (Galley and Manning 2010), allows all possible geometries as shown in Figure 7. However, because our decoder only uses continuous phrases, we cannot hypothesize (ii) and (iii) unless they appear inside of a phrase. But our model could be integrated into a discontinuous phrase-based system to overcome this limitation. 6. Further Comparative Experiments Our model, like the reordering models (Tillmann and Zhang 2005; Galley and Manning 2008) used in phrase-based decoders, is lexicalized. H"
J15-2001,C14-1041,1,0.834353,". It also shows the model sizes when filtered on news-test2013. A similar amount of reduction could be achieved by applying filtering to the OSMs following the language model filtering described by Heafield and Lavie (2010). 15 We also tried to amalgamate lexically driven OSM and generalized OSMs into a single model rather than using these as separate features. However, this attempt was unsuccessful (See Durrani et al. [2014] for details). 16 We also found using morphological tags and automatic word clusters to be useful in our recent IWSLT evaluation campaign (Birch, Durrani, and Koehn 2013; Durrani et al. 2014). 17 The code for the OSM in Moses can be greatly optimized but requires major modifications to source and target phrase classes in Moses. 182 Durrani et al. Operation Sequence Model Table 8 Wall-clock decoding times (in minutes) on WMT-13. Into English From English Pblex Pblex+osm Pblex Pblex+osm DE FR ES 61 108 111 88 Δ 27 163 Δ 55 142 Δ 31 143 113 74 158 Δ 15 154 Δ 41 109 Δ 35 Avg 93 131 Δ 38 110 140 Δ 30 Table 9 Data sizes (in number of sentences) and memory usage (in giga-bytes). Columns: Phrase translation and lexicalized reordering tables give overall model sizes/sizes when filtered on"
J15-2001,P11-1105,1,0.82518,"Missing"
J15-2001,D08-1089,0,0.42418,"d tasks of translating German–English, French–English, and Spanish–English pairs. Our integration gives statistically significant improvements over submission quality baseline systems. Section 7 concludes. 2. Previous Work 2.1 Phrase-Based SMT The phrase-based model (Koehn et al. 2003; Och and Ney 2004) segments a bilingual sentence pair into phrases that are continuous sequences of words. These phrases are then reordered through a lexicalized reordering model that takes into account the orientation of a phrase with respect to its previous phrase (Tillmann and Zhang 2005) or block of phrases (Galley and Manning 2008). Phrase-based models memorize local dependencies such as short reorderings, translations of idioms, and the insertion and deletion of words sensitive to local context. Phrase-based systems, however, have the following drawbacks. Handling of Non-local Dependencies. Phrase-based SMT models dependencies between words and their translations inside of a phrase well. However, dependencies across phrase boundaries are ignored because of the strong phrasal independence assumption. Consider the bilingual sentence pair shown in Figure 1(a). Reordering of the German word stimmen is internal to the phras"
J15-2001,N10-1129,0,0.038666,"Missing"
J15-2001,W11-2123,0,0.0840724,"extracting the MTUs within the phrase pair and using phrase internal alignments. The OSM is used as a feature in the log-linear framework. We also use four supportive features: the Gap, Open Gap, Gap-distance, and Deletion counts, as described earlier (see Section 3.6.1). 6.1 Baseline Our Moses (Koehn et al. 2007) baseline systems are based on the setup described in Durrani et al. (2013b). We trained our systems with the following settings: maximum sentence length 80, grow-diag-final and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield 2011) used at runtime, distortion limit of 6, minimum Bayes-risk decoding (Kumar and Byrne 2004), cube pruning (Huang and Chiang 2007), and the no-reordering-over-punctuation heuristic. We used factored models (Koehn and Hoang 2007), for German–English and English–German. We trained the lexicalized reordering model (Koehn et al. 2005) with msd-bidirectional-fe settings. 6.2 Results Table 5 shows that the OSM results in higher gains than the lexicalized reordering model on top of a plain phrase-based baseline (Pb). The average improvement obtained using the lexicalized reordering model (Pblex ) over"
J15-2001,P07-1019,0,0.00922897,"linear framework. We also use four supportive features: the Gap, Open Gap, Gap-distance, and Deletion counts, as described earlier (see Section 3.6.1). 6.1 Baseline Our Moses (Koehn et al. 2007) baseline systems are based on the setup described in Durrani et al. (2013b). We trained our systems with the following settings: maximum sentence length 80, grow-diag-final and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield 2011) used at runtime, distortion limit of 6, minimum Bayes-risk decoding (Kumar and Byrne 2004), cube pruning (Huang and Chiang 2007), and the no-reordering-over-punctuation heuristic. We used factored models (Koehn and Hoang 2007), for German–English and English–German. We trained the lexicalized reordering model (Koehn et al. 2005) with msd-bidirectional-fe settings. 6.2 Results Table 5 shows that the OSM results in higher gains than the lexicalized reordering model on top of a plain phrase-based baseline (Pb). The average improvement obtained using the lexicalized reordering model (Pblex ) over the baseline (Pb) is 0.50. In comparison, the average improvement obtained by using the OSM (Pbosm ) over the baseline (Pb) is 0"
J15-2001,koen-2004-pharaoh,0,0.186183,"used in our model is the Source Gap Width. This feature only applies in the case of a discontinuous translation unit and computes the distance between the words of a gappy cept. Let f = f1 . . . , fi , . . . , fn be a gappy source cept where xi is the index of the ith source word in the cept f . The value of the gap-width penalty is calculated as: wj = n  xi − xi−1 − 1 i=2 4. MTU-Based Search We explored two decoding strategies in this work. Our first decoder complements the model and only uses minimal translation units in left-to-right stack-based decoding, similar to that used in Pharaoh (Koehn 2004a). The overall process can be roughly divided into the following steps: (i) extraction of translation units, (ii) future cost estimation, (iii) hypothesis extension, and (iv) recombination and pruning. The last two steps are repeated iteratively until all the words in the source sentence have been translated. Our hypotheses maintain the index of the last source word covered (j), the position of the right-most source word covered so far (Z), the number of open gaps, the number of gaps so far inserted, the previously generated operations, the generated target string, and the accumulated values"
J15-2001,W04-3250,1,0.372367,"Missing"
J15-2001,J10-4005,0,0.083559,"ambiguate good reorderings from the bad ones. Moreover, a vast majority of extracted phrases are singletons and the corresponding probability of orientation given phrase-pair estimates are based on a single observation. Due to sparsity, the model falls back to use one-word phrases instead, the orientation of which is ambiguous and can only be judged based on context that is ignored. This drawback has been addressed by Cherry (2013) by using sparse features for reordering models. Hard Distortion Limit. The lexicalized reordering model fails to filter out bad largescale reorderings effectively (Koehn 2010). A hard distortion limit is therefore required during decoding in order to produce good translations. A distortion limit beyond eight words lets the translation accuracy drop because of search errors (Koehn et al. 2005). The use of a hard limit is undesirable for German–English and similar language pairs with significantly different syntactic structures. Several researchers have tried to address this problem. Moore and Quirk (2007) proposed improved future cost estimation to enable higher distortion limits in phrasal MT. Green, Galley, and Manning (2010) additionally proposed discriminative d"
J15-2001,2005.iwslt-1.8,1,0.753813,"Missing"
J15-2001,D07-1091,1,0.765644,"Missing"
J15-2001,N03-1017,1,0.0601065,"information available in phrases can be used to improve the search performance and translation quality. Finally, we probe whether integrating our model into the phrase-based SMT framework addresses the mentioned drawbacks and improves translation quality. Section 6 provides an empirical evaluation of our integration on six standard tasks of translating German–English, French–English, and Spanish–English pairs. Our integration gives statistically significant improvements over submission quality baseline systems. Section 7 concludes. 2. Previous Work 2.1 Phrase-Based SMT The phrase-based model (Koehn et al. 2003; Och and Ney 2004) segments a bilingual sentence pair into phrases that are continuous sequences of words. These phrases are then reordered through a lexicalized reordering model that takes into account the orientation of a phrase with respect to its previous phrase (Tillmann and Zhang 2005) or block of phrases (Galley and Manning 2008). Phrase-based models memorize local dependencies such as short reorderings, translations of idioms, and the insertion and deletion of words sensitive to local context. Phrase-based systems, however, have the following drawbacks. Handling of Non-local Dependenc"
J15-2001,N04-1022,0,0.17139,"Missing"
J15-2001,J06-4004,0,0.0271931,"Missing"
J15-2001,2007.mtsummit-papers.43,0,0.0235298,"rry (2013) by using sparse features for reordering models. Hard Distortion Limit. The lexicalized reordering model fails to filter out bad largescale reorderings effectively (Koehn 2010). A hard distortion limit is therefore required during decoding in order to produce good translations. A distortion limit beyond eight words lets the translation accuracy drop because of search errors (Koehn et al. 2005). The use of a hard limit is undesirable for German–English and similar language pairs with significantly different syntactic structures. Several researchers have tried to address this problem. Moore and Quirk (2007) proposed improved future cost estimation to enable higher distortion limits in phrasal MT. Green, Galley, and Manning (2010) additionally proposed discriminative distortion models to achieve better translation accuracy than the baseline phrase-based system for a distortion limit of 15 words. Bisazza and Federico (2013) recently proposed a novel method to dynamically select which longrange reorderings to consider during the hypothesis extension process in a phrasebased decoder and showed an improvement in a German–English task by increasing the distortion limit to 18. Spurious Phrasal Segmenta"
J15-2001,W11-2124,0,0.297085,"odel to the OSM and to see whether we can improve the performance further by using both models together. Our integration of the OSM into Moses gave a statistically significant improvement over a competitive baseline system in most cases. In order to assess the contribution of improved reordering versus the contribution of better modeling with MTUs in the OSM-augmented Moses system, we removed the reordering operations from the stream of operations. This is equivalent to integrating the conventional N-gram tuple sequence model (Marino ˜ et al. 2006) into a phrasebased decoder, as also tried by Niehues et al. (2011). Small gains were observed in most cases, showing that much of the improvement obtained by the OSM is due to better reordering. Generalized Operation Sequence Model. The primary strength of the OSM over the lexicalized reordering model is its ability to take advantage of the wider contextual information. In an error analysis we found that the lexically driven OSM often falls back to very small context sizes because of data sparsity. We show that this problem can be addressed by learning operation sequences over generalized representations such as POS tags. The article is organized into seven"
J15-2001,P03-1021,0,0.10251,"nments. The corpus conversion algorithm (Algorithm 1) maps each bilingual sentence pair given its alignment into a unique sequence of operations deterministically, thus maintaining a 1-to-1 correspondence. This property of the model is useful because it addresses the spurious phrasal segmentation problem in phrase-based models. A phrase-based model assigns different scores to a derivation based on which phrasal segmentation is chosen. Unlike this, the OSM assigns only one score because the model does not suffer from spurious ambiguity. 3.6.1 Discriminative Model. We use a log-linear approach (Och 2003) to make use of standard features along with several novel features that we introduce to improve endto-end accuracy. We search for a target string E that maximizes a linear combination of feature functions: Eˆ = arg max E ⎧ J ⎨ ⎩ j=1 λj hj (F, E) ⎫ ⎬ ⎭ where λj is the weight associated with the feature hj (F, E). Apart from the OSM and standard features such as target-side language model, length bonus, distortion limit, and IBM lexical features (Koehn, Och, and Marcu 2003), we used the following new features: Deletion Penalty. Deleting a source word (Generate Source Only (X)) is a common oper"
J15-2001,J03-1002,0,0.0268454,"4.2 Initial Evaluation We evaluated our systems on German-to-English, French-to-English, and Spanish-toEnglish news translation for the purpose of development and evaluation. We used data from the eighth version of the Europarl Corpus and the News Commentary made available for the translation task of the Eighth Workshop on Statistical Machine Translation.7 The bilingual corpora contained roughly 2M bilingual sentence pairs, which we obtained by concatenating news commentary (≈ 184K sentences) and Europarl for the estimation of the translation model. Word alignments were generated with GIZA++ (Och and Ney 2003), using the grow-diag-final-and heuristic8 (Koehn et al. 2005). All data are lowercased, and we use the Moses tokenizer. We took news-test-2008 as the dev set for optimization and news-test 2009-2012 for testing. The feature weights are tuned with Z-MERT (Zaidan 2009). 4.2.1 Baseline Systems. We compared our system with (i) Moses9 (Koehn et al. 2007), (ii) Phrasal10 (Cer et al. 2010), and (iii) Ncode11 (Crego, Yvon, and Marino ˜ 2011). We used 7 http://www.statmt.org/wmt13/translation-task.html 8 We also tested other symmetrization heuristics such as “Union” and “Intersection” but found the GD"
J15-2001,J04-4002,0,0.218255,"rd translation tasks. We compare the reordering component of the OSM to the Moses lexical reordering model by integrating it into Moses. Our results show that OSM outperforms lexicalized reordering on all translation tasks. The translation quality is shown to be improved further by learning generalized representations with a POS-based OSM. 1. Introduction Statistical Machine Translation (SMT) advanced near the beginning of the century from word-based models (Brown et al. 1993) towards more advanced models that take contextual information into account. Phrase-based (Koehn, Och, and Marcu 2003; Och and Ney 2004) and N-gram-based (Casacuberta and Vidal 2004; Marino ˜ et al. 2006) models are two instances of such frameworks. Although the two models have some common properties, they are substantially different. The present work is a step towards combining the benefits and remedying the flaws of these two frameworks. Phrase-based systems have a simple but effective mechanism that learns larger chunks of translation called bilingual phrases.1 Memorizing larger units enables the phrase-based model to learn local dependencies such as short-distance reorderings, idiomatic collocations, and insertions and del"
J15-2001,P05-1069,0,0.175007,"l evaluation of our integration on six standard tasks of translating German–English, French–English, and Spanish–English pairs. Our integration gives statistically significant improvements over submission quality baseline systems. Section 7 concludes. 2. Previous Work 2.1 Phrase-Based SMT The phrase-based model (Koehn et al. 2003; Och and Ney 2004) segments a bilingual sentence pair into phrases that are continuous sequences of words. These phrases are then reordered through a lexicalized reordering model that takes into account the orientation of a phrase with respect to its previous phrase (Tillmann and Zhang 2005) or block of phrases (Galley and Manning 2008). Phrase-based models memorize local dependencies such as short reorderings, translations of idioms, and the insertion and deletion of words sensitive to local context. Phrase-based systems, however, have the following drawbacks. Handling of Non-local Dependencies. Phrase-based SMT models dependencies between words and their translations inside of a phrase well. However, dependencies across phrase boundaries are ignored because of the strong phrasal independence assumption. Consider the bilingual sentence pair shown in Figure 1(a). Reordering of th"
J15-2001,P11-1086,0,0.0241791,"ns over a very competitive Moses baseline system. We showed that considering both translation and reordering context is important and ignoring reordering context results in a significant reduction in the performance. We also showed that an OSM based on surface forms suffers from data sparsity and that an OSM based on a generalized representation with part-of-speech tags improves the translation quality by considering a larger context. In the future we would like to study whether the insight of using minimal units for modeling and search based on composed rules would hold for hierarchical SMT. Vaswani et al. (2011) recently showed that a Markov model over the derivation history of minimal rules can obtain the same translation quality as using grammars formed with composed rules, which we believe is quite promising. Acknowledgments We would like to thank the anonymous reviewers and Andreas Maletti and Franc¸ois Yvon for their helpful feedback and suggestions. The research leading to these results has received funding from the European Union Seventh Framework Programme (FP7/2007-2013) under grant agreements 287658 (EU-Bridge) and 287688 (MateCat). Alexander Fraser was funded by Deutsche Forschungsgemeinsc"
J15-2001,N13-1002,0,0.0122981,"(TSM) of Marino ˜ et al. (2006), except that we use phrase-internal reordering rather than POS-based rewrite rules to do the source linearization. Table 6 shows an average improvement of just 0.13 on top of the baseline phrase-based system with lexicalized reordering, which is much lower than the 0.46 points obtained with the full operation sequence model. Bilingual translation models (without reordering) have been integrated into phrase-based systems before, either inside the decoder (Niehues et al. 2011) or to rerank the N-best candidate translations in the output of a phrase-based system (Zhang et al. 2013). Both groups reported improvements of similar magnitude when using a targetorder left-to-right TSM model for German–English and French–English translation with shared task data, but higher gains on other data sets and language pairs. Zhang et al. (2013) showed further gains by combining models with target and source left-to-right and right-to-left orders. The assumption of generating the target in monotonic order is a weakness of our work that can be addressed following Zhang et al. (2013). By generating MTUs in source order and allowing gaps and jumps on the target side, the model will be ab"
J15-2001,N10-1140,0,\N,Missing
J15-2001,2006.iwslt-evaluation.17,0,\N,Missing
J15-2001,P07-2045,1,\N,Missing
J15-2001,J04-2004,0,\N,Missing
J15-2001,2014.iwslt-evaluation.6,1,\N,Missing
J15-2001,2013.iwslt-evaluation.3,1,\N,Missing
K16-1013,Q14-1040,0,0.0517026,"e same novel words out of context. This allows us to study how cognateness and context interact, in a wellcontrolled setting. Cognates and very common words may be easy to translate without context, 1 Both languages mark for number and German occasionally marks for case. 126 Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning (CoNLL), pages 126–135, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics tended into larger models of sentence understanding. Vajjala and Meurers (2012) classify the difficulty level of longer L2 texts. Beinborn et al. (2014b) provide an overview of ways that readability measures and user background may be modeled specifically in the context of L2 learners, including through the use of cognateness features. They include a 17-word pilot study of German L1 speakers’ ability to guess the meanings of Czech cognates with no context, and hypothesize that observing the words in an understandable context would improve guessability (which we confirm in the English-German case in this work). variety of features—that is, we try to identify cues that our learners might plausibly use. 2 Motivation and Related Work In Renduchi"
K16-1013,P07-1033,0,0.0172056,"Missing"
K16-1013,P13-2121,1,0.844967,"Missing"
K16-1013,P14-1053,0,0.106382,"o do this, we must be able to predict when learners will be able to understand a novel L2 vocabulary item. In a previous study (Renduchintala et al., 2016b), we used a small set of simple features to build user-specific models of lexical understanding in macaronic sentences. The present paper evaluates a larger set of features under a more tightly controlled experimental setup. In particular, in the present paper, our model does not have to predict which context words the learner understands, because there is only one L2 word per trial: any context words are always in L1. A similar project by Labutov and Lipson (2014) likewise considers the effect of context on guessing the L2 word. However, it does not consider the effect of the L2 word’s spelling, which we show is also important. Our experimental setup, particularly the cloze task, is closely related to research in the L2 education and computer-assisted language learning (CALL) domains. Educators often use cloze tasks to evaluate learner vocabulary (though these generally use L2 context). Beinborn et al. (2014a) look at automatically predicting the difficulty of C-tests (a cloze-like task where blanks are introduced at the character level, rather than at"
K16-1013,D14-1162,0,0.0896482,"y on training data. As an outside resource for training language models and other resources consulted by our features, we used Simple English Wikipedia (Wikimedia Foundation, 2016). It contains 767,826 sentences, covers a similar set of topics to the NachrichtenLeicht.de data, and uses simple sentence structure. The sentence lengths are also comparable, with a mean of 17.6 tokens and a median of 16 tokens. This makes it well-matched for our task. We also use pre-trained vector representations of words; for these we chose to use the 300-dimensional GloVe vectors trained on a 6Btoken dataset by Pennington et al. (2014). Guessability and Guess Quality We train a log-linear model to predict the words that our subjects guess on training data, and we will check its success at this on test data. However, from an engineering perspective, we do not actually need to predict the user’s specific good or bad answers, but only whether they are good or bad. A language-learning interface should display an L2 word only when the user has a good chance of guessing its L1 translation. Thus we also assess our features and model on the easier task of predicting the guessability of a task instance x—that is, the average empiric"
K16-1013,P16-4023,1,0.611684,"word. We seek to evaluate this quantitatively and qualitatively in “extreme” cases where the context is either completely comprehensible or absent, and where the cognateness information is either present or absent. In doing so, we are able to see how learners react differently to novel words in different contexts. Our controlled experiments can serve as a proxy for incidental learning in other settings: encountering novel words in isolation (e.g. vocabulary lists), while reading in a familiar language, or while using a language-learning interface such as our own mixed-language reading system (Renduchintala et al., 2016a). We train a log-linear model to predict the translations that our novice learners will guess, given what we show them and their L1 knowledge. Within this setup, we evaluate the usefulness of a In this work, we explore how learners can infer second-language noun meanings in the context of their native language. Motivated by an interest in building interactive tools for language learning, we collect data on three word-guessing tasks, analyze their difficulty, and explore the types of errors that novice learners make. We train a log-linear model for predicting our subjects’ guesses of word mea"
K16-1013,P16-1175,1,0.850903,"word. We seek to evaluate this quantitatively and qualitatively in “extreme” cases where the context is either completely comprehensible or absent, and where the cognateness information is either present or absent. In doing so, we are able to see how learners react differently to novel words in different contexts. Our controlled experiments can serve as a proxy for incidental learning in other settings: encountering novel words in isolation (e.g. vocabulary lists), while reading in a familiar language, or while using a language-learning interface such as our own mixed-language reading system (Renduchintala et al., 2016a). We train a log-linear model to predict the translations that our novice learners will guess, given what we show them and their L1 knowledge. Within this setup, we evaluate the usefulness of a In this work, we explore how learners can infer second-language noun meanings in the context of their native language. Motivated by an interest in building interactive tools for language learning, we collect data on three word-guessing tasks, analyze their difficulty, and explore the types of errors that novice learners make. We train a log-linear model for predicting our subjects’ guesses of word mea"
K16-1013,W12-2019,0,0.0289655,"n L2 context each subject understood. We also present novice learners with the same novel words out of context. This allows us to study how cognateness and context interact, in a wellcontrolled setting. Cognates and very common words may be easy to translate without context, 1 Both languages mark for number and German occasionally marks for case. 126 Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning (CoNLL), pages 126–135, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics tended into larger models of sentence understanding. Vajjala and Meurers (2012) classify the difficulty level of longer L2 texts. Beinborn et al. (2014b) provide an overview of ways that readability measures and user background may be modeled specifically in the context of L2 learners, including through the use of cognateness features. They include a 17-word pilot study of German L1 speakers’ ability to guess the meanings of Czech cognates with no context, and hypothesize that observing the words in an understandable context would improve guessability (which we confirm in the English-German case in this work). variety of features—that is, we try to identify cues that our"
K16-1013,J90-1003,0,\N,Missing
K17-1025,D14-1179,0,0.0390085,"Missing"
K17-1025,P16-1174,0,0.130452,"e vector, and the state update rule is also interpretable—it is a type of error-correcting learning rule. In addition, the student’s state is able to predict the student’s actual response and not merely whether the response was correct. We expect that having an interpretable feature vector has better inductive bias (see experiment in section 7.1), and that it may be useful to plan future actions by smart flash card systems. Moreover, in this work we test different plausible state update rules and see how they fit actual student responses, in orer to gain insight about learning. Most recently, Settles and Meeder (2016)’s halflife regression assumes that a student’s retention of a particular skill exponentially decays with time and learns a parameter that models the rate of decay (“half-life regression”). Like Gonz´alez-Brenes et al. (2014) and Settles and Meeder (2016), our model leverages a feature-rich formulation to predict the probability of a learner correctly remembering a skill, but can also capture complex spacing/retention patterns using a neural gating mechanism. Another distinction between our work and half-life regression is that we focus on knowledge tracing within a single session, while half-"
N03-1017,P97-1003,0,0.13436,"slation model: the Giza++ [Och and Ney, 2000] toolkit for the IBM models [Brown et al., 1993]. The extraction heuristic is similar to the one used in the alignment template work by Och et al. [1999]. A number of researchers have proposed to focus on the translation of phrases that have a linguistic motivation [Yamada and Knight, 2001; Imamura, 2002]. They only consider word sequences as phrases, if they are constituents, i.e. subtrees in a syntax tree (such as a noun phrase). To identify these, we use a word-aligned corpus annotated with parse trees generated by statistical syntactic parsers [Collins, 1997; Schmidt and Schulte im Walde, 2000]. The third method for comparison is the joint phrase model proposed by Marcu and Wong [2002]. This model learns directly a phrase-level alignment of the parallel corpus. Another motivation to evaluate the performance of a phrase translation model that contains only syntactic phrases comes from recent efforts to built syntactic translation models [Yamada and Knight, 2001; Wu, 1997]. In these models, reordering of words is restricted to reordering of constituents in well-formed syntactic parse trees. When augmenting such models with phrase translations, typi"
N03-1017,P01-1030,1,0.141267,"Missing"
N03-1017,2002.tmi-papers.9,0,0.138734,"ad sufficient resources available for this language pair. We confirm the major points in experiments on additional language pairs. As the first method, we learn phrase alignments from a corpus that has been word-aligned by a training toolkit for a word-based translation model: the Giza++ [Och and Ney, 2000] toolkit for the IBM models [Brown et al., 1993]. The extraction heuristic is similar to the one used in the alignment template work by Och et al. [1999]. A number of researchers have proposed to focus on the translation of phrases that have a linguistic motivation [Yamada and Knight, 2001; Imamura, 2002]. They only consider word sequences as phrases, if they are constituents, i.e. subtrees in a syntax tree (such as a noun phrase). To identify these, we use a word-aligned corpus annotated with parse trees generated by statistical syntactic parsers [Collins, 1997; Schmidt and Schulte im Walde, 2000]. The third method for comparison is the joint phrase model proposed by Marcu and Wong [2002]. This model learns directly a phrase-level alignment of the parallel corpus. Another motivation to evaluate the performance of a phrase translation model that contains only syntactic phrases comes from rece"
N03-1017,W02-1018,1,0.154807,"Missing"
N03-1017,P00-1056,1,0.180643,"ed phrases could filter out such non-intuitive pairs. We carried out experiments to compare the performance of three different methods to build phrase translation probability tables. We also investigate a number of variations. We report most experimental results on a GermanEnglish translation task, since we had sufficient resources available for this language pair. We confirm the major points in experiments on additional language pairs. As the first method, we learn phrase alignments from a corpus that has been word-aligned by a training toolkit for a word-based translation model: the Giza++ [Och and Ney, 2000] toolkit for the IBM models [Brown et al., 1993]. The extraction heuristic is similar to the one used in the alignment template work by Och et al. [1999]. A number of researchers have proposed to focus on the translation of phrases that have a linguistic motivation [Yamada and Knight, 2001; Imamura, 2002]. They only consider word sequences as phrases, if they are constituents, i.e. subtrees in a syntax tree (such as a noun phrase). To identify these, we use a word-aligned corpus annotated with parse trees generated by statistical syntactic parsers [Collins, 1997; Schmidt and Schulte im Walde,"
N03-1017,W99-0604,1,0.228551,"f phrases that span entire syntactic subtrees is possible. It is important to know if this is a helpful or harmful restriction. 3.1 Phrases from Word-Based Alignments The Giza++ toolkit was developed to train word-based translation models from parallel corpora. As a byproduct, it generates word alignments for this data. We improve this alignment with a number of heuristics, which are described in more detail in Section 4.5. We collect all aligned phrase pairs that are consistent with the word alignment: The words in a legal phrase pair are only aligned to each other, and not to words outside [Och et al., 1999]. Given the collected phrase pairs, we estimate the phrase translation probability distribution by relative frequency:    count           count     No smoothing is performed. 3.2 Syntactic Phrases If we collect all phrase pairs that are consistent with word alignments, this includes many non-intuitive phrases. For instance, translations for phrases such as “house the” may be learned. Intuitively we would be inclined to believe that such phrases do not help: Restricting possible Consistent with Imamura [2002], we define a syntactic phrase as a word sequence that is covere"
N03-1017,W01-1408,1,0.139726,"Missing"
N03-1017,2001.mtsummit-papers.68,0,0.154084,"14k 373k BLEU  .27    .26  Table 1: Size of the phrase translation table in terms of distinct phrase pairs (maximum phrase length 4)   .25    .24    4 Experiments .23 We used the freely available Europarl corpus 2 to carry out experiments. This corpus contains over 20 million words in each of the eleven official languages of the European Union, covering the proceedings of the European Parliament 1996-2001. 1755 sentences of length 5-15 were reserved for testing. In all experiments in Section 4.1-4.6 we translate from German to English. We measure performance using the BLEU score [Papineni et al., 2001], which estimates the accuracy of translation output with respect to a reference translation. .22 4.1 Comparison of Core Methods     .21   .20  .19 AP Joint M4 Syn  .18 10k 20k 40k 80k 160k 320k Training Corpus Size   Figure 1: Comparison of the core methods: all phrase pairs consistent with a word alignment (AP), phrase pairs from the joint model (Joint), IBM Model 4 (M4), and only syntactic phrases (Syn) First, we compared the performance of the three methods for phrase extraction head-on, using the same decoder (Section 2) and the same trigram language model. Figure 1 displays th"
N03-1017,C00-2105,0,0.142196,"Missing"
N03-1017,J97-3002,0,0.294318,"i.e. subtrees in a syntax tree (such as a noun phrase). To identify these, we use a word-aligned corpus annotated with parse trees generated by statistical syntactic parsers [Collins, 1997; Schmidt and Schulte im Walde, 2000]. The third method for comparison is the joint phrase model proposed by Marcu and Wong [2002]. This model learns directly a phrase-level alignment of the parallel corpus. Another motivation to evaluate the performance of a phrase translation model that contains only syntactic phrases comes from recent efforts to built syntactic translation models [Yamada and Knight, 2001; Wu, 1997]. In these models, reordering of words is restricted to reordering of constituents in well-formed syntactic parse trees. When augmenting such models with phrase translations, typically only translation of phrases that span entire syntactic subtrees is possible. It is important to know if this is a helpful or harmful restriction. 3.1 Phrases from Word-Based Alignments The Giza++ toolkit was developed to train word-based translation models from parallel corpora. As a byproduct, it generates word alignments for this data. We improve this alignment with a number of heuristics, which are described"
N03-1017,P01-1067,0,0.277456,"r to investigate this question, we created a uniform evaluation framework that enables the comparison of different ways to build a phrase translation table. Our experiments show that high levels of performance can be achieved with fairly simple means. In fact, for most of the steps necessary to build a phrase-based system, tools and resources are freely available for researchers in the field. More sophisticated approaches that make use of syntax do not lead to better performance. In fact, imposing syntactic restrictions on phrases, as used in recently proposed syntax-based translation models [Yamada and Knight, 2001], proves to be harmful. Our experiments also show, that small phrases of up to three words are sufficient for obtaining high levels of accuracy. Performance differs widely depending on the methods used to build the phrase translation table. We found extraction heuristics based on word alignments to be better than a more principled phrase-based alignment method. However, what constitutes the best heuristic differs from language pair to language pair and varies with the size of the training corpus. 1 Introduction 2 Evaluation Framework Various researchers have improved the quality of statistica"
N03-1017,J93-2003,0,\N,Missing
N03-1017,P02-1040,0,\N,Missing
N03-2026,H01-1033,1,\N,Missing
N06-1003,H05-1085,0,0.0417204,"Missing"
N06-1003,E03-1076,1,0.247316,"more 23 Related Work Previous research on trying to overcome data sparsity issues in statistical machine translation has largely focused on introducing morphological analysis as a way of reducing the number of types observed in a training text. For example, Nissen and Ney (2004) apply morphological analyzers to English and German and are able to reduce the amount of training data needed to reach a certain level of translation quality. Goldwater and McClosky (2005) find that stemming Czech and using lemmas improves the word-to-word correspondences when training Czech-English alignment models. Koehn and Knight (2003) show how monolingual texts and parallel corpora can be used to figure out appropriate places to split German compounds. Still other approaches focus on ways of acquiring data. Resnik and Smith (2003) develop a method for gathering parallel corpora from the web. Oard et al. (2003) describe various methods employed for quickly gathering resources to create a machine translation system for a language with no initial resources. 7 Discussion cal machine translation to larger corpora and longer phrases. In Proceedings of ACL. In this paper we have shown that significant gains in coverage and transl"
N06-1003,N03-1017,1,0.047697,"Missing"
N06-1003,koen-2004-pharaoh,0,0.0150597,"ility, a reverse phrase translation probability, lexical translation probability, a reverse lexical translation probability, a word penalty, a phrase penalty, and a distortion cost. To set the weights, λm , we performed minimum error rate training (Och, 2003) on the development set using Bleu (Papineni et al., 2002) as the objective function. The phrase translation probabilities were determined using maximum likelihood estimation over phrases induced from word-level alignments produced by performing Giza++ training on each of the three training corpora. We used the Pharaoh beamsearch decoder (Koehn, 2004) to produce the translations after all of the model parameters had been set. When the baseline system encountered unknown words in the test set, its behavior was simply to reproduce the foreign word in the translated output. This is the default behavior for many systems, as noted in Section 2.1. 20 4.2 Translation with paraphrases We extracted all source language (Spanish and French) phrases up to length 10 from the test and development sets which did not have translations in phrase tables that were generated for the three training corpora. For each of these phrases we generated a list of para"
N06-1003,2005.mtsummit-papers.11,1,0.103114,"definition of the paraphrase probability to include multiple corpora, as follows: P p(e2 |e1 ) ≈ c∈C f in c p(f |e1 )p(e2 |f ) P |C| (3) where c is a parallel corpus from a set of parallel corpora C. Thus multiple corpora may be used by summing over all paraphrase probabilities calculated from a single corpus (as in Equation 1) and normalized by the number of parallel corpora. 4 Experimental Design We examined the application of paraphrases to deal with unknown phrases when translating from Spanish and French into English. We used the publicly available Europarl multilingual parallel corpus (Koehn, 2005) to create six training corpora for the two language pairs, and used the standard Europarl development and test sets. 4.1 Baseline For a baseline system we produced a phrase-based statistical machine translation system based on the log-linear formulation described in (Och and Ney, 2002) ˆ = arg max p(e|f ) e e = arg max e M X λm hm (e, f ) (4) (5) m=1 The baseline model had a total of eight feature functions, hm (e, f ): a language model probability, a phrase translation probability, a reverse phrase translation probability, lexical translation probability, a reverse lexical translation probab"
N06-1003,W02-1018,0,0.0323214,"Missing"
N06-1003,J04-2003,0,0.064585,"Missing"
N06-1003,P02-1038,0,0.339431,"Missing"
N06-1003,J04-4002,0,0.120477,"Missing"
N06-1003,P03-1021,0,0.0206105,"For a baseline system we produced a phrase-based statistical machine translation system based on the log-linear formulation described in (Och and Ney, 2002) ˆ = arg max p(e|f ) e e = arg max e M X λm hm (e, f ) (4) (5) m=1 The baseline model had a total of eight feature functions, hm (e, f ): a language model probability, a phrase translation probability, a reverse phrase translation probability, lexical translation probability, a reverse lexical translation probability, a word penalty, a phrase penalty, and a distortion cost. To set the weights, λm , we performed minimum error rate training (Och, 2003) on the development set using Bleu (Papineni et al., 2002) as the objective function. The phrase translation probabilities were determined using maximum likelihood estimation over phrases induced from word-level alignments produced by performing Giza++ training on each of the three training corpora. We used the Pharaoh beamsearch decoder (Koehn, 2004) to produce the translations after all of the model parameters had been set. When the baseline system encountered unknown words in the test set, its behavior was simply to reproduce the foreign word in the translated output. This is the default be"
N06-1003,P05-1074,1,0.629984,"slate it instead of encargarnos, and similarly for utilizado instead of usado. 3 Acquiring Paraphrases Paraphrases are alternative ways of expressing the same information within one language. The automatic generation of paraphrases has been the focus of a significant amount of research lately. Many methods for extracting paraphrases (Barzilay and McKeown, 2001; Pang et al., 2003) make use of monolingual parallel corpora, such as multiple translations of classic French novels into English, or the multiple reference translations used by many automatic evaluation metrics for machine translation. Bannard and Callison-Burch (2005) use bilingual parallel corpora to generate paraphrases. Paraphrases are identified by pivoting through phrases in another language. The foreign language translations of an English phrase are identified, all occurrences of those foreign phrases are found, and all English phrases that they translate back to are treated as potential paraphrases of the original English phrase. Figure 2 illustrates how a German phrase can be used as a point of identification for English paraphrases in this way. The method defined in Bannard and CallisonBurch (2005) has several features that make it an ideal candid"
N06-1003,P01-1008,0,0.667567,"that we employ for dealing with unknown source language words is to substitute paraphrases of those words, and then translate the paraphrases. Table 1 gives examples of paraphrases and their translations. If we had learned a translation of garantizar we could translate it instead of encargarnos, and similarly for utilizado instead of usado. 3 Acquiring Paraphrases Paraphrases are alternative ways of expressing the same information within one language. The automatic generation of paraphrases has been the focus of a significant amount of research lately. Many methods for extracting paraphrases (Barzilay and McKeown, 2001; Pang et al., 2003) make use of monolingual parallel corpora, such as multiple translations of classic French novels into English, or the multiple reference translations used by many automatic evaluation metrics for machine translation. Bannard and Callison-Burch (2005) use bilingual parallel corpora to generate paraphrases. Paraphrases are identified by pivoting through phrases in another language. The foreign language translations of an English phrase are identified, all occurrences of those foreign phrases are found, and all English phrases that they translate back to are treated as potent"
N06-1003,J93-2003,0,0.0092731,"nt approaches. 1 • Define a method for incorporating paraphrases of unseen source phrases into the statistical machine translation process. • Show that by translating paraphrases we achieve a marked improvement in coverage and translation quality, especially in the case of unknown words which to date have been left untranslated. • Argue that while we observe an improvement in Bleu score, this metric is particularly poorly suited to measuring the sort of improvements that we achieve. Introduction As with many other statistical natural language processing tasks, statistical machine translation (Brown et al., 1993) produces high quality results when ample training data is available. This is problematic for so called “low density” language pairs which do not have very large parallel corpora. For example, when words occur infrequently in a parallel corpus parameter estimates for word-level alignments can be inaccurate, which can in turn lead to inaccurate phrase translations. Limited amounts of training data can further lead to a problem of low coverage in that many phrases encountered at run-time are not ob• Present an alternative methodology for targeted manual evaluation that may be useful in other res"
N06-1003,P02-1040,0,0.110282,"ed statistical machine translation system based on the log-linear formulation described in (Och and Ney, 2002) ˆ = arg max p(e|f ) e e = arg max e M X λm hm (e, f ) (4) (5) m=1 The baseline model had a total of eight feature functions, hm (e, f ): a language model probability, a phrase translation probability, a reverse phrase translation probability, lexical translation probability, a reverse lexical translation probability, a word penalty, a phrase penalty, and a distortion cost. To set the weights, λm , we performed minimum error rate training (Och, 2003) on the development set using Bleu (Papineni et al., 2002) as the objective function. The phrase translation probabilities were determined using maximum likelihood estimation over phrases induced from word-level alignments produced by performing Giza++ training on each of the three training corpora. We used the Pharaoh beamsearch decoder (Koehn, 2004) to produce the translations after all of the model parameters had been set. When the baseline system encountered unknown words in the test set, its behavior was simply to reproduce the foreign word in the translated output. This is the default behavior for many systems, as noted in Section 2.1. 20 4.2 T"
N06-1003,J03-3002,0,0.0379518,"number of types observed in a training text. For example, Nissen and Ney (2004) apply morphological analyzers to English and German and are able to reduce the amount of training data needed to reach a certain level of translation quality. Goldwater and McClosky (2005) find that stemming Czech and using lemmas improves the word-to-word correspondences when training Czech-English alignment models. Koehn and Knight (2003) show how monolingual texts and parallel corpora can be used to figure out appropriate places to split German compounds. Still other approaches focus on ways of acquiring data. Resnik and Smith (2003) develop a method for gathering parallel corpora from the web. Oard et al. (2003) describe various methods employed for quickly gathering resources to create a machine translation system for a language with no initial resources. 7 Discussion cal machine translation to larger corpora and longer phrases. In Proceedings of ACL. In this paper we have shown that significant gains in coverage and translation quality can be had by integrating paraphrases into statistical machine translation. In effect, paraphrases introduce some amount of generalization into statistical machine translation. Whereas b"
N06-1003,E06-1032,1,\N,Missing
N06-1003,N03-2026,1,\N,Missing
N06-1003,N03-1024,0,\N,Missing
N06-1003,P05-1032,1,\N,Missing
N10-1078,E09-1008,0,0.175387,"ing postediting of machine translation in tools for translation tools are the Google Translator Toolkit (Galvez and Bhansali, 2009) and the WikiBabel project (Kumaran et al., 2008). A recent seminal effort on building interactive machine translation systems (Langlais et al., 2000; Barrachina et al., 2009) looked at a tighter integration of machine translation and human translation by developing a prediction model that interactively suggests translations to the human translator, taking her prior translation decisions into account. This approach was recently re-implemented and extended by Koehn (2009). Our study uses both post-editing and the extended interactive machine translation approach as types of assistance for translations. In our case, however, we look at monolingual translators, while prior work has focused on bilingual translators. Another effort to enable monolingual translators looked at a more linguistically motivated tool using syntactic analysis to inform their translation decisions (Albrecht et al., 2009). The quality of the translations produced by 537 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 537–545, c Los An"
N10-1078,J09-1002,0,0.160937,"Missing"
N10-1078,2009.mtsummit-tutorials.6,0,0.0954906,"Missing"
N10-1078,P09-4005,1,0.648411,"of using postediting of machine translation in tools for translation tools are the Google Translator Toolkit (Galvez and Bhansali, 2009) and the WikiBabel project (Kumaran et al., 2008). A recent seminal effort on building interactive machine translation systems (Langlais et al., 2000; Barrachina et al., 2009) looked at a tighter integration of machine translation and human translation by developing a prediction model that interactively suggests translations to the human translator, taking her prior translation decisions into account. This approach was recently re-implemented and extended by Koehn (2009). Our study uses both post-editing and the extended interactive machine translation approach as types of assistance for translations. In our case, however, we look at monolingual translators, while prior work has focused on bilingual translators. Another effort to enable monolingual translators looked at a more linguistically motivated tool using syntactic analysis to inform their translation decisions (Albrecht et al., 2009). The quality of the translations produced by 537 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 537–545, c Los An"
N10-1078,2009.mtsummit-papers.8,1,0.897478,"Missing"
N10-1078,P07-2045,1,0.00671943,"Missing"
N10-1078,W00-0507,0,0.319493,"Missing"
N10-1078,W09-0401,1,\N,Missing
N13-1116,D07-1090,0,0.0402154,"by common prefixes and suffixes. Introduction This work presents a new algorithm to search a packed data structure for high-scoring hypotheses when the score includes an N –gram language model. Many natural language processing systems have this sort of problem e.g. hypergraph search in hierarchical and syntactic machine translation (Mi et al., 2008; Klein and Manning, 2001), lattice rescoring in speech recognition, and confusion network decoding in optical character recognition (Tong and Evans, 1996). Large language models have been shown to improve quality, especially in machine translation (Brants et al., 2007; Koehn and Haddow, 2012). However, language models make search computationally expensive because they examine surface words without regard to the structure of the packed search space. Prior work, including cube pruning (Chiang, 2007), has largely treated the language model as a black box. Our new search algorithm groups hypotheses by common prefixes and suffixes, exploiting the tendency of the language model to score these hypotheses similarly. An example is shown in Figure 1. The result is a substantial improvement over the time-accuracy trade-off presented by cube pruning. The search spaces"
N13-1116,W11-2103,1,0.819339,"or the beam is full. After the loop terminates, the beam is given to the root node of the state tree; other nodes will be built lazily as described in §3.2. Overall, the algorithm visits hypergraph vertices in bottom-up order. Our beam filling algorithm runs in each vertex, making use of state trees in vertices below. The top of the tree contains full hypotheses. If a K-best list is desired, packing and extraction works the same way as with cube pruning. 4 Experiments Performance is measured by translating the 3003sentence German-English test set from the 2011 Workshop on Machine Translation (Callison-Burch et al., 2011). Two translation models were built, one hierarchical (Chiang, 2007) and one with target syntax. The target-syntax system is based on English parses from the Collins (1999) parser. Both were trained on Europarl (Koehn, 2005). The language model interpolates models built on Europarl, news commentary, and news data provided by the evaluation. Interpolation weights were tuned on the 2010 test set. Language models were built with SRILM (Stolcke, 2002), modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998), default pruning, and order 5. Feature weights were tuned with MERT (O"
N13-1116,W12-3102,1,0.129996,"Missing"
N13-1116,D12-1103,0,0.0589301,"e same as a single query. Moreover, when the language model earlier provided estimate r(wn |win−1 ), it also returned a data-structure pointer t(win ). Pointers are retained in hypotheses, edges, and partial edges for each word with an estimated probability. When context is revealed, our algorithm queries the language model with new context w1i−1 and pointer t(win ). The language model uses this pointer to immediately retrieve denominator r(wn |win−1 ) and as a starting point to retrieve numerator r(wn |w1n−1 ). It can therefore avoid looking 3 We also tested upper bounds (Huang et al., 2012; Carter et al., 2012) but the result is still approximate due to beam pruning and initial experiments showed degraded performance. 963 n−1 up r(wn ), r(wn |wn−1 ), . . . , r(wn |wi+1 ) as would normally be required with a reverse trie. 3.6 Priority Queue Our beam filling algorithm is controlled by a priority queue containing partial edges. The queue is populated by converting all outgoing hypergraph edges into partial edges and pushing them onto the queue. After this initialization, the algorithm loops. Each iteration begins by popping the top-scoring partial edge off the queue. If all nodes are leaves, then the p"
N13-1116,P05-1033,0,0.0725644,"vity makes search difficult because locally optimal hypotheses may not be globally optimal. In order to properly compute the language model score, each hypothesis is annotated with its boundary words, collectively referred to as its state (Li and Khudanpur, 2008). Hypotheses with equal state may be recombined, so a straightforward dynamic programming approach (Bar-Hillel et al., 1964) simply treats state as an additional dimension in the dynamic programming table. However, this approach quickly becomes intractable for large language models where the number of states is too large. Beam search (Chiang, 2005; Lowerre, 1976) approximates the straightforward algorithm by remembering a beam of up to k hypotheses1 in each vertex. It visits each vertex in bottom-up order, each time calling a beam filling algorithm to select k hypotheses. The parameter k is a time-accuracy trade-off: larger k increases both CPU time and accuracy. We contribute a new beam filling algorithm that improves the time-accuracy trade-off over the popular cube pruning algorithm (Chiang, 2007) discussed in §2.3. The algorithm is based on the observation that competing hypotheses come from the same imput, so their language model"
N13-1116,J07-2003,0,0.343299,"this sort of problem e.g. hypergraph search in hierarchical and syntactic machine translation (Mi et al., 2008; Klein and Manning, 2001), lattice rescoring in speech recognition, and confusion network decoding in optical character recognition (Tong and Evans, 1996). Large language models have been shown to improve quality, especially in machine translation (Brants et al., 2007; Koehn and Haddow, 2012). However, language models make search computationally expensive because they examine surface words without regard to the structure of the packed search space. Prior work, including cube pruning (Chiang, 2007), has largely treated the language model as a black box. Our new search algorithm groups hypotheses by common prefixes and suffixes, exploiting the tendency of the language model to score these hypotheses similarly. An example is shown in Figure 1. The result is a substantial improvement over the time-accuracy trade-off presented by cube pruning. The search spaces mentioned in the previous paragraph are special cases of a directed acyclic hypergraph. As used here, the difference from a normal graph is that an edge can go from one vertex to any number of vertices; this number is the arity of th"
N13-1116,P10-4002,0,0.169629,"unt for new language model context. Each edge score includes a log language model probability and possibly additive features. Whenever there is insufficient context to compute the language model probability of a word, an estimate r is used. For example, edge “is v .” incorporates estimate log r(is)r(.) into its score. The same applies to hypotheses: (the few a  ` Korea) includes estimate log r(the)r(few |the) d(n[i]) i=c d(n[c+ ]) = d(n[c]) Partial Edge d(n[c + 1+ ]) 962 because the words in left state are those with insufficient context. In common practice (Chiang, 2007; Hoang et al., 2009; Dyer et al., 2010), the estimate is taken from the language model: r = p. However, querying the language model with incomplete context leads Kneser-Ney smoothing (Kneser and Ney, 1995) to assume that backoff has occurred. An alternative is to use average-case rest costs explicitly stored in the language model (Heafield et al., 2012). Both options are used in the experiments3 . The score of a bread crumb is the maximum score of its descendants as defined in §3.3. For example, the bread crumb (  )[1+ ] has a lower score than (  )[0+ ] because the best child (the  Korea)[0+ ] and its descendants no longer c"
N13-1116,2010.iwslt-papers.8,0,0.14625,"entry then pushing multiple entries. However, our queue entries are a group of hypotheses while cube pruning’s entries are a single hypothesis. Hypotheses are usually fully scored before being placed in the priority queue. An alternative prioritizes hypotheses by their additive score. The additive score is the edge’s score plus the score of each component hypothesis, ignoring the non-additive aspect of the language model. When the additive score is used, the language model is only called k times, once for each hypothesis popped from the queue. Cube pruning can produce duplicate queue entries. Gesmundo and Henderson (2010) modified the algorithm prevent duplicates instead of using a hash table. We include their work in the experiments. Hopkins and Langmead (2009) characterized cube pruning as A* search (Hart et al., 1968) with an inadmissible heuristic. Their analysis showed deep and unbalanced search trees. Our work can be interpreted as a partial rebalancing of these search trees. 2.4 Exact Algorithms A number of exact search algorithms have been developed. We are not aware of an exact algorithm that tractably scales to the size of hypergraphs and language models used in many modern machine translation system"
N13-1116,2011.iwslt-evaluation.24,1,0.848843,"’s score includes estimated log probability log r(is)r(.) as explained earlier. The bread crumb’s score comes from its highest-scoring descendent (the few a  ` Korea) and therefore includes estimate log r(the)r(few |the). Estimates are updated as words are revealed. Continuing the example, “is (  )[0+ ] .” has best child “is (the  Korea)[0+ ] .” In this best child, the estimate r(.) is updated to r(. |Korea). Similarly, r(the) is replaced with r(the |is). Updates examine only words that have been revealed: r(few |the) remains unrevised. Updates are computed efficiently by using pointers (Heafield et al., 2011) with KenLM. To summarize, the language model computes r(wn |w1n−1 ) r(wn |win−1 ) in a single call. In the popular reverse trie data structure, the language model visits win while retrieving w1n , so the cost is the same as a single query. Moreover, when the language model earlier provided estimate r(wn |win−1 ), it also returned a data-structure pointer t(win ). Pointers are retained in hypotheses, edges, and partial edges for each word with an estimated probability. When context is revealed, our algorithm queries the language model with new context w1i−1 and pointer t(win ). The language mo"
N13-1116,D12-1107,1,0.862517,"score. The same applies to hypotheses: (the few a  ` Korea) includes estimate log r(the)r(few |the) d(n[i]) i=c d(n[c+ ]) = d(n[c]) Partial Edge d(n[c + 1+ ]) 962 because the words in left state are those with insufficient context. In common practice (Chiang, 2007; Hoang et al., 2009; Dyer et al., 2010), the estimate is taken from the language model: r = p. However, querying the language model with incomplete context leads Kneser-Ney smoothing (Kneser and Ney, 1995) to assume that backoff has occurred. An alternative is to use average-case rest costs explicitly stored in the language model (Heafield et al., 2012). Both options are used in the experiments3 . The score of a bread crumb is the maximum score of its descendants as defined in §3.3. For example, the bread crumb (  )[1+ ] has a lower score than (  )[0+ ] because the best child (the  Korea)[0+ ] and its descendants no longer contribute to the maximum. The score of partial edge “is (  )[0+ ] .” is the sum of scores from its two parts: edge “is v .” and bread crumb (  )[0+ ]. The edge’s score includes estimated log probability log r(is)r(.) as explained earlier. The bread crumb’s score comes from its highest-scoring descendent (the"
N13-1116,2009.iwslt-papers.4,1,0.374035,"e is updated to account for new language model context. Each edge score includes a log language model probability and possibly additive features. Whenever there is insufficient context to compute the language model probability of a word, an estimate r is used. For example, edge “is v .” incorporates estimate log r(is)r(.) into its score. The same applies to hypotheses: (the few a  ` Korea) includes estimate log r(the)r(few |the) d(n[i]) i=c d(n[c+ ]) = d(n[c]) Partial Edge d(n[c + 1+ ]) 962 because the words in left state are those with insufficient context. In common practice (Chiang, 2007; Hoang et al., 2009; Dyer et al., 2010), the estimate is taken from the language model: r = p. However, querying the language model with incomplete context leads Kneser-Ney smoothing (Kneser and Ney, 1995) to assume that backoff has occurred. An alternative is to use average-case rest costs explicitly stored in the language model (Heafield et al., 2012). Both options are used in the experiments3 . The score of a bread crumb is the maximum score of its descendants as defined in §3.3. For example, the bread crumb (  )[1+ ] has a lower score than (  )[0+ ] because the best child (the  Korea)[0+ ] and its des"
N13-1116,D09-1007,0,0.0138539,"ypotheses are usually fully scored before being placed in the priority queue. An alternative prioritizes hypotheses by their additive score. The additive score is the edge’s score plus the score of each component hypothesis, ignoring the non-additive aspect of the language model. When the additive score is used, the language model is only called k times, once for each hypothesis popped from the queue. Cube pruning can produce duplicate queue entries. Gesmundo and Henderson (2010) modified the algorithm prevent duplicates instead of using a hash table. We include their work in the experiments. Hopkins and Langmead (2009) characterized cube pruning as A* search (Hart et al., 1968) with an inadmissible heuristic. Their analysis showed deep and unbalanced search trees. Our work can be interpreted as a partial rebalancing of these search trees. 2.4 Exact Algorithms A number of exact search algorithms have been developed. We are not aware of an exact algorithm that tractably scales to the size of hypergraphs and language models used in many modern machine translation systems (Callison-Burch et al., 2012). The hypergraph and language model can be compiled into an integer linear program. The best hypothesis can then"
N13-1116,P07-1019,0,0.154769,"algorithm is based on the observation that competing hypotheses come from the same imput, so their language model states are often similar. Grouping hypotheses by these similar words enables our algorithm to reason over multiple hypotheses at once. The algorithm is fully described in §3. 2 Related Work 2.1 Alternatives to Bottom-Up Search Beam search visits each vertex in the hypergraph in bottom-up (topological) order. The hypergraph can also be searched in left-to-right order (Watanabe et al., 2006; Huang and Mi, 2010). Alternatively, hypotheses can be generated on demand with cube growing (Huang and Chiang, 2007), though we note that it showed little improvement in Moses (Xu and Koehn, 2012). All of these options are compatible with our algorithm. However, we only experiment with bottom-up beam search. 1 We use K to denote the number of fully-formed hypotheses requested by the user and k to denote beam size. 959 2.2 Exhaustive Beam Filling Originally, beam search was used with an exhaustive beam filling algorithm (Chiang, 2005). It generates every possible hypothesis (subject to the beams in previous vertices), selects the top k by score, and discards the remaining hypotheses. This is expensive: just"
N13-1116,D10-1027,0,0.0793645,"racy trade-off over the popular cube pruning algorithm (Chiang, 2007) discussed in §2.3. The algorithm is based on the observation that competing hypotheses come from the same imput, so their language model states are often similar. Grouping hypotheses by these similar words enables our algorithm to reason over multiple hypotheses at once. The algorithm is fully described in §3. 2 Related Work 2.1 Alternatives to Bottom-Up Search Beam search visits each vertex in the hypergraph in bottom-up (topological) order. The hypergraph can also be searched in left-to-right order (Watanabe et al., 2006; Huang and Mi, 2010). Alternatively, hypotheses can be generated on demand with cube growing (Huang and Chiang, 2007), though we note that it showed little improvement in Moses (Xu and Koehn, 2012). All of these options are compatible with our algorithm. However, we only experiment with bottom-up beam search. 1 We use K to denote the number of fully-formed hypotheses requested by the user and k to denote beam size. 959 2.2 Exhaustive Beam Filling Originally, beam search was used with an exhaustive beam filling algorithm (Chiang, 2005). It generates every possible hypothesis (subject to the beams in previous verti"
N13-1116,P12-1064,0,0.0293356,", so the cost is the same as a single query. Moreover, when the language model earlier provided estimate r(wn |win−1 ), it also returned a data-structure pointer t(win ). Pointers are retained in hypotheses, edges, and partial edges for each word with an estimated probability. When context is revealed, our algorithm queries the language model with new context w1i−1 and pointer t(win ). The language model uses this pointer to immediately retrieve denominator r(wn |win−1 ) and as a starting point to retrieve numerator r(wn |w1n−1 ). It can therefore avoid looking 3 We also tested upper bounds (Huang et al., 2012; Carter et al., 2012) but the result is still approximate due to beam pruning and initial experiments showed degraded performance. 963 n−1 up r(wn ), r(wn |wn−1 ), . . . , r(wn |wi+1 ) as would normally be required with a reverse trie. 3.6 Priority Queue Our beam filling algorithm is controlled by a priority queue containing partial edges. The queue is populated by converting all outgoing hypergraph edges into partial edges and pushing them onto the queue. After this initialization, the algorithm loops. Each iteration begins by popping the top-scoring partial edge off the queue. If all nodes"
N13-1116,D11-1127,0,0.00569847,"Missing"
N13-1116,W01-1812,0,0.0533662,"nes groups of boundary words on demand. Machine translation experiments show our algorithm makes translation 1.50 to 3.51 times as fast as with cube pruning in common cases. 1  at  in  North Korea with the DPRK Figure 1: Hypotheses are grouped by common prefixes and suffixes. Introduction This work presents a new algorithm to search a packed data structure for high-scoring hypotheses when the score includes an N –gram language model. Many natural language processing systems have this sort of problem e.g. hypergraph search in hierarchical and syntactic machine translation (Mi et al., 2008; Klein and Manning, 2001), lattice rescoring in speech recognition, and confusion network decoding in optical character recognition (Tong and Evans, 1996). Large language models have been shown to improve quality, especially in machine translation (Brants et al., 2007; Koehn and Haddow, 2012). However, language models make search computationally expensive because they examine surface words without regard to the structure of the packed search space. Prior work, including cube pruning (Chiang, 2007), has largely treated the language model as a black box. Our new search algorithm groups hypotheses by common prefixes and"
N13-1116,W12-3139,1,0.1598,"d suffixes. Introduction This work presents a new algorithm to search a packed data structure for high-scoring hypotheses when the score includes an N –gram language model. Many natural language processing systems have this sort of problem e.g. hypergraph search in hierarchical and syntactic machine translation (Mi et al., 2008; Klein and Manning, 2001), lattice rescoring in speech recognition, and confusion network decoding in optical character recognition (Tong and Evans, 1996). Large language models have been shown to improve quality, especially in machine translation (Brants et al., 2007; Koehn and Haddow, 2012). However, language models make search computationally expensive because they examine surface words without regard to the structure of the packed search space. Prior work, including cube pruning (Chiang, 2007), has largely treated the language model as a black box. Our new search algorithm groups hypotheses by common prefixes and suffixes, exploiting the tendency of the language model to score these hypotheses similarly. An example is shown in Figure 1. The result is a substantial improvement over the time-accuracy trade-off presented by cube pruning. The search spaces mentioned in the previou"
N13-1116,2005.mtsummit-papers.11,1,0.0198184,"ing algorithm runs in each vertex, making use of state trees in vertices below. The top of the tree contains full hypotheses. If a K-best list is desired, packing and extraction works the same way as with cube pruning. 4 Experiments Performance is measured by translating the 3003sentence German-English test set from the 2011 Workshop on Machine Translation (Callison-Burch et al., 2011). Two translation models were built, one hierarchical (Chiang, 2007) and one with target syntax. The target-syntax system is based on English parses from the Collins (1999) parser. Both were trained on Europarl (Koehn, 2005). The language model interpolates models built on Europarl, news commentary, and news data provided by the evaluation. Interpolation weights were tuned on the 2010 test set. Language models were built with SRILM (Stolcke, 2002), modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998), default pruning, and order 5. Feature weights were tuned with MERT (Och, 2003), beam size 1000, 100-best output, and cube pruning. Systems were built with the Moses (Hoang et al., 2009) pipeline. Measurements were collected by running the decoder on all 3003 sentences. For consistency, all -1"
N13-1116,W08-0402,0,0.739541,"can be expressed as weights on edges that sum to form hypothesis features. However, log probability from an N –gram language model is non958 Proceedings of NAACL-HLT 2013, pages 958–968, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics additive because it examines surface strings across edge and vertex boundaries. Non-additivity makes search difficult because locally optimal hypotheses may not be globally optimal. In order to properly compute the language model score, each hypothesis is annotated with its boundary words, collectively referred to as its state (Li and Khudanpur, 2008). Hypotheses with equal state may be recombined, so a straightforward dynamic programming approach (Bar-Hillel et al., 1964) simply treats state as an additional dimension in the dynamic programming table. However, this approach quickly becomes intractable for large language models where the number of states is too large. Beam search (Chiang, 2005; Lowerre, 1976) approximates the straightforward algorithm by remembering a beam of up to k hypotheses1 in each vertex. It visits each vertex in bottom-up order, each time calling a beam filling algorithm to select k hypotheses. The parameter k is a"
N13-1116,P08-1023,0,0.0325037,"iteratively refines groups of boundary words on demand. Machine translation experiments show our algorithm makes translation 1.50 to 3.51 times as fast as with cube pruning in common cases. 1  at  in  North Korea with the DPRK Figure 1: Hypotheses are grouped by common prefixes and suffixes. Introduction This work presents a new algorithm to search a packed data structure for high-scoring hypotheses when the score includes an N –gram language model. Many natural language processing systems have this sort of problem e.g. hypergraph search in hierarchical and syntactic machine translation (Mi et al., 2008; Klein and Manning, 2001), lattice rescoring in speech recognition, and confusion network decoding in optical character recognition (Tong and Evans, 1996). Large language models have been shown to improve quality, especially in machine translation (Brants et al., 2007; Koehn and Haddow, 2012). However, language models make search computationally expensive because they examine surface words without regard to the structure of the packed search space. Prior work, including cube pruning (Chiang, 2007), has largely treated the language model as a black box. Our new search algorithm groups hypothes"
N13-1116,D09-1078,0,0.0191413,"al and solving by Lagrangian relaxation (Rush and Collins, 2011). However, that work only dealt with language models up to order three. Iglesias et al. (2011) represent the search space as a recursive transition network and the language model as a weighted finite state transducer. Using standard finite state algorithms, they intersect the two automatons then exactly search for the highestscoring paths. However, the intersected automaton is too large. The authors suggested removing low probability entries from the language model, but this form of pruning negatively impacts translation quality (Moore and Quirk, 2009; Chelba et al., 2010). Their work bears some similarity to our algorithm in that partially overlapping state will be collapsed and efficiently handled together. However, the key advatage to our approach is that groups have a score that can be used for pruning before the group is expanded, enabling pruning without first constructing the intersected automaton. 2.5 Coarse-to-Fine Coarse-to-fine (Petrov et al., 2008) performs multiple pruning passes, each time with more detail. Search is a subroutine of coarse-to-fine and our work is inside search, so the two are compatible. There are several for"
N13-1116,P03-1021,0,0.0146071,"). Two translation models were built, one hierarchical (Chiang, 2007) and one with target syntax. The target-syntax system is based on English parses from the Collins (1999) parser. Both were trained on Europarl (Koehn, 2005). The language model interpolates models built on Europarl, news commentary, and news data provided by the evaluation. Interpolation weights were tuned on the 2010 test set. Language models were built with SRILM (Stolcke, 2002), modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998), default pruning, and order 5. Feature weights were tuned with MERT (Och, 2003), beam size 1000, 100-best output, and cube pruning. Systems were built with the Moses (Hoang et al., 2009) pipeline. Measurements were collected by running the decoder on all 3003 sentences. For consistency, all -101.4 Average model score Average model score -101.4 -101.5 -101.6 This work Additive cube pruning Cube pruning 0 -101.5 -101.6 1 2 CPU seconds/sentence This work Gesmundo 1 Gesmundo 2 Cube pruning 0 1 2 CPU seconds/sentence Figure 5: Hierarchial system in Moses with our algorithm, cube pruning with additive scores, and cube pruning with full scores (§2.3). The two baselines overlap."
N13-1116,P02-1040,0,0.114857,"dec 1.56 to 2.24 times as fast as the best baseline. At first, this seems to suggest that cdec is faster. In fact, the opposite is true: comparing Figures 5 and 6 reveals that cdec has a higher parsing cost than Moses5 , thereby biasing the speed ratio towards 1. In subsequent experiments, we use Moses because it more accurately reflects search costs. 4.3 Average-Case Rest Costs Previous experiments used the common-practice probability estimate described in §3.5. Figure 7 shows the impact of average-case rest costs on our algorithm and on cube pruning in Moses. We also looked at uncased BLEU (Papineni et al., 2002) scores, finding that our algorithm attains near-peak BLEU in less time. The relationship between model score and BLEU is noisy due to model errors. 4 The glue rule builds hypotheses left-to-right. In Moses, glued hypotheses start with <s> and thus have empty left state. In cdec, sentence boundary tokens are normally added last, so intermediate hypotheses have spurious left state. Running cdec with the Moses glue rule led to improved time-accuracy performance. The improved version is used in all results reported. We accounted for constant-factor differences in feature definition i.e. whether <"
N13-1116,D08-1012,0,0.480051,"Missing"
N13-1116,P11-1008,0,0.00997303,"admissible heuristic. Their analysis showed deep and unbalanced search trees. Our work can be interpreted as a partial rebalancing of these search trees. 2.4 Exact Algorithms A number of exact search algorithms have been developed. We are not aware of an exact algorithm that tractably scales to the size of hypergraphs and language models used in many modern machine translation systems (Callison-Burch et al., 2012). The hypergraph and language model can be compiled into an integer linear program. The best hypothesis can then be recovered by taking the dual and solving by Lagrangian relaxation (Rush and Collins, 2011). However, that work only dealt with language models up to order three. Iglesias et al. (2011) represent the search space as a recursive transition network and the language model as a weighted finite state transducer. Using standard finite state algorithms, they intersect the two automatons then exactly search for the highestscoring paths. However, the intersected automaton is too large. The authors suggested removing low probability entries from the language model, but this form of pruning negatively impacts translation quality (Moore and Quirk, 2009; Chelba et al., 2010). Their work bears so"
N13-1116,W96-0108,0,0.0624739,"ast as with cube pruning in common cases. 1  at  in  North Korea with the DPRK Figure 1: Hypotheses are grouped by common prefixes and suffixes. Introduction This work presents a new algorithm to search a packed data structure for high-scoring hypotheses when the score includes an N –gram language model. Many natural language processing systems have this sort of problem e.g. hypergraph search in hierarchical and syntactic machine translation (Mi et al., 2008; Klein and Manning, 2001), lattice rescoring in speech recognition, and confusion network decoding in optical character recognition (Tong and Evans, 1996). Large language models have been shown to improve quality, especially in machine translation (Brants et al., 2007; Koehn and Haddow, 2012). However, language models make search computationally expensive because they examine surface words without regard to the structure of the packed search space. Prior work, including cube pruning (Chiang, 2007), has largely treated the language model as a black box. Our new search algorithm groups hypotheses by common prefixes and suffixes, exploiting the tendency of the language model to score these hypotheses similarly. An example is shown in Figure 1. The"
N13-1116,P06-1098,0,0.0721756,"improves the time-accuracy trade-off over the popular cube pruning algorithm (Chiang, 2007) discussed in §2.3. The algorithm is based on the observation that competing hypotheses come from the same imput, so their language model states are often similar. Grouping hypotheses by these similar words enables our algorithm to reason over multiple hypotheses at once. The algorithm is fully described in §3. 2 Related Work 2.1 Alternatives to Bottom-Up Search Beam search visits each vertex in the hypergraph in bottom-up (topological) order. The hypergraph can also be searched in left-to-right order (Watanabe et al., 2006; Huang and Mi, 2010). Alternatively, hypotheses can be generated on demand with cube growing (Huang and Chiang, 2007), though we note that it showed little improvement in Moses (Xu and Koehn, 2012). All of these options are compatible with our algorithm. However, we only experiment with bottom-up beam search. 1 We use K to denote the number of fully-formed hypotheses requested by the user and k to denote beam size. 959 2.2 Exhaustive Beam Filling Originally, beam search was used with an exhaustive beam filling algorithm (Chiang, 2005). It generates every possible hypothesis (subject to the be"
N13-1116,J03-4003,0,\N,Missing
N19-1209,P17-2061,0,0.0516862,"June 7, 2019. 2019 Association for Computational Linguistics 2 Related Work A few prior studies address the drop in generaldomain NMT performance during continued training. Freitag and Al-Onaizan (2016) found that ensembling general- and in-domain models provides most of the in-domain gain from continued training while retaining most of the generaldomain performance. Ensembling doubles memory and computational requirements at translation time, which may be impractical for some applications and does not address our more fundamental goal of building a single model that is robust across domains. Chu et al. (2017) found that mixing general-domain data with the in-domain data used for continued training improved generaldomain performance of the resulting models, at the expense of training time. Dakwale and Monz (2017) share our goal of improving the general-domain performance of continued training. They introduce two novel approaches which use the initial, general-domain model to supervise the in-domain model during continued training. The first, multi-objective fine-tuning, which they denote MCL, trains the network with a joint objective of standard loglikelihood loss plus a second term based on knowle"
N19-1209,W18-2705,1,0.841876,"the general-domain model. The second, multiple-output layer fine tuning, adds new parameters to the output layer during continued training that are specific to the new domain. They found both methods performed similarly, significantly outperforming ensembling in the more challenging case where domain shift is significant, so we select the simpler MCL as our baseline. We do not assume that the domain of input sentences is known, thus we do not compare to methods such as LHUC (Vilar, 2018). Our work applies a regularization term to continued training, similar to Miceli Barone et al. (2017) and Khayrallah et al. (2018), but for the purpose of retaining generaldomain performance as opposed to improving indomain performance. 3 to-sequence models with large vocabularies. Instead we propose to approximate it with the diagonal of the empirical Fisher (Martens, 2014), which can be computed efficiently using gradients from back-propagation. At a high level, our method works as follows: 1. Train on the general-domain data, resulting in parameters θˆG . 2. Compute the diagonal of the empirical Fisher matrix F¯ . F¯i,i estimates how important the ith parameter θˆiG is to the general-domain translation task. 3. Initia"
N19-1209,D16-1139,0,0.0470448,"with the in-domain data used for continued training improved generaldomain performance of the resulting models, at the expense of training time. Dakwale and Monz (2017) share our goal of improving the general-domain performance of continued training. They introduce two novel approaches which use the initial, general-domain model to supervise the in-domain model during continued training. The first, multi-objective fine-tuning, which they denote MCL, trains the network with a joint objective of standard loglikelihood loss plus a second term based on knowledge distillation (Hinton et al., 2015; Kim and Rush, 2016) of the general-domain model. The second, multiple-output layer fine tuning, adds new parameters to the output layer during continued training that are specific to the new domain. They found both methods performed similarly, significantly outperforming ensembling in the more challenging case where domain shift is significant, so we select the simpler MCL as our baseline. We do not assume that the domain of input sentences is known, thus we do not compare to methods such as LHUC (Vilar, 2018). Our work applies a regularization term to continued training, similar to Miceli Barone et al. (2017) a"
N19-1209,P07-2045,1,0.0158311,"Model Continued Training MCL 25 40 20 20 15 General-Domain BLEU EWC (this work) 30 15 20 En→De 10 En→Ru En→Zh 10 10 5 25 30 35 40 20 25 30 35 10 20 30 40 30 30 40 25 30 20 20 De→En 10 20 Ru→En 15 10 10 35 40 45 50 Zh→En 30 25 35 40 10 20 30 40 In-Domain BLEU Figure 1: Performance trade-off for MCL and EWC: Convex hull of grid search over learning rate and regularization amount. x-axis is in-domain BLEU and y-axis is general-domain BLEU, so the desired operating point is the top right corner. Initial general-domain model (GD) and continued training (CT) points are shown for comparison. enizer (Koehn et al., 2007) and byte-pair encoding (BPE) (Sennrich et al., 2016). We train separate BPE models for the source and target languages, each with a vocabulary size of approximately 30k. BPE is trained on the out-of-domain corpus only and then applied to the training, development, and test data for both out-of-domain and in-domain datasets. Token counts for corpora are shown in Table 1. We implemented5 both EWC and MCL in Sockeye (Hieber et al., 2017). To avoid floating point issues, we normalize the empirical Fisher diagonal to have a mean value of 1.0 instead of dividing by the number of sentences. For effi"
N19-1209,W17-3204,1,0.881107,"erformance as catastrophic forgetting of general-domain knowledge. To mitigate it, we adapt Elastic Weight Consolidation (EWC)—a machine learning method for learning a new task without forgetting previous tasks. Our method retains the majority of general-domain performance lost in continued training without degrading indomain performance, outperforming the previous state-of-the-art. We also explore the full range of general-domain performance available when some in-domain degradation is acceptable. 1 Introduction Neural Machine Translation (NMT) performs poorly without large training corpora (Koehn and Knowles, 2017). Domain adaptation is required when there is sufficient data in the desired language pair but insufficient data in the desired domain (the topic, genre, style or level of formality). This work focuses on the supervised domain adaptation problem where a small in-domain parallel corpus is available for training. Continued training (Luong and Manning, 2015; Sennrich et al., 2015) (also called fine-tuning), where a model is first trained on general-domain data and then domain adapted by training on in-domain data, is a popular approach in this setting as it leads to empirical improvements in the"
N19-1209,E17-3017,0,0.0341829,"U, so the desired operating point is the top right corner. Initial general-domain model (GD) and continued training (CT) points are shown for comparison. enizer (Koehn et al., 2007) and byte-pair encoding (BPE) (Sennrich et al., 2016). We train separate BPE models for the source and target languages, each with a vocabulary size of approximately 30k. BPE is trained on the out-of-domain corpus only and then applied to the training, development, and test data for both out-of-domain and in-domain datasets. Token counts for corpora are shown in Table 1. We implemented5 both EWC and MCL in Sockeye (Hieber et al., 2017). To avoid floating point issues, we normalize the empirical Fisher diagonal to have a mean value of 1.0 instead of dividing by the number of sentences. For efficiency, we compute gradients for a batch of sentences prior to squaring and accumulating them. Fisher regularization is implemented as weight decay (towards θˆG ) in Adam (Kingma and Ba, 2014). Preliminary experiments in Ru→En found no meaningful difference in general-domain or indomain performance when computing the diagonal of F¯ on varying amounts of data ranging from 500k sentences to the full dataset. We also tried computing the d"
N19-1209,L18-1275,0,0.0213701,"Where LSN LL (θ) is the standard NLL loss on DS and λ is a hyper-parameter which weights the importance of the general-domain task. Note that the left-hand side of Equation 3 is still the loss over both the general- and in-domain translation tasks, but the right-hand side is based only on indomain data. All information from the generaldomain data has been collapsed into the second term, which is in the form of a regularizer. 4 Experiments Our general-domain training data is the concatenation of the parallel portions of the WMT17 news translation task (Bojar et al., 2017) and OpenSubtitles18 (Lison et al., 2018) corpora. For De↔En and Ru↔En, we use newstest2017 and the final 2500 lines of OpenSubtitles as our test set. We use newstest2016 and the penultimate 2500 lines of OpenSubtitles as the development set. For Zh↔En, we use the final and penultimate 4000 lines of the UN portion of the WMT data and the final and penultimate 2500 lines of OpenSubtitles as our test and development sets, respectively. We use the World Intellectual Property Organization (WIPO) COPPA-V2 corpus (JunczysDowmunt et al., 2016) as our in-domain dataset. The WIPO data consist of parallel sentences from international patent ap"
N19-1209,2015.iwslt-evaluation.11,0,0.172589,"e previous state-of-the-art. We also explore the full range of general-domain performance available when some in-domain degradation is acceptable. 1 Introduction Neural Machine Translation (NMT) performs poorly without large training corpora (Koehn and Knowles, 2017). Domain adaptation is required when there is sufficient data in the desired language pair but insufficient data in the desired domain (the topic, genre, style or level of formality). This work focuses on the supervised domain adaptation problem where a small in-domain parallel corpus is available for training. Continued training (Luong and Manning, 2015; Sennrich et al., 2015) (also called fine-tuning), where a model is first trained on general-domain data and then domain adapted by training on in-domain data, is a popular approach in this setting as it leads to empirical improvements in the targeted domain. One downside of continued training is that the adapted model’s ability to translate generaldomain sentences is severely degraded during adaptation (Freitag and Al-Onaizan, 2016). We interpret this drop in general-domain performance as catastrophic forgetting (Goodfellow et al., 2013) of general-domain translation knowledge. Degradation o"
N19-1209,D17-1156,0,0.141688,"5; Kim and Rush, 2016) of the general-domain model. The second, multiple-output layer fine tuning, adds new parameters to the output layer during continued training that are specific to the new domain. They found both methods performed similarly, significantly outperforming ensembling in the more challenging case where domain shift is significant, so we select the simpler MCL as our baseline. We do not assume that the domain of input sentences is known, thus we do not compare to methods such as LHUC (Vilar, 2018). Our work applies a regularization term to continued training, similar to Miceli Barone et al. (2017) and Khayrallah et al. (2018), but for the purpose of retaining generaldomain performance as opposed to improving indomain performance. 3 to-sequence models with large vocabularies. Instead we propose to approximate it with the diagonal of the empirical Fisher (Martens, 2014), which can be computed efficiently using gradients from back-propagation. At a high level, our method works as follows: 1. Train on the general-domain data, resulting in parameters θˆG . 2. Compute the diagonal of the empirical Fisher matrix F¯ . F¯i,i estimates how important the ith parameter θˆiG is to the general-domai"
N19-1209,P16-1009,0,0.167347,"Missing"
N19-1209,P16-1162,0,0.179336,"l-Domain BLEU EWC (this work) 30 15 20 En→De 10 En→Ru En→Zh 10 10 5 25 30 35 40 20 25 30 35 10 20 30 40 30 30 40 25 30 20 20 De→En 10 20 Ru→En 15 10 10 35 40 45 50 Zh→En 30 25 35 40 10 20 30 40 In-Domain BLEU Figure 1: Performance trade-off for MCL and EWC: Convex hull of grid search over learning rate and regularization amount. x-axis is in-domain BLEU and y-axis is general-domain BLEU, so the desired operating point is the top right corner. Initial general-domain model (GD) and continued training (CT) points are shown for comparison. enizer (Koehn et al., 2007) and byte-pair encoding (BPE) (Sennrich et al., 2016). We train separate BPE models for the source and target languages, each with a vocabulary size of approximately 30k. BPE is trained on the out-of-domain corpus only and then applied to the training, development, and test data for both out-of-domain and in-domain datasets. Token counts for corpora are shown in Table 1. We implemented5 both EWC and MCL in Sockeye (Hieber et al., 2017). To avoid floating point issues, we normalize the empirical Fisher diagonal to have a mean value of 1.0 instead of dividing by the number of sentences. For efficiency, we compute gradients for a batch of sentences"
N19-1209,W18-6313,1,0.869182,"tles as our test set. We use newstest2016 and the penultimate 2500 lines of OpenSubtitles as the development set. For Zh↔En, we use the final and penultimate 4000 lines of the UN portion of the WMT data and the final and penultimate 2500 lines of OpenSubtitles as our test and development sets, respectively. We use the World Intellectual Property Organization (WIPO) COPPA-V2 corpus (JunczysDowmunt et al., 2016) as our in-domain dataset. The WIPO data consist of parallel sentences from international patent application abstracts. WIPO De↔En data are large enough to train strong indomain systems (Thompson et al., 2018), so we truncate to 100k lines to simulate a more interesting domain adaptation scenario. We reserve 3000 lines each for in-domain development and test sets. We apply the Moses tok2064 General-Domain Model Continued Training MCL 25 40 20 20 15 General-Domain BLEU EWC (this work) 30 15 20 En→De 10 En→Ru En→Zh 10 10 5 25 30 35 40 20 25 30 35 10 20 30 40 30 30 40 25 30 20 20 De→En 10 20 Ru→En 15 10 10 35 40 45 50 Zh→En 30 25 35 40 10 20 30 40 In-Domain BLEU Figure 1: Performance trade-off for MCL and EWC: Convex hull of grid search over learning rate and regularization amount. x-axis is in-domain"
N19-1209,N18-2080,0,0.053336,"dard loglikelihood loss plus a second term based on knowledge distillation (Hinton et al., 2015; Kim and Rush, 2016) of the general-domain model. The second, multiple-output layer fine tuning, adds new parameters to the output layer during continued training that are specific to the new domain. They found both methods performed similarly, significantly outperforming ensembling in the more challenging case where domain shift is significant, so we select the simpler MCL as our baseline. We do not assume that the domain of input sentences is known, thus we do not compare to methods such as LHUC (Vilar, 2018). Our work applies a regularization term to continued training, similar to Miceli Barone et al. (2017) and Khayrallah et al. (2018), but for the purpose of retaining generaldomain performance as opposed to improving indomain performance. 3 to-sequence models with large vocabularies. Instead we propose to approximate it with the diagonal of the empirical Fisher (Martens, 2014), which can be computed efficiently using gradients from back-propagation. At a high level, our method works as follows: 1. Train on the general-domain data, resulting in parameters θˆG . 2. Compute the diagonal of the emp"
P03-1040,P02-1051,1,0.0974785,"mputationally more expensive methods. We go on to tackle the task of noun phrase translation in a maximum entropy reranking framework. Treating translation as a reranking problem instead of as a search problem enables us to use features over the full translation pair. We integrate both empirical and symbolic knowledge sources as features into our system which outperforms the best known methods in statistical machine translation. Previous work on defining subtasks within statistical machine translation has been performed on, e.g., noun-noun pair (Cao and Li, 2002) and named entity translation (Al-Onaizan and Knight, 2002). 2 Noun Phrase Translation as a Subtask In this work, we consider both noun phrases and prepositional phrases, which we will refer to as NP/PPs. We include prepositional phrases for a number of reasons. Both are attached at the clause level. Also, the translation of the preposition often depends heavily on the noun phrase (in the morning). Moreover, the distinction between noun phrases and prepositional phrases is not always clear (note the Japanese bunsetsu) or hard to separate (German joining of preposition and determiner into  in the). one lexical unit, e.g., ins in das 2.1 Definition We"
P03-1040,C02-1011,0,0.163175,"more dedicated modeling, but also the use of computationally more expensive methods. We go on to tackle the task of noun phrase translation in a maximum entropy reranking framework. Treating translation as a reranking problem instead of as a search problem enables us to use features over the full translation pair. We integrate both empirical and symbolic knowledge sources as features into our system which outperforms the best known methods in statistical machine translation. Previous work on defining subtasks within statistical machine translation has been performed on, e.g., noun-noun pair (Cao and Li, 2002) and named entity translation (Al-Onaizan and Knight, 2002). 2 Noun Phrase Translation as a Subtask In this work, we consider both noun phrases and prepositional phrases, which we will refer to as NP/PPs. We include prepositional phrases for a number of reasons. Both are attached at the clause level. Also, the translation of the preposition often depends heavily on the noun phrase (in the morning). Moreover, the distinction between noun phrases and prepositional phrases is not always clear (note the Japanese bunsetsu) or hard to separate (German joining of preposition and determiner into  in"
P03-1040,P97-1003,0,0.118298,"Missing"
P03-1040,P01-1030,1,0.617885,"best list for different sizes  A total of 737,388 NP/PP pairs are collected from the German-English Europarl corpus as training data. Certain German NP/PPs consistently do not align to NP/PPs in English (see the example in Section 2.2). These are detected at this point. The obtained data of unaligned NP/PPs can be used for dealing with these special cases. 3.3 Base Model Given the NP/PP corpus, we can use any general statistical machine translation method to train a translation system for noun phrases. As a baseline, we use an IBM Model 4 (Brown et al., 1993) system 3 with a greedy decoder4 (Germann et al., 2001). We found that phrase based models achieve better translation quality than IBM Model 4. Such models segment the input sequence into a number of (non-linguistic) phrases, translate each phrase using a phrase translation table, and allow for reordering of phrases in the output. No phrases may be dropped or added. We use a phrase translation model that extracts its phrase translation table from word alignments generated by the Giza++ toolkit. Details of this model are described by Koehn et al. (2003). To obtain an n-best list of candidate translations, we developed a beam search decoder. This de"
P03-1040,E03-1076,1,0.635786,"is common in a number of languages (German, Dutch, Finnish, Greek), and poses a serious problem for machine translation: The word Aktionsplan may not be known to the system, but if the word were broken up into Aktion and Plan, the system could easily translate it into action plan, or plan for action. The issues for breaking up compounds are: Knowing the morphological rules for joining words, resolving ambiguities of breaking up a word (Haupt Haupt-Turm or Haupt-Sturm), and finding sturm the right level of splitting granularity (Frei-Tag or Freitag). Here, we follow an approach introduced by Koehn and Knight (2003): First, we collect frequency statistics over words in our training corpus. Compounds may be broken up only into known words in the corpus. For each potential compound we check if morphological splitting rules allow us to break it up into such known words. Finally, we pick a splitting option (perhaps not breaking up the compound at all). This decision is based on the frequency of the words involved. 7 Available at http://www-rohan.sdsu.edu/ mal ouf/pubs.html  Specifically, we pick the splitting option with highest geometric mean of word frequencies of its parts  : best argmaxS  -  co"
P03-1040,N03-1017,1,0.0620163,"es. As a baseline, we use an IBM Model 4 (Brown et al., 1993) system 3 with a greedy decoder4 (Germann et al., 2001). We found that phrase based models achieve better translation quality than IBM Model 4. Such models segment the input sequence into a number of (non-linguistic) phrases, translate each phrase using a phrase translation table, and allow for reordering of phrases in the output. No phrases may be dropped or added. We use a phrase translation model that extracts its phrase translation table from word alignments generated by the Giza++ toolkit. Details of this model are described by Koehn et al. (2003). To obtain an n-best list of candidate translations, we developed a beam search decoder. This decoder employs hypothesis recombination and stores the search states in a search graph – similar to work by Ueffing et al. (2002) – which can be mined with standard finite state machine methods5 for n-best lists. 3 Available at http://www-i6.informatik.rwthaachen.de/ och/software/GIZA++.html 4 Available at http://www.isi.edu/licensed-sw /rewrite-decoder/ 5 We use the Carmel toolkit available at http://www. isi.edu/licensed-sw/carmel/  3.4 Acceptable Translations in the n-Best List One key question"
P03-1040,W02-2018,0,0.00821825,"so that #  !&quot;    *) for our sample  + of candidate translations. Maximum entropy learning finds a set of feature values $  so that ,.-/102& 43 ,6&quot;5- 02& 73 for each feature &  . These expectations are computed as sums over all candidate translations   for all inputs  : #98;:=&lt; &gt; ? @!A  & (&apos;   # 8B:=&lt; &gt;     & (&apos;C . A nice property of maximum entropy training is that it converges to a global optimum. There are a number of methods and tools available to carry out this training of feature values. We use the toolkit 7 developed by Malouf (2002). Berger et al. (1996) and Manning and Sch¨utze (1999) provide good introductions to maximum entropy learning. Note that any other machine learning, such as support vector machines, could be used as well. We chose maximum entropy for its ability to deal with both real-valued and binary features. This method is also similar to work by Och and Ney (2002), who use maximum entropy to tune model parameters. 4 Properties of NP/PP Translation We will now discuss the properties of NP/PP translation that we exploit in order to improve our NP/PP translation subsystem. The first of these (compounding of"
P03-1040,P00-1056,0,0.0124917,"Missing"
P03-1040,P02-1038,0,0.00793707,"&gt;     & (&apos;C . A nice property of maximum entropy training is that it converges to a global optimum. There are a number of methods and tools available to carry out this training of feature values. We use the toolkit 7 developed by Malouf (2002). Berger et al. (1996) and Manning and Sch¨utze (1999) provide good introductions to maximum entropy learning. Note that any other machine learning, such as support vector machines, could be used as well. We chose maximum entropy for its ability to deal with both real-valued and binary features. This method is also similar to work by Och and Ney (2002), who use maximum entropy to tune model parameters. 4 Properties of NP/PP Translation We will now discuss the properties of NP/PP translation that we exploit in order to improve our NP/PP translation subsystem. The first of these (compounding of words) is addressed by preprocessing, while the others motivate features which are used in n-best list reranking. 4.1 Compound Splitting Compounding of words, especially nouns, is common in a number of languages (German, Dutch, Finnish, Greek), and poses a serious problem for machine translation: The word Aktionsplan may not be known to the system, but"
P03-1040,P02-1040,0,0.108367,"d translating them in iso3.1 Model features features n-best list features features Reranker translation Figure 2: Design of the noun phrase translation subsystem: The base model generates an n-best list that is rescored using additional features lation with the same methods as the overall system has little impact on overall translation quality. In fact, we achieved a slight improvement in results due to the fact that NP/PPs are consistently translated as NP/PPs. A perfect NP/PP subsystem would triple the number of correctly translated sentences. Performance is also measured by the BLEU score (Papineni et al., 2002), which measures similarity to the reference translation taken from the English side of the parallel corpus. These findings indicate that solving the NP/PP translation problem would be a significant step toward improving overall translation quality, even if the overall system is not changed in any way. The findings also indicate that isolating the NP/PP translation task as a subtask does not harm performance. 3 Framework When translating a foreign input sentence, we detect its NP/PPs and translate them with an NP/PP translation subsystem. The best translation (or multiple best translations) is"
P03-1040,C00-2105,0,0.0586536,"Missing"
P03-1040,W02-1021,0,0.0178599,"Missing"
P03-1040,J93-2003,0,\N,Missing
P03-1040,J96-1002,0,\N,Missing
P05-1066,P96-1023,0,0.155377,"e-based systems currently represent the state–of–the–art in statistical machine translation. In spite of their success, a key limitation of phrase-based systems is that they make little or no direct use of syntactic information. It appears likely that syntactic information will be crucial in accurately modeling many phenomena during translation, for example systematic differences between the word order of different languages. For this reason there is currently a great deal of interest in methods which incorporate syntactic information within statistical machine translation systems (e.g., see (Alshawi, 1996; Wu, 1997; Yamada and Knight, 2001; Gildea, 2003; Melamed, 2004; Graehl and Knight, 2004; Och et al., 2004; Xia and McCord, 2004)). In this paper we describe an approach for the use of syntactic information within phrase-based SMT systems. The approach constitutes a simple, direct method for the incorporation of syntactic information in a phrase–based system, which we will show leads to significant improvements in translation accuracy. The first step of the method is to parse the source language string that is being translated. The second step is to apply a series of transformations to the re"
P05-1066,N04-1035,0,0.654328,"ranslation from French to English, where reordering rules are acquired automatically. The reordering rules in their approach operate at the level of context-free rules in the parse tree. Our method differs from that of (Xia and McCord, 2004) in a couple of important respects. First, we are considering German, which arguably has more challenging word order phenonema than French. German has relatively free word order, in contrast to both English and French: for example, there is considerable flexibility in terms of which phrases can appear in the first position in a clause. Second, Xia et. al’s (2004) use of reordering rules stated at the context-free level differs from ours. As one example, in our approach we use a single transformation that moves an infinitival verb to the first position in a verb phrase. Xia et. al’s approach would require learning of a different rule transformation for every production of the form VP => .... In practice the German parser that we are using creates relatively “flat” structures at the VP and clause levels, leading to a huge number of context-free rules (the flatness is one consequence of the relatively free word order seen within VP’s and clauses in Germa"
P05-1066,P03-1011,0,0.0444847,"he–art in statistical machine translation. In spite of their success, a key limitation of phrase-based systems is that they make little or no direct use of syntactic information. It appears likely that syntactic information will be crucial in accurately modeling many phenomena during translation, for example systematic differences between the word order of different languages. For this reason there is currently a great deal of interest in methods which incorporate syntactic information within statistical machine translation systems (e.g., see (Alshawi, 1996; Wu, 1997; Yamada and Knight, 2001; Gildea, 2003; Melamed, 2004; Graehl and Knight, 2004; Och et al., 2004; Xia and McCord, 2004)). In this paper we describe an approach for the use of syntactic information within phrase-based SMT systems. The approach constitutes a simple, direct method for the incorporation of syntactic information in a phrase–based system, which we will show leads to significant improvements in translation accuracy. The first step of the method is to parse the source language string that is being translated. The second step is to apply a series of transformations to the resulting parse tree, effectively reordering the su"
P05-1066,J96-1002,0,0.0208231,"2004) was due to the addition of IBM Model 1 translation probabilities, a non-syntactic feature. An alternative use of syntactic information is to employ an existing statistical parsing model as a language model within an SMT system. See (Charniak et al., 2003) for an approach of this form, which shows improvements in accuracy over a baseline system. 2.1.3 Research on Preprocessing Approaches Our approach involves a preprocessing step, where sentences in the language being translated are modified before being passed to an existing phrasebased translation system. A number of other researchers (Berger et al., 1996; Niessen and Ney, 2004; Xia and McCord, 2004) have described previous work on preprocessing methods. (Berger et al., 1996) describe an approach that targets translation of French phrases of the form NOUN de NOUN (e.g., conflit d’int´erˆet). This was a relatively limited study, concentrating on this one syntactic phenomenon which involves relatively local transformations (a parser was not required in this study). (Niessen and Ney, 2004) describe a method that combines morphologically–split verbs in German, and also reorders questions in English and German. Our method goes beyond this approach"
P05-1066,N04-1014,0,0.370674,"translation. In spite of their success, a key limitation of phrase-based systems is that they make little or no direct use of syntactic information. It appears likely that syntactic information will be crucial in accurately modeling many phenomena during translation, for example systematic differences between the word order of different languages. For this reason there is currently a great deal of interest in methods which incorporate syntactic information within statistical machine translation systems (e.g., see (Alshawi, 1996; Wu, 1997; Yamada and Knight, 2001; Gildea, 2003; Melamed, 2004; Graehl and Knight, 2004; Och et al., 2004; Xia and McCord, 2004)). In this paper we describe an approach for the use of syntactic information within phrase-based SMT systems. The approach constitutes a simple, direct method for the incorporation of syntactic information in a phrase–based system, which we will show leads to significant improvements in translation accuracy. The first step of the method is to parse the source language string that is being translated. The second step is to apply a series of transformations to the resulting parse tree, effectively reordering the surface string on the source language side"
P05-1066,W04-3250,1,0.336068,"here itself is translated by the reordered model. We then define 7@    7@   !  7@ ""   @ If / @ If @ If  @F Note that strictly speaking, this definition of  is not valid, as it depends on the entire set of sample 76 @ points ,045454 rather than alone. However, we believe it is a reasonable approximation to an ideal 2 The lack of per-sentence scores means that it is not possible to apply standard statistical tests such as the sign  test or the t-   , test (which would test the hypothesis   where   is the expected value under ). Note that previous work (Koehn, 2004; Zhang and Vogel, 2004) has suggested the use of bootstrap tests (Efron and Tibshirani, 1993) for the calculation of confidence intervals for Bleu scores. (Koehn, 2004) gives empirical evidence that these give accurate estimates for Bleu statistics. However, correctness of the bootstrap method relies on some technical properties of the statistic (e.g., Bleu scores) being used (e.g., see (Wasserman, 2004) theorem 8.3); (Koehn, 2004; Zhang and Vogel, 2004) do not discuss whether Bleu scores meet any such criteria, which makes us uncertain of their correctness when applied to Bleu scores."
P05-1066,2003.mtsummit-papers.6,0,0.0395528,"ehn and Knight, 2003) apply a reranking approach to the sub-task of noun-phrase translation. (Och et al., 2004; Shen et al., 2004) describe the use of syntactic features in reranking the output of a full translation system, but the syntactic features give very small gains: for example the majority of the gain in performance in the experiments in (Och et al., 2004) was due to the addition of IBM Model 1 translation probabilities, a non-syntactic feature. An alternative use of syntactic information is to employ an existing statistical parsing model as a language model within an SMT system. See (Charniak et al., 2003) for an approach of this form, which shows improvements in accuracy over a baseline system. 2.1.3 Research on Preprocessing Approaches Our approach involves a preprocessing step, where sentences in the language being translated are modified before being passed to an existing phrasebased translation system. A number of other researchers (Berger et al., 1996; Niessen and Ney, 2004; Xia and McCord, 2004) have described previous work on preprocessing methods. (Berger et al., 1996) describe an approach that targets translation of French phrases of the form NOUN de NOUN (e.g., conflit d’int´erˆet)."
P05-1066,P03-1040,1,0.323197,"guages. One class of approaches make use of “bitext” grammars which simultaneously parse both the source and target languages. Another class of approaches make use of syntactic information in the target language alone, effectively transforming the translation problem into a parsing problem. Note that these models have radically different structures and parameterizations from phrase–based models for SMT. As yet, these systems have not shown significant gains in accuracy in comparison to phrase-based systems. Reranking methods have also been proposed as a method for using syntactic information (Koehn and Knight, 2003; Och et al., 2004; Shen et al., 2004). In these approaches a baseline system is used to generate -best output. Syntactic features are then used in a second model that reranks the -best lists, in an attempt to improve over the baseline approach. (Koehn and Knight, 2003) apply a reranking approach to the sub-task of noun-phrase translation. (Och et al., 2004; Shen et al., 2004) describe the use of syntactic features in reranking the output of a full translation system, but the syntactic features give very small gains: for example the majority of the gain in performance in the experiments in (Oc"
P05-1066,N03-1017,1,0.133673,"actors, including a cost of skipping over a phrase of length 4 (i.e., Ihnen die entsprechenden Anmerkungen) in the German string. 534 The ability to penalise “skips” of this type, and the potential to model multi-word phrases, are essentially the main strategies that the phrase-based system is able to employ when modeling differing word-order across different languages. In practice, when training the parameters of an SMT system, for example using the discriminative methods of (Och, 2003), the cost for skips of this kind is typically set to a very high value. In experiments with the system of (Koehn et al., 2003) we have found that in practice a large number of complete translations are completely monotonic (i.e., have skips), suggesting that the system has difficulty learning exactly what points in the translation should allow reordering. In summary, phrase-based systems have relatively limited potential to model word-order differences between different languages. The reordering stage described in this paper attempts to modify the source language (e.g., German) in such a way that its word order is very similar to that seen in the target language (e.g., English). In an ideal approach, the resulting tr"
P05-1066,1993.eamt-1.1,0,0.0697396,"7@   !  7@ ""   @ If / @ If @ If  @F Note that strictly speaking, this definition of  is not valid, as it depends on the entire set of sample 76 @ points ,045454 rather than alone. However, we believe it is a reasonable approximation to an ideal 2 The lack of per-sentence scores means that it is not possible to apply standard statistical tests such as the sign  test or the t-   , test (which would test the hypothesis   where   is the expected value under ). Note that previous work (Koehn, 2004; Zhang and Vogel, 2004) has suggested the use of bootstrap tests (Efron and Tibshirani, 1993) for the calculation of confidence intervals for Bleu scores. (Koehn, 2004) gives empirical evidence that these give accurate estimates for Bleu statistics. However, correctness of the bootstrap method relies on some technical properties of the statistic (e.g., Bleu scores) being used (e.g., see (Wasserman, 2004) theorem 8.3); (Koehn, 2004; Zhang and Vogel, 2004) do not discuss whether Bleu scores meet any such criteria, which makes us uncertain of their correctness when applied to Bleu scores.    0 538  that indicates whether the translafunction  tions have improved or not under the"
P05-1066,W02-1018,0,0.0417534,"Missing"
P05-1066,P02-1040,0,0.11539,"Missing"
P05-1066,P04-1083,0,0.0268447,"istical machine translation. In spite of their success, a key limitation of phrase-based systems is that they make little or no direct use of syntactic information. It appears likely that syntactic information will be crucial in accurately modeling many phenomena during translation, for example systematic differences between the word order of different languages. For this reason there is currently a great deal of interest in methods which incorporate syntactic information within statistical machine translation systems (e.g., see (Alshawi, 1996; Wu, 1997; Yamada and Knight, 2001; Gildea, 2003; Melamed, 2004; Graehl and Knight, 2004; Och et al., 2004; Xia and McCord, 2004)). In this paper we describe an approach for the use of syntactic information within phrase-based SMT systems. The approach constitutes a simple, direct method for the incorporation of syntactic information in a phrase–based system, which we will show leads to significant improvements in translation accuracy. The first step of the method is to parse the source language string that is being translated. The second step is to apply a series of transformations to the resulting parse tree, effectively reordering the surface string on"
P05-1066,N04-1023,0,0.170201,"f “bitext” grammars which simultaneously parse both the source and target languages. Another class of approaches make use of syntactic information in the target language alone, effectively transforming the translation problem into a parsing problem. Note that these models have radically different structures and parameterizations from phrase–based models for SMT. As yet, these systems have not shown significant gains in accuracy in comparison to phrase-based systems. Reranking methods have also been proposed as a method for using syntactic information (Koehn and Knight, 2003; Och et al., 2004; Shen et al., 2004). In these approaches a baseline system is used to generate -best output. Syntactic features are then used in a second model that reranks the -best lists, in an attempt to improve over the baseline approach. (Koehn and Knight, 2003) apply a reranking approach to the sub-task of noun-phrase translation. (Och et al., 2004; Shen et al., 2004) describe the use of syntactic features in reranking the output of a full translation system, but the syntactic features give very small gains: for example the majority of the gain in performance in the experiments in (Och et al., 2004) was due to the additio"
P05-1066,J04-2003,0,0.0291282,"addition of IBM Model 1 translation probabilities, a non-syntactic feature. An alternative use of syntactic information is to employ an existing statistical parsing model as a language model within an SMT system. See (Charniak et al., 2003) for an approach of this form, which shows improvements in accuracy over a baseline system. 2.1.3 Research on Preprocessing Approaches Our approach involves a preprocessing step, where sentences in the language being translated are modified before being passed to an existing phrasebased translation system. A number of other researchers (Berger et al., 1996; Niessen and Ney, 2004; Xia and McCord, 2004) have described previous work on preprocessing methods. (Berger et al., 1996) describe an approach that targets translation of French phrases of the form NOUN de NOUN (e.g., conflit d’int´erˆet). This was a relatively limited study, concentrating on this one syntactic phenomenon which involves relatively local transformations (a parser was not required in this study). (Niessen and Ney, 2004) describe a method that combines morphologically–split verbs in German, and also reorders questions in English and German. Our method goes beyond this approach in several respects, fo"
P05-1066,P03-1021,0,0.14218,"e same time absorbing “aushaendigen” from the German string. The cost of this decoding step will involve a number of factors, including a cost of skipping over a phrase of length 4 (i.e., Ihnen die entsprechenden Anmerkungen) in the German string. 534 The ability to penalise “skips” of this type, and the potential to model multi-word phrases, are essentially the main strategies that the phrase-based system is able to employ when modeling differing word-order across different languages. In practice, when training the parameters of an SMT system, for example using the discriminative methods of (Och, 2003), the cost for skips of this kind is typically set to a very high value. In experiments with the system of (Koehn et al., 2003) we have found that in practice a large number of complete translations are completely monotonic (i.e., have skips), suggesting that the system has difficulty learning exactly what points in the translation should allow reordering. In summary, phrase-based systems have relatively limited potential to model word-order differences between different languages. The reordering stage described in this paper attempts to modify the source language (e.g., German) in such a way"
P05-1066,N04-1021,0,0.183195,"their success, a key limitation of phrase-based systems is that they make little or no direct use of syntactic information. It appears likely that syntactic information will be crucial in accurately modeling many phenomena during translation, for example systematic differences between the word order of different languages. For this reason there is currently a great deal of interest in methods which incorporate syntactic information within statistical machine translation systems (e.g., see (Alshawi, 1996; Wu, 1997; Yamada and Knight, 2001; Gildea, 2003; Melamed, 2004; Graehl and Knight, 2004; Och et al., 2004; Xia and McCord, 2004)). In this paper we describe an approach for the use of syntactic information within phrase-based SMT systems. The approach constitutes a simple, direct method for the incorporation of syntactic information in a phrase–based system, which we will show leads to significant improvements in translation accuracy. The first step of the method is to parse the source language string that is being translated. The second step is to apply a series of transformations to the resulting parse tree, effectively reordering the surface string on the source language side of the translatio"
P05-1066,W99-0604,0,0.107073,"Missing"
P05-1066,J97-3002,0,0.887209,"currently represent the state–of–the–art in statistical machine translation. In spite of their success, a key limitation of phrase-based systems is that they make little or no direct use of syntactic information. It appears likely that syntactic information will be crucial in accurately modeling many phenomena during translation, for example systematic differences between the word order of different languages. For this reason there is currently a great deal of interest in methods which incorporate syntactic information within statistical machine translation systems (e.g., see (Alshawi, 1996; Wu, 1997; Yamada and Knight, 2001; Gildea, 2003; Melamed, 2004; Graehl and Knight, 2004; Och et al., 2004; Xia and McCord, 2004)). In this paper we describe an approach for the use of syntactic information within phrase-based SMT systems. The approach constitutes a simple, direct method for the incorporation of syntactic information in a phrase–based system, which we will show leads to significant improvements in translation accuracy. The first step of the method is to parse the source language string that is being translated. The second step is to apply a series of transformations to the resulting pa"
P05-1066,C04-1073,0,0.813091,"key limitation of phrase-based systems is that they make little or no direct use of syntactic information. It appears likely that syntactic information will be crucial in accurately modeling many phenomena during translation, for example systematic differences between the word order of different languages. For this reason there is currently a great deal of interest in methods which incorporate syntactic information within statistical machine translation systems (e.g., see (Alshawi, 1996; Wu, 1997; Yamada and Knight, 2001; Gildea, 2003; Melamed, 2004; Graehl and Knight, 2004; Och et al., 2004; Xia and McCord, 2004)). In this paper we describe an approach for the use of syntactic information within phrase-based SMT systems. The approach constitutes a simple, direct method for the incorporation of syntactic information in a phrase–based system, which we will show leads to significant improvements in translation accuracy. The first step of the method is to parse the source language string that is being translated. The second step is to apply a series of transformations to the resulting parse tree, effectively reordering the surface string on the source language side of the translation system. The goal of t"
P05-1066,P01-1067,0,0.9072,"represent the state–of–the–art in statistical machine translation. In spite of their success, a key limitation of phrase-based systems is that they make little or no direct use of syntactic information. It appears likely that syntactic information will be crucial in accurately modeling many phenomena during translation, for example systematic differences between the word order of different languages. For this reason there is currently a great deal of interest in methods which incorporate syntactic information within statistical machine translation systems (e.g., see (Alshawi, 1996; Wu, 1997; Yamada and Knight, 2001; Gildea, 2003; Melamed, 2004; Graehl and Knight, 2004; Och et al., 2004; Xia and McCord, 2004)). In this paper we describe an approach for the use of syntactic information within phrase-based SMT systems. The approach constitutes a simple, direct method for the incorporation of syntactic information in a phrase–based system, which we will show leads to significant improvements in translation accuracy. The first step of the method is to parse the source language string that is being translated. The second step is to apply a series of transformations to the resulting parse tree, effectively reo"
P05-1066,2004.tmi-1.9,0,0.0405392,"s translated by the reordered model. We then define 7@    7@   !  7@ ""   @ If / @ If @ If  @F Note that strictly speaking, this definition of  is not valid, as it depends on the entire set of sample 76 @ points ,045454 rather than alone. However, we believe it is a reasonable approximation to an ideal 2 The lack of per-sentence scores means that it is not possible to apply standard statistical tests such as the sign  test or the t-   , test (which would test the hypothesis   where   is the expected value under ). Note that previous work (Koehn, 2004; Zhang and Vogel, 2004) has suggested the use of bootstrap tests (Efron and Tibshirani, 1993) for the calculation of confidence intervals for Bleu scores. (Koehn, 2004) gives empirical evidence that these give accurate estimates for Bleu statistics. However, correctness of the bootstrap method relies on some technical properties of the statistic (e.g., Bleu scores) being used (e.g., see (Wasserman, 2004) theorem 8.3); (Koehn, 2004; Zhang and Vogel, 2004) do not discuss whether Bleu scores meet any such criteria, which makes us uncertain of their correctness when applied to Bleu scores.    0 538  that indicate"
P05-1066,C00-2163,0,\N,Missing
P05-1066,J93-2003,0,\N,Missing
P05-1066,P03-1013,0,\N,Missing
P07-2045,N03-2002,0,0.152204,"nfusion networks. This input type has been used successfully for speech to text translation (Shen et al. 2006). Every factor on the target language can have its own language model. Since many factors, like lemmas and POS tags, are less sparse than surface forms, it is possible to create a higher order language models for these factors. This may encourage more syntactically correct output. In Figure 3 we apply two language models, indicated by the shaded arrows, one over the words and another over the lemmas. Moses is also able to integrate factored language models, such as those described in (Bilmes and Kirchhoff 2003) and (Axelrod 2006). 4 Confusion Network Decoding Machine translation input currently takes the form of simple sequences of words. However, there are increasing demands to integrate machine translation technology into larger information processing systems with upstream NLP/speech processing tools (such as named entity recognizers, speech recognizers, morphological analyzers, etc.). These upstream processes tend to generate multiple, erroneous hypotheses with varying confidence. Current MT systems are designed to process only one input hypothesis, making them vulnerable to errors in the input."
P07-2045,koen-2004-pharaoh,0,0.148177,"to be duplicated. This has also hindered effective comparisons of the different elements of the systems. By providing a free and complete toolkit, we hope that this will stimulate the development of the field. For this system to be adopted by the community, it must demonstrate performance that is comparable to the best available systems. Moses has shown that it achieves results comparable to the most competitive and widely used statistical machine translation systems in translation quality and run-time (Shen et al. 2006). It features all the capabilities of the closed sourced Pharaoh decoder (Koehn 2004). Apart from providing an open-source toolkit for SMT, a further motivation for Moses is to extend phrase-based translation with factors and confusion network decoding. The current phrase-based approach to statistical machine translation is limited to the mapping of small text chunks without any explicit use of linguistic information, be it morphological, syntactic, or semantic. These additional sources of information have been shown to be valuable when integrated into pre-processing or post-processing steps. Moses also integrates confusion network decoding, which allows the translation of amb"
P07-2045,D07-1091,1,0.158367,"Missing"
P07-2045,N03-1017,1,0.161374,"informatik.rwth-aachen.de. 5 redpony@umd.edu. 6 bojar@ufal.ms.mff.cuni.cz. 7 07aec_2@williams.edu. 8 evh4@cornell.edu 2 Abstract We describe an open-source toolkit for statistical machine translation whose novel contributions are (a) support for linguistically motivated factors, (b) confusion network decoding, and (c) efficient data formats for translation models and language models. In addition to the SMT decoder, the toolkit also includes a wide variety of tools for training, tuning and applying the system to many translation tasks. 1 Motivation Phrase-based statistical machine translation (Koehn et al. 2003) has emerged as the dominant paradigm in machine translation research. However, until now, most work in this field has been carried out on proprietary and in-house research systems. This lack of openness has created a high barrier to entry for researchers as many of the components required have had to be duplicated. This has also hindered effective comparisons of the different elements of the systems. By providing a free and complete toolkit, we hope that this will stimulate the development of the field. For this system to be adopted by the community, it must demonstrate performance that is co"
P07-2045,P03-1021,0,0.176468,"ent data structures in Moses for the memory-intensive translation model and language model allow the exploitation of much larger data resources with limited hardware. 177 Proceedings of the ACL 2007 Demo and Poster Sessions, pages 177–180, c Prague, June 2007. 2007 Association for Computational Linguistics 2 Toolkit 3 The toolkit is a complete out-of-the-box translation system for academic research. It consists of all the components needed to preprocess data, train the language models and the translation models. It also contains tools for tuning these models using minimum error rate training (Och 2003) and evaluating the resulting translations using the BLEU score (Papineni et al. 2002). Moses uses standard external tools for some of the tasks to avoid duplication, such as GIZA++ (Och and Ney 2003) for word alignments and SRILM for language modeling. Also, since these tasks are often CPU intensive, the toolkit has been designed to work with Sun Grid Engine parallel environment to increase throughput. In order to unify the experimental stages, a utility has been developed to run repeatable experiments. This uses the tools contained in Moses and requires minimal changes to set up and customiz"
P07-2045,J03-1002,0,0.164868,"L 2007 Demo and Poster Sessions, pages 177–180, c Prague, June 2007. 2007 Association for Computational Linguistics 2 Toolkit 3 The toolkit is a complete out-of-the-box translation system for academic research. It consists of all the components needed to preprocess data, train the language models and the translation models. It also contains tools for tuning these models using minimum error rate training (Och 2003) and evaluating the resulting translations using the BLEU score (Papineni et al. 2002). Moses uses standard external tools for some of the tasks to avoid duplication, such as GIZA++ (Och and Ney 2003) for word alignments and SRILM for language modeling. Also, since these tasks are often CPU intensive, the toolkit has been designed to work with Sun Grid Engine parallel environment to increase throughput. In order to unify the experimental stages, a utility has been developed to run repeatable experiments. This uses the tools contained in Moses and requires minimal changes to set up and customize. The toolkit has been hosted and developed under sourceforge.net since inception. Moses has an active research community and has reached over 1000 downloads as of 1st March 2007. The main online pre"
P07-2045,P02-1040,0,0.148118,"d language model allow the exploitation of much larger data resources with limited hardware. 177 Proceedings of the ACL 2007 Demo and Poster Sessions, pages 177–180, c Prague, June 2007. 2007 Association for Computational Linguistics 2 Toolkit 3 The toolkit is a complete out-of-the-box translation system for academic research. It consists of all the components needed to preprocess data, train the language models and the translation models. It also contains tools for tuning these models using minimum error rate training (Och 2003) and evaluating the resulting translations using the BLEU score (Papineni et al. 2002). Moses uses standard external tools for some of the tasks to avoid duplication, such as GIZA++ (Och and Ney 2003) for word alignments and SRILM for language modeling. Also, since these tasks are often CPU intensive, the toolkit has been designed to work with Sun Grid Engine parallel environment to increase throughput. In order to unify the experimental stages, a utility has been developed to run repeatable experiments. This uses the tools contained in Moses and requires minimal changes to set up and customize. The toolkit has been hosted and developed under sourceforge.net since inception. Mo"
P07-2045,N07-1062,1,0.152186,"up gigabytes of disk space, but for the translation of a single sentence only a tiny fraction of this table is needed. Moses implements an efficient representation of the phrase translation table. Its key properties are a prefix tree structure for source words and on demand loading, i.e. only the fraction of the phrase table that is needed to translate a sentence is loaded into the working memory of the decoder. For the Chinese-English NIST task, the memory requirement of the phrase table is reduced from 1.7 gigabytes to less than 20 mega bytes, with no loss in translation quality and speed (Zens and Ney 2007). The other large data resource for statistical machine translation is the language model. Almost unlimited text resources can be collected from the Internet and used as training data for language modeling. This results in language models that are too large to easily fit into memory. The Moses system implements a data structure for language models that is more efficient than the canonical SRILM (Stolcke 2002) implementation used in most systems. The language model on disk is also converted into this binary format, resulting in a minimal loading time during start-up of the decoder. An even more"
P07-2045,D08-1076,0,\N,Missing
P07-2045,2006.iwslt-evaluation.8,1,\N,Missing
P08-1087,W07-0702,1,0.329256,"s with morphotactical knowledge. Habash et al. (2007) also investigated case determination in Arabic. Carpuat and Wu (2007) approached the issue as a Word Sense Disambiguation problem. In their presentation of the factored SMT models, Koehn and Hoang (2007) describe experiments for translating from English to German, Spanish and Czech, using morphology tags added on the morphologically rich side, along with POS tags. The morphological factors are added on the morphologically rich side and scored with a 7-gram sequence model. Probabilistic models for using only source tags were investigated by Birch et al. (2007), who attached syntax hints in factored SMT models by having Combinatorial Categorial Grammar (CCG) supertags as factors on the input words, but in this case English was the target language. This paper reports work that strictly focuses on translation from English to a morphologically richer language. We go one step further than just using easily acquired information (e.g. English POS or lemmata) and extract target-specific information from the source sentence context. We use syntax, not in Figure 2: Classification of the errors on our EnglishGreek baseline system (ch. 4.1), as suggested by Vi"
P08-1087,H92-1022,0,0.0638621,"spectively, following the specifications made by the ACL 2007 2nd Workshop on SMT1 . A Czech model was trained on 57,464 aligned sentences, tuned over 1057 sentences of the News Commentary corpus and and tested on two sets of 964 sentences and 2000 sentences respectively. The training sentences were trimmed to a length of 60 words for reducing perplexity and a standard lexicalised reordering, with distortion limit set to 6. For getting the syntax trees, the latest version of Collins’ parser (Collins, 1997) was used. When needed, part-of-speech (POS) tags were acquired by using Brill’s tagger (Brill, 1992) on v1.14. Results were evaluated with both BLEU (Papineni et al., 2001) and NIST metrics (NIST, 2002). j where each phrase j is translated by one translation table t(j) and each table i has a feature function hTi . as shown in eq. (2). 4.2 Results set baseline person pos+person person+case altpath:POS Figure 5: Decoding using an alternative path with different factorization 4 Experiments This preprocessing led to annotated source data, which were given as an input to a factored SMT system. 4.1 Experiment setup For testing the factored translation systems, we used Moses (Koehn et al., 2007), a"
P08-1087,E06-1032,1,0.454206,"Missing"
P08-1087,D07-1007,0,0.0280456,"features, in order to ensure grammatical agreement on the output. The method, using various grammatical source-side features, achieved higher accuracy when applied directly to the reference translations but it was not tested as a part of an MT system. Similarly, translating English into Turkish (Durgar El-Kahlout and Oflazer, 2006) uses POS and morph stems in the input along with rich Turkish morph tags on the target side, but improvement was gained only after augmenting the generation process with morphotactical knowledge. Habash et al. (2007) also investigated case determination in Arabic. Carpuat and Wu (2007) approached the issue as a Word Sense Disambiguation problem. In their presentation of the factored SMT models, Koehn and Hoang (2007) describe experiments for translating from English to German, Spanish and Czech, using morphology tags added on the morphologically rich side, along with POS tags. The morphological factors are added on the morphologically rich side and scored with a 7-gram sequence model. Probabilistic models for using only source tags were investigated by Birch et al. (2007), who attached syntax hints in factored SMT models by having Combinatorial Categorial Grammar (CCG) supe"
P08-1087,W05-0620,0,0.0160883,"e “missing” morphology information, depending on the syntactic position of the words of interest. Then, contrary to the methods that added only output features or altered the generation procedure, we used this information in order to augment only the source side of a factored translation model, assuming that we do not have resources allowing factors or specialized generation in the target language (a common problem, when translating from English into under-resourced languages). 2 Therefore, the followed approach takes advantage of syntax, following a method similar to Semantic Role Labelling (Carreras and Marquez, 2005; Surdeanu and Turmo, 2005). English, as morphologically poor language, usually follows a fixed word order (subject-verb-object), so that a syntax parser can be easily used for identifying the subject and the object of most sentences. Considering such annotation, a factored translation model is trained to map the word-case pair to the correct inflection of the target noun. Given the agreement restriction, all words that accompany the noun (adjectives, articles, determiners) must follow the case of the noun, so their likely case needs to be identified as well. For this purpose we use a syntax p"
P08-1087,P97-1003,0,0.128471,"ative paths are combined as following (fig. 5): hT (f |e) = X hTt(j) (ej , f j ) (3) News Commentary respectively, following the specifications made by the ACL 2007 2nd Workshop on SMT1 . A Czech model was trained on 57,464 aligned sentences, tuned over 1057 sentences of the News Commentary corpus and and tested on two sets of 964 sentences and 2000 sentences respectively. The training sentences were trimmed to a length of 60 words for reducing perplexity and a standard lexicalised reordering, with distortion limit set to 6. For getting the syntax trees, the latest version of Collins’ parser (Collins, 1997) was used. When needed, part-of-speech (POS) tags were acquired by using Brill’s tagger (Brill, 1992) on v1.14. Results were evaluated with both BLEU (Papineni et al., 2001) and NIST metrics (NIST, 2002). j where each phrase j is translated by one translation table t(j) and each table i has a feature function hTi . as shown in eq. (2). 4.2 Results set baseline person pos+person person+case altpath:POS Figure 5: Decoding using an alternative path with different factorization 4 Experiments This preprocessing led to annotated source data, which were given as an input to a factored SMT system. 4.1"
P08-1087,P05-1066,1,0.636806,"Missing"
P08-1087,W06-3102,0,0.0326234,"Missing"
P08-1087,D07-1116,0,0.0189629,"ested a post-processing system which uses morphological and syntactic features, in order to ensure grammatical agreement on the output. The method, using various grammatical source-side features, achieved higher accuracy when applied directly to the reference translations but it was not tested as a part of an MT system. Similarly, translating English into Turkish (Durgar El-Kahlout and Oflazer, 2006) uses POS and morph stems in the input along with rich Turkish morph tags on the target side, but improvement was gained only after augmenting the generation process with morphotactical knowledge. Habash et al. (2007) also investigated case determination in Arabic. Carpuat and Wu (2007) approached the issue as a Word Sense Disambiguation problem. In their presentation of the factored SMT models, Koehn and Hoang (2007) describe experiments for translating from English to German, Spanish and Czech, using morphology tags added on the morphologically rich side, along with POS tags. The morphological factors are added on the morphologically rich side and scored with a 7-gram sequence model. Probabilistic models for using only source tags were investigated by Birch et al. (2007), who attached syntax hints in fac"
P08-1087,N06-2013,0,0.0164687,"itional statistical machine translation methods are based on mapping on the lexical level, which takes place in a local window of a few words. Hence, they fail to produce adequate output in many cases where more complex linguistic phenomena play a role. Take the example of morphology. Predicting the correct morphological variant for a target word may not depend solely on the source words, but require additional information about its role in the sentence. Recent research on handling rich morphology has largely focused on translating from rich morphology languages, such as Arabic, into English (Habash and Sadat, 2006). There has been less work on the opposite case, translating from English into morphologically richer languages. In a study of translation quality for languages in the Europarl corpus, Koehn (2005) reports that translating into morphologically richer languages is more difficult than translating from them. There are intuitive reasons why generating richer morphology from morphologically poor languages is harder. Take the example of translating noun phrases from English to Greek (or German, Czech, etc.). In English, a noun phrase is rendered the same if it is the subject or the object. However,"
P08-1087,2006.amta-papers.8,0,0.0112658,"mar (CCG) supertags as factors on the input words, but in this case English was the target language. This paper reports work that strictly focuses on translation from English to a morphologically richer language. We go one step further than just using easily acquired information (e.g. English POS or lemmata) and extract target-specific information from the source sentence context. We use syntax, not in Figure 2: Classification of the errors on our EnglishGreek baseline system (ch. 4.1), as suggested by Vilar et al. (2006) order to aid reordering (Yamada and Knight, 2001; Collins et al., 2005; Huang et al., 2006), but as a means for getting the “missing” morphology information, depending on the syntactic position of the words of interest. Then, contrary to the methods that added only output features or altered the generation procedure, we used this information in order to augment only the source side of a factored translation model, assuming that we do not have resources allowing factors or specialized generation in the target language (a common problem, when translating from English into under-resourced languages). 2 Therefore, the followed approach takes advantage of syntax, following a method simil"
P08-1087,2005.mtsummit-papers.11,1,0.0189893,"ore complex linguistic phenomena play a role. Take the example of morphology. Predicting the correct morphological variant for a target word may not depend solely on the source words, but require additional information about its role in the sentence. Recent research on handling rich morphology has largely focused on translating from rich morphology languages, such as Arabic, into English (Habash and Sadat, 2006). There has been less work on the opposite case, translating from English into morphologically richer languages. In a study of translation quality for languages in the Europarl corpus, Koehn (2005) reports that translating into morphologically richer languages is more difficult than translating from them. There are intuitive reasons why generating richer morphology from morphologically poor languages is harder. Take the example of translating noun phrases from English to Greek (or German, Czech, etc.). In English, a noun phrase is rendered the same if it is the subject or the object. However, Greek words in noun phrases are inflected based on their role in the sentence. A purely lexical mapping of English noun phrases to Greek noun phrases suffers from the lack of information about its"
P08-1087,D07-1091,1,0.769511,"ed higher accuracy when applied directly to the reference translations but it was not tested as a part of an MT system. Similarly, translating English into Turkish (Durgar El-Kahlout and Oflazer, 2006) uses POS and morph stems in the input along with rich Turkish morph tags on the target side, but improvement was gained only after augmenting the generation process with morphotactical knowledge. Habash et al. (2007) also investigated case determination in Arabic. Carpuat and Wu (2007) approached the issue as a Word Sense Disambiguation problem. In their presentation of the factored SMT models, Koehn and Hoang (2007) describe experiments for translating from English to German, Spanish and Czech, using morphology tags added on the morphologically rich side, along with POS tags. The morphological factors are added on the morphologically rich side and scored with a 7-gram sequence model. Probabilistic models for using only source tags were investigated by Birch et al. (2007), who attached syntax hints in factored SMT models by having Combinatorial Categorial Grammar (CCG) supertags as factors on the input words, but in this case English was the target language. This paper reports work that strictly focuses o"
P08-1087,P07-2045,1,0.0131712,"s tagger (Brill, 1992) on v1.14. Results were evaluated with both BLEU (Papineni et al., 2001) and NIST metrics (NIST, 2002). j where each phrase j is translated by one translation table t(j) and each table i has a feature function hTi . as shown in eq. (2). 4.2 Results set baseline person pos+person person+case altpath:POS Figure 5: Decoding using an alternative path with different factorization 4 Experiments This preprocessing led to annotated source data, which were given as an input to a factored SMT system. 4.1 Experiment setup For testing the factored translation systems, we used Moses (Koehn et al., 2007), along with a 5-gram SRILM language model (Stolcke, 2002). A Greek model was trained on 440,082 aligned sentences of Europarl v.3, tuned with Minimum Error Training (Och, 2003). It was tuned over a development set of 2,000 Europarl sentences and tested on two sets of 2,000 sentences each, from the Europarl and a 767 BLEU devtest test07 18.13 18.05 18.16 18.17 18.14 18.16 18.08 18.24 18.21 18.20 NIST devtest test07 5.218 5.279 5.224 5.316 5.259 5.316 5.258 5.340 5.285 5.340 Table 1: Translating English to Greek: Using a single translation table may cause sparse data problems, which are address"
P08-1087,P07-1017,0,0.170023,"chunks that are translated (see Figure 1 for an example). 764 In one of the first efforts to enrich the source in word-based SMT, Ueffing and Ney (2003) used partof-speech (POS) tags, in order to deal with the verb conjugation of Spanish and Catalan; so, POS tags were used to identify the pronoun+verb sequence and splice these two words into one term. The approach was clearly motivated by the problems occurring by a single-word-based SMT and have been solved by adopting a phrase-based model. Meanwhile, there is no handling of the case when the pronoun stays in distance with the related verb. Minkov et al. (2007) suggested a post-processing system which uses morphological and syntactic features, in order to ensure grammatical agreement on the output. The method, using various grammatical source-side features, achieved higher accuracy when applied directly to the reference translations but it was not tested as a part of an MT system. Similarly, translating English into Turkish (Durgar El-Kahlout and Oflazer, 2006) uses POS and morph stems in the input along with rich Turkish morph tags on the target side, but improvement was gained only after augmenting the generation process with morphotactical knowle"
P08-1087,P03-1021,0,0.00486831,"t(j) and each table i has a feature function hTi . as shown in eq. (2). 4.2 Results set baseline person pos+person person+case altpath:POS Figure 5: Decoding using an alternative path with different factorization 4 Experiments This preprocessing led to annotated source data, which were given as an input to a factored SMT system. 4.1 Experiment setup For testing the factored translation systems, we used Moses (Koehn et al., 2007), along with a 5-gram SRILM language model (Stolcke, 2002). A Greek model was trained on 440,082 aligned sentences of Europarl v.3, tuned with Minimum Error Training (Och, 2003). It was tuned over a development set of 2,000 Europarl sentences and tested on two sets of 2,000 sentences each, from the Europarl and a 767 BLEU devtest test07 18.13 18.05 18.16 18.17 18.14 18.16 18.08 18.24 18.21 18.20 NIST devtest test07 5.218 5.279 5.224 5.316 5.259 5.316 5.258 5.340 5.285 5.340 Table 1: Translating English to Greek: Using a single translation table may cause sparse data problems, which are addressed using an alternative path to a second translation table We tested several various combinations of tags, while using a single translation component. Some combinations seem to"
P08-1087,2001.mtsummit-papers.68,0,0.0397069,"2nd Workshop on SMT1 . A Czech model was trained on 57,464 aligned sentences, tuned over 1057 sentences of the News Commentary corpus and and tested on two sets of 964 sentences and 2000 sentences respectively. The training sentences were trimmed to a length of 60 words for reducing perplexity and a standard lexicalised reordering, with distortion limit set to 6. For getting the syntax trees, the latest version of Collins’ parser (Collins, 1997) was used. When needed, part-of-speech (POS) tags were acquired by using Brill’s tagger (Brill, 1992) on v1.14. Results were evaluated with both BLEU (Papineni et al., 2001) and NIST metrics (NIST, 2002). j where each phrase j is translated by one translation table t(j) and each table i has a feature function hTi . as shown in eq. (2). 4.2 Results set baseline person pos+person person+case altpath:POS Figure 5: Decoding using an alternative path with different factorization 4 Experiments This preprocessing led to annotated source data, which were given as an input to a factored SMT system. 4.1 Experiment setup For testing the factored translation systems, we used Moses (Koehn et al., 2007), along with a 5-gram SRILM language model (Stolcke, 2002). A Greek model w"
P08-1087,W05-0635,0,0.00954772,"mation, depending on the syntactic position of the words of interest. Then, contrary to the methods that added only output features or altered the generation procedure, we used this information in order to augment only the source side of a factored translation model, assuming that we do not have resources allowing factors or specialized generation in the target language (a common problem, when translating from English into under-resourced languages). 2 Therefore, the followed approach takes advantage of syntax, following a method similar to Semantic Role Labelling (Carreras and Marquez, 2005; Surdeanu and Turmo, 2005). English, as morphologically poor language, usually follows a fixed word order (subject-verb-object), so that a syntax parser can be easily used for identifying the subject and the object of most sentences. Considering such annotation, a factored translation model is trained to map the word-case pair to the correct inflection of the target noun. Given the agreement restriction, all words that accompany the noun (adjectives, articles, determiners) must follow the case of the noun, so their likely case needs to be identified as well. For this purpose we use a syntax parser to acquire the syntax"
P08-1087,vilar-etal-2006-error,0,0.0119775,"Missing"
P08-1087,P01-1067,0,0.0900552,"models by having Combinatorial Categorial Grammar (CCG) supertags as factors on the input words, but in this case English was the target language. This paper reports work that strictly focuses on translation from English to a morphologically richer language. We go one step further than just using easily acquired information (e.g. English POS or lemmata) and extract target-specific information from the source sentence context. We use syntax, not in Figure 2: Classification of the errors on our EnglishGreek baseline system (ch. 4.1), as suggested by Vilar et al. (2006) order to aid reordering (Yamada and Knight, 2001; Collins et al., 2005; Huang et al., 2006), but as a means for getting the “missing” morphology information, depending on the syntactic position of the words of interest. Then, contrary to the methods that added only output features or altered the generation procedure, we used this information in order to augment only the source side of a factored translation model, assuming that we do not have resources allowing factors or specialized generation in the target language (a common problem, when translating from English into under-resourced languages). 2 Therefore, the followed approach takes ad"
P08-1087,P02-1040,0,\N,Missing
P09-4005,N03-1017,1,0.01177,"Missing"
P09-4005,W00-0507,0,0.814054,"Missing"
P09-4005,E09-1008,0,0.208166,"d to color code the options. Up to ten table rows are filled with options. Since the user may click on the options, or may simply type in translations inspired by the options, it is not straight-forward to evaluate their usefulness. We plan to assess this by measuring translation speed and quality. Experience so far has shown that the options help novice users with unknown words and advanced users with suggestions that are not part of their active vocabulary. It may be possible that these options even allow users that do not know the source language to create a translation, as in work done by Albrecht et al. (2009). 5 6 Key Stroke Logging Caitra tracks every key stroke and mouse click of the user, which then allows for a detailed analysis of the user’s interaction with the tool. See Figure 4 for a graphical representation of the user activity during the translation of a sentence. The graph plots sentence length (in characters) against the progression of time. Bars indicate the sentence length at each point in time when a user action takes place (acceptance of predictions are red, DEL key strokes purple, key strokes for cursor movement grey, and key strokes that add characters are black.) In the example"
P09-4005,J09-1002,0,0.349368,"Missing"
P09-4005,P07-2045,1,\N,Missing
P13-1135,W10-1703,1,0.322705,"Missing"
P13-1135,W11-2103,1,0.788665,"BLEU scores for several language pairs before and after adding the mined parallel data to systems trained on data from WMT data. WMT 11 Baseline +Web Data WMT 12 Baseline +Web Data FR-EN 30.96 31.24 FR-EN 29.88 30.08 EN-FR 30.69 31.17 EN-FR 28.50 28.76 Corpus EN-FR EN-ES EN-DE News Commentary 2.99M 50.3M 316M 668M 121M 3.43M 49.2M 281M 68.8M 3.39M 47.9M 88.4M Europarl United Nations FR-EN Gigaword CommonCrawl Table 9: BLEU scores for French-English and English-French before and after adding the mined parallel data to systems trained on data from WMT data including the French-English Gigaword (Callison-Burch et al., 2011). For these experiments, we also include training data mined from Wikipedia using a simplified version of the sentence aligner described by Smith et al. (2010), in order to determine how the effect of such data compares with the effect of webmined data. The baseline system was trained using only the Europarl corpus (Koehn, 2005) as parallel data, and all experiments use the same language model trained on the target sides of Europarl, the English side of all linked SpanishEnglish Wikipedia articles, and the English side of the mined CommonCrawl data. We use a 5gram language model and tune using"
P13-1135,W12-3102,1,0.806207,"Missing"
P13-1135,J93-1004,0,0.134564,", 2003): 1. Candidate pair selection: Retrieve candidate document pairs from the CommonCrawl corpus. 2. Structural Filtering: (a) Convert the HTML of each document 2 3 commoncrawl.org http://aws.amazon.com/s3/pricing/ into a sequence of start tags, end tags, and text chunks. (b) Align the linearized HTML of candidate document pairs. (c) Decide whether to accept or reject each pair based on features of the alignment. 3. Segmentation: For each text chunk, perform sentence and word segmentation. 4. Sentence Alignment: For each aligned pair of text chunks, perform the sentence alignment method of Gale and Church (1993). 5. Sentence Filtering: Remove sentences that appear to be boilerplate. Candidate Pair Selection We adopt a strategy similar to that of Resnik and Smith (2003) for finding candidate parallel documents, adapted to the parallel architecture of Map-Reduce. The mapper operates on each website entry in the CommonCrawl data. It scans the URL string for some indicator of its language. Specifically, we check for: 1. Two/three letter language codes (ISO-639). 2. Language names in English and in the language of origin. If either is present in a URL and surrounded by non-alphanumeric characters, the URL"
P13-1135,P07-2045,1,0.011242,"Missing"
P13-1135,2005.mtsummit-papers.11,1,0.613567,"oehn3 pkoehn@inf.ed.ac.uk Chris Callison-Burch1,2,5 ccb@cs.jhu.edu ∗ Adam Lopez1,2 alopez@cs.jhu.edu Department of Computer Science, Johns Hopkins University Human Language Technology Center of Excellence, Johns Hopkins University 3 School of Informatics, University of Edinburgh 4 Institute of Computational Linguistics, University of Zurich 5 Computer and Information Science Department, University of Pennsylvania 1 2 Abstract are readily available, ordering in the hundreds of millions of words for Chinese-English and ArabicEnglish, and in tens of millions of words for many European languages (Koehn, 2005). In each case, much of this data consists of government and news text. However, for most language pairs and domains there is little to no curated parallel data available. Hence discovery of parallel data is an important first step for translation between most of the world’s languages. Parallel text is the fuel that drives modern machine translation systems. The Web is a comprehensive source of preexisting parallel text, but crawling the entire web is impossible for all but the largest companies. We bring web-scale parallel text to the masses by mining the Common Crawl, a public Web crawl host"
P13-1135,W02-0109,0,0.0188601,"Missing"
P13-1135,P12-3005,0,0.0302301,"Missing"
P13-1135,J05-4003,0,0.0147224,"pairs across a variety of domains and genres, some previously unavailable in curated datasets. Even with minimal cleaning and filtering, the resulting data boosts translation performance across the board for five different language pairs in the news domain, and on open domain test sets we see improvements of up to 5 BLEU. We make our code and data available for other researchers seeking to mine this rich new data resource.1 1 The Web is an important source of parallel text. Many websites are available in multiple languages, and unlike other potential sources— such as multilingual news feeds (Munteanu and Marcu, 2005) or Wikipedia (Smith et al., 2010)— it is common to find document pairs that are direct translations of one another. This natural parallelism simplifies the mining task, since few resources or existing corpora are needed at the outset to bootstrap the extraction process. Introduction A key bottleneck in porting statistical machine translation (SMT) technology to new languages and domains is the lack of readily available parallel corpora beyond curated datasets. For a handful of language pairs, large amounts of parallel data ∗ This research was conducted while Chris CallisonBurch was at Johns H"
P13-1135,P11-1122,1,0.252203,"Missing"
P13-1135,P03-1021,0,0.0312913,"5: A list of 20 topics generated using the MALLET toolkit (McCallum, 2002) and their most likely tokens. 4.1 News Domain Translation Our first set of experiments are based on systems built for the 2012 Workshop on Statistical Machine Translation (WMT) (Callison-Burch et al., 2012) using all available parallel and monolingual data for that task, aside from the French-English Gigaword. In these experiments, we use 5-gram language models when the target language is English or German, and 4-gram language models for French and Spanish. We tune model weights using minimum error rate training (MERT; Och, 2003) on the WMT 2008 test data. The results are given in Table 8. For all language pairs and both test sets (WMT 2011 and WMT 2012), we show an improvement of around 0.5 BLEU. We also included the French-English Gigaword in separate experiments given in Table 9, and Table 10 compares the sizes of the datasets used. These results show that even on top of a different, larger parallel corpus mined from the web, adding CommonCrawl data still yields an improvement. 4.2 Open Domain Translation A substantial appeal of web-mined parallel data is that it might be suitable to translation of domains other th"
P13-1135,J03-3002,0,0.249669,"trap the extraction process. Introduction A key bottleneck in porting statistical machine translation (SMT) technology to new languages and domains is the lack of readily available parallel corpora beyond curated datasets. For a handful of language pairs, large amounts of parallel data ∗ This research was conducted while Chris CallisonBurch was at Johns Hopkins University. 1 github.com/jrs026/CommonCrawlMiner Parallel text mining from the Web was originally explored by individuals or small groups of academic researchers using search engines (Nie et al., 1999; Chen and Nie, 2000; Resnik, 1999; Resnik and Smith, 2003). However, anything more sophisticated generally requires direct access to web-crawled documents themselves along with the computing power to process them. For most researchers, this is prohibitively expensive. As a consequence, web-mined parallel text has become the exclusive purview of large companies with the computational resources to crawl, store, and process the entire Web. To put web-mined parallel text back in the hands of individual researchers, we mine parallel text from the Common Crawl, a regularly updated 81-terabyte snapshot of the public internet hosted 1374 Proceedings of the 5"
P13-1135,P99-1068,0,0.0228645,"utset to bootstrap the extraction process. Introduction A key bottleneck in porting statistical machine translation (SMT) technology to new languages and domains is the lack of readily available parallel corpora beyond curated datasets. For a handful of language pairs, large amounts of parallel data ∗ This research was conducted while Chris CallisonBurch was at Johns Hopkins University. 1 github.com/jrs026/CommonCrawlMiner Parallel text mining from the Web was originally explored by individuals or small groups of academic researchers using search engines (Nie et al., 1999; Chen and Nie, 2000; Resnik, 1999; Resnik and Smith, 2003). However, anything more sophisticated generally requires direct access to web-crawled documents themselves along with the computing power to process them. For most researchers, this is prohibitively expensive. As a consequence, web-mined parallel text has become the exclusive purview of large companies with the computational resources to crawl, store, and process the entire Web. To put web-mined parallel text back in the hands of individual researchers, we mine parallel text from the Common Crawl, a regularly updated 81-terabyte snapshot of the public internet hosted"
P13-1135,N10-1063,1,0.944364,"enres, some previously unavailable in curated datasets. Even with minimal cleaning and filtering, the resulting data boosts translation performance across the board for five different language pairs in the news domain, and on open domain test sets we see improvements of up to 5 BLEU. We make our code and data available for other researchers seeking to mine this rich new data resource.1 1 The Web is an important source of parallel text. Many websites are available in multiple languages, and unlike other potential sources— such as multilingual news feeds (Munteanu and Marcu, 2005) or Wikipedia (Smith et al., 2010)— it is common to find document pairs that are direct translations of one another. This natural parallelism simplifies the mining task, since few resources or existing corpora are needed at the outset to bootstrap the extraction process. Introduction A key bottleneck in porting statistical machine translation (SMT) technology to new languages and domains is the lack of readily available parallel corpora beyond curated datasets. For a handful of language pairs, large amounts of parallel data ∗ This research was conducted while Chris CallisonBurch was at Johns Hopkins University. 1 github.com/jr"
P13-1135,N12-1079,0,0.020081,"Missing"
P13-1135,C10-1124,0,0.0223887,"Missing"
P13-1135,D11-1126,0,0.0714544,"Missing"
P13-1135,P06-4018,0,\N,Missing
P13-2063,P00-1056,0,0.149182,"(a) time vs. BLEU (b) hypo count vs. BLEU Figure 4: Translation quality comparison with the cube pruning baseline. 4 4.1 Experiments sign POS tags for both our training and test data. Setup 4.2 A Moses (Koehn et al., 2007) string-to-tree system is used as our baseline. The training corpus consists of the English-German sections of the Europarl (Koehn, 2005) and the News Commentary corpus. Discarding pairs without targetside parses, the final training data has 2M sentence pairs, with 54M and 52M words on the English and German sides, respectively. Wordalignments are obtained by running GIZA++ (Och and Ney, 2000) in both directions and refined with “grow-diag-final-and” (Koehn et al., 2003). For all experiments, a 5-gram language model with Kneser-Ney smoothing (Chen and Goodman, 1996) built with the SRILM Toolkit (Stolcke and others, 2002) is used. The development and test sets are the 2008 WMT newstest (2,051 sentences) and 2009 WMT newstest (2,525 sentences) respectively. Feature weights are tuned with MERT (Och, 2003) on the development set and output is evaluated using case-sensitive BLEU (Papineni et al., 2002). For both rule extraction and decoding, up to seven terminal/non-terminal symbols on"
P13-2063,P96-1041,0,0.108328,"d test data. Setup 4.2 A Moses (Koehn et al., 2007) string-to-tree system is used as our baseline. The training corpus consists of the English-German sections of the Europarl (Koehn, 2005) and the News Commentary corpus. Discarding pairs without targetside parses, the final training data has 2M sentence pairs, with 54M and 52M words on the English and German sides, respectively. Wordalignments are obtained by running GIZA++ (Och and Ney, 2000) in both directions and refined with “grow-diag-final-and” (Koehn et al., 2003). For all experiments, a 5-gram language model with Kneser-Ney smoothing (Chen and Goodman, 1996) built with the SRILM Toolkit (Stolcke and others, 2002) is used. The development and test sets are the 2008 WMT newstest (2,051 sentences) and 2009 WMT newstest (2,525 sentences) respectively. Feature weights are tuned with MERT (Och, 2003) on the development set and output is evaluated using case-sensitive BLEU (Papineni et al., 2002). For both rule extraction and decoding, up to seven terminal/non-terminal symbols on the source-side are allowed. For decoding, the maximum spanlength is restricted to 15, and the grammar is prefiltered to match the entire test set for both the baseline system"
P13-2063,P03-1021,0,0.0159682,"de parses, the final training data has 2M sentence pairs, with 54M and 52M words on the English and German sides, respectively. Wordalignments are obtained by running GIZA++ (Och and Ney, 2000) in both directions and refined with “grow-diag-final-and” (Koehn et al., 2003). For all experiments, a 5-gram language model with Kneser-Ney smoothing (Chen and Goodman, 1996) built with the SRILM Toolkit (Stolcke and others, 2002) is used. The development and test sets are the 2008 WMT newstest (2,051 sentences) and 2009 WMT newstest (2,525 sentences) respectively. Feature weights are tuned with MERT (Och, 2003) on the development set and output is evaluated using case-sensitive BLEU (Papineni et al., 2002). For both rule extraction and decoding, up to seven terminal/non-terminal symbols on the source-side are allowed. For decoding, the maximum spanlength is restricted to 15, and the grammar is prefiltered to match the entire test set for both the baseline system and the chart pruning decoder. We use two labellers to perform b- and e-tag labelling independently prior to decoding. Training of the labelling models is able to complete in under 2.5 hours and the whole test set is labelled in under 2 seco"
P13-2063,J07-2003,0,0.0696048,"ranslate the same English sentence into Japanese (Figure 2a); unlike the English to German example, the English phrase “the products” will be a valid phrase that has a Japanese translation under a target constituent, since it is syntactically aligned to “製品” (Figure 2b). The key question to consider is how to inject target syntax and word alignment information into our labelling models, so that pruning decisions can be based on the source alone, we address this in the following two sections. The Baseline String-to-Tree Model Our baseline translation model uses the rule extraction algorithm of Chiang (2007) adapted to a string-to-tree grammar. After extracting phrasal pairs using the standard approach of Koehn et al. (2003), all pairs whose target phrases are not exhaustively dominated by a constituent of the parse tree are removed and each remaining pair, hf , ei, together with its constituent label, C, forms a lexical grammar rule: C → hf , ei. The rules r1 , r2 , and r3 in Figure 1 are lexical rules. Non-lexical rules are generated by eliminating one or more pairs of terminal substrings from an existing rule and substituting non-terminals. This process produces the example rules r4 and r5 . O"
P13-2063,P02-1040,0,0.0868433,"he English and German sides, respectively. Wordalignments are obtained by running GIZA++ (Och and Ney, 2000) in both directions and refined with “grow-diag-final-and” (Koehn et al., 2003). For all experiments, a 5-gram language model with Kneser-Ney smoothing (Chen and Goodman, 1996) built with the SRILM Toolkit (Stolcke and others, 2002) is used. The development and test sets are the 2008 WMT newstest (2,051 sentences) and 2009 WMT newstest (2,525 sentences) respectively. Feature weights are tuned with MERT (Och, 2003) on the development set and output is evaluated using case-sensitive BLEU (Papineni et al., 2002). For both rule extraction and decoding, up to seven terminal/non-terminal symbols on the source-side are allowed. For decoding, the maximum spanlength is restricted to 15, and the grammar is prefiltered to match the entire test set for both the baseline system and the chart pruning decoder. We use two labellers to perform b- and e-tag labelling independently prior to decoding. Training of the labelling models is able to complete in under 2.5 hours and the whole test set is labelled in under 2 seconds. A standard perceptron POS tagger (Collins, 2002) trained on Wall Street Journal sections 2-2"
P13-2063,C04-1041,0,0.0318675,"ls will be pruned. The pruning effects of the two types of tags are illustrated in Figure 3. In general, 0-valued b-tags prune a whole column of chart cells and 0-valued e-tags prune a whole diagonal of cells; and the chart cells on the first row and the top-most cell are always kept so that complete translations can always be found. We build a separate labeller for each tag type using gold-standard b- and e-tags, respectively. We train the labellers with maximum-entropy models (Curran and Clark, 2003; Ratnaparkhi, 1996), using features similar to those used for suppertagging for CCG parsing (Clark and Curran, 2004). In each case, features for a pruning tag consist of word and POS uni-grams extracted from the 5word window with the current word in the middle, POS trigrams ending with the current word, as well as two previous tags as a bigram and two separate uni-grams. Our pruning labellers are highly efficient, run in linear time and add little overhead to decoding. During testing, in order to prevent overpruning, a probability cutoff value θ is used. A tag value of 0 is assigned to a word only if its marginal probability is greater than θ. 3.3 procedure C ONSISTENT(i, j, i0 , j 0 ) 12: t ← {A[k] |k ∈ [i"
P13-2063,D08-1012,0,0.605446,"Missing"
P13-2063,W02-1001,0,0.0304112,"valuated using case-sensitive BLEU (Papineni et al., 2002). For both rule extraction and decoding, up to seven terminal/non-terminal symbols on the source-side are allowed. For decoding, the maximum spanlength is restricted to 15, and the grammar is prefiltered to match the entire test set for both the baseline system and the chart pruning decoder. We use two labellers to perform b- and e-tag labelling independently prior to decoding. Training of the labelling models is able to complete in under 2.5 hours and the whole test set is labelled in under 2 seconds. A standard perceptron POS tagger (Collins, 2002) trained on Wall Street Journal sections 2-21 of the Penn Treebank is used to asResults Figures 4a and 4b compare CSP with the cube pruning baseline in terms of BLEU. Decoding speed is measured by the average decoding time and average number of hypotheses generated per sentence. We first run the baseline decoder under various beam settings (b = 100 - 2500) until no further increase in BLEU is observed. We then run the CSP decoder with a range of θ values (θ = 0.91 − 0.99), at the default beam size of 1000 of the baseline decoder. The CSP decoder, which considers far fewer chart cells and gener"
P13-2063,W96-0213,0,0.197145,"hrase. If either the b-tag or the e-tag of an input phrase is 0, the corresponding chart cells will be pruned. The pruning effects of the two types of tags are illustrated in Figure 3. In general, 0-valued b-tags prune a whole column of chart cells and 0-valued e-tags prune a whole diagonal of cells; and the chart cells on the first row and the top-most cell are always kept so that complete translations can always be found. We build a separate labeller for each tag type using gold-standard b- and e-tags, respectively. We train the labellers with maximum-entropy models (Curran and Clark, 2003; Ratnaparkhi, 1996), using features similar to those used for suppertagging for CCG parsing (Clark and Curran, 2004). In each case, features for a pruning tag consist of word and POS uni-grams extracted from the 5word window with the current word in the middle, POS trigrams ending with the current word, as well as two previous tags as a bigram and two separate uni-grams. Our pruning labellers are highly efficient, run in linear time and add little overhead to decoding. During testing, in order to prevent overpruning, a probability cutoff value θ is used. A tag value of 0 is assigned to a word only if its margina"
P13-2063,E03-1071,0,0.0161754,"rd cannot end a source phrase. If either the b-tag or the e-tag of an input phrase is 0, the corresponding chart cells will be pruned. The pruning effects of the two types of tags are illustrated in Figure 3. In general, 0-valued b-tags prune a whole column of chart cells and 0-valued e-tags prune a whole diagonal of cells; and the chart cells on the first row and the top-most cell are always kept so that complete translations can always be found. We build a separate labeller for each tag type using gold-standard b- and e-tags, respectively. We train the labellers with maximum-entropy models (Curran and Clark, 2003; Ratnaparkhi, 1996), using features similar to those used for suppertagging for CCG parsing (Clark and Curran, 2004). In each case, features for a pruning tag consist of word and POS uni-grams extracted from the 5word window with the current word in the middle, POS trigrams ending with the current word, as well as two previous tags as a bigram and two separate uni-grams. Our pruning labellers are highly efficient, run in linear time and add little overhead to decoding. During testing, in order to prevent overpruning, a probability cutoff value θ is used. A tag value of 0 is assigned to a word"
P13-2063,C08-1094,0,0.560688,"ses (i.e. any sequence of consecutive words) that are unlikely to have any consistently aligned target counterparts according to the source context and grammar constraints. We show that by using highly-efficient sequence labelling models learned from the bitext used for translation model training, such phrases can be effectively identified prior to MT decoding, and corresponding chart cells can be excluded for decoding without affecting translation quality. We call our method context-sensitive pruning (CSP); it can be viewed as a bilingual adaptation of similar methods in monolingual parsing (Roark and Hollingshead, 2008; Zhang et al., 2010) which improve parsing efficiency by “closing” chart cells using binary classifiers. Our contribution is that we demonstrate such methods can be applied to synchronous-grammar parsing by labelling the source-side alone. This is achieved through a novel training scheme where the labelling models are trained over the word-aligned bitext and gold-standard pruning labels are obtained by projecting target-side constituents to the source words. To our knowledge, this is the first work to apply this technique to MT decoding. The proposed method is easy to implement and effective"
P13-2063,N09-1026,0,0.086088,"nline n-gram language model integration and high grammar complexity. Various efforts have been devoted to improving decoding efficiency, including hypergraph rescoring (Heafield et al., 2013; Huang and Chiang, 2007), coarse-to-fine processing (Petrov et al., 2008; Zhang and Gildea, 2008) and grammar transformations (Zhang et al., 2006). For more expressive, linguistically-motivated syntactic MT models (Galley et al., 2004; Galley et al., 2006), the grammar complexity has grown considerably over hierarchical phrase-based models (Chiang, 2007), and decoding still suffers from efficiency issues (DeNero et al., 2009). In this paper, we study a chart pruning method for CKY-style MT decoding that is orthogonal to 352 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 352–357, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics TOP KON NP-OA denn PDAT PUNC. VVFIN NN NP-SB NN der NN NN Produkte 製品 (a) en-de but we need that reform process . KON NP - SB NP - OA → → → r4 r5 TOP S - TOP → → h but, denn i h we, wir i h that reform process, diesen Reformprozeß i h X1 . , S - TOP1 . i h but X1 need X2 , denn NP - OA2 brauchen NP - SB1 i 3.1"
P13-2063,N09-1073,0,0.0389563,"Missing"
P13-2063,N04-1035,0,0.137642,"Missing"
P13-2063,P06-1121,0,0.118792,"Missing"
P13-2063,P08-1025,0,0.0495614,"Missing"
P13-2063,N06-1033,0,0.0606546,"Missing"
P13-2063,N13-1116,1,0.859545,"Missing"
P13-2063,C10-2168,1,0.889886,"Missing"
P13-2063,D10-1063,0,0.0154531,"are not exhaustively dominated by a constituent of the parse tree are removed and each remaining pair, hf , ei, together with its constituent label, C, forms a lexical grammar rule: C → hf , ei. The rules r1 , r2 , and r3 in Figure 1 are lexical rules. Non-lexical rules are generated by eliminating one or more pairs of terminal substrings from an existing rule and substituting non-terminals. This process produces the example rules r4 and r5 . Our decoding algorithm is a variant of CKY and is similar to other algorithms tailored for specific syntactic translation grammars (DeNero et al., 2009; Hopkins and Langmead, 2010). By taking the source-side of each rule, projecting onto it the non-terminal labels from the target-side, and weighting the grammar according to the model’s local scoring features, decoding is a straightforward extension of monolingual weighted chart parsing. Non-local features, such as n-gram language model scores, are incorporated through cube pruning (Chiang, 2007). 3 の Figure 2: Two example alignments. In (a) “the products” does not have a consistent alignment on the target side, while it does in (b). Figure 1: A selection of grammar rules extractable from an example word-aligned sentence"
P13-2063,P07-1019,0,0.0456905,"Missing"
P13-2063,N03-1017,1,0.0756894,"hrase “the products” will be a valid phrase that has a Japanese translation under a target constituent, since it is syntactically aligned to “製品” (Figure 2b). The key question to consider is how to inject target syntax and word alignment information into our labelling models, so that pruning decisions can be based on the source alone, we address this in the following two sections. The Baseline String-to-Tree Model Our baseline translation model uses the rule extraction algorithm of Chiang (2007) adapted to a string-to-tree grammar. After extracting phrasal pairs using the standard approach of Koehn et al. (2003), all pairs whose target phrases are not exhaustively dominated by a constituent of the parse tree are removed and each remaining pair, hf , ei, together with its constituent label, C, forms a lexical grammar rule: C → hf , ei. The rules r1 , r2 , and r3 in Figure 1 are lexical rules. Non-lexical rules are generated by eliminating one or more pairs of terminal substrings from an existing rule and substituting non-terminals. This process produces the example rules r4 and r5 . Our decoding algorithm is a variant of CKY and is similar to other algorithms tailored for specific syntactic translatio"
P13-2063,P07-2045,1,0.0119064,"traction constraints, which is crucial for the success of our method. For each training sentence pair, gold-standard b-tags and e-tags are assigned separately to the 354 0.149 0.1485 0.1485 0.148 0.148 BLEU BLEU 0.149 0.1475 0.1475 0.147 0.147 0.1465 0.1465 csp cube pruning 0.146 0 2 4 6 csp cube pruning 0.146 8 10 12 14 16 18 20 0 0.5 1 1.5 2 2.5 6 CPU seconds/sentence Hypothesis Count (x10 ) (a) time vs. BLEU (b) hypo count vs. BLEU Figure 4: Translation quality comparison with the cube pruning baseline. 4 4.1 Experiments sign POS tags for both our training and test data. Setup 4.2 A Moses (Koehn et al., 2007) string-to-tree system is used as our baseline. The training corpus consists of the English-German sections of the Europarl (Koehn, 2005) and the News Commentary corpus. Discarding pairs without targetside parses, the final training data has 2M sentence pairs, with 54M and 52M words on the English and German sides, respectively. Wordalignments are obtained by running GIZA++ (Och and Ney, 2000) in both directions and refined with “grow-diag-final-and” (Koehn et al., 2003). For all experiments, a 5-gram language model with Kneser-Ney smoothing (Chen and Goodman, 1996) built with the SRILM Toolki"
P13-2063,2005.mtsummit-papers.11,1,0.0850506,"ned separately to the 354 0.149 0.1485 0.1485 0.148 0.148 BLEU BLEU 0.149 0.1475 0.1475 0.147 0.147 0.1465 0.1465 csp cube pruning 0.146 0 2 4 6 csp cube pruning 0.146 8 10 12 14 16 18 20 0 0.5 1 1.5 2 2.5 6 CPU seconds/sentence Hypothesis Count (x10 ) (a) time vs. BLEU (b) hypo count vs. BLEU Figure 4: Translation quality comparison with the cube pruning baseline. 4 4.1 Experiments sign POS tags for both our training and test data. Setup 4.2 A Moses (Koehn et al., 2007) string-to-tree system is used as our baseline. The training corpus consists of the English-German sections of the Europarl (Koehn, 2005) and the News Commentary corpus. Discarding pairs without targetside parses, the final training data has 2M sentence pairs, with 54M and 52M words on the English and German sides, respectively. Wordalignments are obtained by running GIZA++ (Och and Ney, 2000) in both directions and refined with “grow-diag-final-and” (Koehn et al., 2003). For all experiments, a 5-gram language model with Kneser-Ney smoothing (Chen and Goodman, 1996) built with the SRILM Toolkit (Stolcke and others, 2002) is used. The development and test sets are the 2008 WMT newstest (2,051 sentences) and 2009 WMT newstest (2,"
P13-2063,D08-1022,0,0.0468076,"Missing"
P13-2071,N12-1047,0,0.023851,"sed 402 No. 1. 2. 3. 4. 5. System Baseline 1+pp 1+pp+tsm 1+pp+osm 1+osm* fr-en 31.89 31.87 31.94 32.17 32.13 es-en 35.07 35.09 35.25 35.50 35.65 cs-en 23.88 23.64 23.85 24.14 24.23 ru-en 33.45 33.04 32.97 33.21 33.91 en-fr 29.89 29.70 29.98 30.35 30.54 en-es 35.03 35.00 35.06 35.34 35.49 en-cs 16.22 16.17 16.30 16.49 16.62 en-ru 23.88 24.05 23.96 24.22 24.25 Table 2: Translating into and from English. Bold: Statistically Significant (Koehn, 2004) w.r.t Baseline 5 the first half for tuning and second for test. We test our systems on news-test 2012. We tune with the k-best batch MIRA algorithm (Cherry and Foster, 2012). Conclusion and Future Work We have addressed the problem of the independence assumption in PBSMT by integrating Ngram-based models inside a phrase-based system using a log-linear framework. We try to replicate the effect of rewrite and split rules as used in the TSM model through phrasal alignments. We presented a novel extension of the OSM model to handle unaligned and discontinuous target MTUs in the OSM model. Phrase-based search helps us to address these problems that are non-trivial to handle in the decoding frameworks of the N-grambased models. We tested our extentions and modification"
P13-2071,2012.iwslt-papers.17,1,0.551072,"continous phrases. The discontinuous MTUs that span beyond a phrasal length of 6 words are therefore never hypothesized. We would like to explore this further by extending the search to use discontinuous phrases (Galley and Manning, 2010). Moses Baseline: We trained a Moses system (Koehn et al., 2007) with the following settings: maximum sentence length 80, grow-diag-finaland symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011) used at runtime, msd-bidirectional-fe lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), distortion limit of 6, 100-best translation options, minimum bayes-risk decoding (Kumar and Byrne, 2004), cube-pruning (Huang and Chiang, 2007) and the no-reordering-overpunctuation heuristic. Results: Table 2 shows uncased BLEU scores (Papineni et al., 2002) on the test set. Row 2 (+pp) shows that the post-editing of alignments to remove unaligned and discontinuous target MTUs decreases the performance in the case of ru-en, csen and en-fr. Row 3 (+pp+tsm) shows that our integration of the TSM model slightly improves the BLEU scores for en-fr, and es-en. Results drop in ru-en and en-ru. Row"
P13-2071,N07-2035,0,0.16661,"Missing"
P13-2071,W11-2123,0,0.0901439,"s to the OSM model enables discontinuous MTUs, we did not fully utilize these during decoding, as Moses only uses continous phrases. The discontinuous MTUs that span beyond a phrasal length of 6 words are therefore never hypothesized. We would like to explore this further by extending the search to use discontinuous phrases (Galley and Manning, 2010). Moses Baseline: We trained a Moses system (Koehn et al., 2007) with the following settings: maximum sentence length 80, grow-diag-finaland symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011) used at runtime, msd-bidirectional-fe lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), distortion limit of 6, 100-best translation options, minimum bayes-risk decoding (Kumar and Byrne, 2004), cube-pruning (Huang and Chiang, 2007) and the no-reordering-overpunctuation heuristic. Results: Table 2 shows uncased BLEU scores (Papineni et al., 2002) on the test set. Row 2 (+pp) shows that the post-editing of alignments to remove unaligned and discontinuous target MTUs decreases the performance in the case of ru-en, csen and en-fr. Row 3 (+pp+tsm) shows that our int"
P13-2071,P07-1019,0,0.372958,"ore this further by extending the search to use discontinuous phrases (Galley and Manning, 2010). Moses Baseline: We trained a Moses system (Koehn et al., 2007) with the following settings: maximum sentence length 80, grow-diag-finaland symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011) used at runtime, msd-bidirectional-fe lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), distortion limit of 6, 100-best translation options, minimum bayes-risk decoding (Kumar and Byrne, 2004), cube-pruning (Huang and Chiang, 2007) and the no-reordering-overpunctuation heuristic. Results: Table 2 shows uncased BLEU scores (Papineni et al., 2002) on the test set. Row 2 (+pp) shows that the post-editing of alignments to remove unaligned and discontinuous target MTUs decreases the performance in the case of ru-en, csen and en-fr. Row 3 (+pp+tsm) shows that our integration of the TSM model slightly improves the BLEU scores for en-fr, and es-en. Results drop in ru-en and en-ru. Row 4 (+pp+osm) shows that the OSM model consistently improves the BLEU scores over the Baseline systems (Row 1) giving significant improvements in h"
P13-2071,2009.eamt-1.10,0,0.0260482,"ave ignored so far are the handling of MTUs which have discontinuous targets, and the handling of unaligned target words. Both TSM and OSM N-gram models generate MTUs linearly in left-to-right order. This assumption becomes problematic in the cases of MTUs that have target-side discontinuities (See Figure 2(a)). The MTU A → g . . . a can not be generated because of the intervening MTUs B → b, C . . . H → c and D → d. In the original TSM model, such cases are dealt with by merging all the intervening MTUs to form a bigger unit t01 in Figure 2(c). A solution that uses split-rules is proposed by Crego and Yvon (2009) but has not been adopted in Ncode (Crego et al., 2011), the state-of-the-art TSM Ngram system. Durrani et al. (2011) dealt with this problem by applying a post-processing (PP) heuristic that modifies the alignments to remove such cases. When a source word is aligned to a discontinuous target-cept, first the link to the least frequent target word is identified, and the group of links containing this word is retained while the others are deleted. The alignment in Figure 2(a), for example, is transformed to that in Figure 2(b). This allows OSM to extract the intervening MTUs t2 . . . t5 (Figure"
P13-2071,E09-1049,0,0.0142488,"-gram-based system. However, they do not use phrase-based models in their work, relying only on the OSM model. This paper combines insights from these recent pieces of work and show that phrase-based search combined with N-gram-based and phrase-based models in decoding is the overall best way to go. We integrate the two N-grambased models, TSM and OSM, into phrase-based Moses and show that the translation quality is improved by taking both translation and reordering context into account. Other approaches that explored such models in syntax-based systems used MTUs for sentence level reranking (Khalilov and Fonollosa, 2009), in dependency translation models (Quirk and Menezes, 2006) and in target language syntax systems (Vaswani et al., 2011). 3 Figure 1: Example (a) Word Alignments (b) Unfolded MTU Sequence (c) Operation Sequence (d) Step-wise Generation tem. Given a bilingual sentence pair (F, E) and its alignment (A), we first identify minimal translation units (MTUs) from it. An MTU is defined as a translation rule that cannot be broken down any further. The MTUs extracted from Figure 1(a) are A → a, B → b, C . . . H → c1 and D → d. These units are then generated left-to-right in two different ways, as we wi"
P13-2071,C10-2023,0,0.141203,"se399 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 399–405, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics baseline system, and shows statistically significant improvements in seven out of eight cases. 2 Previous Work Several researchers have tried to combine the ideas of phrase-based and N-gram-based SMT. Costajuss`a et al. (2007) proposed a method for combining the two approaches by applying sentence level reranking. Feng et al. (2010) added a linearized source-side language model in a phrase-based system. Crego and Yvon (2010) modified the phrasebased lexical reordering model of Tillman (2004) for an N-gram-based system. Niehues et al. (2011) integrated a bilingual language model based on surface word forms and POS tags into a phrasebased system. Zhang et al. (2013) explored multiple decomposition structures for generating MTUs in the task of lexical selection, and to rerank the N-best candidate translations in the output of a phrase-based. A drawback of the TSM model is the assumption that source and target information is generated monotonically. The process of reordering is disconnected from lexical generation wh"
P13-2071,W12-3139,1,0.843905,"y the decoder to arbitrarily generate unaligned MTUs but hypothesize these only when they appear within Parallel ≈39 M ≈15.6 M ≈15.2 M ≈2 M Monolingual ≈91 M ≈43.4 M ≈65.7 M ≈21.7 M ≈287.3 M Lang fr cs es ru en Table 1: Number of Sentences (in Millions) used for Training We follow the approach of Schwenk and Koehn (2008) and trained domain-specific language models separately and then linearly interpolated them using SRILM with weights optimized on the heldout dev-set. We concatenated the news-test sets from four years (2008-2011) to obtain a large devsetin order to obtain more stable weights (Koehn and Haddow, 2012). For Russian-English and English-Russian language pairs, we divided the tuning-set news-test 2012 into two halves and used 402 No. 1. 2. 3. 4. 5. System Baseline 1+pp 1+pp+tsm 1+pp+osm 1+osm* fr-en 31.89 31.87 31.94 32.17 32.13 es-en 35.07 35.09 35.25 35.50 35.65 cs-en 23.88 23.64 23.85 24.14 24.23 ru-en 33.45 33.04 32.97 33.21 33.91 en-fr 29.89 29.70 29.98 30.35 30.54 en-es 35.03 35.00 35.06 35.34 35.49 en-cs 16.22 16.17 16.30 16.49 16.62 en-ru 23.88 24.05 23.96 24.22 24.25 Table 2: Translating into and from English. Bold: Statistically Significant (Koehn, 2004) w.r.t Baseline 5 the first ha"
P13-2071,P11-1105,1,0.732808,"for an N-gram-based system. Niehues et al. (2011) integrated a bilingual language model based on surface word forms and POS tags into a phrasebased system. Zhang et al. (2013) explored multiple decomposition structures for generating MTUs in the task of lexical selection, and to rerank the N-best candidate translations in the output of a phrase-based. A drawback of the TSM model is the assumption that source and target information is generated monotonically. The process of reordering is disconnected from lexical generation which restricts the search to a small set of precomputed reorderings. Durrani et al. (2011) addressed this problem by coupling lexical generation and reordering information into a single generative process and enriching the N-gram models to learn lexical reordering triggers. Durrani et al. (2013) showed that using larger phrasal units during decoding is superior to MTU-based decoding in an N-gram-based system. However, they do not use phrase-based models in their work, relying only on the OSM model. This paper combines insights from these recent pieces of work and show that phrase-based search combined with N-gram-based and phrase-based models in decoding is the overall best way to"
P13-2071,N03-1017,1,0.0357267,"model that captures dependencies across phrasal boundaries. Some work has been done to combine insights from these two frameworks. A recent successful attempt showed the advantage of using phrasebased search on top of an N-gram-based model. We probe this question in the reverse direction by investigating whether integrating N-gram-based translation and reordering models into a phrase-based decoder helps overcome the problematic phrasal independence assumption. A large scale evaluation over 8 language pairs shows that performance does significantly improve. 1 Introduction Phrase-based models (Koehn et al., 2003; Och and Ney, 2004) learn local dependencies such as reorderings, idiomatic collocations, deletions and insertions by memorization. A fundamental drawback is that phrases are translated and reordered independently of each other and contextual information outside of phrasal boundaries is ignored. The monolingual language model somewhat reduces this problem. However i) often the language model cannot overcome the dispreference of the translation model for nonlocal dependencies, ii) source-side contextual dependencies are still ignored and iii) generation of lexical translations and reordering i"
P13-2071,N13-1001,1,0.531868,"on structures for generating MTUs in the task of lexical selection, and to rerank the N-best candidate translations in the output of a phrase-based. A drawback of the TSM model is the assumption that source and target information is generated monotonically. The process of reordering is disconnected from lexical generation which restricts the search to a small set of precomputed reorderings. Durrani et al. (2011) addressed this problem by coupling lexical generation and reordering information into a single generative process and enriching the N-gram models to learn lexical reordering triggers. Durrani et al. (2013) showed that using larger phrasal units during decoding is superior to MTU-based decoding in an N-gram-based system. However, they do not use phrase-based models in their work, relying only on the OSM model. This paper combines insights from these recent pieces of work and show that phrase-based search combined with N-gram-based and phrase-based models in decoding is the overall best way to go. We integrate the two N-grambased models, TSM and OSM, into phrase-based Moses and show that the translation quality is improved by taking both translation and reordering context into account. Other appr"
P13-2071,P07-2045,1,0.0139628,"deration consistently improves the performance of the baseline system. Our modification to the OSM model produces the best results giving significant improvements in most cases. Although our modifications to the OSM model enables discontinuous MTUs, we did not fully utilize these during decoding, as Moses only uses continous phrases. The discontinuous MTUs that span beyond a phrasal length of 6 words are therefore never hypothesized. We would like to explore this further by extending the search to use discontinuous phrases (Galley and Manning, 2010). Moses Baseline: We trained a Moses system (Koehn et al., 2007) with the following settings: maximum sentence length 80, grow-diag-finaland symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011) used at runtime, msd-bidirectional-fe lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), distortion limit of 6, 100-best translation options, minimum bayes-risk decoding (Kumar and Byrne, 2004), cube-pruning (Huang and Chiang, 2007) and the no-reordering-overpunctuation heuristic. Results: Table 2 shows uncased BLEU scores (Papineni et al., 2002) on the test set. Row"
P13-2071,2010.amta-papers.22,0,0.288871,"Missing"
P13-2071,W04-3250,1,0.53813,"Missing"
P13-2071,N04-1022,0,0.483452,"er hypothesized. We would like to explore this further by extending the search to use discontinuous phrases (Galley and Manning, 2010). Moses Baseline: We trained a Moses system (Koehn et al., 2007) with the following settings: maximum sentence length 80, grow-diag-finaland symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011) used at runtime, msd-bidirectional-fe lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), distortion limit of 6, 100-best translation options, minimum bayes-risk decoding (Kumar and Byrne, 2004), cube-pruning (Huang and Chiang, 2007) and the no-reordering-overpunctuation heuristic. Results: Table 2 shows uncased BLEU scores (Papineni et al., 2002) on the test set. Row 2 (+pp) shows that the post-editing of alignments to remove unaligned and discontinuous target MTUs decreases the performance in the case of ru-en, csen and en-fr. Row 3 (+pp+tsm) shows that our integration of the TSM model slightly improves the BLEU scores for en-fr, and es-en. Results drop in ru-en and en-ru. Row 4 (+pp+osm) shows that the OSM model consistently improves the BLEU scores over the Baseline systems (Row"
P13-2071,W11-2124,0,0.0307268,"Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics baseline system, and shows statistically significant improvements in seven out of eight cases. 2 Previous Work Several researchers have tried to combine the ideas of phrase-based and N-gram-based SMT. Costajuss`a et al. (2007) proposed a method for combining the two approaches by applying sentence level reranking. Feng et al. (2010) added a linearized source-side language model in a phrase-based system. Crego and Yvon (2010) modified the phrasebased lexical reordering model of Tillman (2004) for an N-gram-based system. Niehues et al. (2011) integrated a bilingual language model based on surface word forms and POS tags into a phrasebased system. Zhang et al. (2013) explored multiple decomposition structures for generating MTUs in the task of lexical selection, and to rerank the N-best candidate translations in the output of a phrase-based. A drawback of the TSM model is the assumption that source and target information is generated monotonically. The process of reordering is disconnected from lexical generation which restricts the search to a small set of precomputed reorderings. Durrani et al. (2011) addressed this problem by co"
P13-2071,J04-4002,0,0.11205,"dependencies across phrasal boundaries. Some work has been done to combine insights from these two frameworks. A recent successful attempt showed the advantage of using phrasebased search on top of an N-gram-based model. We probe this question in the reverse direction by investigating whether integrating N-gram-based translation and reordering models into a phrase-based decoder helps overcome the problematic phrasal independence assumption. A large scale evaluation over 8 language pairs shows that performance does significantly improve. 1 Introduction Phrase-based models (Koehn et al., 2003; Och and Ney, 2004) learn local dependencies such as reorderings, idiomatic collocations, deletions and insertions by memorization. A fundamental drawback is that phrases are translated and reordered independently of each other and contextual information outside of phrasal boundaries is ignored. The monolingual language model somewhat reduces this problem. However i) often the language model cannot overcome the dispreference of the translation model for nonlocal dependencies, ii) source-side contextual dependencies are still ignored and iii) generation of lexical translations and reordering is separated. The N-g"
P13-2071,P02-1040,0,0.106262,"trained a Moses system (Koehn et al., 2007) with the following settings: maximum sentence length 80, grow-diag-finaland symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011) used at runtime, msd-bidirectional-fe lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), distortion limit of 6, 100-best translation options, minimum bayes-risk decoding (Kumar and Byrne, 2004), cube-pruning (Huang and Chiang, 2007) and the no-reordering-overpunctuation heuristic. Results: Table 2 shows uncased BLEU scores (Papineni et al., 2002) on the test set. Row 2 (+pp) shows that the post-editing of alignments to remove unaligned and discontinuous target MTUs decreases the performance in the case of ru-en, csen and en-fr. Row 3 (+pp+tsm) shows that our integration of the TSM model slightly improves the BLEU scores for en-fr, and es-en. Results drop in ru-en and en-ru. Row 4 (+pp+osm) shows that the OSM model consistently improves the BLEU scores over the Baseline systems (Row 1) giving significant improvements in half the cases. The only result that is lower than the baseline system is that of the ru-en experiment, because OSM i"
P13-2071,N06-1002,0,0.0506902,"in their work, relying only on the OSM model. This paper combines insights from these recent pieces of work and show that phrase-based search combined with N-gram-based and phrase-based models in decoding is the overall best way to go. We integrate the two N-grambased models, TSM and OSM, into phrase-based Moses and show that the translation quality is improved by taking both translation and reordering context into account. Other approaches that explored such models in syntax-based systems used MTUs for sentence level reranking (Khalilov and Fonollosa, 2009), in dependency translation models (Quirk and Menezes, 2006) and in target language syntax systems (Vaswani et al., 2011). 3 Figure 1: Example (a) Word Alignments (b) Unfolded MTU Sequence (c) Operation Sequence (d) Step-wise Generation tem. Given a bilingual sentence pair (F, E) and its alignment (A), we first identify minimal translation units (MTUs) from it. An MTU is defined as a translation rule that cannot be broken down any further. The MTUs extracted from Figure 1(a) are A → a, B → b, C . . . H → c1 and D → d. These units are then generated left-to-right in two different ways, as we will describe next. 3.1 Tuple Sequence Model (TSM) The TSM tra"
P13-2071,I08-2089,1,0.813358,"ration is generated as soon as the MTU containing its previous target word is generated. In Figure 2(a), ε − f is generated immediately after MTU E − e is generated. In a sequence of unaligned source and target MTUs, unaligned source MTUs are generated before the unaligned target MTUs. We do not modify the decoder to arbitrarily generate unaligned MTUs but hypothesize these only when they appear within Parallel ≈39 M ≈15.6 M ≈15.2 M ≈2 M Monolingual ≈91 M ≈43.4 M ≈65.7 M ≈21.7 M ≈287.3 M Lang fr cs es ru en Table 1: Number of Sentences (in Millions) used for Training We follow the approach of Schwenk and Koehn (2008) and trained domain-specific language models separately and then linearly interpolated them using SRILM with weights optimized on the heldout dev-set. We concatenated the news-test sets from four years (2008-2011) to obtain a large devsetin order to obtain more stable weights (Koehn and Haddow, 2012). For Russian-English and English-Russian language pairs, we divided the tuning-set news-test 2012 into two halves and used 402 No. 1. 2. 3. 4. 5. System Baseline 1+pp 1+pp+tsm 1+pp+osm 1+osm* fr-en 31.89 31.87 31.94 32.17 32.13 es-en 35.07 35.09 35.25 35.50 35.65 cs-en 23.88 23.64 23.85 24.14 24.2"
P13-2071,N04-4026,0,0.0506055,"tional Linguistics, pages 399–405, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics baseline system, and shows statistically significant improvements in seven out of eight cases. 2 Previous Work Several researchers have tried to combine the ideas of phrase-based and N-gram-based SMT. Costajuss`a et al. (2007) proposed a method for combining the two approaches by applying sentence level reranking. Feng et al. (2010) added a linearized source-side language model in a phrase-based system. Crego and Yvon (2010) modified the phrasebased lexical reordering model of Tillman (2004) for an N-gram-based system. Niehues et al. (2011) integrated a bilingual language model based on surface word forms and POS tags into a phrasebased system. Zhang et al. (2013) explored multiple decomposition structures for generating MTUs in the task of lexical selection, and to rerank the N-best candidate translations in the output of a phrase-based. A drawback of the TSM model is the assumption that source and target information is generated monotonically. The process of reordering is disconnected from lexical generation which restricts the search to a small set of precomputed reorderings."
P13-2071,P11-1086,0,0.0372821,"s insights from these recent pieces of work and show that phrase-based search combined with N-gram-based and phrase-based models in decoding is the overall best way to go. We integrate the two N-grambased models, TSM and OSM, into phrase-based Moses and show that the translation quality is improved by taking both translation and reordering context into account. Other approaches that explored such models in syntax-based systems used MTUs for sentence level reranking (Khalilov and Fonollosa, 2009), in dependency translation models (Quirk and Menezes, 2006) and in target language syntax systems (Vaswani et al., 2011). 3 Figure 1: Example (a) Word Alignments (b) Unfolded MTU Sequence (c) Operation Sequence (d) Step-wise Generation tem. Given a bilingual sentence pair (F, E) and its alignment (A), we first identify minimal translation units (MTUs) from it. An MTU is defined as a translation rule that cannot be broken down any further. The MTUs extracted from Figure 1(a) are A → a, B → b, C . . . H → c1 and D → d. These units are then generated left-to-right in two different ways, as we will describe next. 3.1 Tuple Sequence Model (TSM) The TSM translation model assumes that MTUs are generated monotonically."
P13-2071,N13-1002,0,0.215931,"improvements in seven out of eight cases. 2 Previous Work Several researchers have tried to combine the ideas of phrase-based and N-gram-based SMT. Costajuss`a et al. (2007) proposed a method for combining the two approaches by applying sentence level reranking. Feng et al. (2010) added a linearized source-side language model in a phrase-based system. Crego and Yvon (2010) modified the phrasebased lexical reordering model of Tillman (2004) for an N-gram-based system. Niehues et al. (2011) integrated a bilingual language model based on surface word forms and POS tags into a phrasebased system. Zhang et al. (2013) explored multiple decomposition structures for generating MTUs in the task of lexical selection, and to rerank the N-best candidate translations in the output of a phrase-based. A drawback of the TSM model is the assumption that source and target information is generated monotonically. The process of reordering is disconnected from lexical generation which restricts the search to a small set of precomputed reorderings. Durrani et al. (2011) addressed this problem by coupling lexical generation and reordering information into a single generative process and enriching the N-gram models to learn"
P13-2071,N10-1140,0,\N,Missing
P13-2071,J06-4004,0,\N,Missing
P13-2121,W13-2212,1,0.735043,"e built an unpruned model (Table 1) on 126 billion tokens. Estimation used a machine with 140 GB RAM and six hard drives in a RAID5 configuration (sustained read: 405 MB/s). It took 123 GB RAM, 2.8 days wall time, and 5.4 CPU days. A summary of Google’s results from 2007 on different data and hardware appears in §2. We then used this language model as an additional feature in unconstrained Czech-English, French-English, and Spanish-English submissions to the 2013 Workshop on Machine Translation.8 Our baseline is the University of Edinburgh’s phrase-based Moses (Koehn et al., 2007) submission (Durrani et al., 2013), which used all constrained data specified by the evaluation (7 billion tokens of English). It placed first by BLEU (Papineni et al., 2002) among constrained submissions in each language pair we consider. In order to translate, the large model was quantized (Whittaker and Raj, 2001) to 10 bits and compressed to 643 GB with KenLM (Heafield, 8 Large 28.2 33.4 32.6 Table 2: Uncased BLEU results from the 2013 Workshop on Machine Translation. We estimated unpruned language models in binary format on sentences randomly sampled from ClueWeb09. SRILM and IRSTLM were run until the test machine ran out"
P13-2121,D10-1026,0,0.0246209,"ing (Koehn et al., 2007), and truecasing, 126 billion tokens remained. 5 Backoffs only exist if the n-gram is the context of some n + 1-gram, so merging skips n-grams that are not contexts. 6 With quantization (Whittaker and Raj, 2001), the quantizer is trained in a first pass and applied in a second pass. 7 693 http://lemurproject.org/clueweb09/ 1 393 2 3,775 3 17,629 4 39,919 Source Czech French Spanish 5 59,794 Table 1: Counts of unique n-grams (in millions) for the 5 orders in the large LM. 5.1 Estimation Comparison 2011) then copied to a machine with 1 TB RAM. Better compression methods (Guthrie and Hepple, 2010; Talbot and Osborne, 2007) and distributed language models (Brants et al., 2007) could reduce hardware requirements. Feature weights were retuned with PRO (Hopkins and May, 2011) for Czech-English and batch MIRA (Cherry and Foster, 2012) for French-English and Spanish-English because these worked best for the baseline. Uncased BLEU scores on the 2013 test set are shown in Table 2. The improvement is remarkably consistent at 0.8 BLEU point in each language pair. 6 Conclusion Our open-source (LGPL) estimation code is available from kheafield.com/code/kenlm/ and should prove useful to the commun"
P13-2121,D07-1090,0,0.84713,"Each MapReduce performs three copies over the network when only one is required. Arrows denote copies over the network (i.e. to and from a distributed filesystem). Both options use local disk within each reducer for merge sort. contributes an efficient multi-pass streaming algorithm using disk and a user-specified amount of RAM. Introduction 2 Relatively low perplexity has made modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998) a popular choice for language modeling. However, existing estimation methods require either large amounts of RAM (Stolcke, 2002) or machines (Brants et al., 2007). As a result, practitioners have chosen to use less data (Callison-Burch et al., 2012) or simpler smoothing methods (Brants et al., 2007). Backoff-smoothed n-gram language models (Katz, 1987) assign probability to a word wn in context w1n−1 according to the recursive equation ( p(wn |w1n−1 ), if w1n was seen n−1 p(wn |w1 ) = b(w1n−1 )p(wn |w2n ), otherwise Related Work Brants et al. (2007) showed how to estimate Kneser-Ney models with a series of five MapReduces (Dean and Ghemawat, 2004). On 31 billion words, estimation took 400 machines for two days. Recently, Google estimated a pruned Knese"
P13-2121,D12-1107,1,0.71805,"on and put forward several scenarios in which a single machine scale-up approach is more cost effective in terms of both raw performance and performance per dollar. Brants et al. (2007) contributed Stupid Backoff, a simpler form of smoothing calculated at runtime from counts. With Stupid Backoff, they scaled to 1.8 trillion tokens. We agree that Stupid Backoff is cheaper to estimate, but contend that this work makes Kneser-Ney smoothing cheap enough. Another advantage of Stupid Backoff has been that it stores one value, a count, per n-gram instead of probability and backoff. In previous work (Heafield et al., 2012), we showed how to collapse probability and backoff into a single value without changing sentence-level probabilities. However, local scores do change and, like Stupid Backoff, are no longer probabilities. MSRLM (Nguyen et al., 2007) aims to scalably estimate language models on a single machine. Counting is performed with streaming algorithms similarly to this work. Their parallel merge sort also has the potential to be faster than ours. The biggest difference is that their pipeline delays some computation (part of normalization and all of interpolation) until query time. This means that it ca"
P13-2121,W12-3102,1,0.6588,"Missing"
P13-2121,W11-2123,1,0.520052,"Missing"
P13-2121,D11-1125,0,0.0714439,"ontexts. 6 With quantization (Whittaker and Raj, 2001), the quantizer is trained in a first pass and applied in a second pass. 7 693 http://lemurproject.org/clueweb09/ 1 393 2 3,775 3 17,629 4 39,919 Source Czech French Spanish 5 59,794 Table 1: Counts of unique n-grams (in millions) for the 5 orders in the large LM. 5.1 Estimation Comparison 2011) then copied to a machine with 1 TB RAM. Better compression methods (Guthrie and Hepple, 2010; Talbot and Osborne, 2007) and distributed language models (Brants et al., 2007) could reduce hardware requirements. Feature weights were retuned with PRO (Hopkins and May, 2011) for Czech-English and batch MIRA (Cherry and Foster, 2012) for French-English and Spanish-English because these worked best for the baseline. Uncased BLEU scores on the 2013 test set are shown in Table 2. The improvement is remarkably consistent at 0.8 BLEU point in each language pair. 6 Conclusion Our open-source (LGPL) estimation code is available from kheafield.com/code/kenlm/ and should prove useful to the community. Sorting makes it scalable; efficient merge sort makes it fast. In future work, we plan to extend to the Common Crawl corpus and improve parallelism. Scaling Acknowledgements"
P13-2121,N12-1047,0,0.255787,"he quantizer is trained in a first pass and applied in a second pass. 7 693 http://lemurproject.org/clueweb09/ 1 393 2 3,775 3 17,629 4 39,919 Source Czech French Spanish 5 59,794 Table 1: Counts of unique n-grams (in millions) for the 5 orders in the large LM. 5.1 Estimation Comparison 2011) then copied to a machine with 1 TB RAM. Better compression methods (Guthrie and Hepple, 2010; Talbot and Osborne, 2007) and distributed language models (Brants et al., 2007) could reduce hardware requirements. Feature weights were retuned with PRO (Hopkins and May, 2011) for Czech-English and batch MIRA (Cherry and Foster, 2012) for French-English and Spanish-English because these worked best for the baseline. Uncased BLEU scores on the 2013 test set are shown in Table 2. The improvement is remarkably consistent at 0.8 BLEU point in each language pair. 6 Conclusion Our open-source (LGPL) estimation code is available from kheafield.com/code/kenlm/ and should prove useful to the community. Sorting makes it scalable; efficient merge sort makes it fast. In future work, we plan to extend to the Common Crawl corpus and improve parallelism. Scaling Acknowledgements We built an unpruned model (Table 1) on 126 billion tokens."
P13-2121,P07-2045,1,0.0338753,"00 800 Tokens (millions) 1000 Figure 5: CPU usage (system plus user). Each n-gram record is an array of n vocabulary identifiers (4 bytes each) and an 8-byte count or probability and backoff. At peak, records are stored twice on disk because lazy merge sort is not easily amenable to overwriting the input file. Additional costs are the secondary backoff file (4 bytes per backoff) and the vocabulary in plaintext. 5 Experiments Experiments use ClueWeb09.7 After spam filtering (Cormack et al., 2011), removing markup, selecting English, splitting sentences (Koehn, 2005), deduplicating, tokenizing (Koehn et al., 2007), and truecasing, 126 billion tokens remained. 5 Backoffs only exist if the n-gram is the context of some n + 1-gram, so merging skips n-grams that are not contexts. 6 With quantization (Whittaker and Raj, 2001), the quantizer is trained in a first pass and applied in a second pass. 7 693 http://lemurproject.org/clueweb09/ 1 393 2 3,775 3 17,629 4 39,919 Source Czech French Spanish 5 59,794 Table 1: Counts of unique n-grams (in millions) for the 5 orders in the large LM. 5.1 Estimation Comparison 2011) then copied to a machine with 1 TB RAM. Better compression methods (Guthrie and Hepple, 2010"
P13-2121,2005.mtsummit-papers.11,1,0.0337569,"simultaneously. 10 8 6 4 2 0 0 200 400 600 800 Tokens (millions) 1000 Figure 5: CPU usage (system plus user). Each n-gram record is an array of n vocabulary identifiers (4 bytes each) and an 8-byte count or probability and backoff. At peak, records are stored twice on disk because lazy merge sort is not easily amenable to overwriting the input file. Additional costs are the secondary backoff file (4 bytes per backoff) and the vocabulary in plaintext. 5 Experiments Experiments use ClueWeb09.7 After spam filtering (Cormack et al., 2011), removing markup, selecting English, splitting sentences (Koehn, 2005), deduplicating, tokenizing (Koehn et al., 2007), and truecasing, 126 billion tokens remained. 5 Backoffs only exist if the n-gram is the context of some n + 1-gram, so merging skips n-grams that are not contexts. 6 With quantization (Whittaker and Raj, 2001), the quantizer is trained in a first pass and applied in a second pass. 7 693 http://lemurproject.org/clueweb09/ 1 393 2 3,775 3 17,629 4 39,919 Source Czech French Spanish 5 59,794 Table 1: Counts of unique n-grams (in millions) for the 5 orders in the large LM. 5.1 Estimation Comparison 2011) then copied to a machine with 1 TB RAM. Bett"
P13-2121,P02-1040,0,0.102307,"ration (sustained read: 405 MB/s). It took 123 GB RAM, 2.8 days wall time, and 5.4 CPU days. A summary of Google’s results from 2007 on different data and hardware appears in §2. We then used this language model as an additional feature in unconstrained Czech-English, French-English, and Spanish-English submissions to the 2013 Workshop on Machine Translation.8 Our baseline is the University of Edinburgh’s phrase-based Moses (Koehn et al., 2007) submission (Durrani et al., 2013), which used all constrained data specified by the evaluation (7 billion tokens of English). It placed first by BLEU (Papineni et al., 2002) among constrained submissions in each language pair we consider. In order to translate, the large model was quantized (Whittaker and Raj, 2001) to 10 bits and compressed to 643 GB with KenLM (Heafield, 8 Large 28.2 33.4 32.6 Table 2: Uncased BLEU results from the 2013 Workshop on Machine Translation. We estimated unpruned language models in binary format on sentences randomly sampled from ClueWeb09. SRILM and IRSTLM were run until the test machine ran out of RAM (64 GB). For our code, the memory limit was set to 3.5 GB because larger limits did not improve performance on this small data. Resu"
P13-2121,P07-1065,0,0.048315,"and truecasing, 126 billion tokens remained. 5 Backoffs only exist if the n-gram is the context of some n + 1-gram, so merging skips n-grams that are not contexts. 6 With quantization (Whittaker and Raj, 2001), the quantizer is trained in a first pass and applied in a second pass. 7 693 http://lemurproject.org/clueweb09/ 1 393 2 3,775 3 17,629 4 39,919 Source Czech French Spanish 5 59,794 Table 1: Counts of unique n-grams (in millions) for the 5 orders in the large LM. 5.1 Estimation Comparison 2011) then copied to a machine with 1 TB RAM. Better compression methods (Guthrie and Hepple, 2010; Talbot and Osborne, 2007) and distributed language models (Brants et al., 2007) could reduce hardware requirements. Feature weights were retuned with PRO (Hopkins and May, 2011) for Czech-English and batch MIRA (Cherry and Foster, 2012) for French-English and Spanish-English because these worked best for the baseline. Uncased BLEU scores on the 2013 test set are shown in Table 2. The improvement is remarkably consistent at 0.8 BLEU point in each language pair. 6 Conclusion Our open-source (LGPL) estimation code is available from kheafield.com/code/kenlm/ and should prove useful to the community. Sorting makes it scala"
P14-2094,W00-0507,0,0.793143,"Missing"
P14-2094,A00-1019,0,0.160293,"Missing"
P14-2094,P11-2068,0,0.303673,"word operates on search graphs (Och et al., 2003). Such search graphs can be efficiently represented and processed with finite state tools (Civera et al., 2004). More recently, the approach has been extended to SCFG-based translation models (Gonz´alez-Rubio et al., 2013). There are several ways the sentence completion predictions can be presented to the user: showing the complete sentence prediction, only a few words, or multiple choices. User actions may be also extended to mouse actions to pinpoint the divergence from an acceptable translation (SanchisTrilles et al., 2008), or hand-writing (Alabau et al., 2011) and speech modalities (Cubel et al., 2009). As machine translation enters the workflow of professional translators, the exact nature of this human-computer interaction is currently an open challenge. Instead of tasking translators to postedit the output of machine translation systems, a more interactive approach may be more fruitful. One such idea is interactive translation prediction (Langlais et al., 2000b): While the user writes the translation for a sentence, the system makes suggestions for sequent words. If the user diverges from the suggestions, the system recalculates its prediction,"
P14-2094,E03-1032,0,0.848146,"Missing"
P14-2094,J09-1002,0,0.35304,"Missing"
P14-2094,2014.eamt-1.5,0,0.247054,"Missing"
P14-2094,2005.eamt-1.6,0,0.816901,"Missing"
P14-2094,D08-1051,0,0.571536,"Missing"
P14-2094,W04-3245,0,0.0362852,"Missing"
P14-2094,W02-1020,0,0.292198,"Missing"
P14-2094,D13-1025,0,0.195445,"Missing"
P14-2094,W12-3139,1,0.801407,"Missing"
P16-1175,W15-3001,1,0.861797,"Missing"
P16-1175,P07-1033,0,0.290306,"Missing"
P16-1175,D09-1011,1,0.780042,"ymbols with any e to be the mean PMI with e of all dictionary words, so that they are essentially uninformative. Inference According to our model, the probability that the user guesses Ei = eˆi is given by a marginal probability from the CRF. Computing these marginals is a combinatorial optimization problem that involves reasoning jointly about the possible values of each Ei (i ∈ / Obs), which range over the English vocabulary V e . We employ loopy belief propagation (Murphy et al., 1999) to obtain approximate marginals over the variables E. A tree-based schedule for message passing was used (Dreyer and Eisner, 2009, footnote 22). We run 3 iterations with a new random root for each iteration. We define the vocabulary V e to consist of all reference translations e∗i and normalized user (8) where the summation is over all submissions in our dataset. The gradient of each summand reduces to a difference between observed and expected values of the feature vector φ = (φef , φee ), summed over all factors in (1). The observed feaˆ. The tures are computed directly by setting E = e expected features (which arise from the log of the normalization constant of (1)) are computed approximately by loopy belief propagat"
P16-1175,K16-1013,1,0.883817,"real numbers: ( PMI(ei , ej ) if |i − j |&gt; 1 φee pmi (ei , ej ) = (6) 0 otherwise ( PMI1 (ei , ej ) if |i − j |= 1 φee pmi1 (ei , ej ) = (7) 0 otherwise 5 At least in short-term memory—this feature currently omits to consider any negative feedback from previous HITs. 1863 where the pointwise mutual information PMI(x, y) measures the degree to which the English words x, y tend to occur in the same English sentence, and PMI1 (x, y) measures how often they tend to occur in adjacent positions. These measurements are estimated from the English side of the WMT corpus, with smoothing performed as in Knowles et al. (2016). For example, if fi = Suppe, the user’s guess of Ei should be influenced by fj = Brot appearing in the same sentence, if the user suspects or observes that its translation is Ej = bread. The PMI feature knows that soup and bread tend to appear in the same English sentences, whereas PMI1 knows that they tend not to appear in the bigram soup bread or bread soup. 4.1.3 4.3 Parameter Estimation We learn our parameter vector θ to approximately maximize the regularized log-likelihood of the users’ guesses: X  ∗ ˆ log Pθ (E = e |EObs = eObs , f , history) −λ||θ||2 User-Specific Features Apart from"
P16-1175,P07-2045,1,0.00803577,"to collect data. Users qualified for tasks by completing a short quiz and survey about their language knowledge. Only users whose results indicated no knowledge of German and self-identified as native speakers of English were allowed to complete tasks. With German as the foreign language, we generated content by crawling a simplifiedGerman news website, nachrichtenleicht. de. We chose simplified German in order to minimize translation errors and to make the task more suitable for novice learners. We translated each German sentence using the Moses Statistical Machine Translation (SMT) toolkit (Koehn et al., 2007). The SMT system was trained on the German-English Commoncrawl parallel text used in WMT 2015 (Bojar et al., 2015). We used 200 German sentences, presenting each to 10 different users. In MTurk jargon, this yielded 2000 Human Intelligence Tasks (HITs). Each HIT required its user to participate in several rounds of guessing as the English translation was incrementally revealed. A user was paid US$0.12 per HIT, with a bonus of US$6 to any user who accumulated more than 2000 total points. Our HIT user interface is shown in the video at https://youtu.be/9PczEcnr4F8. 3.1 HITs and Submissions For ea"
P16-1175,N12-1003,0,0.0248197,"terface that uses macaronic sentences directly as a medium of language instruction: our companion paper (Renduchintala et al., 2016) gives an overview of that project. We briefly review previous work, then describe our data collection setup and the data obtained. Finally, we discuss our model of learner comprehension and validate our model’s predictions. 2 Previous Work Natural language processing (NLP) has long been applied to education, but the majority of this work focuses on evaluation and assessment. Prominent recent examples include Heilman and Madnani (2012), Burstein et al. (2013) and Madnani et al. (2012). Other works fall more along the lines of intelligent and adaptive tutoring systems designed to improve learning outcomes. Most of those are outside of the area of NLP (typically focusing on math or science). An overview of NLPbased work in the education sphere can be found in Litman (2016). There has also been work spe¨ cific to second language acquisition, such as Ozbal et al. (2014), where the focus has been to build a system to help learners retain new vocabulary. However, much of the existing work on incidental learning is found in the education and cognitive science literature rather th"
P16-1175,P14-2058,0,0.028296,"rocessing (NLP) has long been applied to education, but the majority of this work focuses on evaluation and assessment. Prominent recent examples include Heilman and Madnani (2012), Burstein et al. (2013) and Madnani et al. (2012). Other works fall more along the lines of intelligent and adaptive tutoring systems designed to improve learning outcomes. Most of those are outside of the area of NLP (typically focusing on math or science). An overview of NLPbased work in the education sphere can be found in Litman (2016). There has also been work spe¨ cific to second language acquisition, such as Ozbal et al. (2014), where the focus has been to build a system to help learners retain new vocabulary. However, much of the existing work on incidental learning is found in the education and cognitive science literature rather than NLP. Our work is related to Labutov and Lipson (2014), which also tries to leverage incidental learning using mixed L1 and L2 language. Where their work uses surprisal to choose contexts in which to insert L2 vocabulary, we consider both context features and other factors such as cognate features (described in detail in 4.1). We collect data that gives direct evidence of the user’s u"
P16-1175,D14-1162,0,0.0958725,"points as discussed below. Yellow and red shading then fades, to signal to the user that they may try entering a new guess. Correct guesses remain on the screen for the entire task. 3.4 Points Adding points to the process (Figures 1–2) adds a game-like quality and lets us incentivize users by paying them for good performance (see section 3). We award 10 points for each exactly correct guess (case-insensitive). We give additional “effort points” for a guess that is close to the cor1861 rect translation, as measured by cosine similarity in vector space. (We used pre-trained GLoVe word vectors (Pennington et al., 2014); when the guess or correct translation has multiple words, we take the average of the word vectors.) We deduct effort points for guesses that are careless or very poor. Our rubric for effort points is as follows:   −1, if eˆ is repeated or nonsense (red)     −1, if sim(ˆ e, e∗ ) &lt; 0 (red)  ep = 0, if 0 ≤ sim(ˆ e, e∗ ) &lt; 0.4 (red)    0, if eˆ is blank    10 × sim(ˆ e, e∗ ) otherwise (yellow) Here sim(ˆ e, e∗ ) is cosine similarity between the vector embeddings of the user’s guess eˆ and our reference translation e∗ . A “nonsense” guess contains a word that does not appear in the s"
P16-1175,P14-1053,0,0.13683,"he lines of intelligent and adaptive tutoring systems designed to improve learning outcomes. Most of those are outside of the area of NLP (typically focusing on math or science). An overview of NLPbased work in the education sphere can be found in Litman (2016). There has also been work spe¨ cific to second language acquisition, such as Ozbal et al. (2014), where the focus has been to build a system to help learners retain new vocabulary. However, much of the existing work on incidental learning is found in the education and cognitive science literature rather than NLP. Our work is related to Labutov and Lipson (2014), which also tries to leverage incidental learning using mixed L1 and L2 language. Where their work uses surprisal to choose contexts in which to insert L2 vocabulary, we consider both context features and other factors such as cognate features (described in detail in 4.1). We collect data that gives direct evidence of the user’s understanding of words (by asking them to provide English guesses) rather than indirectly (via questions about sentence validity, which runs the risk of overestimating their knowledge of a word, if, for instance, they’ve only learned whether it is animate or inanimate"
P16-1175,P16-4023,1,\N,Missing
P16-4023,J94-4004,0,0.0375436,"btained from Figure 5 only if unit u2 (as labeled in Figure 4) is rendered (in its current language) to the left of unit u3 , which we write as u2 < u3 . In this case, it is possible for the user to change the order of these units, because u3 < u2 in German. Table 2 shows the 8 possible combinations of ordering and translation choices for this pair of units. String Rendered . . .they run. . . . . .they laufen. . . . . .sie run. . . . . .sie laufen. . . . . .run they. . . . . .run sie. . . . . .laufen they. . . . . .laufen sie. . . Cross-linguistic divergences in the expression of information (Dorr, 1994) could be confusing. For example, when moving through macaronic space from Kaffee gef¨ allt Menschen (coffee pleases humans) to its translation humans like coffee, it may not be clear to the learner that the reordering is triggered by the fact that like is not a literal translation of gef¨ allt. One way to improve this might be to have the system pass smoothly through a range of intermediate translations from word-by-word glosses to idiomatic phrasal translations, rather than always directly translating idioms. We might also see benefit in guiding our gradual translations with cognates (for ex"
P16-4023,K16-1013,1,0.444886,"0/ with sample content. Our interface lets the user navigate through the spectrum from L2 to L1, going beyond the single-word or single-phrase translations offered by other online tools such as Swych (2015), or dictionary-like browser plugins. Finally, we discuss plans to extend this prototype and to integrate it with a continuously adapting user model. To this end, our companion paper (Renduchintala et al., 2016) develops an initial model of macaronic sentence comprehension by novice L2 learners, using data collected from human subjects via Amazon’s Mechanical Turk service. In another paper (Knowles et al., 2016), we carry out a controlled study of comprehension of individual L2 words in isolation and in L1 context. 2 (a) Initial sentence state. (b) Mouse hovered under Preis. (c) Preis translated to prize. (d) Mouse hovered above prize. Clicking above will revert the sentence back to the initial state 1a. (e) Sentence with 2 different words translated into English Figure 1: Actions that translate words. Using these fundamental interactions as building blocks, we create an interactive framework for a language learner to explore this continuum of “English-like” to “foreign-like” sentences. By repeated i"
P16-4023,P07-2045,1,0.00930471,"Constructing Macaronic Translations In this section, we describe the details of the underlying data structures needed to allow all the interactions mentioned in the previous section. A key requirement in the design of the data structure was to support orthogonal actions in each sentence. Making all translation and reordering actions independent of one another creates a large space of macaronic states for a learner to explore. At present, the input to our macaronic interface is bitext with word-to-word alignments provided by a phrase-based SMT system (or, if desired, by hand). We employ Moses (Koehn et al., 2007) to translate German sentences and generate phrase alignments. News articles written in simple German from nachrichtenleicht. de (Deutschlandfunk, 2016) were translated after training the SMT system on the WMT15 GermanEnglish corpus (Bojar et al., 2015). We convert the word alignments into “minimal alignments” that are either one-to-one, oneto-many or many-to-one.4 This step ensures consistent reversibility of actions and prevents large phrases from being translated with a single click.5 The resulting bipartite graph can be regarded as Figure 5: A possible state of the sentence, which renders"
P16-4023,W08-1006,0,0.0186697,"rphemes might yield ge-talk or sprech-ed before reaching talked. This could guide learners towards an understanding of German tense marking and stem changes. The space of possible orderings for a sentence pair is defined by a bracketing ITG tree (Wu, 1997), which transforms the German ordering of the units into the English ordering by a collection of nested binary swaps of subsequences.7 The ordering state of the macaronic sentence is given by the subset of these swaps that have been performed. A reordering action toggles one of the swaps in this collection. Since we have a parser for German (Rafferty and Manning, 2008), we take care to select an ITG tree that is “compatible” with the German sentence’s dependency structure, in the following sense: if the ITG tree combines two spans A and B, then there are not dependencies from words in A to words in B and vice-versa. 4 4.1 4.2 User Adaptation and Evaluation We would prefer to show the learner a macaronic sentence that provides just enough clues for the learner to be able to comprehend it, while still pushing them to figure out new vocabulary or new structures. Thus, we plan to situate this interface in a framework that continuously adapts as the user progres"
P16-4023,P16-1175,1,0.712109,"ically generated using existing statistical machine translation (SMT) methods, enabling learners or teachers to choose their own texts to read. Our prototype is currently running on http: //www.clsp.jhu.edu:3030/ with sample content. Our interface lets the user navigate through the spectrum from L2 to L1, going beyond the single-word or single-phrase translations offered by other online tools such as Swych (2015), or dictionary-like browser plugins. Finally, we discuss plans to extend this prototype and to integrate it with a continuously adapting user model. To this end, our companion paper (Renduchintala et al., 2016) develops an initial model of macaronic sentence comprehension by novice L2 learners, using data collected from human subjects via Amazon’s Mechanical Turk service. In another paper (Knowles et al., 2016), we carry out a controlled study of comprehension of individual L2 words in isolation and in L1 context. 2 (a) Initial sentence state. (b) Mouse hovered under Preis. (c) Preis translated to prize. (d) Mouse hovered above prize. Clicking above will revert the sentence back to the initial state 1a. (e) Sentence with 2 different words translated into English Figure 1: Actions that translate word"
P16-4023,J97-3002,0,0.0390427,"cognate Karotte as an intermediate step). Unit Ordering {u2 } < {u3 } {u2 } > {u3 } Table 2: Generating reordered strings using units. We also plan to transition through words that are macaronic at the sub-word level. For example, hovering over the unfamiliar German word gesprochen might decompose it into ge-sprochen; then clicking on one of those morphemes might yield ge-talk or sprech-ed before reaching talked. This could guide learners towards an understanding of German tense marking and stem changes. The space of possible orderings for a sentence pair is defined by a bracketing ITG tree (Wu, 1997), which transforms the German ordering of the units into the English ordering by a collection of nested binary swaps of subsequences.7 The ordering state of the macaronic sentence is given by the subset of these swaps that have been performed. A reordering action toggles one of the swaps in this collection. Since we have a parser for German (Rafferty and Manning, 2008), we take care to select an ITG tree that is “compatible” with the German sentence’s dependency structure, in the following sense: if the ITG tree combines two spans A and B, then there are not dependencies from words in A to wor"
P19-2052,C16-1234,1,0.849279,"en attempted (Vyas et al., 2014) . Shallow parsing of code-mixed data curated from social media has also been tried (Sharma et al., 2016). Work has also been done to support word level identification of languages in code-mixed text (Chittaranjan et al., 2014). Sharma et al. (2015) tried an approach based on lexicon lookup for text normalization and sentiment analysis of code-mixed data. Pravalika et al. (2017) used a lexicon lookup approach to perform domain specific sentiment analysis. Other attempts include using sub-word level compositions with LSTMs to capture sentiment at morpheme level (Joshi et al., 2016), or using contrastive learning with Siamese networks to map code-mixed and standard language text to a common sentiment space (Choudhary et al., 2018). 3 3.1 Generating Subword Representations Word embeddings are now commonplace but are generally trained for one language. They are not ideal for code-mixed data given the transcription of one script into another, and spelling variations in social media data. As a single character does not inherently provide any semantic information that can be used for our purpose, we dismiss characterlevel feature representations as a possible choice of embedd"
P19-2052,E17-2068,0,0.0624558,"Missing"
P19-2052,S13-2053,0,0.121952,"Missing"
P19-2052,N16-1159,1,0.847281,"hybrid framework to exploit lexicons (polarized and emotional words) as well as different word embedding approaches in a polarity classification model. Ensembling simple word embedding based models with surfaceform classifiers has also yielded slight improvements (Araque et al., 2017). Extending standard NLP tasks to code-mixed data has presented peculiar challenges. Methods for POS tagging of code-mixed data obtained from online social media such as Facebook and Twitter has been attempted (Vyas et al., 2014) . Shallow parsing of code-mixed data curated from social media has also been tried (Sharma et al., 2016). Work has also been done to support word level identification of languages in code-mixed text (Chittaranjan et al., 2014). Sharma et al. (2015) tried an approach based on lexicon lookup for text normalization and sentiment analysis of code-mixed data. Pravalika et al. (2017) used a lexicon lookup approach to perform domain specific sentiment analysis. Other attempts include using sub-word level compositions with LSTMs to capture sentiment at morpheme level (Joshi et al., 2016), or using contrastive learning with Siamese networks to map code-mixed and standard language text to a common sentime"
P19-2052,Q17-1010,0,0.0494581,"the neural network framework of our model. Specific Encoder ui = tanh(Wi ki + bi ) (6) • Extended words: Number of words with multiple contiguous repeating characters. • Aggregate positive and negative sentiments: Using SentiWordNet (Esuli and Sebastiani, 2006) for every word bar articles and conjunctions, and combining the sentiment polarity values into net positive aggregate and net negative aggregate features. • Repeated exclamations and other punctuation: Number of sets of two or more contiguous punctuation. • Exclamation at end of sentence: Boolean value. • Monolingual Sentence Vectors: Bojanowski et al. (2017)’s method is used to train word vectors for Hindi words in the code-mixed sentences. 373 t Representation of the Collective Sentiment of the Sentence cmr s = cmr c = h t Representation of the Speciﬁc Sentiment of the Sentence ∑ α ik i i=1 α1 ← h1 ← h2 ← h3 ← ht → h1 → h2 → h3 → ht sw 3 sw t α2 α3 Attention Weights αt ← k1 ← k2 ← k3 ← kt → → → → sw 1 sw 2 sw 3 sw t BiLSTM Layer BiLSTM Layer sw 1 Sub-word Representations sw 2 k1 k2 k3 kt Sub-word Representations (B) Speciﬁc Encoder Models the speciﬁc sentiments of the sentence (A) Collective Encoder Models the overall sentiment of the sentence F"
P19-2052,W14-3908,0,0.0241977,"in a polarity classification model. Ensembling simple word embedding based models with surfaceform classifiers has also yielded slight improvements (Araque et al., 2017). Extending standard NLP tasks to code-mixed data has presented peculiar challenges. Methods for POS tagging of code-mixed data obtained from online social media such as Facebook and Twitter has been attempted (Vyas et al., 2014) . Shallow parsing of code-mixed data curated from social media has also been tried (Sharma et al., 2016). Work has also been done to support word level identification of languages in code-mixed text (Chittaranjan et al., 2014). Sharma et al. (2015) tried an approach based on lexicon lookup for text normalization and sentiment analysis of code-mixed data. Pravalika et al. (2017) used a lexicon lookup approach to perform domain specific sentiment analysis. Other attempts include using sub-word level compositions with LSTMs to capture sentiment at morpheme level (Joshi et al., 2016), or using contrastive learning with Siamese networks to map code-mixed and standard language text to a common sentiment space (Choudhary et al., 2018). 3 3.1 Generating Subword Representations Word embeddings are now commonplace but are ge"
P19-2052,D13-1170,0,0.00807254,"Missing"
P19-2052,esuli-sebastiani-2006-sentiwordnet,0,0.0102381,"king inspiration from (Yang et al., 2016), we quantify the significance of a subword by measuring the similarity of an additional hidden representation ui of each sub-word against a sub-word level context vector X. Then, a normalized significance weight αi is obtained. (3) exp(uTi X) αi = P exp(uTi X) (4) Feature Network We also use linguistic features to augment the neural network framework of our model. Specific Encoder ui = tanh(Wi ki + bi ) (6) • Extended words: Number of words with multiple contiguous repeating characters. • Aggregate positive and negative sentiments: Using SentiWordNet (Esuli and Sebastiani, 2006) for every word bar articles and conjunctions, and combining the sentiment polarity values into net positive aggregate and net negative aggregate features. • Repeated exclamations and other punctuation: Number of sets of two or more contiguous punctuation. • Exclamation at end of sentence: Boolean value. • Monolingual Sentence Vectors: Bojanowski et al. (2017)’s method is used to train word vectors for Hindi words in the code-mixed sentences. 373 t Representation of the Collective Sentiment of the Sentence cmr s = cmr c = h t Representation of the Speciﬁc Sentiment of the Sentence ∑ α ik i i=1"
P19-2052,D14-1105,0,0.0622259,"using SVMs. Keeping in mind a lack of computational resources, Giatsoglou et al. (2017) came up with a hybrid framework to exploit lexicons (polarized and emotional words) as well as different word embedding approaches in a polarity classification model. Ensembling simple word embedding based models with surfaceform classifiers has also yielded slight improvements (Araque et al., 2017). Extending standard NLP tasks to code-mixed data has presented peculiar challenges. Methods for POS tagging of code-mixed data obtained from online social media such as Facebook and Twitter has been attempted (Vyas et al., 2014) . Shallow parsing of code-mixed data curated from social media has also been tried (Sharma et al., 2016). Work has also been done to support word level identification of languages in code-mixed text (Chittaranjan et al., 2014). Sharma et al. (2015) tried an approach based on lexicon lookup for text normalization and sentiment analysis of code-mixed data. Pravalika et al. (2017) used a lexicon lookup approach to perform domain specific sentiment analysis. Other attempts include using sub-word level compositions with LSTMs to capture sentiment at morpheme level (Joshi et al., 2016), or using co"
P19-2052,P12-2018,0,0.125109,"Missing"
P19-2052,N16-1174,0,0.0423438,"ilar to the collective encoder, in that it takes as input a subword representation of the sentence and is built over LSTMs, with one caveat - an affixed attention mechanism. This allows for selection of subwords which contribute the most towards the sentiment of the input text. It can be seen in Figure 2(B). Identifying which subwords play a significant role in deciding the sentiment is crucial. The specific encoder generates a context vector cmrs to this end. We first concatenate the forward and backward states to obtain a combined annotation (k1 , k2 , . . . , kt ). Taking inspiration from (Yang et al., 2016), we quantify the significance of a subword by measuring the similarity of an additional hidden representation ui of each sub-word against a sub-word level context vector X. Then, a normalized significance weight αi is obtained. (3) exp(uTi X) αi = P exp(uTi X) (4) Feature Network We also use linguistic features to augment the neural network framework of our model. Specific Encoder ui = tanh(Wi ki + bi ) (6) • Extended words: Number of words with multiple contiguous repeating characters. • Aggregate positive and negative sentiments: Using SentiWordNet (Esuli and Sebastiani, 2006) for every wor"
W02-0902,J90-2002,0,0.0632522,"Missing"
W02-0902,P98-1069,0,0.730799,"Missing"
W02-0902,W01-0504,1,0.33419,"Missing"
W02-0902,N01-1020,0,0.0613017,"nt and Pr¨asident. Still, these words – often called cognates – maintain a very similar spelling. This can be defined as differing in very few letters. This measurement can be formalized as the number of letters common in sequence between the two words, divided by the length of the longer word. The example word pair friend and freund shares 5 letters (fr-e-nd), and both words have length 6, hence there spelling similarity is 5/6, or 0.83. This measurement is called longest common subsequence ratio [Melamed, 1995]. In related work, string edit distance (or, Levenshtein distance) has been used [Mann and Yarowski, 2001]. With this computational means at hand, we can now measure the spelling similarity between every German and English word, and sort possible word pairs accordingly. By going through this list starting at the top we can collect new word pairs. We do this is in a greedy fashion – once a word is assigned to a word pair, we do not look for another match. Table 2 gives the top 24 generated word pairs by this algorithm. English organization president industries parliament interests institute satellite dividend machine magazine february program premium branch volume january warning parties debate ex"
W02-0902,W95-0115,0,0.00946485,"or words that can be traced back to common language roots, such as friend and Freund, or president and Pr¨asident. Still, these words – often called cognates – maintain a very similar spelling. This can be defined as differing in very few letters. This measurement can be formalized as the number of letters common in sequence between the two words, divided by the length of the longer word. The example word pair friend and freund shares 5 letters (fr-e-nd), and both words have length 6, hence there spelling similarity is 5/6, or 0.83. This measurement is called longest common subsequence ratio [Melamed, 1995]. In related work, string edit distance (or, Levenshtein distance) has been used [Mann and Yarowski, 2001]. With this computational means at hand, we can now measure the spelling similarity between every German and English word, and sort possible word pairs accordingly. By going through this list starting at the top we can collect new word pairs. We do this is in a greedy fashion – once a word is assigned to a word pair, we do not look for another match. Table 2 gives the top 24 generated word pairs by this algorithm. English organization president industries parliament interests institute sa"
W02-0902,P95-1050,0,0.66337,"propose a similar approach: They count how often another word occurs in the same sentence as the target word. The counts are then normalized by a using the tf/idf method which is often used in information retrieval [Jones, 1979]. The need for translating the context poses a chicken-and-egg problem: If we already have a translation lexicon we can translate the context vectors. But we can only construct a translation lexicon with this approach if we are already able to translate the context vectors. Theoretically, it is possible to use these methods to build a translation lexicon from scratch [Rapp, 1995]. The number of possible mappings has complexity O(n!), and the computing cost of each mapping has quadratic complexity O(n2 ). For a large number of words n – at least more than 10,000, maybe more than 100,000 – the combined complexity becomes prohibitively expensive. Because of this, both Rapp and Fung focus on expanding an existing large lexicon to add a few novel terms. Clearly, a seed lexicon to bootstrap these methods is needed. Fortunately, we have outlined in Section 2.1 how such a seed lexicon can be obtained: by finding words spelled identically in both languages. We can then constr"
W02-0902,P99-1067,0,0.925608,"Missing"
W02-0902,W99-0626,0,0.0158485,"Missing"
W02-0902,C98-1066,0,\N,Missing
W04-3250,J90-2002,0,0.0843486,"resampling methods. We also provide empirical evidence that the estimated significance levels are accurate by comparing different systems on a large number of test sets of various sizes. In this paper, after providing some background, we will examine some properties of the widely used BLEU metric, discuss experimental design, introduce bootstrap resampling methods for statistical significance estimation and report on experimental results that demonstrate the accuracy of the methods. 2 Background 2.1 Statistical Machine Translation Statistical machine translation was introduced by work at IBM [Brown et al., 1990, 1993]. Currently, the most successful such systems employ so-called phrase-based methods that translate input text by translating sequences of words at a time [Och, 2002; Zens et al., 2002; Koehn et al., 2003; Vogel et al., 2003; Tillmann, 2003] Phrase-based machine translation systems make use of a language model trained for the target language and a translation model trained from a parallel corpus. The translation model is typically broken down into several components, e.g., reordering model, phrase translation model, and word translation model. 2.2 Automatic Evaluation To adequately evalu"
W04-3250,N03-1010,0,0.0559426,"Missing"
W04-3250,koen-2004-pharaoh,0,0.0362258,"Missing"
W04-3250,N03-1017,1,0.144599,"oviding some background, we will examine some properties of the widely used BLEU metric, discuss experimental design, introduce bootstrap resampling methods for statistical significance estimation and report on experimental results that demonstrate the accuracy of the methods. 2 Background 2.1 Statistical Machine Translation Statistical machine translation was introduced by work at IBM [Brown et al., 1990, 1993]. Currently, the most successful such systems employ so-called phrase-based methods that translate input text by translating sequences of words at a time [Och, 2002; Zens et al., 2002; Koehn et al., 2003; Vogel et al., 2003; Tillmann, 2003] Phrase-based machine translation systems make use of a language model trained for the target language and a translation model trained from a parallel corpus. The translation model is typically broken down into several components, e.g., reordering model, phrase translation model, and word translation model. 2.2 Automatic Evaluation To adequately evaluate the quality of any translation is difficult, since it is not entirely clear what the focus of the evaluation should be. Surely, a good translation has to adequately capture the meaning of the foreign origin"
W04-3250,N04-1022,0,0.367215,"Missing"
W04-3250,N03-2021,0,0.0117616,"Missing"
W04-3250,P03-1021,0,0.433961,"Missing"
W04-3250,P02-1040,0,0.12558,"Missing"
W04-3250,W03-1001,0,0.0464274,"some properties of the widely used BLEU metric, discuss experimental design, introduce bootstrap resampling methods for statistical significance estimation and report on experimental results that demonstrate the accuracy of the methods. 2 Background 2.1 Statistical Machine Translation Statistical machine translation was introduced by work at IBM [Brown et al., 1990, 1993]. Currently, the most successful such systems employ so-called phrase-based methods that translate input text by translating sequences of words at a time [Och, 2002; Zens et al., 2002; Koehn et al., 2003; Vogel et al., 2003; Tillmann, 2003] Phrase-based machine translation systems make use of a language model trained for the target language and a translation model trained from a parallel corpus. The translation model is typically broken down into several components, e.g., reordering model, phrase translation model, and word translation model. 2.2 Automatic Evaluation To adequately evaluate the quality of any translation is difficult, since it is not entirely clear what the focus of the evaluation should be. Surely, a good translation has to adequately capture the meaning of the foreign original. However, pinning down all the nu"
W04-3250,2003.mtsummit-papers.53,0,0.0647408,"und, we will examine some properties of the widely used BLEU metric, discuss experimental design, introduce bootstrap resampling methods for statistical significance estimation and report on experimental results that demonstrate the accuracy of the methods. 2 Background 2.1 Statistical Machine Translation Statistical machine translation was introduced by work at IBM [Brown et al., 1990, 1993]. Currently, the most successful such systems employ so-called phrase-based methods that translate input text by translating sequences of words at a time [Och, 2002; Zens et al., 2002; Koehn et al., 2003; Vogel et al., 2003; Tillmann, 2003] Phrase-based machine translation systems make use of a language model trained for the target language and a translation model trained from a parallel corpus. The translation model is typically broken down into several components, e.g., reordering model, phrase translation model, and word translation model. 2.2 Automatic Evaluation To adequately evaluate the quality of any translation is difficult, since it is not entirely clear what the focus of the evaluation should be. Surely, a good translation has to adequately capture the meaning of the foreign original. However, pinning"
W04-3250,2002.tmi-tutorials.2,0,0.0301945,"his paper, after providing some background, we will examine some properties of the widely used BLEU metric, discuss experimental design, introduce bootstrap resampling methods for statistical significance estimation and report on experimental results that demonstrate the accuracy of the methods. 2 Background 2.1 Statistical Machine Translation Statistical machine translation was introduced by work at IBM [Brown et al., 1990, 1993]. Currently, the most successful such systems employ so-called phrase-based methods that translate input text by translating sequences of words at a time [Och, 2002; Zens et al., 2002; Koehn et al., 2003; Vogel et al., 2003; Tillmann, 2003] Phrase-based machine translation systems make use of a language model trained for the target language and a translation model trained from a parallel corpus. The translation model is typically broken down into several components, e.g., reordering model, phrase translation model, and word translation model. 2.2 Automatic Evaluation To adequately evaluate the quality of any translation is difficult, since it is not entirely clear what the focus of the evaluation should be. Surely, a good translation has to adequately capture the meaning o"
W05-0820,2005.mtsummit-papers.11,1,0.226362,"Missing"
W05-0820,E03-1076,1,0.415417,"02). Finally, we suggested the use of Pharaoh (Koehn, 2004b), a phrase-based machine translation decoder. How well does this setup match the state of the art? The MIT system using the Pharaoh decoder (Koehn, 2004a) proved to be very competitive in last year’s NIST evaluation. However, the field is moving fast, and a number of steps help to improve upon the provided baseline setup, e.g., larger language models trained on general text (up to a billion words have been used), better reodering models (e.g., suggested by Tillman (2004) and Och et al. (2004)), better language-specific preprocessing (Koehn and Knight, 2003) and restructuring (Collins et al., 2005), additional feature functions such as word class language models, and minimum error rate training (Och, 2003) to optimize parameters. Some of these steps (e.g., improved reordering models) go beyond the current capabilities of Pharaoh. However, we are hopeful that freely available software continues to match or at least follow closely the state of the art. We announced the shared task on March 3, and provided all the resources mentioned above (also a development test corpus to track the quality of systems being developed). The test schedule called for"
W05-0820,N03-1017,1,0.0943469,"e provided not only training data from the Europarl corpus (Koehn, 2005), but also additional resources: sentence and word alignments, the decoder Pharaoh1 (Koehn, 2004b), and a language model, so that participation was feasible even as a graduate level class project. Using about 15 million words of translated text, participants were asked to build a phrase-based statistical machine translation system. The focus of the task was to build a probabilistic phrase translation table, since most of the other resources were provided — for more on phrase-based statistical machine translation, refer to Koehn et al. (2003). The participants’ systems were compared by how well they translated 2000 previously unseen test sentences from the same domain. The shared task operated within an extremely short timeframe. The workshop and hence the shared task was accepted on February 22, 2005 and announced on March 3. The official test data was made available on April 3, results were due one week later. Despite this tight schedule, eleven research groups participated and built a total of 32 machine translation systems for the four language pairs. 1 Goals When setting up this competition, we were motivated by a number of g"
W05-0820,P03-1021,0,0.0562852,"IT system using the Pharaoh decoder (Koehn, 2004a) proved to be very competitive in last year’s NIST evaluation. However, the field is moving fast, and a number of steps help to improve upon the provided baseline setup, e.g., larger language models trained on general text (up to a billion words have been used), better reodering models (e.g., suggested by Tillman (2004) and Och et al. (2004)), better language-specific preprocessing (Koehn and Knight, 2003) and restructuring (Collins et al., 2005), additional feature functions such as word class language models, and minimum error rate training (Och, 2003) to optimize parameters. Some of these steps (e.g., improved reordering models) go beyond the current capabilities of Pharaoh. However, we are hopeful that freely available software continues to match or at least follow closely the state of the art. We announced the shared task on March 3, and provided all the resources mentioned above (also a development test corpus to track the quality of systems being developed). The test schedule called for the translation of 2000 sentence for each of the four language pairs in the week between April 3–10. We allowed late submissions up to April 17. 121 3"
W05-0820,N04-1021,0,0.0329865,"l corpus using the SRI language modeling toolkit (Stolke, 2002). Finally, we suggested the use of Pharaoh (Koehn, 2004b), a phrase-based machine translation decoder. How well does this setup match the state of the art? The MIT system using the Pharaoh decoder (Koehn, 2004a) proved to be very competitive in last year’s NIST evaluation. However, the field is moving fast, and a number of steps help to improve upon the provided baseline setup, e.g., larger language models trained on general text (up to a billion words have been used), better reodering models (e.g., suggested by Tillman (2004) and Och et al. (2004)), better language-specific preprocessing (Koehn and Knight, 2003) and restructuring (Collins et al., 2005), additional feature functions such as word class language models, and minimum error rate training (Och, 2003) to optimize parameters. Some of these steps (e.g., improved reordering models) go beyond the current capabilities of Pharaoh. However, we are hopeful that freely available software continues to match or at least follow closely the state of the art. We announced the shared task on March 3, and provided all the resources mentioned above (also a development test corpus to track the"
W05-0820,J03-1002,0,0.0939299,"research: One of our main goals with providing as many resources as possible was to keep the barrier of entry low. Participants could use the word alignment and other resources and focus on phrase extraction. We hoped to attract researchers that are relatively new to the field. We were satisfied to learn that many entries are by graduate students working on their own. Promote and create free resources: Academic research thrives on freely available resources. The field of statistical machine translation has been blessed with a long tradition of freely available software tools — such as GIZA++ (Och and Ney, 2003) — and parallel corpora — such as the Canadian Hansards2 . Following this lead, we made word alignments and a language model available for this competition in addition to our previously published resources (Europarl and Pharaoh). The competition created resources as well. Most teams agreed to share system output and their model files. You can download them from the competition web site3 . Promote work on European language pairs: Finally, we wanted to promote work on European languages. The increasing economic and political ties within the European Union create a huge need for translation servi"
W05-0820,P02-1040,0,0.111328,"developed). The test schedule called for the translation of 2000 sentence for each of the four language pairs in the week between April 3–10. We allowed late submissions up to April 17. 121 3 Results Eleven teams from eight institutions in Europe and North America participated, see Figure 2 for a complete list. The figure also indicates, if a team used the Pharaoh decoder (eight teams), the provided language model (seven teams) and the provided word alignment (four did, three of those with additional preprocessing or additional data). Translation performance was measured using the BLEU score (Papineni et al., 2002), which measures n-gram overlap with a reference translation. In our case, we only used a single reference translation, since the test set was taken from a held-out portion of the Europarl corpus. On the other hand we used a relatively large number of test sentences to guarantee that the BLEU results are stable despite the fact that we used only one reference translation for each sentence. Shared tasks like this one, of course, bring out the competitive spirit of participants and can draw criticisms about being a horse race. From an outside perspective, however, it is far more interesting to l"
W05-0820,N04-4026,0,0.0204291,"task of the Europarl corpus using the SRI language modeling toolkit (Stolke, 2002). Finally, we suggested the use of Pharaoh (Koehn, 2004b), a phrase-based machine translation decoder. How well does this setup match the state of the art? The MIT system using the Pharaoh decoder (Koehn, 2004a) proved to be very competitive in last year’s NIST evaluation. However, the field is moving fast, and a number of steps help to improve upon the provided baseline setup, e.g., larger language models trained on general text (up to a billion words have been used), better reodering models (e.g., suggested by Tillman (2004) and Och et al. (2004)), better language-specific preprocessing (Koehn and Knight, 2003) and restructuring (Collins et al., 2005), additional feature functions such as word class language models, and minimum error rate training (Och, 2003) to optimize parameters. Some of these steps (e.g., improved reordering models) go beyond the current capabilities of Pharaoh. However, we are hopeful that freely available software continues to match or at least follow closely the state of the art. We announced the shared task on March 3, and provided all the resources mentioned above (also a development tes"
W05-0820,koen-2004-pharaoh,0,\N,Missing
W05-0820,P05-1066,1,\N,Missing
W06-3114,W06-3123,1,0.282531,"Missing"
W06-3114,E06-1032,1,0.793931,"uation For the automatic evaluation, we used B LEU, since it is the most established metric in the field. The B LEU metric, as all currently proposed automatic metrics, is occasionally suspected to be biased towards statistical systems, especially the phrase-based systems currently in use. It rewards matches of n-gram sequences, but measures only at most indirectly overall grammatical coherence. The B LEU score has been shown to correlate well with human judgement, when statistical machine translation systems are compared (Doddington, 2002; Przybocki, 2004; Li, 2005). However, a recent study (Callison-Burch et al., 2006), pointed out that this correlation may not always be strong. They demonstrated this with the comparison of statistical systems against (a) manually post-edited MT output, and (b) a rule-based commercial system. The development of automatic scoring methods is an open field of research. It was our hope that this competition, which included the manual and automatic evaluation of statistical systems and one rulebased commercial system, will give further insight into the relation between automatic and manual evaluation. At the very least, we are creating a data resource (the manual annotations) th"
W06-3114,P05-1066,1,0.520404,"Missing"
W06-3114,W06-3120,0,0.0236158,"Missing"
W06-3114,W06-3125,0,0.0522684,"Missing"
W06-3114,2005.iwslt-1.1,0,0.0444919,", prepared training and test sets, resulting in actual machine translation output from several state-of-the-art systems and manual evaluations. All this is available at the workshop website1 . The shared task is a follow-up to the one we organized in the previous year, at a similar venue (Koehn and Monz, 2005). As then, we concentrated on the translation of European languages and the use of the Europarl corpus for training. Again, most systems that participated could be categorized as statistical phrase-based systems. While there is now a number of competitions — DARPA/NIST (Li, 2005), IWSLT (Eck and Hori, 2005), TC-Star — this one focuses on text translation between various European languages. This year’s shared task changed in some aspects from last year’s: • We carried out a manual evaluation in addition to the automatic scoring. Manual evaluation 1 http://www.statmt.org/wmt06/ • We evaluated translation from English, in addition to into English. English was again paired with German, French, and Spanish. We dropped, however, one of the languages, Finnish, partly to keep the number of tracks manageable, partly because we assumed that it would be hard to find enough Finnish speakers for the manual e"
W06-3114,W06-3126,0,0.0173835,"Missing"
W06-3114,W06-3118,0,0.0167429,"Missing"
W06-3114,W04-3250,1,0.375882,"two systems of equal performance. Let say, if we find one system doing better on 20 of the blocks, and worse on 80 of the blocks, is it significantly worse? We check, how likely only up to k = 20 better scores out of n = 100 would have been generated by two equal systems, using the binomial distribution: p(0..k; n, p) = 2.2 Statistical Significance k   X i i=0 Confidence Interval: Since B LEU scores are not computed on the sentence level, traditional methods to compute statistical significance and confidence intervals do not apply. Hence, we use the bootstrap resampling method described by Koehn (2004). Following this method, we repeatedly — say, 1000 times — sample sets of sentences from the out105 = 0.5n n pi pn−i k   X i i=0 (3) n If p(0..k; n, p) < 0.05, or p(0..k; n, p) > 0.95 then we have a statistically significant difference between the systems. Figure 3: Annotation tool for manual judgement of adequacy and fluency of the system output. Translations from 5 randomly selected systems for a randomly selected sentence is presented. No additional information beyond the instructions on this page are given to the judges. The tool tracks and reports annotation speed. 3 Manual Evaluation 3"
W06-3114,W05-0820,1,0.725787,"ranging from commercial companies, industrial research labs to individual graduate students. The motivation for such a competition is to establish baseline performance numbers for defined training scenarios and test sets. We assembled various forms of data and resources: a baseline MT system, language models, prepared training and test sets, resulting in actual machine translation output from several state-of-the-art systems and manual evaluations. All this is available at the workshop website1 . The shared task is a follow-up to the one we organized in the previous year, at a similar venue (Koehn and Monz, 2005). As then, we concentrated on the translation of European languages and the use of the Europarl corpus for training. Again, most systems that participated could be categorized as statistical phrase-based systems. While there is now a number of competitions — DARPA/NIST (Li, 2005), IWSLT (Eck and Hori, 2005), TC-Star — this one focuses on text translation between various European languages. This year’s shared task changed in some aspects from last year’s: • We carried out a manual evaluation in addition to the automatic scoring. Manual evaluation 1 http://www.statmt.org/wmt06/ • We evaluated tr"
W06-3114,W06-3121,0,0.0396613,"Missing"
W06-3114,W06-3122,0,0.0280607,"Missing"
W06-3114,W06-3116,0,0.0257747,"Missing"
W06-3114,W05-0908,0,0.0290753,"Missing"
W06-3114,W06-3117,0,0.0296838,"Missing"
W06-3114,W06-3115,0,0.0225709,"Missing"
W06-3114,W06-3124,0,\N,Missing
W06-3114,2006.amta-papers.2,0,\N,Missing
W06-3114,W06-3119,0,\N,Missing
W06-3123,J93-2003,0,0.0104668,"translation lexicons for both words and phrases. The joint model has been shown to outperform standard models on restricted data sets such as the small data track for Chinese-English in the 2004 NIST MT Evaluation (Przybocki, 2004). Introduction Machine translation is a hard problem because of the highly complex, irregular and diverse nature of natural languages. It is impossible to accurately model all the linguistic rules that shape the translation process, and therefore a principled approach uses statistical methods to make optimal decisions given incomplete data. The original IBM Models (Brown et al., 1993) learn word-to-word alignment probabilities which makes it computationally feasible to estimate model parameters from large amounts of training data. Phrase-based SMT models, such as the alignment template model (Och, 2003), improve on word-based models because phrases provide local context which leads to better lexical choice and more reliable local reordering. However, most phrase-based models extract their phrase pairs from previously word-aligned corpora using adhoc heuristics. These models perform no search 154 However, considering all possible phrases and all their possible alignments va"
W06-3123,P05-1066,1,0.176316,"hill-climbing are retained. Only a very small proportion of the alignment space can be searched and this reduces the chances of finding optimum parameters. The small number of alignments visited would lead to data sparseness and over-fitting. Another factor could be efficiency trade-offs like the fast but not optimal competitive linking search for phrasal alignments. 4.3 German-English submission We also submitted a German-English system using the standard approach to phrase extraction. The purpose of this submission was to validate the syntactic reordering method that we previously proposed (Collins et al., 2005). We parse the German training and test corpus and reorder it according to a set of manually devised rules. Then, we use our phrase-based system with standard phraseextraction, lexicalized reordering, lexical scoring, 5-gram LM, and the Pharaoh decoder. On the development test set, the syntactic reordering improved performance from 26.86 to 27.70. The best submission in last year’s shared task achieved a score of 24.77 on this set. 5 Conclusion We presented the first attempt at creating a systematic framework which uses word alignment constraints to guide phrase-based EM training. This shows c"
W06-3123,N03-1017,1,0.177164,"idence word alignments for each sentence. These high confidence alignments could incorporate information from both statistical and linguistic sources. In this paper we use the points of high confidence from the intersection of the bi-directional Viterbi word alignments to constrain the model, increasing performance and decreasing complexity. Proceedings of the Workshop on Statistical Machine Translation, pages 154–157, c New York City, June 2006. 2006 Association for Computational Linguistics 2 2.1 Translation Models Standard Phrase-based Model Most phrase-based translation models (Och, 2003; Koehn et al., 2003; Vogel et al., 2003) rely on a pre-existing set of word-based alignments from which they induce their parameters. In this project we use the model described by Koehn et al. (2003) which extracts its phrase alignments from a corpus that has been word aligned. From now on we refer to this phrase-based translation model as the standard model. The standard model decomposes the foreign input sentence F into a sequence of I phrases f 1 , . . . , f I . Each foreign phrase fi is translated to an English phrase ei using the probability distribution θ(f i |ei ). English phrases may be reordered using a"
W06-3123,W02-1018,0,0.2878,"the computational complexity of estimating parameters at the phrase level. We present the first model to use word alignments for constraining the space of phrasal alignments searched during Expectation Maximization (EM) training. Constraining the joint model improves performance, showing results that are very close to stateof-the-art phrase-based models. It also allows it to scale up to larger corpora and therefore be more widely applicable. 1 for optimal phrasal alignments. Even though this is an efficient strategy, it is a departure from the rigorous statistical framework of the IBM Models. Marcu and Wong (2002) proposed the joint probability model which directly estimates the phrase translation probabilities from the corpus in a theoretically governed way. This model neither relies on potentially sub-optimal word alignments nor on heuristics for phrase extraction. Instead, it searches the phrasal alignment space, simultaneously learning translation lexicons for both words and phrases. The joint model has been shown to outperform standard models on restricted data sets such as the small data track for Chinese-English in the 2004 NIST MT Evaluation (Przybocki, 2004). Introduction Machine translation i"
W06-3123,P97-1063,0,0.0262337,"ssible alignments C, each of which is defined as the product of the probability of all individual concepts: p(F, E) = X Y p(&lt; ei , f i &gt;) (1) C∈C &lt;ei ,f i &gt;∈C The model is trained by initializing the translation table using Stirling numbers of the second kind to efficiently estimate p(&lt; ei , f i &gt;) by calculating the proportion of alignments which contain p(&lt; ei , f i &gt;) compared to the total number of alignments in the sentence (Marcu and Wong, 2002). EM is then performed by first discovering an initial phrasal alignments using a greedy algorithm similar to the competitive linking algorithm (Melamed, 1997). The highest probability phrase pairs are iteratively selected until all phrases are are linked. Then hill-climbing is performed by searching once for each iteration for all merges, splits, moves and swaps that improve the probability of the initial phrasal alignment. Fractional counts are collected for all alignments visited. Training the IBM models is computationally challenging, but the joint model is much more demanding. Considering all possible segmentations of phrases and all their possible alignments vastly increases the number of possible alignments that can be formed between two sent"
W06-3123,P02-1038,0,0.0182161,"er corpora. After the initialization phase of the training, all phrase pairs with counts less 156 No. Concepts BLEU Time(min) Unconstrained 6,178k 19.93 299 Constrained 1,457k 22.13 169 Table 1. The impact of constraining the joint model trained on 10,000 sentences of the German-English Europarl corpora and tested with the Europarl test set used in Koehn et al. (2003) than 10 million times that of the phrase pair with the highest count, are pruned from the phrase table. The model is also parallelized in order to speed up training. The translation models are included within a log-linear model (Och and Ney, 2002) which allows a weighted combination of features functions. For the comparison of the basic systems in Table 2 only three features were used for both the joint and the standard model: p(e|f ), p(f |e) and the language model, and they were given equal weights. The results in Table 2 show that the joint model is capable of training on large data sets, with a reasonable performance compared to the standard model. However, here it seems that the standard model has a slight advantage. This is almost certainly related to the fact that the joint model results in a much smaller phrase table. Pruning e"
W06-3123,2003.mtsummit-papers.53,0,0.0236842,"ts for each sentence. These high confidence alignments could incorporate information from both statistical and linguistic sources. In this paper we use the points of high confidence from the intersection of the bi-directional Viterbi word alignments to constrain the model, increasing performance and decreasing complexity. Proceedings of the Workshop on Statistical Machine Translation, pages 154–157, c New York City, June 2006. 2006 Association for Computational Linguistics 2 2.1 Translation Models Standard Phrase-based Model Most phrase-based translation models (Och, 2003; Koehn et al., 2003; Vogel et al., 2003) rely on a pre-existing set of word-based alignments from which they induce their parameters. In this project we use the model described by Koehn et al. (2003) which extracts its phrase alignments from a corpus that has been word aligned. From now on we refer to this phrase-based translation model as the standard model. The standard model decomposes the foreign input sentence F into a sequence of I phrases f 1 , . . . , f I . Each foreign phrase fi is translated to an English phrase ei using the probability distribution θ(f i |ei ). English phrases may be reordered using a relative distortion"
W06-3123,koen-2004-pharaoh,0,\N,Missing
W06-3123,P02-1040,0,\N,Missing
W06-3123,P04-1023,1,\N,Missing
W06-3123,J04-4002,0,\N,Missing
W06-3123,W06-3105,0,\N,Missing
W06-3123,W06-3114,1,\N,Missing
W06-3123,2005.mtsummit-papers.11,1,\N,Missing
W06-3123,2005.iwslt-1.8,1,\N,Missing
W06-3123,P00-1056,0,\N,Missing
W06-3123,P03-1021,0,\N,Missing
W07-0702,J99-2004,0,0.392573,"Missing"
W07-0702,N03-2002,0,0.0405063,"general form α/β or αβ where α and β are themselves categories. An example of a CCG parse is given: Peter However, in many cases multiple dependencies are desirable. For instance translating CCG supertags independently of words could introduce errors. Multiple dependencies require some form of backing off to simpler models in order to cover the cases where, for instance, the word has been seen in training, but not with that particular supertag. Different backoff paths are possible, and it would be interesting but prohibitively slow to apply a strategy similar to generalised parallel backoff (Bilmes and Kirchhoff, 2003) which is used in factored language models. Backoff in factored language models is made more difficult because there is no obvious backoff path. This is compounded for factored phrase-based translation models where one has N X eats apples NP (SNP)/NP NP > SNP &lt; S where the derivation proceeds as follows: “eats” is combined with “apples” under the operation of forward application. “eats” can be thought of as a function that takes a NP to the right and returns a SNP. Similarly the phrase “eats apples” can be thought of as a function which takes a noun phrase NP to the left and returns a sente"
W07-0702,2005.mtsummit-papers.11,1,0.0284006,"se, far from the position of the English verb. 4.1 Experimental Setup The experiments were run using Moses2 , an open source factored statistical machine translation system. The SRILM language modelling toolkit (Stolcke, 2002) was used with modified Kneser-Ney discounting and interpolation. The CCG supertagger (Clark, 2002; Clark and Curran, 2004) was provided with the C&C Language Processing Tools3 . The supertagger was trained on the CCGBank in English (Hockenmaier and Steedman, 2005) and in German (Hockenmaier, 2006). The Dutch-English parallel training data comes from the Europarl corpus (Koehn, 2005) and excludes the proceedings from the last quarter of 2000. 2 3 This consists of 855,677 sentences with a maximum of 50 words per sentence. 500 sentences of tuning data and the 2000 sentences of test data are taken from the ACL Workshop on Building and Using Parallel Texts4 . The German-English experiments use data from the NAACL 2006 Workshop on Statistical Machine Translation5 . The data consists of 751,088 sentences of training data, 500 sentences of tuning data and 3064 sentences of test data. The English and German training sets were POS tagged and supertagged before lowercasing. The lan"
W07-0702,N03-1019,0,0.0194211,"actored translation models allow the inclusion of supertags as a factor in the source or target language. We show that this results in an improvement in the quality of translation and that the value of syntactic supertags in flat structured phrase-based models is largely due to better local reorderings. 1 Introduction In large-scale machine translation evaluations, phrase-based models generally outperform syntaxbased models1 . Phrase-based models are effective because they capture the lexical dependencies between languages. However, these models, which are equivalent to finite-state machines (Kumar and Byrne, 2003), are unable to model long range word order differences. Phrase-based models also lack the ability to incorporate the generalisations implicit in syntactic knowledge and they do not respect linguistic phrase boundaries. This makes it difficult to improve reordering in phrase-based models. Syntax-based models can overcome some of the problems associated with phrase-based models because they are able to capture the long range structural mappings that occur in translation. Recently In this paper we investigate the idea of incorporating syntax into phrase-based models, thereby leveraging the stren"
W07-0702,E06-1032,1,0.156668,"Missing"
W07-0702,W06-1606,0,0.0525496,"Missing"
W07-0702,P05-1033,0,0.0609583,".c.birch-mayne@sms.ed.ac.uk Miles Osborne Philipp Koehn miles@inf.ed.ac.uk pkoehn@inf.ed.ac.uk School of Informatics University of Edinburgh 2 Buccleuch Place Edinburgh, EH8 9LW, UK Abstract there have been a few syntax-based models that show performance comparable to the phrase-based models (Chiang, 2005; Marcu et al., 2006). However, reliably learning powerful rules from parallel data is very difficult and prone to problems with sparsity and noise in the data. These models also suffer from a large search space when decoding with an integrated language model, which can lead to search errors (Chiang, 2005). Combinatorial Categorial Grammar (CCG) supertags present phrase-based machine translation with an opportunity to access rich syntactic information at a word level. The challenge is incorporating this information into the translation process. Factored translation models allow the inclusion of supertags as a factor in the source or target language. We show that this results in an improvement in the quality of translation and that the value of syntactic supertags in flat structured phrase-based models is largely due to better local reorderings. 1 Introduction In large-scale machine translation"
W07-0702,P03-1021,0,0.0379746,"s) with more general models (which only depends on words). This paper is the first to suggest this approach for combining multiple information sources in machine translation. Although the addition of supertags to phrasebased translation does show some improvement, their overall impact is limited. Sequence models over supertags clearly result in some improvements in local reordering but syntactic information contains long distance dependencies which are simply not utilised in phrase-based models. 2 Factored Models forms. The factored translation model combines features in a log-linear fashion (Och, 2003). The most likely target sentence tˆ is calculated using the decision rule in Equation 1: ( tˆ = arg max t M X tˆ ∝ M X ) λm hm (sF1 s , tF1 t ) (1) m=1 λm hm (sF1 s , tF1 t ) (2) m=1 where M is the number of features, hm (sF1 s , tF1 t ) are the feature functions over the factors, and λ are the weights which combine the features which are optimised using minimum error rate training (Venugopal and Vogel, 2005). Each function depends on a vector sF1 s of source factors and a vector tF1 t of target factors. An example of a factored model used in upcoming experiments is: tˆ ∝ M X λm hm (sw , twc"
W07-0702,P04-1014,0,0.00805384,"ct of CCG supertags on the source, translating from German into English. These language pairs present a considerable reordering challenge. For example, Dutch and German have SOV word order in subordinate clauses. This means that the verb often appears at the end of the clause, far from the position of the English verb. 4.1 Experimental Setup The experiments were run using Moses2 , an open source factored statistical machine translation system. The SRILM language modelling toolkit (Stolcke, 2002) was used with modified Kneser-Ney discounting and interpolation. The CCG supertagger (Clark, 2002; Clark and Curran, 2004) was provided with the C&C Language Processing Tools3 . The supertagger was trained on the CCGBank in English (Hockenmaier and Steedman, 2005) and in German (Hockenmaier, 2006). The Dutch-English parallel training data comes from the Europarl corpus (Koehn, 2005) and excludes the proceedings from the last quarter of 2000. 2 3 This consists of 855,677 sentences with a maximum of 50 words per sentence. 500 sentences of tuning data and the 2000 sentences of test data are taken from the ACL Workshop on Building and Using Parallel Texts4 . The German-English experiments use data from the NAACL 2006"
W07-0702,P05-1034,0,0.0456775,"ted in range. If it can replace 14 CCG supertags, it suggests that supertags’ influence is also within a local range. 4.7 CCG Supertags on Source Sequence models over supertags improve the performance of phrase-based machine translation. However, this is a limited way of leveraging the rich syntactic information available in the CCG categories. We explore the potential of letting supertags direct translation by including them as a factor on the source. This is similar to syntax-directed translation originally proposed for compiling (Aho and Ullman, 1969), and also used in machine translation (Quirk et al., 2005; Huang et al., 2006). Information about the source words’ syntactic function and subcategorisation can directly influence the hypotheses being searched in decoding. These experiments were performed on the German to English translation task, in contrast to the Dutch to English results given in previous experiments. We use a model which combines more specific dependencies on source words and source CCG supertags, with a more general model which only has dependancies on the source word, see Equation 4. We explore two different ways of balancing the statistical evidence from these multiple source"
W07-0702,W02-2203,0,0.352084,"e “eats apples” can be thought of as a function which takes a noun phrase NP to the left and returns a sentence S. This operation is called backward application. A sentence together with its CCG categories already contains most of the information present in a full parse. Because these categories are lexicalised, they can easily be included into factored phrasebased translation. CCG supertags are categories that have been provided by a supertagger. Supertags were introduced by Bangalore (1999) as a way of increasing parsing efficiency by reducing the number of structures assigned to each word. Clark (2002) developed a suppertagger for CCG which uses a conditional maximum entropy model to estimate the probability of words being assigned particular categories. Here is an example of a sentence that has been supertagged in the training corpus: We all agree on that . NP NPNP (S[dcl]NP)/PP PP/NP NP . The verb “agree” has been assigned a complex supertag (S[dcl]NP)/PP which determines the type and direction of its arguments. This information can be used to improve the quality of translation. 4 Experiments The first set of experiments explores the effect of CCG supertags on the target, translating f"
W07-0702,P07-1037,0,0.518309,"Missing"
W07-0702,P06-1064,0,0.010386,"d order in subordinate clauses. This means that the verb often appears at the end of the clause, far from the position of the English verb. 4.1 Experimental Setup The experiments were run using Moses2 , an open source factored statistical machine translation system. The SRILM language modelling toolkit (Stolcke, 2002) was used with modified Kneser-Ney discounting and interpolation. The CCG supertagger (Clark, 2002; Clark and Curran, 2004) was provided with the C&C Language Processing Tools3 . The supertagger was trained on the CCGBank in English (Hockenmaier and Steedman, 2005) and in German (Hockenmaier, 2006). The Dutch-English parallel training data comes from the Europarl corpus (Koehn, 2005) and excludes the proceedings from the last quarter of 2000. 2 3 This consists of 855,677 sentences with a maximum of 50 words per sentence. 500 sentences of tuning data and the 2000 sentences of test data are taken from the ACL Workshop on Building and Using Parallel Texts4 . The German-English experiments use data from the NAACL 2006 Workshop on Statistical Machine Translation5 . The data consists of 751,088 sentences of training data, 500 sentences of tuning data and 3064 sentences of test data. The Engli"
W07-0702,W06-3601,0,0.00472148,"can replace 14 CCG supertags, it suggests that supertags’ influence is also within a local range. 4.7 CCG Supertags on Source Sequence models over supertags improve the performance of phrase-based machine translation. However, this is a limited way of leveraging the rich syntactic information available in the CCG categories. We explore the potential of letting supertags direct translation by including them as a factor on the source. This is similar to syntax-directed translation originally proposed for compiling (Aho and Ullman, 1969), and also used in machine translation (Quirk et al., 2005; Huang et al., 2006). Information about the source words’ syntactic function and subcategorisation can directly influence the hypotheses being searched in decoding. These experiments were performed on the German to English translation task, in contrast to the Dutch to English results given in previous experiments. We use a model which combines more specific dependencies on source words and source CCG supertags, with a more general model which only has dependancies on the source word, see Equation 4. We explore two different ways of balancing the statistical evidence from these multiple sources. The first way to c"
W07-0702,N03-1017,1,0.00863018,"actors. Phrase-based models are limited to sequences of words as their units with no access to additional linguistic knowledge. Factors allow for richer translation models, for example, the gender or tense of a word can be expressed. Factors also allow the model to generalise, for example, the lemma of a word could be used to generalise to unseen inflected 10 Figure 1. Factored translation with source words determining target words and CCG supertags For our experiments we used the following features: the translation probabilities P r(sF1 s |tF1 t ) and P r(tF1 t |sF1 s ), the lexical weights (Koehn et al., 2003) lex(sF1 s |tF1 t ) and lex(tF1 t |sF1 s ), and a phrase penalty e, which allows the model to learn a preference for longer or shorter phrases. Added to these features is the word penalty e−1 which allows the model to learn a preference for longer or shorter sentences, the distortion model d that prefers monotone word order, and the language model probability P r(t). All these features are logged when combined in the log-linear model in order to retain the impact of very unlikely translations or sequences. One of the strengths of the factored model is it allows for n-gram distributions over fa"
W07-0702,W06-2918,1,0.902719,"overcome some of the problems associated with phrase-based models because they are able to capture the long range structural mappings that occur in translation. Recently In this paper we investigate the idea of incorporating syntax into phrase-based models, thereby leveraging the strengths of both the phrase-based models and syntactic structures. This is done using CCG supertags, which provide a rich source of syntactic information. CCG contains most of the structure of the grammar in the lexicon, which makes it possible to introduce CCG supertags as a factor in a factored translation model (Koehn et al., 2006). Factored models allow words to be vectors of features: one factor could be the surface form and other factors could contain linguistic information. Factored models allow for the easy inclusion of supertags in different ways. The first approach is to generate CCG supertags as a factor in the target and then apply an n-gram model over them, increasing the probability of more frequently seen sequences of supertags. This is a simple way of including syntactic information in a phrase-based model, and has also been suggested by Hassan et al. (2007). For both Arabic-English (Hassan et al., 2007) an"
W07-0702,P05-1003,1,0.828589,"Missing"
W07-0702,N04-4026,0,0.0161138,"solation contributes more to translation performance than any other sequence model. Even with a high order language model, applying the CCG supertag sequence model still seems to improve performance. This means that even if we use a more powerful language model, the structural information contained in the supertags continues to be beneficial. 4.6 Lexicalised Reordering vs. Supertags In this experiment we investigate using a stronger reordering model to see how it compares to the contribution that CCG supertag sequence models make. Moses implements the lexicalised reordering model described by Tillman (2004), which learns whether phrases prefer monotone, inverse or disjoint orientations with regard to adjacent phrases. We apply this reordering models to the following experiments. Model sw , tw sw , twc None 23.97 24.42 Lex. Reord. 24.72 24.78 Table 5. Dutch-English models with and without a lexicalised reordering model. In Table 5 we can see that lexicalised reordering improves translation performance for both models. However, the improvement that was seen using CCG supertags without lexicalised reordering, almost disappears when using a stronger reordering model. This suggests that CCG supertags"
W07-0702,P07-2045,1,\N,Missing
W07-0702,2006.iwslt-evaluation.8,0,\N,Missing
W07-0718,W07-0735,0,0.066797,"Missing"
W07-0718,E06-1032,1,0.501959,"ve the time. 3 The judgment data along with all system translations are available at http://www.statmt.org/wmt07/ 142 4 Automatic evaluation The past two ACL workshops on machine translation used Bleu as the sole automatic measure of translation quality. Bleu was used exclusively since it is the most widely used metric in the field and has been shown to correlate with human judgments of translation quality in many instances (Doddington, 2002; Coughlin, 2003; Przybocki, 2004). However, recent work suggests that Bleu’s correlation with human judgments may not be as strong as previously thought (Callison-Burch et al., 2006). The results of last year’s workshop further suggested that Bleu systematically underestimated the quality of rule-based machine translation systems (Koehn and Monz, 2006). We used the manual evaluation data as a means of testing the correlation of a range of automatic metrics in addition to Bleu. In total we used eleven different automatic evaluation measures to rank the shared task submissions. They are: • Meteor (Banerjee and Lavie, 2005)—Meteor measures precision and recall of unigrams when comparing a hypothesis translation Language Pair English-German German-English English-Spanish Span"
W07-0718,J96-2004,0,0.0978688,"—the inter-annotator agreement was barely considered fair, and the intra-annotator agreement was only moderate. Even when we reassessed fluency and adequacy as relative ranks the agreements increased only minimally. 6 Meta-evaluation In addition to evaluating the translation quality of the shared task entries, we also performed a “metaevaluation” of our evaluation methodologies. 6.1 Inter- and Intra-annotator agreement We measured pairwise agreement among annotators using the kappa coefficient (K) which is widely used in computational linguistics for measuring agreement in category judgments (Carletta, 1996). It is defined as P (A) − P (E) K= 1 − P (E) where P (A) is the proportion of times that the annotators agree, and P (E) is the proportion of time that they would agree by chance. We define chance agreement for fluency and adequacy as 51 , since they are based on five point scales, and for ranking as 13 145 0.02 These timing figures are promising because they indicate that the tasks which the annotators were the most reliable on (constituent ranking and sentence ranking) were also much quicker to complete than the ones that they were unreliable on (assigning fluency and adequacy scores). This"
W07-0718,koen-2004-pharaoh,0,\N,Missing
W07-0718,H05-1100,0,\N,Missing
W07-0718,W07-0721,0,\N,Missing
W07-0718,W07-0731,0,\N,Missing
W07-0718,W07-0728,0,\N,Missing
W07-0718,W07-0738,0,\N,Missing
W07-0718,W07-0727,0,\N,Missing
W07-0718,N03-2021,0,\N,Missing
W07-0718,P05-1038,0,\N,Missing
W07-0718,P02-1040,0,\N,Missing
W07-0718,W05-0820,1,\N,Missing
W07-0718,W06-1610,0,\N,Missing
W07-0718,D07-1091,1,\N,Missing
W07-0718,W07-0730,0,\N,Missing
W07-0718,J04-4002,0,\N,Missing
W07-0718,W05-0909,0,\N,Missing
W07-0718,W07-0734,0,\N,Missing
W07-0718,2005.iwslt-1.1,0,\N,Missing
W07-0718,W07-0733,1,\N,Missing
W07-0718,W07-0707,0,\N,Missing
W07-0718,W07-0725,0,\N,Missing
W07-0718,W07-0723,0,\N,Missing
W07-0718,P05-1074,1,\N,Missing
W07-0718,W07-0732,1,\N,Missing
W07-0718,C04-1046,0,\N,Missing
W07-0718,W07-0724,0,\N,Missing
W07-0718,P05-1039,0,\N,Missing
W07-0718,N07-1006,0,\N,Missing
W07-0718,W06-3114,1,\N,Missing
W07-0718,N03-1017,1,\N,Missing
W07-0718,J03-1002,0,\N,Missing
W07-0718,P06-2003,0,\N,Missing
W07-0718,W07-0722,0,\N,Missing
W07-0718,2005.mtsummit-papers.11,1,\N,Missing
W07-0718,2006.iwslt-evaluation.1,0,\N,Missing
W07-0718,2003.mtsummit-papers.9,0,\N,Missing
W07-0718,W07-0726,0,\N,Missing
W07-0718,W07-0729,0,\N,Missing
W07-0732,2005.mtsummit-papers.28,1,0.773912,"Missing"
W07-0732,E06-1032,1,0.256679,"Missing"
W07-0732,W07-0728,0,0.527007,"Missing"
W07-0732,P07-2045,1,\N,Missing
W07-0733,W07-0702,1,0.569071,"LEU 25.11 25.88 26.69 27.46 27.12 27.30 27.64 Table 1: Results of domain adaptation experiments tion of words is extended to a vector of factors (e.g., surface form, lemma, POS, morphology). The mapping of an input phrase to an output phrase is decomposed into several translation and generation steps, each using a different translation or generation table, respectively. Such a decomposition is called a decoding path. A more recent feature of the factored translation model framework is the possible use of multiple alternative decoding paths. This alternate decoding path model was developed by Birch et al. (2007). For our purposes, we use two decoding paths, each consisting of only one translation step. One decoding path is the in-domain translation table, and the other decoding path is the out-of-domain translation table. Again, respective weights are set with minimum error rate training. 3 Domain adaptation results 4.1 WMT 2007 shared task submissions We participated in all categories. Given the four language pairs, with two translation directions and (ex226 Tuning Minimum error rate training is the most timeconsuming aspects of the training process. Due to time constraints, we did not carry out thi"
W07-0733,P05-1066,1,0.361294,"Missing"
W07-0733,2005.iwslt-1.8,1,0.307834,"he score for the translation e for an input sentence f: X score(e, f) = exp λi hi (e, f) (1) i The weights of the components λi are set by a discriminative training method on held-out development data (Och, 2003). The basic components used in our experiments are: (a) two phrase translation probabilities (both p(e|f ) and p(f |e)), (b) two word translation probabilities (both p(e|f ) and p(f |e)), (c) phrase count, (d) output word count, (e) language model, (f) distance-based reordering model, and (g) lexicalized reordering model. For a more detailed description of this model, please refer to (Koehn et al., 2005). 2 Domain adaption Since training data for statistical machine translation is typically collected opportunistically from wherever it is available, the application domain for a machine translation system may be very different from the domain of the system’s training data. For the WMT 2007 shared task, the challenge was to use a large amount of out-of-domain training data 224 Proceedings of the Second Workshop on Statistical Machine Translation, pages 224–227, c Prague, June 2007. 2007 Association for Computational Linguistics (about 40 million words) combined with a much smaller amount of in-d"
W07-0733,P07-2045,1,0.0609637,"eriment with various ways of adapting a statistical machine translation systems to a special domain (here: news commentary), when most of the training data is from a different domain (here: European Parliament speeches). This paper also gives a description of the submission of the University of Edinburgh to the shared task. 1 Figure 1: Phrase-based statistical machine translation model: Input is split into text chunks (phrases) which are mapped using a large phrase translation table. Phrases are mapped one-to-one, and may be reordered. Our framework: the Moses MT system The open source Moses (Koehn et al., 2007) MT system was originally developed at the University of Edinburgh and received a major boost through a 2007 Johns Hopkins workshop. It is now used at several academic institutions as the basic infrastructure for statistical machine translation research. The Moses system is an implementation of the phrase-based machine translation approach (Koehn et al., 2003). In this approach, an input sentence is first split into text chunks (so-called phrases), which are then mapped one-to-one to target phrases using a large phrase translation table. Phrases may be reordered, but typically a reordering lim"
W07-0733,N03-1017,1,0.0133741,"machine translation model: Input is split into text chunks (phrases) which are mapped using a large phrase translation table. Phrases are mapped one-to-one, and may be reordered. Our framework: the Moses MT system The open source Moses (Koehn et al., 2007) MT system was originally developed at the University of Edinburgh and received a major boost through a 2007 Johns Hopkins workshop. It is now used at several academic institutions as the basic infrastructure for statistical machine translation research. The Moses system is an implementation of the phrase-based machine translation approach (Koehn et al., 2003). In this approach, an input sentence is first split into text chunks (so-called phrases), which are then mapped one-to-one to target phrases using a large phrase translation table. Phrases may be reordered, but typically a reordering limit (in our experiments a maximum movement over 6 words) is used. See Figure 1 for an illustration. Phrase translation probabilities, reordering probabilities and language model probabilities are combined to give each possible sentence translation a score. The best-scoring translation is searched for by the decoding algorithm and outputted by the system as the"
W07-0733,P03-1021,0,0.0687323,"obabilities, reordering probabilities and language model probabilities are combined to give each possible sentence translation a score. The best-scoring translation is searched for by the decoding algorithm and outputted by the system as the best translation. The different system components hi (phrase translation probabilities, language model, etc.) are combined in a log-linear model to obtain the score for the translation e for an input sentence f: X score(e, f) = exp λi hi (e, f) (1) i The weights of the components λi are set by a discriminative training method on held-out development data (Och, 2003). The basic components used in our experiments are: (a) two phrase translation probabilities (both p(e|f ) and p(f |e)), (b) two word translation probabilities (both p(e|f ) and p(f |e)), (c) phrase count, (d) output word count, (e) language model, (f) distance-based reordering model, and (g) lexicalized reordering model. For a more detailed description of this model, please refer to (Koehn et al., 2005). 2 Domain adaption Since training data for statistical machine translation is typically collected opportunistically from wherever it is available, the application domain for a machine translat"
W08-0309,W08-0312,0,0.0979584,"MT 3 f r .717 .708 .706 .704 .702 .699 .699 .695 .678 .674 .661 .654 .652 .638 .637 .633 .628 .627 .624 .616 .615 .615 .612 SAAR f r SAAR - C de RBMT 4 de CUED es RBMT 3 de CMU - SMTes UCB es LIMSI es RBMT 6 de RBMT 5 de LIMSI de LIU de SAAR de CMU - STATXFR f r UMD cz BBN - COMBO de UEDIN de MORPHOLOGIC hu DCU cz UEDIN - COMBO de UEDIN cz CMU - STATXFER de UEDIN hu .584 .574 .573 .572 .552 .548 .547 .537 .509 .493 .469 .447 .445 .444 .429 .407 .402 .387 .380 .327 .293 .280 .188 some of the allowable variation in translation. We use a single reference translation in our experiments. • Meteor (Agarwal and Lavie, 2008)—Meteor measures precision and recall for unigrams and applies a fragmentation penalty. It uses flexible word matching based on stemming and WordNet-synonymy. A number of variants are investigated here: meteor-baseline and meteorranking are optimized for correlation with adequacy and ranking judgments respectively. mbleu and mter are Bleu and TER computed using the flexible matching used in Meteor. Table 7: The average number of times that each system was judged to be better than or equal to all other systems in the sentence ranking task for the All-English condition. The subscript indicates t"
W08-0309,P05-1038,0,0.00937556,"le judge the translations of those syntactic phrases. In order to draw judges’ attention to these regions, we highlighted the selected source phrases and the corresponding phrases in the translations. The corresponding phrases in the translations were located via automatic word alignments. Figure 2 illustrates how the source and reference phrases are highlighted via automatic word alignments. The same is done for sentence and each of the system translations. The English, French, German and Spanish test sets were automatically parsed using high quality parsers for those languages (Bikel, 2002; Arun and Keller, 2005; Dubey, 2005; Bick, 2006). The word alignments were created with Giza++ 76 (Och and Ney, 2003) applied to a parallel corpus containing the complete Europarl training data, plus sets of 4,051 sentence pairs created by pairing the test sentences with the reference translations, and the test sentences paired with each of the system translations. The phrases in the translations were located using standard phrase extraction techniques (Koehn et al., 2003). Because the word-alignments were created automatically, and because the phrase extraction is heuristic, the phrases that were selected may not"
W08-0309,W08-0314,0,0.0198808,"Missing"
W08-0309,W08-0321,0,0.0147788,"Missing"
W08-0309,W08-0316,0,0.0218948,"Missing"
W08-0309,W08-0319,0,0.0287247,"Missing"
W08-0309,E06-1032,1,0.371813,"ranslations are acceptable for each sentence in our test corpus. When we change our system and want to evaluate it, we do not need to manually evaluate those segments that match against the database, and could instead have people evaluate only those phrasal translations which are new. Accumulating these judgments over time would give a very reliable idea of what alternative translations were allowable. This would be useful because it could alleviate the problems associated with Bleu failing to recognize allowable variation in translation when multiple reference translations are not available (Callison-Burch et al., 2006). A large database of human judgments might also be useful as an objective function for minimum error rate training (Och, 2003) or in other system development tasks. 8 Conclusions Similar to previous editions of this workshop we carried out an extensive manual and automatic evaluation of machine translation performance for translating from European languages into English, and vice versa. One important aspect in which this year’s shared task differed from previous years was the introduction of an additional newswire test set that was different in nature to the training data. We 85 also added ne"
W08-0309,W07-0718,1,0.713541,"air that did not include English, but was not manually evaluated since it attracted minimal participation. • System combination – Saarland University entered a system combination over a number of rule-based MT systems, and provided their output, which were also treated as fully fledged entries in the manual evaluation. Three additional groups were invited to apply their system combination algorithms to all systems. Introduction This paper presents the results the shared tasks of the 2008 ACL Workshop on Statistical Machine Translation, which builds on two past workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007). There were two shared tasks this year: a translation task which evaluated translation between 10 pairs of European languages, and an evaluation task which examines automatic evaluation metrics. There were a number of differences between this year’s workshop and last year’s workshop: • Test set selection – Instead of creating our test set by reserving a portion of the training data, we instead hired translators to translate a set of • Refined manual evaluation – Because last year’s study indicated that fluency and adequacy judgments were slow and unreliable, we dropped them from manual evalua"
W08-0309,J96-2004,0,0.0556395,"roving the process of manual evaluation. 7.1 P (E) .333 .333 .333 .5 .5 K .367 .506 .517 .642 .649 Table 12: Kappa coefficient values representing the inter-annotator agreement for the different types of manual evaluation Table 11: The percent of time that each automatic metric was consistent with human judgments for translations into other languages 7 P (A) .578 .671 .678 .821 .825 Inter- and Intra-annotator agreement We measured pairwise agreement among annotators using the kappa coefficient (K) which is widely used in computational linguistics for measuring agreement in category judgments (Carletta, 1996). It is defined as P (A) − P (E) K= 1 − P (E) 83 Evaluation type Sentence ranking Constituent ranking Constituent (w/identicals) Yes/No judgments Yes/No (w/identicals) P (A) .691 .825 .832 .928 .930 P (E) .333 .333 .333 .5 .5 K .537 .737 .748 .855 .861 Table 13: Kappa coefficient values for intraannotator agreement for the different types of manual evaluation where P (A) is the proportion of times that the annotators agree, and P (E) is the proportion of time that they would agree by chance. We define chance agreement for ranking tasks as 13 since there are three possible outcomes when ranking"
W08-0309,W08-0310,0,0.0252447,"Missing"
W08-0309,P05-1039,0,0.00794095,"ns of those syntactic phrases. In order to draw judges’ attention to these regions, we highlighted the selected source phrases and the corresponding phrases in the translations. The corresponding phrases in the translations were located via automatic word alignments. Figure 2 illustrates how the source and reference phrases are highlighted via automatic word alignments. The same is done for sentence and each of the system translations. The English, French, German and Spanish test sets were automatically parsed using high quality parsers for those languages (Bikel, 2002; Arun and Keller, 2005; Dubey, 2005; Bick, 2006). The word alignments were created with Giza++ 76 (Och and Ney, 2003) applied to a parallel corpus containing the complete Europarl training data, plus sets of 4,051 sentence pairs created by pairing the test sentences with the reference translations, and the test sentences paired with each of the system translations. The phrases in the translations were located using standard phrase extraction techniques (Koehn et al., 2003). Because the word-alignments were created automatically, and because the phrase extraction is heuristic, the phrases that were selected may not exactly corre"
W08-0309,W08-0327,1,0.775819,"Missing"
W08-0309,W08-0331,0,0.117642,"asure) by matching part of speech 4grams in a hypothesis translation against the reference translation. In addition to the above metrics, which scored the translations on both the system-level5 and the sentence-level, there were a number of metrics which focused on the sentence-level: • Albrecht and Hwa (2008) use support vector regression to score translations using past WMT manual assessment data as training examples. The metric uses features derived from targetside language models and machine-generated translations (svm-pseudo-ref) as well as reference human translations (svm-human-ref). • Duh (2008) similarly used support vector machines to predict an ordering over a set of 5 We provide the scores assigned to each system by these metrics in Appendix A. Measuring system-level correlation To measure the correlation of the automatic metrics with the human judgments of translation quality at the system-level we used Spearman’s rank correlation coefficient ρ. We converted the raw scores assigned each system into ranks. We assigned a ranking to the systems for each of the three types of manual evaluation based on: • The percent of time that the sentences it produced were judged to be better th"
W08-0309,W07-0729,0,0.0315049,"Missing"
W08-0309,W08-0328,0,0.0942343,"s submitted to shared translation task. We designated the translations of the Europarl set as the development data for combination techniques which weight each system.3 CMU combined the French-English systems, BBN combined the French-English and German-English systems, and Edinburgh submitted combinations for the French-English and GermanEnglish systems as well as a multi-source system combination which combined all systems which translated from any language pair into English for the News test set. The University of Saarland also produced a system combination over six commercial RBMT systems (Eisele et al., 2008). Saarland graciously provided the output of these systems, which we manually evaluated alongside all other entries. For more on the participating systems, please refer to the respective system descriptions in the proceedings of the workshop. 3 Human evaluation As with last year’s workshop, we placed greater emphasis on the human evaluation than on the automatic evaluation metric scores. It is our contention that automatic measures are an imperfect substitute for human assessment of translation quality. Therefore, rather than select an official automatic evaluation metric like the NIST Machine"
W08-0309,W08-0332,0,0.47939,"ning how well various automatic metrics correlate with human judgments. In addition to examining how well the automatic evaluation metrics predict human judgments at the system-level, this year we have also started to measure their ability to predict sentence-level judgments. The automatic metrics that were evaluated in this year’s shared task were the following: • Bleu (Papineni et al., 2002)—Bleu remains the de facto standard in machine translation evaluation. It calculates n-gram precision and a brevity penalty, and can make use of multiple reference translations as a way of capturing 80 • Gimenez and Marquez (2008) measure overlapping grammatical dependency relationships (DP), semantic roles (SR), and discourse representations (DR). The authors further investigate combining these with other metrics including TER, Bleu, GTM, Rouge, and Meteor (ULC and ULCh). • Popovic and Ney (2007) automatically evaluate translation quality by examining sequences of parts of speech, rather than words. They calculate Bleu (posbleu) and F-measure (pos4gramFmeasure) by matching part of speech 4grams in a hypothesis translation against the reference translation. In addition to the above metrics, which scored the translation"
W08-0309,W08-0302,0,0.0296793,"Missing"
W08-0309,P05-3026,0,0.0948555,"Missing"
W08-0309,W08-0315,0,0.0199873,"Missing"
W08-0309,W06-3114,1,0.562417,"s our first language pair that did not include English, but was not manually evaluated since it attracted minimal participation. • System combination – Saarland University entered a system combination over a number of rule-based MT systems, and provided their output, which were also treated as fully fledged entries in the manual evaluation. Three additional groups were invited to apply their system combination algorithms to all systems. Introduction This paper presents the results the shared tasks of the 2008 ACL Workshop on Statistical Machine Translation, which builds on two past workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007). There were two shared tasks this year: a translation task which evaluated translation between 10 pairs of European languages, and an evaluation task which examines automatic evaluation metrics. There were a number of differences between this year’s workshop and last year’s workshop: • Test set selection – Instead of creating our test set by reserving a portion of the training data, we instead hired translators to translate a set of • Refined manual evaluation – Because last year’s study indicated that fluency and adequacy judgments were slow and unreliable, we d"
W08-0309,W07-0733,1,0.572842,"a Europarl test set. The Europarl test data was again drawn from the transcripts of EU parliamentary proceedings from the fourth quarter of 2000, which is excluded from the Europarl training data. Our rationale behind investing a considerable sum to create the News test set was that we believe that it more accurately represents the quality of systems’ translations than when we simply hold out a portion of the training data as the test set, as with the Europarl set. For instance, statistical systems are heavily optimized to their training data, and do not perform as well on out-of-domain data (Koehn and Schroeder, 2007). Having both the News test set and the Europarl test set allows us to contrast the performance of systems on in-domain and out-of-domain data, and provides a fairer comparison between systems trained on the Europarl corpus and systems that were developed without it. 2.2 Provided materials To lower the barrier of entry for newcomers to the field, we provided a complete baseline MT system, along with data resources. We provided: • • • • sentence-aligned training corpora language model data development and dev-test sets Moses open source toolkit for phrase-based statistical translation (Koehn et"
W08-0309,N03-1017,1,0.00762077,"ions. The English, French, German and Spanish test sets were automatically parsed using high quality parsers for those languages (Bikel, 2002; Arun and Keller, 2005; Dubey, 2005; Bick, 2006). The word alignments were created with Giza++ 76 (Och and Ney, 2003) applied to a parallel corpus containing the complete Europarl training data, plus sets of 4,051 sentence pairs created by pairing the test sentences with the reference translations, and the test sentences paired with each of the system translations. The phrases in the translations were located using standard phrase extraction techniques (Koehn et al., 2003). Because the word-alignments were created automatically, and because the phrase extraction is heuristic, the phrases that were selected may not exactly correspond to the translations of the selected source phrase. We noted this in the instructions to judges: Rank each constituent translation from Best to Worst relative to the other choices (ties are allowed). Grade only the highlighted part of each translation. Please note that segments are selected automatically, and they should be taken as an approximate guide. They might include extra words that are not in the actual alignment, or miss wor"
W08-0309,W08-0329,0,0.0334792,"Missing"
W08-0309,W08-0320,0,0.0114639,"Missing"
W08-0309,W08-0313,0,0.036043,"Missing"
W08-0309,W08-0323,0,0.0290179,"Missing"
W08-0309,W08-0317,0,0.0278407,"Missing"
W08-0309,W08-0311,0,0.034369,"Missing"
W08-0309,W08-0326,0,0.025716,"Missing"
W08-0309,J03-1002,0,0.00405346,"regions, we highlighted the selected source phrases and the corresponding phrases in the translations. The corresponding phrases in the translations were located via automatic word alignments. Figure 2 illustrates how the source and reference phrases are highlighted via automatic word alignments. The same is done for sentence and each of the system translations. The English, French, German and Spanish test sets were automatically parsed using high quality parsers for those languages (Bikel, 2002; Arun and Keller, 2005; Dubey, 2005; Bick, 2006). The word alignments were created with Giza++ 76 (Och and Ney, 2003) applied to a parallel corpus containing the complete Europarl training data, plus sets of 4,051 sentence pairs created by pairing the test sentences with the reference translations, and the test sentences paired with each of the system translations. The phrases in the translations were located using standard phrase extraction techniques (Koehn et al., 2003). Because the word-alignments were created automatically, and because the phrase extraction is heuristic, the phrases that were selected may not exactly correspond to the translations of the selected source phrase. We noted this in the inst"
W08-0309,P03-1021,0,0.0375741,"aluate those segments that match against the database, and could instead have people evaluate only those phrasal translations which are new. Accumulating these judgments over time would give a very reliable idea of what alternative translations were allowable. This would be useful because it could alleviate the problems associated with Bleu failing to recognize allowable variation in translation when multiple reference translations are not available (Callison-Burch et al., 2006). A large database of human judgments might also be useful as an objective function for minimum error rate training (Och, 2003) or in other system development tasks. 8 Conclusions Similar to previous editions of this workshop we carried out an extensive manual and automatic evaluation of machine translation performance for translating from European languages into English, and vice versa. One important aspect in which this year’s shared task differed from previous years was the introduction of an additional newswire test set that was different in nature to the training data. We 85 also added new language pairs to our evaluation: Hungarian-English and German-Spanish. As in previous years we were pleased to notice an inc"
W08-0309,P02-1040,0,0.12021,"nt systems. In particular, it is especially useful for validating the automatic metrics which are frequently used by the machine translation research community. We continued the shared task which we debuted last year, by examining how well various automatic metrics correlate with human judgments. In addition to examining how well the automatic evaluation metrics predict human judgments at the system-level, this year we have also started to measure their ability to predict sentence-level judgments. The automatic metrics that were evaluated in this year’s shared task were the following: • Bleu (Papineni et al., 2002)—Bleu remains the de facto standard in machine translation evaluation. It calculates n-gram precision and a brevity penalty, and can make use of multiple reference translations as a way of capturing 80 • Gimenez and Marquez (2008) measure overlapping grammatical dependency relationships (DP), semantic roles (SR), and discourse representations (DR). The authors further investigate combining these with other metrics including TER, Bleu, GTM, Rouge, and Meteor (ULC and ULCh). • Popovic and Ney (2007) automatically evaluate translation quality by examining sequences of parts of speech, rather than"
W08-0309,W07-0707,0,0.0613323,"The automatic metrics that were evaluated in this year’s shared task were the following: • Bleu (Papineni et al., 2002)—Bleu remains the de facto standard in machine translation evaluation. It calculates n-gram precision and a brevity penalty, and can make use of multiple reference translations as a way of capturing 80 • Gimenez and Marquez (2008) measure overlapping grammatical dependency relationships (DP), semantic roles (SR), and discourse representations (DR). The authors further investigate combining these with other metrics including TER, Bleu, GTM, Rouge, and Meteor (ULC and ULCh). • Popovic and Ney (2007) automatically evaluate translation quality by examining sequences of parts of speech, rather than words. They calculate Bleu (posbleu) and F-measure (pos4gramFmeasure) by matching part of speech 4grams in a hypothesis translation against the reference translation. In addition to the above metrics, which scored the translations on both the system-level5 and the sentence-level, there were a number of metrics which focused on the sentence-level: • Albrecht and Hwa (2008) use support vector regression to score translations using past WMT manual assessment data as training examples. The metric use"
W08-0309,P07-1040,0,0.0268048,"at indicated which language the system was originally translating from. This entry was part of ongoing research in multi-lingual, multisource translation. Since there was no official multilingual system combination track, this entry should be viewed only as a contrastive data point. Table 4: Summary results for the sentence ranking judgments. The numbers report the percent of time that each system was judged to be greater than or equal to any other system. Bold indicates the highest score for that task. was the University of Edinburgh’s system combination entry. It uses a technique similar to Rosti et al. (2007) to perform system combination. Like the other system combination entrants, it was tuned on the Europarl test set and tested on the News test set, using systems that submitted entries to both tasks. BBN - COMBO CMU - COMBO CMU - GIMPEL CMU - SMT CMU - STATXFER CU - BOJAR CU - TECTOMT CUED CUED - CONTR DCU LIMSI LIU LIUM - SYSTRAN LIUM - SYS - CONTR MORPHOLOGIC PC - TRANSLATOR RBMT 2 RBMT 3 RBMT 4 RBMT 5 RBMT 6 SAAR SAAR - CONTR SYSTRAN UCB UCL UEDIN UEDIN - COMBO UMD UPC UW XEROX Czech-English Commentary Czech-English News English-Czech Commentary English-Czech News English-French Europarl Eng"
W08-0309,2004.tmi-1.8,0,\N,Missing
W08-0309,W08-0322,0,\N,Missing
W08-0309,W08-0318,1,\N,Missing
W08-0309,W08-0324,0,\N,Missing
W08-0309,W08-0325,0,\N,Missing
W08-0309,P07-1111,0,\N,Missing
W08-0309,P07-1038,0,\N,Missing
W08-0309,W08-0330,0,\N,Missing
W08-0309,2005.eamt-1.20,0,\N,Missing
W08-0309,D08-1076,0,\N,Missing
W08-0318,J95-4004,0,0.00951339,"Missing"
W08-0318,P05-1066,1,0.0612374,"eat-punctuation reordering constraint (MP). Scores show higher gains for out-of-domain news test sets (+0.46) than for in-domain Europarl sets (+0.08). 3 German–English Translating between German and English is surprisingly difficult, given that the languages are closely related. The main sources for this difficulty is the different syntactic structure at the clause level and the rich German morphology, including the merging of noun compounds. In prior work, we addressed reordering with a pre-order model that transforms German for training and testing according to a set of hand-crafted rules (Collins et al., 2005). Employing this method to our baseline system leads to an improvement of +0.8 BLEU on the nc-test2007 set and +0.5 BLEU on the test2007 set. German–English baseline tokenize hyphens tok. hyph. + truecase nc-test2007 20.3 20.1 (–0.2) 20.7 (+0.4) test2007 27.6 27.6 (±0.0) 27.8 (+0.2) Table 2: Impact of truecasing on case-sensitive BLEU In a more integrated approach, factored translation models (Koehn and Hoang, 2007) allow us to consider grammatical coherence in form of partof-speech language models. When translating into output words, we also generate a part-of-speech tag along with each outpu"
W08-0318,D07-1091,1,0.294184,"Missing"
W08-0318,P07-2045,1,0.0194146,"Missing"
W08-0318,E03-1076,1,0.639975,"Missing"
W08-0318,N03-1017,1,0.00821203,"Missing"
W08-0318,N04-1022,0,0.0216904,"est set (news-2008), for which we have no in-domain training data. This may have resulted in lower performance due to less (and very relevant) training data, but it also allowed us to optimize for a true out-of-domain test set. The baseline training uses Moses default parameters. We use a maximum sentence length of 80, a phrase translation table with the five traditional features, lexicalized reordering, and lowercase training and test data. All reported BLEU scores are not casesensitive, computed using the NIST tool. 2.1 Minimum Bayes Risk Decoding Minimum Bayes risk decoding was proposed by Kumar and Byrne (2004). Instead of selecting the translation with the highest probability, minimum Bayes risk decoding selects the translation that is most similar to the highest scoring translations. Intuitively, this avoid the selection of an outlier as the best translation, since the decision rule prefers translations that are similar to other high-scoring translations. Minimum Bayes risk decoding is defined as: X eMBR = argmaxe L(e, e0 ) p(e0 |f) e0 As similarity function L, we use sentence-level with add-one smoothing. As highest scoring translations, we consider the top 100 distinct translations, for which we"
W08-0318,C00-2105,0,0.0371065,"Missing"
W08-0318,J03-1005,0,0.0110307,"boring phrase translations. Allowing any kind of reordering typically reduces translation performance, so reordering is limited to a window of (in our case) six words. One noticeable weakness is that the current model frequently reorders words beyond clause boundaries, which is almost never well-motivated, and leads to confusing translations. Since clause boundaries are often indicated by punctuation such as comma, colon, or semicolon, it is straight-forward to introduce a reordering constraint that addresses this problem. Our implementation of a monotone-at-punctuation reordering constraint (Tillmann and Ney, 2003) requires that all input words before clauseseparating punctuation have be translated, before words afterwards are covered. Note that this con140 straint does not limit in any way phrase translations that span punctuation. 2.3 Results Table 1 summarizes the impact of minimum Bayes risk decoding (MBR) and the monotoneat-punctuation reordering constraint (MP). Scores show higher gains for out-of-domain news test sets (+0.46) than for in-domain Europarl sets (+0.08). 3 German–English Translating between German and English is surprisingly difficult, given that the languages are closely related. Th"
W08-0327,E06-1032,1,0.868211,"Missing"
W08-0327,W07-0728,0,0.0519587,"Missing"
W08-0327,C08-1115,1,\N,Missing
W08-0327,W08-0313,1,\N,Missing
W08-0327,W07-0733,1,\N,Missing
W08-0327,N07-1064,0,\N,Missing
W08-0327,W07-0732,1,\N,Missing
W08-0510,N03-2002,0,0.0139746,"paper. 58 Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pages 58–65, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics As an indication of the take-up of the Moses toolkit, out of over 20 competing teams at the recent IWSLT 2007 conference1, half used Moses. As an indication of the extensibility of the decoder, there are currently four language model implementations which has been integrated with the decoder by various researchers. In addition, the framework exists to integrate language models, such as those described in (Bilmes and Kirchhoff 2003), which takes advantage of the factored representation within Moses. It is noted that Mood/Ramses also supports multiple LM implementations, an internally developed language model, in additional to SRILM, to overcome the latter’s licensing restrictions. In addition, there are two built-in phrase table implementations, one which loads all data into memory for fast decoding, and a binary phrase table as described in (Zens and Ney 2007) which loads on demand to conserve memory usage. The Moses decoder has the ability to accept simple sentence input, confusion network or lattice networks, in commo"
W08-0510,J90-2002,0,0.58913,"Missing"
W08-0510,koen-2004-pharaoh,0,0.104156,"n the input sentence are mapped to contiguous segments of words in the output sentence. In SMT, we are given a source language sentence, s, which is to be translated into a target language sentence, t. The goal of machine translation is to find the translation, tˆ , which is defined as: tˆ = arg max t p(t |s ) where p (t |s ) is the probability model. The argmax implies a search for the best translation tˆ in the There have been numerous implementations of phrase-based decoders for SMT prior to our work. Early systems such as the Alignment Template System (ATS) (Och and Ney 2004) and Pharaoh (Koehn 2004) were widely used and accepted by the research community. ATS is perhaps the crossover system, in that word classes were translated as phrases but the surface words were translated word by word. Pharaoh substituted the word classes with surface words, thereby discarding the use of word classes in decoding altogether. There has been other phrase-based decoders such as PORTAGE (Sadat et al. 2005), Phramer (Olteanu et al. 2006), the MITLL/AFRL system (Shen et al. 2005), ITC-irst (Bertoldi et al. 2004), Ramses/Mood (Patry et al. 2006) to name but a few. Other researchers such as (Kumar and Byrne 2"
W08-0510,D07-1091,1,0.594524,"words, factored translation models augments different factors, such as POS tags or lemma, into source and target sentences to improve translation. This transforms the representation of a word from a string to a vector of strings, and a phrase or sentence from a sequence of words to a sequence of vectors. Such a change to the basic data structure of a decoder propagated throughout the rest of the system, therefore, it was simpler to build the Moses decoder from scratch rather than extend an existing decoder such as Pharaoh. Some research into factored machine translation has been published by (Koehn and Hoang 2007). 3.3 Flexibility Flexibility is an important software design goal which will enable researchers to extend the use of the Moses decoders to tasks that were not originally envisioned. Following (Fabri et al. 2000), we identify four sub-issues which affects flexibility: i. Modularity 3 60 Peak memory usage 46MB 154MB 239MB http://www.statmt.org/wmt07/shared-task.html ii. iii. iv. 3.4 Adaptability Extensibility Openness Modularity Firstly, software modularity enables developers to work on one component of the decoder without affecting other components. A modular design reduces the learning curve"
W08-0510,N03-1019,0,0.0142272,"raoh (Koehn 2004) were widely used and accepted by the research community. ATS is perhaps the crossover system, in that word classes were translated as phrases but the surface words were translated word by word. Pharaoh substituted the word classes with surface words, thereby discarding the use of word classes in decoding altogether. There has been other phrase-based decoders such as PORTAGE (Sadat et al. 2005), Phramer (Olteanu et al. 2006), the MITLL/AFRL system (Shen et al. 2005), ITC-irst (Bertoldi et al. 2004), Ramses/Mood (Patry et al. 2006) to name but a few. Other researchers such as (Kumar and Byrne 2003) have also used weighted finite state transducers but they have more difficulty modeling reordering. Many early systems came with restrictive licenses; ATS has never been publicly released, Pharaoh was released in 2003 as a pre-compiled binary with documentation. This severely limited the extent to which other researchers can study and enhance the decoder. Without access to the decoder source code research was generally restricted to altering the input, augmenting it with extra information, or modifying the output or re-ranking the n-best list output. The main contribution of this paper is to"
W08-0510,P06-1096,0,0.0204745,"Missing"
W08-0510,P03-1021,0,0.00252733,"ering principles into NLP. In contrast to the ‘abandonware’ status of GIZA++, both CGAL and DCMTK are still being developed. 4 Supporting Infrastructure Other factors have contributed to the wide adoption of Moses. 4.1 ‘One-Stop Shop’ for Phrase-Based SMT The Moses project encompasses the decoder and many of the other components necessary to create a translation system which were previously available separately. These include scripts for creating alignments from a parallel corpus, creating phrase tables and language models, binarizing phrase tables, scripts for weight optimization using MERT (Och 2003), and testing scripts. Steps such as MERT and testing which are CPU intensive have been re-engineered to run in parallel using Sun Grid Engine. All scripts have also been extended for factored translation. 4.2 Ongoing support We assist in the adoption of Moses by offering ongoing support to users and developers through the support mailing list 5 . Questions relating to Moses, phrase-based translation or machine translation in general are often asked, and usually answered. The archived emails are publicly available and searchable, and have become an important knowledge source for the community."
W08-0510,J03-1002,0,0.0019265,"ation, we can only measure its performance relative to previous versions and other similar decoders. These differences are minor compared to the similarities Moses has to CGAL and DCMTK, and indeed, to any well developed software project. Design goals such as robustness, flexibility, ease of use and efficiency are commonality that we share and which we will discuss in more detail in the next section. As a contrast to CGAL and DCMTK whose design we would like to emulate, we also looked at a project within the NLP field which contains certain aspect in the design we would like to avoid. GIZA++ (Och and Ney 2003) is a very popular system within SMT for creating word alignment from parallel corpus, in fact, the Moses training scripts uses it. The system was release under the GPL open source license. However, its lack of 2 http://iwslt07.oitc.it/menu/program.html 59 http://cordis.europa.eu/esprit/src/21957.htm clear design, documentation and obscure coding style makes it difficult for other researcher to contribute or extend the system. For a long time, it couldn’t even be compiled on modern GCC compilers. Other systems which seeks to improve word alignment and segmentation, such as MTTK (Deng et al. 20"
W08-0510,J04-4002,0,0.0294973,"contiguous segments of words in the input sentence are mapped to contiguous segments of words in the output sentence. In SMT, we are given a source language sentence, s, which is to be translated into a target language sentence, t. The goal of machine translation is to find the translation, tˆ , which is defined as: tˆ = arg max t p(t |s ) where p (t |s ) is the probability model. The argmax implies a search for the best translation tˆ in the There have been numerous implementations of phrase-based decoders for SMT prior to our work. Early systems such as the Alignment Template System (ATS) (Och and Ney 2004) and Pharaoh (Koehn 2004) were widely used and accepted by the research community. ATS is perhaps the crossover system, in that word classes were translated as phrases but the surface words were translated word by word. Pharaoh substituted the word classes with surface words, thereby discarding the use of word classes in decoding altogether. There has been other phrase-based decoders such as PORTAGE (Sadat et al. 2005), Phramer (Olteanu et al. 2006), the MITLL/AFRL system (Shen et al. 2005), ITC-irst (Bertoldi et al. 2004), Ramses/Mood (Patry et al. 2006) to name but a few. Other researchers s"
W08-0510,W06-3121,0,0.035851,"Missing"
W08-0510,W06-3116,0,0.0536002,"Missing"
W08-0510,W05-0822,0,0.0418257,"Missing"
W08-0510,N07-2047,0,0.0209568,"Missing"
W08-0510,N07-1062,0,0.0141681,"h has been integrated with the decoder by various researchers. In addition, the framework exists to integrate language models, such as those described in (Bilmes and Kirchhoff 2003), which takes advantage of the factored representation within Moses. It is noted that Mood/Ramses also supports multiple LM implementations, an internally developed language model, in additional to SRILM, to overcome the latter’s licensing restrictions. In addition, there are two built-in phrase table implementations, one which loads all data into memory for fast decoding, and a binary phrase table as described in (Zens and Ney 2007) which loads on demand to conserve memory usage. The Moses decoder has the ability to accept simple sentence input, confusion network or lattice networks, in common with SMT decoders such as the MITLL/AFRL or ITC-irst systems. The decoder also produces diverse types of output, ranging from 1-best, n-best lists and word lattices. 2 Comparison with other projects The Moses decoder is designed within a strict modular and object-oriented framework for easy maintainability and extensibility. In designing the decoder, we modeled the software design methodology and aims on some research-oriented soft"
W08-0510,2004.iwslt-evaluation.8,0,\N,Missing
W08-0510,D07-1049,0,\N,Missing
W08-0510,D08-1076,0,\N,Missing
W08-0510,2007.iwslt-1.12,0,\N,Missing
W08-0510,P07-2045,1,\N,Missing
W08-0510,P05-1033,0,\N,Missing
W09-0401,W08-0312,0,0.0796143,"calculates n-gram precision and a brevity penalty, and can make use of multiple reference translations as a way of capturing some of the allowable variation in translation. We use a single reference translation in our experiments. • ULC (Gim´enez and M`arquez, 2008)—ULC is an arithmetic mean over other automatic metrics. The set of metrics used include Rouge, Meteor, measures of overlap between constituent parses, dependency parses, semantic roles, and discourse representations. The ULC metric had the strongest correlation with human judgments in WMT08 (CallisonBurch et al., 2008). • Meteor (Agarwal and Lavie, 2008)—Meteor measures precision and recall for unigrams and applies a fragmentation penalty. It uses flexible word matching based on stemming and WordNet-synonymy. meteor-ranking is optimized for correlation with ranking judgments. • Translation Error Rate (Snover et al., 2006)—TER calculates the number of edits required to change a hypothesis translation into a reference translation. The possible edits in TER include insertion, deletion, and substitution of single words, and an edit which moves sequences of contiguous words. Two variants of TER are also included: TERp (Snover et al., 2009), a new"
W09-0401,1998.amta-tutorials.1,0,0.0723526,"Missing"
W09-0401,W09-0426,0,0.0294171,"Missing"
W09-0401,P07-1038,0,0.0331701,"Missing"
W09-0401,W09-0417,0,0.033793,"Missing"
W09-0401,W09-0411,0,0.034886,"Missing"
W09-0401,W07-0718,1,0.791479,"Missing"
W09-0401,W09-0420,0,0.0243178,"Missing"
W09-0401,W08-0309,1,0.578575,"est sets and public training data with published performance numbers, and to refine evaluation methodologies for machine translation. All of the data, translations, and human judgments produced for our workshop are publicly available.1 We hope they form a valuable resource for research into statistical machine translation, system combination, and automatic evaluation of translation quality. This paper presents the results of the shared tasks of the 2009 EACL Workshop on Statistical Machine Translation, which builds on three previous workshops (Koehn and Monz, 2006; CallisonBurch et al., 2007; Callison-Burch et al., 2008). There were three shared tasks this year: a translation task between English and five other European languages, a task to combine the output of multiple machine translation systems, and a task to predict human judgments of translation quality using automatic evaluation metrics. The performance on each of these shared task was determined after a comprehensive human evaluation. There were a number of differences between this year’s workshop and last year’s workshop: 2 Overview of the shared translation and system combination tasks The workshop examined translation between English and five other"
W09-0401,W09-0408,0,0.0416758,"Missing"
W09-0401,W09-0416,0,0.030911,"Missing"
W09-0401,W09-0406,0,0.0789703,"Missing"
W09-0401,W09-0419,1,0.738361,"Missing"
W09-0401,W09-0413,0,0.0338062,"Missing"
W09-0401,W09-0428,0,0.0340245,"Missing"
W09-0401,2004.tmi-1.8,0,0.00591015,"score to the higher ranked system). We divided this by the total number of pairwise comparisons to get a percentage. Because the systems generally assign real numbers as scores, we excluded pairs that the human annotators ranked as ties. Average terp ter bleusp4114 bleusp bleu bleu (cased) bleu-ter/2 wcd6p4er nist (cased) nist wpF wpbleu en-cz (5 systems) Because the sentence-level judgments collected in the manual evaluation are relative judgments rather than absolute judgments, it is not possible for us to measure correlation at the sentencelevel in the same way that previous work has done (Kulesza and Shieber, 2004; Albrecht and Hwa, 2007a; Albrecht and Hwa, 2007b). en-es (11 systems) Measuring sentence-level consistency en-fr (16 systems) 5.2 Average where di is the difference between the rank for systemi and n is the number of systems. The possible values of ρ range between 1 (where all systems are ranked in the same order) and −1 (where the systems are ranked in the reverse order). Thus an automatic evaluation metric with a higher absolute value for ρ is making predictions that are more similar to the human judgments than an automatic evaluation metric with a lower absolute ρ. hu-en (6 systems) 6 d2i"
W09-0401,W09-0404,0,0.0128365,"-S (Lin and Och, 2004), with tuned n-gram weights, and bleusp, with constant weights. wcd6p4er is an error measure and bleusp is a quality score. In addition to allowing us to analyze the translation quality of different systems, the data gathered during the manual evaluation is useful for validating the automatic evaluation metrics. Last year, NIST began running a similar “Metrics for MAchine TRanslation” challenge (MetricsMATR), and presented their findings at a workshop at AMTA (Przybocki et al., 2008). In this year’s shared task we evaluated a number of different automatic metrics: • RTE (Pado et al., 2009)—The RTE metric follows a semantic approach which applies recent work in rich textual entailment to the problem of MT evaluation. Its predictions are based on a regression model over a feature set adapted from an entailment systems. The features primarily model alignment quality and (mis-)matches of syntactic and semantic structures. • Bleu (Papineni et al., 2002)—Bleu remains the de facto standard in machine translation evaluation. It calculates n-gram precision and a brevity penalty, and can make use of multiple reference translations as a way of capturing some of the allowable variation in"
W09-0401,P02-1040,0,0.123715,"ing a similar “Metrics for MAchine TRanslation” challenge (MetricsMATR), and presented their findings at a workshop at AMTA (Przybocki et al., 2008). In this year’s shared task we evaluated a number of different automatic metrics: • RTE (Pado et al., 2009)—The RTE metric follows a semantic approach which applies recent work in rich textual entailment to the problem of MT evaluation. Its predictions are based on a regression model over a feature set adapted from an entailment systems. The features primarily model alignment quality and (mis-)matches of syntactic and semantic structures. • Bleu (Papineni et al., 2002)—Bleu remains the de facto standard in machine translation evaluation. It calculates n-gram precision and a brevity penalty, and can make use of multiple reference translations as a way of capturing some of the allowable variation in translation. We use a single reference translation in our experiments. • ULC (Gim´enez and M`arquez, 2008)—ULC is an arithmetic mean over other automatic metrics. The set of metrics used include Rouge, Meteor, measures of overlap between constituent parses, dependency parses, semantic roles, and discourse representations. The ULC metric had the strongest correlati"
W09-0401,W09-0407,0,0.0551118,"Missing"
W09-0401,W09-0418,0,0.024243,"Missing"
W09-0401,W09-0424,1,0.0806642,"Missing"
W09-0401,W09-0402,0,0.0228572,"dNet-synonymy. meteor-ranking is optimized for correlation with ranking judgments. • Translation Error Rate (Snover et al., 2006)—TER calculates the number of edits required to change a hypothesis translation into a reference translation. The possible edits in TER include insertion, deletion, and substitution of single words, and an edit which moves sequences of contiguous words. Two variants of TER are also included: TERp (Snover et al., 2009), a new version which introduces a number of different features, and (Bleu − TER)/2, a combination of Bleu and Translation Edit Rate. • wpF and wpBleu (Popovic and Ney, 2009) These metrics are based on words and part of speech sequences. wpF is an n-gram based Fmeasure which takes into account both word n-grams and part of speech n-grams. wpBLEU is a combnination of the normal Blue score and a part of speech-based Bleu score. • SemPOS (Kos and Bojar, 2009) – the SemPOS metric computes overlapping words, as defined in (Gim´enez and M`arquez, 2007), with respect to their semantic part of speech. Moreover, it does not use the surface representation of words but their underlying forms obtained from the TectoMT framework. • MaxSim (Chan and Ng, 2008)—MaxSim calculates"
W09-0401,E09-1082,1,0.692929,"ing the secondary system submissions. Baseline system To lower the barrier of entry for newcomers to the field, we provided Moses, an open source toolkit for phrase-based statistical translation (Koehn et al., 2007). The performance of this baseline system is similar to the best submissions in last year’s shared task. Twelve participating groups used the Moses toolkit for the development of their system. 2.4 System combination In addition to soliciting system combination entries for each of the language pairs, we treated system combination as a way of doing multi-source translation, following Schroeder et al. (2009). For the multi-source system combination task, we provided all 46 primary system submissions from any language into English, along with an additional 32 secondary systems. Table 2 lists the six participants in the system combination task. Submitted systems We received submissions from 22 groups from 20 institutions, as listed in Table 1, a similar turnout to last year’s shared task. Of the 20 groups that participated with regular system submissions in last year’s shared task, 12 groups returned this year. A major hurdle for many was a DARPA/GALE evaluation that occurred at the same time as th"
W09-0401,W09-0423,0,0.0383252,"Missing"
W09-0401,2006.amta-papers.25,0,0.223011,"Missing"
W09-0401,W09-0441,0,0.0797267,"or (Agarwal and Lavie, 2008)—Meteor measures precision and recall for unigrams and applies a fragmentation penalty. It uses flexible word matching based on stemming and WordNet-synonymy. meteor-ranking is optimized for correlation with ranking judgments. • Translation Error Rate (Snover et al., 2006)—TER calculates the number of edits required to change a hypothesis translation into a reference translation. The possible edits in TER include insertion, deletion, and substitution of single words, and an edit which moves sequences of contiguous words. Two variants of TER are also included: TERp (Snover et al., 2009), a new version which introduces a number of different features, and (Bleu − TER)/2, a combination of Bleu and Translation Edit Rate. • wpF and wpBleu (Popovic and Ney, 2009) These metrics are based on words and part of speech sequences. wpF is an n-gram based Fmeasure which takes into account both word n-grams and part of speech n-grams. wpBLEU is a combnination of the normal Blue score and a part of speech-based Bleu score. • SemPOS (Kos and Bojar, 2009) – the SemPOS metric computes overlapping words, as defined in (Gim´enez and M`arquez, 2007), with respect to their semantic part of speech."
W09-0401,D07-1049,0,0.0154831,"systems evaluated in this workshop, since they were trained using only the provided materials. In addition to cleaning the sentence-aligned parallel corpus we also de-duplicated the corpus, removing all sentence pairs that occured more than once in the parallel corpus. Many of the documents gathered in our web crawl were duplicates or near duplicates, and a lot of the text is repeated, as with web site navigation. We further eliminated sentence pairs that varied from previous sentences by only numbers, which helped eliminate template web pages such as expense reports. We used a Bloom Filter (Talbot and Osborne, 2007) to do de-duplication, so it may have discarded more sentence pairs than strictly necessary. After deduplication, the parallel corpus contained 28 million sentence pairs with 0.8 billion French words and 0.7 billion English words. 2.5 In total, we received 87 primary system submissions along with 42 secondary submissions. These were made available to participants in the system combination shared task. Based on feedback that we received on last year’s system combination task, we provided two additional resources to participants: Monolingual news corpora We have crawled the news sources that wer"
W09-0401,moore-2002-fast,0,\N,Missing
W09-0401,W09-0422,0,\N,Missing
W09-0401,W09-0421,0,\N,Missing
W09-0401,W07-0738,0,\N,Missing
W09-0401,W09-0427,0,\N,Missing
W09-0401,W08-0332,0,\N,Missing
W09-0401,W09-0425,0,\N,Missing
W09-0401,W09-0429,1,\N,Missing
W09-0401,P04-1077,0,\N,Missing
W09-0401,W09-0410,0,\N,Missing
W09-0401,W09-0415,0,\N,Missing
W09-0401,P07-1111,0,\N,Missing
W09-0401,W09-0412,0,\N,Missing
W09-0401,W06-3114,1,\N,Missing
W09-0401,W09-0414,0,\N,Missing
W09-0401,W10-1720,0,\N,Missing
W09-0401,W09-0405,0,\N,Missing
W09-0401,W09-0409,0,\N,Missing
W09-0419,2003.mtsummit-papers.46,1,0.715957,"s retained as a bilingual entry. Otherwise, the candidate is excluded. Given that a bilingual entry with a same lemma may have various inflectional forms in corpus, we then sum the lemma counts. Finally, in the current setup, we only keep the most frequent translation for each source. For our secondary submission for EnglishFrench, we extracted such entries from both the News Commentary and the Europarl corpus. Figure 2: Extraction pipeline: from parallel texts to bilingual dictionary 3.1 Manual customization through dictionary entries 3.3 The Systran system provides a dictionary coding tool (Senellart et al., 2003). This tool allows the manual task of coding entries to be partially automated with the use of monolingual dictionaries and probabilistic context-free grammars, while allowing the user to fine-tune it by correcting the automatic coding and/or add more features. However, this remains first of all a time-consuming task. Moreover, it is not easy for humans to select the best translation among a set of alternatives, let alone assign them probabilities. Last but not least, the beneficial effect on translation is not guaranteed (especially, the effect on the rule-based dependency analysis). Validati"
W09-0419,N07-1064,0,0.109192,"Cedex France Philipp Koehn** of Informatics University of Edinburgh 10 Crichton Street, Edinburgh United Kingdom ** School Abstract We describe here the two Systran/University of Edinburgh submissions for WMT2009. They involve a statistical post-editing model with a particular handling of named entities (English to French and German to English) and the extraction of phrasal rules (English to French). 1 Figure 1: Translation with PBMT post-editing Introduction Previous results had shown a rather satisfying performance for hybrid systems such as the Statistical Phrase-based Post-Editing (SPE) (Simard et al., 2007) combination in comparison with purely phrase-based statistical models, reaching similar BLEU scores and often receiving better human judgement (German to English at WMT2007) against the BLEU metric. This last result was in accordance with the previous acknowledgment (Callison-Burch et al., 2006) that systems of too differing structure could not be compared reliably with BLEU. We participated in the recent Workshop on Machine Translation (WMT’09) in the language pairs English to French and German to English. On the one hand we trained a PostEditing system with an additional special treatment t"
W09-0419,C08-1115,1,0.828838,"ries to parallel sentences 4: translate training corpus with current dictionary 5: for each entry do 6: translate all relevant sentences with current dictionary, plus this entry 7: compute BLEU scores without and with the entry 8: end for 9: Select entries with better/worse sentences ratio above threshold 10: add these entries to current dictionary 11: end for 5 Conclusion and future work We presented a few improvements to the Statistical Post Editing setup. They are part of an effort to better integrate a linguistic, rule-based system and the statistical correcting layer also illustrated in (Ueffing et al., 2008). Moreover, we presented a dictionary extraction setup which resulted in an improvement of 2 to 3 BLEU points over the baseline rule-based system when in-domain,as can be seen in table 4. This however improved translation very little on the ”news” domain which was used for evaluation. We think that is a different issue, namely of domain adaptation. In order to push further this rule-extraction approach and according to our previous work (Dugast et al., 2007) (Dugast et al., 2008), the most promising would probably be the use of alternative meanings and a language model to decode the best trans"
W09-0419,E06-1032,1,0.838034,"Missing"
W09-0419,W08-0309,1,0.744222,"to avoid the loss of entities such as dates and numbers. On the other hand we trained an additional English-to-French system (as a secondary submission) that made use of automatically extracted linguistic entries. In this paper, we will present both approaches. The latter is part of ongoing work motivated by the desire to both make use of corpus statistics and keep the advantage of the often (relative to automatic metrics’s scores) higher rank in human judgement given to rulebased systems on out-of-domain data, as seen on the WMT 2008 results for both English to French and German to English (Callison-Burch et al., 2008). 2 Statistical Post Editing systems 2.1 Baseline The basic setup is identical to the one described in (Dugast et al., 2007). A statistical translation model is trained between the rule-based translation of the source-side and the target-side of the parallel corpus. This is done separately for each parallel corpus. Language models are trained on each target half of the parallel corpora and also on additional in-domain corpora. Figure 1 shows the translation process. Here are a few additional details which tend to improve training and limit unwanted statistical effects in translation: • Named e"
W09-0419,D08-1064,0,0.0138034,"ce analysis. 2.2 Trimming In a statistical translation model, trimming of the phrase table had been shown to be beneficial (Johnson et al., 2007). For our post-editing model, we can afford to perform an even more aggressive trimming of the phrase table, since the rule-based system already provides us with a translation and we only aim at correcting the most frequent errors. Therefore, we suppress all unique phrase pairs before calculating the probabilities for the final phrase table. 2.3 Avoiding the loss of entities Deleted and spurious content is a well known problem for statistical models (Chiang et al., 2008). Though we do not know of any study proving it, it seems obvious that Named Entities that would be either deleted or added to the output out of nowhere is an especially problematic kind of 111 POS Noun Adverb Verb English college level on bail badmouth French niveau d’´etudes universitaires sous caution m´edire de headword English level on badmouth headword French niveau sous m´edire Table 2: Example dictionary entries (word alignment using GIZA++ and use of common heuristics to extract phrase pairs (Koehn et al., 2007)) to extract phrase pairs. At this stage the ”phrases” are plain word sequ"
W09-0419,W07-0732,1,0.888185,"secondary submission) that made use of automatically extracted linguistic entries. In this paper, we will present both approaches. The latter is part of ongoing work motivated by the desire to both make use of corpus statistics and keep the advantage of the often (relative to automatic metrics’s scores) higher rank in human judgement given to rulebased systems on out-of-domain data, as seen on the WMT 2008 results for both English to French and German to English (Callison-Burch et al., 2008). 2 Statistical Post Editing systems 2.1 Baseline The basic setup is identical to the one described in (Dugast et al., 2007). A statistical translation model is trained between the rule-based translation of the source-side and the target-side of the parallel corpus. This is done separately for each parallel corpus. Language models are trained on each target half of the parallel corpora and also on additional in-domain corpora. Figure 1 shows the translation process. Here are a few additional details which tend to improve training and limit unwanted statistical effects in translation: • Named entities are replaced by special tokens on both sides. By reducing vocabulary and combined with the next item mentioned, this"
W09-0419,W08-0327,1,0.830087,"Missing"
W09-0419,D07-1103,0,0.0151521,"ce analysis) target inflection. Motivations for adding phrasal dictionary entries (compound words) are twofold: first, just as for statistical translation models which went from word-based to phrase-based models, it helps solve disambiguation and non-literal translations. Second, as the rule-based engine makes use of a syntactic analysis of a source sentence, adding unambiguous phrasal chunks as entries will reduce the overall syntactic ambiguity and lead to a better source analysis. 2.2 Trimming In a statistical translation model, trimming of the phrase table had been shown to be beneficial (Johnson et al., 2007). For our post-editing model, we can afford to perform an even more aggressive trimming of the phrase table, since the rule-based system already provides us with a translation and we only aim at correcting the most frequent errors. Therefore, we suppress all unique phrase pairs before calculating the probabilities for the final phrase table. 2.3 Avoiding the loss of entities Deleted and spurious content is a well known problem for statistical models (Chiang et al., 2008). Though we do not know of any study proving it, it seems obvious that Named Entities that would be either deleted or added t"
W09-0419,P07-2045,1,0.0161351,"target language and all entities in the target language originate from the source language. This point is discussed in section 2.2. We will discuss some of these details further in the upcoming sections. Due to time constraints, we did not use the Giga French-English Parallel corpus provided for the workshop. We only made use of the News Commentary and the Europarl corpora. We used additional in-domain news corpora to train 5 grams language models, according to the baseline recommendations. Weights for these separate models were tuned through the Mert algorithm provided in the Moses toolkit (Koehn et al., 2007), using the provided news tuning set. 3 Rule Extraction The baseline Systran rule-based system is more or less a linguistic-oriented system that makes use of a dependency analysis, general transfer rules and dictionary entries, and finally a synthesis/reordering stage. The dictionary entries have long been the main entry point for customization of the system. Such lexical translation rules are fully linguistically coded dictionary entries, with the following features attached: part-of-speech, inflection category, headword and possibly some semantic tags. Table 2 displays a sample of manually-e"
W09-0429,P97-1003,0,0.176631,"mbr), the monotoneat-punctuation reordering constraint (mp, see Section 3.2), and bigger beam sizes. 2.4 BLEU (uncased) 16.6 20.6 20.6 20.9 20.9 21.7 22.0 22.1 22.3 Table 3: Results for German–English with the incremental addition of methods beyond a baseline trained on the parallel corpus English–German (ued’08: 12.1, best’08: 14.2) baseline + interpolated news LM + minimum Bayes risk decoding + monotone at punctuation + truecasing + morphological LM + big beam German–English For German–English, we additionally incorporated rule-based reordering — We parse the input using the Collins parser (Collins, 1997) and apply a set of reordering rules to re-arrange the German sentence so that it corresponds more closely English word order (Collins et al., 2005). compound splitting — We split German compound words (mostly nouns), based on the frequency of the words in the potential decompositions (Koehn and Knight, 2003a). part-of-speech language model — We use factored translation models (Koehn and Hoang, 2007) to also output part-of-speech tags with each word in a single phrase mapping and run a second n-gram model over them. The EnBLEU (uncased) 13.5 15.2 15.2 15.2 15.2 15.2 15.7 Table 4: Results for E"
W09-0429,P05-1066,1,0.591762,"0.9 21.7 22.0 22.1 22.3 Table 3: Results for German–English with the incremental addition of methods beyond a baseline trained on the parallel corpus English–German (ued’08: 12.1, best’08: 14.2) baseline + interpolated news LM + minimum Bayes risk decoding + monotone at punctuation + truecasing + morphological LM + big beam German–English For German–English, we additionally incorporated rule-based reordering — We parse the input using the Collins parser (Collins, 1997) and apply a set of reordering rules to re-arrange the German sentence so that it corresponds more closely English word order (Collins et al., 2005). compound splitting — We split German compound words (mostly nouns), based on the frequency of the words in the potential decompositions (Koehn and Knight, 2003a). part-of-speech language model — We use factored translation models (Koehn and Hoang, 2007) to also output part-of-speech tags with each word in a single phrase mapping and run a second n-gram model over them. The EnBLEU (uncased) 13.5 15.2 15.2 15.2 15.2 15.2 15.7 Table 4: Results for English–German with the incremental addition of methods beyiond a baseline trained on the parallel corpus glish part-of-speech tags are obtained usin"
W09-0429,P07-1019,0,0.153459,"Missing"
W09-0429,D07-1091,1,0.522879,"ng + monotone at punctuation + truecasing + morphological LM + big beam German–English For German–English, we additionally incorporated rule-based reordering — We parse the input using the Collins parser (Collins, 1997) and apply a set of reordering rules to re-arrange the German sentence so that it corresponds more closely English word order (Collins et al., 2005). compound splitting — We split German compound words (mostly nouns), based on the frequency of the words in the potential decompositions (Koehn and Knight, 2003a). part-of-speech language model — We use factored translation models (Koehn and Hoang, 2007) to also output part-of-speech tags with each word in a single phrase mapping and run a second n-gram model over them. The EnBLEU (uncased) 13.5 15.2 15.2 15.2 15.2 15.2 15.7 Table 4: Results for English–German with the incremental addition of methods beyiond a baseline trained on the parallel corpus glish part-of-speech tags are obtained using MXPOST (Ratnaparkhi, 1996). 2.5 English-German For English–German, we additionally incorporated a morphological language model the same way we incorporated a part-of-speech language model in the other translation direction. The morphological tags were o"
W09-0429,P07-2045,1,0.0207344,"lated them by optimizing perplexity on the provided tuning set. Perplexity numbers are shown in Table 1. Introduction The commitment of the University of Edinburgh to the WMT shared tasks is to provide a strong statistical machine translation baseline with our open source tools for all language pairs. We are again the only institution that participated in all tracks. The shared task is also an opportunity to incorporate novel contributions and test them against the best machine translation systems for these language pairs. In this paper we describe the speed improvements to the Moses decoder (Koehn et al., 2007), as well as a novel framework to specify reordering constraints with XML markup, which we tested with punctuation-based constraints. 2 ep nc 449 486 264 311 785 821 341 392 *:1475 1615 hung:2148 2.2 Truecasing Our traditional method to handle case is to lowercase all training data, and then have a separate recasing (or recapitalization) step. Last year, we used truecasing: all words are normalized to their natural case, e.g. the, John, eBay, meaning that only sentence-leading words may be changed to their most frequent form. To refine last year’s approach, we record the seen truecased instanc"
W09-0429,E03-1076,1,0.831356,"rman (ued’08: 12.1, best’08: 14.2) baseline + interpolated news LM + minimum Bayes risk decoding + monotone at punctuation + truecasing + morphological LM + big beam German–English For German–English, we additionally incorporated rule-based reordering — We parse the input using the Collins parser (Collins, 1997) and apply a set of reordering rules to re-arrange the German sentence so that it corresponds more closely English word order (Collins et al., 2005). compound splitting — We split German compound words (mostly nouns), based on the frequency of the words in the potential decompositions (Koehn and Knight, 2003a). part-of-speech language model — We use factored translation models (Koehn and Hoang, 2007) to also output part-of-speech tags with each word in a single phrase mapping and run a second n-gram model over them. The EnBLEU (uncased) 13.5 15.2 15.2 15.2 15.2 15.2 15.7 Table 4: Results for English–German with the incremental addition of methods beyiond a baseline trained on the parallel corpus glish part-of-speech tags are obtained using MXPOST (Ratnaparkhi, 1996). 2.5 English-German For English–German, we additionally incorporated a morphological language model the same way we incorporated a p"
W09-0429,P03-1040,1,0.654267,"rman (ued’08: 12.1, best’08: 14.2) baseline + interpolated news LM + minimum Bayes risk decoding + monotone at punctuation + truecasing + morphological LM + big beam German–English For German–English, we additionally incorporated rule-based reordering — We parse the input using the Collins parser (Collins, 1997) and apply a set of reordering rules to re-arrange the German sentence so that it corresponds more closely English word order (Collins et al., 2005). compound splitting — We split German compound words (mostly nouns), based on the frequency of the words in the potential decompositions (Koehn and Knight, 2003a). part-of-speech language model — We use factored translation models (Koehn and Hoang, 2007) to also output part-of-speech tags with each word in a single phrase mapping and run a second n-gram model over them. The EnBLEU (uncased) 13.5 15.2 15.2 15.2 15.2 15.2 15.7 Table 4: Results for English–German with the incremental addition of methods beyiond a baseline trained on the parallel corpus glish part-of-speech tags are obtained using MXPOST (Ratnaparkhi, 1996). 2.5 English-German For English–German, we additionally incorporated a morphological language model the same way we incorporated a p"
W09-0429,2007.mtsummit-papers.43,0,0.05017,"Missing"
W09-0429,C00-2105,0,0.0377708,"Missing"
W09-0429,W96-0213,0,\N,Missing
W09-0437,C08-1144,0,0.171475,"e contains is des services postaux / postal services which fails since it does not cover all of the input. This is an example for when the generalisation of the hierarchical model is superior to the phrase-based approach. 6 Related Work and Open Questions Zhang et al. (2008) and Wellington et al. (2006) answer the question: what is the minimal grammar that can be induced to completely describe a training set? We look at the related question of what a heuristically induced ruleset can translate in an unseen test set, considering both phrase- and grammar-based models. We also extend the work of Zollmann et al. (2008) on Chinese-English, performing the analysis in both directions and providing a detailed qualitative explanation. Our focus has been on the induction error of models, a previously unstudied cause of transla230 Source: concurrence des services postaux Reference: competition between postal services Hierarchical: postal services Deviation: ( [0-4: @S -> @Xˆ1 |@Xˆ1 ] ( [0-4: @X -> concurrence @Xˆ1 postaux |postal @Xˆ1 ] ( [1-3: @X ) -> des services | services ] postal services ) ) Figure 5: Derivation of a hierarchical translation which cannot be generated by the phrase-based system, in the format"
W09-0437,P06-1002,0,0.031231,"Missing"
W09-0437,D08-1023,0,0.0286128,"Missing"
W09-0437,J07-2003,0,0.0953474,"tion for the hierarchical system. Table 2: Ruleset size expressed as percentage of available rules when varying the limit of translation options tol per English span and percentage of French spans with up to tol translations. port this hypothesis experimentally in §5.4. 5.1 4 Our phrase-based system is Moses (Koehn et al., 2007). We set its stack size to 105 , disabled the beam threshold, and varied the translation option limit tol. Forced translation was implemented by Schwartz (2008) who ensures that hypothesis are a prefix of the reference to be generated. Our hierarchical system is Hiero (Chiang, 2007), modified to construct rules from a small sample of occurrences of each source phrase in training as described by Lopez (2008b). The search parameters restricting the number of rules or chart entries as well as the minimum threshold were set to very high values (1050 ) to prevent pruning. Forced translation was implemented by discarding rules and chart entries which do not match the reference. How Similar are Model Search Spaces? Most work on hierarchical phrase-based translation focuses quite intently on its structural differences from phrase-based translation. • A hierarchical model can tra"
W09-0437,W07-0414,0,0.210248,"Missing"
W09-0437,P07-1094,0,0.0347971,"t criterion such as WER and measure the amount of deviation from the reference. We could also maximize BLEU with respect to the reference as in Dreyer et al. (2007), but it is less interpretable. 7 Conclusion and Future Work Sparse distributions are common in natural language processing, and machine translation is no exception. We showed that utilizing more of the entire distribution can dramatically improve the coverage of translation models, and possibly their accuracy. Accounting for sparsity explicitly has achieved significant improvements in other areas such as in part of speech tagging (Goldwater and Griffiths, 2007). Considering the entire tail is challenging, since the search space grows exponentially with the number of translation options. A first step might be to use features that facilitate more variety in the top 20 translation options. A more elaborate aim is to look into alternatives to maximum likelihood hood estimation such as in Blunsom and Osborne (2008). Additionally, our expressiveness analysis shows Acknowledgements This research was supported by the Euromatrix Project funded by the European Commission (6th Framework Programme). The experiments were conducted using the resources provided by"
W09-0437,N03-1017,1,0.0368304,"otone translation otherwise. • While both models can indirectly model word deletion in the context of phrases, the hierarchical model can delete words using non-local context due to its use of discontiguous phrases. 5.2 Experimental Systems Experimental Data We conducted experiments in French-English translation, attempting to make the experimental conditions for both systems as equal as possible. Each system was trained on French-English Europarl (Koehn, 2005), version 3 (40M words). The corpus was aligned with GIZA++ (Och and Ney, 2003) and symmetrized with the grow-diag-finaland heuristic (Koehn et al., 2003). A trigram language model with modified Kneser-Ney discounting and interpolation was used as produced by the SRILM toolkit (Stolcke, 2002). Systems were optimized on the WMT08 French-English development data (2000 sentences) using minimum error rate training (Och, 2003) and tested on the WMT08 test data (2000 sentences). Rules based on unaligned words at the edges of foreign and source spans were not allowed unless otherwise stated, this is denoted as the tightness conThe underlying assumption in most discussions of these models is that these differences in their generative stories are respon"
W09-0437,P07-2045,1,0.0272604,"coders to generate the reference via disallowing reference-incompatible hypothesis or chart entries. This leaves only some search restrictions such as the distortion limit for the phrase-based system for which we controlled, or the maximum number of source words involved in a rule application for the hierarchical system. Table 2: Ruleset size expressed as percentage of available rules when varying the limit of translation options tol per English span and percentage of French spans with up to tol translations. port this hypothesis experimentally in §5.4. 5.1 4 Our phrase-based system is Moses (Koehn et al., 2007). We set its stack size to 105 , disabled the beam threshold, and varied the translation option limit tol. Forced translation was implemented by Schwartz (2008) who ensures that hypothesis are a prefix of the reference to be generated. Our hierarchical system is Hiero (Chiang, 2007), modified to construct rules from a small sample of occurrences of each source phrase in training as described by Lopez (2008b). The search parameters restricting the number of rules or chart entries as well as the minimum threshold were set to very high values (1050 ) to prevent pruning. Forced translation was imp"
W09-0437,2005.mtsummit-papers.11,1,0.0556356,"reorder phrases arbitrarily within the distortion limit, while the hierarchical model requires some lexical evidence for movement, resorting to monotone translation otherwise. • While both models can indirectly model word deletion in the context of phrases, the hierarchical model can delete words using non-local context due to its use of discontiguous phrases. 5.2 Experimental Systems Experimental Data We conducted experiments in French-English translation, attempting to make the experimental conditions for both systems as equal as possible. Each system was trained on French-English Europarl (Koehn, 2005), version 3 (40M words). The corpus was aligned with GIZA++ (Och and Ney, 2003) and symmetrized with the grow-diag-finaland heuristic (Koehn et al., 2003). A trigram language model with modified Kneser-Ney discounting and interpolation was used as produced by the SRILM toolkit (Stolcke, 2002). Systems were optimized on the WMT08 French-English development data (2000 sentences) using minimum error rate training (Och, 2003) and tested on the WMT08 test data (2000 sentences). Rules based on unaligned words at the edges of foreign and source spans were not allowed unless otherwise stated, this is"
W09-0437,C08-1064,1,0.811433,"spaces of phrase-based and hierarchical phrase-based models are highly overlapping despite the well known structural differences. 1 • Second, we find that the high-probability regions in the search spaces of phrase-based and hierarchical systems are nearly identical (§4). This means that reported differences between the models are due to their rankings of competing hypotheses, rather than structural differences of the derivations they produce. 2 Introduction Models, Search Spaces, and Errors A translation model consists of two distinct elements: an unweighted ruleset, and a parameterization (Lopez, 2008a; 2009). A ruleset licenses the steps by which a source string f1 ...fI may be rewritten as a target string e1 ...eJ . A parameterization defines a weight function over every sequence of rule applications. In a phrase-based model, the ruleset is simply the unweighted phrase table, where each phrase pair fi ...fi0 /ej ...ej 0 states that phrase fi ...fi0 in the source can be rewritten as ej ...ej 0 in the target. The model operates by iteratively applying rewrites to the source sentence until each source word has been consumed by exactly one rule. There are two additional heuristic rules: The"
W09-0437,E09-1061,1,0.884152,"Missing"
W09-0437,J03-1002,0,0.00694681,"chical model requires some lexical evidence for movement, resorting to monotone translation otherwise. • While both models can indirectly model word deletion in the context of phrases, the hierarchical model can delete words using non-local context due to its use of discontiguous phrases. 5.2 Experimental Systems Experimental Data We conducted experiments in French-English translation, attempting to make the experimental conditions for both systems as equal as possible. Each system was trained on French-English Europarl (Koehn, 2005), version 3 (40M words). The corpus was aligned with GIZA++ (Och and Ney, 2003) and symmetrized with the grow-diag-finaland heuristic (Koehn et al., 2003). A trigram language model with modified Kneser-Ney discounting and interpolation was used as produced by the SRILM toolkit (Stolcke, 2002). Systems were optimized on the WMT08 French-English development data (2000 sentences) using minimum error rate training (Och, 2003) and tested on the WMT08 test data (2000 sentences). Rules based on unaligned words at the edges of foreign and source spans were not allowed unless otherwise stated, this is denoted as the tightness conThe underlying assumption in most discussions of th"
W09-0437,P03-1021,0,0.063762,"nts in French-English translation, attempting to make the experimental conditions for both systems as equal as possible. Each system was trained on French-English Europarl (Koehn, 2005), version 3 (40M words). The corpus was aligned with GIZA++ (Och and Ney, 2003) and symmetrized with the grow-diag-finaland heuristic (Koehn et al., 2003). A trigram language model with modified Kneser-Ney discounting and interpolation was used as produced by the SRILM toolkit (Stolcke, 2002). Systems were optimized on the WMT08 French-English development data (2000 sentences) using minimum error rate training (Och, 2003) and tested on the WMT08 test data (2000 sentences). Rules based on unaligned words at the edges of foreign and source spans were not allowed unless otherwise stated, this is denoted as the tightness conThe underlying assumption in most discussions of these models is that these differences in their generative stories are responsible for differences in performance. We believe that this assumption should be investigated empirically. In an interesting analysis of phrase-based and hierarchical translation, Zollmann et al. (2008) forced a phrase-based system to produce the translations generated by"
W09-0437,P02-1040,0,0.0796532,"e are two additional heuristic rules: The distortion limit dl constrains distances over which phrases can be reordered, and the translation option limit tol constrains the number of target phrases that may be considered for any given source phrase. Together, these rules completely determine the finite set of all possible target sentences for a given source sentence. We call this set of target sentences the model search space. The parameterization of the model includes all information needed to score any particular seMost empirical work in translation analyzes models and algorithms using BLEU (Papineni et al., 2002) and related metrics. Though such metrics are useful as sanity checks in iterative system development, they are less useful as analytical tools. The performance of a translation system depends on the complex interaction of several different components. Since metrics assess only output, they fail to inform us about the consequences of these interactions, and thus provide no insight into the errors made by a system, or into the design tradeoffs of competing systems. In this work, we show that it is possible to obtain such insights by analyzing translation system components in isolation. We focus"
W09-0437,2008.amta-srw.6,0,0.0121521,"n limit for the phrase-based system for which we controlled, or the maximum number of source words involved in a rule application for the hierarchical system. Table 2: Ruleset size expressed as percentage of available rules when varying the limit of translation options tol per English span and percentage of French spans with up to tol translations. port this hypothesis experimentally in §5.4. 5.1 4 Our phrase-based system is Moses (Koehn et al., 2007). We set its stack size to 105 , disabled the beam threshold, and varied the translation option limit tol. Forced translation was implemented by Schwartz (2008) who ensures that hypothesis are a prefix of the reference to be generated. Our hierarchical system is Hiero (Chiang, 2007), modified to construct rules from a small sample of occurrences of each source phrase in training as described by Lopez (2008b). The search parameters restricting the number of rules or chart entries as well as the minimum threshold were set to very high values (1050 ) to prevent pruning. Forced translation was implemented by discarding rules and chart entries which do not match the reference. How Similar are Model Search Spaces? Most work on hierarchical phrase-based tra"
W09-0437,P06-1123,0,0.0248631,"ation involves both the reordering of the translation of postaux and the omittance of a translation for concurrence. This translation could be easily captured by a phrase-pair, however, it requires that the training data contains exactly such an example which was not the case. The closest rule the phrase-based rulestore contains is des services postaux / postal services which fails since it does not cover all of the input. This is an example for when the generalisation of the hierarchical model is superior to the phrase-based approach. 6 Related Work and Open Questions Zhang et al. (2008) and Wellington et al. (2006) answer the question: what is the minimal grammar that can be induced to completely describe a training set? We look at the related question of what a heuristically induced ruleset can translate in an unseen test set, considering both phrase- and grammar-based models. We also extend the work of Zollmann et al. (2008) on Chinese-English, performing the analysis in both directions and providing a detailed qualitative explanation. Our focus has been on the induction error of models, a previously unstudied cause of transla230 Source: concurrence des services postaux Reference: competition between"
W09-0437,C08-1136,0,0.0120129,": The second rule application involves both the reordering of the translation of postaux and the omittance of a translation for concurrence. This translation could be easily captured by a phrase-pair, however, it requires that the training data contains exactly such an example which was not the case. The closest rule the phrase-based rulestore contains is des services postaux / postal services which fails since it does not cover all of the input. This is an example for when the generalisation of the hierarchical model is superior to the phrase-based approach. 6 Related Work and Open Questions Zhang et al. (2008) and Wellington et al. (2006) answer the question: what is the minimal grammar that can be induced to completely describe a training set? We look at the related question of what a heuristically induced ruleset can translate in an unseen test set, considering both phrase- and grammar-based models. We also extend the work of Zollmann et al. (2008) on Chinese-English, performing the analysis in both directions and providing a detailed qualitative explanation. Our focus has been on the induction error of models, a previously unstudied cause of transla230 Source: concurrence des services postaux Re"
W09-1114,P08-1024,1,0.253533,"r an advantage on an appropriately optimised model. 4 Minimum risk training In the previous section, we described how our sampler can be used to search for the best translation under a variety of decoding criteria (max derivation, translation, and minimum risk). However, there appeared to be little benefit to marginalizing over the latent derivations. This is almost certainly a side effect of the MERT training approach that was used to construct the models so as to maximise the performance of the model on its single best derivation, without regard to the shape of the rest of the distribution (Blunsom et al., 2008). In this section we 107 describe a further application of the Gibbs sampler: to do unbiased minimum risk training. While there have been at least two previous attempts to do minimum risk training for MT, both approaches relied on biased k-best approximations (Smith and Eisner, 2006; Zens and Ney, 2007). Since we sample from the whole distribution, we will have a more accurate risk assessment. The risk, or expected loss, of a probabilistic translation model on a corpus D, defined with respect to a particular loss function `eˆ(e), where eˆ is the reference translation and e is a hypothesis tran"
W09-1114,J93-2003,0,0.0151602,"xpected risk training and decoding. 1 Introduction Statistical machine translation (SMT) poses the problem: given a foreign sentence f , find the translation e∗ that maximises the conditional posterior probability p(e|f ). This probabilistic formulation of translation has driven development of state-of-the-art systems which are able to learn from parallel corpora which were generated for other purposes — a direct result of employing a mathematical framework that we can reason about independently of any particular model. For example, we can train SMT models using maximum likelihood estimation (Brown et al., 1993; Och and Ney, 2000; Marcu and Wong, 2002). Alternatively, we can train to minimise probabilistic conceptions of risk (expected loss) with respect to translation metrics, thereby obtaining better results for those metrics (Kumar and Byrne, 2004; Smith and 102 Eisner, 2006; Zens and Ney, 2007). We can also use Bayesian inference techniques to avoid resorting to heuristics that damage the probabilistic interpretation of the models (Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009). Most models define multiple derivations for each translation; the probability of a translation is thus"
W09-1114,D08-1033,0,0.0826863,"n reason about independently of any particular model. For example, we can train SMT models using maximum likelihood estimation (Brown et al., 1993; Och and Ney, 2000; Marcu and Wong, 2002). Alternatively, we can train to minimise probabilistic conceptions of risk (expected loss) with respect to translation metrics, thereby obtaining better results for those metrics (Kumar and Byrne, 2004; Smith and 102 Eisner, 2006; Zens and Ney, 2007). We can also use Bayesian inference techniques to avoid resorting to heuristics that damage the probabilistic interpretation of the models (Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009). Most models define multiple derivations for each translation; the probability of a translation is thus the sum over all of its derivations. Unfortunately, finding the maximum probability translation is NPhard for all but the most trivial of models in this setting (Sima’an, 1996). It is thus necessary to resort to approximations for this sum and the search for its maximum e∗ . The most common of these approximations is the max-derivation approximation, which for many models can be computed in polynomial time via dynamic programming (DP). Though effective for some proble"
W09-1114,W06-1673,0,0.0295365,"rementally improves it via operators such as R ETRANS and M ERGE -S PLIT. It is also similar to the estimator of Marcu and Wong (2002), who employ the same operators to search the alignment space from a heuristic initialisation. Although the operators are similar, the use is different. These previous efforts employed their operators in a greedy hill-climbing search. In contrast, our operators are applied probabilistically, making them theoretically well-founded for a variety of inference problems. Our use of Gibbs sampling follows from its increasing use in Bayesian inference problems in NLP (Finkel et al., 2006; Johnson et al., 2007b). Most closely related is the work of DeNero et al. (2008), who derive a Gibbs sampler for phrase-based alignment, using it to infer phrase translation probabilities. The use of Monte Carlo techniques to calculate posteriors is similar to that of Chappelier and Rajman (2000) who use those techniques to find the best parse under models where the derivation and the parse are not isomorphic. 109 This research was supported in part by the GALE program of the Defense Advanced Research Projects Agency, Contract No. HR0011-06-2-001; and by the EuroMatrix project funded by the"
W09-1114,P01-1030,0,0.032787,"ns. 7 Conclusion We have described a Gibbs sampler for approximating two intractable problems in SMT: maximum translation decoding (and its variant, minimum risk decoding) and minimum risk training. By using Monte Carlo techniques we avoid the biases associated with the more commonly used DP based maxderivation (or k-best derivation) approximation. In doing so we provide a further tool to the translation community that we envision will allow the development and analysis of increasing theoretically well motivated techniques. Acknowledgments Related work Our sampler is similar to the decoder of Germann et al. (2001), which starts with an approximate solution and then incrementally improves it via operators such as R ETRANS and M ERGE -S PLIT. It is also similar to the estimator of Marcu and Wong (2002), who employ the same operators to search the alignment space from a heuristic initialisation. Although the operators are similar, the use is different. These previous efforts employed their operators in a greedy hill-climbing search. In contrast, our operators are applied probabilistically, making them theoretically well-founded for a variety of inference problems. Our use of Gibbs sampling follows from it"
W09-1114,D07-1103,0,0.222812,"on P (a, e|f ) and therefore to determine the maximum of this distribution, in other words the most likely derivation. Furthermore, we can marginalise over the alignments to estimate P (e|f ) ● ● For the German and French systems, the DEV 2006 set was used for model tuning and the TEST 2007 (in-domain) and NEWS - DEV 2009 B (out-of-domain) sets for testing. For the Arabic system, the MT 02 set (10 reference translations) was used for tuning and MT 03 (4 reference translations) was used for evaluation. To reduce the size of the phrase table, we used the association-score technique suggested by Johnson et al. (2007a). Translation quality is reported using case-insensitive BLEU (Papineni et al., 2002). KL Divergence ● ● ● 0.1 ● ● ● ● 0.01 ● ● 0.001 KL divergence ● ● ● ● ● 10 100 1000 10000 100000 ● 3.2 1000000 Iterations Figure 2: The KL divergence of the true posterior distribution and the distribution estimated by the Gibbs sampler at different numbers of iterations for the Arabic source sentence r}ys wzrA’ mAlyzyA yzwr Alflbyn (in English, The prime minister of Malaysia visits the Philippines). and so obtain the most likely translation. The Gibbs sampler can therefore be used as a decoder, either runn"
W09-1114,N07-1018,0,0.206505,"on P (a, e|f ) and therefore to determine the maximum of this distribution, in other words the most likely derivation. Furthermore, we can marginalise over the alignments to estimate P (e|f ) ● ● For the German and French systems, the DEV 2006 set was used for model tuning and the TEST 2007 (in-domain) and NEWS - DEV 2009 B (out-of-domain) sets for testing. For the Arabic system, the MT 02 set (10 reference translations) was used for tuning and MT 03 (4 reference translations) was used for evaluation. To reduce the size of the phrase table, we used the association-score technique suggested by Johnson et al. (2007a). Translation quality is reported using case-insensitive BLEU (Papineni et al., 2002). KL Divergence ● ● ● 0.1 ● ● ● ● 0.01 ● ● 0.001 KL divergence ● ● ● ● ● 10 100 1000 10000 100000 ● 3.2 1000000 Iterations Figure 2: The KL divergence of the true posterior distribution and the distribution estimated by the Gibbs sampler at different numbers of iterations for the Arabic source sentence r}ys wzrA’ mAlyzyA yzwr Alflbyn (in English, The prime minister of Malaysia visits the Philippines). and so obtain the most likely translation. The Gibbs sampler can therefore be used as a decoder, either runn"
W09-1114,N03-1017,1,0.202563,"s for probabilistic inference: 1. It typically differs from the true model maximum. 2. It often requires additional approximations in search, leading to further error. 3. It introduces restrictions on models, such as use of only local features. 4. It provides no good solution to compute the normalization factor Z(f ) required by many probabilistic algorithms. In this work, we solve these problems using a Monte Carlo technique with none of the above drawbacks. Our technique is based on a novel Gibbs sampler that draws samples from the posterior distribution of a phrase-based translation model (Koehn et al., 2003) but operates in linear time with respect to the number of input words (Section 2). We show Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 102–110, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics that it is effective for both decoding (Section 3) and minimum risk training (Section 4). 2 A Gibbs sampler for phrase-based translation models We begin by assuming a phrase-based translation model in which the input sentence, f , is segmented into phrases, which are sequences of adjacent words.1 Each foreign phrase is"
W09-1114,P07-2045,1,0.0245677,"en the distribution is too flat and the sampler spends too much time exploring unimportant probability regions. If it is too large, then the distribution is too peaked and the sampler may concentrate on a very narrow probability region. We optimised the scaling factor on a 200-sentence portion of the tuning set, finding that a multiplicative factor of 10 worked best for fr-en and a multiplicative factor of 6 for de-en. 3 The first experiment shows the effect of different initialisations and numbers of sampler iterations on max-derivation decoding performance of the sampler. The Moses decoder (Koehn et al., 2007) was used to generate the starting hypothesis, either in full DP max-derivation mode, or alternatively with restrictions on the features and reordering, or with zero weights to simulate a random initialisation, and the number of iterations varied from 100 to 200,000, with a 100 iteration burn-in in each case. Figure 3 shows the variation of model score with sampler iteration, for the different starting points, and for both language pairs. 3 We experimented with annealing, where the scale factor is gradually increased to sharpen the distribution while sampling. However, we found no improvements"
W09-1114,N04-1022,0,0.549837,"formulation of translation has driven development of state-of-the-art systems which are able to learn from parallel corpora which were generated for other purposes — a direct result of employing a mathematical framework that we can reason about independently of any particular model. For example, we can train SMT models using maximum likelihood estimation (Brown et al., 1993; Och and Ney, 2000; Marcu and Wong, 2002). Alternatively, we can train to minimise probabilistic conceptions of risk (expected loss) with respect to translation metrics, thereby obtaining better results for those metrics (Kumar and Byrne, 2004; Smith and 102 Eisner, 2006; Zens and Ney, 2007). We can also use Bayesian inference techniques to avoid resorting to heuristics that damage the probabilistic interpretation of the models (Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009). Most models define multiple derivations for each translation; the probability of a translation is thus the sum over all of its derivations. Unfortunately, finding the maximum probability translation is NPhard for all but the most trivial of models in this setting (Sima’an, 1996). It is thus necessary to resort to approximations for this sum and"
W09-1114,W02-1018,0,0.133102,"ntroduction Statistical machine translation (SMT) poses the problem: given a foreign sentence f , find the translation e∗ that maximises the conditional posterior probability p(e|f ). This probabilistic formulation of translation has driven development of state-of-the-art systems which are able to learn from parallel corpora which were generated for other purposes — a direct result of employing a mathematical framework that we can reason about independently of any particular model. For example, we can train SMT models using maximum likelihood estimation (Brown et al., 1993; Och and Ney, 2000; Marcu and Wong, 2002). Alternatively, we can train to minimise probabilistic conceptions of risk (expected loss) with respect to translation metrics, thereby obtaining better results for those metrics (Kumar and Byrne, 2004; Smith and 102 Eisner, 2006; Zens and Ney, 2007). We can also use Bayesian inference techniques to avoid resorting to heuristics that damage the probabilistic interpretation of the models (Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009). Most models define multiple derivations for each translation; the probability of a translation is thus the sum over all of its derivations. Unfo"
W09-1114,N06-1045,0,0.0117505,"led to a situation where entire classes of potentially useful features are not considered because they would be impractical to integrate into a DP based translation system. With the sampler this restriction is mitigated: any function of h(e, f, a) may participate in the translation model subject only to its own computability. Freed from the rusty manacles of dynamic programming, we anticipate development of many useful features. 6 To our knowledge, we are the first to apply Monte Carlo methods to maximum translation and minimum risk translation. Approaches to the former (Blunsom et al., 2008; May and Knight, 2006) rely on dynamic programming techniques which do not scale well without heuristic approximations, while approaches to the latter (Smith and Eisner, 2006; Zens et al., 2007) use biased k-best approximations. 7 Conclusion We have described a Gibbs sampler for approximating two intractable problems in SMT: maximum translation decoding (and its variant, minimum risk decoding) and minimum risk training. By using Monte Carlo techniques we avoid the biases associated with the more commonly used DP based maxderivation (or k-best derivation) approximation. In doing so we provide a further tool to the t"
W09-1114,C00-2163,0,0.042141,"g and decoding. 1 Introduction Statistical machine translation (SMT) poses the problem: given a foreign sentence f , find the translation e∗ that maximises the conditional posterior probability p(e|f ). This probabilistic formulation of translation has driven development of state-of-the-art systems which are able to learn from parallel corpora which were generated for other purposes — a direct result of employing a mathematical framework that we can reason about independently of any particular model. For example, we can train SMT models using maximum likelihood estimation (Brown et al., 1993; Och and Ney, 2000; Marcu and Wong, 2002). Alternatively, we can train to minimise probabilistic conceptions of risk (expected loss) with respect to translation metrics, thereby obtaining better results for those metrics (Kumar and Byrne, 2004; Smith and 102 Eisner, 2006; Zens and Ney, 2007). We can also use Bayesian inference techniques to avoid resorting to heuristics that damage the probabilistic interpretation of the models (Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009). Most models define multiple derivations for each translation; the probability of a translation is thus the sum over all o"
W09-1114,P03-1021,0,0.0606256,"nstructed a phrase-based translation model as described in Koehn et al. (2003), limiting the phrase length to 5. The target side of the parallel corpus was used to train a 3-gram language model. 2 The Arabic-English training data consists of the eTIRR corpus (LDC2004E72), the Arabic news corpus (LDC2004T17), the Ummah corpus (LDC2004T18), and the sentences with confidence c > 0.995 in the ISI automatically extracted web parallel corpus (LDC2006T02). 105 Translation performance For the experiments reported in this section, we used feature weights trained with minimum error rate training (MERT; Och, 2003) . Because MERT ignores the denominator in Equation 1, it is invariant with respect to the scale of the weight vector θ — the Moses implementation simply normalises the weight vector it finds by its `1 -norm. However, when we use these weights in a true probabilistic model, the scaling factor affects the behaviour of the model since it determines how peaked or flat the distribution is. If the scaling factor is too small, then the distribution is too flat and the sampler spends too much time exploring unimportant probability regions. If it is too large, then the distribution is too peaked and t"
W09-1114,P02-1040,0,0.0772047,"words the most likely derivation. Furthermore, we can marginalise over the alignments to estimate P (e|f ) ● ● For the German and French systems, the DEV 2006 set was used for model tuning and the TEST 2007 (in-domain) and NEWS - DEV 2009 B (out-of-domain) sets for testing. For the Arabic system, the MT 02 set (10 reference translations) was used for tuning and MT 03 (4 reference translations) was used for evaluation. To reduce the size of the phrase table, we used the association-score technique suggested by Johnson et al. (2007a). Translation quality is reported using case-insensitive BLEU (Papineni et al., 2002). KL Divergence ● ● ● 0.1 ● ● ● ● 0.01 ● ● 0.001 KL divergence ● ● ● ● ● 10 100 1000 10000 100000 ● 3.2 1000000 Iterations Figure 2: The KL divergence of the true posterior distribution and the distribution estimated by the Gibbs sampler at different numbers of iterations for the Arabic source sentence r}ys wzrA’ mAlyzyA yzwr Alflbyn (in English, The prime minister of Malaysia visits the Philippines). and so obtain the most likely translation. The Gibbs sampler can therefore be used as a decoder, either running in max-derivation and max-translation mode. Using the Gibbs sampler in this way mak"
W09-1114,C96-2215,0,0.0376306,"Missing"
W09-1114,P06-2101,0,0.637386,"ared to be little benefit to marginalizing over the latent derivations. This is almost certainly a side effect of the MERT training approach that was used to construct the models so as to maximise the performance of the model on its single best derivation, without regard to the shape of the rest of the distribution (Blunsom et al., 2008). In this section we 107 describe a further application of the Gibbs sampler: to do unbiased minimum risk training. While there have been at least two previous attempts to do minimum risk training for MT, both approaches relied on biased k-best approximations (Smith and Eisner, 2006; Zens and Ney, 2007). Since we sample from the whole distribution, we will have a more accurate risk assessment. The risk, or expected loss, of a probabilistic translation model on a corpus D, defined with respect to a particular loss function `eˆ(e), where eˆ is the reference translation and e is a hypothesis translation L= X X p(e|f )`eˆ(e) (3) hˆ e,f i∈D e This value can be trivially computed using equation (2). In this section, we are concerned with finding the parameters θ that minimise (3). Fortunately, with the log-linear parameterization of p(e|f ), L is differentiable with respect to"
W09-1114,N07-1062,0,0.101321,"of state-of-the-art systems which are able to learn from parallel corpora which were generated for other purposes — a direct result of employing a mathematical framework that we can reason about independently of any particular model. For example, we can train SMT models using maximum likelihood estimation (Brown et al., 1993; Och and Ney, 2000; Marcu and Wong, 2002). Alternatively, we can train to minimise probabilistic conceptions of risk (expected loss) with respect to translation metrics, thereby obtaining better results for those metrics (Kumar and Byrne, 2004; Smith and 102 Eisner, 2006; Zens and Ney, 2007). We can also use Bayesian inference techniques to avoid resorting to heuristics that damage the probabilistic interpretation of the models (Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009). Most models define multiple derivations for each translation; the probability of a translation is thus the sum over all of its derivations. Unfortunately, finding the maximum probability translation is NPhard for all but the most trivial of models in this setting (Sima’an, 1996). It is thus necessary to resort to approximations for this sum and the search for its maximum e∗ . The most common"
W09-1114,D07-1055,0,0.0381723,"th the sampler this restriction is mitigated: any function of h(e, f, a) may participate in the translation model subject only to its own computability. Freed from the rusty manacles of dynamic programming, we anticipate development of many useful features. 6 To our knowledge, we are the first to apply Monte Carlo methods to maximum translation and minimum risk translation. Approaches to the former (Blunsom et al., 2008; May and Knight, 2006) rely on dynamic programming techniques which do not scale well without heuristic approximations, while approaches to the latter (Smith and Eisner, 2006; Zens et al., 2007) use biased k-best approximations. 7 Conclusion We have described a Gibbs sampler for approximating two intractable problems in SMT: maximum translation decoding (and its variant, minimum risk decoding) and minimum risk training. By using Monte Carlo techniques we avoid the biases associated with the more commonly used DP based maxderivation (or k-best derivation) approximation. In doing so we provide a further tool to the translation community that we envision will allow the development and analysis of increasing theoretically well motivated techniques. Acknowledgments Related work Our sample"
W09-1114,P08-1012,0,0.00643602,"framework that we can reason about independently of any particular model. For example, we can train SMT models using maximum likelihood estimation (Brown et al., 1993; Och and Ney, 2000; Marcu and Wong, 2002). Alternatively, we can train to minimise probabilistic conceptions of risk (expected loss) with respect to translation metrics, thereby obtaining better results for those metrics (Kumar and Byrne, 2004; Smith and 102 Eisner, 2006; Zens and Ney, 2007). We can also use Bayesian inference techniques to avoid resorting to heuristics that damage the probabilistic interpretation of the models (Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009). Most models define multiple derivations for each translation; the probability of a translation is thus the sum over all of its derivations. Unfortunately, finding the maximum probability translation is NPhard for all but the most trivial of models in this setting (Sima’an, 1996). It is thus necessary to resort to approximations for this sum and the search for its maximum e∗ . The most common of these approximations is the max-derivation approximation, which for many models can be computed in polynomial time via dynamic programming (DP). Though effe"
W10-1703,W10-0701,1,0.75236,"Missing"
W10-1703,W10-1706,0,0.0242402,"Missing"
W10-1703,W10-1714,0,0.0249619,"Missing"
W10-1703,W10-1709,0,0.0199718,"Missing"
W10-1703,W06-3114,1,0.791425,"ion – This year we excluded Google translations from the systems used in system combination. In last year’s evaluation, the large margin between Google and many of the other systems meant that it was hard to improve on when combining systems. This year, the system combinations perform better than their component systems more often than last year. Introduction This paper presents the results of the shared tasks of the joint Workshop on statistical Machine Translation (WMT) and Metrics for MAchine TRanslation (MetricsMATR), which was held at ACL 2010. This builds on four previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007; Callison-Burch et al., 2008; Callison-Burch et al., 2009), and one previous MetricsMATR meeting (Przybocki et al., 2008). There were three shared tasks this year: a translation task between English and four other European languages, a task to combine the output of multiple machine translation systems, and a task to predict human judgments of translation quality using automatic evaluation metrics. The • Fewer rule-based systems – This year there were fewer rule-based systems submitted. In past years, University of Saarland compiled a large set of outputs from rule"
W10-1703,W10-1710,0,0.0338164,"Missing"
W10-1703,P07-2045,1,0.00952672,"chine translation. As with past years, all of the data, translations, and human judgments produced for our workshop are publicly available.2 We hope they form a valuable resource for research into statistical machine translation, system combination, and automatic evaluation of translation quality. 2 train language models, and development sets to tune parameters. Some statistics about the training materials are given in Figure 1. 2.3 To lower the barrier of entry for newcomers to the field, we provided two open source toolkits for phrase-based and parsing-based statistical machine translation (Koehn et al., 2007; Li et al., 2009). Overview of the shared translation and system combination tasks 2.4 Test data The test data for this year’s task was created by hiring people to translate news articles that were drawn from a variety of sources from midDecember 2009. A total of 119 articles were selected, in roughly equal amounts from a variety of Czech, English, French, German and Spanish news sites:3 2.5 System combination In total, we received 153 primary system submissions along with 28 secondary submissions. These were made available to participants in the system combination shared task. Based on feedb"
W10-1703,W10-1722,0,0.0405706,"Missing"
W10-1703,W09-0424,1,0.519668,"s with past years, all of the data, translations, and human judgments produced for our workshop are publicly available.2 We hope they form a valuable resource for research into statistical machine translation, system combination, and automatic evaluation of translation quality. 2 train language models, and development sets to tune parameters. Some statistics about the training materials are given in Figure 1. 2.3 To lower the barrier of entry for newcomers to the field, we provided two open source toolkits for phrase-based and parsing-based statistical machine translation (Koehn et al., 2007; Li et al., 2009). Overview of the shared translation and system combination tasks 2.4 Test data The test data for this year’s task was created by hiring people to translate news articles that were drawn from a variety of sources from midDecember 2009. A total of 119 articles were selected, in roughly equal amounts from a variety of Czech, English, French, German and Spanish news sites:3 2.5 System combination In total, we received 153 primary system submissions along with 28 secondary submissions. These were made available to participants in the system combination shared task. Based on feedback that we receiv"
W10-1703,W10-1723,0,0.0273443,"Missing"
W10-1703,W10-1718,1,0.767992,"Missing"
W10-1703,W10-1748,0,0.0371589,"Missing"
W10-1703,W10-1754,0,0.0648908,"Missing"
W10-1703,W10-1731,0,0.0201809,"Missing"
W10-1703,W10-1725,0,0.0253357,"Missing"
W10-1703,W10-1732,0,0.0225271,"Missing"
W10-1703,W10-1733,0,0.0311783,"Missing"
W10-1703,W10-1726,0,0.0139847,"Missing"
W10-1703,W10-1727,0,0.0169981,"Missing"
W10-1703,W10-1728,0,0.0374247,"Missing"
W10-1703,W10-1729,0,0.0349316,"Missing"
W10-1703,W10-1716,0,\N,Missing
W10-1703,W10-1715,1,\N,Missing
W10-1703,W10-1751,0,\N,Missing
W10-1703,W10-1755,0,\N,Missing
W10-1703,W10-1721,0,\N,Missing
W10-1703,W10-1719,0,\N,Missing
W10-1703,W10-1746,0,\N,Missing
W10-1703,W10-1750,0,\N,Missing
W10-1703,W10-1708,0,\N,Missing
W10-1703,W10-1717,0,\N,Missing
W10-1703,W09-0401,1,\N,Missing
W10-1703,W10-1724,0,\N,Missing
W10-1703,W10-1745,0,\N,Missing
W10-1703,W10-1711,0,\N,Missing
W10-1703,2010.iwslt-evaluation.22,0,\N,Missing
W10-1703,D09-1030,1,\N,Missing
W10-1703,W07-0718,1,\N,Missing
W10-1703,W10-1743,0,\N,Missing
W10-1703,W10-1749,0,\N,Missing
W10-1703,W10-1744,0,\N,Missing
W10-1703,W08-0309,1,\N,Missing
W10-1703,W10-1713,0,\N,Missing
W10-1703,W09-0426,0,\N,Missing
W10-1703,W10-1704,0,\N,Missing
W10-1703,W10-1720,0,\N,Missing
W10-1703,W10-1747,0,\N,Missing
W10-1703,W10-1712,0,\N,Missing
W10-1703,W10-1741,0,\N,Missing
W10-1703,W10-1753,0,\N,Missing
W10-1703,W10-1740,0,\N,Missing
W10-1703,W10-1730,0,\N,Missing
W10-1703,W10-1705,0,\N,Missing
W10-1703,W10-1742,0,\N,Missing
W10-1703,W10-1752,0,\N,Missing
W10-1715,P05-1033,0,0.098205,"it. Thus, all our experiments should be replicable with publicly available resources. Tree-Based Models A major extension of the capabilities of the Moses system is the accommodation of tree-based models (Hoang et al., 2009). While we have not yet carried out sufficient experimentation and optimization of the implementation, we took the occasion of the shared translation task as a opportunity to build large-scale systems using such models. We build two translation systems: One using tree-based models without additional linguistic annotation, which are known as hierarchical phrasebased models (Chiang, 2005), and another system that uses linguistic annotation on the target side, which are known under many names such as string-to-tree models or syntactified target models (Marcu et al., 2006). Both models are trained using a very similar pipeline as for the phrase model. The main difference is that the translation rules do not have to be contiguous phrases, but may contain gaps with are labeled and co-ordinated by non-terminal symbols. Decoding with such models requires a very different algorithm, which is related to syntactic chart parsing. In the target syntax model, the target gaps and the entir"
W10-1715,P05-1066,1,0.764923,"er sentence to about 8 sec/sentence. However, this resulted only in minimal gains, on average +0.03 BLEU. For details refer back to Table 1. German–English For the German–English language direction, we used two additional processing steps that have shown to be successful in the past, and again resulted in significant gains. We split large words based on word frequencies to tackle the problem of word compounds in German (Koehn and Knight, 2003). Secondly, we re-order the German input to the decoder (and the German side of the training data) to align more closely to the English target language (Collins et al., 2005). The two methods improve +0.58 and +0.52 over the baseline individually, and +1.34 when combined. See also Table 8. 3.5 109 Corpus Last year, due to time constraints, we were not able to use the billion word 109 corpus for the French–English language pairs. This is largest publicly available parallel corpus, and it does strain computing resources, for instance forcing us to use multi-threaded GIZA++ (Gao and Vogel, 2008). Table 7 shows the gains obtained from using this corpus in both the translation model and the language model opposed to a baseline system trained with otherwise the same set"
W10-1715,W06-1607,0,0.0794072,"Missing"
W10-1715,W08-0509,0,0.0249618,"ehn and Knight, 2003). Secondly, we re-order the German input to the decoder (and the German side of the training data) to align more closely to the English target language (Collins et al., 2005). The two methods improve +0.58 and +0.52 over the baseline individually, and +1.34 when combined. See also Table 8. 3.5 109 Corpus Last year, due to time constraints, we were not able to use the billion word 109 corpus for the French–English language pairs. This is largest publicly available parallel corpus, and it does strain computing resources, for instance forcing us to use multi-threaded GIZA++ (Gao and Vogel, 2008). Table 7 shows the gains obtained from using this corpus in both the translation model and the language model opposed to a baseline system trained with otherwise the same settings. For French–English we see large gains (+1.23), but not for English–French (+0.10). Our official submission for the French–English language pairs used these models. They did not include a part-of-speech language model and bigger beam sizes. 3.7 Translation Model Interpolation Finally, we explored a novel domain adaption method for the translation model. Since the interpolation of language models is very successful,"
W10-1715,W06-1606,0,0.0432787,"on of tree-based models (Hoang et al., 2009). While we have not yet carried out sufficient experimentation and optimization of the implementation, we took the occasion of the shared translation task as a opportunity to build large-scale systems using such models. We build two translation systems: One using tree-based models without additional linguistic annotation, which are known as hierarchical phrasebased models (Chiang, 2005), and another system that uses linguistic annotation on the target side, which are known under many names such as string-to-tree models or syntactified target models (Marcu et al., 2006). Both models are trained using a very similar pipeline as for the phrase model. The main difference is that the translation rules do not have to be contiguous phrases, but may contain gaps with are labeled and co-ordinated by non-terminal symbols. Decoding with such models requires a very different algorithm, which is related to syntactic chart parsing. In the target syntax model, the target gaps and the entire target phrase must map to constituents in the parse tree. This restriction may be relaxed by adding constituent labels such as DET + ADJ or NP DET to group neighboring constituents or"
W10-1715,I08-2089,1,0.821265,"lation: number of tokens, perplexity, and interpolation weight for the different corpora 2.1 Cased 25.25 25.23 19.47 20.74 24.20 23.83 14.68 14.63 Interpolated Language Model 2.2 The WMT training data exhibits an increasing diversity of corpora: Europarl, News Commentary, UN, 109 , News — and seven different sources within the Czeng corpus. It is well known that domain adaptation is an important step in optimizing machine translation systems. A relatively simple and straight-forward method is the linear interpolation of the language model, as we explored previously (Koehn and Schroeder, 2007; Schwenk and Koehn, 2008). We trained domain-specific language models separately and then linearly interpolated them using SRILM toolkit (Stolke, 2002) with weights opTruecasing As last year, we deal with uppercase and lowercase forms of the same words by truecasing the corpus. This means that we change each surface word occurrence of a word to its natural case, e.g., the, Europe. During truecasing, we change the first word of a sentence to its most frequent casing. During de-truecasing, we uppercase the first letter of the first word of a sentence. See Table 3 for the performance of this method. In this table, we com"
W10-1715,2009.iwslt-papers.4,1,0.831497,"and our ability to exploit them. We also saw gains from adding linguistic annotation (in form of 7-gram models over part-ofspeech tags) and promising results for tree-based models. At this point, we are quite satisfied being able to build competitive systems with these new models, which opens up major new research directions. Everything we described here is part of the open source Moses toolkit. Thus, all our experiments should be replicable with publicly available resources. Tree-Based Models A major extension of the capabilities of the Moses system is the accommodation of tree-based models (Hoang et al., 2009). While we have not yet carried out sufficient experimentation and optimization of the implementation, we took the occasion of the shared translation task as a opportunity to build large-scale systems using such models. We build two translation systems: One using tree-based models without additional linguistic annotation, which are known as hierarchical phrasebased models (Chiang, 2005), and another system that uses linguistic annotation on the target side, which are known under many names such as string-to-tree models or syntactified target models (Marcu et al., 2006). Both models are trained"
W10-1715,W09-0429,1,0.851718,"Missing"
W10-1715,W06-3119,0,0.12094,"Missing"
W10-1715,D07-1091,1,0.826346,"we find singleton 357,929,182 phrase pairs and 24,966,751 phrase pairs that occur twice. The Good Turing formula tells us to adapt singleton 24,966,751 counts to 357,929,182 = 0.14. This means for our degenerate example of a single occurrence of a single French phrase that its single English translation has probability 0.14 1 = 0.14 (we do not adjust the denominator). Good Turing smoothing of the translation table gives us a gain of +0.17 BLEU points on average, and improvements for 7 out of 8 language pairs. For details refer back to Table 1. 3.3 POS n-gram Model The factored model approach (Koehn and Hoang, 2007) allows us to integrate 7-gram models over part-of-speech tags. The part-of-speech tags are produced during decoding by the phrase mapping of surface words on the source side to a factored representation of surface words and their part-ofspeech tags on the target side in one translation step. We previously used this additional scoring component for the German–English language pairs with success. Thus we now applied to it all other language pairs (except for English–Czech due to the lack of a Czech part-of-speech tagger). We used the following part-of-speech taggers: • • • • English: mxpost1 Ge"
W10-1715,P07-2045,1,0.0151141,"the ACL Workshop for Statistical Machine Translation 2010 in all language pairs. We continued our efforts to integrate linguistic annotation into the translation process, using factored and treebased translation models. On average we outperformed our submission from last year by 2.16 BLEU points on the same newstest2009 test set. While the submitted system follows the factored phrase-based approach, we also built hierarchical and syntax-based models for the English–German language pair and report on its performance on the development test sets. All our systems are based on the Moses toolkit (Koehn et al., 2007). We achieved gains over the systems from last year by consistently exploiting all available training data, using large-scale domain-interpolated, and consistent use of the factored translation model to integrate n-gram models over speech tags. We also experimented with novel domain adaptation methods, with mixed results. 2 • cube pruning We used most of these setting in our submission last year (Koehn and Haddow, 2009). The main difference to our baseline system from the submission from last year is the use of additional training data: larger releases of the News Commentary, Europarl, Czeng,"
W10-1715,E03-1076,1,0.823663,"e decoder is quite fast, partly due to multithreaded decoding using 4 cores machines (Haddow, 2010). Increasing the beam sizes slowed down decoding speed from about 2 seconds per sentence to about 8 sec/sentence. However, this resulted only in minimal gains, on average +0.03 BLEU. For details refer back to Table 1. German–English For the German–English language direction, we used two additional processing steps that have shown to be successful in the past, and again resulted in significant gains. We split large words based on word frequencies to tackle the problem of word compounds in German (Koehn and Knight, 2003). Secondly, we re-order the German input to the decoder (and the German side of the training data) to align more closely to the English target language (Collins et al., 2005). The two methods improve +0.58 and +0.52 over the baseline individually, and +1.34 when combined. See also Table 8. 3.5 109 Corpus Last year, due to time constraints, we were not able to use the billion word 109 corpus for the French–English language pairs. This is largest publicly available parallel corpus, and it does strain computing resources, for instance forcing us to use multi-threaded GIZA++ (Gao and Vogel, 2008)."
W10-1715,W07-0733,1,0.872608,"Table 2: English LM interpolation: number of tokens, perplexity, and interpolation weight for the different corpora 2.1 Cased 25.25 25.23 19.47 20.74 24.20 23.83 14.68 14.63 Interpolated Language Model 2.2 The WMT training data exhibits an increasing diversity of corpora: Europarl, News Commentary, UN, 109 , News — and seven different sources within the Czeng corpus. It is well known that domain adaptation is an important step in optimizing machine translation systems. A relatively simple and straight-forward method is the linear interpolation of the language model, as we explored previously (Koehn and Schroeder, 2007; Schwenk and Koehn, 2008). We trained domain-specific language models separately and then linearly interpolated them using SRILM toolkit (Stolke, 2002) with weights opTruecasing As last year, we deal with uppercase and lowercase forms of the same words by truecasing the corpus. This means that we change each surface word occurrence of a word to its natural case, e.g., the, Europe. During truecasing, we change the first word of a sentence to its most frequent casing. During de-truecasing, we uppercase the first letter of the first word of a sentence. See Table 3 for the performance of this met"
W10-1737,J03-4003,0,0.0144024,"e appropriate antecedent of a pronoun. The implementation of the algorithm we deal with here is fairly different from the one presented in the original paper, and is largely inspired from the JavaRAP implementation [Qiu et al., 2004]. The first important variation was mentioned earlier and concerns the application of co-reference resolution to machine translation. We concentrate in this work on the resolution of third person pronouns, and we omit reflexive pronouns (itself, themselves) (referred to as lexical anaphora in some works). Another variation comes from the use of the Collins parser [Collins, 2003]. Although work on the original algorithm uses McCord’s Slot Grammar parser [McCord, 1990], work on JavaRAP shows that rules can be created to simulate the categories and predicates used in slot grammar. Also, Preiss [2002] evaluates the use of different parsers for the Lappin and Leass algorithm, showing that performance of the algorithm is not related to the performance of the parser itself. The JavaRAP implementation uses a Charniak parser, which performs worse than the Collins parser in Preiss’ research. 256 weights and the highest scoring one is returned as the antecedent of the pronoun."
W10-1737,W98-1119,0,0.0170291,"Missing"
W10-1737,C96-1079,0,0.0890582,"rn Shared Tasks and Evaluation Although a fairly large amount of research has been done in the field, it is often reported [Mitkov et al., 1995] that there does not yet exist a method to resolve pronouns which is entirely satisfactory and effective. Different kinds of texts (novel, newspaper,...) pose problems [Hobbs, 1978] and the field is also victim of lack of standardization. Algorithms are evaluated on different texts and large annotated corpora with co-reference information is lacking to check results. A response to these problems came with the creation of shared tasks, such as the MUC [Grishman and Sundheim, 1996] which included a co-reference subtask [Chinchor and Hirschmann, 1997] and led to the creation of the MUC-6 and MUC-7 corpora. There are other annotation efforts worth mentioning, such as the ARRAU corpus [Poesio and Artstein, 2008] which include texts from various sources and deals with previous problems in annotation such as anaphora ambiguity and annotation of information on agreement, grammatical function and reference. The Anaphoric Bank and the Phrase Detectives are both part of the Anawiki project [Poesio et al., 2008] and also promise the creation of a standardized corpus. The first o"
W10-1737,J95-2003,0,0.674153,"Missing"
W10-1737,1995.tmi-1.6,0,0.206659,"our algorithm. Indeed, the detection of number information in the algorithm is not good enough and returns many false results which would reduce the performance of the final system. Also, adding the number agreement to the pronoun would mean a high segmentation between all the different possibilities, which we assumed would result in worse performance of the translation system. Once we have created a way to tag the pronouns with gender information, the system needs to learn Shared Tasks and Evaluation Although a fairly large amount of research has been done in the field, it is often reported [Mitkov et al., 1995] that there does not yet exist a method to resolve pronouns which is entirely satisfactory and effective. Different kinds of texts (novel, newspaper,...) pose problems [Hobbs, 1978] and the field is also victim of lack of standardization. Algorithms are evaluated on different texts and large annotated corpora with co-reference information is lacking to check results. A response to these problems came with the creation of shared tasks, such as the MUC [Grishman and Sundheim, 1996] which included a co-reference subtask [Chinchor and Hirschmann, 1997] and led to the creation of the MUC-6 and MUC"
W10-1737,P05-1020,0,0.0187887,"Missing"
W10-1737,N04-1037,0,0.0128011,"Missing"
W10-1737,J75-4037,0,0.831791,"o-Reference : Syntactic Method The first work on the resolution of pronouns was done in the 1970s, largely based on a syntactic approach. This work was based on empirical data and observations about natural languages. For example, Winograd [1972] uses the notion of coreference chains when stating that if a single pronoun is used several times in a sentence or a group of adjunct sentences, all instances of this pronoun should refer to the same entity. Others have also stated that antecedents of a pronoun should be found in one of the n sentences preceding the pronouns, where n should be small [Klapholz and Lockman, 1975]. Hobbs [1978] showed that this number was close to one, although no actual limit could be really imposed. In work by both Hobbs [1978] and Winograd [1972], the resolution of pronouns also involves a syntactic study of the parse tree of sentences. The order with which candidate antecedents are prioritized is similar in both studies. They first look for the antecedent to be a subject, then the direct object of a noun and finally an indirect object. Only thereafter previous sentences are checked for an antecedent, in no particular order, although the left to right order seems to be preferred in"
W10-1737,2005.mtsummit-papers.11,1,0.0821375,"Missing"
W10-1737,poesio-artstein-2008-anaphoric,0,0.00984944,"and effective. Different kinds of texts (novel, newspaper,...) pose problems [Hobbs, 1978] and the field is also victim of lack of standardization. Algorithms are evaluated on different texts and large annotated corpora with co-reference information is lacking to check results. A response to these problems came with the creation of shared tasks, such as the MUC [Grishman and Sundheim, 1996] which included a co-reference subtask [Chinchor and Hirschmann, 1997] and led to the creation of the MUC-6 and MUC-7 corpora. There are other annotation efforts worth mentioning, such as the ARRAU corpus [Poesio and Artstein, 2008] which include texts from various sources and deals with previous problems in annotation such as anaphora ambiguity and annotation of information on agreement, grammatical function and reference. The Anaphoric Bank and the Phrase Detectives are both part of the Anawiki project [Poesio et al., 2008] and also promise the creation of a standardized corpus. The first one allows for the sharing of annotated corpora. The second is a collaborative effort to annotate large corpora through the Web. In its first year of use, the system saw the resolution of 700,000 pronouns. 3 Integration into Machine"
W10-1737,P07-2045,1,0.00905274,"Missing"
W10-1737,poesio-etal-2008-anawiki,0,0.0361017,"Missing"
W10-1737,qiu-etal-2004-public,0,0.0142027,"Missing"
W10-1737,J94-4002,0,0.763605,"Missing"
W10-1737,1988.tmi-1.7,0,0.500124,"nce such as 2 2.1 Related Work Co-Reference and Machine Translation The problem of anaphora resolution applied to machine translation has not been treated much in the literature. Although some papers refer to the problem, their content is mostly concerned with the problem of anaphora resolution and speak very little about the integration of such an algorithm in the bigger theme of machine translation. Mitkov et al. [1995] deplore the lack of study of the question and try to address it with the implementation of an anaphora resolution model and its integration into the CAT2 translation system [Sharp, 1988], a transfer system that uses an abThe window is open. It is blue. the translation of it cannot be determined given only the sentence it occurs in. It is essential that we connect it to the entity the window in the previous sentence. Making such a connection between references to the same entity is called co-reference resolution, or anaphora resolution.1 While this problem 1 In the context of pronouns, anaphora resolution and coreference resolution are identical, but they differ in other contexts. 252 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pa"
W10-1737,J00-4006,0,\N,Missing
W10-1737,P95-1017,0,\N,Missing
W10-1737,P87-1022,0,\N,Missing
W10-1737,sagot-etal-2006-lefff,0,\N,Missing
W10-1737,M98-1029,0,\N,Missing
W10-1756,D08-1023,0,0.0244609,"Missing"
W10-1756,D08-1064,0,0.088056,"While our initial formulation of minimum risk training is similar to that of Arun et al. (2009), in preliminary experiments we observed a tendency for translation performance on held-out data to quickly increase to a maximum and then plateau. Hypothesizing that we were being trapped in local maxima as G is non-convex, we decided to 4.2 Corpus sampling While the objective functions in Equations 5 and 4.1 use a sentence-level variant of BLEU, the model’s test-time performance is evaluated with corpus level BLEU. The lack of correlation between sentence-level BLEU and corpus BLEU is well-known (Chiang et al., 2008a). Therefore, in an effort to address this issue, we tried maximizing expected corpus BLEU directly. In other words, given a training corpus of the form hCF , CEˆ i where CF is a set of source sentences and CEˆ its corresponding reference translations, we consider a gain function defined on the 367 hypothesized translation CE of the input CF with respect to CEˆ . The objective in equation 5 therefore becomes: G= X P (CE |CF )BLEUCEˆ (CE ) f1 f2 f3 SAMPLE FROM P(e,a |f) (7) CE The pair (CE , CF ) is denoted as a corpus sample corresponding to a sequence (e1 , a1 ), . . . , (eN , aN ) of deriva"
W10-1756,D08-1024,0,0.287469,"While our initial formulation of minimum risk training is similar to that of Arun et al. (2009), in preliminary experiments we observed a tendency for translation performance on held-out data to quickly increase to a maximum and then plateau. Hypothesizing that we were being trapped in local maxima as G is non-convex, we decided to 4.2 Corpus sampling While the objective functions in Equations 5 and 4.1 use a sentence-level variant of BLEU, the model’s test-time performance is evaluated with corpus level BLEU. The lack of correlation between sentence-level BLEU and corpus BLEU is well-known (Chiang et al., 2008a). Therefore, in an effort to address this issue, we tried maximizing expected corpus BLEU directly. In other words, given a training corpus of the form hCF , CEˆ i where CF is a set of source sentences and CEˆ its corresponding reference translations, we consider a gain function defined on the 367 hypothesized translation CE of the input CF with respect to CEˆ . The objective in equation 5 therefore becomes: G= X P (CE |CF )BLEUCEˆ (CE ) f1 f2 f3 SAMPLE FROM P(e,a |f) (7) CE The pair (CE , CF ) is denoted as a corpus sample corresponding to a sequence (e1 , a1 ), . . . , (eN , aN ) of deriva"
W10-1756,J07-2003,0,0.0731257,"ebatable (Table 1), running Moses with minimum risk trained weights gives results that are in line with what we would expect - lattice MBR does systematically better than competing decoding algorithms. This suggests that the unbi7 Related Work Expected BLEU training for phrase-based models has been successfully attempted by (Smith and Eisner, 2006; Zens et al., 2007), however they both used biased n-best lists to approximate the posterior distribution. Li and Eisner (2009) present work on performing expected BLEU training with deterministic annealing on translation forests generated by Hiero (Chiang, 2007). Since BLEU does not factorize over the search graph, they use the linear approximation of Tromble et al. (2008) instead. Pauls et al. (2009) present an alternate training criterion over translation forests called CoBLEU, 8 372 up to 1081 as per Tromble et al. (2008) similar in spirit to expected BLEU training, but aimed to maximize the expected counts of n-grams appearing in reference translations. This training criterion is used in conjunction with consensus decoding (DeNero et al., 2009), a linear-time approximation of MBR. In contrast to the approaches above, the algorithms presented in t"
W10-1756,P09-1064,0,0.0820261,"Missing"
W10-1756,D07-1103,0,0.0254345,"Missing"
W10-1756,N03-1017,1,0.0361222,"s samples from the posterior distribution of the translation model. For the work presented in this paper, we use this sampler. The sampler produces a sequence of samples, S1N = (e1 , a1 ) . . . (eN , aN ), that are drawn from the distribution p(e, a|f ). These samples can be used to estimate the expectation of a function h(e, a, f ) as follows: N 1 X Ep(a,e|f ) [h] = lim h(ai , ei , f ) (3) N →∞ N Inference methods for MT i=1 3 We assume a phrase-based machine translation model, defined with a log-linear form, with feature function vector h and parametrized by weight vector θ, as described in Koehn et al. (2003). The input sentence, f , is segmented into phrases, which are sequences of adjacent words. Each source phrase is translated into the target language, to produce an output sentence e and an alignment a representing the mapping from source to target phrases. Phrases are allowed to be reordered. p(e, a|f ; θ) = P Gibbs sampling for phrase-based MT Decoding In this work, we are interested in performing MBR decoding with BLEU. We define the MBR decision rule following Tromble et al. (2008): X 0 0 e∗ = arg max BLEU e (e )p(e |f ) (4) e∈H e0 ∈E where H refers to the hypothesis space from which tr"
W10-1756,P07-2045,1,0.0173102,"Missing"
W10-1756,N04-1022,0,0.0315341,"Missing"
W10-1756,P09-1019,0,0.333748,"mming over the translations in the list. The second step is to find the correct scale factor for the scores using a hyper-parameter search over held-out data. This is needed because the model parameters for the first-pass decoder are normally learnt using MERT (Och, 2003), which is invariant under scaling of the scores. Both these steps are theoretically unsatisfactory methods of estimating the posterior probability distribution since the approximation to Z is an unbounded term and the scaling factor is an artificial way of inducing a probability distribution. Recently, (Tromble et al., 2008; Kumar et al., 2009) have shown that using a search lattice to improve the estimation of the true probability distribution can lead to improved MBR performance. However, these approaches still rely on MERT for training the base model, and in fact introduce several extra parameters which must also be estimated using either grid search or a second MERT run. The lattice pruning required to make these techniques tractable is quite drastic, and is in addition to the pruning already performed during the search. Such extensive pruning is liable to render any probability estimates heavily biased (Blunsom and Osborne, 200"
W10-1756,W09-1114,1,0.574952,"ond MERT run. The lattice pruning required to make these techniques tractable is quite drastic, and is in addition to the pruning already performed during the search. Such extensive pruning is liable to render any probability estimates heavily biased (Blunsom and Osborne, 2008; Bouchard-Cˆot´e et al., 2009). Here, we present a unified approach to training and decoding in a phrase-based translation model (Koehn et al., 2003) which keeps the objective constant across the translation pipeline and so obviates the need for any extra hyper-parameter fitting. We use the phrase-based Gibbs sampler of Arun et al. (2009) at training time to compute the gradient of our minimum risk training objective in order to apply first-order optimization techniques, We present a unified approach to performing minimum risk training and minimum Bayes risk (MBR) decoding with BLEU in a phrase-based model. Key to our approach is the use of a Gibbs sampler that allows us to explore the entire probability distribution and maintain a strict probabilistic formulation across the pipeline. We also describe a new sampling algorithm called corpus sampling which allows us at training time to use BLEU instead of an approximation thereo"
W10-1756,D09-1005,0,0.730952,"adually lowered as the optimization progresses according to some annealing schedule. Differentiating with respect to θk then shows that the annealed gradient is given by the following expression: XX ∂p (BLEUeˆ(e) − T (1 + log p)) ∂θ k e,a where  ∂p = hk − Ep(e,a|f ) [hk ] p(e, a|f ) ∂θk A high value of T leads the optimizer to find weights which describe a fairly flat distribution, whereas a lower value of T pushes the optimizer towards a more peaked distribution. We perform 10 to 20 iterations of SGD at each temperature. In their deterministic annealing formulation, (Smith and Eisner, 2006; Li and Eisner, 2009), express the parameterization of the distribution θ as γ θˆ (where γ is the scaling factor) and perform optimization in two steps, the first optimizing θˆ and the second optimizing γ. We experimented with this two stage optimization process, but found that simply performing an unconstrained optimization on θ gave better results. (6)  ∂p = hk − Ep(e,a|f ) [hk ] p(e, a|f ) ∂θk Since the gradient is expressed in terms of expectations of feature values, it can easily be calculated using the sampler and then first-order optimization techniques can be applied to find optimal values of θ. Because o"
W10-1756,P09-1067,0,0.0449904,"eover, experimental evidence on three language pairs shows that our training regime is more stable than MERT, able to generalize better and generally leads to improvement in translation when used with sampling based MBR (Section 5). An added benefit is that the trained weights also lead to better performance when used with a beam-search based decoder. 2 by the Viterbi maximum with respect to the true model maximum is unbounded. Second, the DP solution requires substantial pruning and restricts the use of non-local features. The latter problem persists even in the variational approximations of Li et al. (2009), which attempt to solve the former. 2.1 An alternate approximate inference method for phrase-based MT without any of the previously mentioned drawbacks is the Gibbs sampler (Geman and Geman, 1984) of Arun et al. (2009) which draws samples from the posterior distribution of the translation model. For the work presented in this paper, we use this sampler. The sampler produces a sequence of samples, S1N = (e1 , a1 ) . . . (eN , aN ), that are drawn from the distribution p(e, a|f ). These samples can be used to estimate the expectation of a function h(e, a, f ) as follows: N 1 X Ep(a,e|f ) [h] ="
W10-1756,P03-1021,0,0.13736,"Missing"
W10-1756,P02-1040,0,0.087488,"Missing"
W10-1756,D09-1147,0,0.329001,"BR does systematically better than competing decoding algorithms. This suggests that the unbi7 Related Work Expected BLEU training for phrase-based models has been successfully attempted by (Smith and Eisner, 2006; Zens et al., 2007), however they both used biased n-best lists to approximate the posterior distribution. Li and Eisner (2009) present work on performing expected BLEU training with deterministic annealing on translation forests generated by Hiero (Chiang, 2007). Since BLEU does not factorize over the search graph, they use the linear approximation of Tromble et al. (2008) instead. Pauls et al. (2009) present an alternate training criterion over translation forests called CoBLEU, 8 372 up to 1081 as per Tromble et al. (2008) similar in spirit to expected BLEU training, but aimed to maximize the expected counts of n-grams appearing in reference translations. This training criterion is used in conjunction with consensus decoding (DeNero et al., 2009), a linear-time approximation of MBR. In contrast to the approaches above, the algorithms presented in this paper are able to explore an unpruned search space. By using corpus sampling, we can perform minimum risk training with corpus BLEU rather"
W10-1756,P06-2101,0,0.732419,"ion he∗ , a∗ i. This approximation can be computed in polynomial time via dynamic programming (DP). Though fast and effective for many problems, it has two serious drawbacks for probabilistic inference. First, the error incurred 2 The ngram precision counts are smoothed by adding 0.01 for n > 1 366 4 Minimum Risk Training employ deterministic annealing (Rose, 1998) to smooth the objective function to ensure that the optimizer explored as large a region as possible of the space before it settled on an optimal weight set. Our instantiation of deterministic annealing (DA) is based on the work of Smith and Eisner (2006), and involves the addition of an entropic prior to the objective""in Equation 5 to give ! # X X Gˆ = p(e, a|f )BLEUeˆ(e) + T.H(p) In order to train models suitable for use with MaxTrans or MBR decoding, we need to employ a training method which takes account of the whole distribution. To this end, we employ minimum risk training to find weights θ for Equation 1 that minimize the expected loss on the training set. We consider two variants of minimum risk training: sentence sampling optimizes an objective defined at the sentence level and corpus sampling a corpusbased objective. 4.1 hˆ e,f i∈D S"
W10-1756,D08-1065,0,0.306714,"ned with a log-linear form, with feature function vector h and parametrized by weight vector θ, as described in Koehn et al. (2003). The input sentence, f , is segmented into phrases, which are sequences of adjacent words. Each source phrase is translated into the target language, to produce an output sentence e and an alignment a representing the mapping from source to target phrases. Phrases are allowed to be reordered. p(e, a|f ; θ) = P Gibbs sampling for phrase-based MT Decoding In this work, we are interested in performing MBR decoding with BLEU. We define the MBR decision rule following Tromble et al. (2008): X 0 0 e∗ = arg max BLEU e (e )p(e |f ) (4) e∈H e0 ∈E where H refers to the hypothesis space from which translations are chosen, E refers to the evidence space used for calculating risk and BLEU e (e0 ) is a gain function that indicates the reward of hypothesising e0 when the reference solution is e. To perform MBR decoding using the sampler, let the function h in Equation 3 be the indicator function h = δ(a, a ˆ)δ(e, eˆ). Then, Equation 3 provides an estimate of p(ˆ a, eˆ|f ), and using h = δ(e, eˆ) marginalizes over all derivations a0 , yielding an estimate of p(ˆ e|f ). MBR is computed"
W10-1756,D07-1055,0,0.34657,"aining over an unpruned search space. Our proposed corpus sampling technique, like MERT, is able to optimize corpus BLEU directly whereas alternate parameter estimation techniques usually employed in SMT optimize approximations of BLEU . Chiang et al. (2008b) accounts for the online nature of the MIRA optimization algorithm by smoothing the sentence-level BLEU precision counts of a translation with a weighted average of the precision counts of previously decoded sentences, thus approximating corpus BLEU. As for minimum risk training, prior implementations have either used sentence-level BLEU (Zens et al., 2007) or a linear approximation to BLEU (Smith and Eisner, 2006; Li and Eisner, 2009). At test time, the sampler works best as an MBR decoder, but also allows us to verify past claims about the benefits of marginalizing over alignments during decoding. We compare the sampler MBR decoder’s performance against MERToptimized Moses run under three different decoding regimes, finding that the sampler does as well or better on 4 out of 5 datasets. Our training and testing pipeline has the advantage of being able to handle a large number of both local and global features so we expect in the future to outp"
W10-1756,P08-1024,0,\N,Missing
W10-1761,2008.amta-srw.1,0,0.0996665,"ngs of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 409–417, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics rule extraction to reduce spurious ambiguity. The resulting translation model does reduces spurious ambiguity but also reduces the search space in an arbitrary manner which adversely affects translation quality. Syntactic labels from parse trees can be used to annotate non-terminals in the translation model. This reduces incorrect rule application by restricting rule extraction and application. However, as noted in (Ambati and Lavie, 2008) and elsewhere,the na¨ıve approach of constraining every non-terminal to a syntactic constituent severely limits the coverage of the resulting grammar, therefore, several approaches have been used to improve coverage when using syntactic information. Zollmann and Venugopal (2006) allow rules to be extracted where non-terminals do not exactly span a target constituent. The non-terminals are then labeled with complex labels which amalgamates multiple labels in the span. This increase coverage at the expense of increasing data sparsity as the non-terminal symbol set increases dramatically. Huang"
W10-1761,2009.mtsummit-posters.2,0,0.0148384,"res. Most SMT systems uses the Viterbi approximation whereby the derivations in the log-linear model is not marginalized, but the maximum derivation is returned. String-to-tree models build on this so that the most probable derivation, including syntactic labels, is assumed to the most probable translation. This fragments the derivation probability and the further partition the search space, leading to pruning errors. Venugopal et al. (2009) attempts to address this by efficiently estimating the score over an equivalent unlabeled derivation from a target syntax model. Ambati and Lavie (2008); Ambati et al. (2009) notes that tree-to-tree often underperform models with parse tree only on one side due to the nonisomorphic structure of languages. This motivates the creation of an isomorphic backbone into the target parse tree, while leaving the source parse unchanged. 3 Model In extending the phrase-based model to the hierarchical model, non-terminals are used in translation rules to denote subphrases. Hierarchical nonterminals are undecorated so are unrestricted to the span they cover. In contrast, SCFG-based syntactic models restrict the extraction and application of non-terminals, typically to constitu"
W10-1761,P05-1033,0,0.941452,"mechanism for both hierarchical and syntactic systems are identical and the rule extraction are similar. Hierarchical and syntax statistical machine translation have made great progress in the last few years and can claim to represent the state of the art in the field. Both use synchronous context free grammar (SCFG) formalism, consisting of rewrite rules which simultaneously parse the input sentence and generate the output sentence. The most common algorithm for decoding with SCFG is currently CKY+ with cube pruning works for both hierarchical and syntactic systems, as implemented in Hiero (Chiang, 2005), Joshua (Li et al., 2009), and Moses (Hoang et al., 2009) Rewrite rules in hierarchical systems have general applicability as their non-terminals are undecorated, giving hierarchical system broad coverage. However, rules may be used in inappropriate situations without the labeled constraints. The general applicability of undecorated rules create spurious ambiguity which decreases translation performance by causing the decoder to spend more time sifting through duplicate hypotheses. Syntactic systems makes use of linguistically motivated information to bias the search space at the expense of l"
W10-1761,D08-1024,0,0.117709,"ng good SMT systems, but again, the rules syntax are strictly constrained by a parser. Others have sought to add soft linguistic constraints to hierarchical models using addition feature functions. Marton and Resnik (2008) add feature functions to penalize or reward non-terminals which cross constituent boundaries of the source sentence. This follows on from earlier work in (Chiang, 2005) but they see gains when finer grain feature functions which different constituency types. The weights for feature function is tuned in batches due to the deficiency of MERT when presented with many features. Chiang et al. (2008) rectified this deficiency by using the MIRA to tune all feature function weights in combination. However, the translation model continues to be hierarchical. Chiang et al. (2009) added thousands of linguistically-motivated features to hierarchical and syntax systems, however, the source syntax features are derived from the research above. The translation model remain constant but the parameterization changes. Shen et al. (2009) discusses soft syntax constraints and context features in a dependency tree translation model. The POS tag of the target head word is used as a soft constraint when ap"
W10-1761,P08-1115,0,0.0219174,"ring translation using a synchronous CFG that constrain the application of non-terminals to matching source span labels. The source words and span labels are represented as an unweighted word lattice, < V, E &gt;, where each edge in the lattice correspond to a word or non-terminal label over the corresponding source span. In the soft Figure 1: Aligned parsed sentence syntax experiments, edges with the default source label, X, are also created for all spans. Nodes in the lattice represent word positions in the sentence. We encode the lattice in a chart, as described → M usharraf s letzter Akt in (Dyer et al., 2008). A chart is is a tuple of 20 # M usharraf s Last Act dimensional matrices < F, R &gt;. Fi,j is the word → N E1 letzter Akt # X1 Last Act or non-terminal label of the j th transition starting → N E1 ADJA2 Akt # X1 X2 Act word position i. Ri,j is the end word position of the node on the right of the j th transition leaving → N E1 letzter N N2 # X1 Last X2 → N E1 ADJA2 Akt ? # X1 X2 Act ? word position i. The input sentence is decoded with a set of translation rules of the form Hierarchical style rules are also extracted where the span doesn’t exactly match a parse constituent. We list 2 below. X →"
W10-1761,P06-1121,0,0.0484414,"ts of source tree fragments and target languages strings. During decoding, a packed forest of the source sentence is used as input, the production rule tree fragments are applied to the packed forest. Liu et al. (2009) uses joint decoding with a hierarchical and tree-to-string model and find that translation performance increase for a Chinese-English task. Galley et al. (2004) creates minimal translation rules which can explain a parallel sentence pair but the rules generated are not optimized to produce good translations or coverage in any SMT system. This work was extended and described in (Galley et al., 2006) which creates rules composed of smaller, minimal rules, as well as dealing with unaligned words. These measures are essential for creating good SMT systems, but again, the rules syntax are strictly constrained by a parser. Others have sought to add soft linguistic constraints to hierarchical models using addition feature functions. Marton and Resnik (2008) add feature functions to penalize or reward non-terminals which cross constituent boundaries of the source sentence. This follows on from earlier work in (Chiang, 2005) but they see gains when finer grain feature functions which different c"
W10-1761,N04-1035,0,0.0569148,"ltiple labels in the span. This increase coverage at the expense of increasing data sparsity as the non-terminal symbol set increases dramatically. Huang and Chiang (2008) use parse information of the source language, production rules consists of source tree fragments and target languages strings. During decoding, a packed forest of the source sentence is used as input, the production rule tree fragments are applied to the packed forest. Liu et al. (2009) uses joint decoding with a hierarchical and tree-to-string model and find that translation performance increase for a Chinese-English task. Galley et al. (2004) creates minimal translation rules which can explain a parallel sentence pair but the rules generated are not optimized to produce good translations or coverage in any SMT system. This work was extended and described in (Galley et al., 2006) which creates rules composed of smaller, minimal rules, as well as dealing with unaligned words. These measures are essential for creating good SMT systems, but again, the rules syntax are strictly constrained by a parser. Others have sought to add soft linguistic constraints to hierarchical models using addition feature functions. Marton and Resnik (2008)"
W10-1761,2009.iwslt-papers.4,1,0.81015,"ms are identical and the rule extraction are similar. Hierarchical and syntax statistical machine translation have made great progress in the last few years and can claim to represent the state of the art in the field. Both use synchronous context free grammar (SCFG) formalism, consisting of rewrite rules which simultaneously parse the input sentence and generate the output sentence. The most common algorithm for decoding with SCFG is currently CKY+ with cube pruning works for both hierarchical and syntactic systems, as implemented in Hiero (Chiang, 2005), Joshua (Li et al., 2009), and Moses (Hoang et al., 2009) Rewrite rules in hierarchical systems have general applicability as their non-terminals are undecorated, giving hierarchical system broad coverage. However, rules may be used in inappropriate situations without the labeled constraints. The general applicability of undecorated rules create spurious ambiguity which decreases translation performance by causing the decoder to spend more time sifting through duplicate hypotheses. Syntactic systems makes use of linguistically motivated information to bias the search space at the expense of limiting model coverage. 2 Past Work Hierarchical machine t"
W10-1761,D08-1022,0,0.0203274,"2008) and elsewhere,the na¨ıve approach of constraining every non-terminal to a syntactic constituent severely limits the coverage of the resulting grammar, therefore, several approaches have been used to improve coverage when using syntactic information. Zollmann and Venugopal (2006) allow rules to be extracted where non-terminals do not exactly span a target constituent. The non-terminals are then labeled with complex labels which amalgamates multiple labels in the span. This increase coverage at the expense of increasing data sparsity as the non-terminal symbol set increases dramatically. Huang and Chiang (2008) use parse information of the source language, production rules consists of source tree fragments and target languages strings. During decoding, a packed forest of the source sentence is used as input, the production rule tree fragments are applied to the packed forest. Liu et al. (2009) uses joint decoding with a hierarchical and tree-to-string model and find that translation performance increase for a Chinese-English task. Galley et al. (2004) creates minimal translation rules which can explain a parallel sentence pair but the rules generated are not optimized to produce good translations or"
W10-1761,P07-2045,1,0.0194604,"ces Words Sentences Sentences Syntactic translation German English 82,306 2,034,373 1,965,325 2000 1026 7.2 Reachability The increased performance using the soft syntax model can be partially explained by studying the effect of changes to the extraction and decodTable 1: Training, tuning, and test conditions ing algorithms has to the capacity of the translation pipeline. We run some analysis in which we The training corpus was cleaned and filtered ustrained the phrase models with a corpus of one ing standard methods found in the Moses toolkit sentence and attempt to decode the same sentence. (Koehn et al., 2007) and aligned using GIZA++ Pruning and recombination were disabled during (Och and Ney, 2003). Standard MERT weight tundecoding to negate the effect of language model ing was used throughout. The English half of the context and model scores. training data was also used to create a trigram lanThe first thousand sentences of the training corguage model which was used for each experiment. pus was analyzed, Table 3. The hierarchical model All experiments use truecase data and results are successfully decode over half of the sentences reported in case-sensitive BLEU scores (Papineni while a translat"
W10-1761,W09-0424,0,0.0134167,"rarchical and syntactic systems are identical and the rule extraction are similar. Hierarchical and syntax statistical machine translation have made great progress in the last few years and can claim to represent the state of the art in the field. Both use synchronous context free grammar (SCFG) formalism, consisting of rewrite rules which simultaneously parse the input sentence and generate the output sentence. The most common algorithm for decoding with SCFG is currently CKY+ with cube pruning works for both hierarchical and syntactic systems, as implemented in Hiero (Chiang, 2005), Joshua (Li et al., 2009), and Moses (Hoang et al., 2009) Rewrite rules in hierarchical systems have general applicability as their non-terminals are undecorated, giving hierarchical system broad coverage. However, rules may be used in inappropriate situations without the labeled constraints. The general applicability of undecorated rules create spurious ambiguity which decreases translation performance by causing the decoder to spend more time sifting through duplicate hypotheses. Syntactic systems makes use of linguistically motivated information to bias the search space at the expense of limiting model coverage. 2"
W10-1761,P09-1065,0,0.0286584,"ules to be extracted where non-terminals do not exactly span a target constituent. The non-terminals are then labeled with complex labels which amalgamates multiple labels in the span. This increase coverage at the expense of increasing data sparsity as the non-terminal symbol set increases dramatically. Huang and Chiang (2008) use parse information of the source language, production rules consists of source tree fragments and target languages strings. During decoding, a packed forest of the source sentence is used as input, the production rule tree fragments are applied to the packed forest. Liu et al. (2009) uses joint decoding with a hierarchical and tree-to-string model and find that translation performance increase for a Chinese-English task. Galley et al. (2004) creates minimal translation rules which can explain a parallel sentence pair but the rules generated are not optimized to produce good translations or coverage in any SMT system. This work was extended and described in (Galley et al., 2006) which creates rules composed of smaller, minimal rules, as well as dealing with unaligned words. These measures are essential for creating good SMT systems, but again, the rules syntax are strictly"
W10-1761,P08-1114,0,0.159418,"sk. Galley et al. (2004) creates minimal translation rules which can explain a parallel sentence pair but the rules generated are not optimized to produce good translations or coverage in any SMT system. This work was extended and described in (Galley et al., 2006) which creates rules composed of smaller, minimal rules, as well as dealing with unaligned words. These measures are essential for creating good SMT systems, but again, the rules syntax are strictly constrained by a parser. Others have sought to add soft linguistic constraints to hierarchical models using addition feature functions. Marton and Resnik (2008) add feature functions to penalize or reward non-terminals which cross constituent boundaries of the source sentence. This follows on from earlier work in (Chiang, 2005) but they see gains when finer grain feature functions which different constituency types. The weights for feature function is tuned in batches due to the deficiency of MERT when presented with many features. Chiang et al. (2008) rectified this deficiency by using the MIRA to tune all feature function weights in combination. However, the translation model continues to be hierarchical. Chiang et al. (2009) added thousands of lin"
W10-1761,P08-1023,0,0.13803,"dges between every node allows undecorated non-terminals to be applied to any span, allowing flexibility in the translation model. This contrasts with the approach by (Zollmann and Venugopal, 2006) in attempting to improve the coverage of syntactic translation. Rather than creating ad-hoc schemes to categories non-terminals with syntactic labels when they do not span syntactic constituencies, we only use labels that are presented by the parser or shallow tagger. Nor do we try to expand the space where rules can apply by propagating uncertainty from the parser in building input forests, as in (Mi et al., 2008), but we build ambiguity into the translation rule. The model also differs from (Marton and Resnik, 2008; Chiang et al., 2008, 2009) by adding informative labels to rule non-terminals and requiring them to match the source span label. The soft constraint in our model pertain not to a additional feature functions based on syntactic information, but to the availability of syntactic and nonsyntactic informed rules. 4 5 Rule extraction follows the algorithm described in (Chiang, 2005). We note the heuristics used for hierarchical phrases extraction include the following constraints: 1. all rules m"
W10-1761,P03-1021,0,0.0214959,"+ 1] X → ADJA1 Akt # X1 Act N P → N E1 X2 # X1 X2 T OP → N E1 letzter X2 # X1 Last X2 Non-Terminal Symbol [X → α • Fj,k βLs , i, j] [X, j, Rj,k ] [X → αFj,k • βLs , i, Rj,k ] 411 not change the problem due to the monotonicity of the log function (hm = log h0m ) X λm hm (t, s) (3) log p(t|s) = Left Hand Side [X → α • Ls , i, Ri,j ] [Fi,j = Ls ] [X → αLs •, i, Ri,j ] m Goal An advantage of our model over (Marton and Resnik, 2008; Chiang et al., 2008, 2009) is the number of feature functions remains the same, therefore, the tuning algorithm does not need to be replaced; we continue to use MERT (Och, 2003). [X → αLs •, 0, |V |− 1] This model allows translation rules to take advantage of both syntactic label and word context. The presence of default label edges between every node allows undecorated non-terminals to be applied to any span, allowing flexibility in the translation model. This contrasts with the approach by (Zollmann and Venugopal, 2006) in attempting to improve the coverage of syntactic translation. Rather than creating ad-hoc schemes to categories non-terminals with syntactic labels when they do not span syntactic constituencies, we only use labels that are presented by the parser"
W10-1761,J03-1002,0,0.00418903,"2000 1026 7.2 Reachability The increased performance using the soft syntax model can be partially explained by studying the effect of changes to the extraction and decodTable 1: Training, tuning, and test conditions ing algorithms has to the capacity of the translation pipeline. We run some analysis in which we The training corpus was cleaned and filtered ustrained the phrase models with a corpus of one ing standard methods found in the Moses toolkit sentence and attempt to decode the same sentence. (Koehn et al., 2007) and aligned using GIZA++ Pruning and recombination were disabled during (Och and Ney, 2003). Standard MERT weight tundecoding to negate the effect of language model ing was used throughout. The English half of the context and model scores. training data was also used to create a trigram lanThe first thousand sentences of the training corguage model which was used for each experiment. pus was analyzed, Table 3. The hierarchical model All experiments use truecase data and results are successfully decode over half of the sentences reported in case-sensitive BLEU scores (Papineni while a translation model constrained by a source et al., 2001). syntax parse tree manages only 113 sentence"
W10-1761,2001.mtsummit-papers.68,0,0.0558405,"Missing"
W10-1761,C00-2105,0,0.0777978,"Missing"
W10-1761,D09-1008,0,0.0246083,"e functions which different constituency types. The weights for feature function is tuned in batches due to the deficiency of MERT when presented with many features. Chiang et al. (2008) rectified this deficiency by using the MIRA to tune all feature function weights in combination. However, the translation model continues to be hierarchical. Chiang et al. (2009) added thousands of linguistically-motivated features to hierarchical and syntax systems, however, the source syntax features are derived from the research above. The translation model remain constant but the parameterization changes. Shen et al. (2009) discusses soft syntax constraints and context features in a dependency tree translation model. The POS tag of the target head word is used as a soft constraint when applying rules. Also, a source context language model and a dependency language model are also used as features. Most SMT systems uses the Viterbi approximation whereby the derivations in the log-linear model is not marginalized, but the maximum derivation is returned. String-to-tree models build on this so that the most probable derivation, including syntactic labels, is assumed to the most probable translation. This fragments th"
W10-1761,N09-1027,0,0.0858438,"The POS tag of the target head word is used as a soft constraint when applying rules. Also, a source context language model and a dependency language model are also used as features. Most SMT systems uses the Viterbi approximation whereby the derivations in the log-linear model is not marginalized, but the maximum derivation is returned. String-to-tree models build on this so that the most probable derivation, including syntactic labels, is assumed to the most probable translation. This fragments the derivation probability and the further partition the search space, leading to pruning errors. Venugopal et al. (2009) attempts to address this by efficiently estimating the score over an equivalent unlabeled derivation from a target syntax model. Ambati and Lavie (2008); Ambati et al. (2009) notes that tree-to-tree often underperform models with parse tree only on one side due to the nonisomorphic structure of languages. This motivates the creation of an isomorphic backbone into the target parse tree, while leaving the source parse unchanged. 3 Model In extending the phrase-based model to the hierarchical model, non-terminals are used in translation rules to denote subphrases. Hierarchical nonterminals are u"
W10-1761,W06-3119,0,0.615121,"rious ambiguity but also reduces the search space in an arbitrary manner which adversely affects translation quality. Syntactic labels from parse trees can be used to annotate non-terminals in the translation model. This reduces incorrect rule application by restricting rule extraction and application. However, as noted in (Ambati and Lavie, 2008) and elsewhere,the na¨ıve approach of constraining every non-terminal to a syntactic constituent severely limits the coverage of the resulting grammar, therefore, several approaches have been used to improve coverage when using syntactic information. Zollmann and Venugopal (2006) allow rules to be extracted where non-terminals do not exactly span a target constituent. The non-terminals are then labeled with complex labels which amalgamates multiple labels in the span. This increase coverage at the expense of increasing data sparsity as the non-terminal symbol set increases dramatically. Huang and Chiang (2008) use parse information of the source language, production rules consists of source tree fragments and target languages strings. During decoding, a packed forest of the source sentence is used as input, the production rule tree fragments are applied to the packed"
W10-1761,P02-1040,0,\N,Missing
W10-1761,W06-1607,0,\N,Missing
W10-1761,N09-1025,0,\N,Missing
W10-1761,D08-1076,0,\N,Missing
W11-2103,W11-2134,0,0.0605238,"Missing"
W11-2103,W11-2137,0,0.0520272,"Missing"
W11-2103,W11-2138,0,0.0321994,"Missing"
W11-2103,W07-0718,1,0.805048,"el to publish a paper about their experience developing translation technology in response to the crisis (Lewis et al., 2011). They provided the data used in the Haitian Creole featured translation task. We hope that the introduction of this new dataset will provide a testbed for dealing with low resource languages and the informal language usage found in SMS messages. Introduction This paper presents the results of the shared tasks of the Workshop on statistical Machine Translation (WMT), which was held at EMNLP 2011. This workshop builds on five previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007; Callison-Burch et al., 2008; Callison-Burch et al., 2009; Callison-Burch et al., 2010). The workshops feature three shared tasks: a translation task between English and other languages, a task to combine the output of multiple machine translation systems, and a task to predict human judgments of translation quality using automatic evaluation metrics. The performance for each of these shared tasks is determined through a comprehensive human eval• Tunable metric shared task – We conducted a pilot of a new shared task to use evaluation metrics to tune the parameters of a machine translation sys"
W11-2103,W08-0309,1,0.779572,"heir experience developing translation technology in response to the crisis (Lewis et al., 2011). They provided the data used in the Haitian Creole featured translation task. We hope that the introduction of this new dataset will provide a testbed for dealing with low resource languages and the informal language usage found in SMS messages. Introduction This paper presents the results of the shared tasks of the Workshop on statistical Machine Translation (WMT), which was held at EMNLP 2011. This workshop builds on five previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007; Callison-Burch et al., 2008; Callison-Burch et al., 2009; Callison-Burch et al., 2010). The workshops feature three shared tasks: a translation task between English and other languages, a task to combine the output of multiple machine translation systems, and a task to predict human judgments of translation quality using automatic evaluation metrics. The performance for each of these shared tasks is determined through a comprehensive human eval• Tunable metric shared task – We conducted a pilot of a new shared task to use evaluation metrics to tune the parameters of a machine translation system. Although previous worksh"
W11-2103,W09-0401,1,0.532823,"anslation technology in response to the crisis (Lewis et al., 2011). They provided the data used in the Haitian Creole featured translation task. We hope that the introduction of this new dataset will provide a testbed for dealing with low resource languages and the informal language usage found in SMS messages. Introduction This paper presents the results of the shared tasks of the Workshop on statistical Machine Translation (WMT), which was held at EMNLP 2011. This workshop builds on five previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007; Callison-Burch et al., 2008; Callison-Burch et al., 2009; Callison-Burch et al., 2010). The workshops feature three shared tasks: a translation task between English and other languages, a task to combine the output of multiple machine translation systems, and a task to predict human judgments of translation quality using automatic evaluation metrics. The performance for each of these shared tasks is determined through a comprehensive human eval• Tunable metric shared task – We conducted a pilot of a new shared task to use evaluation metrics to tune the parameters of a machine translation system. Although previous workshops have shown evaluation met"
W11-2103,W10-1703,1,0.633993,"nse to the crisis (Lewis et al., 2011). They provided the data used in the Haitian Creole featured translation task. We hope that the introduction of this new dataset will provide a testbed for dealing with low resource languages and the informal language usage found in SMS messages. Introduction This paper presents the results of the shared tasks of the Workshop on statistical Machine Translation (WMT), which was held at EMNLP 2011. This workshop builds on five previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007; Callison-Burch et al., 2008; Callison-Burch et al., 2009; Callison-Burch et al., 2010). The workshops feature three shared tasks: a translation task between English and other languages, a task to combine the output of multiple machine translation systems, and a task to predict human judgments of translation quality using automatic evaluation metrics. The performance for each of these shared tasks is determined through a comprehensive human eval• Tunable metric shared task – We conducted a pilot of a new shared task to use evaluation metrics to tune the parameters of a machine translation system. Although previous workshops have shown evaluation metrics other than BLEU are more"
W11-2103,W11-2105,0,0.0708412,"Missing"
W11-2103,W11-2140,0,0.0631771,"Missing"
W11-2103,W11-2141,0,0.049057,"Missing"
W11-2103,W97-0409,0,0.103469,"o’s crowdsourcing translation efforts, the Microsoft Translator team developed a Haitian Creole statistical machine translation engine from scratch in a compressed timeframe (Lewis, 2010). Despite the impressive number of translations completed by volunteers, machine translation was viewed as a potentially useful tool for higher volume applications or to provide translations of English medical documents into Haitian Creole. The Microsoft Translator team quickly assembled parallel data from a number of sources, including Mission 4636 and from the archives of Carnegie Mellon’s DIPLOMAT project (Frederking et al., 1997). Through a series of rapid prototyping efforts, the team improved their system to deal with non-standard orthography, reduced pronouns, and SMS shorthand. They deployed a functional translation system to relief workers in the field in less than 5 days – impressive even when measured against previous rapid MT development efforts like DARPA’s surprise language exercise (Oard, 2003; Oard and Och, 2003). We were inspired by the efforts of Rob Munro and Will Lewis on translating Haitian Creole in the aftermath of the disaster, so we worked with them to create a featured task at WMT11. We thank the"
W11-2103,2003.mtsummit-papers.37,0,0.0310815,"nto Haitian Creole. The Microsoft Translator team quickly assembled parallel data from a number of sources, including Mission 4636 and from the archives of Carnegie Mellon’s DIPLOMAT project (Frederking et al., 1997). Through a series of rapid prototyping efforts, the team improved their system to deal with non-standard orthography, reduced pronouns, and SMS shorthand. They deployed a functional translation system to relief workers in the field in less than 5 days – impressive even when measured against previous rapid MT development efforts like DARPA’s surprise language exercise (Oard, 2003; Oard and Och, 2003). We were inspired by the efforts of Rob Munro and Will Lewis on translating Haitian Creole in the aftermath of the disaster, so we worked with them to create a featured task at WMT11. We thank them for generously sharing the data they assembled in their own efforts. We invited Rob Munro, Will Lewis, and Stephan Vogel to speak at the workshop on the topic of developing translation technology for future 27 crises, and they recorded their thoughts in an invited publication (Lewis et al., 2011). 3.1 Haitian Creole Data For the WMT11 featured translation task, we anonymized the SMS Haitian Creole"
W11-2103,P03-1021,0,0.527543,"an Creole. The Microsoft Translator team quickly assembled parallel data from a number of sources, including Mission 4636 and from the archives of Carnegie Mellon’s DIPLOMAT project (Frederking et al., 1997). Through a series of rapid prototyping efforts, the team improved their system to deal with non-standard orthography, reduced pronouns, and SMS shorthand. They deployed a functional translation system to relief workers in the field in less than 5 days – impressive even when measured against previous rapid MT development efforts like DARPA’s surprise language exercise (Oard, 2003; Oard and Och, 2003). We were inspired by the efforts of Rob Munro and Will Lewis on translating Haitian Creole in the aftermath of the disaster, so we worked with them to create a featured task at WMT11. We thank them for generously sharing the data they assembled in their own efforts. We invited Rob Munro, Will Lewis, and Stephan Vogel to speak at the workshop on the topic of developing translation technology for future 27 crises, and they recorded their thoughts in an invited publication (Lewis et al., 2011). 3.1 Haitian Creole Data For the WMT11 featured translation task, we anonymized the SMS Haitian Creole"
W11-2103,P02-1040,0,0.09575,"ETEOR -1.3- RANK MT E R ATER , MT E R ATER -P LUS MP 4 IBM 1, MP F, WMP F PARSECONF ROSE , ROSE - POS TESLA - B , TESLA - F, TESLA - M TINE BLEU TER Participant National Research Council Canada (Chen and Kuhn, 2011) Koc¸ University (Bicici and Yuret, 2011) Carnegie Mellon University (Denkowski and Lavie, 2011a) Columbia / ETS (Parton et al., 2011) DFKI (Popovi´c, 2011; Popovi´c et al., 2011) DFKI (Avramidis et al., 2011) The University of Sheffield (Song and Cohn, 2011) National University of Singapore (Dahlmeier et al., 2011) University of Wolverhampton (Rios et al., 2011) provided baseline (Papineni et al., 2002) provided baseline (Snover et al., 2006) Table 11: Participants in the evaluation shared task. For comparison purposes, we include the BLEU and TER metrics as baselines. - 15 SYSTEMS - 17 SYSTEMS EN - ES EN - FR AVERAGE W / O CZ - 22 SYSTEMS EN - DE AVERAGE - 10 SYSTEMS EN - CZ 6.1 System-level correlation for translation out of English TESLA - M .90 .95 .96 .94 TESLA - B .81 .90 .91 .87 MP F .72 .63 .87 .89 .78 .80 WMP F .72 .61 .87 .89 .77 .79 MP 4 IBM 1 -.76 -.91 -.71 -.61 .75 .74 ROSE .65 .41 .90 .86 .71 .73 .70 .72 BLEU .65 .44 .87 .86 AMBER- TI .56 .54 .88 .84 .70 .75 AMBER .56 .53 .87 ."
W11-2103,W11-2111,0,0.0968425,"th other MT evaluation metrics and heuristics that take the reference translations into account. Please refer to the proceedings for papers providing detailed descriptions of all of the metrics. Metric IDs AMBER , AMBER - NL , AMBER - IT F15, F15 G 3 METEOR -1.3- ADQ , METEOR -1.3- RANK MT E R ATER , MT E R ATER -P LUS MP 4 IBM 1, MP F, WMP F PARSECONF ROSE , ROSE - POS TESLA - B , TESLA - F, TESLA - M TINE BLEU TER Participant National Research Council Canada (Chen and Kuhn, 2011) Koc¸ University (Bicici and Yuret, 2011) Carnegie Mellon University (Denkowski and Lavie, 2011a) Columbia / ETS (Parton et al., 2011) DFKI (Popovi´c, 2011; Popovi´c et al., 2011) DFKI (Avramidis et al., 2011) The University of Sheffield (Song and Cohn, 2011) National University of Singapore (Dahlmeier et al., 2011) University of Wolverhampton (Rios et al., 2011) provided baseline (Papineni et al., 2002) provided baseline (Snover et al., 2006) Table 11: Participants in the evaluation shared task. For comparison purposes, we include the BLEU and TER metrics as baselines. - 15 SYSTEMS - 17 SYSTEMS EN - ES EN - FR AVERAGE W / O CZ - 22 SYSTEMS EN - DE AVERAGE - 10 SYSTEMS EN - CZ 6.1 System-level correlation for translation out"
W11-2103,W11-2153,0,0.0551069,"Missing"
W11-2103,W11-2110,0,0.0542786,"Missing"
W11-2103,W11-2154,0,0.0525812,"Missing"
W11-2103,W11-2112,0,0.042178,"Missing"
W11-2103,W11-2158,0,0.072375,"Missing"
W11-2103,W11-2120,0,0.0448017,"Missing"
W11-2103,2006.amta-papers.25,0,0.129527,"-P LUS MP 4 IBM 1, MP F, WMP F PARSECONF ROSE , ROSE - POS TESLA - B , TESLA - F, TESLA - M TINE BLEU TER Participant National Research Council Canada (Chen and Kuhn, 2011) Koc¸ University (Bicici and Yuret, 2011) Carnegie Mellon University (Denkowski and Lavie, 2011a) Columbia / ETS (Parton et al., 2011) DFKI (Popovi´c, 2011; Popovi´c et al., 2011) DFKI (Avramidis et al., 2011) The University of Sheffield (Song and Cohn, 2011) National University of Singapore (Dahlmeier et al., 2011) University of Wolverhampton (Rios et al., 2011) provided baseline (Papineni et al., 2002) provided baseline (Snover et al., 2006) Table 11: Participants in the evaluation shared task. For comparison purposes, we include the BLEU and TER metrics as baselines. - 15 SYSTEMS - 17 SYSTEMS EN - ES EN - FR AVERAGE W / O CZ - 22 SYSTEMS EN - DE AVERAGE - 10 SYSTEMS EN - CZ 6.1 System-level correlation for translation out of English TESLA - M .90 .95 .96 .94 TESLA - B .81 .90 .91 .87 MP F .72 .63 .87 .89 .78 .80 WMP F .72 .61 .87 .89 .77 .79 MP 4 IBM 1 -.76 -.91 -.71 -.61 .75 .74 ROSE .65 .41 .90 .86 .71 .73 .70 .72 BLEU .65 .44 .87 .86 AMBER- TI .56 .54 .88 .84 .70 .75 AMBER .56 .53 .87 .84 .70 .74 AMBER- NL .56 .45 .88 .83 .68"
W11-2103,W11-2113,0,0.0828194,"Missing"
W11-2103,W11-2159,0,0.0580674,"Missing"
W11-2103,W11-2155,0,\N,Missing
W11-2103,2010.amta-papers.7,1,\N,Missing
W11-2103,W11-2152,0,\N,Missing
W11-2103,W11-2150,0,\N,Missing
W11-2103,W10-1719,0,\N,Missing
W11-2103,W10-1718,1,\N,Missing
W11-2103,C08-1109,0,\N,Missing
W11-2103,W11-2144,0,\N,Missing
W11-2103,W11-2148,0,\N,Missing
W11-2103,D11-1035,0,\N,Missing
W11-2103,W11-2147,0,\N,Missing
W11-2103,W11-2161,0,\N,Missing
W11-2103,W11-2108,0,\N,Missing
W11-2103,2010.eamt-1.37,0,\N,Missing
W11-2103,W11-2143,0,\N,Missing
W11-2103,W11-2139,0,\N,Missing
W11-2103,P07-2045,1,\N,Missing
W11-2103,W09-0415,0,\N,Missing
W11-2103,W11-2142,0,\N,Missing
W11-2103,W10-1711,0,\N,Missing
W11-2103,2010.iwslt-evaluation.22,0,\N,Missing
W11-2103,2005.eamt-1.12,0,\N,Missing
W11-2103,W11-2146,0,\N,Missing
W11-2103,W11-2104,0,\N,Missing
W11-2103,W11-2109,0,\N,Missing
W11-2103,W06-3114,1,\N,Missing
W11-2103,W05-0904,0,\N,Missing
W11-2103,W11-2157,0,\N,Missing
W11-2103,W11-2156,0,\N,Missing
W11-2103,W11-2151,0,\N,Missing
W11-2103,W11-2136,0,\N,Missing
W11-2103,W11-2135,0,\N,Missing
W11-2103,W11-2118,0,\N,Missing
W11-2103,W11-2145,0,\N,Missing
W11-2103,W11-2163,0,\N,Missing
W11-2103,W11-2160,1,\N,Missing
W11-2103,W09-0407,0,\N,Missing
W11-2103,D08-1076,0,\N,Missing
W11-2103,W11-2149,0,\N,Missing
W11-2103,W11-2117,0,\N,Missing
W11-2103,W11-2107,0,\N,Missing
W11-2103,W11-2164,0,\N,Missing
W11-2103,W11-2121,0,\N,Missing
W11-2103,W11-2119,0,\N,Missing
W11-2103,W11-2106,0,\N,Missing
W11-2103,W11-2116,0,\N,Missing
W11-2126,W08-0319,0,0.0376392,"Missing"
W11-2126,P05-1033,0,0.029787,"features to values, the values themselves being feature structures. →     AGR   Grammar In this section we describe the synchronous grammar used in our string-to-tree model. Rule extraction is similar to the syntax-augmented model of Zollmann and Venugopal (2006), though we do not use extended categories in this work. We then describe how we extend the grammar with target-side constraints. 3.1 Synchronous Grammar Our translation model is based on a synchronous context-free grammar (SCFG) learned from a parallel corpus. Rule extraction follows the hierarchical phrase-based algorithm of Chiang (2005; 2007). Source non-terminals are given the undistinguished label X, whereas the target non-terminals are given part-of-speech and constituent labels obtained from a parse of the target-side of the parallel corpus. Rules in which the target span is not covered by a parse tree constituent are discarded. Compared with the hierarchical phrase-based model, the restriction to constituent target phrases reduces the total grammar size and the addition of linguistic labels reduces the problem of spurious ambiguity. We therefore relax Chiang’s (2007) rule filtering in the following ways: 1. Up to seven"
W11-2126,J07-2003,0,0.0744065,"e cube ordering is based on the score of the nearest hypothesis to the corner that satisifies its constraints (if any exists). This hypothesis is found by exploring neighbours in order of estimated score (that is, without calculating the full language model score) starting at the corner. Decoding 2. When a hypothesis is popped from a cube and its neighbours created, constraint-failing neighbours are added to a ‘bad neighbours’ queue. We use the Moses (Koehn et al., 2007) decoder, a bottom-up synchronous parser that implements the CYK+ algorithm (Chappelier and Rajman, 1998) with cube pruning (Chiang, 2007). The constraint model requires some changes to decoding, which we briefly describe here: 5.1 Hypothesis State Bottom-up constraint evaluation requires a feature structure set for every rule element that participates in a constraint. For lexical rule elements these are obtained from the lexicon. For non-lexical rule elements these are obtained from predecessor hypotheses. After constraint evaluation, each hypothesis therefore stores the resulting, possibly empty, set of feature structures corresponding to its root rule element. Hypothesis recombination must take these feature structure states"
W11-2126,H05-1085,0,0.031286,"machine translation (SMT) has focused on translation into English. Languages with richer inflectional morphologies pose additional challenges for translation and conventional SMT approaches tend to perform poorly when either source or target language has rich morphology (Koehn, 2005). For complex source inflection, a successful approach has been to cluster inflectional variants into equivalence classes. This removes information that is redundant for translation and can be performed as a preprocessing step for input to a conventional surface form based translation model (Nießen and Ney, 2001; Goldwater and McClosky, 2005; Talbot and Osborne, 2006). For complex target inflection, Minkov et al. (2007) investigate how postprocessing can be used to generate inflection for a system that produces uninflected output. Their approach is successfully applied to English-Arabic and English-Russian systems by Toutanova et al. (2008). Another promising line of research involves the direct integration of linguistic information into SMT models. Koehn and Hoang (2007) generalise the phrase-based model’s representation of the word from a string to a vector, allowing additional features such as part-of-speech and morphology to"
W11-2126,W09-0425,0,0.0459625,"ing pipeline. Departing further from traditional SMT models, the transfer-based systems of Riezler and Maxwell (2006), Bojar and Hajiˇc (2008), and Graham et al. (2009) employ rich feature structure representations for linguistic attributes, but have so far been limited by their dependence on nonstochastic parsers with limited coverage. The StatXFER transfer-based framework (Lavie, 2008) is neutral with regard to the rule acquisition method and the author describes a manually developed Hebrew-English transfer grammar, which includes a small number of constraints between agreement features. In Hanneman et al. (2009) the framework is used with a large automatically-extracted grammar, though this does not use feature constraints. In this paper we propose a model that retains the use of surface forms during decoding whilst also checking linguistic constraints defined over associated feature structures. Specifically, we extend a string-to-tree model by adding unification-based 217 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 217–226, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics constraints to the target-side of the synchronous gramma"
W11-2126,D10-1063,0,0.0734666,"-of-speech and constituent labels obtained from a parse of the target-side of the parallel corpus. Rules in which the target span is not covered by a parse tree constituent are discarded. Compared with the hierarchical phrase-based model, the restriction to constituent target phrases reduces the total grammar size and the addition of linguistic labels reduces the problem of spurious ambiguity. We therefore relax Chiang’s (2007) rule filtering in the following ways: 1. Up to seven source-side terminal / non-terminal elements are allowed. 2. Rules with scope greater than three are filtered out (Hopkins and Langmead, 2010). → die AP Katze h NP - SB AGRi = h die AGRi h NP - SB AGRi = h AP AGRi h NP - SB AGRi = h Katze AGRi h NP - SB AGR CASEi = C h die POSi = ART h Katze POSi = NN    0.990, c = NOM  0.005, c = DAT P (C = c) = 0.004, c = GEN    0.001, c = ACC NP - SB 3. Consecutive source non-terminals are permitted. Figure 1: Example target constraint rule 4. Single-word lexical phrases are allowed for hierarchical subphrase subtraction. unification is attempted between all combinations. If no combination can be successfully unified then the constraint fails. Ultimately, all feature structures originate i"
W11-2126,D07-1091,1,0.83112,"ndant for translation and can be performed as a preprocessing step for input to a conventional surface form based translation model (Nießen and Ney, 2001; Goldwater and McClosky, 2005; Talbot and Osborne, 2006). For complex target inflection, Minkov et al. (2007) investigate how postprocessing can be used to generate inflection for a system that produces uninflected output. Their approach is successfully applied to English-Arabic and English-Russian systems by Toutanova et al. (2008). Another promising line of research involves the direct integration of linguistic information into SMT models. Koehn and Hoang (2007) generalise the phrase-based model’s representation of the word from a string to a vector, allowing additional features such as part-of-speech and morphology to be associated with, or even to replace, surface forms during search. Luong et al. (2010) decompose words into morphemes and use this extended representation throughout the training, tuning, and testing pipeline. Departing further from traditional SMT models, the transfer-based systems of Riezler and Maxwell (2006), Bojar and Hajiˇc (2008), and Graham et al. (2009) employ rich feature structure representations for linguistic attributes,"
W11-2126,N03-1017,1,0.0208155,"Missing"
W11-2126,P07-2045,1,0.0189685,"failure. For the hard constraint model, we make the following modifications: 1. Since the corner hypothesis might fail the constraint check, rule cube ordering is based on the score of the nearest hypothesis to the corner that satisifies its constraints (if any exists). This hypothesis is found by exploring neighbours in order of estimated score (that is, without calculating the full language model score) starting at the corner. Decoding 2. When a hypothesis is popped from a cube and its neighbours created, constraint-failing neighbours are added to a ‘bad neighbours’ queue. We use the Moses (Koehn et al., 2007) decoder, a bottom-up synchronous parser that implements the CYK+ algorithm (Chappelier and Rajman, 1998) with cube pruning (Chiang, 2007). The constraint model requires some changes to decoding, which we briefly describe here: 5.1 Hypothesis State Bottom-up constraint evaluation requires a feature structure set for every rule element that participates in a constraint. For lexical rule elements these are obtained from the lexicon. For non-lexical rule elements these are obtained from predecessor hypotheses. After constraint evaluation, each hypothesis therefore stores the resulting, possibly e"
W11-2126,W04-3250,1,0.816547,"Missing"
W11-2126,2005.mtsummit-papers.11,1,0.101448,"s can be penalised or filtered out during search. We use a simple heuristic process to extract agreement constraints for German and test our approach on an English-German system trained on WMT data, achieving a small improvement in translation accuracy as measured by BLEU. 1 Introduction Historically, most work in statistical machine translation (SMT) has focused on translation into English. Languages with richer inflectional morphologies pose additional challenges for translation and conventional SMT approaches tend to perform poorly when either source or target language has rich morphology (Koehn, 2005). For complex source inflection, a successful approach has been to cluster inflectional variants into equivalence classes. This removes information that is redundant for translation and can be performed as a preprocessing step for input to a conventional surface form based translation model (Nießen and Ney, 2001; Goldwater and McClosky, 2005; Talbot and Osborne, 2006). For complex target inflection, Minkov et al. (2007) investigate how postprocessing can be used to generate inflection for a system that produces uninflected output. Their approach is successfully applied to English-Arabic and En"
W11-2126,D10-1015,0,0.0489001,"Missing"
W11-2126,W06-1606,0,0.0334414,"rect and indirect lexical weights (Koehn et al., 2003). Freq 308156 77198 67686 60245 41624 19736 7739 7591 4888 2060 Table 1: The 10 most freqently occurring NP labels with their case frequencies (shown as percentages) • ppcfg (FRAGt ), the monolingual PCFG probability of the tree fragment from which Qn the rule was extracted. This is defined as i=1 p(ri ), where r1 . . . rn are the constituent CFG rules of the fragment. The PCFG parameters are estimated from the parse of the target-side training data. All lexical rules are given the probability 1. This is similar to the pcfg feature used in Marcu et al. (2006) and is intended to encourage the production of syntactically well-formed derivations. • exp(1), a rule penalty. 221 4.2 Constraint Model Features In addition to the string-to-tree features, we add two features related to constraint evaluation: • exp(f ), where f is the derivation’s constraint set failure count. This serves as a penalty feature in a soft constraint variant of the model: for each constraint set in which a unification failure occurs, this count is increased and an empty feature structure is produced, permitting decoding to continue. Q • n pcase (cn ), the product of the derivati"
W11-2126,P07-1017,0,0.0981335,"r inflectional morphologies pose additional challenges for translation and conventional SMT approaches tend to perform poorly when either source or target language has rich morphology (Koehn, 2005). For complex source inflection, a successful approach has been to cluster inflectional variants into equivalence classes. This removes information that is redundant for translation and can be performed as a preprocessing step for input to a conventional surface form based translation model (Nießen and Ney, 2001; Goldwater and McClosky, 2005; Talbot and Osborne, 2006). For complex target inflection, Minkov et al. (2007) investigate how postprocessing can be used to generate inflection for a system that produces uninflected output. Their approach is successfully applied to English-Arabic and English-Russian systems by Toutanova et al. (2008). Another promising line of research involves the direct integration of linguistic information into SMT models. Koehn and Hoang (2007) generalise the phrase-based model’s representation of the word from a string to a vector, allowing additional features such as part-of-speech and morphology to be associated with, or even to replace, surface forms during search. Luong et al"
W11-2126,W01-1407,0,0.0305673,"st work in statistical machine translation (SMT) has focused on translation into English. Languages with richer inflectional morphologies pose additional challenges for translation and conventional SMT approaches tend to perform poorly when either source or target language has rich morphology (Koehn, 2005). For complex source inflection, a successful approach has been to cluster inflectional variants into equivalence classes. This removes information that is redundant for translation and can be performed as a preprocessing step for input to a conventional surface form based translation model (Nießen and Ney, 2001; Goldwater and McClosky, 2005; Talbot and Osborne, 2006). For complex target inflection, Minkov et al. (2007) investigate how postprocessing can be used to generate inflection for a system that produces uninflected output. Their approach is successfully applied to English-Arabic and English-Russian systems by Toutanova et al. (2008). Another promising line of research involves the direct integration of linguistic information into SMT models. Koehn and Hoang (2007) generalise the phrase-based model’s representation of the word from a string to a vector, allowing additional features such as par"
W11-2126,P03-1021,0,0.0247829,"kit. The resulting grammar contained just under 140 million synchronous rules. We used all of the available monolingual German data to train three 5-gram language models (one each for the Europarl, News Commentary, and News data sets). These were interpolated using weights optimised against the development set and the resulting language model was used in experiments. We used the SRILM toolkit (Stolcke, 2002) with Kneser-Ney smoothing (Chen and Goodman, 1998). The baseline system’s feature weights were tuned on the news-test2008 dev set (2,051 sentence pairs) using minimum error rate training (Och, 2003). 6.2 Constraint Model Setup A feature structure lexicon was generated by running the Morphisto3 morphological analyser over the training vocabulary and then extracting feature values from the output. The constraint rules were extracted using the agreement relation identification and filtering methods described in section 3.3. We tested two constraint model systems, one using the rules as hard constraints and the other as soft constraints. The former discarded all hypotheses that failed constraints and used the modified cube pruning search algorithm. The latter allowed constraint failure but u"
W11-2126,P02-1040,0,0.0844537,"Missing"
W11-2126,N06-1032,0,0.0134096,"va et al. (2008). Another promising line of research involves the direct integration of linguistic information into SMT models. Koehn and Hoang (2007) generalise the phrase-based model’s representation of the word from a string to a vector, allowing additional features such as part-of-speech and morphology to be associated with, or even to replace, surface forms during search. Luong et al. (2010) decompose words into morphemes and use this extended representation throughout the training, tuning, and testing pipeline. Departing further from traditional SMT models, the transfer-based systems of Riezler and Maxwell (2006), Bojar and Hajiˇc (2008), and Graham et al. (2009) employ rich feature structure representations for linguistic attributes, but have so far been limited by their dependence on nonstochastic parsers with limited coverage. The StatXFER transfer-based framework (Lavie, 2008) is neutral with regard to the rule acquisition method and the author describes a manually developed Hebrew-English transfer grammar, which includes a small number of constraints between agreement features. In Hanneman et al. (2009) the framework is used with a large automatically-extracted grammar, though this does not use f"
W11-2126,C04-1024,0,0.0325668,"ulting case value. The final two are used to disambiguate between possible parts-of-speech. Constraints are evaluated by attempting to unify the specified feature structures. A rule element may have more than one associated feature structure, so 219 We now describe the German constraints that we use in this paper. Whilst the constraint model described above is language-independent, the actual form of the constraints will largely be language- and corpusspecific. In this work, the linguistic annotation is obtained from a statistical parser and a morphological analyser. We use the BitPar parser (Schmid, 2004) trained on the TIGER treebank (Brants et al., 2002) and the Morphisto morphological analyser (Zielinski and Simon, 2009). We find that we can extract useful constraints for German based on a minimal set of simple manually-developed heuristics. Base NP/PP Agreement German determiners and adjectives are inflected to agree in gender and number with the nouns that they modify. As in English, a distinction is made between singular and plural number, with most nouns having separate forms for each. Grammatical gender has three values: masculine, feminine, and neuter. A noun phrase’s case is usually"
W11-2126,P84-1075,0,0.101718,"ations unseen by the language model • improve search by allowing the early elimination of morphologically-inconsistent hypotheses To evaluate the approach, we develop a system for English-German with constraints to enforce intraNP/PP and subject-verb agreement, and with a simple probabilistic model for NP case. 2 Preliminaries There is an extensive literature on constraint-based approaches to grammar, employing a rich variety of terminology and linguistic devices. We use only a few of the core ideas, which we briefly describe in this section. We borrow the terminology and notation of PATR-II (Shieber, 1984), a minimal constraint-based formalism that extends context-free grammar. Central to our model are the concepts of feature structures and unification. Feature structures are of two kinds: • atomic feature structures are untyped, indivisible values, such as NP, nom, or sg An equivalent representation, and the one we use for implementation, is that of a rooted, labelled, directed acyclic graph. A value belonging to a complex feature structure can be specified using a path notation that describes the chain of features in enclosing feature structures. In the examples above, the path h AGR GENDER i"
W11-2126,P06-1122,0,0.0140444,"focused on translation into English. Languages with richer inflectional morphologies pose additional challenges for translation and conventional SMT approaches tend to perform poorly when either source or target language has rich morphology (Koehn, 2005). For complex source inflection, a successful approach has been to cluster inflectional variants into equivalence classes. This removes information that is redundant for translation and can be performed as a preprocessing step for input to a conventional surface form based translation model (Nießen and Ney, 2001; Goldwater and McClosky, 2005; Talbot and Osborne, 2006). For complex target inflection, Minkov et al. (2007) investigate how postprocessing can be used to generate inflection for a system that produces uninflected output. Their approach is successfully applied to English-Arabic and English-Russian systems by Toutanova et al. (2008). Another promising line of research involves the direct integration of linguistic information into SMT models. Koehn and Hoang (2007) generalise the phrase-based model’s representation of the word from a string to a vector, allowing additional features such as part-of-speech and morphology to be associated with, or even"
W11-2126,P08-1059,0,0.351419,"ection, a successful approach has been to cluster inflectional variants into equivalence classes. This removes information that is redundant for translation and can be performed as a preprocessing step for input to a conventional surface form based translation model (Nießen and Ney, 2001; Goldwater and McClosky, 2005; Talbot and Osborne, 2006). For complex target inflection, Minkov et al. (2007) investigate how postprocessing can be used to generate inflection for a system that produces uninflected output. Their approach is successfully applied to English-Arabic and English-Russian systems by Toutanova et al. (2008). Another promising line of research involves the direct integration of linguistic information into SMT models. Koehn and Hoang (2007) generalise the phrase-based model’s representation of the word from a string to a vector, allowing additional features such as part-of-speech and morphology to be associated with, or even to replace, surface forms during search. Luong et al. (2010) decompose words into morphemes and use this extended representation throughout the training, tuning, and testing pipeline. Departing further from traditional SMT models, the transfer-based systems of Riezler and Maxw"
W11-2126,W06-3119,0,0.0248944,"AGR     CASE DECL   GENDER NUMBER POS NN   CASE    AGR GENDER NUMBER  acc   weak   fem   sg   acc   fem sg 218 "" POS AGR 1  CASE DECL   GENDER NUMBER # NN  acc   weak   fem   sg 1 The index boxes are used to indicate that a value is shared. 3 • complex feature structures are partial functions mapping features to values, the values themselves being feature structures. →     AGR   Grammar In this section we describe the synchronous grammar used in our string-to-tree model. Rule extraction is similar to the syntax-augmented model of Zollmann and Venugopal (2006), though we do not use extended categories in this work. We then describe how we extend the grammar with target-side constraints. 3.1 Synchronous Grammar Our translation model is based on a synchronous context-free grammar (SCFG) learned from a parallel corpus. Rule extraction follows the hierarchical phrase-based algorithm of Chiang (2005; 2007). Source non-terminals are given the undistinguished label X, whereas the target non-terminals are given part-of-speech and constituent labels obtained from a parse of the target-side of the parallel corpus. Rules in which the target span is not covere"
W11-2130,W09-1114,1,0.87215,"dom scores assigned to hypotheses) showed very little difference in performance. Once the initial set of hypotheses for the new batch is created, the SampleRank innermost loop (lines 6-14 in Algorithm 1) proceeds by repeatedly choosing a sample hypothesis set (y 0 ) and an oracle hypothesis set (y + ), corresponding to the source side of the batch (x). Given the current hypothesis set ys−1 = (e1 , . . . , ek ), the sample and oracle are chosen as follows. Firstly, a hypothesis ej is selected randomly from ys−1 , and a neighbourhood of alternate hypotheses N 3 ej generated using operators from Arun et al. (2009) (explained shortly). Model scores are calculated for all the hypotheses in N , converted to probabilities using Equation (1), and a sample e0j taken from N using these probabilities. The sample hypothesis set (y 0 ) is then the current hypothesis set (ys−1 ) with ej replaced by e0j . The oracle is created, analogously Chiang et al. (2008), by choosing e+ j ∈ N to maximise the sum of gain (calculated on the batch) and model score. The oracle hypothesis set (y + ) is then ys−1 with ej replaced by e+ j . We now describe how the neighbourhood is chosen. Given a single hypothesis ej , a neighbourh"
W11-2130,W10-1756,1,0.834184,"as a metric – because evaluation of translation is so difficult, any reasonable gain function is likely to have a complex relationship with the model parameters. Gradient-based tuning methods, such as minimum risk training, have been investigated as possible alternatives to MERT. Expected BLEU is normally adopted as the objective since it is differentiable and so can be optimised by a form of stochastic gradient ascent. The feature expectations required for the gradient calculation can be obtained from n-best lists or lattices (Smith and Eisner, 2006; Li and Eisner, 2009), or using sampling (Arun et al., 2010), both of which can be computationally expensive. 261 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 261–271, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics Margin-based techniques such as perceptron training (Liang et al., 2006) and MIRA (Chiang et al., 2008; Watanabe et al., 2007) have also been shown to be able to tune MT systems and scale to large numbers of features, but these generally involve repeatedly decoding the tuning set (and so are expensive) and require sentence-level approximations to the BLEU objective. In"
W11-2130,P08-1024,0,0.101275,"before making a parameter weight update, as opposed to the current work where weights may be updated after every sample. One novel feature of Arun et al. (2010) is that they were able to train to directly maximise corpus BLEU, instead of its sentence-based approximation, although this only made a small difference to the results. The training methods in (Arun et al., 2010) are very resource intensive, with the experiments running for 48 hours on around 40 cores, on a pruned phrase table derived from Europarl, and a 3-gram language model. Instead of using expected BLEU as a training objective, Blunsom et al. (2008) trained their model to directly maximise the log-likelihood of the discriminative model, estimating feature expectations from a packed chart. Their model treats derivations as a latent variable, directly modelling the translation probability. Margin-based techniques have the advantage that they do not have to employ expensive and complex algorithms to calculate the feature expectations. Typically, either perceptron ((Liang et al., 2006), (Arun and Koehn, 2007)) or MIRA ((Watanabe et al., 2007), (Chiang et al., 2008)) is employed, but in both cases the idea is to repeatedly decode sentences fr"
W11-2130,D08-1024,0,0.702897,"ve since it is differentiable and so can be optimised by a form of stochastic gradient ascent. The feature expectations required for the gradient calculation can be obtained from n-best lists or lattices (Smith and Eisner, 2006; Li and Eisner, 2009), or using sampling (Arun et al., 2010), both of which can be computationally expensive. 261 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 261–271, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics Margin-based techniques such as perceptron training (Liang et al., 2006) and MIRA (Chiang et al., 2008; Watanabe et al., 2007) have also been shown to be able to tune MT systems and scale to large numbers of features, but these generally involve repeatedly decoding the tuning set (and so are expensive) and require sentence-level approximations to the BLEU objective. In this paper we present an alternative method of tuning MT systems known as SampleRank, which has certain advantages over other methods in use today. SampleRank operates by repeatedly sampling pairs of translation hypotheses (for a given source sentence) and updating the feature weights if the ranking induced by the MT model (1) i"
W11-2130,W09-0439,0,0.0745551,"ning) (Och, 2003), to maximise performance Philipp Koehn School of Informatics University of Edinburgh pkoehn@inf.ed.ac.uk as measured by an automated metric such as BLEU (Papineni et al., 2002). MERT training uses a parallel data set (known as the tuning set) consisting of about 1000-2000 sentences, distinct from the data set used to build the generative models. Optimising the weights in Equation (1) is often referred to as tuning the MT system, to differentiate it from the process of training the generative models. MERT’s inability to scale beyond 20-30 features, as well as its instability (Foster and Kuhn, 2009) have led to investigation into alternative ways of tuning MT systems. The development of tuning methods is complicated, however by, the use of BLEU as an objective function. This objective in its usual form is not differentiable, and has a highly non-convex error surface (Och, 2003). Furthermore BLEU is evaluated at the corpus level rather than at the sentence level, so tuning methods either have to consider the entire corpus, or resort to a sentencelevel approximation of BLEU. It is unlikely, however, that the difficulties in discriminative MT tuning are due solely to the use of BLEU as a me"
W11-2130,D07-1091,1,0.744359,"eature to fire for each bigram. Dummy phrases with parts-of-speech < S &gt; and </ S &gt; are inserted at the start and end of the sentence, and also used to construct phrase boundary features. The example in Figure 4 shows the phrase-boundary features from a typical hypothesis. The idea is similar to a part-of-speech language model, but discriminatively trained, and targeted at how phrases are joined together in the hypothesis. The target-side part-of-speech tags are added using the Brill tagger, and incorporated into the phrase table using the factored translation modelling capabilities of Moses (Koehn and Hoang, 2007). Adding the phrase boundary features to the WMTSMALL system increased the feature count from 8 to around 800. Training experiments were run for both the French-English and German-English models, using the same configuration as in Section 3.2, varying the number of cores (8 or 16) and the number of samples per sentence (100 or 500). Training times were similar to those for the WMT- SMALL system. The mean maximum scores on heldout are shown in Table 5. We suspect that these features are fixing some short range reordering problems which 27 26 27 26 27 Bleu 25 ● ● ● ●●● ● ● ●●● ● ● ● ●● ●●● ● ●●●"
W11-2130,W07-0733,1,0.831597,"side of this data set. The features used in the WMT- SMALL translation system were the five Moses translation features, a language model feature, a word penalty feature and a distortion distance feature. To build the WMT- LARGE translation system, both the ep11 data set and the nc11 data set were concatenated together before building the translation model out of the resulting corpus of about 2 million sentences. Separate 5-gram language models were built from the target side of the two data sets and then they were interpolated using weights chosen to minimise the perplexity on the tuning set (Koehn and Schroeder, 2007). In the WMT- LARGE system, the eight core features were supplemented with the six features of the lexicalised reordering model, which was trained on the same data as was used to build the translation model. Whilst a training set size of 2 million sentences would not normally be sufficient to build a competitive system for an MT shared task, it is sufficient to show that how SampleRank training performs on a realistic sized system, whilst still allowing for plenty of experimenation with the algorithm’s parameters. For tuning, the nc-devtest2007 was used, with the first half of nc-test2007 corp"
W11-2130,D09-1005,0,0.275889,"tuning are due solely to the use of BLEU as a metric – because evaluation of translation is so difficult, any reasonable gain function is likely to have a complex relationship with the model parameters. Gradient-based tuning methods, such as minimum risk training, have been investigated as possible alternatives to MERT. Expected BLEU is normally adopted as the objective since it is differentiable and so can be optimised by a form of stochastic gradient ascent. The feature expectations required for the gradient calculation can be obtained from n-best lists or lattices (Smith and Eisner, 2006; Li and Eisner, 2009), or using sampling (Arun et al., 2010), both of which can be computationally expensive. 261 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 261–271, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics Margin-based techniques such as perceptron training (Liang et al., 2006) and MIRA (Chiang et al., 2008; Watanabe et al., 2007) have also been shown to be able to tune MT systems and scale to large numbers of features, but these generally involve repeatedly decoding the tuning set (and so are expensive) and require sentence-level a"
W11-2130,P06-1096,0,0.374556,"Missing"
W11-2130,P03-1021,0,0.438086,"ine translation (PBMT), the standard approach is to express the probability distribution p(a, e|f ) (where f is the source sentence and (a, e) is the aligned target sentence) in terms of a linear model based on a small set of feature functions ! n X p(a, e|f ) ∝ exp wi hi (a, e, f ) (1) i=1 The feature functions {hi } typically include log probabilities of generative models such as translation, language and reordering, as well as nonprobabilistic features such as word, phrase and distortion penalties. The feature weights w = {wi } are normally trained using MERT (minimum error rate training) (Och, 2003), to maximise performance Philipp Koehn School of Informatics University of Edinburgh pkoehn@inf.ed.ac.uk as measured by an automated metric such as BLEU (Papineni et al., 2002). MERT training uses a parallel data set (known as the tuning set) consisting of about 1000-2000 sentences, distinct from the data set used to build the generative models. Optimising the weights in Equation (1) is often referred to as tuning the MT system, to differentiate it from the process of training the generative models. MERT’s inability to scale beyond 20-30 features, as well as its instability (Foster and Kuhn,"
W11-2130,P02-1040,0,0.081806,"sentence) in terms of a linear model based on a small set of feature functions ! n X p(a, e|f ) ∝ exp wi hi (a, e, f ) (1) i=1 The feature functions {hi } typically include log probabilities of generative models such as translation, language and reordering, as well as nonprobabilistic features such as word, phrase and distortion penalties. The feature weights w = {wi } are normally trained using MERT (minimum error rate training) (Och, 2003), to maximise performance Philipp Koehn School of Informatics University of Edinburgh pkoehn@inf.ed.ac.uk as measured by an automated metric such as BLEU (Papineni et al., 2002). MERT training uses a parallel data set (known as the tuning set) consisting of about 1000-2000 sentences, distinct from the data set used to build the generative models. Optimising the weights in Equation (1) is often referred to as tuning the MT system, to differentiate it from the process of training the generative models. MERT’s inability to scale beyond 20-30 features, as well as its instability (Foster and Kuhn, 2009) have led to investigation into alternative ways of tuning MT systems. The development of tuning methods is complicated, however by, the use of BLEU as an objective functio"
W11-2130,2010.amta-papers.31,0,0.0674338,"ate the parameter weights if the best hypothesis according to the model differs from some “oracle” sentence. The approaches differ in the way they compute the oracle sentence, as well as the way the weights are updated. Normally sentences are processed one-by-one, with a weight update after considering each sentence, and sentence BLEU is used as the objective. However Chiang et al. (2008) introduced an approximation to corpus BLEU by using a rolling history. Both papers on MIRA demonstrated its ability to extend to large numbers of features. In the only known application of SampleRank to SMT, Roth et al. (2010) deploys quite a different translation model to the usual phrase-based model, allowing overlapping phrases and implemented as a factor graph. Decoding is with a rather slow stochastic search and performance is quite poor, but this model, in common with the training algorithm presented in the current work, permits features which depend on the whole sentence. 5 Discussion and Conclusions The results presented in Table 6 show that SampleRank is a viable method of parameter tuning for phrase-based MT systems, beating MERT in many cases, and equalling it in others. It is also able to do what MERT c"
W11-2130,P06-2101,0,0.353458,"ies in discriminative MT tuning are due solely to the use of BLEU as a metric – because evaluation of translation is so difficult, any reasonable gain function is likely to have a complex relationship with the model parameters. Gradient-based tuning methods, such as minimum risk training, have been investigated as possible alternatives to MERT. Expected BLEU is normally adopted as the objective since it is differentiable and so can be optimised by a form of stochastic gradient ascent. The feature expectations required for the gradient calculation can be obtained from n-best lists or lattices (Smith and Eisner, 2006; Li and Eisner, 2009), or using sampling (Arun et al., 2010), both of which can be computationally expensive. 261 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 261–271, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics Margin-based techniques such as perceptron training (Liang et al., 2006) and MIRA (Chiang et al., 2008; Watanabe et al., 2007) have also been shown to be able to tune MT systems and scale to large numbers of features, but these generally involve repeatedly decoding the tuning set (and so are expensive) and re"
W11-2130,D07-1080,0,0.710828,"entiable and so can be optimised by a form of stochastic gradient ascent. The feature expectations required for the gradient calculation can be obtained from n-best lists or lattices (Smith and Eisner, 2006; Li and Eisner, 2009), or using sampling (Arun et al., 2010), both of which can be computationally expensive. 261 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 261–271, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics Margin-based techniques such as perceptron training (Liang et al., 2006) and MIRA (Chiang et al., 2008; Watanabe et al., 2007) have also been shown to be able to tune MT systems and scale to large numbers of features, but these generally involve repeatedly decoding the tuning set (and so are expensive) and require sentence-level approximations to the BLEU objective. In this paper we present an alternative method of tuning MT systems known as SampleRank, which has certain advantages over other methods in use today. SampleRank operates by repeatedly sampling pairs of translation hypotheses (for a given source sentence) and updating the feature weights if the ranking induced by the MT model (1) is different from the ran"
W11-2130,N10-1069,0,\N,Missing
W12-3102,W10-1703,1,0.557844,"Missing"
W12-3102,W11-2103,1,0.709065,"- ANNOTATOR AGREEMENT P (A) 0.567 0.576 0.595 0.598 0.540 0.504 0.568 0.519 0.568 0.601 P (A) 0.660 0.566 0.733 0.732 0.792 0.566 0.719 0.634 0.671 0.722 P (E) 0.405 0.383 0.401 0.394 0.408 0.398 0.406 0.388 0.396 0.362 κ 0.272 0.312 0.323 0.336 0.222 0.176 0.272 0.214 0.284 0.375 P (E) 0.405 0.383 0.401 0.394 0.408 0.398 0.406 0.388 0.396 0.362 κ 0.428 0.296 0.554 0.557 0.648 0.279 0.526 0.401 0.455 0.564 Table 3: Inter- and intra-annotator agreement rates for the WMT12 manual evaluation. For comparison, the WMT11 rows contain the results from the European languages individual systems task (Callison-Burch et al. (2011), Table 7). Agreement rates vary widely across languages. For inter-annotator agreements, the range is 0.176 to 0.336, while intra-annotator agreement ranges from 0.279 to 0.648. We note in particular the low agreement rates among judgments in the English-Spanish task, which is reflected in the relative lack of statistical significance Table 4. The agreement rates for this year were somewhat lower than last year. 3.3 Results of the Translation Task We used the results of the manual evaluation to analyze the translation quality of the different systems that were submitted to the workshop. In ou"
W12-3102,D09-1030,1,0.149941,"Missing"
W12-3102,W12-3103,0,0.0434403,"Missing"
W12-3102,W11-2107,0,0.311039,"Missing"
W12-3102,W12-3131,0,0.0259292,"Missing"
W12-3102,W12-3133,0,0.0166865,"Missing"
W12-3102,W12-3134,1,0.0973948,"Missing"
W12-3102,W12-3135,0,0.0202653,"Missing"
W12-3102,W12-3111,0,0.0311487,"Missing"
W12-3102,W12-3136,0,0.0403021,"Missing"
W12-3102,W12-3112,0,0.176358,"Missing"
W12-3102,2011.eamt-1.32,0,0.0209858,"with only two linear equations. System “SDLLW SVM” uses a 20-feature set and an SVM epsilon regression model with radial basis function kernel with parameters C, gamma, and epsilon tuned on a development set (305 training instances). The model was trained with 10-fold cross validation and the tuning process was restarted several times using different starting points and step sizes to avoid overfitting. The final model was selected based on its performance on the development set and the number of support vectors. UU (R, S): System “UU best” uses the 17 baseline features, plus 82 features from Hardmeier (2011) (with some redundancy and some overlap with baseline features), and constituency trees over input sentences generated by the Stanford parser and dependency trees over both input and output sentences generated by the MaltParser. System “UU bltk” uses only the 17 baseline features plus constituency and dependency trees as above. The machine learning component in both cases is SVM regression (SVMlight software). For the ranking task, 29 the ranking induced by the regression output is used. The system uses polynomial kernels of degree 2 (UU best) and 3 (UU bltk) as well as two different types of"
W12-3102,W12-3137,0,0.0533752,"Missing"
W12-3102,W12-3106,0,0.0413119,"Missing"
W12-3102,W12-3145,0,0.0522389,"Missing"
W12-3102,W12-3146,0,0.0998404,"Missing"
W12-3102,W09-0415,0,0.0172168,"Missing"
W12-3102,W12-3150,1,0.188192,"Missing"
W12-3102,W12-3119,0,0.030516,"Missing"
W12-3102,W12-3151,0,0.046244,"Missing"
W12-3102,W12-3117,0,\N,Missing
W12-3102,W12-3138,0,\N,Missing
W12-3102,W12-3130,0,\N,Missing
W12-3102,W12-3148,0,\N,Missing
W12-3102,W12-3114,0,\N,Missing
W12-3102,W12-3107,0,\N,Missing
W12-3102,W12-3139,1,\N,Missing
W12-3102,W11-2108,0,\N,Missing
W12-3102,W09-0401,1,\N,Missing
W12-3102,W12-3140,0,\N,Missing
W12-3102,W12-3132,0,\N,Missing
W12-3102,W12-3109,0,\N,Missing
W12-3102,W10-1711,0,\N,Missing
W12-3102,2010.iwslt-evaluation.22,0,\N,Missing
W12-3102,W07-0718,1,\N,Missing
W12-3102,W06-3114,1,\N,Missing
W12-3102,2009.eamt-1.5,1,\N,Missing
W12-3102,W12-3144,0,\N,Missing
W12-3102,W11-2158,0,\N,Missing
W12-3102,W12-3110,1,\N,Missing
W12-3102,W08-0309,1,\N,Missing
W12-3102,W12-3105,0,\N,Missing
W12-3102,W12-3149,0,\N,Missing
W12-3102,2011.eamt-1.12,1,\N,Missing
W12-3102,W04-3250,1,\N,Missing
W12-3102,W12-3142,0,\N,Missing
W12-3102,W11-2113,0,\N,Missing
W12-3102,W12-3143,0,\N,Missing
W12-3102,W11-2145,0,\N,Missing
W12-3102,W12-3147,0,\N,Missing
W12-3102,W12-3101,0,\N,Missing
W12-3102,W12-3115,0,\N,Missing
W12-3102,W11-2101,0,\N,Missing
W12-3102,W12-3113,0,\N,Missing
W12-3102,W12-3104,0,\N,Missing
W12-3102,W12-3108,0,\N,Missing
W12-3102,W12-3118,1,\N,Missing
W12-3139,D11-1033,0,0.22496,"Missing"
W12-3139,W09-0432,0,0.0272171,"6.2 Hierarchical 21.4 (–.2) 27.6 (–.3) 28.4 (–.5) 22.0 (–.4) 15.5 (–.4) 28.0 (–.8) 30.4 (–.4) 15.6 (–.6) Setup fr-en ep+nc +un en-fr ep+nc +un Table 6: Hierarchical phrase models vs. baseline phrasebased models. to their size. Table 6 shows inferior performance for all language pairs (by about half a BLEU point), although results for German–English are close (–0.2 BLEU). 5.2 Semi-Supervised Learning Other research groups have reported improvements using semi-supervised learning methods to create synthetic parallel data from monolingual data (Schwenk et al., 2008; Abdul-Rauf and Schwenk, 2009; Bertoldi and Federico, 2009; Lambert et al., 2011). The idea is to translate in-domain monolingual data with a baseline system and filter the result for use as an additional parallel corpus. Table 7 shows out results when trying to emulate the approach of Lambert et al. (2011). We translate the some of the 2011 monolingual news data (139 million words for French and 100 million words for English) from the target language into the source language with a baseline system trained on Europarl and News Commentary. Adding all the obtained data hurts (except for minimal improvements over a small French-English system). When we"
W12-3139,W11-2103,1,0.888939,"Missing"
W12-3139,J07-2003,0,0.0177689,"never lead to worse results on the sampled n-best lists. This method (PRO-MERT in the table) applied here, however, did not lead to significantly different results than plain MERT. 5 4 MERT 21.7 (1.01) 29.1 (1.02) 24.2 (1.03) 16.0 (1.00) 29.3 (0.98) 31.5 (0.98) 17.4 (0.97) What did not Work Not everything we tried worked out. Notably, two promising directions — hierarchical models and semi-supervised learning — did not yield any improvements. It is not clear if we failed or if the methods failed, but we will investigate this further in future work. 5.1 Hierarchical Models Hierarchical models (Chiang, 2007) have been supported already for a few years by Moses, and they give significantly better performance for Chinese– English over phrase-based models. While we have not yet seen benefits for many other language pairs, the eight language pairs of WMT12 allowed us to compare these two models more extensively, also in view of recent enhancements resulting in better search accuracy. Since hierarchical models are much larger (roughly 10 times bigger), we trained hierarchical models on downsized training data for most language pairs. For Spanish and French, this excludes UN and GigaFrEn; for Czech som"
W12-3139,W11-2123,0,0.0275481,", but Model 1 sampling made no difference for the Spanish systems, and was harmful for the French systems. 318 Better Language Models In previous years, we were not able to make use of the monolingual LDC Gigaword corpora due to lack of sufficiently powerful computing resources. These corpora exist for English (4.3 billion words), Spanish (1.1 billion words), and French (0.8 billion words). With the acquisition of large memory machines2 , we were now able to train language models on this data. Use of these large language models during decoding is aided by more efficient storage and inference (Heafield, 2011). Still, even with that much RAM it is not possible to train a language model with SRILM (Stolke, 2002) in one pass. Hence, we broke up the training corpus by source (New York Times, Washington Post, ...) and trained separate language model for each. The largest individual corpus was the English New York Times portion which consists of 1.5 billion words and took close to 100GB of RAM. We also trained individual language models for each year of WMT12’s monolingual corpus. We interpolated the language models using the SRILM toolkit. The toolkit has a limit of 10 language models to be merged at o"
W12-3139,D11-1125,0,0.0119275,"er tuning set (7567 sentences) by combining newstest 2008 to 2010. 4.1 Better Tuning Bigger Tuning Sets In recent experiments, mainly geared towards using much larger feature sets, we learned that larger tuning sets may give better and more stable results. We tested this hypothesis here as well. By concatenating the sets from three years (20082010), we constructed a tuning set of 7567 sentences per language. Table 4 shows that we gain on average about +0.2 BLEU points. 4.2 Pairwise Ranked Optimization We recently added an implementation of the pairwise ranked optimization (PRO) tuning method (Hopkins and May, 2011) to Moses as an alternative to Och’s (2003) minimum error rate training (MERT). We checked if this method gives us better results. Table 5 shows a mixed picture. PRO gives slightly shorter translations, probably because it optimises sentence rather than corpus BLEU, which has a noticeable effect on the BLEU score. For 2 language pairs we see better results, for 4 worse, and for 1 there is no difference. On other data and lan319 PRO 21.9 (1.00) +.2 29.1 (1.01) ±.0 24.5 (1.00) +.3 15.7 (0.96) –.3 28.9 (0.96) –.4 31.3 (0.97) –.2 16.9 (0.92) –.5 PRO-MERT 21.7 (1.01) ±.0 29.1 (1.02) ±.0 24.2 (1.03)"
W12-3139,P07-2045,1,0.0200373,"r en-es LP fr-en en-fr Abstract We report on findings of exploiting large data sets for translation modeling, language modeling and tuning for the development of competitive machine translation systems for eight language pairs. 1 Introduction We report on experiments carried out for the development of competitive systems on the datasets of the 2012 Workshop on Statistical Machine Translation. Our main focus was directed on the effective use of all the available training data during training of translation and language models and tuning. We use the open source machine translation system Moses (Koehn et al., 2007) and other standard open source tools, hence all our experiments are straightforwardly replicable1 . Compared to all single system submissions by participants of the workshop we achieved the best BLEU scores for four language pairs (es-en, en-es, cs-en, en-cs), the 2nd best results for two language pairs (fr-en, de-en), as well as a 3rd place (en-de) and a 5th place (en-fr) for the remaining pairs. We improved upon this in the post-evaluation period for some of the language pairs by more systematically applying our methods. During the development of our system, we saw most gains from using lar"
W12-3139,W11-2132,0,0.0271331,"7.6 (–.3) 28.4 (–.5) 22.0 (–.4) 15.5 (–.4) 28.0 (–.8) 30.4 (–.4) 15.6 (–.6) Setup fr-en ep+nc +un en-fr ep+nc +un Table 6: Hierarchical phrase models vs. baseline phrasebased models. to their size. Table 6 shows inferior performance for all language pairs (by about half a BLEU point), although results for German–English are close (–0.2 BLEU). 5.2 Semi-Supervised Learning Other research groups have reported improvements using semi-supervised learning methods to create synthetic parallel data from monolingual data (Schwenk et al., 2008; Abdul-Rauf and Schwenk, 2009; Bertoldi and Federico, 2009; Lambert et al., 2011). The idea is to translate in-domain monolingual data with a baseline system and filter the result for use as an additional parallel corpus. Table 7 shows out results when trying to emulate the approach of Lambert et al. (2011). We translate the some of the 2011 monolingual news data (139 million words for French and 100 million words for English) from the target language into the source language with a baseline system trained on Europarl and News Commentary. Adding all the obtained data hurts (except for minimal improvements over a small French-English system). When we filtered out half of th"
W12-3139,P03-1021,0,0.00997651,"Missing"
W12-3139,2008.iwslt-evaluation.9,0,0.0545166,"Missing"
W12-3139,W11-2158,0,0.0618264,"Missing"
W12-3139,E09-1003,0,\N,Missing
W12-3139,D08-1076,0,\N,Missing
W12-3150,P05-1033,0,0.0429615,"University of Edinburgh 10 Crichton Street EH8 9AB, UK p.j.williams-2@sms.ed.ac.uk pkoehn@inf.ed.ac.uk Abstract We developed a string-to-tree system for English–German, achieving competitive results against a hierarchical model baseline. We provide details of our implementation of GHKM rule extraction and scope-3 parsing in the Moses toolkit. We compare systems trained on the same data using different grammar extraction methods. 1 Introduction Over the last few years, syntax-based rule extraction has largely developed along two lines, one originating in hierarchical phrase-based translation (Chiang, 2005; Chiang, 2007) and the other in GHKM (Galley et al., 2004; Galley et al., 2006). Hierarchical rule extraction generalizes the established phrase-based extraction method to produce formally-syntactic synchronous context-free grammar rules without any requirement for linguistic annotation of the training data. In subsequent work, the approach has been extended to incorporate linguistic annotation on the target side (as in SAMT (Zollmann and Venugopal, 2006)) or on both sides (Chiang, 2010). In contrast, GHKM places target-side syntactic structure at the heart of the rule extraction process, pro"
W12-3150,J07-2003,0,0.705881,"Edinburgh 10 Crichton Street EH8 9AB, UK p.j.williams-2@sms.ed.ac.uk pkoehn@inf.ed.ac.uk Abstract We developed a string-to-tree system for English–German, achieving competitive results against a hierarchical model baseline. We provide details of our implementation of GHKM rule extraction and scope-3 parsing in the Moses toolkit. We compare systems trained on the same data using different grammar extraction methods. 1 Introduction Over the last few years, syntax-based rule extraction has largely developed along two lines, one originating in hierarchical phrase-based translation (Chiang, 2005; Chiang, 2007) and the other in GHKM (Galley et al., 2004; Galley et al., 2006). Hierarchical rule extraction generalizes the established phrase-based extraction method to produce formally-syntactic synchronous context-free grammar rules without any requirement for linguistic annotation of the training data. In subsequent work, the approach has been extended to incorporate linguistic annotation on the target side (as in SAMT (Zollmann and Venugopal, 2006)) or on both sides (Chiang, 2010). In contrast, GHKM places target-side syntactic structure at the heart of the rule extraction process, producing extended"
W12-3150,P10-1146,0,0.0281263,"extraction has largely developed along two lines, one originating in hierarchical phrase-based translation (Chiang, 2005; Chiang, 2007) and the other in GHKM (Galley et al., 2004; Galley et al., 2006). Hierarchical rule extraction generalizes the established phrase-based extraction method to produce formally-syntactic synchronous context-free grammar rules without any requirement for linguistic annotation of the training data. In subsequent work, the approach has been extended to incorporate linguistic annotation on the target side (as in SAMT (Zollmann and Venugopal, 2006)) or on both sides (Chiang, 2010). In contrast, GHKM places target-side syntactic structure at the heart of the rule extraction process, producing extended tree transducer rules that map between strings and tree fragments. Ultimately, both methods define rules according to a sentence pair’s word-alignments. Without any restriction on rule size they will produce an exponentially large set of rules and so in practice only a subgrammar can be extracted. It is the differing rule selection heuristics that distinguish these two approaches, with hierarchical approaches being motivated by phrasal coverage and GHKM by targetside tree"
W12-3150,P11-2072,0,0.811614,"ms trained on the same data. To our knowledge, these are the first GHKM results presented for English-German, a language pair with a high degree of reordering and rich target-side morphology. 2 GHKM Rule Extraction in Moses A basic GHKM rule extractor was first developed for Moses during the fourth Machine Translation Marathon1 in 2010. We have recently extended it to support several key features that are described in the literature, namely: composition of rules (Galley et al., 2006), attachment of unaligned source words (Galley et al., 2004), and elimination of fully non-lexical unary rules (Chung et al., 2011). We provide some basic implementation details in the remainder of this section. In section 4 we present 1 http://www.mtmarathon2010.info 388 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 388–394, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics TOP PUNC. S-TOP das ist . NP-PD PDS VAFIN ART NN der PP-MNR Fall APPR von PN-NK NE NE Alexander Nikitin it is the case of 2.2 Unaligned Source Words Alexander Nikitin . Figure 1: Sentence pair from training data. experimental results comparing performance against Moses’ alternative rule ext"
W12-3150,P11-2031,0,0.00683222,"rule extraction spans: 10 for hierarchical phrase-based, 15 for target syntax, and unlimited for GHKM. 392 We used the scope-3 parsing algorithm (enabled using the option -parsing-algorithm 1) for all systems except the hierarchical system, which used the CYK+ algorithm (Chappelier and Rajman, 1998). For all systems we set the ttable-limit parameter to 50 (increased from the default value of 20). This setting controls the level of grammar pruning that is performed after loading: only the top scoring translations are retained for a given source RHS. 4.5 Results Following the recommendation of Clark et al. (2011), we ran the optimization three times and repeated evaluation with each set of feature weights. Table 2 presents the averaged single-reference BLEU scores. To give a rough indication of how much use the systems make of syntactic information for reordering, we also report glue rule statistics taken from the 1-best derivations. There is a huge variation in decoding time between the systems, much of which can be attributed to the differing chart span limits. To give a comparison of system performance we selected an 80-sentence subset of newstest2011, randomly choosing ten sentences of length 1-10"
W12-3150,D07-1079,0,0.220131,"rmation of a rule with the target side: S - TOP → das ist der Fall von PN - NK since the maximum distance from the rule’s root node to another node is three (to APPR or to PN - NK). However, a rule with the target side: S - TOP is not permitted since it has a rule depth of four (from S - TOP to either of the NE nodes). Node count is defined as the number of target tree nodes in the composed rule, excluding target words. The default limit is 15, which for the example is large enough to permit any possible composed rule (the full tree has a node count of 13). Rule size is the measure defined in DeNeefe et al. (2007): the number of non-part-ofspeech, non-leaf constituent labels in the target tree. The default rule size limit is three. → das ist der Fall von NE Nikitin 389 Unaligned source words are attached to the tree using the following heuristic: if there are aligned source words to both the left and the right of an unaligned source word then it is attached to the lowest common ancestor of its nearest such left and right neighbours. Otherwise, it is attached to the root of the parse tree. 2.3 Unary Rule Elimination Moses’ chart decoder does not currently support the use of grammars containing fully non"
W12-3150,N04-1035,0,0.97703,", UK p.j.williams-2@sms.ed.ac.uk pkoehn@inf.ed.ac.uk Abstract We developed a string-to-tree system for English–German, achieving competitive results against a hierarchical model baseline. We provide details of our implementation of GHKM rule extraction and scope-3 parsing in the Moses toolkit. We compare systems trained on the same data using different grammar extraction methods. 1 Introduction Over the last few years, syntax-based rule extraction has largely developed along two lines, one originating in hierarchical phrase-based translation (Chiang, 2005; Chiang, 2007) and the other in GHKM (Galley et al., 2004; Galley et al., 2006). Hierarchical rule extraction generalizes the established phrase-based extraction method to produce formally-syntactic synchronous context-free grammar rules without any requirement for linguistic annotation of the training data. In subsequent work, the approach has been extended to incorporate linguistic annotation on the target side (as in SAMT (Zollmann and Venugopal, 2006)) or on both sides (Chiang, 2010). In contrast, GHKM places target-side syntactic structure at the heart of the rule extraction process, producing extended tree transducer rules that map between str"
W12-3150,P06-1121,0,0.852129,"ms.ed.ac.uk pkoehn@inf.ed.ac.uk Abstract We developed a string-to-tree system for English–German, achieving competitive results against a hierarchical model baseline. We provide details of our implementation of GHKM rule extraction and scope-3 parsing in the Moses toolkit. We compare systems trained on the same data using different grammar extraction methods. 1 Introduction Over the last few years, syntax-based rule extraction has largely developed along two lines, one originating in hierarchical phrase-based translation (Chiang, 2005; Chiang, 2007) and the other in GHKM (Galley et al., 2004; Galley et al., 2006). Hierarchical rule extraction generalizes the established phrase-based extraction method to produce formally-syntactic synchronous context-free grammar rules without any requirement for linguistic annotation of the training data. In subsequent work, the approach has been extended to incorporate linguistic annotation on the target side (as in SAMT (Zollmann and Venugopal, 2006)) or on both sides (Chiang, 2010). In contrast, GHKM places target-side syntactic structure at the heart of the rule extraction process, producing extended tree transducer rules that map between strings and tree fragment"
W12-3150,W08-0509,0,0.0298641,"|von NE 1 NE 2 NP - PD NP - PD Figure 2: A sample of the rules extractable from the alignment graph in Figure 1. Rules are written in the form LHS → RHS s |RHSt . the case of ✸ of 4.2 Rule Extraction ✸ ✸ case ✸ ✸ Alexander Nikitin Figure 3: Example grammar trie. The filled vertices hold associative array values. 2,043,914 sentence pairs. For the target syntax experiments, the German-side of the parallel corpus was parsed using the BitPar2 parser. If a parse failed then the sentence pair was discarded, leaving a total of 2,028,556 pairs. The parallel corpus was then word-aligned using MGIZA++ (Gao and Vogel, 2008), a multi-threaded implementation of GIZA++ (Och and Ney, 2003). We used all available monolingual German data to train seven 5-gram language models (one each for Europarl, News Commentary, and the five News data sets). These were interpolated using weights optimised against the development set and the resulting language model was used in experiments. We used the SRILM toolkit (Stolcke, 2002) with Kneser-Ney smoothing (Chen and Goodman, 1998). The baseline system’s feature weights were tuned on the news-test2008 dev set (2,051 sentence pairs) using Moses’ implementation of minimum error rate t"
W12-3150,2009.iwslt-papers.4,1,0.27628,"Ultimately, both methods define rules according to a sentence pair’s word-alignments. Without any restriction on rule size they will produce an exponentially large set of rules and so in practice only a subgrammar can be extracted. It is the differing rule selection heuristics that distinguish these two approaches, with hierarchical approaches being motivated by phrasal coverage and GHKM by targetside tree coverage. The Moses toolkit (Koehn et al., 2007) has included support for hierarchical phrase-based rule extraction since the decoder was first extended to support syntax-based translation (Hoang et al., 2009). In this paper we provide some implementation details for the recently-added GHKM rule extractor and for the related scope-3 decoding algorithm. We then describe the University of Edinburgh’s GHKMbased English-German submission to the WMT translation task and present comparisons with hierarchical systems trained on the same data. To our knowledge, these are the first GHKM results presented for English-German, a language pair with a high degree of reordering and rich target-side morphology. 2 GHKM Rule Extraction in Moses A basic GHKM rule extractor was first developed for Moses during the fou"
W12-3150,D10-1063,0,0.102856,"the rule extractor eliminates these rules using the method described in Chung et al. (2011). 2.4 Scope Pruning Unlike hierarchical phrase-based rule extraction, GHKM places no restriction on the rank of the resulting rules. In order that the grammar can be parsed efficiently, one of two approaches is usually taken: (i) synchronous binarization (Zhang et al., 2006), which transforms the original grammar to a weakly equivalent form in which no rule has rank greater than two. This makes the grammar amenable to decoding with a standard chart-parsing algorithm such as CYK, and (ii) scope pruning (Hopkins and Langmead, 2010), which eliminates rules in order to produce a subgrammar that can be parsed in cubic time. Of these two approaches, Moses currently supports only the latter. Both rule extractors prune the extracted grammar to remove rules with scope greater than three. The next section describes the parsing algorithm that is used for scope-3 grammars. 3 Scope-3 Parsing in Moses Hopkins and Langmead (2010) show that a sentence of length n can be parsed using a scope-k grammar in O(nk ) chart updates. In this section, we describe some details of Moses’ implementation of their chart parsing method. mar trie ver"
W12-3150,N03-1017,1,0.0131726,"ar Size 118,649,771 12,748,259 40,661,639 27,002,733 Table 1: Grammar sizes (distinct rule counts) after filtering for the newstest-2011 test set 4.3 Features Our feature functions include the n-gram language model probability of the derivation’s target yield, its word count, and various scores for the synchronous derivation. We score grammar rules according to the following functions: • p(RHSs |RHSt , LHS), the noisy-channel translation probability. • p(LHS, RHSt |RHSs ), the direct translation probability. • plex (RHSt |RHSs ) and plex (RHSs |RHSt ), the direct and indirect lexical weights (Koehn et al., 2003). • ppcfg (FRAGt ), the monolingual PCFG probability of the tree fragment from which the rule was extracted (GHKM and target-annotated Q systems only). This is defined as ni=1 p(ri ), where r1 . . . rn are the constituent CFG rules of the fragment. The PCFG parameters are estimated from the parse of the target-side training data. All lexical CFG rules are given the probability 1. This is similar to the pcfg feature used in Marcu et al. (2006) and is intended to encourage the production of syntactically wellformed derivations. • exp(−1/count(r)), a rule rareness penalty. • exp(1), a rule penalt"
W12-3150,P07-2045,1,0.0159691,"rget-side syntactic structure at the heart of the rule extraction process, producing extended tree transducer rules that map between strings and tree fragments. Ultimately, both methods define rules according to a sentence pair’s word-alignments. Without any restriction on rule size they will produce an exponentially large set of rules and so in practice only a subgrammar can be extracted. It is the differing rule selection heuristics that distinguish these two approaches, with hierarchical approaches being motivated by phrasal coverage and GHKM by targetside tree coverage. The Moses toolkit (Koehn et al., 2007) has included support for hierarchical phrase-based rule extraction since the decoder was first extended to support syntax-based translation (Hoang et al., 2009). In this paper we provide some implementation details for the recently-added GHKM rule extractor and for the related scope-3 decoding algorithm. We then describe the University of Edinburgh’s GHKMbased English-German submission to the WMT translation task and present comparisons with hierarchical systems trained on the same data. To our knowledge, these are the first GHKM results presented for English-German, a language pair with a hi"
W12-3150,W10-1715,1,0.805545,"entation of minimum error rate training (Och, 2003). 2 http://www.ims.uni-stuttgart.de/tcl/ SOFTWARE/BitPar.html 391 For the hierarchical phrase-based model we used the default Moses rule extraction settings, which are taken from Chiang (2007). For target-annotated models, the syntactic constraints imposed by the parse trees reduce the grammar size significantly. This allows us to relax the rule extraction settings, which we have previously found to benefit translation quality, without producing an unusably large grammar. We use identical settings to those used in WMT’s 2010 translation task (Koehn et al., 2010). Specifically, we relax the hierarchical phrase-based extraction settings in the following ways: • Up to seven source-side symbols are allowed. • Consecutive source non-terminals are permitted. • Single-word lexical phrases are allowed for hierarchical subphrase subtraction. • Initial phrases are limited to 15 source words (instead of 10). By using the scope-3 parser we can also relax the restriction on grammar rank. For comparison, we extract two target-annotated grammars, one with a maximum rank of two, and one with an unlimited rank but subject to scope-3 pruning. GHKM rule extraction uses"
W12-3150,W06-1606,0,0.0446094,"ability. • p(LHS, RHSt |RHSs ), the direct translation probability. • plex (RHSt |RHSs ) and plex (RHSs |RHSt ), the direct and indirect lexical weights (Koehn et al., 2003). • ppcfg (FRAGt ), the monolingual PCFG probability of the tree fragment from which the rule was extracted (GHKM and target-annotated Q systems only). This is defined as ni=1 p(ri ), where r1 . . . rn are the constituent CFG rules of the fragment. The PCFG parameters are estimated from the parse of the target-side training data. All lexical CFG rules are given the probability 1. This is similar to the pcfg feature used in Marcu et al. (2006) and is intended to encourage the production of syntactically wellformed derivations. • exp(−1/count(r)), a rule rareness penalty. • exp(1), a rule penalty. The main grammar and glue grammars have distinct penalty features. 4.4 Decoder Settings For the submitted GHKM system we used a maximum chart span setting of 25. For the other systems we used settings that matched the rule extraction spans: 10 for hierarchical phrase-based, 15 for target syntax, and unlimited for GHKM. 392 We used the scope-3 parsing algorithm (enabled using the option -parsing-algorithm 1) for all systems except the hiera"
W12-3150,J03-1002,0,0.00335179,"tractable from the alignment graph in Figure 1. Rules are written in the form LHS → RHS s |RHSt . the case of ✸ of 4.2 Rule Extraction ✸ ✸ case ✸ ✸ Alexander Nikitin Figure 3: Example grammar trie. The filled vertices hold associative array values. 2,043,914 sentence pairs. For the target syntax experiments, the German-side of the parallel corpus was parsed using the BitPar2 parser. If a parse failed then the sentence pair was discarded, leaving a total of 2,028,556 pairs. The parallel corpus was then word-aligned using MGIZA++ (Gao and Vogel, 2008), a multi-threaded implementation of GIZA++ (Och and Ney, 2003). We used all available monolingual German data to train seven 5-gram language models (one each for Europarl, News Commentary, and the five News data sets). These were interpolated using weights optimised against the development set and the resulting language model was used in experiments. We used the SRILM toolkit (Stolcke, 2002) with Kneser-Ney smoothing (Chen and Goodman, 1998). The baseline system’s feature weights were tuned on the news-test2008 dev set (2,051 sentence pairs) using Moses’ implementation of minimum error rate training (Och, 2003). 2 http://www.ims.uni-stuttgart.de/tcl/ SOF"
W12-3150,P03-1021,0,0.003497,"-threaded implementation of GIZA++ (Och and Ney, 2003). We used all available monolingual German data to train seven 5-gram language models (one each for Europarl, News Commentary, and the five News data sets). These were interpolated using weights optimised against the development set and the resulting language model was used in experiments. We used the SRILM toolkit (Stolcke, 2002) with Kneser-Ney smoothing (Chen and Goodman, 1998). The baseline system’s feature weights were tuned on the news-test2008 dev set (2,051 sentence pairs) using Moses’ implementation of minimum error rate training (Och, 2003). 2 http://www.ims.uni-stuttgart.de/tcl/ SOFTWARE/BitPar.html 391 For the hierarchical phrase-based model we used the default Moses rule extraction settings, which are taken from Chiang (2007). For target-annotated models, the syntactic constraints imposed by the parse trees reduce the grammar size significantly. This allows us to relax the rule extraction settings, which we have previously found to benefit translation quality, without producing an unusably large grammar. We use identical settings to those used in WMT’s 2010 translation task (Koehn et al., 2010). Specifically, we relax the hie"
W12-3150,N06-1033,0,0.017673,"ise, it is attached to the root of the parse tree. 2.3 Unary Rule Elimination Moses’ chart decoder does not currently support the use of grammars containing fully non-lexical unary rules (such as NP → X1 |NN1 ). Unless the --AllowUnary option is given, the rule extractor eliminates these rules using the method described in Chung et al. (2011). 2.4 Scope Pruning Unlike hierarchical phrase-based rule extraction, GHKM places no restriction on the rank of the resulting rules. In order that the grammar can be parsed efficiently, one of two approaches is usually taken: (i) synchronous binarization (Zhang et al., 2006), which transforms the original grammar to a weakly equivalent form in which no rule has rank greater than two. This makes the grammar amenable to decoding with a standard chart-parsing algorithm such as CYK, and (ii) scope pruning (Hopkins and Langmead, 2010), which eliminates rules in order to produce a subgrammar that can be parsed in cubic time. Of these two approaches, Moses currently supports only the latter. Both rule extractors prune the extracted grammar to remove rules with scope greater than three. The next section describes the parsing algorithm that is used for scope-3 grammars. 3"
W12-3150,W06-3119,0,0.0650088,"ction Over the last few years, syntax-based rule extraction has largely developed along two lines, one originating in hierarchical phrase-based translation (Chiang, 2005; Chiang, 2007) and the other in GHKM (Galley et al., 2004; Galley et al., 2006). Hierarchical rule extraction generalizes the established phrase-based extraction method to produce formally-syntactic synchronous context-free grammar rules without any requirement for linguistic annotation of the training data. In subsequent work, the approach has been extended to incorporate linguistic annotation on the target side (as in SAMT (Zollmann and Venugopal, 2006)) or on both sides (Chiang, 2010). In contrast, GHKM places target-side syntactic structure at the heart of the rule extraction process, producing extended tree transducer rules that map between strings and tree fragments. Ultimately, both methods define rules according to a sentence pair’s word-alignments. Without any restriction on rule size they will produce an exponentially large set of rules and so in practice only a subgrammar can be extracted. It is the differing rule selection heuristics that distinguish these two approaches, with hierarchical approaches being motivated by phrasal cove"
W12-3154,D11-1033,0,0.252188,"vera and Juan, 2007; Koehn and Schroeder, 2007) is the best approach. Also, other authors (Foster et al., 2010; Niehues and Waibel, 2010; Shah et al., 2010) have tried to weight the input sentences or extracted phrases before the phrase tables are built. In this type of approach, the main problem is how to train the weights of the sentences or phrases, and each of the papers has followed a different approach. Instead of weighting the out-of-domain data, some authors have investigated data selection methods for domain adaptation (Yasuda et al., 2008; Mansour et al., 2011; Schwenk et al., 2011; Axelrod et al., 2011). This is effectively the same as using a 1-0 weighting for input sentences, but has the advantage that it is usually easier to tune a threshold than it is to train weights for all input sentences or phrases. The other advantage of doing data selection is that it can potentially remove noisy (e.g. incorrectly aligned) data. However it will be seen later in this paper that out-of-domain data can usually contribute something useful to the translation system, so the 1-0 weighting of data-selection may be somewhat heavy-handed. 3 Experiments 3.1 mented using the Moses multi-bleu.pl script. Corpora"
W12-3154,2011.iwslt-evaluation.18,0,0.359249,"mixed results, with the overall conclusion being that it is difficult to predict how best to include out-of-domain data in the PBMT training pipeline. Unlike in the current work, Duh et al. do not separate phrase extraction and scoring in order to analyse the effect of domain on them separately. They make the point that adding extra out-of-domain data 423 may degrade translation by introducing unwanted lexical ambiguity, showing anecdotal evidence for this. Similar arguments were presented in (Sennrich, 2012). A recent paper which does attempt to tease apart phrase extraction and scoring is (Bisazza et al., 2011). In this work, the authors try to improve a system trained on in-domain data by including extra entries (termed “fill-up”) from out-of-domain data – this is similar to the nc+epE and st+epE systems in Section 3.4. It is shown by Bisazza et al. that this fill-up technique has a similar effect to using MERT to weight the in and out-of domain phrase tables. In the experiments in Section 3.4 we confirm that fillup techniques mostly provide better results than using a concatenation of in and out-of domain data. There has been quite a lot of work on finding ways of weighting in and out-of domain da"
W12-3154,W07-0722,0,0.0412156,"on 3.4 we confirm that fillup techniques mostly provide better results than using a concatenation of in and out-of domain data. There has been quite a lot of work on finding ways of weighting in and out-of domain data for SMT (as opposed to simply concatenating the data sets), both for language and translation modelling. Interpolating language models using perplexity is fairly well-established (e.g. Koehn and Schroeder (2007)), but for phrase-tables it is unclear whether perplexity minimisation (Foster et al., 2010; Sennrich, 2012) or linear or log-linear interpolation (Foster and Kuhn, 2007; Civera and Juan, 2007; Koehn and Schroeder, 2007) is the best approach. Also, other authors (Foster et al., 2010; Niehues and Waibel, 2010; Shah et al., 2010) have tried to weight the input sentences or extracted phrases before the phrase tables are built. In this type of approach, the main problem is how to train the weights of the sentences or phrases, and each of the papers has followed a different approach. Instead of weighting the out-of-domain data, some authors have investigated data selection methods for domain adaptation (Yasuda et al., 2008; Mansour et al., 2011; Schwenk et al., 2011; Axelrod et al., 201"
W12-3154,P11-2031,0,0.0163226,"systems consisted of phrase tables and lexicalised reordering tables estimated using the standard Moses (Koehn et al., 2007) training pipeline, and 5-gram Kneser-Ney smoothed language models estimated using the SRILM toolkit (Stolcke, 2002), with KenLM (Heafield, 2011) used at runtime. Separate language models were built on the target side of the in-domain and out-ofdomain training data, then linearly interpolated using SRILM to minimise perplexity on the tuning set (e.g. Koehn and Schroeder (2007)). Tuning of models used minimum error rate training (Och, 2003), repeated 3 times and averaged (Clark et al., 2011). Performance is evaluated using caseinsensitive BLEU (Papineni et al., 2002), as imple1 www.opensubtitles.org 424 Europarl (ep) News Commentary (nc) Subtitles (st) Language pairs en↔fr en↔es en↔de en↔cs en↔nl en↔fr en↔es en↔de en↔cs en↔fr en↔es en↔nl en↔cs train tune test 1.8M 1.8M 1.7M 460k 1.8M 114k 130k 135k 122k 200k 200k 200k 200k n/a n/a n/a n/a n/a 1000 1000 1000 1000 2000 2000 2000 2000 n/a n/a n/a n/a n/a 2000 2000 2000 2000 2000 2000 2000 2000 Table 1: Summary of the data sets used, with approximate sentence counts 3.2 Comparing In-domain and Out-of-domain Data The aim of this secti"
W12-3154,W07-0717,0,0.319034,"he experiments in Section 3.4 we confirm that fillup techniques mostly provide better results than using a concatenation of in and out-of domain data. There has been quite a lot of work on finding ways of weighting in and out-of domain data for SMT (as opposed to simply concatenating the data sets), both for language and translation modelling. Interpolating language models using perplexity is fairly well-established (e.g. Koehn and Schroeder (2007)), but for phrase-tables it is unclear whether perplexity minimisation (Foster et al., 2010; Sennrich, 2012) or linear or log-linear interpolation (Foster and Kuhn, 2007; Civera and Juan, 2007; Koehn and Schroeder, 2007) is the best approach. Also, other authors (Foster et al., 2010; Niehues and Waibel, 2010; Shah et al., 2010) have tried to weight the input sentences or extracted phrases before the phrase tables are built. In this type of approach, the main problem is how to train the weights of the sentences or phrases, and each of the papers has followed a different approach. Instead of weighting the out-of-domain data, some authors have investigated data selection methods for domain adaptation (Yasuda et al., 2008; Mansour et al., 2011; Schwenk et al., 20"
W12-3154,D10-1044,0,0.346701,"Missing"
W12-3154,W11-2123,0,0.0271344,"ining corpus was selected from the remaining data. Using test sets from both news-commentary and OpenSubtitles gives two domain adaptation tasks, where in both cases the out-of-domain data is europarl, a significantly larger training set than the indomain data. The three data sets in use in this paper are summarised in Table 1. The translation systems consisted of phrase tables and lexicalised reordering tables estimated using the standard Moses (Koehn et al., 2007) training pipeline, and 5-gram Kneser-Ney smoothed language models estimated using the SRILM toolkit (Stolcke, 2002), with KenLM (Heafield, 2011) used at runtime. Separate language models were built on the target side of the in-domain and out-ofdomain training data, then linearly interpolated using SRILM to minimise perplexity on the tuning set (e.g. Koehn and Schroeder (2007)). Tuning of models used minimum error rate training (Och, 2003), repeated 3 times and averaged (Clark et al., 2011). Performance is evaluated using caseinsensitive BLEU (Papineni et al., 2002), as imple1 www.opensubtitles.org 424 Europarl (ep) News Commentary (nc) Subtitles (st) Language pairs en↔fr en↔es en↔de en↔cs en↔nl en↔fr en↔es en↔de en↔cs en↔fr en↔es en↔n"
W12-3154,W07-0733,1,0.583028,"pE systems in Section 3.4. It is shown by Bisazza et al. that this fill-up technique has a similar effect to using MERT to weight the in and out-of domain phrase tables. In the experiments in Section 3.4 we confirm that fillup techniques mostly provide better results than using a concatenation of in and out-of domain data. There has been quite a lot of work on finding ways of weighting in and out-of domain data for SMT (as opposed to simply concatenating the data sets), both for language and translation modelling. Interpolating language models using perplexity is fairly well-established (e.g. Koehn and Schroeder (2007)), but for phrase-tables it is unclear whether perplexity minimisation (Foster et al., 2010; Sennrich, 2012) or linear or log-linear interpolation (Foster and Kuhn, 2007; Civera and Juan, 2007; Koehn and Schroeder, 2007) is the best approach. Also, other authors (Foster et al., 2010; Niehues and Waibel, 2010; Shah et al., 2010) have tried to weight the input sentences or extracted phrases before the phrase tables are built. In this type of approach, the main problem is how to train the weights of the sentences or phrases, and each of the papers has followed a different approach. Instead of wei"
W12-3154,2011.iwslt-papers.5,0,0.0606385,"ear interpolation (Foster and Kuhn, 2007; Civera and Juan, 2007; Koehn and Schroeder, 2007) is the best approach. Also, other authors (Foster et al., 2010; Niehues and Waibel, 2010; Shah et al., 2010) have tried to weight the input sentences or extracted phrases before the phrase tables are built. In this type of approach, the main problem is how to train the weights of the sentences or phrases, and each of the papers has followed a different approach. Instead of weighting the out-of-domain data, some authors have investigated data selection methods for domain adaptation (Yasuda et al., 2008; Mansour et al., 2011; Schwenk et al., 2011; Axelrod et al., 2011). This is effectively the same as using a 1-0 weighting for input sentences, but has the advantage that it is usually easier to tune a threshold than it is to train weights for all input sentences or phrases. The other advantage of doing data selection is that it can potentially remove noisy (e.g. incorrectly aligned) data. However it will be seen later in this paper that out-of-domain data can usually contribute something useful to the translation system, so the 1-0 weighting of data-selection may be somewhat heavy-handed. 3 Experiments 3.1 mented"
W12-3154,2010.eamt-1.29,0,0.0437338,"domain data. There has been quite a lot of work on finding ways of weighting in and out-of domain data for SMT (as opposed to simply concatenating the data sets), both for language and translation modelling. Interpolating language models using perplexity is fairly well-established (e.g. Koehn and Schroeder (2007)), but for phrase-tables it is unclear whether perplexity minimisation (Foster et al., 2010; Sennrich, 2012) or linear or log-linear interpolation (Foster and Kuhn, 2007; Civera and Juan, 2007; Koehn and Schroeder, 2007) is the best approach. Also, other authors (Foster et al., 2010; Niehues and Waibel, 2010; Shah et al., 2010) have tried to weight the input sentences or extracted phrases before the phrase tables are built. In this type of approach, the main problem is how to train the weights of the sentences or phrases, and each of the papers has followed a different approach. Instead of weighting the out-of-domain data, some authors have investigated data selection methods for domain adaptation (Yasuda et al., 2008; Mansour et al., 2011; Schwenk et al., 2011; Axelrod et al., 2011). This is effectively the same as using a 1-0 weighting for input sentences, but has the advantage that it is usual"
W12-3154,W99-0604,0,0.0919105,"kely to apply to other types of SMT systems. Furthermore, we will mainly be concerned with the effect of domain on the translation model, since it depends on parallel data which is more likely to be in short supply than monolingual data, and domain adaptation for language modelling has been more thoroughly studied. The effect of a shift of domain in the parallel data is complicated by the fact that training a translation model is a multi-stage process. First the parallel data is word-aligned, normally using the IBM models (Brown et al., 1994), then phrases are extracted using some heuristics (Och et al., 1999) and scored using a maximum likelihood estimate. Since the effect of domain may be felt at the alignment stage, the extraction stage, or the scoring stage, we have designed experiments to try to tease these apart. Experiments comparing the effect of domain at the alignment stage with the extraction and scoring stages have already been presented by (Duh et al., 2010), so we focus more on the differences between extraction and scoring. In other words, we examine whether adding more data (in or out-of domain) helps improve coverage of the phrase table, or helps improve the scoring of phrases. A f"
W12-3154,P03-1021,0,0.00721265,"are summarised in Table 1. The translation systems consisted of phrase tables and lexicalised reordering tables estimated using the standard Moses (Koehn et al., 2007) training pipeline, and 5-gram Kneser-Ney smoothed language models estimated using the SRILM toolkit (Stolcke, 2002), with KenLM (Heafield, 2011) used at runtime. Separate language models were built on the target side of the in-domain and out-ofdomain training data, then linearly interpolated using SRILM to minimise perplexity on the tuning set (e.g. Koehn and Schroeder (2007)). Tuning of models used minimum error rate training (Och, 2003), repeated 3 times and averaged (Clark et al., 2011). Performance is evaluated using caseinsensitive BLEU (Papineni et al., 2002), as imple1 www.opensubtitles.org 424 Europarl (ep) News Commentary (nc) Subtitles (st) Language pairs en↔fr en↔es en↔de en↔cs en↔nl en↔fr en↔es en↔de en↔cs en↔fr en↔es en↔nl en↔cs train tune test 1.8M 1.8M 1.7M 460k 1.8M 114k 130k 135k 122k 200k 200k 200k 200k n/a n/a n/a n/a n/a 1000 1000 1000 1000 2000 2000 2000 2000 n/a n/a n/a n/a n/a 2000 2000 2000 2000 2000 2000 2000 2000 Table 1: Summary of the data sets used, with approximate sentence counts 3.2 Comparing In"
W12-3154,P02-1040,0,0.105515,"ated using the standard Moses (Koehn et al., 2007) training pipeline, and 5-gram Kneser-Ney smoothed language models estimated using the SRILM toolkit (Stolcke, 2002), with KenLM (Heafield, 2011) used at runtime. Separate language models were built on the target side of the in-domain and out-ofdomain training data, then linearly interpolated using SRILM to minimise perplexity on the tuning set (e.g. Koehn and Schroeder (2007)). Tuning of models used minimum error rate training (Och, 2003), repeated 3 times and averaged (Clark et al., 2011). Performance is evaluated using caseinsensitive BLEU (Papineni et al., 2002), as imple1 www.opensubtitles.org 424 Europarl (ep) News Commentary (nc) Subtitles (st) Language pairs en↔fr en↔es en↔de en↔cs en↔nl en↔fr en↔es en↔de en↔cs en↔fr en↔es en↔nl en↔cs train tune test 1.8M 1.8M 1.7M 460k 1.8M 114k 130k 135k 122k 200k 200k 200k 200k n/a n/a n/a n/a n/a 1000 1000 1000 1000 2000 2000 2000 2000 n/a n/a n/a n/a n/a 2000 2000 2000 2000 2000 2000 2000 2000 Table 1: Summary of the data sets used, with approximate sentence counts 3.2 Comparing In-domain and Out-of-domain Data The aim of this section is to provide both a qualitative and quantitative comparison of the three"
W12-3154,W11-2158,0,0.0188191,"ter and Kuhn, 2007; Civera and Juan, 2007; Koehn and Schroeder, 2007) is the best approach. Also, other authors (Foster et al., 2010; Niehues and Waibel, 2010; Shah et al., 2010) have tried to weight the input sentences or extracted phrases before the phrase tables are built. In this type of approach, the main problem is how to train the weights of the sentences or phrases, and each of the papers has followed a different approach. Instead of weighting the out-of-domain data, some authors have investigated data selection methods for domain adaptation (Yasuda et al., 2008; Mansour et al., 2011; Schwenk et al., 2011; Axelrod et al., 2011). This is effectively the same as using a 1-0 weighting for input sentences, but has the advantage that it is usually easier to tune a threshold than it is to train weights for all input sentences or phrases. The other advantage of doing data selection is that it can potentially remove noisy (e.g. incorrectly aligned) data. However it will be seen later in this paper that out-of-domain data can usually contribute something useful to the translation system, so the 1-0 weighting of data-selection may be somewhat heavy-handed. 3 Experiments 3.1 mented using the Moses multi-"
W12-3154,E12-1055,0,0.212758,"scoring stages. Extensive experiments on 4 different data sets, and 10 different language pairs show mixed results, with the overall conclusion being that it is difficult to predict how best to include out-of-domain data in the PBMT training pipeline. Unlike in the current work, Duh et al. do not separate phrase extraction and scoring in order to analyse the effect of domain on them separately. They make the point that adding extra out-of-domain data 423 may degrade translation by introducing unwanted lexical ambiguity, showing anecdotal evidence for this. Similar arguments were presented in (Sennrich, 2012). A recent paper which does attempt to tease apart phrase extraction and scoring is (Bisazza et al., 2011). In this work, the authors try to improve a system trained on in-domain data by including extra entries (termed “fill-up”) from out-of-domain data – this is similar to the nc+epE and st+epE systems in Section 3.4. It is shown by Bisazza et al. that this fill-up technique has a similar effect to using MERT to weight the in and out-of domain phrase tables. In the experiments in Section 3.4 we confirm that fillup techniques mostly provide better results than using a concatenation of in and o"
W12-3154,W10-1759,0,0.0154759,"en quite a lot of work on finding ways of weighting in and out-of domain data for SMT (as opposed to simply concatenating the data sets), both for language and translation modelling. Interpolating language models using perplexity is fairly well-established (e.g. Koehn and Schroeder (2007)), but for phrase-tables it is unclear whether perplexity minimisation (Foster et al., 2010; Sennrich, 2012) or linear or log-linear interpolation (Foster and Kuhn, 2007; Civera and Juan, 2007; Koehn and Schroeder, 2007) is the best approach. Also, other authors (Foster et al., 2010; Niehues and Waibel, 2010; Shah et al., 2010) have tried to weight the input sentences or extracted phrases before the phrase tables are built. In this type of approach, the main problem is how to train the weights of the sentences or phrases, and each of the papers has followed a different approach. Instead of weighting the out-of-domain data, some authors have investigated data selection methods for domain adaptation (Yasuda et al., 2008; Mansour et al., 2011; Schwenk et al., 2011; Axelrod et al., 2011). This is effectively the same as using a 1-0 weighting for input sentences, but has the advantage that it is usually easier to tune a"
W12-3154,I08-2088,0,0.00929917,"or linear or log-linear interpolation (Foster and Kuhn, 2007; Civera and Juan, 2007; Koehn and Schroeder, 2007) is the best approach. Also, other authors (Foster et al., 2010; Niehues and Waibel, 2010; Shah et al., 2010) have tried to weight the input sentences or extracted phrases before the phrase tables are built. In this type of approach, the main problem is how to train the weights of the sentences or phrases, and each of the papers has followed a different approach. Instead of weighting the out-of-domain data, some authors have investigated data selection methods for domain adaptation (Yasuda et al., 2008; Mansour et al., 2011; Schwenk et al., 2011; Axelrod et al., 2011). This is effectively the same as using a 1-0 weighting for input sentences, but has the advantage that it is usually easier to tune a threshold than it is to train weights for all input sentences or phrases. The other advantage of doing data selection is that it can potentially remove noisy (e.g. incorrectly aligned) data. However it will be seen later in this paper that out-of-domain data can usually contribute something useful to the translation system, so the 1-0 weighting of data-selection may be somewhat heavy-handed. 3 E"
W12-3154,J93-2003,0,\N,Missing
W12-3154,W09-0401,1,\N,Missing
W12-3154,P07-2045,1,\N,Missing
W12-3154,2010.iwslt-papers.5,0,\N,Missing
W13-2201,W13-2205,0,0.0583032,"Missing"
W13-2201,S13-1034,0,0.0277746,"Missing"
W13-2201,C12-1008,0,0.0091484,"ignments is used to extract quantitative (amount and distribution of the alignments) and qualitative (importance of the aligned terms) features under the assumption that alignment information can help tasks where sentencelevel semantic relations need to be identified (Souza et al., 2013). Three similar EnglishSpanish systems are built and used to provide pseudo-references (Soricut et al., 2012) and back-translations, from which automatic MT evaluation metrics could be computed and used as features. DFKI (T1.2, T1.3): DFKI’s submission for Task 1.2 was based on decomposing rankings into pairs (Avramidis, 2012), where the best system for each pair was predicted with Logistic Regression (LogReg). For GermanEnglish, LogReg was trained with Stepwise Feature Selection (Hosmer, 1989) on two feature sets: Feature Set 24 includes basic counts augmented with PCFG parsing features (number of VPs, alternative parses, parse probability) on both source and target sentences (Avramidis et al., 2011), and pseudo-reference METEOR score; the most successful set, Feature Set 33 combines those 24 features with the 17 baseline features. For English-Spanish, LogReg was used with L2 Regularisation (Lin et al., 2007) and"
W13-2201,W13-2206,0,0.0913052,"ool of Data Analysis (Borisov et al., 2013) Carnegie Mellon University (Ammar et al., 2013) CMU - TREE - TO - TREE CU - BOJAR , Charles University in Prague (Bojar et al., 2013) CU - DEPFIX , CU - TAMCHYNA CU - KAREL , CU - ZEMAN CU - PHRASEFIX , Charles University in Prague (B´ılek and Zeman, 2013) Charles University in Prague (Galuˇscˇ a´ kov´a et al., 2013) CU - TECTOMT DCU DCU - FDA DCU - OKITA DESRT ITS - LATL JHU KIT LIA LIMSI MES -* OMNIFLUENT PROMT QCRI - MES QUAERO RWTH SHEF STANFORD TALP - UPC TUBITAK UCAM UEDIN , Dublin City University (Rubino et al., 2013a) Dublin City University (Bicici, 2013a) Dublin City University (Okita et al., 2013) Universit`a di Pisa (Miceli Barone and Attardi, 2013) University of Geneva Johns Hopkins University (Post et al., 2013) Karlsruhe Institute of Technology (Cho et al., 2013) Universit´e d’Avignon (Huet et al., 2013) LIMSI (Allauzen et al., 2013) Munich / Edinburgh / Stuttgart (Durrani et al., 2013a; Weller et al., 2013) SAIC (Matusov and Leusch, 2013) PROMT Automated Translations Solutions Qatar / Munich / Edinburgh / Stuttgart (Sajjad et al., 2013) QUAERO (Peitz et al., 2013a) RWTH Aachen (Peitz et al., 2013b) University of Sheffield Stanford Univ"
W13-2201,W13-2240,0,0.0186069,"Missing"
W13-2201,W13-2242,0,0.220429,"ool of Data Analysis (Borisov et al., 2013) Carnegie Mellon University (Ammar et al., 2013) CMU - TREE - TO - TREE CU - BOJAR , Charles University in Prague (Bojar et al., 2013) CU - DEPFIX , CU - TAMCHYNA CU - KAREL , CU - ZEMAN CU - PHRASEFIX , Charles University in Prague (B´ılek and Zeman, 2013) Charles University in Prague (Galuˇscˇ a´ kov´a et al., 2013) CU - TECTOMT DCU DCU - FDA DCU - OKITA DESRT ITS - LATL JHU KIT LIA LIMSI MES -* OMNIFLUENT PROMT QCRI - MES QUAERO RWTH SHEF STANFORD TALP - UPC TUBITAK UCAM UEDIN , Dublin City University (Rubino et al., 2013a) Dublin City University (Bicici, 2013a) Dublin City University (Okita et al., 2013) Universit`a di Pisa (Miceli Barone and Attardi, 2013) University of Geneva Johns Hopkins University (Post et al., 2013) Karlsruhe Institute of Technology (Cho et al., 2013) Universit´e d’Avignon (Huet et al., 2013) LIMSI (Allauzen et al., 2013) Munich / Edinburgh / Stuttgart (Durrani et al., 2013a; Weller et al., 2013) SAIC (Matusov and Leusch, 2013) PROMT Automated Translations Solutions Qatar / Munich / Edinburgh / Stuttgart (Sajjad et al., 2013) QUAERO (Peitz et al., 2013a) RWTH Aachen (Peitz et al., 2013b) University of Sheffield Stanford Univ"
W13-2201,W13-2207,0,0.0357036,"Missing"
W13-2201,W11-2104,0,0.00810596,"Missing"
W13-2201,P10-2016,1,0.826191,"Missing"
W13-2201,W13-2208,1,0.743011,"Missing"
W13-2201,P11-1022,0,0.112625,"the intended application. In Task 2 we tested binary word-level classification in a post-editing setting. If such annotation is presented through a user interface we imagine that words marked as incorrect would be hidden from the editor, highlighted as possibly wrong or that a list of alternatives would we generated. With respect to the poor improvements over trivial baselines, we consider that the results for word-level prediction could be mostly connected to limitations of the datasets provided, which are very small for word-level prediction, as compared to successful previous work such as (Bach et al., 2011). Despite the limited amount of training data, several systems were able to predict dubious words (binary variant of the task), showing that this can be a promising task. Extending the granularity even further by predicting the actual editing action necessary for a word yielded less positive results than the binary setting. We cannot directly compare sentence- and word-level results. However, since sentence-level predictions can benefit from more information available and therefore more signal on which the prediction is based, the natural conclusion is that, if there is a choice in the predict"
W13-2201,W13-2209,0,0.034542,"Missing"
W13-2201,W13-2241,1,0.0862471,"Missing"
W13-2201,W07-0718,1,0.697635,"to five alternative translations for a given source sentence, prediction of post-editing time for a sentence, and prediction of word-level scores for a given translation (correct/incorrect and types of edits). The datasets included English-Spanish and GermanEnglish news translations produced by a number of machine translation systems. This marks the second year we have conducted this task. Introduction We present the results of the shared tasks of the Workshop on Statistical Machine Translation (WMT) held at ACL 2013. This workshop builds on seven previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012). This year we conducted three official tasks: a translation task, a human evaluation of translation results, and a quality estimation task.1 In the translation task (§2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data. We held ten translation tasks this year, between English and each of Czech, French, German, Spanish, and Russian. The Russian translation tasks were new this year, and were also the most popular. The system outputs for each task were evaluated both automatically and manually."
W13-2201,W11-2103,1,0.5768,"Missing"
W13-2201,W13-2243,1,0.754887,"Missing"
W13-2201,W13-2213,0,0.0301468,"Missing"
W13-2201,P05-1022,0,0.0401846,"parses of the source and target sentence, the positions of the phrases with the lowest and highest probability and future cost estimate in the translation, the counts of phrases in the decoding graph whose probability or whether the future cost estimate is higher/lower than their standard deviation, counts of verbs and determiners, etc. The second submission (pls8) was trained with Partial Least Squares regression (Stone and Brooks, 1990) including more glass-box features. The following NLP tools were used in feature extraction: the Brown English Wall-StreetJournal-trained statistical parser (Charniak and Johnson, 2005), a Lexical Functional Grammar parser (XLE), together with a hand-crafted Lexical Functional Grammar, the English ParGram grammar (Kaplan et al., 2004), and the TreeTagger part-of-speech tagger (Schmidt, 1994) with off-the-shelf publicly available pre-trained tagging models for English and Spanish. For pseudoreference features, the Bing, Moses and Systran translation systems were used. The Mallet toolkit (McCallum, 2002) was used to build the topic models and features based on a grammar checker were extracted with LanguageTool.16 FBK-Uedin (T1.1, T1.3): The submissions explored features built"
W13-2201,W13-2210,0,0.0366902,"Missing"
W13-2201,W13-2212,1,0.1814,"Missing"
W13-2201,W13-2214,0,0.0302242,"Missing"
W13-2201,W13-2217,0,0.0313645,"Missing"
W13-2201,W13-2215,0,0.0174795,"Missing"
W13-2201,W13-2253,0,0.055006,"Missing"
W13-2201,W13-2246,0,0.0627442,"Missing"
W13-2201,N06-1058,0,0.0556187,"of the official test set size, and provide 4311 unique references. On average, one sentence in our set has 2.94±2.17 unique reference translations. Table 10 provides a histogram. It is well known that automatic MT evaluation methods perform better with more references, because a single one may not confirm a correct part of MT output. This issue is more severe for morphologically rich languages like Czech where about 1/3 of MT output was correct but not confirmed by the reference (Bojar et al., 2010). Advanced evaluation methods apply paraphrasing to smooth out some of the lexical divergence (Kauchak and Barzilay, 2006; Snover et al., 2009; Denkowski and Lavie, 2010). Simpler techniques such as lemmatizing are effective for morphologically rich languages (Tantug et al., 2008; Kos and Bojar, 2009) but they will lose resolution once the systems start performing generally well. WMTs have taken the stance that a big enough test set with just a single reference should compensate for the lack of other references. We use our post-edited reference translations to check this assumption for BLEU and NIST as implemented in mteval-13a (international tokenization switched on, which is not the default setting). We run ma"
W13-2201,W13-2248,0,0.0488662,"Missing"
W13-2201,2012.iwslt-papers.5,1,0.674636,"ts are somewhat artificially more diverse; in narrow domains, source sentences can repeat and even appear verbatim in the training data, and in natural test sets with multiple references, short sentences can receive several identical translations. For each probe, we measure the Spearman’s rank correlation coefficient ρ of the ranks proposed by BLEU or NIST and the manual ranks. We use the same implementation as applied in the WMT13 Shared Metrics Task (Mach´acˇ ek and Bojar, 2013). Note that the WMT13 metrics task still uses the WMT12 evaluation method ignoring ties, not the expected wins. As Koehn (2012) shows, the two methods do not differ much. Overall, the correlation is strongly impacted by Figure 5: Correlation of BLEU and WMT13 manual ranks for English→Czech translation Figure 6: Correlation of NIST and WMT13 manual ranks for English→Czech translation the particular choice of test sentences and reference translations. By picking sentences randomly, similarly or equally sized test sets can reach different correlations. Indeed, e.g. for a test set of about 1500 distinct sentences selected from the 3000-sentence official test set (1 reference translation), we obtain correlations for BLEU b"
W13-2201,W13-2202,1,0.761984,"Missing"
W13-2201,W06-3114,1,0.635019,"ntence, ranking of up to five alternative translations for a given source sentence, prediction of post-editing time for a sentence, and prediction of word-level scores for a given translation (correct/incorrect and types of edits). The datasets included English-Spanish and GermanEnglish news translations produced by a number of machine translation systems. This marks the second year we have conducted this task. Introduction We present the results of the shared tasks of the Workshop on Statistical Machine Translation (WMT) held at ACL 2013. This workshop builds on seven previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012). This year we conducted three official tasks: a translation task, a human evaluation of translation results, and a quality estimation task.1 In the translation task (§2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data. We held ten translation tasks this year, between English and each of Czech, French, German, Spanish, and Russian. The Russian translation tasks were new this year, and were also the most popular. The system outputs for each task were evaluated bot"
W13-2201,W13-2219,0,0.0360343,"Missing"
W13-2201,W13-2220,0,0.017834,"Missing"
W13-2201,W13-2255,0,0.0136612,"Missing"
W13-2201,W12-3113,0,0.012942,"Missing"
W13-2201,W13-2247,0,0.0208658,"Missing"
W13-2201,W13-2221,1,0.762444,"Missing"
W13-2201,W13-2228,0,0.0270572,"Missing"
W13-2201,W13-2224,0,0.0611782,"Missing"
W13-2201,2013.mtsummit-papers.21,1,0.577491,"ve learning to reduce the training set size (and therefore the annotation effort). The initial set features contains all black box and glass box features available within the Q U E ST framework (Specia et al., 2013) for the dataset at hand (160 in total for Task 1.1, and 80 for Task 1.3). The query selection strategy for active learning is based on the informativeness of the instances using Information Density, a measure that leverages between the variance among instances and how dense the region (in the feature space) where the instance is located is. To perform feature selection, following (Shah et al., 2013) features are ranked by the Gaussian Process 23 algorithm according to their learned length scales, which can be interpreted as the relevance of such feature for the model. This information was used for feature selection by discarding the lowest ranked (least useful) ones. based on empirical results found in (Shah et al., 2013), the top 25 features for both models were selected and used to retrain the same regression algorithm. Semantic Roles could bring marginally better accuracy. TCD-CNGL (T1.1) and TCD-DCU-CNGL (T1.3): The system is based on features which are commonly used for style classi"
W13-2201,2012.amta-papers.13,0,0.0208135,"Missing"
W13-2201,W13-2250,0,0.0320711,"Missing"
W13-2201,W13-2225,0,0.0376682,"Missing"
W13-2201,P13-1135,1,0.207781,"Missing"
W13-2201,W13-2226,1,0.759686,"Missing"
W13-2201,2006.amta-papers.25,0,0.295541,"ation Based on the data of Task 1.3, we define Task 2, a word-level annotation task for which participants are asked to produce a label for each token that indicates whether the word should be changed by a post-editor or kept in the final translation. We consider the following two sets of labels for prediction: Sentence-level Quality Estimation Task 1.1 Predicting Post-editing Distance This task is similar to the quality estimation task in WMT12, but with one important difference in the scoring variant: instead of using the post-editing effort scores in the [1, 2, 3, 4, 5] range, we use HTER (Snover et al., 2006) as quality score. This score is to be interpreted as the minimum edit distance between the machine translation and its manually post-edited version, and its range is [0, 1] (0 when no edit needs to be made, and 1 when all words need to be edited). Two variants of the results could be submitted in the shared task: • Binary classification: a keep/change label, the latter meaning that the token should be corrected in the post-editing process. • Multi-class classification: a label specifying the edit action that should be performed on the token (keep as is, delete, or substitute). 6.3 Datasets Ta"
W13-2201,W13-2227,0,0.0389834,"Missing"
W13-2201,W09-0441,0,0.0271208,"ze, and provide 4311 unique references. On average, one sentence in our set has 2.94±2.17 unique reference translations. Table 10 provides a histogram. It is well known that automatic MT evaluation methods perform better with more references, because a single one may not confirm a correct part of MT output. This issue is more severe for morphologically rich languages like Czech where about 1/3 of MT output was correct but not confirmed by the reference (Bojar et al., 2010). Advanced evaluation methods apply paraphrasing to smooth out some of the lexical divergence (Kauchak and Barzilay, 2006; Snover et al., 2009; Denkowski and Lavie, 2010). Simpler techniques such as lemmatizing are effective for morphologically rich languages (Tantug et al., 2008; Kos and Bojar, 2009) but they will lose resolution once the systems start performing generally well. WMTs have taken the stance that a big enough test set with just a single reference should compensate for the lack of other references. We use our post-edited reference translations to check this assumption for BLEU and NIST as implemented in mteval-13a (international tokenization switched on, which is not the default setting). We run many probes, randomly p"
W13-2201,W13-2230,0,0.0166145,"Missing"
W13-2201,W12-3118,1,0.609118,"1.1, T1.3): The submissions explored features built on MT engine resources including automatic word alignment, n-best candidate translation lists, back-translations and word posterior probabilities. Information about word alignments is used to extract quantitative (amount and distribution of the alignments) and qualitative (importance of the aligned terms) features under the assumption that alignment information can help tasks where sentencelevel semantic relations need to be identified (Souza et al., 2013). Three similar EnglishSpanish systems are built and used to provide pseudo-references (Soricut et al., 2012) and back-translations, from which automatic MT evaluation metrics could be computed and used as features. DFKI (T1.2, T1.3): DFKI’s submission for Task 1.2 was based on decomposing rankings into pairs (Avramidis, 2012), where the best system for each pair was predicted with Logistic Regression (LogReg). For GermanEnglish, LogReg was trained with Stepwise Feature Selection (Hosmer, 1989) on two feature sets: Feature Set 24 includes basic counts augmented with PCFG parsing features (number of VPs, alternative parses, parse probability) on both source and target sentences (Avramidis et al., 2011"
W13-2201,P13-2135,0,0.148019,"the difference between selecting the best or the worse translation. 19 ID CMU CNGL DCU DCU-SYMC DFKI FBK-UEdin LIG LIMSI LORIA SHEF TCD-CNGL TCD-DCU-CNGL UMAC UPC Participating team Carnegie Mellon University, USA (Hildebrand and Vogel, 2013) Centre for Next Generation Localization, Ireland (Bicici, 2013b) Dublin City University, Ireland (Almaghout and Specia, 2013) Dublin City University & Symantec, Ireland (Rubino et al., 2013b) German Research Centre for Artificial Intelligence, Germany (Avramidis and Popovic, 2013) Fondazione Bruno Kessler, Italy & University of Edinburgh, UK (Camargo de Souza et al., 2013) Laboratoire d’Informatique Grenoble, France (Luong et al., 2013) Laboratoire d’Informatique pour la M´ecanique et les Sciences de l’Ing´enieur, France (Singh et al., 2013) Lorraine Laboratory of Research in Computer Science and its Applications, France (Langlois and Smaili, 2013) University of Sheffield, UK (Beck et al., 2013) Trinity College Dublin & CNGL, Ireland (Moreau and Rubino, 2013) Trinity College Dublin, Dublin City University & CNGL, Ireland (Moreau and Rubino, 2013) University of Macau, China (Han et al., 2013) Universitat Politecnica de Catalunya, Spain (Formiga et al., 2013b) Ta"
W13-2201,2011.eamt-1.12,1,0.741789,"ar (CCG) features: CCG supertag language model perplexity and log probability, the number of maximal CCG constituents in the translation output which are the highestprobability minimum number of CCG constituents that span the translation output, the percentage of CCG argument mismatches between each subsequent CCG supertags, the percentage of CCG argument mismatches between each subsequent CCG maximal categories and the minimum number of phrases detected in the translation output. A second submission uses the aforementioned CCG features combined with 80 features from Q U E ST as described in (Specia, 2011). For the CCG features, the C&C parser was used to parse the translation output. Moses was used to build the phrase table from the SMT training corpus with maximum phrase length set to 7. The language model of supertags was built using the SRILM toolkit. As learning algorithm, Logistic Regression as provided by the SCIKIT- LEARN toolkit was used. The training data was prepared by converting each ranking of translation outputs to a set of pairwise comparisons according to the approach proposed by Avramidis et al. (2011). The rankings were generated back from pairwise comparisons predicted by th"
W13-2201,P13-4014,1,0.118757,"Missing"
W13-2201,W13-2229,0,0.0371208,"Missing"
W13-2201,tantug-etal-2008-bleu,0,0.0131442,"a histogram. It is well known that automatic MT evaluation methods perform better with more references, because a single one may not confirm a correct part of MT output. This issue is more severe for morphologically rich languages like Czech where about 1/3 of MT output was correct but not confirmed by the reference (Bojar et al., 2010). Advanced evaluation methods apply paraphrasing to smooth out some of the lexical divergence (Kauchak and Barzilay, 2006; Snover et al., 2009; Denkowski and Lavie, 2010). Simpler techniques such as lemmatizing are effective for morphologically rich languages (Tantug et al., 2008; Kos and Bojar, 2009) but they will lose resolution once the systems start performing generally well. WMTs have taken the stance that a big enough test set with just a single reference should compensate for the lack of other references. We use our post-edited reference translations to check this assumption for BLEU and NIST as implemented in mteval-13a (international tokenization switched on, which is not the default setting). We run many probes, randomly picking the test set size (number of distinct sentences) and the number of distinct references per sentence. Note that such test sets are s"
W13-2201,W10-1751,0,\N,Missing
W13-2201,de-marneffe-etal-2006-generating,0,\N,Missing
W13-2201,W13-2249,0,\N,Missing
W13-2201,N04-1013,0,\N,Missing
W13-2201,W02-1001,0,\N,Missing
W13-2201,W12-3102,1,\N,Missing
W13-2201,P12-3024,0,\N,Missing
W13-2201,W09-0401,1,\N,Missing
W13-2201,W13-2222,0,\N,Missing
W13-2201,W10-1711,0,\N,Missing
W13-2201,2010.iwslt-evaluation.22,0,\N,Missing
W13-2201,W10-1703,1,\N,Missing
W13-2201,W08-0309,1,\N,Missing
W13-2201,W13-2218,0,\N,Missing
W13-2201,P13-1004,1,\N,Missing
W13-2201,2013.mtsummit-papers.9,0,\N,Missing
W13-2201,W13-2216,1,\N,Missing
W13-2201,W13-2244,0,\N,Missing
W13-2203,W12-4204,0,0.54368,"valuate machine translation is not new. Gim´enez and M`arquez (2007) proposed using automatically assigned semantic role labels as a feature in a combined MT metric. The main difference between this application of semantic roles and MEANT is that arguments for specific verbs are taken into account, instead of just applying the subset agent, patient and benefactor. This idea would probably help human annotators to handle sentences with passives, copulas and other constructions which do not easily match the most basic arguments. On the other hand, verb specific arguments are language dependent. Bojar and Wu (2012), applying HMEANT to English-to-Czech MT output, identified a number of problems with HMEANT, and suggested a variety of improvements. In some respects, this work is very similar, except that our goal is to evaluate HMEANT along a range of intrinsic properties, to determine how useful the metric really is to evaluation campaigns such as the workshop on machine translation. 3 Evaluation with HMEANT 3.1 Annotation Procedure The goal of the HMEANT metric is to capture essential semantic content, but still be simple and fast. There are two stages to the annotation, the first of which is semantic r"
W13-2203,W07-0718,1,0.869138,"ion, there is still no consensus on how to evaluate machine translation based on human judgements. (Hutchins and Somers, 1992; Przybocki et al., 2009). One obvious approach is to ask annotators to rate translation candidates on a numerical scale. Under the DARPA TIDES program, the Linguistic Data Consortium (2002) developed an evaluation scheme that relies on two five-point scales representing fluency and adequacy. This was also the human evaluation scheme used in the annual MT competitions sponsored by NIST (2005). In an analysis of human evaluation results for the WMT ’07 workshop, however, Callison-Burch et al. (2007) found high correlation between fluency and adequacy scores assigned by individual annotators, suggesting that human annotators are not able to separate these two evaluation dimensions easily. Furthermore these absolute scores show low inter-annotator agreement. Instead of giving absolute quality assessments, annotators appeared to be using their ratings to rank translation candidates according to their overall preference for one over the other. In line with these findings, Callison-Burch et al. (2007) proposed to let annotators rank translation candidates directly, without asking them to assi"
W13-2203,P11-1023,0,0.125845,"on Street Edinburgh, EH8 9AB, UK Abstract tion might be useful for different purposes. If the MT is going to be the basis of a human translator’s work-flow, then post-editing effort seems like a natural fit. However, for people using MT for gisting, what we really want is some measure of how much meaning has been retained. We clearly need a metric which tries to answer the question, how much of the meaning does the translation capture. In this paper, we explore the use of human evaluation metrics which attempt to capture the extent of this meaning retention. In particular, we consider HMEANT (Lo and Wu, 2011a), a metric that uses semantic role labels to measure how much of the “who, why, when, where” has been preserved. For HMEANT evaluation, annotators are instructed to identify verbs as heads of semantic frames. Then they attach role fillers to the heads and finally they align heads and role fillers in the candidate translation with those in a reference translation. In a series of papers, Lo and Wu (2010, 2011b,a, 2012) explored a number of questions, evaluating HMEANT by using correlation statistics to compare it to judgements of human adequacy and contrastive evaluations. Given the drawbacks"
W13-2203,lo-wu-2010-evaluating,0,0.0158008,"ing does the translation capture. In this paper, we explore the use of human evaluation metrics which attempt to capture the extent of this meaning retention. In particular, we consider HMEANT (Lo and Wu, 2011a), a metric that uses semantic role labels to measure how much of the “who, why, when, where” has been preserved. For HMEANT evaluation, annotators are instructed to identify verbs as heads of semantic frames. Then they attach role fillers to the heads and finally they align heads and role fillers in the candidate translation with those in a reference translation. In a series of papers, Lo and Wu (2010, 2011b,a, 2012) explored a number of questions, evaluating HMEANT by using correlation statistics to compare it to judgements of human adequacy and contrastive evaluations. Given the drawbacks of those evaluation measures, which we discuss in Sec. 2, they could just as well have been evaluating the human adequacy and contrastive judgements using HMEANT. Human evaluation metrics need to be judged on other intrinsic qualities, which we describe below. The aim of this paper is to evaluate the effectiveness of HMEANT, with the goal of using it to judge the relative merits of different MT systems,"
W13-2203,W11-1002,0,0.330392,"on Street Edinburgh, EH8 9AB, UK Abstract tion might be useful for different purposes. If the MT is going to be the basis of a human translator’s work-flow, then post-editing effort seems like a natural fit. However, for people using MT for gisting, what we really want is some measure of how much meaning has been retained. We clearly need a metric which tries to answer the question, how much of the meaning does the translation capture. In this paper, we explore the use of human evaluation metrics which attempt to capture the extent of this meaning retention. In particular, we consider HMEANT (Lo and Wu, 2011a), a metric that uses semantic role labels to measure how much of the “who, why, when, where” has been preserved. For HMEANT evaluation, annotators are instructed to identify verbs as heads of semantic frames. Then they attach role fillers to the heads and finally they align heads and role fillers in the candidate translation with those in a reference translation. In a series of papers, Lo and Wu (2010, 2011b,a, 2012) explored a number of questions, evaluating HMEANT by using correlation statistics to compare it to judgements of human adequacy and contrastive evaluations. Given the drawbacks"
W13-2203,N12-1017,0,0.0331991,"eded by a human to convert the machine translation so as to convey the same meaning as the reference. This type of evaluation is of some use when one is using MT to aid human translation (although the relationship between number of edits and actual effort is not straightforward (Koponen, 2012)), but it is not so helpful when one’s task is gisting. The number of edits need not correlate with the severity of the semantic differences between the two sentences. The loss of a negative, for instance, is only one edit away from the original, but the semantics change completely. Alternatively, HyTER (Dreyer and Marcu, 2012) is an annotation tool which allows a user to create an exponential number of correct translations for a given sentence. These references are then efficiently exploited to compare with machine translation output. The authors argue that the current metrics fail simply because they have access to sets of reference translations which are simply too small. However, the fact is that even if one does have access to large numbers of translations, it is very difficult to determine whether the reference correctly captures the essential semantic content of the references. The idea of using semantic role"
W13-2203,W12-4206,0,0.0163075,"evaluation, as it is essential for building better automatic metrics, and therefore a more fundamental problem. The overall HMEANT score for MT evaluation is computed as the f-score from the counts of matches of frames and their role fillers between the reference and the MT output. Unmatched frames are excluded from the calculation together with all their corresponding roles. In recognition that preservation of some types of semantic relations may be more important than others for a human to understand a sentence, one may want to weight them differently in the computation of the HMEANT score. Lo and Wu (2012) train weights for each role filler type to optimise correlation with human adequacy judgements. As an unsupervised alternative, they suggest weighting roles according to their frequency as approximation to their importance. Since the main focus of the current paper is the annotation of the actions, roles and alignments that HMEANT depends on, we do not explore such different weight-setting schemes, but set the weights uniformly, with the exception of a partial alignment, which is given a weight of 0.5. HMEANT is thus defined as follows: 4 4.1 Systems and Data Sets We performed HMEANT evaluati"
W13-2203,P08-4006,1,0.829538,"espective sentence. They were then presented with the output of several machine translation systems for the same source sentence, one system at a time, with the reference translation and its annotations visible in the left half of the screen (cf. Fig. 1). For each system, the annotators were asked to annotate semantic frames and slot fillers in the translation first, and then align them with frame heads and slot fillers in the human reference translation. Annotations and alignment were performed with Edi-HMEANT2 , a web-based annotation tool for HMEANT that we developed on the basis of Yawat (Germann, 2008). The tool allows the alignment of slots from different semantic frames, and the alignment of slots of different types; however, such alignments are not considered in the computation of the final HMEANT score. The annotation guidelines were essentially those used in Bojar and Wu (2012), with some additional English examples, and a complete set of German examples. For ease of comparison with prior work, we used the same set of semantic role labels as Bojar and Wu (2012), shown in Table 1. Given the restriction that the head of a frame can consist of only one word, a convention was made that all"
W13-2203,W12-3101,0,0.0132441,"02), METEOR (Lavie and Denkowski, 2009) and TER (Snover et al., 2009b), which have greatly accelerated progress in MT research, rely on shallow surface properties of the translations, and only indirectly capture whether or not the translation preserves the meaning. This has meant that 53 sent a subset of the competing systems, and these rankings must be combined with other annotators judgements on five other system outputs to compute an overall ranking. The methodology for interpreting the contrastive evaluations has been the subject of much recent debate in the community (Bojar et al., 2011; Lopez, 2012). There has been some effort to overcome these problems. HTER (Snover et al., 2009a) is a metric which counts the number of edits needed by a human to convert the machine translation so as to convey the same meaning as the reference. This type of evaluation is of some use when one is using MT to aid human translation (although the relationship between number of edits and actual effort is not straightforward (Koponen, 2012)), but it is not so helpful when one’s task is gisting. The number of edits need not correlate with the severity of the semantic differences between the two sentences. The lo"
W13-2203,J05-1004,0,0.0443523,"s used for the WMT shared task, it is still reasonably efficient considering the fine-grained nature of the evaluation. On average, annotators evaluated about 10 sentences per hour. 2 Related Work Even though the idea that machine translation requires a semantic representation of the translated content is as old as the idea of computer-based translation itself (Weaver, 1955), it has not been until recently that people have begun to combine statistical models with semantic representations. Jones et al. (2012), for example, represent meaning as directed acyclic graphs and map these to PropBank (Palmer et al., 2005) style dependencies. To evaluate such approaches properly, we need evaluation metrics that capture the accuracy of the translation. Current automatic metrics of machine translation, such as BLEU (Papineni et al., 2002), METEOR (Lavie and Denkowski, 2009) and TER (Snover et al., 2009b), which have greatly accelerated progress in MT research, rely on shallow surface properties of the translations, and only indirectly capture whether or not the translation preserves the meaning. This has meant that 53 sent a subset of the competing systems, and these rankings must be combined with other annotator"
W13-2203,C12-1083,0,0.0311324,"uage. Efficiency Whilst HMEANT evaluation will never be as fast as, for example, the contrastive judgements used for the WMT shared task, it is still reasonably efficient considering the fine-grained nature of the evaluation. On average, annotators evaluated about 10 sentences per hour. 2 Related Work Even though the idea that machine translation requires a semantic representation of the translated content is as old as the idea of computer-based translation itself (Weaver, 1955), it has not been until recently that people have begun to combine statistical models with semantic representations. Jones et al. (2012), for example, represent meaning as directed acyclic graphs and map these to PropBank (Palmer et al., 2005) style dependencies. To evaluate such approaches properly, we need evaluation metrics that capture the accuracy of the translation. Current automatic metrics of machine translation, such as BLEU (Papineni et al., 2002), METEOR (Lavie and Denkowski, 2009) and TER (Snover et al., 2009b), which have greatly accelerated progress in MT research, rely on shallow surface properties of the translations, and only indirectly capture whether or not the translation preserves the meaning. This has mea"
W13-2203,P02-1040,0,0.101424,"a that machine translation requires a semantic representation of the translated content is as old as the idea of computer-based translation itself (Weaver, 1955), it has not been until recently that people have begun to combine statistical models with semantic representations. Jones et al. (2012), for example, represent meaning as directed acyclic graphs and map these to PropBank (Palmer et al., 2005) style dependencies. To evaluate such approaches properly, we need evaluation metrics that capture the accuracy of the translation. Current automatic metrics of machine translation, such as BLEU (Papineni et al., 2002), METEOR (Lavie and Denkowski, 2009) and TER (Snover et al., 2009b), which have greatly accelerated progress in MT research, rely on shallow surface properties of the translations, and only indirectly capture whether or not the translation preserves the meaning. This has meant that 53 sent a subset of the competing systems, and these rankings must be combined with other annotators judgements on five other system outputs to compute an overall ranking. The methodology for interpreting the contrastive evaluations has been the subject of much recent debate in the community (Bojar et al., 2011; Lop"
W13-2203,W12-3123,0,0.0290208,"ts to compute an overall ranking. The methodology for interpreting the contrastive evaluations has been the subject of much recent debate in the community (Bojar et al., 2011; Lopez, 2012). There has been some effort to overcome these problems. HTER (Snover et al., 2009a) is a metric which counts the number of edits needed by a human to convert the machine translation so as to convey the same meaning as the reference. This type of evaluation is of some use when one is using MT to aid human translation (although the relationship between number of edits and actual effort is not straightforward (Koponen, 2012)), but it is not so helpful when one’s task is gisting. The number of edits need not correlate with the severity of the semantic differences between the two sentences. The loss of a negative, for instance, is only one edit away from the original, but the semantics change completely. Alternatively, HyTER (Dreyer and Marcu, 2012) is an annotation tool which allows a user to create an exponential number of correct translations for a given sentence. These references are then efficiently exploited to compare with machine translation output. The authors argue that the current metrics fail simply bec"
W13-2203,W09-0441,0,0.049194,"translated content is as old as the idea of computer-based translation itself (Weaver, 1955), it has not been until recently that people have begun to combine statistical models with semantic representations. Jones et al. (2012), for example, represent meaning as directed acyclic graphs and map these to PropBank (Palmer et al., 2005) style dependencies. To evaluate such approaches properly, we need evaluation metrics that capture the accuracy of the translation. Current automatic metrics of machine translation, such as BLEU (Papineni et al., 2002), METEOR (Lavie and Denkowski, 2009) and TER (Snover et al., 2009b), which have greatly accelerated progress in MT research, rely on shallow surface properties of the translations, and only indirectly capture whether or not the translation preserves the meaning. This has meant that 53 sent a subset of the competing systems, and these rankings must be combined with other annotators judgements on five other system outputs to compute an overall ranking. The methodology for interpreting the contrastive evaluations has been the subject of much recent debate in the community (Bojar et al., 2011; Lopez, 2012). There has been some effort to overcome these problems."
W13-2203,Y12-1062,0,0.0303318,"Missing"
W13-2203,W07-0738,0,\N,Missing
W13-2203,W12-3129,0,\N,Missing
W13-2203,P13-1023,0,\N,Missing
W13-2203,W11-2101,0,\N,Missing
W13-2212,D07-1090,0,0.0383023,"4.02 30.04 22.70 25.70 31.87 24.00 17.95 20.06 28.76 30.03 33.87 29.66 15.81 18.35 23.75 18.44 The large language model was then quantized to 10 bits and compressed to 643 GB with KenLM (Heafield, 2011), loaded onto a machine with 1 TB RAM, and used as an additional feature in unconstrained French–English, Spanish–English, and Czech–English submissions. This additional language model is the only difference between our final constrained and unconstrained submissions; no additional parallel data was used. Results are shown in Table 18. Improvement from large language models is not a new result (Brants et al., 2007); the primary contribution is estimating on a single machine. +OSM 2012 2013 24.11 +.26 26.83 +.29 30.96 +.19 31.46 +.37 34.51 +.49 30.94 +.90 23.03 +.33 25.79 +.09 32.33 +.46 24.33 +.33 18.02 +.07 20.26 +.20 29.36 +.60 30.39 +.36 34.44 +.57 30.10 +.44 16.16 +.35 18.62 +.27 24.05 +.30 18.84 +.40 Constrained Unconstrained ∆ fr-en 31.46 32.24 +.78 es-en 30.59 31.37 +.78 cs-en 27.38 28.16 +.78 24.33 25.14 +.81 ru-en Table 18: Gain on newstest2013 from the unconstrained language model. Our time on shared machines with 1 TB is limited so Russian–English was run after the deadline and German–English"
W13-2212,W12-3102,1,0.0720198,"ut we did not use the factored decomposition of translation options into multiple mapping steps, since this usually lead to much slower systems with usually worse results. A good place, however, for factored decomposition is the handling of rare and unknown source words which have more frequent morphological variants (Koehn and Haddow, 2012a). Here, we used only factored backoff for unknown words, giving gains in BLEU of +.12 for German–English. Initial System Development We start with systems (Haddow and Koehn, 2012) that we developed for the 2012 Workshop on Statistical Machine Translation (Callison-Burch et al., 2012). The notable features of these systems are: • Moses phrase-based models with mostly default settings • training on all available parallel data, including the large UN parallel data, the FrenchEnglish 109 parallel data and the LDC Gigaword data 1.2 Tuning with k-best MIRA In preparation for training with sparse features, we moved away from MERT which is known to fall 114 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 114–121, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics apart with many more than a couple of dozen features. Ins"
W13-2212,N12-1047,0,0.0890278,"hese systems are: • Moses phrase-based models with mostly default settings • training on all available parallel data, including the large UN parallel data, the FrenchEnglish 109 parallel data and the LDC Gigaword data 1.2 Tuning with k-best MIRA In preparation for training with sparse features, we moved away from MERT which is known to fall 114 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 114–121, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics apart with many more than a couple of dozen features. Instead, we used k-best MIRA (Cherry and Foster, 2012). For the different language pairs, we saw improvements in BLEU of -.05 to +.39, with an average of +.09. There was only a minimal change in the length ratio (Table 1) de-en fr-en es-en cs-en en-de en-fr en-es en-cs avg MERT 22.11 (1.010) 30.00 (1.023) 30.42 (1.021) 25.54 (1.022) 16.08 (0.995) 29.26 (0.980) 31.92 (0.985) 17.38 (0.967) – k-best MIRA 22.10 (1.008) 30.11 (1.026) 30.63 (1.020) 25.49 (1.024) 16.04 (1.001) 29.65 (0.982) 31.95 (0.985) 17.42 (0.974) – The lexical features were restricted to the 50 most frequent words. All these features together only gave minor improvements (Table 3)."
W13-2212,N09-1025,0,0.025899,"f domain features: Translation Table Smoothing with Kneser-Ney Discounting de-en fr-en es-en cs-en en-de en-fr en-es en-cs avg sparse 22.02 30.24 30.61 25.49 15.93 29.81 32.02 17.28 – Table 3: Sparse features Table 1: Tuning with k-best MIRA instead of MERT (cased BLEU scores with length ratio) 1.3 baseline 22.10 30.11 30.63 25.49 16.04 29.65 31.95 17.42 – Sparse Features A significant extension of the Moses system over the last couple of years was the support for large numbers of sparse features. This year, we tested this capability on our big WMT systems. First, we used features proposed by Chiang et al. (2009): #d base. 2 22.10 4 30.11 3 30.63 9 25.49 2 16.122 4 29.65 3 31.95 9 17.42 – indicator 22.14 +.04 30.34 +.23 30.88 +.25 25.58 +.09 16.14 +.02 29.75 +.10 32.06 +.11 17.45 +.03 +.11 ratio 22.07 –.03 30.29 +.18 30.64 +.01 25.58 +.09 15.96 –.16 29.71 +.05 32.13 +.18 17.35 –.07 +.03 subset 22.12 +.02 30.15 +.04 30.82 +.19 25.46 –.03 16.01 –.11 29.70 +.05 32.02 +.07 17.44 +.02 +.03 Table 4: Sparse domain features When combining the domain features and the other sparse features, we see roughly additive gains (Table 5). We use the domain indicator feature and the other sparse features in subsequent e"
W13-2212,P05-1066,1,0.569778,"Missing"
W13-2212,P11-1105,1,0.298607,"us (translation and reordering) decisions spanning across phrasal boundaries thus overcoming the problematic phrasal independence assumption in the phrase-based model. In the OSM model, the reordering decisions influence lexical selection and vice versa. Lexical generation is strongly coupled with reordering thus improving the overall reordering mechanism. We used the modified version of the OSM model (Durrani et al., 2013b) that additionally handles discontinuous and unaligned target MTUs3 . We borrow 4 count-based supportive features, the Gap, Open Gap, Gap-width and Deletion penalties from Durrani et al. (2011). Inst. Wt (scale) – 33.98 ±.00 23.13 –.06 31.62 –.05 28.63 –.04 34.03 +.03 15.89 +.11 23.72 –.06 Table 14: Comparison of MML filtering and weighting with baseline. The MML uses monolingual news as in-domain, and selects from all training data after alignment.The weighting uses the MML weights, optionally downscaled by 10, then exponentiated. Baselines are as Table 13. Training: During training, each bilingual sentence pair is deterministically converted to a unique sequence of operations. Please refer to Durrani et al. (2011) for a list of operations and the conversion algorithm and see Figur"
W13-2212,D10-1044,0,0.0468666,"Training with new data (newstest2012 scores) 2 We adopted a number of changes that improved our baseline system by an average of +.30, see Table 10 for a breakdown. Domain Adaptation Techniques We explored two additional domain adaptation techniques: phrase table interpolation and modified Moore-Lewis filtering. method factored backoff kbest MIRA sparse features and domain indicator tuning with 25 iterations maximum phrase length 5 unpruned 4-gram LM translation table limit 100 total 2.1 Phrase Table Interpolation We experimented with phrase-table interpolation using perplexity minimisation (Foster et al., 2010; Sennrich, 2012). In particular, we used the implementation released with Sennrich (2012) and available in Moses, comparing both the naive and modified interpolation methods from that paper. For each language pair, we took the alignments created from all the data concatenated, built separate phrase tables from each of the individual corpora, and interpolated using each method. The results are shown in Table 13 Table 10: Summary of impact of changes Minor improvements that we did not adopt was avoiding reducing maximum phrase length to 5 (average +.03) and tuning with 1000-best lists (+.02). T"
W13-2212,N13-1035,1,0.80094,"was found to be better. The modified interpolation was not possible in fr↔en as it uses to much RAM. The final experiment of the initial system development phase was to train the systems on the new data, adding newstest2011 to the tuning set (now 10,068 sentences). Table 12 reports the gains on newstest2012 due to added data, indicating very clearly that valuable new data resources became available this year. The results from the phrase-table interpolation are quite mixed, and we only used the technique 117 for the final system in en-es. An interpolation based on PRO has recently been shown (Haddow, 2013) to improve on perplexity minimisation is some cases, but the current implementation of this method is limited to 2 phrase-tables, so we did not use it in this evaluation. 2.2 Figure 1: Bilingual Sentence with Alignments sequence of operations (o1 , o2 , . . . , oJ ) and learn a Markov model over this sequence as: Modified Moore-Lewis Filtering posm (F, E, A) = p(oJ1 ) = In last year’s evaluation (Koehn and Haddow, 2012b) we had some success with modified Moore-Lewis filtering (Moore and Lewis, 2010; Axelrod et al., 2011) of the training data. This year we conducted experiments in most of the"
W13-2212,W12-3154,1,0.0809865,"ems for the German–English language pairs to include POS and morphological target sequence models. But we did not use the factored decomposition of translation options into multiple mapping steps, since this usually lead to much slower systems with usually worse results. A good place, however, for factored decomposition is the handling of rare and unknown source words which have more frequent morphological variants (Koehn and Haddow, 2012a). Here, we used only factored backoff for unknown words, giving gains in BLEU of +.12 for German–English. Initial System Development We start with systems (Haddow and Koehn, 2012) that we developed for the 2012 Workshop on Statistical Machine Translation (Callison-Burch et al., 2012). The notable features of these systems are: • Moses phrase-based models with mostly default settings • training on all available parallel data, including the large UN parallel data, the FrenchEnglish 109 parallel data and the LDC Gigaword data 1.2 Tuning with k-best MIRA In preparation for training with sparse features, we moved away from MERT which is known to fall 114 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 114–121, c Sofia, Bulgaria, August 8-9, 2013"
W13-2212,W11-2123,1,0.674625,"do not go Generate Source Only (ja) Ich gehe ja ↓ nicht I do not go Jump Forward Ich gehe ja nicht ↓ I do not go Generate (zum, to the) . . . gehe ja nicht zum ↓ . . . not go to the Generate (haus, house) . . . ja nicht zum haus ↓ . . . go to the house Table 15: Step-wise Generation of Figure 1 LP newstest de-en fr-en es-en cs-en ru-en en-de en-fr en-es en-cs en-ru Baseline 2012 2013 23.85 26.54 30.77 31.09 34.02 30.04 22.70 25.70 31.87 24.00 17.95 20.06 28.76 30.03 33.87 29.66 15.81 18.35 23.75 18.44 The large language model was then quantized to 10 bits and compressed to 643 GB with KenLM (Heafield, 2011), loaded onto a machine with 1 TB RAM, and used as an additional feature in unconstrained French–English, Spanish–English, and Czech–English submissions. This additional language model is the only difference between our final constrained and unconstrained submissions; no additional parallel data was used. Results are shown in Table 18. Improvement from large language models is not a new result (Brants et al., 2007); the primary contribution is estimating on a single machine. +OSM 2012 2013 24.11 +.26 26.83 +.29 30.96 +.19 31.46 +.37 34.51 +.49 30.94 +.90 23.03 +.33 25.79 +.09 32.33 +.46 24.33"
W13-2212,P13-2121,1,0.843378,"reaks down the gains over the final system from Section 1 from using the operation sequence models (OSM), modified Moore-Lewis filtering (MML), fixing a bug with the sparse lexical features (Sparse-Lex Bugfix), and instance weighting (Instance Wt.), translation model combination (TM-Combine), and use of the huge language model (ClueWeb09 LM). Huge Language Models To overcome the memory limitations of SRILM, we implemented modified Kneser-Ney (Kneser and Ney, 1995; Chen and Goodman, 1998) smoothing from scratch using disk-based streaming algorithms. This open-source4 tool is described fully by Heafield et al. (2013). We used it to estimate an unpruned 5–gram language model on web pages from ClueWeb09.5 The corpus was preprocessed by removing spam (Cormack et al., 2011), selecting English documents, splitting sentences, deduplicating, tokenizing, and truecasing. Estimation on the remaining 126 billion tokens took 2.8 days on a single machine with 140 GB RAM (of which 123 GB was used at peak) and six hard drives in a RAID5 configuration. Statistics about the resulting model are shown in Table 17. 4 5 Summary Acknowledgments Thanks to Miles Osborne for preprocessing the ClueWeb09 corpus. The research leadin"
W13-2212,D07-1103,0,0.0605594,"Missing"
W13-2212,2012.amta-papers.9,1,0.770604,"anguage model trained on 126 billion tokens with a new training tool (Section 4). 1 1.1 Factored Backoff (German–English) We have consistently used factored models in past WMT systems for the German–English language pairs to include POS and morphological target sequence models. But we did not use the factored decomposition of translation options into multiple mapping steps, since this usually lead to much slower systems with usually worse results. A good place, however, for factored decomposition is the handling of rare and unknown source words which have more frequent morphological variants (Koehn and Haddow, 2012a). Here, we used only factored backoff for unknown words, giving gains in BLEU of +.12 for German–English. Initial System Development We start with systems (Haddow and Koehn, 2012) that we developed for the 2012 Workshop on Statistical Machine Translation (Callison-Burch et al., 2012). The notable features of these systems are: • Moses phrase-based models with mostly default settings • training on all available parallel data, including the large UN parallel data, the FrenchEnglish 109 parallel data and the LDC Gigaword data 1.2 Tuning with k-best MIRA In preparation for training with sparse f"
W13-2212,W12-3139,1,0.0637938,"anguage model trained on 126 billion tokens with a new training tool (Section 4). 1 1.1 Factored Backoff (German–English) We have consistently used factored models in past WMT systems for the German–English language pairs to include POS and morphological target sequence models. But we did not use the factored decomposition of translation options into multiple mapping steps, since this usually lead to much slower systems with usually worse results. A good place, however, for factored decomposition is the handling of rare and unknown source words which have more frequent morphological variants (Koehn and Haddow, 2012a). Here, we used only factored backoff for unknown words, giving gains in BLEU of +.12 for German–English. Initial System Development We start with systems (Haddow and Koehn, 2012) that we developed for the 2012 Workshop on Statistical Machine Translation (Callison-Burch et al., 2012). The notable features of these systems are: • Moses phrase-based models with mostly default settings • training on all available parallel data, including the large UN parallel data, the FrenchEnglish 109 parallel data and the LDC Gigaword data 1.2 Tuning with k-best MIRA In preparation for training with sparse f"
W13-2212,D07-1091,1,0.809959,"Missing"
W13-2212,E03-1076,1,0.320668,"Missing"
W13-2212,2012.iwslt-papers.7,0,0.0521392,"Missing"
W13-2212,P10-2041,0,0.0792206,"Missing"
W13-2212,2001.mtsummit-papers.68,0,0.0249874,"e model, and domain adaptation techniques. We also report on utilizing a huge language model trained on 126 billion tokens. Note that while our final 2012 systems included subsampling of training data with modified Moore-Lewis filtering (Axelrod et al., 2011), we did not use such filtering at the starting point of our development. We will report on such filtering in Section 2. Moreover, our system development initially used the WMT 2012 data condition, since it took place throughout 2012, and we switched to WMT 2013 training data at a later stage. In this section, we report cased BLEU scores (Papineni et al., 2001) on newstest2011. The annual machine translation evaluation campaign for European languages organized around the ACL Workshop on Statistical Machine Translation offers the opportunity to test recent advancements in machine translation in large data condition across several diverse language pairs. Building on our own developments and external contributions to the Moses open source toolkit, we carried out extensive experiments that, by early indications, led to a strong showing in the evaluation campaign. We would like to stress especially two contributions: the use of the new operation sequence"
W13-2212,E12-1055,0,0.0108725,"ta (newstest2012 scores) 2 We adopted a number of changes that improved our baseline system by an average of +.30, see Table 10 for a breakdown. Domain Adaptation Techniques We explored two additional domain adaptation techniques: phrase table interpolation and modified Moore-Lewis filtering. method factored backoff kbest MIRA sparse features and domain indicator tuning with 25 iterations maximum phrase length 5 unpruned 4-gram LM translation table limit 100 total 2.1 Phrase Table Interpolation We experimented with phrase-table interpolation using perplexity minimisation (Foster et al., 2010; Sennrich, 2012). In particular, we used the implementation released with Sennrich (2012) and available in Moses, comparing both the naive and modified interpolation methods from that paper. For each language pair, we took the alignments created from all the data concatenated, built separate phrase tables from each of the individual corpora, and interpolated using each method. The results are shown in Table 13 Table 10: Summary of impact of changes Minor improvements that we did not adopt was avoiding reducing maximum phrase length to 5 (average +.03) and tuning with 1000-best lists (+.02). The improvements d"
W13-2212,D11-1033,0,\N,Missing
W13-2212,P02-1040,0,\N,Missing
W13-2212,P13-2071,1,\N,Missing
W13-2212,N13-1001,1,\N,Missing
W13-2221,P05-1066,1,0.758259,"max-chart-span=25: This limits sub derivations to a maximum span of 25 source words. Glue rules are used to combine sub derivations allowing the full sentence to be covered. ttable-limit=200: Moses prunes the translation grammar on loading, removing low scoring rules. This option increases the number of translation rules that are retained for any given source side RHSs . cube-pruning-pop-limit=1000: Number of hypotheses created for each chart span. parsed as separate constituents. For German–English, we carried out the usual compound splitting (Koehn and Knight, 2003), but not pre-reordering (Collins et al., 2005). 3.3 Rule Extraction Some preliminary experiments were carried out for the German-English language pair to determine the parameters for the rule extraction step: MaxRuleDepth, MaxRuleSize, MaxNodes. Table 3 shows the BLEU score on different test sets for various parameter settings. For efficiency reasons less training data was used, therefore the grammar sizes, measured as the total number of extracted rules, are smaller than the final systems (Table 1). The parameters on the third line Depth=5, Nodes=20, Size=4 were chosen as the average BLEU score did not increase although the size of the e"
W13-2221,D07-1079,0,0.150541,"lation rules is extracted that can explain the example and is consistent with the alignment. The resulting rules can be composed in a non-overlapping fashion in order to cover the string-tree pair. Two or more minimal rules that are in a parentchild relationship can be composed together to obtain larger rules with more syntactic context. To avoid generating an exponential number of composed rules, several limitation have to be imposed. One such limitation is on the size of the composed rules, which is defined as the number of non-part-of-speech, non-leaf constituent labels in the target tree (DeNeefe et al., 2007). The corresponding parameter in the Moses implementation is MaxRuleSize and its default value is 3. We have also tried building systems for FrenchEnglish and Spanish-English but the data size proved to be problematic given the time constraints. We give a brief description of the syntaxbased model and its implementation within the Moses system. Some of the available features are described as well as some of the pre-processing steps. Several experiments are described and final results are presented for each language pair. 2 → RHSs |RHSt where LHS is a target-side non-terminal label and RHSs and"
W13-2221,P06-1121,0,0.453969,"Missing"
W13-2221,N04-1035,0,0.267992,"d grammatical coherence of the output. We are especially interested in string-to-tree models that focus syntactic annotation on the target side, especially for morphologically rich target languages (Williams and Koehn, 2011). We have trained syntax-based systems for the language pairs NN → Haus |house If our grammar also contains the translation rule S → das ist ein X1 |this is a NN1 then we can apply the two rules to an input das ist ein Haus to produce the output this is a house. 2.2 English-German, German-English, Czech-English, and Russian-English. Rule Extraction The GHKM rule extractor (Galley et al., 2004, 2006) learns translation rules from a word-aligned parallel corpora for which the target sentences are syntactically annotated. Given a string-tree pair, the set of minimally-sized translation rules is extracted that can explain the example and is consistent with the alignment. The resulting rules can be composed in a non-overlapping fashion in order to cover the string-tree pair. Two or more minimal rules that are in a parentchild relationship can be composed together to obtain larger rules with more syntactic context. To avoid generating an exponential number of composed rules, several lim"
W13-2221,W08-0509,0,0.100226,"tly separate the verbs, negation and possessives that are Data We made use of all available data for each language pair except for the Russian-English where the Commoncrawl corpus was not used. Table 1 shows the size of the parallel corpus used for each language pair. The English side of the parallel corpus was parsed using the Berkeley parser (Petrov et al., 2006) and the German side of the parallel corpus was parsed using the BitPar parser (Schmid, 2004). For German-English, German compounds were split using the script provided with Moses. The parallel corpus was word-aligned using MGIZA++ (Gao and Vogel, 2008). All available monolingual data was used for training the language models for each language 1 The PennTree bank tokenization rules considered were taken from http://www.cis.upenn.edu/˜treebank/ tokenizer.sed. Further examples of contractions were added. 172 Parameters Depth=3, Nodes=15, Size=3 Depth=4, Nodes=20, Size=4 Depth=5, Nodes=20, Size=5 Depth=5, Nodes=30, Size=5 Depth=5, Nodes=30, Size=6 Grammar Size Full Filtered 2,572,222 751,355 3,188,970 901,710 3,668,205 980,057 3,776,961 980,061 4,340,716 1,006,174 BLEU 2009-40 18.57 18.88 19.04 18.90 18.98 2010-40 20.43 20.38 20.47 20.59 20.52"
W13-2221,E03-1076,1,0.809257,"system submitted to the shared translation task. max-chart-span=25: This limits sub derivations to a maximum span of 25 source words. Glue rules are used to combine sub derivations allowing the full sentence to be covered. ttable-limit=200: Moses prunes the translation grammar on loading, removing low scoring rules. This option increases the number of translation rules that are retained for any given source side RHSs . cube-pruning-pop-limit=1000: Number of hypotheses created for each chart span. parsed as separate constituents. For German–English, we carried out the usual compound splitting (Koehn and Knight, 2003), but not pre-reordering (Collins et al., 2005). 3.3 Rule Extraction Some preliminary experiments were carried out for the German-English language pair to determine the parameters for the rule extraction step: MaxRuleDepth, MaxRuleSize, MaxNodes. Table 3 shows the BLEU score on different test sets for various parameter settings. For efficiency reasons less training data was used, therefore the grammar sizes, measured as the total number of extracted rules, are smaller than the final systems (Table 1). The parameters on the third line Depth=5, Nodes=20, Size=4 were chosen as the average BLEU sc"
W13-2221,J10-2004,0,0.156169,"implements scope-3 pruning and therefore the resulting grammar can be parsed in cubic time. Tree Restructuring The coverage of the extracted grammar depends partly on the structure of the target trees. If the target trees have flat constructions such as long noun phrases with many sibling nodes, the rules extracted will not generalize well to unseen data since there will be many constraints given by the types of different sibling nodes. In order to improve the grammar coverage to generalize over such cases, the target tree can be restructured. One restructuring strategy is tree binarization. Wang et al. (2010) give an extensive overview of different tree binarization strategies applied for the Chinese-English language pair. Moses currently supports left binarization and right binarization. By left binarization all the left-most children of a parent node n except the right most child are grouped under a new node. This node is inserted as the left child of n and receives the label n ¯ . Left binarization is then applied recursively on all newly inserted nodes until the leaves are reached. Right binarization implies a similar procedure but in this case the right-most children of the parent node are gr"
W13-2221,N03-1017,1,0.0385253,"weights tuned on the newstest2011 development set. The feature weights for each system were tuned on development sets using the Moses implementation of minimum error rate training (Och, 2003). The size of the tuning data varied for different languages depending on the amount of available data. In the case of the the German-English pair a filtering criteria based on sentence level BLEU score was applied which is briefly described in Section 3.5. Table 2 shows the size of the tuning set for each language pair. • plex (RHSt |RHSs ) and plex (RHSs |RHSt ), the direct and indirect lexical weights (Koehn et al., 2003). • ppcfg (FRAGt ), the monolingual PCFG probability of the tree fragment from which theQrule was extracted. This is defined as ni=1 p(ri ), where r1 . . . rn are the constituent CFG rules of the fragment. The PCFG parameters are estimated from the parse of the target-side training data. All lexical CFG rules are given the probability 1. This is similar to the pcfg feature proposed by Marcu et al. (2006) and is intended to encourage the production of syntactically wellformed derivations. Lang. pair en-de de-en cs-en ru-en • exp(−1/count(r)), a rule rareness penalty. • exp(1), a rule penalty. T"
W13-2221,W11-2126,1,0.889363,". We report on adapting parameters, targeted reduction of the tuning set, and post-evaluation experiments on rule binarization and preventing dropping of verbs. 1 LHS Syntax-based machine translation models hold the promise to overcome some of the fundamental problems of the currently dominating phrasebased approach, most importantly handling reordering for syntactically divergent language pairs and grammatical coherence of the output. We are especially interested in string-to-tree models that focus syntactic annotation on the target side, especially for morphologically rich target languages (Williams and Koehn, 2011). We have trained syntax-based systems for the language pairs NN → Haus |house If our grammar also contains the translation rule S → das ist ein X1 |this is a NN1 then we can apply the two rules to an input das ist ein Haus to produce the output this is a house. 2.2 English-German, German-English, Czech-English, and Russian-English. Rule Extraction The GHKM rule extractor (Galley et al., 2004, 2006) learns translation rules from a word-aligned parallel corpora for which the target sentences are syntactically annotated. Given a string-tree pair, the set of minimally-sized translation rules is e"
W13-2221,W12-3150,1,0.895776,"re presented for each language pair. 2 → RHSs |RHSt where LHS is a target-side non-terminal label and RHSs and RHSt are strings of terminals and nonterminals for the source and target sides, respectively. We use subscripted indices to indicate the correspondences between source and target nonterminals. For example, a translation rule to translate the German Haus into the English house is Overview • • • • Grammar System Description The syntax-based system used in all experiments is the Moses string-to-tree toolkit implementing GHKM rule extraction and Scope-3 parsing previously described in by Williams and Koehn (2012) 170 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 170–176, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics Another binarization strategy that is not currently integrated in Moses, but is worth investigating for different language pairs, is parallel head binarization. The result of parallel binarization of a parse tree is a binarization forest. To generate a binarization forest node, both right binarization and left binarization are applied recursively to a parent node with more than two children. Parallel head binarization is a"
W13-2221,W06-1606,0,0.0393713,"hich is briefly described in Section 3.5. Table 2 shows the size of the tuning set for each language pair. • plex (RHSt |RHSs ) and plex (RHSs |RHSt ), the direct and indirect lexical weights (Koehn et al., 2003). • ppcfg (FRAGt ), the monolingual PCFG probability of the tree fragment from which theQrule was extracted. This is defined as ni=1 p(ri ), where r1 . . . rn are the constituent CFG rules of the fragment. The PCFG parameters are estimated from the parse of the target-side training data. All lexical CFG rules are given the probability 1. This is similar to the pcfg feature proposed by Marcu et al. (2006) and is intended to encourage the production of syntactically wellformed derivations. Lang. pair en-de de-en cs-en ru-en • exp(−1/count(r)), a rule rareness penalty. • exp(1), a rule penalty. The main grammar and glue grammars have distinct penalty features. Sentences 7,065 2,400 10,068 1,501 Table 2: Corpus statistics for tuning data. Experiments 3.2 This section describes details for the syntax-based systems submitted by the University of Edinburgh. Additional post-evaluation experiments were carried out for the German-English language pair. 3.1 Grammar Size 31,568,480 55,310,162 209,841,388"
W13-2221,P03-1021,0,0.01161,"target yield, its word 171 Lang. pair en-de de-en cs-en ru-en count, and various scores for the synchronous derivation. Our grammar rules are scored according to the following functions: • p(RHSs |RHSt , LHS), translation probability. the noisy-channel pair. 5-gram language models were trained using SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998) and then interpolated using weights tuned on the newstest2011 development set. The feature weights for each system were tuned on development sets using the Moses implementation of minimum error rate training (Och, 2003). The size of the tuning data varied for different languages depending on the amount of available data. In the case of the the German-English pair a filtering criteria based on sentence level BLEU score was applied which is briefly described in Section 3.5. Table 2 shows the size of the tuning set for each language pair. • plex (RHSt |RHSs ) and plex (RHSs |RHSt ), the direct and indirect lexical weights (Koehn et al., 2003). • ppcfg (FRAGt ), the monolingual PCFG probability of the tree fragment from which theQrule was extracted. This is defined as ni=1 p(ri ), where r1 . . . rn are the const"
W13-2221,P06-1055,0,0.109912,"ks, which appear quite often in the parallel corpora, to opening and closing quotation marks. We also added some PennTree bank style tokenization rules1 . These rules split contractions such as I’ll, It’s, Don’t, Gonna, Commissioner’s in order to correctly separate the verbs, negation and possessives that are Data We made use of all available data for each language pair except for the Russian-English where the Commoncrawl corpus was not used. Table 1 shows the size of the parallel corpus used for each language pair. The English side of the parallel corpus was parsed using the Berkeley parser (Petrov et al., 2006) and the German side of the parallel corpus was parsed using the BitPar parser (Schmid, 2004). For German-English, German compounds were split using the script provided with Moses. The parallel corpus was word-aligned using MGIZA++ (Gao and Vogel, 2008). All available monolingual data was used for training the language models for each language 1 The PennTree bank tokenization rules considered were taken from http://www.cis.upenn.edu/˜treebank/ tokenizer.sed. Further examples of contractions were added. 172 Parameters Depth=3, Nodes=15, Size=3 Depth=4, Nodes=20, Size=4 Depth=5, Nodes=20, Size=5"
W13-2221,C04-1024,0,0.117485,"o added some PennTree bank style tokenization rules1 . These rules split contractions such as I’ll, It’s, Don’t, Gonna, Commissioner’s in order to correctly separate the verbs, negation and possessives that are Data We made use of all available data for each language pair except for the Russian-English where the Commoncrawl corpus was not used. Table 1 shows the size of the parallel corpus used for each language pair. The English side of the parallel corpus was parsed using the Berkeley parser (Petrov et al., 2006) and the German side of the parallel corpus was parsed using the BitPar parser (Schmid, 2004). For German-English, German compounds were split using the script provided with Moses. The parallel corpus was word-aligned using MGIZA++ (Gao and Vogel, 2008). All available monolingual data was used for training the language models for each language 1 The PennTree bank tokenization rules considered were taken from http://www.cis.upenn.edu/˜treebank/ tokenizer.sed. Further examples of contractions were added. 172 Parameters Depth=3, Nodes=15, Size=3 Depth=4, Nodes=20, Size=4 Depth=5, Nodes=20, Size=5 Depth=5, Nodes=30, Size=5 Depth=5, Nodes=30, Size=6 Grammar Size Full Filtered 2,572,222 751"
W13-2221,A97-1014,0,0.0821177,", default 3). The third limitation considered is the number of nodes in the composed rule, not counting target words (parameter MaxNodes, default 15). These parameters are language-dependent and should be set to values that best represent the characteristics of the target trees on which the rule extractor is trained on. Therefore the style of the treebanks used for training the syntactic parsers will also influence these numbers. The default values have been set based on experiments on the English-German language pair (Williams and Koehn, 2012). It is worth noting that the German parse trees (Skut et al., 1997) tend to be broader and shallower than those for English. In Section 3 we present some experiments where we choose different settings of these parameters for the German-English language pair. We use those settings for all language pairs where the target language is English. 2.3 2.4 Pruning The Grammar Decoding for syntax-based model relies on a bottom-up chart parsing algorithm. Therefore decoding efficiency is influenced by the following combinatorial problem: given an input sentence of length n and a context-free grammar rule  with n+1 s consecutive non-terminals, there are s ways to choose"
W13-2221,D10-1063,0,\N,Missing
W13-2322,P13-1091,1,0.350566,"uses VerbNet roles (Schuler, 2005), and AMR uses frame-specific PropBank relations. UNL has a dedicated set of over 30 frequently used relations. Formalism. GMB meanings are written in DRT (Kamp et al., 2011), exploiting full first4 8 Future Work Sembanking. Our main goal is to continue sembanking. We would like to employ a large sembank to create shared tasks for natural language understanding and generation. These tasks may additionally drive interest in theoretical frameworks for probabilistically mapping between graphs and strings (Quernheim and Knight, 2012b; Quernheim and Knight, 2012a; Chiang et al., 2013). Applications. Just as syntactic parsing has found many unanticipated applications, we expect sembanks and statistical semantic processors to be used for many purposes. To get started, we are exploring the use of statistical NLU and NLG in 5 amr.isi.edu/download.html 184 UNL guidelines: www.undl.org/unlsys/unl/unl2005 a semantics-based machine translation (MT) system. In this system, we annotate bilingual Chinese/English data with AMR, then train components to map Chinese to AMR, and AMR to English. A prototype is described by Jones et al. (2012). Disjunctive AMR. AMR aims to canonicalize mul"
W13-2322,N12-1017,0,0.0148132,"Missing"
W13-2322,kingsbury-palmer-2002-treebank,1,0.410787,"ive. We draw on this work to design an Abstract Meaning Representation (AMR) appropriate for sembanking. Our basic principles are: • AMRs are rooted, labeled graphs that are easy for people to read, and easy for programs to traverse. • AMR aims to abstract away from syntactic idiosyncrasies. We attempt to assign the same AMR to sentences that have the same basic meaning. For example, the sentences “he described her as a genius”, “his description of her: genius”, and “she was a genius, according to his description” are all assigned the same AMR. • AMR makes extensive use of PropBank framesets (Kingsbury and Palmer, 2002; Palmer et al., 2005). For example, we represent a phrase like “bond investor” using the frame “invest-01”, even though no verbs appear in the phrase. • AMR is agnostic about how we might want to derive meanings from strings, or viceversa. In translating sentences to AMR, we do not dictate a particular sequence of rule applications or provide alignments that reflect such rule sequences. This makes sembanking very fast, and it allows researchers to explore their own ideas about how strings We describe Abstract Meaning Representation (AMR), a semantic representation language in which we are wri"
W13-2322,martins-2012-le,0,0.0102149,"re able to translate a full sentence into AMR in 7-10 minutes and postedit an AMR in 1-3 minutes. 7 Related Work Researchers working on whole-sentence semantic parsing today typically use small, domain-specific sembanks like GeoQuery (Wong and Mooney, 2006). The need for larger, broad-coverage sembanks has sparked several projects, including the Groningen Meaning Bank (GMB) (Basile et al., 2012a), UCCA (Abend and Rappoport, 2013), the Semantic Treebank (ST) (Butler and Yoshimoto, 2012), the Prague Dependency Treebank (B¨ohmov´a et al., 2003), and UNL (Uchida et al., 1999; Uchida et al., 1996; Martins, 2012). Concepts. Most systems use English words as concepts. AMR uses PropBank frames (e.g., “describe-01”), and UNL uses English WordNet synsets (e.g., “200752493”). Relations. GMB uses VerbNet roles (Schuler, 2005), and AMR uses frame-specific PropBank relations. UNL has a dedicated set of over 30 frequently used relations. Formalism. GMB meanings are written in DRT (Kamp et al., 2011), exploiting full first4 8 Future Work Sembanking. Our main goal is to continue sembanking. We would like to employ a large sembank to create shared tasks for natural language understanding and generation. These tas"
W13-2322,W04-2705,0,0.0181197,"Missing"
W13-2322,W13-0101,0,0.0287813,"nces from CCTV broadcast conversation (*) Collections marked with a star (*) are also in the OntoNotes corpus (Pradhan et al., 2007; Weischedel et al., 2011). Using the AMR Editor, annotators are able to translate a full sentence into AMR in 7-10 minutes and postedit an AMR in 1-3 minutes. 7 Related Work Researchers working on whole-sentence semantic parsing today typically use small, domain-specific sembanks like GeoQuery (Wong and Mooney, 2006). The need for larger, broad-coverage sembanks has sparked several projects, including the Groningen Meaning Bank (GMB) (Basile et al., 2012a), UCCA (Abend and Rappoport, 2013), the Semantic Treebank (ST) (Butler and Yoshimoto, 2012), the Prague Dependency Treebank (B¨ohmov´a et al., 2003), and UNL (Uchida et al., 1999; Uchida et al., 1996; Martins, 2012). Concepts. Most systems use English words as concepts. AMR uses PropBank frames (e.g., “describe-01”), and UNL uses English WordNet synsets (e.g., “200752493”). Relations. GMB uses VerbNet roles (Schuler, 2005), and AMR uses frame-specific PropBank relations. UNL has a dedicated set of over 30 frequently used relations. Formalism. GMB meanings are written in DRT (Kamp et al., 2011), exploiting full first4 8 Future"
W13-2322,P98-1013,0,0.597537,"Missing"
W13-2322,basile-etal-2012-developing,0,0.035603,". AMR and UNL remain agnostic about the relation between strings and their meanings, considering this a topic of open research. ST and GMB annotate words and phrases directly, recording derivations as (for example) Montaguestyle compositional semantic rules operating on CCG parses. Top-down verus bottom-up. AMR annotators find it fast to construct meanings from the top down, starting with the main idea of the sentence (though the AMR Editor allows bottom-up construction). GMB and UCCA annotators work bottom-up. Editors, guidelines, genres. These projects have graphical sembanking tools (e.g., Basile et al. (2012b)), annotation guidelines,5 and sembanks that cover a wide range of genres, from news to fiction. UNL and AMR have both annotated many of the same sentences, providing the potential for direct comparison. We currently have a manually-constructed AMR bank of several thousand sentences, a subset of which can be freely downloaded,4 the rest being distributed via the LDC catalog. In initially developing AMR, the authors built consensus AMRs for: • 225 short sentences for tutorial purposes • 142 sentences of newswire (*) • 100 sentences of web data (*) Trained annotators at LDC then produced AMRs"
W13-2322,J05-1004,1,0.185231,"design an Abstract Meaning Representation (AMR) appropriate for sembanking. Our basic principles are: • AMRs are rooted, labeled graphs that are easy for people to read, and easy for programs to traverse. • AMR aims to abstract away from syntactic idiosyncrasies. We attempt to assign the same AMR to sentences that have the same basic meaning. For example, the sentences “he described her as a genius”, “his description of her: genius”, and “she was a genius, according to his description” are all assigned the same AMR. • AMR makes extensive use of PropBank framesets (Kingsbury and Palmer, 2002; Palmer et al., 2005). For example, we represent a phrase like “bond investor” using the frame “invest-01”, even though no verbs appear in the phrase. • AMR is agnostic about how we might want to derive meanings from strings, or viceversa. In translating sentences to AMR, we do not dictate a particular sequence of rule applications or provide alignments that reflect such rule sequences. This makes sembanking very fast, and it allows researchers to explore their own ideas about how strings We describe Abstract Meaning Representation (AMR), a semantic representation language in which we are writing down the meanings"
W13-2322,P02-1040,0,0.108678,"ted script.3 Smatch reports the semantic overlap between two AMRs by viewing each AMR as a conjunction of logical triples (see Figure 1). Smatch computes precision, recall, and F-score of one AMR’s triples against the other’s. To match up variables from two input AMRs, smatch needs to execute a brief search, looking for the variable mapping that yields the highest F-score. Smatch makes no reference to English strings or word indices, as we do not enforce any particular string-to-meaning derivation. Instead, we compare semantic representations directly, in the same way that the MT metric Bleu (Papineni et al., 2002) compares target strings without making reference to the source. For an initial IAA study, and prior to adjusting the AMR Editor to encourage consistency, 4 expert AMR annotators annotated 100 newswire sentences and 80 web text sentences. They then created consensus AMRs through discussion. The average annotator vs. consensus IAA (smatch) was 0.83 for newswire and 0.79 for web text. When newly trained annotators doubly annotated 382 web text sentences, their annotator vs. annotator IAA was 0.71. (m / marble :location (j / jar)) the marble in the jar ... (b / be-located-at-91 :arg1 (m / marble)"
W13-2322,W12-6207,1,0.412113,"glish WordNet synsets (e.g., “200752493”). Relations. GMB uses VerbNet roles (Schuler, 2005), and AMR uses frame-specific PropBank relations. UNL has a dedicated set of over 30 frequently used relations. Formalism. GMB meanings are written in DRT (Kamp et al., 2011), exploiting full first4 8 Future Work Sembanking. Our main goal is to continue sembanking. We would like to employ a large sembank to create shared tasks for natural language understanding and generation. These tasks may additionally drive interest in theoretical frameworks for probabilistically mapping between graphs and strings (Quernheim and Knight, 2012b; Quernheim and Knight, 2012a; Chiang et al., 2013). Applications. Just as syntactic parsing has found many unanticipated applications, we expect sembanks and statistical semantic processors to be used for many purposes. To get started, we are exploring the use of statistical NLU and NLG in 5 amr.isi.edu/download.html 184 UNL guidelines: www.undl.org/unlsys/unl/unl2005 a semantics-based machine translation (MT) system. In this system, we annotate bilingual Chinese/English data with AMR, then train components to map Chinese to AMR, and AMR to English. A prototype is described by Jones et al. ("
W13-2322,W12-4209,1,0.36463,"glish WordNet synsets (e.g., “200752493”). Relations. GMB uses VerbNet roles (Schuler, 2005), and AMR uses frame-specific PropBank relations. UNL has a dedicated set of over 30 frequently used relations. Formalism. GMB meanings are written in DRT (Kamp et al., 2011), exploiting full first4 8 Future Work Sembanking. Our main goal is to continue sembanking. We would like to employ a large sembank to create shared tasks for natural language understanding and generation. These tasks may additionally drive interest in theoretical frameworks for probabilistically mapping between graphs and strings (Quernheim and Knight, 2012b; Quernheim and Knight, 2012a; Chiang et al., 2013). Applications. Just as syntactic parsing has found many unanticipated applications, we expect sembanks and statistical semantic processors to be used for many purposes. To get started, we are exploring the use of statistical NLU and NLG in 5 amr.isi.edu/download.html 184 UNL guidelines: www.undl.org/unlsys/unl/unl2005 a semantics-based machine translation (MT) system. In this system, we annotate bilingual Chinese/English data with AMR, then train components to map Chinese to AMR, and AMR to English. A prototype is described by Jones et al. ("
W13-2322,W13-0215,0,0.00777424,"ry))”, because “profess-01” 2 3 183 AMR Editor: amr.isi.edu/editor.html Smatch: amr.isi.edu/evaluation.html 6 Current AMR Bank order logic. GMB and ST both include universal quantification. Granularity. GMB and UCCA annotate short texts, so that the same entity can participate in events described in different sentences; other systems annotate individual sentences. Entities. AMR uses 80 entity types, while GMB uses 7. Manual versus automatic. AMR, UNL, and UCCA annotation is fully manual. GMB and ST produce meaning representations automatically, and these can be corrected by experts or crowds (Venhuizen et al., 2013). Derivations. AMR and UNL remain agnostic about the relation between strings and their meanings, considering this a topic of open research. ST and GMB annotate words and phrases directly, recording derivations as (for example) Montaguestyle compositional semantic rules operating on CCG parses. Top-down verus bottom-up. AMR annotators find it fast to construct meanings from the top down, starting with the main idea of the sentence (though the AMR Editor allows bottom-up construction). GMB and UCCA annotators work bottom-up. Editors, guidelines, genres. These projects have graphical sembanking"
W13-2322,N06-1056,0,0.0133733,"AMRs for: • 1546 sentences from the novel “The Little Prince” • 1328 sentences of web data • 1110 sentences of web data (*) • 926 sentences from Xinhua news (*) • 214 sentences from CCTV broadcast conversation (*) Collections marked with a star (*) are also in the OntoNotes corpus (Pradhan et al., 2007; Weischedel et al., 2011). Using the AMR Editor, annotators are able to translate a full sentence into AMR in 7-10 minutes and postedit an AMR in 1-3 minutes. 7 Related Work Researchers working on whole-sentence semantic parsing today typically use small, domain-specific sembanks like GeoQuery (Wong and Mooney, 2006). The need for larger, broad-coverage sembanks has sparked several projects, including the Groningen Meaning Bank (GMB) (Basile et al., 2012a), UCCA (Abend and Rappoport, 2013), the Semantic Treebank (ST) (Butler and Yoshimoto, 2012), the Prague Dependency Treebank (B¨ohmov´a et al., 2003), and UNL (Uchida et al., 1999; Uchida et al., 1996; Martins, 2012). Concepts. Most systems use English words as concepts. AMR uses PropBank frames (e.g., “describe-01”), and UNL uses English WordNet synsets (e.g., “200752493”). Relations. GMB uses VerbNet roles (Schuler, 2005), and AMR uses frame-specific Pr"
W13-2322,E12-2019,0,\N,Missing
W13-2322,C12-1083,1,\N,Missing
W13-2322,C98-1013,0,\N,Missing
W14-0307,W13-2212,1,0.891936,"Missing"
W14-0307,2012.amta-papers.22,0,0.164962,"hey also compare post-editing styles of different post-editors working on identical post-editing tasks. Another study by Koponen (2013) showed that inter-translator variance is lower in a controlled language setting when translators are given the choice of output from three different machine translation systems. In the realm of machine translation research, there has been an increasing interest in the use of MT technology by post-editors. A major push are the two EU-funded research projects MATE CAT 3 and CASMACAT 4 , which are developing an open source translation and post-editing workbench (Federico et al., 2012; Alabau et al., 2013). At this point, we are not aware of any study that compares directly the impact of different machine translation systems on post-editor productivity and behaviour. 3 3.2 A total of 15 different machine translation systems participated in the evaluation campaign. We selected four different systems that differ in their architecture and use of training data: Experimental Design • an anonymized popular online translation system built by a large Internet company (ONLINE - B) We thus carried out an experiment on an English– German news translation task, using output from four"
W14-0307,W12-3123,0,0.0422099,"ineni et al., 2001), and more subjective human evaluation criteria such as correctness, accuracy, and fluency. How the quality increases measured by automatic metrics and subjective evaluation criteria relate to actual increases in the productivity of posteditors is still an open research question. It is also not clear yet if some machine translation approaches — say, syntax-based models — are better suited for post-editing than others. These relationships may very well also depend on the lan1 Ulrich Germann? ugermann@inf.ed.ac.uk ? School of Informatics University of Edinburgh 2 Related Work Koponen (2012) examined the relationship between human assessment of post-editing efforts and objective measures such as post-editing time and number of edit operations. She found that segments that require a lot of reordering are perceived as being more difficult, and that long sentences are considered harder, even if only few words changed. She also reports larger variance between translators in post-editing time than in post-editing operations — a finding that we confirm here as well. From a detailed analysis of the types of edits performed in sentences with long versus short post-edit times, Koponen et"
W14-0307,2013.mtsummit-wptp.1,0,0.390584,"o convenience (the authors of this study are fluent in both languages), but also because this language pair poses special challenges to current machine translation technology, due to the syntactic divergence of the two languages. We selected data from the most recent evaluation campaign. The subset chosen for our postediting task comprises 9 different news stories, originally written in English, with a total of 500 sentences. Details are shown in Table 1. and takes longer. They also compare post-editing styles of different post-editors working on identical post-editing tasks. Another study by Koponen (2013) showed that inter-translator variance is lower in a controlled language setting when translators are given the choice of output from three different machine translation systems. In the realm of machine translation research, there has been an increasing interest in the use of MT technology by post-editors. A major push are the two EU-funded research projects MATE CAT 3 and CASMACAT 4 , which are developing an open source translation and post-editing workbench (Federico et al., 2012; Alabau et al., 2013). At this point, we are not aware of any study that compares directly the impact of differen"
W14-0307,2012.amta-wptp.2,0,0.0913981,"onen (2012) examined the relationship between human assessment of post-editing efforts and objective measures such as post-editing time and number of edit operations. She found that segments that require a lot of reordering are perceived as being more difficult, and that long sentences are considered harder, even if only few words changed. She also reports larger variance between translators in post-editing time than in post-editing operations — a finding that we confirm here as well. From a detailed analysis of the types of edits performed in sentences with long versus short post-edit times, Koponen et al. (2012) conclude that the observed differences in edit times can be explained at least in part also by the types of necessary edits and the associated cognitive effort. Deleting superfluous function words, for example, appears to be cognitively simple and takes little time, whereas inserting translations for untranslated words requires more cognitive effort 2 http://www.nist.gov/itl/iad/mig/openmt.cfm http://www.casmacat.eu/index.php?n=Main.Downloads 38 Workshop on Humans and Computer-assisted Translation, pages 38–46, c Gothenburg, Sweden, 26 April 2014. 2014 Association for Computational Linguistic"
W14-0307,2013.mtsummit-wptp.10,0,0.29281,"Missing"
W14-0307,W13-2221,1,0.886948,"Missing"
W14-0307,2001.mtsummit-papers.68,0,0.0634897,"009; Plitt and Masselot, 2010; Garcia, 2011; Pouliquen et al., 2011; Skadin¸sˇ et al., 2011; den Bogaert and Sutter, 2013; Vazquez et al., 2013; Green et al., 2013; L¨aubli et al., 2013). The advances in statistical machine translation over the past years have been driven to a large extent by frequent (friendly) competitive MT evaluation campaigns, such as the shared tasks at the ACL WMT workshop series (Bojar et al., 2013) and IWSLT (Cettolo et al., 2013), and the NIST Open MT Evaluation.1 These evaluations usually apply a mix of automatic evaluation metrics, most prominently the BLEU score (Papineni et al., 2001), and more subjective human evaluation criteria such as correctness, accuracy, and fluency. How the quality increases measured by automatic metrics and subjective evaluation criteria relate to actual increases in the productivity of posteditors is still an open research question. It is also not clear yet if some machine translation approaches — say, syntax-based models — are better suited for post-editing than others. These relationships may very well also depend on the lan1 Ulrich Germann? ugermann@inf.ed.ac.uk ? School of Informatics University of Edinburgh 2 Related Work Koponen (2012) exam"
W14-0307,2011.eamt-1.2,0,0.194111,"Missing"
W14-0307,2006.amta-papers.25,0,0.145761,"anslation output. This is what we will do in this section. In Section 6, we will consider which parts of the final translation were actually changed by the post-editor and discuss the difference. 5.1 sub 18.9 20.0 19.8 21.9 HTER as Quality Measure The edit distance between machine translation output and human reference translation can be measured in the number of insertions, deletions, substitutions and (phrasal) moves. A metric that simply counts the minimal number of such edit operations and divides it by the length of the human reference translation is the translation edit rate, short TER (Snover et al., 2006). If the human reference translation is created from the machine translation output to minimise the number of edit operations needed for an acceptable translation, this variant is called humanmediated TER, or HTER. Note that in our experiment the post-editors are not strictly trying to minimise the number of edit operations — they may be inclined to make additional changes due to arbitrary considerations of style or perform edits that are faster rather than minimise the number of operations (e.g., deleting whole passages and rewriting them). 5.3 Edits by Post-Editor Table 5 displays the edit r"
W14-0307,W13-2229,0,0.0275354,"Missing"
W14-0307,2013.mtsummit-posters.7,0,0.087959,"Missing"
W14-0307,W12-3102,1,\N,Missing
W14-0307,P02-1040,0,\N,Missing
W14-0307,N10-1078,1,\N,Missing
W14-0307,federico-etal-2012-iwslt,0,\N,Missing
W14-0307,W13-2201,1,\N,Missing
W14-0307,2009.mtsummit-btm.7,0,\N,Missing
W14-0307,2011.eamt-1.7,0,\N,Missing
W14-1005,P84-1075,0,0.124744,"haben or sein, the other is a pastparticiple. 3 The Grammar Our baseline translation model is learned from a parallel corpus with automatically-derived word alignments. In the literature, string-to-tree translation models are typically based on either synchronous context-free grammars (SCFGs) (as in Chiang et al. (2007)) or tree transducers (as in Galley et al. (2004)). In this work, we use an SCFGbased model but our extensions are applicable in both cases. Following Williams and Koehn (2011), each rule of our grammar is supplemented with a (possibly-empty) set of PATR - II-style identities (Shieber, 1984). Figure 2 shows two example rules with identities. The identities should be interpreted as constraints that the feature structures of the corresponding rule elements are compatible under unification. During decoding, this imposes a hard constraint on rule application. 2.2 The Lexicon Our model uses a lexicon that maps each German verb in the target-side terminal vocabulary to a set of features structures. Each feature structure contains two top-level features: POS, a part-of-speech feature, and VC, a verbal complex feature of the form described above. Since a verbal complex can comprise multi"
W14-1005,W12-4211,0,0.0689356,"Missing"
W14-1005,W11-2126,1,0.944634,"specifies that for this type, the verbal complex comprises exactly two verbs: one is a finite, indicative form of the auxiliary haben or sein, the other is a pastparticiple. 3 The Grammar Our baseline translation model is learned from a parallel corpus with automatically-derived word alignments. In the literature, string-to-tree translation models are typically based on either synchronous context-free grammars (SCFGs) (as in Chiang et al. (2007)) or tree transducers (as in Galley et al. (2004)). In this work, we use an SCFGbased model but our extensions are applicable in both cases. Following Williams and Koehn (2011), each rule of our grammar is supplemented with a (possibly-empty) set of PATR - II-style identities (Shieber, 1984). Figure 2 shows two example rules with identities. The identities should be interpreted as constraints that the feature structures of the corresponding rule elements are compatible under unification. During decoding, this imposes a hard constraint on rule application. 2.2 The Lexicon Our model uses a lexicon that maps each German verb in the target-side terminal vocabulary to a set of features structures. Each feature structure contains two top-level features: POS, a part-of-spe"
W14-1005,W12-3102,1,0.837677,"Missing"
W14-1005,W12-3150,1,0.876663,"Missing"
W14-1005,J07-2003,0,0.1269,"Missing"
W14-1005,N04-1035,0,0.0544187,"ue for the verbal complex hat . . . gespielt belongs to the perfect, active, indicative, non-modal type. Additionally, it specifies that for this type, the verbal complex comprises exactly two verbs: one is a finite, indicative form of the auxiliary haben or sein, the other is a pastparticiple. 3 The Grammar Our baseline translation model is learned from a parallel corpus with automatically-derived word alignments. In the literature, string-to-tree translation models are typically based on either synchronous context-free grammars (SCFGs) (as in Chiang et al. (2007)) or tree transducers (as in Galley et al. (2004)). In this work, we use an SCFGbased model but our extensions are applicable in both cases. Following Williams and Koehn (2011), each rule of our grammar is supplemented with a (possibly-empty) set of PATR - II-style identities (Shieber, 1984). Figure 2 shows two example rules with identities. The identities should be interpreted as constraints that the feature structures of the corresponding rule elements are compatible under unification. During decoding, this imposes a hard constraint on rule application. 2.2 The Lexicon Our model uses a lexicon that maps each German verb in the target-side"
W14-1005,E12-1074,0,0.0180571,"of error (either incomplete or mismatching values). By developing model features that use source-side information to influence the production of verbal complexes we are able to substantially improve the type accuracy as compared to the reference. 2 VP - OC → h rebuilt , wieder aufgebaut i h VP - OC VC i = h aufgebaut VC i h aufgebaut POS i = VVPP S - TOP → h X 1 have X 2 been X 3 , PP - MO 1 wurde NP - SB 2 VP - OC 3 i h S - TOP VC i = h wurde VC i h S - TOP VC i = h VP - OC VC i h wurde POS i = VAFIN Figure 2: SCFG rules with constraints Verbal Complex Structures Adopting the terminology of Gojun and Fraser (2012), we use the term ‘verbal complex’ to mean a main verb and any associated auxiliaries within a single clause. The lexicon’s POS values are derived from the parse trees on the target-side of the training data. The VC values are assigned according to POS value from a small set of hand-written feature structures. Every main verb is assigned VC values from one of three possible groups, selected according to whether the verb is finite, a past-participle, or an infinitive. For the closed class of modal and nonmodal auxiliary verbs, VC values were manually assigned. 2.1 Feature Structures We use feat"
W14-1005,P07-2045,1,0.00970674,"Missing"
W14-1005,P02-1038,0,0.14981,"h the VC value of a VP - OC subderivation (such as the subderivation produced by applying the first rule). 4 Baseline 15.7 14.9 16.5 15.4 2. We modify the decoder to produce derivations in chart cells only if the cell span is consistent with the set of clause spans (i.e. if source span [i,j] is a clause span then no derivation is built over span [m,n] where i < m ≤ j and n > j, etc.) 3. We modify the decoder so that grammar rules can only be applied over clause spans if they have a clause label (‘S’ or ‘CS’, since the parser we use is trained on the Tiger treebank). Source-side Features Since Och and Ney (2002), most SMT models have been defined as a log-linear sum of weighted feature functions. In this section, we define two verbal-complex-specific feature functions. In order to do so, we first describe ‘clause projection,’ a simple source-syntactic restriction on decoding. We then describe our heuristic method of obtaining probability estimates for a target verbal complex value given the source clause. 4.2 Verbal Complex Probabilities When translating a clause, the source-side verbal complex will often provide sufficient information to select a reasonable type for the target verbal complex, or to"
W14-1005,P03-1021,0,0.0849736,"Missing"
W14-1005,N07-1051,0,\N,Missing
W14-3302,bojar-etal-2014-hindencorp,1,0.801715,"Missing"
W14-3302,W13-2242,0,0.189283,"Missing"
W14-3302,W14-3305,0,0.0227897,"Missing"
W14-3302,W14-3326,1,0.729814,"Missing"
W14-3302,W14-3313,0,0.0328356,"Missing"
W14-3302,W14-3342,0,0.0770851,"Missing"
W14-3302,2012.iwslt-papers.5,1,0.897779,"Each system SJ in the pool {Sj } is represented by an associated relative ability µj and a variance σa2 (fixed across all systems) which serve as the parameters of a Gaussian distribution. Samples from this distribution represent the quality of sentence translations, with higher quality samples having higher values. Pairwise annotations (S1 , S2 , π) are generated according to the following process: Method 1: Expected Wins (EW) Introduced for WMT13, the E XPECTED W INS has an intuitive score demonstrated to be accurate in ranking systems according to an underlying model of “relative ability” (Koehn, 2012a). The idea is to gauge the probability that a system Si will be ranked better than another system randomly chosen from a pool of opponents {Sj : j 6= i}. If we define the function win(A, B) as the number of times system A is ranked better than system B, 19 1. Select two systems S1 and S2 from the pool of systems {Sj } This score is then used to sort the systems and produce the ranking. 2. Draw two “translations”, adding random 2 to simulate Gaussian noise with variance σobs the subjectivity of the task and the differences among annotators: 3.4 Method Selection We have three methods which, pr"
W14-3302,W06-3114,1,0.176114,"estimation of machine translation quality, and a metrics task. This year, 143 machine translation systems from 23 institutions were submitted to the ten translation directions in the standard translation task. An additional 6 anonymized systems were included, and were then evaluated both automatically and manually. The quality estimation task had four subtasks, with a total of 10 teams, submitting 57 entries. 1 Introduction We present the results of the shared tasks of the Workshop on Statistical Machine Translation (WMT) held at ACL 2014. This workshop builds on eight previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013). This year we conducted four official tasks: a translation task, a quality estimation task, a metrics task1 and a medical translation task. In the translation task (§2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data. We held ten translation tasks this year, between English and each of Czech, French, German, Hindi, and Russian. The Hindi translation tasks were new this year, providing a lesser resourced data condition on a challenging languag"
W14-3302,W12-3101,0,0.0505246,"Missing"
W14-3302,P02-1040,0,0.10401,"inuous-space language model is also used in a post-processing step for each system. POSTECH submitted a phrase-based SMT system and query translation system for the DE–EN language pair in both subtasks. They analysed three types of query formation, generated query translation candidates using term-to-term dictionaries and a phrase-based system, and then scored them using a co-occurrence word frequency measure to select the best candidate. UEDIN applied the Moses phrase-based system to 5.5 Results MT quality in the Medical Translation Task is evaluated using automatic evaluation metrics: BLEU (Papineni et al., 2002), TER (Snover et al., 2006), PER (Tillmann et al., 1997), and CDER (Leusch et al., 2006). BLEU scores are reported as percentage and all error rates are reported as one minus the original value, also as percentage, so that all metrics are in the 0-100 range, and higher scores indicate better translations. The main reason for not conducting human evaluation, as it happens in the standard Trans45 original ID BLEU normalized truecased normalized lowercased 1-TER 1-PER 1-CDER BLEU 1-TER 1-PER 1-CDER Czech→English CUNI 29.64 29.79±1.07 47.45±1.15 61.64±1.06 52.18±0.98 31.68±1.14 49.84±1.10 64.38±1."
W14-3302,W14-3328,0,0.0383178,"Missing"
W14-3302,W14-3301,1,0.485672,"guage pair by translating newspaper articles and provided training data. 2.1 2.2 As in past years we provided parallel corpora to train translation models, monolingual corpora to train language models, and development sets to tune system parameters. Some training corpora were identical from last year (Europarl4 , United Nations, French-English 109 corpus, CzEng, Common Crawl, Russian-English Wikipedia Headlines provided by CMU), some were updated (Russian-English parallel data provided by Yandex, News Commentary, monolingual data), and a new corpus was added (HindiEnglish corpus, Bojar et al. (2014)), Hindi-English Wikipedia Headline corpus). Some statistics about the training materials are given in Figure 1. Test data The test data for this year’s task was selected from news stories from online sources, as before. However, we changed our method to create the test sets. In previous years, we took equal amounts of source sentences from all six languages involved (around 500 sentences each), and translated them into all other languages. While this produced a multi-parallel test corpus that could be also used for language pairs (such as Czech-Russian) that we did not include in the evaluati"
W14-3302,W14-3330,0,0.0464303,"Missing"
W14-3302,W14-3320,0,0.0387654,"Missing"
W14-3302,W14-3317,0,0.0337314,"Missing"
W14-3302,W14-3343,1,0.726267,"Finally, with respect to out-of-domain (different 41 It is interesting to mention the indirect use of human translations by USHEFF for Tasks 1.1-1.3: given a translation for a source segment, all other translations for the same segment were used as pseudo-references. Apart from when this translation was actually the human translation, the human translation was effectively used as a reference. While this reference was mixed with 23 other pseudo-references (other machine translations) for the feature computations, these features led to significant gains in performance over the baseline features Scarton and Specia (2014). We believe that more investigation is needed for human translation quality prediction. Tasks dedicated to this type of data at both sentence- and word-level in the next editions of this shared task would be a possible starting point. The acquisition of such data is however much more costly, as it is arguably hard to find examples of low quality human translation, unless specific settings, such as translation learner corpora, are considered. text domain and MT system) test data, for Task 1.1, none of the papers submitted included experiments. (Shah and Specia, 2014) applied the models trained"
W14-3302,W02-1001,0,\N,Missing
W14-3302,E06-1031,0,\N,Missing
W14-3302,W12-3102,1,\N,Missing
W14-3302,P13-2135,0,\N,Missing
W14-3302,W14-3311,0,\N,Missing
W14-3302,W14-3314,0,\N,Missing
W14-3302,W09-0401,1,\N,Missing
W14-3302,W14-3319,0,\N,Missing
W14-3302,W14-3344,0,\N,Missing
W14-3302,W14-3327,0,\N,Missing
W14-3302,W07-0718,1,\N,Missing
W14-3302,P11-1132,0,\N,Missing
W14-3302,W13-2248,0,\N,Missing
W14-3302,W14-3307,0,\N,Missing
W14-3302,W10-1703,1,\N,Missing
W14-3302,W14-3323,0,\N,Missing
W14-3302,P13-4014,1,\N,Missing
W14-3302,W08-0309,1,\N,Missing
W14-3302,W14-3340,1,\N,Missing
W14-3302,W14-3312,0,\N,Missing
W14-3302,P13-1139,0,\N,Missing
W14-3302,W14-3310,1,\N,Missing
W14-3302,P13-1004,1,\N,Missing
W14-3302,W14-3304,0,\N,Missing
W14-3302,W14-3321,0,\N,Missing
W14-3302,W14-3308,0,\N,Missing
W14-3302,uresova-etal-2014-multilingual,1,\N,Missing
W14-3302,W14-3336,1,\N,Missing
W14-3302,W14-3331,0,\N,Missing
W14-3302,W14-3332,0,\N,Missing
W14-3302,W14-3325,0,\N,Missing
W14-3302,W14-3329,0,\N,Missing
W14-3302,W14-3315,0,\N,Missing
W14-3302,W13-2201,1,\N,Missing
W14-3302,W14-3339,0,\N,Missing
W14-3302,W14-3322,1,\N,Missing
W14-3302,W14-3303,0,\N,Missing
W14-3302,W14-3318,0,\N,Missing
W14-3302,W14-3341,0,\N,Missing
W14-3302,W11-2101,1,\N,Missing
W14-3302,W14-3338,1,\N,Missing
W14-3309,D11-1033,0,0.150733,"Missing"
W14-3309,2013.iwslt-evaluation.3,1,0.854043,"8 10.39 20.85 19.39 30.82 19.67 10.52 +0.25 +0.55 +0.09 +0.89 +0.13 27.44 26.42 31.64 24.45 15.48 27.34 26.42 31.76 24.63 15.26 ∆ -0.10 ±0.00 +0.12 +0.18 -0.22 Table 1: Using Word Clusters in Phrase-based and OSM models – B0 = System without Clusters, +Cid = with Cluster We also trained OSM models over POS and morph tags. For the English-to-German system we added an OSM model over [pos, morph] (source:pos, target:morph) and for the Germanto-English system we added an OSM model over [morph,pos] (source:morph, target:pos), a configuration that was found to work best in our previous experiments (Birch et al., 2013). Table 2 shows gains from additionally using OSM models over POS/morph tags. Lang B0 +OSMp,m ∆ en-de de-en 20.44 27.24 20.60 27.44 +0.16 +0.20 Unsupervised Transliteration Model Pair Training OOV B0 +Tr ∆ ru-en en-ru hi-en en-hi 232K 232K 38K 38K 1356 681 503 394 24.63 19.67 14.67 11.76 25.06 19.91 15.48 12.83 +0.41 +0.24 +0.81 +1.07 Table 3: Using Unsupervised Transliteration Model – Training = Extracted Transliteration Corpus (types), OOV = Out-of-vocabulary words (tokens) B0 = System without Transliteration, +Tr = Transliterating OOVs Table 3 shows the number (types) of transliteration pai"
W14-3309,D08-1023,0,0.0296702,".ed.ac.uk Kenneth Heafield Computer Science Department Stanford University heafield@cs.stanford.edu Abstract Our baseline systems are based on the setup described in (Durrani et al., 2013b) that we used for the Eighth Workshop on Statistical Machine Translation (Bojar et al., 2013). The notable features of these systems are described in the following section. The experiments that we carried out for this year’s translation task are described in the following sections. (Durrani et al., 2013a) with 4 count-based supportive features, sparse domain indicator, phrase length, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the noreordering-over-punctuation heuristic (Koehn and Haddow, 2009). We used POS and morphological tags as additional factors in phrase translation models (Koehn and Hoang, 2007) for GermanEnglish language pairs. We also trained target sequence models on the in-domain subset of the parallel corpus using Kneser-Ney smoothed 7-gram mod"
W14-3309,W13-2201,1,0.872446,"Missing"
W14-3309,buck-etal-2014-n,1,0.885994,"Missing"
W14-3309,E14-4029,1,0.915819,"tatistical Machine Translation (WMT). We participated in all language pairs. We have improved upon our 2013 system by i) using generalized representations, specifically automatic word clusters for translations out of English, ii) using unsupervised character-based models to translate unknown words in RussianEnglish and Hindi-English pairs, iii) synthesizing Hindi data from closely-related Urdu data, and iv) building huge language on the common crawl corpus. 1 Translation Task Baseline Using Generalized Word Representations We explored the use of automatic word clusters in phrase-based models (Durrani et al., 2014a). We computed the clusters with GIZA++’s mkcls (Och, 1999) on the source and target side of the parallel training corpus. Clusters are word classes that are optimized to reduce n-gram perplexity. By generating a cluster identifier for each output word, we are able to add an n-gram model We trained our systems with the following settings: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, hierarchical lexicalized reordering (Galley and Manning, 2008), a l"
W14-3309,N09-1025,0,0.082017,"Missing"
W14-3309,P11-1105,1,0.910095,"Missing"
W14-3309,P05-1066,1,0.793483,"limit of 6, maximum phrase-length of 5, 100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the noreordering-over-punctuation heuristic (Koehn and Haddow, 2009). We used POS and morphological tags as additional factors in phrase translation models (Koehn and Hoang, 2007) for GermanEnglish language pairs. We also trained target sequence models on the in-domain subset of the parallel corpus using Kneser-Ney smoothed 7-gram models. We used syntactic-preordering (Collins et al., 2005) and compound splitting (Koehn and Knight, 2003) for German-to-English systems. We used trivia tokenizer for tokenizing Hindi. The systems were tuned on a very large tuning set consisting of the test sets from 2008-2012, with a total of 13,071 sentences. We used newstest 2013 for the dev experiments. For RussianEnglish pairs news-test 2012 was used for tuning and for Hindi-English pairs, we divided the newsdev 2014 into two halves, used the first half for tuning and second for dev experiments. 1.1 1.2 This paper describes the University of Edinburgh’s (UEDIN) phrase-based submissions to the tr"
W14-3309,P13-2071,1,0.866776,"s for WMT-14 Nadir Durrani Barry Haddow Philipp Koehn School of Informatics University of Edinburgh {dnadir,bhaddow,pkoehn}@inf.ed.ac.uk Kenneth Heafield Computer Science Department Stanford University heafield@cs.stanford.edu Abstract Our baseline systems are based on the setup described in (Durrani et al., 2013b) that we used for the Eighth Workshop on Statistical Machine Translation (Bojar et al., 2013). The notable features of these systems are described in the following section. The experiments that we carried out for this year’s translation task are described in the following sections. (Durrani et al., 2013a) with 4 count-based supportive features, sparse domain indicator, phrase length, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the noreordering-over-punctuation heuristic (Koehn and Haddow, 2009). We used POS and morphological tags as additional factors in phrase translation models (Koehn and Hoang, 2007) for GermanEnglish langua"
W14-3309,W14-3310,1,0.89194,"Missing"
W14-3309,W13-2212,1,0.838117,"s for WMT-14 Nadir Durrani Barry Haddow Philipp Koehn School of Informatics University of Edinburgh {dnadir,bhaddow,pkoehn}@inf.ed.ac.uk Kenneth Heafield Computer Science Department Stanford University heafield@cs.stanford.edu Abstract Our baseline systems are based on the setup described in (Durrani et al., 2013b) that we used for the Eighth Workshop on Statistical Machine Translation (Bojar et al., 2013). The notable features of these systems are described in the following section. The experiments that we carried out for this year’s translation task are described in the following sections. (Durrani et al., 2013a) with 4 count-based supportive features, sparse domain indicator, phrase length, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the noreordering-over-punctuation heuristic (Koehn and Haddow, 2009). We used POS and morphological tags as additional factors in phrase translation models (Koehn and Hoang, 2007) for GermanEnglish langua"
W14-3309,D08-1089,0,0.557258,"d models (Durrani et al., 2014a). We computed the clusters with GIZA++’s mkcls (Och, 1999) on the source and target side of the parallel training corpus. Clusters are word classes that are optimized to reduce n-gram perplexity. By generating a cluster identifier for each output word, we are able to add an n-gram model We trained our systems with the following settings: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, hierarchical lexicalized reordering (Galley and Manning, 2008), a lexicallydriven 5-gram operation sequence model (OSM) 97 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 97–104, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics over these identifiers as an additional scoring function. The inclusion of such an additional factor is trivial given the factored model implementation (Koehn and Hoang, 2007) of Moses (Koehn et al., 2007). The n-gram model is trained in the similar way as the regular language model. We trained domain-specific language models separately and then linearly interp"
W14-3309,2014.eamt-1.17,1,0.831092,"The Hindi-English segment of this corpus is a subset of parallel data made available for the translation task but is completely disjoint from the Urdu-English segment. We initially trained a Urdu-to-Hindi SMT system using a very tiny EMILLE1 corpus (Baker Table 2: Using POS and Morph Tags in OSM models – B0 = Baseline, +OSMp,m = POS/Morph-based OSM 1 EMILLE corpus contains roughly 12000 sentences of Hindi and Urdu comparable data. From these we were able to sentence align 7000 sentences to build an Urdu-to-Hindi system. 98 using transliteration and triangulated phrase-tables are presented in Durrani and Koehn (2014). Using our best Urdu-to-Hindi system, we translated the Urdu part of the multi-indic corpus to form HindiEnglish parallel data. Table 4 shows results from using the synthesized Hindi-English corpus in isolation (Syn) and on top of the baseline system (B0 + Syn). et al., 2002). But we found this system to be useless for translating the Urdu part of Indic data due to domain mismatch and huge number of OOV words (approximately 310K tokens). To reduce sparsity we synthesized additional phrase-tables using interpolation and transliteration. Interpolation: We trained two phrase translation tables p"
W14-3309,W11-2123,1,0.79041,"ntations We explored the use of automatic word clusters in phrase-based models (Durrani et al., 2014a). We computed the clusters with GIZA++’s mkcls (Och, 1999) on the source and target side of the parallel training corpus. Clusters are word classes that are optimized to reduce n-gram perplexity. By generating a cluster identifier for each output word, we are able to add an n-gram model We trained our systems with the following settings: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, hierarchical lexicalized reordering (Galley and Manning, 2008), a lexicallydriven 5-gram operation sequence model (OSM) 97 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 97–104, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics over these identifiers as an additional scoring function. The inclusion of such an additional factor is trivial given the factored model implementation (Koehn and Hoang, 2007) of Moses (Koehn et al., 2007). The n-gram model is trained in the similar way as the regular language model"
W14-3309,C14-1041,1,0.92685,"tatistical Machine Translation (WMT). We participated in all language pairs. We have improved upon our 2013 system by i) using generalized representations, specifically automatic word clusters for translations out of English, ii) using unsupervised character-based models to translate unknown words in RussianEnglish and Hindi-English pairs, iii) synthesizing Hindi data from closely-related Urdu data, and iv) building huge language on the common crawl corpus. 1 Translation Task Baseline Using Generalized Word Representations We explored the use of automatic word clusters in phrase-based models (Durrani et al., 2014a). We computed the clusters with GIZA++’s mkcls (Och, 1999) on the source and target side of the parallel training corpus. Clusters are word classes that are optimized to reduce n-gram perplexity. By generating a cluster identifier for each output word, we are able to add an n-gram model We trained our systems with the following settings: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, hierarchical lexicalized reordering (Galley and Manning, 2008), a l"
W14-3309,P13-2121,1,0.908258,"Missing"
W14-3309,P07-1019,0,0.118146,"sed for the Eighth Workshop on Statistical Machine Translation (Bojar et al., 2013). The notable features of these systems are described in the following section. The experiments that we carried out for this year’s translation task are described in the following sections. (Durrani et al., 2013a) with 4 count-based supportive features, sparse domain indicator, phrase length, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the noreordering-over-punctuation heuristic (Koehn and Haddow, 2009). We used POS and morphological tags as additional factors in phrase translation models (Koehn and Hoang, 2007) for GermanEnglish language pairs. We also trained target sequence models on the in-domain subset of the parallel corpus using Kneser-Ney smoothed 7-gram models. We used syntactic-preordering (Collins et al., 2005) and compound splitting (Koehn and Knight, 2003) for German-to-English systems. We used trivia tokenizer for tokenizing Hindi. The systems w"
W14-3309,E99-1010,0,0.288751,"pairs. We have improved upon our 2013 system by i) using generalized representations, specifically automatic word clusters for translations out of English, ii) using unsupervised character-based models to translate unknown words in RussianEnglish and Hindi-English pairs, iii) synthesizing Hindi data from closely-related Urdu data, and iv) building huge language on the common crawl corpus. 1 Translation Task Baseline Using Generalized Word Representations We explored the use of automatic word clusters in phrase-based models (Durrani et al., 2014a). We computed the clusters with GIZA++’s mkcls (Och, 1999) on the source and target side of the parallel training corpus. Clusters are word classes that are optimized to reduce n-gram perplexity. By generating a cluster identifier for each output word, we are able to add an n-gram model We trained our systems with the following settings: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, hierarchical lexicalized reordering (Galley and Manning, 2008), a lexicallydriven 5-gram operation sequence model (OSM) 97 Proc"
W14-3309,2005.mtsummit-papers.11,1,0.0781164,"Language Models Our unconstrained submissions use an additional language model trained on web pages from the 2012, 2013, and winter 2013 CommonCrawl.2 The additional language model is the only difference between the constrained and unconstrained submissions; we did not use additional parallel data. These language models were trained on text provided by the CommonCrawl foundation, which they converted to UTF-8 after stripping HTML. Languages were detected using the Compact Language Detection 23 and, except for Hindi where we lack tools, sentences were split with the Europarl sentence splitter (Koehn, 2005). All text was then deduplicated, minimizing the impact of boilerplate, such as social media sharing buttons. We then tokenized and truecased the text as usual. Statistics are shown in Table 5. A full description of the pipeline, including a public data release, appears in Buck et al. (2014). Transliteration: Urdu and Hindi are written in different scripts (Arabic and Devanagri respectively). We added a transliteration component to our Urdu-to-Hindi system. An unsupervised transliteration model is learned from the wordalignments of Urdu-Hindi parallel data. We were able to extract around 2800"
W14-3309,W12-3152,0,0.0414194,"on, +Tr = Transliterating OOVs Table 3 shows the number (types) of transliteration pairs extracted using unsupervised mining, number of OOV words (tokens) in each pair and the gains achieved by transliterating unknown words. 1.4 Synthesizing Hindi Data from Urdu Hindi and Urdu are closely related language pairs that share grammatical structure and have a large overlap in vocabulary. This provides a strong motivation to transform any Urdu-English parallel data into Hindi-English by translating the Urdu part into Hindi. We made use of the Urdu-English segment of the Indic multi-parallel corpus (Post et al., 2012) which contains roughly 87K sentence pairs. The Hindi-English segment of this corpus is a subset of parallel data made available for the translation task but is completely disjoint from the Urdu-English segment. We initially trained a Urdu-to-Hindi SMT system using a very tiny EMILLE1 corpus (Baker Table 2: Using POS and Morph Tags in OSM models – B0 = Baseline, +OSMp,m = POS/Morph-based OSM 1 EMILLE corpus contains roughly 12000 sentences of Hindi and Urdu comparable data. From these we were able to sentence align 7000 sentences to build an Urdu-to-Hindi system. 98 using transliteration and t"
W14-3309,W13-2228,1,0.84762,"ne, discontinuous, swap), and one that distinguishes the discontinuous orientations to the left and right. Table 8 shows slight improvements with these models, so we used them in our baseline. Russian-English: We tried to improve wordalignments by integrating a transliteration submodel into GIZA++ word aligner. The probability of a word pair is calculated as an interpolation of the transliteration probability and translation probability stored in the t-table of the different alignment models used by the GIZA++ aligner. This interpolation is done for all iterations of all alignment models (See Sajjad et al. (2013) for details). Due to shortage of time we could only run it for Russian-to-English. The improved alignments gave a gain of +0.21 on news-test 2013 and +0.40 on news-test 2014. Threshold filtering of phrase table: We experimented with discarding some phrase table entry due to their low probability. We found that phrase translations with the phrase translation probability 100 φ(f |e)&lt;10−4 can be safely discarded with almost no change in translations. However, discarding phrase translations with the inverse phrase translation probability φ(e|f )&lt;10−4 is more risky, especially with morphologically"
W14-3309,D07-1091,1,0.922736,"n the following sections. (Durrani et al., 2013a) with 4 count-based supportive features, sparse domain indicator, phrase length, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the noreordering-over-punctuation heuristic (Koehn and Haddow, 2009). We used POS and morphological tags as additional factors in phrase translation models (Koehn and Hoang, 2007) for GermanEnglish language pairs. We also trained target sequence models on the in-domain subset of the parallel corpus using Kneser-Ney smoothed 7-gram models. We used syntactic-preordering (Collins et al., 2005) and compound splitting (Koehn and Knight, 2003) for German-to-English systems. We used trivia tokenizer for tokenizing Hindi. The systems were tuned on a very large tuning set consisting of the test sets from 2008-2012, with a total of 13,071 sentences. We used newstest 2013 for the dev experiments. For RussianEnglish pairs news-test 2012 was used for tuning and for Hindi-English pa"
W14-3309,I08-2089,1,0.83166,"edings of the Ninth Workshop on Statistical Machine Translation, pages 97–104, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics over these identifiers as an additional scoring function. The inclusion of such an additional factor is trivial given the factored model implementation (Koehn and Hoang, 2007) of Moses (Koehn et al., 2007). The n-gram model is trained in the similar way as the regular language model. We trained domain-specific language models separately and then linearly interpolated them using SRILM with weights optimized on the tuning set (Schwenk and Koehn, 2008). We also trained OSM models over cluster-ids (?). The lexically driven OSM model falls back to very small context sizes of two to three operations due to data sparsity. Learning operation sequences over cluster-ids enables us to learn richer translation and reordering patterns that can generalize better in sparse data conditions. Table 1 shows gains from adding target LM and OSM models over cluster-ids. Using word clusters was found more useful translating from English-to-*. from English Lang de cs fr ru hi 1.3 Last year, our Russian-English systems performed badly on the human evaluation. In"
W14-3309,J82-2005,0,0.208115,"Missing"
W14-3309,N07-1061,0,0.119594,"Missing"
W14-3309,E03-1076,1,0.896383,"est translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the noreordering-over-punctuation heuristic (Koehn and Haddow, 2009). We used POS and morphological tags as additional factors in phrase translation models (Koehn and Hoang, 2007) for GermanEnglish language pairs. We also trained target sequence models on the in-domain subset of the parallel corpus using Kneser-Ney smoothed 7-gram models. We used syntactic-preordering (Collins et al., 2005) and compound splitting (Koehn and Knight, 2003) for German-to-English systems. We used trivia tokenizer for tokenizing Hindi. The systems were tuned on a very large tuning set consisting of the test sets from 2008-2012, with a total of 13,071 sentences. We used newstest 2013 for the dev experiments. For RussianEnglish pairs news-test 2012 was used for tuning and for Hindi-English pairs, we divided the newsdev 2014 into two halves, used the first half for tuning and second for dev experiments. 1.1 1.2 This paper describes the University of Edinburgh’s (UEDIN) phrase-based submissions to the translation and medical translation shared tasks o"
W14-3309,P07-1108,0,0.0994085,"Missing"
W14-3309,N04-1022,0,0.220347,"d in (Durrani et al., 2013b) that we used for the Eighth Workshop on Statistical Machine Translation (Bojar et al., 2013). The notable features of these systems are described in the following section. The experiments that we carried out for this year’s translation task are described in the following sections. (Durrani et al., 2013a) with 4 count-based supportive features, sparse domain indicator, phrase length, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the noreordering-over-punctuation heuristic (Koehn and Haddow, 2009). We used POS and morphological tags as additional factors in phrase translation models (Koehn and Hoang, 2007) for GermanEnglish language pairs. We also trained target sequence models on the in-domain subset of the parallel corpus using Kneser-Ney smoothed 7-gram models. We used syntactic-preordering (Collins et al., 2005) and compound splitting (Koehn and Knight, 2003) for German-to-English systems. We used trivia tokeni"
W14-3309,P10-2041,0,0.127106,"Missing"
W14-3309,P12-2059,0,0.0266614,". We added a transliteration component to our Urdu-to-Hindi system. An unsupervised transliteration model is learned from the wordalignments of Urdu-Hindi parallel data. We were able to extract around 2800 transliteration pairs. To learn a richer transliteration model, we additionally fed the interpolated phrase-table, as described above, to the transliteration miner. We were able to mine additional 21000 transliteration pairs and built a Urdu-Hindi character-based model from it. The transliteration module can be used to translate the 50K OOV words but previous research (Durrani et al., 2010; Nakov and Tiedemann, 2012) has shown that transliteration is useful for more than just translating OOV words when translating closely related language pairs. To fully capitalize on the large overlap in Hindi–Urdu vocabulary, we transliterated each word in the Urdu test-data into Hindi and produced a phrase-table with 100-best transliterations. The two synthesized (triangulated and transliterated) phrase-tables are then used along with the baseline Urdu-to-Hindi phrase-table in a log-linear model. Detailed results on Urdu-toHindi baseline and improvements obtained from Lang en de fr ru cs hi Lines (B) Tokens (B) 59.13 3"
W14-3310,E14-2008,1,0.50059,"nburgh, Scotland † Karlsruhe Institute of Technology, Karlsruhe, Germany ∗ {freitag,peitz,wuebker,ney}@cs.rwth-aachen.de ‡ {mhuck,ndurrani,pkoehn}@inf.ed.ac.uk ‡ v1rsennr@staffmail.ed.ac.uk ‡ maria.nadejde@gmail.com,p.j.williams-2@sms.ed.ac.uk † {teresa.herrmann,eunah.cho,alex.waibel}@kit.edu ‡ Matthias Abstract joint WMT submission of three EU-BRIDGE project partners. RWTH Aachen University (RWTH), the University of Edinburgh (UEDIN) and Karlsruhe Institute of Technology (KIT) all provided several individual systems which were combined by means of the RWTH Aachen system combination approach (Freitag et al., 2014). As distinguished from our EU-BRIDGE joint submission to the IWSLT 2013 evaluation campaign (Freitag et al., 2013), we particularly focused on translation of news text (instead of talks) for WMT. Besides, we put an emphasis on engineering syntaxbased systems in order to combine them with our more established phrase-based engines. We built combined system setups for translation from German to English as well as from English to German. This paper gives some insight into the technology behind the system combination framework and the combined engines which have been used to produce the joint EU-B"
W14-3310,D08-1089,0,0.0336771,"sequence model over cluster IDs. Furthermore, it learns OSM models over POS, morph and word classes. et al., 2010; Wuebker et al., 2012). The model weights of all systems have been tuned with standard Minimum Error Rate Training (Och, 2003) on a concatenation of the newstest2011 and newstest2012 sets. RWTH used B LEU as optimization objective. Both for language model estimation and querying at decoding, the KenLM toolkit (Heafield et al., 2013) is used. All RWTH systems include the standard set of models provided by Jane. Both systems have been augmented with a hierarchical orientation model (Galley and Manning, 2008; Huck et al., 2013) and a cluster language model (Wuebker et al., 2013). The phrasebased system (RWTH scss) has been further improved by maximum expected B LEU training similar to (He and Deng, 2012). The latter has been performed on a selection from the News Commentary, Europarl and Common Crawl corpora based on language and translation model cross-entropies (Mansour et al., 2011). 3 3.2 UEDIN’s syntax-based systems (Williams et al., 2014) follow the GHKM syntax approach as proposed by Galley, Hopkins, Knight, and Marcu (Galley et al., 2004). The open source Moses implementation has been emp"
W14-3310,N04-1035,0,0.0285873,"gmented with a hierarchical orientation model (Galley and Manning, 2008; Huck et al., 2013) and a cluster language model (Wuebker et al., 2013). The phrasebased system (RWTH scss) has been further improved by maximum expected B LEU training similar to (He and Deng, 2012). The latter has been performed on a selection from the News Commentary, Europarl and Common Crawl corpora based on language and translation model cross-entropies (Mansour et al., 2011). 3 3.2 UEDIN’s syntax-based systems (Williams et al., 2014) follow the GHKM syntax approach as proposed by Galley, Hopkins, Knight, and Marcu (Galley et al., 2004). The open source Moses implementation has been employed to extract GHKM rules (Williams and Koehn, 2012). Composed rules (Galley et al., 2006) are extracted in addition to minimal rules, but only up to the following limits: at most twenty tree nodes per rule, a maximum depth of five, and a maximum size of five. Singleton hierarchical rules are dropped. The features for the syntax-based systems comprise Good-Turing-smoothed phrase translation probabilities, lexical translation probabilities in both directions, word and phrase penalty, a rule rareness penalty, a monolingual PCFG probability, an"
W14-3310,P05-1066,1,0.0515024,"employs both the phrase-based (RWTH scss) and the hierarchical (RWTH hiero) decoder implemented in RWTH’s publicly available translation toolkit Jane (Vilar 1 http://www.eu-bridge.eu 105 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 105–113, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics els over POS and morph sequences following Durrani et al. (2013c). The English→German system additionally comprises a target-side LM over automatically built word classes (Birch et al., 2013). UEDIN has applied syntactic prereordering (Collins et al., 2005) and compound splitting (Koehn and Knight, 2003) of the source side for the German→English system. The systems have been tuned on a very large tuning set consisting of the test sets from 2008-2012, with a total of 13,071 sentences. UEDIN used newstest2013 as held-out test set. On top of UEDIN phrase-based 1 system, UEDIN phrase-based 2 augments word classes as additional factor and learns an interpolated target sequence model over cluster IDs. Furthermore, it learns OSM models over POS, morph and word classes. et al., 2010; Wuebker et al., 2012). The model weights of all systems have been tune"
W14-3310,P06-1121,0,0.0128251,". The phrasebased system (RWTH scss) has been further improved by maximum expected B LEU training similar to (He and Deng, 2012). The latter has been performed on a selection from the News Commentary, Europarl and Common Crawl corpora based on language and translation model cross-entropies (Mansour et al., 2011). 3 3.2 UEDIN’s syntax-based systems (Williams et al., 2014) follow the GHKM syntax approach as proposed by Galley, Hopkins, Knight, and Marcu (Galley et al., 2004). The open source Moses implementation has been employed to extract GHKM rules (Williams and Koehn, 2012). Composed rules (Galley et al., 2006) are extracted in addition to minimal rules, but only up to the following limits: at most twenty tree nodes per rule, a maximum depth of five, and a maximum size of five. Singleton hierarchical rules are dropped. The features for the syntax-based systems comprise Good-Turing-smoothed phrase translation probabilities, lexical translation probabilities in both directions, word and phrase penalty, a rule rareness penalty, a monolingual PCFG probability, and a 5-gram language model. UEDIN has used the SRILM toolkit (Stolcke, 2002) to train the language model and relies on KenLM for language model"
W14-3310,P13-2071,1,0.0664637,"ction 2 EU-BRIDGE1 is a European research project which is aimed at developing innovative speech translation technology. This paper describes a RWTH Aachen University RWTH (Peitz et al., 2014) employs both the phrase-based (RWTH scss) and the hierarchical (RWTH hiero) decoder implemented in RWTH’s publicly available translation toolkit Jane (Vilar 1 http://www.eu-bridge.eu 105 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 105–113, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics els over POS and morph sequences following Durrani et al. (2013c). The English→German system additionally comprises a target-side LM over automatically built word classes (Birch et al., 2013). UEDIN has applied syntactic prereordering (Collins et al., 2005) and compound splitting (Koehn and Knight, 2003) of the source side for the German→English system. The systems have been tuned on a very large tuning set consisting of the test sets from 2008-2012, with a total of 13,071 sentences. UEDIN used newstest2013 as held-out test set. On top of UEDIN phrase-based 1 system, UEDIN phrase-based 2 augments word classes as additional factor and learns an interpolate"
W14-3310,2012.iwslt-papers.17,1,0.805256,".1 Syntax-based Systems Phrase-based Systems UEDIN’s phrase-based systems (Durrani et al., 2014) have been trained using the Moses toolkit (Koehn et al., 2007), replicating the settings described in (Durrani et al., 2013b). The features include: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, a lexically-driven 5-gram operation sequence model (OSM) (Durrani et al., 2013a), msd-bidirectional-fe lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), a distortion limit of 6, a maximum phrase length of 5, 100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-overpunctuation heuristic. UEDIN uses POS and morphological target sequence models built on the indomain subset of the parallel corpus using KneserNey smoothed 7-gram models as additional factors in phrase translation models (Koehn and Hoang, 2007). UEDIN has furthermore built OSM mod106 model. The monolingual part of those parallel"
W14-3310,W13-2212,1,0.898684,"ction 2 EU-BRIDGE1 is a European research project which is aimed at developing innovative speech translation technology. This paper describes a RWTH Aachen University RWTH (Peitz et al., 2014) employs both the phrase-based (RWTH scss) and the hierarchical (RWTH hiero) decoder implemented in RWTH’s publicly available translation toolkit Jane (Vilar 1 http://www.eu-bridge.eu 105 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 105–113, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics els over POS and morph sequences following Durrani et al. (2013c). The English→German system additionally comprises a target-side LM over automatically built word classes (Birch et al., 2013). UEDIN has applied syntactic prereordering (Collins et al., 2005) and compound splitting (Koehn and Knight, 2003) of the source side for the German→English system. The systems have been tuned on a very large tuning set consisting of the test sets from 2008-2012, with a total of 13,071 sentences. UEDIN used newstest2013 as held-out test set. On top of UEDIN phrase-based 1 system, UEDIN phrase-based 2 augments word classes as additional factor and learns an interpolate"
W14-3310,P12-1031,0,0.00714997,"um Error Rate Training (Och, 2003) on a concatenation of the newstest2011 and newstest2012 sets. RWTH used B LEU as optimization objective. Both for language model estimation and querying at decoding, the KenLM toolkit (Heafield et al., 2013) is used. All RWTH systems include the standard set of models provided by Jane. Both systems have been augmented with a hierarchical orientation model (Galley and Manning, 2008; Huck et al., 2013) and a cluster language model (Wuebker et al., 2013). The phrasebased system (RWTH scss) has been further improved by maximum expected B LEU training similar to (He and Deng, 2012). The latter has been performed on a selection from the News Commentary, Europarl and Common Crawl corpora based on language and translation model cross-entropies (Mansour et al., 2011). 3 3.2 UEDIN’s syntax-based systems (Williams et al., 2014) follow the GHKM syntax approach as proposed by Galley, Hopkins, Knight, and Marcu (Galley et al., 2004). The open source Moses implementation has been employed to extract GHKM rules (Williams and Koehn, 2012). Composed rules (Galley et al., 2006) are extracted in addition to minimal rules, but only up to the following limits: at most twenty tree nodes"
W14-3310,P13-2121,1,0.0604984,"Missing"
W14-3310,W14-3309,1,0.840972,"btained high sentence-level B LEU scores when being translated with a baseline phrasebased system, and each contain less than 30 words for more rapid tuning. Decoding for the syntaxbased systems is carried out with cube pruning using Moses’ hierarchical decoder (Hoang et al., 2009). UEDIN’s German→English syntax-based setup is a string-to-tree system with compound splitting University of Edinburgh UEDIN contributed phrase-based and syntaxbased systems to both the German→English and the English→German joint submission. 3.1 Syntax-based Systems Phrase-based Systems UEDIN’s phrase-based systems (Durrani et al., 2014) have been trained using the Moses toolkit (Koehn et al., 2007), replicating the settings described in (Durrani et al., 2013b). The features include: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, a lexically-driven 5-gram operation sequence model (OSM) (Durrani et al., 2013a), msd-bidirectional-fe lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), a distortion limit of 6, a maximum phrase length of 5, 100-best translatio"
W14-3310,W11-2123,0,0.00995075,"a string-to-tree system with compound splitting University of Edinburgh UEDIN contributed phrase-based and syntaxbased systems to both the German→English and the English→German joint submission. 3.1 Syntax-based Systems Phrase-based Systems UEDIN’s phrase-based systems (Durrani et al., 2014) have been trained using the Moses toolkit (Koehn et al., 2007), replicating the settings described in (Durrani et al., 2013b). The features include: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, a lexically-driven 5-gram operation sequence model (OSM) (Durrani et al., 2013a), msd-bidirectional-fe lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), a distortion limit of 6, a maximum phrase length of 5, 100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-overpunctuation heuristic. UEDIN uses POS and morphological target sequence models built on the indomain subset of the parallel corpus"
W14-3310,W06-1607,0,0.0222136,"., 2013). It uses a modified syntactic label set, target-side compound splitting, and additional syntactic constraints. UEDIN GHKM S2T (BitPar): A string-to-tree system trained with target-side syntactic annotation obtained with BitPar (Schmid, 2004). The word alignment for German→English is generated using the GIZA++ toolkit (Och and Ney, 2003). For English→German, KIT uses discriminative word alignment (Niehues and Vogel, 2008). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified KneserNey smoothing as in (Foster et al., 2006). UEDIN GHKM S2T (Stanford): A string-totree system trained with target-side syntactic annotation obtained with the German Stanford Parser (Rafferty and Manning, 2008a). In both systems KIT applies short-range reorderings (Rottmann and Vogel, 2007) and longrange reorderings (Niehues and Kolss, 2009) based on POS tags (Schmid, 1994) to perform source sentence reordering according to the target language word order. The long-range reordering rules are applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering"
W14-3310,W13-0805,1,0.0274022,"GHKM S2T (Stanford): A string-totree system trained with target-side syntactic annotation obtained with the German Stanford Parser (Rafferty and Manning, 2008a). In both systems KIT applies short-range reorderings (Rottmann and Vogel, 2007) and longrange reorderings (Niehues and Kolss, 2009) based on POS tags (Schmid, 1994) to perform source sentence reordering according to the target language word order. The long-range reordering rules are applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008b; Klein and Manning, 2003) as well as a lexicalized reordering model (Koehn et al., 2005) are applied. UEDIN GHKM S2T (Berkeley): A string-totree system trained with target-side syntactic annotation obtained with the German Berkeley Parser (Petrov and Klein, 2007; Petrov and Klein, 2008). UEDIN GHKM T2S (Berkeley): A tree-tostring system trained with source-side syntactic annotation obtained with the English Berkeley Parser (Petrov et al., 2006). UEDIN GHKM S2S (Berkeley): A string-tostring system. The extraction is GHKMbased with s"
W14-3310,2011.iwslt-papers.5,1,0.681344,"rying at decoding, the KenLM toolkit (Heafield et al., 2013) is used. All RWTH systems include the standard set of models provided by Jane. Both systems have been augmented with a hierarchical orientation model (Galley and Manning, 2008; Huck et al., 2013) and a cluster language model (Wuebker et al., 2013). The phrasebased system (RWTH scss) has been further improved by maximum expected B LEU training similar to (He and Deng, 2012). The latter has been performed on a selection from the News Commentary, Europarl and Common Crawl corpora based on language and translation model cross-entropies (Mansour et al., 2011). 3 3.2 UEDIN’s syntax-based systems (Williams et al., 2014) follow the GHKM syntax approach as proposed by Galley, Hopkins, Knight, and Marcu (Galley et al., 2004). The open source Moses implementation has been employed to extract GHKM rules (Williams and Koehn, 2012). Composed rules (Galley et al., 2006) are extracted in addition to minimal rules, but only up to the following limits: at most twenty tree nodes per rule, a maximum depth of five, and a maximum size of five. Singleton hierarchical rules are dropped. The features for the syntax-based systems comprise Good-Turing-smoothed phrase t"
W14-3310,2009.iwslt-papers.4,1,0.346713,"bility, and a 5-gram language model. UEDIN has used the SRILM toolkit (Stolcke, 2002) to train the language model and relies on KenLM for language model scoring during decoding. Model weights are optimized to maximize B LEU. 2000 sentences from the newstest2008-2012 sets have been selected as a development set. The selected sentences obtained high sentence-level B LEU scores when being translated with a baseline phrasebased system, and each contain less than 30 words for more rapid tuning. Decoding for the syntaxbased systems is carried out with cube pruning using Moses’ hierarchical decoder (Hoang et al., 2009). UEDIN’s German→English syntax-based setup is a string-to-tree system with compound splitting University of Edinburgh UEDIN contributed phrase-based and syntaxbased systems to both the German→English and the English→German joint submission. 3.1 Syntax-based Systems Phrase-based Systems UEDIN’s phrase-based systems (Durrani et al., 2014) have been trained using the Moses toolkit (Koehn et al., 2007), replicating the settings described in (Durrani et al., 2013b). The features include: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser"
W14-3310,D09-1022,1,0.0473567,"ingle generic nonterminal instead of syntactic ones, plus rules that have been added from plain phrase-based extraction (Huck et al., 2014). 4 Language models are trained with the SRILM toolkit (Stolcke, 2002) and use modified KneserNey smoothing. Both systems utilize a language model based on automatically learned word classes using the MKCLS algorithm (Och, 1999). The English→German system comprises language models based on fine-grained part-ofspeech tags (Schmid and Laws, 2008). In addition, a bilingual language model (Niehues et al., 2011) is used as well as a discriminative word lexicon (Mauser et al., 2009) using source context to guide the word choices in the target sentence. Karlsruhe Institute of Technology The KIT translations (Herrmann et al., 2014) are generated by an in-house phrase-based translations system (Vogel, 2003). The provided News Commentary, Europarl, and Common Crawl parallel corpora are used for training the translation 107 dividual system engines have been optimized on different test sets which partially or fully include newstest2011 or newstest2012. System combination weights are either optimized on newstest2011 or newstest2012. We kept newstest2013 as an unseen test set wh"
W14-3310,P07-1019,0,0.0254758,"he settings described in (Durrani et al., 2013b). The features include: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, a lexically-driven 5-gram operation sequence model (OSM) (Durrani et al., 2013a), msd-bidirectional-fe lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), a distortion limit of 6, a maximum phrase length of 5, 100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-overpunctuation heuristic. UEDIN uses POS and morphological target sequence models built on the indomain subset of the parallel corpus using KneserNey smoothed 7-gram models as additional factors in phrase translation models (Koehn and Hoang, 2007). UEDIN has furthermore built OSM mod106 model. The monolingual part of those parallel corpora, the News Shuffle corpus for both directions and additionally the Gigaword corpus for German→English are used as monolingual training data for the different language mod"
W14-3310,2011.iwslt-evaluation.9,1,0.882056,"n, UEDIN has trained various string-to-tree GHKM syntax systems which differ with respect to the syntactic annotation. A tree-to-string system and a string-to-string system (with rules that are not syntactically decorated) have been trained as well. The English→German UEDIN GHKM system names in Table 3 denote: Compound splitting (Koehn and Knight, 2003) is performed on the source side of the corpus for German→English translation before training. In order to improve the quality of the web-crawled Common Crawl corpus, noisy sentence pairs are filtered out using an SVM classifier as described by Mediani et al. (2011). UEDIN GHKM S2T (ParZu): A string-to-tree system trained with target-side syntactic annotation obtained with ParZu (Sennrich et al., 2013). It uses a modified syntactic label set, target-side compound splitting, and additional syntactic constraints. UEDIN GHKM S2T (BitPar): A string-to-tree system trained with target-side syntactic annotation obtained with BitPar (Schmid, 2004). The word alignment for German→English is generated using the GIZA++ toolkit (Och and Ney, 2003). For English→German, KIT uses discriminative word alignment (Niehues and Vogel, 2008). Phrase extraction and scoring is d"
W14-3310,W13-2258,1,0.0602517,"r IDs. Furthermore, it learns OSM models over POS, morph and word classes. et al., 2010; Wuebker et al., 2012). The model weights of all systems have been tuned with standard Minimum Error Rate Training (Och, 2003) on a concatenation of the newstest2011 and newstest2012 sets. RWTH used B LEU as optimization objective. Both for language model estimation and querying at decoding, the KenLM toolkit (Heafield et al., 2013) is used. All RWTH systems include the standard set of models provided by Jane. Both systems have been augmented with a hierarchical orientation model (Galley and Manning, 2008; Huck et al., 2013) and a cluster language model (Wuebker et al., 2013). The phrasebased system (RWTH scss) has been further improved by maximum expected B LEU training similar to (He and Deng, 2012). The latter has been performed on a selection from the News Commentary, Europarl and Common Crawl corpora based on language and translation model cross-entropies (Mansour et al., 2011). 3 3.2 UEDIN’s syntax-based systems (Williams et al., 2014) follow the GHKM syntax approach as proposed by Galley, Hopkins, Knight, and Marcu (Galley et al., 2004). The open source Moses implementation has been employed to extract GHK"
W14-3310,P10-2041,0,0.0226125,"for tuning the system combination or any of the individual systems. In total, the English→German system uses the following language models: two 4-gram wordbased language models trained on the parallel data and the filtered Common Crawl data separately, two 5-gram POS-based language models trained on the same data as the word-based language models, and a 4-gram cluster-based language model trained on 1,000 MKCLS word classes. The German→English system uses a 4-gram word-based language model trained on all monolingual data and an additional language model trained on automatically selected data (Moore and Lewis, 2010). Again, a 4-gram cluster-based language model trained on 1000 MKCLS word classes is applied. 5 6.1 The automatic scores of all individual systems as well as of our final system combination submission are given in Table 1. KIT, UEDIN and RWTH are each providing one individual phrasebased system output. RWTH (hiero) and UEDIN (GHKM) are providing additional systems based on the hierarchical translation model and a stringto-tree syntax model. The pairwise difference of the single system performances is up to 1.3 points in B LEU and 2.5 points in T ER. For German→English, our system combination p"
W14-3310,W14-3362,1,0.700694,"ned with the German Berkeley Parser (Petrov and Klein, 2007; Petrov and Klein, 2008). UEDIN GHKM T2S (Berkeley): A tree-tostring system trained with source-side syntactic annotation obtained with the English Berkeley Parser (Petrov et al., 2006). UEDIN GHKM S2S (Berkeley): A string-tostring system. The extraction is GHKMbased with syntactic target-side annotation from the German Berkeley Parser, but we strip off the syntactic labels. The final grammar contains rules with a single generic nonterminal instead of syntactic ones, plus rules that have been added from plain phrase-based extraction (Huck et al., 2014). 4 Language models are trained with the SRILM toolkit (Stolcke, 2002) and use modified KneserNey smoothing. Both systems utilize a language model based on automatically learned word classes using the MKCLS algorithm (Och, 1999). The English→German system comprises language models based on fine-grained part-ofspeech tags (Schmid and Laws, 2008). In addition, a bilingual language model (Niehues et al., 2011) is used as well as a discriminative word lexicon (Mauser et al., 2009) using source context to guide the word choices in the target sentence. Karlsruhe Institute of Technology The KIT trans"
W14-3310,W09-0435,0,0.00463497,"erated using the GIZA++ toolkit (Och and Ney, 2003). For English→German, KIT uses discriminative word alignment (Niehues and Vogel, 2008). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified KneserNey smoothing as in (Foster et al., 2006). UEDIN GHKM S2T (Stanford): A string-totree system trained with target-side syntactic annotation obtained with the German Stanford Parser (Rafferty and Manning, 2008a). In both systems KIT applies short-range reorderings (Rottmann and Vogel, 2007) and longrange reorderings (Niehues and Kolss, 2009) based on POS tags (Schmid, 1994) to perform source sentence reordering according to the target language word order. The long-range reordering rules are applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008b; Klein and Manning, 2003) as well as a lexicalized reordering model (Koehn et al., 2005) are applied. UEDIN GHKM S2T (Berkeley): A string-totree system trained with target-side syntactic annotation obtained"
W14-3310,P03-1054,0,0.00425242,"ion obtained with the German Stanford Parser (Rafferty and Manning, 2008a). In both systems KIT applies short-range reorderings (Rottmann and Vogel, 2007) and longrange reorderings (Niehues and Kolss, 2009) based on POS tags (Schmid, 1994) to perform source sentence reordering according to the target language word order. The long-range reordering rules are applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008b; Klein and Manning, 2003) as well as a lexicalized reordering model (Koehn et al., 2005) are applied. UEDIN GHKM S2T (Berkeley): A string-totree system trained with target-side syntactic annotation obtained with the German Berkeley Parser (Petrov and Klein, 2007; Petrov and Klein, 2008). UEDIN GHKM T2S (Berkeley): A tree-tostring system trained with source-side syntactic annotation obtained with the English Berkeley Parser (Petrov et al., 2006). UEDIN GHKM S2S (Berkeley): A string-tostring system. The extraction is GHKMbased with syntactic target-side annotation from the German Berkeley Parser, but we strip off the sy"
W14-3310,W08-0303,0,0.0192279,"sing an SVM classifier as described by Mediani et al. (2011). UEDIN GHKM S2T (ParZu): A string-to-tree system trained with target-side syntactic annotation obtained with ParZu (Sennrich et al., 2013). It uses a modified syntactic label set, target-side compound splitting, and additional syntactic constraints. UEDIN GHKM S2T (BitPar): A string-to-tree system trained with target-side syntactic annotation obtained with BitPar (Schmid, 2004). The word alignment for German→English is generated using the GIZA++ toolkit (Och and Ney, 2003). For English→German, KIT uses discriminative word alignment (Niehues and Vogel, 2008). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified KneserNey smoothing as in (Foster et al., 2006). UEDIN GHKM S2T (Stanford): A string-totree system trained with target-side syntactic annotation obtained with the German Stanford Parser (Rafferty and Manning, 2008a). In both systems KIT applies short-range reorderings (Rottmann and Vogel, 2007) and longrange reorderings (Niehues and Kolss, 2009) based on POS tags (Schmid, 1994) to perform source sentence reordering according to the target language word ord"
W14-3310,D07-1091,1,0.0290057,"2013a), msd-bidirectional-fe lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), a distortion limit of 6, a maximum phrase length of 5, 100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-overpunctuation heuristic. UEDIN uses POS and morphological target sequence models built on the indomain subset of the parallel corpus using KneserNey smoothed 7-gram models as additional factors in phrase translation models (Koehn and Hoang, 2007). UEDIN has furthermore built OSM mod106 model. The monolingual part of those parallel corpora, the News Shuffle corpus for both directions and additionally the Gigaword corpus for German→English are used as monolingual training data for the different language models. Optimization is done with Minimum Error Rate Training as described in (Venugopal et al., 2005), using newstest2012 and newstest2013 as development and test data respectively. on the German source-language side and syntactic annotation from the Berkeley Parser (Petrov et al., 2006) on the English target-language side. For English→"
W14-3310,E03-1076,1,0.31096,"the hierarchical (RWTH hiero) decoder implemented in RWTH’s publicly available translation toolkit Jane (Vilar 1 http://www.eu-bridge.eu 105 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 105–113, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics els over POS and morph sequences following Durrani et al. (2013c). The English→German system additionally comprises a target-side LM over automatically built word classes (Birch et al., 2013). UEDIN has applied syntactic prereordering (Collins et al., 2005) and compound splitting (Koehn and Knight, 2003) of the source side for the German→English system. The systems have been tuned on a very large tuning set consisting of the test sets from 2008-2012, with a total of 13,071 sentences. UEDIN used newstest2013 as held-out test set. On top of UEDIN phrase-based 1 system, UEDIN phrase-based 2 augments word classes as additional factor and learns an interpolated target sequence model over cluster IDs. Furthermore, it learns OSM models over POS, morph and word classes. et al., 2010; Wuebker et al., 2012). The model weights of all systems have been tuned with standard Minimum Error Rate Training (Och"
W14-3310,W11-2124,1,0.0228284,"trip off the syntactic labels. The final grammar contains rules with a single generic nonterminal instead of syntactic ones, plus rules that have been added from plain phrase-based extraction (Huck et al., 2014). 4 Language models are trained with the SRILM toolkit (Stolcke, 2002) and use modified KneserNey smoothing. Both systems utilize a language model based on automatically learned word classes using the MKCLS algorithm (Och, 1999). The English→German system comprises language models based on fine-grained part-ofspeech tags (Schmid and Laws, 2008). In addition, a bilingual language model (Niehues et al., 2011) is used as well as a discriminative word lexicon (Mauser et al., 2009) using source context to guide the word choices in the target sentence. Karlsruhe Institute of Technology The KIT translations (Herrmann et al., 2014) are generated by an in-house phrase-based translations system (Vogel, 2003). The provided News Commentary, Europarl, and Common Crawl parallel corpora are used for training the translation 107 dividual system engines have been optimized on different test sets which partially or fully include newstest2011 or newstest2012. System combination weights are either optimized on news"
W14-3310,2005.iwslt-1.8,1,0.035532,"2008a). In both systems KIT applies short-range reorderings (Rottmann and Vogel, 2007) and longrange reorderings (Niehues and Kolss, 2009) based on POS tags (Schmid, 1994) to perform source sentence reordering according to the target language word order. The long-range reordering rules are applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008b; Klein and Manning, 2003) as well as a lexicalized reordering model (Koehn et al., 2005) are applied. UEDIN GHKM S2T (Berkeley): A string-totree system trained with target-side syntactic annotation obtained with the German Berkeley Parser (Petrov and Klein, 2007; Petrov and Klein, 2008). UEDIN GHKM T2S (Berkeley): A tree-tostring system trained with source-side syntactic annotation obtained with the English Berkeley Parser (Petrov et al., 2006). UEDIN GHKM S2S (Berkeley): A string-tostring system. The extraction is GHKMbased with syntactic target-side annotation from the German Berkeley Parser, but we strip off the syntactic labels. The final grammar contains rules with a single"
W14-3310,J03-1002,1,0.0147027,"of the web-crawled Common Crawl corpus, noisy sentence pairs are filtered out using an SVM classifier as described by Mediani et al. (2011). UEDIN GHKM S2T (ParZu): A string-to-tree system trained with target-side syntactic annotation obtained with ParZu (Sennrich et al., 2013). It uses a modified syntactic label set, target-side compound splitting, and additional syntactic constraints. UEDIN GHKM S2T (BitPar): A string-to-tree system trained with target-side syntactic annotation obtained with BitPar (Schmid, 2004). The word alignment for German→English is generated using the GIZA++ toolkit (Och and Ney, 2003). For English→German, KIT uses discriminative word alignment (Niehues and Vogel, 2008). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified KneserNey smoothing as in (Foster et al., 2006). UEDIN GHKM S2T (Stanford): A string-totree system trained with target-side syntactic annotation obtained with the German Stanford Parser (Rafferty and Manning, 2008a). In both systems KIT applies short-range reorderings (Rottmann and Vogel, 2007) and longrange reorderings (Niehues and Kolss, 2009) based on POS tags (Schmid,"
W14-3310,E99-1010,0,0.0419329,"l., 2006). UEDIN GHKM S2S (Berkeley): A string-tostring system. The extraction is GHKMbased with syntactic target-side annotation from the German Berkeley Parser, but we strip off the syntactic labels. The final grammar contains rules with a single generic nonterminal instead of syntactic ones, plus rules that have been added from plain phrase-based extraction (Huck et al., 2014). 4 Language models are trained with the SRILM toolkit (Stolcke, 2002) and use modified KneserNey smoothing. Both systems utilize a language model based on automatically learned word classes using the MKCLS algorithm (Och, 1999). The English→German system comprises language models based on fine-grained part-ofspeech tags (Schmid and Laws, 2008). In addition, a bilingual language model (Niehues et al., 2011) is used as well as a discriminative word lexicon (Mauser et al., 2009) using source context to guide the word choices in the target sentence. Karlsruhe Institute of Technology The KIT translations (Herrmann et al., 2014) are generated by an in-house phrase-based translations system (Vogel, 2003). The provided News Commentary, Europarl, and Common Crawl parallel corpora are used for training the translation 107 div"
W14-3310,P07-2045,1,0.0154876,"th a baseline phrasebased system, and each contain less than 30 words for more rapid tuning. Decoding for the syntaxbased systems is carried out with cube pruning using Moses’ hierarchical decoder (Hoang et al., 2009). UEDIN’s German→English syntax-based setup is a string-to-tree system with compound splitting University of Edinburgh UEDIN contributed phrase-based and syntaxbased systems to both the German→English and the English→German joint submission. 3.1 Syntax-based Systems Phrase-based Systems UEDIN’s phrase-based systems (Durrani et al., 2014) have been trained using the Moses toolkit (Koehn et al., 2007), replicating the settings described in (Durrani et al., 2013b). The features include: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, a lexically-driven 5-gram operation sequence model (OSM) (Durrani et al., 2013a), msd-bidirectional-fe lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), a distortion limit of 6, a maximum phrase length of 5, 100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004),"
W14-3310,P03-1021,0,0.0102254,"03) of the source side for the German→English system. The systems have been tuned on a very large tuning set consisting of the test sets from 2008-2012, with a total of 13,071 sentences. UEDIN used newstest2013 as held-out test set. On top of UEDIN phrase-based 1 system, UEDIN phrase-based 2 augments word classes as additional factor and learns an interpolated target sequence model over cluster IDs. Furthermore, it learns OSM models over POS, morph and word classes. et al., 2010; Wuebker et al., 2012). The model weights of all systems have been tuned with standard Minimum Error Rate Training (Och, 2003) on a concatenation of the newstest2011 and newstest2012 sets. RWTH used B LEU as optimization objective. Both for language model estimation and querying at decoding, the KenLM toolkit (Heafield et al., 2013) is used. All RWTH systems include the standard set of models provided by Jane. Both systems have been augmented with a hierarchical orientation model (Galley and Manning, 2008; Huck et al., 2013) and a cluster language model (Wuebker et al., 2013). The phrasebased system (RWTH scss) has been further improved by maximum expected B LEU training similar to (He and Deng, 2012). The latter has"
W14-3310,W14-3317,1,0.820032,"ty, the University of Edinburgh, and Karlsruhe Institute of Technology developed several individual systems which serve as system combination input. We devoted special attention to building syntax-based systems and combining them with the phrasebased ones. The joint setups yield empirical gains of up to 1.6 points in B LEU and 1.0 points in T ER on the WMT newstest2013 test set compared to the best single systems. 1 Introduction 2 EU-BRIDGE1 is a European research project which is aimed at developing innovative speech translation technology. This paper describes a RWTH Aachen University RWTH (Peitz et al., 2014) employs both the phrase-based (RWTH scss) and the hierarchical (RWTH hiero) decoder implemented in RWTH’s publicly available translation toolkit Jane (Vilar 1 http://www.eu-bridge.eu 105 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 105–113, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics els over POS and morph sequences following Durrani et al. (2013c). The English→German system additionally comprises a target-side LM over automatically built word classes (Birch et al., 2013). UEDIN has applied syntactic prereordering"
W14-3310,N04-1022,0,0.063305,"it (Koehn et al., 2007), replicating the settings described in (Durrani et al., 2013b). The features include: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, a lexically-driven 5-gram operation sequence model (OSM) (Durrani et al., 2013a), msd-bidirectional-fe lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), a distortion limit of 6, a maximum phrase length of 5, 100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-overpunctuation heuristic. UEDIN uses POS and morphological target sequence models built on the indomain subset of the parallel corpus using KneserNey smoothed 7-gram models as additional factors in phrase translation models (Koehn and Hoang, 2007). UEDIN has furthermore built OSM mod106 model. The monolingual part of those parallel corpora, the News Shuffle corpus for both directions and additionally the Gigaword corpus for German→English are used as monolingual train"
W14-3310,W10-1738,1,0.159493,"Missing"
W14-3310,W08-1005,0,0.0331415,"nce reordering according to the target language word order. The long-range reordering rules are applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008b; Klein and Manning, 2003) as well as a lexicalized reordering model (Koehn et al., 2005) are applied. UEDIN GHKM S2T (Berkeley): A string-totree system trained with target-side syntactic annotation obtained with the German Berkeley Parser (Petrov and Klein, 2007; Petrov and Klein, 2008). UEDIN GHKM T2S (Berkeley): A tree-tostring system trained with source-side syntactic annotation obtained with the English Berkeley Parser (Petrov et al., 2006). UEDIN GHKM S2S (Berkeley): A string-tostring system. The extraction is GHKMbased with syntactic target-side annotation from the German Berkeley Parser, but we strip off the syntactic labels. The final grammar contains rules with a single generic nonterminal instead of syntactic ones, plus rules that have been added from plain phrase-based extraction (Huck et al., 2014). 4 Language models are trained with the SRILM toolkit (Stolcke, 2"
W14-3310,P06-1055,0,0.0182004,"dditional factors in phrase translation models (Koehn and Hoang, 2007). UEDIN has furthermore built OSM mod106 model. The monolingual part of those parallel corpora, the News Shuffle corpus for both directions and additionally the Gigaword corpus for German→English are used as monolingual training data for the different language models. Optimization is done with Minimum Error Rate Training as described in (Venugopal et al., 2005), using newstest2012 and newstest2013 as development and test data respectively. on the German source-language side and syntactic annotation from the Berkeley Parser (Petrov et al., 2006) on the English target-language side. For English→German, UEDIN has trained various string-to-tree GHKM syntax systems which differ with respect to the syntactic annotation. A tree-to-string system and a string-to-string system (with rules that are not syntactically decorated) have been trained as well. The English→German UEDIN GHKM system names in Table 3 denote: Compound splitting (Koehn and Knight, 2003) is performed on the source side of the corpus for German→English translation before training. In order to improve the quality of the web-crawled Common Crawl corpus, noisy sentence pairs ar"
W14-3310,W12-3150,1,0.544959,"uster language model (Wuebker et al., 2013). The phrasebased system (RWTH scss) has been further improved by maximum expected B LEU training similar to (He and Deng, 2012). The latter has been performed on a selection from the News Commentary, Europarl and Common Crawl corpora based on language and translation model cross-entropies (Mansour et al., 2011). 3 3.2 UEDIN’s syntax-based systems (Williams et al., 2014) follow the GHKM syntax approach as proposed by Galley, Hopkins, Knight, and Marcu (Galley et al., 2004). The open source Moses implementation has been employed to extract GHKM rules (Williams and Koehn, 2012). Composed rules (Galley et al., 2006) are extracted in addition to minimal rules, but only up to the following limits: at most twenty tree nodes per rule, a maximum depth of five, and a maximum size of five. Singleton hierarchical rules are dropped. The features for the syntax-based systems comprise Good-Turing-smoothed phrase translation probabilities, lexical translation probabilities in both directions, word and phrase penalty, a rule rareness penalty, a monolingual PCFG probability, and a 5-gram language model. UEDIN has used the SRILM toolkit (Stolcke, 2002) to train the language model a"
W14-3310,W08-1006,0,0.0232292,"e system trained with target-side syntactic annotation obtained with BitPar (Schmid, 2004). The word alignment for German→English is generated using the GIZA++ toolkit (Och and Ney, 2003). For English→German, KIT uses discriminative word alignment (Niehues and Vogel, 2008). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified KneserNey smoothing as in (Foster et al., 2006). UEDIN GHKM S2T (Stanford): A string-totree system trained with target-side syntactic annotation obtained with the German Stanford Parser (Rafferty and Manning, 2008a). In both systems KIT applies short-range reorderings (Rottmann and Vogel, 2007) and longrange reorderings (Niehues and Kolss, 2009) based on POS tags (Schmid, 1994) to perform source sentence reordering according to the target language word order. The long-range reordering rules are applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008b; Klein and Manning, 2003) as well as a lexicalized reordering model (Koeh"
W14-3310,W14-3324,1,0.0949085,"Missing"
W14-3310,2007.tmi-papers.21,0,0.125992,", 2004). The word alignment for German→English is generated using the GIZA++ toolkit (Och and Ney, 2003). For English→German, KIT uses discriminative word alignment (Niehues and Vogel, 2008). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified KneserNey smoothing as in (Foster et al., 2006). UEDIN GHKM S2T (Stanford): A string-totree system trained with target-side syntactic annotation obtained with the German Stanford Parser (Rafferty and Manning, 2008a). In both systems KIT applies short-range reorderings (Rottmann and Vogel, 2007) and longrange reorderings (Niehues and Kolss, 2009) based on POS tags (Schmid, 1994) to perform source sentence reordering according to the target language word order. The long-range reordering rules are applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008b; Klein and Manning, 2003) as well as a lexicalized reordering model (Koehn et al., 2005) are applied. UEDIN GHKM S2T (Berkeley): A string-totree system tra"
W14-3310,C12-3061,1,0.125144,"013). UEDIN has applied syntactic prereordering (Collins et al., 2005) and compound splitting (Koehn and Knight, 2003) of the source side for the German→English system. The systems have been tuned on a very large tuning set consisting of the test sets from 2008-2012, with a total of 13,071 sentences. UEDIN used newstest2013 as held-out test set. On top of UEDIN phrase-based 1 system, UEDIN phrase-based 2 augments word classes as additional factor and learns an interpolated target sequence model over cluster IDs. Furthermore, it learns OSM models over POS, morph and word classes. et al., 2010; Wuebker et al., 2012). The model weights of all systems have been tuned with standard Minimum Error Rate Training (Och, 2003) on a concatenation of the newstest2011 and newstest2012 sets. RWTH used B LEU as optimization objective. Both for language model estimation and querying at decoding, the KenLM toolkit (Heafield et al., 2013) is used. All RWTH systems include the standard set of models provided by Jane. Both systems have been augmented with a hierarchical orientation model (Galley and Manning, 2008; Huck et al., 2013) and a cluster language model (Wuebker et al., 2013). The phrasebased system (RWTH scss) has"
W14-3310,C08-1098,0,0.00933067,"target-side annotation from the German Berkeley Parser, but we strip off the syntactic labels. The final grammar contains rules with a single generic nonterminal instead of syntactic ones, plus rules that have been added from plain phrase-based extraction (Huck et al., 2014). 4 Language models are trained with the SRILM toolkit (Stolcke, 2002) and use modified KneserNey smoothing. Both systems utilize a language model based on automatically learned word classes using the MKCLS algorithm (Och, 1999). The English→German system comprises language models based on fine-grained part-ofspeech tags (Schmid and Laws, 2008). In addition, a bilingual language model (Niehues et al., 2011) is used as well as a discriminative word lexicon (Mauser et al., 2009) using source context to guide the word choices in the target sentence. Karlsruhe Institute of Technology The KIT translations (Herrmann et al., 2014) are generated by an in-house phrase-based translations system (Vogel, 2003). The provided News Commentary, Europarl, and Common Crawl parallel corpora are used for training the translation 107 dividual system engines have been optimized on different test sets which partially or fully include newstest2011 or newst"
W14-3310,D13-1138,1,0.0721313,", morph and word classes. et al., 2010; Wuebker et al., 2012). The model weights of all systems have been tuned with standard Minimum Error Rate Training (Och, 2003) on a concatenation of the newstest2011 and newstest2012 sets. RWTH used B LEU as optimization objective. Both for language model estimation and querying at decoding, the KenLM toolkit (Heafield et al., 2013) is used. All RWTH systems include the standard set of models provided by Jane. Both systems have been augmented with a hierarchical orientation model (Galley and Manning, 2008; Huck et al., 2013) and a cluster language model (Wuebker et al., 2013). The phrasebased system (RWTH scss) has been further improved by maximum expected B LEU training similar to (He and Deng, 2012). The latter has been performed on a selection from the News Commentary, Europarl and Common Crawl corpora based on language and translation model cross-entropies (Mansour et al., 2011). 3 3.2 UEDIN’s syntax-based systems (Williams et al., 2014) follow the GHKM syntax approach as proposed by Galley, Hopkins, Knight, and Marcu (Galley et al., 2004). The open source Moses implementation has been employed to extract GHKM rules (Williams and Koehn, 2012). Composed rules ("
W14-3310,C04-1024,0,0.0194264,"f the corpus for German→English translation before training. In order to improve the quality of the web-crawled Common Crawl corpus, noisy sentence pairs are filtered out using an SVM classifier as described by Mediani et al. (2011). UEDIN GHKM S2T (ParZu): A string-to-tree system trained with target-side syntactic annotation obtained with ParZu (Sennrich et al., 2013). It uses a modified syntactic label set, target-side compound splitting, and additional syntactic constraints. UEDIN GHKM S2T (BitPar): A string-to-tree system trained with target-side syntactic annotation obtained with BitPar (Schmid, 2004). The word alignment for German→English is generated using the GIZA++ toolkit (Och and Ney, 2003). For English→German, KIT uses discriminative word alignment (Niehues and Vogel, 2008). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified KneserNey smoothing as in (Foster et al., 2006). UEDIN GHKM S2T (Stanford): A string-totree system trained with target-side syntactic annotation obtained with the German Stanford Parser (Rafferty and Manning, 2008a). In both systems KIT applies short-range reorderings (Rottman"
W14-3310,R13-1079,1,0.28133,"stem and a string-to-string system (with rules that are not syntactically decorated) have been trained as well. The English→German UEDIN GHKM system names in Table 3 denote: Compound splitting (Koehn and Knight, 2003) is performed on the source side of the corpus for German→English translation before training. In order to improve the quality of the web-crawled Common Crawl corpus, noisy sentence pairs are filtered out using an SVM classifier as described by Mediani et al. (2011). UEDIN GHKM S2T (ParZu): A string-to-tree system trained with target-side syntactic annotation obtained with ParZu (Sennrich et al., 2013). It uses a modified syntactic label set, target-side compound splitting, and additional syntactic constraints. UEDIN GHKM S2T (BitPar): A string-to-tree system trained with target-side syntactic annotation obtained with BitPar (Schmid, 2004). The word alignment for German→English is generated using the GIZA++ toolkit (Och and Ney, 2003). For English→German, KIT uses discriminative word alignment (Niehues and Vogel, 2008). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified KneserNey smoothing as in (Foster e"
W14-3310,N07-1051,0,\N,Missing
W14-3310,W05-0836,1,\N,Missing
W14-3310,W05-0909,0,\N,Missing
W14-3310,N13-1001,1,\N,Missing
W14-3310,2010.iwslt-evaluation.11,1,\N,Missing
W14-3310,2013.iwslt-evaluation.16,1,\N,Missing
W14-3310,W13-2213,1,\N,Missing
W14-3310,W14-3313,1,\N,Missing
W14-3310,W11-2145,1,\N,Missing
W14-3310,2013.iwslt-evaluation.3,1,\N,Missing
W14-3324,D10-1063,0,0.0712692,"rench and Czech1 . We also experimented with random subsets of size 2,000. For the filtering technique, we make the assumption that finding suitable weights for all the feature functions requires the optimizer to see a range of feature values and to see hypotheses that can partially match the reference translations in order to rank the hypotheses. For example, if a Table 1: Parameter settings for rule composition. Further to the restrictions on rule composition, fully non-lexical unary rules were eliminated using the method described in Chung et al. (2011) and rules with scope greater than 3 (Hopkins and Langmead, 2010) were pruned from the translation grammar. Scope pruning makes parsing tractable without the need for grammar binarization. 2.3 Language Model We used all available monolingual data to train 5-gram language models. Language models for each monolingual corpus were trained using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998) and then interpolated using weights tuned to minimize perplexity on the development set. 2.4 Tuning Feature Functions Our feature functions are unchanged from the previous two years. They include the n-gram lan1 For Russian and"
W14-3324,N12-1047,0,0.0432105,"ons: • p (C, β |α, ∼) and p (α |C, β, ∼), the direct and indirect translation probabilities. • plex (β |α) and plex (α |β), the direct and indirect lexical weights (Koehn et al., 2003). • ppcfg (π), the monolingual PCFG probability of the tree fragment π from which the rule was extracted. • exp(−1/count(r)), a rule rareness penalty. • exp(1), a rule penalty. The main grammar and glue grammars have distinct penalty features. Value 5 20 5 2.5 The feature weights were tuned using the Moses implementation of MERT (Och, 2003) for all systems except English-to-German, for which we used k-best MIRA (Cherry and Foster, 2012) due to the larger number of features. We used tuning sentences drawn from all of the previous years’ test sets (except newstest2013, which was used as the development test set). In order to speed up the tuning process, we used subsets of the full tuning sets with sentence pairs up to length 30 (Max-30) and further applied a filtering technique to reduce the tuning set size to 2,000 sentence pairs for the language pairs involving German, French and Czech1 . We also experimented with random subsets of size 2,000. For the filtering technique, we make the assumption that finding suitable weights"
W14-3324,N03-1017,1,0.0126398,"ngs shown in Table 1, which were chosen empirically during the development of last years’ systems: Parameter Rule depth Node count Rule size C → hα, β, ∼i where C is a target-side non-terminal label, α is a string of source terminals and non-terminals, β is a string of target terminals and non-terminals, and ∼ is a one-to-one correspondence between source and target non-terminals, we score the rule according to the following functions: • p (C, β |α, ∼) and p (α |C, β, ∼), the direct and indirect translation probabilities. • plex (β |α) and plex (α |β), the direct and indirect lexical weights (Koehn et al., 2003). • ppcfg (π), the monolingual PCFG probability of the tree fragment π from which the rule was extracted. • exp(−1/count(r)), a rule rareness penalty. • exp(1), a rule penalty. The main grammar and glue grammars have distinct penalty features. Value 5 20 5 2.5 The feature weights were tuned using the Moses implementation of MERT (Och, 2003) for all systems except English-to-German, for which we used k-best MIRA (Cherry and Foster, 2012) due to the larger number of features. We used tuning sentences drawn from all of the previous years’ test sets (except newstest2013, which was used as the deve"
W14-3324,P11-2072,0,0.0390442,"sentence pairs for the language pairs involving German, French and Czech1 . We also experimented with random subsets of size 2,000. For the filtering technique, we make the assumption that finding suitable weights for all the feature functions requires the optimizer to see a range of feature values and to see hypotheses that can partially match the reference translations in order to rank the hypotheses. For example, if a Table 1: Parameter settings for rule composition. Further to the restrictions on rule composition, fully non-lexical unary rules were eliminated using the method described in Chung et al. (2011) and rules with scope greater than 3 (Hopkins and Langmead, 2010) were pruned from the translation grammar. Scope pruning makes parsing tractable without the need for grammar binarization. 2.3 Language Model We used all available monolingual data to train 5-gram language models. Language models for each monolingual corpus were trained using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998) and then interpolated using weights tuned to minimize perplexity on the development set. 2.4 Tuning Feature Functions Our feature functions are unchanged from the"
W14-3324,P07-2045,1,0.0160827,"of the parser. For the English-German system we used the default Moses tokenization scheme, which is similar to that of the German parsers. For the systems that translate into English, we used the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007) to parse the target-side of the training corpus. As we will describe in Section 3, we tried a variety of parsers for German. We did not perform any corpus filtering other than the standard Moses method, which removes As last year (Nadejde et al., 2013), our systems are based on the string-to-tree pipeline implemented in the Moses toolkit (Koehn et al., 2007). We paid particular attention to the production of grammatical German, trying various parsers and incorporating target-side compound splitting and morphosyntactic constraints; for Hindi and Russian, we employed the new Moses transliteration model to handle out-of-vocabulary words; and for German to English, we experimented with tree binarization, obtaining good results from right binarization. We also present our first syntax-based results for French-English, the scale of which defeated us 207 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 207–214, c Baltimore, Ma"
W14-3324,E14-4029,1,0.429224,"o English results on the devtest (newsdev2013) and test (newstest2014) sets. • passive clauses are not allowed to have accusative objects. 210 system baseline devtest 86,341,766 test 88,657,327 (OOV) input words are therefore a comparatively large source of translation error: in the devtest set (newsdev2014) and filtered test set (newstest2014) the average OOV rates are 1.08 and 1.16 unknown words per sentence, respectively. Assuming a significant fraction of OOV words to be named entities and thus amenable to transliteration, we applied the post-processing transliteration method described in Durrani et al. (2014) and implemented in Moses. In brief, this is an unsupervised method that i) uses EM to induce a corpus of transliteration examples from the parallel training data; ii) learns a monotone character-level phrasebased SMT model from the transliteration corpus; and iii) substitutes transliterations for OOVs in the system output by using the monolingual language model and other features to select between transliteration candidates.5 Table 10 shows B LEU scores with and without transliteration on the devtest and filtered test sets. Due to a bug in the submitted system, the language model trained on t"
W14-3324,W13-2221,1,0.776308,", we used the Moses tokenizer’s -penn option, which uses a tokenization scheme that more closely matches that of the parser. For the English-German system we used the default Moses tokenization scheme, which is similar to that of the German parsers. For the systems that translate into English, we used the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007) to parse the target-side of the training corpus. As we will describe in Section 3, we tried a variety of parsers for German. We did not perform any corpus filtering other than the standard Moses method, which removes As last year (Nadejde et al., 2013), our systems are based on the string-to-tree pipeline implemented in the Moses toolkit (Koehn et al., 2007). We paid particular attention to the production of grammatical German, trying various parsers and incorporating target-side compound splitting and morphosyntactic constraints; for Hindi and Russian, we employed the new Moses transliteration model to handle out-of-vocabulary words; and for German to English, we experimented with tree binarization, obtaining good results from right binarization. We also present our first syntax-based results for French-English, the scale of which defeated"
W14-3324,N13-1073,0,0.0302293,"lative clauses must contain a relative (or interrogative) pronoun in their first constituent. Table 4 shows B LEU scores with systems trained with different parsers, and for our extensions of the baseline system. 4 Czech to English For Czech to English we used the core setup described in Section 2 without modification. Table 5 shows the B LEU scores. system baseline B LEU devtest test 24.8 27.0 Table 5: Czech to English results on the devtest (newstest2013) and test (newstest2014) sets. 5 French to English For French to English, alignment of the parallel corpus was performed using fast_align (Dyer et al., 2013) instead of MGIZA++ due to the large volume of parallel data. Table 6 shows B LEU scores for the system and Table 7 shows the resulting grammar sizes after filtering for the evaluation sets. • correct subcategorization of auxiliary/modal verbs in regards to the inflection of the full verb. system baseline B LEU devtest test 29.4 32.3 Table 6: French to English results on the devtest (newsdev2013) and test (newstest2014) sets. • passive clauses are not allowed to have accusative objects. 210 system baseline devtest 86,341,766 test 88,657,327 (OOV) input words are therefore a comparatively large"
W14-3324,W14-3310,1,0.0748258,"of Informatics, University of Edinburgh 2 Center for Speech and Language Processing, The Johns Hopkins University Abstract last year. This year we were able to train a system using all available training data, a task that was made considerably easier through principled filtering of the tuning set. Although our system was not ready in time for human evaluation, we present B LEU scores in this paper. In addition to the five single-system submissions described here, we also contributed our English-German and German-English systems for use in the collaborative EU-BRIDGE system combination effort (Freitag et al., 2014). This paper is organised as follows. In Section 2 we describe the core setup that is common to all systems. In subsequent sections we describe language-pair specific variations and extensions. For each language pair, we present results for both the development test set (newstest2013 in most cases) and for the filtered test set (newstest2014) that was provided after the system submission deadline. We refer to these as ‘devtest’ and ‘test’, respectively. This paper describes the string-to-tree systems built at the University of Edinburgh for the WMT 2014 shared translation task. We developed sy"
W14-3324,J03-1002,0,0.00637422,"he form sentence pairs with dubious length ratios and sentence pairs where parsing fails for the target-side sentence. 2.2 Translation Model Our translation grammar is a synchronous contextfree grammar (SCFG) with phrase-structure labels on the target side and the generic non-terminal label X on the source side. The grammar was extracted from the wordaligned parallel data using the Moses implementation (Williams and Koehn, 2012) of the GHKM algorithm (Galley et al., 2004; Galley et al., 2006). For word alignment we used MGIZA++ (Gao and Vogel, 2008), a multi-threaded implementation of GIZA++ (Och and Ney, 2003). Minimal GHKM rules were composed into larger rules subject to parameterized restrictions on size defined in terms of the resulting target tree fragment. A good choice of parameter settings depends on the annotation style of the target-side parse trees. We used the settings shown in Table 1, which were chosen empirically during the development of last years’ systems: Parameter Rule depth Node count Rule size C → hα, β, ∼i where C is a target-side non-terminal label, α is a string of source terminals and non-terminals, β is a string of target terminals and non-terminals, and ∼ is a one-to-one"
W14-3324,P03-1021,0,0.00672489,"source and target non-terminals, we score the rule according to the following functions: • p (C, β |α, ∼) and p (α |C, β, ∼), the direct and indirect translation probabilities. • plex (β |α) and plex (α |β), the direct and indirect lexical weights (Koehn et al., 2003). • ppcfg (π), the monolingual PCFG probability of the tree fragment π from which the rule was extracted. • exp(−1/count(r)), a rule rareness penalty. • exp(1), a rule penalty. The main grammar and glue grammars have distinct penalty features. Value 5 20 5 2.5 The feature weights were tuned using the Moses implementation of MERT (Och, 2003) for all systems except English-to-German, for which we used k-best MIRA (Cherry and Foster, 2012) due to the larger number of features. We used tuning sentences drawn from all of the previous years’ test sets (except newstest2013, which was used as the development test set). In order to speed up the tuning process, we used subsets of the full tuning sets with sentence pairs up to length 30 (Max-30) and further applied a filtering technique to reduce the tuning set size to 2,000 sentence pairs for the language pairs involving German, French and Czech1 . We also experimented with random subsets"
W14-3324,W10-1734,0,0.0101538,"cht Figure 1: Syntactic representation of split compound Bundesberufungsgericht (Engl: federal appeals court). B LEU devtest test 19.0 18.3 19.3 18.6 19.5 18.6 19.6 19.1 19.8 19.1 19.9 19.2 20.0 19.8 20.2 20.1 Table 4: English to German translation results on devtest (newstest2013) and test (newstest2014) sets. We discriminatively learn non-terminal labels for unknown words using sparse features, rather than estimating a probability distribution of nonterminal labels from singleton statistics in the training corpus. We perform target-side compound splitting, using a hybrid method described by Fritzinger and Fraser (2010) that combines a finite-state morphology and corpus statistics. As finite-state morphology analyzer, we use Zmorge (Sennrich and Kunz, 2014). An original contribution of our experiments is a syntactic representation of split compounds which eliminates typical problems with target-side compound splitting, namely erroneous reorderings and compound merging. We represent split compounds as a syntactic tree with the last segment as head, preceded by a modifier. A modifier consists of an optional modifier, a segment and a (possibly empty) joining element. An example is shown in Figure 1. This hierar"
W14-3324,N04-1035,0,0.0656309,"s word count, and various scores for the synchronous derivation. Each grammar rule has a number of precomputed scores. For a grammar rule r of the form sentence pairs with dubious length ratios and sentence pairs where parsing fails for the target-side sentence. 2.2 Translation Model Our translation grammar is a synchronous contextfree grammar (SCFG) with phrase-structure labels on the target side and the generic non-terminal label X on the source side. The grammar was extracted from the wordaligned parallel data using the Moses implementation (Williams and Koehn, 2012) of the GHKM algorithm (Galley et al., 2004; Galley et al., 2006). For word alignment we used MGIZA++ (Gao and Vogel, 2008), a multi-threaded implementation of GIZA++ (Och and Ney, 2003). Minimal GHKM rules were composed into larger rules subject to parameterized restrictions on size defined in terms of the resulting target tree fragment. A good choice of parameter settings depends on the annotation style of the target-side parse trees. We used the settings shown in Table 1, which were chosen empirically during the development of last years’ systems: Parameter Rule depth Node count Rule size C → hα, β, ∼i where C is a target-side non-t"
W14-3324,W08-1005,0,0.0619769,"e 19.2 19.2 19.1 19.4 De-En 26.9 27.0 27.2 27.0 Table 3: B LEU results on devtest and test sets with different tuning sets: Full, Max-30, filtered subsets of Max-30 and average of three random subsets of Max-30 (size of filtered/random subsets: 2,000). 3 English to German We use the projective output of the dependency parser ParZu (Sennrich et al., 2013) for the syntactic annotation of our primary submission. Contrastive systems were built with other parsers: BitPar (Schmid, 2004), the German Stanford Parser (Rafferty and Manning, 2008), and the German Berkeley Parser (Petrov and Klein, 2007; Petrov and Klein, 2008). The set of syntactic labels provided by ParZu has been refined to reduce overgeneralization phenomena. Specifically, we disambiguate the labels ROOT (used for the root of a sentence, but also commas, punctuation marks, and sentence fragments), KON and CJ (coordinations of different constituents), and GMOD (pre- or postmodifying genitive modifier). 2 These can be arbitrary tokens that do not match any reference token. 3 For random subsets from the full tuning set the performance was similar but resulted in standard deviations of up to 0.36 across three random sets. 4 Note however that due to"
W14-3324,P06-1121,0,0.026303,"ious scores for the synchronous derivation. Each grammar rule has a number of precomputed scores. For a grammar rule r of the form sentence pairs with dubious length ratios and sentence pairs where parsing fails for the target-side sentence. 2.2 Translation Model Our translation grammar is a synchronous contextfree grammar (SCFG) with phrase-structure labels on the target side and the generic non-terminal label X on the source side. The grammar was extracted from the wordaligned parallel data using the Moses implementation (Williams and Koehn, 2012) of the GHKM algorithm (Galley et al., 2004; Galley et al., 2006). For word alignment we used MGIZA++ (Gao and Vogel, 2008), a multi-threaded implementation of GIZA++ (Och and Ney, 2003). Minimal GHKM rules were composed into larger rules subject to parameterized restrictions on size defined in terms of the resulting target tree fragment. A good choice of parameter settings depends on the annotation style of the target-side parse trees. We used the settings shown in Table 1, which were chosen empirically during the development of last years’ systems: Parameter Rule depth Node count Rule size C → hα, β, ∼i where C is a target-side non-terminal label, α is a"
W14-3324,P06-1055,0,0.08415,"German • Czech-English • French-English 2 2.1 • German-English • Hindi-English • Russian-English System Overview Pre-processing The training data was normalized using the WMT normalize-punctuation.perl script then tokenized and truecased. Where the target language was English, we used the Moses tokenizer’s -penn option, which uses a tokenization scheme that more closely matches that of the parser. For the English-German system we used the default Moses tokenization scheme, which is similar to that of the German parsers. For the systems that translate into English, we used the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007) to parse the target-side of the training corpus. As we will describe in Section 3, we tried a variety of parsers for German. We did not perform any corpus filtering other than the standard Moses method, which removes As last year (Nadejde et al., 2013), our systems are based on the string-to-tree pipeline implemented in the Moses toolkit (Koehn et al., 2007). We paid particular attention to the production of grammatical German, trying various parsers and incorporating target-side compound splitting and morphosyntactic constraints; for Hindi and Russian, we employed th"
W14-3324,W08-0509,0,0.0379218,"ule has a number of precomputed scores. For a grammar rule r of the form sentence pairs with dubious length ratios and sentence pairs where parsing fails for the target-side sentence. 2.2 Translation Model Our translation grammar is a synchronous contextfree grammar (SCFG) with phrase-structure labels on the target side and the generic non-terminal label X on the source side. The grammar was extracted from the wordaligned parallel data using the Moses implementation (Williams and Koehn, 2012) of the GHKM algorithm (Galley et al., 2004; Galley et al., 2006). For word alignment we used MGIZA++ (Gao and Vogel, 2008), a multi-threaded implementation of GIZA++ (Och and Ney, 2003). Minimal GHKM rules were composed into larger rules subject to parameterized restrictions on size defined in terms of the resulting target tree fragment. A good choice of parameter settings depends on the annotation style of the target-side parse trees. We used the settings shown in Table 1, which were chosen empirically during the development of last years’ systems: Parameter Rule depth Node count Rule size C → hα, β, ∼i where C is a target-side non-terminal label, α is a string of source terminals and non-terminals, β is a strin"
W14-3324,C04-1024,0,0.606547,"t En-De De-En 19.9 26.7 19.8 26.2 19.8 26.2 19.7 26.4 Tuning set Full Max-30 Filtered Random Cs-En 27.5 27.2 27.5 27.3 test En-De 19.2 19.2 19.1 19.4 De-En 26.9 27.0 27.2 27.0 Table 3: B LEU results on devtest and test sets with different tuning sets: Full, Max-30, filtered subsets of Max-30 and average of three random subsets of Max-30 (size of filtered/random subsets: 2,000). 3 English to German We use the projective output of the dependency parser ParZu (Sennrich et al., 2013) for the syntactic annotation of our primary submission. Contrastive systems were built with other parsers: BitPar (Schmid, 2004), the German Stanford Parser (Rafferty and Manning, 2008), and the German Berkeley Parser (Petrov and Klein, 2007; Petrov and Klein, 2008). The set of syntactic labels provided by ParZu has been refined to reduce overgeneralization phenomena. Specifically, we disambiguate the labels ROOT (used for the root of a sentence, but also commas, punctuation marks, and sentence fragments), KON and CJ (coordinations of different constituents), and GMOD (pre- or postmodifying genitive modifier). 2 These can be arbitrary tokens that do not match any reference token. 3 For random subsets from the full tuni"
W14-3324,sennrich-kunz-2014-zmorge,1,0.826405,"18.6 19.5 18.6 19.6 19.1 19.8 19.1 19.9 19.2 20.0 19.8 20.2 20.1 Table 4: English to German translation results on devtest (newstest2013) and test (newstest2014) sets. We discriminatively learn non-terminal labels for unknown words using sparse features, rather than estimating a probability distribution of nonterminal labels from singleton statistics in the training corpus. We perform target-side compound splitting, using a hybrid method described by Fritzinger and Fraser (2010) that combines a finite-state morphology and corpus statistics. As finite-state morphology analyzer, we use Zmorge (Sennrich and Kunz, 2014). An original contribution of our experiments is a syntactic representation of split compounds which eliminates typical problems with target-side compound splitting, namely erroneous reorderings and compound merging. We represent split compounds as a syntactic tree with the last segment as head, preceded by a modifier. A modifier consists of an optional modifier, a segment and a (possibly empty) joining element. An example is shown in Figure 1. This hierarchical representation ensures that compounds can be easily merged in post-processing (by removing the spaces and special characters around j"
W14-3324,R13-1079,1,0.108908,"ble 2: Size of full tuning sets and with sentence length up to 30. Tuning set Full Max-30 Filtered Random Cs-En 25.1 24.7 24.9 24.8 devtest En-De De-En 19.9 26.7 19.8 26.2 19.8 26.2 19.7 26.4 Tuning set Full Max-30 Filtered Random Cs-En 27.5 27.2 27.5 27.3 test En-De 19.2 19.2 19.1 19.4 De-En 26.9 27.0 27.2 27.0 Table 3: B LEU results on devtest and test sets with different tuning sets: Full, Max-30, filtered subsets of Max-30 and average of three random subsets of Max-30 (size of filtered/random subsets: 2,000). 3 English to German We use the projective output of the dependency parser ParZu (Sennrich et al., 2013) for the syntactic annotation of our primary submission. Contrastive systems were built with other parsers: BitPar (Schmid, 2004), the German Stanford Parser (Rafferty and Manning, 2008), and the German Berkeley Parser (Petrov and Klein, 2007; Petrov and Klein, 2008). The set of syntactic labels provided by ParZu has been refined to reduce overgeneralization phenomena. Specifically, we disambiguate the labels ROOT (used for the root of a sentence, but also commas, punctuation marks, and sentence fragments), KON and CJ (coordinations of different constituents), and GMOD (pre- or postmodifying g"
W14-3324,D07-1078,0,0.252787,"ystem after filtering for the devtest (newstest2013) and test (newstest2014) sets. 6 German to English German compounds were split using the script provided with Moses. For training the primary system, the target parse trees were restructured before rule extraction by right binarization. Since binarization strategies increase the tree depth and number of nodes by adding virtual non-terminals, we increased the extraction parameters to: Rule Depth = 7, Node Count = 100, Rule Size = 7. A thorough investigation of binarization methods for restructuring Penn Treebank style trees was carried out by Wang et al. (2007). Table 8 shows B LEU scores for the baseline system and two systems employing different binarization strategies. Table 9 shows the resulting grammar sizes after filtering for the evaluation sets. Results on the development set showed no improvement when left binarization was used for restructuring the trees, although the grammar size increased significantly. system baseline + right binarization (primary) + left binarization system baseline + transliteration (submission) + transliteration (fixed) B LEU devtest test 26.2 27.2 26.8 28.2 26.3 - Table 10: Hindi to English results with and without"
W14-3324,W13-2230,0,0.0742053,"ender Voice Definiteness Aspect Case 7 3 3 3 3 7 3 7 3 3 Data sparsity issues for this language pair are exacerbated by the rich inflectional morphology of Russian. Many Russian word forms express grammatical distinctions that are either absent from English translations (like grammatical gender) or are expressed by different means (like grammatical function being expressed through syntactic configuration rather than case). We adopt the widelyused approach of simplifying morphologicallycomplex source forms to remove distinctions that we believe to be redundant. Our method is similar to that of Weller et al. (2013) except that ours is much more conservative (in their experiments, Weller et al. (2013) found morphological reduction to harm translation indicating that useful information was likely to have been discarded). Table 11: Feature values that are retained (3) or deleted (7) during morphological reduction of Russian. We used TreeTagger (Schmid, 1994) to obtain a lemma-tag pair for each Russian word. The tag specifies the word class and various morphosyntactic feature values. For example, the adjective республиканская (‘republican’) gets the lemmatag pair республиканский + Afpfsnf, where the code A"
W14-3324,W11-2126,1,0.895685,"present split compounds as a syntactic tree with the last segment as head, preceded by a modifier. A modifier consists of an optional modifier, a segment and a (possibly empty) joining element. An example is shown in Figure 1. This hierarchical representation ensures that compounds can be easily merged in post-processing (by removing the spaces and special characters around joining elements), and that no segments are placed outside of a compound in the translation. We use unification-based constraints to model morphological agreement within German noun phrases, and between subjects and verbs (Williams and Koehn, 2011). Additionally, we add constraints that operate on the internal tree structure of the translation hypotheses, to enforce several syntactic constraints that were frequently violated in the baseline system: • relative clauses must contain a relative (or interrogative) pronoun in their first constituent. Table 4 shows B LEU scores with systems trained with different parsers, and for our extensions of the baseline system. 4 Czech to English For Czech to English we used the core setup described in Section 2 without modification. Table 5 shows the B LEU scores. system baseline B LEU devtest test 24."
W14-3324,W12-3150,1,0.794398,"probability of the derivation’s target yield, its word count, and various scores for the synchronous derivation. Each grammar rule has a number of precomputed scores. For a grammar rule r of the form sentence pairs with dubious length ratios and sentence pairs where parsing fails for the target-side sentence. 2.2 Translation Model Our translation grammar is a synchronous contextfree grammar (SCFG) with phrase-structure labels on the target side and the generic non-terminal label X on the source side. The grammar was extracted from the wordaligned parallel data using the Moses implementation (Williams and Koehn, 2012) of the GHKM algorithm (Galley et al., 2004; Galley et al., 2006). For word alignment we used MGIZA++ (Gao and Vogel, 2008), a multi-threaded implementation of GIZA++ (Och and Ney, 2003). Minimal GHKM rules were composed into larger rules subject to parameterized restrictions on size defined in terms of the resulting target tree fragment. A good choice of parameter settings depends on the annotation style of the target-side parse trees. We used the settings shown in Table 1, which were chosen empirically during the development of last years’ systems: Parameter Rule depth Node count Rule size C"
W14-3324,N07-1051,0,\N,Missing
W14-3324,Q15-1013,1,\N,Missing
W14-3324,W08-1006,0,\N,Missing
W14-3324,D09-1108,0,\N,Missing
W14-3324,E99-1010,0,\N,Missing
W14-3324,N13-3005,0,\N,Missing
W14-3324,N09-2019,0,\N,Missing
W14-3324,P13-2121,1,\N,Missing
W14-3324,P03-2041,0,\N,Missing
W14-3324,W05-0904,0,\N,Missing
W14-3324,herrmann-etal-2014-manual,0,\N,Missing
W14-3324,D14-1082,0,\N,Missing
W14-3324,W15-1004,0,\N,Missing
W14-3324,D15-1248,1,\N,Missing
W14-3324,P14-2024,0,\N,Missing
W14-3324,P14-1129,0,\N,Missing
W14-3324,J07-2003,0,\N,Missing
W14-3324,vilar-etal-2006-error,0,\N,Missing
W14-3324,W14-4018,1,\N,Missing
W14-3324,2014.eamt-1.38,0,\N,Missing
W14-3324,D13-1140,0,\N,Missing
W14-3358,P13-1126,0,0.0656518,"re learned over training sentences, they have to compare the current test sentence to the latent vector of every training instance associated with a translation unit. The highest similarity value is then used as a feature value. Instead, our model learns latent distributional representations of phrase pairs that can be directly compared to test contexts and are likely to be more robust. Because context words of a phrase pair are tied together in the distributional representations, we can use sparse priors to cluster context words associated with the same phrase pair into few topics. Recently, Chen et al. (2013) have proposed a vector space model for domain adaptation where phrase pairs are assigned vectors that are defined in terms of the training corpora. A similar vector is built for an in-domain development set and the similarity to the development set is used as a feature during translation. While their vector representations are similar to our latent topic represen3 Phrase pair topic model (PPT) Our proposed model aims to capture the relationship between phrase pairs and source words that frequently occur in the local context of a phrase pair, that is, context words occurring in the same senten"
W14-3358,D10-1113,0,0.029345,"cument. The score of the next word in a sequence is computed as the sum of the scores of both networks, but they do not consider alternative ways of combining contextual information. Most work in the WSD literature has modelled disambiguation using a limited window of context around the word to disambiguate. Cai et al. (2007), Boyd-graber and Blei (2007) and Li et al. (2010) further tried to integrate the notion of latent topics to address the sparsity problem of the lexicalised features typically used in WSD classifiers. The most closely related work in the area of sense disambiguation is by Dinu and Lapata (2010) who propose a disambiguation method for solving lexical similarity and substitution tasks. They measure word similarity in context by learning distributions over senses for each target word in the form of lower-dimensional distributional representations. Before computing word similarities, they contextualise the global sense distribution of a word using the sense distribution of words in the test context, thereby shifting the sense distribution towards the test context. We adopt a similar distributional representation, but argue that our representation does not need this disambiguation step b"
W14-3358,P12-2023,0,0.0789407,"ion model dynamically to a given test context by measuring their similarity. We show that combining information from both local and global test contexts helps to improve lexical selection and outperforms a baseline system by up to 1.15 B LEU. We test our topic-adapted model on a diverse data set containing documents from three different domains and achieve competitive performance in comparison with two supervised domain-adapted systems. 1 More recent work in SMT uses latent representations of the document context to dynamically adapt the translation model with either monolingual topic models (Eidelman et al., 2012; Hewavitharana et al., 2013) or bilingual topic models (Hasler et al., 2014), thereby allowing the translation system to disambiguate source phrases using document context. Eidelman et al. (2012) also apply a topic model to each test sentence and find that sentence context is sufficient for picking good translations, but they do not attempt to combine sentence and document level information. Sentence-level topic adaptation for SMT has also been employed by Hasler et al. (2012). Other approaches to topic adaptation for SMT include Zhao and Xing (2007) and Tam et al. (2008), both of which use a"
W14-3358,W11-1014,0,0.216649,"Missing"
W14-3358,H92-1045,0,0.547336,"Missing"
W14-3358,2011.iwslt-evaluation.18,0,0.122144,"Missing"
W14-3358,2012.iwslt-papers.17,1,0.848445,"esentations of the document context to dynamically adapt the translation model with either monolingual topic models (Eidelman et al., 2012; Hewavitharana et al., 2013) or bilingual topic models (Hasler et al., 2014), thereby allowing the translation system to disambiguate source phrases using document context. Eidelman et al. (2012) also apply a topic model to each test sentence and find that sentence context is sufficient for picking good translations, but they do not attempt to combine sentence and document level information. Sentence-level topic adaptation for SMT has also been employed by Hasler et al. (2012). Other approaches to topic adaptation for SMT include Zhao and Xing (2007) and Tam et al. (2008), both of which use adapted lexical weights. Introduction The task of lexical selection plays an important role in statistical machine translation (SMT). It strongly depends on context and is particularly difficult when the domain of a test document is unknown, for example when translating web documents from diverse sources. Selecting translations of words or phrases that preserve the sense of the source words is closely related to the field of word sense disambiguation (WSD), which has been studie"
W14-3358,W13-2201,1,0.825427,"domain. While a C C document contains 29.1 sentences on average, documents from N C and T ED are on average more than twice as long. The length of a document could have an influence on how reliable global topic information is but also on how important it is to have information from both local and global test contexts. Data Train Dev Test Data and experimental setup Our experiments were carried out on a mixed French-English data set containing the T ED corpus (Cettolo et al., 2012), parts of the News Commentary corpus (N C) and parts of the Commoncrawl corpus (C C) from the WMT13 shared task (Bojar et al., 2013) as described in Table 1. To ensure that the baseline model does not have an implicit preference for any particular domain, we selected subsets of the N C and C C corpora such that the training data contains 2.7M English words per domain. We were guided by two constraints in chosing our data set in order to simulate an environment where very diverse documents have to be translated, which is a typical scenario for web translation engines: 1) the data has document boundaries and the content of each document is assumed to be topically related, 2) there is some degree of topical variation within e"
W14-3358,E14-1035,1,0.921877,"e show that combining information from both local and global test contexts helps to improve lexical selection and outperforms a baseline system by up to 1.15 B LEU. We test our topic-adapted model on a diverse data set containing documents from three different domains and achieve competitive performance in comparison with two supervised domain-adapted systems. 1 More recent work in SMT uses latent representations of the document context to dynamically adapt the translation model with either monolingual topic models (Eidelman et al., 2012; Hewavitharana et al., 2013) or bilingual topic models (Hasler et al., 2014), thereby allowing the translation system to disambiguate source phrases using document context. Eidelman et al. (2012) also apply a topic model to each test sentence and find that sentence context is sufficient for picking good translations, but they do not attempt to combine sentence and document level information. Sentence-level topic adaptation for SMT has also been employed by Hasler et al. (2012). Other approaches to topic adaptation for SMT include Zhao and Xing (2007) and Tam et al. (2008), both of which use adapted lexical weights. Introduction The task of lexical selection plays an i"
W14-3358,D07-1109,0,0.0336596,"k on neural language models, Huang et al. (2012) combine the scores of two neural networks modelling the word embeddings of previous words in a sequence as well as those of words from the surrounding document by averaging over all word embeddings occurring in the same document. The score of the next word in a sequence is computed as the sum of the scores of both networks, but they do not consider alternative ways of combining contextual information. Most work in the WSD literature has modelled disambiguation using a limited window of context around the word to disambiguate. Cai et al. (2007), Boyd-graber and Blei (2007) and Li et al. (2010) further tried to integrate the notion of latent topics to address the sparsity problem of the lexicalised features typically used in WSD classifiers. The most closely related work in the area of sense disambiguation is by Dinu and Lapata (2010) who propose a disambiguation method for solving lexical similarity and substitution tasks. They measure word similarity in context by learning distributions over senses for each target word in the form of lower-dimensional distributional representations. Before computing word similarities, they contextualise the global sense distri"
W14-3358,P13-2122,0,0.126372,"o a given test context by measuring their similarity. We show that combining information from both local and global test contexts helps to improve lexical selection and outperforms a baseline system by up to 1.15 B LEU. We test our topic-adapted model on a diverse data set containing documents from three different domains and achieve competitive performance in comparison with two supervised domain-adapted systems. 1 More recent work in SMT uses latent representations of the document context to dynamically adapt the translation model with either monolingual topic models (Eidelman et al., 2012; Hewavitharana et al., 2013) or bilingual topic models (Hasler et al., 2014), thereby allowing the translation system to disambiguate source phrases using document context. Eidelman et al. (2012) also apply a topic model to each test sentence and find that sentence context is sufficient for picking good translations, but they do not attempt to combine sentence and document level information. Sentence-level topic adaptation for SMT has also been employed by Hasler et al. (2012). Other approaches to topic adaptation for SMT include Zhao and Xing (2007) and Tam et al. (2008), both of which use adapted lexical weights. Intro"
W14-3358,D07-1108,0,0.0816148,"Missing"
W14-3358,2007.tmi-papers.6,0,0.0678035,"hine translation (SMT). It strongly depends on context and is particularly difficult when the domain of a test document is unknown, for example when translating web documents from diverse sources. Selecting translations of words or phrases that preserve the sense of the source words is closely related to the field of word sense disambiguation (WSD), which has been studied extensively in the past. Most approaches to WSD model context at the sentence level and do not take the wider context of a word into account. Some of the ideas from the field of WSD have been adapted for machine translation (Carpuat and Wu, 2007b; Carpuat and Wu, 2007a; Chan et al., 2007). For example, Carpuat and Wu (2007a) extend word sense disambiguation to phrase sense disambiguation and In this paper, we present a topic model that learns latent distributional representations of the context of a phrase pair which can be applied to both local and global contexts at test time. We introduce similarity features that compare latent representations of phrase pair types to test contexts to disambiguate senses for improved lexical selection. We also propose different strategies for combining local and global topical context and show that"
W14-3358,D11-1125,0,0.0595404,"Missing"
W14-3358,P12-1092,0,0.020399,"orkshop on Statistical Machine Translation, pages 445–456, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics 2 Related work tations, their model has no notion of structure beyond corpus boundaries and is adapted towards a single target domain (cross-domain). Instead, our model learns the latent topical structure automatically and the translation model is adapted dynamically to each test instance. We are not aware of prior work in the field of MT that investigates combinations of local and global context. In their recent work on neural language models, Huang et al. (2012) combine the scores of two neural networks modelling the word embeddings of previous words in a sequence as well as those of words from the surrounding document by averaging over all word embeddings occurring in the same document. The score of the next word in a sequence is computed as the sum of the scores of both networks, but they do not consider alternative ways of combining contextual information. Most work in the WSD literature has modelled disambiguation using a limited window of context around the word to disambiguate. Cai et al. (2007), Boyd-graber and Blei (2007) and Li et al. (2010)"
W14-3358,D07-1007,0,0.0808304,"hine translation (SMT). It strongly depends on context and is particularly difficult when the domain of a test document is unknown, for example when translating web documents from diverse sources. Selecting translations of words or phrases that preserve the sense of the source words is closely related to the field of word sense disambiguation (WSD), which has been studied extensively in the past. Most approaches to WSD model context at the sentence level and do not take the wider context of a word into account. Some of the ideas from the field of WSD have been adapted for machine translation (Carpuat and Wu, 2007b; Carpuat and Wu, 2007a; Chan et al., 2007). For example, Carpuat and Wu (2007a) extend word sense disambiguation to phrase sense disambiguation and In this paper, we present a topic model that learns latent distributional representations of the context of a phrase pair which can be applied to both local and global contexts at test time. We introduce similarity features that compare latent representations of phrase pair types to test contexts to disambiguate senses for improved lexical selection. We also propose different strategies for combining local and global topical context and show that"
W14-3358,W09-2404,0,0.211019,"Missing"
W14-3358,2012.eamt-1.60,0,0.0933124,"are trained on the concatenation of all training data and fixed for all models. Table 2 shows the average length of a document for each domain. While a C C document contains 29.1 sentences on average, documents from N C and T ED are on average more than twice as long. The length of a document could have an influence on how reliable global topic information is but also on how important it is to have information from both local and global test contexts. Data Train Dev Test Data and experimental setup Our experiments were carried out on a mixed French-English data set containing the T ED corpus (Cettolo et al., 2012), parts of the News Commentary corpus (N C) and parts of the Commoncrawl corpus (C C) from the WMT13 shared task (Bojar et al., 2013) as described in Table 1. To ensure that the baseline model does not have an implicit preference for any particular domain, we selected subsets of the N C and C C corpora such that the training data contains 2.7M English words per domain. We were guided by two constraints in chosing our data set in order to simulate an environment where very diverse documents have to be translated, which is a typical scenario for web translation engines: 1) the data has document"
W14-3358,P07-1005,0,0.0897345,"context and is particularly difficult when the domain of a test document is unknown, for example when translating web documents from diverse sources. Selecting translations of words or phrases that preserve the sense of the source words is closely related to the field of word sense disambiguation (WSD), which has been studied extensively in the past. Most approaches to WSD model context at the sentence level and do not take the wider context of a word into account. Some of the ideas from the field of WSD have been adapted for machine translation (Carpuat and Wu, 2007b; Carpuat and Wu, 2007a; Chan et al., 2007). For example, Carpuat and Wu (2007a) extend word sense disambiguation to phrase sense disambiguation and In this paper, we present a topic model that learns latent distributional representations of the context of a phrase pair which can be applied to both local and global contexts at test time. We introduce similarity features that compare latent representations of phrase pair types to test contexts to disambiguate senses for improved lexical selection. We also propose different strategies for combining local and global topical context and show that using clues from both levels of contexts is"
W14-3358,W04-3250,1,0.65101,"results on the documents from each domain. While all topic settings yield improvements over the baseline, the largest improvement on the mixed test set (+0.48 B LEU) is achieved with 50 topics. Topic adaptation is most effective on the T ED portion of the test set where the increase in B LEU is 0.59. 7.2 Mixed -26.86 *27.15 *27.19 *27.34 *27.26 Table 3: B LEU scores of baseline system + phrSim-local feature for different numbers of topics. In this section we present experimental results of our model with different context settings and against different baselines. We used bootstrap resampling (Koehn, 2004) to measure significance on the mixed test set and marked all statistically significant results compared to the respective baselines with asterisk (*: p ≤ 0.01). 7.1 Model Baseline 10 topics 20 topics 50 topics 100 topics Model Baseline 10 topics 20 topics 50 topics 100 topics Mixed -26.86 *27.30 *27.34 *27.27 *27.24 CC 19.61 20.01 20.07 20.12 19.95 NC 29.42 29.61 29.56 29.48 29.66 T ED 31.88 32.64 32.71 32.55 32.52 &gt;Baseline +0.48 +0.51 +0.24 +0.83 Table 4: B LEU scores of baseline system + phrSim-global feature for different numbers of topics. 7.3 Relation to properties of test documents To"
W14-3358,P10-1116,0,0.0236438,"ng et al. (2012) combine the scores of two neural networks modelling the word embeddings of previous words in a sequence as well as those of words from the surrounding document by averaging over all word embeddings occurring in the same document. The score of the next word in a sequence is computed as the sum of the scores of both networks, but they do not consider alternative ways of combining contextual information. Most work in the WSD literature has modelled disambiguation using a limited window of context around the word to disambiguate. Cai et al. (2007), Boyd-graber and Blei (2007) and Li et al. (2010) further tried to integrate the notion of latent topics to address the sparsity problem of the lexicalised features typically used in WSD classifiers. The most closely related work in the area of sense disambiguation is by Dinu and Lapata (2010) who propose a disambiguation method for solving lexical similarity and substitution tasks. They measure word similarity in context by learning distributions over senses for each target word in the form of lower-dimensional distributional representations. Before computing word similarities, they contextualise the global sense distribution of a word usin"
W14-3358,E12-1055,0,0.15772,"Missing"
W14-3362,2012.eamt-1.44,0,0.0176638,"93 5.4 SPMT Model 2 rules. Special additional rules allow for combination of those non-syntactic lefthand side non-terminals with genuine syntactic non-terminals on the right-hand sides of other rules during decoding. Another line of research took the hierarchical phrase-based model (Chiang, 2005; Chiang, 2007) as a starting point and extended it with syntactic enhancements. In their SAMT system, Zollmann and Venugopal (2006) labeled the non-terminals of the hierarchical model with composite symbols derived from the syntactic tree annotation. Similar methods have been applied with CCG labels (Almaghout et al., 2012). Venugopal et al. (2009) and Stein et al. (2010) keep the grammar of the non-terminals of the hierarchical model unlabeled and apply the syntactic information in a separate model. Other authors added features which fire for phrases complying with certain syntactic properties while retaining all phrase pairs of the hierarchical model (Marton and Resnik, 2008; Vilar et al., 2008). In a tree-to-tree translation setting, Chiang (2010) proposed techniques to soften the syntactic constraints. A fuzzy approach with complex non-terminal symbols as in SAMT is employed to overcome the limitations durin"
W14-3362,D07-1079,0,0.213723,"hypotheses that make use of the alternative segmentations and translation options provided through non-syntactic phrases. The search space is more diverse, and in some cases all hypotheses from purely syntax-based derivations score worse than a translation that applies one or more non-syntactic phrases. We empirically demonstrate that this technique can lead to substantial gains in translation quality. Our syntactic translation models conform to the GHKM syntax approach as proposed by Galley, Hopkins, Knight, and Marcu (Galley et al., 2004) with composed rules as in (Galley et al., 2006) and (DeNeefe et al., 2007). State-of-theart GHKM string-to-tree systems have recently shown very competitive performance in public We present an effective technique to easily augment GHKM-style syntax-based machine translation systems (Galley et al., 2006) with phrase pairs that do not comply with any syntactic well-formedness constraints. Non-syntactic phrase pairs are distinguished from syntactic ones in order to avoid harming effects. We apply our technique in state-of-the-art string-totree and tree-to-string setups. For tree-tostring translation, we furthermore investigate novel approaches for translating with sour"
W14-3362,W09-0432,0,0.0268617,"ree-to-tree translation setting, Chiang (2010) proposed techniques to soften the syntactic constraints. A fuzzy approach with complex non-terminal symbols as in SAMT is employed to overcome the limitations during phrase extraction. In decoding, substitutions of non-terminals are not restricted to matching ones. Any lefthand side non-terminal can substitute any righthand side non-terminal. The decoder decides on the best derivation based on the tuned weights of a large number of binary features. Joining phrase inventories that come from multiple origins is a common method in domain adaptation (Bertoldi and Federico, 2009; Niehues and Waibel, 2012) but has also been applied in the contexts of lightly-supervised training (Schwenk, 2008; Huck et al., 2011) and of forced alignment training (Wuebker et al., 2010). For our purposes, we apply a fill-up method in the manner of the one that has been shown to perform well for domain adaptation in earlier work (Bisazza et al., 2011). Previous research that resembles our work most has been presented by Liu et al. (2006) and by Hanneman and Lavie (2009). Liu et al. (2006) allow for application of nonsyntactic phrase pairs in their tree-to-string alignment template (TAT) s"
W14-3362,W08-0306,0,0.0204307,"ation. 6 Related Work Issues with overly restrictive syntactic grammars for statistical machine translation, inadequate syntactic parses, and insufficient coverage have been tackled from several different directions in the literature. A proposed approach to attain better syntactic phrase inventories is to restructure the syntactic parse trees in a preprocessing step (Wang et al., 2007; Wang et al., 2010; Burkett and Klein, 2012). This line of research aims at rearranging parse trees in a way that makes them a better fit for the requirements of the bilingual downstream application. Conversely, Fossum et al. (2008) retain the structure of the parse trees and modify the word alignments. Marcu et al. (2006) relax syntactic phrase extraction constraints in their SPMT Model 2 to allow for phrases that do not match the span of one single constituent in the parse tree. SPMT Model 2 rules are created from spans that are consistent with the word alignment and covered by multiple constituents such that the union of the constituents matches the span. Pseudo non-syntactic nonterminals are introduced for the left-hand sides of 494 phrases are not summed up to obtain new estimates. • Non-syntactic phrase pairs are d"
W14-3362,2011.iwslt-evaluation.18,0,0.484519,"he set BP is extracted from all training instances, and phrase translation probabilities are computed separately from those in the syntactic phrase inventory. • Non-syntactic phrases are converted to rules by providing a special left-hand side nonterminal X. • A phrase table fill-up method is applied to enhance the syntactic phrase inventory with entries from the non-syntactic phrase inventory. Non-syntactic rules are only added to the final grammar if no syntactic rule with the same (source and target) right-hand side is present. This method is inspired by previous work in domain adaptation (Bisazza et al., 2011). • The glue grammar is extended with a new glue rule X, Q → hX ∼0 X ∼1 , Q∼0 X ∼1 i that enables the system to make use of nonsyntactic rules in decoding. • A binary feature is added to the log-linear model (Och and Ney, 2002) to distinguish non-syntactic rules from syntactic ones, and to be able to assign a tuned weight to the nonsyntactic part of the grammar. 2 http://www.statmt.org/wmt14/ translation-task.html 3 We remove grammatical case and function information from the annotation obtained with BitPar. 489 system dev B LEU T ER phrase-based + lexicalized reordering 33.0 34.2 48.8 48.1 st"
W14-3362,D08-1089,0,0.15454,"red in truecase with B LEU and T ER (Snover et al., 2006).4 We apply a phrase length limit of five when extracting non-syntactic phrases for the fill-up of syntactic phrase tables. 5.2 Translation Results Table 1 comprises the results of our empirical evaluation of the translation quality achieved by the different systems. 5.2.1 Phrase-based Baselines We set up two phrase-based baselines for comparison. Their set of models is the same as for the syntax-based baselines, with the exception of the PCFG probability. One of the phrase-based systems moreover utilizes a lexicalized reordering model (Galley and Manning, 2008). No nonstandard advanced features (like an operation sequence model or class-based LMs) are engrafted. The maximum phrase length is five, search is carried out with cube pruning at a k-best limit of 1000. A maximum number of 100 translation options per source side are taken into account. 5.2.2 String-to-String Contrastive System A further contrastive experiment is done with a string-to-string system. The extraction method for this string-to-string system is GHKM syntaxdirected with syntactic target-side annotation from BitPar, as in the string-to-tree setup. We actually extract the same rules"
W14-3362,N04-1035,0,0.887783,"derivations which resort to both types of phrases. Such derivations yield hypotheses that make use of the alternative segmentations and translation options provided through non-syntactic phrases. The search space is more diverse, and in some cases all hypotheses from purely syntax-based derivations score worse than a translation that applies one or more non-syntactic phrases. We empirically demonstrate that this technique can lead to substantial gains in translation quality. Our syntactic translation models conform to the GHKM syntax approach as proposed by Galley, Hopkins, Knight, and Marcu (Galley et al., 2004) with composed rules as in (Galley et al., 2006) and (DeNeefe et al., 2007). State-of-theart GHKM string-to-tree systems have recently shown very competitive performance in public We present an effective technique to easily augment GHKM-style syntax-based machine translation systems (Galley et al., 2006) with phrase pairs that do not comply with any syntactic well-formedness constraints. Non-syntactic phrase pairs are distinguished from syntactic ones in order to avoid harming effects. We apply our technique in state-of-the-art string-totree and tree-to-string setups. For tree-tostring transla"
W14-3362,D12-1079,0,0.0188008,"f nonsyntactic rules into hierarchical rules (other than the glue rules) but did not see improvements with it as yet. Furthermore, efficiency concerns become more relevant in such an implementation. 6 Related Work Issues with overly restrictive syntactic grammars for statistical machine translation, inadequate syntactic parses, and insufficient coverage have been tackled from several different directions in the literature. A proposed approach to attain better syntactic phrase inventories is to restructure the syntactic parse trees in a preprocessing step (Wang et al., 2007; Wang et al., 2010; Burkett and Klein, 2012). This line of research aims at rearranging parse trees in a way that makes them a better fit for the requirements of the bilingual downstream application. Conversely, Fossum et al. (2008) retain the structure of the parse trees and modify the word alignments. Marcu et al. (2006) relax syntactic phrase extraction constraints in their SPMT Model 2 to allow for phrases that do not match the span of one single constituent in the parse tree. SPMT Model 2 rules are created from spans that are consistent with the word alignment and covered by multiple constituents such that the union of the constitu"
W14-3362,W08-0509,0,0.0304451,"sing and cube pruning (Hoang et al., 2009). ∧ ∀(i, j) ∈ A : i1 ≤ i ≤ i2 ↔ j1 ≤ j ≤ j2 Experimental Setup We work with an English–German parallel training corpus of around 4.5 M sentence pairs (after corpus cleaning). The parallel data originates from three different sources which have been eligible for the constrained track of the ACL 2014 Ninth Workshop on Statistical Machine Translation shared translation task: Europarl (Koehn, 2005), News Commentary, and the Common Crawl corpus as provided on the WMT website. Word alignments are created by aligning the data in both directions with MGIZA++ (Gao and Vogel, 2008) and symmetrizing the two trained alignments (Och and Ney, 2003; Koehn et al., 2003). For string-to-tree translation, we parse the German target side with BitPar (Schmid, 2004).3 For tree-to-string translation, we parse the English source side of the parallel data with the English Berkeley Parser (Petrov et al., 2006). When extracting syntactic phrases, we impose several restrictions for composed rules, in particular a maximum number of twenty tree nodes per rule, a maximum depth of five, and a maximum size of five. We discard rules with non-terminals on their right-hand side if they are singl"
W14-3362,N12-1047,0,0.0487444,"he rule was extracted (Williams et al., 2014). Phrase translation probabilities are smoothed via Good-Turing smoothing. The language model (LM) is a large interpolated 5-gram LM with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). The target side of the parallel corpus and the monolingual German News Crawl corpora are employed as training data. We use the SRILM toolkit (Stolcke, 2002) to train the LM and rely on KenLM (Heafield, 2011) for language model scoring during decoding. Model weights are optimized to maximize B LEU (Papineni et al., 2002) with batch MIRA (Cherry and Foster, 2012) on 1000-best lists. We selected 2000 sentences from the newstest20082012 sets as a development set. The selected sentences obtained high sentence-level B LEU scores when being translated with a baseline phrasebased system, and do each contain less than 30 words for more rapid tuning. newstest2013 and newstest2014 are used as unseen test sets. Translation quality is measured in truecase with B LEU and T ER (Snover et al., 2006).4 We apply a phrase length limit of five when extracting non-syntactic phrases for the fill-up of syntactic phrase tables. 5.2 Translation Results Table 1 comprises the"
W14-3362,W09-2301,0,0.0810515,"f binary features. Joining phrase inventories that come from multiple origins is a common method in domain adaptation (Bertoldi and Federico, 2009; Niehues and Waibel, 2012) but has also been applied in the contexts of lightly-supervised training (Schwenk, 2008; Huck et al., 2011) and of forced alignment training (Wuebker et al., 2010). For our purposes, we apply a fill-up method in the manner of the one that has been shown to perform well for domain adaptation in earlier work (Bisazza et al., 2011). Previous research that resembles our work most has been presented by Liu et al. (2006) and by Hanneman and Lavie (2009). Liu et al. (2006) allow for application of nonsyntactic phrase pairs in their tree-to-string alignment template (TAT) system. The translation probabilities for the non-syntactic phrases are obtained from a standard phrase-based extraction Discussion A drawback of our method is that it increases the size of the synchronous context-free grammar massively. Most phrase pairs from standard phrase-based extraction are actually not present in the GHKM rule set, even with composed rules. A large fraction of the extracted non-syntactic phrases is such added to the phrase inventory through phrase tabl"
W14-3362,P05-1033,0,0.498759,"ystems utilize linguistic information that is obtained by parsing the training data. In tree-to-string translation, source-side syntactic tree annotation is employed, while string-to-tree translation exploits target-side syntax. The syntactic parse tree annotation constrains phrase extraction to syntactically well-formed phrase pairs: spans of syntactic phrases must match constituents in the parse tree. Standard phrase-based and hierarchical phrasebased statistical machine translation systems, in contrast, allow all phrase pairs that are consistent with the word alignment (Koehn et al., 2003; Chiang, 2005). A restriction of the phrase inventory to syntactically well-formed phrase pairs entails that possibly valuable information from the training data remains disregarded. While we would expect phrase pairs that are not linguistically motivated to be less reliable, discarding them altogether might be an overly harsh decision. The quality of an inventory of syntactic phrases depends heavily on the tree 486 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 486–498, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics to-tree). A synta"
W14-3362,W11-2123,0,0.0256518,"word and phrase penalty, an n-gram language model, a rule rareness penalty, and the monolingual PCFG probability of the tree fragment from which the rule was extracted (Williams et al., 2014). Phrase translation probabilities are smoothed via Good-Turing smoothing. The language model (LM) is a large interpolated 5-gram LM with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). The target side of the parallel corpus and the monolingual German News Crawl corpora are employed as training data. We use the SRILM toolkit (Stolcke, 2002) to train the LM and rely on KenLM (Heafield, 2011) for language model scoring during decoding. Model weights are optimized to maximize B LEU (Papineni et al., 2002) with batch MIRA (Cherry and Foster, 2012) on 1000-best lists. We selected 2000 sentences from the newstest20082012 sets as a development set. The selected sentences obtained high sentence-level B LEU scores when being translated with a baseline phrasebased system, and do each contain less than 30 words for more rapid tuning. newstest2013 and newstest2014 are used as unseen test sets. Translation quality is measured in truecase with B LEU and T ER (Snover et al., 2006).4 We apply a"
W14-3362,J07-2003,0,0.580655,"and NE denote the source and target non-terminal vocabulary, respectively. The non-terminals on the source side and on the target side of rules are linked in a one-toone correspondence. The ∼ relation defines this one-to-one correspondence. The left-hand side of the rule is a pair of source and target nonterminals, A ∈ NF and B ∈ NE . Decoding is typically carried out with a parsingbased algorithm, in our case a customized version of CYK+ (Chappelier and Rajman, 1998). The parsing algorithm is extended to handle translation candidates and to incorporate language model scores via cube pruning (Chiang, 2007). evaluation campaigns (Nadejde et al., 2013; Bojar et al., 2013). We apply the GHKM approach not only in a string-to-tree setting as in previous work, but employ it to build tree-to-string systems as well. We conduct tree-to-string translation with text input and additionally adopt translation with tree input and input tree constraints as suggested for hierarchical translation by Hoang and Koehn (2010). We also implement translation with tree input and feature-driven soft tree matching. The effect of augmenting the systems with nonsyntactic phrases is evaluated for all variants. 2 Outline The"
W14-3362,W10-1761,1,0.925206,"thm, in our case a customized version of CYK+ (Chappelier and Rajman, 1998). The parsing algorithm is extended to handle translation candidates and to incorporate language model scores via cube pruning (Chiang, 2007). evaluation campaigns (Nadejde et al., 2013; Bojar et al., 2013). We apply the GHKM approach not only in a string-to-tree setting as in previous work, but employ it to build tree-to-string systems as well. We conduct tree-to-string translation with text input and additionally adopt translation with tree input and input tree constraints as suggested for hierarchical translation by Hoang and Koehn (2010). We also implement translation with tree input and feature-driven soft tree matching. The effect of augmenting the systems with nonsyntactic phrases is evaluated for all variants. 2 Outline The remainder of the paper is structured as follows: We review some of the basics of syntaxbased translation in the next section (Section 3) and sketch the characteristics of our GHKM string-to-tree and tree-to-string translation frameworks. In Section 4, we describe our technique to augment GHKM-style syntax-based systems with phrase pairs that do not comply with any syntactic well-formedness constraints."
W14-3362,P10-1146,0,0.0207005,"-terminals of the hierarchical model with composite symbols derived from the syntactic tree annotation. Similar methods have been applied with CCG labels (Almaghout et al., 2012). Venugopal et al. (2009) and Stein et al. (2010) keep the grammar of the non-terminals of the hierarchical model unlabeled and apply the syntactic information in a separate model. Other authors added features which fire for phrases complying with certain syntactic properties while retaining all phrase pairs of the hierarchical model (Marton and Resnik, 2008; Vilar et al., 2008). In a tree-to-tree translation setting, Chiang (2010) proposed techniques to soften the syntactic constraints. A fuzzy approach with complex non-terminal symbols as in SAMT is employed to overcome the limitations during phrase extraction. In decoding, substitutions of non-terminals are not restricted to matching ones. Any lefthand side non-terminal can substitute any righthand side non-terminal. The decoder decides on the best derivation based on the tuned weights of a large number of binary features. Joining phrase inventories that come from multiple origins is a common method in domain adaptation (Bertoldi and Federico, 2009; Niehues and Waibe"
W14-3362,2009.iwslt-papers.4,1,0.883828,"obey the following procedure: 5.1 Empirical Evaluation We evaluate the effect of augmenting GHKM syntax-based translation systems—both string-totree and tree-to-string—with non-syntactic phrase pairs on the English→German language pair using the standard newstest sets of the Workshop on Statistical Machine Translation (WMT) for testing.2 The experiments are conducted with the openBP( f1J , eI1 , A) = source Moses implementations of GHKM rule exn j2 i2 traction (Williams and Koehn, 2012) and decoding h f j1 , ei1 i : ∃(i, j) ∈ A : i1 ≤ i ≤ i2 ∧ j1 ≤ j ≤ j2 o with CYK+ parsing and cube pruning (Hoang et al., 2009). ∧ ∀(i, j) ∈ A : i1 ≤ i ≤ i2 ↔ j1 ≤ j ≤ j2 Experimental Setup We work with an English–German parallel training corpus of around 4.5 M sentence pairs (after corpus cleaning). The parallel data originates from three different sources which have been eligible for the constrained track of the ACL 2014 Ninth Workshop on Statistical Machine Translation shared translation task: Europarl (Koehn, 2005), News Commentary, and the Common Crawl corpus as provided on the WMT website. Word alignments are created by aligning the data in both directions with MGIZA++ (Gao and Vogel, 2008) and symmetrizing the"
W14-3362,P02-1038,0,0.497403,"eft-hand side nonterminal X. • A phrase table fill-up method is applied to enhance the syntactic phrase inventory with entries from the non-syntactic phrase inventory. Non-syntactic rules are only added to the final grammar if no syntactic rule with the same (source and target) right-hand side is present. This method is inspired by previous work in domain adaptation (Bisazza et al., 2011). • The glue grammar is extended with a new glue rule X, Q → hX ∼0 X ∼1 , Q∼0 X ∼1 i that enables the system to make use of nonsyntactic rules in decoding. • A binary feature is added to the log-linear model (Och and Ney, 2002) to distinguish non-syntactic rules from syntactic ones, and to be able to assign a tuned weight to the nonsyntactic part of the grammar. 2 http://www.statmt.org/wmt14/ translation-task.html 3 We remove grammatical case and function information from the annotation obtained with BitPar. 489 system dev B LEU T ER phrase-based + lexicalized reordering 33.0 34.2 48.8 48.1 string-to-string (syntax-directed extraction) + non-syntactic phrases 32.6 33.4 49.4 49.0 string-to-tree + non-syntactic phrases 33.6 34.3 48.7 48.0 tree-to-string + non-syntactic phrases 34.0 33.9 48.5 48.4 + input tree constrai"
W14-3362,W11-2211,1,0.849754,"al symbols as in SAMT is employed to overcome the limitations during phrase extraction. In decoding, substitutions of non-terminals are not restricted to matching ones. Any lefthand side non-terminal can substitute any righthand side non-terminal. The decoder decides on the best derivation based on the tuned weights of a large number of binary features. Joining phrase inventories that come from multiple origins is a common method in domain adaptation (Bertoldi and Federico, 2009; Niehues and Waibel, 2012) but has also been applied in the contexts of lightly-supervised training (Schwenk, 2008; Huck et al., 2011) and of forced alignment training (Wuebker et al., 2010). For our purposes, we apply a fill-up method in the manner of the one that has been shown to perform well for domain adaptation in earlier work (Bisazza et al., 2011). Previous research that resembles our work most has been presented by Liu et al. (2006) and by Hanneman and Lavie (2009). Liu et al. (2006) allow for application of nonsyntactic phrase pairs in their tree-to-string alignment template (TAT) system. The translation probabilities for the non-syntactic phrases are obtained from a standard phrase-based extraction Discussion A dr"
W14-3362,J03-1002,0,0.0100869,"≤ i2 ↔ j1 ≤ j ≤ j2 Experimental Setup We work with an English–German parallel training corpus of around 4.5 M sentence pairs (after corpus cleaning). The parallel data originates from three different sources which have been eligible for the constrained track of the ACL 2014 Ninth Workshop on Statistical Machine Translation shared translation task: Europarl (Koehn, 2005), News Commentary, and the Common Crawl corpus as provided on the WMT website. Word alignments are created by aligning the data in both directions with MGIZA++ (Gao and Vogel, 2008) and symmetrizing the two trained alignments (Och and Ney, 2003; Koehn et al., 2003). For string-to-tree translation, we parse the German target side with BitPar (Schmid, 2004).3 For tree-to-string translation, we parse the English source side of the parallel data with the English Berkeley Parser (Petrov et al., 2006). When extracting syntactic phrases, we impose several restrictions for composed rules, in particular a maximum number of twenty tree nodes per rule, a maximum depth of five, and a maximum size of five. We discard rules with non-terminals on their right-hand side if they are singletons in the training data. Only the 100 best translation optio"
W14-3362,W99-0604,0,0.190978,"1: the highlighted phrase pair halso wanted, wollten auchi cannot be extracted from this training instance for string-to-tree translation. The described techniques for GHKM string-totree translation can be adjusted for tree-to-string translation in a straightforward manner. Rules are extracted from training instances which consist of a source sentence along with its constituent parse tree, a target sentence, and a word alignment matrix. We omit the details. 488 In the standard phrase-based approach, in contrast, all continuous phrases that are consistent with the word alignment are extracted (Och et al., 1999; Och, 2002). The set of continuous bilingual phrases BP( f1J , eI1 , A), given a training instance comprising a source sentence f1J , a target sentence eI1 , and a word alignment A ⊆ {1, ..., I}×{1, ..., J}, is defined as follows: 5 Consistency for continuous phrases is based upon merely two constraints in this definition: (1.) At least one source and target position within the phrase must be aligned, and (2.) words from inside the source phrase may only be aligned to words from inside the target phrase and vice versa. The highlighted phrase pair from the example does not violate these constr"
W14-3362,N03-1017,1,0.112598,"achine translation systems utilize linguistic information that is obtained by parsing the training data. In tree-to-string translation, source-side syntactic tree annotation is employed, while string-to-tree translation exploits target-side syntax. The syntactic parse tree annotation constrains phrase extraction to syntactically well-formed phrase pairs: spans of syntactic phrases must match constituents in the parse tree. Standard phrase-based and hierarchical phrasebased statistical machine translation systems, in contrast, allow all phrase pairs that are consistent with the word alignment (Koehn et al., 2003; Chiang, 2005). A restriction of the phrase inventory to syntactically well-formed phrase pairs entails that possibly valuable information from the training data remains disregarded. While we would expect phrase pairs that are not linguistically motivated to be less reliable, discarding them altogether might be an overly harsh decision. The quality of an inventory of syntactic phrases depends heavily on the tree 486 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 486–498, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics to"
W14-3362,2005.mtsummit-papers.11,1,0.115944,"source Moses implementations of GHKM rule exn j2 i2 traction (Williams and Koehn, 2012) and decoding h f j1 , ei1 i : ∃(i, j) ∈ A : i1 ≤ i ≤ i2 ∧ j1 ≤ j ≤ j2 o with CYK+ parsing and cube pruning (Hoang et al., 2009). ∧ ∀(i, j) ∈ A : i1 ≤ i ≤ i2 ↔ j1 ≤ j ≤ j2 Experimental Setup We work with an English–German parallel training corpus of around 4.5 M sentence pairs (after corpus cleaning). The parallel data originates from three different sources which have been eligible for the constrained track of the ACL 2014 Ninth Workshop on Statistical Machine Translation shared translation task: Europarl (Koehn, 2005), News Commentary, and the Common Crawl corpus as provided on the WMT website. Word alignments are created by aligning the data in both directions with MGIZA++ (Gao and Vogel, 2008) and symmetrizing the two trained alignments (Och and Ney, 2003; Koehn et al., 2003). For string-to-tree translation, we parse the German target side with BitPar (Schmid, 2004).3 For tree-to-string translation, we parse the English source side of the parallel data with the English Berkeley Parser (Petrov et al., 2006). When extracting syntactic phrases, we impose several restrictions for composed rules, in particula"
W14-3362,P02-1040,0,0.0918955,"bility of the tree fragment from which the rule was extracted (Williams et al., 2014). Phrase translation probabilities are smoothed via Good-Turing smoothing. The language model (LM) is a large interpolated 5-gram LM with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). The target side of the parallel corpus and the monolingual German News Crawl corpora are employed as training data. We use the SRILM toolkit (Stolcke, 2002) to train the LM and rely on KenLM (Heafield, 2011) for language model scoring during decoding. Model weights are optimized to maximize B LEU (Papineni et al., 2002) with batch MIRA (Cherry and Foster, 2012) on 1000-best lists. We selected 2000 sentences from the newstest20082012 sets as a development set. The selected sentences obtained high sentence-level B LEU scores when being translated with a baseline phrasebased system, and do each contain less than 30 words for more rapid tuning. newstest2013 and newstest2014 are used as unseen test sets. Translation quality is measured in truecase with B LEU and T ER (Snover et al., 2006).4 We apply a phrase length limit of five when extracting non-syntactic phrases for the fill-up of syntactic phrase tables. 5.2"
W14-3362,P06-1055,0,0.0261399,"track of the ACL 2014 Ninth Workshop on Statistical Machine Translation shared translation task: Europarl (Koehn, 2005), News Commentary, and the Common Crawl corpus as provided on the WMT website. Word alignments are created by aligning the data in both directions with MGIZA++ (Gao and Vogel, 2008) and symmetrizing the two trained alignments (Och and Ney, 2003; Koehn et al., 2003). For string-to-tree translation, we parse the German target side with BitPar (Schmid, 2004).3 For tree-to-string translation, we parse the English source side of the parallel data with the English Berkeley Parser (Petrov et al., 2006). When extracting syntactic phrases, we impose several restrictions for composed rules, in particular a maximum number of twenty tree nodes per rule, a maximum depth of five, and a maximum size of five. We discard rules with non-terminals on their right-hand side if they are singletons in the training data. Only the 100 best translation options per distinct source side with respect to the weighted phrase-level model scores are loaded by the decoder. The decoder is configured with a maximum chart span of 25 and a rule limit of 100. A standard set of models is used in the baselines, comprising p"
W14-3362,P06-1077,0,0.184599,"ights of a large number of binary features. Joining phrase inventories that come from multiple origins is a common method in domain adaptation (Bertoldi and Federico, 2009; Niehues and Waibel, 2012) but has also been applied in the contexts of lightly-supervised training (Schwenk, 2008; Huck et al., 2011) and of forced alignment training (Wuebker et al., 2010). For our purposes, we apply a fill-up method in the manner of the one that has been shown to perform well for domain adaptation in earlier work (Bisazza et al., 2011). Previous research that resembles our work most has been presented by Liu et al. (2006) and by Hanneman and Lavie (2009). Liu et al. (2006) allow for application of nonsyntactic phrase pairs in their tree-to-string alignment template (TAT) system. The translation probabilities for the non-syntactic phrases are obtained from a standard phrase-based extraction Discussion A drawback of our method is that it increases the size of the synchronous context-free grammar massively. Most phrase pairs from standard phrase-based extraction are actually not present in the GHKM rule set, even with composed rules. A large fraction of the extracted non-syntactic phrases is such added to the phr"
W14-3362,C04-1024,0,0.244259,"ce pairs (after corpus cleaning). The parallel data originates from three different sources which have been eligible for the constrained track of the ACL 2014 Ninth Workshop on Statistical Machine Translation shared translation task: Europarl (Koehn, 2005), News Commentary, and the Common Crawl corpus as provided on the WMT website. Word alignments are created by aligning the data in both directions with MGIZA++ (Gao and Vogel, 2008) and symmetrizing the two trained alignments (Och and Ney, 2003; Koehn et al., 2003). For string-to-tree translation, we parse the German target side with BitPar (Schmid, 2004).3 For tree-to-string translation, we parse the English source side of the parallel data with the English Berkeley Parser (Petrov et al., 2006). When extracting syntactic phrases, we impose several restrictions for composed rules, in particular a maximum number of twenty tree nodes per rule, a maximum depth of five, and a maximum size of five. We discard rules with non-terminals on their right-hand side if they are singletons in the training data. Only the 100 best translation options per distinct source side with respect to the weighted phrase-level model scores are loaded by the decoder. The"
W14-3362,W06-1606,0,0.0259403,"ne translation, inadequate syntactic parses, and insufficient coverage have been tackled from several different directions in the literature. A proposed approach to attain better syntactic phrase inventories is to restructure the syntactic parse trees in a preprocessing step (Wang et al., 2007; Wang et al., 2010; Burkett and Klein, 2012). This line of research aims at rearranging parse trees in a way that makes them a better fit for the requirements of the bilingual downstream application. Conversely, Fossum et al. (2008) retain the structure of the parse trees and modify the word alignments. Marcu et al. (2006) relax syntactic phrase extraction constraints in their SPMT Model 2 to allow for phrases that do not match the span of one single constituent in the parse tree. SPMT Model 2 rules are created from spans that are consistent with the word alignment and covered by multiple constituents such that the union of the constituents matches the span. Pseudo non-syntactic nonterminals are introduced for the left-hand sides of 494 phrases are not summed up to obtain new estimates. • Non-syntactic phrase pairs are distinguished from syntactic ones with an additional feature. pipeline. A non-syntactic phras"
W14-3362,2008.iwslt-papers.6,0,0.0274629,"plex non-terminal symbols as in SAMT is employed to overcome the limitations during phrase extraction. In decoding, substitutions of non-terminals are not restricted to matching ones. Any lefthand side non-terminal can substitute any righthand side non-terminal. The decoder decides on the best derivation based on the tuned weights of a large number of binary features. Joining phrase inventories that come from multiple origins is a common method in domain adaptation (Bertoldi and Federico, 2009; Niehues and Waibel, 2012) but has also been applied in the contexts of lightly-supervised training (Schwenk, 2008; Huck et al., 2011) and of forced alignment training (Wuebker et al., 2010). For our purposes, we apply a fill-up method in the manner of the one that has been shown to perform well for domain adaptation in earlier work (Bisazza et al., 2011). Previous research that resembles our work most has been presented by Liu et al. (2006) and by Hanneman and Lavie (2009). Liu et al. (2006) allow for application of nonsyntactic phrase pairs in their tree-to-string alignment template (TAT) system. The translation probabilities for the non-syntactic phrases are obtained from a standard phrase-based extrac"
W14-3362,P08-1114,0,0.0221182,"tic enhancements. In their SAMT system, Zollmann and Venugopal (2006) labeled the non-terminals of the hierarchical model with composite symbols derived from the syntactic tree annotation. Similar methods have been applied with CCG labels (Almaghout et al., 2012). Venugopal et al. (2009) and Stein et al. (2010) keep the grammar of the non-terminals of the hierarchical model unlabeled and apply the syntactic information in a separate model. Other authors added features which fire for phrases complying with certain syntactic properties while retaining all phrase pairs of the hierarchical model (Marton and Resnik, 2008; Vilar et al., 2008). In a tree-to-tree translation setting, Chiang (2010) proposed techniques to soften the syntactic constraints. A fuzzy approach with complex non-terminal symbols as in SAMT is employed to overcome the limitations during phrase extraction. In decoding, substitutions of non-terminals are not restricted to matching ones. Any lefthand side non-terminal can substitute any righthand side non-terminal. The decoder decides on the best derivation based on the tuned weights of a large number of binary features. Joining phrase inventories that come from multiple origins is a common"
W14-3362,2006.amta-papers.25,0,0.070592,"nd rely on KenLM (Heafield, 2011) for language model scoring during decoding. Model weights are optimized to maximize B LEU (Papineni et al., 2002) with batch MIRA (Cherry and Foster, 2012) on 1000-best lists. We selected 2000 sentences from the newstest20082012 sets as a development set. The selected sentences obtained high sentence-level B LEU scores when being translated with a baseline phrasebased system, and do each contain less than 30 words for more rapid tuning. newstest2013 and newstest2014 are used as unseen test sets. Translation quality is measured in truecase with B LEU and T ER (Snover et al., 2006).4 We apply a phrase length limit of five when extracting non-syntactic phrases for the fill-up of syntactic phrase tables. 5.2 Translation Results Table 1 comprises the results of our empirical evaluation of the translation quality achieved by the different systems. 5.2.1 Phrase-based Baselines We set up two phrase-based baselines for comparison. Their set of models is the same as for the syntax-based baselines, with the exception of the PCFG probability. One of the phrase-based systems moreover utilizes a lexicalized reordering model (Galley and Manning, 2008). No nonstandard advanced featur"
W14-3362,W13-2221,1,0.859277,"on-terminal vocabulary, respectively. The non-terminals on the source side and on the target side of rules are linked in a one-toone correspondence. The ∼ relation defines this one-to-one correspondence. The left-hand side of the rule is a pair of source and target nonterminals, A ∈ NF and B ∈ NE . Decoding is typically carried out with a parsingbased algorithm, in our case a customized version of CYK+ (Chappelier and Rajman, 1998). The parsing algorithm is extended to handle translation candidates and to incorporate language model scores via cube pruning (Chiang, 2007). evaluation campaigns (Nadejde et al., 2013; Bojar et al., 2013). We apply the GHKM approach not only in a string-to-tree setting as in previous work, but employ it to build tree-to-string systems as well. We conduct tree-to-string translation with text input and additionally adopt translation with tree input and input tree constraints as suggested for hierarchical translation by Hoang and Koehn (2010). We also implement translation with tree input and feature-driven soft tree matching. The effect of augmenting the systems with nonsyntactic phrases is evaluated for all variants. 2 Outline The remainder of the paper is structured as fol"
W14-3362,2010.amta-papers.8,0,0.0314091,"llow for combination of those non-syntactic lefthand side non-terminals with genuine syntactic non-terminals on the right-hand sides of other rules during decoding. Another line of research took the hierarchical phrase-based model (Chiang, 2005; Chiang, 2007) as a starting point and extended it with syntactic enhancements. In their SAMT system, Zollmann and Venugopal (2006) labeled the non-terminals of the hierarchical model with composite symbols derived from the syntactic tree annotation. Similar methods have been applied with CCG labels (Almaghout et al., 2012). Venugopal et al. (2009) and Stein et al. (2010) keep the grammar of the non-terminals of the hierarchical model unlabeled and apply the syntactic information in a separate model. Other authors added features which fire for phrases complying with certain syntactic properties while retaining all phrase pairs of the hierarchical model (Marton and Resnik, 2008; Vilar et al., 2008). In a tree-to-tree translation setting, Chiang (2010) proposed techniques to soften the syntactic constraints. A fuzzy approach with complex non-terminal symbols as in SAMT is employed to overcome the limitations during phrase extraction. In decoding, substitutions o"
W14-3362,N09-1027,0,0.0197082,". Special additional rules allow for combination of those non-syntactic lefthand side non-terminals with genuine syntactic non-terminals on the right-hand sides of other rules during decoding. Another line of research took the hierarchical phrase-based model (Chiang, 2005; Chiang, 2007) as a starting point and extended it with syntactic enhancements. In their SAMT system, Zollmann and Venugopal (2006) labeled the non-terminals of the hierarchical model with composite symbols derived from the syntactic tree annotation. Similar methods have been applied with CCG labels (Almaghout et al., 2012). Venugopal et al. (2009) and Stein et al. (2010) keep the grammar of the non-terminals of the hierarchical model unlabeled and apply the syntactic information in a separate model. Other authors added features which fire for phrases complying with certain syntactic properties while retaining all phrase pairs of the hierarchical model (Marton and Resnik, 2008; Vilar et al., 2008). In a tree-to-tree translation setting, Chiang (2010) proposed techniques to soften the syntactic constraints. A fuzzy approach with complex non-terminal symbols as in SAMT is employed to overcome the limitations during phrase extraction. In d"
W14-3362,2008.iwslt-papers.7,0,0.0193742,"r SAMT system, Zollmann and Venugopal (2006) labeled the non-terminals of the hierarchical model with composite symbols derived from the syntactic tree annotation. Similar methods have been applied with CCG labels (Almaghout et al., 2012). Venugopal et al. (2009) and Stein et al. (2010) keep the grammar of the non-terminals of the hierarchical model unlabeled and apply the syntactic information in a separate model. Other authors added features which fire for phrases complying with certain syntactic properties while retaining all phrase pairs of the hierarchical model (Marton and Resnik, 2008; Vilar et al., 2008). In a tree-to-tree translation setting, Chiang (2010) proposed techniques to soften the syntactic constraints. A fuzzy approach with complex non-terminal symbols as in SAMT is employed to overcome the limitations during phrase extraction. In decoding, substitutions of non-terminals are not restricted to matching ones. Any lefthand side non-terminal can substitute any righthand side non-terminal. The decoder decides on the best derivation based on the tuned weights of a large number of binary features. Joining phrase inventories that come from multiple origins is a common method in domain adap"
W14-3362,D07-1078,0,0.162882,"Missing"
W14-3362,J10-2004,0,0.183007,"ows for embedding of nonsyntactic rules into hierarchical rules (other than the glue rules) but did not see improvements with it as yet. Furthermore, efficiency concerns become more relevant in such an implementation. 6 Related Work Issues with overly restrictive syntactic grammars for statistical machine translation, inadequate syntactic parses, and insufficient coverage have been tackled from several different directions in the literature. A proposed approach to attain better syntactic phrase inventories is to restructure the syntactic parse trees in a preprocessing step (Wang et al., 2007; Wang et al., 2010; Burkett and Klein, 2012). This line of research aims at rearranging parse trees in a way that makes them a better fit for the requirements of the bilingual downstream application. Conversely, Fossum et al. (2008) retain the structure of the parse trees and modify the word alignments. Marcu et al. (2006) relax syntactic phrase extraction constraints in their SPMT Model 2 to allow for phrases that do not match the span of one single constituent in the parse tree. SPMT Model 2 rules are created from spans that are consistent with the word alignment and covered by multiple constituents such that"
W14-3362,W12-3150,1,0.87546,"from the example does not violate these constraints. In order to augment our GHKM syntax-based systems with non-syntactic phrases, we obey the following procedure: 5.1 Empirical Evaluation We evaluate the effect of augmenting GHKM syntax-based translation systems—both string-totree and tree-to-string—with non-syntactic phrase pairs on the English→German language pair using the standard newstest sets of the Workshop on Statistical Machine Translation (WMT) for testing.2 The experiments are conducted with the openBP( f1J , eI1 , A) = source Moses implementations of GHKM rule exn j2 i2 traction (Williams and Koehn, 2012) and decoding h f j1 , ei1 i : ∃(i, j) ∈ A : i1 ≤ i ≤ i2 ∧ j1 ≤ j ≤ j2 o with CYK+ parsing and cube pruning (Hoang et al., 2009). ∧ ∀(i, j) ∈ A : i1 ≤ i ≤ i2 ↔ j1 ≤ j ≤ j2 Experimental Setup We work with an English–German parallel training corpus of around 4.5 M sentence pairs (after corpus cleaning). The parallel data originates from three different sources which have been eligible for the constrained track of the ACL 2014 Ninth Workshop on Statistical Machine Translation shared translation task: Europarl (Koehn, 2005), News Commentary, and the Common Crawl corpus as provided on the WMT websi"
W14-3362,W14-3324,1,0.7738,"Missing"
W14-3362,P10-1049,0,0.0435401,"itations during phrase extraction. In decoding, substitutions of non-terminals are not restricted to matching ones. Any lefthand side non-terminal can substitute any righthand side non-terminal. The decoder decides on the best derivation based on the tuned weights of a large number of binary features. Joining phrase inventories that come from multiple origins is a common method in domain adaptation (Bertoldi and Federico, 2009; Niehues and Waibel, 2012) but has also been applied in the contexts of lightly-supervised training (Schwenk, 2008; Huck et al., 2011) and of forced alignment training (Wuebker et al., 2010). For our purposes, we apply a fill-up method in the manner of the one that has been shown to perform well for domain adaptation in earlier work (Bisazza et al., 2011). Previous research that resembles our work most has been presented by Liu et al. (2006) and by Hanneman and Lavie (2009). Liu et al. (2006) allow for application of nonsyntactic phrase pairs in their tree-to-string alignment template (TAT) system. The translation probabilities for the non-syntactic phrases are obtained from a standard phrase-based extraction Discussion A drawback of our method is that it increases the size of th"
W14-3362,W06-3119,0,0.0266323,"y statistics for the different English→German translation systems. “hier.” denotes hierarchical phrases, i.e. rules with non-terminals on their right-hand side, “lexical” denotes continuous phrases. 493 5.4 SPMT Model 2 rules. Special additional rules allow for combination of those non-syntactic lefthand side non-terminals with genuine syntactic non-terminals on the right-hand sides of other rules during decoding. Another line of research took the hierarchical phrase-based model (Chiang, 2005; Chiang, 2007) as a starting point and extended it with syntactic enhancements. In their SAMT system, Zollmann and Venugopal (2006) labeled the non-terminals of the hierarchical model with composite symbols derived from the syntactic tree annotation. Similar methods have been applied with CCG labels (Almaghout et al., 2012). Venugopal et al. (2009) and Stein et al. (2010) keep the grammar of the non-terminals of the hierarchical model unlabeled and apply the syntactic information in a separate model. Other authors added features which fire for phrases complying with certain syntactic properties while retaining all phrase pairs of the hierarchical model (Marton and Resnik, 2008; Vilar et al., 2008). In a tree-to-tree trans"
W14-3362,P06-1121,0,\N,Missing
W14-3362,W13-2201,1,\N,Missing
W14-4018,J07-2003,0,0.21977,"orkshop on Syntax, Semantics and Structure in Statistical Translation, pages 148–156, c October 25, 2014, Doha, Qatar. 2014 Association for Computational Linguistics one correspondence. The ∼ relation defines this one-to-one correspondence. The left-hand side of the rule is a pair of source and target nonterminals, A ∈ NF and B ∈ NE . Decoding is typically carried out with a parsingbased algorithm, in our case a customized version of CYK+ (Chappelier and Rajman, 1998). The parsing algorithm is extended to handle translation candidates and to incorporate language model scores via cube pruning (Chiang, 2007). this paradigm have recently been among the topranked submissions to public evaluation campaigns (Williams et al., 2014; Bojar et al., 2014). Our soft source syntactic constraints features borrow ideas from Marton and Resnik (2008) who proposed a comparable approach for hierarchical machine translation. The major difference is that the features of Marton and Resnik (2008) are only based on the labels from the input trees as seen in tuning and decoding. They penalize violations of constituent boundaries but do not employ syntactic parse annotation of the source side of the training data. We, i"
W14-4018,D07-1079,0,0.569203,"ntax-based (i.e., hierarchical) and linguistically syntax-based statistical machine translation has demonstrated that significant quality gains can be achieved via integration of syntactic information as features in a non-obtrusive manner, rather than as hard constraints. We implemented two feature-based extensions for a GHKM-style string-to-tree translation system (Galley et al., 2004): 3 Related Work Our syntactic translation model conforms to the GHKM syntax approach as proposed by Galley, Hopkins, Knight, and Marcu (Galley et al., 2004) with composed rules as in (Galley et al., 2006) and (DeNeefe et al., 2007). Systems based on 148 Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 148–156, c October 25, 2014, Doha, Qatar. 2014 Association for Computational Linguistics one correspondence. The ∼ relation defines this one-to-one correspondence. The left-hand side of the rule is a pair of source and target nonterminals, A ∈ NF and B ∈ NE . Decoding is typically carried out with a parsingbased algorithm, in our case a customized version of CYK+ (Chappelier and Rajman, 1998). The parsing algorithm is extended to handle translation candidates and t"
W14-4018,P06-1121,0,0.861703,"search in both formally syntax-based (i.e., hierarchical) and linguistically syntax-based statistical machine translation has demonstrated that significant quality gains can be achieved via integration of syntactic information as features in a non-obtrusive manner, rather than as hard constraints. We implemented two feature-based extensions for a GHKM-style string-to-tree translation system (Galley et al., 2004): 3 Related Work Our syntactic translation model conforms to the GHKM syntax approach as proposed by Galley, Hopkins, Knight, and Marcu (Galley et al., 2004) with composed rules as in (Galley et al., 2006) and (DeNeefe et al., 2007). Systems based on 148 Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 148–156, c October 25, 2014, Doha, Qatar. 2014 Association for Computational Linguistics one correspondence. The ∼ relation defines this one-to-one correspondence. The left-hand side of the rule is a pair of source and target nonterminals, A ∈ NF and B ∈ NE . Decoding is typically carried out with a parsingbased algorithm, in our case a customized version of CYK+ (Chappelier and Rajman, 1998). The parsing algorithm is extended to handle t"
W14-4018,W08-0509,0,0.117113,"ict than the first one and give the system a more detailed clue about the magnitude of mismatch. 6.2 7.1 We work with an English–German parallel training corpus of around 4.5 M sentence pairs (after corpus cleaning). The parallel data originates from three different sources which have been eligible for the constrained track of the ACL 2014 Ninth Workshop on Statistical Machine Translation shared translation task: Europarl (Koehn, 2005), News Commentary, and the Common Crawl corpus as provided on the WMT website. Word alignments are created by aligning the data in both directions with MGIZA++ (Gao and Vogel, 2008) and symmetrizing the two trained alignments (Och and Ney, 2003; Koehn et al., 2003). The German target side training data is parsed with BitPar (Schmid, 2004). We remove grammatical case and function information from the annotation obtained with BitPar and apply right binarization of the German parse trees prior to rule extraction (Wang et al., 2007; Wang et al., 2010; Nadejde et al., 2013). For the soft source syntactic constraints, we parse the English source side of the parallel data with the English Berkeley Parser (Petrov et al., 2006) and produce composite SAMT-style labels as discussed"
W14-4018,W11-2123,0,0.0267157,"test2008-2012 sets is used as development set. model, a rule rareness penalty, and the monolingual PCFG probability of the tree fragment from which the rule was extracted (Williams et al., 2014). Rule translation probabilities are smoothed via Good-Turing smoothing. The language model (LM) is a large interpolated 5-gram LM with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). The target side of the parallel corpus and the monolingual German News Crawl corpora are employed as training data. We use the SRILM toolkit (Stolcke, 2002) to train the LM and rely on KenLM (Heafield, 2011) for language model scoring during decoding. Model weights are optimized to maximize B LEU (Papineni et al., 2002) with batch MIRA (Cherry and Foster, 2012) on 1000-best lists. We selected 2000 sentences from the newstest20082012 sets as a development set. The selected sentences obtained high sentence-level B LEU scores when being translated with a baseline phrasebased system, and do each contain less than 30 words for more rapid tuning. newstest2013 and newstest2014 are used as unseen test sets. Translation quality is measured in truecase with B LEU and T ER (Snover et al., 2006).5 7.2 7.2.1"
W14-4018,2009.iwslt-papers.4,1,0.876407,"hat depend on the identity of labels within this set. All sparse features for source labels outside of the core set are inactive. 7 Experimental Setup Experiments We empirically evaluate the effectiveness of preference grammars and soft source syntactic constraints for GHKM translation on the English→German language pair using the standard newstest sets of the Workshop on Statistical Machine Translation (WMT) for testing.4 The experiments are conducted with the open-source Moses implementations of GHKM rule extraction (Williams and Koehn, 2012) and decoding with CYK+ parsing and cube pruning (Hoang et al., 2009). 4 http://www.statmt.org/wmt14/ translation-task.html 152 system GHKM string-to-tree baseline + soft source syntactic constraints + sparse features + sparse features (core = non-composite) + sparse features (core = dev-min-occ100) + sparse features (core = dev-min-occ1000) + hard source syntactic constraints string-to-string (GHKM syntax-directed rule extraction) + preference grammar + soft source syntactic constraints + drop derivations with tsyn (d) = 0 dev B LEU T ER 34.7 47.3 35.1 47.0 35.8 46.5 35.4 46.8 35.6 46.7 35.4 46.9 34.6 47.4 33.8 48.0 33.9 47.7 34.6 47.0 34.0 47.5 newstest2013 B"
W14-4018,D13-1053,0,0.0405122,"or hierarchical machine translation. The major difference is that the features of Marton and Resnik (2008) are only based on the labels from the input trees as seen in tuning and decoding. They penalize violations of constituent boundaries but do not employ syntactic parse annotation of the source side of the training data. We, in contrast, equip the rules with latent source label properties, allowing for features that can check for conformance of input tree labels and source labels that have been seen in training. Other groups have applied similar techniques to a string-to-dependency system (Huang et al., 2013) and—like in our work—a GHKM stringto-tree system (Zhang et al., 2011). Both Huang et al. (2013) and Zhang et al. (2011) store source labels as additional information with the rules. They however investigate somewhat different feature functions than we do. Marton and Resnik (2008) evaluated their method on the NIST Chinese→English and Arabic→English tasks. Huang et al. (2013) and Zhang et al. (2011) present results on the NIST Chinese→English task. We focus our attention on a very different task: English→German. 4 4.1 GHKM String-to-Tree Translation In GHKM string-to-tree translation (Galley e"
W14-4018,N12-1047,0,0.169755,"the rule was extracted (Williams et al., 2014). Rule translation probabilities are smoothed via Good-Turing smoothing. The language model (LM) is a large interpolated 5-gram LM with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). The target side of the parallel corpus and the monolingual German News Crawl corpora are employed as training data. We use the SRILM toolkit (Stolcke, 2002) to train the LM and rely on KenLM (Heafield, 2011) for language model scoring during decoding. Model weights are optimized to maximize B LEU (Papineni et al., 2002) with batch MIRA (Cherry and Foster, 2012) on 1000-best lists. We selected 2000 sentences from the newstest20082012 sets as a development set. The selected sentences obtained high sentence-level B LEU scores when being translated with a baseline phrasebased system, and do each contain less than 30 words for more rapid tuning. newstest2013 and newstest2014 are used as unseen test sets. Translation quality is measured in truecase with B LEU and T ER (Snover et al., 2006).5 7.2 7.2.1 Soft Source Syntactic Constraints Adding the three dense soft source syntactic constraints features from Section 6.1 improves the baseline scores by 0.3 poi"
W14-4018,N03-1017,1,0.0302173,"f mismatch. 6.2 7.1 We work with an English–German parallel training corpus of around 4.5 M sentence pairs (after corpus cleaning). The parallel data originates from three different sources which have been eligible for the constrained track of the ACL 2014 Ninth Workshop on Statistical Machine Translation shared translation task: Europarl (Koehn, 2005), News Commentary, and the Common Crawl corpus as provided on the WMT website. Word alignments are created by aligning the data in both directions with MGIZA++ (Gao and Vogel, 2008) and symmetrizing the two trained alignments (Och and Ney, 2003; Koehn et al., 2003). The German target side training data is parsed with BitPar (Schmid, 2004). We remove grammatical case and function information from the annotation obtained with BitPar and apply right binarization of the German parse trees prior to rule extraction (Wang et al., 2007; Wang et al., 2010; Nadejde et al., 2013). For the soft source syntactic constraints, we parse the English source side of the parallel data with the English Berkeley Parser (Petrov et al., 2006) and produce composite SAMT-style labels as discussed in Section 6. When extracting syntactic rules, we impose several restrictions for c"
W14-4018,P07-2045,1,0.0133524,"of the rule have matches. • A binary feature that fires if a rule is applied which does not possess any source syntactic label vector with a match of the label for the left-hand side non-terminal. This feature penalizes left-hand side mismatches. • A count feature that for each rule application adds a cost equal to the number of right-hand side non-terminals that do not have a match with a corresponding input label in any of the source syntactic label vectors. This feature penalizes right-hand side mismatches. 3 Specifically, we apply relax-parse - -SAMT 2 as implemented in the Moses toolkit (Koehn et al., 2007). 151 The second and third feature are less strict than the first one and give the system a more detailed clue about the magnitude of mismatch. 6.2 7.1 We work with an English–German parallel training corpus of around 4.5 M sentence pairs (after corpus cleaning). The parallel data originates from three different sources which have been eligible for the constrained track of the ACL 2014 Ninth Workshop on Statistical Machine Translation shared translation task: Europarl (Koehn, 2005), News Commentary, and the Common Crawl corpus as provided on the WMT website. Word alignments are created by alig"
W14-4018,N09-1027,0,0.0680691,"rules in the glue grammar are of the following form: Initial rule: X, Q → h<s> X ∼0 , <s> Q∼0 i Glue rules: X, Q → hX ∼0 X ∼1 , Q∼0 B∼1 i for all B ∈ NE Final rule: X, Q → hX ∼0 </s>, Q∼0 </s>i Top rules: X, Q → h<s> X ∼0 </s>, <s> B ∼0 n hsyn (d) = tˆsyn (d) + ∑ hsyn (d j ) . </s>i In this equation, tˆsyn (d) is a simple auxiliary function: ( logtsyn (d) if tsyn (d) 6= 0 tˆsyn (d) = (2) 0 otherwise for all B ∈ NE 5 Preference Grammars Preference grammars store a set of implicit label vectors as additional information with each SCFG rule, along with their relative frequencies given the rule. Venugopal et al. (2009) have introduced this technique for hierarchical phrase-based translation. The implicit label set refines the label set of the underlying synchronous context-free grammar. We apply this idea to GHKM translation by not decorating the target-side non-terminals of the extracted GHKM rules with syntactic labels, but with a single generic label. The (explicit) target non-terminal vocabulary NE thus also contains only the generic non-terminal symbol X, just like the source non-terminal vocabulary NF . The extraction method remains syntax-directed and is still guided by the syntactic annotation over"
W14-4018,2005.mtsummit-papers.11,1,0.0517676,"-hand side mismatches. 3 Specifically, we apply relax-parse - -SAMT 2 as implemented in the Moses toolkit (Koehn et al., 2007). 151 The second and third feature are less strict than the first one and give the system a more detailed clue about the magnitude of mismatch. 6.2 7.1 We work with an English–German parallel training corpus of around 4.5 M sentence pairs (after corpus cleaning). The parallel data originates from three different sources which have been eligible for the constrained track of the ACL 2014 Ninth Workshop on Statistical Machine Translation shared translation task: Europarl (Koehn, 2005), News Commentary, and the Common Crawl corpus as provided on the WMT website. Word alignments are created by aligning the data in both directions with MGIZA++ (Gao and Vogel, 2008) and symmetrizing the two trained alignments (Och and Ney, 2003; Koehn et al., 2003). The German target side training data is parsed with BitPar (Schmid, 2004). We remove grammatical case and function information from the annotation obtained with BitPar and apply right binarization of the German parse trees prior to rule extraction (Wang et al., 2007; Wang et al., 2010; Nadejde et al., 2013). For the soft source syn"
W14-4018,P08-1114,0,0.0213265,"one correspondence. The left-hand side of the rule is a pair of source and target nonterminals, A ∈ NF and B ∈ NE . Decoding is typically carried out with a parsingbased algorithm, in our case a customized version of CYK+ (Chappelier and Rajman, 1998). The parsing algorithm is extended to handle translation candidates and to incorporate language model scores via cube pruning (Chiang, 2007). this paradigm have recently been among the topranked submissions to public evaluation campaigns (Williams et al., 2014; Bojar et al., 2014). Our soft source syntactic constraints features borrow ideas from Marton and Resnik (2008) who proposed a comparable approach for hierarchical machine translation. The major difference is that the features of Marton and Resnik (2008) are only based on the labels from the input trees as seen in tuning and decoding. They penalize violations of constituent boundaries but do not employ syntactic parse annotation of the source side of the training data. We, in contrast, equip the rules with latent source label properties, allowing for features that can check for conformance of input tree labels and source labels that have been seen in training. Other groups have applied similar techniqu"
W14-4018,D07-1078,0,0.520276,"Missing"
W14-4018,W13-2221,1,0.808679,"hared translation task: Europarl (Koehn, 2005), News Commentary, and the Common Crawl corpus as provided on the WMT website. Word alignments are created by aligning the data in both directions with MGIZA++ (Gao and Vogel, 2008) and symmetrizing the two trained alignments (Och and Ney, 2003; Koehn et al., 2003). The German target side training data is parsed with BitPar (Schmid, 2004). We remove grammatical case and function information from the annotation obtained with BitPar and apply right binarization of the German parse trees prior to rule extraction (Wang et al., 2007; Wang et al., 2010; Nadejde et al., 2013). For the soft source syntactic constraints, we parse the English source side of the parallel data with the English Berkeley Parser (Petrov et al., 2006) and produce composite SAMT-style labels as discussed in Section 6. When extracting syntactic rules, we impose several restrictions for composed rules, in particular a maximum number of 100 tree nodes per rule, a maximum depth of seven, and a maximum size of seven. We discard rules with non-terminals on their right-hand side if they are singletons in the training data. For efficiency reasons, we also enforce a limit on the number of label vect"
W14-4018,J10-2004,0,0.0429484,"chine Translation shared translation task: Europarl (Koehn, 2005), News Commentary, and the Common Crawl corpus as provided on the WMT website. Word alignments are created by aligning the data in both directions with MGIZA++ (Gao and Vogel, 2008) and symmetrizing the two trained alignments (Och and Ney, 2003; Koehn et al., 2003). The German target side training data is parsed with BitPar (Schmid, 2004). We remove grammatical case and function information from the annotation obtained with BitPar and apply right binarization of the German parse trees prior to rule extraction (Wang et al., 2007; Wang et al., 2010; Nadejde et al., 2013). For the soft source syntactic constraints, we parse the English source side of the parallel data with the English Berkeley Parser (Petrov et al., 2006) and produce composite SAMT-style labels as discussed in Section 6. When extracting syntactic rules, we impose several restrictions for composed rules, in particular a maximum number of 100 tree nodes per rule, a maximum depth of seven, and a maximum size of seven. We discard rules with non-terminals on their right-hand side if they are singletons in the training data. For efficiency reasons, we also enforce a limit on t"
W14-4018,J03-1002,0,0.0062763,"out the magnitude of mismatch. 6.2 7.1 We work with an English–German parallel training corpus of around 4.5 M sentence pairs (after corpus cleaning). The parallel data originates from three different sources which have been eligible for the constrained track of the ACL 2014 Ninth Workshop on Statistical Machine Translation shared translation task: Europarl (Koehn, 2005), News Commentary, and the Common Crawl corpus as provided on the WMT website. Word alignments are created by aligning the data in both directions with MGIZA++ (Gao and Vogel, 2008) and symmetrizing the two trained alignments (Och and Ney, 2003; Koehn et al., 2003). The German target side training data is parsed with BitPar (Schmid, 2004). We remove grammatical case and function information from the annotation obtained with BitPar and apply right binarization of the German parse trees prior to rule extraction (Wang et al., 2007; Wang et al., 2010; Nadejde et al., 2013). For the soft source syntactic constraints, we parse the English source side of the parallel data with the English Berkeley Parser (Petrov et al., 2006) and produce composite SAMT-style labels as discussed in Section 6. When extracting syntactic rules, we impose sever"
W14-4018,W12-3150,1,0.830324,"such a core set is specified, then only those sparse features are active that depend on the identity of labels within this set. All sparse features for source labels outside of the core set are inactive. 7 Experimental Setup Experiments We empirically evaluate the effectiveness of preference grammars and soft source syntactic constraints for GHKM translation on the English→German language pair using the standard newstest sets of the Workshop on Statistical Machine Translation (WMT) for testing.4 The experiments are conducted with the open-source Moses implementations of GHKM rule extraction (Williams and Koehn, 2012) and decoding with CYK+ parsing and cube pruning (Hoang et al., 2009). 4 http://www.statmt.org/wmt14/ translation-task.html 152 system GHKM string-to-tree baseline + soft source syntactic constraints + sparse features + sparse features (core = non-composite) + sparse features (core = dev-min-occ100) + sparse features (core = dev-min-occ1000) + hard source syntactic constraints string-to-string (GHKM syntax-directed rule extraction) + preference grammar + soft source syntactic constraints + drop derivations with tsyn (d) = 0 dev B LEU T ER 34.7 47.3 35.1 47.0 35.8 46.5 35.4 46.8 35.6 46.7 35.4"
W14-4018,P02-1040,0,0.0923949,"bability of the tree fragment from which the rule was extracted (Williams et al., 2014). Rule translation probabilities are smoothed via Good-Turing smoothing. The language model (LM) is a large interpolated 5-gram LM with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). The target side of the parallel corpus and the monolingual German News Crawl corpora are employed as training data. We use the SRILM toolkit (Stolcke, 2002) to train the LM and rely on KenLM (Heafield, 2011) for language model scoring during decoding. Model weights are optimized to maximize B LEU (Papineni et al., 2002) with batch MIRA (Cherry and Foster, 2012) on 1000-best lists. We selected 2000 sentences from the newstest20082012 sets as a development set. The selected sentences obtained high sentence-level B LEU scores when being translated with a baseline phrasebased system, and do each contain less than 30 words for more rapid tuning. newstest2013 and newstest2014 are used as unseen test sets. Translation quality is measured in truecase with B LEU and T ER (Snover et al., 2006).5 7.2 7.2.1 Soft Source Syntactic Constraints Adding the three dense soft source syntactic constraints features from Section 6"
W14-4018,W14-3324,1,0.771058,"Missing"
W14-4018,P06-1055,0,0.117397,"y aligning the data in both directions with MGIZA++ (Gao and Vogel, 2008) and symmetrizing the two trained alignments (Och and Ney, 2003; Koehn et al., 2003). The German target side training data is parsed with BitPar (Schmid, 2004). We remove grammatical case and function information from the annotation obtained with BitPar and apply right binarization of the German parse trees prior to rule extraction (Wang et al., 2007; Wang et al., 2010; Nadejde et al., 2013). For the soft source syntactic constraints, we parse the English source side of the parallel data with the English Berkeley Parser (Petrov et al., 2006) and produce composite SAMT-style labels as discussed in Section 6. When extracting syntactic rules, we impose several restrictions for composed rules, in particular a maximum number of 100 tree nodes per rule, a maximum depth of seven, and a maximum size of seven. We discard rules with non-terminals on their right-hand side if they are singletons in the training data. For efficiency reasons, we also enforce a limit on the number of label vectors that are stored as additional properties. Label vectors are only stored if they occur at least as often as the 50th most frequent label vector of the"
W14-4018,D11-1019,0,0.0567444,"features of Marton and Resnik (2008) are only based on the labels from the input trees as seen in tuning and decoding. They penalize violations of constituent boundaries but do not employ syntactic parse annotation of the source side of the training data. We, in contrast, equip the rules with latent source label properties, allowing for features that can check for conformance of input tree labels and source labels that have been seen in training. Other groups have applied similar techniques to a string-to-dependency system (Huang et al., 2013) and—like in our work—a GHKM stringto-tree system (Zhang et al., 2011). Both Huang et al. (2013) and Zhang et al. (2011) store source labels as additional information with the rules. They however investigate somewhat different feature functions than we do. Marton and Resnik (2008) evaluated their method on the NIST Chinese→English and Arabic→English tasks. Huang et al. (2013) and Zhang et al. (2011) present results on the NIST Chinese→English task. We focus our attention on a very different task: English→German. 4 4.1 GHKM String-to-Tree Translation In GHKM string-to-tree translation (Galley et al., 2004; Galley et al., 2006; DeNeefe et al., 2007), rules are ext"
W14-4018,C04-1024,0,0.248432,"ound 4.5 M sentence pairs (after corpus cleaning). The parallel data originates from three different sources which have been eligible for the constrained track of the ACL 2014 Ninth Workshop on Statistical Machine Translation shared translation task: Europarl (Koehn, 2005), News Commentary, and the Common Crawl corpus as provided on the WMT website. Word alignments are created by aligning the data in both directions with MGIZA++ (Gao and Vogel, 2008) and symmetrizing the two trained alignments (Och and Ney, 2003; Koehn et al., 2003). The German target side training data is parsed with BitPar (Schmid, 2004). We remove grammatical case and function information from the annotation obtained with BitPar and apply right binarization of the German parse trees prior to rule extraction (Wang et al., 2007; Wang et al., 2010; Nadejde et al., 2013). For the soft source syntactic constraints, we parse the English source side of the parallel data with the English Berkeley Parser (Petrov et al., 2006) and produce composite SAMT-style labels as discussed in Section 6. When extracting syntactic rules, we impose several restrictions for composed rules, in particular a maximum number of 100 tree nodes per rule, a"
W14-4018,W06-3119,0,0.0454667,"of the stringto-tree system remains untouched. The target nonterminals of the SCFG stay syntactified, and the source non-terminal vocabulary is not extended beyond the single generic non-terminal. Source-side syntactic labels are an additional latent property of the rules. We obtain this property by parsing the source side of the training data and collecting the source labels that cover the sourceside span of non-terminals during GHKM rule extraction. As the source-side span is frequently not covered by a constituent in the syntactic parse tree, we employ the composite symbols as suggested by Zollmann and Venugopal (2006) for the SAMT system.3 In cases where a span is still not covered by a symbol, we nevertheless memorize a sourceside syntactic label vector but indicate the failure for the uncovered non-terminal with a special label. The set of source label vectors that are seen with a rule during extraction is stored with it in the rule table as an additional property. This information can be used to implement feature-based soft source syntactic constraints. Table 1 shows an example of a set of source label vectors stored with a grammar rule. The first element of each vector is an implicit sourcesyntactic la"
W14-4018,2006.amta-papers.25,0,0.0274879,"nd rely on KenLM (Heafield, 2011) for language model scoring during decoding. Model weights are optimized to maximize B LEU (Papineni et al., 2002) with batch MIRA (Cherry and Foster, 2012) on 1000-best lists. We selected 2000 sentences from the newstest20082012 sets as a development set. The selected sentences obtained high sentence-level B LEU scores when being translated with a baseline phrasebased system, and do each contain less than 30 words for more rapid tuning. newstest2013 and newstest2014 are used as unseen test sets. Translation quality is measured in truecase with B LEU and T ER (Snover et al., 2006).5 7.2 7.2.1 Soft Source Syntactic Constraints Adding the three dense soft source syntactic constraints features from Section 6.1 improves the baseline scores by 0.3 points B LEU and 0.6 points T ER on newstest2013 and by 0.3 points B LEU and 0.7 points T ER on newstest2014. Somewhat surprisingly, the sparse features from Section 6.2 do not boost translation quality further on any of the two test sets. We observe a considerable improvement on the development set, but it does not carry over to the test sets. We attributed this to an overfitting effect. Our source-side soft syntactic label set o"
W14-4018,2010.amta-papers.8,0,0.0347156,"inally, the function th (Y |d) is defined as th (Y |d) = Feature Computation ∑ Two features are added to the log-linear model combination in order to rate the syntactic wellformedness of derivations. The first feature is similar to the one suggested by Venugopal et al. (2009) and computes a score based on the relative frequencies of implicit label vectors of those rules which are involved in the derivation. The second s∈Sn+1 :s[1]=Y n+1 ! p(s|r) · ∏ ph (s[k]|dk−1 ) k=2 . (5) Note that the denominator in Equation (4) thus equals tsyn (d). 2 Our notational conventions roughly follow the ones by Stein et al. (2010). 150 source label vector (IN+NP, NN, NN) (IN+NP, NNP, NNP) (IN++NP, NNS, NNS) (IN+NP, NP, NP) (PP//SBAR, NP, NP) This concludes the formal specification of the first features. The second feature hauxSyn (d) penalizes rule applications in cases where tsyn (d) evaluates to 0: ( 0 if tsyn (d) 6= 0 hauxSyn (d) = (6) 1 otherwise Table 1: The set of source label vectors (along with their frequencies in the training data) for the rule X, PP-MO → hbetween X ∼1 and X ∼0 , zwischen NN ∼0 und NN ∼1 i. The overall rule frequency is 15. Its intuition is that rule applications that do not contribute to hsy"
W14-4018,N04-1035,0,\N,Missing
W14-4018,W14-3302,1,\N,Missing
W15-3001,W05-0909,0,0.0473932,"asks 1 and 2 provide the same dataset with English-Spanish translations generated by the statistical machine translation (SMT) system, while Task 3 provides two different datasets, for two language pairs: English-German (EN-DE) and German-English (DE-EN) translations taken from all participating systems in WMT13 (Bojar et al., 2013). These datasets were annotated with different labels for quality: for Tasks 1 and 2, the labels were automatically derived from the post-editing of the machine translation output, while for Task 3, scores were computed based on reference translations using Meteor (Banerjee and Lavie, 2005). Any external resource, including additional quality estimation training data, could be used by participants (no distinction between open and close tracks was made). As presented in Section 4.1, participants were also provided with a baseline set of features for each task, and a software package to extract these and other quality estimation features and perform model learning, with suggested methods for all levels of prediction. Participants, described in Section 4.2, could submit up to two systems for each task. Data used to build MT systems or internal system information (such as model scor"
W15-3001,2011.mtsummit-papers.35,0,0.348896,"Missing"
W15-3001,W13-2241,1,0.851171,"Missing"
W15-3001,P13-2097,1,0.801912,"Missing"
W15-3001,W13-2242,0,0.0386206,"Missing"
W15-3001,W14-3339,0,0.0452066,"Missing"
W15-3001,W15-3035,0,0.054515,"Missing"
W15-3001,W11-2103,1,0.524514,"Missing"
W15-3001,W13-2201,1,0.499374,"Missing"
W15-3001,W14-3302,1,0.498006,"Missing"
W15-3001,W14-3340,1,0.54686,"Missing"
W15-3001,W15-3007,0,0.0166876,"Missing"
W15-3001,W15-3006,1,0.827804,"Missing"
W15-3001,W07-0718,1,0.664541,"om 24 institutions were submitted to the ten translation directions in the standard translation task. An additional 7 anonymized systems were included, and were then evaluated both automatically and manually. The quality estimation task had three subtasks, with a total of 10 teams, submitting 34 entries. The pilot automatic postediting task had a total of 4 teams, submitting 7 entries. 1 Introduction We present the results of the shared tasks of the Workshop on Statistical Machine Translation (WMT) held at EMNLP 2015. This workshop builds on eight previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014). This year we conducted five official tasks: a translation task, a quality estimation task, a metrics task, a tuning task1 , and a automatic postediting task. In the translation task (§2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data. We held ten translation tasks this year, between English and each of Czech, French, German, Finnish, and Russian. The Finnish translation 1 The metrics and tuning tasks are reported in separate papers (Stanojevi´c et al., 2015a,b)."
W15-3001,2014.amta-researchers.13,0,0.0200349,"Missing"
W15-3001,W08-0309,1,0.406319,"2. machine translation and automatic evaluation or prediction of translation quality. 2 Overview of the Translation Task The recurring task of the workshop examines translation between English and other languages. As in the previous years, the other languages include German, French, Czech and Russian. Finnish replaced Hindi as the special language this year. Finnish is a lesser resourced language compared to the other languages and has challenging morphological properties. Finnish represents also a different language family that we had not tackled since we included Hungarian in 2008 and 2009 (Callison-Burch et al., 2008, 2009). We created a test set for each language pair by translating newspaper articles and provided training data, except for French, where the test set was drawn from user-generated comments on the news articles. 2.1 2.3 We received 68 submissions from 24 institutions. The participating institutions and their entry names are listed in Table 2; each system did not necessarily appear in all translation tasks. We also included 1 commercial off-the-shelf MT system and 6 online statistical MT systems, which we anonymized. For presentation of the results, systems are treated as either constrained"
W15-3001,W15-3025,1,0.914094,"teps aimed at removing duplicates and those triplets in which any of the elements (source, target, post-edition) was either too long or too short compared to the others, or included tags or special problematic symbols. The main reason for random sampling was to induce some homogeneity across the three datasets and, in turn, Automatic Post-editing Task This year WMT hosted for the first time a shared task on automatic post-editing (APE) for machine translation. The task requires to automatically correct the errors present in a machine translated text. As pointed out in Parton et al. (2012) and Chatterjee et al. (2015b), from the application point of view, APE components would make it possible to: • Improve MT output by exploiting information unavailable to the decoder, or by per22 The original triplets were provided by Unbabel (https: //unbabel.com/). 28 in terms of Translation Error Rate (TER) (Snover et al., 2006a), an evaluation metric commonly used in MT-related tasks (e.g. in quality estimation) to measure the minimum edit distance between an automatic translation and a reference translation.23 Systems are ranked based on the average TER calculated on the test set by using the TERcom24 software: lowe"
W15-3001,W10-1703,1,0.163282,"Missing"
W15-3001,P15-2026,1,0.797338,"teps aimed at removing duplicates and those triplets in which any of the elements (source, target, post-edition) was either too long or too short compared to the others, or included tags or special problematic symbols. The main reason for random sampling was to induce some homogeneity across the three datasets and, in turn, Automatic Post-editing Task This year WMT hosted for the first time a shared task on automatic post-editing (APE) for machine translation. The task requires to automatically correct the errors present in a machine translated text. As pointed out in Parton et al. (2012) and Chatterjee et al. (2015b), from the application point of view, APE components would make it possible to: • Improve MT output by exploiting information unavailable to the decoder, or by per22 The original triplets were provided by Unbabel (https: //unbabel.com/). 28 in terms of Translation Error Rate (TER) (Snover et al., 2006a), an evaluation metric commonly used in MT-related tasks (e.g. in quality estimation) to measure the minimum edit distance between an automatic translation and a reference translation.23 Systems are ranked based on the average TER calculated on the test set by using the TERcom24 software: lowe"
W15-3001,W12-3102,1,0.571688,"ator agreement, both for inter- and intra-annotator agreement scores. not included to make the graphs viewable). The plots cleary suggest that a fair comparison of systems of different kinds cannot rely on automatic scores. Rule-based systems receive a much lower BLEU score than statistical systems (see for instance English–German, e.g., PROMT- RULE). The same is true to a lesser degree for statistical syntax-based systems (see English–German, UEDIN - SYNTAX ) and online systems that were not tuned to the shared task (see Czech–English, CU TECTO vs. the cluster of tuning task systems TT*). 4 (Callison-Burch et al., 2012; Bojar et al., 2013, 2014), with tasks including both sentence and word-level estimation, using new training and test datasets, and an additional task: document-level prediction. The goals of this year’s shared task were: • Advance work on sentence- and wordlevel quality estimation by providing larger datasets. • Investigate the effectiveness of quality labels, features and learning methods for documentlevel prediction. Quality Estimation Task • Explore differences between sentence-level and document-level prediction. The fourth edition of the WMT shared task on quality estimation (QE) of mac"
W15-3001,W15-3008,0,0.0135172,"secondary key. The results for the scoring variant are presented in Table 9, sorted from best to worst by using the MAE metric scores as primary key and the RMSE metric scores as secondary key. Pearson’s r coefficients for all systems against HTER is given in Table 10. As discussed in (Graham, 2015), the results according to this metric can rank participating systems differently. In particular, we note the SHEF/GP submission, are which is deemed significantly worse than the baseline system according to MAE, but substantially better than the baseline according to Pearson’s correlation. Graham (2015) argues that the use of MAE as evaluation score for quality estimation tasks is inadequate, as MAE is very sensitive to variance. This means that a system that outputs predictions with high variance is more likely to have high MAE score, even if the distribution follows that of the true labels. Interestingly, according to Pearson’s correlation, the systems are • Source sentence (English). • Automatic translation (Spanish). • Manual post-edition of the automatic translation. • Word-level binary (“OK”/“BAD”) labelling of the automatic translation. The binary labels for the datasets were acquired"
W15-3001,W09-0401,1,0.251315,"Missing"
W15-3001,W15-3009,0,0.0460438,"Missing"
W15-3001,W15-3010,0,0.035772,"Missing"
W15-3001,P10-4002,0,0.00861255,"t sentences and number of punctuation marks in source and target sentences. All other features were averaged. The implementation for document-level feature extraction is available in Q U E ST ++ (Specia et al., 2015).12 These features were then used to train a SVR algorithm with RBF kernel using the SCIKIT- LEARN toolkit. The γ,  and C parameters were optimised via grid search with 5-fold cross validation on the training set. • Source token aligned to the target token, its left and right contexts of one word. The alignments were produced with the force align.py script, which is part of cdec (Dyer et al., 2010). It allows to align new parallel data with a pre-trained alignment model built with the cdec word aligner (fast align). The alignment model was trained on the Europarl corpus (Koehn, 2005). • Boolean dictionary features: whether target token is a stopword, a punctuation mark, a proper noun, a number. 4.2 Participants Table 7 lists all participating teams submitting systems to any of the tasks. Each team was allowed up to two submissions for each task and language pair. In the descriptions below, participation in specific tasks is denoted by a task identifier. • Target language model features:"
W15-3001,W15-3011,0,0.0373266,"Missing"
W15-3001,W15-3036,0,0.0738999,"Missing"
W15-3001,W15-3012,0,0.0168435,"secondary key. The results for the scoring variant are presented in Table 9, sorted from best to worst by using the MAE metric scores as primary key and the RMSE metric scores as secondary key. Pearson’s r coefficients for all systems against HTER is given in Table 10. As discussed in (Graham, 2015), the results according to this metric can rank participating systems differently. In particular, we note the SHEF/GP submission, are which is deemed significantly worse than the baseline system according to MAE, but substantially better than the baseline according to Pearson’s correlation. Graham (2015) argues that the use of MAE as evaluation score for quality estimation tasks is inadequate, as MAE is very sensitive to variance. This means that a system that outputs predictions with high variance is more likely to have high MAE score, even if the distribution follows that of the true labels. Interestingly, according to Pearson’s correlation, the systems are • Source sentence (English). • Automatic translation (Spanish). • Manual post-edition of the automatic translation. • Word-level binary (“OK”/“BAD”) labelling of the automatic translation. The binary labels for the datasets were acquired"
W15-3001,W15-4903,0,0.0623526,"Missing"
W15-3001,W15-3013,1,0.843414,"Missing"
W15-3001,W11-2123,0,0.0240468,"27,101 5,966 8,816 SRC 13,701 3,765 5,307 Lemmas TGT PE 7,624 7,689 2,810 2,819 3,778 3,814 Table 18: Data statistics. and classifying each word of a sentence as good or bad. An automatic translation to be post-edited is first decoded by our SPE system, then fed into one of the classifiers identified as SVM180feat and RNN. The HTER estimator selects the translation with the lower score while the binary word-level classifier selects the translation with the fewer amount of bad tags. The official evaluation of the shared task show an advantage of the RNN approach compared to SVM. KenLM toolkit (Heafield, 2011) for standard ngram modeling with an n-gram length of 5. Finally, the APE system was tuned on the development set, optimizing TER with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison, computed for both evaluation modalities (case sensitive/insensitive), are also reported in Tables 20 and 21. For each submitted run, the statistical significance of performance differences with respect to the baseline and the re-implementation of Simard et al. (2007) is calculated with the bootstrap test (Koehn, 2004). 5.2 FBK. The two runs submitted by FBK (Chatterjee e"
W15-3001,W08-0509,0,0.0268564,"ere collected by means of a crowdsourcing platform developed by the data provider. Monolingual translation as another term of comparison. To get further insights about the progress with respect to previous APE methods, participants’ results are also analysed with respect to another term of comparison: a reimplementation of the state-of-the-art approach firstly proposed by Simard et al. (2007).27 For this purpose, a phrase-based SMT system based on Moses (Koehn et al., 2007) is used. Translation and reordering models were estimated following the Moses protocol with default setup using MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the Test data (1, 817 instances) consists of (source, target) pairs having similar characteristics of those in the training set. Human post-editions of the test target instances were left apart to measure system performance. The data creation procedure adopted, as well as the origin and the domain of the texts pose specific challenges to the participating systems. As discussed in Section 5.4, the results of this pilot task can be partially explained in light of such challenges. This dataset, however, has three major advantages that made it sui"
W15-3001,W15-3014,0,0.0344469,"Missing"
W15-3001,P15-1174,0,0.0105686,"415 for test. Since no human annotation exists for the quality of entire paragraphs (or documents), Meteor against reference translations was used as quality label for this task. Meteor was calculated using its implementation within the Asyia toolkit, with the following settings: exact match, tokenised and case insensitive (Gim´enez and M`arquez, 2010). guage pairs. All systems were significantly better than the baseline. However, the difference between the baseline system and all submissions was much lower in the scoring evaluation than in the ranking evaluation. Following the suggestion in (Graham, 2015), Table 16 shows an alternative ranking of systems considering Pearson’s r correlation results. The alternative ranking differs from the official ranking in terms of MAE: for EN-DE, RTMDCU/RTM-FS-SVR is no longer in the winning group, while for DE-EN, USHEF/QUEST-DISCBO and USAAR-USHEF/BFF did not show statistically significant difference against the baseline. However, as with Task 1 these results are the same as the official ones in terms of DeltaAvg. 4.6 Discussion In what follows, we discuss the main findings of this year’s shared task based on the goals we had previously identified for it."
W15-3001,W04-3250,1,0.485885,"f the RNN approach compared to SVM. KenLM toolkit (Heafield, 2011) for standard ngram modeling with an n-gram length of 5. Finally, the APE system was tuned on the development set, optimizing TER with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison, computed for both evaluation modalities (case sensitive/insensitive), are also reported in Tables 20 and 21. For each submitted run, the statistical significance of performance differences with respect to the baseline and the re-implementation of Simard et al. (2007) is calculated with the bootstrap test (Koehn, 2004). 5.2 FBK. The two runs submitted by FBK (Chatterjee et al., 2015a) are based on combining the statistical phrase-based post-editing approach proposed by Simard et al. (2007) and its most significant variant proposed by B´echara et al. (2011). The APE systems are built-in an incremental manner. At each stage of the APE pipeline, the best configuration of a component is decided and then used in the next stage. The APE pipeline begins with the selection of the best language model from several language models trained on different types and quantities of data. The next stage addresses the possible"
W15-3001,2005.mtsummit-papers.11,1,0.0759255,"(Specia et al., 2015).12 These features were then used to train a SVR algorithm with RBF kernel using the SCIKIT- LEARN toolkit. The γ,  and C parameters were optimised via grid search with 5-fold cross validation on the training set. • Source token aligned to the target token, its left and right contexts of one word. The alignments were produced with the force align.py script, which is part of cdec (Dyer et al., 2010). It allows to align new parallel data with a pre-trained alignment model built with the cdec word aligner (fast align). The alignment model was trained on the Europarl corpus (Koehn, 2005). • Boolean dictionary features: whether target token is a stopword, a punctuation mark, a proper noun, a number. 4.2 Participants Table 7 lists all participating teams submitting systems to any of the tasks. Each team was allowed up to two submissions for each task and language pair. In the descriptions below, participation in specific tasks is denoted by a task identifier. • Target language model features: – The order of the highest order n-gram which starts or ends with the target token. – Backoff behaviour of the n-grams (ti−2 , ti−1 , ti ), (ti−1 , ti , ti+1 ), (ti , ti+1 , ti+2 ), where"
W15-3001,W15-3039,1,0.80659,"Missing"
W15-3001,J10-4005,0,0.0619684,"Missing"
W15-3001,J82-2005,0,0.818658,"Missing"
W15-3001,W14-3342,0,0.0353204,"t although the system is referred to as “baseline”, it is in fact a strong system. It has proved robust across a range of language pairs, MT systems, and text domains for predicting various forms of post-editing effort (Callison-Burch et al., 2012; Bojar et al., 2013, 2014). Word-level baseline system: For Task 2, the baseline features were extracted with the M AR MOT tool9 . For the baseline system we used a number of features that have been found the most informative in previous research on word-level quality estimation. Our baseline set of features is loosely based on the one described in (Luong et al., 2014). It contains the following 25 features: Baseline systems Sentence-level baseline system: For Task 1, Q U E ST7 (Specia et al., 2013) was used to extract 17 MT system-independent features from the source and translation (target) files and parallel corpora: • Word count in the source and target sentences, source and target token count ratio. Although these features are sentence-level (i.e. their values will be the same for all words in a sentence), but the length of a sentence might influence the probability of a word being incorrect. • Number of tokens in the source and target sentences. • Ave"
W15-3001,W13-2248,0,0.0824189,"Missing"
W15-3001,W06-3114,1,0.427665,"translation systems from 24 institutions were submitted to the ten translation directions in the standard translation task. An additional 7 anonymized systems were included, and were then evaluated both automatically and manually. The quality estimation task had three subtasks, with a total of 10 teams, submitting 34 entries. The pilot automatic postediting task had a total of 4 teams, submitting 7 entries. 1 Introduction We present the results of the shared tasks of the Workshop on Statistical Machine Translation (WMT) held at EMNLP 2015. This workshop builds on eight previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014). This year we conducted five official tasks: a translation task, a quality estimation task, a metrics task, a tuning task1 , and a automatic postediting task. In the translation task (§2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data. We held ten translation tasks this year, between English and each of Czech, French, German, Finnish, and Russian. The Finnish translation 1 The metrics and tuning tasks are reported in separate papers (S"
W15-3001,W15-3015,0,0.0443946,"Missing"
W15-3001,W15-3016,0,0.0442638,"Missing"
W15-3001,W15-3037,0,0.0800739,"Missing"
W15-3001,P03-1021,0,0.0587969,"utomatic translation to be post-edited is first decoded by our SPE system, then fed into one of the classifiers identified as SVM180feat and RNN. The HTER estimator selects the translation with the lower score while the binary word-level classifier selects the translation with the fewer amount of bad tags. The official evaluation of the shared task show an advantage of the RNN approach compared to SVM. KenLM toolkit (Heafield, 2011) for standard ngram modeling with an n-gram length of 5. Finally, the APE system was tuned on the development set, optimizing TER with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison, computed for both evaluation modalities (case sensitive/insensitive), are also reported in Tables 20 and 21. For each submitted run, the statistical significance of performance differences with respect to the baseline and the re-implementation of Simard et al. (2007) is calculated with the bootstrap test (Koehn, 2004). 5.2 FBK. The two runs submitted by FBK (Chatterjee et al., 2015a) are based on combining the statistical phrase-based post-editing approach proposed by Simard et al. (2007) and its most significant variant proposed by B´echara"
W15-3001,padro-stanilovsky-2012-freeling,0,0.011024,"o rankings are not identical, none of the systems was particularly penalized by the case sensitive evaluation. Indeed, individual differences in the two modes are always close to the same value (∼ 0.7 TER difference) measured for the two baselines. USAAR-SAPE. The USAAR-SAPE system (Pal et al., 2015b) is designed with three basic components: corpus preprocessing, hybrid word alignment and a state-of-the-art phrase-based SMT system integrated with the hybrid word alignment. The preprocessing of the training corpus is carried out by stemming the Spanish MT output and the PE data using Freeling (Padr and Stanilovsky, 2012). The hybrid word alignment method combines different kinds of word alignment: GIZA++ word alignment with the 31 ID Baseline FBK Primary LIMSI Primary USAAR-SAPE LIMSI Contrastive Abu-MaTran Primary FBK Contrastive (Simard et al., 2007) Abu-MaTran Contrastive Avg. TER 22.913 23.228 23.331 23.426 23.573 23.639 23.649 23.839 24.715 ID Baseline LIMSI Primary FBK Primary USAAR-SAPE Abu-MaTran Primary LIMSI Contrastive FBK Contrastive (Simard et al., 2007) Abu-MaTran Contrastive Table 20: Official results for the WMT15 Automatic Post-editing task – average TER (↓) case sensitive. Table 21: Official"
W15-3001,W15-3038,0,0.0668224,"Missing"
W15-3001,W13-2814,0,0.0155219,"ject (Prompsit) Fondazione Bruno Kessler, Italy (Chatterjee et al., 2015a) Laboratoire d’Informatique pour la M´ecanique et les Sciences de l’Ing´enieur, France (Wisniewski et al., 2015) Saarland University, Germany & Jadavpur University, India (Pal et al., 2015b) Table 19: Participants in the WMT15 Automatic Post-editing pilot task. grow-diag-final-and (GDFA) heuristic (Koehn, 2010), SymGiza++ (Junczys-Dowmunt and Szal, 2011), the Berkeley aligner (Liang et al., 2006), and the edit distance-based aligners (Snover et al., 2006a; Lavie and Agarwal, 2007). These different word alignment tables (Pal et al., 2013) are combined by a mathematical union method. For the phrase-based SMT system various maximum phrase lengths for the translation model and n–gram settings for the language model are used. The best results in terms of BLEU (Papineni et al., 2002) score are achieved by a maximum phrase length of 7 for the translation model and a 5-gram language model. the model. These features measure the similarity and the reliability of the translation options and help to improve the precision of the resulting APE system. LIMSI. For the first edition of the APE shared task LIMSI submitted two systems (Wisniews"
W15-3001,W07-0734,0,0.0277649,"Abu-MaTran FBK LIMSI USAAR-SAPE Participating team Abu-MaTran Project (Prompsit) Fondazione Bruno Kessler, Italy (Chatterjee et al., 2015a) Laboratoire d’Informatique pour la M´ecanique et les Sciences de l’Ing´enieur, France (Wisniewski et al., 2015) Saarland University, Germany & Jadavpur University, India (Pal et al., 2015b) Table 19: Participants in the WMT15 Automatic Post-editing pilot task. grow-diag-final-and (GDFA) heuristic (Koehn, 2010), SymGiza++ (Junczys-Dowmunt and Szal, 2011), the Berkeley aligner (Liang et al., 2006), and the edit distance-based aligners (Snover et al., 2006a; Lavie and Agarwal, 2007). These different word alignment tables (Pal et al., 2013) are combined by a mathematical union method. For the phrase-based SMT system various maximum phrase lengths for the translation model and n–gram settings for the language model are used. The best results in terms of BLEU (Papineni et al., 2002) score are achieved by a maximum phrase length of 7 for the translation model and a 5-gram language model. the model. These features measure the similarity and the reliability of the translation options and help to improve the precision of the resulting APE system. LIMSI. For the first edition of"
W15-3001,W15-3017,0,0.0328586,"Missing"
W15-3001,W15-3026,0,0.0488223,"Missing"
W15-3001,W15-3040,1,0.889327,"HIDDEN Participating team Dublin City University, Ireland and University of Sheffield, UK (Logacheva et al., 2015) Heidelberg University, Germany (Kreutzer et al., 2015) Lorraine Laboratory of Research in Computer Science and its Applications, France (Langlois, 2015) Dublin City University, Ireland (Bicici et al., 2015) Shenyang Aerospace University, China (Shang et al., 2015) University of Sheffield Team 1, UK (Shah et al., 2015) Alicant University, Spain (Espl`a-Gomis et al., 2015a) Ghent University, Belgium (Tezcan et al., 2015) University of Sheffield, UK and Saarland University, Germany (Scarton et al., 2015a) University of Sheffield, UK (Scarton et al., 2015a) Undisclosed Table 7: Participants in the WMT15 quality estimation shared task. one from the official training data. Pseudoreferences were produced by three online systems. These features measure the intersection between n-gram sets of the target sentence and of the pseudo-references. Three sets of features were extracted from each online system, and a fourth feature was extracted measuring the inter-agreement among the three online systems and the target system. and fine-tuned for the quality estimation classification task by back-propagat"
W15-3001,W15-4916,1,0.80858,"Missing"
W15-3001,P02-1040,0,0.108957,"ndia (Pal et al., 2015b) Table 19: Participants in the WMT15 Automatic Post-editing pilot task. grow-diag-final-and (GDFA) heuristic (Koehn, 2010), SymGiza++ (Junczys-Dowmunt and Szal, 2011), the Berkeley aligner (Liang et al., 2006), and the edit distance-based aligners (Snover et al., 2006a; Lavie and Agarwal, 2007). These different word alignment tables (Pal et al., 2013) are combined by a mathematical union method. For the phrase-based SMT system various maximum phrase lengths for the translation model and n–gram settings for the language model are used. The best results in terms of BLEU (Papineni et al., 2002) score are achieved by a maximum phrase length of 7 for the translation model and a 5-gram language model. the model. These features measure the similarity and the reliability of the translation options and help to improve the precision of the resulting APE system. LIMSI. For the first edition of the APE shared task LIMSI submitted two systems (Wisniewski et al., 2015). The first one is based on the approach of Simard et al. (2007) and considers the APE task as a monolingual translation between a translation hypothesis and its post-edition. This straightforward approach does not succeed in imp"
W15-3001,2012.eamt-1.34,0,0.0799883,"Missing"
W15-3001,W15-3023,0,0.0267499,"Missing"
W15-3001,W15-3018,0,0.0412931,"Missing"
W15-3001,W15-3041,1,0.847113,"Missing"
W15-3001,potet-etal-2012-collection,0,0.011237,"Missing"
W15-3001,W15-3042,0,0.0858502,"Missing"
W15-3001,W15-3019,0,0.0362545,"Missing"
W15-3001,N07-1064,0,0.644928,"nsitive) are reported in Tables 20 and 21. • The target (TGT) is a tokenized Spanish translation of the source, produced by an unknown MT system; • The human post-edition (PE) is a manuallyrevised version of the target. PEs were collected by means of a crowdsourcing platform developed by the data provider. Monolingual translation as another term of comparison. To get further insights about the progress with respect to previous APE methods, participants’ results are also analysed with respect to another term of comparison: a reimplementation of the state-of-the-art approach firstly proposed by Simard et al. (2007).27 For this purpose, a phrase-based SMT system based on Moses (Koehn et al., 2007) is used. Translation and reordering models were estimated following the Moses protocol with default setup using MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the Test data (1, 817 instances) consists of (source, target) pairs having similar characteristics of those in the training set. Human post-editions of the test target instances were left apart to measure system performance. The data creation procedure adopted, as well as the origin and the domain of the texts pose specifi"
W15-3001,W15-3022,0,0.0629217,"Missing"
W15-3001,P15-4020,1,0.0689693,"://github.com/lspecia/quest 14 http://scikit-learn.org/ https://github.com/qe-team/marmot • Target token, its left and right contexts of one word. Document-level baseline system: For Task 3, the baseline features for sentence-level prediction were used. These are aggregated by summing or averaging their values for the entire document. Features that were summed: number of tokens in the source and target sentences and number of punctuation marks in source and target sentences. All other features were averaged. The implementation for document-level feature extraction is available in Q U E ST ++ (Specia et al., 2015).12 These features were then used to train a SVR algorithm with RBF kernel using the SCIKIT- LEARN toolkit. The γ,  and C parameters were optimised via grid search with 5-fold cross validation on the training set. • Source token aligned to the target token, its left and right contexts of one word. The alignments were produced with the force align.py script, which is part of cdec (Dyer et al., 2010). It allows to align new parallel data with a pre-trained alignment model built with the cdec word aligner (fast align). The alignment model was trained on the Europarl corpus (Koehn, 2005). • Boole"
W15-3001,W15-3027,0,0.0625354,"Missing"
W15-3001,P13-4014,1,0.762876,"Missing"
W15-3001,2013.mtsummit-papers.15,0,0.055239,"these rules is based on an analysis of the most frequent error corrections and aims at: i) predicting word case; ii) predicting exclamation and interrogation marks; and iii) predicting verbal endings. Experiments with this approach show that this system also hurts translation quality. An in-depth analysis revealed that this negative result is mainly explained by two reasons: i) most of the post-edition operations are nearly unique, which makes very difficult to generalize from a small amount of data; and ii) even when they are not, the high variability of post-editing, already pointed out by Wisniewski et al. (2013), results in predicting legitimate corrections that have not been made by the annotators, therefore preventing from improving over the baseline. 5.3 Results The official results achieved by the participating systems are reported in Tables 20 and 21. The seven runs submitted are sorted based on the average TER they achieve on test data. Table 20 shows the results computed in case sensitive mode, while Table 21 provides scores computed in the case insensitive mode. Both rankings reveal an unexpected outcome: none of the submitted runs was able to beat the baselines (i.e. average TER scores of 22"
W15-3001,W15-3032,1,0.810291,"Missing"
W15-3001,W15-3031,1,0.376914,"Missing"
W15-3001,W15-3020,1,0.808362,"Missing"
W15-3001,W15-3043,0,0.0443554,"Missing"
W15-3001,W15-3021,0,0.038175,"Missing"
W15-3001,P07-2045,1,\N,Missing
W15-3001,W15-3004,0,\N,Missing
W15-3001,2015.eamt-1.4,0,\N,Missing
W15-3001,N06-1014,0,\N,Missing
W15-3001,2015.eamt-1.17,1,\N,Missing
W15-3001,2012.iwslt-papers.12,0,\N,Missing
W15-3013,D14-1132,0,0.0417768,"igated sparse lexicalized reordering features (Section 2.4) on the German-English language pair in both translation directions. Two methods for learning the weights of the sparse lexicalized reordering feature set have been compared: (1.) direct tuning in MIRA along with all other features in the model combination (sparse LR (MIRA)), and (2.) separate optimization with stochastic gradient descent (SGD) with a maximum expected B LEU objective (sparse LR (SGD)). For the latter variant, we used the MT tuning set for training (13 573 sentence pairs) and otherwise followed the approach outlined by Auli et al. (2014). We tuned the baseline feature weights with MIRA before SGD training and ran two final MIRA iterations after it. SGD training was stopped after 80 epochs. Empirical results for the German-English language pair are presented in Table 5. We observe minor gains of up to +0.2 points B LEU. The results are not consistent in the two translation directions: The MIRA-trained variant seems to perform better when translating from German, the SGD-trained variant when translating to German. However, in both cases the baseline score is almost identical to the best results with sparse lexicalized reorderin"
W15-3013,D11-1033,0,0.0915467,"Missing"
W15-3013,N12-1047,0,0.185989,"Missing"
W15-3013,N13-1003,0,0.0324335,"a to train 5gram language models with modified Kneser-Ney smoothing (Chen and Goodman, 1998). Typically, language models for each monolingual corpus were first trained using either KenLM (Heafield et al., 2013) or the SRILM toolkit (Stolcke, 2002) and then linearly interpolated using weights tuned to minimize perplexity on the development set. 3.4 Sparse Lexicalized Reordering Baseline Features We follow the standard approach to SMT of scoring translation hypotheses using a weighted linear combination of features. The core features of our We implemented sparse lexicalized reordering features (Cherry, 2013) in Moses and evaluated 127 Baseline (no clusters) Comprehensive setup w/o sparse features w/o language model w/o reordering model w/o operation sequence model de-en 28.0 28.5 (+.5) 28.2 (–.3) 28.3 (–.2) 28.5 (±.0) 28.3 (–.2) en-de 20.5 20.5 (±.0) 20.4 (–.1) 20.5 (±.0) 20.5 (±.0) 20.3 (–.1) cs-en 29.1 29.7 (+.6) 29.6 (–.1) 29.5 (–.2) 29.7 (±.0) en-cs 21.2 21.8 (+.6) 21.7 (–.1) 21.4 (–.4) 21.8 (±.0) 21.7 (–.1) ru-en 31.8 32.3 (+.5) 32.2 (–.1) 31.5 (–.8) 32.3 (±.0) 32.0 (–.3) en-ru 29.1 29.7 (+.6) 30.0 (+.3) 29.2 (–.6) 29.8 (+.1) 29.5 (–.2) avg ∆ +.5 –.2 –.4 ±.0 –.2 Table 1: Use of additional fe"
W15-3013,P05-1066,1,0.679903,"the OPUS (Tiedemann, 2012) par129 4.3 System Baseline Submitted BiLM source & combined & NPLM Czech↔English The development of the Czech↔English systems followed the ideas in Section 2.3, i.e., with a focus on word classes (50, 200, 600 classes) for all component models. We combined the test sets from 2008 to 2012 for tuning. No neural language model or bilingual language model was used. 4.4 Table 4: Experimental results (cased B LEU) for English→Russian averaged over newstest2013 and newstest2014. Russian↔English From German. For translation from German, we applied syntactic pre-reordering (Collins et al., 2005) and compound splitting (Koehn and Knight, 2003) in a preprocessing step on the source side. A rich set of translation factors was exploited in addition to word surface forms: Och clusters (50 classes), morphological tags, partof-speech tags, and word stems on the German side (Schmid, 2000), as well as Och clusters (50 classes), part-of-speech tags (Ratnaparkhi, 1996), and word stems (Porter, 1980) on the English side. The factors were utilized in the translation model and in OSMs. The lexicalized reordering model was trained on stems. Individual 7gram Och cluster LMs were trained with KenLM’s"
W15-3013,2014.iwslt-evaluation.7,1,0.858337,"mission we tested bilingual LMs on the French↔English tasks and on English→Russian task. For French↔English, we had resource issues4 in training such large Introduction The Edinburgh/JHU phrase-based translation systems for our participation in the WMT 2015 shared translation task1 are based on the open source Moses toolkit (Koehn et al., 2007). We built upon Edinburgh’s strong baselines from WMT submissions in previous years (Durrani et al., 2014a) as well as our recent research within the framework of other evaluation campaigns and projects such as IWSLT2 and EU-BRIDGE3 (Birch et al., 2014; Freitag et al., 2014a; Freitag et al., 2014b). We first discuss novel features that we integrated into our systems for the 2015 Edinburgh/JHU submission. Next we give a general system overview with details on our training pipeline and decoder configuration. We finally present empirical results for the individual language pairs and translation directions. 1 http://www.statmt.org/wmt15/ http://workshop2014.iwslt.org 3 http://www.eu-bridge.eu 4 These can now be addressed using the -mmap option to create a binarized version of the corpus which is then memory-mapped. 2 126 Proceedings of the Tenth Workshop on Statisti"
W15-3013,P14-1129,0,0.0621031,"Missing"
W15-3013,P13-2071,1,0.880126,"-based feature functions are used in all cases. B LEU scores on newstest2014 are reported. 4 model are a 5-gram LM score, phrase translation and lexical translation scores, word and phrase penalties, and a linear distortion score. The phrase translation probabilities are smoothed with GoodTuring smoothing (Foster et al., 2006). We used the hierarchical lexicalized reordering model (Galley and Manning, 2008) with 4 possible orientations (monotone, swap, discontinuous left and discontinuous right) in both left-to-right and rightto-left direction. We also used the operation sequence model (OSM) (Durrani et al., 2013) with 4 count based supportive features. We further employed domain indicator features (marking which training corpus each phrase pair was found in), binary phrase count indicator features, sparse phrase length features, and sparse source word deletion, target word insertion, and word translation features (limited to the top K words in each language, typically with K = 50). 3.5 In this section we describe peculiarities of individual systems and present experimental results. 4.1 French↔English Our submitted systems for the French-English language pair are quite similar for the two translation d"
W15-3013,D08-1089,0,0.29008,"ature functions based on Och clusters (see Section 2.3). The last four lines refer to ablation studies where one of the sets of clustered feature functions is removed from the comprehensive setup. Note that the word-based feature functions are used in all cases. B LEU scores on newstest2014 are reported. 4 model are a 5-gram LM score, phrase translation and lexical translation scores, word and phrase penalties, and a linear distortion score. The phrase translation probabilities are smoothed with GoodTuring smoothing (Foster et al., 2006). We used the hierarchical lexicalized reordering model (Galley and Manning, 2008) with 4 possible orientations (monotone, swap, discontinuous left and discontinuous right) in both left-to-right and rightto-left direction. We also used the operation sequence model (OSM) (Durrani et al., 2013) with 4 count based supportive features. We further employed domain indicator features (marking which training corpus each phrase pair was found in), binary phrase count indicator features, sparse phrase length features, and sparse source word deletion, target word insertion, and word translation features (limited to the top K words in each language, typically with K = 50). 3.5 In this"
W15-3013,W08-0509,0,0.0395025,"morphological tags, and basically no additional gains were observed due to the class based feature functions. 2.4 3 3.1 System Overview Preprocessing The training data was preprocessed using scripts from the Moses toolkit. We first normalized the data using the normalize-punctuation.perl script, then performed tokenization (using the -a option), and then truecasing. We did not perform any corpus filtering other than the standard Moses method, which removes sentence pairs with extreme length ratios. 3.2 Word Alignment For word alignment we used either fast_align (Dyer et al., 2013) or MGIZA++ (Gao and Vogel, 2008), followed by the standard grow-diag-final-and symmetrization heuristic. An empirical comparison of fast_align and MGIZA++ on the FinnishEnglish and English-Russian language pairs using the constrained data sets did not reveal any significant difference. 3.3 Language Model We used all available monolingual data to train 5gram language models with modified Kneser-Ney smoothing (Chen and Goodman, 1998). Typically, language models for each monolingual corpus were first trained using either KenLM (Heafield et al., 2013) or the SRILM toolkit (Stolcke, 2002) and then linearly interpolated using weig"
W15-3013,W14-3309,1,0.921919,"at test time the alignment is supplied by the decoder. The bilingual LM is trained using a feedforward neural network and we use the NPLM toolkit for this. Prior to submission we tested bilingual LMs on the French↔English tasks and on English→Russian task. For French↔English, we had resource issues4 in training such large Introduction The Edinburgh/JHU phrase-based translation systems for our participation in the WMT 2015 shared translation task1 are based on the open source Moses toolkit (Koehn et al., 2007). We built upon Edinburgh’s strong baselines from WMT submissions in previous years (Durrani et al., 2014a) as well as our recent research within the framework of other evaluation campaigns and projects such as IWSLT2 and EU-BRIDGE3 (Birch et al., 2014; Freitag et al., 2014a; Freitag et al., 2014b). We first discuss novel features that we integrated into our systems for the 2015 Edinburgh/JHU submission. Next we give a general system overview with details on our training pipeline and decoder configuration. We finally present empirical results for the individual language pairs and translation directions. 1 http://www.statmt.org/wmt15/ http://workshop2014.iwslt.org 3 http://www.eu-bridge.eu 4 These"
W15-3013,P13-2121,1,0.902744,"Missing"
W15-3013,P07-1019,0,0.0483912,"ed language models, we tested with 50 Och clusters, 200 Och clusters, and with both class-based LMs. For the bilingual LM, we created both “combined” (a 5-gram on the target and a 9-gram on the source) and “source” (1-gram on the target and 15-gram on Tuning Since our feature set (generally around 500 to 1000 features) was too large for MERT, we used k-best batch MIRA for tuning (Cherry and Foster, 2012). To speed up tuning we applied threshold pruning to the phrase table, based on the direct translation model probability. 3.6 Experimental Results Decoding In decoding we applied cube pruning (Huang and Chiang, 2007) with a stack size of 5000 (reduced to 1000 for tuning), Minimum Bayes Risk decoding (Kumar and Byrne, 2004), a maximum phrase length of 5, a distortion limit of 6, 100best translation options and the no-reorderingover-punctuation heuristic (Koehn and Haddow, 2009). 128 System Baseline Submitted 50 classes 200 classes 50+200 classes BiLM combined BiLM source & combined NPLM fr-en 33.0 32.7 32.8 32.9 32.9 32.9 33.2 33.0 System Baseline Submitted Without OPUS 50 classes 200 classes 50+200 classes BiLM combined BiLM source & combined NPLM en-fr 33.5 33.6 33.8 33.9 33.7 33.6 33.5 34.2 Table 2: Com"
W15-3013,E14-4029,1,0.939408,"at test time the alignment is supplied by the decoder. The bilingual LM is trained using a feedforward neural network and we use the NPLM toolkit for this. Prior to submission we tested bilingual LMs on the French↔English tasks and on English→Russian task. For French↔English, we had resource issues4 in training such large Introduction The Edinburgh/JHU phrase-based translation systems for our participation in the WMT 2015 shared translation task1 are based on the open source Moses toolkit (Koehn et al., 2007). We built upon Edinburgh’s strong baselines from WMT submissions in previous years (Durrani et al., 2014a) as well as our recent research within the framework of other evaluation campaigns and projects such as IWSLT2 and EU-BRIDGE3 (Birch et al., 2014; Freitag et al., 2014a; Freitag et al., 2014b). We first discuss novel features that we integrated into our systems for the 2015 Edinburgh/JHU submission. Next we give a general system overview with details on our training pipeline and decoder configuration. We finally present empirical results for the individual language pairs and translation directions. 1 http://www.statmt.org/wmt15/ http://workshop2014.iwslt.org 3 http://www.eu-bridge.eu 4 These"
W15-3013,N13-1073,0,0.0525479,"ure functions based on POS and morphological tags, and basically no additional gains were observed due to the class based feature functions. 2.4 3 3.1 System Overview Preprocessing The training data was preprocessed using scripts from the Moses toolkit. We first normalized the data using the normalize-punctuation.perl script, then performed tokenization (using the -a option), and then truecasing. We did not perform any corpus filtering other than the standard Moses method, which removes sentence pairs with extreme length ratios. 3.2 Word Alignment For word alignment we used either fast_align (Dyer et al., 2013) or MGIZA++ (Gao and Vogel, 2008), followed by the standard grow-diag-final-and symmetrization heuristic. An empirical comparison of fast_align and MGIZA++ on the FinnishEnglish and English-Russian language pairs using the constrained data sets did not reveal any significant difference. 3.3 Language Model We used all available monolingual data to train 5gram language models with modified Kneser-Ney smoothing (Chen and Goodman, 1998). Typically, language models for each monolingual corpus were first trained using either KenLM (Heafield et al., 2013) or the SRILM toolkit (Stolcke, 2002) and then"
W15-3013,E03-1076,1,0.79605,"Baseline Submitted BiLM source & combined & NPLM Czech↔English The development of the Czech↔English systems followed the ideas in Section 2.3, i.e., with a focus on word classes (50, 200, 600 classes) for all component models. We combined the test sets from 2008 to 2012 for tuning. No neural language model or bilingual language model was used. 4.4 Table 4: Experimental results (cased B LEU) for English→Russian averaged over newstest2013 and newstest2014. Russian↔English From German. For translation from German, we applied syntactic pre-reordering (Collins et al., 2005) and compound splitting (Koehn and Knight, 2003) in a preprocessing step on the source side. A rich set of translation factors was exploited in addition to word surface forms: Och clusters (50 classes), morphological tags, partof-speech tags, and word stems on the German side (Schmid, 2000), as well as Och clusters (50 classes), part-of-speech tags (Ratnaparkhi, 1996), and word stems (Porter, 1980) on the English side. The factors were utilized in the translation model and in OSMs. The lexicalized reordering model was trained on stems. Individual 7gram Och cluster LMs were trained with KenLM’s --discount_fallback --prune '0 0 1' parameters,"
W15-3013,P07-2045,1,0.00919205,"gned source token. At training time, the aligned source token is found from the automatic alignment, and at test time the alignment is supplied by the decoder. The bilingual LM is trained using a feedforward neural network and we use the NPLM toolkit for this. Prior to submission we tested bilingual LMs on the French↔English tasks and on English→Russian task. For French↔English, we had resource issues4 in training such large Introduction The Edinburgh/JHU phrase-based translation systems for our participation in the WMT 2015 shared translation task1 are based on the open source Moses toolkit (Koehn et al., 2007). We built upon Edinburgh’s strong baselines from WMT submissions in previous years (Durrani et al., 2014a) as well as our recent research within the framework of other evaluation campaigns and projects such as IWSLT2 and EU-BRIDGE3 (Birch et al., 2014; Freitag et al., 2014a; Freitag et al., 2014b). We first discuss novel features that we integrated into our systems for the 2015 Edinburgh/JHU submission. Next we give a general system overview with details on our training pipeline and decoder configuration. We finally present empirical results for the individual language pairs and translation d"
W15-3013,W06-1607,0,0.204091,"29.8 (+.1) 29.5 (–.2) avg ∆ +.5 –.2 –.4 ±.0 –.2 Table 1: Use of additional feature functions based on Och clusters (see Section 2.3). The last four lines refer to ablation studies where one of the sets of clustered feature functions is removed from the comprehensive setup. Note that the word-based feature functions are used in all cases. B LEU scores on newstest2014 are reported. 4 model are a 5-gram LM score, phrase translation and lexical translation scores, word and phrase penalties, and a linear distortion score. The phrase translation probabilities are smoothed with GoodTuring smoothing (Foster et al., 2006). We used the hierarchical lexicalized reordering model (Galley and Manning, 2008) with 4 possible orientations (monotone, swap, discontinuous left and discontinuous right) in both left-to-right and rightto-left direction. We also used the operation sequence model (OSM) (Durrani et al., 2013) with 4 count based supportive features. We further employed domain indicator features (marking which training corpus each phrase pair was found in), binary phrase count indicator features, sparse phrase length features, and sparse source word deletion, target word insertion, and word translation features"
W15-3013,W14-3310,1,0.838548,"mission we tested bilingual LMs on the French↔English tasks and on English→Russian task. For French↔English, we had resource issues4 in training such large Introduction The Edinburgh/JHU phrase-based translation systems for our participation in the WMT 2015 shared translation task1 are based on the open source Moses toolkit (Koehn et al., 2007). We built upon Edinburgh’s strong baselines from WMT submissions in previous years (Durrani et al., 2014a) as well as our recent research within the framework of other evaluation campaigns and projects such as IWSLT2 and EU-BRIDGE3 (Birch et al., 2014; Freitag et al., 2014a; Freitag et al., 2014b). We first discuss novel features that we integrated into our systems for the 2015 Edinburgh/JHU submission. Next we give a general system overview with details on our training pipeline and decoder configuration. We finally present empirical results for the individual language pairs and translation directions. 1 http://www.statmt.org/wmt15/ http://workshop2014.iwslt.org 3 http://www.eu-bridge.eu 4 These can now be addressed using the -mmap option to create a binarized version of the corpus which is then memory-mapped. 2 126 Proceedings of the Tenth Workshop on Statisti"
W15-3013,P10-2041,0,0.0404632,"Missing"
W15-3013,E99-1010,0,0.349917,"Missing"
W15-3013,2014.amta-researchers.3,0,0.061645,"rved a small improvement in translation performance. 2.3 Comprehensive Use of Word Classes In Edinburgh’s submission from the previous year, we used automatically generated word classes in additional language models and in additional operation sequence models (Durrani et al., 2014b). This year, we pushed the use of word classes into the remaining feature functions: the reordering model and the sparse word features. We generated Och clusters (Och, 1999) — a variant of Brown clusters — using mkcls. We have to choose a hyper parameter: the number of clusters. Our experiments and also prior work (Stewart et al., 2014) suggest that instead of committing to a single value, it is beneficial to use multiple numbers and use them in multiple feature functions concurrently. We used 50, 200, 600, and 2000 clusters, hence having 4 additional interpolated language models, 4 additional operation sequence models, 4 additional lexicalized reordering models, and 4 additional sets of sparse features. The feature functions for word classes were trained exactly the same way as the corresponding feature functions for words. For instance, this means that the word class language model required training of individual models on"
W15-3013,tiedemann-2012-parallel,0,0.0549009,"ploying dropout to prevent overfitting (Srivastava et al., 2014), enabling us to train the models for at least 2 epochs. We note that, as with French↔English, our application of bilingual LM did not result in significant improvement. Finnish and English are quite distantly related, but we can speculate that using words as a representation for Finnish is not appropriate. The NPLM, however, offers modest (+0.4) improvements over the baseline in both directions. Finnish↔English For the Finnish-English language pair we built systems using only the constrained data, and systems using all the OPUS (Tiedemann, 2012) par129 4.3 System Baseline Submitted BiLM source & combined & NPLM Czech↔English The development of the Czech↔English systems followed the ideas in Section 2.3, i.e., with a focus on word classes (50, 200, 600 classes) for all component models. We combined the test sets from 2008 to 2012 for tuning. No neural language model or bilingual language model was used. 4.4 Table 4: Experimental results (cased B LEU) for English→Russian averaged over newstest2013 and newstest2014. Russian↔English From German. For translation from German, we applied syntactic pre-reordering (Collins et al., 2005) and c"
W15-3013,D13-1140,0,0.0464299,"n of the University of Edinburgh and the Johns Hopkins University for the shared translation task of the EMNLP 2015 Tenth Workshop on Statistical Machine Translation (WMT 2015). We set up phrase-based statistical machine translation systems for all ten language pairs of this year’s evaluation campaign, which are English paired with Czech, Finnish, French, German, and Russian in both translation directions. 2.1 Neural Network LM with NPLM For some language pairs (notably French↔English and Finnish↔English) we experimented with feed-forward neural network language models using the NPLM toolkit (Vaswani et al., 2013). This toolkit enables such language models to be trained efficiently on large datasets, and provides a querying API which is fast enough to be used during decoding. NPLM is fully integrated into Moses, including appropriate wrapper scripts for training the language models within the Moses experiment management system. Novel research directions we investigated include: neural network language models and bilingual neural network language models, a comprehensive use of word classes, and sparse lexicalized reordering features. 1 Novel Methods 2.2 Bilingual Neural Network LM We also experimented w"
W15-3013,W15-3024,1,0.849652,"not the English. In fact French→English was the only language pair where NPLM did not improve B LEU after building the LM on all data. It is possible that the limited morphology of English means that the improved generalisation of the NPLM is not as helpful, and also that the conventional n-gram LM is already strong for this language pair. 4.2 fi-en 19.6 19.7 17.0 19.4 19.8 19.7 19.1 19.1 20.0 allel data. Our baselines include this extra data, but we also show results just using the constrained parallel data. We did not employ the morphological splitting as in Edinburgh’s syntax-based system (Williams et al., 2015) and consequently the English→Finnish systems performed poorly in development and we did not submit a phrase-based system for this pair. Our development setup was similar to French↔English; we used the newsdev2015 for tuning and test during system development (in 2-fold cross-validation) then for the submission and subsequent experiments we used the whole of newsdev2015 for tuning. Also in common with our work on French↔English, we performed several post-submission experiments to examine the effect of class-based language models, bilingual LM and NPLM. We show the results in Table 3. For train"
W15-3013,W09-0429,1,\N,Missing
W15-3013,N04-1022,0,\N,Missing
W15-3013,2014.iwslt-evaluation.6,1,\N,Missing
W15-3013,C14-1041,1,\N,Missing
W15-3013,W14-3324,1,\N,Missing
W15-3024,D14-1082,0,0.137464,"Missing"
W15-3024,P14-1129,0,0.137886,"Missing"
W15-3024,N13-1073,0,0.0469731,"2.1 Introduction 2.2 System Overview Pre-processing The training data was pre-processed using scripts from the Moses toolkit. We first normalized the data using the normalize-punctuation.perl script then performed tokenization, parsing, and truecasing. To parse the English data, we used the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007). To parse the German data, we used the ParZu dependency parser (Sennrich et al., 2013). Word Alignment For word alignment we used either MGIZA++ (Gao and Vogel, 2008), a multi-threaded implementation of GIZA++ (Och and Ney, 2003), or fast_align (Dyer et al., 2013). In preliminary experiments, we found that the tree-to-string systems were particularly sensitive to the choice of word aligner, echoing a previous observation by Neubig and Duh (2014). See the individual treeto-string system descriptions in Section 3. This year’s WMT shared translation task featured five language pairs: English paired with Czech, Finnish, French, German, and Russian. We built syntax-based systems in both translation directions for all language pairs except English-French. For English → German, we continued to develop our string-to-tree system, which has proven highly competi"
W15-3024,P03-2041,0,0.147009,"Missing"
W15-3024,W15-1004,0,0.0358237,"Missing"
W15-3024,P06-1121,0,0.275709,"Missing"
W15-3024,W13-2221,1,0.922677,"Missing"
W15-3024,W08-0509,0,0.0425569,"e our English-German system; and source-side morphological segmentation of Finnish using Morfessor. 2.1 Introduction 2.2 System Overview Pre-processing The training data was pre-processed using scripts from the Moses toolkit. We first normalized the data using the normalize-punctuation.perl script then performed tokenization, parsing, and truecasing. To parse the English data, we used the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007). To parse the German data, we used the ParZu dependency parser (Sennrich et al., 2013). Word Alignment For word alignment we used either MGIZA++ (Gao and Vogel, 2008), a multi-threaded implementation of GIZA++ (Och and Ney, 2003), or fast_align (Dyer et al., 2013). In preliminary experiments, we found that the tree-to-string systems were particularly sensitive to the choice of word aligner, echoing a previous observation by Neubig and Duh (2014). See the individual treeto-string system descriptions in Section 3. This year’s WMT shared translation task featured five language pairs: English paired with Czech, Finnish, French, German, and Russian. We built syntax-based systems in both translation directions for all language pairs except English-French. For En"
W15-3024,P14-2024,0,0.0198394,"tion.perl script then performed tokenization, parsing, and truecasing. To parse the English data, we used the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007). To parse the German data, we used the ParZu dependency parser (Sennrich et al., 2013). Word Alignment For word alignment we used either MGIZA++ (Gao and Vogel, 2008), a multi-threaded implementation of GIZA++ (Och and Ney, 2003), or fast_align (Dyer et al., 2013). In preliminary experiments, we found that the tree-to-string systems were particularly sensitive to the choice of word aligner, echoing a previous observation by Neubig and Duh (2014). See the individual treeto-string system descriptions in Section 3. This year’s WMT shared translation task featured five language pairs: English paired with Czech, Finnish, French, German, and Russian. We built syntax-based systems in both translation directions for all language pairs except English-French. For English → German, we continued to develop our string-to-tree system, which has proven highly competitive in previous years. Additions this year included the use of a dependency language model, an alternative tuning metric, and soft source-syntactic constraints. For translation from En"
W15-3024,P05-1013,0,0.0872144,"Missing"
W15-3024,P13-2121,1,0.887764,"Missing"
W15-3024,J03-1002,0,0.0231219,"tation of Finnish using Morfessor. 2.1 Introduction 2.2 System Overview Pre-processing The training data was pre-processed using scripts from the Moses toolkit. We first normalized the data using the normalize-punctuation.perl script then performed tokenization, parsing, and truecasing. To parse the English data, we used the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007). To parse the German data, we used the ParZu dependency parser (Sennrich et al., 2013). Word Alignment For word alignment we used either MGIZA++ (Gao and Vogel, 2008), a multi-threaded implementation of GIZA++ (Och and Ney, 2003), or fast_align (Dyer et al., 2013). In preliminary experiments, we found that the tree-to-string systems were particularly sensitive to the choice of word aligner, echoing a previous observation by Neubig and Duh (2014). See the individual treeto-string system descriptions in Section 3. This year’s WMT shared translation task featured five language pairs: English paired with Czech, Finnish, French, German, and Russian. We built syntax-based systems in both translation directions for all language pairs except English-French. For English → German, we continued to develop our string-to-tree syst"
W15-3024,herrmann-etal-2014-manual,0,0.0544561,"Missing"
W15-3024,E99-1010,0,0.163731,"Missing"
W15-3024,D10-1063,0,0.128747,"Missing"
W15-3024,P03-1021,0,0.122835,"Missing"
W15-3024,W14-4018,1,0.8972,"Missing"
W15-3024,N03-1017,1,0.0581822,"Missing"
W15-3024,P06-1055,0,0.105477,"ared translation task. We developed systems for all language pairs except French-English. This year we focused on: translation out of English using tree-to-string models; continuing to improve our English-German system; and source-side morphological segmentation of Finnish using Morfessor. 2.1 Introduction 2.2 System Overview Pre-processing The training data was pre-processed using scripts from the Moses toolkit. We first normalized the data using the normalize-punctuation.perl script then performed tokenization, parsing, and truecasing. To parse the English data, we used the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007). To parse the German data, we used the ParZu dependency parser (Sennrich et al., 2013). Word Alignment For word alignment we used either MGIZA++ (Gao and Vogel, 2008), a multi-threaded implementation of GIZA++ (Och and Ney, 2003), or fast_align (Dyer et al., 2013). In preliminary experiments, we found that the tree-to-string systems were particularly sensitive to the choice of word aligner, echoing a previous observation by Neubig and Duh (2014). See the individual treeto-string system descriptions in Section 3. This year’s WMT shared translation task featured five la"
W15-3024,W05-0904,0,0.182994,"Missing"
W15-3024,D15-1248,1,0.828811,"Missing"
W15-3024,D14-2005,1,0.826874,"Missing"
W15-3024,R13-1079,1,0.817325,"d on: translation out of English using tree-to-string models; continuing to improve our English-German system; and source-side morphological segmentation of Finnish using Morfessor. 2.1 Introduction 2.2 System Overview Pre-processing The training data was pre-processed using scripts from the Moses toolkit. We first normalized the data using the normalize-punctuation.perl script then performed tokenization, parsing, and truecasing. To parse the English data, we used the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007). To parse the German data, we used the ParZu dependency parser (Sennrich et al., 2013). Word Alignment For word alignment we used either MGIZA++ (Gao and Vogel, 2008), a multi-threaded implementation of GIZA++ (Och and Ney, 2003), or fast_align (Dyer et al., 2013). In preliminary experiments, we found that the tree-to-string systems were particularly sensitive to the choice of word aligner, echoing a previous observation by Neubig and Duh (2014). See the individual treeto-string system descriptions in Section 3. This year’s WMT shared translation task featured five language pairs: English paired with Czech, Finnish, French, German, and Russian. We built syntax-based systems in"
W15-3024,W14-4011,1,0.893791,"Missing"
W15-3024,Q15-1013,1,0.867045,"Missing"
W15-3024,N13-3005,0,0.0310419,"Missing"
W15-3024,D13-1140,0,0.162963,"Missing"
W15-3024,vilar-etal-2006-error,0,0.110074,"Missing"
W15-3024,W12-3150,1,0.927673,"Missing"
W15-3024,W14-3324,1,0.644531,"oft source-syntactic constraints. For translation from English into Czech, Finnish, and Russian, we built STSG-based treeto-string systems. Support for this type of model is a recent addition to the Moses toolkit. In previous years, our systems have all used string-to-tree models and have only translated into English and German. For Finnish → English, we experimented with unsupervised morphological segmentation using Morfessor 2.0 (Virpioja et al., 2013). For the remaining systems (Czech → English, German → English, and Russian → English), our systems were essentially the same as last year’s (Williams et al., 2014) except for the addition of this year’s training data. 2.3 Language Model We used all available monolingual data to train one interpolated 5-gram language model for each system. Using either lmplz (Heafield et al., 2013) or the SRILM toolkit (Stolcke, 2002), we first trained an individual language model for each of the supplied monolingual training corpora. These models all used modified Kneser-Ney smoothing (Chen and Goodman, 1998). We then interpolated the individual models using SRILM, providing the target-side of the system’s tuning set (Section 2.7) for perplexity-based weight optimizatio"
W15-3031,W06-3114,1,0.788111,"system outputs by ranking translated sentences relative to each other. For each source segment that was included in the procedure, the annotator was shown five different outputs to which he or she was supposed to assign ranks. Ties were allowed. Introduction Automatic machine translation metrics play a very important role in the development of MT systems and their evaluation. There are many different metrics of diverse nature and one would like to assess their quality. For this reason, the Metrics Shared Task is held annually at the Workshop of Statistical Machine Translation1 , starting with Koehn and Monz (2006) and following up to Mach´acˇ ek and Bojar (2014). The systems’ outputs, human judgements and evaluated metrics are described in Section 2. The quality of the metrics in terms of system level correlation is reported in Section 3. Section 4 is devoted to segment level correlation. 2 These collected rank labels for each five-tuple of outputs were then interpreted as pairwise comparisons of systems and used to assign each system a score that reflects how high that system was usually ranked by the annotators. Several methods have been tested in the past for the exact score calculation and WMT15 ha"
W15-3031,W15-3032,1,0.838791,"Missing"
W15-3031,E06-1031,0,0.0403601,"e flag --international-tokenization since it performs slightly better (Mach´acˇ ek and Bojar, 2013). 2.2.8 U OW- LSTM U OW- LSTM uses dependency-tree recursive neural network to represent both the hypothesis and the reference with a dense vector. The final score is obtained from a neural network trained on judgements from previous years converted to similarity scores, taking into account both the distance and angle of the two representations. U OW- LSTM tied for the best place in fr-en system-level evaluation with DPMF. • Moses Scorer. The metrics TER (Snover et al., 2006), WER, PER and CDER (Leusch et al., 2006) were computed using the Moses scorer which is used in Moses model optimization. To tokenize the sentences, we used the standard tokenizer script as available in Moses toolkit. 2.2.9 UPF - COBALT UPF - COBALT pays an increased attention to syntactic context (for example arguments, complements, modifiers etc.) both in aligning the words of the hypothesis and reference as well as in scoring of the matched words. It relies on additional resources including stemmers, WordNet synsets, paraphrase databases and distributed word representations. UPF - COBALT system-level score was calculated by taking"
W15-3031,W15-3051,0,0.0746818,"Missing"
W15-3031,W15-3052,0,0.0615378,"Missing"
W15-3031,W15-3053,0,0.092353,"Missing"
W15-3031,W15-3046,0,\N,Missing
W15-3031,W15-3044,0,\N,Missing
W15-3031,W15-3001,1,\N,Missing
W15-3031,W13-2202,1,\N,Missing
W15-3031,W11-2101,1,\N,Missing
W15-3031,D15-1124,0,\N,Missing
W15-3031,W15-3047,0,\N,Missing
W16-2204,P13-2074,0,0.0218328,"a lexical semantic taxonomy and clustering words based on cooccurrences within a window or syntactic features extracted from dependency-parsed data. Modeling reordering and deletion of semantic roles (Wu and Fung, 2009; Liu and Gildea, 2010; Li et al., 2013) has been another line of research on improving translation of predicate-argument structures. Liu and Gildea (2010) propose modeling reordering of a complete semantic frame while Li et al. (2013) propose finer grained features that distinguish between predicate-argument reordering and argument-argument reordering. Gao and Vogel (2011) and Bazrafshan and Gildea (2013) annotate target non-terminals with the semantic roles they cover in order to extract synchronous grammar rules that cover the entire predicate argument structure. These models however do not encode information about the lexical semantic affinities between target predicates and their arguments. In this work we focus on using selectional preference over predicate and arguments in the target as this is a simple way of leveraging external knowledge in the translation framework. 3 3.1 SelP ref (p, r) = KL(P (c|p, r) k P (c|r)) X P (c|p, r) = P (c|p, r)log P (c|r) c (1) where KL is the Kullback - L"
W16-2204,D15-1158,0,0.0306369,"Missing"
W16-2204,N12-1047,0,0.0390144,"ated sentence, its dependency tree produced by the string-to-tree system and the triples extracted at decoding time. We consider the following main arguments: nsubj, nsubjpass, dobj, iobj and prep arguments attached to both verbs and nouns. Table 4 shows the number of extracted triples. Type of relation main prep nsubj nsubjpass dobj iobj the harmonic mean of precision and recall over head-word chains of length 1 to 4. The head-word chains are extracted directly from the dependency tree produced by the string-to-tree decoder and from the parsed reference. Tuning is performed using batch MIRA (Cherry and Foster, 2012) on 1000-best lists. We report evaluation scores averaged over the newstest2013, newstest2014 and newstest2015 data sets provided by WMT15. 5 5.1 Error analysis We wanted to get an idea about how often the verb and its arguments are mistranslated. For this purpose we manually annotated errors in sentences with more than 5 words and at most 15 words. With this criterion we avoided translations with scrambled predicate-argument structures. Each sentence had roughly one main verb. To have a more reliable error annotation we first post-edited 100 translations from the baseline system. We then comp"
W16-2204,N10-1000,0,0.0597296,"Missing"
W16-2204,D12-1096,0,0.0528117,"Missing"
W16-2204,W12-3308,0,0.0157896,"istributional semantics SelAssoc(p, r, c) = P (c|p, r)log PP(c|p,r) (c|r) SelStr(p, r) (2) We give examples of the selectional preference strength and selectional association scores for different verbs and their arguments in Table 2. The verb see takes on many arguments as direct objects and therefore has a lower selectional preference strength for this syntactic relation. In contrast the predicate hereditary takes on fewer arguments for which it has a stronger selectional preference. Several selectional preference models have been used as features in discriminative syntactic parsing systems. Cohen et al. (2012) observe 34 Verb see Relation dobj SelPref 0.56 is–hereditary nsubj 1.69 drink dobj 3.90 Argument PRN movie episode disease monarchy title water wine glass SelAssoc 0.123 0.022 0.001 0.267 0.148 0.082 0.144 0.061 0.027 Table 2: Example of selectional preference (SelPref) and selectional association (SelAssoc) scores for different verbs. PRN is the class of pronouns. propriate for a machine translation task where the vocabulary has millions of words and English is not the only targeted language. Therefore we adapt Resnik’s selectional association measure in two ways. that when parsing out-of-do"
W16-2204,N13-1060,0,0.0277343,"Missing"
W16-2204,D14-1004,0,0.0228193,"Missing"
W16-2204,J10-4007,0,0.0190829,"Missing"
W16-2204,W05-0904,0,0.0221325,"26 percent of the verbs are mistranslated and about 10 percent of the arguments. Mistranslated verbs are problematic since the feature produces the selectional association scores for the wrong verb. Although the semantic affinity is mutual, the formulation of the score conditions on the verb. In the cases when both the verb and the argument are mistranslated the association score might be high although the translation is not faithful to the source. For both tuning and evaluation of all machine translation systems we use a combination of the cased BLEU score and head-word chain metric (HWCM ) (Liu and Gildea, 2005). The HWCM metric implemented in the Moses toolkit computes 4 Evaluation Coordination is not resolved at decoding time. 37 5.2 Evaluation of the Selectional Preference Feature not improve the evaluation scores as shown in the fifth row of Table 6. The lack of variance in automatic evaluation scores can be explained by: a) the feature touches only a few words in the translation and b) the relation between a predicate and its argument is identified at later stages of the bottom-up chart-based decoding when many lexical choices have already been pruned out. The SelAssoc scores, similar to mutual"
W16-2204,P06-1121,0,0.125949,"Missing"
W16-2204,C10-1081,0,0.138729,"lack of affinity. presents the results of automatic evaluation as well as a qualitative analysis of the machine translated output. their core and prepositional arguments as well as selectional preferences of nouns for their prepositional arguments. Previous work has addressed the selectional preferences of prepositions for noun classes (Weller et al., 2014) but not the semantic affinities between a predicate and its argument class. Another line of research on improving translation of predicate-argument structures includes modeling reordering and deletion of semantic roles (Wu and Fung, 2009; Liu and Gildea, 2010; Li et al., 2013). These models however do not encode information about the lexical semantic affinities between target predicates and their arguments. Sennrich (2015) proposes a relational dependency language model (RDLM) for string-to-tree machine translation. One component of RDLM predicts the head word of a dependent conditioned on a wide syntactic context. Our feature is different as it quantifies the amount of information that the predicate carries about the argument class filling a particular syntactic function. 2 Related work From a syntactic perspective, a correct predicate-argument s"
W16-2204,D14-1162,0,0.08159,"lization capabilities as a measure of distributional similarity between word embeddings. van Noord (2007) has shown that bilexical association scores computed using PMI for all types of dependency relations are a useful feature for improving dependency parsing in Dutch. 3.2 In the first model SelAssoc L we compute the co-occurrence statistics defined in Eq 2 over lemmas of the predicate and argument head words. In the second model SelAssoc C we replace the WordNet classes in Eq 2 with word clusters1 . We obtain the word clusters by applying the k-means algorithm to the glovec word embeddings (Pennington et al., 2014). Prepositional phrase attachment remains a frequent and challenging error for syntactic parsers (Kummerfeld et al., 2012) and translation of prepositions is a challenge for SMT (Weller et al., 2014). Therefore we decide to use two separate features: one for main arguments (nsubj, nsubjpass, dobj, iobj) and one for prepositional arguments. Adaptation of Selectional Preference Models for Syntax-Based Machine Translation. We are interested in modeling selectional preferences of verbs for their core and prepositional arguments as well as selectional preferences of nouns for their prepositional ar"
W16-2204,P13-1058,0,0.0139115,"lexical semantic affinities between target predicates and their arguments. Sennrich (2015) proposes a relational dependency language model (RDLM) for string-to-tree machine translation. One component of RDLM predicts the head word of a dependent conditioned on a wide syntactic context. Our feature is different as it quantifies the amount of information that the predicate carries about the argument class filling a particular syntactic function. 2 Related work From a syntactic perspective, a correct predicate-argument structure will have the sub-categorization frame of the predicate filled in. Weller et al. (2013) use sub-categorization information to improve case-prediction for noun phrases when translating into German. Case prediction for noun phrases is important in the German language as it indicates the grammatical function. Their approach however did not produce strong improvements over the baseline. From a large corpus annotated with dependency relations, they extract verb-noun tuples and their associated syntactic functions: direct object, indirect object, subject. They also extract triples of verb-preposition-noun in order to predict the case of noun-phrases within prepositionalphrases. The pr"
W16-2204,P10-1044,0,0.017205,"l as an increase in precision for verb translation. However the features generally did not improve automatic evaluation metrics. We conclude that mistranslated verbs, errors in the target syntactic trees produced by the decoder and underspecified syntactic relations are negatively impacting these features. The paper is structured as follows. Section 2 describes related work on improving translation of predicate-argument structures. Section 3 introduces the selectional preference feature. Section 4 describes the experimental setup and Section 5 33 similarity (Erk et al., 2010; S´eaghdha, 2010; Ritter et al., 2010), clustering (Sun and Korhonen, 2009), multi-modal datasets (Shutova et al., 2015), and neural networks (Cruys, 2014). Our feature is based on the measure proposed by Resnik (1996). It uses unsupervised clusters to generalize over seen arguments. Resnik (1996) uses selectional preferences of predicates for word sense disambiguation. The information theoretic measure for selectional preference proposed by Resnik quantifies the difference between the posterior distribution of an argument class given the verb and the prior distribution of the class. For instance, ”person” has a higher prior proba"
W16-2204,P10-1045,0,0.056561,"Missing"
W16-2204,2014.amta-researchers.21,0,0.43219,"1: Examples of errors in the predicate-argument structure produced by a syntax-based MT system. a) mistranslated verb b) mistranslated noun. Semantic affinity scores are shown on the right. Higher scores indicate a stronger affinity. Negative scores indicate a lack of affinity. presents the results of automatic evaluation as well as a qualitative analysis of the machine translated output. their core and prepositional arguments as well as selectional preferences of nouns for their prepositional arguments. Previous work has addressed the selectional preferences of prepositions for noun classes (Weller et al., 2014) but not the semantic affinities between a predicate and its argument class. Another line of research on improving translation of predicate-argument structures includes modeling reordering and deletion of semantic roles (Wu and Fung, 2009; Liu and Gildea, 2010; Li et al., 2013). These models however do not encode information about the lexical semantic affinities between target predicates and their arguments. Sennrich (2015) proposes a relational dependency language model (RDLM) for string-to-tree machine translation. One component of RDLM predicts the head word of a dependent conditioned on a"
W16-2204,W12-3150,1,0.857394,"ng arc at position i, RDLM is defined as: 1 35 We have not done experiments with WordNet classes. root det nn NNP DT NNP Minister the Prime met prep IN of pobj NNP cc conj:and India CC NNP and Japan punct prep VBD nsubj IN pobj . in NNP . Tokyo relation nsubj prep in prep of predicate met met Minister argument Minister Tokyo India Figure 1: Example of a translation and its dependency tree in constituency representation produced by the string-to-tree SMT system. Triples extracted during decoding are shown on the right. P (S, D) ≈ n Y i=1 menting GHKM rule extraction (Galley et al., 2004, 2006; Williams and Koehn, 2012). The string-to-tree translation model is based on a synchronous context-free grammar (SCFG) that is extracted from word-aligned parallel data with target-side syntactic annotation. The system was trained on all available data provided at WMT15 2 (Bojar et al., 2015). The number of sentences in the training, tuning and test sets are shown in Table 3. We use the following rule extraction parameters: Rule Depth = 5, Node Count = 20, Rule Size = 5. At decoding time we give a high penalty to glue rules and allow non-terminals to span a maximum of 50 words. We train a 5-gram language model on all a"
W16-2204,Q15-1013,0,0.0878161,"per explores whether knowledge about semantic affinities between the target predicates and their argument fillers is useful for translating ambiguous predicates and arguments. We propose a selectional preference feature based on the selectional association measure of Resnik (1996) and integrate it in a string-to-tree decoder. The feature models selectional preferences of verbs for their core and prepositional arguments as well as selectional preferences of nouns for their prepositional arguments. We compare our features with a variant of the neural relational dependency language model (RDLM) (Sennrich, 2015) and find that neither of the features improves automatic evaluation metrics. We conclude that mistranslated verbs, errors in the target syntactic trees produced by the decoder and underspecified syntactic relations are negatively impacting these features. 1 root → hRB ∼0 V BZ ∼1 sich nsubj ∼2 prep∼3 , RB ∼0 nsubj ∼2 V BZ ∼1 prep∼3 i This rule is useful for reordering the verb and its arguments according to the target side word order. However the rule does not contain the lexical head for the verb, the subject and the prepositional modifier. Therefore the entire predicate argument structure is"
W16-2204,N09-2004,0,0.0606923,"e scores indicate a lack of affinity. presents the results of automatic evaluation as well as a qualitative analysis of the machine translated output. their core and prepositional arguments as well as selectional preferences of nouns for their prepositional arguments. Previous work has addressed the selectional preferences of prepositions for noun classes (Weller et al., 2014) but not the semantic affinities between a predicate and its argument class. Another line of research on improving translation of predicate-argument structures includes modeling reordering and deletion of semantic roles (Wu and Fung, 2009; Liu and Gildea, 2010; Li et al., 2013). These models however do not encode information about the lexical semantic affinities between target predicates and their arguments. Sennrich (2015) proposes a relational dependency language model (RDLM) for string-to-tree machine translation. One component of RDLM predicts the head word of a dependent conditioned on a wide syntactic context. Our feature is different as it quantifies the amount of information that the predicate carries about the argument class filling a particular syntactic function. 2 Related work From a syntactic perspective, a correc"
W16-2204,P15-1092,0,0.0231686,"Missing"
W16-2204,P11-4010,0,0.0122939,"wstest2015 data sets provided by WMT15. 5 5.1 Error analysis We wanted to get an idea about how often the verb and its arguments are mistranslated. For this purpose we manually annotated errors in sentences with more than 5 words and at most 15 words. With this criterion we avoided translations with scrambled predicate-argument structures. Each sentence had roughly one main verb. To have a more reliable error annotation we first post-edited 100 translations from the baseline system. We then compared the translations with their post-editions and annotated error categories using the BLAST tool (Stymne, 2011). We considered a sense error category when there was a wrong lexical choice for the head of a main argument, a prepositional modifier or the main verb. We also annotated mistranslated prepositions. Number of triples 540,109,283 810,118,653 315,852,775 32,111,962 188,412,178 3,732,368 Table 4: Number of relation triples extracted from parsed data. The data consists of the English side of the parallel data and Gigaword. main arguments include: nsubj, nsubjpass, dobj, iobj. Error Category Preposition Sense Main argument Prep modifier Main verb We integrate the feature in a bottom-up chart decode"
W16-2204,D09-1067,0,0.0114607,"verb translation. However the features generally did not improve automatic evaluation metrics. We conclude that mistranslated verbs, errors in the target syntactic trees produced by the decoder and underspecified syntactic relations are negatively impacting these features. The paper is structured as follows. Section 2 describes related work on improving translation of predicate-argument structures. Section 3 introduces the selectional preference feature. Section 4 describes the experimental setup and Section 5 33 similarity (Erk et al., 2010; S´eaghdha, 2010; Ritter et al., 2010), clustering (Sun and Korhonen, 2009), multi-modal datasets (Shutova et al., 2015), and neural networks (Cruys, 2014). Our feature is based on the measure proposed by Resnik (1996). It uses unsupervised clusters to generalize over seen arguments. Resnik (1996) uses selectional preferences of predicates for word sense disambiguation. The information theoretic measure for selectional preference proposed by Resnik quantifies the difference between the posterior distribution of an argument class given the verb and the prior distribution of the class. For instance, ”person” has a higher prior probability than ”insect” to appear in the"
W16-2204,N04-1035,0,\N,Missing
W16-2204,W07-2201,0,\N,Missing
W16-2204,W11-1012,0,\N,Missing
W16-2204,D14-1082,0,\N,Missing
W16-2204,W15-3001,1,\N,Missing
W16-2204,W12-3018,0,\N,Missing
W16-2204,W11-2123,0,\N,Missing
W16-2301,W13-3520,0,0.0148252,"Missing"
W16-2301,2011.mtsummit-papers.35,0,0.440323,"Missing"
W16-2301,W15-3001,1,0.419439,"Missing"
W16-2301,W16-2305,0,0.0273073,"Missing"
W16-2301,L16-1662,0,0.00889007,"introduce a new set of features for the sentence-Level QE task. The features extracted include three alignment-based features, three bilingual embedding-based features, two embeddingbased features constrained on alignment links, as well as a set of 74 bigrams used as boolean features. The set of bigrams represents the most frequent bigrams in translations that have changed after the post-edition, and they are compiled by aligning translations to their post-editions provided in the WMT QE datasets. To produce these features, GIZA++ (Och and Ney, 2003) was used for word alignment and Multivec (Berard et al., 2016) was used for the bilingual model, which jointly learns distributed representations for source and target languages using a parallel corpus. To build the bilingual model, domain-specific data compiled from the resources made available for the WMT 16 IT-Domain shared task was used. As prediction model, a Linear Regression model using scikit-learn was built using a combination of QuEst++ baseline features and the new features proposed. tional features for both tasks. UNBABEL (Task 2): Two systems were submitted for the word-level task. UNBABEL 2 linear is a feature-based linear sequential model."
W16-2301,W11-2101,1,0.851508,"Missing"
W16-2301,W16-2306,0,0.0263259,"Missing"
W16-2301,W16-2382,0,0.0451767,"Missing"
W16-2301,W16-2302,1,0.346609,"Missing"
W16-2301,W16-2307,1,0.756182,"Missing"
W16-2301,W16-2308,0,0.0373021,"Missing"
W16-2301,buck-etal-2014-n,0,0.0150538,"Missing"
W16-2301,W13-2201,1,0.287085,"Missing"
W16-2301,W14-3302,1,0.399147,"Missing"
W16-2301,W14-3340,1,0.419045,"Missing"
W16-2301,W07-0718,1,0.759293,"oth automatically and manually. The human evaluation (§3) involves asking human judges to rank sentences output by anonymized systems. We obtained large numbers of rankings from researchers who contributed The quality estimation task had three subtasks, with a total of 14 teams, submitting 39 entries. The automatic post-editing task had a total of 6 teams, submitting 11 entries. 1 Introduction We present the results of the shared tasks of the First Conference on Statistical Machine Translation (WMT) held at ACL 2016. This conference builds on nine previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015). 131 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 131–198, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics to refine evaluation and estimation methodologies for machine translation. As before, all of the data, translations, and collected human judgments are publicly available.1 We hope these datasets serve as a valuable resource for research into statistical machine translation and automatic evaluation or prediction of translation quality. New"
W16-2301,W15-3025,1,0.888663,"used for the Task 2p. The submissions to the word-level task are modified in order to comply with the phrase-level task. sequences. Nevertheless, it is worth noticing the phraselevel QE systems introduced a number of interesting strategies that allowed them to outperform a strong baseline phrase-level model. Finally, we recall that the evaluation metric – word-level F1 mult – has difficulties to distinguish phrase-level systems. This suggests that we may need to find a different metric for evaluation of the phrase-level task, with phrase-level F1 -mult one of the candidates. 7 pointed out by Chatterjee et al. (2015b), from the application point of view the task is motivated by its possible uses to: • Improve MT output by exploiting information unavailable to the decoder, or by performing deeper text analysis that is too expensive at the decoding stage; • Cope with systematic errors of an MT system whose decoding process is not accessible; • Provide professional translators with improved MT output quality to reduce (human) post-editing effort; Automatic Post-editing Task This year WMT hosted the second round of the shared task on MT automatic post-editing (APE), which consists in automatically correcting"
W16-2301,P15-2026,1,0.895785,"used for the Task 2p. The submissions to the word-level task are modified in order to comply with the phrase-level task. sequences. Nevertheless, it is worth noticing the phraselevel QE systems introduced a number of interesting strategies that allowed them to outperform a strong baseline phrase-level model. Finally, we recall that the evaluation metric – word-level F1 mult – has difficulties to distinguish phrase-level systems. This suggests that we may need to find a different metric for evaluation of the phrase-level task, with phrase-level F1 -mult one of the candidates. 7 pointed out by Chatterjee et al. (2015b), from the application point of view the task is motivated by its possible uses to: • Improve MT output by exploiting information unavailable to the decoder, or by performing deeper text analysis that is too expensive at the decoding stage; • Cope with systematic errors of an MT system whose decoding process is not accessible; • Provide professional translators with improved MT output quality to reduce (human) post-editing effort; Automatic Post-editing Task This year WMT hosted the second round of the shared task on MT automatic post-editing (APE), which consists in automatically correcting"
W16-2301,W08-0309,1,0.809107,"Missing"
W16-2301,W16-2309,0,0.0305938,"Missing"
W16-2301,W10-1703,1,0.312318,"Missing"
W16-2301,W16-2336,0,0.0356983,"Missing"
W16-2301,W12-3102,1,0.235434,"ir native language. Consequently, health professionals may use the translated information to make clinical decisions impacting patients care. It is vital that translation systems do not contribute to the dissemination of incorrect clinical information. Therefore, the evaluation of biomedical translation systems should include an assessment at the document level indicating whether a translation conveyed erroneous clinical information. 6 Quality Estimation The fifth edition of the WMT shared task on quality estimation (QE) of machine translation (MT) builds on the previous editions of the task (Callison-Burch et al., 2012; Bojar et al., 2013, 2014, 2015), with “traditional” tasks at sentence and word levels, a new task for entire documents quality prediction, and a variant of the word-level task: phrase-level estimation. The goals of this year’s shared task were: • To advance work on sentence and wordlevel quality estimation by providing domainspecific, larger and professionally annotated datasets. • To analyse the effectiveness of different types of quality labels provided by humans for longer texts in document-level prediction. • To investigate quality estimation at a new level of granularity: phrases. Plan"
W16-2301,W16-2330,0,0.0328047,"Missing"
W16-2301,W16-2310,1,0.835279,"Missing"
W16-2301,W11-2103,1,0.65163,"Missing"
W16-2301,W16-2331,0,0.0203904,"cance test results for pairs of systems competing in the news domain translation task (en-ru), where a green cell denotes a significantly higher DA adequacy score for the system in a given row over the system in a given column. cs-en fi-en tr-en de-en ru-en ro-en 0.997 0.996 0.988 0.964 0.961 0.920 en-ru 0.975 DA Correlation with RR Table 11: Correlation between overall DA standardized mean adequacy scores and RR Trueskill scores. 150 4 IT Translation Task from the QTLeap project: HF&FCUL for Portuguese, UPV/EHU for Spanish and Basque, IICT-BAS for Bulgarian, CUNI for Czech and UG for Dutch). Duma and Menzel (2016) describe UHDS- DOC 2 VEC and UHBS- LMI (University of Hamburg). Pahari et al. (2016) describe JU-USAAR (Jadavpur University & Saarland University). Cuong et al. (2016) describe ILLC-U VA-S CORPIO (University of Amsterdam). IILC-U VA-DS is based on Hoang and Sima’an (2014). PROMT-RULE - BASED and PROMT-H YBRID systems were submitted by the PROMT LLC company and they are not described in any paper. QTL -M OSES is the standard Moses setup (MERT-tuned on the in-domain training data, but otherwise without any domain-adaptation) and serves as a baseline. The IT-domain translation task introduced th"
W16-2301,W15-3009,1,0.788321,"Missing"
W16-2301,P15-1174,1,0.28154,"endently inserted in another part of this sentence, i.e. to correct an unrelated error. The statistics of the datasets are outlined in Table 20. Evaluation Evaluation was performed against the true HTER label and/or ranking, using the following metrics: • Scoring: Pearson’s r correlation score (primary metric, official score for ranking submissions), Mean Average Error (MAE) and Root Mean Squared Error (RMSE). • Ranking: Spearman’s ρ rank correlation and DeltaAvg. Statistical significance on Pearson r and Spearman rho was computed using the William’s test, following the approach suggested in (Graham, 2015). Results Table 19 summarises the results for Task 1, ranking participating systems best to worst using Pearson’s r correlation as primary key. Spearman’s ρ correlation scores should be used to rank systems according to the ranking variant. We note that three systems have not submitted results ranking evaluation variant. 6.4 Task 2: Predicting word-level quality The goal of this task is to evaluate the extent to which we can detect word-level errors in MT output. Various classes of errors can be found in translations, but for this task we consider all error types together, aiming at making a b"
W16-2301,W16-2311,0,0.0122367,"016) PROMT Automated Translation Solutions (Molchanov and Bykov, 2016) QT21 System Combination (Peter et al., 2016b) RWTH Aachen (Peter et al., 2016a) ¨ TUBITAK (Bektas¸ et al., 2016) University of Edinburgh (Sennrich et al., 2016) University of Edinburgh (Williams et al., 2016) UEDIN - SYNTAX UEDIN - LMU UH -* USFD - RESCORING UUT YSDA ONLINE -[ A , B , F, G ] University of Edinburgh / University of Munich (Huck et al., 2016) University of Helsinki (Tiedemann et al., 2016) University of Sheffield (Blain et al., 2016) Uppsala University (Tiedemann et al., 2016) Yandex School of Data Analysis (Dvorkovich et al., 2016) Four online statistical machine translation systems Table 2: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the commercial and online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion consistent with previous years of the workshop. 136 Figure 2: Screenshot of the Appraise interface used in the human evaluation campaign. The annotator is presented with a source segment, a reference translation, and up to five outputs from competing systems (anonymized"
W16-2301,W13-2305,1,0.509145,"Missing"
W16-2301,W15-3036,0,0.0603245,"Missing"
W16-2301,E14-1047,1,0.831614,"Missing"
W16-2301,W16-2378,0,0.168663,"ts for pairs of systems competing in the news domain translation task (en-ru), where a green cell denotes a significantly higher DA adequacy score for the system in a given row over the system in a given column. cs-en fi-en tr-en de-en ru-en ro-en 0.997 0.996 0.988 0.964 0.961 0.920 en-ru 0.975 DA Correlation with RR Table 11: Correlation between overall DA standardized mean adequacy scores and RR Trueskill scores. 150 4 IT Translation Task from the QTLeap project: HF&FCUL for Portuguese, UPV/EHU for Spanish and Basque, IICT-BAS for Bulgarian, CUNI for Czech and UG for Dutch). Duma and Menzel (2016) describe UHDS- DOC 2 VEC and UHBS- LMI (University of Hamburg). Pahari et al. (2016) describe JU-USAAR (Jadavpur University & Saarland University). Cuong et al. (2016) describe ILLC-U VA-S CORPIO (University of Amsterdam). IILC-U VA-DS is based on Hoang and Sima’an (2014). PROMT-RULE - BASED and PROMT-H YBRID systems were submitted by the PROMT LLC company and they are not described in any paper. QTL -M OSES is the standard Moses setup (MERT-tuned on the in-domain training data, but otherwise without any domain-adaptation) and serves as a baseline. The IT-domain translation task introduced th"
W16-2301,W11-2123,0,0.0116622,"s were also evaluated against a reimplementation of the approach firstly proposed by Simard et al. (2007).36 Last year, in fact, this statistical post-editing approach represented the common backbone of all submissions (this is also reflected by the close results achieved by participants in the pilot task). For this purpose, a phrasebased SMT system based on Moses (Koehn et al., 2007) was used. Translation and reordering models were estimated following the Moses protocol with default setup using MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the KenLM toolkit (Heafield, 2011) for standard n-gram modeling with an n-gram length of 5. Finally, the APE system was tuned on Adam Mickiewicz University. This system is among the very first ones exploring the application of neural translation models to the APE task. In particular, it investigates the following aspects: i) the use of artificially-created postedited data to train the neural models, ii) the loglinear combination of monolingual and bilingual models in an ensemble-like manner, iii) the addition of task-specific features in the log-linear model to control the final output quality. Concerning the data, in addition"
W16-2301,W16-2384,0,0.0416841,"Missing"
W16-2301,W13-0805,0,0.0110515,"of words in the reference. Lower TER values indicate lower distance from the reference as a proxy for higher MT quality. 33 http://www.cs.umd.edu/˜snover/tercom/ 34 https://github.com/moses-smt/mosesdecoder/ blob/master/scripts/generic/multi-bleu.perl 27 The source sentences (together with their reference translations which were not used for the task) were provided by TAUS (https://www.taus.net/) and originally come from a unique IT vendor. 28 It consists of a phrase-based machine translation system leveraging generic and in-domain parallel training data and using a pre-reordering technique (Herrmann et al., 2013). It takes also advantages of POS and word class-based language models. 29 German native speakers working at Text&Form https: //www.textform.com/. 176 Train (12,000) Dev (1,000) Test (2,000) SRC 201,505 17,827 31,477 Tokens TGT 210,573 19,355 34,332 PE 214,720 19,763 35,276 Table 31: Types SRC TGT 9,328 14,185 2,931 3,333 3,908 4,695 Data statistics. APE@WMT15 APE@WMT16 (EN-ES, news, crowd) (EN-DE, IT, prof.) SRC 2.905 6.616 TGT 3.312 8.845 PE 3.085 8.245 Table 32: Repetition Rate (RR) of the WMT15 (EnglishSpanish, news domain, crowdsourced post-edits) and WMT16 (English-German, IT domain, pro"
W16-2301,W16-2315,1,0.824148,"Missing"
W16-2301,P07-2045,1,0.017773,"gets unmodified.35 Baseline results are reported in Table 34. 7.2 Participants Monolingual translation as another term of comparison. To get some insights about the progress with respect to the first pilot task, participating systems were also evaluated against a reimplementation of the approach firstly proposed by Simard et al. (2007).36 Last year, in fact, this statistical post-editing approach represented the common backbone of all submissions (this is also reflected by the close results achieved by participants in the pilot task). For this purpose, a phrasebased SMT system based on Moses (Koehn et al., 2007) was used. Translation and reordering models were estimated following the Moses protocol with default setup using MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the KenLM toolkit (Heafield, 2011) for standard n-gram modeling with an n-gram length of 5. Finally, the APE system was tuned on Adam Mickiewicz University. This system is among the very first ones exploring the application of neural translation models to the APE task. In particular, it investigates the following aspects: i) the use of artificially-created postedited data to train the neural models, ii)"
W16-2301,W16-2337,0,0.0376798,"Missing"
W16-2301,W16-2303,1,0.821661,"Missing"
W16-2301,L16-1582,1,0.790825,"Missing"
W16-2301,W16-2385,0,0.0304494,"Missing"
W16-2301,P16-2095,1,0.885979,"e package to extract these and other quality estimation features and perform model learning, with suggested methods for all levels of prediction. Participants, described in Section 6.2, could submit up to two systems for each task. Data used to build MT systems or internal system information (such as model scores or n-best lists) were made available on request for Tasks 1 and 2. 6.1 The γ,  and C parameters were optimised via grid search with 5-fold cross validation on the training set. Word-level baseline system: For Tasks 2 and 2p, the baseline features were extracted with the Marmot tool (Logacheva et al., 2016b). For the baseline system we used a number of features that have been found the most informative in previous research on word-level QE. Our baseline set of features is loosely based on the one described in (Luong et al., 2014). It contains the following 22 features: • Word count in the source and target sentences, source and target token count ratio. Although these features are sentence-level (i.e. their values will be the same for all words in a sentence), the length of a sentence might influence the probability of a word being wrong. Baseline systems Sentence-level baseline system: For Tas"
W16-2301,W15-3037,0,0.00849978,"stems were submitted for the word-level task. UNBABEL 2 linear is a feature-based linear sequential model. It uses the baseline features provided by the shared task organisers (with slight changes) conjoined with individual labels and pairs of consecutive labels. It also uses various syntactic dependency-based features (dependency relations, heads, and second-order structures like siblings and grandparents). The syntactic dependencies are predicted with TurboParser trained on the TIGER German treebank. UNBABEL 2 ensemble uses a stacked architecture, inspired by the last year’s QUETCH+ system (Kreutzer et al., 2015), which combines three neural systems: one feedforward and two recurrent ones. The predictions of these systems are added as additional features in the linear system above. The following external resources were used: part-of-speech tags and extra syntactic dependency information obtained with TurboTagger and TurboParser (Martins et al., 2013), trained on the Penn Treebank (for English) and on the version of the German TIGER corpus used in the SPMRL shared task (Seddah et al., 2014) for German. For the neural models, pretrained word embeddings from Polyglot (AlRfou et al., 2013) and those produ"
W16-2301,W14-3342,0,0.0237077,"Data used to build MT systems or internal system information (such as model scores or n-best lists) were made available on request for Tasks 1 and 2. 6.1 The γ,  and C parameters were optimised via grid search with 5-fold cross validation on the training set. Word-level baseline system: For Tasks 2 and 2p, the baseline features were extracted with the Marmot tool (Logacheva et al., 2016b). For the baseline system we used a number of features that have been found the most informative in previous research on word-level QE. Our baseline set of features is loosely based on the one described in (Luong et al., 2014). It contains the following 22 features: • Word count in the source and target sentences, source and target token count ratio. Although these features are sentence-level (i.e. their values will be the same for all words in a sentence), the length of a sentence might influence the probability of a word being wrong. Baseline systems Sentence-level baseline system: For Task 1, QuEst++13 (Specia et al., 2015) was used to extract 17 features from the SMT source/target language training corpus: • Number of tokens in source & target sentences. • Target token, its left and right contexts of one word."
W16-2301,W16-2318,0,0.0145349,"Missing"
W16-2301,N06-1014,0,0.0215966,"d on n previous translation and reordering decisions. This technique is able to model both local and long-range reorderings that are quite useful when dealing with the German language. To improve the capability of choosing the correct edit to process, eight new features are added to the loglinear model. These features capture the cost of deleting a phrase and different information on possible gaps in reordering operations. The monolingual alignments between the MT outputs and their post-edits are computed using different methods based on TER, METEOR (Snover et al., 2006) and Berkeley Aligner (Liang et al., 2006). Only the task data is used for these submissions. 7.3 179 TER/BLEU results ID AMU Primary AMU Contrastive FBK Contrastive FBK Primary USAAR Primary USAAR Constrastive CUNI Primary (Simard et al., 2007) Baseline DCU Contrastive JUSAAR Primary JUSAAR Contrastive DCU Primary Avg. TER 21.52 23.06 23.92 23.94 24.14 24.14 24.31 24.64 24.76 26.79 26.92 26.97 28.97 BLEU 67.65 66.09 64.75 64.75 64.10 64.00 63.32 63.47 62.11 58.60 59.44 59.18 55.19 tained this year by the top runs can only be reached by moving from the basic statistical MT backbone shared by all last year’s participants to new and mor"
W16-2301,P13-2109,0,0.0179517,"eads, and second-order structures like siblings and grandparents). The syntactic dependencies are predicted with TurboParser trained on the TIGER German treebank. UNBABEL 2 ensemble uses a stacked architecture, inspired by the last year’s QUETCH+ system (Kreutzer et al., 2015), which combines three neural systems: one feedforward and two recurrent ones. The predictions of these systems are added as additional features in the linear system above. The following external resources were used: part-of-speech tags and extra syntactic dependency information obtained with TurboTagger and TurboParser (Martins et al., 2013), trained on the Penn Treebank (for English) and on the version of the German TIGER corpus used in the SPMRL shared task (Seddah et al., 2014) for German. For the neural models, pretrained word embeddings from Polyglot (AlRfou et al., 2013) and those produced with a neural MT system (Bahdanau et al., 2014) were used. UGENT-LT3 (Task 1, Task 2): The submissions for the word-level task use 41 features in combination with the baseline feature set to train binary classifiers. The 41 additional features attempt to capture accuracy errors (concerned with the meaning transfer from the source to targe"
W16-2301,W16-2387,0,0.0253511,"Missing"
W16-2301,W16-2317,0,0.0414065,"Missing"
W16-2301,N13-1090,0,0.00833788,"Missing"
W16-2301,2015.iwslt-papers.4,1,0.737239,"Missing"
W16-2301,W16-2319,0,0.0339318,"Missing"
W16-2301,W16-2386,1,0.814188,"e package to extract these and other quality estimation features and perform model learning, with suggested methods for all levels of prediction. Participants, described in Section 6.2, could submit up to two systems for each task. Data used to build MT systems or internal system information (such as model scores or n-best lists) were made available on request for Tasks 1 and 2. 6.1 The γ,  and C parameters were optimised via grid search with 5-fold cross validation on the training set. Word-level baseline system: For Tasks 2 and 2p, the baseline features were extracted with the Marmot tool (Logacheva et al., 2016b). For the baseline system we used a number of features that have been found the most informative in previous research on word-level QE. Our baseline set of features is loosely based on the one described in (Luong et al., 2014). It contains the following 22 features: • Word count in the source and target sentences, source and target token count ratio. Although these features are sentence-level (i.e. their values will be the same for all words in a sentence), the length of a sentence might influence the probability of a word being wrong. Baseline systems Sentence-level baseline system: For Tas"
W16-2301,L16-1470,1,0.826657,"Missing"
W16-2301,P03-1021,0,0.0340784,"7 Tokens TGT 210,573 19,355 34,332 PE 214,720 19,763 35,276 Table 31: Types SRC TGT 9,328 14,185 2,931 3,333 3,908 4,695 Data statistics. APE@WMT15 APE@WMT16 (EN-ES, news, crowd) (EN-DE, IT, prof.) SRC 2.905 6.616 TGT 3.312 8.845 PE 3.085 8.245 Table 32: Repetition Rate (RR) of the WMT15 (EnglishSpanish, news domain, crowdsourced post-edits) and WMT16 (English-German, IT domain, professional posteditors) APE Task data. 7.1.3 PE 16,388 3,506 5,047 SRC 5,628 1,922 2,479 Lemmas TGT PE 11,418 13,244 2,686 2,806 3,753 4,050 the development set, optimizing TER/BLEU with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison are also reported in Table 34. For each submitted run, the statistical significance of performance differences with respect to the baselines and the re-implementation of Simard et al. (2007) was calculated with the bootstrap test (Koehn, 2004). Baseline The official baseline results are the TER and BLEU scores calculated by comparing the raw MT output with the human post-edits. In practice, the baseline APE system is a system that leaves all the test targets unmodified.35 Baseline results are reported in Table 34. 7.2 Participants Monolingual"
W16-2301,W16-2338,0,0.0374669,"Missing"
W16-2301,J03-1002,0,0.0268639,"word alignments and bilingual distributed representations to introduce a new set of features for the sentence-Level QE task. The features extracted include three alignment-based features, three bilingual embedding-based features, two embeddingbased features constrained on alignment links, as well as a set of 74 bigrams used as boolean features. The set of bigrams represents the most frequent bigrams in translations that have changed after the post-edition, and they are compiled by aligning translations to their post-editions provided in the WMT QE datasets. To produce these features, GIZA++ (Och and Ney, 2003) was used for word alignment and Multivec (Berard et al., 2016) was used for the bilingual model, which jointly learns distributed representations for source and target languages using a parallel corpus. To build the bilingual model, domain-specific data compiled from the resources made available for the WMT 16 IT-Domain shared task was used. As prediction model, a Linear Regression model using scikit-learn was built using a combination of QuEst++ baseline features and the new features proposed. tional features for both tasks. UNBABEL (Task 2): Two systems were submitted for the word-level tas"
W16-2301,W16-2321,0,0.0372138,"Missing"
W16-2301,W16-2388,1,0.748184,"Missing"
W16-2301,W16-2333,0,0.0259256,"Missing"
W16-2301,W15-3026,0,0.0339859,"Missing"
W16-2301,W16-2379,1,0.819672,"Missing"
W16-2301,W16-2334,1,0.811656,"Missing"
W16-2301,P02-1040,0,0.105557,", eventually, make the APE task more feasible by automatic systems. Other changes concern the language combination and the evaluation mode. As regards the languages, we moved from English-Spanish to English-German, which is one of the language pairs covered by the QT21 Project26 that supported data collection and post-editing. Concerning the evaluation, we changed from TER scores computed both in case-sensitive and caseinsensitive mode to a single ranking based on case sensitive measurements. Besides these changes the new round of the APE task included some extensions in the evaluation. BLEU (Papineni et al., 2002) has been introduced as a secondary evaluation metric to measure the improvements over the rough MT output. In addition, to gain further insights on final output quality, a subset of the outputs of the submitted systems has also been manually evaluated. Based on these changes and extensions, the goals of this year’s shared task were to: i) improve and stabilize the evaluation framework in view of future rounds, ii) analyze the effect on task 26 feasibility of data coming from a narrow domain, iii) analyze the effect of post-edits collected from professional translators, iv) analyze how humans"
W16-2301,W16-2389,0,0.0210658,"Missing"
W16-2301,W16-2390,0,0.0283246,"Missing"
W16-2301,W16-2322,0,0.0139204,"ng and evaluating the manual translations has settled into the following pattern. We ask human annotators to rank the outputs of five systems. From these rankings, we produce pairwise translation comparisons, and then evaluate them with a version of the TrueSkill algorithm adapted to our task. We refer to this approach (described in Section 3.4) as the relative ranking approach (RR), so named because the pairwise comparisons denote only relative ability between a pair of systems, and cannot be used to infer their absolute quality. These results are used to produce the official ranking for the WMT 2016 tasks. However, work in evaluation over the past few years has provided fresh insight into ways to collect direct assessments (DA) of machine translation quality. In this setting, annotators are asked to provide an assessment of the direct quality of the output of a system relative to a reference translation. In order to evaluate the potential of this approach for future WMT evaluations, we conducted a direct assessment evaluation in parallel. This evaluation, together with a comparison of the official results, is described in Section 3.5. 2.3 3.1 2.2 Training data As in past years we provide"
W16-2301,W16-2392,1,0.772695,"Missing"
W16-2301,L16-1649,1,0.704564,"): one for the 17 baseline features and another for the 500 features from the embeddings. Since each kernel has its own set of hyperparameters, the full model can leverage the contributions from the two different sets. The second system (GRAPH-DISC) combines the baseline features with discourse-aware features. The discourse aware features are the same as the ones used by Scarton et al. (2015a) plus Latent Semantic Analysis (LSA) cohesion features (Scarton and Specia, 2014), number of subtrees and height of the Rhetorical Structure Theory (RST) tree and entity graph-based coherence scores (Sim Smith et al., 2016). Discourseaware and RST tree features were extracted only for English (tools are only available for this language), LSA features were extracted for both languages, and entity graph-based coherence scores were extracted for the target language only (Spanish), as the source documents are expected to be coherent. This QE model was trained with an SVR algorithm. YSDA (Task 1): The YSDA submission is based on a simple idea that the more complex the sentence is the more difficult it is to translate. For this purpose, it uses information provided by syntactic parsing (information from parsing trees,"
W16-2301,W16-2391,1,0.796337,"Missing"
W16-2301,2014.eamt-1.21,1,0.643048,"e extracted by averaging word embeddings in the document. The GP model was trained with two Rational Quadratic kernels (Rasmussen and Williams, 2006): one for the 17 baseline features and another for the 500 features from the embeddings. Since each kernel has its own set of hyperparameters, the full model can leverage the contributions from the two different sets. The second system (GRAPH-DISC) combines the baseline features with discourse-aware features. The discourse aware features are the same as the ones used by Scarton et al. (2015a) plus Latent Semantic Analysis (LSA) cohesion features (Scarton and Specia, 2014), number of subtrees and height of the Rhetorical Structure Theory (RST) tree and entity graph-based coherence scores (Sim Smith et al., 2016). Discourseaware and RST tree features were extracted only for English (tools are only available for this language), LSA features were extracted for both languages, and entity graph-based coherence scores were extracted for the target language only (Spanish), as the source documents are expected to be coherent. This QE model was trained with an SVR algorithm. YSDA (Task 1): The YSDA submission is based on a simple idea that the more complex the sentence"
W16-2301,W15-3040,1,0.909092,"on-word corpus,18 with a vocabulary size of 527K words. Document embeddings are extracted by averaging word embeddings in the document. The GP model was trained with two Rational Quadratic kernels (Rasmussen and Williams, 2006): one for the 17 baseline features and another for the 500 features from the embeddings. Since each kernel has its own set of hyperparameters, the full model can leverage the contributions from the two different sets. The second system (GRAPH-DISC) combines the baseline features with discourse-aware features. The discourse aware features are the same as the ones used by Scarton et al. (2015a) plus Latent Semantic Analysis (LSA) cohesion features (Scarton and Specia, 2014), number of subtrees and height of the Rhetorical Structure Theory (RST) tree and entity graph-based coherence scores (Sim Smith et al., 2016). Discourseaware and RST tree features were extracted only for English (tools are only available for this language), LSA features were extracted for both languages, and entity graph-based coherence scores were extracted for the target language only (Spanish), as the source documents are expected to be coherent. This QE model was trained with an SVR algorithm. YSDA (Task 1)"
W16-2301,2006.amta-papers.25,0,0.850121,"ombinations of several features. A regression model was training to predict BLEU as target metric instead HTER. The machine learning pipeline uses an SVR with RBF kernel to predict BLEU scores, followed by a linear SVR to predict HTER scores from BLEU scores. As external resources, the system uses a syntactic parser, pseudo-references and back-translation from web-scale MT system, and a web-scale language model. 6.3 Task 1: Predicting sentence-level quality This task consists in scoring (and ranking) translation sentences according to the percentage of their words that need to be fixed. HTER (Snover et al., 2006) is used as quality score, i.e. the minimum edit distance between the machine translation and its manually post-edited version in [0,1]. As in previous years, two variants of the results could be submitted: • Scoring: An absolute HTER score for each sentence translation, to be interpreted as an error metric: lower scores mean better translations. • Ranking: A ranking of sentence translations for all source sentences from best to worst. For this variant, it does not matter how the ranking is produced (from HTER predictions or by other means). The reference ranking is defined based on the true H"
W16-2301,W15-4916,1,0.824025,"Missing"
W16-2301,W16-2346,1,0.0602876,"Missing"
W16-2301,W16-2325,1,0.816227,"Missing"
W16-2301,W16-2393,0,0.033041,"Missing"
W16-2301,W16-2326,0,0.0221449,"Missing"
W16-2301,W16-2327,1,0.737603,"Missing"
W16-2301,W16-2328,0,0.0403946,"Missing"
W16-2301,C00-2137,0,0.0384283,"Missing"
W16-2301,D07-1091,1,\N,Missing
W16-2301,W09-0401,1,\N,Missing
W16-2301,P11-1105,0,\N,Missing
W16-2301,N07-1064,0,\N,Missing
W16-2301,W04-3250,1,\N,Missing
W16-2301,P15-4020,1,\N,Missing
W16-2301,W16-2377,1,\N,Missing
W16-2301,aziz-etal-2012-pet,1,\N,Missing
W16-2301,2012.eamt-1.31,0,\N,Missing
W16-2301,C14-1182,0,\N,Missing
W16-2301,W16-2316,0,\N,Missing
W16-2301,W16-2332,1,\N,Missing
W16-2301,W16-2320,1,\N,Missing
W16-2301,W16-2335,0,\N,Missing
W16-2301,W16-2329,0,\N,Missing
W16-2301,W16-2383,0,\N,Missing
W16-2301,W16-2381,1,\N,Missing
W16-2301,W16-2312,0,\N,Missing
W16-2301,P16-1162,1,\N,Missing
W16-2301,W13-2243,1,\N,Missing
W16-2301,W08-0509,0,\N,Missing
W16-2301,2015.eamt-1.17,1,\N,Missing
W16-2301,W14-6111,0,\N,Missing
W16-2301,2012.tc-1.5,1,\N,Missing
W16-2301,W16-2347,1,\N,Missing
W16-2301,W16-2314,0,\N,Missing
W16-2310,P14-1129,0,0.0580228,"Missing"
W16-2310,P13-2071,1,0.895955,"e investigated include: neural probabilistic language models, bilingual neural network language models, morphological segmentation, and the attentionbased neural machine translation model as reranking feature. 1 2.1 Basic Configuration We trained our systems with the following settings: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, hierarchical lexicalized reordering (Galley and Manning, 2008), a lexicallydriven 5-gram operation sequence model (OSM) (Durrani et al., 2013) with 4 count-based supportive features, sparse domain indicator, phrase length, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 100-best translation options, compact phrase table (JunczysDowmunt, 2012) minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the noreordering-over-punctuation heuristic (Koehn and Haddow, 2009). We optimize feature function weights with k-best MIRA (Cherry and Foster, 2012). We used POS"
W16-2310,D08-1023,0,0.228814,"tion, and the attentionbased neural machine translation model as reranking feature. 1 2.1 Basic Configuration We trained our systems with the following settings: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, hierarchical lexicalized reordering (Galley and Manning, 2008), a lexicallydriven 5-gram operation sequence model (OSM) (Durrani et al., 2013) with 4 count-based supportive features, sparse domain indicator, phrase length, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 100-best translation options, compact phrase table (JunczysDowmunt, 2012) minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the noreordering-over-punctuation heuristic (Koehn and Haddow, 2009). We optimize feature function weights with k-best MIRA (Cherry and Foster, 2012). We used POS and morphological tags as additional factors in phrase translation models (Koehn and Hoang, 2007) for the German-English language"
W16-2310,D08-1089,0,0.0787452,"ll 12 language pairs of this year’s evaluation campaign. Novel research directions we investigated include: neural probabilistic language models, bilingual neural network language models, morphological segmentation, and the attentionbased neural machine translation model as reranking feature. 1 2.1 Basic Configuration We trained our systems with the following settings: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, hierarchical lexicalized reordering (Galley and Manning, 2008), a lexicallydriven 5-gram operation sequence model (OSM) (Durrani et al., 2013) with 4 count-based supportive features, sparse domain indicator, phrase length, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 100-best translation options, compact phrase table (JunczysDowmunt, 2012) minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the noreordering-over-punctuation heuristic (Koehn and Haddow, 2009). We optimize"
W16-2310,buck-etal-2014-n,0,0.124801,"Missing"
W16-2310,W15-3013,1,0.821897,"l language pairs, the use of all fea275 Language Turkish Turkish Turkish Turkish Turkish Turkish English Threshold none 0 2 5 10 20 none Token count 8806 9606 9935 10169 10416 10720 11514 Method baseline Byte-Pair Chipmunk Chipmunk Chipmunk Chipmunk Morfessor Morfessor Morfessor Morfessor Morfessor Table 5: Token counts for different thresholds for Morfessor segmentation consecutive bytes with a symbol that does not occur elsewhere. Each such replacement is called a merge, and the number of merges is a tunable parameter. The original text can be recovered using a lookup-table. Sennrich et al. (2015) applied this to word segmentation, and demonstrate its success at solving the large vocabulary problem in neural machine translation. To create our training data for the Morfessor and ChipMunk experiments, we augment the original training data with a second copy that has been segmented. For the tuning and test data, we only segment words that occur infrequently. This allows frequent words to be translated directly, but also allows the system to learn from the subword units of all words, including frequent ones. The number and type of subword units in each word segmented by byte pair encoding"
W16-2310,N12-1047,0,0.270491,"ce model (OSM) (Durrani et al., 2013) with 4 count-based supportive features, sparse domain indicator, phrase length, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 100-best translation options, compact phrase table (JunczysDowmunt, 2012) minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the noreordering-over-punctuation heuristic (Koehn and Haddow, 2009). We optimize feature function weights with k-best MIRA (Cherry and Foster, 2012). We used POS and morphological tags as additional factors in phrase translation models (Koehn and Hoang, 2007) for the German-English language pairs. We also trained target sequence models on the in-domain subset of the parallel corpus using Kneser-Ney smoothed 7-gram models. We used syntactic preordering (Collins et al., 2005) Introduction The JHU 2016 WMT submission consists of phrase-based systems, hierarchical phrase-based systems, and syntax-based systems. In this paper we discuss features that we integrated into our system submissions. We also discuss the experiments we did with morphol"
W16-2310,W11-2123,0,0.143406,"phrase-based, hierarchical phrase-based and syntax-based systems for all 12 language pairs of this year’s evaluation campaign. Novel research directions we investigated include: neural probabilistic language models, bilingual neural network language models, morphological segmentation, and the attentionbased neural machine translation model as reranking feature. 1 2.1 Basic Configuration We trained our systems with the following settings: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, hierarchical lexicalized reordering (Galley and Manning, 2008), a lexicallydriven 5-gram operation sequence model (OSM) (Durrani et al., 2013) with 4 count-based supportive features, sparse domain indicator, phrase length, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 100-best translation options, compact phrase table (JunczysDowmunt, 2012) minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and th"
W16-2310,P07-1019,0,0.143673,"nments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, hierarchical lexicalized reordering (Galley and Manning, 2008), a lexicallydriven 5-gram operation sequence model (OSM) (Durrani et al., 2013) with 4 count-based supportive features, sparse domain indicator, phrase length, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 100-best translation options, compact phrase table (JunczysDowmunt, 2012) minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the noreordering-over-punctuation heuristic (Koehn and Haddow, 2009). We optimize feature function weights with k-best MIRA (Cherry and Foster, 2012). We used POS and morphological tags as additional factors in phrase translation models (Koehn and Hoang, 2007) for the German-English language pairs. We also trained target sequence models on the in-domain subset of the parallel corpus using Kneser-Ney smoothed 7-gram models. We used syntactic preordering (Collins et al., 2005) Introduction The JHU 2016 WMT submission consists of"
W16-2310,N09-1025,0,0.0734239,"Missing"
W16-2310,2012.eamt-1.58,0,0.410346,"Missing"
W16-2310,P05-1066,1,0.655469,"isk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the noreordering-over-punctuation heuristic (Koehn and Haddow, 2009). We optimize feature function weights with k-best MIRA (Cherry and Foster, 2012). We used POS and morphological tags as additional factors in phrase translation models (Koehn and Hoang, 2007) for the German-English language pairs. We also trained target sequence models on the in-domain subset of the parallel corpus using Kneser-Ney smoothed 7-gram models. We used syntactic preordering (Collins et al., 2005) Introduction The JHU 2016 WMT submission consists of phrase-based systems, hierarchical phrase-based systems, and syntax-based systems. In this paper we discuss features that we integrated into our system submissions. We also discuss the experiments we did with morphological pre-processing and neural reranking. The JHU phrase-based translation systems for our participation in the WMT 2016 shared translation task1 are based on the open source Moses toolkit (Koehn et al., 2007). We built upon strong baselines of the Edinburgh-JHU joint WMT submissions from the last year (Haddow et al., 2015), t"
W16-2310,K15-1017,0,0.0842479,"Missing"
W16-2310,D07-1091,1,0.808965,"gth, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 100-best translation options, compact phrase table (JunczysDowmunt, 2012) minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the noreordering-over-punctuation heuristic (Koehn and Haddow, 2009). We optimize feature function weights with k-best MIRA (Cherry and Foster, 2012). We used POS and morphological tags as additional factors in phrase translation models (Koehn and Hoang, 2007) for the German-English language pairs. We also trained target sequence models on the in-domain subset of the parallel corpus using Kneser-Ney smoothed 7-gram models. We used syntactic preordering (Collins et al., 2005) Introduction The JHU 2016 WMT submission consists of phrase-based systems, hierarchical phrase-based systems, and syntax-based systems. In this paper we discuss features that we integrated into our system submissions. We also discuss the experiments we did with morphological pre-processing and neural reranking. The JHU phrase-based translation systems for our participation in t"
W16-2310,E03-1076,1,0.762597,"ny, August 11-12, 2016. 2016 Association for Computational Linguistics Language Pair German–English Czech–English Finnish–English Romanian–English Russian-English Turkish–English Language Czech German English Finnish Romanian Russian Turkish Sentences 19,074 19,074 1,500 943 9,006 500 Tokens 6.7 billion 65.2 billion 65.1 billion 2.9 billion 8.1 billion 23.3 billion 11.9 billion LM Size 13GB 107GB 89GB 8GB 13GB 41GB 23GB Table 1: Tuning set sizes for phrase-based system Table 2: Sizes of the language model trained on the monomlingual corpora extracted from Common Crawl. and compound splitting (Koehn and Knight, 2003) for the German-to-English systems. We did no language-specific processing for any other language. The systems were tuned on a very large tuning set consisting of the test sets from 2008-2014, with a total of 19,074 sentences (see Table 1). We used news-test 2015 as development test set. Significantly less tuning data was available for Finnish, Romanian, and Turkish. 2.2 2.3 Huge Language Model This year, large corpora of monolingual data were extracted from Common Crawl (Buck et al., 2014). We used this data to train 5-gram KneserNey smoothed language models, pruning out 3– 5 gram singletons."
W16-2310,P07-2045,1,0.0124021,"the in-domain subset of the parallel corpus using Kneser-Ney smoothed 7-gram models. We used syntactic preordering (Collins et al., 2005) Introduction The JHU 2016 WMT submission consists of phrase-based systems, hierarchical phrase-based systems, and syntax-based systems. In this paper we discuss features that we integrated into our system submissions. We also discuss the experiments we did with morphological pre-processing and neural reranking. The JHU phrase-based translation systems for our participation in the WMT 2016 shared translation task1 are based on the open source Moses toolkit (Koehn et al., 2007). We built upon strong baselines of the Edinburgh-JHU joint WMT submissions from the last year (Haddow et al., 2015), the Edinburgh syntax-based system submissions from the last year (Williams et al., 2015) as well as recent research in the field (Vaswani et al., 2013; Devlin et al., 2014). We also used the Apache Joshua translation toolkit (Post et al., 2015) to build hierarchical systems for two language tasks. 1 Moses Phrase-Based Systems http://www.statmt.org/wmt16 272 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 272–280, c Berlin, Germany"
W16-2310,N04-1022,0,0.203242,"inal-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, hierarchical lexicalized reordering (Galley and Manning, 2008), a lexicallydriven 5-gram operation sequence model (OSM) (Durrani et al., 2013) with 4 count-based supportive features, sparse domain indicator, phrase length, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 100-best translation options, compact phrase table (JunczysDowmunt, 2012) minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the noreordering-over-punctuation heuristic (Koehn and Haddow, 2009). We optimize feature function weights with k-best MIRA (Cherry and Foster, 2012). We used POS and morphological tags as additional factors in phrase translation models (Koehn and Hoang, 2007) for the German-English language pairs. We also trained target sequence models on the in-domain subset of the parallel corpus using Kneser-Ney smoothed 7-gram models. We used syntactic preordering (Collins et al., 2005) Introduction T"
W16-2310,E99-1010,0,0.314445,"lz, as we did all other language models. We compressed the language models with KenLM with 4-bit quantization and use of the trie data structure. The resulting size of the language model is listed in Table 2. The largest language model is the German model at 107GB, trained on 65.2 billion tokens, about an order of magnitude larger than previous data. Och Clusters As in last year’s system, we use word classes in four feature functions: (i) the language model, (ii) the operation sequence model, (iii) the reordering model, and the (iv) sparse word translation features. We generated Och clusters (Och, 1999) — a variant of Brown clusters — using mkcls. We have to choose a hyper parameter: the number of clusters. Our experiments and also prior work (Stewart et al., 2014) suggest that instead of committing to a single value, it is beneficial to use multiple numbers and use them in multiple feature functions concurrently. We used 50, 200, 600, and 2000 clusters, hence having 4 additional interpolated language models, 4 additional operation sequence models, 4 additional lexicalized reordering models, and 4 additional sets of sparse features. The feature functions for word classes were trained exactly"
W16-2310,2014.amta-researchers.3,0,0.0345605,"lting size of the language model is listed in Table 2. The largest language model is the German model at 107GB, trained on 65.2 billion tokens, about an order of magnitude larger than previous data. Och Clusters As in last year’s system, we use word classes in four feature functions: (i) the language model, (ii) the operation sequence model, (iii) the reordering model, and the (iv) sparse word translation features. We generated Och clusters (Och, 1999) — a variant of Brown clusters — using mkcls. We have to choose a hyper parameter: the number of clusters. Our experiments and also prior work (Stewart et al., 2014) suggest that instead of committing to a single value, it is beneficial to use multiple numbers and use them in multiple feature functions concurrently. We used 50, 200, 600, and 2000 clusters, hence having 4 additional interpolated language models, 4 additional operation sequence models, 4 additional lexicalized reordering models, and 4 additional sets of sparse features. The feature functions for word classes were trained exactly the same way as the corresponding feature functions for words. For instance, this means that the word class language model required training of individual models on"
W16-2310,D13-1140,0,0.0599603,"ystems. In this paper we discuss features that we integrated into our system submissions. We also discuss the experiments we did with morphological pre-processing and neural reranking. The JHU phrase-based translation systems for our participation in the WMT 2016 shared translation task1 are based on the open source Moses toolkit (Koehn et al., 2007). We built upon strong baselines of the Edinburgh-JHU joint WMT submissions from the last year (Haddow et al., 2015), the Edinburgh syntax-based system submissions from the last year (Williams et al., 2015) as well as recent research in the field (Vaswani et al., 2013; Devlin et al., 2014). We also used the Apache Joshua translation toolkit (Post et al., 2015) to build hierarchical systems for two language tasks. 1 Moses Phrase-Based Systems http://www.statmt.org/wmt16 272 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 272–280, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics Language Pair German–English Czech–English Finnish–English Romanian–English Russian-English Turkish–English Language Czech German English Finnish Romanian Russian Turkish Sentences 19,074 19,074 1,50"
W16-2310,W09-0429,1,\N,Missing
W16-2310,P03-1020,0,\N,Missing
W16-2310,P16-1162,0,\N,Missing
W16-2310,W14-3324,1,\N,Missing
W16-2347,W11-1218,0,0.216759,"uent processing steps like text normalization and deduplication. 2.2 2.4 A final stage of the processing pipeline filters out bad sentence pairs. These exist either because the original web site did not have any actual parallel data (garbage in, garbage out), or due to failures of earlier processing steps. As Rarrick et al. (2011) point out, a key problem for parallel corpora extracted from the web is filtering out translations that have been created by machine translation. Venugopal et al. (2011) propose a method to watermark the output of machine translation systems to aid this distinction. Antonova and Misyurev (2011) report that rulebased machine translation output can be detected due to certain word choices, and machine translation output due to lack of reordering. This year, a shared task on sentence pair filtering8 was organized, albeit in the context of cleaning translation memories which tend to be cleaner that the data at the end of a pipeline that starts with web crawls. Document Alignment Document alignment can be defined as a matching task that takes a pair of documents and computes a score that reflects the likelihood that they are translations of each others. Common choices include edit-distanc"
W16-2347,W16-2364,0,0.0335494,"Missing"
W16-2347,bojar-etal-2010-evaluating,0,0.0694751,"Missing"
W16-2347,C10-2010,0,0.0910781,"hardly any description of the methods used. Our data preparation for the shared task builds partly on Bitextor10 , which is a comprehensive pipeline from corpus crawling to sentence pair cleaning (Espl`a-Gomis, 2009). Sentence Alignment The topic of sentence alignment has received a lot of attention, dating back to the early 1990s with the influential Church and Gale algorithm that is language-independent and easy to implement. It relies on relative sentence lengths for alignment decisions and hence is not tolerant to noisy input. Popular tools are Hunalign4 (Varga et al., 2005), Gargantua5 (Braune and Fraser, 2010), Bilingual Sentence Aligner (Moore, 2002) Bleualign6 (Sennrich and Volk, 2010), and Champollion7 (Ma, 2006). Shi and Zhou (2008) make use of the HTML structure to guide alignment. All of these use bilingual lexicons which may have to be provided upfront or are learned unsupervised. 2 https://www.httrack.com/ http://nlp.ilsp.gr/redmine/projects/ilsp-fc 4 http://mokk.bme.hu/en/resources/hunalign/ 5 https://sourceforge.net/projects/gargantua/ 6 https://github.com/rsennrich/Bleualign 7 https://sourceforge.net/projects/champollion/ 3 8 NLP4TM 2016: Shared task http://rgcl.wlv.ac.uk/nlp4tm2016/shar"
W16-2347,W16-2365,1,0.754207,"Missing"
W16-2347,W09-0401,1,0.791977,"ocument alignment can be defined as a matching task that takes a pair of documents and computes a score that reflects the likelihood that they are translations of each others. Common choices include edit-distance between linearized documents (Resnik and Smith, 2003), cosine distance of idfweighted bigram vectors (Uszkoreit et al., 2010), and probability of a probabilistic DOM-tree alignment model (Shi et al., 2006). 2.3 Filtering 2.5 Comprehensive Tools For a few language pairs, there have been individual efforts to cast a wider net, such as the billion word French–English corpus collected by Callison-Burch et al. (2009), or a 200 million word Czech–English corpus collected by Bojar et al. (2010). Smith et al. (2013) present a set of fairly basic tools to extract parallel data from the publicly available web crawl CommonCrawl9 . In all these cases, the corpus collection effort reinvented the wheel and wrote dedicated scripts to download web pages, extract text, and align sentences, with hardly any description of the methods used. Our data preparation for the shared task builds partly on Bitextor10 , which is a comprehensive pipeline from corpus crawling to sentence pair cleaning (Espl`a-Gomis, 2009). Sentence"
W16-2347,W16-2366,0,0.0731457,"Missing"
W16-2347,W16-2371,0,0.070723,"Missing"
W16-2347,2009.mtsummit-btm.6,0,0.0680966,"Missing"
W16-2347,W16-2372,0,0.0618504,"Missing"
W16-2347,ma-2006-champollion,0,0.0399383,"s a comprehensive pipeline from corpus crawling to sentence pair cleaning (Espl`a-Gomis, 2009). Sentence Alignment The topic of sentence alignment has received a lot of attention, dating back to the early 1990s with the influential Church and Gale algorithm that is language-independent and easy to implement. It relies on relative sentence lengths for alignment decisions and hence is not tolerant to noisy input. Popular tools are Hunalign4 (Varga et al., 2005), Gargantua5 (Braune and Fraser, 2010), Bilingual Sentence Aligner (Moore, 2002) Bleualign6 (Sennrich and Volk, 2010), and Champollion7 (Ma, 2006). Shi and Zhou (2008) make use of the HTML structure to guide alignment. All of these use bilingual lexicons which may have to be provided upfront or are learned unsupervised. 2 https://www.httrack.com/ http://nlp.ilsp.gr/redmine/projects/ilsp-fc 4 http://mokk.bme.hu/en/resources/hunalign/ 5 https://sourceforge.net/projects/gargantua/ 6 https://github.com/rsennrich/Bleualign 7 https://sourceforge.net/projects/champollion/ 3 8 NLP4TM 2016: Shared task http://rgcl.wlv.ac.uk/nlp4tm2016/shared-task/ 9 http://commoncrawl.org/ 10 https://sourceforge.net/p/bitextor/wiki/Home/ 555 3 Training and Test"
W16-2347,W16-2367,0,0.0688636,"Missing"
W16-2347,W16-2373,0,0.0400566,"Missing"
W16-2347,W16-2368,0,0.0646341,"Missing"
W16-2347,W16-2374,0,0.0855075,"Missing"
W16-2347,W16-2369,0,0.142776,"Missing"
W16-2347,moore-2002-fast,0,0.344353,"preparation for the shared task builds partly on Bitextor10 , which is a comprehensive pipeline from corpus crawling to sentence pair cleaning (Espl`a-Gomis, 2009). Sentence Alignment The topic of sentence alignment has received a lot of attention, dating back to the early 1990s with the influential Church and Gale algorithm that is language-independent and easy to implement. It relies on relative sentence lengths for alignment decisions and hence is not tolerant to noisy input. Popular tools are Hunalign4 (Varga et al., 2005), Gargantua5 (Braune and Fraser, 2010), Bilingual Sentence Aligner (Moore, 2002) Bleualign6 (Sennrich and Volk, 2010), and Champollion7 (Ma, 2006). Shi and Zhou (2008) make use of the HTML structure to guide alignment. All of these use bilingual lexicons which may have to be provided upfront or are learned unsupervised. 2 https://www.httrack.com/ http://nlp.ilsp.gr/redmine/projects/ilsp-fc 4 http://mokk.bme.hu/en/resources/hunalign/ 5 https://sourceforge.net/projects/gargantua/ 6 https://github.com/rsennrich/Bleualign 7 https://sourceforge.net/projects/champollion/ 3 8 NLP4TM 2016: Shared task http://rgcl.wlv.ac.uk/nlp4tm2016/shared-task/ 9 http://commoncrawl.org/ 10 http"
W16-2347,W16-2370,0,0.0441817,"Missing"
W16-2347,I13-2006,0,0.012596,"g steps in acquiring parallel corpora from the web: aligning bilingual documents from crawled web sites. The task is to identify pairs of English and French documents from a given collection of documents such that one document is the translation of the other. As possible pairs we consider all pairs of documents from the same webdomain for which the source side has been identified as (mostly) English and the target side as (mostly) French. Lack of data in some cases has held back research. To give an example, there are significant research efforts on various Indic languages (Post et al., 2012; Joshi et al., 2013; Singh, 2013), but this work has been severely hampered, since it uses very small amounts of data. But even for the language pairs tackled in high profile evaluation campaigns, such as the ones organized around WMT, IWSLT, and even NIST, we use magnitudes of data less than what has been reported to be used in the large-scale efforts of Google or Microsoft. This diminishes the value of research findings: reported improvements for methods may not hold up • • • • • Identifying web sites with bilingual content Crawling web sites Document alignment Sentence alignment Sentence pair filtering For ea"
W16-2347,W16-2375,0,0.0754983,"Missing"
W16-2347,2005.mtsummit-papers.11,1,0.394173,"Missing"
W16-2347,W13-2506,0,0.0227119,"awling is a topic that has not received much attention from a specific natural language processing perspective. There are a number of challenges, such as identification of web sites with multilingual content, avoiding to crawl web pages with identical textual content, learning how often to recrawl web sites based on frequency of newly appearing content, avoiding crawling of large sites that have content in different languages that is not parallel, and so on. We used for the preparation of this shared task the tool Httrack2 which is a general web crawler that can be configured in various ways. Papavassiliou et al. (2013) present the focused crawler ILSP-FC3 that integrates crawling more closely with subsequent processing steps like text normalization and deduplication. 2.2 2.4 A final stage of the processing pipeline filters out bad sentence pairs. These exist either because the original web site did not have any actual parallel data (garbage in, garbage out), or due to failures of earlier processing steps. As Rarrick et al. (2011) point out, a key problem for parallel corpora extracted from the web is filtering out translations that have been created by machine translation. Venugopal et al. (2011) propose a"
W16-2347,P07-2045,1,0.0108621,"l later occurrences are ignored. To facilitate use of the .lett files we provide a simple reader class in Python. We make sure that the language id is reliable, at least for the documents in the train and test pairs. Text extraction was performed using an HTML5 parser. As the original HTML pages are available, participants are welcome to implement their own text extraction, for example to remove boilerplate. Additionally, we have identified spans of French text in French documents for which we produced English translations using MT. We use a basic Moses statistical machine translation engine (Koehn et al., 2007) trained on Europarl and News Commentary with decoding settings geared towards speed (no lexicalized reordering model, no additional language model, cube pruning with pop limit 500). These translations are not part of the lett files but provided separately. The format for the source segments and target segments is URL&lt;TAB&gt;Text where the same URL might occur multiple times if several lines/spans of French text were found. The URLs can be used to identify the corresponding documents in the .lett files. 3.6 Evaluation After we released the gold standard alignments, a number of participants pointe"
W16-2347,W12-3152,0,0.0466157,"Missing"
W16-2347,2009.mtsummit-posters.15,0,0.81965,"Missing"
W16-2347,skadins-etal-2014-billions,0,0.0713698,"Missing"
W16-2347,P13-1135,1,0.895537,"t reflects the likelihood that they are translations of each others. Common choices include edit-distance between linearized documents (Resnik and Smith, 2003), cosine distance of idfweighted bigram vectors (Uszkoreit et al., 2010), and probability of a probabilistic DOM-tree alignment model (Shi et al., 2006). 2.3 Filtering 2.5 Comprehensive Tools For a few language pairs, there have been individual efforts to cast a wider net, such as the billion word French–English corpus collected by Callison-Burch et al. (2009), or a 200 million word Czech–English corpus collected by Bojar et al. (2010). Smith et al. (2013) present a set of fairly basic tools to extract parallel data from the publicly available web crawl CommonCrawl9 . In all these cases, the corpus collection effort reinvented the wheel and wrote dedicated scripts to download web pages, extract text, and align sentences, with hardly any description of the methods used. Our data preparation for the shared task builds partly on Bitextor10 , which is a comprehensive pipeline from corpus crawling to sentence pair cleaning (Espl`a-Gomis, 2009). Sentence Alignment The topic of sentence alignment has received a lot of attention, dating back to the ear"
W16-2347,2011.mtsummit-papers.48,0,0.352994,"ages that is not parallel, and so on. We used for the preparation of this shared task the tool Httrack2 which is a general web crawler that can be configured in various ways. Papavassiliou et al. (2013) present the focused crawler ILSP-FC3 that integrates crawling more closely with subsequent processing steps like text normalization and deduplication. 2.2 2.4 A final stage of the processing pipeline filters out bad sentence pairs. These exist either because the original web site did not have any actual parallel data (garbage in, garbage out), or due to failures of earlier processing steps. As Rarrick et al. (2011) point out, a key problem for parallel corpora extracted from the web is filtering out translations that have been created by machine translation. Venugopal et al. (2011) propose a method to watermark the output of machine translation systems to aid this distinction. Antonova and Misyurev (2011) report that rulebased machine translation output can be detected due to certain word choices, and machine translation output due to lack of reordering. This year, a shared task on sentence pair filtering8 was organized, albeit in the context of cleaning translation memories which tend to be cleaner tha"
W16-2347,P99-1068,0,0.916487,"Missing"
W16-2347,2011.eamt-1.25,0,0.272709,"Missing"
W16-2347,J03-3002,0,0.592938,"ranslation output can be detected due to certain word choices, and machine translation output due to lack of reordering. This year, a shared task on sentence pair filtering8 was organized, albeit in the context of cleaning translation memories which tend to be cleaner that the data at the end of a pipeline that starts with web crawls. Document Alignment Document alignment can be defined as a matching task that takes a pair of documents and computes a score that reflects the likelihood that they are translations of each others. Common choices include edit-distance between linearized documents (Resnik and Smith, 2003), cosine distance of idfweighted bigram vectors (Uszkoreit et al., 2010), and probability of a probabilistic DOM-tree alignment model (Shi et al., 2006). 2.3 Filtering 2.5 Comprehensive Tools For a few language pairs, there have been individual efforts to cast a wider net, such as the billion word French–English corpus collected by Callison-Burch et al. (2009), or a 200 million word Czech–English corpus collected by Bojar et al. (2010). Smith et al. (2013) present a set of fairly basic tools to extract parallel data from the publicly available web crawl CommonCrawl9 . In all these cases, the c"
W16-2347,2010.amta-papers.14,0,0.174028,"d task builds partly on Bitextor10 , which is a comprehensive pipeline from corpus crawling to sentence pair cleaning (Espl`a-Gomis, 2009). Sentence Alignment The topic of sentence alignment has received a lot of attention, dating back to the early 1990s with the influential Church and Gale algorithm that is language-independent and easy to implement. It relies on relative sentence lengths for alignment decisions and hence is not tolerant to noisy input. Popular tools are Hunalign4 (Varga et al., 2005), Gargantua5 (Braune and Fraser, 2010), Bilingual Sentence Aligner (Moore, 2002) Bleualign6 (Sennrich and Volk, 2010), and Champollion7 (Ma, 2006). Shi and Zhou (2008) make use of the HTML structure to guide alignment. All of these use bilingual lexicons which may have to be provided upfront or are learned unsupervised. 2 https://www.httrack.com/ http://nlp.ilsp.gr/redmine/projects/ilsp-fc 4 http://mokk.bme.hu/en/resources/hunalign/ 5 https://sourceforge.net/projects/gargantua/ 6 https://github.com/rsennrich/Bleualign 7 https://sourceforge.net/projects/champollion/ 3 8 NLP4TM 2016: Shared task http://rgcl.wlv.ac.uk/nlp4tm2016/shared-task/ 9 http://commoncrawl.org/ 10 https://sourceforge.net/p/bitextor/wiki/H"
W16-2347,C10-1124,0,0.0413177,"ne translation output due to lack of reordering. This year, a shared task on sentence pair filtering8 was organized, albeit in the context of cleaning translation memories which tend to be cleaner that the data at the end of a pipeline that starts with web crawls. Document Alignment Document alignment can be defined as a matching task that takes a pair of documents and computes a score that reflects the likelihood that they are translations of each others. Common choices include edit-distance between linearized documents (Resnik and Smith, 2003), cosine distance of idfweighted bigram vectors (Uszkoreit et al., 2010), and probability of a probabilistic DOM-tree alignment model (Shi et al., 2006). 2.3 Filtering 2.5 Comprehensive Tools For a few language pairs, there have been individual efforts to cast a wider net, such as the billion word French–English corpus collected by Callison-Burch et al. (2009), or a 200 million word Czech–English corpus collected by Bojar et al. (2010). Smith et al. (2013) present a set of fairly basic tools to extract parallel data from the publicly available web crawl CommonCrawl9 . In all these cases, the corpus collection effort reinvented the wheel and wrote dedicated scripts"
W16-2347,W16-2376,0,0.0435553,"Missing"
W16-2347,P06-1062,0,0.0322113,"pair filtering8 was organized, albeit in the context of cleaning translation memories which tend to be cleaner that the data at the end of a pipeline that starts with web crawls. Document Alignment Document alignment can be defined as a matching task that takes a pair of documents and computes a score that reflects the likelihood that they are translations of each others. Common choices include edit-distance between linearized documents (Resnik and Smith, 2003), cosine distance of idfweighted bigram vectors (Uszkoreit et al., 2010), and probability of a probabilistic DOM-tree alignment model (Shi et al., 2006). 2.3 Filtering 2.5 Comprehensive Tools For a few language pairs, there have been individual efforts to cast a wider net, such as the billion word French–English corpus collected by Callison-Burch et al. (2009), or a 200 million word Czech–English corpus collected by Bojar et al. (2010). Smith et al. (2013) present a set of fairly basic tools to extract parallel data from the publicly available web crawl CommonCrawl9 . In all these cases, the corpus collection effort reinvented the wheel and wrote dedicated scripts to download web pages, extract text, and align sentences, with hardly any descr"
W16-2347,D11-1126,0,0.215759,"ways. Papavassiliou et al. (2013) present the focused crawler ILSP-FC3 that integrates crawling more closely with subsequent processing steps like text normalization and deduplication. 2.2 2.4 A final stage of the processing pipeline filters out bad sentence pairs. These exist either because the original web site did not have any actual parallel data (garbage in, garbage out), or due to failures of earlier processing steps. As Rarrick et al. (2011) point out, a key problem for parallel corpora extracted from the web is filtering out translations that have been created by machine translation. Venugopal et al. (2011) propose a method to watermark the output of machine translation systems to aid this distinction. Antonova and Misyurev (2011) report that rulebased machine translation output can be detected due to certain word choices, and machine translation output due to lack of reordering. This year, a shared task on sentence pair filtering8 was organized, albeit in the context of cleaning translation memories which tend to be cleaner that the data at the end of a pipeline that starts with web crawls. Document Alignment Document alignment can be defined as a matching task that takes a pair of documents an"
W16-2347,D08-1053,0,0.0291333,"ensive pipeline from corpus crawling to sentence pair cleaning (Espl`a-Gomis, 2009). Sentence Alignment The topic of sentence alignment has received a lot of attention, dating back to the early 1990s with the influential Church and Gale algorithm that is language-independent and easy to implement. It relies on relative sentence lengths for alignment decisions and hence is not tolerant to noisy input. Popular tools are Hunalign4 (Varga et al., 2005), Gargantua5 (Braune and Fraser, 2010), Bilingual Sentence Aligner (Moore, 2002) Bleualign6 (Sennrich and Volk, 2010), and Champollion7 (Ma, 2006). Shi and Zhou (2008) make use of the HTML structure to guide alignment. All of these use bilingual lexicons which may have to be provided upfront or are learned unsupervised. 2 https://www.httrack.com/ http://nlp.ilsp.gr/redmine/projects/ilsp-fc 4 http://mokk.bme.hu/en/resources/hunalign/ 5 https://sourceforge.net/projects/gargantua/ 6 https://github.com/rsennrich/Bleualign 7 https://sourceforge.net/projects/champollion/ 3 8 NLP4TM 2016: Shared task http://rgcl.wlv.ac.uk/nlp4tm2016/shared-task/ 9 http://commoncrawl.org/ 10 https://sourceforge.net/p/bitextor/wiki/Home/ 555 3 Training and Test Data We restricted th"
W16-2347,W13-0802,0,0.0229811,"parallel corpora from the web: aligning bilingual documents from crawled web sites. The task is to identify pairs of English and French documents from a given collection of documents such that one document is the translation of the other. As possible pairs we consider all pairs of documents from the same webdomain for which the source side has been identified as (mostly) English and the target side as (mostly) French. Lack of data in some cases has held back research. To give an example, there are significant research efforts on various Indic languages (Post et al., 2012; Joshi et al., 2013; Singh, 2013), but this work has been severely hampered, since it uses very small amounts of data. But even for the language pairs tackled in high profile evaluation campaigns, such as the ones organized around WMT, IWSLT, and even NIST, we use magnitudes of data less than what has been reported to be used in the large-scale efforts of Google or Microsoft. This diminishes the value of research findings: reported improvements for methods may not hold up • • • • • Identifying web sites with bilingual content Crawling web sites Document alignment Sentence alignment Sentence pair filtering For each of these st"
W16-2347,L16-1561,0,\N,Missing
W16-2365,J03-3002,0,0.837771,"Missing"
W16-2365,P99-1068,0,0.837557,"Missing"
W16-2365,I08-6003,0,0.0424008,"Missing"
W16-2365,P06-1062,0,0.497941,"Missing"
W16-2365,P13-1135,1,0.937478,"Missing"
W16-2365,C10-1124,0,0.543694,"Missing"
W17-3204,D16-1025,0,0.0554999,"statistical machine translation (SMT). We find that: 6. Beam search decoding only improves translation quality for narrow beams and deteriorates when exposed to a larger search space. We note a 7th challenge that we do not examine empirically: NMT systems are much less interpretable. The answer to the question of why the training data leads these systems to decide on specific word choices during decoding is buried in large matrices of real-numbered values. There is a clear need to develop better analytics for NMT. Other studies have looked at the comparable performance of NMT and SMT systems. Bentivogli et al. (2016) considered different linguistic categories for English–German and Toral and S´anchez-Cartagena (2017) compared different broad aspects such as fluency and reordering for nine language directions. 1. NMT systems have lower quality out of domain, to the point that they completely sacrifice adequacy for the sake of fluency. 2. NMT systems have a steeper learning curve with respect to the amount of training data, resulting in worse quality in low-resource settings, but better performance in highresource settings. 2 Experimental Setup We use common toolkits for neural machine translation (Nematus)"
W17-3204,W16-2301,1,0.770086,"Missing"
W17-3204,2016.amta-researchers.10,0,0.0193792,"ny alignments11 In out experiment, we use the neural machine translation models provided by Edinburgh12 (Sennrich et al., 2016a). We run fast-align on the same parallel data sets to obtain alignment models and used them to align the input and output of the NMT system. Table 3 shows alignment scores for the systems. The results suggest that, while drastic, the divergence for German–English is an outlier. We note, however, that we have seen such large a divergence also under different data conditions. Note that the attention model may produce better word alignments by guided alignment training (Chen et al., 2016; Liu et al., 2016) where supervised word alignments (such as the ones produced by fast-align) are provided to model training. 11 (1) NMT operates on subwords, but fast-align is run on full words. (2) If an input word is split into subwords by byte pair encoding, then we add their attention scores. (3) If an output word is split into subwords, then we take the average of their attention vectors. (4) The match scores and probability mass scores are computed as average over output word-level scores. (5) If an output word has no fast-align alignment point, it is ignored in this computation. (6) I"
W17-3204,J07-2003,0,0.0249736,"r Law, Medical, and Koran), the out-of-domain performance for the NMT systems is worse in almost all cases, sometimes dramatically so. For instance the MedStatistical Machine Translation Our machine translation systems are trained using Moses3 (Koehn et al., 2007). We build phrasebased systems using standard features that are commonly used in recent system submissions to WMT (Williams et al., 2016; Ding et al., 2016a). While we use the shorthand SMT for these phrase-based systems, we note that there are other statistical machine translation approaches such as hierarchical phrase-based models (Chiang, 2007) and syntax-based models (Galley et al., 2004, 2006) that have been shown to give superior performance for language pairs such as Chinese– English and German–English. 2.3 Words 18,128,173 14,301,472 3,041,677 9,848,539 114,371,754 Data Conditions We carry out our experiments on English–Spanish and German–English. For these language pairs, large training data sets are available. We use datasets from the shared translation task organized alongside the Conference on Machine Translation (WMT)4 . For the domain experiments, we use the OPUS corpus5 (Tiedemann, 2012). Except for the domain experiment"
W17-3204,W14-4012,0,0.246659,"Missing"
W17-3204,D16-1162,0,0.0261561,"lingual data for a big language model in contrastive SMT systems. Results are shown in Figure 3. NMT exhibits a much steeper learning curve, starting with abysmal results (BLEU score of 1.6 vs. 16.4 for 1 1024 of the data), outperforming SMT 25.7 vs. 1 24.7 with 16 of the data (24.1 million words), and even beating the SMT system with a big language model with the full data set (31.1 for NMT, 28.4 for SMT, 30.4 for SMT+BigLM). 3.3 Rare Words Conventional wisdom states that neural machine translation models perform particularly poorly on rare words, (Luong et al., 2015; Sennrich et al., 2016b; Arthur et al., 2016) due in part to the smaller vocabularies used by NMT systems. We examine this claim by comparing performance on rare word translation between NMT and SMT systems of similar quality for German–English and find that NMT systems actually outperform SMT systems on translation of very infrequent words. However, both NMT and SMT systems do continue to have difficulty translating some infrequent words, particularly those belonging to highly-inflected categories. For the neural machine translation model, we use a publicly available model8 with the training settings of Edinburgh’s WMT submission (Sennr"
W17-3204,W16-2310,1,0.869585,"ining). A common byte-pair encoding is used for all training runs. See Figure 1 for results. While the in-domain NMT and SMT systems are similar (NMT is better for IT and Subtitles, SMT is better for Law, Medical, and Koran), the out-of-domain performance for the NMT systems is worse in almost all cases, sometimes dramatically so. For instance the MedStatistical Machine Translation Our machine translation systems are trained using Moses3 (Koehn et al., 2007). We build phrasebased systems using standard features that are commonly used in recent system submissions to WMT (Williams et al., 2016; Ding et al., 2016a). While we use the shorthand SMT for these phrase-based systems, we note that there are other statistical machine translation approaches such as hierarchical phrase-based models (Chiang, 2007) and syntax-based models (Galley et al., 2004, 2006) that have been shown to give superior performance for language pairs such as Chinese– English and German–English. 2.3 Words 18,128,173 14,301,472 3,041,677 9,848,539 114,371,754 Data Conditions We carry out our experiments on English–Spanish and German–English. For these language pairs, large training data sets are available. We use datasets from the"
W17-3204,N13-1073,0,0.0158595,"or years strained been have and Netanyahu Obama 33.9 between 34.7 34.7 35 relations BLEU Scores with Varying Sentence Length 16 79 98 42 11 38 seit 22 54 10 Jahren angespannt . 98 84 11 14 23 49 Figure 8: Word alignment for English–German: comparing the attention model states (green boxes with probability in percent if over 10) with alignments obtained from fast-align (blue outlines). Word Alignment means? To examine this, we compare the soft alignment matrix (the sequence of attention vectors) with word alignments obtained by traditional word alignment methods. We use incremental fast-align (Dyer et al., 2013) to align the input and output of the neural machine system. See Figure 8 for an illustration. We compare the word attention states (green boxes) with the word alignments obtained with fast align (blue outlines). For most words, these match up pretty well. Both attention states and fast-align alignment points are a bit fuzzy around the function words have-been/sind. However, the attention model may settle on alignments that do not correspond with our intuition or alignment points obtained with fast-align. See Figure 9 for the reverse language direction, German–English. All the alignment points"
W17-3204,C16-1291,0,0.0369621,"out experiment, we use the neural machine translation models provided by Edinburgh12 (Sennrich et al., 2016a). We run fast-align on the same parallel data sets to obtain alignment models and used them to align the input and output of the NMT system. Table 3 shows alignment scores for the systems. The results suggest that, while drastic, the divergence for German–English is an outlier. We note, however, that we have seen such large a divergence also under different data conditions. Note that the attention model may produce better word alignments by guided alignment training (Chen et al., 2016; Liu et al., 2016) where supervised word alignments (such as the ones produced by fast-align) are provided to model training. 11 (1) NMT operates on subwords, but fast-align is run on full words. (2) If an input word is split into subwords by byte pair encoding, then we add their attention scores. (3) If an output word is split into subwords, then we take the average of their attention vectors. (4) The match scores and probability mass scores are computed as average over output word-level scores. (5) If an output word has no fast-align alignment point, it is ignored in this computation. (6) If an output word is"
W17-3204,P06-1121,0,0.0226943,"Missing"
W17-3204,2015.iwslt-evaluation.11,0,0.129339,"ain-specific systems, taken from the OPUS repository. IT corpora are GNOME, KDE, PHP, Ubuntu, and OpenOffice. 3 3.1 Challenges Domain Mismatch A known challenge in translation is that in different domains,6 words have different translations and meaning is expressed in different styles. Hence, a crucial step in developing machine translation systems targeted at a specific use case is domain adaptation. We expect that methods for domain adaptation will be developed for NMT. A currently popular approach is to train a general domain system, followed by training on in-domain data for a few epochs (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016). Often, large amounts of training data are only available out of domain, but we still seek to have robust performance. To test how well NMT and SMT hold up, we trained five different systems using different corpora obtained from OPUS (Tiedemann, 2012). An additional system was trained on all the training data. Statistics about corpus sizes are shown in Table 1. Note that these domains are quite distant from each other, much more so than, say, Europarl, TED Talks, News Commentary, and Global Voices. We trained both SMT and NMT systems for all domains. All systems"
W17-3204,P15-1002,0,0.0205059,"we also used all additionally provided monolingual data for a big language model in contrastive SMT systems. Results are shown in Figure 3. NMT exhibits a much steeper learning curve, starting with abysmal results (BLEU score of 1.6 vs. 16.4 for 1 1024 of the data), outperforming SMT 25.7 vs. 1 24.7 with 16 of the data (24.1 million words), and even beating the SMT system with a big language model with the full data set (31.1 for NMT, 28.4 for SMT, 30.4 for SMT+BigLM). 3.3 Rare Words Conventional wisdom states that neural machine translation models perform particularly poorly on rare words, (Luong et al., 2015; Sennrich et al., 2016b; Arthur et al., 2016) due in part to the smaller vocabularies used by NMT systems. We examine this claim by comparing performance on rare word translation between NMT and SMT systems of similar quality for German–English and find that NMT systems actually outperform SMT systems on translation of very infrequent words. However, both NMT and SMT systems do continue to have difficulty translating some infrequent words, particularly those belonging to highly-inflected categories. For the neural machine translation model, we use a publicly available model8 with the training"
W17-3204,N04-1035,0,0.0425892,"domain performance for the NMT systems is worse in almost all cases, sometimes dramatically so. For instance the MedStatistical Machine Translation Our machine translation systems are trained using Moses3 (Koehn et al., 2007). We build phrasebased systems using standard features that are commonly used in recent system submissions to WMT (Williams et al., 2016; Ding et al., 2016a). While we use the shorthand SMT for these phrase-based systems, we note that there are other statistical machine translation approaches such as hierarchical phrase-based models (Chiang, 2007) and syntax-based models (Galley et al., 2004, 2006) that have been shown to give superior performance for language pairs such as Chinese– English and German–English. 2.3 Words 18,128,173 14,301,472 3,041,677 9,848,539 114,371,754 Data Conditions We carry out our experiments on English–Spanish and German–English. For these language pairs, large training data sets are available. We use datasets from the shared translation task organized alongside the Conference on Machine Translation (WMT)4 . For the domain experiments, we use the OPUS corpus5 (Tiedemann, 2012). Except for the domain experiments, we use the WMT test sets composed of news"
W17-3204,W13-2233,0,0.0166096,"Missing"
W17-3204,W14-4009,0,0.0132432,"Missing"
W17-3204,E17-3017,0,0.0697317,"Missing"
W17-3204,D13-1176,0,0.208995,"ural machine translation (Nematus) and traditional phrase-based statistical machine translation (Moses) with common data sets, drawn from WMT and OPUS. 1 https://www.nist.gov/itl/iad/mig/lorehlt16evaluations 28 Proceedings of the First Workshop on Neural Machine Translation, pages 28–39, c Vancouver, Canada, August 4, 2017. 2017 Association for Computational Linguistics 2.1 Corpus Law (Acquis) Medical (EMEA) IT Koran (Tanzil) Subtitles Neural Machine Translation While a variety of neural machine translation approaches were initially proposed — such as the use of convolutional neural networks (Kalchbrenner and Blunsom, 2013) — practically all recent work has been focused on the attention-based encoder-decoder model (Bahdanau et al., 2015). We use the toolkit Nematus2 (Sennrich et al., 2017) which has been shown to give state-of-theart results (Sennrich et al., 2016a) at the WMT 2016 evaluation campaign (Bojar et al., 2016). Unless noted otherwise, we use default settings, such as beam search and single model decoding. The training data is processed with byte-pair encoding (Sennrich et al., 2016b) into subwords to fit a 50,000 word vocabulary limit. 2.2 Sentences 715,372 1,104,752 337,817 480,421 13,873,398 W/S 25"
W17-3204,W16-2323,0,0.208889,"Machine Translation, pages 28–39, c Vancouver, Canada, August 4, 2017. 2017 Association for Computational Linguistics 2.1 Corpus Law (Acquis) Medical (EMEA) IT Koran (Tanzil) Subtitles Neural Machine Translation While a variety of neural machine translation approaches were initially proposed — such as the use of convolutional neural networks (Kalchbrenner and Blunsom, 2013) — practically all recent work has been focused on the attention-based encoder-decoder model (Bahdanau et al., 2015). We use the toolkit Nematus2 (Sennrich et al., 2017) which has been shown to give state-of-theart results (Sennrich et al., 2016a) at the WMT 2016 evaluation campaign (Bojar et al., 2016). Unless noted otherwise, we use default settings, such as beam search and single model decoding. The training data is processed with byte-pair encoding (Sennrich et al., 2016b) into subwords to fit a 50,000 word vocabulary limit. 2.2 Sentences 715,372 1,104,752 337,817 480,421 13,873,398 W/S 25.3 12.9 9.0 20.5 8.2 Table 1: Corpora used to train domain-specific systems, taken from the OPUS repository. IT corpora are GNOME, KDE, PHP, Ubuntu, and OpenOffice. 3 3.1 Challenges Domain Mismatch A known challenge in translation is that in dif"
W17-3204,2012.amta-papers.9,1,0.760121,"Missing"
W17-3204,P16-1162,0,0.955692,"Machine Translation, pages 28–39, c Vancouver, Canada, August 4, 2017. 2017 Association for Computational Linguistics 2.1 Corpus Law (Acquis) Medical (EMEA) IT Koran (Tanzil) Subtitles Neural Machine Translation While a variety of neural machine translation approaches were initially proposed — such as the use of convolutional neural networks (Kalchbrenner and Blunsom, 2013) — practically all recent work has been focused on the attention-based encoder-decoder model (Bahdanau et al., 2015). We use the toolkit Nematus2 (Sennrich et al., 2017) which has been shown to give state-of-theart results (Sennrich et al., 2016a) at the WMT 2016 evaluation campaign (Bojar et al., 2016). Unless noted otherwise, we use default settings, such as beam search and single model decoding. The training data is processed with byte-pair encoding (Sennrich et al., 2016b) into subwords to fit a 50,000 word vocabulary limit. 2.2 Sentences 715,372 1,104,752 337,817 480,421 13,873,398 W/S 25.3 12.9 9.0 20.5 8.2 Table 1: Corpora used to train domain-specific systems, taken from the OPUS repository. IT corpora are GNOME, KDE, PHP, Ubuntu, and OpenOffice. 3 3.1 Challenges Domain Mismatch A known challenge in translation is that in dif"
W17-3204,tiedemann-2012-parallel,0,0.382961,"ce, a crucial step in developing machine translation systems targeted at a specific use case is domain adaptation. We expect that methods for domain adaptation will be developed for NMT. A currently popular approach is to train a general domain system, followed by training on in-domain data for a few epochs (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016). Often, large amounts of training data are only available out of domain, but we still seek to have robust performance. To test how well NMT and SMT hold up, we trained five different systems using different corpora obtained from OPUS (Tiedemann, 2012). An additional system was trained on all the training data. Statistics about corpus sizes are shown in Table 1. Note that these domains are quite distant from each other, much more so than, say, Europarl, TED Talks, News Commentary, and Global Voices. We trained both SMT and NMT systems for all domains. All systems were trained for GermanEnglish, with tuning and test sets sub-sampled from the data (these were not used in training). A common byte-pair encoding is used for all training runs. See Figure 1 for results. While the in-domain NMT and SMT systems are similar (NMT is better for IT and"
W17-3204,E17-1100,0,0.0512634,"Missing"
W17-3204,P16-1008,0,0.0362803,"Missing"
W17-3204,W08-0305,0,0.0915087,"Missing"
W17-3204,W16-2327,0,0.0115154,"se were not used in training). A common byte-pair encoding is used for all training runs. See Figure 1 for results. While the in-domain NMT and SMT systems are similar (NMT is better for IT and Subtitles, SMT is better for Law, Medical, and Koran), the out-of-domain performance for the NMT systems is worse in almost all cases, sometimes dramatically so. For instance the MedStatistical Machine Translation Our machine translation systems are trained using Moses3 (Koehn et al., 2007). We build phrasebased systems using standard features that are commonly used in recent system submissions to WMT (Williams et al., 2016; Ding et al., 2016a). While we use the shorthand SMT for these phrase-based systems, we note that there are other statistical machine translation approaches such as hierarchical phrase-based models (Chiang, 2007) and syntax-based models (Galley et al., 2004, 2006) that have been shown to give superior performance for language pairs such as Chinese– English and German–English. 2.3 Words 18,128,173 14,301,472 3,041,677 9,848,539 114,371,754 Data Conditions We carry out our experiments on English–Spanish and German–English. For these language pairs, large training data sets are available. We use"
W17-3204,P07-2045,1,\N,Missing
W17-4707,W07-0702,1,0.671821,"oposed serializing the target constituency trees and Eriguchi et al. (2017) model target dependency relations by augmenting the NMT decoder with a RNN grammar (Dyer et al., 2016). In our work, we use CCG supertags which are a more compact representation of global syntax. Furthermore, we do not focus on model architectures, and instead we explore the more general problem of including target syntax in NMT: comparing tightly and loosely coupled syntactic information and showing source and target syntax are complementary. Previous work on integrating CCG supertags in factored phrase-based models (Birch et al., 2007) made strong independence assumptions between the target word sequence and the CCG categories. In this work we take advantage of the expressive power of recurrent neural networks to learn representations that generate both words and CCG supertags, conditioned on the entire lexical and syntactic target history. ments, and also tense and morphological aspects of the word in a given context. Consider the sentence in Figure 1. This sentence contains two PP attachments and could lead to several disambiguation possibilities (“in” can attach to “Netanyahu” or “receives”, and “of” can attach to “capit"
W17-4707,J07-2003,0,0.0156397,"ining. • We show that incorporating source-side linguistic information is complimentary to our method, further improving the translation quality. • We present a fine-grained analysis of SNMT and show consistent gains for different linguistic phenomena and sentence lengths. 2 Related work Syntax has helped in statistical machine translation (SMT) to capture dependencies between distant words that impact morphological agreement, subcategorisation and word order (Galley et al., 2004; Menezes and Quirk, 2007; Williams and Koehn, 2012; Nadejde et al., 2013; Sennrich, 2015; Nadejde et al., 2016a,b; Chiang, 2007). There has been some work in NMT on modeling source-side syntax implicitly or explicitly. Kalchbrenner and Blunsom (2013); Cho et al. (2014a) capture the hierarchical aspects of language implicitly by using convolutional neural networks, while Eriguchi et al. (2016) use the parse tree of the source sentence to guide the recurrence and attention model in tree-to-sequence NMT. Luong 3 Modeling Syntax in NMT CCG is a lexicalised formalism in which words are assigned with syntactic categories, i.e., supertags, that indicate context-sensitive morpho-syntactic properties of a word in a sentence. Th"
W17-4707,W14-4012,0,0.028354,"Missing"
W17-4707,P17-2021,0,0.0761778,"ework to show source and target syntax provide complementary information. Applying more tightly coupled linguistic factors on the target for NMT has been previously investigated. Niehues et al. (2016) proposed a factored RNN-based language model for re-scoring an n-best list produced by a phrase-based MT system. In recent work, Mart´ınez et al. (2016) implemented a factored NMT decoder which generated both lemmas and morphological tags. The two factors were then post-processed to generate the word form. Unfortunately no real gain was reported for these experiments. Concurrently with our work, Aharoni and Goldberg (2017) proposed serializing the target constituency trees and Eriguchi et al. (2017) model target dependency relations by augmenting the NMT decoder with a RNN grammar (Dyer et al., 2016). In our work, we use CCG supertags which are a more compact representation of global syntax. Furthermore, we do not focus on model architectures, and instead we explore the more general problem of including target syntax in NMT: comparing tightly and loosely coupled syntactic information and showing source and target syntax are complementary. Previous work on integrating CCG supertags in factored phrase-based model"
W17-4707,D14-1179,0,0.00956885,"Missing"
W17-4707,N16-1024,0,0.0146616,"al. (2016) proposed a factored RNN-based language model for re-scoring an n-best list produced by a phrase-based MT system. In recent work, Mart´ınez et al. (2016) implemented a factored NMT decoder which generated both lemmas and morphological tags. The two factors were then post-processed to generate the word form. Unfortunately no real gain was reported for these experiments. Concurrently with our work, Aharoni and Goldberg (2017) proposed serializing the target constituency trees and Eriguchi et al. (2017) model target dependency relations by augmenting the NMT decoder with a RNN grammar (Dyer et al., 2016). In our work, we use CCG supertags which are a more compact representation of global syntax. Furthermore, we do not focus on model architectures, and instead we explore the more general problem of including target syntax in NMT: comparing tightly and loosely coupled syntactic information and showing source and target syntax are complementary. Previous work on integrating CCG supertags in factored phrase-based models (Birch et al., 2007) made strong independence assumptions between the target word sequence and the CCG categories. In this work we take advantage of the expressive power of recurr"
W17-4707,P16-1231,0,0.0172413,"butions over separate target vocabularies for the words and the tags: (7) (8) pword y At training time we pre-process the target sequence to add the syntactic annotation and then split only the words into byte-pair-encoding (BPE) (Sennrich et al., 2016b) sub-units. At = T1 Y word p(yjword |x, y1:j−1 ) T2 Y tag p(yktag |x, y1:k−1 ) j ptag y = k 71 (9) (10) The final loss is the sum of the losses for the two decoders: l = −(log(pword ) + log(ptag y y )) DE-EN RO-EN (11) test 2,994 1,984 sets in Table 1. Dependency labels are annotated with ParZU (Sennrich et al., 2013) for German and SyntaxNet (Andor et al., 2016) for Romanian. All the neural MT systems are attentional encoder-decoder networks (Bahdanau et al., 2015) as implemented in the Nematus toolkit (Sennrich et al., 2017).4 We use similar hyper-parameters to those reported by (Sennrich et al., 2016a; Sennrich and Haddow, 2016) with minor modifications: we used mini-batches of size 60 and Adam optimizer (Kingma and Ba, 2014). We select the best single models according to BLEU on the development set and use the four best single models for the ensembles. To show that we report results over strong baselines, table 2 compares the scores obtained by ou"
W17-4707,P16-1078,0,0.0552534,"ntence lengths. 2 Related work Syntax has helped in statistical machine translation (SMT) to capture dependencies between distant words that impact morphological agreement, subcategorisation and word order (Galley et al., 2004; Menezes and Quirk, 2007; Williams and Koehn, 2012; Nadejde et al., 2013; Sennrich, 2015; Nadejde et al., 2016a,b; Chiang, 2007). There has been some work in NMT on modeling source-side syntax implicitly or explicitly. Kalchbrenner and Blunsom (2013); Cho et al. (2014a) capture the hierarchical aspects of language implicitly by using convolutional neural networks, while Eriguchi et al. (2016) use the parse tree of the source sentence to guide the recurrence and attention model in tree-to-sequence NMT. Luong 3 Modeling Syntax in NMT CCG is a lexicalised formalism in which words are assigned with syntactic categories, i.e., supertags, that indicate context-sensitive morpho-syntactic properties of a word in a sentence. The combinators of CCG allow the supertags to capture global syntactic constraints locally. Though NMT 69 Source-side BPE: Obama IOB: O CCG: NP Target-side receives O ((S[dcl]NP)/PP)/NP Net+ B NP an+ I NP yahu E NP in O PP/NP the O NP/N capital O N of O (NPNP)/NP USA"
W17-4707,D16-1025,0,0.0250595,"Missing"
W17-4707,N04-1035,0,0.0228682,"ht coupling of target words and syntax (by interleaving) improves translation quality more than the decoupled signal from multitask training. • We show that incorporating source-side linguistic information is complimentary to our method, further improving the translation quality. • We present a fine-grained analysis of SNMT and show consistent gains for different linguistic phenomena and sentence lengths. 2 Related work Syntax has helped in statistical machine translation (SMT) to capture dependencies between distant words that impact morphological agreement, subcategorisation and word order (Galley et al., 2004; Menezes and Quirk, 2007; Williams and Koehn, 2012; Nadejde et al., 2013; Sennrich, 2015; Nadejde et al., 2016a,b; Chiang, 2007). There has been some work in NMT on modeling source-side syntax implicitly or explicitly. Kalchbrenner and Blunsom (2013); Cho et al. (2014a) capture the hierarchical aspects of language implicitly by using convolutional neural networks, while Eriguchi et al. (2016) use the parse tree of the source sentence to guide the recurrence and attention model in tree-to-sequence NMT. Luong 3 Modeling Syntax in NMT CCG is a lexicalised formalism in which words are assigned wi"
W17-4707,W16-2208,0,0.0269734,"Missing"
W17-4707,P02-1040,0,0.120603,"of the training data is annotated with CCG lexical tags2 using EasySRL (Lewis et al., 2015) and the available pre-trained model3 . Some longer sentences cannot be processed by the parser and therefore we eliminate them from our training and test data. We report the sentence counts for the filtered data This work 31.0 27.8 28.0 25.6 Sennrich et. al 28.5 26.8 27.8 23.9 Table 2: Comparison of baseline systems in this work and in Sennrich et al. (2016a). Casesensitive BLEU scores reported over newstest2016 with mteval-13a.perl. 1 Normalized diacritics. During training we validate our models with (Papineni et al., 2002) on development sets: newstest2013 for German↔English and newsdev2016 for Romanian↔English. We evaluate the systems on newstest2016 test sets for both lanBLEU 1 We use the same data and annotations for the interleaving approach. 2 The CCG tags include features such as the verb tense (e.g. [ng] for continuous form) or the sentence type (e.g. [pss] for passive). 3 https://github.com/uwnlp/EasySRL 4 https://github.com/rsennrich/nematus There are different encodings for letters with cedilla (s¸,t¸) used interchangeably throughout the corpus. https://en.wikipedia.org/wiki/Romanian_ alphabet#ISO_885"
W17-4707,D13-1176,0,0.032968,"urther improving the translation quality. • We present a fine-grained analysis of SNMT and show consistent gains for different linguistic phenomena and sentence lengths. 2 Related work Syntax has helped in statistical machine translation (SMT) to capture dependencies between distant words that impact morphological agreement, subcategorisation and word order (Galley et al., 2004; Menezes and Quirk, 2007; Williams and Koehn, 2012; Nadejde et al., 2013; Sennrich, 2015; Nadejde et al., 2016a,b; Chiang, 2007). There has been some work in NMT on modeling source-side syntax implicitly or explicitly. Kalchbrenner and Blunsom (2013); Cho et al. (2014a) capture the hierarchical aspects of language implicitly by using convolutional neural networks, while Eriguchi et al. (2016) use the parse tree of the source sentence to guide the recurrence and attention model in tree-to-sequence NMT. Luong 3 Modeling Syntax in NMT CCG is a lexicalised formalism in which words are assigned with syntactic categories, i.e., supertags, that indicate context-sensitive morpho-syntactic properties of a word in a sentence. The combinators of CCG allow the supertags to capture global syntactic constraints locally. Though NMT 69 Source-side BPE: O"
W17-4707,W05-0908,0,0.0147936,"lish and newsdev2016 for Romanian↔English. We evaluate the systems on newstest2016 test sets for both lanBLEU 1 We use the same data and annotations for the interleaving approach. 2 The CCG tags include features such as the verb tense (e.g. [ng] for continuous form) or the sentence type (e.g. [pss] for passive). 3 https://github.com/uwnlp/EasySRL 4 https://github.com/rsennrich/nematus There are different encodings for letters with cedilla (s¸,t¸) used interchangeably throughout the corpus. https://en.wikipedia.org/wiki/Romanian_ alphabet#ISO_8859 5 72 guage pairs and use bootstrap resampling (Riezler and Maxwell, 2005) to test statistical significance. We compute BLEU with multi-bleu.perl over tokenized sentences both on the development sets, for early stopping, and on the test sets for evaluating our systems. Words are segmented into sub-units that are learned jointly for source and target using BPE (Sennrich et al., 2016b), resulting in a vocabulary size of 85,000. The vocabulary size for CCG supertags was 500. For the experiments with source-side features we use the BPE sub-units and the IOB tags as baseline features. We keep the total word embedding size fixed to 500 dimensions. We allocate 10 dimension"
W17-4707,Q15-1013,1,0.864408,"n the decoupled signal from multitask training. • We show that incorporating source-side linguistic information is complimentary to our method, further improving the translation quality. • We present a fine-grained analysis of SNMT and show consistent gains for different linguistic phenomena and sentence lengths. 2 Related work Syntax has helped in statistical machine translation (SMT) to capture dependencies between distant words that impact morphological agreement, subcategorisation and word order (Galley et al., 2004; Menezes and Quirk, 2007; Williams and Koehn, 2012; Nadejde et al., 2013; Sennrich, 2015; Nadejde et al., 2016a,b; Chiang, 2007). There has been some work in NMT on modeling source-side syntax implicitly or explicitly. Kalchbrenner and Blunsom (2013); Cho et al. (2014a) capture the hierarchical aspects of language implicitly by using convolutional neural networks, while Eriguchi et al. (2016) use the parse tree of the source sentence to guide the recurrence and attention model in tree-to-sequence NMT. Luong 3 Modeling Syntax in NMT CCG is a lexicalised formalism in which words are assigned with syntactic categories, i.e., supertags, that indicate context-sensitive morpho-syntacti"
W17-4707,D15-1169,0,0.0163034,"DE→EN EN→DE RO→EN EN→RO1 Experimental Setup and Evaluation 4.1 dev 2,986 1,984 Table 1: Number of sentences in the training, development and test sets. We use EasySRL to label the English side of the parallel corpus with CCG supertags1 instead of using a corpus with gold annotations as in Luong et al. (2016). 4 train 4,468,314 605,885 Data and methods We train the neural MT systems on all the parallel data available at WMT16 (Bojar et al., 2016) for the German↔English and Romanian↔English language pairs. The English side of the training data is annotated with CCG lexical tags2 using EasySRL (Lewis et al., 2015) and the available pre-trained model3 . Some longer sentences cannot be processed by the parser and therefore we eliminate them from our training and test data. We report the sentence counts for the filtered data This work 31.0 27.8 28.0 25.6 Sennrich et. al 28.5 26.8 27.8 23.9 Table 2: Comparison of baseline systems in this work and in Sennrich et al. (2016a). Casesensitive BLEU scores reported over newstest2016 with mteval-13a.perl. 1 Normalized diacritics. During training we validate our models with (Papineni et al., 2002) on development sets: newstest2013 for German↔English and newsdev2016"
W17-4707,E17-3017,1,0.894752,"Missing"
W17-4707,W16-2209,1,0.896448,") show that NMT significantly improves over phrase-based SMT, in particular with respect to morphology and word order, but that results can still be improved for longer sentences and complex syntactic phenomena such as prepositional phrase (PP) attachment. Another study by Shi et al. (2016) shows that the encoder layer of NMT partially learns syntactic information about the source language, however complex syntactic phenomena such as coordination or PP attachment are poorly modeled. Recent work which incorporates additional source-side linguistic information in NMT models (Luong et al., 2016; Sennrich and Haddow, 2016) show that even though neural models have strong learning capabilities, explicit features can still improve translation quality. In this work, we examine the benefit of incorporating global syntactic information on the target-side. We also address the question of how best to incorporate this information. For language pairs where syntactic resources are available on both the source and target-side, we show that approaches to incorporate source syntax and target syntax are complementary. We propose a method for tightly coupling words and syntax by interleaving the target syntactic representation"
W17-4707,W07-0701,0,0.202057,"words and syntax (by interleaving) improves translation quality more than the decoupled signal from multitask training. • We show that incorporating source-side linguistic information is complimentary to our method, further improving the translation quality. • We present a fine-grained analysis of SNMT and show consistent gains for different linguistic phenomena and sentence lengths. 2 Related work Syntax has helped in statistical machine translation (SMT) to capture dependencies between distant words that impact morphological agreement, subcategorisation and word order (Galley et al., 2004; Menezes and Quirk, 2007; Williams and Koehn, 2012; Nadejde et al., 2013; Sennrich, 2015; Nadejde et al., 2016a,b; Chiang, 2007). There has been some work in NMT on modeling source-side syntax implicitly or explicitly. Kalchbrenner and Blunsom (2013); Cho et al. (2014a) capture the hierarchical aspects of language implicitly by using convolutional neural networks, while Eriguchi et al. (2016) use the parse tree of the source sentence to guide the recurrence and attention model in tree-to-sequence NMT. Luong 3 Modeling Syntax in NMT CCG is a lexicalised formalism in which words are assigned with syntactic categories,"
W17-4707,W16-2323,1,0.729608,"ish, a lowresource pair and also several syntactic phenomena including prepositional phrase attachment. Furthermore, a tight coupling of words and syntax improves translation quality more than multitask training. By combining target-syntax with adding source-side dependency labels in the embedding layer, we obtain a total improvement of 0.9 BLEU for German→English and 1.2 BLEU for Romanian→English. 1 Introduction Sequence-to-sequence neural machine translation (NMT) models (Sutskever et al., 2014; Cho et al., 2014b; Bahdanau et al., 2015) are state-of-the-art on a multitude of language-pairs (Sennrich et al., 2016a; Junczys-Dowmunt et al., 2016). Part of the appeal of neural models is that they can learn to implicitly model phenomena which underlie high quality output, and some syntax is indeed cap68 Proceedings of the Conference on Machine Translation (WMT), Volume 1: Research Papers, pages 68–79 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics et al. (2016) co-train a translation model and a source-side syntactic parser which share the encoder. Our multitask models extend their work to attention-based NMT models and to predicting target-side syntax as the sec"
W17-4707,W16-2204,1,0.813749,"signal from multitask training. • We show that incorporating source-side linguistic information is complimentary to our method, further improving the translation quality. • We present a fine-grained analysis of SNMT and show consistent gains for different linguistic phenomena and sentence lengths. 2 Related work Syntax has helped in statistical machine translation (SMT) to capture dependencies between distant words that impact morphological agreement, subcategorisation and word order (Galley et al., 2004; Menezes and Quirk, 2007; Williams and Koehn, 2012; Nadejde et al., 2013; Sennrich, 2015; Nadejde et al., 2016a,b; Chiang, 2007). There has been some work in NMT on modeling source-side syntax implicitly or explicitly. Kalchbrenner and Blunsom (2013); Cho et al. (2014a) capture the hierarchical aspects of language implicitly by using convolutional neural networks, while Eriguchi et al. (2016) use the parse tree of the source sentence to guide the recurrence and attention model in tree-to-sequence NMT. Luong 3 Modeling Syntax in NMT CCG is a lexicalised formalism in which words are assigned with syntactic categories, i.e., supertags, that indicate context-sensitive morpho-syntactic properties of a word"
W17-4707,P16-1162,1,0.46286,"ish, a lowresource pair and also several syntactic phenomena including prepositional phrase attachment. Furthermore, a tight coupling of words and syntax improves translation quality more than multitask training. By combining target-syntax with adding source-side dependency labels in the embedding layer, we obtain a total improvement of 0.9 BLEU for German→English and 1.2 BLEU for Romanian→English. 1 Introduction Sequence-to-sequence neural machine translation (NMT) models (Sutskever et al., 2014; Cho et al., 2014b; Bahdanau et al., 2015) are state-of-the-art on a multitude of language-pairs (Sennrich et al., 2016a; Junczys-Dowmunt et al., 2016). Part of the appeal of neural models is that they can learn to implicitly model phenomena which underlie high quality output, and some syntax is indeed cap68 Proceedings of the Conference on Machine Translation (WMT), Volume 1: Research Papers, pages 68–79 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics et al. (2016) co-train a translation model and a source-side syntactic parser which share the encoder. Our multitask models extend their work to attention-based NMT models and to predicting target-side syntax as the sec"
W17-4707,R13-1079,1,0.793726,"1 and T2 . This results in two probability distributions over separate target vocabularies for the words and the tags: (7) (8) pword y At training time we pre-process the target sequence to add the syntactic annotation and then split only the words into byte-pair-encoding (BPE) (Sennrich et al., 2016b) sub-units. At = T1 Y word p(yjword |x, y1:j−1 ) T2 Y tag p(yktag |x, y1:k−1 ) j ptag y = k 71 (9) (10) The final loss is the sum of the losses for the two decoders: l = −(log(pword ) + log(ptag y y )) DE-EN RO-EN (11) test 2,994 1,984 sets in Table 1. Dependency labels are annotated with ParZU (Sennrich et al., 2013) for German and SyntaxNet (Andor et al., 2016) for Romanian. All the neural MT systems are attentional encoder-decoder networks (Bahdanau et al., 2015) as implemented in the Nematus toolkit (Sennrich et al., 2017).4 We use similar hyper-parameters to those reported by (Sennrich et al., 2016a; Sennrich and Haddow, 2016) with minor modifications: we used mini-batches of size 60 and Adam optimizer (Kingma and Ba, 2014). We select the best single models according to BLEU on the development set and use the four best single models for the ensembles. To show that we report results over strong baselin"
W17-4707,D16-1159,0,0.0475338,"Alexandra Birch1 1 School of Informatics, University of Edinburgh 2 Adam Mickiewicz University 3 Dep. of Computer Science, Johns Hopkins University {m.nadejde,siva.reddy, rico.sennrich, a.birch}@ed.ac.uk {t.dwojak,junczys}@amu.edu.pl, phi@jhu.edu Abstract tured by these models. In a detailed analysis, Bentivogli et al. (2016) show that NMT significantly improves over phrase-based SMT, in particular with respect to morphology and word order, but that results can still be improved for longer sentences and complex syntactic phenomena such as prepositional phrase (PP) attachment. Another study by Shi et al. (2016) shows that the encoder layer of NMT partially learns syntactic information about the source language, however complex syntactic phenomena such as coordination or PP attachment are poorly modeled. Recent work which incorporates additional source-side linguistic information in NMT models (Luong et al., 2016; Sennrich and Haddow, 2016) show that even though neural models have strong learning capabilities, explicit features can still improve translation quality. In this work, we examine the benefit of incorporating global syntactic information on the target-side. We also address the question of h"
W17-4707,W12-3150,1,0.846453,"erleaving) improves translation quality more than the decoupled signal from multitask training. • We show that incorporating source-side linguistic information is complimentary to our method, further improving the translation quality. • We present a fine-grained analysis of SNMT and show consistent gains for different linguistic phenomena and sentence lengths. 2 Related work Syntax has helped in statistical machine translation (SMT) to capture dependencies between distant words that impact morphological agreement, subcategorisation and word order (Galley et al., 2004; Menezes and Quirk, 2007; Williams and Koehn, 2012; Nadejde et al., 2013; Sennrich, 2015; Nadejde et al., 2016a,b; Chiang, 2007). There has been some work in NMT on modeling source-side syntax implicitly or explicitly. Kalchbrenner and Blunsom (2013); Cho et al. (2014a) capture the hierarchical aspects of language implicitly by using convolutional neural networks, while Eriguchi et al. (2016) use the parse tree of the source sentence to guide the recurrence and attention model in tree-to-sequence NMT. Luong 3 Modeling Syntax in NMT CCG is a lexicalised formalism in which words are assigned with syntactic categories, i.e., supertags, that indi"
W17-4717,W17-4758,0,0.0835228,"ese submissions investigate alternative machine learning models for the prediction of the HTER score on the sentencelevel task. Instead of directly predicting the HTER score, the systems use a single-layer perceptron with four outputs that jointly predict the number of each of the four distinct post-editing operations that are then used to calculate the HTER score. This also gives the possibility to correct invalid (e.g. negative) predicted values prior to the calculation of the HTER score. The two submissions use the baseline features and the EnglishGerman submission also uses features from (Avramidis, 2017a). JXNU (T1): The JXNU submissions use features extracted from a neural network, including embedding features and cross-entropy features of the source sentences and their machine translations. The sentence embedding features are extracted through global average pooling from word embedding, which are trained using the WORD 2 VEC toolkit. The sentence cross-entropy features are calculated by a recurrent neural network language model. They experimented with different sentence embedding dimensions of the source sentences and translation outputs, as well as different sizes of the training corpus."
W17-4717,W17-4772,0,0.0310859,"Missing"
W17-4717,W17-4759,0,0.0327025,"Missing"
W17-4717,L16-1356,1,0.770826,"or BAD. The ﬁrst two matrices are built from 300 dimension word vectors computed with pretrained in-domain word embeddings. They train their model over 100 iterations with the l2 norm as regulariser and using the forward-backward splitting algorithm (FOBOS) (Duchi and Singer, 2009) as optimisation method. They report results considering the word and its context versus the word in isolation, as well as variants with and without the gold labels at training time. Finally, for the phrase-level task, SHEF made use of predictions generated by BMAPS for task 2 and the phrase labelling approaches in (Blain et al., 2016). These approaches use the number of BAD word-level predictions in a phrase: an optimistic version labels the phrase as OK if at least half of the words in it are predicted to be OK, and a superpessimistic version labels the phrase as BAD if any word is in is predicted to be BAD. UHH (T1): The UHH-STK submission is based on sequence and tree kernels applied on the source and target input data for predicting the HTER score. The kernels use a backtranslation of the MT output into the source language as an additional input data representation. Further hand-crafted features were deﬁned in the form"
W17-4717,W17-4760,1,0.841464,"Missing"
W17-4717,W17-4755,1,0.0393895,"nd Conference on Statistical Machine Translation (WMT) held at EMNLP 2017. This conference builds on eleven previous editions of WMT as workshops and conference (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016a). This year we conducted several ofﬁcial tasks. We report in this paper on three tasks: • news translation (Section 2, Section 3) • quality estimation (Section 4) • automatic post-editing (Section 5) The conference featured additional shared tasks that are described in separate papers in these proceedings: • metrics (Bojar et al., 2017a) • multimodal machine translation and multilingual image description (Elliott et al., 2017) • biomedical translation (Jimeno Yepes et al., 2017) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (constraint condition). We held 14 translation tasks this year, between English and each of Chinese, Czech, German, Finnish, Latvian, Russian, and Turkish. The Latvian and Chinese translation tasks were new this year. Latvian is a lesser resourced data condition on challenging language pair"
W17-4717,W07-0718,1,0.696979,") • bandit learning (Sokolov et al., 2017) This paper presents the results of the WMT17 shared tasks, which included three machine translation (MT) tasks (news, biomedical, and multimodal), two evaluation tasks (metrics and run-time estimation of MT quality), an automatic post-editing task, a neural MT training task, and a bandit learning task. 1 Introduction We present the results of the shared tasks of the Second Conference on Statistical Machine Translation (WMT) held at EMNLP 2017. This conference builds on eleven previous editions of WMT as workshops and conference (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016a). This year we conducted several ofﬁcial tasks. We report in this paper on three tasks: • news translation (Section 2, Section 3) • quality estimation (Section 4) • automatic post-editing (Section 5) The conference featured additional shared tasks that are described in separate papers in these proceedings: • metrics (Bojar et al., 2017a) • multimodal machine translation and multilingual image description (Elliott et al., 2017) • biomedical translation (Jimeno Yepes et al., 2017) In the news translation task (Section 2), part"
W17-4717,W08-0309,1,0.537902,"Missing"
W17-4717,W10-1703,1,0.603181,"Missing"
W17-4717,W12-3102,1,0.508067,"Missing"
W17-4717,W17-4761,0,0.0349831,"Missing"
W17-4717,W17-4723,0,0.0362034,"Missing"
W17-4717,W17-4724,1,0.839009,"Missing"
W17-4717,W11-2103,1,0.744276,"Missing"
W17-4717,W17-4773,1,0.839713,"Missing"
W17-4717,W15-3025,1,0.905105,"Instead of predicting the HTER score, the systems attempted to predict the number of each of the four postediting operations (add, replace, shift, delete) at the sentence level. However, this did not lead to positive results. In future editions of the task, we plan to make this detailed post-editing information available again and suggest clear ways of using it. 5 Automatic Post-editing Task The WMT shared task on MT automatic postediting (APE), this year at its third round at WMT, aims to evaluate systems for the automatic correction of errors in a machine translated text. As pointed out by (Chatterjee et al., 2015b), from the application point of view the task is motivated by its possible uses to: • Improve MT output by exploiting information unavailable to the decoder, or by performing deeper text analysis that is too expensive at the decoding stage; • Cope with systematic errors of an MT system whose decoding process is not accessible; • Provide professional translators with improved MT output quality to reduce (human) post-editing effort; • Adapt the output of a general-purpose MT system to the lexicon/style requested in a speciﬁc application domain. The third round of the APE task proposed to parti"
W17-4717,W17-4718,1,0.0662673,"builds on eleven previous editions of WMT as workshops and conference (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016a). This year we conducted several ofﬁcial tasks. We report in this paper on three tasks: • news translation (Section 2, Section 3) • quality estimation (Section 4) • automatic post-editing (Section 5) The conference featured additional shared tasks that are described in separate papers in these proceedings: • metrics (Bojar et al., 2017a) • multimodal machine translation and multilingual image description (Elliott et al., 2017) • biomedical translation (Jimeno Yepes et al., 2017) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (constraint condition). We held 14 translation tasks this year, between English and each of Chinese, Czech, German, Finnish, Latvian, Russian, and Turkish. The Latvian and Chinese translation tasks were new this year. Latvian is a lesser resourced data condition on challenging language pair. Chinese allowed us to co-operate with an ongoing evaluation campaign on Asian languages org"
W17-4717,W17-4725,0,0.0391077,"Missing"
W17-4717,W08-0509,0,0.00859402,"l post-editors) and WMT17 DE-EN (German-English, pharmacological domain, professional post-editors) APE task data. TER BLEU APE15 23.84 n/a APE16 24.76 62.11 APE17 EN-DE 24.48 62.49 APE17 DE-EN 15.55 79.54 Table 26: Translation quality (TER/BLEU of TGT and proportion of TGTs with TER=0) of the WMT15, WMT16, WMT17 EN-DE and WMT17 DE-EN data. Figure 7: TER distribution over the EN-DE test set Figure 8: TER distribution over the DE-EN test set TERcom27 software: lower average TER scores correspond to higher ranks. BLEU was computed using the multi-bleu.perl package28 available in MOSES. MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the KenLM toolkit (Heaﬁeld, 2011) for standard n-gram modeling with an n-gram length of 5. Finally, the system was tuned on the development set, optimizing TER/BLEU with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison are also reported in Tables 28-29. For each submitted run, the statistical signiﬁcance of performance differences with respect to the baseline and our re-implementation of Simard et al. (2007) was calculated with the bootstrap test (Koehn, 2004). 5.1.3 Baselines Also this year, the ofﬁci"
W17-4717,W17-4726,0,0.0387381,"Missing"
W17-4717,W13-2305,1,0.899702,"(RR) approach, so named because the pairwise comparisons denote only relative ability between a pair of systems, and cannot be used to infer absolute quality. For example, RR can be used to discover which systems perform better than others, but RR does not provide any information about the absolute quality of system translations, i.e. it provides no information about how far a given system is from producing perfect output according to a human user. Work on evaluation over the past few years has provided fresh insight into ways to collect direct assessments (DA) of machine translation quality (Graham et al., 2013, 2014, 2016), and last year’s evaluation campaign included parallel assessment of a subset of News task language pairs evaluated with RR and DA. DA has some clear advantages over RR, namely the evaluation of absolute translation quality and the ability to carry out evaluations through quality controlled crowd-sourcing. As established last year (Bojar et al., 2016a), DA results (via crowd-sourcing) and RR results (produced by researchers) correlate strongly, with Pearson correlation ranging from 0.920 to 0.997 across several source languages into English and at 0.975 for English-to-Russian (th"
W17-4717,E14-1047,1,0.508986,"Missing"
W17-4717,N15-1124,1,0.658439,"Missing"
W17-4717,W13-0805,0,0.0140095,"ely contain 25,000 and 1,000 triplets, while the test set consists of 2,000 instances. Table 24 provides some basic statistics about the data (the same used for the sentence-level quality estimation task), which has been released by the European Project QT21 (Specia et al., 2017b).25 In addition, Tables 25 and 26 provide a view of the data from a task difﬁculty standpoint. Table 25 shows the repetition rate (RR) values of the data sets released in the three rounds 24 We used phrase-based MT systems trained with generic and in-domain parallel training data, leveraging prereordering techniques (Herrmann et al., 2013), and taking advantage of POS and word class-based language models. 25 For both language directions, the source sentences and reference translations were provided by TAUS (https://www.taus.net/). 197 EN-DE Train (23,000) Dev (1,000) Test (2,000) DE-EN Train (25,000) Dev (1,000) Test (2,000) SRC Tokens TGT PE SRC Types TGT PE SRC Lemmas TGT PE 384448 17827 65120 403306 19355 69812 411246 19763 71483 18220 2931 8061 27382 3333 9765 31652 3506 10502 10946 1922 2626 21959 2686 3976 25550 2806 4282 437833 17578 35087 453096 18130 36082 456163 18313 36480 29745 4426 6987 19866 3583 5391 19172 3642 5"
W17-4717,W17-4775,0,0.0513454,"Missing"
W17-4717,W17-4730,1,0.829861,"Missing"
W17-4717,W17-4731,0,0.029579,"Missing"
W17-4717,W17-4727,0,0.046917,"Missing"
W17-4717,W16-2378,0,0.0869978,"a manuallyrevised version of the target, done by professional translators. Test data consists of (source, target) pairs having similar characteristics of those in the training set. Human post-edits of the test target instances were left apart to measure system performance. English-German data were drawn from the Information Technology (IT) domain. Training and test sets respectively contain 11,000 and 2,000 triplets. The data released for the 2016 round of the task (15,000 instances) and the artiﬁcially generated post-editing triplets (4 million instances) used by last year’s winning system (Junczys-Dowmunt and Grundkiewicz, 2016) were also provided as additional training material. German-English data were drawn from the Pharmacological domain. Training and development sets respectively contain 25,000 and 1,000 triplets, while the test set consists of 2,000 instances. Table 24 provides some basic statistics about the data (the same used for the sentence-level quality estimation task), which has been released by the European Project QT21 (Specia et al., 2017b).25 In addition, Tables 25 and 26 provide a view of the data from a task difﬁculty standpoint. Table 25 shows the repetition rate (RR) values of the data sets rele"
W17-4717,W11-2123,0,0.00943973,"-editors) APE task data. TER BLEU APE15 23.84 n/a APE16 24.76 62.11 APE17 EN-DE 24.48 62.49 APE17 DE-EN 15.55 79.54 Table 26: Translation quality (TER/BLEU of TGT and proportion of TGTs with TER=0) of the WMT15, WMT16, WMT17 EN-DE and WMT17 DE-EN data. Figure 7: TER distribution over the EN-DE test set Figure 8: TER distribution over the DE-EN test set TERcom27 software: lower average TER scores correspond to higher ranks. BLEU was computed using the multi-bleu.perl package28 available in MOSES. MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the KenLM toolkit (Heaﬁeld, 2011) for standard n-gram modeling with an n-gram length of 5. Finally, the system was tuned on the development set, optimizing TER/BLEU with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison are also reported in Tables 28-29. For each submitted run, the statistical signiﬁcance of performance differences with respect to the baseline and our re-implementation of Simard et al. (2007) was calculated with the bootstrap test (Koehn, 2004). 5.1.3 Baselines Also this year, the ofﬁcial baseline results are the TER and BLEU scores calculated by comparing the raw MT o"
W17-4717,I17-1013,0,0.0334345,"Missing"
W17-4717,W16-2384,0,0.0137431,"features are calculated by a recurrent neural network language model. They experimented with different sentence embedding dimensions of the source sentences and translation outputs, as well as different sizes of the training corpus. The experimental results show that the neural network features lead to signiﬁcant improvements over the baseline, and that combining the neural network features with baseline features leads to further improvement. POSTECH (T1, T2, T3): POSTECH’s submissions to the sentence/word/phrase-level QE tasks are based on predictor-estimator architecture (Kim et al., 2017; Kim and Lee, 2016), which is the two-stage end-to-end ID CDACM DCU DFKI JXNU POSTECH RTM SHEF UHH Unbabel Participating team Centre for Development of Advanced Computing, India (Patel and M, 2016) Dublin City University (Hokamp, 2017) German Research Centre for Artiﬁcial Intelligence, Germany (Avramidis, 2017b) Jiangxi Normal University, China (Chen et al., 2017) Pohang University of Science and Technology, Republic of Korea (Kim et al., 2017) Referential Translation Machines, Turkey (Bic¸ici, 2017) University of Shefﬁeld, UK (Blain et al., 2017; Paetzold and Specia, 2017) University of Hamburg, Germany (Duma a"
W17-4717,W17-4763,0,0.0300648,"ence cross-entropy features are calculated by a recurrent neural network language model. They experimented with different sentence embedding dimensions of the source sentences and translation outputs, as well as different sizes of the training corpus. The experimental results show that the neural network features lead to signiﬁcant improvements over the baseline, and that combining the neural network features with baseline features leads to further improvement. POSTECH (T1, T2, T3): POSTECH’s submissions to the sentence/word/phrase-level QE tasks are based on predictor-estimator architecture (Kim et al., 2017; Kim and Lee, 2016), which is the two-stage end-to-end ID CDACM DCU DFKI JXNU POSTECH RTM SHEF UHH Unbabel Participating team Centre for Development of Advanced Computing, India (Patel and M, 2016) Dublin City University (Hokamp, 2017) German Research Centre for Artiﬁcial Intelligence, Germany (Avramidis, 2017b) Jiangxi Normal University, China (Chen et al., 2017) Pohang University of Science and Technology, Republic of Korea (Kim et al., 2017) Referential Translation Machines, Turkey (Bic¸ici, 2017) University of Shefﬁeld, UK (Blain et al., 2017; Paetzold and Specia, 2017) University of Hamb"
W17-4717,W04-3250,1,0.380171,"age28 available in MOSES. MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the KenLM toolkit (Heaﬁeld, 2011) for standard n-gram modeling with an n-gram length of 5. Finally, the system was tuned on the development set, optimizing TER/BLEU with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison are also reported in Tables 28-29. For each submitted run, the statistical signiﬁcance of performance differences with respect to the baseline and our re-implementation of Simard et al. (2007) was calculated with the bootstrap test (Koehn, 2004). 5.1.3 Baselines Also this year, the ofﬁcial baseline results are the TER and BLEU scores calculated by comparing the raw MT output with the human post-edits. In practice, the baseline APE system is a “donothing” system that leaves all the test targets unmodiﬁed. Baseline results, the same shown in Table 26, are also reported in Tables 28-29 for comparison with participants’ submissions. In continuity with the previous rounds, we used as additional term of comparison a reimplementation of the method ﬁrstly proposed by Simard et al. (2007). It relies on a phrasebased post-editing approach to t"
W17-4717,P07-2045,1,0.0149362,"t with the human post-edits. In practice, the baseline APE system is a “donothing” system that leaves all the test targets unmodiﬁed. Baseline results, the same shown in Table 26, are also reported in Tables 28-29 for comparison with participants’ submissions. In continuity with the previous rounds, we used as additional term of comparison a reimplementation of the method ﬁrstly proposed by Simard et al. (2007). It relies on a phrasebased post-editing approach to the task, which represented the common backbone of APE systems before the spread of neural solutions. The system is based on Moses (Koehn et al., 2007); translation and reordering models were estimated following the Moses protocol with default setup using 27 http://www.cs.umd.edu/˜snover/tercom/ https://github.com/moses-smt/mosesdecoder/ blob/master/scripts/generic/multi-bleu.perl 28 5.2 Participants Seven teams participated in the English-German task by submitting a total of ﬁfteen runs. Two of them also participated in the German-English task with ﬁve submitted runs. Participants are listed in Table 27, and a short description of their systems is provided in the following. Adam Mickiewicz University. AMU’s (ENDE) participation explores and"
W17-4717,C14-1017,0,0.0141812,"ord embeddings approach used by (Scarton et al., 2016) for document-level QE. Here in-domain word embeddings are used instead of embeddings obtained general purpose data (same as in task 2, below). Word embeddings were averaged to generate a single vector for each sentence. Source and target word embeddings were then concatenated with the baseline features and given to an SVM regressor for model building. For the word-level task SHEF investigated a new approach based on predicting the strength of the lexical relationships between the source and target sentences (BMAPS). Following the work by (Madhyastha et al., 2014), a bilinear model is trained from three matrices corresponding to the training data, the development set and a “truth” matrix between them, which is built from the word alignments and the gold labels to indicate which lexical items form a pair, and whether or not their lexical relation is OK or BAD. The ﬁrst two matrices are built from 300 dimension word vectors computed with pretrained in-domain word embeddings. They train their model over 100 iterations with the l2 norm as regulariser and using the forward-backward splitting algorithm (FOBOS) (Duchi and Singer, 2009) as optimisation method."
W17-4717,Q17-1015,0,0.0112193,"ID CDACM DCU DFKI JXNU POSTECH RTM SHEF UHH Unbabel Participating team Centre for Development of Advanced Computing, India (Patel and M, 2016) Dublin City University (Hokamp, 2017) German Research Centre for Artiﬁcial Intelligence, Germany (Avramidis, 2017b) Jiangxi Normal University, China (Chen et al., 2017) Pohang University of Science and Technology, Republic of Korea (Kim et al., 2017) Referential Translation Machines, Turkey (Bic¸ici, 2017) University of Shefﬁeld, UK (Blain et al., 2017; Paetzold and Specia, 2017) University of Hamburg, Germany (Duma and Menzel, 2017) Unbabel, Portugal (Martins et al., 2017b) Table 10: Participants in the WMT17 quality estimation shared task. neural QE model. The predictor-estimator architecture consists of two types of stacked neural network models: 1) a word prediction model based on bidirectional and bilingual recurrent neural network language model trained on additional large-scale parallel corpora and 2) a neural quality estimation model trained on quality-annotated noisy parallel corpora. To jointly learn the two-stage model, a stack propagation method was applied (Zhang and Weiss, 2016). In addition, a “multilevel model” was developed where a task-speciﬁc"
W17-4717,W16-2387,0,0.0126259,"the SMT parallel corps. These features were used to train a Support Vector Regression (SVR) algorithm using a Radial Basis Function (RBF) kernel within the SCIKITLEARN toolkit.13 The γ, � and C parameters were optimised via grid search with 5-fold cross validation on the training set, resulting in γ=0.01, � = 0.0825, C = 20. This baseline system has proved robust across a range of language pairs, MT systems, and text domains for predicting various In addition to that, 6 new features were included which contain combinations of other features, and which proved useful in (Kreutzer et al., 2015; Martins et al., 2016): 12 https://github.com/ghpaetzold/ questplusplus 13 http://scikit-learn.org/ 184 • • • • Target word + left context. Target word + right context. Target word + aligned source word. POS of target word + POS of aligned source word. • Target word + left context + source word. • Target word + right context + source word. • Punctuation features: The baseline system models the task as a sequence prediction problem using the LinearChain Conditional Random Fields (CRF) algorithm within the CRFSuite tool (Okazaki, 2007). The model was trained using passive-aggressive optimisation algorithm. We note th"
W17-4717,W17-4764,0,0.0178514,"ID CDACM DCU DFKI JXNU POSTECH RTM SHEF UHH Unbabel Participating team Centre for Development of Advanced Computing, India (Patel and M, 2016) Dublin City University (Hokamp, 2017) German Research Centre for Artiﬁcial Intelligence, Germany (Avramidis, 2017b) Jiangxi Normal University, China (Chen et al., 2017) Pohang University of Science and Technology, Republic of Korea (Kim et al., 2017) Referential Translation Machines, Turkey (Bic¸ici, 2017) University of Shefﬁeld, UK (Blain et al., 2017; Paetzold and Specia, 2017) University of Hamburg, Germany (Duma and Menzel, 2017) Unbabel, Portugal (Martins et al., 2017b) Table 10: Participants in the WMT17 quality estimation shared task. neural QE model. The predictor-estimator architecture consists of two types of stacked neural network models: 1) a word prediction model based on bidirectional and bilingual recurrent neural network language model trained on additional large-scale parallel corpora and 2) a neural quality estimation model trained on quality-annotated noisy parallel corpora. To jointly learn the two-stage model, a stack propagation method was applied (Zhang and Weiss, 2016). In addition, a “multilevel model” was developed where a task-speciﬁc"
W17-4717,W06-3114,1,0.104699,"g (Bojar et al., 2017b) • bandit learning (Sokolov et al., 2017) This paper presents the results of the WMT17 shared tasks, which included three machine translation (MT) tasks (news, biomedical, and multimodal), two evaluation tasks (metrics and run-time estimation of MT quality), an automatic post-editing task, a neural MT training task, and a bandit learning task. 1 Introduction We present the results of the shared tasks of the Second Conference on Statistical Machine Translation (WMT) held at EMNLP 2017. This conference builds on eleven previous editions of WMT as workshops and conference (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016a). This year we conducted several ofﬁcial tasks. We report in this paper on three tasks: • news translation (Section 2, Section 3) • quality estimation (Section 4) • automatic post-editing (Section 5) The conference featured additional shared tasks that are described in separate papers in these proceedings: • metrics (Bojar et al., 2017a) • multimodal machine translation and multilingual image description (Elliott et al., 2017) • biomedical translation (Jimeno Yepes et al., 2017) In the news trans"
W17-4717,P03-1021,0,0.144477,"ed better than monolingual models. The code for these models is freely available.15 DCU (T2): DCU’s submission is an ensemble of neural MT systems with different input factors, designed to jointly tackle both the automatic post-editing and word-level QE. 15 https://github.com/patelrajnath/rnn4nlp 186 Word-level features which have proven effective for QE, such as part-of-speech tags and dependency labels are included as input factors to NMT systems. NMT systems using different input representations are ensembled together in a log-linear model which is tuned for the F1 -mult metric using MERT (Och, 2003). The output of the ensemble is a pseudo-reference that is then TER aligned with the original MT to obtain OK/BAD tags for each word in the MT hypothesis. DFKI (T1): These submissions investigate alternative machine learning models for the prediction of the HTER score on the sentencelevel task. Instead of directly predicting the HTER score, the systems use a single-layer perceptron with four outputs that jointly predict the number of each of the four distinct post-editing operations that are then used to calculate the HTER score. This also gives the possibility to correct invalid (e.g. negativ"
W17-4717,W15-3037,0,0.0138062,"n in the source side of the SMT parallel corps. These features were used to train a Support Vector Regression (SVR) algorithm using a Radial Basis Function (RBF) kernel within the SCIKITLEARN toolkit.13 The γ, � and C parameters were optimised via grid search with 5-fold cross validation on the training set, resulting in γ=0.01, � = 0.0825, C = 20. This baseline system has proved robust across a range of language pairs, MT systems, and text domains for predicting various In addition to that, 6 new features were included which contain combinations of other features, and which proved useful in (Kreutzer et al., 2015; Martins et al., 2016): 12 https://github.com/ghpaetzold/ questplusplus 13 http://scikit-learn.org/ 184 • • • • Target word + left context. Target word + right context. Target word + aligned source word. POS of target word + POS of aligned source word. • Target word + left context + source word. • Target word + right context + source word. • Punctuation features: The baseline system models the task as a sequence prediction problem using the LinearChain Conditional Random Fields (CRF) algorithm within the CRFSuite tool (Okazaki, 2007). The model was trained using passive-aggressive optimisatio"
W17-4717,P16-1160,0,0.0330991,"t attention (looking at information anywhere in the source sequence during decoding) and hard monotonic attention (looking at one encoder state at a time from left to right, thus being more conservative and faithful to the original input), which are combined in different ways in the case of multi-source models. The artiﬁcial data provided by JunczysDowmunt and Grundkiewicz (2016) are used to boost performance by increasing the size of the corpus used for training. Univerzita Karlova v Praze. CUNI’s (EN-DE) system is based on the character-to-character neural network architecture described in (Lee et al., 2016). This architecture was compared with the standard neural network architecture proposed by Bahdanau et al. (2014) which uses byte-pair encoding (Sennrich et al., 2015) for generating translation tokens. During the experiments, two setups have been compared for each architecture: i) a single encoder with SRC and MT sentences concatenated, and ii) a two-encoder system, where each SRC and MT sentence is fed to a separate encoder. The submitted system uses the two-encoder architecture with a character-level encoder and decoder. The initial state of the decoder is a weighted combination of the ﬁnal"
W17-4717,W17-4733,0,0.0368107,"Missing"
W17-4717,W17-4765,1,0.786172,"Missing"
W17-4717,L16-1582,1,0.768758,"T 183 translations labelled with task-speciﬁc labels. Participants were also provided with a baseline set of features for each task, and a software package to extract these and other quality estimation features and perform model learning (Section 4.1). Participants (Section 4.2) could submit up to two systems for each task. A discussion on the main goals and ﬁndings from this year’s task is given in Section 4.7. 4.1 Baseline systems forms of post-editing effort (2012; 2013; 2014; 2015; 2016a). Word-level baseline system: For Task 2, the baseline features were extracted with the M AR MOT tool (Logacheva et al., 2016). These are 28 features that have been deemed the most informative in previous research on word-level QE. 22 of them were taken from the feature set described in (Luong et al., 2014), and had also been used as a baseline feature set at WMT16: Sentence-level baseline system: For Task 1, Q U E ST ++12 (2015) was used to extract 17 MT system-independent features from the source and translation (target) ﬁles and parallel corpora: • Word count in the source and target sentences, and source and target token count ratio. Although these features are sentencelevel (i.e. their values will be the same fo"
W17-4717,C16-1241,0,0.0353857,"Missing"
W17-4717,W14-3342,0,0.0181229,"lity estimation features and perform model learning (Section 4.1). Participants (Section 4.2) could submit up to two systems for each task. A discussion on the main goals and ﬁndings from this year’s task is given in Section 4.7. 4.1 Baseline systems forms of post-editing effort (2012; 2013; 2014; 2015; 2016a). Word-level baseline system: For Task 2, the baseline features were extracted with the M AR MOT tool (Logacheva et al., 2016). These are 28 features that have been deemed the most informative in previous research on word-level QE. 22 of them were taken from the feature set described in (Luong et al., 2014), and had also been used as a baseline feature set at WMT16: Sentence-level baseline system: For Task 1, Q U E ST ++12 (2015) was used to extract 17 MT system-independent features from the source and translation (target) ﬁles and parallel corpora: • Word count in the source and target sentences, and source and target token count ratio. Although these features are sentencelevel (i.e. their values will be the same for all words in a sentence), the length of a sentence might inﬂuence the probability of a word being wrong. • Target token, its left and right contexts of 1 word. • Source word aligne"
W17-4717,P16-2046,0,0.0171489,"Missing"
W17-4717,W16-2379,0,0.0228966,"Missing"
W17-4717,P02-1040,0,0.119222,"Missing"
W17-4717,W16-2389,0,0.038207,"Missing"
W17-4717,W17-4736,0,0.0268357,"Missing"
W17-4717,W17-4737,0,0.0383776,"Missing"
W17-4717,W17-4738,0,0.0427074,"Missing"
W17-4717,W14-3301,1,0.655758,"Missing"
W17-4717,W16-2391,1,0.761669,"target sentences into sequences of character embeddings, and then passes them through a series of deep parallel stacked convolution/max pooling layers. The baseline features are provided through a multi-layer perceptron, 187 and then concatenated with the characterlevel information. Finally, the concatenation is passed onto another multi-layer perceptron and the very last layer outputs HTER values. The two submissions differ in the the use of standard (CNN+BASE-Single) and multi-task learning (CNN+BASE-Multi) for training. The QUEST-EMB submission follows the word embeddings approach used by (Scarton et al., 2016) for document-level QE. Here in-domain word embeddings are used instead of embeddings obtained general purpose data (same as in task 2, below). Word embeddings were averaged to generate a single vector for each sentence. Source and target word embeddings were then concatenated with the baseline features and given to an SVM regressor for model building. For the word-level task SHEF investigated a new approach based on predicting the strength of the lexical relationships between the source and target sentences (BMAPS). Following the work by (Madhyastha et al., 2014), a bilinear model is trained"
W17-4717,W16-2323,1,0.349233,"theses. Concatenated source + MT hypothesis are also used as an input representation for some models. The system makes extensive use of the synthetic training data provided by Junczys-Dowmunt and Grundkiewicz (2016), as well as min-risk training for ﬁne-tuning (Shen et al., 2016). The neural systems, which use different input representations but share the same output vocabulary, are then ensembled together in a loglinear model which is tuned for the TER metric using MERT. Fondazione Bruno Kessler. FBK’s (EN-DE & DE-EN) submission extends the existing NMT implementation in the Nematus toolkit (Sennrich et al., 2016) to train an ensemble of multi-source neural APE systems. Building on previous participations based on the phrase-based paradigm (Chatterjee et al., 2015a, 2016), and similar to (Libovick´y et al., 2016), such systems jointly learn from source and target information in order to increase robustness and precision of the automatic corrections. The n-best hypotheses produced by 200 this ensemble are further re-ranked using features based on the edit distance between the original MT output and each APE hypothesis, as well as other statistical models (n-gram language model and operation sequence mod"
W17-4717,P16-1159,0,0.0156896,"different input factors, designed to jointly tackle both the APE task and the Word-Level QE task. Word-Level features which have proven effective for QE, such as word-alignments, partof-speech tags, and dependency labels, are included as input factors to neural machine translation systems, which are trained to output PostEdited MT hypotheses. Concatenated source + MT hypothesis are also used as an input representation for some models. The system makes extensive use of the synthetic training data provided by Junczys-Dowmunt and Grundkiewicz (2016), as well as min-risk training for ﬁne-tuning (Shen et al., 2016). The neural systems, which use different input representations but share the same output vocabulary, are then ensembled together in a loglinear model which is tuned for the TER metric using MERT. Fondazione Bruno Kessler. FBK’s (EN-DE & DE-EN) submission extends the existing NMT implementation in the Nematus toolkit (Sennrich et al., 2016) to train an ensemble of multi-source neural APE systems. Building on previous participations based on the phrase-based paradigm (Chatterjee et al., 2015a, 2016), and similar to (Libovick´y et al., 2016), such systems jointly learn from source and target inf"
W17-4717,2006.amta-papers.25,0,0.817721,"trast to last year, we also provide datasets for two language pairs. The structure used for the data have been the same since WMT15. Each data instance consists of (i) a source sentence, (ii) its automatic translation into the target language, (iii) the manually post-edited version of the automatic translation, (iv) a free reference translation of the source sentence. Post-edits are used to extract labels for the 4.4 Task 1: Predicting sentence-level quality This task consists in scoring (and ranking) translation sentences according to the proportion of their words that need to be ﬁxed. HTER (Snover et al., 2006b) is used as quality score, i.e. the minimum edit distance between the machine translation and its manually post-edited version. Labels HTER labels were computed using the TERCOM tool16 with default settings (tokenised, case insensitive, exact matching only), with scores capped to 1. 188 16 http://www.cs.umd.edu/˜snover/tercom/ Evaluation Evaluation was performed against the true HTER label and/or ranking, using the following metrics: with an edit operation: insertion, deletion, substitution or no edit (correct word). We mark each edited word as BAD, and the remainingn as OK. • Scoring: Pears"
W17-4717,W17-4756,0,0.0529328,"Missing"
W17-4717,P15-4020,1,0.738308,"Missing"
W17-4717,P13-4014,1,0.634274,"Missing"
W17-4717,W17-4720,1,0.830567,"Missing"
W17-4717,W17-4776,0,0.0596806,"Missing"
W17-4717,W17-4777,1,0.832756,"Missing"
W17-4717,W17-4742,0,0.0342495,"Missing"
W17-4717,W17-4744,0,0.0607458,"Missing"
W17-4717,C00-2137,0,0.0420443,"ch edited word as BAD, and the remainingn as OK. • Scoring: Pearson’s r correlation score (primary metric, ofﬁcial score for ranking submissions), Mean Average Error (MAE) and Root Mean Squared Error (RMSE). Evaluation Analogously to the last year’s task, the primary evaluation metric is the multiplication of F1 -scores for the OK and BAD classes, denoted as F1 -mult. Unlike previously used F1 BAD score this metric is not biased towards “pessimistic” labellings. We also report F1 -scores for individual classes for completeness. We test the signiﬁcance of the results using randomisation tests (Yeh, 2000) with Bonferroni correction (Abdi, 2007). • Ranking: Spearman’s ρ rank correlation and DeltaAvg. Statistical signiﬁcance on Pearson r was computed using the William’s test.17 Results Tables 13 and 14 summarise the results for Task 1 on German–English and English– German datasets, respectively, ranking participating systems best to worst using Pearson’s r correlation as primary key. Spearman’s ρ correlation scores should be used to rank systems for the ranking variant. The top three systems are the same for both datasets, and the ranking of systems according to their performance is similar for"
W17-4717,W17-4745,1,0.825998,"Missing"
W17-4717,P16-1147,0,0.0102821,"Missing"
W17-4724,W16-2310,1,0.682889,"tional language models trained on 50, 200, Introduction The JHU 2017 WMT submission consists of phrase-based systems, syntax-based systems and neural machine translation systems. In this paper we discuss features that we integrated into our system submissions. We also discuss lattice rescoring as a form of system combination of phrase-based and neural machine translation systems. The JHU phrase-based translation systems for our participation in the WMT 2017 shared translation task are based on the open source Moses toolkit (Koehn et al., 2007) and strong baselines of our submission last year (Ding et al., 2016). The JHU neural machine translation systems were built with the Nematus (Sennrich et al., 2016c) and Marian (Junczys-Dowmunt et al., 2016) toolkits. Our lattice rescoring experiments are also based on a combination of these three toolkits. 2 Configuration Phrase-Based Model Baselines Although the focus of research in machine translation has firmly moved onto neural machine translation, we still built traditional phrase-based statistical machine translation systems for all language pairs. These submissions also serve as a baseline 276 Proceedings of the Conference on Machine Translation (WMT),"
W17-4724,P13-2071,1,0.854174,"phrase-based, syntax-based and/or neural machine translation systems for all 14 language pairs of this year’s evaluation campaign. We also performed neural rescoring of phrasebased systems for English-Turkish and English-Finnish. 1 2.1 We trained our systems with the following settings: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, hierarchical lexicalized reordering (Galley and Manning, 2008), a lexicallydriven 5-gram operation sequence model (OSM) (Durrani et al., 2013) with 4 count-based supportive features, sparse domain indicator, phrase length, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 100-best translation options, compact phrase table (JunczysDowmunt, 2012) minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the noreordering-over-punctuation heuristic (Koehn and Haddow, 2009). We optimize feature function weights with k-best MIRA (Cherry and Foster, 2012). We used POS"
W17-4724,D08-1089,0,0.046349,"n task of EMNLP 2017 Second Conference on Machine Translation (WMT 2017). We set up phrase-based, syntax-based and/or neural machine translation systems for all 14 language pairs of this year’s evaluation campaign. We also performed neural rescoring of phrasebased systems for English-Turkish and English-Finnish. 1 2.1 We trained our systems with the following settings: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, hierarchical lexicalized reordering (Galley and Manning, 2008), a lexicallydriven 5-gram operation sequence model (OSM) (Durrani et al., 2013) with 4 count-based supportive features, sparse domain indicator, phrase length, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 100-best translation options, compact phrase table (JunczysDowmunt, 2012) minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the noreordering-over-punctuation heuristic (Koehn and Haddow, 2009). We optimize"
W17-4724,D08-1023,0,0.193388,"ign. We also performed neural rescoring of phrasebased systems for English-Turkish and English-Finnish. 1 2.1 We trained our systems with the following settings: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, hierarchical lexicalized reordering (Galley and Manning, 2008), a lexicallydriven 5-gram operation sequence model (OSM) (Durrani et al., 2013) with 4 count-based supportive features, sparse domain indicator, phrase length, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 100-best translation options, compact phrase table (JunczysDowmunt, 2012) minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the noreordering-over-punctuation heuristic (Koehn and Haddow, 2009). We optimize feature function weights with k-best MIRA (Cherry and Foster, 2012). We used POS and morphological tags as additional factors in phrase translation models (Koehn and Hoang, 2007) for the German-English language"
W17-4724,W11-2123,0,0.0496754,"ibes the Johns Hopkins University submissions to the shared translation task of EMNLP 2017 Second Conference on Machine Translation (WMT 2017). We set up phrase-based, syntax-based and/or neural machine translation systems for all 14 language pairs of this year’s evaluation campaign. We also performed neural rescoring of phrasebased systems for English-Turkish and English-Finnish. 1 2.1 We trained our systems with the following settings: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, hierarchical lexicalized reordering (Galley and Manning, 2008), a lexicallydriven 5-gram operation sequence model (OSM) (Durrani et al., 2013) with 4 count-based supportive features, sparse domain indicator, phrase length, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 100-best translation options, compact phrase table (JunczysDowmunt, 2012) minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and th"
W17-4724,P07-1019,0,0.0412146,"nments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, hierarchical lexicalized reordering (Galley and Manning, 2008), a lexicallydriven 5-gram operation sequence model (OSM) (Durrani et al., 2013) with 4 count-based supportive features, sparse domain indicator, phrase length, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 100-best translation options, compact phrase table (JunczysDowmunt, 2012) minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the noreordering-over-punctuation heuristic (Koehn and Haddow, 2009). We optimize feature function weights with k-best MIRA (Cherry and Foster, 2012). We used POS and morphological tags as additional factors in phrase translation models (Koehn and Hoang, 2007) for the German-English language pairs. We also trained target sequence models on the in-domain subset of the parallel corpus using Kneser-Ney smoothed 7-gram models. We used syntactic preordering (Collins et al., 2005) and compound splitting (Koehn and Knight, 2003) for t"
W17-4724,buck-etal-2014-n,0,0.0365342,"Missing"
W17-4724,W08-0336,0,0.0357741,"rs. These submissions also serve as a baseline 276 Proceedings of the Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 276–282 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics Language Pair German–English Czech–English Finnish–English Latvian–English Russian-English Turkish–English Chinese–English Sentences 21,243 21,730 2,870 984 11,824 1,001 1,000 For English data, we used the scripts from Moses (Koehn et al., 2007) to tokenize our data, while for Chinese data we carried out word segmentation with Stanford word segmenter (Chang et al., 2008). We also normalized all the Chinese punctuations to their English counterparts to avoid disagreement across sentences. We parsed the tokenized data with Berkeley Parser (Petrov and Klein, 2007) using the pre-trained grammar provided with the toolkit, followed by right binarization of the parse. Finally, truecasing was performed on all the English texts. Due to the lack of casing system, we did not perform truecasing for any Chinese texts. We performed word alignment with fast-align (Dyer et al., 2013) due to the huge scale of this year’s training data and grow-diag-final-and heuristic for ali"
W17-4724,2012.eamt-1.58,0,0.0622113,"Missing"
W17-4724,N12-1047,0,0.032334,"ce model (OSM) (Durrani et al., 2013) with 4 count-based supportive features, sparse domain indicator, phrase length, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 100-best translation options, compact phrase table (JunczysDowmunt, 2012) minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the noreordering-over-punctuation heuristic (Koehn and Haddow, 2009). We optimize feature function weights with k-best MIRA (Cherry and Foster, 2012). We used POS and morphological tags as additional factors in phrase translation models (Koehn and Hoang, 2007) for the German-English language pairs. We also trained target sequence models on the in-domain subset of the parallel corpus using Kneser-Ney smoothed 7-gram models. We used syntactic preordering (Collins et al., 2005) and compound splitting (Koehn and Knight, 2003) for the German-to-English systems. We did no language-specific processing for other languages. We included Och cluster language model, with 4 additional language models trained on 50, 200, Introduction The JHU 2017 WMT su"
W17-4724,N09-1025,0,0.0519466,"Missing"
W17-4724,D07-1091,1,0.65067,"gth, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 100-best translation options, compact phrase table (JunczysDowmunt, 2012) minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the noreordering-over-punctuation heuristic (Koehn and Haddow, 2009). We optimize feature function weights with k-best MIRA (Cherry and Foster, 2012). We used POS and morphological tags as additional factors in phrase translation models (Koehn and Hoang, 2007) for the German-English language pairs. We also trained target sequence models on the in-domain subset of the parallel corpus using Kneser-Ney smoothed 7-gram models. We used syntactic preordering (Collins et al., 2005) and compound splitting (Koehn and Knight, 2003) for the German-to-English systems. We did no language-specific processing for other languages. We included Och cluster language model, with 4 additional language models trained on 50, 200, Introduction The JHU 2017 WMT submission consists of phrase-based systems, syntax-based systems and neural machine translation systems. In this"
W17-4724,P16-1162,0,0.735349,"of phrase-based systems, syntax-based systems and neural machine translation systems. In this paper we discuss features that we integrated into our system submissions. We also discuss lattice rescoring as a form of system combination of phrase-based and neural machine translation systems. The JHU phrase-based translation systems for our participation in the WMT 2017 shared translation task are based on the open source Moses toolkit (Koehn et al., 2007) and strong baselines of our submission last year (Ding et al., 2016). The JHU neural machine translation systems were built with the Nematus (Sennrich et al., 2016c) and Marian (Junczys-Dowmunt et al., 2016) toolkits. Our lattice rescoring experiments are also based on a combination of these three toolkits. 2 Configuration Phrase-Based Model Baselines Although the focus of research in machine translation has firmly moved onto neural machine translation, we still built traditional phrase-based statistical machine translation systems for all language pairs. These submissions also serve as a baseline 276 Proceedings of the Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 276–282 c Copenhagen, Denmark, September 711, 2017. 2017 A"
W17-4724,P07-2045,1,0.0439969,"r other languages. We included Och cluster language model, with 4 additional language models trained on 50, 200, Introduction The JHU 2017 WMT submission consists of phrase-based systems, syntax-based systems and neural machine translation systems. In this paper we discuss features that we integrated into our system submissions. We also discuss lattice rescoring as a form of system combination of phrase-based and neural machine translation systems. The JHU phrase-based translation systems for our participation in the WMT 2017 shared translation task are based on the open source Moses toolkit (Koehn et al., 2007) and strong baselines of our submission last year (Ding et al., 2016). The JHU neural machine translation systems were built with the Nematus (Sennrich et al., 2016c) and Marian (Junczys-Dowmunt et al., 2016) toolkits. Our lattice rescoring experiments are also based on a combination of these three toolkits. 2 Configuration Phrase-Based Model Baselines Although the focus of research in machine translation has firmly moved onto neural machine translation, we still built traditional phrase-based statistical machine translation systems for all language pairs. These submissions also serve as a bas"
W17-4724,E03-1076,1,0.540903,"ning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the noreordering-over-punctuation heuristic (Koehn and Haddow, 2009). We optimize feature function weights with k-best MIRA (Cherry and Foster, 2012). We used POS and morphological tags as additional factors in phrase translation models (Koehn and Hoang, 2007) for the German-English language pairs. We also trained target sequence models on the in-domain subset of the parallel corpus using Kneser-Ney smoothed 7-gram models. We used syntactic preordering (Collins et al., 2005) and compound splitting (Koehn and Knight, 2003) for the German-to-English systems. We did no language-specific processing for other languages. We included Och cluster language model, with 4 additional language models trained on 50, 200, Introduction The JHU 2017 WMT submission consists of phrase-based systems, syntax-based systems and neural machine translation systems. In this paper we discuss features that we integrated into our system submissions. We also discuss lattice rescoring as a form of system combination of phrase-based and neural machine translation systems. The JHU phrase-based translation systems for our participation in the"
W17-4724,N04-1022,0,0.0775843,"inal-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, hierarchical lexicalized reordering (Galley and Manning, 2008), a lexicallydriven 5-gram operation sequence model (OSM) (Durrani et al., 2013) with 4 count-based supportive features, sparse domain indicator, phrase length, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 100-best translation options, compact phrase table (JunczysDowmunt, 2012) minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the noreordering-over-punctuation heuristic (Koehn and Haddow, 2009). We optimize feature function weights with k-best MIRA (Cherry and Foster, 2012). We used POS and morphological tags as additional factors in phrase translation models (Koehn and Hoang, 2007) for the German-English language pairs. We also trained target sequence models on the in-domain subset of the parallel corpus using Kneser-Ney smoothed 7-gram models. We used syntactic preordering (Collins et al., 2005) and compound s"
W17-4724,N16-1046,0,0.0530386,"scores). Bold scores indicate best and submitted systems. 2 million sentences from news crawl 2016 monolingual corpus and 1.5 million sentences from preprocessed CWMT Chinese monolingual corpus from our syntax-based system run and backtranslated them with our trained base system. These back-translated pseudo-parallel data were then mixed with an equal amount of random samples from real parallel training data and used as the data for continued training. All the hyperparameters used for the continued training are exactly the same as those in the initial training stage. Following the effort of (Liu et al., 2016) and (Sennrich et al., 2016a), we also trained right-toleft (r2l) models with a random sample of 4 million sentence pairs for both translation directions of Chinese-English language pairs, in the hope that they could lead to better reordering on the target side. But they were not included in the final submission because they turned out to hurt the performance on development set. We conjecture that our r2l model is too weak compared to both base and back-trans models to yield good reordering hypotheses. We performed model averaging over the 4-best models for both base and back-trans systems as"
W17-4724,E99-1010,0,0.227759,"systems (Blunsom and Osborne, 2008)(Chiang et al., 2009). We used the same language model and tuning settings as the phrase-based systems. While BLEU score was used both for tuning and our development experiments, it is ambiguous when applied for Chinese outputs because Chinese does not have explicit word boundaries. For discriminative training and development tests, we evaluate the Chinese output against the automatically-segmented Chinese reference with multi-bleu.perl scripts in Moses (Koehn et al., 2007). Table 1: Tuning set sizes for phrase and syntaxbased system 500, and 2000 clusters (Och, 1999) using mkcls. In addition, we included a large language model based on the CommonCrawl monolingual data (Buck et al., 2014). The systems were tuned on a very large tuning set consisting of the test sets from 2008-2015, with a total of up to 21,730 sentences (see Table 1). We used newstest2016 as development test set. Significantly less tuning data was available for Finnish, Latvian, and Turkish. 2.2 Results Table 2 shows results for all language pairs, except for Chinese–English, for which we did not built phrase-based systems. Our phrase-based systems were clearly outperformed by NMT systems"
W17-4724,N07-1051,0,0.0711363,"1, 2017. 2017 Association for Computational Linguistics Language Pair German–English Czech–English Finnish–English Latvian–English Russian-English Turkish–English Chinese–English Sentences 21,243 21,730 2,870 984 11,824 1,001 1,000 For English data, we used the scripts from Moses (Koehn et al., 2007) to tokenize our data, while for Chinese data we carried out word segmentation with Stanford word segmenter (Chang et al., 2008). We also normalized all the Chinese punctuations to their English counterparts to avoid disagreement across sentences. We parsed the tokenized data with Berkeley Parser (Petrov and Klein, 2007) using the pre-trained grammar provided with the toolkit, followed by right binarization of the parse. Finally, truecasing was performed on all the English texts. Due to the lack of casing system, we did not perform truecasing for any Chinese texts. We performed word alignment with fast-align (Dyer et al., 2013) due to the huge scale of this year’s training data and grow-diag-final-and heuristic for alignment symmetrization. We used the GHKM rule extractor implemented in Moses to extract SCFG rules from the parallel corpus. We set the maximum number of nodes (except target words) in the rules"
W17-4724,E17-3017,0,0.0705342,"Missing"
W17-4724,W16-2323,0,0.530034,"of phrase-based systems, syntax-based systems and neural machine translation systems. In this paper we discuss features that we integrated into our system submissions. We also discuss lattice rescoring as a form of system combination of phrase-based and neural machine translation systems. The JHU phrase-based translation systems for our participation in the WMT 2017 shared translation task are based on the open source Moses toolkit (Koehn et al., 2007) and strong baselines of our submission last year (Ding et al., 2016). The JHU neural machine translation systems were built with the Nematus (Sennrich et al., 2016c) and Marian (Junczys-Dowmunt et al., 2016) toolkits. Our lattice rescoring experiments are also based on a combination of these three toolkits. 2 Configuration Phrase-Based Model Baselines Although the focus of research in machine translation has firmly moved onto neural machine translation, we still built traditional phrase-based statistical machine translation systems for all language pairs. These submissions also serve as a baseline 276 Proceedings of the Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 276–282 c Copenhagen, Denmark, September 711, 2017. 2017 A"
W18-1812,J84-3009,0,0.691446,"Missing"
W18-1812,P17-1080,0,0.0387798,"ous question that arises is: how well are state-of-the-art NMT systems doing at detecting linguistic features? This question is not new. Statistical machine translation (SMT) systems have achieved consistently high BLEU scores because they explicitly try to model features such as word or phrase alignments. For lower-resource languages, SMT systems have been shown to outperform NMT systems, but NMT systems overtake SMT once there is enough training data (Koehn and Knowles, 2017). Recent work has looked at the ability of neural systems to learn syntactic and morphological features. Speciﬁcally, Belinkov et al. (2017) showed that recurrent neural networks are able to achieve high accuracy on tasks such as predicting morphological or part of speech tags and Linzen et al. (2016) showed that RNNs follow similar patterns as humans with respect to sentences that are grammatical or ungrammatical in agreement structure. Additionally, speciﬁc RNN cells can be shown to have high correlation with features such as sentence length (Karpathy et al., 2016), part of speech (Ding et al., 2017), or whether or not the RNN has ﬁnished a relative clause (Linzen et al., 2016). Another linguistic issue NMT systems have to deal"
W18-1812,W13-0801,0,0.0205533,"like that of most people here, is ﬂawless. 2. speech: We were like, what do we do? 3. enjoy: Of the youngers, I really like the work of Leo Arill. 4. request: I would like to be a part of them, but I cannot. It is crucial for NMT systems to excel at this task in order to produce ﬂuent translations. If the NMT systems do not correctly translate ambiguous words, the resulting translations could be incomprehensible or misleading. Evaluation metrics have been proposed for assessing word sense disambiguation performance in the past. Lexical choice in MT systems has been evaluated using WSD tasks (Carpuat, 2013) or ﬁll-in-the-blank tasks where the blank represents an ambiguous word (Vickrey et al., 2005), to name a couple methods. These are based on the idea that the entire sentential context should disambiguate the intended word sense. If MT systems are paying attention to the full context, they should be able to succeed at this task. 2 Methodology We present experiments for examining the word sense disambiguation abilities of the attentionbased encoder-decoder model (Bahdanau et al., 2015). In this model, since the encoder computes both forward and backward hidden states after reading the input seq"
W18-1812,D07-1007,0,0.109352,"Missing"
W18-1812,P07-1005,0,0.186296,"he source language that might have multiple translations in the target language. When these words don’t differ orthographically, this task is known as word sense disambiguation. Typically, humans can successfully translate these kinds of words by looking at the contexts in which they appear. If NMT systems are able to successfully translate these words, it seems likely that they would have had to learn something about word sense disambiguation. There has been much research on improving machine translation performance by simultaneously improving word sense disambiguation (Vickrey et al., 2005; Chan et al., 2007; Carpuat Proceedings of AMTA 2018, vol. 1: MT Research Track Boston, March 17 - 21, 2018 |Page 125 and Wu, 2007) for SMT systems, showing that adding word sense disambiguation to a baseline SMT system greatly improves translation performance. For NMT, recent work points out that NMT systems are not very reliable at translating rare word senses, but that disambiguation performance can be improved by using sense embeddings either as additional input to the encoder or to extract more structured lexical chains from the training data (Rios et al., 2017), or by using context-aware embeddings (Liu e"
W18-1812,P17-1106,0,0.0188008,"es, 2017). Recent work has looked at the ability of neural systems to learn syntactic and morphological features. Speciﬁcally, Belinkov et al. (2017) showed that recurrent neural networks are able to achieve high accuracy on tasks such as predicting morphological or part of speech tags and Linzen et al. (2016) showed that RNNs follow similar patterns as humans with respect to sentences that are grammatical or ungrammatical in agreement structure. Additionally, speciﬁc RNN cells can be shown to have high correlation with features such as sentence length (Karpathy et al., 2016), part of speech (Ding et al., 2017), or whether or not the RNN has ﬁnished a relative clause (Linzen et al., 2016). Another linguistic issue NMT systems have to deal with is translating words in the source language that might have multiple translations in the target language. When these words don’t differ orthographically, this task is known as word sense disambiguation. Typically, humans can successfully translate these kinds of words by looking at the contexts in which they appear. If NMT systems are able to successfully translate these words, it seems likely that they would have had to learn something about word sense disamb"
W18-1812,P17-4012,0,0.0388228,"s, cx is the median of cluster x, σx is the average distance of all points in cluster x to cx , and d(ci , cj ) is the distance between the medians of clusters i and j. A lower DB Index corresponds to clusters that are dense and well-separated. We hope to ﬁnd that the Dunn Index increases and the DB Index decreases as we compute these scores for deeper layers of the NMT encoder. This would signify that our word senses were becoming more separated, which would likely correlate with disambiguation performance. 3 Experimental Design We trained all of our NMT systems using the OpenNMT-py toolkit (Klein et al., 2017), which trains an attentional encoder-decoder model with the attention from Luong et al. (2015). We tokenized, cleaned, and truecased our data using the standard tools from the Moses toolkit (Koehn et al., 2007). We did not use byte-pair encoding in order to more easily do manual annotation of the data later. We used the default parameters of the OpenNMT-py toolkit for training, with the exception of the number of encoder layers, which we varied from 1 to 4. For the current study, we extensively analyzed WSD performance on sentences containing four possible ambiguous words: right, like, last,"
W18-1812,2005.mtsummit-papers.11,1,0.177231,"Missing"
W18-1812,P07-2045,1,0.0141862,"hat are dense and well-separated. We hope to ﬁnd that the Dunn Index increases and the DB Index decreases as we compute these scores for deeper layers of the NMT encoder. This would signify that our word senses were becoming more separated, which would likely correlate with disambiguation performance. 3 Experimental Design We trained all of our NMT systems using the OpenNMT-py toolkit (Klein et al., 2017), which trains an attentional encoder-decoder model with the attention from Luong et al. (2015). We tokenized, cleaned, and truecased our data using the standard tools from the Moses toolkit (Koehn et al., 2007). We did not use byte-pair encoding in order to more easily do manual annotation of the data later. We used the default parameters of the OpenNMT-py toolkit for training, with the exception of the number of encoder layers, which we varied from 1 to 4. For the current study, we extensively analyzed WSD performance on sentences containing four possible ambiguous words: right, like, last, or case. We manually annotated English sentences with their most appropriate sense (these were our “gold” sense labels), and fed the (un-annotated) sentences into our English-French NMT system. After feeding in"
W18-1812,W17-3204,1,0.782668,"have to be able to deal with syntactic reordering, semantic relationships, co-reference, and discourse roles, among other phenomena. The obvious question that arises is: how well are state-of-the-art NMT systems doing at detecting linguistic features? This question is not new. Statistical machine translation (SMT) systems have achieved consistently high BLEU scores because they explicitly try to model features such as word or phrase alignments. For lower-resource languages, SMT systems have been shown to outperform NMT systems, but NMT systems overtake SMT once there is enough training data (Koehn and Knowles, 2017). Recent work has looked at the ability of neural systems to learn syntactic and morphological features. Speciﬁcally, Belinkov et al. (2017) showed that recurrent neural networks are able to achieve high accuracy on tasks such as predicting morphological or part of speech tags and Linzen et al. (2016) showed that RNNs follow similar patterns as humans with respect to sentences that are grammatical or ungrammatical in agreement structure. Additionally, speciﬁc RNN cells can be shown to have high correlation with features such as sentence length (Karpathy et al., 2016), part of speech (Ding et a"
W18-1812,Q16-1037,0,0.0420707,"tion (SMT) systems have achieved consistently high BLEU scores because they explicitly try to model features such as word or phrase alignments. For lower-resource languages, SMT systems have been shown to outperform NMT systems, but NMT systems overtake SMT once there is enough training data (Koehn and Knowles, 2017). Recent work has looked at the ability of neural systems to learn syntactic and morphological features. Speciﬁcally, Belinkov et al. (2017) showed that recurrent neural networks are able to achieve high accuracy on tasks such as predicting morphological or part of speech tags and Linzen et al. (2016) showed that RNNs follow similar patterns as humans with respect to sentences that are grammatical or ungrammatical in agreement structure. Additionally, speciﬁc RNN cells can be shown to have high correlation with features such as sentence length (Karpathy et al., 2016), part of speech (Ding et al., 2017), or whether or not the RNN has ﬁnished a relative clause (Linzen et al., 2016). Another linguistic issue NMT systems have to deal with is translating words in the source language that might have multiple translations in the target language. When these words don’t differ orthographically, thi"
W18-1812,D15-1166,0,0.0904593,"and d(ci , cj ) is the distance between the medians of clusters i and j. A lower DB Index corresponds to clusters that are dense and well-separated. We hope to ﬁnd that the Dunn Index increases and the DB Index decreases as we compute these scores for deeper layers of the NMT encoder. This would signify that our word senses were becoming more separated, which would likely correlate with disambiguation performance. 3 Experimental Design We trained all of our NMT systems using the OpenNMT-py toolkit (Klein et al., 2017), which trains an attentional encoder-decoder model with the attention from Luong et al. (2015). We tokenized, cleaned, and truecased our data using the standard tools from the Moses toolkit (Koehn et al., 2007). We did not use byte-pair encoding in order to more easily do manual annotation of the data later. We used the default parameters of the OpenNMT-py toolkit for training, with the exception of the number of encoder layers, which we varied from 1 to 4. For the current study, we extensively analyzed WSD performance on sentences containing four possible ambiguous words: right, like, last, or case. We manually annotated English sentences with their most appropriate sense (these were"
W18-1812,P02-1040,0,0.100394,"Missing"
W18-1812,W17-4702,0,0.0970883,"d sense disambiguation (Vickrey et al., 2005; Chan et al., 2007; Carpuat Proceedings of AMTA 2018, vol. 1: MT Research Track Boston, March 17 - 21, 2018 |Page 125 and Wu, 2007) for SMT systems, showing that adding word sense disambiguation to a baseline SMT system greatly improves translation performance. For NMT, recent work points out that NMT systems are not very reliable at translating rare word senses, but that disambiguation performance can be improved by using sense embeddings either as additional input to the encoder or to extract more structured lexical chains from the training data (Rios et al., 2017), or by using context-aware embeddings (Liu et al., 2017). To the best of our knowledge, no work has yet attempted to examine the hidden activations of an NMT system to see whether it is able to disambiguate word senses. In this paper, we present means for evaluating the word sense disambiguation performance of NMT systems. Speciﬁcally, we visualize the hidden activations of an NMT encoder to see whether it is able to disambiguate word senses at deeper layers. We also present metrics that represent how well-disambiguated the senses are, with the hope that these metrics can be used to evaluate"
W18-1812,H05-1097,0,0.806148,"translating words in the source language that might have multiple translations in the target language. When these words don’t differ orthographically, this task is known as word sense disambiguation. Typically, humans can successfully translate these kinds of words by looking at the contexts in which they appear. If NMT systems are able to successfully translate these words, it seems likely that they would have had to learn something about word sense disambiguation. There has been much research on improving machine translation performance by simultaneously improving word sense disambiguation (Vickrey et al., 2005; Chan et al., 2007; Carpuat Proceedings of AMTA 2018, vol. 1: MT Research Track Boston, March 17 - 21, 2018 |Page 125 and Wu, 2007) for SMT systems, showing that adding word sense disambiguation to a baseline SMT system greatly improves translation performance. For NMT, recent work points out that NMT systems are not very reliable at translating rare word senses, but that disambiguation performance can be improved by using sense embeddings either as additional input to the encoder or to extract more structured lexical chains from the training data (Rios et al., 2017), or by using context-awar"
W18-2102,W12-3102,1,0.833937,"Missing"
W18-2102,W03-0413,0,0.164086,", the system instead feeds the translator’s token(s) into the model, then continues producing the translation token by token. Knowles and Koehn (2016) note that the neural interactive translation prediction system recovers well from failure (predicting an incorrect token) when the correct token’s model score is also (relatively) high. This suggests the feasibility of using features like the model score (which is already generated by the system) to predict when the system should be more or less confident in the quality of its predictions. Early work on word-level confidence estimation, such as Gandrabur and Foster (2003), focused on estimating the system’s confidence in translations in a similar interactive translation prediction setting (using a maxent MT model). Gonz´alez-Rubio et al. (2010b) explored how confidence information might be able to be used in an interactive machine translation setting to lessen human effort, and Gonz´alez-Rubio et al. (2010a) suggested using confidence measures to 3 As described in detail in Wuebker et al. (2016) and Knowles and Koehn (2016). determine which sentences need human intervention in the form of interactive translation prediction and which are likely to be of high en"
W18-2102,P10-2032,0,0.0381476,"Missing"
W18-2102,2010.eamt-1.18,0,0.145559,"Missing"
W18-2102,E17-3017,0,0.043564,"Missing"
W18-2102,W16-2323,0,0.02296,"e next token (potentially conditioning on the previous tokens). Additionally, in the standard word-level quality estimation task, it is possible to extract features from both the full source sentence and the full machine translation output. In the interactive translation prediction setting as we have described it, the target output is produced one word at a time, through interaction with the user, meaning that target side features can only be extracted from the prefix produced so far. 3 3.1 Experiments & Results Data and MT Systems We use University of Edinburgh’s neural models from WMT 2016 (Sennrich et al., 2016) for the following language pairs and directions: Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018 |Page 36 Input: An dieser Stelle sollte ich zugeben, dass ich kein Experte, sondern nur ein erdgebundener Enthusiast bin. Label BAD OK OK BAD OK OK OK BAD OK OK BAD BAD BAD BAD OK OK Reference here I should confess that I am no expert , just an earth@@ bound enthusiast . Suggestion at I should admit that I am not expert , but a Earth ed enthusiast . Figure 2: An example sentence demonstrating how the labels are obtained. A “BAD”"
W18-2102,P15-4020,0,0.180911,"form of interactive translation prediction and which are likely to be of high enough quality for the MT output to be used without editing. Both of these focus on interactive machine translation using statistical machine translation. Today, the task of word-level quality estimation typically focuses on assigning “OK”/“BAD” labels to individual tokens in a full sentence translation (Bojar et al., 2017). This task has been explored in-depth through the shared task on Quality Estimation at WMT, which was initially introduced in 2012 (Callison-Burch et al., 2012). The open-source tool Q U E ST ++ (Specia et al., 2015) provides an implementation of word-, sentence-, and document-level quality estimation, using an extensive set of features that have been found to be useful for the task. The vital difference between the word-level quality estimation task and confidence estimation for interactive translation prediction is that each human interaction in the interactive translation prediction setting provides a gold-standard “OK”/“BAD” label for a token, such that the full prefix of the sentence is labeled, and the task is now to predict the quality of the next token (potentially conditioning on the previous tok"
W18-2102,2005.eamt-1.35,0,0.046433,"sequence of tokens generated so far. model; in particular we did not optimize specifically for either of the metrics, nor did we make significant efforts to elegantly handle the label imbalance in labels. Attention to both of these areas could easily result in improvement. While we evaluated with F1 -BAD and F1 -mult, it may also be useful to evaluate the system in terms of the computational costs saved by holding off on making full sentence predictions following low-confidence tokens. This, or a usercentric metric (like those described in Gandrabur and Foster (2003)) could also be valuable. Ueffing and Ney (2005) propose an evaluation metric called prediction F-measure, which incorporates the keystroke ratio that models human effort by the number of keystroke actions needed to complete translations. Additionally, there is work to be done on the user interface side to determine how best to use confidence estimation for interactive translation prediction. What is the best way to communicate the confidence estimate to the user? Is it sufficient to use a visual representation (color, shading), or would it be preferable to show multiple suggestions or (no suggestions) when the system is not confident? Answ"
W18-2102,P16-1007,0,0.153497,"s translation suggestions (“auto-complete” functionality) for human translators. These translation suggestions may be rejected by the translator in predictable ways; being able to estimate confidence in the quality of translation suggestions could be useful in providing additional information for users of the system. We show that a very small set of features (which are already generated as byproducts of the process of translation prediction) can be used in a simple model to estimate confidence for interactive translation prediction. 1 Introduction In neural interactive translation prediction (Wuebker et al., 2016; Knowles and Koehn, 2016), a human translator interacts with machine translation output by accepting or rejecting suggestions as they type a translation from beginning to end. By accepting a system suggestion, the translator implicitly provides an “OK” quality label for that token. Similarly, by rejecting a suggestion (and providing a correction), they implicitly provide a “BAD” quality label for the system’s suggestion. The system’s suggestions may be wrong (“BAD”) in predictable ways. For example, if one suggestion is incorrect, the subsequent suggestion may then be more likely to be incorr"
W18-2102,2016.amta-researchers.9,1,0.727664,"ons (“auto-complete” functionality) for human translators. These translation suggestions may be rejected by the translator in predictable ways; being able to estimate confidence in the quality of translation suggestions could be useful in providing additional information for users of the system. We show that a very small set of features (which are already generated as byproducts of the process of translation prediction) can be used in a simple model to estimate confidence for interactive translation prediction. 1 Introduction In neural interactive translation prediction (Wuebker et al., 2016; Knowles and Koehn, 2016), a human translator interacts with machine translation output by accepting or rejecting suggestions as they type a translation from beginning to end. By accepting a system suggestion, the translator implicitly provides an “OK” quality label for that token. Similarly, by rejecting a suggestion (and providing a correction), they implicitly provide a “BAD” quality label for the system’s suggestion. The system’s suggestions may be wrong (“BAD”) in predictable ways. For example, if one suggestion is incorrect, the subsequent suggestion may then be more likely to be incorrect. We seek to show that"
W18-2102,P16-2095,0,0.0203013,"ecause it does match. Table 1 shows baseline word prediction accuracy scores on the WMT 2017 test data. Word prediction accuracy (WPA) is calculated as the percentage of the time that the system correctly predicts the next token of the sentence. The WPA is the percentage of the data that has the “OK” label. The slightly lower WPA scores for the Czech language tasks are consistent with the expectation that Czech-English translation is more difficult than German-English. We show the BLEU scores reported on standard decoding with beam size of 1 on WMT 2017 data in Table 1.5 3.2 Metrics Following Logacheva et al. (2016), we report scores for F1 -BAD and F1 -mult (the product of F1 -BAD and F1 -OK scores). F1 -BAD is of interest because we seek in particular to be able to label incorrect predictions (of which there are fewer than correct predictions). F1 -mult has been shown to be more robust to pessimistic classifiers (those which label most tokens as “BAD”). 3.3 Features Here we describe the small set of simple features we explored, all of which are generated as byproducts of the neural interactive translation prediction system’s computations. In Table 2 we show baseline results of using simple heuristics ("
W18-2108,2015.mtsummit-papers.10,0,0.0935908,"Missing"
W18-2108,W15-4904,0,0.079775,"Missing"
W18-2108,P10-1064,0,0.0745354,"Missing"
W18-2108,W17-4775,0,0.187593,"address the question of how NMT systems, which are particularly sensitive to changes in domain or style (Koehn and Knowles, 2017) will perform when used to translate sub-segments rather than full sentences. Neural MT systems have recently produced state-ofthe-art performance across a number of language pairs (Bojar et al., 2017). While NMT has been applied to other CAT applications, namely interactive translation prediction, (Knowles and Koehn, 2016; Wuebker et al., 2016) and neural approaches have been used for automatic post-editing (Pal et al., 2016; Junczys-Dowmunt and Grundkiewicz, 2016; Hokamp, 2017), this is the first work we are aware of that uses NMT for FMR. 3 Black-Box MT for FMR Here we provide an overview of an algorithm for using black-box MT for FMR. For full details, see Ortega et al. (2016) (Sections 2 and 3), whose algorithm we follow. Black-box approaches allow one system to be used for many tasks, rather than requiring speciallytailored MT systems for every task. Given a new source-language sentence s0 to translate, the FMR system selects (by fuzzy-match score, or FMS) the source-target pair of segments (s, t) from the TM that most closely matches s0 . The FMS takes on value"
W18-2108,W16-2378,0,0.0622283,"n the two) for the task of FMR. We also address the question of how NMT systems, which are particularly sensitive to changes in domain or style (Koehn and Knowles, 2017) will perform when used to translate sub-segments rather than full sentences. Neural MT systems have recently produced state-ofthe-art performance across a number of language pairs (Bojar et al., 2017). While NMT has been applied to other CAT applications, namely interactive translation prediction, (Knowles and Koehn, 2016; Wuebker et al., 2016) and neural approaches have been used for automatic post-editing (Pal et al., 2016; Junczys-Dowmunt and Grundkiewicz, 2016; Hokamp, 2017), this is the first work we are aware of that uses NMT for FMR. 3 Black-Box MT for FMR Here we provide an overview of an algorithm for using black-box MT for FMR. For full details, see Ortega et al. (2016) (Sections 2 and 3), whose algorithm we follow. Black-box approaches allow one system to be used for many tasks, rather than requiring speciallytailored MT systems for every task. Given a new source-language sentence s0 to translate, the FMR system selects (by fuzzy-match score, or FMS) the source-target pair of segments (s, t) from the TM that most closely matches s0 . The FMS"
W18-2108,2016.amta-researchers.9,1,0.801427,"ill compare to neural MT systems (which may provide greater fluency) or phrase-based statistical MT systems (which may fall between the two) for the task of FMR. We also address the question of how NMT systems, which are particularly sensitive to changes in domain or style (Koehn and Knowles, 2017) will perform when used to translate sub-segments rather than full sentences. Neural MT systems have recently produced state-ofthe-art performance across a number of language pairs (Bojar et al., 2017). While NMT has been applied to other CAT applications, namely interactive translation prediction, (Knowles and Koehn, 2016; Wuebker et al., 2016) and neural approaches have been used for automatic post-editing (Pal et al., 2016; Junczys-Dowmunt and Grundkiewicz, 2016; Hokamp, 2017), this is the first work we are aware of that uses NMT for FMR. 3 Black-Box MT for FMR Here we provide an overview of an algorithm for using black-box MT for FMR. For full details, see Ortega et al. (2016) (Sections 2 and 3), whose algorithm we follow. Black-box approaches allow one system to be used for many tasks, rather than requiring speciallytailored MT systems for every task. Given a new source-language sentence s0 to translate, t"
W18-2108,2005.mtsummit-papers.11,1,0.114985,"Missing"
W18-2108,P09-5002,1,0.784204,"Missing"
W18-2108,P07-2045,1,0.0121511,"Missing"
W18-2108,W17-3204,1,0.81397,"o be truly useful in a live system, FMR will require some form of quality estimation in order to select the best repaired segment. Research in that area is ongoing. Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018 |Page 249 In the trade-off between adequacy (translations with the same meaning as the source) and fluency (translations that sound fluid or natural), neural machine translation systems, tend towards greater fluency, while sometimes producing fluent-sounding but semantically inappropriate output (Bojar et al., 2016; Koehn and Knowles, 2017; Toral and S´anchez-Cartagena, 2017). In the FMR application, the full segment from the translation memory may already provide the (fluent) backbone for the translation, while only containing a few subsegment mismatches (such as numbers, names, noun phrases, and so on). This differs from automatic post-editing, where there may be structural issues to repair as a result of errors in the machine translation output. All of this naturally raises the question of how rule-based MT (which may provide greater adequacy for individual subsegments) will compare to neural MT systems (which may provide gr"
W18-2108,2010.jec-1.4,1,0.908013,"4, we describe the algorithm used in FMR and the MT systems we tested as sources of bilingual information, respectively. Then, in Section 5 we show that while phrase-based statistical machine translation (henceforth SMT) and neural MT (henceforth NMT) systems both outperform a rule-based (RB) system, these two types of systems perform in markedly different ways as black-box input to the FMR system. 2 Related Work Attempting to “repair” and propose translations that are closer to the desired translation is a common approach to combining TMs and MT. Simard and Isabelle (2009); He et al. (2010); Koehn and Senellart (2010) all combine TMs and statistical MT in ways that require either a glass-box or explicitly modified MT. Our work focuses on ways of applying any MT system to the task of FMR, without requiring knowledge of the system’s inner workings. We use the approach from Ortega et al. (2016) (described in more detail in Section 3). That particular fuzzy-match repair system allows the CAT tool to use any source of bilingual information, but in their publications, they focus only on Apertium (Forcada et al., 2011) as the source of bilingual information. Their work, as well as ours in this paper, depends on a"
W18-2108,kranias-samiotou-2004-automatic,0,0.773783,"und in the TM and its corresponding validated translation segment t. The translator can modify mismatched sub-segments2 of t to produce a correct translation of the new segment s0 , rather than translating it from scratch. The goal of FMR is to use a source of bilingual information (for example, a dictionary, MT system, phrase table, etc.) to translate the mismatched sub-segments and correctly combine them with the target segment prior to presenting it to the translator. Delivering a correctly repaired segment should save the human translator time, by decreasing 1 Or fuzzy-match post-editing (Kranias and Samiotou, 2004). The use of the term “fuzzy-match” references the fuzzy-match score used to find similar source sentences. 2 Throughout this work, we refer to a complete line of text as a segment (rather than a sentence, as a number of the lines of text in the data we use do not constitute full grammatical sentences, but may include things like titles). Sequences of one or more tokens within the segment are sub-segments. Philipp Koehn Department of Computer Science Johns Hopkins University phi@jhu.edu the number of changes they need to make in order to complete the translation. A “perfectly” repaired segment"
W18-2108,2015.iwslt-evaluation.11,0,0.125278,"Missing"
W18-2108,2014.amta-researchers.4,1,0.875908,"Missing"
W18-2108,2016.amta-researchers.3,1,0.744414,"Missing"
W18-2108,P16-1007,0,0.0310898,"systems (which may provide greater fluency) or phrase-based statistical MT systems (which may fall between the two) for the task of FMR. We also address the question of how NMT systems, which are particularly sensitive to changes in domain or style (Koehn and Knowles, 2017) will perform when used to translate sub-segments rather than full sentences. Neural MT systems have recently produced state-ofthe-art performance across a number of language pairs (Bojar et al., 2017). While NMT has been applied to other CAT applications, namely interactive translation prediction, (Knowles and Koehn, 2016; Wuebker et al., 2016) and neural approaches have been used for automatic post-editing (Pal et al., 2016; Junczys-Dowmunt and Grundkiewicz, 2016; Hokamp, 2017), this is the first work we are aware of that uses NMT for FMR. 3 Black-Box MT for FMR Here we provide an overview of an algorithm for using black-box MT for FMR. For full details, see Ortega et al. (2016) (Sections 2 and 3), whose algorithm we follow. Black-box approaches allow one system to be used for many tasks, rather than requiring speciallytailored MT systems for every task. Given a new source-language sentence s0 to translate, the FMR system selects ("
W18-2108,P16-2046,0,0.226505,"Missing"
W18-2108,E17-3017,0,0.0657081,"Missing"
W18-2108,P16-1162,0,0.060387,"Missing"
W18-2108,2009.mtsummit-papers.14,0,0.701185,"discussion of related work. In Sections 3 and 4, we describe the algorithm used in FMR and the MT systems we tested as sources of bilingual information, respectively. Then, in Section 5 we show that while phrase-based statistical machine translation (henceforth SMT) and neural MT (henceforth NMT) systems both outperform a rule-based (RB) system, these two types of systems perform in markedly different ways as black-box input to the FMR system. 2 Related Work Attempting to “repair” and propose translations that are closer to the desired translation is a common approach to combining TMs and MT. Simard and Isabelle (2009); He et al. (2010); Koehn and Senellart (2010) all combine TMs and statistical MT in ways that require either a glass-box or explicitly modified MT. Our work focuses on ways of applying any MT system to the task of FMR, without requiring knowledge of the system’s inner workings. We use the approach from Ortega et al. (2016) (described in more detail in Section 3). That particular fuzzy-match repair system allows the CAT tool to use any source of bilingual information, but in their publications, they focus only on Apertium (Forcada et al., 2011) as the source of bilingual information. Their wor"
W18-2108,steinberger-etal-2012-dgt,0,0.0606868,"Missing"
W18-2108,E17-1100,0,0.0614479,"Missing"
W18-2108,W18-2100,0,0.189337,"Missing"
W18-2703,W11-2138,0,0.144125,"is used to build better translation systems in forward and backward directions, which in turn is used to reback-translate monolingual data. This process can be “iterated” several times. This is a form of co-training (Blum and Mitchell, 1998) where the two models over both translation directions can be used to train one another. We show that iterative back-translation leads to improved results over simple back-translation, under both high and 2 Related Work The idea of back-translation dates back at least to statistical machine translation, where it has been used for semi-supervised learning (Bojar and Tamchyna, 2011), or self-training (Goutte et al., 2009, ch.12, p.237). In modern NMT research, Sennrich et al. (2017) reported significant gains on the WMT and IWSLT shared tasks. They showed that even simply duplicating the monolingual target data into the source was sufficient to realise some benefits. Currey et al. (2017) reported similar findings for low resource conditions, showing that even poor translations can be beneficial. Gwinnup et al. (2017) mention in their system description iteratively applying back-translation, but did not report successful experiments. A more refined idea of back-translatio"
W18-2703,W16-2323,0,0.520255,"uding the best reported BLEU scores for the WMT 2017 German↔English tasks. 1 real+synthetic reverse system final system synthetic Figure 1: Creating a synthetic parallel corpus through back-translation. First, a system in the reverse direction is trained and then used to translate monolingual data from the target side backward into the source side, to be used in the final system. Introduction low resource conditions, improving over the state of the art. The exploitation of monolingual training data for neural machine translation is an open challenge. One successful method is back-translation (Sennrich et al., 2016b), whereby an NMT system is trained in the reverse translation direction (targetto-source), and is then used to translate target-side monolingual data back into the source language (in the backward direction, hence the name backtranslation). The resulting sentence pairs constitute a synthetic parallel corpus that can be added to the existing training data to learn a source-totarget model. Figure 1 illustrates this idea. In this paper, we show that the quality of backtranslation matters and propose iterative backtranslation, where back-translated data is used to build better translation system"
W18-2703,P16-1009,0,0.485876,"uding the best reported BLEU scores for the WMT 2017 German↔English tasks. 1 real+synthetic reverse system final system synthetic Figure 1: Creating a synthetic parallel corpus through back-translation. First, a system in the reverse direction is trained and then used to translate monolingual data from the target side backward into the source side, to be used in the final system. Introduction low resource conditions, improving over the state of the art. The exploitation of monolingual training data for neural machine translation is an open challenge. One successful method is back-translation (Sennrich et al., 2016b), whereby an NMT system is trained in the reverse translation direction (targetto-source), and is then used to translate target-side monolingual data back into the source language (in the backward direction, hence the name backtranslation). The resulting sentence pairs constitute a synthetic parallel corpus that can be added to the existing training data to learn a source-totarget model. Figure 1 illustrates this idea. In this paper, we show that the quality of backtranslation matters and propose iterative backtranslation, where back-translated data is used to build better translation system"
W18-2703,E17-3017,0,0.0244524,"gs) with droption that the back-translation approach still imout of 0.2 for the RNN parameters, and 0.1 otherproves the translation accuracy in all language wise. Training is smoothed with moving average. pairs with a low-resource setting. In the English– It takes about 2–4 days. French experiments, large improvements over the The deep system uses matches the setup of baseline are observed in both directions, with +3.5 Edinburgh’s WMT 2017 system (Sennrich et al., 3 The difference here is on the NMT toolkit used — we 2017). It uses 4 encoder and 4 decoder layers opted to use Amazon’s Sockeye (Hieber et al., 2017). We (Marian setting best-deep) with LSTM cells. used Sockeye’s default configuration with dropout 0.5. 21 Setting NMT baseline back-translation back-translation iterative+1 back-translation iterative+2 back-translation (w/ Moses) French–English 100K 1M English–French 100K 1M 16.7 22.1 22.5 22.6 23.7 18.0 21.5 22.7 22.6 23.5 24.7 27.8 27.9 25.6 27.0 27.3 Farsi–English 100K English-Farsi 100K 21.7 22.1 22.7 22.6 21.8 16.4 16.7 17.1 17.2 16.8 Table 4: Low Resource setting: Impact of the quality of the back-translation systems on the benefit of the synthetic parallel for the final system in a low"
W18-2703,P16-1162,0,0.808379,"uding the best reported BLEU scores for the WMT 2017 German↔English tasks. 1 real+synthetic reverse system final system synthetic Figure 1: Creating a synthetic parallel corpus through back-translation. First, a system in the reverse direction is trained and then used to translate monolingual data from the target side backward into the source side, to be used in the final system. Introduction low resource conditions, improving over the state of the art. The exploitation of monolingual training data for neural machine translation is an open challenge. One successful method is back-translation (Sennrich et al., 2016b), whereby an NMT system is trained in the reverse translation direction (targetto-source), and is then used to translate target-side monolingual data back into the source language (in the backward direction, hence the name backtranslation). The resulting sentence pairs constitute a synthetic parallel corpus that can be added to the existing training data to learn a source-totarget model. Figure 1 illustrates this idea. In this paper, we show that the quality of backtranslation matters and propose iterative backtranslation, where back-translated data is used to build better translation system"
W18-2703,U16-1001,1,0.520715,"nt of parallel data to reach reasonable performance (Koehn and Knowles, 2017). In a lowthe parallel data and the synthetic data generresource setting, only small amount of parallel ated by the base translation system. For better data exist. Previous work has attempted to inperformance, we train a deep model with 8corporate prior or external knowledge to compencheckpoint ensembling; again we use a beam sate for the lack of parallel data, e.g. injecting insize of 2. ductive bias via linguistic constraints (Cohn et al., The final back-translation systems were trained 2016) or linguistic factors (Hoang et al., 2016). using several different systems: a shallow arHowever, it is much cheaper and easier to obtain chitecture, a deep architecture, and an ensemmonolingual data in either the source or target lanble system of 4 independent training runs. guage. An interesting question is whether the (iterAcross the board, the final systems with reative) back-translation can compensate for the lack back-translation outperform the final systems with of parallel data in such low-resource settings. simple back-translation, by a margin of 0.5–1.1 BLEU. To explore this question, we conducted experiments on two datasets"
W18-2703,P18-4020,0,0.0750003,"Missing"
W18-2703,W17-3204,1,0.74613,"ely) in Table 3. Best WMT 2017 28.3 For all experiments, the true-casing model and Table 3: WMT News Translation Task German– the list of BPE operations is left constant. Both English, comparing the quality of different backwere learned from the original parallel training translation systems with different final system arcorpus. chitectures. *Note that the quality for the backtranslation system (Back) is measured in the op4.2 Experiments on Low Resource Scenario posite language direction. NMT is a data-hungry approach, requiring a large amount of parallel data to reach reasonable performance (Koehn and Knowles, 2017). In a lowthe parallel data and the synthetic data generresource setting, only small amount of parallel ated by the base translation system. For better data exist. Previous work has attempted to inperformance, we train a deep model with 8corporate prior or external knowledge to compencheckpoint ensembling; again we use a beam sate for the lack of parallel data, e.g. injecting insize of 2. ductive bias via linguistic constraints (Cohn et al., The final back-translation systems were trained 2016) or linguistic factors (Hoang et al., 2016). using several different systems: a shallow arHowever, it"
W18-2703,W17-4710,0,0.0258173,"ons. Under high-resource conditions, we improve the state of the art with re-back-translation. Under low-resource conditions, we demonstrate Experiments on High Resource Scenario In §3 we demonstrated that the quality of the backtranslation system has significant impact on the effectiveness of the back-translation approach under high-resource data conditions such as WMT 2017 German–English. Here we ask: how much additional benefit can be realised for repeating this process? Also, do the gains for state-of-the-art systems that use deeper models, i.e., more layers in encoder and decoder (Miceli Barone et al., 2017) still apply in this setting? We evaluate on German–English and English– German, under the same data conditions as in Section 3. We experiment with both shallow and deep stacked-layer encoder/decoder architectures. The base translation system is trained on the parallel data only. We train a shallow system using 4-checkpoint ensembling (Chen et al., 2017). The system is used to translate the monolingual data using a beam size of 2. The first back-translation system is trained on 20 German–English Back* Shallow Deep Ensemble back-translation 23.7 32.5 35.0 35.6 re-back-translation 27.9 33.6 36.1"
W18-2705,D16-1139,0,0.132757,"Missing"
W18-2705,2012.eamt-1.60,0,0.0122739,"corpus), and the EU Press Releases. We use newstest2015 as the out-of-domain development set and newstest2016 as the outof-domain test set. These consist of professionally translated news articles released by the WMT shared task. We perform adaptation into two different domains: EMEA (descriptions of medicines) and TED Talks (rehearsed presentations). For EMEA, we use the data split from (Koehn and Knowles, 2017),7 which was extracted from from OPUS (Tiedemann, 2009, 2012).8 For TED, we use the data split from the Multitarget TED Talks Task (MTTT) (Duh, 2018).9 which was extracted from WIT3 (Cettolo et al., 2012).10 Tables 1–3 give the number of words and sentences of each of the corpora in the train, dev, and test sets, respectively. In addition to experiments on the full training sets, we also conduct experiments adapting to each given domain using only the first 2,000 sentences of each in-domain training set to simulate adaptation into a low-resource domain. For all experiments we translate from English to German as well as from German to English. Table 3: Tokenized test set sizes. 3.2 NMT settings Our neural machine translation systems are trained using a modified version of OpenNMT-py (Klein et a"
W18-2705,P17-2061,0,0.0679878,"Missing"
W18-2705,2005.mtsummit-papers.11,1,0.0896777,"domain model (which replaces the parent) on in-domain training data that the outof-domain model was not trained on. 3 Table 1: Tokenized training set sizes. corpus de words en words sentences EMEA 26479 28838 2000 TED 37509 38717 1958 newstest15 44869 47569 2169 Experiments 3.1 Table 2: Tokenized development set sizes. Data corpus de words en words sentences EMEA 31737 33884 2000 TED 35516 36857 1982 newstest16 64379 65647 2999 For our large, out-of-domain corpus we utilize bitext from WMT2017 (Bojar et al., 2017),4 which contains data from several sources: Europarl parliamentary proceedings (Koehn, 2005),5 News Commentary (political and economic news commentary),6 Common Crawl (web-crawled parallel corpus), and the EU Press Releases. We use newstest2015 as the out-of-domain development set and newstest2016 as the outof-domain test set. These consist of professionally translated news articles released by the WMT shared task. We perform adaptation into two different domains: EMEA (descriptions of medicines) and TED Talks (rehearsed presentations). For EMEA, we use the data split from (Koehn and Knowles, 2017),7 which was extracted from from OPUS (Tiedemann, 2009, 2012).8 For TED, we use the dat"
W18-2705,W17-4123,0,0.0210706,"old-standard distribution 1{yi = v} (which is simply a one-hot vector that indicates if the correct word was produced), and the model’s distribution p(yi = v |x; θ; yj&lt;i ). 2.2 Continued Training L(θ) = (1 − α) LNLL (θ) + α Lreg (θ) Continued training is a simple yet effective technique for domain adaptation. It consists of three steps: (3) The added regularization term is formulated in the spirit of knowledge distillation (Kim and Rush, 2 For a detailed explanation of attention based NMT see Bahdanau et al. (2015) (the original paper), or for a gentle introduction see the textbook chapter by Koehn (2017). 3 The out-of-domain model is fixed while training the indomain model. 37 corpus de words en words sentences EMEA 13,572,552 14,774,808 1,104,752 TED 2,966,837 3,161,544 152,609 WMT 139,449,418 146,569,151 5,919,142 2016), where a student model is trained to match the output distribution of a parent model. In wordlevel knowledge distillation, the student model’s output distribution is trained on the same data that the parent model was trained. In contrast, our domain specific model (which replaces the student) is trained with a loss term that encourages it to match the out-of-domain model (wh"
W18-2705,W17-3204,1,0.93447,"distribution and that of the out-of-domain model.1 This prevents the distribution of words produced from differing too much from the original distribution. Introduction Neural Machine Translation (NMT) (Bahdanau et al., 2015) is currently the state-of-the art paradigm for machine translation. It dominated the recent WMT shared task (Bojar et al., 2017), and is used commercially (Wu et al., 2016; Crego et al., 2016; Junczys-Dowmunt et al., 2016). Despite their successes, NMT systems require a large amount of training data and do not perform well in low resource and domain adaptation scenarios (Koehn and Knowles, 2017). Domain adaptation is required when there is sufficient data to train an NMT system in the desired language pair, but the domain (the topic, genre, style or level of We show that this method improves upon standard continued training by as much as 1.5 BLEU. 1 The code is available: github.com/khayrallah/OpenNMT-py-reg 36 Proceedings of the 2nd Workshop on Neural Machine Translation and Generation, pages 36–44 c Melbourne, Australia, July 20, 2018. 2018 Association for Computational Linguistics 2 Method 1. Train a model until convergence on large outof-domain bitext using LNLL as the training o"
W18-2705,2015.iwslt-evaluation.11,0,0.137405,"out-of-domain model to prevent the model’s output from differing too much from the original out-ofdomain model. We perform experiments on EMEA (descriptions of medicines) and TED (rehearsed presentations), initialized from a general domain (WMT) model. Our method shows improvements over standard continued training by up to 1.5 BLEU. 1 In this paper, we focus on the supervised domain adaptation problem, where in addition to a large out-of-domain corpus, we also have a smaller in-domain parallel corpus available for training. A technique commonly applied in this situation is continued training (Luong and Manning, 2015), where a model is first trained on the out-of-domain corpus, and then that model is used to initialize a new model that is trained on the in-domain corpus. This simple method leads to empirical improvements on in-domain test sets. However, we hypothesize that some knowledge available in the out-of-domain data—which is not observed in the smaller in-domain data but would be useful at test time—is being forgotten during continued training, due to overfitting. (This phenomena can be viewed as a version of catastrophic forgetting (Goodfellow et al., 2013)). For this reason, we add an additional t"
W18-2705,D17-1156,0,0.175941,"Missing"
W18-2705,P16-1162,0,0.291622,"Missing"
W18-2705,tiedemann-2012-parallel,0,0.0777348,"Missing"
W18-2708,W17-4713,0,0.104345,"roaches. These approaches typically use larger amounts of in-domain data to do adaptation, far greater than the amounts that might be available in a CAT setting. Cettolo et al. (2014) proposed adapting statistical phrase-based machine translation systems to particular projects (multiple documents) and Peris and Casacuberta (2018) propose adapting neural machine translation systems in CAT settings. Neither explore very small amounts of data at the sub-document level. Two recent papers have tried a domain adaptation approach using very small data sizes, ranging from 1 sentence to 128 sentences (Farajian et al., 2017; Li et al., 2016). They adapt models for new sentences by training on sentence pairs from a training corpus (or translation memory) that are similar to the new sentence, which means they cannot adapt to novel vocabulary. of the document. See Algorithm 1 for details. Such an approach could be applied in a computer aided translation tool, which would allow the machine translation system to adapt to translator corrections as produced by post-editing or through an interactive translation prediction interface (Wuebker et al., 2016; Knowles and Koehn, 2016). Single-sentence adaptation allows the mo"
W18-2708,D16-1162,0,0.103803,"Missing"
W18-2708,H92-1045,0,0.16559,"n The challenge of adapting to a new domain is a well-studied problem in machine translation research. But even within a particular domain, each new document may pose unique challenges due to novelty of vocabulary, word senses, style, and more.1 It stands to reason that fine-grained adaptation using sentences from within a document (for example, as it is being translated by ∗ 2 These authors contributed equally to this work. Carpuat et al. (2012) decompose errors into seen, sense, score, and search; the first two are most relevant to our work. This work follows from “one sense per discourse” (Gale et al., 1992), which found that the vast majority of polysemous words share only one sense within a given document. 1 64 Proceedings of the 2nd Workshop on Neural Machine Translation and Generation, pages 64–73 c Melbourne, Australia, July 20, 2018. 2018 Association for Computational Linguistics Source Ambirix (Ambi/rix) Prepandemic (Prep/an/demic) Cataplexy (Cat/ap/lex/y) hormone-dependent (hormon/e-/dependent) Reference Ambirix (Ambi/rix) Pr¨apandemischer (Pr¨a/pandem/ischer) Kataplexie (Kat/ap/lex/ie) hormonabh¨angig (hormon/abh¨angig) Baseline MT Output Hampshire, Glaurix, Tandemrix, ... Proteasehemmer"
W18-2708,W09-2404,0,0.0535347,"Missing"
W18-2708,P16-1154,0,0.0841559,"Missing"
W18-2708,P17-1141,0,0.0571615,"Missing"
W18-2708,W18-2709,1,0.889012,"Missing"
W18-2708,P17-2061,0,0.039501,"nity and is relevant both to the translation of new words and to more general improvements in translation quality. Recent work (Freitag and Al-Onaizan, 2016; Luong and Manning, 2015) has proposed to do domain adaptation for NMT systems by training a general system then fine-tuning by continuing to train using only in-domain data (typically a smaller dataset). Wang et al. (2017) present a similar approach where they weight each source-target sentence pair during training based on scores from in-domain and out-of-domain language models. Kobus et al. (2017) use special tokens to indicate domain. Chu et al. (2017) compare the approaches. These approaches typically use larger amounts of in-domain data to do adaptation, far greater than the amounts that might be available in a CAT setting. Cettolo et al. (2014) proposed adapting statistical phrase-based machine translation systems to particular projects (multiple documents) and Peris and Casacuberta (2018) propose adapting neural machine translation systems in CAT settings. Neither explore very small amounts of data at the sub-document level. Two recent papers have tried a domain adaptation approach using very small data sizes, ranging from 1 sentence to"
W18-2708,W17-4715,0,0.0737914,"Missing"
W18-2708,2016.amta-researchers.9,1,0.905201,"Missing"
W18-2708,N13-1073,0,0.0398411,"eficial not to reset the model between documents, while in heterogeneous domains it may be desirable to do so always. We leave this issue to future work. 66 4.1 producing a model specifically adapted to this document’s novel vocabulary, which we can then use to decode the complete document. Note that this is a very small bitext to train on, and each line of the bitext contains a single word (segmented into multiple tokens by byte-pair encoding). To simulate a translator-produced dictionary, we build a dictionary of novel word translations from the source and reference. First we run fastalign (Dyer et al., 2013) over the byte-pair encoded representations of the source and reference sentences.4 The target-side token whose subword segments most frequently align to the subword segments of the source-side token is selected as a candidate translation, and a single final translation is selected based on the most common candidate translation within the document.5 3.3 WMT Data: We test on the full WMT 2017 news translation test set, splitting it into 130 unique documents (derived from the document splits in the original SGM file). Each document is a short news story. These stories are drawn from a number of"
W18-2708,kobus-etal-2017-domain,0,0.0530326,"65 terest for researchers in the machine translation community and is relevant both to the translation of new words and to more general improvements in translation quality. Recent work (Freitag and Al-Onaizan, 2016; Luong and Manning, 2015) has proposed to do domain adaptation for NMT systems by training a general system then fine-tuning by continuing to train using only in-domain data (typically a smaller dataset). Wang et al. (2017) present a similar approach where they weight each source-target sentence pair during training based on scores from in-domain and out-of-domain language models. Kobus et al. (2017) use special tokens to indicate domain. Chu et al. (2017) compare the approaches. These approaches typically use larger amounts of in-domain data to do adaptation, far greater than the amounts that might be available in a CAT setting. Cettolo et al. (2014) proposed adapting statistical phrase-based machine translation systems to particular projects (multiple documents) and Peris and Casacuberta (2018) propose adapting neural machine translation systems in CAT settings. Neither explore very small amounts of data at the sub-document level. Two recent papers have tried a domain adaptation approac"
W18-2708,L18-1146,0,0.23899,"Missing"
W18-2708,P17-2090,0,0.0657008,"Missing"
W18-2708,2015.iwslt-evaluation.11,0,0.696058,"rdini, 2005). Thus, even expanding beyond words with multiple senses, we expect that learning from the translator’s lexical, syntactic, and stylistic choices at the beginning of a document should result in a well-tailored system that is better at translating subsequent sentences. We can think of fine-grained adaptation over a document as producing a document-specific machine translation system that encodes or highlights document context. Continued training of neural machine translation (NMT) systems has been shown to be an effective and efficient way to tune them for a specific target domain (Luong and Manning, 2015). One such technique is incremental updating – comparing the system’s predicted translation of an input sentence to a reference translation and then updatIt is common practice to adapt machine translation systems to novel domains, but even a well-adapted system may be able to perform better on a particular document if it were to learn from a translator’s corrections within the document itself. We focus on adaptation within a single document – appropriate for an interactive translation scenario where a model adapts to a human translator’s input over the course of a document. We propose two meth"
W18-2708,N18-1031,0,0.0359042,"Missing"
W18-2708,P02-1040,0,0.103669,"Missing"
W18-2708,E17-3017,0,0.0941065,"Missing"
W18-2708,W16-2323,0,0.150556,"n ideal use case for exploring model adaptation at such a fine granularity. As a human translator works, each sentence that they translate (or each novel word for which they provide a translation) can then be used as a new training example for a neural machine translation system. In an interactive translation setting or a post-editing scenario, rapid incremental updating of the neural model will allow the neural system to adapt to an individual translator, a particular new domain, or novel vocabulary over the course of a document. In an open-vocabulary NMT system that uses byte-pair encoding (Sennrich et al., 2016b), tokens that were never seen in training data are represented as sequences of known subword units. These may sometimes be successfully translated (or copied, subword by subword, when appropriate) on the first try, but sometimes systems generate incorrect translations or even nonsensical words. Table 1 shows example mistranslations of novel words. We test our two complementary approaches to document-level NMT adaptation (dictionary training and single-sentence adaptation) on two very different domains: news and formal descriptions of medications, each of which provide their own challenges. I"
W18-2708,P16-1162,0,0.493405,"n ideal use case for exploring model adaptation at such a fine granularity. As a human translator works, each sentence that they translate (or each novel word for which they provide a translation) can then be used as a new training example for a neural machine translation system. In an interactive translation setting or a post-editing scenario, rapid incremental updating of the neural model will allow the neural system to adapt to an individual translator, a particular new domain, or novel vocabulary over the course of a document. In an open-vocabulary NMT system that uses byte-pair encoding (Sennrich et al., 2016b), tokens that were never seen in training data are represented as sequences of known subword units. These may sometimes be successfully translated (or copied, subword by subword, when appropriate) on the first try, but sometimes systems generate incorrect translations or even nonsensical words. Table 1 shows example mistranslations of novel words. We test our two complementary approaches to document-level NMT adaptation (dictionary training and single-sentence adaptation) on two very different domains: news and formal descriptions of medications, each of which provide their own challenges. I"
W18-2708,D17-1155,0,0.0175934,"ne novel word. We show that models can learn to correctly translate novel vocabulary items and can adapt to document-specific terminology usage and style, even in short documents. 65 terest for researchers in the machine translation community and is relevant both to the translation of new words and to more general improvements in translation quality. Recent work (Freitag and Al-Onaizan, 2016; Luong and Manning, 2015) has proposed to do domain adaptation for NMT systems by training a general system then fine-tuning by continuing to train using only in-domain data (typically a smaller dataset). Wang et al. (2017) present a similar approach where they weight each source-target sentence pair during training based on scores from in-domain and out-of-domain language models. Kobus et al. (2017) use special tokens to indicate domain. Chu et al. (2017) compare the approaches. These approaches typically use larger amounts of in-domain data to do adaptation, far greater than the amounts that might be available in a CAT setting. Cettolo et al. (2014) proposed adapting statistical phrase-based machine translation systems to particular projects (multiple documents) and Peris and Casacuberta (2018) propose adaptin"
W18-2708,P16-1007,0,0.0209719,"g very small data sizes, ranging from 1 sentence to 128 sentences (Farajian et al., 2017; Li et al., 2016). They adapt models for new sentences by training on sentence pairs from a training corpus (or translation memory) that are similar to the new sentence, which means they cannot adapt to novel vocabulary. of the document. See Algorithm 1 for details. Such an approach could be applied in a computer aided translation tool, which would allow the machine translation system to adapt to translator corrections as produced by post-editing or through an interactive translation prediction interface (Wuebker et al., 2016; Knowles and Koehn, 2016). Single-sentence adaptation allows the model to learn the translator’s preferred translations, which may be specific to the particular document. For example, the system might initially produce a valid translation for a word in the document, while the translator prefers an alternate translation; after single-sentence adaptation, the system can learn to produce the translator’s preferred translation in future sentences. Algorithm 1 Single-Sentence Adaptation 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 3 Approaches 16: 17: We propose two complementary approaches"
W18-2709,W11-1218,0,0.0812966,"Neural Machine Translation and Generation, pages 74–83 c Melbourne, Australia, July 20, 2018. 2018 Association for Computational Linguistics Type of Noise Okay Misaligned sentences Third language Both English Both German Untranslated sentences Short segments (≤2 tokens) Short segments (3–5 tokens) Non-linguistic characters As Rarrick et al. (2011) point out, one problem of parallel corpora extracted from the web is translations that have been created by machine translation. Venugopal et al. (2011) propose a method to watermark the output of machine translation systems to aid this distinction. Antonova and Misyurev (2011) report that rule-based machine translation output can be detected due to certain word choices, and statistical machine translation output due to lack of reordering. In 2016, a shared task on sentence pair filtering was organized1 (Barbu et al., 2016), albeit in the context of cleaning translation memories which tend to be cleaner than web crawled data. This year, a shared task is planned for the type of noise that we examine in this paper.2 Belinkov and Bisk (2017) investigate noise in neural machine translation, but they focus on creating systems that can translate the kinds of orthographic"
W18-2709,J07-2003,0,0.0773317,"ing settings: a maximum sentence length of 80, grow-diag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011), hierarchical lexicalized reordering (Galley and Manning, 2008), a lexicallydriven 5-gram operation sequence model (OSM) (Durrani et al., 2013), sparse domain indicator, phrase length, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a maximum phrase-length of 5, compact phrase table (Junczys-Dowmunt, 2012) minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning. We optimize feature function weights with k-best MIRA (Cherry and Foster, 2012). While we focus on phrase based systems as our SMT paradigm, we note that there are other statistical machine translation approaches such as hierarchical phrase-based models (Chiang, 2007) and syntax-based models (Galley et al., 2004, 2006) that may have better performance in certain language pairs and in low resource conditions. 5.4 For MISALIGNED SENTENCE and MISORDERED WORD noise, we use the clean corpus (above) and perturb the data. To create UNTRANSLATED SEN TENCE noi"
W18-2709,D11-1033,0,0.142503,"h tend to be cleaner than web crawled data. This year, a shared task is planned for the type of noise that we examine in this paper.2 Belinkov and Bisk (2017) investigate noise in neural machine translation, but they focus on creating systems that can translate the kinds of orthographic errors (typos, misspellings, etc.) that humans can comprehend. In contrast, we address noisy training data and focus on types of noise occurring in web-crawled corpora. There is a rich literature on data selection which aims at sub-sampling parallel data relevant for a task-specific machine translation system (Axelrod et al., 2011). van der Wees et al. (2017) find that the existing data selection methods developed for statistical machine translation are less effective for neural machine translation. This is different from our goals of handling noise since those methods tend to discard perfectly fine sentence pairs (say, about cooking recipes) that are just not relevant for the targeted domain (say, software manuals). Our work is focused on noise that is harmful for all domains. Since we begin with a clean parallel corpus and potentially noisy data to it, this work can be seen as a type of data augmentation. Sennrich et"
W18-2709,N09-1025,0,0.092143,"Missing"
W18-2709,P13-2061,0,0.31517,"ural machine translation. The added data cannot be too noisy. But what kind of noise harms neural machine translation models? In this paper, we explore several types of noise and assess their impact by adding synthetic noise 2 Related Work There is a robust body of work on filtering out noise in parallel data. For example: Taghipour et al. (2011) use an outlier detection algorithm to filter a parallel corpus; Xu and Koehn (2017) generate synthetic noisy data (inadequate and nonfluent translations) and use this data to train a classifier to identify good sentence pairs from a noisy corpus; and Cui et al. (2013) use a graph-based random walk algorithm and extract phrase pair scores to weight the phrase translation probabilities to bias towards more trustworthy ones. Most of this work was done in the context of statistical machine translation, but more recent work (Carpuat et al., 2017) targets neural models. That work focuses on identifying semantic differences in translation pairs using cross-lingual textual entailment and additional length-based features, and demonstrates that removing such sentences improves neural machine translation performance. 74 Proceedings of the 2nd Workshop on Neural Machi"
W18-2709,W17-4715,0,0.0491348,"Missing"
W18-2709,D08-1023,0,0.0138176,"., 2007).4 We build phrase-based systems using standard features commonly used in recent system submissions to WMT (Haddow et al., 2015; Ding et al., 2016, 2017). We trained our systems with the following settings: a maximum sentence length of 80, grow-diag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011), hierarchical lexicalized reordering (Galley and Manning, 2008), a lexicallydriven 5-gram operation sequence model (OSM) (Durrani et al., 2013), sparse domain indicator, phrase length, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a maximum phrase-length of 5, compact phrase table (Junczys-Dowmunt, 2012) minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning. We optimize feature function weights with k-best MIRA (Cherry and Foster, 2012). While we focus on phrase based systems as our SMT paradigm, we note that there are other statistical machine translation approaches such as hierarchical phrase-based models (Chiang, 2007) and syntax-based models (Galley et al., 2004, 2006) that may have better performance in certain lan"
W18-2709,W16-2310,1,0.844067,"2–4 days on a single GPU (GTX 1080ti). While we focus on RNN-based models with attention as our NMT architecture, we note that different architectures have been proposed, including based on convolutional neural networks (Kalchbrenner and Blunsom, 2013; Gehring et al., 2017) and the self-attention based Transformer model (Vaswani et al., 2017). 5.2 Statistical Machine Translation Our statistical machine translation systems are trained using Moses (Koehn et al., 2007).4 We build phrase-based systems using standard features commonly used in recent system submissions to WMT (Haddow et al., 2015; Ding et al., 2016, 2017). We trained our systems with the following settings: a maximum sentence length of 80, grow-diag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011), hierarchical lexicalized reordering (Galley and Manning, 2008), a lexicallydriven 5-gram operation sequence model (OSM) (Durrani et al., 2013), sparse domain indicator, phrase length, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a maximum phrase-length of 5, compact phrase table (Junczys-Dowmunt, 2012) minimum Bayes risk decoding (K"
W18-2709,W11-2123,0,0.022495,"Blunsom, 2013; Gehring et al., 2017) and the self-attention based Transformer model (Vaswani et al., 2017). 5.2 Statistical Machine Translation Our statistical machine translation systems are trained using Moses (Koehn et al., 2007).4 We build phrase-based systems using standard features commonly used in recent system submissions to WMT (Haddow et al., 2015; Ding et al., 2016, 2017). We trained our systems with the following settings: a maximum sentence length of 80, grow-diag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011), hierarchical lexicalized reordering (Galley and Manning, 2008), a lexicallydriven 5-gram operation sequence model (OSM) (Durrani et al., 2013), sparse domain indicator, phrase length, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a maximum phrase-length of 5, compact phrase table (Junczys-Dowmunt, 2012) minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning. We optimize feature function weights with k-best MIRA (Cherry and Foster, 2012). While we focus on phrase based systems as our SMT pa"
W18-2709,P13-2071,1,0.813458,"lation Our statistical machine translation systems are trained using Moses (Koehn et al., 2007).4 We build phrase-based systems using standard features commonly used in recent system submissions to WMT (Haddow et al., 2015; Ding et al., 2016, 2017). We trained our systems with the following settings: a maximum sentence length of 80, grow-diag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011), hierarchical lexicalized reordering (Galley and Manning, 2008), a lexicallydriven 5-gram operation sequence model (OSM) (Durrani et al., 2013), sparse domain indicator, phrase length, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a maximum phrase-length of 5, compact phrase table (Junczys-Dowmunt, 2012) minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning. We optimize feature function weights with k-best MIRA (Cherry and Foster, 2012). While we focus on phrase based systems as our SMT paradigm, we note that there are other statistical machine translation approaches such as hierarchical phrase-based models (Chiang, 2007) and synt"
W18-2709,P07-1019,0,0.0213568,"the following settings: a maximum sentence length of 80, grow-diag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011), hierarchical lexicalized reordering (Galley and Manning, 2008), a lexicallydriven 5-gram operation sequence model (OSM) (Durrani et al., 2013), sparse domain indicator, phrase length, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a maximum phrase-length of 5, compact phrase table (Junczys-Dowmunt, 2012) minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning. We optimize feature function weights with k-best MIRA (Cherry and Foster, 2012). While we focus on phrase based systems as our SMT paradigm, we note that there are other statistical machine translation approaches such as hierarchical phrase-based models (Chiang, 2007) and syntax-based models (Galley et al., 2004, 2006) that may have better performance in certain language pairs and in low resource conditions. 5.4 For MISALIGNED SENTENCE and MISORDERED WORD noise, we use the clean corpus (above) and perturb the data. To create UNTRANSLATED SEN TENCE noi"
W18-2709,P17-2090,0,0.0398085,"Missing"
W18-2709,2012.eamt-1.58,0,0.011582,"ssions to WMT (Haddow et al., 2015; Ding et al., 2016, 2017). We trained our systems with the following settings: a maximum sentence length of 80, grow-diag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011), hierarchical lexicalized reordering (Galley and Manning, 2008), a lexicallydriven 5-gram operation sequence model (OSM) (Durrani et al., 2013), sparse domain indicator, phrase length, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a maximum phrase-length of 5, compact phrase table (Junczys-Dowmunt, 2012) minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning. We optimize feature function weights with k-best MIRA (Cherry and Foster, 2012). While we focus on phrase based systems as our SMT paradigm, we note that there are other statistical machine translation approaches such as hierarchical phrase-based models (Chiang, 2007) and syntax-based models (Galley et al., 2004, 2006) that may have better performance in certain language pairs and in low resource conditions. 5.4 For MISALIGNED SENTENCE and MISORDERED WORD noise,"
W18-2709,P06-1121,0,0.0715398,"Missing"
W18-2709,P18-4020,0,0.055566,"Missing"
W18-2709,N04-1035,0,0.0337356,"ndicator, phrase length, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a maximum phrase-length of 5, compact phrase table (Junczys-Dowmunt, 2012) minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning. We optimize feature function weights with k-best MIRA (Cherry and Foster, 2012). While we focus on phrase based systems as our SMT paradigm, we note that there are other statistical machine translation approaches such as hierarchical phrase-based models (Chiang, 2007) and syntax-based models (Galley et al., 2004, 2006) that may have better performance in certain language pairs and in low resource conditions. 5.4 For MISALIGNED SENTENCE and MISORDERED WORD noise, we use the clean corpus (above) and perturb the data. To create UNTRANSLATED SEN TENCE noise, we also use the clean corpus and create pairs of identical sentences. For WRONG LANGUAGE noise, we do not have French–English and German–French data of the same size. Hence, we use the EU Bookstore corpus (Skadin¸sˇ et al., 2014).8 The SHORT SEGMENTS are extracted from OPUS corpora (Tiedemann, 2009, 2012; Lison and Tiedemann, 2016):9 EMEA (descriptio"
W18-2709,D13-1176,0,0.0336535,"t we do not add monolingual data to our systems since this would make our study more complex. So, we always train our language model on the target side of the parallel corpus for that experiment. While using monolingual data for language modelling is standard practice in statistical machine translation, how to use such data for neural models is less obvious. each system takes 2–4 days on a single GPU (GTX 1080ti). While we focus on RNN-based models with attention as our NMT architecture, we note that different architectures have been proposed, including based on convolutional neural networks (Kalchbrenner and Blunsom, 2013; Gehring et al., 2017) and the self-attention based Transformer model (Vaswani et al., 2017). 5.2 Statistical Machine Translation Our statistical machine translation systems are trained using Moses (Koehn et al., 2007).4 We build phrase-based systems using standard features commonly used in recent system submissions to WMT (Haddow et al., 2015; Ding et al., 2016, 2017). We trained our systems with the following settings: a maximum sentence length of 80, grow-diag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011"
W18-2709,D08-1089,0,0.100273,"Missing"
W18-2709,2005.mtsummit-papers.11,1,0.286522,"Missing"
W18-2709,P17-1012,0,0.0274485,"to our systems since this would make our study more complex. So, we always train our language model on the target side of the parallel corpus for that experiment. While using monolingual data for language modelling is standard practice in statistical machine translation, how to use such data for neural models is less obvious. each system takes 2–4 days on a single GPU (GTX 1080ti). While we focus on RNN-based models with attention as our NMT architecture, we note that different architectures have been proposed, including based on convolutional neural networks (Kalchbrenner and Blunsom, 2013; Gehring et al., 2017) and the self-attention based Transformer model (Vaswani et al., 2017). 5.2 Statistical Machine Translation Our statistical machine translation systems are trained using Moses (Koehn et al., 2007).4 We build phrase-based systems using standard features commonly used in recent system submissions to WMT (Haddow et al., 2015; Ding et al., 2016, 2017). We trained our systems with the following settings: a maximum sentence length of 80, grow-diag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011), hierarchical lexical"
W18-2709,W15-3013,1,0.893716,"Missing"
W18-2709,W17-3204,1,0.78628,"We find that for almost all types of noise, neural machine translation systems are harmed more than statistical machine translation systems. We discovered that one type of noise, copied source language segments, has a catastrophic impact on neural machine translation quality, leading it to learn a copying behavior that it then exceedingly applies. Introduction While neural machine translation (NMT) has shown large gains in quality over statistical machine translation (SMT) (Bojar et al., 2017), there are significant exceptions to this, such as low resource and domain mismatch data conditions (Koehn and Knowles, 2017). In this work, we consider another challenge to neural machine translation: noisy parallel data. As a motivating example, consider the numbers in Table 1. Here, we add an equally sized noisy web crawled corpus to high quality training data provided by the shared task of the Conference on Machine Translation (WMT). This addition leads to a 1.2 BLEU point increase for the statistical machine translation system, but degrades the neural machine translation system by 9.9 BLEU. The maxim more data is better that holds true for statistical machine translation does seem to come with some caveats for"
W18-2709,N04-1022,0,0.111998,"6, 2017). We trained our systems with the following settings: a maximum sentence length of 80, grow-diag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011), hierarchical lexicalized reordering (Galley and Manning, 2008), a lexicallydriven 5-gram operation sequence model (OSM) (Durrani et al., 2013), sparse domain indicator, phrase length, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a maximum phrase-length of 5, compact phrase table (Junczys-Dowmunt, 2012) minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning. We optimize feature function weights with k-best MIRA (Cherry and Foster, 2012). While we focus on phrase based systems as our SMT paradigm, we note that there are other statistical machine translation approaches such as hierarchical phrase-based models (Chiang, 2007) and syntax-based models (Galley et al., 2004, 2006) that may have better performance in certain language pairs and in low resource conditions. 5.4 For MISALIGNED SENTENCE and MISORDERED WORD noise, we use the clean corpus (above) and perturb the dat"
W18-2709,L16-1147,0,0.0216285,"and syntax-based models (Galley et al., 2004, 2006) that may have better performance in certain language pairs and in low resource conditions. 5.4 For MISALIGNED SENTENCE and MISORDERED WORD noise, we use the clean corpus (above) and perturb the data. To create UNTRANSLATED SEN TENCE noise, we also use the clean corpus and create pairs of identical sentences. For WRONG LANGUAGE noise, we do not have French–English and German–French data of the same size. Hence, we use the EU Bookstore corpus (Skadin¸sˇ et al., 2014).8 The SHORT SEGMENTS are extracted from OPUS corpora (Tiedemann, 2009, 2012; Lison and Tiedemann, 2016):9 EMEA (descriptions of medicines),10 Tanzil (religious text),11 Open Subtitles 2016,12 Acquis (legislative text),13 GNOME (software localization files),14 KDE (localization files), PHP (technical manual),15 Ubuntu (localization files),16 and Open Office.17 We use only pairs where both the English and German segments are at most 2 or 5 words long. Since this results in small data sets (2 million and 15 tokens per language, respectively), they are duplicated multiple times. We also show the results for naturally occurring noisy web data from the raw 2016 ParaCrawl corpus (deduplicated raw set)"
W18-2709,tiedemann-2012-parallel,0,0.0810466,"Missing"
W18-2709,J82-2005,0,0.444737,"Missing"
W18-2709,D17-1147,0,0.0927084,"Missing"
W18-2709,P02-1040,0,0.101669,"Missing"
W18-2709,2011.mtsummit-papers.48,0,0.186719,"uses on identifying semantic differences in translation pairs using cross-lingual textual entailment and additional length-based features, and demonstrates that removing such sentences improves neural machine translation performance. 74 Proceedings of the 2nd Workshop on Neural Machine Translation and Generation, pages 74–83 c Melbourne, Australia, July 20, 2018. 2018 Association for Computational Linguistics Type of Noise Okay Misaligned sentences Third language Both English Both German Untranslated sentences Short segments (≤2 tokens) Short segments (3–5 tokens) Non-linguistic characters As Rarrick et al. (2011) point out, one problem of parallel corpora extracted from the web is translations that have been created by machine translation. Venugopal et al. (2011) propose a method to watermark the output of machine translation systems to aid this distinction. Antonova and Misyurev (2011) report that rule-based machine translation output can be detected due to certain word choices, and statistical machine translation output due to lack of reordering. In 2016, a shared task on sentence pair filtering was organized1 (Barbu et al., 2016), albeit in the context of cleaning translation memories which tend to"
W18-2709,D11-1126,0,0.147644,"ates that removing such sentences improves neural machine translation performance. 74 Proceedings of the 2nd Workshop on Neural Machine Translation and Generation, pages 74–83 c Melbourne, Australia, July 20, 2018. 2018 Association for Computational Linguistics Type of Noise Okay Misaligned sentences Third language Both English Both German Untranslated sentences Short segments (≤2 tokens) Short segments (3–5 tokens) Non-linguistic characters As Rarrick et al. (2011) point out, one problem of parallel corpora extracted from the web is translations that have been created by machine translation. Venugopal et al. (2011) propose a method to watermark the output of machine translation systems to aid this distinction. Antonova and Misyurev (2011) report that rule-based machine translation output can be detected due to certain word choices, and statistical machine translation output due to lack of reordering. In 2016, a shared task on sentence pair filtering was organized1 (Barbu et al., 2016), albeit in the context of cleaning translation memories which tend to be cleaner than web crawled data. This year, a shared task is planned for the type of noise that we examine in this paper.2 Belinkov and Bisk (2017) inv"
W18-2709,P16-1009,0,0.0701124,"Missing"
W18-2709,D17-1319,1,0.798701,"Missing"
W18-2709,P16-1162,0,0.321811,"Missing"
W18-2709,skadins-etal-2014-billions,0,0.0316809,"Missing"
W18-2709,2011.mtsummit-papers.47,0,0.743973,"ion leads to a 1.2 BLEU point increase for the statistical machine translation system, but degrades the neural machine translation system by 9.9 BLEU. The maxim more data is better that holds true for statistical machine translation does seem to come with some caveats for neural machine translation. The added data cannot be too noisy. But what kind of noise harms neural machine translation models? In this paper, we explore several types of noise and assess their impact by adding synthetic noise 2 Related Work There is a robust body of work on filtering out noise in parallel data. For example: Taghipour et al. (2011) use an outlier detection algorithm to filter a parallel corpus; Xu and Koehn (2017) generate synthetic noisy data (inadequate and nonfluent translations) and use this data to train a classifier to identify good sentence pairs from a noisy corpus; and Cui et al. (2013) use a graph-based random walk algorithm and extract phrase pair scores to weight the phrase translation probabilities to bias towards more trustworthy ones. Most of this work was done in the context of statistical machine translation, but more recent work (Carpuat et al., 2017) targets neural models. That work focuses on identif"
W18-6313,D11-1033,0,0.0803148,"M 138.6 M De–En Subtitles 22.5 M 171.6 M Ko–En Subtitles 1.4 M 11.5 M Table 1: Number of parameters in each component. Korean, and Russian into English. Our out-ofdomain models are trained on WMT and/or subtitles corpora, and we adapt each model to translate patent abstracts. 2 Sentences Continued training has recently become a standard for domain or cross-lingual adaptation in several neural NLP applications. In PBMT, the most prominent methods focus on adapting the language model component (Moore and Lewis, 2010), and/or the translation model (Matsoukas et al., 2009; Mansour and Ney, 2014; Axelrod et al., 2011), or on interpolating in-domain and out-of-domain models (Lu et al., 2007; Foster et al., 2010; Koehn and Schroeder, 2007). In contrast, the methods employed in NMT tend to utilize continued training, which involves initializing the model with pre-trained weights (trained on out-of-domain data) and training/adapting it to the in-domain data. Among others, Luong and Manning (2015) and Freitag and Al-Onaizan (2016) applied this method for domain adaptation. Chu et al. (2017) mix in-domain and out-of-domain data during continued training in order to adapt to multiple domains. Continued training h"
W18-6313,P10-2016,0,0.057732,"Missing"
W18-6313,P17-2061,0,0.0587364,"model component (Moore and Lewis, 2010), and/or the translation model (Matsoukas et al., 2009; Mansour and Ney, 2014; Axelrod et al., 2011), or on interpolating in-domain and out-of-domain models (Lu et al., 2007; Foster et al., 2010; Koehn and Schroeder, 2007). In contrast, the methods employed in NMT tend to utilize continued training, which involves initializing the model with pre-trained weights (trained on out-of-domain data) and training/adapting it to the in-domain data. Among others, Luong and Manning (2015) and Freitag and Al-Onaizan (2016) applied this method for domain adaptation. Chu et al. (2017) mix in-domain and out-of-domain data during continued training in order to adapt to multiple domains. Continued training has also been applied to cross-lingual transfer learning for NMT, with Zoph et al. (2016) and Nguyen and Chiang (2017) using it for transfer between high- and lowresource language pairs. Continued training is effective on a range of data sizes. In-domain gains have been shown with as few as dozens of in-domain training sentences (Miceli Barone et al., 2017), and recent work has explored continued training on single sentences (Farajian et al., 2017; Kothur et al., 2018). Sim"
W18-6313,2010.iwslt-papers.5,1,0.80886,"ain adaptation methods, because PBMT adaptation methods primarily rely on adapting the language model and phrase table using interpolation or back-off schemes (see §2). Continued training (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016), also referred to as fine-tuning, is one of the most popular methods for NMT adaptation, due to its strong performance. In contrast to the PBMT literature, little research has focused on why continued training is effective or on what happens to NMT models during continued training. Motivated by domain adaptation analysis in PBMT (Haddow and Koehn, 2012; Duh et al., 2010; Irvine et al., 2013), this work proposes a simple freezing subnetworks technique and uses it to gain insight into how the various components of an NMT system behave during continued training. We segment the model into five subnetworks, which we refer to as components, denoted in Figure 1: the source embeddings, encoder, decoder (which includes the attention mechanism), the softmax (used to denote the decoder output embeddings and biases), and the target embeddings. We freeze components one at a time during continued training to see how much the adaptation depends on each component. We also e"
W18-6313,W18-2705,1,0.827312,"Missing"
W18-6313,W17-4713,0,0.124642,"method for domain adaptation. Chu et al. (2017) mix in-domain and out-of-domain data during continued training in order to adapt to multiple domains. Continued training has also been applied to cross-lingual transfer learning for NMT, with Zoph et al. (2016) and Nguyen and Chiang (2017) using it for transfer between high- and lowresource language pairs. Continued training is effective on a range of data sizes. In-domain gains have been shown with as few as dozens of in-domain training sentences (Miceli Barone et al., 2017), and recent work has explored continued training on single sentences (Farajian et al., 2017; Kothur et al., 2018). Similar adaptation techniques are also employed in the field of Automatic Speech Recognition, where continued training has been the basis of 595.9 M 212.4 M 131.8 M 185.8 M 11.9 M In-domain training sets Ru–En WIPO 29 k 620 k De–En WIPO 821 k 19 M Ko–En WIPO 81 k 2.2 M 812 k 23 M 2.0 M In-domain test sets 3k 82 k 3k 132 k 3k 187 k 109 k 162 k 165 k Ru–En WIPO De–En WIPO Ko–En WIPO Related Work Target Table 2: Dataset statistics. The number of tokens is computed before segmentation into subwords. The in-domain development sets (not shown) have similar statistics to the t"
W18-6313,D10-1044,0,0.0664357,"Missing"
W18-6313,W07-0733,1,0.69704,"t. Korean, and Russian into English. Our out-ofdomain models are trained on WMT and/or subtitles corpora, and we adapt each model to translate patent abstracts. 2 Sentences Continued training has recently become a standard for domain or cross-lingual adaptation in several neural NLP applications. In PBMT, the most prominent methods focus on adapting the language model component (Moore and Lewis, 2010), and/or the translation model (Matsoukas et al., 2009; Mansour and Ney, 2014; Axelrod et al., 2011), or on interpolating in-domain and out-of-domain models (Lu et al., 2007; Foster et al., 2010; Koehn and Schroeder, 2007). In contrast, the methods employed in NMT tend to utilize continued training, which involves initializing the model with pre-trained weights (trained on out-of-domain data) and training/adapting it to the in-domain data. Among others, Luong and Manning (2015) and Freitag and Al-Onaizan (2016) applied this method for domain adaptation. Chu et al. (2017) mix in-domain and out-of-domain data during continued training in order to adapt to multiple domains. Continued training has also been applied to cross-lingual transfer learning for NMT, with Zoph et al. (2016) and Nguyen and Chiang (2017) usin"
W18-6313,W12-3154,1,0.843127,"has necessitated new domain adaptation methods, because PBMT adaptation methods primarily rely on adapting the language model and phrase table using interpolation or back-off schemes (see §2). Continued training (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016), also referred to as fine-tuning, is one of the most popular methods for NMT adaptation, due to its strong performance. In contrast to the PBMT literature, little research has focused on why continued training is effective or on what happens to NMT models during continued training. Motivated by domain adaptation analysis in PBMT (Haddow and Koehn, 2012; Duh et al., 2010; Irvine et al., 2013), this work proposes a simple freezing subnetworks technique and uses it to gain insight into how the various components of an NMT system behave during continued training. We segment the model into five subnetworks, which we refer to as components, denoted in Figure 1: the source embeddings, encoder, decoder (which includes the attention mechanism), the softmax (used to denote the decoder output embeddings and biases), and the target embeddings. We freeze components one at a time during continued training to see how much the adaptation depends on each co"
W18-6313,E17-3017,0,0.0649422,"Missing"
W18-6313,Q13-1035,0,0.0453818,"Missing"
W18-6313,W18-2708,1,0.893091,"tation. Chu et al. (2017) mix in-domain and out-of-domain data during continued training in order to adapt to multiple domains. Continued training has also been applied to cross-lingual transfer learning for NMT, with Zoph et al. (2016) and Nguyen and Chiang (2017) using it for transfer between high- and lowresource language pairs. Continued training is effective on a range of data sizes. In-domain gains have been shown with as few as dozens of in-domain training sentences (Miceli Barone et al., 2017), and recent work has explored continued training on single sentences (Farajian et al., 2017; Kothur et al., 2018). Similar adaptation techniques are also employed in the field of Automatic Speech Recognition, where continued training has been the basis of 595.9 M 212.4 M 131.8 M 185.8 M 11.9 M In-domain training sets Ru–En WIPO 29 k 620 k De–En WIPO 821 k 19 M Ko–En WIPO 81 k 2.2 M 812 k 23 M 2.0 M In-domain test sets 3k 82 k 3k 132 k 3k 187 k 109 k 162 k 165 k Ru–En WIPO De–En WIPO Ko–En WIPO Related Work Target Table 2: Dataset statistics. The number of tokens is computed before segmentation into subwords. The in-domain development sets (not shown) have similar statistics to the test sets. 125 cross-li"
W18-6313,W17-2620,0,0.0293601,"field of Automatic Speech Recognition, where continued training has been the basis of 595.9 M 212.4 M 131.8 M 185.8 M 11.9 M In-domain training sets Ru–En WIPO 29 k 620 k De–En WIPO 821 k 19 M Ko–En WIPO 81 k 2.2 M 812 k 23 M 2.0 M In-domain test sets 3k 82 k 3k 132 k 3k 187 k 109 k 162 k 165 k Ru–En WIPO De–En WIPO Ko–En WIPO Related Work Target Table 2: Dataset statistics. The number of tokens is computed before segmentation into subwords. The in-domain development sets (not shown) have similar statistics to the test sets. 125 cross-lingual transfer learning approaches (Gr´ezl et al., 2014; Kunze et al., 2017). Usually, the lower layers of the network, which perform acoustic modeling, are frozen and only the upper layers are updated. In a similar vein, other works (Swietojanski and Renals, 2014; Vilar, 2018) adapt a network to a new domain by learning additional weights that re-scale the hidden units. 3 Data Our experiments are carried out across three language pairs, from Russian, Korean, and German into English. Basic statistics on the datasets used for our experiments are summarized in Table 2. The three languages represent three different domain adaptation scenarios: • In German, both the in- a"
W18-6313,L18-1146,0,0.0799127,"Missing"
W18-6313,L16-1147,0,0.0169409,"fied communication and sharing of information between the project partners enables the transfer of expertise in rural tourism. WIPO The films coated therewith, in particular polycarbonate films coated therewith, have improved properties with regard to scratch resistance, solvent resistance, and reduced oiling effect, said films thus being especially suitable for use in producing plastic parts in film insert molding methods. Table 3: Example sentences to illustrate domain differences. 3.1 Out-of-domain Data For our out-of-domain dataset we utilize the OpenSubtitles2018 corpus (Tiedemann, 2016; Lison and Tiedemann, 2016), which consists of translated movie subtitles.1 For De–En and Ru– En, we also use data from WMT 2017 (Bojar et al., 2017),2 which contains data from several sources: Europarl (parliamentary proceedings) (Koehn, 2005),3 News Commentary (political and economic news commentary),4 Common Crawl (web-crawled parallel corpus), and the EU Press Releases. We use the final 2500 lines of OpenSubtitles2018 for the development set. For German and Russian we also concatenate newstest2016 as part of the development set. newstest2016 consists of translated news articles released by WMT for its shared task. I"
W18-6313,D07-1036,0,0.042488,"Number of parameters in each component. Korean, and Russian into English. Our out-ofdomain models are trained on WMT and/or subtitles corpora, and we adapt each model to translate patent abstracts. 2 Sentences Continued training has recently become a standard for domain or cross-lingual adaptation in several neural NLP applications. In PBMT, the most prominent methods focus on adapting the language model component (Moore and Lewis, 2010), and/or the translation model (Matsoukas et al., 2009; Mansour and Ney, 2014; Axelrod et al., 2011), or on interpolating in-domain and out-of-domain models (Lu et al., 2007; Foster et al., 2010; Koehn and Schroeder, 2007). In contrast, the methods employed in NMT tend to utilize continued training, which involves initializing the model with pre-trained weights (trained on out-of-domain data) and training/adapting it to the in-domain data. Among others, Luong and Manning (2015) and Freitag and Al-Onaizan (2016) applied this method for domain adaptation. Chu et al. (2017) mix in-domain and out-of-domain data during continued training in order to adapt to multiple domains. Continued training has also been applied to cross-lingual transfer learning for NMT, with Zop"
W18-6313,2015.iwslt-evaluation.11,0,0.369812,"t the out-ofdomain model can provide a good generic initialization for the new domain. 1 hands your Wash Decoder Encoder Source Embedding Wasch dir die Hände Figure 1: Visualization of an NMT system segmented into components. Introduction Neural Machine Translation (NMT) has supplanted Phrase-Based Machine Translation (PBMT) as the standard for high-resource machine translation. This has necessitated new domain adaptation methods, because PBMT adaptation methods primarily rely on adapting the language model and phrase table using interpolation or back-off schemes (see §2). Continued training (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016), also referred to as fine-tuning, is one of the most popular methods for NMT adaptation, due to its strong performance. In contrast to the PBMT literature, little research has focused on why continued training is effective or on what happens to NMT models during continued training. Motivated by domain adaptation analysis in PBMT (Haddow and Koehn, 2012; Duh et al., 2010; Irvine et al., 2013), this work proposes a simple freezing subnetworks technique and uses it to gain insight into how the various components of an NMT system behave during continued training. We"
W18-6313,D15-1166,0,0.13368,"Missing"
W18-6313,N18-2080,0,0.103829,"2.2 M 812 k 23 M 2.0 M In-domain test sets 3k 82 k 3k 132 k 3k 187 k 109 k 162 k 165 k Ru–En WIPO De–En WIPO Ko–En WIPO Related Work Target Table 2: Dataset statistics. The number of tokens is computed before segmentation into subwords. The in-domain development sets (not shown) have similar statistics to the test sets. 125 cross-lingual transfer learning approaches (Gr´ezl et al., 2014; Kunze et al., 2017). Usually, the lower layers of the network, which perform acoustic modeling, are frozen and only the upper layers are updated. In a similar vein, other works (Swietojanski and Renals, 2014; Vilar, 2018) adapt a network to a new domain by learning additional weights that re-scale the hidden units. 3 Data Our experiments are carried out across three language pairs, from Russian, Korean, and German into English. Basic statistics on the datasets used for our experiments are summarized in Table 2. The three languages represent three different domain adaptation scenarios: • In German, both the in- and out-of-domain datasets are large. • In Russian, the in-domain dataset is large but the out-of-domain dataset is small. • In Korean, both in- and out-of-domain datasets are small. OpenSubtitles You’re"
W18-6313,2014.eamt-1.6,0,0.0183159,"M 179.8 M De–En WMT 5.8 M 138.6 M De–En Subtitles 22.5 M 171.6 M Ko–En Subtitles 1.4 M 11.5 M Table 1: Number of parameters in each component. Korean, and Russian into English. Our out-ofdomain models are trained on WMT and/or subtitles corpora, and we adapt each model to translate patent abstracts. 2 Sentences Continued training has recently become a standard for domain or cross-lingual adaptation in several neural NLP applications. In PBMT, the most prominent methods focus on adapting the language model component (Moore and Lewis, 2010), and/or the translation model (Matsoukas et al., 2009; Mansour and Ney, 2014; Axelrod et al., 2011), or on interpolating in-domain and out-of-domain models (Lu et al., 2007; Foster et al., 2010; Koehn and Schroeder, 2007). In contrast, the methods employed in NMT tend to utilize continued training, which involves initializing the model with pre-trained weights (trained on out-of-domain data) and training/adapting it to the in-domain data. Among others, Luong and Manning (2015) and Freitag and Al-Onaizan (2016) applied this method for domain adaptation. Chu et al. (2017) mix in-domain and out-of-domain data during continued training in order to adapt to multiple domain"
W18-6313,D16-1163,0,0.0343017,"007; Foster et al., 2010; Koehn and Schroeder, 2007). In contrast, the methods employed in NMT tend to utilize continued training, which involves initializing the model with pre-trained weights (trained on out-of-domain data) and training/adapting it to the in-domain data. Among others, Luong and Manning (2015) and Freitag and Al-Onaizan (2016) applied this method for domain adaptation. Chu et al. (2017) mix in-domain and out-of-domain data during continued training in order to adapt to multiple domains. Continued training has also been applied to cross-lingual transfer learning for NMT, with Zoph et al. (2016) and Nguyen and Chiang (2017) using it for transfer between high- and lowresource language pairs. Continued training is effective on a range of data sizes. In-domain gains have been shown with as few as dozens of in-domain training sentences (Miceli Barone et al., 2017), and recent work has explored continued training on single sentences (Farajian et al., 2017; Kothur et al., 2018). Similar adaptation techniques are also employed in the field of Automatic Speech Recognition, where continued training has been the basis of 595.9 M 212.4 M 131.8 M 185.8 M 11.9 M In-domain training sets Ru–En WIPO"
W18-6313,D09-1074,0,0.0328423,"M Ru–En Subtitles 25.9 M 179.8 M De–En WMT 5.8 M 138.6 M De–En Subtitles 22.5 M 171.6 M Ko–En Subtitles 1.4 M 11.5 M Table 1: Number of parameters in each component. Korean, and Russian into English. Our out-ofdomain models are trained on WMT and/or subtitles corpora, and we adapt each model to translate patent abstracts. 2 Sentences Continued training has recently become a standard for domain or cross-lingual adaptation in several neural NLP applications. In PBMT, the most prominent methods focus on adapting the language model component (Moore and Lewis, 2010), and/or the translation model (Matsoukas et al., 2009; Mansour and Ney, 2014; Axelrod et al., 2011), or on interpolating in-domain and out-of-domain models (Lu et al., 2007; Foster et al., 2010; Koehn and Schroeder, 2007). In contrast, the methods employed in NMT tend to utilize continued training, which involves initializing the model with pre-trained weights (trained on out-of-domain data) and training/adapting it to the in-domain data. Among others, Luong and Manning (2015) and Freitag and Al-Onaizan (2016) applied this method for domain adaptation. Chu et al. (2017) mix in-domain and out-of-domain data during continued training in order to a"
W18-6313,D17-1156,0,0.0510244,"data. Among others, Luong and Manning (2015) and Freitag and Al-Onaizan (2016) applied this method for domain adaptation. Chu et al. (2017) mix in-domain and out-of-domain data during continued training in order to adapt to multiple domains. Continued training has also been applied to cross-lingual transfer learning for NMT, with Zoph et al. (2016) and Nguyen and Chiang (2017) using it for transfer between high- and lowresource language pairs. Continued training is effective on a range of data sizes. In-domain gains have been shown with as few as dozens of in-domain training sentences (Miceli Barone et al., 2017), and recent work has explored continued training on single sentences (Farajian et al., 2017; Kothur et al., 2018). Similar adaptation techniques are also employed in the field of Automatic Speech Recognition, where continued training has been the basis of 595.9 M 212.4 M 131.8 M 185.8 M 11.9 M In-domain training sets Ru–En WIPO 29 k 620 k De–En WIPO 821 k 19 M Ko–En WIPO 81 k 2.2 M 812 k 23 M 2.0 M In-domain test sets 3k 82 k 3k 132 k 3k 187 k 109 k 162 k 165 k Ru–En WIPO De–En WIPO Ko–En WIPO Related Work Target Table 2: Dataset statistics. The number of tokens is computed before segmentatio"
W18-6313,P18-2050,0,0.0137192,"hat is, continued training is able to adapt the overall system to a new domain by modifying only parameters in a single component. This finding goes against the intuitive hypothesis that source embeddings must account for domain changes in the source vocabulary, target embeddings must account for changes in the target vocabulary, etc. We note that the encoder and decoder, despite having the least parameters (3.7M and 6.8M, respectively, out of 56M), perform strongly across all languages. This suggests further work on adapting only a subset of parameters may be warranted (see also Vilar, 2018; Michel and Neubig, 2018). 129 Acknowledgements Softmax Encoder Decoder Source Embed Target Embed 30 20 The authors would like to thank Lane Schwartz and Graham Neubig for their roles in organizing the MT Marathon in the Americas (MTMA), where this work began. The authors would also like to thank Michael Denkowski and David Vilar for assistance with S OCKEYE. This material is based upon work supported in part by the DARPA LORELEI and IARPA MATERIAL programs. Brian Thompson is supported by the Department of Defense through the National Defense Science & Engineering Graduate Fellowship (NDSEG) Program. Antonios Anastaso"
W18-6313,P10-2041,0,0.0713404,"rce Out-of-domain training sets Ru–En WMT 25.2 M 563.9 M Ru–En Subtitles 25.9 M 179.8 M De–En WMT 5.8 M 138.6 M De–En Subtitles 22.5 M 171.6 M Ko–En Subtitles 1.4 M 11.5 M Table 1: Number of parameters in each component. Korean, and Russian into English. Our out-ofdomain models are trained on WMT and/or subtitles corpora, and we adapt each model to translate patent abstracts. 2 Sentences Continued training has recently become a standard for domain or cross-lingual adaptation in several neural NLP applications. In PBMT, the most prominent methods focus on adapting the language model component (Moore and Lewis, 2010), and/or the translation model (Matsoukas et al., 2009; Mansour and Ney, 2014; Axelrod et al., 2011), or on interpolating in-domain and out-of-domain models (Lu et al., 2007; Foster et al., 2010; Koehn and Schroeder, 2007). In contrast, the methods employed in NMT tend to utilize continued training, which involves initializing the model with pre-trained weights (trained on out-of-domain data) and training/adapting it to the in-domain data. Among others, Luong and Manning (2015) and Freitag and Al-Onaizan (2016) applied this method for domain adaptation. Chu et al. (2017) mix in-domain and out-"
W18-6313,I17-2050,0,0.0190351,"0; Koehn and Schroeder, 2007). In contrast, the methods employed in NMT tend to utilize continued training, which involves initializing the model with pre-trained weights (trained on out-of-domain data) and training/adapting it to the in-domain data. Among others, Luong and Manning (2015) and Freitag and Al-Onaizan (2016) applied this method for domain adaptation. Chu et al. (2017) mix in-domain and out-of-domain data during continued training in order to adapt to multiple domains. Continued training has also been applied to cross-lingual transfer learning for NMT, with Zoph et al. (2016) and Nguyen and Chiang (2017) using it for transfer between high- and lowresource language pairs. Continued training is effective on a range of data sizes. In-domain gains have been shown with as few as dozens of in-domain training sentences (Miceli Barone et al., 2017), and recent work has explored continued training on single sentences (Farajian et al., 2017; Kothur et al., 2018). Similar adaptation techniques are also employed in the field of Automatic Speech Recognition, where continued training has been the basis of 595.9 M 212.4 M 131.8 M 185.8 M 11.9 M In-domain training sets Ru–En WIPO 29 k 620 k De–En WIPO 821 k"
W18-6313,P16-1162,0,0.0613931,"based on the WIPO development set perplexity and report results on the WIPO test sets. In-domain Data We perform adaptation into the World International Property Organization (WIPO) COPPA-V2 dataset (Junczys-Dowmunt et al., 2016).5 The WIPO data consist of parallel sentences from international patent application abstracts. We reserve 3000 lines each for the in-domain development and test sets. See Table 3 for an example WIPO sentence. 3.3 mented into words using the KoNLPy wrapper of the Mecab-Ko segmenter.7 As a final preprocessing step, we train Byte Pair Encoding (BPE) segmentation models (Sennrich et al., 2016) on the out-of-domain training corpus. We train separate BPE models for each language, each with a vocabulary size of 30,000. For each language, BPE is trained on the out-of-domain corpus only and then applied to the training, development, and test data for both out-of-domain and in-domain datasets. This mimics the realistic setting where a generic, computationally-expensive-to-train NMT model is trained once. This NMT model is then adapted to new domains as they emerge, without retraining on the out-of-domain corpus. Training BPE on the in-domain data would change the vocabulary and thus requ"
W18-6313,L16-1559,0,0.0130136,"boat. WMT Intensified communication and sharing of information between the project partners enables the transfer of expertise in rural tourism. WIPO The films coated therewith, in particular polycarbonate films coated therewith, have improved properties with regard to scratch resistance, solvent resistance, and reduced oiling effect, said films thus being especially suitable for use in producing plastic parts in film insert molding methods. Table 3: Example sentences to illustrate domain differences. 3.1 Out-of-domain Data For our out-of-domain dataset we utilize the OpenSubtitles2018 corpus (Tiedemann, 2016; Lison and Tiedemann, 2016), which consists of translated movie subtitles.1 For De–En and Ru– En, we also use data from WMT 2017 (Bojar et al., 2017),2 which contains data from several sources: Europarl (parliamentary proceedings) (Koehn, 2005),3 News Commentary (political and economic news commentary),4 Common Crawl (web-crawled parallel corpus), and the EU Press Releases. We use the final 2500 lines of OpenSubtitles2018 for the development set. For German and Russian we also concatenate newstest2016 as part of the development set. newstest2016 consists of translated news articles released b"
W18-6313,2005.mtsummit-papers.11,1,\N,Missing
W18-6401,W18-6432,1,0.779979,"Missing"
W18-6401,W07-0718,1,0.492999,"Participants were asked to build machine translation systems for any of 7 language pairs in both directions, to be evaluated on a test set of news stories. The main metric for this task is human judgment of translation quality. This year, we also opened up the task to additional test sets to probe specific aspects of translation. 1 Introduction The Third Conference on Machine Translation (WMT) held at EMNLP 20181 host a number of shared tasks on various aspects of machine translation. This conference builds on twelve previous editions of WMT as workshops and conferences (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016a, 2017). This year we conducted several official tasks. We report in this paper on the news translation task. Additional shared tasks are described in separate papers in these proceedings: • biomedical translation (Neves et al., 2018), • multimodal machine translation (Barrault et al., 2018), • metrics (Ma et al., 2018), • quality estimation (Specia et al., 2018), • automatic post-editing (Chatterjee et al., 2018), and • parallel corpus filtering (Koehn et al., 2018b). In the news translation task (Section 2), participants we"
W18-6401,W11-2101,1,0.731598,"Missing"
W18-6401,W08-0309,1,0.64499,"Missing"
W18-6401,W10-1703,1,0.498736,"Missing"
W18-6401,W12-3102,1,0.647494,"Missing"
W18-6401,E14-2008,0,0.0242419,"n the WMT18 bitexts provided, including ParaCrawl. Some models employed pretrained word embeddings built on BPE’d corpora (Sennrich et al., 2016). A Marian transformer model performed right-to-left rescoring for this system. The third system is trained with Moses (Koehn et al., 2007), using the same data as the Marian system. Hierarchical reordering and Operation Sequence Model were employed. The 5-gram English language model was trained with KenLM (Heafield, 2011) on the same corpus as the AFRL WMT15 system with the same BPE used in the Marian systems. Lastly, RWTH Jane’s system combination (Freitag et al., 2014) was applied yielding approximately a +0.5 gain in BLEU. 2.3.3 CUNI-KOCMI (Kocmi et al., 2018) 2.3.6 FACEBOOK -FAIR ? (Edunov et al., 2018) FACEBOOK -FAIR is an ensemble of six selfattentional models with back-translation data according to Edunov et al. (2018). Synthetic sources are sampled instead of beam search, oversampling the real bitext at a rate of 16, i.e., each bitext is sampled 16 times more often per epoch than the back-translated data. At inference time, translations which are copies of the source are filtered out, replacing them with the output of a very small news-commentary only"
W18-6401,E17-2058,0,0.0576994,"Missing"
W18-6401,W18-6406,0,0.107817,"t (Junczys-Dowmunt, 2018) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2018) ModernMT, MMT s.r.l. (no associated paper) University of Tartu (Tars and Fishel, 2018) National Institute of Information and Communications Technology (Marie et al., 2018) Northeastern University / NiuTrans Co., Ltd. (Wang et al., 2018b) NLP Group, Nanjing University (no associated paper) NTT Corporation (Morishita et al., 2018) Bo˘gaziçi University (Biçici, 2018) PROMT LLC (Molchanov, 2018) RWTH Aachen (Schamper et al., 2018) RWTH Aachen (Graça et al., 2018) TALP, Technical University of Catalonia (Casas et al., 2018) Tencent (Wang et al., 2018a) Tilde (Pinnis et al., 2018) Ubiqus (no associated paper) University of Cambridge (Stahlberg et al., 2018) University of Edinburgh (Haddow et al., 2018) University of Maryland (Xu and Carpuat, 2018) Unisound (no associated paper) University of Tartu (Del et al., 2018) Table 2: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the commercial and online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion consistent with previous y"
W18-6401,W13-2305,1,0.876146,"017 (English) and news 2011 (Chinese). Subwords (BPE) are used for both English and Chinese sentences. 3 Human Evaluation A human evaluation campaign is run each year to assess translation quality and to determine the final ranking of systems taking part in the competition. This section describes how preparation of evaluation data, collection of human assessments, and computation of the official results of the shared task was carried out this year. Work on evaluation over the past few years has provided fresh insight into ways to collect direct assessments (DA) of machine translation quality (Graham et al., 2013, 2014, 2016), and two years ago the evaluation campaign included parallel assessment of a subset of News task language pairs evaluated with relative ranking (RR) and DA. DA has some clear advantages over RR, namely the evaluation of absolute translation quality and the ability to carry out evaluations through quality controlled crowd-sourcing. As established in 2016 (Bojar et al., 2016a), DA results (via crowd-sourcing) and RR results (produced by researchers) correlate strongly, with Pearson correlation ranging from 0.920 to 0.997 across several source languages into English and at 0.975 for"
W18-6401,E14-1047,1,0.89975,"Missing"
W18-6401,W18-6407,1,0.888041,"ns Co., Ltd. (Wang et al., 2018b) NLP Group, Nanjing University (no associated paper) NTT Corporation (Morishita et al., 2018) Bo˘gaziçi University (Biçici, 2018) PROMT LLC (Molchanov, 2018) RWTH Aachen (Schamper et al., 2018) RWTH Aachen (Graça et al., 2018) TALP, Technical University of Catalonia (Casas et al., 2018) Tencent (Wang et al., 2018a) Tilde (Pinnis et al., 2018) Ubiqus (no associated paper) University of Cambridge (Stahlberg et al., 2018) University of Edinburgh (Haddow et al., 2018) University of Maryland (Xu and Carpuat, 2018) Unisound (no associated paper) University of Tartu (Del et al., 2018) Table 2: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the commercial and online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion consistent with previous years of the workshop. “?” indicates invited participation with a late submission, where the team is not considered a regular participant. 276 and Estonian. The system is based on the Transformer (Vaswani et al., 2017) implementation in OpenNMT-py (Klein et al., 2017). It is trained on filtered pa"
W18-6401,W18-6410,0,0.0609282,"ions, organized into 35 teams are listed in Table 2 and detailed in the rest of this section. Each system did not necessarily appear in all translation tasks. We also included 39 online MT systems (originating from 5 services), which we anonymized as ONLINE -A,B,F,G. For presentation of the results, systems are treated as either constrained or unconstrained, depending on whether their models were trained only on the provided data. Since we do not know how they were built, these online and commercial systems are treated as unconstrained during the automatic and human evaluations. 2.3.1 A ALTO (Grönroos et al., 2018) Aalto participated in the constrained condition of the multi-lingual subtrack, with a single system trained to translate from English to both Finnish 3 http://www.yandex.com/ Estonian Research Council institutional research grant IUT20-56: “Computational models of the Estonian Language” 4 5 As of Fall 2011, the proceedings of the European Parliament are no longer translated into all official languages. 273 Europarl Parallel Corpus German ↔ English Czech ↔ English Finnish ↔ English Estonian ↔ English Sentences 1,920,209 646,605 1,926,114 652,944 Words 50,486,398 53,008,851 14,946,399 17,376,43"
W18-6401,D18-1045,0,0.0609466,"Missing"
W18-6401,W11-2123,0,0.0087119,"t sets. The second is a Marian (Junczys-Dowmunt et al., 2018) system ensembling 5 Univ. Edinburgh “bi-deep” and 6 transformer models all trained on the WMT18 bitexts provided, including ParaCrawl. Some models employed pretrained word embeddings built on BPE’d corpora (Sennrich et al., 2016). A Marian transformer model performed right-to-left rescoring for this system. The third system is trained with Moses (Koehn et al., 2007), using the same data as the Marian system. Hierarchical reordering and Operation Sequence Model were employed. The 5-gram English language model was trained with KenLM (Heafield, 2011) on the same corpus as the AFRL WMT15 system with the same BPE used in the Marian systems. Lastly, RWTH Jane’s system combination (Freitag et al., 2014) was applied yielding approximately a +0.5 gain in BLEU. 2.3.3 CUNI-KOCMI (Kocmi et al., 2018) 2.3.6 FACEBOOK -FAIR ? (Edunov et al., 2018) FACEBOOK -FAIR is an ensemble of six selfattentional models with back-translation data according to Edunov et al. (2018). Synthetic sources are sampled instead of beam search, oversampling the real bitext at a rate of 16, i.e., each bitext is sampled 16 times more often per epoch than the back-translated da"
W18-6401,E17-3017,1,0.773822,"eriment, right-toleft reranking does not help. Another focus is 277 (SMT) submission for the Finnish morphology test suite (Burlot et al., 2018). given to data filtering through rules, translation model and language model including parallel data and monolingual data. The language model is based the Transformer architecture as well. The final system is trained with four different seeds and mixed data. 2.3.8 2.3.9 JHU (Koehn et al., 2018a) The JHU systems are the result of two relatively independent efforts on German–English language directions and Russian–English, using the Marian and Sockeye (Hieber et al., 2017) neural machine translation toolkits, respectively. The novel contributions are iterative back-translation (for German) and fine-tuning on test sets from prior years (for both languages). HY (Raganato et al., 2018; Hurskainen and Tiedemann, 2017) The University of Helsinki (HY) submitted four systems: HY-AH, HY-NMT, HY-NMT-2 STEP and HY-SMT. 2.3.10 JUCBNMT (Mahata et al., 2018) JUCBNMT is an encoder-decoder sequence-tosequence NMT model with character level encoding. The submission uses preprocessing like tokenization, truecasing and corpus cleaning. Both encoder and decoder use a single LSTM"
W18-6401,P17-4012,0,0.0266435,"ted paper) University of Tartu (Del et al., 2018) Table 2: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the commercial and online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion consistent with previous years of the workshop. “?” indicates invited participation with a late submission, where the team is not considered a regular participant. 276 and Estonian. The system is based on the Transformer (Vaswani et al., 2017) implementation in OpenNMT-py (Klein et al., 2017). It is trained on filtered parallel and filtered back-translated monolingual data. The main contribution is a novel cross-lingual Morfessor (Virpioja et al., 2013) segmentation using cognates extracted from the parallel data. The aim is to improve the consistency of the morphological segmentation. Aalto decode using an ensemble of 3 (et) or 8 (fi) models. 2.3.4 2.3.2 2.3.5 AFRL The CUNI-KOCMI submission focuses on the low-resource language neural machine translation (NMT). The final submission uses a method of transfer learning: the model is pretrained on a related high-resource language (her"
W18-6401,W18-6413,0,0.01703,"he constrained systems, however, the data, taking into account its relatively large size, was not factored. T ENCENT (Wang et al., 2018a) T ENCENT- ENSEMBLE (called TenTrans) is an improved NMT system on Transformer based on self-attention mechanism. In addition to the basic settings of Transformer training, T ENCENTENSEMBLE uses multi-model fusion techniques, multiple features reranking, different segmentation models and joint learning. Additionally, data selection strategies were adopted to fine-tune the trained system, achieving a stable performance improvement. An additional system paper (Hu et al., 2018) describes a non-primary submission. 2.3.29 TILDE 2.3.30 U BIQUS The U BIQUS -NMT system is probably developed by the Ubiqus company (www.ubiqus.com). No further information is available. 2.3.31 UCAM (Stahlberg et al., 2018) UCAM is a generalization of previous work (de Gispert et al., 2017) to multiple architectures. It is a system combination of two Transformer-like models, a recurrent model, a convolutional model, and a phrase-based SMT system. The output is probably dominated by the Transformer, and to some extend by the SMT system. (Pinnis et al., 2018) submitted four systems: TILDE - C -"
W18-6401,W18-6416,1,0.791378,"Missing"
W18-6401,W17-4730,0,0.0118952,". HY-SMT (Tiedemann et al., 2016) is the Helsinki SMT system submitted at WMT 2016 (the constrained-basic+back-translated version). The system was not retrained and it may thus suffer from poor lexical coverage on recent test data. The main motivation for including this baseline was to have a statistical machine translation 2.3.13 LMU- NMT (Huck et al., 2018) For the WMT18 news translation shared task, LMU Munich (Huck et al., 2018) has trained ba278 2.3.15 sic shallow attentional encoder-decoder systems (Bahdanau et al., 2014) with the Nematus toolkit (Sennrich et al., 2017), like last year (Huck et al., 2017a). LMU has participated with these NMT systems for the English–German language pair in both translation directions. The training data is a concatenation of Europarl, News Commentary, Common Crawl, and some synthetic data in the form of backtranslated monolingual news texts. The 2017 monolingual News Crawl is not employed, nor are the parallel Rapid and ParaCrawl corpora. The German data is preprocessed with a linguistically informed word segmentation technique (Huck et al., 2017b). By using a linguistically more sound word segmentation, advantages over plain BPE segmentation are expected in t"
W18-6401,W18-6417,1,0.821949,"as workshops and conferences (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016a, 2017). This year we conducted several official tasks. We report in this paper on the news translation task. Additional shared tasks are described in separate papers in these proceedings: • biomedical translation (Neves et al., 2018), • multimodal machine translation (Barrault et al., 2018), • metrics (Ma et al., 2018), • quality estimation (Specia et al., 2018), • automatic post-editing (Chatterjee et al., 2018), and • parallel corpus filtering (Koehn et al., 2018b). In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (“constrained” condition). We held 1 2 http://www.statmt.org/wmt18/ 272 http://statmt.org/wmt18/results.html Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 272–303 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64028 tions are also available for interactive visualization and comparison of diff"
W18-6401,W17-4706,0,0.0132191,". HY-SMT (Tiedemann et al., 2016) is the Helsinki SMT system submitted at WMT 2016 (the constrained-basic+back-translated version). The system was not retrained and it may thus suffer from poor lexical coverage on recent test data. The main motivation for including this baseline was to have a statistical machine translation 2.3.13 LMU- NMT (Huck et al., 2018) For the WMT18 news translation shared task, LMU Munich (Huck et al., 2018) has trained ba278 2.3.15 sic shallow attentional encoder-decoder systems (Bahdanau et al., 2014) with the Nematus toolkit (Sennrich et al., 2017), like last year (Huck et al., 2017a). LMU has participated with these NMT systems for the English–German language pair in both translation directions. The training data is a concatenation of Europarl, News Commentary, Common Crawl, and some synthetic data in the form of backtranslated monolingual news texts. The 2017 monolingual News Crawl is not employed, nor are the parallel Rapid and ParaCrawl corpora. The German data is preprocessed with a linguistically informed word segmentation technique (Huck et al., 2017b). By using a linguistically more sound word segmentation, advantages over plain BPE segmentation are expected in t"
W18-6401,W18-6430,0,0.299738,"of Helsinki (Raganato et al., 2018) Johns Hopkins University (Koehn et al., 2018a) Jadavpur University (Mahata et al., 2018) Karlsruhe Institute of Technology (Pham et al., 2018) Li Muze (no associated paper) LMU Munich (Huck et al., 2018) LMU Munich (Stojanovski et al., 2018) Microsoft (Junczys-Dowmunt, 2018) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2018) ModernMT, MMT s.r.l. (no associated paper) University of Tartu (Tars and Fishel, 2018) National Institute of Information and Communications Technology (Marie et al., 2018) Northeastern University / NiuTrans Co., Ltd. (Wang et al., 2018b) NLP Group, Nanjing University (no associated paper) NTT Corporation (Morishita et al., 2018) Bo˘gaziçi University (Biçici, 2018) PROMT LLC (Molchanov, 2018) RWTH Aachen (Schamper et al., 2018) RWTH Aachen (Graça et al., 2018) TALP, Technical University of Catalonia (Casas et al., 2018) Tencent (Wang et al., 2018a) Tilde (Pinnis et al., 2018) Ubiqus (no associated paper) University of Cambridge (Stahlberg et al., 2018) University of Edinburgh (Haddow et al., 2018) University of Maryland (Xu and Carpuat, 2018) Unisound (no associated paper) University of Tartu (Del et al., 2018) Table 2: Part"
W18-6401,W18-6427,0,0.0533796,"Missing"
W18-6401,W18-6428,0,0.0966755,"D U NISOUND U NSUP TARTU Institution Aalto University (Grönroos et al., 2018) Air Force Research Laboratory (Gwinnup et al., 2018) Alibaba Group (Deng et al., 2018) Charles University (Kocmi et al., 2018) Charles University (Popel, 2018) Facebook AI Research (Edunov et al., 2018) Global Tone Communication Technology (Bei et al., 2018) University of Helsinki (Raganato et al., 2018) Johns Hopkins University (Koehn et al., 2018a) Jadavpur University (Mahata et al., 2018) Karlsruhe Institute of Technology (Pham et al., 2018) Li Muze (no associated paper) LMU Munich (Huck et al., 2018) LMU Munich (Stojanovski et al., 2018) Microsoft (Junczys-Dowmunt, 2018) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2018) ModernMT, MMT s.r.l. (no associated paper) University of Tartu (Tars and Fishel, 2018) National Institute of Information and Communications Technology (Marie et al., 2018) Northeastern University / NiuTrans Co., Ltd. (Wang et al., 2018b) NLP Group, Nanjing University (no associated paper) NTT Corporation (Morishita et al., 2018) Bo˘gaziçi University (Biçici, 2018) PROMT LLC (Molchanov, 2018) RWTH Aachen (Schamper et al., 2018) RWTH Aachen (Graça et al., 2018) TALP, Technical University of Ca"
W18-6401,W18-6431,0,0.0938702,"nications Technology (Marie et al., 2018) Northeastern University / NiuTrans Co., Ltd. (Wang et al., 2018b) NLP Group, Nanjing University (no associated paper) NTT Corporation (Morishita et al., 2018) Bo˘gaziçi University (Biçici, 2018) PROMT LLC (Molchanov, 2018) RWTH Aachen (Schamper et al., 2018) RWTH Aachen (Graça et al., 2018) TALP, Technical University of Catalonia (Casas et al., 2018) Tencent (Wang et al., 2018a) Tilde (Pinnis et al., 2018) Ubiqus (no associated paper) University of Cambridge (Stahlberg et al., 2018) University of Edinburgh (Haddow et al., 2018) University of Maryland (Xu and Carpuat, 2018) Unisound (no associated paper) University of Tartu (Del et al., 2018) Table 2: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the commercial and online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion consistent with previous years of the workshop. “?” indicates invited participation with a late submission, where the team is not considered a regular participant. 276 and Estonian. The system is based on the Transformer (Vaswani et al., 2017) implement"
W18-6401,W16-2326,0,0.0278975,"Missing"
W18-6417,W17-4724,1,0.846102,"Missing"
W18-6417,W18-1820,0,0.0303837,"Missing"
W18-6417,P16-1009,0,0.044916,"lish–German), but fell short against this year’s best performing systems (45.3 vs. 48.4 (-3.1) and 43.4 vs. 48.3 (-4.9), respectively)1 . The best models this year used the Transformer model instead of the recurrent neural networks that our models are based on. Our novel contributions are iterative back-translation and fine-tuning on prior test sets. For Russian–English, we carried out extensive hyperparameter search, with different numbers of layers, embedding and hidden state sizes, and drop-out settings. 2 2.1 We started with shallow systems similar to Edinburgh’s submission two years ago (Sennrich et al., 2016a). It uses byte pair encoding with a vocabulary of 50,000 (Sennrich et al., 2016c) and backtranslation of the news2016 monolingual corpus (Sennrich et al., 2016b), about twice the size of the original training data. For each training run, we compare different ways to obtain a single best model. • Use the single model that performed best on the dev set (newstest2016). German–English and English–German The systems for the German–English language pairs were developed with the Marian toolkit (Junczys-Dowmunt et al., 2018). We developed models with both shallow and deep architectures, based on rec"
W18-6417,P16-1162,0,0.0980537,"lish–German), but fell short against this year’s best performing systems (45.3 vs. 48.4 (-3.1) and 43.4 vs. 48.3 (-4.9), respectively)1 . The best models this year used the Transformer model instead of the recurrent neural networks that our models are based on. Our novel contributions are iterative back-translation and fine-tuning on prior test sets. For Russian–English, we carried out extensive hyperparameter search, with different numbers of layers, embedding and hidden state sizes, and drop-out settings. 2 2.1 We started with shallow systems similar to Edinburgh’s submission two years ago (Sennrich et al., 2016a). It uses byte pair encoding with a vocabulary of 50,000 (Sennrich et al., 2016c) and backtranslation of the news2016 monolingual corpus (Sennrich et al., 2016b), about twice the size of the original training data. For each training run, we compare different ways to obtain a single best model. • Use the single model that performed best on the dev set (newstest2016). German–English and English–German The systems for the German–English language pairs were developed with the Marian toolkit (Junczys-Dowmunt et al., 2018). We developed models with both shallow and deep architectures, based on rec"
W18-6417,W18-2703,1,0.880545,"Missing"
W18-6417,P18-4020,0,0.0578053,"Missing"
W18-6417,N16-1046,0,0.0700336,"Missing"
W18-6417,2015.iwslt-evaluation.11,0,0.236565,"Missing"
W18-6417,W17-4739,0,0.0416451,"Missing"
W18-6417,W16-2323,0,0.0284616,"lish–German), but fell short against this year’s best performing systems (45.3 vs. 48.4 (-3.1) and 43.4 vs. 48.3 (-4.9), respectively)1 . The best models this year used the Transformer model instead of the recurrent neural networks that our models are based on. Our novel contributions are iterative back-translation and fine-tuning on prior test sets. For Russian–English, we carried out extensive hyperparameter search, with different numbers of layers, embedding and hidden state sizes, and drop-out settings. 2 2.1 We started with shallow systems similar to Edinburgh’s submission two years ago (Sennrich et al., 2016a). It uses byte pair encoding with a vocabulary of 50,000 (Sennrich et al., 2016c) and backtranslation of the news2016 monolingual corpus (Sennrich et al., 2016b), about twice the size of the original training data. For each training run, we compare different ways to obtain a single best model. • Use the single model that performed best on the dev set (newstest2016). German–English and English–German The systems for the German–English language pairs were developed with the Marian toolkit (Junczys-Dowmunt et al., 2018). We developed models with both shallow and deep architectures, based on rec"
W18-6453,W18-6475,0,0.0511142,"Missing"
W18-6453,W11-1218,0,0.0394428,"ine translation, but more recent work targets neural models. Carpuat et al. (2017) focus on identifying semantic differences in translation pairs using cross-lingual textual entailment and additional length-based features, and demonstrates that removing such sentences improves neural machine translation performance. As Rarrick et al. (2011) point out, one type of noise in parallel corpora extracted from the web are translations that have been created by machine translation. Venugopal et al. (2011) propose a method to watermark the output of machine translation systems to aid this distinction. Antonova and Misyurev (2011) report that rule-based machine translation output can be detected due to certain word choices, and statistical machine translation output can be detected due to lack of reordering. Belinkov and Bisk (2017) investigate the impact of noise on neural machine translation. They focus on creating systems that can translate the kinds of orthographic errors (typos, misspellings, etc.) that humans can comprehend. In contrast, Khayrallah and Koehn (2018) address noisy training data and focus on types of noise occurring in web-crawled corpora. They carried out a study how noise that occurs in crawled pa"
W18-6453,W18-6476,0,0.042088,"Missing"
W18-6453,W18-6477,0,0.0952092,"Missing"
W18-6453,W18-6472,0,0.0451806,"Missing"
W18-6453,W18-6478,0,0.0570614,"Missing"
W18-6453,D11-1033,0,0.165705,"investigate the impact of noise on neural machine translation. They focus on creating systems that can translate the kinds of orthographic errors (typos, misspellings, etc.) that humans can comprehend. In contrast, Khayrallah and Koehn (2018) address noisy training data and focus on types of noise occurring in web-crawled corpora. They carried out a study how noise that occurs in crawled parallel text impacts statistical and neural machine translation. There is a rich literature on data selection which aims at sub-sampling parallel data relevant for a task-specific machine translation system (Axelrod et al., 2011). van der Wees et al. (2017) find that the existing data selection methods developed for statistical machine translation are less effective for neural machine translation. This is different from our goals of handling noise since those methods tend to discard perfectly fine sentence pairs (say, about cooking recipes) that are just not relevant for the targeted domain (say, software manuals). Our task is focused on data quality that is relevant for all domains. 4 https://drive.google.com/drive/folders/ 1zZNPlAThm-Rnvxsy8rXzChC49bc0 TGO 727 Type of Noise Okay Misaligned sentences Third language B"
W18-6453,P18-4020,1,0.833453,"Missing"
W18-6453,W18-6473,0,0.0626345,"Missing"
W18-6453,W18-2709,1,0.900845,"and adherence to sentence-bysentence correspondences. The other extreme are sentence pairs extracted with fully automatic processes from indiscriminate crawling of the World Wide Web. The Shared Task on Parallel Corpus Filtering targets the second extreme, although the methods developed for this data condition should also carry over to less noisy parallel corpora. In setting this task, we were motivated by our ongoing efforts to create large publicly available parallel corpora from web sources and the recognition that noisy parallel data is especially a concern for neural machine translation (Khayrallah and Koehn, 2018). This paper gives an overview of the task, presents its results and provides some analysis. 2 Related Work 1 http://opus.lingfil.uu.se/ http://www.paracrawl.eu/ 3 NLP4TM 2016: Shared task http://rgcl.wlv.ac.uk/nlp4tm2016/shared-task/ Although the idea of crawling the web indiscriminately for parallel data goes back to the 20th century (Resnik, 1999), work in the academic com2 726 Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 726–739 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https:"
W18-6453,W18-6474,0,0.0714008,"Missing"
W18-6453,W18-6479,1,0.790029,"Missing"
W18-6453,2005.mtsummit-papers.11,1,0.26682,"Missing"
W18-6453,W16-2347,1,0.895759,"Missing"
W18-6453,P07-2045,1,0.0147689,"Missing"
W18-6453,W17-3209,0,0.0604803,"ed corpora, and evaluation translation quality on blind test sets using the BLEU score. For development purposes, we released configuration files and scripts that mirror the official testing procedure with a development test set. The development pack consists of • a script to subsample corpora based on quality scores • a Moses configuration file to train and test a statistical machine translation system • Marian scripts to train and test a neural machine translation system Most of this work was done in the context of statistical machine translation, but more recent work targets neural models. Carpuat et al. (2017) focus on identifying semantic differences in translation pairs using cross-lingual textual entailment and additional length-based features, and demonstrates that removing such sentences improves neural machine translation performance. As Rarrick et al. (2011) point out, one type of noise in parallel corpora extracted from the web are translations that have been created by machine translation. Venugopal et al. (2011) propose a method to watermark the output of machine translation systems to aid this distinction. Antonova and Misyurev (2011) report that rule-based machine translation output can"
W18-6453,P13-2061,0,0.0539603,"ea of crawling the web indiscriminately for parallel data goes back to the 20th century (Resnik, 1999), work in the academic com2 726 Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 726–739 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64080 3 to filter a parallel corpus; Xu and Koehn (2017) generate synthetic noisy data (inadequate and nonfluent translations) and use this data to train a classifier to identify good sentence pairs from a noisy corpus; and Cui et al. (2013) use a graph-based random walk algorithm and extract phrase pair scores to weight the phrase translation probabilities to bias towards more trustworthy ones. Task The shared task tackled the problem of filtering parallel corpora. Given a noisy parallel corpus (crawled from the web), participants developed methods to filter it to a smaller size of high quality sentence pairs. Specifically, we provided a very noisy 1 billion word (English token count) German–English corpus crawled from the web by the Paracrawl project. We asked participants to subselect sentence pairs that amount to (a) 10 milli"
W18-6453,W18-6480,0,0.0668994,"Missing"
W18-6453,skadins-etal-2014-billions,0,0.0402284,"Missing"
W18-6453,W18-6481,0,0.0862833,"Missing"
W18-6453,W18-6488,0,0.0644507,"Missing"
W18-6453,W18-6482,0,0.0428675,"Missing"
W18-6453,2011.eamt-1.25,0,0.413479,"Missing"
W18-6453,W18-6483,0,0.069782,"Missing"
W18-6453,2011.mtsummit-papers.47,0,0.73575,"Missing"
W18-6453,W18-6484,0,0.0645499,"Missing"
W18-6453,tiedemann-2012-parallel,0,0.0910955,"It contains news stories that were either translated from German to English or from English to German. NEWSTEST 2018 IWSLT 2017 ACQUIS IWSLT 2017 The test set from the IWSLT 2017 evaluation campaign. It consists of transcripts of talks given at the TED conference. They cover generally accessible topics in the area of technology, entertainment, and design. EMEA GLOBALVOICES KDE ate the machine translation systems trained on the subsampled data sets. Word counts are obtained with wc on untokenized text. This test set was extracted from the Acquis Communtaire corpus, which is available on OPUS7 (Tiedemann, 2012) (which was the source to create the subsequent 3 test sets). The test set consists of laws of the European Union that have to be incorporated into the national laws of the EU member countries. We only used sentences with 15 to 80 words, and removed any duplicate sentence pairs. 5 Evaluation Protocol The testing setup mirrors the development environment that we provided to the participants. 5.1 Particpants We received submissions from 17 different organizations. See Table 3 for the complete list of participants. The participant’s organizations are quite diverse, with 3 participants from Spain,"
W18-6453,W18-6485,0,0.0339437,"Missing"
W18-6453,D17-1147,0,0.150155,"Missing"
W18-6453,W18-6486,0,0.0808535,"Missing"
W18-6453,2009.mtsummit-posters.15,0,0.47964,"Missing"
W18-6453,2011.mtsummit-papers.48,0,0.14183,"of • a script to subsample corpora based on quality scores • a Moses configuration file to train and test a statistical machine translation system • Marian scripts to train and test a neural machine translation system Most of this work was done in the context of statistical machine translation, but more recent work targets neural models. Carpuat et al. (2017) focus on identifying semantic differences in translation pairs using cross-lingual textual entailment and additional length-based features, and demonstrates that removing such sentences improves neural machine translation performance. As Rarrick et al. (2011) point out, one type of noise in parallel corpora extracted from the web are translations that have been created by machine translation. Venugopal et al. (2011) propose a method to watermark the output of machine translation systems to aid this distinction. Antonova and Misyurev (2011) report that rule-based machine translation output can be detected due to certain word choices, and statistical machine translation output can be detected due to lack of reordering. Belinkov and Bisk (2017) investigate the impact of noise on neural machine translation. They focus on creating systems that can tran"
W18-6453,P99-1068,0,0.254476,"In setting this task, we were motivated by our ongoing efforts to create large publicly available parallel corpora from web sources and the recognition that noisy parallel data is especially a concern for neural machine translation (Khayrallah and Koehn, 2018). This paper gives an overview of the task, presents its results and provides some analysis. 2 Related Work 1 http://opus.lingfil.uu.se/ http://www.paracrawl.eu/ 3 NLP4TM 2016: Shared task http://rgcl.wlv.ac.uk/nlp4tm2016/shared-task/ Although the idea of crawling the web indiscriminately for parallel data goes back to the 20th century (Resnik, 1999), work in the academic com2 726 Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 726–739 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64080 3 to filter a parallel corpus; Xu and Koehn (2017) generate synthetic noisy data (inadequate and nonfluent translations) and use this data to train a classifier to identify good sentence pairs from a noisy corpus; and Cui et al. (2013) use a graph-based random walk algorithm and extract phrase pair scores to weight the"
W18-6453,D11-1126,0,0.109697,"ripts to train and test a neural machine translation system Most of this work was done in the context of statistical machine translation, but more recent work targets neural models. Carpuat et al. (2017) focus on identifying semantic differences in translation pairs using cross-lingual textual entailment and additional length-based features, and demonstrates that removing such sentences improves neural machine translation performance. As Rarrick et al. (2011) point out, one type of noise in parallel corpora extracted from the web are translations that have been created by machine translation. Venugopal et al. (2011) propose a method to watermark the output of machine translation systems to aid this distinction. Antonova and Misyurev (2011) report that rule-based machine translation output can be detected due to certain word choices, and statistical machine translation output can be detected due to lack of reordering. Belinkov and Bisk (2017) investigate the impact of noise on neural machine translation. They focus on creating systems that can translate the kinds of orthographic errors (typos, misspellings, etc.) that humans can comprehend. In contrast, Khayrallah and Koehn (2018) address noisy training d"
W18-6453,W18-6487,0,0.0438357,"Missing"
W18-6453,W18-6489,0,0.0366714,"Missing"
W18-6453,D17-1319,1,0.672175,"s and provides some analysis. 2 Related Work 1 http://opus.lingfil.uu.se/ http://www.paracrawl.eu/ 3 NLP4TM 2016: Shared task http://rgcl.wlv.ac.uk/nlp4tm2016/shared-task/ Although the idea of crawling the web indiscriminately for parallel data goes back to the 20th century (Resnik, 1999), work in the academic com2 726 Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 726–739 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64080 3 to filter a parallel corpus; Xu and Koehn (2017) generate synthetic noisy data (inadequate and nonfluent translations) and use this data to train a classifier to identify good sentence pairs from a noisy corpus; and Cui et al. (2013) use a graph-based random walk algorithm and extract phrase pair scores to weight the phrase translation probabilities to bias towards more trustworthy ones. Task The shared task tackled the problem of filtering parallel corpora. Given a noisy parallel corpus (crawled from the web), participants developed methods to filter it to a smaller size of high quality sentence pairs. Specifically, we provided a very nois"
W18-6479,2015.iwslt-papers.9,0,0.0367467,"riment with using the noisy corpus to be filtered to compute the combination weights, and thus avoids generating synthetic data as in standard Zipporah. 1 Related Work Zipporah builds upon prior work in data cleaning and data selection. For data selection, work has focused on selecting a subset of data based on domain-matching. Moore and Lewis (2010) computed cross-entropy between in-domain and out-of-domain language models to select data for training domain-relevant language models. XenC (Rousseau, 2013), an open-source tool, also selects data based on crossentropy scores on language models. Axelrod et al. (2015) utilized part-of-speech tags and used a class-based n-gram language model for selecting in-domain data and Duh et al. (2013) used a neural network based language model trained on a small in-domain corpus to select from a larger mixeddomain data pool. L¨u et al. (2007) redistributed different weights for sentence pairs/predefined sub-models. Shah and Specia (2014) described experiments on quality estimation which, given a source sentence, select the best translation among several options. For data cleaning, work has focused on removing noisy data. Taghipour et al. (2011) proposed an outlier de"
W18-6479,P13-2061,0,0.164286,"and Duh et al. (2013) used a neural network based language model trained on a small in-domain corpus to select from a larger mixeddomain data pool. L¨u et al. (2007) redistributed different weights for sentence pairs/predefined sub-models. Shah and Specia (2014) described experiments on quality estimation which, given a source sentence, select the best translation among several options. For data cleaning, work has focused on removing noisy data. Taghipour et al. (2011) proposed an outlier detection algorithm which leads to an improved translation quality when trimming a small portion of data. Cui et al. (2013) used a graph-based random walk algorithm to do bilingual data cleaning. BiTextor (Espl´a-Gomis and Forcada, 2009) utilizes sentence alignment scores and source URL information to filter out bad URL pairs and selects good sentence pairs. Similar to this work, the qe-clean system (Denkowski et al., 2012; Dyer et al., 2010; Heafield, 2011) uses word alignments and language models to select sentence pairs that are likely to be good translations of one another. We focus on data cleaning for all purposes, as opposed to data selection for a given domain. We Introduction Todays machine translation sy"
W18-6479,W12-3131,0,0.0213234,"timation which, given a source sentence, select the best translation among several options. For data cleaning, work has focused on removing noisy data. Taghipour et al. (2011) proposed an outlier detection algorithm which leads to an improved translation quality when trimming a small portion of data. Cui et al. (2013) used a graph-based random walk algorithm to do bilingual data cleaning. BiTextor (Espl´a-Gomis and Forcada, 2009) utilizes sentence alignment scores and source URL information to filter out bad URL pairs and selects good sentence pairs. Similar to this work, the qe-clean system (Denkowski et al., 2012; Dyer et al., 2010; Heafield, 2011) uses word alignments and language models to select sentence pairs that are likely to be good translations of one another. We focus on data cleaning for all purposes, as opposed to data selection for a given domain. We Introduction Todays machine translation systems require large amounts of training data in form of sentences paired with their translation, which are often compiled from online sources. This has not changed fundamentally with the move from statistical machine translation to neural machine translation, also we observed that neural models require"
W18-6479,2014.eamt-1.22,0,0.0479975,"ss-entropy between in-domain and out-of-domain language models to select data for training domain-relevant language models. XenC (Rousseau, 2013), an open-source tool, also selects data based on crossentropy scores on language models. Axelrod et al. (2015) utilized part-of-speech tags and used a class-based n-gram language model for selecting in-domain data and Duh et al. (2013) used a neural network based language model trained on a small in-domain corpus to select from a larger mixeddomain data pool. L¨u et al. (2007) redistributed different weights for sentence pairs/predefined sub-models. Shah and Specia (2014) described experiments on quality estimation which, given a source sentence, select the best translation among several options. For data cleaning, work has focused on removing noisy data. Taghipour et al. (2011) proposed an outlier detection algorithm which leads to an improved translation quality when trimming a small portion of data. Cui et al. (2013) used a graph-based random walk algorithm to do bilingual data cleaning. BiTextor (Espl´a-Gomis and Forcada, 2009) utilizes sentence alignment scores and source URL information to filter out bad URL pairs and selects good sentence pairs. Similar"
W18-6479,P13-2119,0,0.025891,"s in standard Zipporah. 1 Related Work Zipporah builds upon prior work in data cleaning and data selection. For data selection, work has focused on selecting a subset of data based on domain-matching. Moore and Lewis (2010) computed cross-entropy between in-domain and out-of-domain language models to select data for training domain-relevant language models. XenC (Rousseau, 2013), an open-source tool, also selects data based on crossentropy scores on language models. Axelrod et al. (2015) utilized part-of-speech tags and used a class-based n-gram language model for selecting in-domain data and Duh et al. (2013) used a neural network based language model trained on a small in-domain corpus to select from a larger mixeddomain data pool. L¨u et al. (2007) redistributed different weights for sentence pairs/predefined sub-models. Shah and Specia (2014) described experiments on quality estimation which, given a source sentence, select the best translation among several options. For data cleaning, work has focused on removing noisy data. Taghipour et al. (2011) proposed an outlier detection algorithm which leads to an improved translation quality when trimming a small portion of data. Cui et al. (2013) use"
W18-6479,2011.mtsummit-papers.47,0,0.423855,"ores on language models. Axelrod et al. (2015) utilized part-of-speech tags and used a class-based n-gram language model for selecting in-domain data and Duh et al. (2013) used a neural network based language model trained on a small in-domain corpus to select from a larger mixeddomain data pool. L¨u et al. (2007) redistributed different weights for sentence pairs/predefined sub-models. Shah and Specia (2014) described experiments on quality estimation which, given a source sentence, select the best translation among several options. For data cleaning, work has focused on removing noisy data. Taghipour et al. (2011) proposed an outlier detection algorithm which leads to an improved translation quality when trimming a small portion of data. Cui et al. (2013) used a graph-based random walk algorithm to do bilingual data cleaning. BiTextor (Espl´a-Gomis and Forcada, 2009) utilizes sentence alignment scores and source URL information to filter out bad URL pairs and selects good sentence pairs. Similar to this work, the qe-clean system (Denkowski et al., 2012; Dyer et al., 2010; Heafield, 2011) uses word alignments and language models to select sentence pairs that are likely to be good translations of one ano"
W18-6479,P10-4002,0,0.0404024,"source sentence, select the best translation among several options. For data cleaning, work has focused on removing noisy data. Taghipour et al. (2011) proposed an outlier detection algorithm which leads to an improved translation quality when trimming a small portion of data. Cui et al. (2013) used a graph-based random walk algorithm to do bilingual data cleaning. BiTextor (Espl´a-Gomis and Forcada, 2009) utilizes sentence alignment scores and source URL information to filter out bad URL pairs and selects good sentence pairs. Similar to this work, the qe-clean system (Denkowski et al., 2012; Dyer et al., 2010; Heafield, 2011) uses word alignments and language models to select sentence pairs that are likely to be good translations of one another. We focus on data cleaning for all purposes, as opposed to data selection for a given domain. We Introduction Todays machine translation systems require large amounts of training data in form of sentences paired with their translation, which are often compiled from online sources. This has not changed fundamentally with the move from statistical machine translation to neural machine translation, also we observed that neural models require more training data"
W18-6479,D17-1319,1,0.829853,"es. This has not changed fundamentally with the move from statistical machine translation to neural machine translation, also we observed that neural models require more training data (Koehn and Knowles, 2017) and are more sensitive to noise (Khayrallah and Koehn, 2018). Thus both the acquisition of more training data such as indiscriminate web crawling and corpus filtering will have large impact on the quality of state-of-the-art machine translation systems. The JHU submission to the WMT18 Parallel Corpus Filtering shared task uses a modified version of the Zipporah Corpus Filtering toolkit (Xu and Koehn, 2017). For a sentence pair, Zipporah uses a bag-of-words model to generate an adequacy score, and an n-gram language model to generate fluency score. The two scores are combined based on weights trained in order to separate clean data from noisy data. The original version of Zipporah generates artificial noisy training data to train such classifier, in this submission we also treat the Paracrawl corpus as the negative examples. 896 Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 896–899 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 A"
W18-6479,2009.mtsummit-btm.6,0,0.0682437,"Missing"
W18-6479,W11-2123,0,0.0327664,"lect the best translation among several options. For data cleaning, work has focused on removing noisy data. Taghipour et al. (2011) proposed an outlier detection algorithm which leads to an improved translation quality when trimming a small portion of data. Cui et al. (2013) used a graph-based random walk algorithm to do bilingual data cleaning. BiTextor (Espl´a-Gomis and Forcada, 2009) utilizes sentence alignment scores and source URL information to filter out bad URL pairs and selects good sentence pairs. Similar to this work, the qe-clean system (Denkowski et al., 2012; Dyer et al., 2010; Heafield, 2011) uses word alignments and language models to select sentence pairs that are likely to be good translations of one another. We focus on data cleaning for all purposes, as opposed to data selection for a given domain. We Introduction Todays machine translation systems require large amounts of training data in form of sentences paired with their translation, which are often compiled from online sources. This has not changed fundamentally with the move from statistical machine translation to neural machine translation, also we observed that neural models require more training data (Koehn and Knowl"
W18-6479,W18-2709,1,0.92511,"o select sentence pairs that are likely to be good translations of one another. We focus on data cleaning for all purposes, as opposed to data selection for a given domain. We Introduction Todays machine translation systems require large amounts of training data in form of sentences paired with their translation, which are often compiled from online sources. This has not changed fundamentally with the move from statistical machine translation to neural machine translation, also we observed that neural models require more training data (Koehn and Knowles, 2017) and are more sensitive to noise (Khayrallah and Koehn, 2018). Thus both the acquisition of more training data such as indiscriminate web crawling and corpus filtering will have large impact on the quality of state-of-the-art machine translation systems. The JHU submission to the WMT18 Parallel Corpus Filtering shared task uses a modified version of the Zipporah Corpus Filtering toolkit (Xu and Koehn, 2017). For a sentence pair, Zipporah uses a bag-of-words model to generate an adequacy score, and an n-gram language model to generate fluency score. The two scores are combined based on weights trained in order to separate clean data from noisy data. The"
W18-6479,W17-3204,1,0.817968,"Heafield, 2011) uses word alignments and language models to select sentence pairs that are likely to be good translations of one another. We focus on data cleaning for all purposes, as opposed to data selection for a given domain. We Introduction Todays machine translation systems require large amounts of training data in form of sentences paired with their translation, which are often compiled from online sources. This has not changed fundamentally with the move from statistical machine translation to neural machine translation, also we observed that neural models require more training data (Koehn and Knowles, 2017) and are more sensitive to noise (Khayrallah and Koehn, 2018). Thus both the acquisition of more training data such as indiscriminate web crawling and corpus filtering will have large impact on the quality of state-of-the-art machine translation systems. The JHU submission to the WMT18 Parallel Corpus Filtering shared task uses a modified version of the Zipporah Corpus Filtering toolkit (Xu and Koehn, 2017). For a sentence pair, Zipporah uses a bag-of-words model to generate an adequacy score, and an n-gram language model to generate fluency score. The two scores are combined based on weights"
W18-6479,D07-1036,0,0.114573,"Missing"
W18-6479,P10-2041,0,0.0430065,"ghtly modified version of the Zipporah Corpus Filtering toolkit (Xu and Koehn, 2017), which computes an adequacy score and a fluency score on a sentence pair, and use a weighted sum of the scores as the selection criteria. This work differs from Zipporah in that we experiment with using the noisy corpus to be filtered to compute the combination weights, and thus avoids generating synthetic data as in standard Zipporah. 1 Related Work Zipporah builds upon prior work in data cleaning and data selection. For data selection, work has focused on selecting a subset of data based on domain-matching. Moore and Lewis (2010) computed cross-entropy between in-domain and out-of-domain language models to select data for training domain-relevant language models. XenC (Rousseau, 2013), an open-source tool, also selects data based on crossentropy scores on language models. Axelrod et al. (2015) utilized part-of-speech tags and used a class-based n-gram language model for selecting in-domain data and Duh et al. (2013) used a neural network based language model trained on a small in-domain corpus to select from a larger mixeddomain data pool. L¨u et al. (2007) redistributed different weights for sentence pairs/predefined"
W19-1501,P16-1139,0,0.0196668,", which operates by analyzing the single-instance model definition and deterministically convert the operations into their minibatched counterparts. While such mechanism eliminated the need to traverse the whole computation graph, it cannot homogenize the operations in each branch of if. Instead, it needs to perform each operation separately and apply masking on the result, while our method does not require any masking. Unfortunately we are also not able to compare with the toolkit at the time of this work as it lacks support for several operations we need. Similar to the spirit of our work, Bowman et al. (2016) attempted to parallelize StackLSTM by using Thin-stack, a data structure that reduces the space complexity by storing all the intermediate stack top elements in a tensor and use a queue to control element access. However, thanks to PyTorch, our implementation is not directly dependent on the notion of Thin-stack. Instead, when an element is popped from the stack, we simply shift the stack top pointer and potentially re-write the corresponding sub-tensor later. In other words, there is no need for us to directly maintain all the intermediate stack top elements, because in PyTorch, when the ele"
W19-1501,D14-1082,0,0.138247,"Missing"
W19-1501,P15-1033,0,0.29396,"nd string-to-tree neural machine translation, but it is also known to be notoriously difficult to parallelize for GPU training due to the fact that the computations are dependent on discrete operations. In this paper, we tackle this problem by utilizing state access patterns of StackLSTM to homogenize computations with regard to different discrete operations. Our parsing experiments show that the method scales up almost linearly with increasing batch size, and our parallelized PyTorch implementation trains significantly faster compared to the Dynet C++ implementation. 1 2 StackLSTM StackLSTM (Dyer et al., 2015) is an LSTM architecture (Hochreiter and Schmidhuber, 1997) augmented with a stack H that stores some of the hidden states built in the past. Unlike traditional LSTMs that always build state ht from ht−1 , the states of StackLSTM are built from the head of the state stack H, maintained by a stack top pointer p(H). At each time step, StackLSTM takes a realvalued input vector together with an additional discrete operation on the stack, which determines what computation needs to be conducted and how the stack top pointer should be updated. Throughout this section, we index the input vector (e.g."
W19-1501,N16-1024,0,0.0612295,"Missing"
W19-1501,P17-2012,0,0.0213941,"t it is fed into the network, and hidden states in the stack hj using their position j in the stack H, j being defined as the 0-base index starting from the stack bottom. The set of input discrete actions typically contains at least Push and Pop operations. When these operations are taken as input, the corresponding computations on the StackLSTM are listed below:1 Introduction Tree-structured representation of language has been successfully applied to various applications including dependency parsing (Dyer et al., 2015), sentiment analysis (Socher et al., 2011) and neural machine translation (Eriguchi et al., 2017). However, most of the neural network architectures used to build tree-structured representations are not able to exploit full parallelism of GPUs by minibatched training, as the computation that happens for each instance is conditioned on the input/output structures, and hence cannot be naïvely grouped together as a batch. This lack of parallelism is one of the major hurdles that prevent these representations from wider adoption practically (e.g., neural machine translation), as many natural language processing tasks currently require the ability to scale up to very large training corpora in"
W19-1501,P82-1020,0,0.787814,"Missing"
W19-1501,J17-2002,0,0.0193293,"ort-Term Memory Shuoyang Ding Philipp Koehn Center for Language and Speech Processing Johns Hopkins University {dings, phi}@jhu.edu Abstract successfully applied to dependency parsing (Dyer et al., 2015, 2016; Ballesteros et al., 2017) and syntax-aware neural machine translation (Eriguchi et al., 2017) in the previous research literature, but none of these research results were produced with minibatched training. We show that our parallelization scheme is feasible in practice by showing that it scales up near-linearly with increasing batch size, while reproducing a set of results reported in (Ballesteros et al., 2017). Stack Long Short-Term Memory (StackLSTM) is useful for various applications such as parsing and string-to-tree neural machine translation, but it is also known to be notoriously difficult to parallelize for GPU training due to the fact that the computations are dependent on discrete operations. In this paper, we tackle this problem by utilizing state access patterns of StackLSTM to homogenize computations with regard to different discrete operations. Our parsing experiments show that the method scales up almost linearly with increasing batch size, and our parallelized PyTorch implementation"
W19-1501,Q16-1023,0,0.0547665,"Missing"
W19-1501,N03-1033,0,0.103076,"Missing"
W19-1501,P11-1068,0,0.0321465,"that any read operation on these states will happen after another overwrite. This allows us to do the extra computation anyway when Pop operation is fed, because the extra computation, especially updating hp(H)+1 , will not harm the validity of the hidden states at any time step. Algorithm 1 gives the final forward computation for the Parallelizable StackLSTM. Note that this algorithm does not contain any if-statements that depends on stack operations and hence is homogeneous when grouped into batches that are consisted of multiple operations trajectories. In transition systems (Nivre, 2008; Kuhlmann et al., 2011) used in real tasks (e.g., transition-based parsing) as shown in Table 1, it should be noted that more than push and pop operations are needed for the StackLSTM. Fortunately, for Arc-Eager and Parallelizable StackLSTM Continuing the formulation in the previous section, we will start by discussing our proposed solution under the case where the set of discrete actions contains only Push and Pop operations; we then move on to discussion of the applicability of our proposed solution to the transition systems that are used for building representations for dependency trees. The first modification we"
W19-1501,de-marneffe-etal-2006-generating,0,0.181302,"Missing"
W19-1501,J08-4003,0,0.0630166,"e it is known that any read operation on these states will happen after another overwrite. This allows us to do the extra computation anyway when Pop operation is fed, because the extra computation, especially updating hp(H)+1 , will not harm the validity of the hidden states at any time step. Algorithm 1 gives the final forward computation for the Parallelizable StackLSTM. Note that this algorithm does not contain any if-statements that depends on stack operations and hence is homogeneous when grouped into batches that are consisted of multiple operations trajectories. In transition systems (Nivre, 2008; Kuhlmann et al., 2011) used in real tasks (e.g., transition-based parsing) as shown in Table 1, it should be noted that more than push and pop operations are needed for the StackLSTM. Fortunately, for Arc-Eager and Parallelizable StackLSTM Continuing the formulation in the previous section, we will start by discussing our proposed solution under the case where the set of discrete actions contains only Push and Pop operations; we then move on to discussion of the applicability of our proposed solution to the transition systems that are used for building representations for dependency trees. T"
W19-1501,P17-2018,0,0.0452313,"Missing"
W19-4439,E17-2025,0,0.0372202,"from among the exponentially many choices. Our approach assumes a 1-to-1 correspondence (i.e. gloss) is available for each L1 token. Clearly, this is not true in general, so we only focus on mixed-language configurations when 1-to-1 glosses are possible. If a particular L1 token does not have a gloss, we only consider configurations where that token is always represented in L1. 1 By “meaning” we mean the L1 token that was originally in the sentence before it was replaced by an L2 gloss. Note that the softmax layer also uses the word embedding matrix E when generating the output distribution (Press and Wolf, 2017). This cloze language model encodes left-and-right contextual dependence rather than the typical sequence dependence of standard (unidirectional) language models. We train the parameters θ = [θ f ; θ b ; θ h ; E] using P Adam (Kingma and Ba, 2014) to maximize x L(x), where the summation is over sentences x in a large L1 training corpus. X L(x) = log p(xt |[hf t : hb t ]) (4) t We assume that the resulting model represents the entirety of the student’s L1 knowledge, and that the L1 parameters θ will not change further. 2.2 Incremental L2 Vocabulary Learning The model so far can assign probabili"
W19-4439,Q17-1010,0,0.0932253,"Missing"
W19-4439,N19-1423,0,0.0525518,"Missing"
W19-4439,P14-1053,0,0.512754,"ries, articles etc.). The machine teacher will take a sentence in the student’s native language (L1) and replace certain words with their foreign-language (L2) translations, resulting in a mixed-language sentence. We hope that reading mixed-language documents does not feel like a traditional vocabulary learning drill even though novel L2 words can be picked up over time. We envision our method being used alongside traditional foreign-language instruction. Typically, a machine teacher would require supervised data, meaning data on student behaviors and capabilities (Renduchintala et al., 2016; Labutov and Lipson, 2014). This step is expensive, not only from a data collection point of view, but also from the point of view of students, as they would have to give feedback (i.e. generate labeled data) on the actions of an initially untrained machine teacher. However, our machine teacher requires no supervised data from human students. Instead, it uses a cloze language model trained on corpora from the student’s native language as a proxy for a human student. Our machine teacher consults this proxy to guide its construction of mixed-language data. Moreover, we create an evaluation dataset that allows us to deter"
W19-4439,E17-1096,0,0.139818,"s are expected to use the surrounding words to make their guesses). Furthermore, we select the best performing variant and evaluate if participants can actually learn the L2 words by letting participants read a mixed-language passage and give a L2 vocabulary quiz at the end of passage, where the L2 words are presented in isolation. 2 Approach Student Proxy Model Before we address the aforementioned question, we must introduce our student proxy model. Concretely, our student proxy model is a cloze language model that uses bidirectional LSTMs to predicts L1 words from their surrounding context (Mousa and Schuller, 2017; Hochreiter and Schmidhuber, 1997). We refer to it as the cLM (cloze language model). Given a L1 sentence [x1 , x2 , ... , xT ], the model defines a distribution p(xt |[hf : hf ]) at each position in the sentence. Here, hf and hb are D−dimensional hidden states from forward and backward LSTMs. hf t = LSTMf ([x1 ,...,xt−1 ];θ f ) b b b h t = LSTM ([xt+1 ,...,xT ];θ ) (1) (2) The cLM assumes a fixed L1 vocabulary of size V , and the vectors xt above are embeddings of these word types, which correspond to the rows of a matrix E ∈ RV ×D . The output distribution (over V word types) is obtained by"
W19-4439,P16-1175,1,0.824807,"occur in narrative text (stories, articles etc.). The machine teacher will take a sentence in the student’s native language (L1) and replace certain words with their foreign-language (L2) translations, resulting in a mixed-language sentence. We hope that reading mixed-language documents does not feel like a traditional vocabulary learning drill even though novel L2 words can be picked up over time. We envision our method being used alongside traditional foreign-language instruction. Typically, a machine teacher would require supervised data, meaning data on student behaviors and capabilities (Renduchintala et al., 2016; Labutov and Lipson, 2014). This step is expensive, not only from a data collection point of view, but also from the point of view of students, as they would have to give feedback (i.e. generate labeled data) on the actions of an initially untrained machine teacher. However, our machine teacher requires no supervised data from human students. Instead, it uses a cloze language model trained on corpora from the student’s native language as a proxy for a human student. Our machine teacher consults this proxy to guide its construction of mixed-language data. Moreover, we create an evaluation data"
W19-5201,W18-6318,0,0.0362914,"n order to improve the translation of rare words, Nguyen and Chiang (2018) introduced LexNet, a feed-forward neural network that directly predicts the target word from a weighted sum of the source embeddings, on top of an RNN-based Seq2Seq models. Their goal was to improve translation output and hence they did not empirically show AER improvements on manually-aligned corpora. There are also a few other studies that inject alignment supervision during NMT training (Mi et al., 2016; Liu et al., 2016). In terms of improvements in word alignment quality, Legrand et al. (2016); Wang et al. (2018); Alkhouli et al. (2018) proposed neuRelated Work We start with work that combines word alignments with NMT. Research in this area generally falls into one of three themes: (1) employing the notion of word alignments to interpret the prediction of NMT; (2) making use of word alignments to improve NMT performance; (3) making use of NMT to improve word alignments. We mainly focus on related work in the first theme as this is the problem we are addressing in this work. Then we 2 ral word alignment modules decoupled from NMT systems, while Zenkel et al. (2019) introduced a separate module to extract alignment from NMT de"
W19-5201,2014.amta-researchers.5,0,0.112294,"d Ney, 2003) and fast-align (Dyer et al., 2013), which are all external models invented in SMT era and need to be run as a separate post-processing step after the full sentence translation is complete. As a direct result, they are not suitable for analyzing the internal decision processes of the neural machine translation models. Besides, these models are hard to apply in the online fashion, i.e. in the middle of left-to-right translation process, such as the scenario in certain constrained decoding algorithms (Hasler et al., 2018) and in computeraided translation (Bouma and Parmentier, 2014; Arcan et al., 2014). 1 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 1: Research Papers, pages 1–12 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics For these cases, the current common practice is to simply generate word alignments from attention weights between the encoder and decoder. However, there are problems with this practice. Koehn and Knowles (2017) showed that attention-based word alignment interpretation may be subject to “off-by-one” errors. Zenkel et al. (2019); Tang et al. (2018b); Raganato and Tiedemann (2018) pointed out that the atte"
W19-5201,P17-1106,0,0.0657226,"et al. (2019) introduced a separate module to extract alignment from NMT decoder states, with which they achieved comparable AER with fast-align with Transformer models. The saliency method we propose in this work draws its inspiration from visual saliency proposed by Simonyan et al. (2013); Springenberg et al. (2014); Smilkov et al. (2017). It should be noted that these methods were mostly applied to computer vision tasks. To the best of our knowledge, Li et al. (2016) presented the only work that directly employs saliency methods to interpret NLP models. Most similar to our work in spirit, Ding et al. (2017) used Layer-wise Relevance Propagation (LRP; Bach et al. 2015), an interpretation method resembling saliency, to interpret the internal working mechanisms of RNN-based neural machine translation systems. Although conceptually LRP is also a good fit for word alignment interpretation, we have some concerns with the mathematical soundness of LRP when applied to attention models. Our proposed method is also considerably more flexible and easier to implement than LRP. 3 less of whether it is the most likely label predicted by the model. As a final note, the term “attention weights” here refers to t"
W19-5201,N13-1073,0,0.681971,"i et al., 2015; Chan et al., 2016). Although word alignment is no longer a integral step like the case for Statistical Machine Translation (SMT) systems (Brown et al., 1993; Koehn et al., 2003), there is a resurgence of interest in the community to study word alignment for NMT models. Even for NMT, word alignments are useful for error analysis, inserting external vocabularies, and providing guidance for human translators in computer-aided translation. When aiming for the most accurate alignments, the state-of-the-art tools include GIZA++ (Brown et al., 1993; Och and Ney, 2003) and fast-align (Dyer et al., 2013), which are all external models invented in SMT era and need to be run as a separate post-processing step after the full sentence translation is complete. As a direct result, they are not suitable for analyzing the internal decision processes of the neural machine translation models. Besides, these models are hard to apply in the online fashion, i.e. in the middle of left-to-right translation process, such as the scenario in certain constrained decoding algorithms (Hasler et al., 2018) and in computeraided translation (Bouma and Parmentier, 2014; Arcan et al., 2014). 1 Proceedings of the Fourt"
W19-5201,C16-1291,0,0.0246354,"o focus on local dependencies in lower layers but finds long dependencies on higher ones. Beyond interpretation, in order to improve the translation of rare words, Nguyen and Chiang (2018) introduced LexNet, a feed-forward neural network that directly predicts the target word from a weighted sum of the source embeddings, on top of an RNN-based Seq2Seq models. Their goal was to improve translation output and hence they did not empirically show AER improvements on manually-aligned corpora. There are also a few other studies that inject alignment supervision during NMT training (Mi et al., 2016; Liu et al., 2016). In terms of improvements in word alignment quality, Legrand et al. (2016); Wang et al. (2018); Alkhouli et al. (2018) proposed neuRelated Work We start with work that combines word alignments with NMT. Research in this area generally falls into one of three themes: (1) employing the notion of word alignments to interpret the prediction of NMT; (2) making use of word alignments to improve NMT performance; (3) making use of NMT to improve word alignments. We mainly focus on related work in the first theme as this is the problem we are addressing in this work. Then we 2 ral word alignment modul"
W19-5201,D15-1166,0,0.0739911,"induced by automatic alignment tools. do we Abstract liev e tha t we Shuoyang Ding Hainan Xu Philipp Koehn Center for Language and Speech Processing Johns Hopkins University {dings, hxu31, phi}@jhu.edu (d) word saliency with SmoothGrad Figure 1: Comparison of our saliency-based word alignment interpretation of convolutional NMT model with reference and attention interpretation. Neural Machine Translation (NMT) has made lots of advancements since its inception. One of the key innovations that led to the largest improvements is the introduction of the attention mechanism (Bahdanau et al., 2014; Luong et al., 2015), which jointly learns word alignment and translation. Since then, the attention mechanism has gradually become a general technique in various NLP tasks, including summarization (Rush et al., 2015; See et al., 2017), natural language inference (Parikh et al., 2016) and speech recognition (Chorowski et al., 2015; Chan et al., 2016). Although word alignment is no longer a integral step like the case for Statistical Machine Translation (SMT) systems (Brown et al., 1993; Koehn et al., 2003), there is a resurgence of interest in the community to study word alignment for NMT models. Even for NMT, wo"
W19-5201,I17-1004,0,0.214574,"pretation method, but our empirical results also uncover that, contrary to common beliefs, architectures such as convolutional sequenceto-sequence models (Gehring et al., 2017) have already implicitly learned highly interpretable word alignments, which sheds light on how future improvement should be made on these architectures. 2 briefly introduce work in the other themes that is relevant to our study. We conclude by briefly summarizing related work to our proposed interpretation method. For the attention in RNN-based sequence-tosequence model, the first comprehensive analysis is conducted by Ghader and Monz (2017). They argued that the attention in such systems agree with word alignment to a certain extent by showing that the RNN-based system achieves comparable alignment error rate comparable to that of bidirectional GIZA++ with symmetrization. However, they also point out that they are not exactly the same, as training the attention with alignments would occasionally cause the model to forget important information. Lee et al. (2017) presented a toolkit that facilitates study for the attention in RNN-based models. There is also a number of other studies that analyze the attention in Transformer models"
W19-5201,D16-1249,0,0.20469,"es. Beyond interpretation, in order to improve the translation of rare words, Nguyen and Chiang (2018) introduced LexNet, a feed-forward neural network that directly predicts the target word from a weighted sum of the source embeddings, on top of an RNN-based Seq2Seq models. Their goal was to improve translation output and hence they did not empirically show AER improvements on manually-aligned corpora. There are also a few other studies that inject alignment supervision during NMT training (Mi et al., 2016; Liu et al., 2016). In terms of improvements in word alignment quality, Legrand et al. (2016); Wang et al. (2018); Alkhouli et al. (2018) proposed neuRelated Work We start with work that combines word alignments with NMT. Research in this area generally falls into one of three themes: (1) employing the notion of word alignments to interpret the prediction of NMT; (2) making use of word alignments to improve NMT performance; (3) making use of NMT to improve word alignments. We mainly focus on related work in the first theme as this is the problem we are addressing in this work. Then we 2 ral word alignment modules decoupled from NMT systems, while Zenkel et al. (2019) introduced a sepa"
W19-5201,N18-2081,0,0.0819047,"Missing"
W19-5201,N18-1031,0,0.10252,"Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics For these cases, the current common practice is to simply generate word alignments from attention weights between the encoder and decoder. However, there are problems with this practice. Koehn and Knowles (2017) showed that attention-based word alignment interpretation may be subject to “off-by-one” errors. Zenkel et al. (2019); Tang et al. (2018b); Raganato and Tiedemann (2018) pointed out that the attention-induced alignment is particularly noisy with Transformer models. Because of this, some studies, such as Nguyen and Chiang (2018); Zenkel et al. (2019) proposed either to add extra modules to generate higher quality word alignments, or to use these modules to further improve the model performance or interpretability. This paper is a step towards interpreting word alignments from NMT without relying on external models. We argue that using only attention weights is insufficient for generating clean word alignment interpretations, which we demonstrate both conceptually and empirically. We propose to use the notion of saliency to obtain word alignment interpretation of NMT predictions. Different from previous alignment mode"
W19-5201,W17-3204,1,0.849284,"in the middle of left-to-right translation process, such as the scenario in certain constrained decoding algorithms (Hasler et al., 2018) and in computeraided translation (Bouma and Parmentier, 2014; Arcan et al., 2014). 1 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 1: Research Papers, pages 1–12 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics For these cases, the current common practice is to simply generate word alignments from attention weights between the encoder and decoder. However, there are problems with this practice. Koehn and Knowles (2017) showed that attention-based word alignment interpretation may be subject to “off-by-one” errors. Zenkel et al. (2019); Tang et al. (2018b); Raganato and Tiedemann (2018) pointed out that the attention-induced alignment is particularly noisy with Transformer models. Because of this, some studies, such as Nguyen and Chiang (2018); Zenkel et al. (2019) proposed either to add extra modules to generate higher quality word alignments, or to use these modules to further improve the model performance or interpretability. This paper is a step towards interpreting word alignments from NMT without relyi"
W19-5201,N03-1017,1,0.0826726,"that led to the largest improvements is the introduction of the attention mechanism (Bahdanau et al., 2014; Luong et al., 2015), which jointly learns word alignment and translation. Since then, the attention mechanism has gradually become a general technique in various NLP tasks, including summarization (Rush et al., 2015; See et al., 2017), natural language inference (Parikh et al., 2016) and speech recognition (Chorowski et al., 2015; Chan et al., 2016). Although word alignment is no longer a integral step like the case for Statistical Machine Translation (SMT) systems (Brown et al., 1993; Koehn et al., 2003), there is a resurgence of interest in the community to study word alignment for NMT models. Even for NMT, word alignments are useful for error analysis, inserting external vocabularies, and providing guidance for human translators in computer-aided translation. When aiming for the most accurate alignments, the state-of-the-art tools include GIZA++ (Brown et al., 1993; Och and Ney, 2003) and fast-align (Dyer et al., 2013), which are all external models invented in SMT era and need to be run as a separate post-processing step after the full sentence translation is complete. As a direct result,"
W19-5201,J03-1002,0,0.0838048,"6) and speech recognition (Chorowski et al., 2015; Chan et al., 2016). Although word alignment is no longer a integral step like the case for Statistical Machine Translation (SMT) systems (Brown et al., 1993; Koehn et al., 2003), there is a resurgence of interest in the community to study word alignment for NMT models. Even for NMT, word alignments are useful for error analysis, inserting external vocabularies, and providing guidance for human translators in computer-aided translation. When aiming for the most accurate alignments, the state-of-the-art tools include GIZA++ (Brown et al., 1993; Och and Ney, 2003) and fast-align (Dyer et al., 2013), which are all external models invented in SMT era and need to be run as a separate post-processing step after the full sentence translation is complete. As a direct result, they are not suitable for analyzing the internal decision processes of the neural machine translation models. Besides, these models are hard to apply in the online fashion, i.e. in the middle of left-to-right translation process, such as the scenario in certain constrained decoding algorithms (Hasler et al., 2018) and in computeraided translation (Bouma and Parmentier, 2014; Arcan et al."
W19-5201,D16-1244,0,0.0652843,"saliency-based word alignment interpretation of convolutional NMT model with reference and attention interpretation. Neural Machine Translation (NMT) has made lots of advancements since its inception. One of the key innovations that led to the largest improvements is the introduction of the attention mechanism (Bahdanau et al., 2014; Luong et al., 2015), which jointly learns word alignment and translation. Since then, the attention mechanism has gradually become a general technique in various NLP tasks, including summarization (Rush et al., 2015; See et al., 2017), natural language inference (Parikh et al., 2016) and speech recognition (Chorowski et al., 2015; Chan et al., 2016). Although word alignment is no longer a integral step like the case for Statistical Machine Translation (SMT) systems (Brown et al., 1993; Koehn et al., 2003), there is a resurgence of interest in the community to study word alignment for NMT models. Even for NMT, word alignments are useful for error analysis, inserting external vocabularies, and providing guidance for human translators in computer-aided translation. When aiming for the most accurate alignments, the state-of-the-art tools include GIZA++ (Brown et al., 1993; Oc"
W19-5201,D17-2021,0,0.0208847,"rizing related work to our proposed interpretation method. For the attention in RNN-based sequence-tosequence model, the first comprehensive analysis is conducted by Ghader and Monz (2017). They argued that the attention in such systems agree with word alignment to a certain extent by showing that the RNN-based system achieves comparable alignment error rate comparable to that of bidirectional GIZA++ with symmetrization. However, they also point out that they are not exactly the same, as training the attention with alignments would occasionally cause the model to forget important information. Lee et al. (2017) presented a toolkit that facilitates study for the attention in RNN-based models. There is also a number of other studies that analyze the attention in Transformer models. Tang et al. (2018a,b) conducted targeted evaluation of neural machine translation models in two different evaluation tasks, namely subject-verb agreement and word sense disambiguation. During the analysis, they noted that the pattern in Transformer model (what they refer to as advanced attention mechanism) is very different from that of the attention in RNN-based architecture, in that a lot of the probability mass is focuse"
W19-5201,W18-5431,0,0.27875,"lation (Bouma and Parmentier, 2014; Arcan et al., 2014). 1 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 1: Research Papers, pages 1–12 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics For these cases, the current common practice is to simply generate word alignments from attention weights between the encoder and decoder. However, there are problems with this practice. Koehn and Knowles (2017) showed that attention-based word alignment interpretation may be subject to “off-by-one” errors. Zenkel et al. (2019); Tang et al. (2018b); Raganato and Tiedemann (2018) pointed out that the attention-induced alignment is particularly noisy with Transformer models. Because of this, some studies, such as Nguyen and Chiang (2018); Zenkel et al. (2019) proposed either to add extra modules to generate higher quality word alignments, or to use these modules to further improve the model performance or interpretability. This paper is a step towards interpreting word alignments from NMT without relying on external models. We argue that using only attention weights is insufficient for generating clean word alignment interpretations, which we demonstrate both conceptua"
W19-5201,W16-2207,0,0.136567,"es. Beyond interpretation, in order to improve the translation of rare words, Nguyen and Chiang (2018) introduced LexNet, a feed-forward neural network that directly predicts the target word from a weighted sum of the source embeddings, on top of an RNN-based Seq2Seq models. Their goal was to improve translation output and hence they did not empirically show AER improvements on manually-aligned corpora. There are also a few other studies that inject alignment supervision during NMT training (Mi et al., 2016; Liu et al., 2016). In terms of improvements in word alignment quality, Legrand et al. (2016); Wang et al. (2018); Alkhouli et al. (2018) proposed neuRelated Work We start with work that combines word alignments with NMT. Research in this area generally falls into one of three themes: (1) employing the notion of word alignments to interpret the prediction of NMT; (2) making use of word alignments to improve NMT performance; (3) making use of NMT to improve word alignments. We mainly focus on related work in the first theme as this is the problem we are addressing in this work. Then we 2 ral word alignment modules decoupled from NMT systems, while Zenkel et al. (2019) introduced a sepa"
W19-5201,D15-1044,0,0.0543771,"edu (d) word saliency with SmoothGrad Figure 1: Comparison of our saliency-based word alignment interpretation of convolutional NMT model with reference and attention interpretation. Neural Machine Translation (NMT) has made lots of advancements since its inception. One of the key innovations that led to the largest improvements is the introduction of the attention mechanism (Bahdanau et al., 2014; Luong et al., 2015), which jointly learns word alignment and translation. Since then, the attention mechanism has gradually become a general technique in various NLP tasks, including summarization (Rush et al., 2015; See et al., 2017), natural language inference (Parikh et al., 2016) and speech recognition (Chorowski et al., 2015; Chan et al., 2016). Although word alignment is no longer a integral step like the case for Statistical Machine Translation (SMT) systems (Brown et al., 1993; Koehn et al., 2003), there is a resurgence of interest in the community to study word alignment for NMT models. Even for NMT, word alignments are useful for error analysis, inserting external vocabularies, and providing guidance for human translators in computer-aided translation. When aiming for the most accurate alignmen"
W19-5201,N16-1082,0,0.0836096,"st theme as this is the problem we are addressing in this work. Then we 2 ral word alignment modules decoupled from NMT systems, while Zenkel et al. (2019) introduced a separate module to extract alignment from NMT decoder states, with which they achieved comparable AER with fast-align with Transformer models. The saliency method we propose in this work draws its inspiration from visual saliency proposed by Simonyan et al. (2013); Springenberg et al. (2014); Smilkov et al. (2017). It should be noted that these methods were mostly applied to computer vision tasks. To the best of our knowledge, Li et al. (2016) presented the only work that directly employs saliency methods to interpret NLP models. Most similar to our work in spirit, Ding et al. (2017) used Layer-wise Relevance Propagation (LRP; Bach et al. 2015), an interpretation method resembling saliency, to interpret the internal working mechanisms of RNN-based neural machine translation systems. Although conceptually LRP is also a good fit for word alignment interpretation, we have some concerns with the mathematical soundness of LRP when applied to attention models. Our proposed method is also considerably more flexible and easier to implement"
W19-5201,P17-1099,0,0.0176764,"cy with SmoothGrad Figure 1: Comparison of our saliency-based word alignment interpretation of convolutional NMT model with reference and attention interpretation. Neural Machine Translation (NMT) has made lots of advancements since its inception. One of the key innovations that led to the largest improvements is the introduction of the attention mechanism (Bahdanau et al., 2014; Luong et al., 2015), which jointly learns word alignment and translation. Since then, the attention mechanism has gradually become a general technique in various NLP tasks, including summarization (Rush et al., 2015; See et al., 2017), natural language inference (Parikh et al., 2016) and speech recognition (Chorowski et al., 2015; Chan et al., 2016). Although word alignment is no longer a integral step like the case for Statistical Machine Translation (SMT) systems (Brown et al., 1993; Koehn et al., 2003), there is a resurgence of interest in the community to study word alignment for NMT models. Even for NMT, word alignments are useful for error analysis, inserting external vocabularies, and providing guidance for human translators in computer-aided translation. When aiming for the most accurate alignments, the state-of-th"
W19-5201,D18-1458,0,0.0717529,"n computeraided translation (Bouma and Parmentier, 2014; Arcan et al., 2014). 1 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 1: Research Papers, pages 1–12 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics For these cases, the current common practice is to simply generate word alignments from attention weights between the encoder and decoder. However, there are problems with this practice. Koehn and Knowles (2017) showed that attention-based word alignment interpretation may be subject to “off-by-one” errors. Zenkel et al. (2019); Tang et al. (2018b); Raganato and Tiedemann (2018) pointed out that the attention-induced alignment is particularly noisy with Transformer models. Because of this, some studies, such as Nguyen and Chiang (2018); Zenkel et al. (2019) proposed either to add extra modules to generate higher quality word alignments, or to use these modules to further improve the model performance or interpretability. This paper is a step towards interpreting word alignments from NMT without relying on external models. We argue that using only attention weights is insufficient for generating clean word alignment interpretations, wh"
W19-5201,W18-6304,0,0.185232,"ntier, 2014; Arcan et al., 2014). 1 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 1: Research Papers, pages 1–12 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics For these cases, the current common practice is to simply generate word alignments from attention weights between the encoder and decoder. However, there are problems with this practice. Koehn and Knowles (2017) showed that attention-based word alignment interpretation may be subject to “off-by-one” errors. Zenkel et al. (2019); Tang et al. (2018b); Raganato and Tiedemann (2018) pointed out that the attention-induced alignment is particularly noisy with Transformer models. Because of this, some studies, such as Nguyen and Chiang (2018); Zenkel et al. (2019) proposed either to add extra modules to generate higher quality word alignments, or to use these modules to further improve the model performance or interpretability. This paper is a step towards interpreting word alignments from NMT without relying on external models. We argue that using only attention weights is insufficient for generating clean word alignment interpretations, which we demonstrate both conceptua"
W19-5201,P18-2060,0,0.0201533,"nd interpretation, in order to improve the translation of rare words, Nguyen and Chiang (2018) introduced LexNet, a feed-forward neural network that directly predicts the target word from a weighted sum of the source embeddings, on top of an RNN-based Seq2Seq models. Their goal was to improve translation output and hence they did not empirically show AER improvements on manually-aligned corpora. There are also a few other studies that inject alignment supervision during NMT training (Mi et al., 2016; Liu et al., 2016). In terms of improvements in word alignment quality, Legrand et al. (2016); Wang et al. (2018); Alkhouli et al. (2018) proposed neuRelated Work We start with work that combines word alignments with NMT. Research in this area generally falls into one of three themes: (1) employing the notion of word alignments to interpret the prediction of NMT; (2) making use of word alignments to improve NMT performance; (3) making use of NMT to improve word alignments. We mainly focus on related work in the first theme as this is the problem we are addressing in this work. Then we 2 ral word alignment modules decoupled from NMT systems, while Zenkel et al. (2019) introduced a separate module to extra"
W19-5201,J93-2003,0,\N,Missing
W19-5201,W16-2206,0,\N,Missing
W19-5201,D18-1537,0,\N,Missing
W19-5301,W19-5424,1,0.858444,"Missing"
W19-5301,W19-5306,0,0.248769,"al., 2019) University of Kyoto (Cromieres and Kurohashi, 2019) Lingua Custodia (Burlot, 2019) LIUM (Bougares et al., 2019) LMU Munich (Stojanovski and Fraser, 2019; Stojanovski et al., 2019) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of Information and Communications Technology (Dabre et al., 2019; Marie et al., 2019b) National Research Council of Canada (Littell et al., 2019) Bo˘gaziçi University (Biçici, 2019) PROMT LLC (Molchanov, 2019) University of Groningen (Toral et al., 2019) RWTH Aachen (Rosendahl et al., 2019) TALP Research Center, Universitat Politècnica de Catalunya (Casas et al., 2019) University of Tartu (Tättar et al., 2019) Tilde (Pinnis et al., 2019) Universitat d’Alacant (Sánchez-Cartagena et al., 2019) University of Cambridge (Stahlberg et al., 2019) Saarland University, DFKI (España-Bonet and Ruiter, 2019) University of Edinburgh (Bawden et al., 2019a) University of Maryland (Briakou and Carpuat, 2019) (no associated paper) University of Sydney (Ding and Tao, 2019) (no associated"
W19-5301,D18-1549,0,0.116951,"s are available for this system. 2.5.7 BASELINE - RE - RERANK (no associated CUNI-T RANSFORMER -T2T2018 (Popel, 2018) is the exact same system as used last year. paper) BASELINE - RE - RERANK is a standard Transformer, with corpus filtering, pre-processing, postprocessing, averaging and ensembling as well as n-best list reranking. 2.5.8 CUNI-T RANSFORMER -M ARIAN (Popel et al., 2019) is a “reimplementation” of the last year’s system (Popel, 2018) in Marian (JunczysDowmunt et al., 2018). CA I RE (Liu et al., 2019) CUNI-U NSUPERVISED -NER- POST (Kvapilíková et al., 2019) follows the strategy of Artetxe et al. (2018), creating a seed phrase-based system where the phrase table is initialized from cross-lingual embedding mappings trained on monolingual data, followed by a neural machine translation system trained on synthetic parallel corpus. The synthetic corpus is produced by the seed phrase-based MT system or by a such a model refined through iterative back-translation. CUNI-U NSUPERVISED -NER- POST further focuses on the handling of named entities, i.e. the part of vocabulary where the cross-lingual embedding mapping suffer most. CA I RE is a hybrid system that took part only in the unsupervised track."
W19-5301,D18-1332,0,0.0215805,"et al., 2018). For English↔Gujarati, synthetic parallel data from two sources, backtranslation and pivoting through Hindi, is produced using unsupervised and semi-supervised NMT models, pre-trained using a cross-lingual language objective (Lample and Conneau, 2019) For German→English, the impact of vast amounts of back-translated training data on translation quality is studied, and some additional insights are gained over (Edunov et al., 2018). Towards the end of training, for German→English and Chinese↔English, the mini-batch size was increased up to fifty-fold by delaying gradient updates (Bogoychev et al., 2018) as an alternative to learning rate cooldown (Smith, 2018). For Chinese↔English, a comparison of different segmentation strategies showed that character-based decoding was superior to the translation of subwords when translating into Chinese. Pre-processing strategies were also investigated for English→Czech, showing that preprocessing can be simplified without loss to MT quality. UEDIN’s main findings on the Chinese↔English translation task are that character-level model on the Chinese side can be used when translating into Chinese to improve the BLEU score. The same does not hold when transl"
W19-5301,W19-5351,0,0.0505622,"Missing"
W19-5301,W19-5423,0,0.0419015,"Missing"
W19-5301,W18-6412,1,0.856623,"Missing"
W19-5301,W19-5305,0,0.0496912,"Missing"
W19-5301,W12-3102,1,0.474924,"Missing"
W19-5301,W19-5310,0,0.0432396,"Missing"
W19-5301,W19-5425,0,0.0236032,"ys-Dowmunt et al., 2018) and Phrase-based machine translation system (implemented with Moses) and for the Spanish-Portuguese task. The system combination included features formerly presented in (Marie and Fujita, 2018), including scores left-to-right and right-to-left, sentence level translation probabilities and language model scores. Also authors provide contrastive results with an unsupervised phrase-based MT system which achieves quite close results to their primary system. Authors associate high performance of the unsupervised system to the language similarity. Incomslav: Team INCOMSLAV (Chen and Avgustinova, 2019) by Saarlad University participated in the Czech to Polish translation task only. The team’s primary submission builds on a transformer-based NMT baseline with back translation which has been submitted one of their contrastive submission. Incomslav’s primary system is a phoneme-based system re-scored using their NMT baseline. A second contrastive submission builds our phrase-based SMT system combined with a joint BPE model. NITS-CNLP: The NITS-CNLP team (Laskar et al., 2019) by the National Institute of Technology Silchar in India submitted results to the HI-NE translation task in both directi"
W19-5301,W07-0718,1,0.530103,"ojar Charles University Yvette Graham Barry Haddow Dublin City University University of Edinburgh Philipp Koehn JHU / University of Edinburgh Mathias Müller University of Zurich Marta R. Costa-jussà Christian Federmann UPC Microsoft Cloud + AI Shervin Malmasi Harvard Medical School Santanu Pal Saarland University Matt Post JHU Abstract Introduction The Fourth Conference on Machine Translation (WMT) held at ACL 20191 hosts a number of shared tasks on various aspects of machine translation. This conference builds on 13 previous editions of WMT as workshops and conferences (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016, 2017, 2018). This year we conducted several official tasks. We report in this paper on the news and similar translation tasks. Additional shared tasks are described in separate papers in these proceedings: • biomedical translation (Bawden et al., 2019b) • automatic post-editing (Chatterjee et al., 2019) • metrics (Ma et al., 2019) • quality estimation (Fonseca et al., 2019) • parallel corpus filtering (Koehn et al., 2019) • robustness (Li et al., 2019b) In the news translation task (Section 2), participants were asked to tra"
W19-5301,P16-2058,1,0.819865,"ipated with the Transformer (Vaswani et al., 2017) implemented in the OpenNMT toolkit. They focused on word segmentation methods and compared a cognate-aware segmentation method, Cognate Morfessor (Grönroos et al., 2018), with character segmentation and unsupervised segmentation methods. As primary submission they submitted this Cognate Morfessor that optimizes subword segmentations consistently for cognates. They participated for all translation directions in Spanish-Portuguese and Czech-Polish, and this Cognate Morfessor performed better for Czech-Polish, while characterbased segmentations (Costa-jussà and Fonollosa, 2016), while much more inefficient, were superior for Spanish-Portuguese. UPC-TALP: The UPC-TALP team (Biesialska et al., 2019) by the Universitat Politècnica de Catalunya submitted a Transformer (implemented with Fairseq (Ott et al., 2019)) for the Czechto-Polish task and a Phrase-based system (implemented with Moses (Koehn et al., 2007)) for Spanish-to-Portuguese. They tested adding monolingual data to the NMT system by copying the same data on the source and target sides, with negative results. Also, their system combination based on sentence-level BLEU in back-translation 5.4 Conclusion of Simi"
W19-5301,W08-0309,1,0.659809,"Missing"
W19-5301,W18-3931,1,0.820211,"or they use English as a pivot language to translate between resource-poorer languages. The interest in English is reflected, for example, in the WMT translation tasks (e.g. News, Biomedical) which have always included language pairs in which texts are translated to and/or from English. With the widespread use of MT technology, there is more and more interest in training systems to translate between languages other than English. One evidence of this is the need of directly translating between pairs of similar languages, varieties, and dialects (Zhang, 1998; Marujo et al., 2011; Hassani, 2017; Costa-jussà et al., 2018). The main challenge is to take advantage of the similarity between languages to overcome the limitation given the low amount of available parallel data to produce an accurate output. Given the interest of the community in this topic we organize, for the first time at WMT, a shared task on ""Similar Language Translation"" to evaluate the performance of state-of-the-art translation systems on translating between pairs of languages from the same language family. We provide participants with training and testing data from three language pairs: Spanish - Portuguese (Romance languages), Czech - Polis"
W19-5301,W19-5312,0,0.0773587,"Missing"
W19-5301,W19-5313,0,0.0931699,"University (Marchisio et al., 2019) (no associated paper) University of Saarland (Mondal et al., 2019) Kingsoft AI (Guo et al., 2019) University of Kyoto (Cromieres and Kurohashi, 2019) Lingua Custodia (Burlot, 2019) LIUM (Bougares et al., 2019) LMU Munich (Stojanovski and Fraser, 2019; Stojanovski et al., 2019) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of Information and Communications Technology (Dabre et al., 2019; Marie et al., 2019b) National Research Council of Canada (Littell et al., 2019) Bo˘gaziçi University (Biçici, 2019) PROMT LLC (Molchanov, 2019) University of Groningen (Toral et al., 2019) RWTH Aachen (Rosendahl et al., 2019) TALP Research Center, Universitat Politècnica de Catalunya (Casas et al., 2019) University of Tartu (Tättar et al., 2019) Tilde (Pinnis et al., 2019) Universitat d’Alacant (Sánchez-Cartagena et al., 2019) University of Cambridge (Stahlberg et al., 2019) Saarland University, DFKI (España-Bonet and Ruiter, 2019) University of Edinburgh (Bawden et al., 2019a) University of"
W19-5301,W19-5314,0,0.0200465,"Bo˘gaziçi University (Biçici, 2019) PROMT LLC (Molchanov, 2019) University of Groningen (Toral et al., 2019) RWTH Aachen (Rosendahl et al., 2019) TALP Research Center, Universitat Politècnica de Catalunya (Casas et al., 2019) University of Tartu (Tättar et al., 2019) Tilde (Pinnis et al., 2019) Universitat d’Alacant (Sánchez-Cartagena et al., 2019) University of Cambridge (Stahlberg et al., 2019) Saarland University, DFKI (España-Bonet and Ruiter, 2019) University of Edinburgh (Bawden et al., 2019a) University of Maryland (Briakou and Carpuat, 2019) (no associated paper) University of Sydney (Ding and Tao, 2019) (no associated paper) 8 participated in all language pairs. The translations from the Table 5: Participants in the shared translation task. Not all teams online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion consistent with previous years of the workshop. 2.5.6 BTRANS only the middle sentence was considered for the final translation hypothesis, otherwise shorter context of two sentences or just a single sentence was used. Unfortunately, no details are available for this system. 2.5.7 BASELINE - RE - RERANK (no associ"
W19-5301,D18-1045,0,0.0285693,"xt was morphologically segmented with Apertium. The UEDIN systems are supervised NMT systems based on the transformer architecture and trained using Marian (Junczys-Dowmunt et al., 2018). For English↔Gujarati, synthetic parallel data from two sources, backtranslation and pivoting through Hindi, is produced using unsupervised and semi-supervised NMT models, pre-trained using a cross-lingual language objective (Lample and Conneau, 2019) For German→English, the impact of vast amounts of back-translated training data on translation quality is studied, and some additional insights are gained over (Edunov et al., 2018). Towards the end of training, for German→English and Chinese↔English, the mini-batch size was increased up to fifty-fold by delaying gradient updates (Bogoychev et al., 2018) as an alternative to learning rate cooldown (Smith, 2018). For Chinese↔English, a comparison of different segmentation strategies showed that character-based decoding was superior to the translation of subwords when translating into Chinese. Pre-processing strategies were also investigated for English→Czech, showing that preprocessing can be simplified without loss to MT quality. UEDIN’s main findings on the Chinese↔Engl"
W19-5301,W18-6410,0,0.0193718,"ormance can be found in Hindi-Nepali (both directions), where the best performing system is around 50 BLEU (53 for Hindi-to-Nepali and 49.1 for Nepali-toHindi), and the lowest entry is 1.4 for Hindi-toNepali and 0 for Nepali-to-Hindi. The lowest variance is for Polish-to-Czech and it may be because only two teams participated. UHelsinki: The University of Helsinki team (Scherrer et al., 2019) participated with the Transformer (Vaswani et al., 2017) implemented in the OpenNMT toolkit. They focused on word segmentation methods and compared a cognate-aware segmentation method, Cognate Morfessor (Grönroos et al., 2018), with character segmentation and unsupervised segmentation methods. As primary submission they submitted this Cognate Morfessor that optimizes subword segmentations consistently for cognates. They participated for all translation directions in Spanish-Portuguese and Czech-Polish, and this Cognate Morfessor performed better for Czech-Polish, while characterbased segmentations (Costa-jussà and Fonollosa, 2016), while much more inefficient, were superior for Spanish-Portuguese. UPC-TALP: The UPC-TALP team (Biesialska et al., 2019) by the Universitat Politècnica de Catalunya submitted a Transform"
W19-5301,W19-5317,0,0.114089,"ed paper) (Liu et al., 2019) Charles University (Popel et al., 2019; Kocmi and Bojar, 2019) and (Kvapilíková et al., 2019) Kumamoto University, Telkom University, Indonesian Institute of Sciences (Budiwati et al., 2019) DFKI (Zhang and van Genabith, 2019) eTranslation (Oravecz et al., 2019) Facebook AI Research (Ng et al., 2019) GTCOM (Bei et al., 2019) University of Helsinki (Talman et al., 2019) IIIT Hyderabad (Goyal and Sharma, 2019) IIT Patna (Sen et al., 2019) Johns Hopkins University (Marchisio et al., 2019) (no associated paper) University of Saarland (Mondal et al., 2019) Kingsoft AI (Guo et al., 2019) University of Kyoto (Cromieres and Kurohashi, 2019) Lingua Custodia (Burlot, 2019) LIUM (Bougares et al., 2019) LMU Munich (Stojanovski and Fraser, 2019; Stojanovski et al., 2019) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of Information and Communications Technology (Dabre et al., 2019; Marie et al., 2019b) National Research Council of Canada (Littell et al., 2019) Bo˘gaziçi University (Biçici, 201"
W19-5301,W19-5315,0,0.0266353,"Missing"
W19-5301,W19-5318,0,0.198338,"ance of the systems when translating from French to German seems to heavily depend on the 7 http://data.statmt.org/wmt19/ translation-task/dev.tgz 6 Systems MSRA.MADL eTranslation LIUM MLLP-UPV onlineA TartuNLP onlineB onlineY onlineG onlineX FULL 47.3 45.4 43.7 41.5 40.8 39.2 39.1 39.0 38.5 38.1 source FR 38.3 37.4 37.5 36.4 35.4 34.8 35.3 34.7 34.6 35.6 source DE 50.0 47.8 45.5 43.0 42.3 40.5 40.2 40.2 39.7 38.8 evaluations. In the rest of this sub-section, we provide brief details of the submitted systems, for those in cases where the authors provided such details. 2.5.1 AFRL - SYSCOMB 19 (Gwinnup et al., 2019) is a system combination of a Marian ensemble system, two distinct OpenNMT systems, a Sockeyebased Elastic Weight Consolidation system, and one Moses phrase-based system. Table 3: French→German Meteor scores. Systems MSRA.MADL LinguaCustodia MLLP_UPV Kyoto_University_T2T LIUM onlineY onlineB TartuNLP onlineA onlineX onlineG FULL 52.0 51.3 49.5 48.8 48.3 47.5 46.4 46.3 45.3 42.7 41.7 source FR 51.9 52.5 49.9 49.7 46.5 43.7 43.7 45.0 43.7 41.6 40.9 source DE 52.0 51.0 49.4 48.6 48.7 48.4 47.0 46.7 45.8 42.9 41.9 AFRL- EWC (Gwinnup et al., 2019) is a Sockeye Transformer system trained with the de"
W19-5301,W19-5316,0,0.109691,"boratory (Gwinnup et al., 2019) Apertium (Pirinen, 2019) Apprentice (Li and Specia, 2019) Aylien Ltd. (Hokamp et al., 2019) Baidu (Sun et al., 2019) (no associated paper) (no associated paper) (Liu et al., 2019) Charles University (Popel et al., 2019; Kocmi and Bojar, 2019) and (Kvapilíková et al., 2019) Kumamoto University, Telkom University, Indonesian Institute of Sciences (Budiwati et al., 2019) DFKI (Zhang and van Genabith, 2019) eTranslation (Oravecz et al., 2019) Facebook AI Research (Ng et al., 2019) GTCOM (Bei et al., 2019) University of Helsinki (Talman et al., 2019) IIIT Hyderabad (Goyal and Sharma, 2019) IIT Patna (Sen et al., 2019) Johns Hopkins University (Marchisio et al., 2019) (no associated paper) University of Saarland (Mondal et al., 2019) Kingsoft AI (Guo et al., 2019) University of Kyoto (Cromieres and Kurohashi, 2019) Lingua Custodia (Burlot, 2019) LIUM (Bougares et al., 2019) LMU Munich (Stojanovski and Fraser, 2019; Stojanovski et al., 2019) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of"
W19-5301,E14-1047,1,0.904335,"Missing"
W19-5301,W19-5427,0,0.0467733,"Missing"
W19-5301,W19-5322,1,0.807321,"Missing"
W19-5301,W19-5302,1,0.715203,"20191 hosts a number of shared tasks on various aspects of machine translation. This conference builds on 13 previous editions of WMT as workshops and conferences (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016, 2017, 2018). This year we conducted several official tasks. We report in this paper on the news and similar translation tasks. Additional shared tasks are described in separate papers in these proceedings: • biomedical translation (Bawden et al., 2019b) • automatic post-editing (Chatterjee et al., 2019) • metrics (Ma et al., 2019) • quality estimation (Fonseca et al., 2019) • parallel corpus filtering (Koehn et al., 2019) • robustness (Li et al., 2019b) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (“constrained” condition). We 1 Christof Monz University of Amsterdam Marcos Zampieri University of Wolverhampton held 18 translation tasks this year, between English and each of Chinese, Czech (into Czech only), German, Finnish, Lithuanian, and Russian. New this year were Gujarati↔English and Kazakh↔English. B"
W19-5301,W19-5333,0,0.0923169,"Missing"
W19-5301,W19-5353,0,0.0658382,"Missing"
W19-5301,W19-5430,1,0.873847,"Missing"
W19-5301,W19-5431,0,0.0199622,"Universitat Politècnica de València (UPV) participated with a Transformer (implemented with FairSeq (Ott et al., 2019)) and a finetuning strategy for domain adaptaion in the task of Spanish-Portuguese. Fine-tunning on the development data provide improvements of almost 12 BLEU points, which may explain their clear best performance in the task for this language pair. As a contrastive system authors provided only for the Portuguese-to-Spanish a novel 2D alternating RNN model which did not respond so well when fine-tunning. UBC-NLP: Team UBC-NLP from the University of British Columbia in Canada (Przystupa and Abdul-Mageed, 2019) compared the performance of the LSTM plus attention (Bahdanau et al., 2015) and Transformer (Vaswani et al., 2017) (implemented in OpenNMT toolkit22 ) perform for the three tasks at hand. Authors use backtranslation to introduce monolingual data in their systems. LSTM plus attention outperformed Transformer for Hindi-Nepali, and viceversa for the other two tasks. As reported by the authors, Hindi-Nepali task provides much more shorter sentences than KYOTOUNIVERSITY: Kyoto University’s submission, listed simply as KYOTO in Table 25 for PT → ES task is based on transformer NMT system. They used"
W19-5301,P02-1040,0,0.11337,"ation of the source (CS), and a second encoder to encode sub-word (byte-pair-encoding) information of the source (CS). The results obtained by their system in translating from Czech→Polish and comment on the impact of out-of-domain test data in the performance of their system. UDSDFKI ranked second among ten teams in Czech– Polish translation. 5.3 Results We present results for the three language pairs, each of them in the two possible directions. For this first edition of the Similar Translation Task and differently from News task, evaluation was only performed on automatic basis using BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) measures. Each language direction is reported in one different table which contain information of the team; type of system, either contrastive (C) or primary (P), and the BLEU and TER results. In general, primary systems tend to be better than contrastive systems, as expected, but there are some exceptions. Even if we are presenting 3 pairs of languages each pair belonging to the same family, translation quality in terms of BLEU varies signficantly. While the best systems for Spanish-Portuguese are above 64 BLEU and below 21 TER (see Tables 26 and 27), best syste"
W19-5301,W19-5354,0,0.0611791,"Missing"
W19-5301,W18-6486,0,0.0189853,"the agglutinative nature of Kazakh, (ii) data from an additional language (Russian), given the scarcity of English–Kazakh data and (iii) synthetic data for the source language filtered using language-independent sentence similarity. RUG _ KKEN _ MORFESSOR Tilde developed both constrained and unconstrained NMT systems for English-Lithuanian and Lithuanian-English using the Marian toolkit. All systems feature ensembles of four to five transformer models that were trained using the quasi-hyperbolic Adam optimiser (Ma and Yarats, 2018). Data for the systems were prepared using TildeMT filtering (Pinnis, 2018) and preprocessing (Pinnis et al., 2018) methods. For unconstrained systems, data were additionally filtered using dual conditional cross-entropy filtering (Junczys-Dowmunt, 2018a). All systems were trained using iterative back-translation (Rikters, 2018) and feature synthetic data that allows training NMT systems to support handling of unknown phenomena (Pinnis et al., 2017). During translation, automatic named entity and nontranslatable phrase post-editing were performed. For constrained systems, named entities and nontranslatable phrase lists were extracted from the parallel training data."
W19-5301,W19-5335,0,0.0408704,"Missing"
W19-5301,W19-5344,1,0.904781,"n Institute of Sciences (Budiwati et al., 2019) DFKI (Zhang and van Genabith, 2019) eTranslation (Oravecz et al., 2019) Facebook AI Research (Ng et al., 2019) GTCOM (Bei et al., 2019) University of Helsinki (Talman et al., 2019) IIIT Hyderabad (Goyal and Sharma, 2019) IIT Patna (Sen et al., 2019) Johns Hopkins University (Marchisio et al., 2019) (no associated paper) University of Saarland (Mondal et al., 2019) Kingsoft AI (Guo et al., 2019) University of Kyoto (Cromieres and Kurohashi, 2019) Lingua Custodia (Burlot, 2019) LIUM (Bougares et al., 2019) LMU Munich (Stojanovski and Fraser, 2019; Stojanovski et al., 2019) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of Information and Communications Technology (Dabre et al., 2019; Marie et al., 2019b) National Research Council of Canada (Littell et al., 2019) Bo˘gaziçi University (Biçici, 2019) PROMT LLC (Molchanov, 2019) University of Groningen (Toral et al., 2019) RWTH Aachen (Rosendahl et al., 2019) TALP Research Center, Universitat Politècnica de Catalunya (Casas e"
W19-5301,W19-5346,0,0.197908,"rtium (Pirinen, 2019) Apprentice (Li and Specia, 2019) Aylien Ltd. (Hokamp et al., 2019) Baidu (Sun et al., 2019) (no associated paper) (no associated paper) (Liu et al., 2019) Charles University (Popel et al., 2019; Kocmi and Bojar, 2019) and (Kvapilíková et al., 2019) Kumamoto University, Telkom University, Indonesian Institute of Sciences (Budiwati et al., 2019) DFKI (Zhang and van Genabith, 2019) eTranslation (Oravecz et al., 2019) Facebook AI Research (Ng et al., 2019) GTCOM (Bei et al., 2019) University of Helsinki (Talman et al., 2019) IIIT Hyderabad (Goyal and Sharma, 2019) IIT Patna (Sen et al., 2019) Johns Hopkins University (Marchisio et al., 2019) (no associated paper) University of Saarland (Mondal et al., 2019) Kingsoft AI (Guo et al., 2019) University of Kyoto (Cromieres and Kurohashi, 2019) Lingua Custodia (Burlot, 2019) LIUM (Bougares et al., 2019) LMU Munich (Stojanovski and Fraser, 2019; Stojanovski et al., 2019) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of Information and Communicatio"
W19-5301,W19-5341,0,0.0172601,"A,B,G,X,Y. For presentation of the results, systems are treated as either constrained or unconstrained, depending on whether their models were trained only on the provided data. Since we do not know how they were built, the online systems are treated as unconstrained during the automatic and human AYLIEN _ MULTILINGUAL (Hokamp et al., 2019) The Aylien research team built a Multilingual NMT system which is trained on all WMT2019 language pairs in all directions, then fine-tuned for a small number of iterations on Gujarati-English data only, including some self-backtranslated data. 2.5.5 BAIDU (Sun et al., 2019) Baidu systems are based on the Transformer architecture with several improvements. Data selection, back translation, data augmentation, knowledge distillation, domain adaptation, model ensemble and re-ranking are employed and proven effective in our experiments. 7 Team AFRL A PERTIUM - FIN - ENG A PPRENTICE - C AYLIEN _ MULTILINGUAL BAIDU BTRANS BASELINE - RE - RERANK CA I RE CUNI DBMS-KU DFKI - NMT E T RANSLATION FACEBOOK FAIR GTCOM H ELSINKI NLP IIITH-MT IITP JHU JUMT JU_S AARLAND KSAI K YOTO U NIVERSITY L INGUA C USTODIA LIUM LMU-NMT MLLP-UPV MS T RANSLATOR MSRA N IU T RANS NICT NRC PARFDA"
W19-5301,P16-1162,1,0.310296,"ssible, 2.5.13 E T RANSLATION (Oravecz et al., 2019) E T RANSLATION En-De E T RANSLATION ’s EnDe system is an ensemble of 3 base Transformers and a Transformer-type language model, trained from all available parallel data (cleaned up and filtered with dual conditional cross-entropy filtering) and with additional back-translated data generated 9 2.5.17 from monolingual news. Each Transformer model is fine tuned on previous years’ test sets. H ELSINKI NLP is a Transformer (Vaswani et al., 2017) style model implemented in OpenNMTpy using a variety of corpus filtering techniques, truecasing, BPE (Sennrich et al., 2016), backtranslation, ensembling and fine-tuning for domain adaptation. E T RANSLATION Fr-De The Fr-De system is an ensemble of 2 big Transformers (with size 8192 FFN layers). Back-translation data was selected using topic modelling techniques to tune the model towards the domain defined in the task. 2.5.18 En-Lt The En-Lt system is an ensemble of 2 big Transformers (as for Fr-De) and a Transformer type language model. The training data contains the Rapid corpus and the news domain back-translated data sets 2 times oversampled. E T RANSLATION 2.5.19 FACEBOOK FAIR (Ng et al., 2019) 2.5.20 JHU (Mar"
W19-5301,W19-5339,0,0.0767002,"Missing"
W19-5301,W19-5347,0,0.0328257,"Missing"
W19-5301,W19-5342,1,0.887858,"encia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of Information and Communications Technology (Dabre et al., 2019; Marie et al., 2019b) National Research Council of Canada (Littell et al., 2019) Bo˘gaziçi University (Biçici, 2019) PROMT LLC (Molchanov, 2019) University of Groningen (Toral et al., 2019) RWTH Aachen (Rosendahl et al., 2019) TALP Research Center, Universitat Politècnica de Catalunya (Casas et al., 2019) University of Tartu (Tättar et al., 2019) Tilde (Pinnis et al., 2019) Universitat d’Alacant (Sánchez-Cartagena et al., 2019) University of Cambridge (Stahlberg et al., 2019) Saarland University, DFKI (España-Bonet and Ruiter, 2019) University of Edinburgh (Bawden et al., 2019a) University of Maryland (Briakou and Carpuat, 2019) (no associated paper) University of Sydney (Ding and Tao, 2019) (no associated paper) 8 participated in all language pairs. The translations from the Table 5: Participants in the shared translation task. Not all teams online systems were not submitted by their respective companies but were obtained by us, and"
W19-5301,W19-5355,1,0.869964,"Missing"
W19-5301,W19-5350,0,0.0441915,"Missing"
W19-5301,P98-2238,0,0.38957,"n trained to translate texts from and to English or they use English as a pivot language to translate between resource-poorer languages. The interest in English is reflected, for example, in the WMT translation tasks (e.g. News, Biomedical) which have always included language pairs in which texts are translated to and/or from English. With the widespread use of MT technology, there is more and more interest in training systems to translate between languages other than English. One evidence of this is the need of directly translating between pairs of similar languages, varieties, and dialects (Zhang, 1998; Marujo et al., 2011; Hassani, 2017; Costa-jussà et al., 2018). The main challenge is to take advantage of the similarity between languages to overcome the limitation given the low amount of available parallel data to produce an accurate output. Given the interest of the community in this topic we organize, for the first time at WMT, a shared task on ""Similar Language Translation"" to evaluate the performance of state-of-the-art translation systems on translating between pairs of languages from the same language family. We provide participants with training and testing data from three language"
W19-5303,2012.eamt-1.60,0,0.0358,"indicate the data domain. For data augmentation, they back-translate from a target language to its noisy source. The intuition, also observed by Michel and Neubig (2018), is that the source sentences are noisier than their target translations. They include out-ofdomain clean data during this step and differentiate data types with a special symbol on the target side. In addition, they also run a model ensemble. Training Data In the constrained setting, participants were allowed to use the WMT15 training data3 for Eng↔Fra and any of the KFTT (Neubig, 2011), JESC (Pryzant et al.) and TED talks (Cettolo et al., 2012) corpora for Jpn↔Eng. Additionally, the use of the MTNT corpus (Michel and Neubig, 2018) was allowed in order to adapt models on limited in-domain data. 3.3 Evaluation protocol Test Data The test sets were collected following the same protocol as the MTNT dataset, i.e. collected from 3 http://www.statmt.org/wmt15/ translation-task.html 93 Figure 1: Annotation interface for human evaluations. 94 Eng-Fra Fra-Eng Eng-Jpn Jpn-Eng # samples 1,401 1,233 1,392 1,111 # source tokens # target tokens 20.0k 22.8k 19.8k 19.2k 20.0k 33.6k 18.7k 13.4k Table 1: Statistics of the test sets. model which was al"
W19-5303,P19-1425,0,0.113812,"inkov and Glass (2019) for a categorization of such work. In particular, some have focused on specific variations of naturally-occurring noise, such as grammatical errors produced by non-native speakers (Anastasopoulos et al., 2019) or errors extracted from Wikipedia edits (Belinkov and Bisk, 2018). It has also been shown that adding synthetic noise does not trivially increase robustness to natural noise (Belinkov and Bisk, 2018) and may require specific recipes (Karpukhin et al., 2019). Michel et al. (2019) recently emphasized the importance of meaning-preserving perturbations and along with Cheng et al. (2019) demonstrated the utility of adversarial training without significantly impairing performance on clean data and domain. Durrani et al. (2019) showed that character-based representations are more robust towards noise compared to such learned using BPE-based sub-word units in the task of machine translation. Jpn→Eng. We describe the dataset and the task setup in Section 3. The shared-task attracted a total of 23 submissions from 11 teams. The teams employed a variety of methods to improve robustness. A specific challenge was the small size of the in-domain noisy parallel dataset. We summarize th"
W19-5303,P17-4012,0,0.0548466,"data, improve existing semisupervised approach such as backtranslation. We provide both in-domain (MTNT) and outof-domain (News Commentary, News Crawl, etc) monolingual data. 3.2 Participants and System Descriptions We received 23 submissions from 11 teams. Except two submissions on the Eng-Fra language pair, all systems used the constrained setup. Below we briefly describe the systems from the 8 teams which submitted corresponding system description papers: Baidu & Oregon State University’s submission (Zheng et al., 2019): Their system is based on the Transformer implementation in OpenNMTpy (Klein et al., 2017). The main methods applied in their submission are: domain-sensitive data mixing and data augmentation with backtranslation. For data mixing, they used a special symbol on the source side to indicate the data domain. For data augmentation, they back-translate from a target language to its noisy source. The intuition, also observed by Michel and Neubig (2018), is that the source sentences are noisier than their target translations. They include out-ofdomain clean data during this step and differentiate data types with a special symbol on the target side. In addition, they also run a model ensem"
W19-5303,P18-1163,0,0.189473,"Missing"
W19-5303,W17-3204,1,0.848798,"re efforts from the community in building robust MT models. 2 Related Work The fragility of neural networks (Szegedy et al., 2013) has been shown to extend to neural machine translation models (Belinkov and Bisk, 2018; Heigold et al., 2017) and recent work focused on various aspects of the problem. From the identification of the causes of this brittleness, to the induction of (adversarial) inputs that trigger the unwanted behavior (attacks) and making such models robust against various types of noisy inputs (defenses); improving robustness has been receiving increasing attention in NMT. While Koehn and Knowles (2017) mentioned domain mismatch as a challenge for neural machine translation, Khayrallah and Koehn (2018) addressed noisy training data and focus on the types of noise occurring in web-crawled corpora. Michel and Neubig (2018) proposed a new dataset (MTNT) to test MT models for robustness to the types of noise encountered in the Internet and demonstrated that these challenges cannot be overcome by simple domain adaptation techniques alone. Belinkov and Bisk (2018) and Heigold et al. (2017) showed that NMT systems are very sensitive to slightly perturbed input forms, and hinted at the importance of"
W19-5303,W19-5362,0,0.149193,"this campaign. Unlike other participants, the winning team Naver Labs B´erard et al. (2019) and NTT (Murakami et al., 2019) applied data cleaning techniques in order to filter noisy parallel sentences. They filtered i) identical sentences on source and target side, ii) sentences that belonged to a language other than the source and target language, iii) sentences with length mismatch, and iv) also applied attention-based filtering. Data cleaning gave an improvement of more than 5 BLEU points with substantial reduction in the hallucination of the model for the winning team. NICT’s submission (Dabre and Sumita, 2019): The authors used Transformer models to train their systems and employed two strategies namely: i) mixed fine-tuning and ii) multilingual models for making the systems robust. The former helps as the in-domain data is available in a very small quantity. Using a mix of in-domain and outdomain data for fine-tuning helps overcome the problem of adjusting learning rate, applying better regularization and other complicated strategies. It is not clear how these two methods contributed towards making the models more robust. According to the authors, mixed fine-tuning and multilingual training (bidir"
W19-5303,D18-2012,0,0.020004,"mer-Big architecture, whereas improvements were substantially larger when the base models were RNN-Based MTNT baselines, about 8+ BLEU points. Participants emphasized the importance of their strong Transformer-Big base JHU’s submission (Post and Duh, 2019): This submission participated in the Fra→Eng and Jpn↔Eng tasks. The participants used data dual cross-entropy filtering for reducing the monolingual data, then back-translate these, and train their Transformer models (Vaswani et al., 2017). They compared Moses tokenization+Byte Pair Encoding (BPE) (Sennrich et al., 2016), and sentencepiece (Kudo and Richardson, 2018) (without any pre-processing) and found the two comparable, and that using larger sentence-piece models improved over smaller ones. For Jpn↔Eng (both di4 http://www.statmt.org/wmt19/biomedical-translationtask.html 95 sis, they found that their system performs poorly in translating emojis. The segmentation errors generated by KyTea resulted in further errors in the translation. rections) they first used both in-domain (MTNT) and out-of-domain data (other constrained), and then continued training (fine-tune) using MTNT only. They also reported many results from their hyper-parameter search (albe"
W19-5303,N19-1154,1,0.767588,"ise, such as grammatical errors produced by non-native speakers (Anastasopoulos et al., 2019) or errors extracted from Wikipedia edits (Belinkov and Bisk, 2018). It has also been shown that adding synthetic noise does not trivially increase robustness to natural noise (Belinkov and Bisk, 2018) and may require specific recipes (Karpukhin et al., 2019). Michel et al. (2019) recently emphasized the importance of meaning-preserving perturbations and along with Cheng et al. (2019) demonstrated the utility of adversarial training without significantly impairing performance on clean data and domain. Durrani et al. (2019) showed that character-based representations are more robust towards noise compared to such learned using BPE-based sub-word units in the task of machine translation. Jpn→Eng. We describe the dataset and the task setup in Section 3. The shared-task attracted a total of 23 submissions from 11 teams. The teams employed a variety of methods to improve robustness. A specific challenge was the small size of the in-domain noisy parallel dataset. We summarize the participating systems in Section 4 and the notable methods in Section 5. The contributions were evaluated both automatically and via a huma"
W19-5303,W18-6459,0,0.0413765,"ish (Eng) and French (Fra) and English and Japanese (Jpn), in four translation directions: Eng→Fra, Fra→Eng, Eng→Jpn, and www.reddit.com https://github.com/neulab/compare-mt 91 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 2: Shared Task Papers (Day 1) pages 91–102 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics examples are generated with access to the model parameters (Ebrahimi et al., 2018; Cheng et al., 2018a,b, 2019) and ii) black-box attacks, where examples are generated without accessing model internals (Zhao et al., 2018; Lee et al., 2018; ?; Anastasopoulos et al., 2019; Vaibhav et al., 2019); see Belinkov and Glass (2019) for a categorization of such work. In particular, some have focused on specific variations of naturally-occurring noise, such as grammatical errors produced by non-native speakers (Anastasopoulos et al., 2019) or errors extracted from Wikipedia edits (Belinkov and Bisk, 2018). It has also been shown that adding synthetic noise does not trivially increase robustness to natural noise (Belinkov and Bisk, 2018) and may require specific recipes (Karpukhin et al., 2019). Michel et al. (2019) recently emphasized th"
W19-5303,N19-1314,1,0.8427,"ernals (Zhao et al., 2018; Lee et al., 2018; ?; Anastasopoulos et al., 2019; Vaibhav et al., 2019); see Belinkov and Glass (2019) for a categorization of such work. In particular, some have focused on specific variations of naturally-occurring noise, such as grammatical errors produced by non-native speakers (Anastasopoulos et al., 2019) or errors extracted from Wikipedia edits (Belinkov and Bisk, 2018). It has also been shown that adding synthetic noise does not trivially increase robustness to natural noise (Belinkov and Bisk, 2018) and may require specific recipes (Karpukhin et al., 2019). Michel et al. (2019) recently emphasized the importance of meaning-preserving perturbations and along with Cheng et al. (2019) demonstrated the utility of adversarial training without significantly impairing performance on clean data and domain. Durrani et al. (2019) showed that character-based representations are more robust towards noise compared to such learned using BPE-based sub-word units in the task of machine translation. Jpn→Eng. We describe the dataset and the task setup in Section 3. The shared-task attracted a total of 23 submissions from 11 teams. The teams employed a variety of methods to improve ro"
W19-5303,C18-1055,0,0.064971,"n this first iteration, the shared-task used the MTNT dataset (Michel and Neubig, 2018) that contains noisy social media texts and their translations between English (Eng) and French (Fra) and English and Japanese (Jpn), in four translation directions: Eng→Fra, Fra→Eng, Eng→Jpn, and www.reddit.com https://github.com/neulab/compare-mt 91 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 2: Shared Task Papers (Day 1) pages 91–102 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics examples are generated with access to the model parameters (Ebrahimi et al., 2018; Cheng et al., 2018a,b, 2019) and ii) black-box attacks, where examples are generated without accessing model internals (Zhao et al., 2018; Lee et al., 2018; ?; Anastasopoulos et al., 2019; Vaibhav et al., 2019); see Belinkov and Glass (2019) for a categorization of such work. In particular, some have focused on specific variations of naturally-occurring noise, such as grammatical errors produced by non-native speakers (Anastasopoulos et al., 2019) or errors extracted from Wikipedia edits (Belinkov and Bisk, 2018). It has also been shown that adding synthetic noise does not trivially increase"
W19-5303,D18-1050,1,0.625161,"erstand the overall challenges in translating social media text and identify major themes of efforts which needs more research from the community. In recent years, Machine Translation (MT) systems have seen great progress, with neural models becoming the de-facto methods and even approaching human quality in news domain (Hassan et al., 2018). However, like other deep learning models, neural machine translation (NMT) models are found to be sensitive to synthetic and natural noise in input, distributional shift, and adversarial 1 2 In this first iteration, the shared-task used the MTNT dataset (Michel and Neubig, 2018) that contains noisy social media texts and their translations between English (Eng) and French (Fra) and English and Japanese (Jpn), in four translation directions: Eng→Fra, Fra→Eng, Eng→Jpn, and www.reddit.com https://github.com/neulab/compare-mt 91 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 2: Shared Task Papers (Day 1) pages 91–102 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics examples are generated with access to the model parameters (Ebrahimi et al., 2018; Cheng et al., 2018a,b, 2019) and ii) black-box attacks, where ex"
W19-5303,W19-5363,0,0.0447228,"e of tied multitask learning, where the noisy source sentences are first decoded by a same-language denoising decoder, and both information is passed on to the translation decoder. This approach requires data triples of noisy source, clean source, translation, which they created by data augmentation over the provided data, using tag-informed translation systems trained on either noisy (MTNT) or clean (Europarl) data. As the participants point out though, their performance improvements seems to be attributed to data augmentation and not to the intermediate denoising decoder. FOKUS’ submission (Grozea, 2019): This team participated in three directions: Eng→Fra, Fra→Eng and Jpn→Eng. For the Eng→Fra and Fra→Eng language pairs, the submissions are unconstrained systems, where the model was trained on the medical domain corpus provided by the WMT biomedical shared task 4 . Despite the training data being out-of-domain, removing “lowquality” parallel data such as “Subtitles” as the author hypothesized helped to bring 2 to 4 BLEU points improvement over the baseline models. Their Jpn→Eng submission is a constrained system, using the same model architecture as the Eng→Fra language pair. To improve robus"
W19-5303,N19-4007,1,0.827549,"ype tags (real or backtranslated) for further categorization of the training data. Compared to fine-tuning, adding tags provides them additional flexibility, resulting in a generalized system, robust towards a variety of input data. Human Evaluation The results of human evaluation following the evaluation protocol described in Section 3.4 are outlined in Table 2. Automatic Evaluation The automatic evaluation (BLEU) results of the Shared Task are summarized in Table 3. 6.2 Qualitative Analysis In order to discover salient differences between the methods, we performed analysis using compare-mt (Neubig et al., 2019), and present a few of the salient findings below. Fine-tuning Along with the noisy in-domain MTNT data, general domain data typically made available for WMT campaign was also allowed for this task. Most participants (Murakami et al., 2019; Dabre and Sumita, 2019; Helcl et al., 2019) trained on general domain data and fine-tuned the models towards the task. Murakami et al. (2019) did not see a consistent improvement with finetuning. Due to the small size of the in-domain data, Dabre and Sumita (2019) fine-tuned on a mix of in-domain and a subset of the out-of-domain data. Stronger Submissions"
W19-5303,W19-5364,0,0.144816,"Missing"
W19-5303,P02-1040,0,0.110507,"he translators were presented the original source sentence, the reference and the system output side by side. The order between the reference and the system output was randomized by the user interface. The translators rated both the reference and the translation on a scale from 1 to 100. For both the original source sentence and the reference, the original text was presented except for Eng-Jpn where the Japanese reference tokenized with KyTea was presented in order to be consistent with the systems’ outputs. The user interface for annotation is illustrated in Figure 1. We also evaluated BLEU (Papineni et al., 2002) for each system using SacreBLEU (Post, 2018). For all language pairs except Eng-Jpn, we used the original reference and SacreBLEU with the default options. In the case of Eng-Jpn, we used the reference tokenized with KyTea and the option --tokenize none. Task Setup The task includes two tracks, constrained and unconstrained depending on whether the system is trained on a predefined training datasets or not. The two tracks are evaluated by the same automatic and human evaluation protocol, however, they are compared separately. For the constrained system track, the task specifies two types of t"
W19-5303,D19-5506,0,0.114843,"thout accessing model internals (Zhao et al., 2018; Lee et al., 2018; ?; Anastasopoulos et al., 2019; Vaibhav et al., 2019); see Belinkov and Glass (2019) for a categorization of such work. In particular, some have focused on specific variations of naturally-occurring noise, such as grammatical errors produced by non-native speakers (Anastasopoulos et al., 2019) or errors extracted from Wikipedia edits (Belinkov and Bisk, 2018). It has also been shown that adding synthetic noise does not trivially increase robustness to natural noise (Belinkov and Bisk, 2018) and may require specific recipes (Karpukhin et al., 2019). Michel et al. (2019) recently emphasized the importance of meaning-preserving perturbations and along with Cheng et al. (2019) demonstrated the utility of adversarial training without significantly impairing performance on clean data and domain. Durrani et al. (2019) showed that character-based representations are more robust towards noise compared to such learned using BPE-based sub-word units in the task of machine translation. Jpn→Eng. We describe the dataset and the task setup in Section 3. The shared-task attracted a total of 23 submissions from 11 teams. The teams employed a variety of"
W19-5303,W18-2709,1,0.860168,"networks (Szegedy et al., 2013) has been shown to extend to neural machine translation models (Belinkov and Bisk, 2018; Heigold et al., 2017) and recent work focused on various aspects of the problem. From the identification of the causes of this brittleness, to the induction of (adversarial) inputs that trigger the unwanted behavior (attacks) and making such models robust against various types of noisy inputs (defenses); improving robustness has been receiving increasing attention in NMT. While Koehn and Knowles (2017) mentioned domain mismatch as a challenge for neural machine translation, Khayrallah and Koehn (2018) addressed noisy training data and focus on the types of noise occurring in web-crawled corpora. Michel and Neubig (2018) proposed a new dataset (MTNT) to test MT models for robustness to the types of noise encountered in the Internet and demonstrated that these challenges cannot be overcome by simple domain adaptation techniques alone. Belinkov and Bisk (2018) and Heigold et al. (2017) showed that NMT systems are very sensitive to slightly perturbed input forms, and hinted at the importance of injecting noisy examples during training, also known as adversarial examples. Further research propo"
W19-5303,W18-6319,0,0.0373317,"ce, the reference and the system output side by side. The order between the reference and the system output was randomized by the user interface. The translators rated both the reference and the translation on a scale from 1 to 100. For both the original source sentence and the reference, the original text was presented except for Eng-Jpn where the Japanese reference tokenized with KyTea was presented in order to be consistent with the systems’ outputs. The user interface for annotation is illustrated in Figure 1. We also evaluated BLEU (Papineni et al., 2002) for each system using SacreBLEU (Post, 2018). For all language pairs except Eng-Jpn, we used the original reference and SacreBLEU with the default options. In the case of Eng-Jpn, we used the reference tokenized with KyTea and the option --tokenize none. Task Setup The task includes two tracks, constrained and unconstrained depending on whether the system is trained on a predefined training datasets or not. The two tracks are evaluated by the same automatic and human evaluation protocol, however, they are compared separately. For the constrained system track, the task specifies two types of training data in addition to MTNT train set: •"
W19-5303,P16-1162,0,0.0719206,"on top of the base models with the Transformer-Big architecture, whereas improvements were substantially larger when the base models were RNN-Based MTNT baselines, about 8+ BLEU points. Participants emphasized the importance of their strong Transformer-Big base JHU’s submission (Post and Duh, 2019): This submission participated in the Fra→Eng and Jpn↔Eng tasks. The participants used data dual cross-entropy filtering for reducing the monolingual data, then back-translate these, and train their Transformer models (Vaswani et al., 2017). They compared Moses tokenization+Byte Pair Encoding (BPE) (Sennrich et al., 2016), and sentencepiece (Kudo and Richardson, 2018) (without any pre-processing) and found the two comparable, and that using larger sentence-piece models improved over smaller ones. For Jpn↔Eng (both di4 http://www.statmt.org/wmt19/biomedical-translationtask.html 95 sis, they found that their system performs poorly in translating emojis. The segmentation errors generated by KyTea resulted in further errors in the translation. rections) they first used both in-domain (MTNT) and out-of-domain data (other constrained), and then continued training (fine-tune) using MTNT only. They also reported many"
W19-5303,N19-1190,1,0.747855,"e (Jpn), in four translation directions: Eng→Fra, Fra→Eng, Eng→Jpn, and www.reddit.com https://github.com/neulab/compare-mt 91 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 2: Shared Task Papers (Day 1) pages 91–102 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics examples are generated with access to the model parameters (Ebrahimi et al., 2018; Cheng et al., 2018a,b, 2019) and ii) black-box attacks, where examples are generated without accessing model internals (Zhao et al., 2018; Lee et al., 2018; ?; Anastasopoulos et al., 2019; Vaibhav et al., 2019); see Belinkov and Glass (2019) for a categorization of such work. In particular, some have focused on specific variations of naturally-occurring noise, such as grammatical errors produced by non-native speakers (Anastasopoulos et al., 2019) or errors extracted from Wikipedia edits (Belinkov and Bisk, 2018). It has also been shown that adding synthetic noise does not trivially increase robustness to natural noise (Belinkov and Bisk, 2018) and may require specific recipes (Karpukhin et al., 2019). Michel et al. (2019) recently emphasized the importance of meaning-preserving perturbations and al"
W19-5303,D18-1316,0,0.0279519,"ations between English (Eng) and French (Fra) and English and Japanese (Jpn), in four translation directions: Eng→Fra, Fra→Eng, Eng→Jpn, and www.reddit.com https://github.com/neulab/compare-mt 91 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 2: Shared Task Papers (Day 1) pages 91–102 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics examples are generated with access to the model parameters (Ebrahimi et al., 2018; Cheng et al., 2018a,b, 2019) and ii) black-box attacks, where examples are generated without accessing model internals (Zhao et al., 2018; Lee et al., 2018; ?; Anastasopoulos et al., 2019; Vaibhav et al., 2019); see Belinkov and Glass (2019) for a categorization of such work. In particular, some have focused on specific variations of naturally-occurring noise, such as grammatical errors produced by non-native speakers (Anastasopoulos et al., 2019) or errors extracted from Wikipedia edits (Belinkov and Bisk, 2018). It has also been shown that adding synthetic noise does not trivially increase robustness to natural noise (Belinkov and Bisk, 2018) and may require specific recipes (Karpukhin et al., 2019). Michel et al. (2019) rece"
W19-5303,W19-5368,1,0.878409,"not experimented. Finally, participants point out one peculiarity they’ve noticed in the train/validation partitioning of the original MTNT dataset; validation source sentences being started with the letter “Y” followed by alphabetically sorted sentences (test partition not effected). The team experimented with the Fra→Eng and Eng→Fra translation directions, obtaining 43.6 and 36.4 BLEU-cased, respectively (3rd place in both). Their ablations show significant benefit from domain-sensitive training (+3 BLEU), with additional improvements from back-translation and ensembling. CMU’s submission (Zhou et al., 2019): This submission only participated in the Fra→Eng direction. They proposed the use of tied multitask learning, where the noisy source sentences are first decoded by a same-language denoising decoder, and both information is passed on to the translation decoder. This approach requires data triples of noisy source, clean source, translation, which they created by data augmentation over the provided data, using tag-informed translation systems trained on either noisy (MTNT) or clean (Europarl) data. As the participants point out though, their performance improvements seems to be attributed to da"
W19-5303,Q19-1004,1,\N,Missing
W19-5303,N19-1311,1,\N,Missing
W19-5303,W19-5366,0,\N,Missing
W19-5329,2015.iwslt-evaluation.11,0,0.015423,"ssion to the Fourth Conference on Machine Translation (WMT19) news translation shared task (Bojar et al., 2019). We built systems for both German-English and EnglishGerman. Our attempts are based on previous year’s submissions by Edinburgh (model architectures) (Sennrich et al., 2017), Microsoft (data filtering) (Junczys-Dowmunt, 2018), Facebook (backtranslation using sampling) (Edunov et al., 2018), and JHU (continued training on previous years’ test sets) (Koehn et al., 2018). Our models leverage several techniques popular in neural machine translation – backtranslation, continued training (Luong and Manning, 2015) and sentence filtering. We use Transformer-big (Vaswani et al., 2017) models trained on available bitext to generate backtranslations via sampling. These backtranslations are then scored and filtered using dual conditional cross-entropy and cross-entropy difference scores, then added to upMarian Marian3 (Junczys-Dowmunt et al., 2018) is a purely C++11 toolkit that allows for creation and training of neural machine translation models efficiently. Most of our models were built using Marian and the sample scripts therein. 1.2 Fairseq Fairseq4 (Ott et al., 2019) is a sequence-tosequence learning"
W19-5329,P10-2041,0,0.121565,"Missing"
W19-5329,N19-4009,0,0.0120926,"lation, continued training (Luong and Manning, 2015) and sentence filtering. We use Transformer-big (Vaswani et al., 2017) models trained on available bitext to generate backtranslations via sampling. These backtranslations are then scored and filtered using dual conditional cross-entropy and cross-entropy difference scores, then added to upMarian Marian3 (Junczys-Dowmunt et al., 2018) is a purely C++11 toolkit that allows for creation and training of neural machine translation models efficiently. Most of our models were built using Marian and the sample scripts therein. 1.2 Fairseq Fairseq4 (Ott et al., 2019) is a sequence-tosequence learning toolkit created with a focus on neural machine translation. It contains implementations for various standard NMT architectures and system components. Using this toolkit allows us to use sampling as a method for inference (Edunov et al., 2018). 1 https://ParaCrawl.eu/index.html http://CommonCrawl.org 3 https://marian-nmt.github.io/ 4 https://github.com/pytorch/fairseq 2 287 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 2: Shared Task Papers (Day 1) pages 287–293 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational"
W19-5329,D18-1045,0,0.142132,"Center for Language and Speech Processing Johns Hopkins University phi@jhu.edu We built our systems using the Marian and Fairseq toolkits. Introduction 1.1 This paper describes the Johns Hopkins University (JHU) submission to the Fourth Conference on Machine Translation (WMT19) news translation shared task (Bojar et al., 2019). We built systems for both German-English and EnglishGerman. Our attempts are based on previous year’s submissions by Edinburgh (model architectures) (Sennrich et al., 2017), Microsoft (data filtering) (Junczys-Dowmunt, 2018), Facebook (backtranslation using sampling) (Edunov et al., 2018), and JHU (continued training on previous years’ test sets) (Koehn et al., 2018). Our models leverage several techniques popular in neural machine translation – backtranslation, continued training (Luong and Manning, 2015) and sentence filtering. We use Transformer-big (Vaswani et al., 2017) models trained on available bitext to generate backtranslations via sampling. These backtranslations are then scored and filtered using dual conditional cross-entropy and cross-entropy difference scores, then added to upMarian Marian3 (Junczys-Dowmunt et al., 2018) is a purely C++11 toolkit that allows for"
W19-5329,P02-1040,0,0.105541,"Missing"
W19-5329,W18-2703,1,0.841732,"final dataset of 40.3M for En-De and 33.9M for De-En. Multiple Transformer-base models were trained over this data using Marian to serve as the primary translation models. A similar method was used to create training data for reranking models, except for these, we reused models whose backtranslations had been generated using beam search. The filtering methods described above resulted in slightly smaller subsets of backtranslated German and English data for the reranking models. Furthermore, the training set for the De-En reranking models was generated by exploiting iterative backtranslation (Hoang et al., 2018; Koehn et al., 2018) along with the filtering methods described. The adequacy score threshold used to filter backtranslations generated via beam search was e−4 . 4.3 • Use backtranslated data, upsampled bitext, and filtered ParaCrawl + Common Crawl to train Transformer-base translation models. • Perform continued training. • Ensemble decode using translation models. • Rerank translations using Transformer-base translation models for both language directions, and a language model for De-En. Reranking models were trained similar to Junczys-Dowmunt (2018) and Sennrich et al. (2017). Our training"
W19-5329,W18-6415,0,0.154373,"iments conducted in the process are also described. 1 Philipp Koehn Center for Language and Speech Processing Johns Hopkins University phi@jhu.edu We built our systems using the Marian and Fairseq toolkits. Introduction 1.1 This paper describes the Johns Hopkins University (JHU) submission to the Fourth Conference on Machine Translation (WMT19) news translation shared task (Bojar et al., 2019). We built systems for both German-English and EnglishGerman. Our attempts are based on previous year’s submissions by Edinburgh (model architectures) (Sennrich et al., 2017), Microsoft (data filtering) (Junczys-Dowmunt, 2018), Facebook (backtranslation using sampling) (Edunov et al., 2018), and JHU (continued training on previous years’ test sets) (Koehn et al., 2018). Our models leverage several techniques popular in neural machine translation – backtranslation, continued training (Luong and Manning, 2015) and sentence filtering. We use Transformer-big (Vaswani et al., 2017) models trained on available bitext to generate backtranslations via sampling. These backtranslations are then scored and filtered using dual conditional cross-entropy and cross-entropy difference scores, then added to upMarian Marian3 (Junczy"
W19-5329,P16-1162,0,0.0156556,"tree/master/wmt2017-transformer, using the same data and similar preprocessing. The data is the parallel training bitext provided in the WMT17 shared task, excluding Rapid. Punctuation normalization, tokenization, corpus cleaning and truecasing was applied using Moses (Koehn et al., 2007). The truecaser applied to the clean bitext was trained over the punctuation normalized, tokenized, and cleaned bitext, whereas the truecaser applied to other data, such as the data to backtranslate, was trained on ParaCrawl. We deviated slightly from the example and applied a joint byte pair encoding (BPE) (Sennrich et al., 2016) model that was trained previously over the ParaCrawl German-English bitext to form 32,000 subword units. For the 10 million lines of German monolingual news data to backtranslate, any sentences longer than 100 tokens as well as pairs with source/target length ratio exceeding 9 were discarded after BPE was applied using Moses’ clean-corpus-n.perl. Just as Junczys-Dowmunt (2018) replicated Edinburgh’s WMT17 results for En-De and upgraded to using the Transformer, we have replicated Junczys-Dowmunt (2018)’s replication with the Transformer-base model. The models were trained on upsampled WMT17 b"
W19-5329,P18-4020,0,0.0213018,"Missing"
W19-5329,W18-6417,1,0.86073,"e built our systems using the Marian and Fairseq toolkits. Introduction 1.1 This paper describes the Johns Hopkins University (JHU) submission to the Fourth Conference on Machine Translation (WMT19) news translation shared task (Bojar et al., 2019). We built systems for both German-English and EnglishGerman. Our attempts are based on previous year’s submissions by Edinburgh (model architectures) (Sennrich et al., 2017), Microsoft (data filtering) (Junczys-Dowmunt, 2018), Facebook (backtranslation using sampling) (Edunov et al., 2018), and JHU (continued training on previous years’ test sets) (Koehn et al., 2018). Our models leverage several techniques popular in neural machine translation – backtranslation, continued training (Luong and Manning, 2015) and sentence filtering. We use Transformer-big (Vaswani et al., 2017) models trained on available bitext to generate backtranslations via sampling. These backtranslations are then scored and filtered using dual conditional cross-entropy and cross-entropy difference scores, then added to upMarian Marian3 (Junczys-Dowmunt et al., 2018) is a purely C++11 toolkit that allows for creation and training of neural machine translation models efficiently. Most of"
W19-5329,P07-2045,1,0.0170442,"BLEU score gain when performing continued training using previous years’ test sets. Models were slightly different for the En-De and De-En directions, which is noted in the subsequent sections. 3 Model Description Our reproduction of Junczys-Dowmunt (2018), follows the example at https://github. com/marian-nmt/marian-examples/ tree/master/wmt2017-transformer, using the same data and similar preprocessing. The data is the parallel training bitext provided in the WMT17 shared task, excluding Rapid. Punctuation normalization, tokenization, corpus cleaning and truecasing was applied using Moses (Koehn et al., 2007). The truecaser applied to the clean bitext was trained over the punctuation normalized, tokenized, and cleaned bitext, whereas the truecaser applied to other data, such as the data to backtranslate, was trained on ParaCrawl. We deviated slightly from the example and applied a joint byte pair encoding (BPE) (Sennrich et al., 2016) model that was trained previously over the ParaCrawl German-English bitext to form 32,000 subword units. For the 10 million lines of German monolingual news data to backtranslate, any sentences longer than 100 tokens as well as pairs with source/target length ratio e"
W19-5404,D11-1033,0,0.45229,"irs from a noisy corpus; and Cui et al. (2013) use a graph-based random walk algorithm and extract phrase pair scores to weight the phrase translation probabilities to bias towards more trustworthy ones. Most of this work was done in the context of statistical machine translation, but more recent work targets neural models. Carpuat et al. (2017) focus on identifying semantic differences in translation pairs using cross-lingual textual entailment and additional length-based features, and demonstrate that removing such sentences improves neural machine translation performance. As Rarrick et al. (2011) point out, one type of noise in parallel corpora extracted from the web are translations that have been created by machine translation. Venugopal et al. (2011) propose a method to watermark the output of machine translation systems to aid this distinction, with a negligible loss of quality. Antonova and Misyurev (2011) report that rule-based machine translation output can be detected due to certain word choices, and statistical machine translation output can be detected due to lack of reordering. It is notable that none of the participants in our shared task have tried to detect machine trans"
W19-5404,W18-6477,0,0.0322741,"nce pair. Their method has shown promising results in filtering WMT Paracrawl data and has achieved state-of-the-art performance on the BUCC corpus mining task. 2.5 Use of embeddings. While the participant’s methods were dominated by non-neural components, sometimes using neural machine translation outputs and scores, some participants used word and sentence embeddings. Given crosslingual word embeddings, sentence match scores based on the difference between the average of the word embeddings (Paetzold, 2018), or, for each word in the sentence, the closest match in the corresponding sentence (Hangya and Fraser, 2018). Matching of word embeddings may also be done monolingually, after machine translating the foreign sentence into English (Lo et al., 2018). Cross-lingual word embeddings were obtained using uses monolingual word embedding spaces which were aligned with an unsupervised method, or using pre-trained cross-lingual word embeddings. Littell et al. (2018) used monolingual sentence embedding spaces to discount outliers. Pham et al. (2018) use a neural model that takes a sentence pair and predicts a matching score. Some participants made a distinction between unsupervised methods that did not use exis"
W19-5404,W16-2347,1,0.85374,"hn, 2005), the United Nations (Rafalovitch and Dale, 2009; Ziemski et al., 2015), or European Patents (T¨ager, 2011). A nice collection of the products of these efforts is the OPUS web site2 (Tiedemann, 2012). 2.1 Parallel Corpus Acquisition The Paracrawl project is currently engaged in a large-scale effort to crawl text from the web. That work is funded by the European Union via the Connecting Europe Facility. The Paracrawl infrastructure was used to generate the noisy parallel data for this shared task. In previous years, as part of the Paracrawl effort, a shared task on document alignment (Buck and Koehn, 2016) and a shared task on parallel corpus filtering was organized (Koehn et al., 2018). Acquiring parallel corpora from the web typically goes through the stages of identifying web sites with parallel text, downloading the pages of the web site, aligning document pairs, and aligning sentence pairs. A final stage of the processing pipeline filters out non parallel sentence pairs. These exist either because the original web site did not have any actual parallel data (garbage in, garbage out), only partial parallel data, or due to failures of earlier processing steps. 2.2 Filtering Noisy Parallel Cor"
W19-5404,W18-6478,0,0.0401905,"y consider sentence length, number of real words vs. other tokens, matching names, numbers, dates, email addresses, or URLs, too similar sentences (copied content), and language identification (Pinnis, 2018; Lu et al., 2018; Ash et al., 2018). Scoring functions. Sentence pairs that pass the pre-filtering stage are assessed with scoring functions which provide scores that hopefully correlate with quality of sentence pairs. Participants used a variety of such scoring functions, including language models, neural translation models and lexical translation probabilities, e.g., IBM Model 1 scores. (Junczys-Dowmunt, 2018; Rossenbach et al., 2018; Lo et al., 2018). 3 Low-Resource Corpus Filtering Task The shared task tackled the problem of filtering parallel corpora. Given a noisy parallel corpus (crawled from the web), participants developed methods to filter it to a smaller size of high quality sentence pairs. 56 Corpus Specifically, we provided a very noisy 5060 million word (English token count) Nepali– English and Sinhala–English corpora crawled from the web using the Paracrawl processing pipeline (see Section 4.4 for details). We asked participants to generate sentence-level quality scores that allow sel"
W19-5404,W17-3209,0,0.038209,"://www.paracrawl.eu/ 54 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 3: Shared Task Papers (Day 2) pages 54–72 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics 2 Related Work sifier to identify good sentence pairs from a noisy corpus; and Cui et al. (2013) use a graph-based random walk algorithm and extract phrase pair scores to weight the phrase translation probabilities to bias towards more trustworthy ones. Most of this work was done in the context of statistical machine translation, but more recent work targets neural models. Carpuat et al. (2017) focus on identifying semantic differences in translation pairs using cross-lingual textual entailment and additional length-based features, and demonstrate that removing such sentences improves neural machine translation performance. As Rarrick et al. (2011) point out, one type of noise in parallel corpora extracted from the web are translations that have been created by machine translation. Venugopal et al. (2011) propose a method to watermark the output of machine translation systems to aid this distinction, with a negligible loss of quality. Antonova and Misyurev (2011) report that rule-ba"
W19-5404,W18-2709,1,0.842012,"h web crawls. There is a robust body of work on filtering out noise in parallel data. For example: Taghipour et al. (2011) use an outlier detection algorithm to filter a parallel corpus; Xu and Koehn (2017) generate synthetic noisy data (inadequate and nonfluent translations) and use this data to train a clas2.3 Impact of Noise on Neural Machine Translation Belinkov and Bisk (2017) investigate the impact of noise on neural machine translation. They focus on creating systems that can translate the kinds of orthographic errors (typos, misspellings, etc.) that humans can comprehend. In contrast, Khayrallah and Koehn (2018) examine noisy training data and focus on types of noise occurring in web-crawled corpora. They carried out a study about how noise that occurs in crawled parallel text impacts statistical and neural machine translation. 2 http://opus.nlpl.eu NLP4TM 2016: Shared task http://rgcl.wlv.ac.uk/nlp4tm2016/shared-task/ 3 55 Learning weights for scoring functions. Given a large number of scoring functions, simply averaging their resulting scores may be inadequate. Learning weights to optimize machine translation system quality is computationally intractable due to the high cost of training these syste"
W19-5404,W19-5435,1,0.728844,"Missing"
W19-5404,2005.mtsummit-papers.11,1,0.234451,"ve for neural machine translation. This is different from our goals of handling noise since those methods tend to discard perfectly fine sentence pairs that are just not relevant for the targeted domain. Our task is focused on data quality that is relevant for all domains. Although the idea of crawling the web indiscriminately for parallel data goes back to the 20th century (Resnik, 1999), work in the academic community on extraction of parallel corpora from the web has so far mostly focused on large stashes of multilingual content in homogeneous form, such as the Canadian Hansards, Europarl (Koehn, 2005), the United Nations (Rafalovitch and Dale, 2009; Ziemski et al., 2015), or European Patents (T¨ager, 2011). A nice collection of the products of these efforts is the OPUS web site2 (Tiedemann, 2012). 2.1 Parallel Corpus Acquisition The Paracrawl project is currently engaged in a large-scale effort to crawl text from the web. That work is funded by the European Union via the Connecting Europe Facility. The Paracrawl infrastructure was used to generate the noisy parallel data for this shared task. In previous years, as part of the Paracrawl effort, a shared task on document alignment (Buck and"
W19-5404,P13-2061,0,0.0514257,"ngual noisy data such as web-crawls (e.g. from Wikipedia, Paracrawl1 ) is an important option. 1 This paper gives an overview of the task, presents the results for the participating systems and provides analysis on additional subset sizes and the average sentence length of sub-selected data. http://www.paracrawl.eu/ 54 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 3: Shared Task Papers (Day 2) pages 54–72 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics 2 Related Work sifier to identify good sentence pairs from a noisy corpus; and Cui et al. (2013) use a graph-based random walk algorithm and extract phrase pair scores to weight the phrase translation probabilities to bias towards more trustworthy ones. Most of this work was done in the context of statistical machine translation, but more recent work targets neural models. Carpuat et al. (2017) focus on identifying semantic differences in translation pairs using cross-lingual textual entailment and additional length-based features, and demonstrate that removing such sentences improves neural machine translation performance. As Rarrick et al. (2011) point out, one type of noise in paralle"
W19-5404,P07-2045,1,0.0139382,"ecifically, we provided a very noisy 5060 million word (English token count) Nepali– English and Sinhala–English corpora crawled from the web using the Paracrawl processing pipeline (see Section 4.4 for details). We asked participants to generate sentence-level quality scores that allow selecting subsets of sentence pairs that amount to (a) 1 million words, and (b) 5 million words, counted on the English side. These values were chosen as an approximation to the conditions on the WMT 2018 task. The resulting subsets were scored by building a statistical phrase-based machine translation system (Koehn et al., 2007) and a neural machine translation system (Ott et al., 2019) trained on this data, and then measuring their BLEU score on the flores Wikipedia test sets (Guzm´an et al., 2019). Participants in the shared task submitted a file with quality scores, one per line, corresponding to the sentence pairs. Scores are only required to have the property that higher scores indicate better quality. The scores were uploaded to a Google Drive folder which remains publicly accessible.4 For development purposes, we released configuration files and scripts that mirror the official testing procedure with a develop"
W19-5404,W19-5436,0,0.0439724,"Missing"
W19-5404,W18-6453,1,0.468977,"European Patents (T¨ager, 2011). A nice collection of the products of these efforts is the OPUS web site2 (Tiedemann, 2012). 2.1 Parallel Corpus Acquisition The Paracrawl project is currently engaged in a large-scale effort to crawl text from the web. That work is funded by the European Union via the Connecting Europe Facility. The Paracrawl infrastructure was used to generate the noisy parallel data for this shared task. In previous years, as part of the Paracrawl effort, a shared task on document alignment (Buck and Koehn, 2016) and a shared task on parallel corpus filtering was organized (Koehn et al., 2018). Acquiring parallel corpora from the web typically goes through the stages of identifying web sites with parallel text, downloading the pages of the web site, aligning document pairs, and aligning sentence pairs. A final stage of the processing pipeline filters out non parallel sentence pairs. These exist either because the original web site did not have any actual parallel data (garbage in, garbage out), only partial parallel data, or due to failures of earlier processing steps. 2.2 Filtering Noisy Parallel Corpora In 2016, a shared task on sentence pair filtering3 was organized, albeit in t"
W19-5404,W19-5437,0,0.044404,"Missing"
W19-5404,W17-3204,1,0.941154,"ural machine translation systems on these subsets, and measured their quality with the BLEU score on a test set of multi-domain Wikipedia content (Guzm´an et al., 2019). Introduction Machine Translation (MT) has experienced significant advances in recent years thanks to improvements in modeling, and in particular neural models (Bahdanau et al., 2015; Gehring et al., 2016; Vaswani et al., 2017). Unfortunately, today’s neural machine translation models, perform poorly on low-resource language pairs, for which clean, parallel training data is high-quality training data is lacking, by definition (Koehn and Knowles, 2017). Improving performance on low resource language pairs is very impactful considering that these languages are spoken by a large fraction of the world population. This is a particular challenge for industrial machine translation systems that need to support hundreds of languages in order to provide adequate services to their multilingual user base. In face of the scarcity of clean parallel data, learning to translate from any multilingual noisy data such as web-crawls (e.g. from Wikipedia, Paracrawl1 ) is an important option. 1 This paper gives an overview of the task, presents the results for"
W19-5404,W18-6317,0,0.0989903,"g high-quality sentence pairs or by using the raw crawled data (S´anchez-Cartagena et al., 2018). Neural machine translation model training may combine data selection and model training, taking advantage of the increasing quality of the model to better detect noisy data or to increasingly focus on cleaner parts of the data (Wang et al., 2018; Kumar et al., 2019). 2.4 Sentence Embeddings Bouamor and Sajjad (2018) learned sentence embeddings for the source and target languages and selected the nearest translation from a list of candidate sentences for a given source sentence using a classifier. Guo et al. (2018) leveraged hard negatives to correctly identify translation pairs. Artetxe and Schwenk (2018) use multilingual sentence embeddings to compute cosine similarity between the source and the target sentence. They further normalize the score by the average cosine similarity of the nearest neighbors for the given sentence pair. Their method has shown promising results in filtering WMT Paracrawl data and has achieved state-of-the-art performance on the BUCC corpus mining task. 2.5 Use of embeddings. While the participant’s methods were dominated by non-neural components, sometimes using neural machin"
W19-5404,N19-1208,0,0.0556731,"Missing"
W19-5404,L18-1548,0,0.0289109,"he same Devanagari script as Hindi and the languages are closely related. Neural machine translation models for low-resource language pairs have particularly benefited from training data in other language pairs, so parallel Hindi– English data and monolingual Hindi data may be beneficial to train models for our shared task. As shown in Table 4, we provide a relatively large 20 million word parallel corpus and almost 2 billion words of monolingual Hindi. This data was created from a variety of public domain sources and corpora developed at the Center for Indian Language Technology, IIT Bombay (Kunchukuttan et al., 2018). 4.4 English Words 58,537,167 60,999,374 not translations of each other, bad language (incoherent mix of words and non-words), incomplete or bad translations, etc. We used the processing pipeline of the Paracrawl project to create the data, using the clean parallel data to train underlying models such as the dictionary used by Hunalign (Varga et al., 2007) and a statistical translation model used by the document aligner. One modification was necessary to run the pipeline for Nepali due to the end-of-sentence symbol of the script that was previously not recognized by the sentence splitter. The"
W19-5404,W19-5438,0,0.0423381,"otal of 21 different submissions for Nepali and 23 different submissions for Sinhala that we scored. 5.2 A novel method that was central to the bestperforming submission was the use of crosslingual sentence embeddings that were directly trained from parallel sentence pairs (Chaudhary et al., 2019). Other submissions used monolingual word embeddings. These were first trained monolingually for each language from monolingual data. The resulting embedding spaces were mapped either in an unsupervised fashion (Soares and Costa-juss`a, 2019) or based on a dictionary ¨ learned from the parallel data (Kurfalı and Ostling, 2019). Bernier-Colborne and Lo (2019) use both monolingually trained word embeddings aligned in an unsupervised fashion and bilingually trained word embeddings. Methods used by Participants Almost all submissions used basic filtering rules as a first filtering step. These rules typically involve language identification and length considAnother approach is to first train a translation 59 Acronym AFRL DiDi Facebook Helsinki IITP Webinterpret NRC Stockholm SUNY Buffalo Sciling TALP-UPC Participant and System Description Citation Air Force Research Lab, USA (Erdmann and Gwinnup, 2019) DiDi, USA (Axelro"
W19-5404,W18-6486,0,0.0231679,"of submissions, many different approaches were explored for this task. However, most participants used a system using three components: (1) pre-filtering rules, (2) scoring functions for sentence pairs, and (3) a classifier that learned weights for feature functions. Pre-filtering rules. Some of the training data can be discarded based on simple deterministic filtering rules. These may include rules may consider sentence length, number of real words vs. other tokens, matching names, numbers, dates, email addresses, or URLs, too similar sentences (copied content), and language identification (Pinnis, 2018; Lu et al., 2018; Ash et al., 2018). Scoring functions. Sentence pairs that pass the pre-filtering stage are assessed with scoring functions which provide scores that hopefully correlate with quality of sentence pairs. Participants used a variety of such scoring functions, including language models, neural translation models and lexical translation probabilities, e.g., IBM Model 1 scores. (Junczys-Dowmunt, 2018; Rossenbach et al., 2018; Lo et al., 2018). 3 Low-Resource Corpus Filtering Task The shared task tackled the problem of filtering parallel corpora. Given a noisy parallel corpus (crawl"
W19-5404,W18-6480,0,0.0190114,"ntence embeddings. Given crosslingual word embeddings, sentence match scores based on the difference between the average of the word embeddings (Paetzold, 2018), or, for each word in the sentence, the closest match in the corresponding sentence (Hangya and Fraser, 2018). Matching of word embeddings may also be done monolingually, after machine translating the foreign sentence into English (Lo et al., 2018). Cross-lingual word embeddings were obtained using uses monolingual word embedding spaces which were aligned with an unsupervised method, or using pre-trained cross-lingual word embeddings. Littell et al. (2018) used monolingual sentence embedding spaces to discount outliers. Pham et al. (2018) use a neural model that takes a sentence pair and predicts a matching score. Some participants made a distinction between unsupervised methods that did not use existing parallel corpora to train parts of the system, and supervise methods that did. Unsupervised methods have the advantage that they can be readily deployed for language pairs for which no seed parallel corpora exist. Findings of the 2018 Shared Task The WMT 2018 Shared Task on Parallel Corpus Filtering (Koehn et al., 2018) attracted 18 submissions"
W19-5404,W18-6319,0,0.0276321,"with no additional data, and decoder beam size of 5,000 hypotheses. NMT For neural machine translation, we used fairseq (Ott et al., 2019) transformer model with the parameter settings shown in Figure 1. Preprocessing was done with sentence piece for a 5000 subword vocabulary on tokenized text using the Moses tokenizer (but no truecasing was used). Decoding was done with beam size 5 and length normalization 1.2. Training a system for the 1 million, and 5 million subsets took about 3, and 13 hours, respectively, on a single GTX 1080ti GPU. Scores on the test sets were computed with Sacrebleu (Post, 2018). We report case-insensitive scores. 9 https://github.com/facebookresearch/ flores#train-a-baseline-transformer-model 62 Nepali Submission AFRL 50k AFRL 150k Facebook main Facebook contrastive Helsinki Helsinki contrastive IITP IITP geom NRC ensemble NRC xlm NRC yisi-2-sup NRC yisi-2-unsup Stockholm Stockholm ngram SUNY Buffalo Sciling TALP-UPC primary TALP-UPC secondary Webinterpret primary Webinterpret cov Webinterpret prob 1 million SMT NMT test devt test devt 4.0 3.8 2.7 2.5 1.5 3.6 2.3 2.4 4.2 4.0 6.8 6.9 4.2 4.0 6.9 6.6 3.2 3.1 0.9 0.9 1.3 1.2 0.1 0.1 3.8 3.6 5.5 5.9 3.9 3.6 5.3 5.6 4.1"
W19-5404,W18-6481,0,0.165057,"mining task. 2.5 Use of embeddings. While the participant’s methods were dominated by non-neural components, sometimes using neural machine translation outputs and scores, some participants used word and sentence embeddings. Given crosslingual word embeddings, sentence match scores based on the difference between the average of the word embeddings (Paetzold, 2018), or, for each word in the sentence, the closest match in the corresponding sentence (Hangya and Fraser, 2018). Matching of word embeddings may also be done monolingually, after machine translating the foreign sentence into English (Lo et al., 2018). Cross-lingual word embeddings were obtained using uses monolingual word embedding spaces which were aligned with an unsupervised method, or using pre-trained cross-lingual word embeddings. Littell et al. (2018) used monolingual sentence embedding spaces to discount outliers. Pham et al. (2018) use a neural model that takes a sentence pair and predicts a matching score. Some participants made a distinction between unsupervised methods that did not use existing parallel corpora to train parts of the system, and supervise methods that did. Unsupervised methods have the advantage that they can b"
W19-5404,2009.mtsummit-posters.15,0,0.268115,"This is different from our goals of handling noise since those methods tend to discard perfectly fine sentence pairs that are just not relevant for the targeted domain. Our task is focused on data quality that is relevant for all domains. Although the idea of crawling the web indiscriminately for parallel data goes back to the 20th century (Resnik, 1999), work in the academic community on extraction of parallel corpora from the web has so far mostly focused on large stashes of multilingual content in homogeneous form, such as the Canadian Hansards, Europarl (Koehn, 2005), the United Nations (Rafalovitch and Dale, 2009; Ziemski et al., 2015), or European Patents (T¨ager, 2011). A nice collection of the products of these efforts is the OPUS web site2 (Tiedemann, 2012). 2.1 Parallel Corpus Acquisition The Paracrawl project is currently engaged in a large-scale effort to crawl text from the web. That work is funded by the European Union via the Connecting Europe Facility. The Paracrawl infrastructure was used to generate the noisy parallel data for this shared task. In previous years, as part of the Paracrawl effort, a shared task on document alignment (Buck and Koehn, 2016) and a shared task on parallel corpu"
W19-5404,2011.mtsummit-papers.48,0,0.0642035,"ood sentence pairs from a noisy corpus; and Cui et al. (2013) use a graph-based random walk algorithm and extract phrase pair scores to weight the phrase translation probabilities to bias towards more trustworthy ones. Most of this work was done in the context of statistical machine translation, but more recent work targets neural models. Carpuat et al. (2017) focus on identifying semantic differences in translation pairs using cross-lingual textual entailment and additional length-based features, and demonstrate that removing such sentences improves neural machine translation performance. As Rarrick et al. (2011) point out, one type of noise in parallel corpora extracted from the web are translations that have been created by machine translation. Venugopal et al. (2011) propose a method to watermark the output of machine translation systems to aid this distinction, with a negligible loss of quality. Antonova and Misyurev (2011) report that rule-based machine translation output can be detected due to certain word choices, and statistical machine translation output can be detected due to lack of reordering. It is notable that none of the participants in our shared task have tried to detect machine trans"
W19-5404,W18-6482,0,0.0212432,"s, many different approaches were explored for this task. However, most participants used a system using three components: (1) pre-filtering rules, (2) scoring functions for sentence pairs, and (3) a classifier that learned weights for feature functions. Pre-filtering rules. Some of the training data can be discarded based on simple deterministic filtering rules. These may include rules may consider sentence length, number of real words vs. other tokens, matching names, numbers, dates, email addresses, or URLs, too similar sentences (copied content), and language identification (Pinnis, 2018; Lu et al., 2018; Ash et al., 2018). Scoring functions. Sentence pairs that pass the pre-filtering stage are assessed with scoring functions which provide scores that hopefully correlate with quality of sentence pairs. Participants used a variety of such scoring functions, including language models, neural translation models and lexical translation probabilities, e.g., IBM Model 1 scores. (Junczys-Dowmunt, 2018; Rossenbach et al., 2018; Lo et al., 2018). 3 Low-Resource Corpus Filtering Task The shared task tackled the problem of filtering parallel corpora. Given a noisy parallel corpus (crawled from the web),"
W19-5404,P99-1068,0,0.360125,"l data relevant for a task-specific machine translation system (Axelrod et al., 2011). van der Wees et al. (2017) find that the existing data selection methods developed for statistical machine translation are less effective for neural machine translation. This is different from our goals of handling noise since those methods tend to discard perfectly fine sentence pairs that are just not relevant for the targeted domain. Our task is focused on data quality that is relevant for all domains. Although the idea of crawling the web indiscriminately for parallel data goes back to the 20th century (Resnik, 1999), work in the academic community on extraction of parallel corpora from the web has so far mostly focused on large stashes of multilingual content in homogeneous form, such as the Canadian Hansards, Europarl (Koehn, 2005), the United Nations (Rafalovitch and Dale, 2009; Ziemski et al., 2015), or European Patents (T¨ager, 2011). A nice collection of the products of these efforts is the OPUS web site2 (Tiedemann, 2012). 2.1 Parallel Corpus Acquisition The Paracrawl project is currently engaged in a large-scale effort to crawl text from the web. That work is funded by the European Union via the C"
W19-5404,N19-4009,0,0.190586,"sh token count) Nepali– English and Sinhala–English corpora crawled from the web using the Paracrawl processing pipeline (see Section 4.4 for details). We asked participants to generate sentence-level quality scores that allow selecting subsets of sentence pairs that amount to (a) 1 million words, and (b) 5 million words, counted on the English side. These values were chosen as an approximation to the conditions on the WMT 2018 task. The resulting subsets were scored by building a statistical phrase-based machine translation system (Koehn et al., 2007) and a neural machine translation system (Ott et al., 2019) trained on this data, and then measuring their BLEU score on the flores Wikipedia test sets (Guzm´an et al., 2019). Participants in the shared task submitted a file with quality scores, one per line, corresponding to the sentence pairs. Scores are only required to have the property that higher scores indicate better quality. The scores were uploaded to a Google Drive folder which remains publicly accessible.4 For development purposes, we released configuration files and scripts that mirror the official testing procedure with a development test set. The development pack consists of: Bible (two"
W19-5404,W18-6487,0,0.0336372,"Missing"
W19-5404,W18-6483,0,0.0192377,"her normalize the score by the average cosine similarity of the nearest neighbors for the given sentence pair. Their method has shown promising results in filtering WMT Paracrawl data and has achieved state-of-the-art performance on the BUCC corpus mining task. 2.5 Use of embeddings. While the participant’s methods were dominated by non-neural components, sometimes using neural machine translation outputs and scores, some participants used word and sentence embeddings. Given crosslingual word embeddings, sentence match scores based on the difference between the average of the word embeddings (Paetzold, 2018), or, for each word in the sentence, the closest match in the corresponding sentence (Hangya and Fraser, 2018). Matching of word embeddings may also be done monolingually, after machine translating the foreign sentence into English (Lo et al., 2018). Cross-lingual word embeddings were obtained using uses monolingual word embedding spaces which were aligned with an unsupervised method, or using pre-trained cross-lingual word embeddings. Littell et al. (2018) used monolingual sentence embedding spaces to discount outliers. Pham et al. (2018) use a neural model that takes a sentence pair and pred"
W19-5404,W18-6488,0,0.0961965,"Missing"
W19-5404,W19-5439,0,0.0372012,"Missing"
W19-5404,W19-5440,0,0.0387972,"Missing"
W19-5404,D17-1319,1,0.853992,"ther because the original web site did not have any actual parallel data (garbage in, garbage out), only partial parallel data, or due to failures of earlier processing steps. 2.2 Filtering Noisy Parallel Corpora In 2016, a shared task on sentence pair filtering3 was organized, albeit in the context of cleaning translation memories which tend to be cleaner than the data at the end of a pipeline that starts with web crawls. There is a robust body of work on filtering out noise in parallel data. For example: Taghipour et al. (2011) use an outlier detection algorithm to filter a parallel corpus; Xu and Koehn (2017) generate synthetic noisy data (inadequate and nonfluent translations) and use this data to train a clas2.3 Impact of Noise on Neural Machine Translation Belinkov and Bisk (2017) investigate the impact of noise on neural machine translation. They focus on creating systems that can translate the kinds of orthographic errors (typos, misspellings, etc.) that humans can comprehend. In contrast, Khayrallah and Koehn (2018) examine noisy training data and focus on types of noise occurring in web-crawled corpora. They carried out a study about how noise that occurs in crawled parallel text impacts st"
W19-5404,2011.eamt-1.25,0,0.259112,"Missing"
W19-5404,2011.mtsummit-papers.47,0,0.140127,"stage of the processing pipeline filters out non parallel sentence pairs. These exist either because the original web site did not have any actual parallel data (garbage in, garbage out), only partial parallel data, or due to failures of earlier processing steps. 2.2 Filtering Noisy Parallel Corpora In 2016, a shared task on sentence pair filtering3 was organized, albeit in the context of cleaning translation memories which tend to be cleaner than the data at the end of a pipeline that starts with web crawls. There is a robust body of work on filtering out noise in parallel data. For example: Taghipour et al. (2011) use an outlier detection algorithm to filter a parallel corpus; Xu and Koehn (2017) generate synthetic noisy data (inadequate and nonfluent translations) and use this data to train a clas2.3 Impact of Noise on Neural Machine Translation Belinkov and Bisk (2017) investigate the impact of noise on neural machine translation. They focus on creating systems that can translate the kinds of orthographic errors (typos, misspellings, etc.) that humans can comprehend. In contrast, Khayrallah and Koehn (2018) examine noisy training data and focus on types of noise occurring in web-crawled corpora. They"
W19-5404,tiedemann-2012-parallel,0,0.494303,"domain. Our task is focused on data quality that is relevant for all domains. Although the idea of crawling the web indiscriminately for parallel data goes back to the 20th century (Resnik, 1999), work in the academic community on extraction of parallel corpora from the web has so far mostly focused on large stashes of multilingual content in homogeneous form, such as the Canadian Hansards, Europarl (Koehn, 2005), the United Nations (Rafalovitch and Dale, 2009; Ziemski et al., 2015), or European Patents (T¨ager, 2011). A nice collection of the products of these efforts is the OPUS web site2 (Tiedemann, 2012). 2.1 Parallel Corpus Acquisition The Paracrawl project is currently engaged in a large-scale effort to crawl text from the web. That work is funded by the European Union via the Connecting Europe Facility. The Paracrawl infrastructure was used to generate the noisy parallel data for this shared task. In previous years, as part of the Paracrawl effort, a shared task on document alignment (Buck and Koehn, 2016) and a shared task on parallel corpus filtering was organized (Koehn et al., 2018). Acquiring parallel corpora from the web typically goes through the stages of identifying web sites with"
W19-5404,W19-5441,0,0.0427315,"Missing"
W19-5404,D11-1126,0,0.0528814,"slation probabilities to bias towards more trustworthy ones. Most of this work was done in the context of statistical machine translation, but more recent work targets neural models. Carpuat et al. (2017) focus on identifying semantic differences in translation pairs using cross-lingual textual entailment and additional length-based features, and demonstrate that removing such sentences improves neural machine translation performance. As Rarrick et al. (2011) point out, one type of noise in parallel corpora extracted from the web are translations that have been created by machine translation. Venugopal et al. (2011) propose a method to watermark the output of machine translation systems to aid this distinction, with a negligible loss of quality. Antonova and Misyurev (2011) report that rule-based machine translation output can be detected due to certain word choices, and statistical machine translation output can be detected due to lack of reordering. It is notable that none of the participants in our shared task have tried to detect machine translation. There is a rich literature on data selection which aims at sub-sampling parallel data relevant for a task-specific machine translation system (Axelrod e"
W19-5404,W18-6314,0,0.0587621,"ticipants used instead a classifier that learns how to distinguish between high-quality and low-quality sentence pairs. High-quality sentence pairs are selected from existing high-quality parallel corpora, while low-quality sentence pairs are either synthesized by scrambling high-quality sentence pairs or by using the raw crawled data (S´anchez-Cartagena et al., 2018). Neural machine translation model training may combine data selection and model training, taking advantage of the increasing quality of the model to better detect noisy data or to increasingly focus on cleaner parts of the data (Wang et al., 2018; Kumar et al., 2019). 2.4 Sentence Embeddings Bouamor and Sajjad (2018) learned sentence embeddings for the source and target languages and selected the nearest translation from a list of candidate sentences for a given source sentence using a classifier. Guo et al. (2018) leveraged hard negatives to correctly identify translation pairs. Artetxe and Schwenk (2018) use multilingual sentence embeddings to compute cosine similarity between the source and the target sentence. They further normalize the score by the average cosine similarity of the nearest neighbors for the given sentence pair. Th"
W19-5404,D17-1147,0,0.070816,"Missing"
W19-5435,D18-1549,0,0.141799,"e trained using the same setting as the public LASER encoder which involves normalizing texts and tokenization with Moses tools (falling back to the English mode). We first learn a joint 50k BPE vocabulary on the concatenated training data using fastBPE11 . The encoder sees Sinhala, Nepali, Hindi and English sentences at the input, without having any information about the current language. This input is always translated into English.12 We experimented with various techniques to add noise to the English input sentences, similar to what is used in unsupervised neural machine translation, e.g. (Artetxe et al., 2018; Lample et al., 2018), but this did not improve the results. The encoder is a five-layer BLSTM with 512 dimensional layers. The LSTM decoder has one hidden layer of size 2048, trained with the Adam optimizer. For development, we calculate similarity error on the concatenation of the flores dev sets for Sinhala–English and Nepali–English. Our models were trained for seven epochs for about 2.5 hours on 8 Nvidia GPUs. We experimented with various methods using a setup that closely mirrors the official scoring of the shared task. All methods are trained on the provided clean parallel data (see Ta"
W19-5435,D19-1632,1,0.878038,"Missing"
W19-5435,N19-4009,0,0.0476785,", we are provided with a very noisy 40.6 million-word (English token count) Nepali– English corpus and a 59.6 million-word Sinhala– English corpus crawled from the web as part of the Paracrawl project. The challenge consists of providing scores for each sentence pair in both noisy parallel sets. The scores will be used to subsample sentence pairs that amount to 1 million and 5 million English words. The quality of the resulting subsets is determined by the quality of a statistical machine translation (Moses, phrase-based (Koehn et al., 2007)) and the neural machine translation system fairseq (Ott et al., 2019) trained on this data. The quality of the machine translation system will be measured by BLEU score using SacreBLEU (Post, 2018) on a held-out test set of Wikipedia translations for Sinhala–English and Nepali–English from the flores dataset (Guzm´an et al., 2019). In our submission for this shared task, we use of multilingual sentence embeddings obtained from LASER2 which uses an encoder-decoder architecture to train a multilingual sentence representation model using a relatively small parallel corpus. Our experiments demonstrate that the proposed approach outperforms other existing approaches"
W19-5435,P13-2121,1,0.88624,"Missing"
W19-5435,W18-6319,0,0.0232643,"glish corpus crawled from the web as part of the Paracrawl project. The challenge consists of providing scores for each sentence pair in both noisy parallel sets. The scores will be used to subsample sentence pairs that amount to 1 million and 5 million English words. The quality of the resulting subsets is determined by the quality of a statistical machine translation (Moses, phrase-based (Koehn et al., 2007)) and the neural machine translation system fairseq (Ott et al., 2019) trained on this data. The quality of the machine translation system will be measured by BLEU score using SacreBLEU (Post, 2018) on a held-out test set of Wikipedia translations for Sinhala–English and Nepali–English from the flores dataset (Guzm´an et al., 2019). In our submission for this shared task, we use of multilingual sentence embeddings obtained from LASER2 which uses an encoder-decoder architecture to train a multilingual sentence representation model using a relatively small parallel corpus. Our experiments demonstrate that the proposed approach outperforms other existing approaches. Moreover we make use of an ensemble of multiple scoring functions to further boost the filtering performance. In this paper, w"
W19-5435,W18-6478,0,0.23575,"orks better than the global one. In that setting, LASER is on average 0.71 BLEU above the best non-LASER system. These gaps are higher for the 1M condition (0.94 BLEU). (iii) The best ensemble configuration provides small improvements over the best LASER configuration. For Sinhala–English the best configuration includes every other scoring method (ALL). For Nepali–English the best configuration is an ensemble of LASER scores. (iv) Dual cross entropy shows mixed results. For Sinhala–English, it only works once the language id filtering is enabled which is consistent with previous observations (Junczys-Dowmunt, 2018). For Nepali– English, it provides scores well below the rest of the scoring methods. Note that we did not perform an architecture exploration. LASER Encoder Training For our experiments and the official submission, we trained a multilingual sentence encoder using the permitted resources in Table 1. We trained a single encoder using all the parallel data for Sinhala–English, Nepali–English and HindiEnglish. Since Hindi and Nepali share the same script, we concatenated their corpora into a single parallel corpus. To account for the difference in size of the parallel training data, we over-sampl"
W19-5435,W18-6488,0,0.112392,"Missing"
W19-5435,W18-2709,1,0.554005,"ods and obtain additional gains. Our submission achieved the best overall performance for both the Nepali–English and Sinhala–English 1M tasks by a margin of 1.3 and 1.4 BLEU respectively, as compared to the second best systems. Moreover, our experiments show that this technique is promising for low and even no-resource scenarios. 1 Introduction The availability of high-quality parallel training data is critical for obtaining good translation performance, as neural machine translation (NMT) systems are less robust against noisy parallel data than statistical machine translation (SMT) systems (Khayrallah and Koehn, 2018). Recently, there is an increased interest in the filtering of noisy parallel corpora (such as Paracrawl1 ) to increase the amount of data that can be used to train translation systems (Koehn et al., 2018). While the state-of-the-art methods that use NMT models have proven effective in mining 1 2 http://www.paracrawl.eu/ https://github.com/facebookresearch/LASER 261 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 3: Shared Task Papers (Day 2) pages 261–266 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics 2 Methodology 2.1 LASER Multi"
W19-5435,P18-2037,1,0.903256,"tion systems (Koehn et al., 2018). While the state-of-the-art methods that use NMT models have proven effective in mining 1 2 http://www.paracrawl.eu/ https://github.com/facebookresearch/LASER 261 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 3: Shared Task Papers (Day 2) pages 261–266 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics 2 Methodology 2.1 LASER Multilingual Representations The underlying idea is to use the distances between two multilingual representations as a notion of parallelism between the two embedded sentences (Schwenk, 2018). To do this, we first train an encoder that learns to produce a multilingual, fixed-size sentence representation; and then compute a distance between two sentences in the learned embedding space. In addition, we use a margin criterion, which uses a k nearest neighbors approach to normalize the similarity scores given that cosine similarity is not globally consistent (Artetxe and Schwenk, 2018a). The WMT 2018 shared task for parallel corpus filtering (Koehn et al., 2018)3 introduced several methods to tackle a high-resource GermanEnglish data condition.While many of these methods were successf"
W19-5435,W18-6479,1,0.802465,"econd a local method, in which we only scored the noisy data using the noisy neighborhood, or the clean data using the clean neighborhood.5 4 We explored the absolute, distance and ratio margin criteria, but the latter worked best 5 this last part was only done for training an ensemble 3 http://statmt.org/wmt18/ parallel-corpus-filtering.html 262 2.2 Other Similarity Methods Forward and backward cross entropy scores, HF (y|x) and HB (x|y) respectively, are then averaged with an additional penalty on a large difference between the two scores |HF (y|x) − HB (x|y)|. Zipporah (Xu and Koehn, 2017; Khayrallah et al., 2018), which is often used as a baseline comparison, uses language model and word translation scores, with weights optimized to separate clean and synthetic noise data. In our setup, we trained Zipporah models for both language pairs Sinhala–English and Nepali–English. We used the open source release6 of the Zipporah tool without modifications. All components of the Zipporah model (probabilistic translation dictionaries and language models) were trained on the provided clean data (excluding the dictionaries). Language models were trained using KenLM (Heafield et al., 2013) over the clean parallel d"
W19-5435,D17-1319,1,0.834528,"th the clean data. Second a local method, in which we only scored the noisy data using the noisy neighborhood, or the clean data using the clean neighborhood.5 4 We explored the absolute, distance and ratio margin criteria, but the latter worked best 5 this last part was only done for training an ensemble 3 http://statmt.org/wmt18/ parallel-corpus-filtering.html 262 2.2 Other Similarity Methods Forward and backward cross entropy scores, HF (y|x) and HB (x|y) respectively, are then averaged with an additional penalty on a large difference between the two scores |HF (y|x) − HB (x|y)|. Zipporah (Xu and Koehn, 2017; Khayrallah et al., 2018), which is often used as a baseline comparison, uses language model and word translation scores, with weights optimized to separate clean and synthetic noise data. In our setup, we trained Zipporah models for both language pairs Sinhala–English and Nepali–English. We used the open source release6 of the Zipporah tool without modifications. All components of the Zipporah model (probabilistic translation dictionaries and language models) were trained on the provided clean data (excluding the dictionaries). Language models were trained using KenLM (Heafield et al., 2013)"
W19-5435,W19-5404,1,0.727968,"Missing"
W19-5435,P07-2045,1,0.0160857,"t known yet. For the task of low-resource filtering (Koehn et al., 2019), we are provided with a very noisy 40.6 million-word (English token count) Nepali– English corpus and a 59.6 million-word Sinhala– English corpus crawled from the web as part of the Paracrawl project. The challenge consists of providing scores for each sentence pair in both noisy parallel sets. The scores will be used to subsample sentence pairs that amount to 1 million and 5 million English words. The quality of the resulting subsets is determined by the quality of a statistical machine translation (Moses, phrase-based (Koehn et al., 2007)) and the neural machine translation system fairseq (Ott et al., 2019) trained on this data. The quality of the machine translation system will be measured by BLEU score using SacreBLEU (Post, 2018) on a held-out test set of Wikipedia translations for Sinhala–English and Nepali–English from the flores dataset (Guzm´an et al., 2019). In our submission for this shared task, we use of multilingual sentence embeddings obtained from LASER2 which uses an encoder-decoder architecture to train a multilingual sentence representation model using a relatively small parallel corpus. Our experiments demons"
W19-5435,W18-6453,1,\N,Missing
W19-6602,N12-1047,0,0.0245879,"thed 5-gram language model with KenLM (Heafield, 2011) used at runtime, hierarchical lexicalized reordering (Galley and Manning, 2008), a lexically-driven 5-gram operation sequence model (OSM) (Durrani et al., 2013) with count bin features (Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 200-best translation options, compact phrase table (Junczys-Dowmunt, 2012) minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test. We optimize feature function weights with k-best MIRA (Cherry and Foster, 2012). The NMT systems are LSTM sequence-tosequence models (Luong et al., 2015). The layer size is 512, and the number of layers is 4 for Swahili and Tagalog, 2 for Somali. The models were developed using the Fairseq4 toolkit. For NMT, we applied Byte Pair Encoding (BPE) (Sennrich et al., 2016) to split word into subword segments for both source and target languages. The number of BPE operations is 3000 for all three languages. We observed improvements in BLEU scores under small BPE settings for all three language pairs. We filtered noisy crawled bitext using Zipporah (Xu and Koehn, 2017) and appli"
W19-6602,N09-1025,0,0.130762,"Missing"
W19-6602,P13-2071,1,0.869211,"Missing"
W19-6602,W11-2123,0,0.0624649,"Missing"
W19-6602,P07-1019,0,0.0151049,", BLEU scores for PBMT and NMT systems. length of 80, grow-diag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011) used at runtime, hierarchical lexicalized reordering (Galley and Manning, 2008), a lexically-driven 5-gram operation sequence model (OSM) (Durrani et al., 2013) with count bin features (Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 200-best translation options, compact phrase table (Junczys-Dowmunt, 2012) minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test. We optimize feature function weights with k-best MIRA (Cherry and Foster, 2012). The NMT systems are LSTM sequence-tosequence models (Luong et al., 2015). The layer size is 512, and the number of layers is 4 for Swahili and Tagalog, 2 for Somali. The models were developed using the Fairseq4 toolkit. For NMT, we applied Byte Pair Encoding (BPE) (Sennrich et al., 2016) to split word into subword segments for both source and target languages. The number of BPE operations is 3000 for all three languages. We observed improvements in BL"
W19-6602,2012.eamt-1.58,0,0.0599057,"Missing"
W19-6602,P07-2045,1,0.0214013,"Missing"
W19-6602,N04-1022,0,0.0982591,"R1) and semi-supervised (ASR2) systems, BLEU scores for PBMT and NMT systems. length of 80, grow-diag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011) used at runtime, hierarchical lexicalized reordering (Galley and Manning, 2008), a lexically-driven 5-gram operation sequence model (OSM) (Durrani et al., 2013) with count bin features (Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 200-best translation options, compact phrase table (Junczys-Dowmunt, 2012) minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test. We optimize feature function weights with k-best MIRA (Cherry and Foster, 2012). The NMT systems are LSTM sequence-tosequence models (Luong et al., 2015). The layer size is 512, and the number of layers is 4 for Swahili and Tagalog, 2 for Somali. The models were developed using the Fairseq4 toolkit. For NMT, we applied Byte Pair Encoding (BPE) (Sennrich et al., 2016) to split word into subword segments for both source and target languages. The number of BPE operations is 3000 for all three la"
W19-6602,D15-1166,0,0.0210252,"hical lexicalized reordering (Galley and Manning, 2008), a lexically-driven 5-gram operation sequence model (OSM) (Durrani et al., 2013) with count bin features (Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 200-best translation options, compact phrase table (Junczys-Dowmunt, 2012) minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test. We optimize feature function weights with k-best MIRA (Cherry and Foster, 2012). The NMT systems are LSTM sequence-tosequence models (Luong et al., 2015). The layer size is 512, and the number of layers is 4 for Swahili and Tagalog, 2 for Somali. The models were developed using the Fairseq4 toolkit. For NMT, we applied Byte Pair Encoding (BPE) (Sennrich et al., 2016) to split word into subword segments for both source and target languages. The number of BPE operations is 3000 for all three languages. We observed improvements in BLEU scores under small BPE settings for all three language pairs. We filtered noisy crawled bitext using Zipporah (Xu and Koehn, 2017) and applied the unsupervised morphology induction tool Morfessor (Virpioja et al.,"
W19-6602,P16-1162,0,0.0535767,"mum phrase-length of 5, 200-best translation options, compact phrase table (Junczys-Dowmunt, 2012) minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test. We optimize feature function weights with k-best MIRA (Cherry and Foster, 2012). The NMT systems are LSTM sequence-tosequence models (Luong et al., 2015). The layer size is 512, and the number of layers is 4 for Swahili and Tagalog, 2 for Somali. The models were developed using the Fairseq4 toolkit. For NMT, we applied Byte Pair Encoding (BPE) (Sennrich et al., 2016) to split word into subword segments for both source and target languages. The number of BPE operations is 3000 for all three languages. We observed improvements in BLEU scores under small BPE settings for all three language pairs. We filtered noisy crawled bitext using Zipporah (Xu and Koehn, 2017) and applied the unsupervised morphology induction tool Morfessor (Virpioja et al., 2013) to split words up into putative morphemes, with keeping numbers and names unchanged. We noticed that splitting the words to morphemes improves BLEU scores for Somali and Swahili, but does not help for Tagalog."
W19-6602,D17-1319,1,0.933621,"M 808k 5.2M 759k 12.3M test (#sent) 9.5k 11.7k 11.4k Table 2: ASR and MT data statistics. We used parallel corpora (bitext) of around 800k English words to train our MT systems for translating from Somali, Swahili, or Tagalog to English. This data is provided in the BUILD package of the MATERIAL project and contains news, topical, and blog texts with provided source URLs. In addition, we harvested and filtered bitext from Web to augment this baseline bitext. We made this data publicly available2 . It is important to filter web bitext to reduce noise. We filtered the web bitext using Zipporah (Xu and Koehn, 2017) and chose filter thresholds optimized on tune sets. The crawled data improved the MT system by 1 point BLEU or more for these languages. We also added monolingual WMT news and LDC Gigaword data, which include 8.2 billion English tokens in total to train the language models of our MT systems. The IR system indexes and searches ”test” documents that are either speech or text. There are around 20 hours of test speech data and 10k foreign sentences of test text data for each language. We have the reference transcripts and translations of ”test”, hence, we can measure the performance of our ASR an"
W19-6619,R15-1014,0,0.0180029,"tajner and Popovi´c (2016) investigate how simplifying source-side sentences affects adequacy and fluency in English–Serbian translation. Interestingly, we notice qualitative similarities between our “complex” translations and the formal output of Niu et al. (2017), though the authors did not frame these qualitative differences as increases in complexity. Prior work in machine translation and NLP has focused on readability assessment and text simplification. For readability assessment, a data-driven method is proposed in Le et al. (2018) for assessing the readability of document text, whereas Ciobanu et al. (2015) investigated the readability of the MT system output with standard metrics. Jones et al. (2005) also investigated the readability of MT and ASR system output but with human evaluation. As for text simplification, Hardmeier et al. (2013) proposes a document-level decoder for SMT and mentioned a case study that utilizes document-wide features to improve the readability of text. Contrary to Stymne et al. (2013), Xu et al. (2016) designed a new training objective for SMT text simplification. Similar to Le et al. (2018), Ciobanu et al. (2015), and Jones et al. (2005), we adopted evaluation metrics"
W19-6619,W17-4912,0,0.016077,"of the data. This lower-data condition likely contributes to the lower BLEU score for double-decoder models, and explains why the data tagging approach does not suffer the same loss in BLEU. This also suggests that the data tagging approach may be preferable in low-resource settings. That said, human evaluators rated translations from the double-decoder approach higher than a baseline with similar BLEU performance. 7 Related Work Our work is similar to style transfer and work controlling style during natural language generation (e.g., (Carlson et al., 2017; Fu et al., 2017; John et al., 2018; Ficler and Goldberg, 2017)), and to the text simplification literature (e.g., (Napoles and Dredze, 2010; Nisioi et al., 2017)). In style transfer, NMT methods using double-decoder architectures have been used, for instance, to output formal vs. informal or positive vs. negative versions of a source sentence (e.g., (Fu et al., 2017; Prabhumoye et al., 2018)). Sennrich et al. (2016) use tokens similar to our complexity tags in NMT to specify politeness in their English-German output. Vanmassenhove et al. (2018) and Kobus et al. (2016) retain gender information and domain information, respectively, in NMT through a tag to"
W19-6619,W12-2207,0,0.0798718,"Missing"
W19-6619,P13-4033,0,0.0215926,"al output of Niu et al. (2017), though the authors did not frame these qualitative differences as increases in complexity. Prior work in machine translation and NLP has focused on readability assessment and text simplification. For readability assessment, a data-driven method is proposed in Le et al. (2018) for assessing the readability of document text, whereas Ciobanu et al. (2015) investigated the readability of the MT system output with standard metrics. Jones et al. (2005) also investigated the readability of MT and ASR system output but with human evaluation. As for text simplification, Hardmeier et al. (2013) proposes a document-level decoder for SMT and mentioned a case study that utilizes document-wide features to improve the readability of text. Contrary to Stymne et al. (2013), Xu et al. (2016) designed a new training objective for SMT text simplification. Similar to Le et al. (2018), Ciobanu et al. (2015), and Jones et al. (2005), we adopted evaluation metrics for assessing the MT output. However, the readability constraint is taken into account during training in our proposed approaches. Stymne et al. (2013) introduces document-level features such as type/token ratios and lexical consistency"
W19-6619,W18-6415,0,0.0265194,"NMT systems with no architectural changes. We demonstrate a trade-off between more effective control of reading level and BLEU score, particularly with the double-decoder approach. As the data partition becomes more aggressive, the difference in reading level between the two modes increases, but BLEU score drops. We show that this effect can be mitigated by oversampling. In the future, we plan to experiment with different language pairs and readability scorers. We also plan to discard sentences with very low readability scores and filter training corpora to exclude low-quality examples, which Junczys-Dowmunt (2018) demonstrated can severely degrade model performance. We expect these methods will help us retain better BLEU. Furthermore, we will use the state-of-the-art transformer model which we expect to provide improved BLEU and greater control over reading level in the data tagging method, because the complexity tag will contribute to each word’s representation via self-attention (Vaswani et al., 2017). Finally, we observed exciting effects related to formality which are outside the scope of this paper. Particularly when training on Europarl and OpenSubtitles2018 data, we observed that sentences train"
W19-6619,P17-4012,0,0.0613625,"Missing"
W19-6619,P07-2045,1,0.0163797,"Missing"
W19-6619,2005.mtsummit-papers.11,1,0.182505,"Missing"
W19-6619,W18-3714,0,0.0256489,"le Niu et al. (2017) control formality in French–English ˇ translation. Stajner and Popovi´c (2016) investigate how simplifying source-side sentences affects adequacy and fluency in English–Serbian translation. Interestingly, we notice qualitative similarities between our “complex” translations and the formal output of Niu et al. (2017), though the authors did not frame these qualitative differences as increases in complexity. Prior work in machine translation and NLP has focused on readability assessment and text simplification. For readability assessment, a data-driven method is proposed in Le et al. (2018) for assessing the readability of document text, whereas Ciobanu et al. (2015) investigated the readability of the MT system output with standard metrics. Jones et al. (2005) also investigated the readability of MT and ASR system output but with human evaluation. As for text simplification, Hardmeier et al. (2013) proposes a document-level decoder for SMT and mentioned a case study that utilizes document-wide features to improve the readability of text. Contrary to Stymne et al. (2013), Xu et al. (2016) designed a new training objective for SMT text simplification. Similar to Le et al. (2018),"
W19-6619,L16-1147,0,0.0666189,"Missing"
W19-6619,D15-1166,0,0.0392706,"OS+Europarl baseline, and 3,000 held-out lines from ParaCrawl for the ParaCrawl baseline. Double-decoder models are validated by assessing the performance of each decoder separately on the development set. The test sets are newstest2013 (3,000 lines), a combined test set of newstest2013 plus 10,000 held-out lines from OpenSubtitles2018, and 3,000 held-out lines from ParaCrawl. 3 https://ParaCrawl.eu/releases.html, version 1 Proceedings of MT Summit XVII, volume 1 5.3 Data Preprocessing Models & Training The basic model architecture is the default RNNbased encoder-decoder model with attention (Luong et al., 2015) from OpenNMT. The encoder and decoder are two-layer LSTMs (Hochreiter and Schmidhuber, 1997) with a 500-dimension hidden size and 500-dimension word embeddings. The models were trained with batch size 64 using stochastic gradient descent with the default initial learning rate of 1.0. We decay the learning rate by a factor of 0.5 starting at 50,000 steps, and further decay every subsequent 10,000 steps. Each model was trained until performance on the validation set ceased to improve. For testing, we chose the model with lowest validation perplexity. For double-decoder models, lowest perplexity"
W19-6619,W10-0406,0,0.0335329,"re for double-decoder models, and explains why the data tagging approach does not suffer the same loss in BLEU. This also suggests that the data tagging approach may be preferable in low-resource settings. That said, human evaluators rated translations from the double-decoder approach higher than a baseline with similar BLEU performance. 7 Related Work Our work is similar to style transfer and work controlling style during natural language generation (e.g., (Carlson et al., 2017; Fu et al., 2017; John et al., 2018; Ficler and Goldberg, 2017)), and to the text simplification literature (e.g., (Napoles and Dredze, 2010; Nisioi et al., 2017)). In style transfer, NMT methods using double-decoder architectures have been used, for instance, to output formal vs. informal or positive vs. negative versions of a source sentence (e.g., (Fu et al., 2017; Prabhumoye et al., 2018)). Sennrich et al. (2016) use tokens similar to our complexity tags in NMT to specify politeness in their English-German output. Vanmassenhove et al. (2018) and Kobus et al. (2016) retain gender information and domain information, respectively, in NMT through a tag to improve the Dublin, Aug. 19-23, 2019 |p. 200 translation quality. As far as"
W19-6619,P17-2014,0,0.0498848,"ls, and explains why the data tagging approach does not suffer the same loss in BLEU. This also suggests that the data tagging approach may be preferable in low-resource settings. That said, human evaluators rated translations from the double-decoder approach higher than a baseline with similar BLEU performance. 7 Related Work Our work is similar to style transfer and work controlling style during natural language generation (e.g., (Carlson et al., 2017; Fu et al., 2017; John et al., 2018; Ficler and Goldberg, 2017)), and to the text simplification literature (e.g., (Napoles and Dredze, 2010; Nisioi et al., 2017)). In style transfer, NMT methods using double-decoder architectures have been used, for instance, to output formal vs. informal or positive vs. negative versions of a source sentence (e.g., (Fu et al., 2017; Prabhumoye et al., 2018)). Sennrich et al. (2016) use tokens similar to our complexity tags in NMT to specify politeness in their English-German output. Vanmassenhove et al. (2018) and Kobus et al. (2016) retain gender information and domain information, respectively, in NMT through a tag to improve the Dublin, Aug. 19-23, 2019 |p. 200 translation quality. As far as we are aware, we are t"
W19-6619,Q16-1029,0,0.0434805,"nt and text simplification. For readability assessment, a data-driven method is proposed in Le et al. (2018) for assessing the readability of document text, whereas Ciobanu et al. (2015) investigated the readability of the MT system output with standard metrics. Jones et al. (2005) also investigated the readability of MT and ASR system output but with human evaluation. As for text simplification, Hardmeier et al. (2013) proposes a document-level decoder for SMT and mentioned a case study that utilizes document-wide features to improve the readability of text. Contrary to Stymne et al. (2013), Xu et al. (2016) designed a new training objective for SMT text simplification. Similar to Le et al. (2018), Ciobanu et al. (2015), and Jones et al. (2005), we adopted evaluation metrics for assessing the MT output. However, the readability constraint is taken into account during training in our proposed approaches. Stymne et al. (2013) introduces document-level features such as type/token ratios and lexical consistency as input to the MT system. On the other hand, our approaches at most require an additional simplicity/complexity tag. Different from Xu et al. (2016) in which new training objective is propose"
W19-6619,D17-1299,0,0.0436116,"to specify politeness in their English-German output. Vanmassenhove et al. (2018) and Kobus et al. (2016) retain gender information and domain information, respectively, in NMT through a tag to improve the Dublin, Aug. 19-23, 2019 |p. 200 translation quality. As far as we are aware, we are the first authors to use NMT to both reduce and increase the complexity of translations. Unlike most of the text simplification literature, we simplify output crosslinguistically and also increase text complexity. In statistical machine translation, Stymne et al. (2013) translate and simplify output, while Niu et al. (2017) control formality in French–English ˇ translation. Stajner and Popovi´c (2016) investigate how simplifying source-side sentences affects adequacy and fluency in English–Serbian translation. Interestingly, we notice qualitative similarities between our “complex” translations and the formal output of Niu et al. (2017), though the authors did not frame these qualitative differences as increases in complexity. Prior work in machine translation and NLP has focused on readability assessment and text simplification. For readability assessment, a data-driven method is proposed in Le et al. (2018) for"
W19-6619,P02-1040,0,0.108107,"Missing"
W19-6619,P18-1080,0,0.0134561,"om the double-decoder approach higher than a baseline with similar BLEU performance. 7 Related Work Our work is similar to style transfer and work controlling style during natural language generation (e.g., (Carlson et al., 2017; Fu et al., 2017; John et al., 2018; Ficler and Goldberg, 2017)), and to the text simplification literature (e.g., (Napoles and Dredze, 2010; Nisioi et al., 2017)). In style transfer, NMT methods using double-decoder architectures have been used, for instance, to output formal vs. informal or positive vs. negative versions of a source sentence (e.g., (Fu et al., 2017; Prabhumoye et al., 2018)). Sennrich et al. (2016) use tokens similar to our complexity tags in NMT to specify politeness in their English-German output. Vanmassenhove et al. (2018) and Kobus et al. (2016) retain gender information and domain information, respectively, in NMT through a tag to improve the Dublin, Aug. 19-23, 2019 |p. 200 translation quality. As far as we are aware, we are the first authors to use NMT to both reduce and increase the complexity of translations. Unlike most of the text simplification literature, we simplify output crosslinguistically and also increase text complexity. In statistical machi"
W19-6619,N16-1005,0,0.222204,"t encoder-decoder pair. For a simple sentence, the encoder is paired with the “simple” decoder. In this way, the encoder learns a shared representation for all source sentences, while separate decoders tune themselves to sentences that have the desired reading level. At inference time, we pass a flag indicating whether we want the output to be “simple” or “complex”. The corresponding decoder then translates the test set. Proposed Approaches In this paper, we develop two training methods which allow some control over the reading level of machine translation output. 4.1 Data Tagging Inspired by Sennrich et al. (2016)’s work controlling politeness, our first approach utilizes a short text token added to the end of each source-side training sentence, which corresponds to the matching target-side sentence’s readability. The intuition behind this method is that the attention mechanism will learn to pay attention to the complexity token when decoding in the simple or complex setting. A token indicating whether each training sentence pair is of low or high reading level is used if the target sentence meets a preset readability threshold. A third token indicating intermediate reading level is added to sentences"
W19-6619,W16-3411,0,0.0605909,"Missing"
W19-6619,W13-5634,0,0.120754,"l. (2016) use tokens similar to our complexity tags in NMT to specify politeness in their English-German output. Vanmassenhove et al. (2018) and Kobus et al. (2016) retain gender information and domain information, respectively, in NMT through a tag to improve the Dublin, Aug. 19-23, 2019 |p. 200 translation quality. As far as we are aware, we are the first authors to use NMT to both reduce and increase the complexity of translations. Unlike most of the text simplification literature, we simplify output crosslinguistically and also increase text complexity. In statistical machine translation, Stymne et al. (2013) translate and simplify output, while Niu et al. (2017) control formality in French–English ˇ translation. Stajner and Popovi´c (2016) investigate how simplifying source-side sentences affects adequacy and fluency in English–Serbian translation. Interestingly, we notice qualitative similarities between our “complex” translations and the formal output of Niu et al. (2017), though the authors did not frame these qualitative differences as increases in complexity. Prior work in machine translation and NLP has focused on readability assessment and text simplification. For readability assessment, a"
W19-6619,D18-1334,0,0.0349136,"lling style during natural language generation (e.g., (Carlson et al., 2017; Fu et al., 2017; John et al., 2018; Ficler and Goldberg, 2017)), and to the text simplification literature (e.g., (Napoles and Dredze, 2010; Nisioi et al., 2017)). In style transfer, NMT methods using double-decoder architectures have been used, for instance, to output formal vs. informal or positive vs. negative versions of a source sentence (e.g., (Fu et al., 2017; Prabhumoye et al., 2018)). Sennrich et al. (2016) use tokens similar to our complexity tags in NMT to specify politeness in their English-German output. Vanmassenhove et al. (2018) and Kobus et al. (2016) retain gender information and domain information, respectively, in NMT through a tag to improve the Dublin, Aug. 19-23, 2019 |p. 200 translation quality. As far as we are aware, we are the first authors to use NMT to both reduce and increase the complexity of translations. Unlike most of the text simplification literature, we simplify output crosslinguistically and also increase text complexity. In statistical machine translation, Stymne et al. (2013) translate and simplify output, while Niu et al. (2017) control formality in French–English ˇ translation. Stajner and P"
W19-6624,P17-1080,0,0.125148,"ifferent morphologically rich languages. Furthermore, tuning deep character-level models is expensive, even for low-resource settings.1 A middle-ground alternative is character-aware word-level modeling. Here, the NMT system operates over words but uses word embeddings that are sensitive to spellings and thereby has the ability to learn morphological patterns in the language. Such character-aware approaches have been applied successfully in NMT to the source-side word embedding layer (Costa-juss`a and Fonollosa, 2016), but surprisingly, similar gains have not been achieved on the target side (Belinkov et al., 2017). While source-side character-aware models only need to make the source embedding layer character-aware, on the target-side we require both the target embedding layer and the softmax layer 2 1 The dropout rate was found to be critical in Cherry et al. (2018), and each tuning run takes much longer due to longer sequence lengths. 2 Also referred to as generator, final output layer or final linear Dublin, Aug. 19-23, 2019 |p. 244 to be character-aware, which presents additional challenges. We find that the trivial application of methods from Costa-juss`a and Fonollosa (2016) to these target-side"
W19-6624,W16-4117,0,0.012734,"with Noam optimization and 100 warmup steps (Gehring et al., 2017). As Table 3 shows, our CG model with 30k BPE compares favorably to even deep characterlevel models for this low-resource setting. 6 Analysis We are interested in understanding whether our character-aware model is exploiting morphological patterns in the target language. We investigate this by inspecting the relationship between a set of hand-picked features and improvements obtained by our model over the baseline at wordlevel inputs. These features fall into two categories, corpus-dependent and corpus-independent. We following Bentz et al. (2016), and extract features known to correlate with human judgments of morphological complexity. The following corpusdependent features were used: 7 Increasing the recurrent size for deep models resulted in significant drop in BLEU scores. We set the dropout rate to 0.1. Dublin, Aug. 19-23, 2019 |p. 249 (i) Type-Token Ratio (TT): the ratio of the number of word types to the total number of word tokens in the target side. We note that a large corpus tends to have a smaller type-token ratio compared to small corpus. (ii) Word-Alignment Score (A): computed as A = |many-to-one|−|one-to-many| . One-to|a"
W19-6624,D18-1461,0,0.20156,"XVII, volume 1 tice to mitigate the vocabulary size problem with Byte-Pair Encoding (BPE) (Gage, 1994; Sennrich et al., 2016). BPE iteratively merges consecutive characters into larger chunks based on their frequency, which results in the breaking up of less common words into “subword units.” While BPE addresses the vocabulary size problem, the spellings of the subword units are still ignored. On the other hand, purely character-level NMT translates one character at a time and can implicitly learn about morphological patterns within words as well as generalize to unseen vocabulary. Recently, Cherry et al. (2018) show that very deep character-level models can outperform BPE, however, the smallest data size evaluated was 2 million sentences, so it is unclear if the results hold for low-resource settings and when translating into a range of different morphologically rich languages. Furthermore, tuning deep character-level models is expensive, even for low-resource settings.1 A middle-ground alternative is character-aware word-level modeling. Here, the NMT system operates over words but uses word embeddings that are sensitive to spellings and thereby has the ability to learn morphological patterns in the"
W19-6624,P16-1160,0,0.0161513,"ard word representations, and found no improvements. Our work is also aligned with the characteraware models proposed in (Kim et al., 2016), but projection. Proceedings of MT Summit XVII, volume 1 we additionally employ a gating mechanism between character-aware representations and standard word representations similar to language modeling work by (Miyamoto and Cho, 2016). However, our gating is a learned type-specific vector rather than a fixed hyperparameter. There is additionally a line of work on purely character-level NMT, which generates words one character at a time (Ling et al., 2015; Chung et al., 2016; Passban et al., 2018). While initial results here were not strong, Cherry et al. (2018) revisit this with deeper architectures and sweeping dropout parameters and find that they outperform BPE across settings of the merge hyperparameter. They examine different data sizes and observe improvements in the smaller data size settings— however, the smallest size is about 2 million sentence pairs. In contrast, we look at a smaller order of magnitude data size and present an alternate approach which doesn’t require substantial tuning of parameters across different languages. Finally, Byte-Pair Encod"
W19-6624,P16-2058,0,0.0466523,"Missing"
W19-6624,P07-1033,0,0.352858,"Missing"
W19-6624,N13-1073,0,0.0363299,"ge list of inflected words (in several languages) along with the word’s lemma and a set of morphological tags. For example, the French UniMorph corpus contains the word marchai (walked), which is associated with its lemma, marcher and a set of morphological tags {V,IND,PST,1,SG,PFV}. There are 19 such tags in the French UniMorph corpus. A morphologically richer language like Hungarian, for example, has 36 distinct tags. We used the number of distinct tags (UT) and the number of different tag combinations (UTC) that appear in the UniMorph corpus for each language. Note that 8 We use FastAlign (Dyer et al., 2013) for word alignments with the grow-diag-final-and heuristic from (Och and Ney, 2003) for symmetrization. Proceedings of MT Summit XVII, volume 1 we do not filter out words (and its associated tags) from the UniMorph corpus that are absent in our parallel data. This ensures that the UT and UTC features are completely corpus independent. The Pearson’s correlation between these handpicked features and relative gain observed by our model is shown in Table 4. For this analysis we used the relative gain obtained from the wordlevel experiments. Concretely, the relative gain for Czech was computed as"
W19-6624,P15-1001,0,0.033363,"dings and gated embeddings. We used en-de language pair from the TED multi-target dataset. Std. is our baseline with standard word embeddings, model C is the composition only model and CG combines the characteraware (composed) embedding and standard embedding via a gating function. spellings (character embeddings) of entire target vocabulary, placing a limitation on the target vocabulary size for our model. Which is problematic for word-level modeling (without BPE). To make our character-aware model accommodate large target vocabulary sizes, we incorporate an approximation mechanism based on (Jean et al., 2015). Instead of computing the softmax over the entire vocabulary, we uniformly sample 20k vocabulary types and the vocabulary types that are present in the training batch. During decoding, we compute the forward pass Wo sj in Equation 2 in several splits of the target vocabulary. As no backward pass is required we clear the memory (i.e. delete the computation graph) after each split is computed. 5 Experiments We evaluate our character aware model on 14 different languages in a low-resource setting. Additionally, we sweep over several BPE merge hyperparameter settings from character-level to fully"
W19-6624,P17-4012,0,0.0190775,"krainian to around 174k sentences pairs for Russian (provided in Appendix A), but the validation and test sets are “multi-way parallel”, meaning the English sentences (the source side in our experiments) are the same across all 14 languages, and are about 2k sentences each. We filter out training pairs where the source sentence was longer that 50 tokens (before applying BPE). For word-level results, we used a vocabulary size of 100k (keeping the most frequent types) and replaced rare words by an &lt;UNK&gt; token. Lang uk cs de bg tr pl ru ro pt hu fr fa ar he 5.2 NMT Setup We work with OpenNMT-py (Klein et al., 2017), and modify the target-side embedding layer and softmax layer to use our proposed character-aware composition function. A 2 layer encoder and decoder, with 1000 recurrent units were used in all experiments The embeddings sizes were made to match the RNN recurrent size. We set the character embedding size to 50 and use four CNNs with kernel widths 3, 4, 5 and 6. The four CNN outputs are concatenated into a compositional embeddings and gated with a standard word embedding. The same composition function (with shared parameters) was used for the target embedding layer and the softmax layer. We op"
W19-6624,D15-1166,0,0.36944,"ern present in the language, for example the token politely in our dataset is split into pol+itely, instead of the linguistically plausible split polite+ly.3 Our approach can be applied to word-level sequences and sequences at any BPE merge hyperparameter greater than 0. Increasing the hyperparameter results in more words and longer subwords that can exhibit morphological patterns. Our goal is to exploit these morphological patterns and enrich the word (or subword) representations with characterawareness. 3 Encoder-Decoder NMT An attention-based encoder-decoder network (Bahdanau et al., 2015; Luong et al., 2015) models the probability of a target sentence y of length J given a source sentence x as: p(y |x) = J Y j=1 p(yj |y0:j−1 , x; θ) (1) where θ represents all the parameters of the network. At each time-step the j 0 th output token is 3 We observe this split when merge parameter was 15k. Dublin, Aug. 19-23, 2019 |p. 245 (2) where sj ∈ RD×1 is the decoder hidden state at time j and Wo ∈ R|V|×D is the weight matrix of the softmax layer, which provides a continuous representation for target words. sj is computed using the following recurrence: sj = tanh(Wc [cj ; ˜ sj ]) ˜ sj = f ([sj−1 ; ws yj−1 ;˜ s"
W19-6624,P18-1118,0,0.0415482,"Missing"
W19-6624,D16-1209,0,0.0426477,"Missing"
W19-6624,J03-1002,0,0.0232058,"set of morphological tags. For example, the French UniMorph corpus contains the word marchai (walked), which is associated with its lemma, marcher and a set of morphological tags {V,IND,PST,1,SG,PFV}. There are 19 such tags in the French UniMorph corpus. A morphologically richer language like Hungarian, for example, has 36 distinct tags. We used the number of distinct tags (UT) and the number of different tag combinations (UTC) that appear in the UniMorph corpus for each language. Note that 8 We use FastAlign (Dyer et al., 2013) for word alignments with the grow-diag-final-and heuristic from (Och and Ney, 2003) for symmetrization. Proceedings of MT Summit XVII, volume 1 we do not filter out words (and its associated tags) from the UniMorph corpus that are absent in our parallel data. This ensures that the UT and UTC features are completely corpus independent. The Pearson’s correlation between these handpicked features and relative gain observed by our model is shown in Table 4. For this analysis we used the relative gain obtained from the wordlevel experiments. Concretely, the relative gain for Czech was computed as 21.49−18.44 We see a 18.44 strong correlation between the corpus-independent feature"
W19-6624,N18-1006,0,0.0164014,"ions, and found no improvements. Our work is also aligned with the characteraware models proposed in (Kim et al., 2016), but projection. Proceedings of MT Summit XVII, volume 1 we additionally employ a gating mechanism between character-aware representations and standard word representations similar to language modeling work by (Miyamoto and Cho, 2016). However, our gating is a learned type-specific vector rather than a fixed hyperparameter. There is additionally a line of work on purely character-level NMT, which generates words one character at a time (Ling et al., 2015; Chung et al., 2016; Passban et al., 2018). While initial results here were not strong, Cherry et al. (2018) revisit this with deeper architectures and sweeping dropout parameters and find that they outperform BPE across settings of the merge hyperparameter. They examine different data sizes and observe improvements in the smaller data size settings— however, the smallest size is about 2 million sentence pairs. In contrast, we look at a smaller order of magnitude data size and present an alternate approach which doesn’t require substantial tuning of parameters across different languages. Finally, Byte-Pair Encoding (BPE) (Sennrich et"
W19-6624,P16-1162,0,0.336552,"attention-based encoder-decoder neural machine translation (NMT) models learn wordlevel embeddings, with a continuous representation for each unique word type (Bahdanau et al., 2015). However, this results in a long tail of rare words for which we do not learn good representations. More recently, it has become standard pracc 2019 The authors. This article is licensed under a Creative Commons 4.0 licence, no derivative works, attribution, CCBY-ND. ∗ Equal Contribution Proceedings of MT Summit XVII, volume 1 tice to mitigate the vocabulary size problem with Byte-Pair Encoding (BPE) (Gage, 1994; Sennrich et al., 2016). BPE iteratively merges consecutive characters into larger chunks based on their frequency, which results in the breaking up of less common words into “subword units.” While BPE addresses the vocabulary size problem, the spellings of the subword units are still ignored. On the other hand, purely character-level NMT translates one character at a time and can implicitly learn about morphological patterns within words as well as generalize to unseen vocabulary. Recently, Cherry et al. (2018) show that very deep character-level models can outperform BPE, however, the smallest data size evaluated"
