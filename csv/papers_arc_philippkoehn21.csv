2021.naacl-main.399,Evaluating Saliency Methods for Neural Language Models,2021,-1,-1,2,1,4416,shuoyang ding,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Saliency methods are widely used to interpret neural network predictions, but different variants of saliency methods often disagree even on the interpretations of the same prediction made by the same model. In these cases, how do we identify when are these interpretations trustworthy enough to be used in analyses? To address this question, we conduct a comprehensive and quantitative evaluation of saliency methods on a fundamental category of NLP models: neural language models. We evaluate the quality of prediction interpretations from two perspectives that each represents a desirable property of these interpretations: plausibility and faithfulness. Our evaluation is conducted on four different datasets constructed from the existing human annotation of syntactic and semantic agreements, on both sentence-level and document-level. Through our evaluation, we identified various ways saliency methods could yield interpretations of low quality. We recommend that future work deploying such methods to neural language models should carefully validate their interpretations before drawing insights."
2021.mtsummit-research.1,Learning Curricula for Multilingual Neural Machine Translation Training,2021,-1,-1,2,0.562478,5013,gaurav kumar,Proceedings of Machine Translation Summit XVIII: Research Track,0,Low-resource Multilingual Neural Machine Translation (MNMT) is typically tasked with improving the translation performance on one or more language pairs with the aid of high-resource language pairs. In this paper and we propose two simple search based curricula {--} orderings of the multilingual training data {--} which help improve translation performance in conjunction with existing techniques such as fine-tuning. Additionally and we attempt to learn a curriculum for MNMT from scratch jointly with the training of the translation system using contextual multi-arm bandits. We show on the FLORES low-resource translation dataset that these learned curricula can provide better starting points for fine tuning and improve overall performance of the translation system.
2021.mtsummit-research.24,An Alignment-Based Approach to Semi-Supervised Bilingual Lexicon Induction with Small Parallel Corpora,2021,-1,-1,2,1,5067,kelly marchisio,Proceedings of Machine Translation Summit XVIII: Research Track,0,Aimed at generating a seed lexicon for use in downstream natural language tasks and unsupervised methods for bilingual lexicon induction have received much attention in the academic literature recently. While interesting and fully unsupervised settings are unrealistic; small amounts of bilingual data are usually available due to the existence of massively multilingual parallel corpora and or linguists can create small amounts of parallel data. In this work and we demonstrate an effective bootstrapping approach for semi-supervised bilingual lexicon induction that capitalizes upon the complementary strengths of two disparate methods for inducing bilingual lexicons. Whereas statistical methods are highly effective at inducing correct translation pairs for words frequently occurring in a parallel corpus and monolingual embedding spaces have the advantage of having been trained on large amounts of data and and therefore may induce accurate translations for words absent from the small corpus. By combining these relative strengths and our method achieves state-of-the-art results on 3 of 4 language pairs in the challenging VecMap test set using minimal amounts of parallel data and without the need for a translation dictionary. We release our implementation at www.blind-review.code.
2021.findings-emnlp.64,An Analysis of {E}uclidean vs. Graph-Based Framing for Bilingual Lexicon Induction from Word Embedding Spaces,2021,-1,-1,7,1,5067,kelly marchisio,Findings of the Association for Computational Linguistics: EMNLP 2021,0,"Much recent work in bilingual lexicon induction (BLI) views word embeddings as vectors in Euclidean space. As such, BLI is typically solved by finding a linear transformation that maps embeddings to a common space. Alternatively, word embeddings may be understood as nodes in a weighted graph. This framing allows us to examine a node{'}s graph neighborhood without assuming a linear transform, and exploits new techniques from the graph matching optimization literature. These contrasting approaches have not been compared in BLI so far. In this work, we study the behavior of Euclidean versus graph-based approaches to BLI under differing data conditions and show that they complement each other when combined. We release our code at https://github.com/kellymarchisio/euc-v-graph-bli."
2021.emnlp-main.539,{L}evenshtein Training for Word-level Quality Estimation,2021,-1,-1,4,1,4416,shuoyang ding,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"We propose a novel scheme to use the Levenshtein Transformer to perform the task of word-level quality estimation. A Levenshtein Transformer is a natural fit for this task: trained to perform decoding in an iterative manner, a Levenshtein Transformer can learn to post-edit without explicit supervision. To further minimize the mismatch between the translation task and the word-level QE task, we propose a two-stage transfer learning procedure on both augmented data and human post-editing data. We also propose heuristics to construct reference labels that are compatible with subword-level finetuning and inference. Results on WMT 2020 QE shared task dataset show that our proposed method has superior data efficiency under the data-constrained setting and competitive performance under the unconstrained setting."
2021.emnlp-main.814,{XLE}nt: Mining a Large Cross-lingual Entity Dataset with Lexical-Semantic-Phonetic Word Alignment,2021,-1,-1,5,0.862069,7813,ahmed elkishky,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Cross-lingual named-entity lexica are an important resource to multilingual NLP tasks such as machine translation and cross-lingual wikification. While knowledge bases contain a large number of entities in high-resource languages such as English and French, corresponding entities for lower-resource languages are often missing. To address this, we propose Lexical-Semantic-Phonetic Align (LSP-Align), a technique to automatically mine cross-lingual entity lexica from mined web data. We demonstrate LSP-Align outperforms baselines at extracting cross-lingual entity pairs and mine 164 million entity pairs from 120 different languages aligned with English. We release these cross-lingual entity pairs along with the massively multilingual tagged named entity corpus as a resource to the NLP community."
2021.adaptnlp-1.21,Zero-Shot Cross-Lingual Dependency Parsing through Contextual Embedding Transformation,2021,-1,-1,2,0,8932,haoran xu,Proceedings of the Second Workshop on Domain Adaptation for NLP,0,"Linear embedding transformation has been shown to be effective for zero-shot cross-lingual transfer tasks and achieve surprisingly promising results. However, cross-lingual embedding space mapping is usually studied in static word-level embeddings, where a space transformation is derived by aligning representations of translation pairs that are referred from dictionaries. We move further from this line and investigate a contextual embedding alignment approach which is sense-level and dictionary-free. To enhance the quality of the mapping, we also provide a deep view of properties of contextual embeddings, i.e., the anisotropy problem and its solution. Experiments on zero-shot dependency parsing through the concept-shared space built by our embedding transformation substantially outperform state-of-the-art methods using multilingual embeddings."
2021.acl-long.66,Adapting High-resource {NMT} Models to Translate Low-resource Related Languages without Parallel Data,2021,-1,-1,8,0,12795,weijen ko,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"The scarcity of parallel data is a major obstacle for training high-quality machine translation systems for low-resource languages. Fortunately, some low-resource languages are linguistically related or similar to high-resource languages; these related languages may share many lexical or syntactic structures. In this work, we exploit this linguistic overlap to facilitate translating to and from a low-resource language with only monolingual data, in addition to any parallel data in the related high-resource language. Our method, NMT-Adapt, combines denoising autoencoding, back-translation and adversarial objectives to utilize monolingual data for low-resource adaptation. We experiment on 7 languages from three different language families and show that our technique significantly improves translation into low-resource language compared to other translation baselines."
2020.wmt-1.1,Findings of the 2020 Conference on Machine Translation ({WMT}20),2020,-1,-1,12,0,8740,loic barrault,Proceedings of the Fifth Conference on Machine Translation,0,"This paper presents the results of the news translation task and the similar language translation task, both organised alongside the Conference on Machine Translation (WMT) 2020. In the news task, participants were asked to build machine translation systems for any of 11 language pairs, to be evaluated on test sets consisting mainly of news stories. The task was also opened up to additional test suites to probe specific aspects of translation. In the similar language translation task, participants built machine translation systems for translating between closely related pairs of languages."
2020.wmt-1.4,Findings of the {WMT} 2020 Shared Task on Machine Translation Robustness,2020,-1,-1,9,0,2509,lucia specia,Proceedings of the Fifth Conference on Machine Translation,0,"We report the findings of the second edition of the shared task on improving robustness in Machine Translation (MT). The task aims to test current machine translation systems in their ability to handle challenges facing MT models to be deployed in the real world, including domain diversity and non-standard texts common in user generated content, especially in social media. We cover two language pairs {--} English-German and English-Japanese and provide test sets in zero-shot and few-shot variants. Participating systems are evaluated both automatically and manually, with an additional human evaluation for {''}catastrophic errors{''}. We received 59 submissions by 11 participating teams from a variety of types of institutions."
2020.wmt-1.68,When Does Unsupervised Machine Translation Work?,2020,44,2,3,1,5067,kelly marchisio,Proceedings of the Fifth Conference on Machine Translation,0,"Despite the reported success of unsupervised machine translation (MT), the field has yet to examine the conditions under which the methods succeed and fail. We conduct an extensive empirical evaluation using dissimilar language pairs, dissimilar domains, and diverse datasets. We find that performance rapidly deteriorates when source and target corpora are from different domains, and that stochasticity during embedding training can dramatically affect downstream results. We additionally find that unsupervised MT performance declines when source and target languages use different scripts, and observe very poor performance on authentic low-resource language pairs. We advocate for extensive empirical evaluation of unsupervised MT systems to highlight failure points and encourage continued research on the most promising paradigms. We release our preprocessed dataset to encourage evaluations that stress-test systems under multiple data conditions."
2020.wmt-1.78,Findings of the {WMT} 2020 Shared Task on Parallel Corpus Filtering and Alignment,2020,-1,-1,1,1,4417,philipp koehn,Proceedings of the Fifth Conference on Machine Translation,0,"Following two preceding WMT Shared Task on Parallel Corpus Filtering (Koehn et al., 2018, 2019), we posed again the challenge of assigning sentence-level quality scores for very noisy corpora of sentence pairs crawled from the web, with the goal of sub-selecting the highest-quality data to be used to train ma-chine translation systems. This year, the task tackled the low resource condition of Pashto{--}English and Khmer{--}English and also included the challenge of sentence alignment from document pairs."
2020.wmt-1.108,An exploratory approach to the Parallel Corpus Filtering shared task {WMT}20,2020,-1,-1,2,0,13947,ankur kejriwal,Proceedings of the Fifth Conference on Machine Translation,0,"In this document we describe our submission to the parallel corpus filtering task using multilingual word embedding, language models and an ensemble of pre and post filtering rules. We use the norms of embedding and the perplexities of language models along with pre/post filtering rules to complement the LASER baseline scores and in the end get an improvement on the dev set in both language pairs."
2020.wmt-1.109,Dual Conditional Cross Entropy Scores and {LASER} Similarity Scores for the {WMT}20 Parallel Corpus Filtering Shared Task,2020,-1,-1,2,0,13948,felicia koerner,Proceedings of the Fifth Conference on Machine Translation,0,"This paper describes our submission to the WMT20 Parallel Corpus Filtering and Alignment for Low-Resource Conditions Shared Task. This year{'}s corpora are noisy Khmer-English and Pashto-English, with 58.3 million and 11.6 million words respectively (English token count). Our submission focuses on filtering Pashto-English, building on previously successful methods to produce two sets of scores: LASER{\_}LM, a combination of the LASER similarity scores provided in the shared task and perplexity scores from language models, and DCCEF{\_}DUP, dual conditional cross entropy scores combined with a duplication penalty. We improve slightly on the LASER similarity score and find that the provided clean data can successfully be supplemented with a subsampled set of the noisy data, effectively increasing the training data for the models used for dual conditional cross entropy scoring."
2020.nlpcovid19-2.5,{TICO}-19: the Translation Initiative for {CO}vid-19,2020,-1,-1,10,0.167089,832,antonios anastasopoulos,Proceedings of the 1st Workshop on {NLP} for {COVID}-19 (Part 2) at {EMNLP} 2020,0,"The COVID-19 pandemic is the worst pandemic to strike the world in over a century. Crucial to stemming the tide of the SARS-CoV-2 virus is communicating to vulnerable populations the means by which they can protect themselves. To this end, the collaborators forming the Translation Initiative for COvid-19 (TICO-19) have made test and development data available to AI and MT researchers in 35 different languages in order to foster the development of tools and resources for improving access to information about COVID-19 in these languages. In addition to 9 high-resourced, {''}pivot{''} languages, the team is targeting 26 lesser resourced languages, in particular languages of Africa, South Asia and South-East Asia, whose populations may be the most vulnerable to the spread of the virus. The same data is translated into all of the languages represented, meaning that testing or development can be done for any pairing of languages in the set. Further, the team is converting the test and development data into translation memories (TMXs) that can be used by localizers from and to any of the languages."
2020.emnlp-main.6,Statistical Power and Translationese in Machine Translation Evaluation,2020,-1,-1,3,0,9403,yvette graham,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"The term translationese has been used to describe features of translated text, and in this paper, we provide detailed analysis of potential adverse effects of translationese on machine translation evaluation. Our analysis shows differences in conclusions drawn from evaluations that include translationese in test data compared to experiments that tested only with text originally composed in that language. For this reason we recommend that reverse-created test data be omitted from future machine translation test sets. In addition, we provide a re-evaluation of a past machine translation evaluation claiming human-parity of MT. One important issue not previously considered is statistical power of significance tests applied to comparison of human and machine translation. Since the very aim of past evaluations was investigation of ties between human and MT systems, power analysis is of particular importance, to avoid, for example, claims of human parity simply corresponding to Type II error resulting from the application of a low powered test. We provide detailed analysis of tests used in such evaluations to provide an indication of a suitable minimum sample size for future studies."
2020.emnlp-main.7,Simulated multiple reference training improves low-resource machine translation,2020,30,0,4,1,4575,huda khayrallah,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"Many valid translations exist for a given sentence, yet machine translation (MT) is trained with a single reference translation, exacerbating data sparsity in low-resource settings. We introduce Simulated Multiple Reference Training (SMRT), a novel MT training method that approximates the full space of possible translations by sampling a paraphrase of the reference sentence from a paraphraser and training the MT model to predict the paraphraser{'}s distribution over possible tokens. We demonstrate the effectiveness of SMRT in low-resource settings when translating to English, with improvements of 1.2 to 7.0 BLEU. We also find SMRT is complementary to back-translation."
2020.emnlp-main.480,{CCA}ligned: A Massive Collection of Cross-Lingual Web-Document Pairs,2020,-1,-1,4,0.862069,7813,ahmed elkishky,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"Cross-lingual document alignment aims to identify pairs of documents in two distinct languages that are of comparable content or translations of each other. In this paper, we exploit the signals embedded in URLs to label web documents at scale with an average precision of 94.5{\%} across different language pairs. We mine sixty-eight snapshots of the Common Crawl corpus and identify web document pairs that are translations of each other. We release a new web dataset consisting of over 392 million URL pairs from Common Crawl covering documents in 8144 language pairs of which 137 pairs include English. In addition to curating this massive dataset, we introduce baseline methods that leverage cross-lingual representations to identify aligned documents based on their textual content. Finally, we demonstrate the value of this parallel documents dataset through a downstream task of mining parallel sentences and measuring the quality of machine translations from models trained on this mined data. Our objective in releasing this dataset is to foster new research in cross-lingual NLP across a variety of low, medium, and high-resource languages."
2020.emnlp-main.483,Exploiting Sentence Order in Document Alignment,2020,30,0,2,1,13891,brian thompson,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"We present a simple document alignment method that incorporates sentence order information in both candidate generation and candidate re-scoring. Our method results in 61{\%} relative reduction in error compared to the best previously published result on the WMT16 document alignment shared task. Our method improves downstream MT performance on web-scraped Sinhala{--}English documents from ParaCrawl, outperforming the document alignment method used in the most recent ParaCrawl release. It also outperforms a comparable corpora method which uses the same multilingual embeddings, demonstrating that exploiting sentence order is beneficial even if the end goal is sentence-level bitext."
2020.amta-user.3,A Survey of Qualitative Error Analysis for Neural Machine Translation Systems,2020,-1,-1,5,0,12509,denise diaz,Proceedings of the 14th Conference of the Association for Machine Translation in the Americas (Volume 2: User Track),0,None
2020.acl-main.417,{P}ara{C}rawl: Web-Scale Acquisition of Parallel Corpora,2020,-1,-1,10,0,20851,marta banon,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"We report on methods to create the largest publicly available parallel corpora by crawling the web, using open source software. We empirically compare alternative methods and publish benchmark data sets for sentence alignment and sentence pair filtering. We also describe the parallel corpora released and evaluate their quality and their usefulness to create machine translation systems."
2020.aacl-main.58,{S}imul{MT} to {S}imul{ST}: Adapting Simultaneous Text Translation to End-to-End Simultaneous Speech Translation,2020,-1,-1,3,0.652174,5713,xutai ma,Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing,0,"We investigate how to adapt simultaneous text translation methods such as wait-$k$ and monotonic multihead attention to end-to-end simultaneous speech translation by introducing a pre-decision module. A detailed analysis is provided on the latency-quality trade-offs of combining fixed and flexible pre-decision with fixed and flexible policies. We also design a novel computation-aware latency metric, adapted from Average Lagging."
W19-6602,Robust Document Representations for Cross-Lingual Information Retrieval in Low-Resource Settings,2019,0,0,8,0,8930,mahsa yarmohammadi,Proceedings of Machine Translation Summit XVII: Research Track,0,None
W19-6619,Controlling the Reading Level of Machine Translation Output,2019,0,3,4,1,5067,kelly marchisio,Proceedings of Machine Translation Summit XVII: Research Track,0,None
W19-6624,Character-Aware Decoder for Translation into Morphologically Rich Languages,2019,-1,-1,4,1,10240,adithya renduchintala,Proceedings of Machine Translation Summit XVII: Research Track,0,None
W19-5404,Findings of the {WMT} 2019 Shared Task on Parallel Corpus Filtering for Low-Resource Conditions,2019,0,9,1,1,4417,philipp koehn,"Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)",0,"Following the WMT 2018 Shared Task on Parallel Corpus Filtering, we posed the challenge of assigning sentence-level quality scores for very noisy corpora of sentence pairs crawled from the web, with the goal of sub-selecting 2{\%} and 10{\%} of the highest-quality data to be used to train machine translation systems. This year, the task tackled the low resource condition of Nepali-English and Sinhala-English. Eleven participants from companies, national research labs, and universities participated in this task."
W19-5435,Low-Resource Corpus Filtering Using Multilingual Sentence Embeddings,2019,18,0,5,1,4499,vishrav chaudhary,"Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)",0,"In this paper, we describe our submission to the WMT19 low-resource parallel corpus filtering shared task. Our main approach is based on the LASER toolkit (Language-Agnostic SEntence Representations), which uses an encoder-decoder architecture trained on a parallel corpus to obtain multilingual sentence representations. We then use the representations directly to score and filter the noisy parallel sentences without additionally training a scoring function. We contrast our approach to other promising methods and show that LASER yields strong results. Finally, we produce an ensemble of different scoring methods and obtain additional gains. Our submission achieved the best overall performance for both the Nepali-English and Sinhala-English 1M tasks by a margin of 1.3 and 1.4 BLEU respectively, as compared to the second best systems. Moreover, our experiments show that this technique is promising for low and even no-resource scenarios."
W19-5301,Findings of the 2019 Conference on Machine Translation ({WMT}19),2019,0,50,9,0,8740,loic barrault,"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)",0,"This paper presents the results of the premier shared task organized alongside the Conference on Machine Translation (WMT) 2019. Participants were asked to build machine translation systems for any of 18 language pairs, to be evaluated on a test set of news stories. The main metric for this task is human judgment of translation quality. The task was also opened up to additional test suites to probe specific aspects of translation."
W19-5303,Findings of the First Shared Task on Machine Translation Robustness,2019,34,1,7,0,5769,xian li,"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)",0,"We share the findings of the first shared task on improving robustness of Machine Translation (MT). The task provides a testbed representing challenges facing MT models deployed in the real world, and facilitates new approaches to improve models{'} robustness to noisy input and domain mismatch. We focus on two language pairs (English-French and English-Japanese), and the submitted systems are evaluated on a blind test set consisting of noisy comments on Reddit and professionally sourced translations. As a new task, we received 23 submissions by 11 participating teams from universities, companies, national labs, etc. All submitted systems achieved large improvements over baselines, with the best improvement having +22.33 BLEU. We evaluated submissions by both human judgment and automatic evaluation (BLEU), which shows high correlations (Pearson{'}s r = 0.94 and 0.95). Furthermore, we conducted a qualitative analysis of the submitted systems using compare-mt, which revealed their salient differences in handling challenges in this task. Such analysis provides additional insights when there is occasional disagreement between human judgment and BLEU, e.g. systems better at producing colloquial expressions received higher score from human judgment."
W19-5329,{J}ohns {H}opkins {U}niversity Submission for {WMT} News Translation Task,2019,-1,-1,3,1,5067,kelly marchisio,"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)",0,"We describe the work of Johns Hopkins University for the shared task of news translation organized by the Fourth Conference on Machine Translation (2019). We submitted systems for both directions of the English-German language pair. The systems combine multiple techniques {--} sampling, filtering, iterative backtranslation, and continued training {--} previously used to improve performance of neural machine translation models. At submission time, we achieve a BLEU score of 38.1 for De-En and 42.5 for En-De translation directions on newstest2019. Post-submission, the score is 38.4 for De-En and 42.8 for En-De. Various experiments conducted in the process are also described."
W19-5201,Saliency-driven Word Alignment Interpretation for Neural Machine Translation,2019,41,1,3,1,4416,shuoyang ding,Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers),0,"Despite their original goal to jointly learn to align and translate, Neural Machine Translation (NMT) models, especially Transformer, are often perceived as not learning interpretable word alignments. In this paper, we show that NMT models do learn interpretable word alignments, which could only be revealed with proper interpretation methods. We propose a series of such methods that are model-agnostic, are able to be applied either offline or online, and do not require parameter update or architectural change. We show that under the force decoding setup, the alignments induced by our interpretation method are of better quality than fast-align for some systems, and when performing free decoding, they agree well with the alignments induced by automatic alignment tools."
W19-4439,Simple Construction of Mixed-Language Texts for Vocabulary Learning,2019,0,0,2,1,10240,adithya renduchintala,Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications,0,"We present a machine foreign-language teacher that takes documents written in a student{'}s native language and detects situations where it can replace words with their foreign glosses such that new foreign vocabulary can be learned simply through reading the resulting mixed-language text. We show that it is possible to design such a machine teacher without any supervised data from (human) students. We accomplish this by modifying a cloze language model to incrementally learn new vocabulary items, and use this language model as a proxy for the word guessing and learning ability of real students. Our machine foreign-language teacher decides which subset of words to replace by consulting this language model. We evaluate three variants of our student proxy language models through a study on Amazon Mechanical Turk (MTurk). We find that MTurk {``}students{''} were able to guess the meanings of foreign words introduced by the machine teacher with high accuracy for both function words as well as content words in two out of the three models. In addition, we show that students are able to retain their knowledge about the foreign words after they finish reading the document."
W19-1501,Parallelizable Stack Long Short-Term Memory,2019,21,0,2,1,4416,shuoyang ding,Proceedings of the Third Workshop on Structured Prediction for {NLP},0,"Stack Long Short-Term Memory (StackLSTM) is useful for various applications such as parsing and string-to-tree neural machine translation, but it is also known to be notoriously difficult to parallelize for GPU training due to the fact that the computations are dependent on discrete operations. In this paper, we tackle this problem by utilizing state access patterns of StackLSTM to homogenize computations with regard to different discrete operations. Our parsing experiments show that the method scales up almost linearly with increasing batch size, and our parallelized PyTorch implementation trains significantly faster compared to the Dynet C++ implementation."
P19-2052,De-Mixing Sentiment from Code-Mixed Text,2019,0,1,5,0,7627,yash lal,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop,0,"Code-mixing is the phenomenon of mixing the vocabulary and syntax of multiple languages in the same sentence. It is an increasingly common occurrence in today{'}s multilingual society and poses a big challenge when encountered in different downstream tasks. In this paper, we present a hybrid architecture for the task of Sentiment Analysis of English-Hindi code-mixed data. Our method consists of three components, each seeking to alleviate different issues. We first generate subword level representations for the sentences using a CNN architecture. The generated representations are used as inputs to a Dual Encoder Network which consists of two different BiLSTMs - the Collective and Specific Encoder. The Collective Encoder captures the overall sentiment of the sentence, while the Specific Encoder utilizes an attention mechanism in order to focus on individual sentiment-bearing sub-words. This, combined with a Feature Network consisting of orthographic features and specially trained word embeddings, achieves state-of-the-art results - 83.54{\%} accuracy and 0.827 F1 score - on a benchmark dataset."
N19-1209,Overcoming Catastrophic Forgetting During Domain Adaptation of Neural Machine Translation,2019,0,14,5,1,13891,brian thompson,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",0,"Continued training is an effective method for domain adaptation in neural machine translation. However, in-domain gains from adaptation come at the expense of general-domain performance. In this work, we interpret the drop in general-domain performance as catastrophic forgetting of general-domain knowledge. To mitigate it, we adapt Elastic Weight Consolidation (EWC){---}a machine learning method for learning a new task without forgetting previous tasks. Our method retains the majority of general-domain performance lost in continued training without degrading in-domain performance, outperforming the previous state-of-the-art. We also explore the full range of general-domain performance available when some in-domain degradation is acceptable."
D19-1136,{V}ecalign: Improved Sentence Alignment in Linear Time and Space,2019,0,4,2,1,13891,brian thompson,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"We introduce Vecalign, a novel bilingual sentence alignment method which is linear in time and space with respect to the number of sentences being aligned and which requires only bilingual sentence embeddings. On a standard German{--}French test set, Vecalign outperforms the previous state-of-the-art method (which has quadratic time complexity and requires a machine translation system) by 5 F1 points. It substantially outperforms the popular Hunalign toolkit at recovering Bible verse alignments in medium- to low-resource language pairs, and it improves downstream MT quality by 1.7 and 1.6 BLEU in Sinhala-English and Nepali-English, respectively, compared to the Hunalign-based Paracrawl pipeline."
D19-1142,{HABL}ex: Human Annotated Bilingual Lexicons for Experiments in Machine Translation,2019,0,2,6,1,13891,brian thompson,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"Bilingual lexicons are valuable resources used by professional human translators. While these resources can be easily incorporated in statistical machine translation, it is unclear how to best do so in the neural framework. In this work, we present the HABLex dataset, designed to test methods for bilingual lexicon integration into neural machine translation. Our data consists of human generated alignments of words and phrases in machine translation test sets in three language pairs (Russian-English, Chinese-English, and Korean-English), resulting in clean bilingual lexicons which are well matched to the reference. We also present two simple baselines - constrained decoding and continued training - and an improvement to continued training to address overfitting."
D19-1632,The {FLORES} Evaluation Datasets for Low-Resource Machine Translation: {N}epali{--}{E}nglish and {S}inhala{--}{E}nglish,2019,40,20,6,0.249128,7331,francisco guzman,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"For machine translation, a vast majority of language pairs in the world are considered low-resource because they have little parallel data available. Besides the technical challenges of learning with limited supervision, it is difficult to evaluate methods trained on low-resource language pairs because of the lack of freely and publicly available benchmarks. In this work, we introduce the FLORES evaluation datasets for Nepali{--}English and Sinhala{--} English, based on sentences translated from Wikipedia. Compared to English, these are languages with very different morphology and syntax, for which little out-of-domain parallel data is available and for which relatively large amounts of monolingual data are freely available. We describe our process to collect and cross-check the quality of translations, and we report baseline performance using several learning settings: fully supervised, weakly supervised, semi-supervised, and fully unsupervised. Our experiments demonstrate that current state-of-the-art methods perform rather poorly on this benchmark, posing a challenge to the research community working on low-resource MT. Data and code to reproduce our experiments are available at https://github.com/facebookresearch/flores."
D19-1679,Spelling-Aware Construction of Macaronic Texts for Teaching Foreign-Language Vocabulary,2019,0,0,2,1,10240,adithya renduchintala,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"We present a machine foreign-language teacher that modifies text in a student{'}s native language (L1) by replacing some word tokens with glosses in a foreign language (L2), in such a way that the student can acquire L2 vocabulary simply by reading the resulting macaronic text. The machine teacher uses no supervised data from human students. Instead, to guide the machine teacher{'}s choice of which words to replace, we equip a cloze language model with a training procedure that can incrementally learn representations for novel words, and use this model as a proxy for the word guessing and learning ability of real human students. We use Mechanical Turk to evaluate two variants of the student model: (i) one that generates a representation for a novel word using only surrounding context and (ii) an extension that also uses the spelling of the novel word."
W18-6401,Findings of the 2018 Conference on Machine Translation ({WMT}18),2018,0,84,6,0,292,ondvrej bojar,Proceedings of the Third Conference on Machine Translation: Shared Task Papers,0,"This paper presents the results of the premier shared task organized alongside the Conference on Machine Translation (WMT) 2018. Participants were asked to build machine translation systems for any of 7 language pairs in both directions, to be evaluated on a test set of news stories. The main metric for this task is human judgment of translation quality. This year, we also opened up the task to additional test sets to probe specific aspects of translation."
W18-6417,The {JHU} Machine Translation Systems for {WMT} 2018,2018,0,3,1,1,4417,philipp koehn,Proceedings of the Third Conference on Machine Translation: Shared Task Papers,0,"We report on the efforts of the Johns Hopkins University to develop neural machine translation systems for the shared task for news translation organized around the Conference for Machine Translation (WMT) 2018. We developed systems for German{--}English, English{--} German, and Russian{--}English. Our novel contributions are iterative back-translation and fine-tuning on test sets from prior years."
W18-6453,Findings of the {WMT} 2018 Shared Task on Parallel Corpus Filtering,2018,0,19,1,1,4417,philipp koehn,Proceedings of the Third Conference on Machine Translation: Shared Task Papers,0,"We posed the shared task of assigning sentence-level quality scores for a very noisy corpus of sentence pairs crawled from the web, with the goal of sub-selecting 1{\%} and 10{\%} of high-quality data to be used to train machine translation systems. Seventeen participants from companies, national research labs, and universities participated in this task."
W18-6479,The {JHU} Parallel Corpus Filtering Systems for {WMT} 2018,2018,0,2,3,1,4575,huda khayrallah,Proceedings of the Third Conference on Machine Translation: Shared Task Papers,0,"This work describes our submission to the WMT18 Parallel Corpus Filtering shared task. We use a slightly modified version of the Zipporah Corpus Filtering toolkit (Xu and Koehn, 2017), which computes an adequacy score and a fluency score on a sentence pair, and use a weighted sum of the scores as the selection criteria. This work differs from Zipporah in that we experiment with using the noisy corpus to be filtered to compute the combination weights, and thus avoids generating synthetic data as in standard Zipporah."
W18-6313,Freezing Subnetworks to Analyze Domain Adaptation in Neural Machine Translation,2018,35,13,10,1,13891,brian thompson,Proceedings of the Third Conference on Machine Translation: Research Papers,0,"To better understand the effectiveness of continued training, we analyze the major components of a neural machine translation system (the encoder, decoder, and each embedding space) and consider each component{'}s contribution to, and capacity for, domain adaptation. We find that freezing any single component during continued training has minimal impact on performance, and that performance is surprisingly good when a single component is adapted while holding the rest of the model fixed. We also find that continued training does not move the model very far from the out-of-domain model, compared to a sensitivity analysis metric, suggesting that the out-of-domain model can provide a good generic initialization for the new domain."
W18-2703,Iterative Back-Translation for Neural Machine Translation,2018,0,14,2,0,25447,vu hoang,Proceedings of the 2nd Workshop on Neural Machine Translation and Generation,0,"We present iterative back-translation, a method for generating increasingly better synthetic parallel data from monolingual data to train neural machine translation systems. Our proposed method is very simple yet effective and highly applicable in practice. We demonstrate improvements in neural machine translation quality in both high and low resourced scenarios, including the best reported BLEU scores for the WMT 2017 GermanâEnglish tasks."
W18-2705,Regularized Training Objective for Continued Training for Domain Adaptation in Neural Machine Translation,2018,0,10,4,1,4575,huda khayrallah,Proceedings of the 2nd Workshop on Neural Machine Translation and Generation,0,"Supervised domain adaptation{---}where a large generic corpus and a smaller in-domain corpus are both available for training{---}is a challenge for neural machine translation (NMT). Standard practice is to train a generic model and use it to initialize a second model, then continue training the second model on in-domain data to produce an in-domain model. We add an auxiliary term to the training objective during continued training that minimizes the cross entropy between the in-domain model{'}s output word distribution and that of the out-of-domain model to prevent the model{'}s output from differing too much from the original out-of-domain model. We perform experiments on EMEA (descriptions of medicines) and TED (rehearsed presentations), initialized from a general domain (WMT) model. Our method shows improvements over standard continued training by up to 1.5 BLEU."
W18-2708,Document-Level Adaptation for Neural Machine Translation,2018,0,8,3,0,28398,sachith kothur,Proceedings of the 2nd Workshop on Neural Machine Translation and Generation,0,"It is common practice to adapt machine translation systems to novel domains, but even a well-adapted system may be able to perform better on a particular document if it were to learn from a translator{'}s corrections within the document itself. We focus on adaptation within a single document {--} appropriate for an interactive translation scenario where a model adapts to a human translator{'}s input over the course of a document. We propose two methods: single-sentence adaptation (which performs online adaptation one sentence at a time) and dictionary adaptation (which specifically addresses the issue of translating novel words). Combining the two models results in improvements over both approaches individually, and over baseline systems, even on short documents. On WMT news test data, we observe an improvement of +1.8 BLEU points and +23.3{\%} novel word translation accuracy and on EMEA data (descriptions of medications) we observe an improvement of +2.7 BLEU points and +49.2{\%} novel word translation accuracy."
W18-2709,On the Impact of Various Types of Noise on Neural Machine Translation,2018,0,19,2,1,4575,huda khayrallah,Proceedings of the 2nd Workshop on Neural Machine Translation and Generation,0,We examine how various types of noise in the parallel training data impact the quality of neural machine translation systems. We create five types of artificial noise and analyze how they degrade performance in neural and statistical machine translation. We find that neural models are generally more harmed by noise than statistical models. For one especially egregious type of noise they learn to just copy the input sentence.
W18-2102,Lightweight Word-Level Confidence Estimation for Neural Interactive Translation Prediction,2018,0,0,2,1,5048,rebecca knowles,Proceedings of the {AMTA} 2018 Workshop on Translation Quality Estimation and Automatic Post-Editing,0,None
W18-2108,A Comparison of Machine Translation Paradigms for Use in Black-Box Fuzzy-Match Repair,2018,0,0,3,1,5048,rebecca knowles,Proceedings of the {AMTA} 2018 Workshop on Translation Quality Estimation and Automatic Post-Editing,0,None
W18-1812,Exploring Word Sense Disambiguation Abilities of Neural Machine Translation Systems (Non-archival Extended Abstract),2018,0,4,2,0,8316,rebecca marvin,Proceedings of the 13th Conference of the Association for Machine Translation in the {A}mericas (Volume 1: Research Track),0,None
D18-1339,Context and Copying in Neural Machine Translation,2018,0,4,2,1,5048,rebecca knowles,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"Neural machine translation systems with subword vocabularies are capable of translating or copying unknown words. In this work, we show that they learn to copy words based on both the context in which the words appear as well as features of the words themselves. In contexts that are particularly copy-prone, they even copy words that they have already learned they should translate. We examine the influence of context and subword features on this and other types of copying behavior."
W17-4707,Predicting Target Language {CCG} Supertags Improves Neural Machine Translation,2017,0,19,6,1,25421,maria nuadejde,Proceedings of the Second Conference on Machine Translation,0,None
W17-4717,Findings of the 2017 Conference on Machine Translation ({WMT}17),2017,0,109,8,0,292,ondvrej bojar,Proceedings of the Second Conference on Machine Translation,0,"This paper presents the results of the WMT17 shared tasks, which includedn three machine translation (MT) tasks (news, biomedical, and multimodal), two evaluation tasks (metrics and run-time estimation of MT quality), an automatic post-editing task, a neural MT training task, and a bandit learning task."
W17-4724,The {JHU} Machine Translation Systems for {WMT} 2017,2017,0,2,3,1,4416,shuoyang ding,Proceedings of the Second Conference on Machine Translation,0,None
W17-3204,Six Challenges for Neural Machine Translation,2017,12,121,1,1,4417,philipp koehn,Proceedings of the First Workshop on Neural Machine Translation,0,"We explore six challenges for neural machine translation: domain mismatch, amount of training data, rare words, long sentences, word alignment, and beam search. We show both deficiencies and improvements over the quality of phrase-based statistical machine translation."
K17-1025,Knowledge Tracing in Sequential Learning of Inflected Vocabulary,2017,8,0,2,1,10240,adithya renduchintala,Proceedings of the 21st Conference on Computational Natural Language Learning ({C}o{NLL} 2017),0,"We present a feature-rich knowledge tracing method that captures a student{'}s acquisition and retention of knowledge during a foreign language phrase learning task. We model the student{'}s behavior as making predictions under a log-linear model, and adopt a neural gating mechanism to model how the student updates their log-linear parameters in response to feedback. The gating mechanism allows the model to learn complex patterns of retention and acquisition for each feature, while the log-linear parameterization results in an interpretable knowledge state. We collect human data and evaluate several versions of the model."
I17-3002,{CADET}: Computer Assisted Discovery Extraction and Translation,2017,6,0,10,0,668,benjamin durme,"Proceedings of the {IJCNLP} 2017, System Demonstrations",0,"Computer Assisted Discovery Extraction and Translation (CADET) is a workbench for helping knowledge workers find, label, and translate documents of interest. It combines a multitude of analytics together with a flexible environment for customizing the workflow for different users. This open-source framework allows for easy development of new research prototypes using a micro-service architecture based atop Docker and Apache Thrift."
I17-2004,Neural Lattice Search for Domain Adaptation in Machine Translation,2017,17,7,5,1,4575,huda khayrallah,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"Domain adaptation is a major challenge for neural machine translation (NMT). Given unknown words or new domains, NMT systems tend to generate fluent translations at the expense of adequacy. We present a stack-based lattice search algorithm for NMT and show that constraining its search space with lattices generated by phrase-based machine translation (PBMT) improves robustness. We report consistent BLEU score gains across four diverse domain adaptation tasks involving medical, IT, Koran, or subtitles texts."
D17-1319,{Z}ipporah: a Fast and Scalable Data Cleaning System for Noisy Web-Crawled Parallel Corpora,2017,13,15,2,0,23613,hainan xu,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,0,"We introduce Zipporah, a fast and scalable data cleaning system. We propose a novel type of bag-of-words translation feature, and train logistic regression models to classify good data and synthetic noisy data in the proposed feature space. The trained model is used to score parallel sentences in the data pool for selection. As shown in experiments, Zipporah selects a high-quality parallel corpus from a large, mixed quality data pool. In particular, for one noisy dataset, Zipporah achieves a 2.1 BLEU score improvement with using 1/5 of the data over using the entire corpus."
W16-2301,Findings of the 2016 Conference on Machine Translation,2016,113,137,8,0,292,ondvrej bojar,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,"This paper presents the results of the WMT16 shared tasks, which included five machine translation (MT) tasks (standard news, IT-domain, biomedical, multimodal, pronoun), three evaluation tasks (metrics, tuning, run-time estimation of MT quality), and an automatic post-editing task and bilingual document alignment task. This year, 102 MT systems from 24 institutions (plus 36 anonymized online systems) were submitted to the 12 translation directions in the news translation task. The IT-domain task received 31 submissions from 12 institutions in 7 directions and the Biomedical task received 15 submissions systems from 5 institutions. Evaluation was both automatic and manual (relative ranking and 100-point scale assessments). The quality estimation task had three subtasks, with a total of 14 teams, submitting 39 entries. The automatic post-editing task had a total of 6 teams, submitting 11 entries."
W16-2310,The {JHU} Machine Translation Systems for {WMT} 2016,2016,29,8,4,1,4416,shuoyang ding,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,"This paper describes the submission of Johns Hopkins University for the shared translation task of ACL 2016 First Conference on Machine Translation (WMT 2016). We set up phrase-based, hierarchical phrase-based and syntax-based systems for all 12 language pairs of this yearxe2x80x99s evaluation campaign. Novel research directions we investigated include: neural probabilistic language models, bilingual neural network language models, morphological segmentation, and the attentionbased neural machine translation model as reranking feature."
W16-2347,Findings of the {WMT} 2016 Bilingual Document Alignment Shared Task,2016,39,11,2,1,33897,christian buck,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,"This paper presents the results of the WMT16 Bilingual Document Alignment Shared Task. Given crawls of web sites, we asked participants to align documents that are translations of each other. 11 research groups submitted 19 systems, with a top performance of 95.0%."
W16-2365,Quick and Reliable Document Alignment via {TF}/{IDF}-weighted Cosine Distance,2016,7,7,2,1,33897,christian buck,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,None
W16-2204,Modeling Selectional Preferences of Verbs and Nouns in String-to-Tree Machine Translation,2016,26,2,3,1,25421,maria nuadejde,"Proceedings of the First Conference on Machine Translation: Volume 1, Research Papers",0,None
P16-5003,Computer Aided Translation,2016,0,1,1,1,4417,philipp koehn,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts,0,"Moving beyond post-editing machine translation, a number of recent research efforts have advanced computer aided translation methods that allow for more interactivity, richer information such as confidence scores, and the completed feedback loop of instant adaptation of machine translation models to user translations.This tutorial will explain the main techniques for several aspects of computer aided translation: confidence measures;interactive machine translation (interactive translation prediction);bilingual concordancers;translation option display;paraphrasing (alternative translation suggestions);visualization of word alignment;online adaptation;automatic reviewing;integration of translation memory;eye tracking, logging, and cognitive user models;For each of these, the state of the art and open challenges are presented. The tutorial will also look under the hood of the open source CASMACAT toolkit that is based on MATECAT, and available as a ``Home Edition'' to be installed on a desktop machine. The target audience of this tutorials are researchers interested in computer aided machine translation and practitioners who want to use or deploy advanced CAT technology."
P16-4023,Creating Interactive Macaronic Interfaces for Language Learning,2016,13,3,3,1,10240,adithya renduchintala,Proceedings of {ACL}-2016 System Demonstrations,0,"We present a prototype of a novel technology for second language instruction. Our learn-by-reading approach lets a human learner acquire new words and constructions by encountering them in context. To facilitate reading comprehension, our technology presents mixed native language (L1) and second language (L2) sentences to a learner and allows them to interact with the sentences to make the sentences easier (more L1-like) or harder (more L2-like) to read. Eventually, our system should continuously track a learnerxe2x80x99s knowledge and learning style by modeling their interactions, including performance on a pop quiz feature. This will allow our system to generate personalized mixed-language texts for learners."
P16-1175,User Modeling in Language Learning with Macaronic Texts,2016,14,2,3,1,10240,adithya renduchintala,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,None
K16-1013,Analyzing Learner Understanding of Novel {L}2 Vocabulary,2016,15,3,3,1,5048,rebecca knowles,Proceedings of The 20th {SIGNLL} Conference on Computational Natural Language Learning,0,"In this work, we explore how learners can infer second-language noun meanings in the context of their native language. Motivated by an interest in building interactive tools for language learning, we collect data on three word-guessing tasks, analyze their difficulty, and explore the types of errors that novice learners make. We train a log-linear model for predicting our subjectsxe2x80x99 guesses of word meanings in varying kinds of contexts. The modelxe2x80x99s predictions correlate well with subject performance, and we provide quantitative and qualitative analyses of both human and model performance."
2016.amta-researchers.2,Machine Translation Quality and Post-Editor Productivity,2016,12,5,2,0,36250,marina sancheztorron,Conferences of the Association for Machine Translation in the Americas: MT Researchers' Track,0,"We assessed how different machine translation (MT) systems affect the post-editing (PE) process and product of professional English{--}Spanish translators. Our model found that for each 1-point increase in BLEU, there is a PE time decrease of 0.16 seconds per word, about 3-4{\%}. The MT system with the lowest BLEU score produced the output that was post-edited to the lowest quality and with the highest PE effort, measured both in HTER and actual PE operations."
2016.amta-researchers.9,Neural Interactive Translation Prediction,2016,19,23,2,1,5048,rebecca knowles,Conferences of the Association for Machine Translation in the Americas: MT Researchers' Track,0,"We present an interactive translation prediction method based on neural machine translation. Even with the same translation quality of the underlying machine translation systems, the neural prediction method yields much higher word prediction accuracy (61.6{\%} vs. 43.3{\%}) than the traditional method based on search graphs, mainly due to better recovery from errors. We also develop efficient means to enable practical deployment."
2016.amta-researchers.13,Translation of Unknown Words in Low Resource Languages,2016,25,4,3,0,24225,biman gujral,Conferences of the Association for Machine Translation in the Americas: MT Researchers' Track,0,None
W15-3001,Findings of the 2015 Workshop on Statistical Machine Translation,2015,78,107,7,0.07424,292,ondvrej bojar,Proceedings of the Tenth Workshop on Statistical Machine Translation,0,"This paper presents the results of the WMT15 shared tasks, which included a standard news translation task, a metrics task, a tuning task, a task for run-time estimation of machine translation quality, and an automatic post-editing task. This year, 68 machine translation systems from 24 institutions were submitted to the ten translation directions in the standard translation task. An additional 7 anonymized systems were included, and were then evaluated both automatically and manually. The quality estimation task had three subtasks, with a total of 10 teams, submitting 34 entries. The pilot automatic postediting task had a total of 4 teams, submitting 7 entries."
W15-3013,The {E}dinburgh/{JHU} Phrase-based Machine Translation Systems for {WMT} 2015,2015,30,13,5,0.663119,5032,barry haddow,Proceedings of the Tenth Workshop on Statistical Machine Translation,0,"This paper describes the submission of the University of Edinburgh and the Johns Hopkins University for the shared translation task of the EMNLP 2015 Tenth Workshop on Statistical Machine Translation (WMT 2015). We set up phrase-based statistical machine translation systems for all ten language pairs of this yearxe2x80x99s evaluation campaign, which are English paired with Czech, Finnish, French, German, and Russian in both translation directions. Novel research directions we investigated include: neural network language models and bilingual neural network language models, a comprehensive use of word classes, and sparse lexicalized reordering features."
W15-3024,{E}dinburgh{'}s Syntax-Based Systems at {WMT} 2015,2015,-1,-1,5,1,11121,philip williams,Proceedings of the Tenth Workshop on Statistical Machine Translation,0,None
W15-3031,Results of the {WMT}15 Metrics Shared Task,2015,14,30,3,0,10251,milovs stanojevic,Proceedings of the Tenth Workshop on Statistical Machine Translation,0,"This paper presents the results of the WMT15 Metrics Shared Task. We asked participants of this task to score the outputs of the MT systems involved in the WMT15 Shared Translation Task. We collected scores of 46 metrics from 11 research groups. In addition to that, we computed scores of 7 standard metrics (BLEU, SentBLEU, NIST, WER, PER, TER and CDER) as baselines. The collected scores were evaluated in terms of system level correlation (how well each metricxe2x80x99s scores correlate with WMT15 official manual ranking of systems) and in terms of segment level correlation (how often a metric agrees with humans in comparing two translations of a particular sentence)."
J15-2001,The Operation Sequence {M}odel{---}{C}ombining N-Gram-Based and Phrase-Based Statistical Machine Translation,2015,48,18,4,1,3159,nadir durrani,Computational Linguistics,0,"In this article, we present a novel machine translation model, the Operation Sequence Model OSM, which combines the benefits of phrase-based and N-gram-based statistical machine translation SMT and remedies their drawbacks. The model represents the translation process as a linear sequence of operations. The sequence includes not only translation operations but also reordering operations. As in N-gram-based SMT, the model is: i based on minimal translation units, ii takes both source and target information into account, iii does not make a phrasal independence assumption, and iv avoids the spurious phrasal segmentation problem. As in phrase-based SMT, the model i has the ability to memorize lexical reordering triggers, ii builds the search graph dynamically, and iii decodes with large translation units during search. The unique properties of the model are i its strong coupling of reordering and translation where translation and reordering decisions are conditioned on n previous translation and reordering decisions, and ii the ability to model local and long-range reorderings consistently. Using BLEU as a metric of translation accuracy, we found that our system performs significantly better than state-of-the-art phrase-based systems Moses and Phrasal and N-gram-based systems Ncode on standard translation tasks. We compare the reordering component of the OSM to the Moses lexical reordering model by integrating it into Moses. Our results show that OSM outperforms lexicalized reordering on all translation tasks. The translation quality is shown to be improved further by learning generalized representations with a POS-based OSM."
W14-4018,Preference Grammars and Soft Syntactic Constraints for {GHKM} Syntax-based Statistical Machine Translation,2014,31,8,3,0.462483,5061,matthias huck,"Proceedings of {SSST}-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation",0,"In this work, we investigate the effectiveness of two techniques for a featurebased integration of syntactic information into GHKM string-to-tree statistical machine translation (Galley et al., 2004): (1.) Preference grammars on the target language side promote syntactic wellformedness during decoding while also allowing for derivations that are not linguistically motivated (as in hierarchical translation). (2.) Soft syntactic constraints augment the system with additional sourceside syntax features while not modifying the set of string-to-tree translation rules or the baseline feature scores. We conduct experiments with a stateof-the-art setup on an English!German translation task. Our results suggest that preference grammars for GHKM translation are inferior to the plain targetsyntactified model, whereas the enhancement with soft source syntactic constraints provides consistent gains. By employing soft source syntactic constraints with sparse features, we are able to achieve improvements of up to 0.7 points BLEU and 1.0 points TER."
W14-3302,Findings of the 2014 Workshop on Statistical Machine Translation,2014,75,148,5,0.0864455,292,ondvrej bojar,Proceedings of the Ninth Workshop on Statistical Machine Translation,0,"This paper presents the results of the WMT14 shared tasks, which included a standard news translation task, a separate medical translation task, a task for run-time estimation of machine translation quality, and a metrics task. This year, 143 machine translation systems from 23 institutions were submitted to the ten translation directions in the standard translation task. An additional 6 anonymized systems were included, and were then evaluated both automatically and manually. The quality estimation task had four subtasks, with a total of 10 teams, submitting 57 entries"
W14-3309,{E}dinburgh{'}s Phrase-based Machine Translation Systems for {WMT}-14,2014,0,2,3,1,3159,nadir durrani,Proceedings of the Ninth Workshop on Statistical Machine Translation,0,None
W14-3310,{EU-BRIDGE} {MT}: Combined Machine Translation,2014,59,18,10,0.810811,3519,markus freitag,Proceedings of the Ninth Workshop on Statistical Machine Translation,0,"This paper describes one of the collaborative efforts within EU-BRIDGE to further advance the state of the art in machine translation between two European language pairs, Germanxe2x86x92English and Englishxe2x86x92German. Three research institutes involved in the EU-BRIDGE project combined their individual machine translation systems and participated with a joint setup in the shared translation task of the evaluation campaign at the ACL 2014 Eighth Workshop on Statistical Machine Translation (WMT 2014). We combined up to nine different machine translation engines via system combination. RWTH Aachen University, the University of Edinburgh, and Karlsruhe Institute of Technology developed several individual systems which serve as system combination input. We devoted special attention to building syntax-based systems and combining them with the phrasebased ones. The joint setups yield empirical gains of up to 1.6 points in BLEU and 1.0 points in TER on the WMT newstest2013 test set compared to the best single systems."
W14-3324,{E}dinburgh{'}s Syntax-Based Systems at {WMT} 2014,2014,49,19,6,1,11121,philip williams,Proceedings of the Ninth Workshop on Statistical Machine Translation,0,"This paper describes the string-to-tree systems built at the University of Edinburgh for the WMT 2014 shared translation task. We developed systems for English-German, Czech-English, FrenchEnglish, German-English, Hindi-English, and Russian-English. This year we improved our English-German system through target-side compound splitting, morphosyntactic constraints, and refinements to parse tree annotation; we addressed the out-of-vocabulary problem using transliteration for Hindi and Russian and using morphological reduction for Russian; we improved our GermanEnglish system through tree binarization; and we reduced system development time by filtering the tuning sets."
W14-3358,Dynamic Topic Adaptation for {SMT} using Distributional Profiles,2014,26,12,3,1,9981,eva hasler,Proceedings of the Ninth Workshop on Statistical Machine Translation,0,"Despite its potential to improve lexical selection, most state-of-the-art machine translation systems take only minimal contextual information into account. We capture context with a topic model over distributional profiles built from the context words of each translation unit. Topic distributions are inferred for each translation unit and used to adapt the translation model dynamically to a given test context by measuring their similarity. We show that combining information from both local and global test contexts helps to improve lexical selection and outperforms a baseline system by up to 1.15 BLEU. We test our topic-adapted model on a diverse data set containing documents from three different domains and achieve competitive performance in comparison with two supervised domain-adapted systems."
W14-3362,Augmenting String-to-Tree and Tree-to-String Translation with Non-Syntactic Phrases,2014,47,8,3,0.462483,5061,matthias huck,Proceedings of the Ninth Workshop on Statistical Machine Translation,0,"We present an effective technique to easily augment GHKM-style syntax-based machine translation systems (Galley et al., 2006) with phrase pairs that do not comply with any syntactic well-formedness constraints. Non-syntactic phrase pairs are distinguished from syntactic ones in order to avoid harming effects. We apply our technique in state-of-the-art string-totree and tree-to-string setups. For tree-tostring translation, we furthermore investigate novel approaches for translating with source-syntax GHKM rules in association with input tree constraints and input tree features."
W14-1005,Using Feature Structures to Improve Verb Translation in {E}nglish-to-{G}erman Statistical {MT},2014,12,1,2,1,11121,philip williams,Proceedings of the 3rd Workshop on Hybrid Approaches to Machine Translation ({H}y{T}ra),0,"SCFG-based statistical MT models have proven effective for modelling syntactic aspects of translation, but still suffer problems of overgeneration. The production of German verbal complexes is particularly challenging since highly discontiguous constructions must be formed consistently, often from multiple independent rules. We extend a strong SCFG-based string-to-tree model to incorporate a rich feature-structure based representation of German verbal complex types and compare verbal complex production against that of the reference translations, finding a high baseline rate of error. By developing model features that use source-side information to influence the production of verbal complexes we are able to substantially improve the type accuracy as compared to the reference."
W14-0307,The Impact of Machine Translation Quality on Human Post-Editing,2014,21,11,1,1,4417,philipp koehn,Proceedings of the {EACL} 2014 Workshop on Humans and Computer-assisted Translation,0,"We investigate the effect of four different competitive machine translation systems on post-editor productivity and behaviour. The study involves four volunteers postediting automatic translations of news stories from English to German. We see significant difference in productivity due to the systems (about 20%), and even bigger variance between post-editors."
P14-2094,Refinements to Interactive Translation Prediction Based on Search Graphs,2014,13,7,1,1,4417,philipp koehn,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"We propose a number of refinements to the canonical approach to interactive translation prediction. By more permissive matching criteria, placing emphasis on matching the last word of the user prefix, and dealing with predictions to partially typed words, we observe gains in both word prediction accuracy (5.4%) and letter prediction accuracy (9.3%)."
E14-4029,Integrating an Unsupervised Transliteration Model into Statistical Machine Translation,2014,35,63,4,1,3159,nadir durrani,"Proceedings of the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics, volume 2: Short Papers",0,"We investigate three methods for integrating an unsupervised transliteration model into an end-to-end SMT system. We induce a transliteration model from parallel data and use it to translate OOV words. Our approach is fully unsupervised and language independent. In the methods to integrate transliterations, we observed improvements from 0.23-0.75 ( 0.41) BLEU points across 7 language pairs. We also show that our mined transliteration corpora provide better rule coverage and translation quality compared to the gold standard transliteration corpora."
E14-2007,{CASMACAT}: A Computer-assisted Translation Workbench,2014,6,10,9,0.666667,38851,vicent alabau,Proceedings of the Demonstrations at the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"CASMACAT is a modular, web-based translation workbench that offers advanced functionalities for computer-aided translation and the scientific study of human translation: automatic interaction with machine translation (MT) engines and translation memories (TM) to obtain raw translations or close TM matches for conventional post-editing; interactive translation prediction based on an MT enginexe2x80x99s search graph, detailed recording and replay of edit actions and translatorxe2x80x99s gaze (the latter via eye-tracking), and the support of e-pen as an alternative input device. The system is open source sofware and interfaces with multiple MT systems."
E14-1035,Dynamic Topic Adaptation for Phrase-based {MT},2014,23,26,3,1,9981,eva hasler,Proceedings of the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"Translating text from diverse sources poses a challenge to current machine translation systems which are rarely adapted to structure beyond corpus level. We explore topic adaptation on a diverse data set and present a new bilingual variant of Latent Dirichlet Allocation to compute topic-adapted, probabilistic phrase translation features. We dynamically infer document-specific translation probabilities for test sets of unknown origin, thereby capturing the effects of document context on phrase translations. We show gains of up to 1.26 BLEU over the baseline and 1.04 over a domain adaptation benchmark. We further provide an analysis of the domain-specific data and show additive gains of our model in combination with other types of topic-adapted features."
D14-2005,Syntax-Based Statistical Machine Translation,2014,-1,-1,2,1,11121,philip williams,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts,0,"The tutorial explains in detail syntax-based statistical machine translation with synchronous context free grammars (SCFG). It is aimed at researchers who have little background in this area, and gives a comprehensive overview about the main models and methods.While syntax-based models in statistical machine translation have a long history, spanning back almost 20 years, they have only recently shown superior translation quality over the more commonly used phrase-based models, and are now considered state of the art for some language pairs, such as Chinese-English (since ISI's submission to NIST 2006), and English-German (since Edinburgh's submission to WMT 2012).While the field is very dynamic, there is a core set of methods that have become dominant. Such SCFG models are implemented in the open source machine translation toolkit Moses, and the tutors draw from the practical experience of its development.The tutorial focuses on explaining core established concepts in SCFG-based approaches, which are the most popular in this area. The main goal of the tutorial is for the audience to understand how these systems work end-to-end. We review as much relevant literature as necessary, but the tutorial is not a primarily research survey.The tutorial is rounded up with open problems and advanced topics, such as computational challenges, different formalisms for syntax-based models and inclusion of semantics."
C14-2028,The {M}ate{C}at Tool,2014,7,36,15,0,3526,marcello federico,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: System Demonstrations",0,"We present a new web-based CAT tool providing translators with a professional work environment, integrating translation memories, terminology bases, concordancers, and machine translation. The tool is completely developed as open source software and has been already successfully deployed for business, research and education. The MateCat Tool represents today probably the best available open source platform for investigating, integrating, and evaluating under realistic conditions the impact of new machine translation technology on human post-editing."
C14-1041,Investigating the Usefulness of Generalized Word Representations in {SMT},2014,48,26,2,1,3159,nadir durrani,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"We investigate the use of generalized representations (POS, morphological analysis and word clusters) in phrase-based models and the N-gram-based Operation Sequence Model (OSM). Our integration enables these models to learn richer lexical and reordering patterns, consider wider contextual information and generalize better in sparse data conditions. When interpolating generalized OSM models on the standard IWSLT and WMT tasks we observed improvements of up to 1.35 on the English-to-German task and 0.63 for the German-to-English task. Using automatically generated word classes in standard phrase-based models and the OSM models yields an average improvement of 0.80 across 8 language pairs on the IWSLT shared task."
2014.iwslt-evaluation.6,{E}dinburgh {SLT} and {MT} system description for the {IWSLT} 2014 evaluation,2014,48,14,5,1,5031,alexandra birch,Proceedings of the 11th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"This paper describes the University of Edinburgh{'}s spoken language translation (SLT) and machine translation (MT) systems for the IWSLT 2014 evaluation campaign. In the SLT track, we participated in the GermanâEnglish and EnglishâFrench tasks. In the MT track, we participated in the GermanâEnglish, EnglishâFrench, ArabicâEnglish, FarsiâEnglish, HebrewâEnglish, SpanishâEnglish, and Portuguese-BrazilâEnglish tasks. For our SLT submissions, we experimented with comparing operation sequence models with bilingual neural network language models. For our MT submissions, we explored using unsupervised transliteration for languages which have a different script than English, in particular for Arabic, Farsi, and Hebrew. We also investigated syntax-based translation and system combination."
2014.iwslt-evaluation.7,Combined spoken language translation,2014,55,6,8,0.810811,3519,markus freitag,Proceedings of the 11th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"EU-BRIDGE is a European research project which is aimed at developing innovative speech translation technology. One of the collaborative efforts within EU-BRIDGE is to produce joint submissions of up to four different partners to the evaluation campaign at the 2014 International Workshop on Spoken Language Translation (IWSLT). We submitted combined translations to the GermanâEnglish spoken language translation (SLT) track as well as to the GermanâEnglish, EnglishâGerman and EnglishâFrench machine translation (MT) tracks. In this paper, we present the techniques which were applied by the different individual translation systems of RWTH Aachen University, the University of Edinburgh, Karlsruhe Institute of Technology, and Fondazione Bruno Kessler. We then show the combination approach developed at RWTH Aachen University which combined the individual systems. The consensus translations yield empirical gains of up to 2.3 points in BLEU and 1.2 points in TER compared to the best individual system."
2014.eamt-1.12,{CASMACAT}: cognitive analysis and statistical methods for advanced computer aided translation,2014,-1,-1,1,1,4417,philipp koehn,Proceedings of the 17th Annual conference of the European Association for Machine Translation,0,None
2014.eamt-1.17,Improving machine translation via triangulation and transliteration,2014,33,12,2,1,3159,nadir durrani,Proceedings of the 17th Annual conference of the European Association for Machine Translation,0,"In this paper we improve Urduxe2x86x92Hindi English machine translation through triangulation and transliteration. First we built an Urduxe2x86x92Hindi SMT system by inducing triangulated and transliterated phrase-tables from Urduxe2x80x93English and Hindixe2x80x93English phrase translation models. We then use it to translate the Urdu part of the Urdu-English parallel data into Hindi, thus creating an artificial Hindi-English parallel data. Our phrase-translation strategies give an improvement of up to 3.35 BLEU points over a baseline Urduxe2x86x92Hindi system. The synthesized data improve Hindixe2x86x92English system by 0.35 and Englishxe2x86x92Hindi system by 1.0 BLEU points."
2014.amta-tutorials.5,Statistical machine translation with the {M}oses toolkit,2014,-1,-1,3,1,22899,hieu hoang,Proceedings of the 11th Conference of the Association for Machine Translation in the Americas: Tutorials,0,None
2014.amta-researchers.11,Combining domain and topic adaptation for {SMT},2014,22,6,3,1,9981,eva hasler,Proceedings of the 11th Conference of the Association for Machine Translation in the Americas: MT Researchers Track,0,"Recent years have seen increased interest in adapting translation models to test domains that are known in advance as well as using latent topic representations to adapt to unknown test domains. However, the relationship between domains and latent topics is still somewhat unclear and topic adaptation approaches typically do not make use of domain knowledge in the training data. We show empirically that combining domain and topic adaptation approaches can be beneficial and that topic representations can be used to predict the domain of a test document. Our best combined model yields gains of up to 0.82 BLEU over a domain-adapted translation system and up to 1.67 BLEU over an unadapted system, measured on the stronger of two training conditions."
W13-2322,{A}bstract {M}eaning {R}epresentation for Sembanking,2013,23,386,8,0,40939,laura banarescu,Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse,0,"We describe Abstract Meaning Representation (AMR), a semantic representation language in which we are writing down the meanings of thousands of English sentences. We hope that a sembank of simple, whole-sentence semantic structures will spur new work in statistical natural language understanding and generation, like the Penn Treebank encouraged work on statistical parsing. This paper gives an overview of AMR and tools associated with it."
W13-2201,Findings of the 2013 {W}orkshop on {S}tatistical {M}achine {T}ranslation,2013,86,192,6,0.0864455,292,ondvrej bojar,Proceedings of the Eighth Workshop on Statistical Machine Translation,0,"We present the results of the WMT13 shared tasks, which included a translation task, a task for run-time estimation of machine translation quality, and an unofficial metrics task. This year, 143 machine translation systems were submitted to the ten translation tasks from 23 institutions. An additional 6 anonymized systems were included, and were then evaluated both automatically and manually, in our largest manual evaluation to date. The quality estimation task had four subtasks, with a total of 14 teams, submitting 55 entries."
W13-2203,The Feasibility of {HMEANT} as a Human {MT} Evaluation Metric,2013,27,11,6,1,5031,alexandra birch,Proceedings of the Eighth Workshop on Statistical Machine Translation,0,"There has been a recent surge of interest in semantic machine translation, which standard automatic metrics struggle to evaluate. A family of measures called MEANT has been proposed which uses semantic role labels (SRL) to overcome this problem. The human variant, HMEANT, has largely been evaluated using correlation with human contrastive evaluations, the standard human evaluation metric for the WMT shared tasks. In this paper we claim that for a human metric to be useful, it needs to be evaluated on intrinsic properties. It needs to be reliable; it needs to work across different language pairs; and it needs to be lightweight. Most importantly, however, a human metric must be discerning. We conclude that HMEANT is a step in the right direction, but has some serious flaws. The reliance on verbs as heads of frames, and the assumption that annotators need minimal guidelines are particularly problematic."
W13-2212,{E}dinburgh{'}s Machine Translation Systems for {E}uropean Language Pairs,2013,26,28,4,1,3159,nadir durrani,Proceedings of the Eighth Workshop on Statistical Machine Translation,0,"We validated various novel and recently proposed methods for statistical machine translation on 10 language pairs, using large data resources. We saw gains from optimizing parameters, training with sparse features, the operation sequence model, and domain adaptation techniques. We also report on utilizing a huge language model trained on 126 billion tokens. The annual machine translation evaluation campaign for European languages organized around the ACL Workshop on Statistical Machine Translation offers the opportunity to test recent advancements in machine translation in large data condition across several diverse language pairs. Building on our own developments and external contributions to the Moses open source toolkit, we carried out extensive experiments that, by early indications, led to a strong showing in the evaluation campaign. We would like to stress especially two contributions: the use of the new operation sequence model (Section 3) within Moses, and xe2x80x94 in a separate unconstraint track submission xe2x80x94 the use of a huge language model trained on 126 billion tokens with a new training tool (Section 4). 1 Initial System Development We start with systems (Haddow and Koehn, 2012) that we developed for the 2012 Workshop on Statistical Machine Translation (Callison-Burch et al., 2012). The notable features of these systems are: xe2x80xa2 Moses phrase-based models with mostly default settings xe2x80xa2 training on all available parallel data, including the large UN parallel data, the FrenchEnglish 109 parallel data and the LDC Gigaword data xe2x80xa2 very large tuning set consisting of the test sets from 2008-2010, with a total of 7,567 sentences per language xe2x80xa2 Germanxe2x80x93English with syntactic prereordering (Collins et al., 2005), compound splitting (Koehn and Knight, 2003) and use of factored representation for a POS target sequence model (Koehn and Hoang, 2007) xe2x80xa2 Englishxe2x80x93German with morphological target sequence model Note that while our final 2012 systems included subsampling of training data with modified Moore-Lewis filtering (Axelrod et al., 2011), we did not use such filtering at the starting point of our development. We will report on such filtering in Section 2. Moreover, our system development initially used the WMT 2012 data condition, since it took place throughout 2012, and we switched to WMT 2013 training data at a later stage. In this section, we report cased BLEU scores (Papineni et al., 2001) on newstest2011. 1.1 Factored Backoff (Germanxe2x80x93English) We have consistently used factored models in past WMT systems for the Germanxe2x80x93English language pairs to include POS and morphological target sequence models. But we did not use the factored decomposition of translation options into multiple mapping steps, since this usually lead to much slower systems with usually worse results. A good place, however, for factored decomposition is the handling of rare and unknown source words which have more frequent morphological variants (Koehn and Haddow, 2012a). Here, we used only factored backoff for unknown words, giving gains in BLEU of .12 for Germanxe2x80x93English. 1.2 Tuning with k-best MIRA In preparation for training with sparse features, we moved away from MERT which is known to fall"
W13-2221,{E}dinburgh{'}s Syntax-Based Machine Translation Systems,2013,18,16,3,1,26553,maria nadejde,Proceedings of the Eighth Workshop on Statistical Machine Translation,0,"We present the syntax-based string-totree statistical machine translation systems built for the WMT 2013 shared translation task. Systems were developed for four language pairs. We report on adapting parameters, targeted reduction of the tuning set, and post-evaluation experiments on rule binarization and preventing dropping of verbs. 1 Overview Syntax-based machine translation models hold the promise to overcome some of the fundamental problems of the currently dominating phrasebased approach, most importantly handling reordering for syntactically divergent language pairs and grammatical coherence of the output. We are especially interested in string-to-tree models that focus syntactic annotation on the target side, especially for morphologically rich target languages (Williams and Koehn, 2011). We have trained syntax-based systems for the language pairs xe2x80xa2 English-German, xe2x80xa2 German-English, xe2x80xa2 Czech-English, and xe2x80xa2 Russian-English. We have also tried building systems for FrenchEnglish and Spanish-English but the data size proved to be problematic given the time constraints. We give a brief description of the syntaxbased model and its implementation within the Moses system. Some of the available features are described as well as some of the pre-processing steps. Several experiments are described and final results are presented for each language pair."
P13-2063,Learning to Prune: Context-Sensitive Pruning for Syntactic {MT},2013,26,2,4,0,34672,wenduan xu,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"We present a context-sensitive chart pruning method for CKY-style MT decoding. Source phrases that are unlikely to have aligned target constituents are identified using sequence labellers learned from the parallel corpus, and speed-up is obtained by pruning corresponding chart cells. The proposed method is easy to implement, orthogonal to cube pruning and additive to its pruning power. On a full-scale Englishto-German experiment with a string-totree model, we obtain a speed-up of more than 60% over a strong baseline, with no loss in BLEU."
P13-2071,Can {M}arkov Models Over Minimal Translation Units Help Phrase-Based {SMT}?,2013,29,43,5,1,3159,nadir durrani,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"The phrase-based and N-gram-based SMT frameworks complement each other. While the former is better able to memorize, the latter provides a more principled model that captures dependencies across phrasal boundaries. Some work has been done to combine insights from these two frameworks. A recent successful attempt showed the advantage of using phrasebased search on top of an N-gram-based model. We probe this question in the reverse direction by investigating whether integrating N-gram-based translation and reordering models into a phrase-based decoder helps overcome the problematic phrasal independence assumption. A large scale evaluation over 8 language pairs shows that performance does significantly improve."
P13-2121,Scalable Modified {K}neser-{N}ey Language Model Estimation,2013,25,197,4,1,10335,kenneth heafield,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"We present an efficient algorithm to estimate large modified Kneser-Ney models including interpolation. Streaming and sorting enables the algorithm to scale to much larger models by using a fixed amount of RAM and variable amount of disk. Using one machine with 140 GB RAM for 2.8 days, we built an unpruned model on 126 billion tokens. Machine translation experiments with this model show improvement of 0.8 BLEU point over constrained systems for the 2013 Workshop on Machine Translation task in three language pairs. Our algorithm is also faster for small models: we estimated a model on 302 million tokens using 7.7% of the RAM and 14.0% of the wall time taken by SRILM. The code is open source as part of KenLM."
P13-1135,Dirt Cheap Web-Scale Parallel Text from the {C}ommon {C}rawl,2013,23,68,4,0,41506,jason smith,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Parallel text is the fuel that drives modern machine translation systems. The Web is a comprehensive source of preexisting parallel text, but crawling the entire web is impossible for all but the largest companies. We bring web-scale parallel text to the masses by mining the Common Crawl, a public Web crawl hosted on Amazonxe2x80x99s Elastic Cloud. Starting from nothing more than a set of common two-letter language codes, our open-source extension of the STRAND algorithm mined 32 terabytes of the crawl in just under a day, at a cost of about $500. Our large-scale experiment uncovers large amounts of parallel text in dozens of language pairs across a variety of domains and genres, some previously unavailable in curated datasets. Even with minimal cleaning and filtering, the resulting data boosts translation performance across the board for five different language pairs in the news domain, and on open domain test sets we see improvements of up to 5 BLEU. We make our code and data available for other researchers seeking to mine this rich new data resource. 1"
N13-1116,Grouping Language Model Boundary Words to Speed K{--}Best Extraction from Hypergraphs,2013,36,20,2,1,10335,kenneth heafield,Proceedings of the 2013 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"We propose a new algorithm to approximately extract top-scoring hypotheses from a hypergraph when the score includes an N xe2x80x90gram language model. In the popular cube pruning algorithm, every hypothesis is annotated with boundary words and permitted to recombine only if all boundary words are equal. However, many hypotheses share some, but not all, boundary words. We use these common boundary words to group hypotheses and do so recursively, resulting in a tree of hypotheses. This tree forms the basis for our new search algorithm that iteratively refines groups of boundary words on demand. Machine translation experiments show our algorithm makes translation 1.50 to 3.51 times as fast as with cube pruning in common cases."
2013.mtsummit-wptp.7,Advanced computer aided translation with a web-based workbench,2013,11,5,8,0.666667,38851,vicent alabau,Proceedings of the 2nd Workshop on Post-editing Technology and Practice,0,"We describe a web-based workbench that offers advanced computer aided translation (CAT) functionality: post-editing machine translation (MT), interactive translation prediction (ITP), visualization of word alignment, extensive logging with replay mode, integration with eye trackers and epen. It is available open source and integrates with multiple MT systems. The goal of the CASMACAT project1 is to develop an advanced computer aided translation workbench. At the mid-point of the 3-year project, we release this tool as open source software. It already includes a wide range of novel advanced types of assistance and other functionalities that do not exist together in any other computer aided translation tool. The CASMACAT is working in close collaboration with the MATECAT project2, which also has the goal of developing a new open source webbased computer aided translation tool, and focuses mainly on post-editing machine translation, adaptation methods, and ease of use that make such a tool suitable for professional users. Through this combined effort, we hope to kickstart broader research into computer aided translation methods, facilitating diverse translation process studies, and reach volunteer and professional translators without advanced technical skills. The tool is developed as a web-based platform using HTML5 and Javascript in the Browser and PHP in the backend, supported by a CAT and MT server that run as independent process (both implemented in Python but integrating tools written in various other programming languages). http://www.casmacat.eu/ http://www.matecat.com/ 1 Related Work There is increasing evidence for productivity gains of professional translators when they post-edit machine translation output. For instance, Plitt and Masselot (2010) compare post-editing machine translation against unassisted translation in a web-based tool for a number of language pairs, showing productivity gains of up to 80%. Skadixc5x86s et al. (2011) show a 30 percent increase for English-Latvian translation with a slight but acceptable degradation in quality. Federico et al. (2012) assess the benefit of offering machine translation output in addition to translation memory matches (marked as such) in a realistic work environment for translators working on legal and information technology documents. They observe productivity gains of 20-50%, roughly independent from the original translator speed and segment length, but with different results for different language pairs and domains. Moreover, Pouliquen et al. (2011) show that, aided by machine translation, non-professional post-editors may be able to create high-quality translations, comparable to a professional translation agency. So far, usage of machine translation technology has concentrated on human-computer interaction involving the human translator as a post-editor, but rarely involves the human translator influencing the decisions of the machine translation system. Recent efforts on building interactive machine translation systems include work by Langlais et al. (2000) and Barrachina et al. (2009). Both studies develop research systems looking into a tighter integration of human translators in MT processes by developing a prediction model that interactively suggests translations to the human translator as he or she types. Related work displays several word and phrase translation choices to human translators (Koehn, 2010). Figure 1: View for uploading new documents"
2013.mtsummit-european.3,{CASMACAT}: Cognitive Analysis and Statistical Methods for Advanced Computer Aided Translation,2013,0,5,1,1,4417,philipp koehn,Proceedings of Machine Translation Summit XIV: European projects,0,None
2013.iwslt-evaluation.3,{E}nglish {SLT} and {MT} system description for the {IWSLT} 2013 evaluation,2013,40,12,3,1,5031,alexandra birch,Proceedings of the 10th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"This paper gives a description of the University of Edinburgh{'}s (UEDIN) systems for IWSLT 2013. We participated in all the MT tracks and the German-to-English and Englishto-French SLT tracks. Our SLT submissions experimented with including ASR uncertainty into the decoding process via confusion networks, and looked at different ways of punctuating ASR output. Our MT submissions are mainly based on a system used in the recent evaluation campaign at the Workshop on Statistical Machine Translation [1]. We additionally explored the use of generalized representations (Brown clusters, POS and morphological tags) translating out of English into European languages."
2013.iwslt-evaluation.16,{EU}-{BRIDGE} {MT}: text translation of talks in the {EU}-{BRIDGE} project,2013,52,8,7,0.810811,3519,markus freitag,Proceedings of the 10th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"EU-BRIDGE1 is a European research project which is aimed at developing innovative speech translation technology. This paper describes one of the collaborative efforts within EUBRIDGE to further advance the state of the art in machine translation between two European language pairs, EnglishâFrench and GermanâEnglish. Four research institutions involved in the EU-BRIDGE project combined their individual machine translation systems and participated with a joint setup in the machine translation track of the evaluation campaign at the 2013 International Workshop on Spoken Language Translation (IWSLT). We present the methods and techniques to achieve high translation quality for text translation of talks which are applied at RWTH Aachen University, the University of Edinburgh, Karlsruhe Institute of Technology, and Fondazione Bruno Kessler. We then show how we have been able to considerably boost translation performance (as measured in terms of the metrics BLEU and TER) by means of system combination. The joint setups yield empirical gains of up to 1.4 points in BLEU and 2.8 points in TER on the IWSLT test sets compared to the best single systems."
W12-3102,Findings of the 2012 Workshop on Statistical Machine Translation,2012,63,247,2,0.254765,3274,chris callisonburch,Proceedings of the Seventh Workshop on Statistical Machine Translation,0,"This paper presents the results of the WMT12 shared tasks, which included a translation task, a task for machine translation evaluation metrics, and a task for run-time estimation of machine translation quality. We conducted a large-scale manual evaluation of 103 machine translation systems submitted by 34 teams. We used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality for 12 evaluation metrics. We introduced a new quality estimation task this year, and evaluated submissions from 11 teams."
W12-3139,Towards Effective Use of Training Data in Statistical Machine Translation,2012,14,32,1,1,4417,philipp koehn,Proceedings of the Seventh Workshop on Statistical Machine Translation,0,"We report on findings of exploiting large data sets for translation modeling, language modeling and tuning for the development of competitive machine translation systems for eight language pairs."
W12-3150,{GHKM} Rule Extraction and Scope-3 Parsing in {M}oses,2012,22,23,2,1,11121,philip williams,Proceedings of the Seventh Workshop on Statistical Machine Translation,0,"We developed a string-to-tree system for English--German, achieving competitive results against a hierarchical model baseline. We provide details of our implementation of GHKM rule extraction and scope-3 parsing in the Moses toolkit. We compare systems trained on the same data using different grammar extraction methods."
W12-3154,Analysing the Effect of Out-of-Domain Data on {SMT} Systems,2012,26,40,2,1,5032,barry haddow,Proceedings of the Seventh Workshop on Statistical Machine Translation,0,"In statistical machine translation (SMT), it is known that performance declines when the training data is in a different domain from the test data. Nevertheless, it is frequently necessary to supplement scarce in-domain training data with out-of-domain data. In this paper, we first try to relate the effect of the out-of-domain data on translation performance to measures of corpus similarity, then we separately analyse the effect of adding the out-of-domain data at different parts of the training pipeline (alignment, phrase extraction, and phrase scoring). Through experiments in 2 domains and 8 language pairs it is shown that the out-of-domain data improves coverage and translation of rare words, but may degrade the translation quality for more common words."
D12-1107,Language Model Rest Costs and Space-Efficient Storage,2012,27,6,2,1,10335,kenneth heafield,Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,0,"Approximate search algorithms, such as cube pruning in syntactic machine translation, rely on the language model to estimate probabilities of sentence fragments. We contribute two changes that trade between accuracy of these estimates and memory, holding sentence-level scores constant. Common practice uses lower-order entries in an N-gram model to score the first few words of a fragment; this violates assumptions made by common smoothing strategies, including Kneser-Ney. Instead, we use a unigram model to score the first word, a bigram for the second, etc. This improves search at the expense of memory. Conversely, we show how to save memory by collapsing probability and backoff into a single value without changing sentence-level scores, at the expense of less accurate estimates for sentence fragments. These changes can be stacked, achieving better estimates with unchanged memory usage. In order to interpret changes in search accuracy, we adjust the pop limit so that accuracy is unchanged and report the change in CPU time. In a German-English Moses system with target-side syntax, improved estimates yielded a 63% reduction in CPU time; for a Hiero-style version, the reduction is 21%. The compressed language model uses 26% less RAM while equivalent search quality takes 27% more CPU. Source code is released as part of KenLM."
2012.iwslt-papers.5,Simulating human judgment in machine translation evaluation campaigns,2012,73,17,1,1,4417,philipp koehn,Proceedings of the 9th International Workshop on Spoken Language Translation: Papers,0,"We present a Monte Carlo model to simulate human judgments in machine translation evaluation campaigns, such as WMT or IWSLT. We use the model to compare different ranking methods and to give guidance on the number of judgments that need to be collected to obtain sufficiently significant distinctions between systems."
2012.iwslt-papers.17,Sparse lexicalised features and topic adaptation for {SMT},2012,20,31,3,1,9981,eva hasler,Proceedings of the 9th International Workshop on Spoken Language Translation: Papers,0,"We present a new approach to domain adaptation for SMT that enriches standard phrase-based models with lexicalised word and phrase pair features to help the model select appropriate translations for the target domain (TED talks). In addition, we show how source-side sentence-level topics can be incorporated to make the features differentiate between more fine-grained topics within the target domain (topic adaptation). We compare tuning our sparse features on a development set versus on the entire in-domain corpus and introduce a new method of porting them to larger mixed-domain models. Experimental results show that our features improve performance over a MIRA baseline and that in some cases we can get additional improvements with topic features. We evaluate our methods on two language pairs, English-French and German-English, showing promising results."
2012.iwslt-evaluation.4,The {UEDIN} systems for the {IWSLT} 2012 evaluation,2012,18,16,5,1,9981,eva hasler,Proceedings of the 9th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"This paper describes the University of Edinburgh (UEDIN) systems for the IWSLT 2012 Evaluation. We participated in the ASR (English), MT (English-French, German-English) and SLT (English-French) tracks."
2012.amta-tutorials.5,Open Source Statistical Machine Translation,2012,-1,-1,1,1,4417,philipp koehn,Proceedings of the 10th Conference of the Association for Machine Translation in the Americas: Tutorials,0,"If you are interested in open-source machine translation but lack hands-on experience, this is the tutorial for you! We will start with background knowledge of statistical machine translation and then walk you through the process of installing and running an SMT system. We will show you how to prepare input data, and the most efficient way to train and use your translation systems. We shall also discuss solutions to some of the most common issues that face LSPs when using SMT, including how to tailor systems to specific clients, preserving document layout and formatting, and efficient ways of incorporating new translation memories. Previous years{'} participants have included software engineers and managers who need to have a detailed understanding of the SMT process. This is a fast-paced, hands-on tutorial that will cover the skills you need to get you up and running with open-source SMT. The teaching will be based on the Moses toolkit, the most popular open-source machine translation software currently available. No prior knowledge of MT is necessary, only an interest in it. A laptop is required for this tutorial, and you should have rudimentary knowledge of using the command line on Windows or Linux."
2012.amta-papers.9,Interpolated Backoff for Factored Translation Models,2012,-1,-1,1,1,4417,philipp koehn,Proceedings of the 10th Conference of the Association for Machine Translation in the Americas: Research Papers,0,"We propose interpolated backoff methods to strike the balance between traditional surface form translation models and factored models that decompose translation into lemma and morphological feature mapping steps. We show that this approach improves translation quality by 0.5 BLEU (German{--}English) over phrase-based models, due to the better translation of rare nouns and adjectives."
W11-2103,Findings of the 2011 Workshop on Statistical Machine Translation,2011,81,8,2,0.28644,3274,chris callisonburch,Proceedings of the Sixth Workshop on Statistical Machine Translation,0,"This paper presents the results of the WMT11 shared tasks, which included a translation task, a system combination task, and a task for machine translation evaluation metrics. We conducted a large-scale manual evaluation of 148 machine translation systems and 41 system combination entries. We used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality for 21 evaluation metrics. This year featured a Haitian Creole to English task translating SMS messages sent to an emergency response service in the aftermath of the Haitian earthquake. We also conducted a pilot 'tunable metrics' task to test whether optimizing a fixed system to different metrics would result in perceptibly different translation quality."
W11-2126,Agreement Constraints for Statistical Machine Translation into {G}erman,2011,30,20,2,1,11121,philip williams,Proceedings of the Sixth Workshop on Statistical Machine Translation,0,"Languages with rich inflectional morphology pose a difficult challenge for statistical machine translation. To address the problem of morphologically inconsistent output, we add unification-based constraints to the target-side of a string-to-tree model. By integrating constraint evaluation into the decoding process, implausible hypotheses can be penalised or filtered out during search. We use a simple heuristic process to extract agreement constraints for German and test our approach on an English-German system trained on WMT data, achieving a small improvement in translation accuracy as measured by BLEU."
W11-2130,{S}ample{R}ank Training for Phrase-Based Machine Translation,2011,19,11,3,1,5032,barry haddow,Proceedings of the Sixth Workshop on Statistical Machine Translation,0,"Statistical machine translation systems are normally optimised for a chosen gain function (metric) by using MERT to find the best model weights. This algorithm suffers from stability problems and cannot scale beyond 20-30 features. We present an alternative algorithm for discriminative training of phrase-based MT systems, SampleRank, which scales to hundreds of features, equals or beats MERT on both small and medium sized systems, and permits the use of sentence or document level features. SampleRank proceeds by repeatedly updating the model weights to ensure that the ranking of output sentences induced by the model is the same as that induced by the gain function."
D11-1079,Soft Dependency Constraints for Reordering in Hierarchical Phrase-Based Translation,2011,48,28,2,0,7649,yang gao,Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,0,"Long-distance reordering remains one of the biggest challenges facing machine translation. We derive soft constraints from the source dependency parsing to directly address the reordering problem for the hierarchical phrase-based model. Our approach significantly improves Chinese--English machine translation on a large-scale task by 0.84 BLEU points on average. Moreover, when we switch the tuning function from BLEU to the LRscore which promotes reordering, we observe total improvements of 1.21 BLEU, 1.30 LRscore and 3.36 TER over the baseline. On average our approach improves reordering precision and recall by 6.9 and 0.3 absolute points, respectively, and is found to be especially effective for long-distance reodering."
2011.iwslt-evaluation.24,Left language model state for syntactic machine translation,2011,15,13,3,1,10335,kenneth heafield,Proceedings of the 8th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"Many syntactic machine translation decoders, including Moses, cdec, and Joshua, implement bottom-up dynamic programming to integrate N-gram language model probabilities into hypothesis scoring. These decoders concatenate hypotheses according to grammar rules, yielding larger hypotheses and eventually complete translations. When hypotheses are concatenated, the language model score is adjusted to account for boundary-crossing n-grams. Words on the boundary of each hypothesis are encoded in state, consisting of left state (the first few words) and right state (the last few words). We speed concatenation by encoding left state using data structure pointers in lieu of vocabulary indices and by avoiding unnecessary queries. To increase the decoder{'}s opportunities to recombine hypothesis, we minimize the number of words encoded by left state. This has the effect of reducing search errors made by the decoder. The resulting gain in model score is smaller than for right state minimization, which we explain by observing a relationship between state minimization and language model probability. With a fixed cube pruning pop limit, we show a 3-6{\%} reduction in CPU time and improved model scores. Reducing the pop limit to the point where model scores tie the baseline yields a net 11{\%} reduction in CPU time."
W10-1703,Findings of the 2010 Joint Workshop on Statistical Machine Translation and Metrics for Machine Translation,2010,57,155,2,0.360461,3274,chris callisonburch,Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and {M}etrics{MATR},0,"This paper presents the results of the WMT10 and MetricsMATR10 shared tasks, which included a translation task, a system combination task, and an evaluation task. We conducted a large-scale manual evaluation of 104 machine translation systems and 41 system combination entries. We used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality for 26 metrics. This year we also investigated increasing the number of human judgments by hiring non-expert annotators through Amazon's Mechanical Turk."
W10-1715,More Linguistic Annotation for Statistical Machine Translation,2010,17,15,1,1,4417,philipp koehn,Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and {M}etrics{MATR},0,"We report on efforts to build large-scale translation systems for eight European language pairs. We achieve most gains from the use of larger training corpora and basic modeling, but also show promising results from integrating more linguistic annotation."
W10-1737,Aiding Pronoun Translation with Co-Reference Resolution,2010,28,70,2,0,45418,ronan nagard,Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and {M}etrics{MATR},0,We propose a method to improve the translation of pronouns by resolving their co-reference to prior mentions. We report results using two different co-reference resolution methods and point to remaining challenges.
W10-1756,A Unified Approach to Minimum Risk Training and Decoding,2010,24,7,3,1,44213,abhishek arun,Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and {M}etrics{MATR},0,"We present a unified approach to performing minimum risk training and minimum Bayes risk (MBR) decoding with BLEU in a phrase-based model. Key to our approach is the use of a Gibbs sampler that allows us to explore the entire probability distribution and maintain a strict probabilistic formulation across the pipeline. We also describe a new sampling algorithm called corpus sampling which allows us at training time to use BLEU instead of an approximation thereof. Our approach is theoretically sound and gives better (up to 0.6%BLEU) and more stable results than the standard MERT optimization algorithm. By comparing our approach to lattice MBR, we are also able to gain crucial insights about both methods."
W10-1761,Improved Translation with Source Syntax Labels,2010,25,20,2,1,22899,hieu hoang,Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and {M}etrics{MATR},0,"We present a new translation model that include undecorated hierarchical-style phrase rules, decorated source-syntax rules, and partially decorated rules.n n Results show an increase in translation performance of up to 0.8% BLEU for German--English translation when trained on the news-commentary corpus, using syntactic annotation from a source language parser. We also experimented with annotation from shallow taggers and found this increased performance by 0.5% BLEU."
N10-1078,Enabling Monolingual Translators: Post-Editing vs. Options,2010,10,27,1,1,4417,philipp koehn,Human Language Technologies: The 2010 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,"We carried out a study on monolingual translators with no knowledge of the source language, but aided by post-editing and the display of translation options. On Arabic-English and Chinese-English, using standard test data and current statistical machine translation systems, 10 monolingual translators were able to translate 35% of Arabic and 28% of Chinese sentences correctly on average, with some of the participants coming close to professional bilingual performance on some of the documents."
2010.jec-1.4,Convergence of Translation Memory and Statistical Machine Translation,2010,15,47,1,1,4417,philipp koehn,Proceedings of the Second Joint EM+/CNGL Workshop: Bringing MT to the User: Research on Integrating MT in the Translation Industry,0,"We present two methods that merge ideas from statistical machine translation (SMT) and translation memories (TM). We use a TM to retrieve matches for source segments, and replace the mismatched parts with instructions to an SMT system to fill in the gap. We show that for fuzzy matches of over 70{\%}, one method outperforms both SMT and TM baselines."
2010.amta-tutorials.5,Machine Translation with Open source Software,2010,-1,-1,1,1,4417,philipp koehn,Proceedings of the 9th Conference of the Association for Machine Translation in the Americas: Tutorials,0,None
2010.amta-papers.2,Fast Approximate String Matching with Suffix Arrays and A* Parsing,2010,-1,-1,1,1,4417,philipp koehn,Proceedings of the 9th Conference of the Association for Machine Translation in the Americas: Research Papers,0,"We present a novel exact solution to the approximate string matching problem in the context of translation memories, where a text segment has to be matched against a large corpus, while allowing for errors. We use suffix arrays to detect exact n-gram matches, A* search heuristics to discard matches and A* parsing to validate candidate segments. The method outperforms the canonical baseline by a factor of 100, with average lookup times of 4.3{--}247ms for a segment in a realistic scenario."
W09-1114,{M}onte {C}arlo inference and maximization for phrase-based translation,2009,26,25,6,1,44213,abhishek arun,Proceedings of the Thirteenth Conference on Computational Natural Language Learning ({C}o{NLL}-2009),0,Recent advances in statistical machine translation have used beam search for approximate NP-complete inference within probabilistic translation models. We present an alternative approach of sampling from the posterior distribution defined by a translation model. We define a novel Gibbs sampler for sampling translations given a source sentence and show that it effectively explores this posterior distribution. In doing so we overcome the limitations of heuristic beam search and obtain theoretically sound solutions to inference problems such as finding the maximum probability translation and minimum expected risk training and decoding.
W09-0401,Findings of the 2009 {W}orkshop on {S}tatistical {M}achine {T}ranslation,2009,48,212,2,0.52779,3274,chris callisonburch,Proceedings of the Fourth Workshop on Statistical Machine Translation,0,"This paper presents the results of the WMT09 shared tasks, which included a translation task, a system combination task, and an evaluation task. We conducted a large-scale manual evaluation of 87 machine translation systems and 22 system combination entries. We used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality, for more than 20 metrics. We present a new evaluation technique whereby system output is edited and judged for correctness."
W09-0419,Statistical Post Editing and Dictionary Extraction: {S}ystran/{E}dinburgh Submissions for {ACL}-{WMT}2009,2009,9,16,3,1,4946,loic dugast,Proceedings of the Fourth Workshop on Statistical Machine Translation,0,We describe here the two Systran/University of Edinburgh submissions for WMT2009. They involve a statistical post-editing model with a particular handling of named entities (English to French and German to English) and the extraction of phrasal rules (English to French).
W09-0429,{E}dinburgh{'}s Submission to all Tracks of the {WMT} 2009 Shared Task with Reordering and Speed Improvements to {M}oses,2009,10,29,1,1,4417,philipp koehn,Proceedings of the Fourth Workshop on Statistical Machine Translation,0,"Edinburgh University participated in the WMT 2009 shared task using the Moses phrase-based statistical machine translation decoder, building systems for all language pairs. The system configuration was identical for all language pairs (with a few additional components for the German-English language pairs). This paper describes the configuration of the systems, plus novel contributions to Moses including truecasing, more efficient decoding methods, and a framework to specify reordering constraints."
W09-0437,A Systematic Analysis of Translation Model Search Spaces,2009,20,24,4,0,4501,michael auli,Proceedings of the Fourth Workshop on Statistical Machine Translation,0,"Translation systems are complex, and most metrics do little to pinpoint causes of error or isolate system differences. We use a simple technique to discover induction errors, which occur when good translations are absent from model search spaces. Our results show that a common pruning heuristic drastically increases induction error, and also strongly suggest that the search spaces of phrase-based and hierarchical phrase-based models are highly overlapping despite the well known structural differences."
P09-5002,Topics in Statistical Machine Translation,2009,0,1,2,0,10368,kevin knight,Tutorial Abstracts of {ACL}-{IJCNLP} 2009,0,"In the past, we presented tutorials called Introduction to Statistical Machine Translation, aimed at people who know little or nothing about the field and want to get acquainted with the basic concepts. This tutorial, by contrast, goes more deeply into selected topics of intense current interest. We aim at two types of participants:n n 1. People who understand the basic idea of statistical machine translation and want to get a survey of hot-topic current research, in terms that they can understand.n n 2. People associated with statistical machine translation work, who have not had time to study the most current topics in depth."
P09-4005,A Web-Based Interactive Computer Aided Translation Tool,2009,6,40,1,1,4417,philipp koehn,Proceedings of the {ACL}-{IJCNLP} 2009 Software Demonstrations,0,"We developed caitra, a novel tool that aids human translators by (a) making suggestions for sentence completion in an interactive machine translation setting, (b) providing alternative word and phrase translations, and (c) allowing them to post-edit machine translation output. The tool uses the Moses decoder, is implemented in Ruby on Rails and C and delivered over the web."
E09-1043,Improving Mid-Range Re-Ordering Using Templates of Factors,2009,16,10,2,1,22899,hieu hoang,Proceedings of the 12th Conference of the {E}uropean Chapter of the {ACL} ({EACL} 2009),0,"We extend the factored translation model (Koehn and Hoang, 2007) to allow translations of longer phrases composed of factors such as POS and morphological tags to act as templates for the selection and reordering of surface phrase translation. We also reintroduce the use of alignment information within the decoder, which forms an integral part of decoding in the Alignment Template System (Och, 2002), into phrase-based decoding.n n Results show an increase in translation performance of up to 1.0% bleu for out-of-domain French-English translation. We also show how this method compares and relates to lexicalized reordering."
E09-1082,Word Lattices for Multi-Source Translation,2009,81,37,3,1,47106,josh schroeder,Proceedings of the 12th Conference of the {E}uropean Chapter of the {ACL} ({EACL} 2009),0,"Multi-source statistical machine translation is the process of generating a single translation from multiple inputs. Previous work has focused primarily on selecting from potential outputs of separate translation systems, and solely on multi-parallel corpora and test sets. We demonstrate how multi-source translation can be adapted for multiple monolingual inputs. We also examine different approaches to dealing with multiple sources, including consensus decoding, and we present a novel method of input combination to generate lattices for multi-source translation within a single translation model."
2009.mtsummit-wpt.15,Panel on Patent Translation,2009,-1,-1,1,1,4417,philipp koehn,Proceedings of the Third Workshop on Patent Translation,0,None
2009.mtsummit-posters.6,Selective addition of corpus-extracted phrasal lexical rules to a rule-based machine translation system,2009,-1,-1,3,1,4946,loic dugast,Proceedings of Machine Translation Summit XII: Posters,0,None
2009.mtsummit-papers.7,462 Machine Translation Systems for {E}urope,2009,9,65,1,1,4417,philipp koehn,Proceedings of Machine Translation Summit XII: Papers,0,"We built 462 machine translation systems for all language pairs of the Acquis Communautaire corpus. We report and analyse the performance of these system, and compare them against pivot translation and a number of system combination methods (multi-pivot, multisource) that are possible due to the available systems."
2009.mtsummit-papers.8,Interactive Assistance to Human Translators using Statistical Machine Translation Methods,2009,7,34,1,1,4417,philipp koehn,Proceedings of Machine Translation Summit XII: Papers,0,None
2009.iwslt-papers.4,"A unified framework for phrase-based, hierarchical, and syntax-based statistical machine translation",2009,24,47,2,1,22899,hieu hoang,Proceedings of the 6th International Workshop on Spoken Language Translation: Papers,0,"Despite many differences between phrase-based, hierarchical, and syntax-based translation models, their training and testing pipelines are strikingly similar. Drawing on this fact, we extend the Moses toolkit to implement hierarchical and syntactic models, making it the first open source toolkit with end-to-end support for all three of these popular models in a single package. This extension substantially lowers the barrier to entry for machine translation research across multiple models."
2009.iwslt-keynotes.1,Human translation and machine translation,2009,0,0,1,1,4417,philipp koehn,Proceedings of the 6th International Workshop on Spoken Language Translation: Plenaries,0,None
W08-0510,Design of the {M}oses Decoder for Statistical Machine Translation,2008,25,29,2,1,22899,hieu hoang,"Software Engineering, Testing, and Quality Assurance for Natural Language Processing",0,"We present a description of the implementation of the open source decoder for statistical machine translation which has become popular with many researchers in SMT research. The goal of the project is to create an open, high quality phrase-based decoder which can reduce the time and barrier to entry for researchers wishing to do SMT research. We discuss the major design objective for the Moses decoder, its performance relative to other SMT decoders, and the steps we are taking to ensure that its success will continue."
W08-0309,Further Meta-Evaluation of Machine Translation,2008,45,218,3,0.746821,3274,chris callisonburch,Proceedings of the Third Workshop on Statistical Machine Translation,0,"This paper analyzes the translation quality of machine translation systems for 10 language pairs translating between Czech, English, French, German, Hungarian, and Spanish. We report the translation quality of over 30 diverse translation systems based on a large-scale manual evaluation involving hundreds of hours of effort. We use the human judgments of the systems to analyze automatic evaluation metrics for translation quality, and we report the strength of the correlation with human judgments at both the system-level and at the sentence-level. We validate our manual evaluation methodology by measuring intra- and inter-annotator agreement, and collecting timing information."
W08-0318,Towards better Machine Translation Quality for the {G}erman-{E}nglish Language Pairs,2008,9,29,1,1,4417,philipp koehn,Proceedings of the Third Workshop on Statistical Machine Translation,0,"The Edinburgh submissions to the shared task of the Third Workshop on Statistical Machine Translation (WMT-2008) incorporate recent advances to the open source Moses system. We made a special effort on the German--English and English--German language pairs, leading to substantial improvements."
W08-0327,Can we Relearn an {RBMT} System?,2008,7,15,3,1,4946,loic dugast,Proceedings of the Third Workshop on Statistical Machine Translation,0,"This paper describes SYSTRAN submissions for the shared task of the third Workshop on Statistical Machine Translation at ACL. Our main contribution consists in a French-English statistical model trained without the use of any human-translated parallel corpus. In substitution, we translated a monolingual corpus with SYSTRAN rule-based translation engine to produce the parallel corpus. The results are provided herein, along with a measure of error analysis."
P08-1087,Enriching Morphologically Poor Languages for Statistical Machine Translation,2008,23,85,2,0,5140,eleftherios avramidis,Proceedings of ACL-08: HLT,1,"We address the problem of translating from morphologically poor to morphologically rich languages by adding per-word linguistic information to the source language. We use the syntax of the source sentence to extract information for noun cases and verb persons and annotate the corresponding words accordingly. In experiments, we show improved performance for translating from English into Greek and Czech. For Englishxe2x80x90Greek, we reduce the error on the verb conjugation from 19% to 5.4% and noun case agreement from 9% to 6%."
I08-2089,Large and Diverse Language Models for Statistical Machine Translation,2008,14,36,2,0,5770,holger schwenk,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{II},0,This paper presents methods to combine large language models trained from diverse text sources and applies them to a state-ofart Frenchxe2x80x93English and Arabicxe2x80x93English machine translation system. We show gains of over 2 BLEU points over a strong baseline by using continuous space language models in re-ranking.
D08-1078,Predicting Success in Machine Translation,2008,13,50,3,1,5031,alexandra birch,Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,0,"The performance of machine translation systems varies greatly depending on the source and target languages involved. Determining the contribution of different characteristics of language pairs on system performance is key to knowing what aspects of machine translation to improve and which are irrelevant. This paper investigates the effect of different explanatory variables on the performance of a phrase-based system for 110 European language pairs. We show that three factors are strong predictors of performance in isolation: the amount of reordering, the morphological complexity of the target language and the historical relatedness of the two languages. Together, these factors contribute 75% to the variability of the performance of the system."
W07-0702,{CCG} Supertags in Factored Statistical Machine Translation,2007,22,82,3,1,5031,alexandra birch,Proceedings of the Second Workshop on Statistical Machine Translation,0,Combinatorial Categorial Grammar (CCG) supertags present phrase-based machine translation with an opportunity to access rich syntactic information at a word level. The challenge is incorporating this information into the translation process. Factored translation models allow the inclusion of supertags as a factor in the source or target language. We show that this results in an improvement in the quality of translation and that the value of syntactic supertags in flat structured phrase-based models is largely due to better local reorderings.
W07-0718,(Meta-) Evaluation of Machine Translation,2007,50,280,3,1,3274,chris callisonburch,Proceedings of the Second Workshop on Statistical Machine Translation,0,"This paper evaluates the translation quality of machine translation systems for 8 language pairs: translating French, German, Spanish, and Czech to English and back. We carried out an extensive human evaluation which allowed us not only to rank the different MT systems, but also to perform higher-level analysis of the evaluation process. We measured timing and intra- and inter-annotator agreement for three types of subjective evaluation. We measured the correlation of automatic evaluation metrics with human judgments. This meta-evaluation reveals surprising facts about the most commonly used methodologies."
W07-0732,Statistical Post-Editing on {SYSTRAN}{`}s Rule-Based Translation System,2007,4,98,3,1,4946,loic dugast,Proceedings of the Second Workshop on Statistical Machine Translation,0,This article describes the combination of a SYSTRAN system with a statistical post-editing (SPE) system. We document qualitative analysis on two experiments performed in the shared task of the ACL 2007 Workshop on Statistical Machine Translation. Comparative results and more integrated hybrid techniques are discussed.
W07-0733,Experiments in Domain Adaptation for Statistical Machine Translation,2007,8,260,1,1,4417,philipp koehn,Proceedings of the Second Workshop on Statistical Machine Translation,0,"The special challenge of the WMT 2007 shared task was domain adaptation. We took this opportunity to experiment with various ways of adapting a statistical machine translation systems to a special domain (here: news commentary), when most of the training data is from a different domain (here: European Parliament speeches). This paper also gives a description of the submission of the University of Edinburgh to the shared task."
P07-2045,{M}oses: Open Source Toolkit for Statistical Machine Translation,2007,13,3819,1,1,4417,philipp koehn,Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,0,"We describe an open-source toolkit for statistical machine translation whose novel contributions are (a) support for linguistically motivated factors, (b) confusion network decoding, and (c) efficient data formats for translation models and language models. In addition to the SMT decoder, the toolkit also includes a wide variety of tools for training, tuning and applying the system to many translation tasks."
D07-1077,{C}hinese Syntactic Reordering for Statistical Machine Translation,2007,19,193,3,0,4722,chao wang,Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL}),0,"Syntactic reordering approaches are an effective method for handling word-order differences between source and target languages in statistical machine translation (SMT) systems. This paper introduces a reordering approach for translation from Chinese to English. We describe a set of syntactic reordering rules that exploit systematic differences between Chinese and English word order. The resulting system is used as a preprocessor for both training and test sentences, transforming Chinese sentences to be much closer to English in terms of their word order. We evaluated the reordering approach within the MOSES phrase-based SMT system (Koehn et al., 2007). The reordering approach improved the BLEU score for the MOSES system from 28.52 to 30.86 on the NIST 2006 evaluation data. We also conducted a series of experiments to analyze the accuracy and impact of different types of reordering rules."
D07-1091,Factored Translation Models,2007,26,443,1,1,4417,philipp koehn,Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL}),0,"We present an extension of phrase-based statistical machine translation models that enables the straight-forward integration of additional annotation at the word-level xe2x80x94 may it be linguistic markup or automatically generated word classes. In a number of experiments we show that factored translation models lead to better translation performance, both in terms of automatic scores, as well as more grammatical coherence."
2007.mtsummit-tutorials.1,Statistical machine translation,2007,2,17,2,0,10368,kevin knight,Proceedings of Machine Translation Summit XI: Tutorials,0,"A method of statistical machine translation (SMT) is provided. The method comprises generating reordering knowledge based on the syntax of a source language (SL) and a number of alignment matrices that map sample SL sentences with sample target language (TL) sentences. The method further comprises receiving a SL word string and parsing the SL word string into a parse tree that represents the syntactic properties of the SL word string. The nodes on the parse tree are reordered based on the generated reordering knowledge in order to provide reordered word strings. The method further comprises translating a number of reordered word strings to create a number of TL word strings, and identifying a statistically preferred TL word string as a preferred translation of the SL word string."
2007.mtsummit-papers.3,Online learning methods for discriminative training of phrase based statistical machine translation,2007,-1,-1,2,1,44213,abhishek arun,Proceedings of Machine Translation Summit XI: Papers,0,None
2007.mtsummit-invited.3,{E}uro{M}atrix {--} machine translation for all {E}uropean languages,2007,-1,-1,1,1,4417,philipp koehn,Proceedings of Machine Translation Summit XI: Invited papers,0,None
2007.mtsummit-aptme.2,Evaluating evaluation {--} lessons from the {WMT} 2007 shared task,2007,-1,-1,1,1,4417,philipp koehn,Proceedings of the Workshop on Automatic procedures in MT evaluation,0,None
2007.iwslt-1.6,The {U}niversity of {E}dinburgh system description for {IWSLT} 2007,2007,11,4,2,1,47106,josh schroeder,Proceedings of the Fourth International Workshop on Spoken Language Translation,0,"We present the University of Edinburgh{'}s submission for the IWSLT 2007 shared task. Our efforts focused on adapting our statistical machine translation system to the open data conditions for the Italian-English task of the evaluation campaign. We examine the challenges of building a system with a limited set of in-domain development data (SITAL), a small training corpus in a related but distinct domain (BTEC), and a large out of domain corpus (Europarl). We concentrated on the corrected text track, and present additional results of our experiments using the open-source Moses MT system with speech input."
W06-3114,Manual and Automatic Evaluation of Machine Translation between {E}uropean Languages,2006,18,184,1,1,4417,philipp koehn,Proceedings on the Workshop on Statistical Machine Translation,0,"We evaluated machine translation performance for six European language pairs that participated in a shared task: translating French, German, Spanish texts to English and back. Evaluation was done automatically using the Bleu score and manually on fluency and adequacy."
W06-3123,"Constraining the Phrase-Based, Joint Probability Statistical Translation Model",2006,20,45,4,1,5031,alexandra birch,Proceedings on the Workshop on Statistical Machine Translation,0,"The joint probability model proposed by Marcu and Wong (2002) provides a strong probabilistic framework for phrase-based statistical machine translation (SMT). The model's usefulness is, however, limited by the computational complexity of estimating parameters at the phrase level. We present the first model to use word alignments for constraining the space of phrasal alignments searched during Expectation Maximization (EM) training. Constraining the joint model improves performance, showing results that are very close to state-of-the-art phrase-based models. It also allows it to scale up to larger corpora and therefore be more widely applicable."
N06-1003,Improved Statistical Machine Translation Using Paraphrases,2006,19,242,2,1,3274,chris callisonburch,"Proceedings of the Human Language Technology Conference of the {NAACL}, Main Conference",0,"Parallel corpora are crucial for training SMT systems. However, for many language pairs they are available only in very limited quantities. For these language pairs a huge portion of phrases encountered at run-time will be unknown. We show how techniques from paraphrasing can be used to deal with these otherwise unknown source language phrases. Our results show that augmenting a state-of-the-art SMT system with paraphrases leads to significantly improved coverage and translation quality. For a training corpus with 10,000 sentence pairs we increase the coverage of unique test set unigrams from 48% to 90%, with more than half of the newly covered items accurately translated, as opposed to none in current approaches."
E06-1032,Re-evaluating the Role of {B}leu in Machine Translation Research,2006,14,409,3,1,3274,chris callisonburch,11th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"We argue that the machine translation community is overly reliant on the Bleu machine translation evaluation metric. We show that an improved Bleu score is neither necessary nor sufficient for achieving an actual improvement in translation quality, and give two significant counterexamples to Bleuxe2x80x99s correlation with human judgments of quality. This offers new potential for research which was previously deemed unpromising by an inability to improve upon Bleu scores."
2006.amta-panels.3,Statistical machine translation and hybrid machine translation,2006,0,2,1,1,4417,philipp koehn,Proceedings of the 7th Conference of the Association for Machine Translation in the Americas: Panel on hybrid machine translation: why and how?,0,"PROBLEM TO BE SOLVED: To provide a machine translation device and a machine translation program for displaying an original to be translated and a translation that is a translation result side by side. SOLUTION: In a Japanese-English translation screen 100, a sentence number window 130 is provided on the left side thereof, and the remaining right area is divided almost equally to two parts, in which a Japanese original window 110 is provided on the left side and an English translate window 120 on the left side. In an English-Japanese translation screen, a sentence number window is provided on the left side, and the remaining right area is divided almost equally to two parts in which an English original window is provided on the left side, and a Japanese translate window on the right side. The background color of the Japanese original window 110 is red, and that of the English translate window 120 is blue in the Japanese-English translation screen 100, while the background color of the English original window is blue and that of the Japanese translate window is red in the English-Japanese translation screen. Since the background colors of Japanese and English are differed, with red for Japanese and blue for English, the discriminating property between the Japanese window and the English window is enhanced. COPYRIGHT: (C)2006,JPO&NCIPI"
2006.amta-panels.3,Social impact of {MT},2006,0,2,1,1,4417,philipp koehn,Proceedings of the 7th Conference of the Association for Machine Translation in the Americas: Panel on machine translation for social impact,0,"PROBLEM TO BE SOLVED: To provide a machine translation device and a machine translation program for displaying an original to be translated and a translation that is a translation result side by side. SOLUTION: In a Japanese-English translation screen 100, a sentence number window 130 is provided on the left side thereof, and the remaining right area is divided almost equally to two parts, in which a Japanese original window 110 is provided on the left side and an English translate window 120 on the left side. In an English-Japanese translation screen, a sentence number window is provided on the left side, and the remaining right area is divided almost equally to two parts in which an English original window is provided on the left side, and a Japanese translate window on the right side. The background color of the Japanese original window 110 is red, and that of the English translate window 120 is blue in the Japanese-English translation screen 100, while the background color of the English original window is blue and that of the Japanese translate window is red in the English-Japanese translation screen. Since the background colors of Japanese and English are differed, with red for Japanese and blue for English, the discriminating property between the Japanese window and the English window is enhanced. COPYRIGHT: (C)2006,JPO&NCIPI"
W05-0820,Shared Task: Statistical Machine Translation between {E}uropean Languages,2005,11,42,1,1,4417,philipp koehn,Proceedings of the {ACL} Workshop on Building and Using Parallel Texts,0,"The ACL-2005 Workshop on Parallel Texts hosted a shared task on building statistical machine translation systems for four European language pairs: French-English, German-English, Spanish-English, and Finnish-English. Eleven groups participated in the event. This paper describes the goals, the task definition and resources, as well as results and some analysis."
P05-1066,Clause Restructuring for Statistical Machine Translation,2005,27,482,2,0,11246,michael collins,Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics ({ACL}{'}05),1,"We describe a method for incorporating syntactic information in statistical machine translation systems. The first step of the method is to parse the source language string that is being translated. The second step is to apply a series of transformations to the parse tree, effectively reordering the surface string on the source language side of the translation system. The goal of this step is to recover an underlying word order that is closer to the target language word-order than the original string. The reordering approach is applied as a pre-processing step in both the training and decoding phases of a phrase-based statistical MT system. We describe experiments on translation from German to English, showing an improvement from 25.2% Bleu score for a baseline system to 26.8% Bleu score for the system with reordering, a statistically significant improvement."
2005.mtsummit-papers.11,{E}uroparl: A Parallel Corpus for Statistical Machine Translation,2005,9,2092,1,1,4417,philipp koehn,Proceedings of Machine Translation Summit X: Papers,0,"We collected a corpus of parallel text in 11 languages from the proceedings of the European Parliament, which are published on the web. This corpus has found widespread use in the NLP community. Here, we focus on its acquisition and its application as training data for statistical machine translation (SMT). We trained SMT systems for 110 language pairs, which reveal interesting clues into the challenges ahead."
2005.iwslt-1.8,{E}dinburgh System Description for the 2005 {IWSLT} Speech Translation Evaluation,2005,9,307,1,1,4417,philipp koehn,Proceedings of the Second International Workshop on Spoken Language Translation,0,Our participation in the IWSLT 2005 speech translation task is our first effort to work on limited domain speech data. We adapted our statistical machine translation system that performed successfully in previous DARPA competitions on open domain text translations. We participated in the supplied corpora transcription track. We achieved the highest BLEU score in 2 out of 5 language pairs and had competitive results for the other language pairs.
W04-3250,Statistical Significance Tests for Machine Translation Evaluation,2004,15,822,1,1,4417,philipp koehn,Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing,0,"If two translation systems differ differ in performance on a test set, can we trust that this indicates a difference in true system quality? To answer this question, we describe bootstrap resampling methods to compute statistical significance of test results, and validate them on the concrete example of the BLEU score. Even for small test sizes of only 300 sentences, our methods may give us assurances that test result differences are real."
2004.amta-tutorials.3,Introduction to statistical machine translation,2004,-1,-1,1,1,4417,philipp koehn,Proceedings of the 6th Conference of the Association for Machine Translation in the Americas: Tutorial Descriptions,0,None
P03-1040,Feature-Rich Statistical Translation of Noun Phrases,2003,16,96,1,1,4417,philipp koehn,Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,1,We define noun phrase translation as a subtask of machine translation. This enables us to build a dedicated noun phrase translation subsystem that improves over the currently best general statistical machine translation methods by incorporating special modeling and special features. We achieved 65.5% translation accuracy in a German-English translation task vs. 53.2% with IBM Model 4.
N03-5005,What{'}s New in Statistical Machine Translation,2003,0,17,2,0,10368,kevin knight,Companion Volume of the Proceedings of {HLT}-{NAACL} 2003 - Tutorial Abstracts,0,"Automatic translation from one human language to another using computers, better known as machine translation (MT), is a long-standing goal of computer science. Accurate translation requires a great deal of knowledge about the usage and meaning of words, the structure of phrases, the meaning of sentences, and which real-life situations are plausible. For general-purpose translation, the amount of required knowledge is staggering, and it is not clear how to prioritize knowledge acquisition efforts.Recently, there has been a fair amount of research into extracting translation-relevant knowledge automatically from bilingual texts. In the early 1990s, IBM pioneered automatic bilingual-text analysis. A 1999 workshop at Johns Hopkins University saw a re-implementation of many of the core components of this work, aimed at attracting more researchers into the field. Over the past years, several statistical MT projects have appeared in North America, Europe, and Asia, and the literature is growing substantially. We will provide a technical overview of the state-of-the-art."
N03-2026,Desparately Seeking {C}ebuano,2003,2,7,11,0,12879,douglas oard,Companion Volume of the Proceedings of {HLT}-{NAACL} 2003 - Short Papers,0,This paper describes an effort to rapidly develop language resources and component technology to support searching Cebuano news stories using English queries. Results from the first 60 hours of the exercise are presented.
N03-1017,Statistical Phrase-Based Translation,2003,15,2805,1,1,4417,philipp koehn,Proceedings of the 2003 Human Language Technology Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,"We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, previously proposed phrase-based translation models. Within our framework, we carry out a large number of experiments to understand better and explain why phrase-based models out-perform word-based models. Our empirical results, which hold for all examined language pairs, suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations. Surprisingly, learning phrases longer than three words and learning phrases from high-accuracy word-level alignment models does not have a strong impact on performance. Learning only syntactically motivated phrases degrades the performance of our systems."
E03-1076,Empirical Methods for Compound Splitting,2003,11,210,1,1,4417,philipp koehn,10th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,Compounded words are a challenge for NLP applications such as machine translation (MT). We introduce methods to learn splitting rules from monolingual and parallel corpora. We evaluate them against a gold standard and measure their impact on performance of statistical MT systems. Results show accuracy of 99.1% and performance gains for MT of 0.039 BLEU on a German-English noun phrase translation task.
W02-0902,Learning a Translation Lexicon from Monolingual Corpora,2002,13,204,1,1,4417,philipp koehn,Proceedings of the {ACL}-02 Workshop on Unsupervised Lexical Acquisition,0,"This paper presents work on the task of constructing a word-level translation lexicon purely from unrelated monolingual corpora. We combine various clues such as cognates, similar context, preservation of word similarity, and word frequency. Experimental results for the construction of a German-English noun lexicon are reported. Noun translation accuracy of 39% scored against a parallel test corpus could be achieved."
W01-0504,Knowledge Sources for Word-Level Translation Models,2001,0,82,1,1,4417,philipp koehn,Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing,0,None
