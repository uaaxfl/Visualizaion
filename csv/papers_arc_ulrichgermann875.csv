2021.iwslt-1.4,The {U}niversity of {E}dinburgh{'}s Submission to the {IWSLT}21 Simultaneous Translation Task,2021,-1,-1,2,0,5731,sukanta sen,Proceedings of the 18th International Conference on Spoken Language Translation (IWSLT 2021),0,"We describe our submission to the IWSLT 2021 shared task on simultaneous text-to-text English-German translation. Our system is based on the re-translation approach where the agent re-translates the whole source prefix each time it receives a new source token. This approach has the advantage of being able to use a standard neural machine translation (NMT) inference engine with beam search, however, there is a risk that incompatibility between successive re-translations will degrade the output. To improve the quality of the translations, we experiment with various approaches: we use a fixed size wait at the beginning of the sentence, we use a language model score to detect translatable units, and we apply dynamic masking to determine when the translation is unstable. We find that a combination of dynamic masking and language model score obtains the best latency-quality trade-off."
2021.eacl-demos.26,{E}uropean Language Grid: A Joint Platform for the {E}uropean Language Technology Community,2021,-1,-1,9,0.162294,60,georg rehm,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations,0,"Europe is a multilingual society, in which dozens of languages are spoken. The only option to enable and to benefit from multilingualism is through Language Technologies (LT), i.e., Natural Language Processing and Speech Technologies. We describe the European Language Grid (ELG), which is targeted to evolve into the primary platform and marketplace for LT in Europe by providing one umbrella platform for the European LT landscape, including research and industry, enabling all stakeholders to upload, share and distribute their services, products and resources. At the end of our EU project, which will establish a legal entity in 2022, the ELG will provide access to approx. 1300 services for all European languages as well as thousands of data sets."
2020.wmt-1.17,"Speed-optimized, Compact Student Models that Distill Knowledge from a Larger Teacher Model: the {UEDIN}-{CUNI} Submission to the {WMT} 2020 News Translation Task",2020,-1,-1,1,1,5732,ulrich germann,Proceedings of the Fifth Conference on Machine Translation,0,"We describe the joint submission of the University of Edinburgh and Charles University, Prague, to the Czech/English track in the WMT 2020 Shared Task on News Translation. Our fast and compact student models distill knowledge from a larger, slower teacher. They are designed to offer a good trade-off between translation quality and inference efficiency. On the WMT 2020 Czech â English test sets, they achieve translation speeds of over 700 whitespace-delimited source words per second on a single CPU thread, thus making neural translation feasible on consumer hardware without a GPU."
2020.wmt-1.18,The {U}niversity of {E}dinburgh{'}s submission to the {G}erman-to-{E}nglish and {E}nglish-to-{G}erman Tracks in the {WMT} 2020 News Translation and Zero-shot Translation Robustness Tasks,2020,-1,-1,1,1,5732,ulrich germann,Proceedings of the Fifth Conference on Machine Translation,0,This paper describes the University of Edinburgh{'}s submission of German {\textless}-{\textgreater} English systems to the WMT2020 Shared Tasks on News Translation and Zero-shot Robustness.
2020.lrec-1.413,{E}uropean Language Grid: An Overview,2020,11,6,34,0.162294,60,georg rehm,Proceedings of the 12th Language Resources and Evaluation Conference,0,"With 24 official EU and many additional languages, multilingualism in Europe and an inclusive Digital Single Market can only be enabled through Language Technologies (LTs). European LT business is dominated by hundreds of SMEs and a few large players. Many are world-class, with technologies that outperform the global players. However, European LT business is also fragmented {--} by nation states, languages, verticals and sectors, significantly holding back its impact. The European Language Grid (ELG) project addresses this fragmentation by establishing the ELG as the primary platform for LT in Europe. The ELG is a scalable cloud platform, providing, in an easy-to-integrate way, access to hundreds of commercial and non-commercial LTs for all European languages, including running tools and services as well as data sets and resources. Once fully operational, it will enable the commercial and non-commercial European LT community to deposit and upload their technologies and data sets into the ELG, to deploy them through the grid, and to connect with other resources. The ELG will boost the Multilingual Digital Single Market towards a thriving European LT community, creating new jobs and opportunities. Furthermore, the ELG project organises two open calls for up to 20 pilot projects. It also sets up 32 national competence centres and the European LT Council for outreach and coordination purposes."
2020.iwslt-1.14,Character Mapping and Ad-hoc Adaptation: {E}dinburgh{'}s {IWSLT} 2020 Open Domain Translation System,2020,-1,-1,3,0,5887,pinzhen chen,Proceedings of the 17th International Conference on Spoken Language Translation,0,"This paper describes the University of Edinburgh{'}s neural machine translation systems submitted to the IWSLT 2020 open domain Japanese$\leftrightarrow$Chinese translation task. On top of commonplace techniques like tokenisation and corpus cleaning, we explore character mapping and unsupervised decoding-time adaptation. Our techniques focus on leveraging the provided data, and we show the positive impact of each technique through the gradual improvement of BLEU."
W19-5304,The {U}niversity of {E}dinburgh{'}s Submissions to the {WMT}19 News Translation Task,2019,24,0,3,0,7687,rachel bawden,"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)",0,"The University of Edinburgh participated in the WMT19 Shared Task on News Translation in six language directions: EnglishâGujarati, EnglishâChinese, GermanâEnglish, and EnglishâCzech. For all translation directions, we created or used back-translations of monolingual data in the target language as additional synthetic training data. For EnglishâGujarati, we also explored semi-supervised MT with cross-lingual language model pre-training, and translation pivoting through Hindi. For translation to and from Chinese, we investigated character-based tokenisation vs. sub-word segmentation of Chinese text. For GermanâEnglish, we studied the impact of vast amounts of back-translated training data on translation quality, gaining a few additional insights over Edunov et al. (2018). For EnglishâCzech, we compared different preprocessing and tokenisation regimes."
W18-6412,The {U}niversity of {E}dinburgh{'}s Submissions to the {WMT}18 News Translation Task,2018,0,4,4,0,5032,barry haddow,Proceedings of the Third Conference on Machine Translation: Shared Task Papers,0,"The University of Edinburgh made submissions to all 14 language pairs in the news translation task, with strong performances in most pairs. We introduce new RNN-variant, mixed RNN/Transformer ensembles, data selection and weighting, and extensions to back-translation."
W18-2508,Integrating Multiple {NLP} Technologies into an Open-source Platform for Multilingual Media Monitoring,2018,0,0,1,1,5732,ulrich germann,Proceedings of Workshop for {NLP} Open Source Software ({NLP}-{OSS}),0,"The open-source SUMMA Platform is a highly scalable distributed architecture for monitoring a large number of media broadcasts in parallel, with a lag behind actual broadcast time of at most a few minutes. It assembles numerous state-of-the-art NLP technologies into a fully automated media ingestion pipeline that can record live broadcasts, detect and transcribe spoken content, translate from several languages (original text or transcribed speech) into English, recognize Named Entities, detect topics, cluster and summarize documents across language barriers, and extract and store factual claims in these news items. This paper describes the intended use cases and discusses the system design decisions that allowed us to integrate state-of-the-art NLP modules into an effective workflow with comparatively little effort."
P18-4017,The {SUMMA} Platform: A Scalable Infrastructure for Multi-lingual Multi-media Monitoring,2018,0,1,1,1,5732,ulrich germann,"Proceedings of {ACL} 2018, System Demonstrations",0,"The open-source SUMMA Platform is a highly scalable distributed architecture for monitoring a large number of media broadcasts in parallel, with a lag behind actual broadcast time of at most a few minutes. The Platform offers a fully automated media ingestion pipeline capable of recording live broadcasts, detection and transcription of spoken content, translation of all text (original or transcribed) into English, recognition and linking of Named Entities, topic detection, clustering and cross-lingual multi-document summarization of related media items, and last but not least, extraction and storage of factual claims in these news items. Browser-based graphical user interfaces provide humans with aggregated information as well as structured access to individual news items stored in the Platform{'}s database. This paper describes the intended use cases and provides an overview over the system{'}s implementation."
P18-4020,{M}arian: Fast Neural Machine Translation in {C}++,2018,8,28,8,0,3523,marcin junczysdowmunt,"Proceedings of {ACL} 2018, System Demonstrations",0,"We present Marian, an efficient and self-contained Neural Machine Translation framework with an integrated automatic differentiation engine based on dynamic computation graphs. Marian is written entirely in C++. We describe the design of the encoder-decoder framework and demonstrate that a research-friendly toolkit can achieve high training and translation speed."
W17-4739,The {U}niversity of {E}dinburgh{'}s Neural {MT} Systems for {WMT}17,2017,6,37,4,0,2690,rico sennrich,Proceedings of the Second Conference on Machine Translation,0,"This paper describes the University of Edinburgh's submissions to the WMT17 shared news translation and biomedical translation tasks. We participated in 12 translation directions for news, translating between English and Czech, German, Latvian, Russian, Turkish and Chinese. For the biomedical task we submitted systems for English to Czech, German, Polish and Romanian. Our systems are neural machine translation systems trained with Nematus, an attentional encoder-decoder. We follow our setup from last year and build BPE-based models with parallel and back-translated monolingual training data. Novelties this year include the use of deep architectures, layer normalization, and more compact models due to weight tying and improvements in BPE segmentations. We perform extensive ablative experiments, reporting on the effectivenes of layer normalization, deep architectures, and different ensembling techniques."
E17-3029,The {SUMMA} Platform Prototype,2017,8,1,2,0,28433,renars liepins,Proceedings of the Software Demonstrations of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"We present the first prototype of the SUMMA Platform: an integrated platform for multilingual media monitoring. The platform contains a rich suite of low-level and high-level natural language processing technologies: automatic speech recognition of broadcast media, machine translation, automated tagging and classification of named entities, semantic parsing to detect relationships between entities, and automatic construction / augmentation of factual knowledge bases. Implemented on the Docker platform, it can easily be deployed, customised, and scaled to large volumes of incoming media streams."
D17-1156,Regularization techniques for fine-tuning in neural machine translation,2017,18,2,3,0,5033,antonio barone,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,0,"We investigate techniques for supervised domain adaptation for neural machine translation where an existing model trained on a large out-of-domain dataset is adapted to a small in-domain dataset. In this scenario, overfitting is a major challenge. We investigate a number of techniques to reduce overfitting and improve transfer learning, including regularization techniques such as dropout and L2-regularization towards an out-of-domain prior. In addition, we introduce tuneout, a novel regularization technique inspired by dropout. We apply these techniques, alone and in combination, to neural machine translation, obtaining improvements on IWSLT datasets for EnglishâGerman and EnglishâRussian. We also investigate the amounts of in-domain training data needed for domain adaptation in NMT, and find a logarithmic relationship between the amount of training data and gain in BLEU score."
W16-2368,Bilingual Document Alignment with Latent Semantic Indexing,2016,10,3,1,1,5732,ulrich germann,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,"We apply cross-lingual Latent Semantic Indexing to the Bilingual Document Alignment Task at WMT16. Reduced-rank singular value decomposition of a bilingual term-document matrix derived from known English/French page pairs in the training data allows us to map monolingual documents into a joint semantic space. Two variants of cosine similarity between the vectors that place each document into the joint semantic space are combined with a measure of string similarity between corresponding URLs to produce 1:1 alignments of English/French web pages in a variety of domains. The system achieves a recall of ca. 88% if no in-domain data is used for building the latent semantic model, and 93% if such data is included. Analysing the systemxe2x80x99s errors on the training data, we argue that evaluating aligner performance based on exact URL matches under-estimates their true performance and propose an alternative that is able to account for duplicates and near-duplicates in the underlying data."
W14-0307,The Impact of Machine Translation Quality on Human Post-Editing,2014,21,11,2,0,4417,philipp koehn,Proceedings of the {EACL} 2014 Workshop on Humans and Computer-assisted Translation,0,"We investigate the effect of four different competitive machine translation systems on post-editor productivity and behaviour. The study involves four volunteers postediting automatic translations of news stories from English to German. We see significant difference in productivity due to the systems (about 20%), and even bigger variance between post-editors."
E14-2007,{CASMACAT}: A Computer-assisted Translation Workbench,2014,6,10,6,0,38851,vicent alabau,Proceedings of the Demonstrations at the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"CASMACAT is a modular, web-based translation workbench that offers advanced functionalities for computer-aided translation and the scientific study of human translation: automatic interaction with machine translation (MT) engines and translation memories (TM) to obtain raw translations or close TM matches for conventional post-editing; interactive translation prediction based on an MT enginexe2x80x99s search graph, detailed recording and replay of edit actions and translatorxe2x80x99s gaze (the latter via eye-tracking), and the support of e-pen as an alternative input device. The system is open source sofware and interfaces with multiple MT systems."
C14-2028,The {M}ate{C}at Tool,2014,7,36,17,0,3526,marcello federico,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: System Demonstrations",0,"We present a new web-based CAT tool providing translators with a professional work environment, integrating translation memories, terminology bases, concordancers, and machine translation. The tool is completely developed as open source software and has been already successfully deployed for business, research and education. The MateCat Tool represents today probably the best available open source platform for investigating, integrating, and evaluating under realistic conditions the impact of new machine translation technology on human post-editing."
2014.amta-workshop.3,Dynamic phrase tables for machine translation in an interactive post-editing scenario,2014,13,8,1,1,5732,ulrich germann,Workshop on interactive and adaptive machine translation,0,"This paper presents a phrase table implementation for the Moses system that computes phrase table entries for phrase-based statistical machine translation (PBSMT) on demand by sampling an indexed bitext. While this approach has been used for years in hierarchical phrase-based translation, the PBSMT community has been slow to adopt this paradigm, due to concerns that this would be slow and lead to lower translation quality. The experiments conducted in the course of this work provide evidence to the contrary: without loss in translation quality, the sampling phrase table ranks second out of four in terms of speed, being slightly slower than hash table look-up (Junczys-Dowmunt, 2012) and considerably faster than current implementations of the approach suggested by Zens and Ney (2007). In addition, the underlying parallel corpus can be updated in real time, so that professionally produced translations can be used to improve the quality of the machine translation engine immediately."
W13-2816,Two Approaches to Correcting Homophone Confusions in a Hybrid Machine Translation System,2013,14,1,3,0,2866,pierrette bouillon,Proceedings of the Second Workshop on Hybrid Approaches to Translation,0,"In the context of a hybrid French-to-English SMT system for translating online forum posts, we present two methods for addressing the common problem of homophone confusions in colloquial written language. The first is based on hand-coded rules; the second on weighted graphs derived from a large-scale pronunciation resource, with weights trained from a small bicorpus of domain language. With automatic evaluation, the weighted graph method yields an improvement of about 0.63 BLEU points, while the rulebased method scores about the same as the baseline. On contrastive manual evaluation, both methods give highly significant improvements (p < 0.0001) and score about equally when compared against each other."
W13-2203,The Feasibility of {HMEANT} as a Human {MT} Evaluation Metric,2013,27,11,3,0.198165,5031,alexandra birch,Proceedings of the Eighth Workshop on Statistical Machine Translation,0,"There has been a recent surge of interest in semantic machine translation, which standard automatic metrics struggle to evaluate. A family of measures called MEANT has been proposed which uses semantic role labels (SRL) to overcome this problem. The human variant, HMEANT, has largely been evaluated using correlation with human contrastive evaluations, the standard human evaluation metric for the WMT shared tasks. In this paper we claim that for a human metric to be useful, it needs to be evaluated on intrinsic properties. It needs to be reliable; it needs to work across different language pairs; and it needs to be lightweight. Most importantly, however, a human metric must be discerning. We conclude that HMEANT is a step in the right direction, but has some serious flaws. The reliance on verbs as heads of frames, and the assumption that annotators need minimal guidelines are particularly problematic."
W12-3135,Syntax-aware Phrase-based Statistical Machine Translation: System Description,2012,22,2,1,1,5732,ulrich germann,Proceedings of the Seventh Workshop on Statistical Machine Translation,0,We present a variant of phrase-based SMT that uses source-side parsing and a constituent reordering model based on word alignments in the word-aligned training corpus to predict hierarchical block-wise reordering of the input. Multiple possible translation orders are represented compactly in a source order lattice. This source order lattice is then annotated with phrase-level translations to form a lattice of tokens in the target language. Various feature functions are combined in a log-linear fashion to evaluate paths through that lattice.
W10-1717,Lessons from {NRC}{'}s Portage System at {WMT} 2010,2010,9,6,4,1,5046,samuel larkin,Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and {M}etrics{MATR},0,"NRC's Portage system participated in the English-French (E-F) and French-English (F-E) translation tasks of the ACL WMT 2010 evaluation. The most notable improvement over earlier versions of Portage is an efficient implementation of lattice MERT. While Portage has typically performed well in Chinese to English MT evaluations, most recently in the NIST09 evaluation, our participation in WMT 2010 revealed some interesting differences between Chinese-English and E-F/F-E translation, and alerted us to certain weak spots in our system. Most of this paper discusses the problems we found in our system and ways of fixing them. We learned several lessons that we think will be of general interest."
W09-1505,"Tightly Packed Tries: How to Fit Large Models into Memory, and Make them Load Fast, Too",2009,20,33,1,1,5732,ulrich germann,"Proceedings of the Workshop on Software Engineering, Testing, and Quality Assurance for Natural Language Processing ({SETQA}-{NLP} 2009)",0,"We present Tightly Packed Tries (TPTs), a compact implementation of read-only, compressed trie structures with fast on-demand paging and short load times.n n We demonstrate the benefits of TPTs for storing n-gram back-off language models and phrase tables for statistical machine translation. Encoded as TPTs, these databases require less space than flat text file representations of the same data compressed with the gzip utility. At the same time, they can be mapped into memory quickly and be searched directly in time linear in the length of the key, without the need to decompress the entire file. The overhead for local decompression during search is marginal."
2009.mtsummit-plenaries.15,{P}ortage{L}ive: delivering machine translation technology via virtualization,2009,6,0,3,0,47483,patrick paul,Proceedings of Machine Translation Summit XII: Plenaries,0,None
P08-4006,{Y}awat: {Y}et {A}nother {W}ord {A}lignment {T}ool,2008,9,35,1,1,5732,ulrich germann,Proceedings of the {ACL}-08: {HLT} Demo Session,0,"Yawat is a tool for the visualization and manipulation of word- and phrase-level alignments of parallel text. Unlike most other tools for manual word alignment, it relies on dynamic markup to visualize alignment relations, that is, markup is shown and hidden depending on the current mouse position. This reduces the visual complexity of the visualization and allows the annotator to focus on one item at a time. For a bird's-eye view of alignment patterns within a sentence, the tool is also able to display alignments as alignment matrices. In addition, it allows for manual labeling of alignment relations with customizable tag sets. Different text colors are used to indicate which words in a given sentence pair have already been aligned, and which ones still need to be aligned. Tag sets and color schemes can easily be adapted to the needs of specific annotation projects through configuration files. The tool is implemented in JavaScript and designed to run as a web application."
W07-1520,Two Tools for Creating and Visualizing Sub-sentential Alignments of Parallel Text,2007,4,5,1,1,5732,ulrich germann,Proceedings of the Linguistic Annotation Workshop,0,"We present two web-based, interactive tools for creating and visualizing sub-sentential alignments of parallel text. Yawat is a tool to support distributed, manual word- and phrase-alignment of parallel text through an intuitive, web-based interface. Kwipc is an interface for displaying words or bilingual word pairs in parallel, word-aligned context.n n A key element of the tools presented here is the interactive visualization: alignment information is shown only for one pair of aligned words or phrases at a time. This allows users to explore the alignment space interactively without being overwhelmed by the amount of information available."
N03-1010,Greedy Decoding for Statistical Machine Translation in Almost Linear Time,2003,14,65,1,1,5732,ulrich germann,Proceedings of the 2003 Human Language Technology Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,"We present improvements to a greedy decoding algorithm for statistical machine translation that reduce its time complexity from at least cubic (O(n6) when applied naively) to practically linear time1 without sacrificing translation quality. We achieve this by integrating hypothesis evaluation into hypothesis creation, tiling improvements over the translation hypothesis at the end of each search iteration, and by imposing restrictions on the amount of word reordering during decoding."
W01-1409,Building a Statistical Machine Translation System from Scratch: How Much Bang for the Buck Can We Expect?,2001,3,66,1,1,5732,ulrich germann,Proceedings of the {ACL} 2001 Workshop on Data-Driven Methods in Machine Translation,0,"We report on our experience with building a statistical MT system from scratch, including the creation of a small parallel Tamil-English corpus, and the results of a task-based pilot evaluation of statistical MT systems trained on sets of ca. 1300 and ca. 5000 parallel sentences of Tamil and English data. Our results show that even with apparently incomprehensible system output, humans without any knowledge of Tamil can achieve performance rates as high as 86% accuracy for topic identification, 93% recall for document retrieval, and 64% recall on question answering (plus an additional 14% partially correct answers)."
P01-1030,Fast Decoding and Optimal Decoding for Machine Translation,2001,11,250,1,1,5732,ulrich germann,Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics,1,"A good decoding algorithm is critical to the success of any statistical machine translation system. The decoder's job is to find the translation that is most likely according to set of previously learned parameters (and a formula for combining them). Since the space of possible translations is extremely large, typical decoding algorithms are only able to examine a portion of it, thus risking to miss good solutions. In this paper, we compare the speed and output quality of a traditional stack-based decoding algorithm with two new decoders: a fast greedy decoder and a slow but optimal decoder that treats decoding as an integer-programming optimization problem."
1999.mtsummit-1.81,A deterministic dependency parser for {J}apanese,1999,13,2,1,1,5732,ulrich germann,Proceedings of Machine Translation Summit VII,0,"We present a rule-based, deterministic dependency parser for Japanese. It was implemented in C++, using object classes that reflect linguistic concepts and thus facilitate the transfer of linguistic intuitions into code. The parser first chunks morphemes into one-word phrases and then parses from the right to the left. The average parsing accuracy is 83.6{\%}."
W98-0212,Visualization of Protocols of the Parsing and Semantic Interpretation Steps in a Machine Translation System,1998,3,2,1,1,5732,ulrich germann,Content Visualization and Intermedia Representations ({CVIR}{'}98),0,None
germann-1998-making,Making semantic interpretation parser-independent,1998,16,3,1,1,5732,ulrich germann,Proceedings of the Third Conference of the Association for Machine Translation in the Americas: Technical Papers,0,"We present an approach to semantic interpretation of syntactically parsed Japanese sentences that works largely parser-independent. The approach relies on a standardized parse tree format that restricts the number of syntactic configurations that the semantic interpretation rules have to anticipate. All parse trees are converted to this format prior to semantic interpretation. This setup allows us not only to apply the same set of semantic interpretation rules to output from different parsers, but also to independently develop parsers and semantic interpretation rules."
