C16-2027,Opinion Retrieval Systems using Tweet-external Factors,2016,0,0,3,0,35648,yoonsung kim,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: System Demonstrations",0,"Opinion mining is a natural language processing technique which extracts subjective information from natural language text. To estimate an opinion about a query in large data collection, an opinion retrieval system that retrieves subjective and relevant information about the query can be useful. We present an opinion retrieval system that retrieves subjective and query-relevant tweets from Twitter, which is a useful source of obtaining real-time opinions. Our system outperforms previous opinion retrieval systems, and it further provides subjective information about Twitter authors and hashtags to describe their subjective tendencies."
D14-1071,Joint Relational Embeddings for Knowledge-based Question Answering,2014,18,58,4,0,40111,minchul yang,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"Transforming a natural language (NL) question into a corresponding logical form (LF) is central to the knowledge-based question answering (KB-QA) task. Unlike most previous methods that achieve this goal based on mappings between lexicalized phrases and logical predicates, this paper goes one step further and proposes a novel embedding-based approach that maps NL-questions into LFs for KBQA by leveraging semantic associations between lexical representations and KBproperties in the latent space. Experimental results demonstrate that our proposed method outperforms three KB-QA baseline methods on two publicly released QA data sets."
W13-3617,{KUNLP} Grammatical Error Correction System For {C}o{NLL}-2013 Shared Task,2013,3,3,3,0,40850,bongjun yi,Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task,0,"This paper describes an English grammatical error correction system for CoNLL2013 shared task. Error types covered by our system are article/determiner, preposition, and noun number agreement. This work is our first attempt on grammatical error correction research. In this work, we only focus on reimplementing the techniques presented before and optimizing the performance. As a result of the implementation, our systemxe2x80x99s final F1-score by m2 scorer is 0.1282 in our internal test set."
W12-4103,Using Link Analysis to Discover Interesting Messages Spread Across {T}witter,2012,9,2,3,0,40111,minchul yang,Workshop Proceedings of {T}ext{G}raphs-7: Graph-based Methods for Natural Language Processing,0,"Twitter, a popular social networking service, enables its users to not only send messages but re-broadcast or retweet a message from another Twitter user to their own followers. Considering the number of times that a message is retweeted across Twitter is a straightforward way to estimate how interesting it is. However, a considerable number of messages in Twitter with high retweet counts are actually mundane posts by celebrities that are of interest to themselves and possibly their followers. In this paper, we leverage retweets as implicit relationships between Twitter users and messages and address the problem of automatically finding messages in Twitter that may be of potential interest to a wide audience by using link analysis methods that look at more than just the sheer number of retweets. Experimental results on real world data demonstrate that the proposed method can achieve better performance than several baseline methods."
W12-2029,{K}orea {U}niversity System in the {HOO} 2012 Shared Task,2012,9,2,3,0,42389,jieun lee,Proceedings of the Seventh Workshop on Building Educational Applications Using {NLP},0,"In this paper, we describe the Korea University system that participated in the HOO 2012 Shared Task on the correction of preposition and determiner errors in non-native speaker texts. We focus our work on training the system on a large collection of error-tagged texts provided by the HOO 2012 Shared Task organizers and incrementally applying several methods to achieve better performance."
P12-2057,Translation Model Size Reduction for Hierarchical Phrase-based Statistical Machine Translation,2012,14,9,5,0,10589,seungwook lee,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"In this paper, we propose a novel method of reducing the size of translation model for hierarchical phrase-based machine translation systems. Previous approaches try to prune infrequent entries or unreliable entries based on statistics, but cause a problem of reducing the translation coverage. On the contrary, the proposed method try to prune only ineffective entries based on the estimation of the information redundancy encoded in phrase pairs and hierarchical rules, and thus preserve the search space of SMT decoders as much as possible. Experimental results on Chinese-to-English machine translation tasks show that our method is able to reduce almost the half size of the translation model with very tiny degradation of translation performance."
C12-3038,Open Information Extraction for {SOV} Language Based on Entity-Predicate Pair Detection,2012,6,0,5,0,43666,woongki lee,Proceedings of {COLING} 2012: Demonstration Papers,0,"Open IE usually has been studied for English which of one of subject-verb-object(SVO) languages where a relation between two entities tends to occur in order of entity-relational phrase-entity within a sentence. However, in SOV languages, two entities occur before the relational phrase so that the subject and the relation have a long distance. The conventional methods for Open IE mostly dealing with SVO languages have difficulties of extracting relations from SOV style sentences. In this paper, we propose a new method of extracting relations from SOV languages. Our approach tries to solve long distance problems by identifying an entity-predicate pair and recognizing a relation within a predicate. Furthermore, we propose a post-processing approach using a language model, so that the system can detect more fluent and precise relations. Experimental results on Korean corpus show that the proposed approach is effective in improving the performance of relation extraction."
C12-2060,Decoder-based Discriminative Training of Phrase Segmentation for Statistical Machine Translation,2012,17,0,2,1,35656,hyounggyu lee,Proceedings of {COLING} 2012: Posters,0,"In this paper, we propose a new method of training phrase segmentation model for phrasebased statistical machine translation(SMT). We define a good segmentation as the segmentation producing a good translation. According to this definition, we propose a method that can discriminate between a good segmentation and a bad segmentation based on the translation quality. The proposed approach constructs the phrase labeled data by using the SMT decoder, so that the phrase segmentations supporting good translations can be acquired. Furthermore, our iterative training algorithm of the segmentation model can gradually improve the performance of the SMT decoder. Experimental results show that the proposed method is effective in improving the translation quality of the phrase-based SMT system. TITLE AND ABSTRACT IN ANOTHER LANGUAGE (KOREAN)"
2011.mtsummit-papers.21,Phrase Segmentation Model using Collocation and Translational Entropy,2011,-1,-1,4,1,35656,hyounggyu lee,Proceedings of Machine Translation Summit XIII: Papers,0,None
C10-2071,A Post-processing Approach to Statistical Word Alignment Reflecting Alignment Tendency between Part-of-speeches,2010,7,3,6,0,46460,jaehee lee,Coling 2010: Posters,0,"Statistical word alignment often suffers from data sparseness. Part-of-speeches are often incorporated in NLP tasks to reduce data sparseness. In this paper, we attempt to mitigate such problem by reflecting alignment tendency between part-of-speeches to statistical word alignment. Because our approach does not rely on any language-dependent knowledge, it is very simple and purely statistic to be applied to any language pairs. End-to-end evaluation shows that the proposed method can improve not only the quality of statistical word alignment but the performance of statistical machine translation."
C10-1054,An Empirical Study on Web Mining of Parallel Data,2010,21,18,4,1,46461,gumwon hong,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),0,"This paper presents an empirical approach to mining parallel corpora. Conventional approaches use a readily available collection of comparable, non-parallel corpora to extract parallel sentences. This paper attempts the much more challenging task of directly searching for high-quality sentence pairs from the Web. We tackle the problem by formulating good search query using 'Learning to Rank' and by filtering noisy document pairs using IBM Model 1 alignment. End-to-end evaluation shows that the proposed approach significantly improves the performance of statistical machine translation."
W09-3524,A Hybrid Approach to {E}nglish-{K}orean Name Transliteration,2009,5,15,4,1,46461,gumwon hong,Proceedings of the 2009 Named Entities Workshop: Shared Task on Transliteration ({NEWS} 2009),0,"This paper presents a hybrid approach to English-Korean name transliteration. The base system is built on MOSES with enabled factored translation features. We expand the base system by combining with various transliteration methods including a Web-based n-best re-ranking, a dictionary-based method, and a rule-based method. Our standard run and best non-standard run achieve 45.1 and 78.5, respectively, in top-1 accuracy. Experimental results show that expanding training data size significantly contributes to the performance. Also we discover that the Web-based re-ranking method can be successfully applied to the English-Korean transliteration."
W09-1415,A Multi-Phase Approach to Biomedical Event Extraction,2009,5,5,6,1,35656,hyounggyu lee,Proceedings of the {B}io{NLP} 2009 Workshop Companion Volume for Shared Task,0,"In this paper, we propose a system for biomedical event extraction using multi-phase approach. It consists of event trigger detector, event type classifier, and relation recognizer and event compositor. The system firstly identifies triggers in a given sentence. Then, it classifies the triggers into one of nine predefined classes. Lastly, the system examines each trigger whether it has a relation with participant candidates, and composites events with the extracted relations. The official score of the proposed system recorded 61.65 precision, 9.40 recall and 16.31 f-score in approximate span matching. However, we found that the threshold tuning for the third phase had negative effect. Without the threshold tuning, the system showed 55.32 precision, 16.18 recall and 25.04 f-score."
P09-2008,A Novel Word Segmentation Approach for Written Languages with Word Boundary Markers,2009,3,0,6,1,44432,hancheol cho,Proceedings of the {ACL}-{IJCNLP} 2009 Conference Short Papers,0,"Most NLP applications work under the assumption that a user input is error-free; thus, word segmentation (WS) for written languages that use word boundary markers (WBMs), such as spaces, has been regarded as a trivial issue. However, noisy real-world texts, such as blogs, e-mails, and SMS, may contain spacing errors that require correction before further processing may take place. For the Korean language, many researchers have adopted a traditional WS approach, which eliminates all spaces in the user input and re-inserts proper word boundaries. Unfortunately, such an approach often exacerbates the word spacing quality for user input, which has few or no spacing errors; such is the case, because a perfect WS model does not exist. In this paper, we propose a novel WS method that takes into consideration the initial word spacing information of the user input. Our method generates a better output than the original user input, even if the user input has few spacing errors. Moreover, the proposed method significantly outperforms a state-of-the-art Korean WS model when the user input initially contains less than 10% spacing errors, and performs comparably for cases containing more spacing errors. We believe that the proposed method will be a very practical pre-processing module."
P09-2059,Bridging Morpho-Syntactic Gap between Source and Target Sentences for {E}nglish-{K}orean Statistical Machine Translation,2009,9,17,3,1,46461,gumwon hong,Proceedings of the {ACL}-{IJCNLP} 2009 Conference Short Papers,0,"Often, Statistical Machine Translation (SMT) between English and Korean suffers from null alignment. Previous studies have attempted to resolve this problem by removing unnecessary function words, or by reordering source sentences. However, the removal of function words can cause a serious loss in information. In this paper, we present a possible method of bridging the morpho-syntactic gap for English-Korean SMT. In particular, the proposed method tries to transform a source sentence by inserting pseudo words, and by reordering the sentence in such a way that both sentences have a similar length and word order. The proposed method achieves 2.4 increase in BLEU score over baseline phrase-based system."
P09-2081,The Contribution of Stylistic Information to Content-based Mobile Spam Filtering,2009,14,28,3,0,47200,daeneung sohn,Proceedings of the {ACL}-{IJCNLP} 2009 Conference Short Papers,0,"Content-based approaches to detecting mobile spam to date have focused mainly on analyzing the topical aspect of a SMS message (what it is about) but not on the stylistic aspect (how it is written). In this paper, as a preliminary step, we investigate the utility of commonly used stylistic features based on shallow linguistic analysis for learning mobile spam filters. Experimental results show that the use of stylistic information is potentially effective for enhancing the performance of the mobile spam filters."
P09-1118,Word or Phrase? Learning Which Unit to Stress for Information Retrieval,2009,26,3,3,1,6566,youngin song,Proceedings of the Joint Conference of the 47th Annual Meeting of the {ACL} and the 4th International Joint Conference on Natural Language Processing of the {AFNLP},1,"The use of phrases in retrieval models has been proven to be helpful in the literature, but no particular research addresses the problem of discriminating phrases that are likely to degrade the retrieval performance from the ones that do not. In this paper, we present a retrieval framework that utilizes both words and phrases flexibly, followed by a general learning-to-rank method for learning the potential contribution of a phrase in retrieval. We also present useful features that reflect the compositionality and discriminative power of a phrase and its constituent words for optimizing the weights of phrase use in phrase-based retrieval models. Experimental results on the TREC collections show that our proposed method is effective."
W08-2133,Semantic Dependency Parsing using N-best Semantic Role Sequences and Roleset Information,2008,9,0,3,0,44899,jooyoung lee,{C}o{NLL} 2008: Proceedings of the Twelfth Conference on Computational Natural Language Learning,0,"In this paper, we describe a syntactic and semantic dependency parsing system submitted to the shared task of CoNLL 2008. The proposed system consists of five modules: syntactic dependency parser, predicate identifier, local semantic role labeler, global role sequence candidate generator, and role sequence selector. The syntactic dependency parser is based on Malt Parser and the sequence candidate generator is based on CKY style algorithm. The remaining three modules are implemented by using maximum entropy classifiers. The proposed system achieves 76.90 of labeled F1 for the overall task, 84.82 of labeled attachment, and 68.71 of labeled F1 on the WSJBrown test set."
D08-1043,Bridging Lexical Gaps between Queries and Questions on Large Online {Q}{\\&}{A} Collections with Compact Translation Models,2008,11,47,4,1,42175,jungtae lee,Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,0,"Lexical gaps between queries and questions (documents) have been a major issue in question retrieval on large online question and answer (Q&A) collections. Previous studies address the issue by implicitly expanding queries with the help of translation models pre-constructed using statistical techniques. However, since it is possible for unimportant words (e.g., non-topical words, common words) to be included in the translation models, a lack of noise control on the models can cause degradation of retrieval performance. This paper investigates a number of empirical methods for eliminating unimportant words in order to construct compact translation models for retrieval purposes. Experiments conducted on a real world Q&A collection show that substantial improvements in retrieval performance can be achieved by using compact translation models."
P06-4008,{K}-{QARD}: A Practical {K}orean Question Answering Framework for Restricted Domain,2006,4,0,5,1,6566,youngin song,Proceedings of the {COLING}/{ACL} 2006 Interactive Presentation Sessions,0,"We present a Korean question answering framework for restricted domains, called K-QARD. K-QARD is developed to achieve domain portability and robustness, and the framework is successfully applied to build question answering systems for several domains."
W05-0632,Maximum Entropy Based Semantic Role Labeling,2005,5,12,2,1,50824,kyungmi park,Proceedings of the Ninth Conference on Computational Natural Language Learning ({C}o{NLL}-2005),0,"The semantic role labeling (SRL) refers to finding the semantic relation (e.g. Agent, Patient, etc.) between a predicate and syntactic constituents in the sentences. Especially, with the argument information of the predicate, we can derive the predicate-argument structures, which are useful for the applications such as automatic information extraction. As previous work on the SRL, there have been many machine learning approaches. (Gildea and Jurafsky, 2002; Pradhan et al., 2003; Lim et al., 2004)."
I05-2034,Probabilistic Models for {K}orean Morphological Analysis,2005,2,7,2,1,46874,dogil lee,Companion Volume to the Proceedings of Conference including Posters/Demos and tutorial abstracts,0,"This paper discusses Korean morphological analysis and presents three probabilistic models for morphological analysis. Each model exploits a distinct linguistic unit as a processing unit. The three models can compensate for each otherxe2x80x99s weaknesses. Contrary to the previous systems that depend on manually constructed linguistic knowledge, the proposed system can fully automatically acquire the linguistic knowledge from annotated corpora (e.g. part-ofspeech tagged corpora). Besides, without any modification of the system, it can be applied to other corpora having different tagsets and annotation guidelines. We describe the models and present evaluation results on three corpora with a wide range of conditions."
I05-2041,Tree Annotation Tool using Two-phase Parsing to Reduce Manual Effort for Building a Treebank,2005,0,1,5,1,6343,soyoung park,Companion Volume to the Proceedings of Conference including Posters/Demos and tutorial abstracts,0,None
I05-1057,Two-Phase Biomedical Named Entity Recognition Using A Hybrid Method,2005,13,21,4,1,51070,seonho kim,Second International Joint Conference on Natural Language Processing: Full Papers,0,"Biomedical named entity recognition (NER) is a difficult problem in biomedical information processing due to the widespread ambiguity of terms out of context and extensive lexical variations. This paper presents a two-phase biomedical NER consisting of term boundary detection and semantic labeling. By dividing the problem, we can adopt an effective model for each process. In our study, we use two exponential models, conditional random fields and maximum entropy, at each phase. Moreover, results by this machine learning based model are refined by rule-based postprocessing implemented using a finite state method. Experiments show it achieves the performance of F-score 71.19% on the JNLPBA 2004 shared task of identifying 5 classes of biomedical NEs."
I05-1080,Word Sense Disambiguation by Relative Selection,2005,20,2,2,1,8788,heecheol seo,Second International Joint Conference on Natural Language Processing: Full Papers,0,"This paper describes a novel method for a word sense disambiguation that utilizes relatives (i.e. synonyms, hypernyms, meronyms, etc in WordNet) of a target word and raw corpora. The method disambiguates senses of a target word by selecting a relative that most probably occurs in a new sentence including the target word. Only one co-occurrence frequency matrix is utilized to efficiently disambiguate senses of many target words. Experiments on several English datum present that our proposed method achieves a good performance."
W04-2419,Semantic Role Labeling using Maximum Entropy Model,2004,6,25,4,0,19725,joonho lim,Proceedings of the Eighth Conference on Computational Natural Language Learning ({C}o{NLL}-2004) at {HLT}-{NAACL} 2004,0,"In this paper, we propose a semantic role labeling method using a maximum entropy model, which enables not only to exploit rich features but also to alleviate the data sparseness problem in a well-founded model. For applying the maximum entropy model to semantic role labeling, we take a incremental approach as follows: firstly, the semantic roles are assigned to the arguments in the immediate clause including a predicate, and then, the semantic roles are assigned to the arguments in the upper clauses by using previously assigned labels. The experimental result shows that the proposed method has about 64.76% (F1-measure) on the test set."
W04-2420,Two-Phase Semantic Role Labeling based on Support Vector Machines,2004,3,10,3,1,50824,kyungmi park,Proceedings of the Eighth Conference on Computational Natural Language Learning ({C}o{NLL}-2004) at {HLT}-{NAACL} 2004,0,None
W04-1214,Incorporating Lexical Knowledge into Biomedical {NE} Recognition,2004,2,14,5,1,50824,kyungmi park,Proceedings of the International Joint Workshop on Natural Language Processing in Biomedicine and its Applications ({NLPBA}/{B}io{NLP}),0,"In this paper, we propose a two-phase biomedical named entity(NE) recognition method based on SVMs. We first recognize biomedical terms, and then assign appropriate semantic classes to the recognized terms. In the two-phase NE recognition method, the performance of term recognition is critical to the overall performance of the system because term recognition errors can be propagated to the semantic classification phase. In this study, we try to improve the performance of term recognition by using lexical knowledge. We utilize salient NPs and collocations as lexical knowledge extracted from raw corpus. In addition, we use morphological knowledge extracted from training data as features for learning SVM classifiers. Experimental results show that our system obtains an F-measure of 62.97% on the test data, and that the performance can be improved upto 2.82% by using lexical knowledge."
W04-0854,{KUNLP} system in Senseval-3,2004,2,5,2,1,8788,heecheol seo,"Proceedings of {SENSEVAL}-3, the Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text",0,"We have participated in both English all words task and English lexical sample task of SENSEVAL3. Our system disambiguates senses of a target word in a context by selecting a substituent among WordNet relatives of the target word, such as synonyms, hypernyms, meronyms and so on. The decision is made based on co-occurrence frequency between candidate relatives and each of the context words. Since the co-occurrence frequency is obtainable from raw corpus, our method is considered to be an unsupervised learning algorithm that does not require a sense-tagged corpus."
W04-0507,A Practical {QA} System in Restricted Domains,2004,10,38,6,0,49891,hoojung chung,Proceedings of the Conference on Question Answering in Restricted Domains,0,None
P04-3010,Part-of-Speech Tagging Considering Surface Form for an Agglutinative Language,2004,1,4,2,1,46874,dogil lee,Proceedings of the {ACL} Interactive Poster and Demonstration Sessions,0,"The previous probabilistic part-of-speech tagging models for agglutinative languages have considered only lexical forms of morphemes, not surface forms of words. This causes an inaccurate calculation of the probability. The proposed model is based on the observation that when there exist words (surface forms) that share the same lexical forms, the probabilities to appear are different from each other. Also, it is designed to consider lexical form of word. By experiments, we show that the proposed model outperforms the bigram Hidden Markov model (HMM)-based tagging model."
W03-3013,{GLR} Parser with Conditional Action Model using Surface Phrasal Types for {K}orean,2003,5,1,3,0,52565,yongjae kwak,Proceedings of the Eighth International Conference on Parsing Technologies,0,"In this paper, we propose a new probabilistic GLR parsing method that can solve the problems of conventional methods. Our proposed Conditional Action Model uses Surface Phrasal Types (SPTs) encoding the functional word sequences of the sub-trees for describing structural characteristics of the partial parse. And, the proposed GLR model outperforms the previous methods by about 6{\textasciitilde}8{\%}."
W03-3022,Automatic Acquistion of Constraints for Efficient {K}orean Parsing,2003,0,0,5,1,6343,soyoung park,Proceedings of the Eighth International Conference on Parsing Technologies,0,None
W03-1305,Two-Phase Biomedical {NE} Recognition based on {SVM}s,2003,12,84,3,0,51592,kijoong lee,Proceedings of the {ACL} 2003 Workshop on Natural Language Processing in Biomedicine,0,"Using SVMs for named entity recognition, we are often confronted with the multi-class problem. Larger as the number of classes is, more severe the multi-class problem is. Especially, one-vs-rest method is apt to drop the performance by generating severe unbalanced class distribution. In this study, to tackle the problem, we take a two-phase named entity recognition method based on SVMs and dictionary; at the first phase, we try to identify each entity by a SVM classifier and post-process the identified entities by a simple dictionary look-up; at the second phase, we try to classify the semantic class of the identified entity by SVMs. By dividing the task into two subtasks, i.e. the entity identification and the semantic classification, the unbalanced class distribution problem can be alleviated. Furthermore, we can select the features relevant to each task and take an alternative classification method according to the task. The experimental results on the GENIA corpus show that the proposed method is effective not only in the reduction of training cost but also in performance improvement: the identification performance is about 79.9(Fxcexb2 = 1), the semantic classification accuracy is about 66.5(Fxcexb2 = 1)."
W03-1105,{P}oisson Naive {B}ayes for Text Classification with Feature Weighting,2003,13,23,3,0,27977,sangbum kim,Proceedings of the Sixth International Workshop on Information Retrieval with {A}sian Languages,0,"In this paper, we investigate the use of multivariate Poisson model and feature weighting to learn naive Bayes text classifier. Our new naive Bayes text classification model assumes that a document is generated by a multivariate Poisson model while the previous works consider a document as a vector of binary term features based on the presence or absence of each term. We also explore the use of feature weighting for the naive Bayes text classification rather than feature selection, which is a quite costly process when a small number of the new training documents are continuously provided.Experimental results on the two test collections indicate that our new model with the proposed parameter estimation and the feature weighting technique leads to substantial improvements compared to the unigram language model classifiers that are known to outperform the original pure naive Bayes text classifiers."
P03-1038,Self-Organizing {M}arkov Models and Their Application to Part-of-Speech Tagging,2003,12,7,2,1,15849,jindong kim,Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,1,This paper presents a method to develop a class of variable memory Markov models that have higher memory capacity than traditional (uniform memory) Markov models. The structure of the variable memory models is induced from a manually annotated corpus through a decision tree learning algorithm. A series of comparative experiments show the resulting models outperform uniform memory Markov models in a part-of-speech tagging task.
P03-1060,A Syllable Based Word Recognition Model for {K}orean Noun Extraction,2003,4,6,2,1,46874,dogil lee,Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,1,"Noun extraction is very important for many NLP applications such as information retrieval, automatic text classification, and information extraction. Most of the previous Korean noun extraction systems use a morphological analyzer or a Part-of-Speech (POS) tagger. Therefore, they require much of the linguistic knowledge such as morpheme dictionaries and rules (e.g. morphosyntactic rules and morphological rules).This paper proposes a new noun extraction method that uses the syllable based word recognition model. It finds the most probable syllable-tag sequence of the input sentence by using automatically acquired statistical information from the POS tagged corpus and extracts nouns by detecting word boundaries. Furthermore, it does not require any labor for constructing and maintaining linguistic knowledge. We have performed various experiments with a wide range of variables influencing the performance. The experimental results show that without morphological analysis or POS tagging, the proposed method achieves comparable performance with the previous methods."
W02-1208,Automatic Word Spacing Using Hidden {M}arkov Model for Refining {K}orean Text Corpora,2002,5,11,3,1,46874,dogil lee,{COLING}-02: The 3rd Workshop on {A}sian Language Resources and International Standardization,0,"This paper proposes a word spacing model using a hidden Markov model (HMM) for refining Korean raw text corpora. Previous statistical approaches for automatic word spacing have used models that make use of inaccurate probabilities because they do not consider the previous spacing state. We consider word spacing problem as a classification problem such as Part-of-Speech (POS) tagging and have experimented with various models considering extended context. Experimental result shows that the performance of the model becomes better as the more context considered. In case of the same number of parameters are used with other method, it is proved that our model is more effective by showing the better results."
S01-1036,{KUNLP} system using Classification Information Model at {SENSEVAL}-2,2001,4,4,3,1,8788,heecheol seo,Proceedings of {SENSEVAL}-2 Second International Workshop on Evaluating Word Sense Disambiguation Systems,0,"The classification information model or CIM classifies instances by considering the discrimination ability of their features, which was proven to be useful for word sense disambiguation at Senseval-1. But the CIM has a problem of information loss. KUNLP system at Senseval-2 uses a modified version of the CIM for word sense disambiguation.n n We used three types of features for word sense disambiguation: local, topical, and bigram context. Local and topical context are similar to Chodorow's context and refer to only unigram information. The window of a bigram context is similar to that of a local context but a bigram context refers to only bigram information.n n We participated in the English lexical sample task and the Korean lexical sample task, where our systems ranked high."
P00-1034,Part-of-Speech Tagging Based on Hidden {M}arkov Model Assuming Joint Independence,2000,13,10,3,1,53188,sangzoo lee,Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,1,"In this paper we present part-of-speech taggers based on hidden Markov models, which adopt a less strict Markov assumption to consider rich contexts. In models whose parameters are very specific like lexicalized ones, sparse-data problem is very serious and also conditional probabilities tend to be estimated unreliably. To overcome data-sparseness, a simplified version of the well-known back-off smoothing method is used. To mitigate unreliable estimation problem, our models assume joint independence instead of conditional independence because joint probabilities have the same degree of estimation reliability. In experiments for the Brown corpus, models with rich contexts achieve relatively high accuracy and some models assuming joint independence show better results than the corresponding HMMs."
P00-1048,"Hidden {M}arkov Model-Based {K}orean Part-of-Speech Tagging Considering High Agglutinativity, Word-Spacing, and Lexical Correlativity",2000,7,11,3,1,53188,sangzoo lee,Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,1,"In this paper we present hidden Markov models for Korean part-of-speech tagging, which consider Korean characteristics such as high agglutinativity, word-spacing, and high lexical correlativity. In order ot consider rich information in contexts, the models adopt a less strict Markov assumption. In the models, sparse-data problem is very serious and their parameters tend to be estimated unreliably because they have a large number of parameters. To overcome sparse-data problem, our model uses a simplified version of the well-known back-off smoothing method. To mitigate unreliable estimation problem, our models assume joint independence instead of conditional independence because joint probabilities have the same degree of estimation reliability. Experimental results show that models with rich contexts perform even better than standard HMMs and that joint independent assumption is effective in some models."
C00-2165,{KCAT}: A {K}orean Corpus Annotating Tool Minimizing Human Intervention,2000,1,1,3,0,54661,wonhe ryu,{COLING} 2000 Volume 2: The 18th International Conference on Computational Linguistics,0,"While large POS (part-of-speech) annotated corpora play an important role in natural language processing, the annotated corpus requires very high accuracy and consistency. To build such an accurate and consistent corpus, we often use a manual tagging method. But the manual tagging is very labor intensive and expensive. Furthermore, it is not easy to get consistent results from the human experts. In this paper, we present an efficient tool for building large accurate and consistent corpora with minimal human labor. The proposed tool supports semiautomatic tagging. Using disambiguation rules acquired from human experts. it minimizes the human intervention in both the manual tagging and post-editing steps."
C00-1070,Lexicalized Hidden {M}arkov Models for Part-of-Speech Tagging,2000,11,28,3,1,53188,sangzoo lee,{COLING} 2000 Volume 1: The 18th International Conference on Computational Linguistics,0,"Since most previous works for HMM-based tagging consider only part-of-speech information in contexts, their models cannot utilize lexical information which is crucial for resolving some morphological ambiguity. In this paper we introduce uniformly lexicalized HMMs for part-of-speech tagging in both English and Korean. The lexicalized models use a simplified back-off smoothing technique to overcome data sparseness. In experiments, lexicalized models achieve higher accuracy than non-lexicalized models and the back-off smoothing method mitigates data sparseness better than simple smoothing methods."
W99-0615,{HMM} Specialization with Selective Lexicalization,1999,6,8,3,1,15849,jindong kim,1999 Joint {SIGDAT} Conference on Empirical Methods in Natural Language Processing and Very Large Corpora,0,None
O97-1004,Word Sense Disambiguation Based on The Information Theory,1997,0,6,3,0,53856,ho lee,Proceedings of the 10th Research on Computational Linguistics International Conference,0,None
O97-1020,Recognizing {K}orean Unknown Proper Nouns by Using Automatically Extracted Lexical Clues,1997,0,0,3,0,55674,bongrae park,Proceedings of the 10th Research on Computational Linguistics International Conference,0,None
P90-1007,Transforming Syntactic Graphs Into Semantic Graphs,1990,9,3,1,1,35649,haechang rim,28th Annual Meeting of the Association for Computational Linguistics,1,"In this paper, we present a computational method for transforming a syntactic graph, which represents all syntactic interpretations of a sentence, into a semantic graph which filters out certain interpretations, but also incorporates any remaining ambiguities. We argue that the resulting ambiguous graph, supported by an exclusion matrix, is a useful data structure for question answering and other semantic processing. Our research is based on the principle that ambiguity is an inherent aspect of natural language communication."
