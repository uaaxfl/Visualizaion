2020.aacl-main.52,W13-2322,0,0.0127586,"The tags are transformed to a “highlight-on” embedding for each token if it is chosen by the content selector, and a “highlight-off ” embedding for each token not chosen. The highlight-on/off embeddings are added to token embeddings in an element-wise manner; both highlight and token embeddings are learned. An illustration is shown in Figure 1. Highlights provide a valuable intermediate representation suitable for shallow abstraction. Our approach thus provides an alternative to methods that use more sophisticated representations such as syntactic/semantic graphs (Filippova and Strube, 2008; Banarescu et al., 2013; Liu et al., 2015). It is more straightforward to incorporate highlights into an encoder-decoder fusion model, and obtaining highlights through sequence tagging can be potentially adapted to new domains. 3 Experimental Results Data and Annotation To enable direct comparison with end-to-end systems, we conduct experiments on the widely used CNN/DM dataset (See et al., 2017) to report results of our cascade approach. We use the procedure described in Lebanoff et al. (2019b) to create training instances for the sentence selector and fine-grained content selector. Our training data contains 1,053"
2020.aacl-main.52,N19-1348,0,0.0283632,"ate the feasibility of a cascade approach to neural text summarization. We explore a constrained summarization task, where an abstract is created one sentence at a time through a cascaded pipeline. Our pipeline architecture chooses one or two sentences from the source document, then highlights their summary-worthy segments and uses those as a basis for composing a summary sentence. When a pair of sentences are selected, it is important to ensure that they are fusible—there exists cohesive devices that tie the two sentences together into a coherent text—to avoid generating nonsensical outputs (Geva et al., 2019; Lebanoff et al., 2020). Highlighting sentence segments allows us to perform fine-grained content selection that guides the neural text generator to stitch selected segments into a coherent sentence. The contributions of this work are summarized as follows. 529 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 529–535 c December 4 - 7, 2020. 2020 Association for Computational Linguistics Sentence Selection Sent Pred Fine-Grained Content Selection NonHi"
2020.aacl-main.52,N18-1150,0,0.0202951,"t is comparable to or outranks that of end-to-end systems, whereas a pipeline architecture allows for flexible content selection. We finally discuss how we can take advantage of a cascaded pipeline in neural text summarization and shed light on important directions for future research. 1 We advocate for explicit content selection as it allows for a rigorous evaluation and visualization of intermediate results of such a module, rather than associating it with text generation. Existing neural abstractive systems can perform content selection implicitly using end-to-end models (See et al., 2017; Celikyilmaz et al., 2018; Raffel et al., 2019; Lewis et al., 2020), or more explicitly, with an external module to select important sentences or words to aid generation (Tan et al., 2017; Gehrmann et al., 2018; Chen and Bansal, 2018; Kry´sci´nski et al., 2018; Hsu et al., 2018; Lebanoff et al., 2018, 2019b; Liu and Lapata, 2019). However, content selection concerns not only the selection of important segments from a document, but also the cohesiveness of selected segments and the amount of text to be selected in order for a neural text generator to produce a summary. Introduction There is a variety of successful summ"
2020.aacl-main.52,N19-1355,0,0.0195614,"nces are selected to generate a fusion sentence, it is desirable to identify segments of text from these sentences that are potentially compatible with each other. The coarse-tofine method allows us to examine the intermediate results and compare them with ground-truth. Concretely, we add a classification layer to the final layer representation hL i for each token wi (Eq. (2)). The per-target-word loss is then interpolated with instance prediction (one or two sentences) loss using a coefficient λ. Such a multi-task learning objective has been shown to improve performance on a number of tasks (Guo et al., 2019). phighlight (wi ) = σ(v> hL i ) (2) where v is a vector of weights and σ is the sigmoid function. The model predicts phighlight for each token – whether the token should be included in the output fusion, calculated based on the given token’s representation. Information Fusion Given one or two sentences taken from a document and their fine-grained highlights, we proceed by describing a fusion process that generates a summary sentence from the selected content. Our model employs an encoderdecoder architecture based on pointer-generator 531 networks that has shown strong performance on its own a"
2020.aacl-main.52,P18-1013,0,0.0165201,"ions for future research. 1 We advocate for explicit content selection as it allows for a rigorous evaluation and visualization of intermediate results of such a module, rather than associating it with text generation. Existing neural abstractive systems can perform content selection implicitly using end-to-end models (See et al., 2017; Celikyilmaz et al., 2018; Raffel et al., 2019; Lewis et al., 2020), or more explicitly, with an external module to select important sentences or words to aid generation (Tan et al., 2017; Gehrmann et al., 2018; Chen and Bansal, 2018; Kry´sci´nski et al., 2018; Hsu et al., 2018; Lebanoff et al., 2018, 2019b; Liu and Lapata, 2019). However, content selection concerns not only the selection of important segments from a document, but also the cohesiveness of selected segments and the amount of text to be selected in order for a neural text generator to produce a summary. Introduction There is a variety of successful summarization applications but few can afford to have a large number of annotated examples that are sufficient to meet the requirement of end-to-end neural abstractive summarization. Examples range from summarizing radiology reports (Jing et al., 2019; Zhan"
2020.aacl-main.52,P19-1657,0,0.0201954,"2018; Hsu et al., 2018; Lebanoff et al., 2018, 2019b; Liu and Lapata, 2019). However, content selection concerns not only the selection of important segments from a document, but also the cohesiveness of selected segments and the amount of text to be selected in order for a neural text generator to produce a summary. Introduction There is a variety of successful summarization applications but few can afford to have a large number of annotated examples that are sufficient to meet the requirement of end-to-end neural abstractive summarization. Examples range from summarizing radiology reports (Jing et al., 2019; Zhang et al., 2020) to congressional bills (Kornilova and Eidelman, 2019) and meeting conversations (Mehdad et al., 2013; Li et al., 2019; Koay et al., 2020). The lack of annotated resources suggests that end-toend systems may not be a “one-size-fits-all” solution to neural text summarization. There is an increasing need to develop cascaded architectures to allow for customized content selectors to be combined with general-purpose neural text generators In this paper, we aim to investigate the feasibility of a cascade approach to neural text summarization. We explore a constrained summarizat"
2020.aacl-main.52,A00-2024,0,0.428793,"Missing"
2020.aacl-main.52,2020.coling-main.499,1,0.701122,"from a document, but also the cohesiveness of selected segments and the amount of text to be selected in order for a neural text generator to produce a summary. Introduction There is a variety of successful summarization applications but few can afford to have a large number of annotated examples that are sufficient to meet the requirement of end-to-end neural abstractive summarization. Examples range from summarizing radiology reports (Jing et al., 2019; Zhang et al., 2020) to congressional bills (Kornilova and Eidelman, 2019) and meeting conversations (Mehdad et al., 2013; Li et al., 2019; Koay et al., 2020). The lack of annotated resources suggests that end-toend systems may not be a “one-size-fits-all” solution to neural text summarization. There is an increasing need to develop cascaded architectures to allow for customized content selectors to be combined with general-purpose neural text generators In this paper, we aim to investigate the feasibility of a cascade approach to neural text summarization. We explore a constrained summarization task, where an abstract is created one sentence at a time through a cascaded pipeline. Our pipeline architecture chooses one or two sentences from the sour"
2020.aacl-main.52,D19-5406,0,0.0238845,"Lapata, 2019). However, content selection concerns not only the selection of important segments from a document, but also the cohesiveness of selected segments and the amount of text to be selected in order for a neural text generator to produce a summary. Introduction There is a variety of successful summarization applications but few can afford to have a large number of annotated examples that are sufficient to meet the requirement of end-to-end neural abstractive summarization. Examples range from summarizing radiology reports (Jing et al., 2019; Zhang et al., 2020) to congressional bills (Kornilova and Eidelman, 2019) and meeting conversations (Mehdad et al., 2013; Li et al., 2019; Koay et al., 2020). The lack of annotated resources suggests that end-toend systems may not be a “one-size-fits-all” solution to neural text summarization. There is an increasing need to develop cascaded architectures to allow for customized content selectors to be combined with general-purpose neural text generators In this paper, we aim to investigate the feasibility of a cascade approach to neural text summarization. We explore a constrained summarization task, where an abstract is created one sentence at a time through a cas"
2020.aacl-main.52,D19-1051,0,0.0327316,"Missing"
2020.aacl-main.52,D18-1207,0,0.0406238,"Missing"
2020.aacl-main.52,D19-5413,1,0.896174,"ative to methods that use more sophisticated representations such as syntactic/semantic graphs (Filippova and Strube, 2008; Banarescu et al., 2013; Liu et al., 2015). It is more straightforward to incorporate highlights into an encoder-decoder fusion model, and obtaining highlights through sequence tagging can be potentially adapted to new domains. 3 Experimental Results Data and Annotation To enable direct comparison with end-to-end systems, we conduct experiments on the widely used CNN/DM dataset (See et al., 2017) to report results of our cascade approach. We use the procedure described in Lebanoff et al. (2019b) to create training instances for the sentence selector and fine-grained content selector. Our training data contains 1,053,993 instances; every instance contains one or two candidate sentences. It is a positive instance if a groundtruth summary sentence can be formed by compressing or merging sentences of the instance, negative otherwise. For positive instances, we highlight all lemmatized unigrams appearing in the summary, excluding punctuation. We further add smoothing to the labels by highlighting single words that conSystem R-1 R-2 R-L SumBasic (Vanderwende et al., 2007) LexRank (Erkan"
2020.aacl-main.52,2020.acl-srw.26,1,0.704652,"of a cascade approach to neural text summarization. We explore a constrained summarization task, where an abstract is created one sentence at a time through a cascaded pipeline. Our pipeline architecture chooses one or two sentences from the source document, then highlights their summary-worthy segments and uses those as a basis for composing a summary sentence. When a pair of sentences are selected, it is important to ensure that they are fusible—there exists cohesive devices that tie the two sentences together into a coherent text—to avoid generating nonsensical outputs (Geva et al., 2019; Lebanoff et al., 2020). Highlighting sentence segments allows us to perform fine-grained content selection that guides the neural text generator to stitch selected segments into a coherent sentence. The contributions of this work are summarized as follows. 529 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 529–535 c December 4 - 7, 2020. 2020 Association for Computational Linguistics Sentence Selection Sent Pred Fine-Grained Content Selection NonHighlight Highlight Highli"
2020.aacl-main.52,P19-1209,1,0.933037,"ative to methods that use more sophisticated representations such as syntactic/semantic graphs (Filippova and Strube, 2008; Banarescu et al., 2013; Liu et al., 2015). It is more straightforward to incorporate highlights into an encoder-decoder fusion model, and obtaining highlights through sequence tagging can be potentially adapted to new domains. 3 Experimental Results Data and Annotation To enable direct comparison with end-to-end systems, we conduct experiments on the widely used CNN/DM dataset (See et al., 2017) to report results of our cascade approach. We use the procedure described in Lebanoff et al. (2019b) to create training instances for the sentence selector and fine-grained content selector. Our training data contains 1,053,993 instances; every instance contains one or two candidate sentences. It is a positive instance if a groundtruth summary sentence can be formed by compressing or merging sentences of the instance, negative otherwise. For positive instances, we highlight all lemmatized unigrams appearing in the summary, excluding punctuation. We further add smoothing to the labels by highlighting single words that conSystem R-1 R-2 R-L SumBasic (Vanderwende et al., 2007) LexRank (Erkan"
2020.aacl-main.52,D18-1446,1,0.84409,"Missing"
2020.aacl-main.52,2020.acl-main.703,0,0.0462508,"nd systems, whereas a pipeline architecture allows for flexible content selection. We finally discuss how we can take advantage of a cascaded pipeline in neural text summarization and shed light on important directions for future research. 1 We advocate for explicit content selection as it allows for a rigorous evaluation and visualization of intermediate results of such a module, rather than associating it with text generation. Existing neural abstractive systems can perform content selection implicitly using end-to-end models (See et al., 2017; Celikyilmaz et al., 2018; Raffel et al., 2019; Lewis et al., 2020), or more explicitly, with an external module to select important sentences or words to aid generation (Tan et al., 2017; Gehrmann et al., 2018; Chen and Bansal, 2018; Kry´sci´nski et al., 2018; Hsu et al., 2018; Lebanoff et al., 2018, 2019b; Liu and Lapata, 2019). However, content selection concerns not only the selection of important segments from a document, but also the cohesiveness of selected segments and the amount of text to be selected in order for a neural text generator to produce a summary. Introduction There is a variety of successful summarization applications but few can afford"
2020.aacl-main.52,P19-1210,0,0.0631557,"mportant segments from a document, but also the cohesiveness of selected segments and the amount of text to be selected in order for a neural text generator to produce a summary. Introduction There is a variety of successful summarization applications but few can afford to have a large number of annotated examples that are sufficient to meet the requirement of end-to-end neural abstractive summarization. Examples range from summarizing radiology reports (Jing et al., 2019; Zhang et al., 2020) to congressional bills (Kornilova and Eidelman, 2019) and meeting conversations (Mehdad et al., 2013; Li et al., 2019; Koay et al., 2020). The lack of annotated resources suggests that end-toend systems may not be a “one-size-fits-all” solution to neural text summarization. There is an increasing need to develop cascaded architectures to allow for customized content selectors to be combined with general-purpose neural text generators In this paper, we aim to investigate the feasibility of a cascade approach to neural text summarization. We explore a constrained summarization task, where an abstract is created one sentence at a time through a cascaded pipeline. Our pipeline architecture chooses one or two sen"
2020.aacl-main.52,W04-1013,0,0.0391224,"s may be placed on accurate content selection. nect two highlighted phrases and by dehighlighting isolated stopwords. At test time, four highestscored instances are selected per document; their important segments are highlighted by content selector then passed to the fusion step to produce a summary sentence each. The hyperparameter λ for weighing the per-target-word loss is set to 0.2 and highlighting threshold value is 0.15. The model hyperparameters are tuned on the validation split. Summarization Results We show experimental results on the standard test set and evaluated by ROUGE metrics (Lin, 2004) in Table 1. The performance of our cascade approaches, Cascade-Fusion and Cascade-Tag, is comparable to or outranks a number of extractive and abstractive baselines. Particularly, Cascade-Tag does not use a fusion step (§2) and is the output of fine-grained content selection. Cascade-Fusion provides a direct comparison against BERT-Abs (Lebanoff et al., 2019b) that uses sentence selection and fusion but lacks a fine-grained content selector. Our results suggest that a coarse-to-fine content selection strategy remains necessary to guide the fusion model to produce informative sentences. We obs"
2020.aacl-main.52,N15-1114,1,0.83228,"d to a “highlight-on” embedding for each token if it is chosen by the content selector, and a “highlight-off ” embedding for each token not chosen. The highlight-on/off embeddings are added to token embeddings in an element-wise manner; both highlight and token embeddings are learned. An illustration is shown in Figure 1. Highlights provide a valuable intermediate representation suitable for shallow abstraction. Our approach thus provides an alternative to methods that use more sophisticated representations such as syntactic/semantic graphs (Filippova and Strube, 2008; Banarescu et al., 2013; Liu et al., 2015). It is more straightforward to incorporate highlights into an encoder-decoder fusion model, and obtaining highlights through sequence tagging can be potentially adapted to new domains. 3 Experimental Results Data and Annotation To enable direct comparison with end-to-end systems, we conduct experiments on the widely used CNN/DM dataset (See et al., 2017) to report results of our cascade approach. We use the procedure described in Lebanoff et al. (2019b) to create training instances for the sentence selector and fine-grained content selector. Our training data contains 1,053,993 instances; eve"
2020.aacl-main.52,P19-1500,0,0.0403015,"icit content selection as it allows for a rigorous evaluation and visualization of intermediate results of such a module, rather than associating it with text generation. Existing neural abstractive systems can perform content selection implicitly using end-to-end models (See et al., 2017; Celikyilmaz et al., 2018; Raffel et al., 2019; Lewis et al., 2020), or more explicitly, with an external module to select important sentences or words to aid generation (Tan et al., 2017; Gehrmann et al., 2018; Chen and Bansal, 2018; Kry´sci´nski et al., 2018; Hsu et al., 2018; Lebanoff et al., 2018, 2019b; Liu and Lapata, 2019). However, content selection concerns not only the selection of important segments from a document, but also the cohesiveness of selected segments and the amount of text to be selected in order for a neural text generator to produce a summary. Introduction There is a variety of successful summarization applications but few can afford to have a large number of annotated examples that are sufficient to meet the requirement of end-to-end neural abstractive summarization. Examples range from summarizing radiology reports (Jing et al., 2019; Zhang et al., 2020) to congressional bills (Kornilova and"
2020.aacl-main.52,W13-2117,0,0.0180037,"ly the selection of important segments from a document, but also the cohesiveness of selected segments and the amount of text to be selected in order for a neural text generator to produce a summary. Introduction There is a variety of successful summarization applications but few can afford to have a large number of annotated examples that are sufficient to meet the requirement of end-to-end neural abstractive summarization. Examples range from summarizing radiology reports (Jing et al., 2019; Zhang et al., 2020) to congressional bills (Kornilova and Eidelman, 2019) and meeting conversations (Mehdad et al., 2013; Li et al., 2019; Koay et al., 2020). The lack of annotated resources suggests that end-toend systems may not be a “one-size-fits-all” solution to neural text summarization. There is an increasing need to develop cascaded architectures to allow for customized content selectors to be combined with general-purpose neural text generators In this paper, we aim to investigate the feasibility of a cascade approach to neural text summarization. We explore a constrained summarization task, where an abstract is created one sentence at a time through a cascaded pipeline. Our pipeline architecture choos"
2020.aacl-main.52,P17-1099,0,0.633251,"nto a coherent text is comparable to or outranks that of end-to-end systems, whereas a pipeline architecture allows for flexible content selection. We finally discuss how we can take advantage of a cascaded pipeline in neural text summarization and shed light on important directions for future research. 1 We advocate for explicit content selection as it allows for a rigorous evaluation and visualization of intermediate results of such a module, rather than associating it with text generation. Existing neural abstractive systems can perform content selection implicitly using end-to-end models (See et al., 2017; Celikyilmaz et al., 2018; Raffel et al., 2019; Lewis et al., 2020), or more explicitly, with an external module to select important sentences or words to aid generation (Tan et al., 2017; Gehrmann et al., 2018; Chen and Bansal, 2018; Kry´sci´nski et al., 2018; Hsu et al., 2018; Lebanoff et al., 2018, 2019b; Liu and Lapata, 2019). However, content selection concerns not only the selection of important segments from a document, but also the cohesiveness of selected segments and the amount of text to be selected in order for a neural text generator to produce a summary. Introduction There is a"
2020.aacl-main.52,P17-1108,0,0.0244664,"Missing"
2020.aacl-main.52,2020.acl-main.458,0,0.0330706,"2018; Lebanoff et al., 2018, 2019b; Liu and Lapata, 2019). However, content selection concerns not only the selection of important segments from a document, but also the cohesiveness of selected segments and the amount of text to be selected in order for a neural text generator to produce a summary. Introduction There is a variety of successful summarization applications but few can afford to have a large number of annotated examples that are sufficient to meet the requirement of end-to-end neural abstractive summarization. Examples range from summarizing radiology reports (Jing et al., 2019; Zhang et al., 2020) to congressional bills (Kornilova and Eidelman, 2019) and meeting conversations (Mehdad et al., 2013; Li et al., 2019; Koay et al., 2020). The lack of annotated resources suggests that end-toend systems may not be a “one-size-fits-all” solution to neural text summarization. There is an increasing need to develop cascaded architectures to allow for customized content selectors to be combined with general-purpose neural text generators In this paper, we aim to investigate the feasibility of a cascade approach to neural text summarization. We explore a constrained summarization task, where an ab"
2020.acl-main.715,H05-1091,0,0.126333,"entations learned by the deep learning models for RE, we introduce a new inductive bias to promote the similarity between the representation vectors for the overall sentences and the words along the shortest dependency paths between the two entity mentions. The intuition is that the relation between the two entity mentions of interest in a sentence for RE can be inferred from either the entire sentence or the shortest dependency path between the two entity mentions (due to the demonstrated ability of the shortest dependency path to capture the important context words for RE in the prior work (Bunescu and Mooney, 2005)). We thus expect that the representation vectors for the sentence and the dependency path should be similar (as both capture the semantic relation) and explicitly exploiting such similarity can help the models to induce more effective representations for RE. Our extensive experiments on three benchmark datasets (i.e., ACE 2005, SPOUSE and SciERC) demonstrate the effectiveness of the proposed model for RE, leading to the state-of-the-art performance for these datasets. 2 Related Work RE has been traditionally solved by the featurebased or kernel-based approaches (Zelenko et al., 8022 2003; Zho"
2020.acl-main.715,C10-1018,0,0.0389467,"ntence and the dependency path should be similar (as both capture the semantic relation) and explicitly exploiting such similarity can help the models to induce more effective representations for RE. Our extensive experiments on three benchmark datasets (i.e., ACE 2005, SPOUSE and SciERC) demonstrate the effectiveness of the proposed model for RE, leading to the state-of-the-art performance for these datasets. 2 Related Work RE has been traditionally solved by the featurebased or kernel-based approaches (Zelenko et al., 8022 2003; Zhou et al., 2005; Bunescu and Mooney, 2005; Sun et al., 2011; Chan and Roth, 2010; Nguyen and Grishman, 2014; Nguyen et al., 2015c). One of the issues in these approaches is the requirement for extensive feature or kernel engineering effort that hinder the generalization and applicability of the RE models. Recently, deep learning has been applied to address these problems for the traditional RE approaches, producing the state-ofthe-art performance for RE. The typical network architectures for RE include the Convolutional Neural Networks (Zeng et al., 2014; Nguyen and Grishman, 2015a; dos Santos et al., 2015; Wang et al., 2016), Recurrent Neural Networks (Nguyen and Grishma"
2020.acl-main.715,N19-1423,0,0.0381472,"sition embeddings and entity type embeddings, 200 hidden units for the CEON-LSTM model and all the other hidden vectors in the model (i.e., the hidden vectors in the final feed-forward neural network (with 2 layers) and the intermediate vectors in the weighted sum vector for x0t ), 1.0 for both loss tradeoff parameters α and β, and 0.001 for the initial learning rate with the Adam optimizer. The batch size is set to 50. Finally, we use either the uncontextualized word embeddings word2vec (with 300 dimensions) or the hidden vectors in the last layer of the BERTbase model (with 768 dimensions) (Devlin et al., 2019) to obtain the pre-trained word embeddings for the sentences (Devlin et al., 2019). We find it better to fix BERT in the experiments. Note that besides this section, we provide some additional analysis for the models in the Appendix. 4.2 Comparison with the state of the art We fist compare the proposed model (called CEONLSTM) with the baselines on the popular ACE 2005 dataset. In particular, the four following groups of RE models in the prior work on RE with the ACE 2005 dataset is chosen for comparison: (i) Feature based models: These models handdesign linguistic features for RE, i.e., FCM, H"
2020.acl-main.715,I17-2072,1,0.936545,"e negative log-likelihood function is then obtained to serve as the loss function for the model: Llabel = − log P (y|W, ws , wo ) (y is the golden relation label for ws and wo in W ). Eventually, the overall loss function of the model in this work is: L = Llabel + αLimport + βLpath (6) where α and β are trade-off parameters. The model is trained with shuffled mini-batching. 4 4.1 Experiments Datasets and Hyper-parameters We evaluate the models in this work using three benchmark datasets, i.e., ACE 2005, SPOUSE, and SciERC. For ACE 2005, similar to the previous work (Nguyen and Grishman, 2016; Fu et al., 2017; Shi et al., 2018; Veyseh et al., 2019), we use the dataset preprocessed and provided by (Yu et al., 2015) for compatible comparison. There are 6 different domains in this dataset, i.e., (bc, bn, cts, nw, un, and wl), covering text from news, conversations and web blogs. Following the prior work, the union of the domains bn and nw (called news) is used as the training data (called the source domain); a half of the documents in bc is reserved for the development data, and the remainder (cts, wl and the other half of bc) serve as the test data (called the target domains). This data separation f"
2020.acl-main.715,P19-1024,0,0.272518,"e semantic relationships between two entity mentions in text. Due to its importance, RE has been studied extensively in the literature. The recent studies on RE has focused on deep learning to develop methods to automatically induce sentence representations from data (Zeng et al., 2014; Nguyen and Grishman, 2015a; Verga et al., 2018). A notable insight in these recent studies is that the syntactic trees of the input sentences (i.e., the dependency trees) can provide effective information for the deep learning models, leading to the stateof-the-art performance for RE recently (Xu et al., 2015; Guo et al., 2019; Tran et al., 2019). In particular, the previous deep learning models for RE has mostly exploited the syntactic trees to structure the network architectures according to the word connections presented in the trees (e.g., performing Graph Convolutional Neural Networks (GCN) over the dependency trees (Zhang et al., 2018)). Unfortunately, these models might not be able to generalize well as the tree structures of the training data might significantly differ from those in the test data (i.e., the models are overfit to the syntactic structures in the training data). For instance, in the cross-doma"
2020.acl-main.715,P18-1175,0,0.0301798,"Missing"
2020.acl-main.715,D19-6203,1,0.888149,"Missing"
2020.acl-main.715,S10-1006,0,0.0317503,"Missing"
2020.acl-main.715,P15-2047,0,0.0384495,"problems for the traditional RE approaches, producing the state-ofthe-art performance for RE. The typical network architectures for RE include the Convolutional Neural Networks (Zeng et al., 2014; Nguyen and Grishman, 2015a; dos Santos et al., 2015; Wang et al., 2016), Recurrent Neural Networks (Nguyen and Grishman, 2016; Zhou et al., 2016; Zhang et al., 2017; Nguyen et al., 2019a), and self-attentions in Transformer (Verga et al., 2018). The syntactic information from the dependency trees has also been shown to be useful for the deep learning models for RE (Tai et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016; Peng et al., 2017; Zhang et al., 2018; Guo et al., 2019; Tran et al., 2019; Song et al., 2019; Veyseh et al., 2019). However, these methods tend to poorly generalize to new syntactic structures due to the direct reliance on the syntactic trees (e.g., in different domains) or fail to exploit the syntax-based importance of the words for RE due to the sole focus on edges of the dependency trees (Veyseh et al., 2019). 3 Model The RE problem can be formulated as a multi-class classification problem. Formally, given an input sentence W = w1 , w2 , . . . , wN where wt is the"
2020.acl-main.715,D18-1360,0,0.0227953,"data (called the target domains). This data separation facilitates the evaluation of the cross-domain generalization of the models due to the domain difference of the training and test data. The SPOUSE dataset is recently introduced by (Hancock et al., 2018), involving 22,195 sentences for the training data, 2,796 sentences for the validation data, and 2,697 sentences for the test data. Each sentence in this dataset contains two marked person names (i.e., the entity mentions) and the goal is to identify whether the two people mentioned in the sentence are spouses. Finally, the SciERC dataset (Luan et al., 2018) annotates 500 scientific abstracts for the entity mentions along with the coreferences and relations between them. For RE, this dataset provides 3,219 sentences in the training data, 455 sentences in the validation data and 974 sentences in the test data. We fine tune the hyper-parameters for the models in this work on the validation data of the ACE 2005 dataset. The best parameters suggested by this process include: 30 dimensions for the position embeddings and entity type embeddings, 200 hidden units for the CEON-LSTM model and all the other hidden vectors in the model (i.e., the hidden vec"
2020.acl-main.715,P16-1105,0,0.0410265,"raditional RE approaches, producing the state-ofthe-art performance for RE. The typical network architectures for RE include the Convolutional Neural Networks (Zeng et al., 2014; Nguyen and Grishman, 2015a; dos Santos et al., 2015; Wang et al., 2016), Recurrent Neural Networks (Nguyen and Grishman, 2016; Zhou et al., 2016; Zhang et al., 2017; Nguyen et al., 2019a), and self-attentions in Transformer (Verga et al., 2018). The syntactic information from the dependency trees has also been shown to be useful for the deep learning models for RE (Tai et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016; Peng et al., 2017; Zhang et al., 2018; Guo et al., 2019; Tran et al., 2019; Song et al., 2019; Veyseh et al., 2019). However, these methods tend to poorly generalize to new syntactic structures due to the direct reliance on the syntactic trees (e.g., in different domains) or fail to exploit the syntax-based importance of the words for RE due to the sole focus on edges of the dependency trees (Veyseh et al., 2019). 3 Model The RE problem can be formulated as a multi-class classification problem. Formally, given an input sentence W = w1 , w2 , . . . , wN where wt is the t-th word in the senten"
2020.acl-main.715,C18-1193,1,0.849838,"contextual importance of wt with respect to the relation prediction between ws and wo in W . In this section, we first describe the ON-LSTM model to achieve these importance scores (i.e., the model-based scores). A new model (called CEON-LSTM) that integrates the representation of the entire sentence into the cells of ON-LSTM will be presented afterward. ON-LSTM: Long-short Term Memory Networks (LSTM) (Hochreiter and Schmidhuber, 1997) has been widely used in Natural Language Processing (NLP) due to its natural mechanism to obtain the abstract representations for a sequence of input vectors (Nguyen and Nguyen, 2018b, 2019). Given the input representation vector sequence X = x1 , x2 , . . . , xN , LSTM produces a sequence of hidden vectors H = h1 , h2 , . . . , hN using the following recurrent functions at the time step (word) wt (assuming the zero vector for h0 ): ft = σ(Wf xt + Uf ht−1 + bf ) it = σ(Wi xt + Ui ht−1 + bi ) ot = σ(Wo xt + Uo ht−1 + bo ) cˆt = tanh(Wc xt + Uc ht−1 + bc ) (1) ct = ft ◦ ct−1 + it ◦ cˆt ht = ot ◦ tanh(ct ) where ft , it and ot are called the forget, input and output gates (respectively). In order to compute the importance score for each word wt , ON-LSTM introduce into the m"
2020.acl-main.715,N16-1034,1,0.872658,"posed model CEON-LSTM achieves significantly better performance than HIS-CEON-LSTM (with large performance gap), thus testifying to the importance of the master gates to obtain the model-based importance scores for CEON-LSTM. 5 whole input sentences and the shortest dependency paths between the two entity mentions for RE. Extensive experiments are conducted to demonstrate the benefits of the proposed model. We achieve the state-of-the-art performance on three datasets for RE. In the future, we plan to apply CEON-LSTM to other related NLP tasks (e.g., Event Extraction, Semantic Role Labeling) (Nguyen et al., 2016a; Nguyen and Grishman, 2018a). Acknowledgments This research has been supported in part by Vingroup Innovation Foundation (VINIF) in project code VINIF.2019.DA18, the NSF grant CNS1747798 to the IUCRC Center for Big Learning, and a gift from Adobe Research. This research is also based upon work supported in part by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via IARPA Contract No. 201919051600006 under the Better Extraction from Text Towards Enhanced Retrieval (BETTER) Program. The views and conclusions contained herein"
2020.acl-main.715,Q17-1008,0,0.0380413,"s, producing the state-ofthe-art performance for RE. The typical network architectures for RE include the Convolutional Neural Networks (Zeng et al., 2014; Nguyen and Grishman, 2015a; dos Santos et al., 2015; Wang et al., 2016), Recurrent Neural Networks (Nguyen and Grishman, 2016; Zhou et al., 2016; Zhang et al., 2017; Nguyen et al., 2019a), and self-attentions in Transformer (Verga et al., 2018). The syntactic information from the dependency trees has also been shown to be useful for the deep learning models for RE (Tai et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016; Peng et al., 2017; Zhang et al., 2018; Guo et al., 2019; Tran et al., 2019; Song et al., 2019; Veyseh et al., 2019). However, these methods tend to poorly generalize to new syntactic structures due to the direct reliance on the syntactic trees (e.g., in different domains) or fail to exploit the syntax-based importance of the words for RE due to the sole focus on edges of the dependency trees (Veyseh et al., 2019). 3 Model The RE problem can be formulated as a multi-class classification problem. Formally, given an input sentence W = w1 , w2 , . . . , wN where wt is the t-th word in the sentence W of length N ,"
2020.acl-main.715,P15-1061,0,0.0149081,"03; Zhou et al., 2005; Bunescu and Mooney, 2005; Sun et al., 2011; Chan and Roth, 2010; Nguyen and Grishman, 2014; Nguyen et al., 2015c). One of the issues in these approaches is the requirement for extensive feature or kernel engineering effort that hinder the generalization and applicability of the RE models. Recently, deep learning has been applied to address these problems for the traditional RE approaches, producing the state-ofthe-art performance for RE. The typical network architectures for RE include the Convolutional Neural Networks (Zeng et al., 2014; Nguyen and Grishman, 2015a; dos Santos et al., 2015; Wang et al., 2016), Recurrent Neural Networks (Nguyen and Grishman, 2016; Zhou et al., 2016; Zhang et al., 2017; Nguyen et al., 2019a), and self-attentions in Transformer (Verga et al., 2018). The syntactic information from the dependency trees has also been shown to be useful for the deep learning models for RE (Tai et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016; Peng et al., 2017; Zhang et al., 2018; Guo et al., 2019; Tran et al., 2019; Song et al., 2019; Veyseh et al., 2019). However, these methods tend to poorly generalize to new syntactic structures due to the di"
2020.acl-main.715,D18-1125,0,0.23386,"liance on the syntactic trees (e.g., in different domains) or fail to exploit the syntax-based importance of the words for RE due to the sole focus on edges of the dependency trees (Veyseh et al., 2019). 3 Model The RE problem can be formulated as a multi-class classification problem. Formally, given an input sentence W = w1 , w2 , . . . , wN where wt is the t-th word in the sentence W of length N , and two entity mentions of interest at indexes s and o (1 ≤ s < o ≤ N ), our goal is to predict the semantic relation between ws and wo in W . Similar to the previous work on deep learning for RE (Shi et al., 2018; Veyseh et al., 2019), we first transform each word wt into a representation vector xt using the concatenation of the three following vectors: (i) the pre-trained word embeddings of wt , (ii) the position embedding vectors (to encode the relative distances of wt to the two entity mentions of interest ws and wo (i.e., t − s and t − o)), and (iii) the entity type embeddings (i.e., the embeddings of the BIO labels for the words to capture the entity mentions present in X). This word-to-vector transformation converts the input sentence W into a sequence of representation vectors X = x1 , x2 , . ."
2020.acl-main.715,P11-1053,0,0.0729171,"vectors for the sentence and the dependency path should be similar (as both capture the semantic relation) and explicitly exploiting such similarity can help the models to induce more effective representations for RE. Our extensive experiments on three benchmark datasets (i.e., ACE 2005, SPOUSE and SciERC) demonstrate the effectiveness of the proposed model for RE, leading to the state-of-the-art performance for these datasets. 2 Related Work RE has been traditionally solved by the featurebased or kernel-based approaches (Zelenko et al., 8022 2003; Zhou et al., 2005; Bunescu and Mooney, 2005; Sun et al., 2011; Chan and Roth, 2010; Nguyen and Grishman, 2014; Nguyen et al., 2015c). One of the issues in these approaches is the requirement for extensive feature or kernel engineering effort that hinder the generalization and applicability of the RE models. Recently, deep learning has been applied to address these problems for the traditional RE approaches, producing the state-ofthe-art performance for RE. The typical network architectures for RE include the Convolutional Neural Networks (Zeng et al., 2014; Nguyen and Grishman, 2015a; dos Santos et al., 2015; Wang et al., 2016), Recurrent Neural Network"
2020.acl-main.715,P14-2012,1,0.902936,"ency path should be similar (as both capture the semantic relation) and explicitly exploiting such similarity can help the models to induce more effective representations for RE. Our extensive experiments on three benchmark datasets (i.e., ACE 2005, SPOUSE and SciERC) demonstrate the effectiveness of the proposed model for RE, leading to the state-of-the-art performance for these datasets. 2 Related Work RE has been traditionally solved by the featurebased or kernel-based approaches (Zelenko et al., 8022 2003; Zhou et al., 2005; Bunescu and Mooney, 2005; Sun et al., 2011; Chan and Roth, 2010; Nguyen and Grishman, 2014; Nguyen et al., 2015c). One of the issues in these approaches is the requirement for extensive feature or kernel engineering effort that hinder the generalization and applicability of the RE models. Recently, deep learning has been applied to address these problems for the traditional RE approaches, producing the state-ofthe-art performance for RE. The typical network architectures for RE include the Convolutional Neural Networks (Zeng et al., 2014; Nguyen and Grishman, 2015a; dos Santos et al., 2015; Wang et al., 2016), Recurrent Neural Networks (Nguyen and Grishman, 2016; Zhou et al., 2016;"
2020.acl-main.715,P15-1150,0,0.0245195,"has been applied to address these problems for the traditional RE approaches, producing the state-ofthe-art performance for RE. The typical network architectures for RE include the Convolutional Neural Networks (Zeng et al., 2014; Nguyen and Grishman, 2015a; dos Santos et al., 2015; Wang et al., 2016), Recurrent Neural Networks (Nguyen and Grishman, 2016; Zhou et al., 2016; Zhang et al., 2017; Nguyen et al., 2019a), and self-attentions in Transformer (Verga et al., 2018). The syntactic information from the dependency trees has also been shown to be useful for the deep learning models for RE (Tai et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016; Peng et al., 2017; Zhang et al., 2018; Guo et al., 2019; Tran et al., 2019; Song et al., 2019; Veyseh et al., 2019). However, these methods tend to poorly generalize to new syntactic structures due to the direct reliance on the syntactic trees (e.g., in different domains) or fail to exploit the syntax-based importance of the words for RE due to the sole focus on edges of the dependency trees (Veyseh et al., 2019). 3 Model The RE problem can be formulated as a multi-class classification problem. Formally, given an input sentence W = w1"
2020.acl-main.715,W15-1506,1,0.957182,"ion injection. We perform extensive experiments to demonstrate the effectiveness of the proposed method, leading to the state-of-the-art performance on three RE benchmark datasets. 1 Introduction One of the fundamental tasks in Information Extraction (IE) is Relation Extraction (RE) where the goal is to find the semantic relationships between two entity mentions in text. Due to its importance, RE has been studied extensively in the literature. The recent studies on RE has focused on deep learning to develop methods to automatically induce sentence representations from data (Zeng et al., 2014; Nguyen and Grishman, 2015a; Verga et al., 2018). A notable insight in these recent studies is that the syntactic trees of the input sentences (i.e., the dependency trees) can provide effective information for the deep learning models, leading to the stateof-the-art performance for RE recently (Xu et al., 2015; Guo et al., 2019; Tran et al., 2019). In particular, the previous deep learning models for RE has mostly exploited the syntactic trees to structure the network architectures according to the word connections presented in the trees (e.g., performing Graph Convolutional Neural Networks (GCN) over the dependency tr"
2020.acl-main.715,N19-1286,0,0.231821,"nships between two entity mentions in text. Due to its importance, RE has been studied extensively in the literature. The recent studies on RE has focused on deep learning to develop methods to automatically induce sentence representations from data (Zeng et al., 2014; Nguyen and Grishman, 2015a; Verga et al., 2018). A notable insight in these recent studies is that the syntactic trees of the input sentences (i.e., the dependency trees) can provide effective information for the deep learning models, leading to the stateof-the-art performance for RE recently (Xu et al., 2015; Guo et al., 2019; Tran et al., 2019). In particular, the previous deep learning models for RE has mostly exploited the syntactic trees to structure the network architectures according to the word connections presented in the trees (e.g., performing Graph Convolutional Neural Networks (GCN) over the dependency trees (Zhang et al., 2018)). Unfortunately, these models might not be able to generalize well as the tree structures of the training data might significantly differ from those in the test data (i.e., the models are overfit to the syntactic structures in the training data). For instance, in the cross-domain setting for RE, t"
2020.acl-main.715,N18-1080,0,0.145516,"tensive experiments to demonstrate the effectiveness of the proposed method, leading to the state-of-the-art performance on three RE benchmark datasets. 1 Introduction One of the fundamental tasks in Information Extraction (IE) is Relation Extraction (RE) where the goal is to find the semantic relationships between two entity mentions in text. Due to its importance, RE has been studied extensively in the literature. The recent studies on RE has focused on deep learning to develop methods to automatically induce sentence representations from data (Zeng et al., 2014; Nguyen and Grishman, 2015a; Verga et al., 2018). A notable insight in these recent studies is that the syntactic trees of the input sentences (i.e., the dependency trees) can provide effective information for the deep learning models, leading to the stateof-the-art performance for RE recently (Xu et al., 2015; Guo et al., 2019; Tran et al., 2019). In particular, the previous deep learning models for RE has mostly exploited the syntactic trees to structure the network architectures according to the word connections presented in the trees (e.g., performing Graph Convolutional Neural Networks (GCN) over the dependency trees (Zhang et al., 201"
2020.acl-main.715,P15-1062,1,0.890147,"(as both capture the semantic relation) and explicitly exploiting such similarity can help the models to induce more effective representations for RE. Our extensive experiments on three benchmark datasets (i.e., ACE 2005, SPOUSE and SciERC) demonstrate the effectiveness of the proposed model for RE, leading to the state-of-the-art performance for these datasets. 2 Related Work RE has been traditionally solved by the featurebased or kernel-based approaches (Zelenko et al., 8022 2003; Zhou et al., 2005; Bunescu and Mooney, 2005; Sun et al., 2011; Chan and Roth, 2010; Nguyen and Grishman, 2014; Nguyen et al., 2015c). One of the issues in these approaches is the requirement for extensive feature or kernel engineering effort that hinder the generalization and applicability of the RE models. Recently, deep learning has been applied to address these problems for the traditional RE approaches, producing the state-ofthe-art performance for RE. The typical network architectures for RE include the Convolutional Neural Networks (Zeng et al., 2014; Nguyen and Grishman, 2015a; dos Santos et al., 2015; Wang et al., 2016), Recurrent Neural Networks (Nguyen and Grishman, 2016; Zhou et al., 2016; Zhang et al., 2017;"
2020.acl-main.715,P19-1132,0,0.0351558,"Missing"
2020.acl-main.715,P16-1123,0,0.0658141,"Missing"
2020.acl-main.715,D15-1206,0,0.261466,"oal is to find the semantic relationships between two entity mentions in text. Due to its importance, RE has been studied extensively in the literature. The recent studies on RE has focused on deep learning to develop methods to automatically induce sentence representations from data (Zeng et al., 2014; Nguyen and Grishman, 2015a; Verga et al., 2018). A notable insight in these recent studies is that the syntactic trees of the input sentences (i.e., the dependency trees) can provide effective information for the deep learning models, leading to the stateof-the-art performance for RE recently (Xu et al., 2015; Guo et al., 2019; Tran et al., 2019). In particular, the previous deep learning models for RE has mostly exploited the syntactic trees to structure the network architectures according to the word connections presented in the trees (e.g., performing Graph Convolutional Neural Networks (GCN) over the dependency trees (Zhang et al., 2018)). Unfortunately, these models might not be able to generalize well as the tree structures of the training data might significantly differ from those in the test data (i.e., the models are overfit to the syntactic structures in the training data). For instance,"
2020.acl-main.715,N15-1155,0,0.106033,"− log P (y|W, ws , wo ) (y is the golden relation label for ws and wo in W ). Eventually, the overall loss function of the model in this work is: L = Llabel + αLimport + βLpath (6) where α and β are trade-off parameters. The model is trained with shuffled mini-batching. 4 4.1 Experiments Datasets and Hyper-parameters We evaluate the models in this work using three benchmark datasets, i.e., ACE 2005, SPOUSE, and SciERC. For ACE 2005, similar to the previous work (Nguyen and Grishman, 2016; Fu et al., 2017; Shi et al., 2018; Veyseh et al., 2019), we use the dataset preprocessed and provided by (Yu et al., 2015) for compatible comparison. There are 6 different domains in this dataset, i.e., (bc, bn, cts, nw, un, and wl), covering text from news, conversations and web blogs. Following the prior work, the union of the domains bn and nw (called news) is used as the training data (called the source domain); a half of the documents in bc is reserved for the development data, and the remainder (cts, wl and the other half of bc) serve as the test data (called the target domains). This data separation facilitates the evaluation of the cross-domain generalization of the models due to the domain difference of"
2020.acl-main.715,C14-1220,0,0.439643,"syntactic information injection. We perform extensive experiments to demonstrate the effectiveness of the proposed method, leading to the state-of-the-art performance on three RE benchmark datasets. 1 Introduction One of the fundamental tasks in Information Extraction (IE) is Relation Extraction (RE) where the goal is to find the semantic relationships between two entity mentions in text. Due to its importance, RE has been studied extensively in the literature. The recent studies on RE has focused on deep learning to develop methods to automatically induce sentence representations from data (Zeng et al., 2014; Nguyen and Grishman, 2015a; Verga et al., 2018). A notable insight in these recent studies is that the syntactic trees of the input sentences (i.e., the dependency trees) can provide effective information for the deep learning models, leading to the stateof-the-art performance for RE recently (Xu et al., 2015; Guo et al., 2019; Tran et al., 2019). In particular, the previous deep learning models for RE has mostly exploited the syntactic trees to structure the network architectures according to the word connections presented in the trees (e.g., performing Graph Convolutional Neural Networks ("
2020.acl-main.715,D18-1244,0,0.514323,"rga et al., 2018). A notable insight in these recent studies is that the syntactic trees of the input sentences (i.e., the dependency trees) can provide effective information for the deep learning models, leading to the stateof-the-art performance for RE recently (Xu et al., 2015; Guo et al., 2019; Tran et al., 2019). In particular, the previous deep learning models for RE has mostly exploited the syntactic trees to structure the network architectures according to the word connections presented in the trees (e.g., performing Graph Convolutional Neural Networks (GCN) over the dependency trees (Zhang et al., 2018)). Unfortunately, these models might not be able to generalize well as the tree structures of the training data might significantly differ from those in the test data (i.e., the models are overfit to the syntactic structures in the training data). For instance, in the cross-domain setting for RE, the domains for the training data and test data are dissimilar, often leading to a mismatch between the syntactic structures of the training data and test data. In order to overcome this issue, the overall strategy is to obtain a more general representation of the syntactic trees that can be used to i"
2020.acl-main.715,D17-1004,0,0.022091,"Nguyen et al., 2015c). One of the issues in these approaches is the requirement for extensive feature or kernel engineering effort that hinder the generalization and applicability of the RE models. Recently, deep learning has been applied to address these problems for the traditional RE approaches, producing the state-ofthe-art performance for RE. The typical network architectures for RE include the Convolutional Neural Networks (Zeng et al., 2014; Nguyen and Grishman, 2015a; dos Santos et al., 2015; Wang et al., 2016), Recurrent Neural Networks (Nguyen and Grishman, 2016; Zhou et al., 2016; Zhang et al., 2017; Nguyen et al., 2019a), and self-attentions in Transformer (Verga et al., 2018). The syntactic information from the dependency trees has also been shown to be useful for the deep learning models for RE (Tai et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016; Peng et al., 2017; Zhang et al., 2018; Guo et al., 2019; Tran et al., 2019; Song et al., 2019; Veyseh et al., 2019). However, these methods tend to poorly generalize to new syntactic structures due to the direct reliance on the syntactic trees (e.g., in different domains) or fail to exploit the syntax-based importance"
2020.acl-main.715,P05-1053,0,0.426203,"05)). We thus expect that the representation vectors for the sentence and the dependency path should be similar (as both capture the semantic relation) and explicitly exploiting such similarity can help the models to induce more effective representations for RE. Our extensive experiments on three benchmark datasets (i.e., ACE 2005, SPOUSE and SciERC) demonstrate the effectiveness of the proposed model for RE, leading to the state-of-the-art performance for these datasets. 2 Related Work RE has been traditionally solved by the featurebased or kernel-based approaches (Zelenko et al., 8022 2003; Zhou et al., 2005; Bunescu and Mooney, 2005; Sun et al., 2011; Chan and Roth, 2010; Nguyen and Grishman, 2014; Nguyen et al., 2015c). One of the issues in these approaches is the requirement for extensive feature or kernel engineering effort that hinder the generalization and applicability of the RE models. Recently, deep learning has been applied to address these problems for the traditional RE approaches, producing the state-ofthe-art performance for RE. The typical network architectures for RE include the Convolutional Neural Networks (Zeng et al., 2014; Nguyen and Grishman, 2015a; dos Santos et al., 2015;"
2020.acl-main.715,P16-2034,0,0.02743,"and Grishman, 2014; Nguyen et al., 2015c). One of the issues in these approaches is the requirement for extensive feature or kernel engineering effort that hinder the generalization and applicability of the RE models. Recently, deep learning has been applied to address these problems for the traditional RE approaches, producing the state-ofthe-art performance for RE. The typical network architectures for RE include the Convolutional Neural Networks (Zeng et al., 2014; Nguyen and Grishman, 2015a; dos Santos et al., 2015; Wang et al., 2016), Recurrent Neural Networks (Nguyen and Grishman, 2016; Zhou et al., 2016; Zhang et al., 2017; Nguyen et al., 2019a), and self-attentions in Transformer (Verga et al., 2018). The syntactic information from the dependency trees has also been shown to be useful for the deep learning models for RE (Tai et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016; Peng et al., 2017; Zhang et al., 2018; Guo et al., 2019; Tran et al., 2019; Song et al., 2019; Veyseh et al., 2019). However, these methods tend to poorly generalize to new syntactic structures due to the direct reliance on the syntactic trees (e.g., in different domains) or fail to exploit the synt"
2020.acl-main.762,P18-1017,0,0.0248552,"Missing"
2020.acl-main.762,L18-1027,0,0.0593803,"Missing"
2020.acl-main.762,C18-1244,1,0.843876,"s. Then the output is fed to two dense layers yielding the class predictions. We implement our model based on the Hugging Face’s BERT implementation (Wolf et al., 2019). 5.2 Evaluation Settings We evaluate the performance by using two different evaluation metrics for this new task. Font Recall (FR) Less popular fonts could be underrepresented by the models. Therefore we need an evaluation metric that measures the performance of models in learning individual labels. Since we are dealing with an unbalanced dataset, motivated by evaluation methodology used in previous recommendation systems like Kar et al. (2018); Carneiro et al. (2007), we compute Font Recall, i.e. the average recall per font, to measure the performance of the models in learning individual labels. P|F | FR := Emoji Model In this model, we use the DeepMoji pre-trained model (Felbo et al., 2017) to generate emoji vectors by encoding the text into 2304dimensional feature vectors. We treat these features as embedding and pass them to the model with two dense layers. Deepmoji6 is a sentence-level model containing rich representations of emotional content which is trained on a 1,246 million tweet corpus in the emoji prediction task. 5 Expe"
2020.acl-main.762,L18-1010,0,0.0366152,"Missing"
2020.acl-main.762,E17-1083,0,0.0660149,"Missing"
2020.acl-main.762,D14-1162,0,0.0821799,"Missing"
2020.acl-main.762,P19-1112,1,0.773681,"sess annotators’ reliability (Yang et al., 2018; Srinivasan and Chander, 2019; Rodrigues et al., 2014). All of these methods rely on the assumption that only one answer is correct and should be considered as ground truth (Nguyen et al., 2016). Whereas in tasks like ours, sentiment analysis (Brew et al., 2010) or facial expression (Barsoum et al., 2016), the answer is likely to be more subjective due to its non-deterministic nature (Urkullu et al., 2019). We follow previous studies that successfully employed label distribution learning to handle ambiguity in the annotations (Geng et al., 2013; Shirani et al., 2019; Yang et al., 2015). 3 Font Dataset The proposed dataset includes 1,309 short text instances from Adobe Spark3 . The dataset is a collection of publicly available sample texts created by different designers. It covers a variety of topics found in posters, flyers, motivational quotes and advertisements.4 3 https://spark.adobe.com. The dataset along with the annotations can be found online: https://github.com/RiTUAL-UH/ Font-prediction-dataset 4 Choice of Fonts A vast number of fonts and typefaces are used in contemporary printed literature. To narrow down the task, we had a font expert select"
2020.acl-main.762,D17-1169,0,\N,Missing
2020.acl-srw.26,J05-3002,0,0.148665,"improper use of points of correspondence between sentences, yielding nonsensical output. Summaries are manually re-cased for readability. tences, which has a great potential for improving content selection and deep sentence fusion. Sentence fusion (or multi-sentence compression) plays a prominent role in automated summarization and its importance has long been recognized (Barzilay et al., 1999). Early attempts to fuse sentences build a dependency graph from sentences, then decode a tree from the graph using integer linear programming, finally linearize the tree to generate a summary sentence (Barzilay and McKeown, 2005; Filippova and Strube, 2008; Thadani and McKeown, 2013a). Despite valuable insights gained from 191 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 191–198 c July 5 - July 10, 2020. 2020 Association for Computational Linguistics PoC Type Source Sentences Summary Sentence Pronominal Referencing [S1] The bodies showed signs of torture. [S2] They were left on the side of a highway in Chilpancingo, about an hour north of the tourist resort of Acapulco in the state of Guerrero. [S1] Bahamian R&B singer Johnny Kemp , best kno"
2020.acl-srw.26,P99-1071,0,0.883194,"://github.com/ucfnlp/ points-of-correspondence Table 1: Unfaithful summary sentences generated by neural abstractive summarizers, in-house and PG (See et al., 2017). They attempt to merge two sentences into one sentence with improper use of points of correspondence between sentences, yielding nonsensical output. Summaries are manually re-cased for readability. tences, which has a great potential for improving content selection and deep sentence fusion. Sentence fusion (or multi-sentence compression) plays a prominent role in automated summarization and its importance has long been recognized (Barzilay et al., 1999). Early attempts to fuse sentences build a dependency graph from sentences, then decode a tree from the graph using integer linear programming, finally linearize the tree to generate a summary sentence (Barzilay and McKeown, 2005; Filippova and Strube, 2008; Thadani and McKeown, 2013a). Despite valuable insights gained from 191 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 191–198 c July 5 - July 10, 2020. 2020 Association for Computational Linguistics PoC Type Source Sentences Summary Sentence Pronominal Referencing ["
2020.acl-srw.26,N18-1150,0,0.0704995,"Missing"
2020.acl-srw.26,P18-1063,0,0.0365296,"it was actually Richards. Had the models explicitly recognized the points of correspondence in the sentences—that the journalist is a separate entity from Robert Downey Jr. and that Richards is separate from police officer—then a more accurate summary could have been generated. Related Work Uncovering hidden correspondences between sentences is essential for producing proper summary sentences. A number of recent efforts select important words and sentences from a given document, then let the summarizer attend to selected content to generate a summary (Gehrmann et al., 2018; Hsu et al., 2018; Chen and Bansal, 2018; Putra et al., 2018; Lebanoff et al., 2018; Liu and Lapata, 2019). These systems are largely agnostic to sentence correspondences, which can have two undesirable consequences. If only a single sentence is selected, it can be impossible for the summarizer to produce a fusion sentence from it. Moreover, if non-fusible textual units are selected, the summarizer is forced to fuse them into a summary sentence, yielding output summaries that often fail to keep the original meaning intact. Therefore, in this paper we had investigated the correspondences between sentences to gain an understanding of"
2020.acl-srw.26,2020.acl-main.454,0,0.0297295,"hanam, 2011; Thadani and McKeown, 2013b; Mehdad et al., 2013; Nayeem et al., 2018). These systems rely on common words to derive a connected graph from input sentences or subject-verb-object triples (Moryossef et al., 2019). When there are no common words in sentences, systems tend to break apart. There has been a lack of annotated datasets and guidelines for sentence fusion. Few studies have investigated the types of correspondence between sentences such as entity and event coreference. Evaluating sentence fusion systems requires not only novel metrics (Zhao et al., 2019; Zhang et al., 2020; Durmus et al., 2020; Wang et al., 2020) but also high-quality ground-truth annotations. It is therefore necessary to conduct a first study to look into cues humans use to establish correspondence between disparate sentences. We envision sentence correspondence to be related to text cohesion and coherence, which help establish correspondences between two pieces of text. Halliday and Hasan (1976) describe text cohesion as cohesive devices that tie two textual elements together. They identify five categories of cohesion: 195 human annotations of points of correspondence between sentences. The dataset fills a notabl"
2020.acl-srw.26,W11-1607,0,0.91516,"public memory in the West these days, but it is being used by the Islamic State in Iraq and Syria (ISIS) to support its sectarian narrative. In its propaganda, ISIS has been using Abu Ghraib and other cases of Western abuse to legitimize its current actions [...] [Summary] In its propaganda, ISIS is being used by the Islamic State in Iraq and Syria. Introduction Stitching portions of text together into a sentence is a crucial first step in abstractive summarization. It involves choosing which sentences to fuse, what content from each of them to retain and how best to present that information (Elsner and Santhanam, 2011). A major challenge in fusing sentences is to establish correspondence between sentences. If there exists no correspondence, it would be difficult, if not impossible, to fuse sentences. In Table 1, we present example source and fusion sentences, where the summarizer attempts to merge two sentences into a summary sentence with improper use of points of correspondence. In this paper, we seek to uncover hidden correspondences between sen1 https://github.com/ucfnlp/ points-of-correspondence Table 1: Unfaithful summary sentences generated by neural abstractive summarizers, in-house and PG (See et a"
2020.acl-srw.26,D08-1019,0,0.808411,"rrespondence between sentences, yielding nonsensical output. Summaries are manually re-cased for readability. tences, which has a great potential for improving content selection and deep sentence fusion. Sentence fusion (or multi-sentence compression) plays a prominent role in automated summarization and its importance has long been recognized (Barzilay et al., 1999). Early attempts to fuse sentences build a dependency graph from sentences, then decode a tree from the graph using integer linear programming, finally linearize the tree to generate a summary sentence (Barzilay and McKeown, 2005; Filippova and Strube, 2008; Thadani and McKeown, 2013a). Despite valuable insights gained from 191 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 191–198 c July 5 - July 10, 2020. 2020 Association for Computational Linguistics PoC Type Source Sentences Summary Sentence Pronominal Referencing [S1] The bodies showed signs of torture. [S2] They were left on the side of a highway in Chilpancingo, about an hour north of the tourist resort of Acapulco in the state of Guerrero. [S1] Bahamian R&B singer Johnny Kemp , best known for the 1988 party anthem"
2020.acl-srw.26,D18-1443,0,0.0637824,"Missing"
2020.acl-srw.26,P19-1209,1,0.865209,"able 2. If multiple PoC co-exist in an example, an annotator is expected to label them all; a separate PoC type will be assigned to each PoC occurrence. We are particularly interested in annotating inter-sentence PoC. If entity mentions (“John” and “he”) are found in the same sentence, we do not explicitly label them but assume such intra-sentence referencing can be captured by an existing coreference resolver. Instances of source sentences and summary sentences are obtained from the test and validation splits of the CNN/DailyMail corpus (See et al., 2017) following the procedure described by Lebanoff et al. (2019a). We take a human summary sentence as an anchor point to find two document sentences that are most similar to it based on ROUGE. It becomes an instance containing a pair of source sentences and their summary. The method allows us to identify a large quantity of candidate fusion instances. Annotations are performed in two stages. Stage one removes all spurious pairs that are generated by the heuristic, i.e. a summary sentence that is not a valid fusion of the corresponding two source sentences. Human annotators are given a pair of sentences and a summary sentence and are asked Common-Noun Rep"
2020.acl-srw.26,D18-1446,1,0.878761,"xplicitly recognized the points of correspondence in the sentences—that the journalist is a separate entity from Robert Downey Jr. and that Richards is separate from police officer—then a more accurate summary could have been generated. Related Work Uncovering hidden correspondences between sentences is essential for producing proper summary sentences. A number of recent efforts select important words and sentences from a given document, then let the summarizer attend to selected content to generate a summary (Gehrmann et al., 2018; Hsu et al., 2018; Chen and Bansal, 2018; Putra et al., 2018; Lebanoff et al., 2018; Liu and Lapata, 2019). These systems are largely agnostic to sentence correspondences, which can have two undesirable consequences. If only a single sentence is selected, it can be impossible for the summarizer to produce a fusion sentence from it. Moreover, if non-fusible textual units are selected, the summarizer is forced to fuse them into a summary sentence, yielding output summaries that often fail to keep the original meaning intact. Therefore, in this paper we had investigated the correspondences between sentences to gain an understanding of sentence fusion. Establishing correspondenc"
2020.acl-srw.26,W04-1013,0,0.141756,"Missing"
2020.acl-srw.26,P19-1500,0,0.0479364,"e points of correspondence in the sentences—that the journalist is a separate entity from Robert Downey Jr. and that Richards is separate from police officer—then a more accurate summary could have been generated. Related Work Uncovering hidden correspondences between sentences is essential for producing proper summary sentences. A number of recent efforts select important words and sentences from a given document, then let the summarizer attend to selected content to generate a summary (Gehrmann et al., 2018; Hsu et al., 2018; Chen and Bansal, 2018; Putra et al., 2018; Lebanoff et al., 2018; Liu and Lapata, 2019). These systems are largely agnostic to sentence correspondences, which can have two undesirable consequences. If only a single sentence is selected, it can be impossible for the summarizer to produce a fusion sentence from it. Moreover, if non-fusible textual units are selected, the summarizer is forced to fuse them into a summary sentence, yielding output summaries that often fail to keep the original meaning intact. Therefore, in this paper we had investigated the correspondences between sentences to gain an understanding of sentence fusion. Establishing correspondence between sentences goe"
2020.acl-srw.26,N19-1348,0,0.0567723,"een sentences. The dataset fills a notable gap of coreference resolution and summarization research. Our findings shed light on the importance of modeling points of correspondence, suggesting important future directions for sentence fusion. (McKeown et al., 2010) [S1] Palin actually turned against the bridge project only after it became a national symbol of wasteful spending. [S2] Ms. Palin supported the bridge project while running for governor, and abandoned it after it became a national scandal. [Fusion] Palin turned against the bridge project after it became a national scandal. DiscoFuse (Geva et al., 2019) [S1] Melvyn Douglas originally was signed to play Sam Bailey. [S2] The role ultimately went to Walter Pidgeon. [Fusion] Melvyn Douglas originally was signed to play Sam Bailey, but the role ultimately went to Walter Pidgeon. Acknowledgments We are grateful to the anonymous reviewers for their helpful comments and suggestions. This research was supported in part by the National Science Foundation grant IIS-1909603. Points of Correspondence Dataset (Our Work) [S1] The bodies showed signs of torture. [S2] They were left on the side of a highway in Chilpancingo, about an hour north of the tourist"
2020.acl-srw.26,P19-1330,0,0.0358079,"Missing"
2020.acl-srw.26,P14-5010,0,0.00319992,"ask of identifying points of correspondence. Thus, a natural question we ask is how well state-of-theart coreference resolvers can be adapted to this task. If coreference resolvers can perform reasonably well on PoC identification, then these resolvers can be used to extract PoC annotations to potentially enhance sentence fusion. If they perform poorly, coreference performance results can indicate areas of improvement for future work on detecting points of correspondence. In this paper, we compare three coreference resolvers on our dataset, provided by open-source libraries: Stanford CoreNLP (Manning et al., 2014), SpaCy (Honnibal and Montani, 2017), and AllenNLP (Gardner et al., 2017). We base our evaluation on the standard metric used for coreference resolution, B-CUBED algorithm (Bagga and Baldwin, 1998), with some modifications. Each resolver is run on an input pair of sentences to obtain multiple clusters, each representing an entity (e.g., Johnny Kemp) containing multiple mentions (e.g., Johnny Kemp; he; the singer) of that entity. More than one cluster can be detected by the coreference resolver, as additional entities may exist in the given sentence pair (e.g., Johnny Kemp and the police). Simi"
2020.acl-srw.26,W05-1612,0,0.632817,"it. Moreover, if non-fusible textual units are selected, the summarizer is forced to fuse them into a summary sentence, yielding output summaries that often fail to keep the original meaning intact. Therefore, in this paper we had investigated the correspondences between sentences to gain an understanding of sentence fusion. Establishing correspondence between sentences goes beyond finding common words. Humans can fuse sentences sharing few or no common words if they can find other types of correspondence. Fusing such disparate sentences poses a serious challenge for automated fusion systems (Marsi and Krahmer, 2005; Filippova and Strube, 2008; McKeown et al., 2010; Elsner and Santhanam, 2011; Thadani and McKeown, 2013b; Mehdad et al., 2013; Nayeem et al., 2018). These systems rely on common words to derive a connected graph from input sentences or subject-verb-object triples (Moryossef et al., 2019). When there are no common words in sentences, systems tend to break apart. There has been a lack of annotated datasets and guidelines for sentence fusion. Few studies have investigated the types of correspondence between sentences such as entity and event coreference. Evaluating sentence fusion systems requi"
2020.acl-srw.26,P18-1013,0,0.0336924,"used to leave when it was actually Richards. Had the models explicitly recognized the points of correspondence in the sentences—that the journalist is a separate entity from Robert Downey Jr. and that Richards is separate from police officer—then a more accurate summary could have been generated. Related Work Uncovering hidden correspondences between sentences is essential for producing proper summary sentences. A number of recent efforts select important words and sentences from a given document, then let the summarizer attend to selected content to generate a summary (Gehrmann et al., 2018; Hsu et al., 2018; Chen and Bansal, 2018; Putra et al., 2018; Lebanoff et al., 2018; Liu and Lapata, 2019). These systems are largely agnostic to sentence correspondences, which can have two undesirable consequences. If only a single sentence is selected, it can be impossible for the summarizer to produce a fusion sentence from it. Moreover, if non-fusible textual units are selected, the summarizer is forced to fuse them into a summary sentence, yielding output summaries that often fail to keep the original meaning intact. Therefore, in this paper we had investigated the correspondences between sentences to ga"
2020.acl-srw.26,N10-1044,0,0.346869,"ed, the summarizer is forced to fuse them into a summary sentence, yielding output summaries that often fail to keep the original meaning intact. Therefore, in this paper we had investigated the correspondences between sentences to gain an understanding of sentence fusion. Establishing correspondence between sentences goes beyond finding common words. Humans can fuse sentences sharing few or no common words if they can find other types of correspondence. Fusing such disparate sentences poses a serious challenge for automated fusion systems (Marsi and Krahmer, 2005; Filippova and Strube, 2008; McKeown et al., 2010; Elsner and Santhanam, 2011; Thadani and McKeown, 2013b; Mehdad et al., 2013; Nayeem et al., 2018). These systems rely on common words to derive a connected graph from input sentences or subject-verb-object triples (Moryossef et al., 2019). When there are no common words in sentences, systems tend to break apart. There has been a lack of annotated datasets and guidelines for sentence fusion. Few studies have investigated the types of correspondence between sentences such as entity and event coreference. Evaluating sentence fusion systems requires not only novel metrics (Zhao et al., 2019; Zha"
2020.acl-srw.26,D19-1051,0,0.0583459,"Missing"
2020.acl-srw.26,D19-5413,1,0.825439,"able 2. If multiple PoC co-exist in an example, an annotator is expected to label them all; a separate PoC type will be assigned to each PoC occurrence. We are particularly interested in annotating inter-sentence PoC. If entity mentions (“John” and “he”) are found in the same sentence, we do not explicitly label them but assume such intra-sentence referencing can be captured by an existing coreference resolver. Instances of source sentences and summary sentences are obtained from the test and validation splits of the CNN/DailyMail corpus (See et al., 2017) following the procedure described by Lebanoff et al. (2019a). We take a human summary sentence as an anchor point to find two document sentences that are most similar to it based on ROUGE. It becomes an instance containing a pair of source sentences and their summary. The method allows us to identify a large quantity of candidate fusion instances. Annotations are performed in two stages. Stage one removes all spurious pairs that are generated by the heuristic, i.e. a summary sentence that is not a valid fusion of the corresponding two source sentences. Human annotators are given a pair of sentences and a summary sentence and are asked Common-Noun Rep"
2020.acl-srw.26,N19-1236,0,0.0283394,"an understanding of sentence fusion. Establishing correspondence between sentences goes beyond finding common words. Humans can fuse sentences sharing few or no common words if they can find other types of correspondence. Fusing such disparate sentences poses a serious challenge for automated fusion systems (Marsi and Krahmer, 2005; Filippova and Strube, 2008; McKeown et al., 2010; Elsner and Santhanam, 2011; Thadani and McKeown, 2013b; Mehdad et al., 2013; Nayeem et al., 2018). These systems rely on common words to derive a connected graph from input sentences or subject-verb-object triples (Moryossef et al., 2019). When there are no common words in sentences, systems tend to break apart. There has been a lack of annotated datasets and guidelines for sentence fusion. Few studies have investigated the types of correspondence between sentences such as entity and event coreference. Evaluating sentence fusion systems requires not only novel metrics (Zhao et al., 2019; Zhang et al., 2020; Durmus et al., 2020; Wang et al., 2020) but also high-quality ground-truth annotations. It is therefore necessary to conduct a first study to look into cues humans use to establish correspondence between disparate sentences"
2020.acl-srw.26,D18-1206,0,0.0634743,"Missing"
2020.acl-srw.26,C18-1102,0,0.0172749,"ften fail to keep the original meaning intact. Therefore, in this paper we had investigated the correspondences between sentences to gain an understanding of sentence fusion. Establishing correspondence between sentences goes beyond finding common words. Humans can fuse sentences sharing few or no common words if they can find other types of correspondence. Fusing such disparate sentences poses a serious challenge for automated fusion systems (Marsi and Krahmer, 2005; Filippova and Strube, 2008; McKeown et al., 2010; Elsner and Santhanam, 2011; Thadani and McKeown, 2013b; Mehdad et al., 2013; Nayeem et al., 2018). These systems rely on common words to derive a connected graph from input sentences or subject-verb-object triples (Moryossef et al., 2019). When there are no common words in sentences, systems tend to break apart. There has been a lack of annotated datasets and guidelines for sentence fusion. Few studies have investigated the types of correspondence between sentences such as entity and event coreference. Evaluating sentence fusion systems requires not only novel metrics (Zhao et al., 2019; Zhang et al., 2020; Durmus et al., 2020; Wang et al., 2020) but also high-quality ground-truth annotat"
2020.acl-srw.26,P17-1099,0,0.479974,"m, 2011). A major challenge in fusing sentences is to establish correspondence between sentences. If there exists no correspondence, it would be difficult, if not impossible, to fuse sentences. In Table 1, we present example source and fusion sentences, where the summarizer attempts to merge two sentences into a summary sentence with improper use of points of correspondence. In this paper, we seek to uncover hidden correspondences between sen1 https://github.com/ucfnlp/ points-of-correspondence Table 1: Unfaithful summary sentences generated by neural abstractive summarizers, in-house and PG (See et al., 2017). They attempt to merge two sentences into one sentence with improper use of points of correspondence between sentences, yielding nonsensical output. Summaries are manually re-cased for readability. tences, which has a great potential for improving content selection and deep sentence fusion. Sentence fusion (or multi-sentence compression) plays a prominent role in automated summarization and its importance has long been recognized (Barzilay et al., 1999). Early attempts to fuse sentences build a dependency graph from sentences, then decode a tree from the graph using integer linear programming"
2020.acl-srw.26,W13-3508,0,0.187253,"es, yielding nonsensical output. Summaries are manually re-cased for readability. tences, which has a great potential for improving content selection and deep sentence fusion. Sentence fusion (or multi-sentence compression) plays a prominent role in automated summarization and its importance has long been recognized (Barzilay et al., 1999). Early attempts to fuse sentences build a dependency graph from sentences, then decode a tree from the graph using integer linear programming, finally linearize the tree to generate a summary sentence (Barzilay and McKeown, 2005; Filippova and Strube, 2008; Thadani and McKeown, 2013a). Despite valuable insights gained from 191 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 191–198 c July 5 - July 10, 2020. 2020 Association for Computational Linguistics PoC Type Source Sentences Summary Sentence Pronominal Referencing [S1] The bodies showed signs of torture. [S2] They were left on the side of a highway in Chilpancingo, about an hour north of the tourist resort of Acapulco in the state of Guerrero. [S1] Bahamian R&B singer Johnny Kemp , best known for the 1988 party anthem “Just Got Paid,” died this"
2020.acl-srw.26,I13-1198,0,0.554407,"es, yielding nonsensical output. Summaries are manually re-cased for readability. tences, which has a great potential for improving content selection and deep sentence fusion. Sentence fusion (or multi-sentence compression) plays a prominent role in automated summarization and its importance has long been recognized (Barzilay et al., 1999). Early attempts to fuse sentences build a dependency graph from sentences, then decode a tree from the graph using integer linear programming, finally linearize the tree to generate a summary sentence (Barzilay and McKeown, 2005; Filippova and Strube, 2008; Thadani and McKeown, 2013a). Despite valuable insights gained from 191 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 191–198 c July 5 - July 10, 2020. 2020 Association for Computational Linguistics PoC Type Source Sentences Summary Sentence Pronominal Referencing [S1] The bodies showed signs of torture. [S2] They were left on the side of a highway in Chilpancingo, about an hour north of the tourist resort of Acapulco in the state of Guerrero. [S1] Bahamian R&B singer Johnny Kemp , best known for the 1988 party anthem “Just Got Paid,” died this"
2020.acl-srw.26,2020.acl-main.450,0,0.0398594,"and McKeown, 2013b; Mehdad et al., 2013; Nayeem et al., 2018). These systems rely on common words to derive a connected graph from input sentences or subject-verb-object triples (Moryossef et al., 2019). When there are no common words in sentences, systems tend to break apart. There has been a lack of annotated datasets and guidelines for sentence fusion. Few studies have investigated the types of correspondence between sentences such as entity and event coreference. Evaluating sentence fusion systems requires not only novel metrics (Zhao et al., 2019; Zhang et al., 2020; Durmus et al., 2020; Wang et al., 2020) but also high-quality ground-truth annotations. It is therefore necessary to conduct a first study to look into cues humans use to establish correspondence between disparate sentences. We envision sentence correspondence to be related to text cohesion and coherence, which help establish correspondences between two pieces of text. Halliday and Hasan (1976) describe text cohesion as cohesive devices that tie two textual elements together. They identify five categories of cohesion: 195 human annotations of points of correspondence between sentences. The dataset fills a notable gap of coreference"
2020.acl-srw.26,D19-1053,1,0.846561,"; McKeown et al., 2010; Elsner and Santhanam, 2011; Thadani and McKeown, 2013b; Mehdad et al., 2013; Nayeem et al., 2018). These systems rely on common words to derive a connected graph from input sentences or subject-verb-object triples (Moryossef et al., 2019). When there are no common words in sentences, systems tend to break apart. There has been a lack of annotated datasets and guidelines for sentence fusion. Few studies have investigated the types of correspondence between sentences such as entity and event coreference. Evaluating sentence fusion systems requires not only novel metrics (Zhao et al., 2019; Zhang et al., 2020; Durmus et al., 2020; Wang et al., 2020) but also high-quality ground-truth annotations. It is therefore necessary to conduct a first study to look into cues humans use to establish correspondence between disparate sentences. We envision sentence correspondence to be related to text cohesion and coherence, which help establish correspondences between two pieces of text. Halliday and Hasan (1976) describe text cohesion as cohesive devices that tie two textual elements together. They identify five categories of cohesion: 195 human annotations of points of correspondence betw"
2020.coling-main.292,W15-3822,0,0.452357,"plications including slotfilling (Veyseh et al., 2020), definition extraction (Kang et al., 2020; Veyseh et al., 2020), and question answering (Ackermann et al., 2020; Veyseh, 2016) Over the past two decades, several approaches and resources have been proposed to solve the two subtasks for acronyms. These approaches extend from rule-based methods for AI and feature-based models for AD (i.e., SVM and Naive Bayes) (Schwartz and Hearst, 2002; Nadeau and Turney, 2005; Okazaki and Ananiadou, 2006; Yu et al., 2007) to the recent deep learning methods (Li et al., 2015; Charbonnier and Wartena, 2018; Wu et al., 2015; Ciosici et al., 2019; Jin et al., 2019; Li et al., 2019). While the prior work has made substantial progress on this task by providing new approaches and datasets, there are some limitations in the existing datasets which hinder further improvement. First, most of the existing datasets for AI are either limited in their sizes or created using simple rule-based methods (i.e., not humnanannotated). For instance, Ciosici (2019) exploits the rules proposed by Schwartz (2002) to generate a corpus for acronym disambiguation from Wikipedia. This is unfortunate as rules are in general not able to ca"
2020.coling-main.456,N19-1423,0,0.0187883,"n. Our model achieves state-of-the-art performance on two popular question answering datasets (i.e. TrecQA and WikiQA). Via further analysis, we show that this model can reliably trace the errors it has made in the validation step to the training instances that might have caused these errors. We believe that this error-tracing capability provides significant benefit in improving dataset quality in many applications. 1 Introduction Interpretability of neural networks is an active research field in machine learning. Deep neural networks might have tens if not hundreds of millions of parameters (Devlin et al., 2019; Liu et al., 2019a) organized into intricate architectures. The sheer amount of parameters and the complexity of the architectures largely prevent human to directly make sense of which concepts and how the network truly learns. The comparative lack of explainable intuition behind deep neural networks might hamper the development and adoption of those models. In certain scenarios, prediction accuracy alone is not sufficient (Caruana et al., 2015; Lapuschkin et al., 2019). For example, as discussed in (Zhang et al., 2018b; Zhang et al., 2018a), it is difficult to trust a deep model even if it h"
2020.coling-main.456,C18-1181,1,0.847203,"cond, we show the utility of the error-tracing capability of our model to find the noisy instances in the training data that degrades the performance on the validation data. This capability might be very useful in real-life machine learning scenarios where the training labels are noisy or the inter-annotator agreement is low. 2 Proposed Framework Question answering (or answer selection) is the task of identifying the correct answer to a question from a pool of candidate answers. It is an active research problem with applications in many areas (Tay et al., 2018a; Tayyar Madabushi et al., 2018; Lai et al., 2018; Rao et al., 2019; Lai et al., 2020). Similar to most recent papers on this topic (Tay et al., 2018b; Lai et al., 2019; Garg et al., 2020), we cast the question answering problem as a binary classification problem by concatenating the question with each of the candidate answers and assigning positive label to the concatenation containing the correct answer. In most supervised learning scenarios, performing a full distance calculation between the current data point and every training data point would be computationally intractable. To overcome this burden, we propose a memory controller based"
2020.coling-main.456,D19-1610,1,0.910586,"that degrades the performance on the validation data. This capability might be very useful in real-life machine learning scenarios where the training labels are noisy or the inter-annotator agreement is low. 2 Proposed Framework Question answering (or answer selection) is the task of identifying the correct answer to a question from a pool of candidate answers. It is an active research problem with applications in many areas (Tay et al., 2018a; Tayyar Madabushi et al., 2018; Lai et al., 2018; Rao et al., 2019; Lai et al., 2020). Similar to most recent papers on this topic (Tay et al., 2018b; Lai et al., 2019; Garg et al., 2020), we cast the question answering problem as a binary classification problem by concatenating the question with each of the candidate answers and assigning positive label to the concatenation containing the correct answer. In most supervised learning scenarios, performing a full distance calculation between the current data point and every training data point would be computationally intractable. To overcome this burden, we propose a memory controller based on NTM to summarize the dataset into meta-evidence nodes. Similar to NTM, the controller is characterized by reading an"
2020.coling-main.456,2020.aacl-demo.3,1,0.696459,"or-tracing capability of our model to find the noisy instances in the training data that degrades the performance on the validation data. This capability might be very useful in real-life machine learning scenarios where the training labels are noisy or the inter-annotator agreement is low. 2 Proposed Framework Question answering (or answer selection) is the task of identifying the correct answer to a question from a pool of candidate answers. It is an active research problem with applications in many areas (Tay et al., 2018a; Tayyar Madabushi et al., 2018; Lai et al., 2018; Rao et al., 2019; Lai et al., 2020). Similar to most recent papers on this topic (Tay et al., 2018b; Lai et al., 2019; Garg et al., 2020), we cast the question answering problem as a binary classification problem by concatenating the question with each of the candidate answers and assigning positive label to the concatenation containing the correct answer. In most supervised learning scenarios, performing a full distance calculation between the current data point and every training data point would be computationally intractable. To overcome this burden, we propose a memory controller based on NTM to summarize the dataset into"
2020.coling-main.456,2020.lrec-1.676,0,0.746655,"Missing"
2020.coling-main.456,D19-1540,0,0.012851,"utility of the error-tracing capability of our model to find the noisy instances in the training data that degrades the performance on the validation data. This capability might be very useful in real-life machine learning scenarios where the training labels are noisy or the inter-annotator agreement is low. 2 Proposed Framework Question answering (or answer selection) is the task of identifying the correct answer to a question from a pool of candidate answers. It is an active research problem with applications in many areas (Tay et al., 2018a; Tayyar Madabushi et al., 2018; Lai et al., 2018; Rao et al., 2019; Lai et al., 2020). Similar to most recent papers on this topic (Tay et al., 2018b; Lai et al., 2019; Garg et al., 2020), we cast the question answering problem as a binary classification problem by concatenating the question with each of the candidate answers and assigning positive label to the concatenation containing the correct answer. In most supervised learning scenarios, performing a full distance calculation between the current data point and every training data point would be computationally intractable. To overcome this burden, we propose a memory controller based on NTM to summariz"
2020.coling-main.456,J00-3003,0,0.521538,"Missing"
2020.coling-main.456,C18-1278,0,0.228612,"r selection datasets. Second, we show the utility of the error-tracing capability of our model to find the noisy instances in the training data that degrades the performance on the validation data. This capability might be very useful in real-life machine learning scenarios where the training labels are noisy or the inter-annotator agreement is low. 2 Proposed Framework Question answering (or answer selection) is the task of identifying the correct answer to a question from a pool of candidate answers. It is an active research problem with applications in many areas (Tay et al., 2018a; Tayyar Madabushi et al., 2018; Lai et al., 2018; Rao et al., 2019; Lai et al., 2020). Similar to most recent papers on this topic (Tay et al., 2018b; Lai et al., 2019; Garg et al., 2020), we cast the question answering problem as a binary classification problem by concatenating the question with each of the candidate answers and assigning positive label to the concatenation containing the correct answer. In most supervised learning scenarios, performing a full distance calculation between the current data point and every training data point would be computationally intractable. To overcome this burden, we propose a memory"
2020.coling-main.456,N18-1115,1,0.806227,"Missing"
2020.coling-main.456,D07-1003,0,0.133511,"echanisms are similar to the memory network, which is simpler than the NTM. However, the memory network only stores intermediate computation steps in the memory, and these memories can be considered as internal layers of the network. Our memory, on the contrary, is external and not trained, only updated by the writing mechanism. In this regard, the memory bank of our model is more similar to the NTM. 3 3.1 Experimental Results Question answering performance In this subsection, we present our core results on two most popular datasets for answer selection: WikiQA (Yang et al., 2015) and TrecQA (Wang et al., 2007). Due to space constraint, details of these datasets are described in the Appendix. Similar to previous work, we use two standard measures for the task: mean average precision (MAP) and mean reciprocal rank (MRR). Our models make use of the RoBERTa contextual embedding (Liu et al., 2019b), pretrained on the ANSQ dataset (Garg et al., 2020). For our model, we vary the number of memory cells from 2 to 64. The base configuration with 2 memory cells mimics the prototypical network with one cell for each prototype class representation. Table 1 shows our 5207 TrecQA Model RoBERTa + Evidence Memory ("
2020.coling-main.456,D15-1237,0,0.104262,"rithms in writing and reading mechanisms are similar to the memory network, which is simpler than the NTM. However, the memory network only stores intermediate computation steps in the memory, and these memories can be considered as internal layers of the network. Our memory, on the contrary, is external and not trained, only updated by the writing mechanism. In this regard, the memory bank of our model is more similar to the NTM. 3 3.1 Experimental Results Question answering performance In this subsection, we present our core results on two most popular datasets for answer selection: WikiQA (Yang et al., 2015) and TrecQA (Wang et al., 2007). Due to space constraint, details of these datasets are described in the Appendix. Similar to previous work, we use two standard measures for the task: mean average precision (MAP) and mean reciprocal rank (MRR). Our models make use of the RoBERTa contextual embedding (Liu et al., 2019b), pretrained on the ANSQ dataset (Garg et al., 2020). For our model, we vary the number of memory cells from 2 to 64. The base configuration with 2 memory cells mimics the prototypical network with one cell for each prototype class representation. Table 1 shows our 5207 TrecQA Mo"
2020.emnlp-main.274,P17-2090,0,0.0485458,"Missing"
2020.emnlp-main.274,D17-1206,0,0.0268247,"in the same dialog state tracker three times with different seeds. We compare the aggregated results from all nine trials with the baseline results. Ultimately, we repeat this procedure for all combinations of state trackers and datasets. For non-augmented baselines, we repeat the experiments ten times. Implementation Details. The hidden size of dialog vectors is 1000, and the hidden size of utterance, dialog act specification, turn state, and turn goal representations is 500. The dimensionality for latent variables is between 100 and 200. We use GloVe (Pennington et al., 2014) and character (Hashimoto et al., 2017) embeddings as pre-trained word emebddings (400 dimensions total) for word and dialog act tokens. All models used Adam optimizer (Kingma and Ba, 2014) with the initial learning rate of 1e-3, We annealed the KL-divergence weights over 250,000 training steps. For data synthesis, we employ ancestral sampling to generate samples from the empirical posterior distribution. We fixed the ratio of synthetic to original data samples to 1. Datasets. We conduct experiments on four state tracking corpora: WoZ2.0 (Wen et al., 2017), DSTC2 (Henderson et al., 2014), MultiWoZ (Budzianowski et al., 2018), and D"
2020.emnlp-main.274,W14-4337,0,0.481331,"iscovery of novel class-preserving samples to machine learning. In this work, we explore GDA in the context of dialog modeling and contextual understanding. Goal-oriented dialogs occur between a user and a system that communicates verbally to accomplish the user’s goals (Table 6). However, because the user’s goals and the system’s possible actions are not transparent to each other, both parties must rely on verbal communications to infer and take appropriate actions to resolve the goals. Dialog state tracker is a core component of such systems, enabling it to track the dialog’s latest status (Henderson et al., 2014). A dialog state typically consists of inform and request types of slot values. 3406 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 3406–3425, c November 16–20, 2020. 2020 Association for Computational Linguistics For example, a user may verbally refer to a previously mentioned food type as the preferred one - e.g., Asian (inform(food=asian)). Given the user utterance and historical turns, the state tracker must infer the user’s current goals. As such, we can view dialog state tracking as a sparse sequential multi-class classification problem. Mod"
2020.emnlp-main.274,K16-1002,0,0.0131797,"(r) (r) (g) (g) (g) (s) (s) (s) (u) (u) (u) pθ (rt |ht , z(c) , zt ) = D θ (ht , z(c) , zt ) pθ (gt |ht , . . . , zt ) = D θ (ht , . . . , zt ) pθ (st |ht , . . . , zt ) = D θ (ht , . . . , zt ) Inference collapse is a relatively common phenomenon among autoregressive VAE structures (Zhao et al., 2017). The hierarchical and recurrent nature of our model makes it especially vulnerable. The standard treatment for alleviating the inference collapse problem include (1) annealing the KL-divergence term weight during the initial training stage and (2) employing word dropouts on the decoder inputs (Bowman et al., 2016). For our model, we observe that the basic techniques are insufficient (Table 3). While more recent treatments exist (Kim et al., 2018; He et al., 2019), they incur high computational costs that prohibit practical deployment in our cases. We introduce two simpler but effective methods to prevent encoder degeneration. Mutual Information Maximization. The KLdivergence term in the standard VAE ELBO can be decomposed to reveal the mutual information term (Hoffman and Johnson, 2016): pθ (ut |ht , . . . , zt ) = D θ (ht , . . . , zt ). Epd [DKL (qφ (z |x)kp(z))] = The utterance decoder D (u) is impl"
2020.emnlp-main.274,W04-1013,0,0.0100549,"81 0.153 0.162 0.154 1 User Ours 3 User W O Z2.0 a VHUS VHDAb VHDAb a W/O GT (G¨ur et al., 2018) E NT ACC E NT 0.322 0.408 0.460 0.056 0.079 0.080 0.367 0.460 0.554 0.024 0.034 0.043 b 5 User Ours 7 User dropouts while generating more coherent samples (evident from higher data augmentation results). Language Evaluation To understand the effect of joint learning of various dialog features on language generation, we compare our model with a model that only learns linguistic features. Following the evaluation protocol from prior work (Wen et al., 2017; Bak and Oh, 2019), we use ROUGE-L F1-score (Lin, 2004) to evaluate the linguistic quality and utterance-level unigram cross-entropy (Serban et al., 2017) (regarding the training corpus distribution) to evaluate diversity. Table 4 shows that our model generates better and more diverse utterances than the previous strong baseline on conversation modeling. These results supports the idea that joint learning of dialog annotations improves utterance generation, thereby increasing the chance of generating novel samples that improve the downstream trackers. 4.4 User Simulation Evaluation Simulating human participants has become a crucial feature for tra"
2020.emnlp-main.274,W18-4701,1,0.844142,"gs as pre-trained word emebddings (400 dimensions total) for word and dialog act tokens. All models used Adam optimizer (Kingma and Ba, 2014) with the initial learning rate of 1e-3, We annealed the KL-divergence weights over 250,000 training steps. For data synthesis, we employ ancestral sampling to generate samples from the empirical posterior distribution. We fixed the ratio of synthetic to original data samples to 1. Datasets. We conduct experiments on four state tracking corpora: WoZ2.0 (Wen et al., 2017), DSTC2 (Henderson et al., 2014), MultiWoZ (Budzianowski et al., 2018), and DialEdit (Manuvinakurike et al., 2018). These corpora cover a variety of domains (restaurant booking, hotel reservation, and image editing). Note that, because the MultiWoZ dataset is a multi-domain corpus, we extract single-domain dialog samples from the two most prominent domains (hotel and restaurant, denoted by MultiWoZ-H and MultiWoZ-R, respectively). Dialog State Trackers. We use GLAD and GCE as the two competitive baselines for state tracking. Besides, modifications are applied to these trackers to stabilize the performance on random seeds (denoted as GLAD+ and GCE+ ). Specifically, we enrich the word embeddings with subwor"
2020.emnlp-main.274,W14-4340,0,\N,Missing
2020.emnlp-main.274,D14-1162,0,\N,Missing
2020.emnlp-main.274,Q17-1010,0,\N,Missing
2020.emnlp-main.274,P18-1135,0,\N,Missing
2020.emnlp-main.274,P18-1133,0,\N,Missing
2020.emnlp-main.274,N18-1162,0,\N,Missing
2020.emnlp-main.274,D19-1202,0,\N,Missing
2020.emnlp-main.274,D19-1670,0,\N,Missing
2020.emnlp-main.274,W13-4067,0,\N,Missing
2020.emnlp-main.274,N07-2038,0,\N,Missing
2020.emnlp-main.274,P17-1163,0,\N,Missing
2020.emnlp-main.338,P99-1071,0,0.588256,"Missing"
2020.emnlp-main.338,P18-1063,0,0.042947,"Missing"
2020.emnlp-main.338,W19-4828,0,0.126475,"ake effective use of these expressions to establish correspondence between sentences, often leading to ungrammatical and nonsensical outputs. 2.1 Transformer with Linking It is advantageous for a Transformer model to make use of PoC information for sentence fusion. While Transformer-based pretrained models have had considerable success (Devlin et al., 2019; Dong et al., 2019; Lewis et al., 2020), they primarily feature pairwise relationships between tokens, but not PoC mentions, which are are text chunks of varying size. Only to a limited extent do these models embed knowledge of coreference (Clark et al., 2019), and there is a growing need for incorporating PoC linkages explicitly in a Transformer model to enhance its ability to perform sentence fusion. We propose to enrich Transformer’s source sequence with markups that indicate PoC linkages. Here PoC information is assumed to be available for any fusion instance (details in §3). We introduce special tokens ([Sk ] and [Ek ]) to mark the start and 1 end of each PoC mention; all mentions pertaining to the k-th PoC share the same start/end tokens. An example is illustrated in Figure 1, where Allan Donald and The 48-year-old former Test paceman are enr"
2020.emnlp-main.338,W04-1016,0,0.229077,"Missing"
2020.emnlp-main.338,N19-1423,0,0.0292351,"h Africa bowling coach vs. part of the coaching team are two PoCs. The use of alternative expressions for conveying the same meanings is standard practice in writing, as it increases lexical variety and reduces redundancy. However, existing summarizers cannot make effective use of these expressions to establish correspondence between sentences, often leading to ungrammatical and nonsensical outputs. 2.1 Transformer with Linking It is advantageous for a Transformer model to make use of PoC information for sentence fusion. While Transformer-based pretrained models have had considerable success (Devlin et al., 2019; Dong et al., 2019; Lewis et al., 2020), they primarily feature pairwise relationships between tokens, but not PoC mentions, which are are text chunks of varying size. Only to a limited extent do these models embed knowledge of coreference (Clark et al., 2019), and there is a growing need for incorporating PoC linkages explicitly in a Transformer model to enhance its ability to perform sentence fusion. We propose to enrich Transformer’s source sequence with markups that indicate PoC linkages. Here PoC information is assumed to be available for any fusion instance (details in §3). We introduce"
2020.emnlp-main.338,2020.acl-main.454,0,0.0104413,"en compared to fusions that reuse the source text. Our results call for a System Pointer-Generator Transformer Trans-S HARE R EPR Reference Extractiveness Truthful. 1-gram 2-gram 3-gram 63.6 71.7 70.9 67.2 97.5 91.9 92.0 72.0 83.1 68.6 70.1 34.9 72.8 54.2 56.4 20.9 Table 3: Fusion sentences are evaluated by their level of truthfulness and extractivenss. Our system fusions attain a high level of truthfulness with moderate extractivenss. reexamination of sentence fusion using better evaluation metrics including semantics and questionanswering-based metrics (Zhao et al., 2019; Wang et al., 2020; Durmus et al., 2020). 4 Conclusion We address the challenge of information fusion in the context of neural abstractive summarization by making crucial use of points of correspondence between sentences. We enrich Transformers with PoC information and report model performance on a new test bed for information fusion. Our findings suggest that modeling points of correspondence is crucial for effective sentence fusion, and sentence fusion remains a challenging direction of research. Future work may explore the use of points of correspondence and sentence fusion in the standard setting of document summarization. Perfo"
2020.emnlp-main.338,W11-1607,0,0.44119,"Missing"
2020.emnlp-main.338,P19-1213,0,0.0319973,"Missing"
2020.emnlp-main.338,D08-1019,0,0.305742,"Missing"
2020.emnlp-main.338,D18-1443,0,0.0752772,"Missing"
2020.emnlp-main.338,N19-1348,0,0.122116,"Missing"
2020.emnlp-main.338,J95-2003,0,0.0428999,"e shifting distance, allowing the model attention to shift from “John” to the tokens “[E]” then to “loves” for predicting the next summary word. 2.2 Transformer with Shared Representation We explore an alternative method to allow mentions of the same PoC to be connected with each other. Particularly, we direct one attention head to focus on tokens belonging to the same PoC, allowing these tokens to share semantic representations, similar to Strubell et al. (2018). Sharing representation is meaningful as these mentions are related by complex morpho-syntactic, syntactic or semantic constraints (Grosz et al., 1995). Let z={z1 , . . . , z|z |} be a sequence containing PoC information, where zi ∈ {0, . . . , K} indicates the index of PoC to which the token xi belongs. zi =0 indicates xi is not associated with any PoC. Our T RANS -S HARE R EPR model selects an attention head h from the l-th layer of the Transformer model. The attention head h governs tokens that belong to PoCs (zi 6= 0). Its hidden representation hli is computed by modeling only pairwise relationships between token i and any token j of the same PoC (zi = zj ; Eq. (3)), while other tokens 21.0 ROUGE-2 We fine-tune the model on a sentence fu"
2020.emnlp-main.338,2020.acl-main.453,0,0.0280677,"Missing"
2020.emnlp-main.338,D19-5413,1,0.908158,"Missing"
2020.emnlp-main.338,2020.acl-srw.26,1,0.833527,"Missing"
2020.emnlp-main.338,2020.acl-main.703,0,0.0296695,"oaching team are two PoCs. The use of alternative expressions for conveying the same meanings is standard practice in writing, as it increases lexical variety and reduces redundancy. However, existing summarizers cannot make effective use of these expressions to establish correspondence between sentences, often leading to ungrammatical and nonsensical outputs. 2.1 Transformer with Linking It is advantageous for a Transformer model to make use of PoC information for sentence fusion. While Transformer-based pretrained models have had considerable success (Devlin et al., 2019; Dong et al., 2019; Lewis et al., 2020), they primarily feature pairwise relationships between tokens, but not PoC mentions, which are are text chunks of varying size. Only to a limited extent do these models embed knowledge of coreference (Clark et al., 2019), and there is a growing need for incorporating PoC linkages explicitly in a Transformer model to enhance its ability to perform sentence fusion. We propose to enrich Transformer’s source sequence with markups that indicate PoC linkages. Here PoC information is assumed to be available for any fusion instance (details in §3). We introduce special tokens ([Sk ] and [Ek ]) to mar"
2020.emnlp-main.338,W04-1013,0,0.052362,"stigation into the situation in Palestinian territories. Reference: Israel and the United States opposed the move, which could open the door to war crimes investigations against Israelis. Table 2: Example output of sentence fusion systems. PG only performs sentence shortening rather than fusion. Transformer fails to retain the original meaning and Transformer-S HARE R EPR performs best. Reference demonstrates a high level of abstraction. Sentences are manually de-tokenized for readability. We compare system outputs and references using a number of automatic evaluation metrics including ROUGE (Lin, 2004), BLEU (Papineni et al., 2002) and BERTScore (Zhang et al., 2020). Results are presented in Table 1. We observe that all Transformer models outperform PG, suggesting that these models can benefit substantially from unsupervised pretraining on a large corpus of text. On the heuristic test set where training and testing conditions match (they both use automatically identified PoC), T RANS -L INKING performs better than T RANS -S HARE R EPR, and vice versa on the final test set. We conjecture that this is because the linking model has a stronger requirement on PoC boundaries and the training/test"
2020.emnlp-main.338,P19-1500,0,0.036562,"Missing"
2020.emnlp-main.338,W05-1612,0,0.579999,"Missing"
2020.emnlp-main.338,2020.acl-main.450,0,0.0116104,"hful, especially when compared to fusions that reuse the source text. Our results call for a System Pointer-Generator Transformer Trans-S HARE R EPR Reference Extractiveness Truthful. 1-gram 2-gram 3-gram 63.6 71.7 70.9 67.2 97.5 91.9 92.0 72.0 83.1 68.6 70.1 34.9 72.8 54.2 56.4 20.9 Table 3: Fusion sentences are evaluated by their level of truthfulness and extractivenss. Our system fusions attain a high level of truthfulness with moderate extractivenss. reexamination of sentence fusion using better evaluation metrics including semantics and questionanswering-based metrics (Zhao et al., 2019; Wang et al., 2020; Durmus et al., 2020). 4 Conclusion We address the challenge of information fusion in the context of neural abstractive summarization by making crucial use of points of correspondence between sentences. We enrich Transformers with PoC information and report model performance on a new test bed for information fusion. Our findings suggest that modeling points of correspondence is crucial for effective sentence fusion, and sentence fusion remains a challenging direction of research. Future work may explore the use of points of correspondence and sentence fusion in the standard setting of documen"
2020.emnlp-main.338,W13-2117,0,0.268015,"Missing"
2020.emnlp-main.338,D18-1206,0,0.0800122,"Missing"
2020.emnlp-main.338,P02-1040,0,0.107704,"situation in Palestinian territories. Reference: Israel and the United States opposed the move, which could open the door to war crimes investigations against Israelis. Table 2: Example output of sentence fusion systems. PG only performs sentence shortening rather than fusion. Transformer fails to retain the original meaning and Transformer-S HARE R EPR performs best. Reference demonstrates a high level of abstraction. Sentences are manually de-tokenized for readability. We compare system outputs and references using a number of automatic evaluation metrics including ROUGE (Lin, 2004), BLEU (Papineni et al., 2002) and BERTScore (Zhang et al., 2020). Results are presented in Table 1. We observe that all Transformer models outperform PG, suggesting that these models can benefit substantially from unsupervised pretraining on a large corpus of text. On the heuristic test set where training and testing conditions match (they both use automatically identified PoC), T RANS -L INKING performs better than T RANS -S HARE R EPR, and vice versa on the final test set. We conjecture that this is because the linking model has a stronger requirement on PoC boundaries and the training/testing conditions must match for"
2020.emnlp-main.338,D19-1053,1,0.839589,"e that as less truthful, especially when compared to fusions that reuse the source text. Our results call for a System Pointer-Generator Transformer Trans-S HARE R EPR Reference Extractiveness Truthful. 1-gram 2-gram 3-gram 63.6 71.7 70.9 67.2 97.5 91.9 92.0 72.0 83.1 68.6 70.1 34.9 72.8 54.2 56.4 20.9 Table 3: Fusion sentences are evaluated by their level of truthfulness and extractivenss. Our system fusions attain a high level of truthfulness with moderate extractivenss. reexamination of sentence fusion using better evaluation metrics including semantics and questionanswering-based metrics (Zhao et al., 2019; Wang et al., 2020; Durmus et al., 2020). 4 Conclusion We address the challenge of information fusion in the context of neural abstractive summarization by making crucial use of points of correspondence between sentences. We enrich Transformers with PoC information and report model performance on a new test bed for information fusion. Our findings suggest that modeling points of correspondence is crucial for effective sentence fusion, and sentence fusion remains a challenging direction of research. Future work may explore the use of points of correspondence and sentence fusion in the standard"
2020.emnlp-main.338,P17-1099,0,0.0383353,"-stopword tokens from each sentence that do not already exist in the other sentence. to find two document sentences that are most similar to it, which forms a fusion instance containing a pair of source sentences and their summary. PoCs have been annotated based on Halliday and Hasan’s theory of cohesion (1976) for 1,494 fusion instances, taken from 1,174 documents in the test and valid splits of CNN/DM with a moderate to high inter-annotator agreement (0.58). Automatic Evaluation We proceed by investigating the effectiveness of various sentence fusion models, including (a) Pointer-Generator (See et al., 2017) that employs an encoder-decoder architecture to condense input sentences to a vector representation, then decode it into a fusion sentence. (b) Transformer, our baseline Transformer architecture w/o PoC information. It is a strong baseline that resembles the UniLM model described in (Dong et al., 2019). (c) T RANS -L INKING uses special tokens to mark the boundaries of PoC mentions (§2.1). (d) T RANS -S HARE R EPR allows tokens of the same PoC to share representations (§2.2). All Transformer models are initialized with BERT-BASE parameters and are fine-tuned using UniLM’s sequenceto-sequence"
2020.emnlp-main.338,D18-1548,0,0.0524907,"Missing"
2020.emnlp-main.338,I13-1198,0,0.287394,"Missing"
2020.emnlp-main.719,N19-1423,0,0.0292875,"ax-Model Consistency, (iii) Graph Convolutional Neural Networks, and (iv) Representation Regularization. 3.1 sition embedding table (initialized randomly). The position embeddings are fine-tuned during training in this work. The resulting vector sequence X = x1 , x2 , . . . , xN for W will be then sent to the next computation step. Sentence Encoding In order to represent the input sentence W , we encode each word wi into a real-valued vector xi based on the concatenation of the two following vectors: (1) the hidden vector of the first wordpiece of wi from the last layer of the BERTbase model (Devlin et al., 2019), and (2) the position embedding for wi . For this vector, we first compute the relative distance di from wi to the target word wt (i.e., ri = i − t). Afterward, we retrieve the position embedding for wi by looking up ri in a poP exp(−dsyn ) i syn . ) j=1..N exp(−dj In order to implement the possibility score consistency, our deep learning model needs to produce syn syn ssyn 1 , s2 , . . . , sN as the model-based possibility scores the words w1 , w2 , . . . , wN in W respectively. While the model-based score computation would be explained later, given the model-based scores, the syntax-model c"
2020.emnlp-main.719,N19-1259,0,0.772959,"disappointing.”, “disappointing” is the opinion word for the target word “warranties” while the opinion words for the target word “company” would involve “reputable”. Among others, TOWE finds its applications in target-oriented sentiment analysis (Tang et al., 2016; Xue and Li, 2018; Veyseh et al., 2020) and opinion summarization (Wu et al., 2020). Equal contribution. honored XYZ Introduction ∗ are The early approach for TOWE has involved the rule-based and lexicon-based methods (Hu and Liu, 2004; Zhuang et al., 2006) while the recent work has focused on deep learning models for this problem (Fan et al., 2019; Wu et al., 2020). One of the insights from the rule-based methods is that the syntactic structures (i.e., the parsing trees) of the sentences can provide useful information to improve the performance for TOWE (Zhuang et al., 2006). However, these syntactic structures have not been exploited in the current deep learning models for TOWE (Fan et al., 2019; Wu et al., 2020). Consequently, in this work, we seek to fill in this gap by extracting useful knowledge from the syntactic structures to help the deep learning models learn better representations for TOWE. In particular, based on the depende"
2020.emnlp-main.719,D17-1310,0,0.136145,"). A key difference between OWE and TOWE is that OWE does not require the opinion words to tie to any target words 8948 in the sentence while the opinion words in TOWE should be explicitly paired with a given target word. Another related task for TOWE is opinion target extraction (OTE) that attempts to identify the target words in the sentences (Qiu et al., 2011; Liu et al., 2015; Poria et al., 2016; Yin et al., 2016; Xu et al., 2018). Note that some previous works have also attempted to jointly predict the target and opinion words (Qiu et al., 2011; Liu et al., 2013; Wang et al., 2016, 2017; Li and Lam, 2017); however, the target words are still not paired with their corresponding opinion words in these studies. As mentioned in the introduction, among a few previous work on TOWE, the main approaches include the rule-based methods (i.e., based on word distances or syntactic patterns) (Zhuang et al., 2006; Hu and Liu, 2004) and the recent deep learning models (Fan et al., 2019; Wu et al., 2020). Our model is different from the previous deep learning models as we exploit the syntactic information (i.e., dependency trees) for TOWE with deep learning. 3 3.2 Syntax-Model Consistency As presented in the"
2020.emnlp-main.719,D15-1168,0,0.130936,"n relatively less explored in the literature. In particular, the most related task of TOWE is opinion word extraction (OWE) that aims to locate the terms used to express attitude in the sentences (Htay and Lynn, 2013; Shamshurin, 2012). A key difference between OWE and TOWE is that OWE does not require the opinion words to tie to any target words 8948 in the sentence while the opinion words in TOWE should be explicitly paired with a given target word. Another related task for TOWE is opinion target extraction (OTE) that attempts to identify the target words in the sentences (Qiu et al., 2011; Liu et al., 2015; Poria et al., 2016; Yin et al., 2016; Xu et al., 2018). Note that some previous works have also attempted to jointly predict the target and opinion words (Qiu et al., 2011; Liu et al., 2013; Wang et al., 2016, 2017; Li and Lam, 2017); however, the target words are still not paired with their corresponding opinion words in these studies. As mentioned in the introduction, among a few previous work on TOWE, the main approaches include the rule-based methods (i.e., based on word distances or syntactic patterns) (Zhuang et al., 2006; Hu and Liu, 2004) and the recent deep learning models (Fan et a"
2020.emnlp-main.719,D17-1159,0,0.0250308,"rtance of the contextual information from wj with respect to the representation vector computation for wi in W . In particular, one score matrix would be used to capture the syntactic neighboring words of the current words (i.e., wi ) while the other score matrix would 8950 be reserved for the neighboring words of the target word wt . These two matrices would then be combined and consumed by a GCN model (Kipf and Welling, 2017) for representation learning. GCN model would be computed by: Specifically, for the syntactic neighbors of the current words, following the previous GCN models for NLP (Marcheggiani and Titov, 2017; Nguyen and Grishman, 2018; Veyseh et al., 2019), we directly use the adjacency binary matrix Ad = {adi,j }i,j=1..N of the dependency tree for W as the importance score matrix for this group of words. Note that adi,j is only set to 1 if wi is directly connected to wj in the dependency tree or i = j in this case. In the next step for the neighboring words of the target word wt , as we expect the closer words to the target word wt to have larger contributions for the representation vectors of the words in W for TOWE, we propose to use the syntactic distances (to the target word) dsyn and dsyn o"
2020.emnlp-main.719,C16-1311,0,0.0487846,") in the input sentence, the goal of TOWE is to identify the words in the sentence (called the target-oriented opinion words) that help to express the attitude of the author toward the aspect represented by the target word. For instance, as a running example, in the sentence “All warranties honored by XYZ (what I thought was a reputable company) are disappointing.”, “disappointing” is the opinion word for the target word “warranties” while the opinion words for the target word “company” would involve “reputable”. Among others, TOWE finds its applications in target-oriented sentiment analysis (Tang et al., 2016; Xue and Li, 2018; Veyseh et al., 2020) and opinion summarization (Wu et al., 2020). Equal contribution. honored XYZ Introduction ∗ are The early approach for TOWE has involved the rule-based and lexicon-based methods (Hu and Liu, 2004; Zhuang et al., 2006) while the recent work has focused on deep learning models for this problem (Fan et al., 2019; Wu et al., 2020). One of the insights from the rule-based methods is that the syntactic structures (i.e., the parsing trees) of the sentences can provide useful information to improve the performance for TOWE (Zhuang et al., 2006). However, these"
2020.emnlp-main.719,P19-1432,1,0.781312,"to the representation vector computation for wi in W . In particular, one score matrix would be used to capture the syntactic neighboring words of the current words (i.e., wi ) while the other score matrix would 8950 be reserved for the neighboring words of the target word wt . These two matrices would then be combined and consumed by a GCN model (Kipf and Welling, 2017) for representation learning. GCN model would be computed by: Specifically, for the syntactic neighbors of the current words, following the previous GCN models for NLP (Marcheggiani and Titov, 2017; Nguyen and Grishman, 2018; Veyseh et al., 2019), we directly use the adjacency binary matrix Ad = {adi,j }i,j=1..N of the dependency tree for W as the importance score matrix for this group of words. Note that adi,j is only set to 1 if wi is directly connected to wj in the dependency tree or i = j in this case. In the next step for the neighboring words of the target word wt , as we expect the closer words to the target word wt to have larger contributions for the representation vectors of the words in W for TOWE, we propose to use the syntactic distances (to the target word) dsyn and dsyn of i j wi and wj as the features to learn the impo"
2020.emnlp-main.719,D16-1059,0,0.0377237,"n, 2013; Shamshurin, 2012). A key difference between OWE and TOWE is that OWE does not require the opinion words to tie to any target words 8948 in the sentence while the opinion words in TOWE should be explicitly paired with a given target word. Another related task for TOWE is opinion target extraction (OTE) that attempts to identify the target words in the sentences (Qiu et al., 2011; Liu et al., 2015; Poria et al., 2016; Yin et al., 2016; Xu et al., 2018). Note that some previous works have also attempted to jointly predict the target and opinion words (Qiu et al., 2011; Liu et al., 2013; Wang et al., 2016, 2017; Li and Lam, 2017); however, the target words are still not paired with their corresponding opinion words in these studies. As mentioned in the introduction, among a few previous work on TOWE, the main approaches include the rule-based methods (i.e., based on word distances or syntactic patterns) (Zhuang et al., 2006; Hu and Liu, 2004) and the recent deep learning models (Fan et al., 2019; Wu et al., 2020). Our model is different from the previous deep learning models as we exploit the syntactic information (i.e., dependency trees) for TOWE with deep learning. 3 3.2 Syntax-Model Consist"
2020.emnlp-main.719,P18-2094,0,0.0904373,"lar, the most related task of TOWE is opinion word extraction (OWE) that aims to locate the terms used to express attitude in the sentences (Htay and Lynn, 2013; Shamshurin, 2012). A key difference between OWE and TOWE is that OWE does not require the opinion words to tie to any target words 8948 in the sentence while the opinion words in TOWE should be explicitly paired with a given target word. Another related task for TOWE is opinion target extraction (OTE) that attempts to identify the target words in the sentences (Qiu et al., 2011; Liu et al., 2015; Poria et al., 2016; Yin et al., 2016; Xu et al., 2018). Note that some previous works have also attempted to jointly predict the target and opinion words (Qiu et al., 2011; Liu et al., 2013; Wang et al., 2016, 2017; Li and Lam, 2017); however, the target words are still not paired with their corresponding opinion words in these studies. As mentioned in the introduction, among a few previous work on TOWE, the main approaches include the rule-based methods (i.e., based on word distances or syntactic patterns) (Zhuang et al., 2006; Hu and Liu, 2004) and the recent deep learning models (Fan et al., 2019; Wu et al., 2020). Our model is different from"
2020.emnlp-main.719,P18-1234,0,0.0359451,"ence, the goal of TOWE is to identify the words in the sentence (called the target-oriented opinion words) that help to express the attitude of the author toward the aspect represented by the target word. For instance, as a running example, in the sentence “All warranties honored by XYZ (what I thought was a reputable company) are disappointing.”, “disappointing” is the opinion word for the target word “warranties” while the opinion words for the target word “company” would involve “reputable”. Among others, TOWE finds its applications in target-oriented sentiment analysis (Tang et al., 2016; Xue and Li, 2018; Veyseh et al., 2020) and opinion summarization (Wu et al., 2020). Equal contribution. honored XYZ Introduction ∗ are The early approach for TOWE has involved the rule-based and lexicon-based methods (Hu and Liu, 2004; Zhuang et al., 2006) while the recent work has focused on deep learning models for this problem (Fan et al., 2019; Wu et al., 2020). One of the insights from the rule-based methods is that the syntactic structures (i.e., the parsing trees) of the sentences can provide useful information to improve the performance for TOWE (Zhuang et al., 2006). However, these syntactic structur"
2020.emnlp-main.719,J11-1002,0,0.0962642,"asks, TOWE has been relatively less explored in the literature. In particular, the most related task of TOWE is opinion word extraction (OWE) that aims to locate the terms used to express attitude in the sentences (Htay and Lynn, 2013; Shamshurin, 2012). A key difference between OWE and TOWE is that OWE does not require the opinion words to tie to any target words 8948 in the sentence while the opinion words in TOWE should be explicitly paired with a given target word. Another related task for TOWE is opinion target extraction (OTE) that attempts to identify the target words in the sentences (Qiu et al., 2011; Liu et al., 2015; Poria et al., 2016; Yin et al., 2016; Xu et al., 2018). Note that some previous works have also attempted to jointly predict the target and opinion words (Qiu et al., 2011; Liu et al., 2013; Wang et al., 2016, 2017; Li and Lam, 2017); however, the target words are still not paired with their corresponding opinion words in these studies. As mentioned in the introduction, among a few previous work on TOWE, the main approaches include the rule-based methods (i.e., based on word distances or syntactic patterns) (Zhuang et al., 2006; Hu and Liu, 2004) and the recent deep learnin"
2020.eval4nlp-1.4,D19-1220,0,0.0210595,"). Related Work Caption Evaluation We provide a summary of the widely used metrics for evaluating image captions such as n-gram similarity metrics, embedding based metrics, and other task-specific metrics for captioning. Captioning Specific Metrics After CIDEr is introduced, several metrics for image captioning are proposed. SPICE (Anderson et al., 2016) uses scene graph and LEIC (Cui et al., 2018) uses the trainable model to evaluate the captions. VIFIDEL (Madhyastha et al., 2019) is an extension of Wasserstein distance that utilizes the information from detected objects in the image. TIGEr (Jiang et al., 2019) uses the output of the visual grounding task. BERT-TBR (Yi et al., 2020) focuses on the variance of the captions and combine multiple reference captions to get improved BERTScore. N-gram Similarity Metrics The most widely used metrics for evaluating the quality of text generation tasks are n-gram similarity metrics that compute the exact number of n-gram matches between reference and generated text. One example of these metrics is BLEU (Papineni et al., 2002) that computes the precision of overlap n-gram between reference and candidate. ROUGE (Lin, 2004) is a set of commonly used metrics for"
2020.eval4nlp-1.4,D19-1051,0,0.0539631,"Missing"
2020.eval4nlp-1.4,P19-1264,0,0.0161365,"s with consideration to the image. Most of the previous studies on evaluating image captioning tasks rely on n-gram similarity metrics such as BLEU (Papineni et al., 2002) or CIDEr (Vedantam et al., 2015). These approaches bear limitations in dealing with the text’s diverse nature, similarly found in other text generation tasks (e.g., abstractive summarization and dialog) (Kryscinski et al., 2019; Liu et al., 2016). To alleviate the issues in the n-gram based approaches, researchers proposed word embedding-based techniques (Kusner et al., 2015; Zhang et al., 2019; Zhao et al., 2019; Lo, 2019; Clark et al., 2019). These techniques shows robust performance and achieve higher correlation with human judgment than that of other previous metrics in many text 1 https://github.com/hwanheelee1993/ViLBERTScore 34 Proceedings of the First Workshop on Evaluation and Comparison of NLP Systems (Eval4NLP), pages 34–39, c November 20, 2020. 2020 Association for Computational Linguistics Reference Caption ? A boy with an orange shirt smiles , while a boy in a blue shirt looks on ?? ? ?? ? Pairwise Cosine Similarity Maximum Similarity ?? ? : ViLBERT ෝ Candidate Caption ? A young boy in a blue shirt is sitting on a wo"
2020.eval4nlp-1.4,D16-1230,0,0.0145981,"ation algorithms (Vinyals et al., 2015; Anderson et al., 2018) and target datasets (Fang et al., 2015; Sharma et al., 2018), few studies have focused on assessing the quality of the generated captions with consideration to the image. Most of the previous studies on evaluating image captioning tasks rely on n-gram similarity metrics such as BLEU (Papineni et al., 2002) or CIDEr (Vedantam et al., 2015). These approaches bear limitations in dealing with the text’s diverse nature, similarly found in other text generation tasks (e.g., abstractive summarization and dialog) (Kryscinski et al., 2019; Liu et al., 2016). To alleviate the issues in the n-gram based approaches, researchers proposed word embedding-based techniques (Kusner et al., 2015; Zhang et al., 2019; Zhao et al., 2019; Lo, 2019; Clark et al., 2019). These techniques shows robust performance and achieve higher correlation with human judgment than that of other previous metrics in many text 1 https://github.com/hwanheelee1993/ViLBERTScore 34 Proceedings of the First Workshop on Evaluation and Comparison of NLP Systems (Eval4NLP), pages 34–39, c November 20, 2020. 2020 Association for Computational Linguistics Reference Caption ? A boy with a"
2020.eval4nlp-1.4,N19-1423,0,0.0360551,"I, reference caption x and candidate caption x ˆ, we compute contextual embeddings with ViLBERT for x and x ˆ respectively. Then, we extract the text embeddings HX V and HXˆ V for each output embedding. Finally, we compute the pairwise cosine similarity between HX V and HXˆ V as in (Zhang et al., 2019). effective in evaluating image captioning tasks. 2 2.1 WMD computes minimum transportation distance among tokens using pre-trained word embeddings (i.e., GloVe (Pennington et al., 2014)). On the other hand, BERTScore computes cosine similarity among tokens using contextual embeddings from BERT (Devlin et al., 2019). Related Work Caption Evaluation We provide a summary of the widely used metrics for evaluating image captions such as n-gram similarity metrics, embedding based metrics, and other task-specific metrics for captioning. Captioning Specific Metrics After CIDEr is introduced, several metrics for image captioning are proposed. SPICE (Anderson et al., 2016) uses scene graph and LEIC (Cui et al., 2018) uses the trainable model to evaluate the captions. VIFIDEL (Madhyastha et al., 2019) is an extension of Wasserstein distance that utilizes the information from detected objects in the image. TIGEr (J"
2020.eval4nlp-1.4,W19-5358,0,0.0153432,"ed captions with consideration to the image. Most of the previous studies on evaluating image captioning tasks rely on n-gram similarity metrics such as BLEU (Papineni et al., 2002) or CIDEr (Vedantam et al., 2015). These approaches bear limitations in dealing with the text’s diverse nature, similarly found in other text generation tasks (e.g., abstractive summarization and dialog) (Kryscinski et al., 2019; Liu et al., 2016). To alleviate the issues in the n-gram based approaches, researchers proposed word embedding-based techniques (Kusner et al., 2015; Zhang et al., 2019; Zhao et al., 2019; Lo, 2019; Clark et al., 2019). These techniques shows robust performance and achieve higher correlation with human judgment than that of other previous metrics in many text 1 https://github.com/hwanheelee1993/ViLBERTScore 34 Proceedings of the First Workshop on Evaluation and Comparison of NLP Systems (Eval4NLP), pages 34–39, c November 20, 2020. 2020 Association for Computational Linguistics Reference Caption ? A boy with an orange shirt smiles , while a boy in a blue shirt looks on ?? ? ?? ? Pairwise Cosine Similarity Maximum Similarity ?? ? : ViLBERT ෝ Candidate Caption ? A young boy in a blue shi"
2020.eval4nlp-1.4,P19-1654,0,0.0896887,"., 2014)). On the other hand, BERTScore computes cosine similarity among tokens using contextual embeddings from BERT (Devlin et al., 2019). Related Work Caption Evaluation We provide a summary of the widely used metrics for evaluating image captions such as n-gram similarity metrics, embedding based metrics, and other task-specific metrics for captioning. Captioning Specific Metrics After CIDEr is introduced, several metrics for image captioning are proposed. SPICE (Anderson et al., 2016) uses scene graph and LEIC (Cui et al., 2018) uses the trainable model to evaluate the captions. VIFIDEL (Madhyastha et al., 2019) is an extension of Wasserstein distance that utilizes the information from detected objects in the image. TIGEr (Jiang et al., 2019) uses the output of the visual grounding task. BERT-TBR (Yi et al., 2020) focuses on the variance of the captions and combine multiple reference captions to get improved BERTScore. N-gram Similarity Metrics The most widely used metrics for evaluating the quality of text generation tasks are n-gram similarity metrics that compute the exact number of n-gram matches between reference and generated text. One example of these metrics is BLEU (Papineni et al., 2002) th"
2020.eval4nlp-1.4,D19-1053,0,0.0203989,"lity of the generated captions with consideration to the image. Most of the previous studies on evaluating image captioning tasks rely on n-gram similarity metrics such as BLEU (Papineni et al., 2002) or CIDEr (Vedantam et al., 2015). These approaches bear limitations in dealing with the text’s diverse nature, similarly found in other text generation tasks (e.g., abstractive summarization and dialog) (Kryscinski et al., 2019; Liu et al., 2016). To alleviate the issues in the n-gram based approaches, researchers proposed word embedding-based techniques (Kusner et al., 2015; Zhang et al., 2019; Zhao et al., 2019; Lo, 2019; Clark et al., 2019). These techniques shows robust performance and achieve higher correlation with human judgment than that of other previous metrics in many text 1 https://github.com/hwanheelee1993/ViLBERTScore 34 Proceedings of the First Workshop on Evaluation and Comparison of NLP Systems (Eval4NLP), pages 34–39, c November 20, 2020. 2020 Association for Computational Linguistics Reference Caption ? A boy with an orange shirt smiles , while a boy in a blue shirt looks on ?? ? ?? ? Pairwise Cosine Similarity Maximum Similarity ?? ? : ViLBERT ෝ Candidate Caption ? A young boy in"
2020.eval4nlp-1.4,P02-1040,0,0.107127,"rics. This result demonstrates that the use of contextualized embedding from vision and language is Introduction Image captioning is a task that aims to generate a text that describes a given image. While there have been many advances for caption generation algorithms (Vinyals et al., 2015; Anderson et al., 2018) and target datasets (Fang et al., 2015; Sharma et al., 2018), few studies have focused on assessing the quality of the generated captions with consideration to the image. Most of the previous studies on evaluating image captioning tasks rely on n-gram similarity metrics such as BLEU (Papineni et al., 2002) or CIDEr (Vedantam et al., 2015). These approaches bear limitations in dealing with the text’s diverse nature, similarly found in other text generation tasks (e.g., abstractive summarization and dialog) (Kryscinski et al., 2019; Liu et al., 2016). To alleviate the issues in the n-gram based approaches, researchers proposed word embedding-based techniques (Kusner et al., 2015; Zhang et al., 2019; Zhao et al., 2019; Lo, 2019; Clark et al., 2019). These techniques shows robust performance and achieve higher correlation with human judgment than that of other previous metrics in many text 1 https:"
2020.eval4nlp-1.4,D14-1162,0,0.0946659,"t is sitting on a wooden bench : Image Embedding ?? ? : Text Embedding Figure 2: Overall computation of ViLBERTScore. Given the image I, reference caption x and candidate caption x ˆ, we compute contextual embeddings with ViLBERT for x and x ˆ respectively. Then, we extract the text embeddings HX V and HXˆ V for each output embedding. Finally, we compute the pairwise cosine similarity between HX V and HXˆ V as in (Zhang et al., 2019). effective in evaluating image captioning tasks. 2 2.1 WMD computes minimum transportation distance among tokens using pre-trained word embeddings (i.e., GloVe (Pennington et al., 2014)). On the other hand, BERTScore computes cosine similarity among tokens using contextual embeddings from BERT (Devlin et al., 2019). Related Work Caption Evaluation We provide a summary of the widely used metrics for evaluating image captions such as n-gram similarity metrics, embedding based metrics, and other task-specific metrics for captioning. Captioning Specific Metrics After CIDEr is introduced, several metrics for image captioning are proposed. SPICE (Anderson et al., 2016) uses scene graph and LEIC (Cui et al., 2018) uses the trainable model to evaluate the captions. VIFIDEL (Madhyast"
2020.eval4nlp-1.4,P18-1238,0,0.151175,"Missing"
2020.eval4nlp-1.4,2020.acl-main.93,0,0.654145,"etrics for evaluating image captions such as n-gram similarity metrics, embedding based metrics, and other task-specific metrics for captioning. Captioning Specific Metrics After CIDEr is introduced, several metrics for image captioning are proposed. SPICE (Anderson et al., 2016) uses scene graph and LEIC (Cui et al., 2018) uses the trainable model to evaluate the captions. VIFIDEL (Madhyastha et al., 2019) is an extension of Wasserstein distance that utilizes the information from detected objects in the image. TIGEr (Jiang et al., 2019) uses the output of the visual grounding task. BERT-TBR (Yi et al., 2020) focuses on the variance of the captions and combine multiple reference captions to get improved BERTScore. N-gram Similarity Metrics The most widely used metrics for evaluating the quality of text generation tasks are n-gram similarity metrics that compute the exact number of n-gram matches between reference and generated text. One example of these metrics is BLEU (Papineni et al., 2002) that computes the precision of overlap n-gram between reference and candidate. ROUGE (Lin, 2004) is a set of commonly used metrics for text summarization. In particular, ROUGE-N, the longest common subsequenc"
2020.findings-emnlp.328,P17-1022,0,0.021054,"el corpora, and Gella et al. (2017) where the images are used as pivot between languages to learn multimodal multilingual common representations. Our work leverages only unpaired data and does not aim to train a machine translation model or obtain multimodal representations explicitly. Related to our goals is also work OUTPUTS a desk with a laptop and a book. Figure 2: Here we showcase interesting examples of the types of translations obtained with our approach. Casing and color coding were added manually. aiming to translate neural network internal representations into natural language e.g. (Andreas et al., 2017; Evtimova et al., 2018). Moreover, general work in multimodal machine translation under supervised/unsupervised learning is also related to our work. Elliott and K´ad´ar (2017) and Helcl et al. (2018) investigate visually grounded representations to improve supervised multimodal machine translation, and ignore input images at test time. Using reinforcement learning, Chen et al. (2018) jointly optimizes a captioner and a neural machine translator to achieve unsupervised multimodal machine translation, while Su et al. (2019) and Huang et al. (2020) explore transformers (Vaswani et al., 3676 201"
2020.findings-emnlp.328,D18-1329,0,0.018042,"is used for learning mappings across an exhaustive number of language pairs among target languages. We demonstrate our approach on two datasets of images annotated with German, English, and Japanese, English respectively. 3673 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3673–3678 c November 16 - 20, 2020. 2020 Association for Computational Linguistics 2 Background Our work is different from work in both general neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2015; Luong et al., 2015), and multimodal machine translation (MMT) (Elliott, 2018; Caglayan et al., 2019; Raunak et al., 2019) in that we do not use parallel corpora across languages. This distinction is important and perhaps confusing as we rely on the Multi30k dataset for which several versions and tasks exist (Elliott et al., 2016; Barrault et al., 2018). The first task, task 1, is perhaps the most popular, containing parallel text among languages (German, English, French and Czech) describing 30,000 images from the Flickr30k dataset (Young et al., 2014) with a single caption in each language. This task has often been used also as a pure machine translation benchmark by"
2020.findings-emnlp.328,W16-3210,0,0.0455424,"Missing"
2020.findings-emnlp.328,I17-1014,0,0.0477002,"Missing"
2020.findings-emnlp.328,D17-1303,0,0.0197562,"いる Related Work Our work is closely related to the problem of lexicon induction from images which has been used to address the issue when paired texts are not available for machine translation. Works that have leveraged visual features to build such lexicon include Bergsma and Van Durme (2011); Kiela et al. (2015); Hewitt et al. (2018). Other works with similar goals include Hitschler et al. (2016) where visual features are used to assemble a weakly supervised set of text pairs, Gu et al. (2018) where the objective is to leverage both image-caption pairs and multilingual parallel corpora, and Gella et al. (2017) where the images are used as pivot between languages to learn multimodal multilingual common representations. Our work leverages only unpaired data and does not aim to train a machine translation model or obtain multimodal representations explicitly. Related to our goals is also work OUTPUTS a desk with a laptop and a book. Figure 2: Here we showcase interesting examples of the types of translations obtained with our approach. Casing and color coding were added manually. aiming to translate neural network internal representations into natural language e.g. (Andreas et al., 2017; Evtimova et a"
2020.findings-emnlp.328,W18-6402,0,0.0166852,"nal Linguistics: EMNLP 2020, pages 3673–3678 c November 16 - 20, 2020. 2020 Association for Computational Linguistics 2 Background Our work is different from work in both general neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2015; Luong et al., 2015), and multimodal machine translation (MMT) (Elliott, 2018; Caglayan et al., 2019; Raunak et al., 2019) in that we do not use parallel corpora across languages. This distinction is important and perhaps confusing as we rely on the Multi30k dataset for which several versions and tasks exist (Elliott et al., 2016; Barrault et al., 2018). The first task, task 1, is perhaps the most popular, containing parallel text among languages (German, English, French and Czech) describing 30,000 images from the Flickr30k dataset (Young et al., 2014) with a single caption in each language. This task has often been used also as a pure machine translation benchmark by discarding the image information. The second task, task 2, is the one that concerns our work and is one of the tasks we leverage for training, which is the mutilingual image description generation task, where each of the 30,000 images is annotated with 5 independent (unpaired)"
2020.findings-emnlp.328,W18-6441,0,0.0418254,"Missing"
2020.findings-emnlp.328,P18-1239,0,0.0167053,"of grass. デスク に パソコン が 置い て ある a laptop computer sitting on top of a desk. ⽊製 の テーブル と 棚 に パソ コン と プリンター が 置い て ある a room with a wooden door and a door. デスク 上 の パソコン の 横 に ⽔ が ⼊っ た ペットボトル が 置か れ て いる a black cat sitting on top of a computer desk. デスク の 上 に パソコン や ラ イト 、 本 が 置か れ て いる Related Work Our work is closely related to the problem of lexicon induction from images which has been used to address the issue when paired texts are not available for machine translation. Works that have leveraged visual features to build such lexicon include Bergsma and Van Durme (2011); Kiela et al. (2015); Hewitt et al. (2018). Other works with similar goals include Hitschler et al. (2016) where visual features are used to assemble a weakly supervised set of text pairs, Gu et al. (2018) where the objective is to leverage both image-caption pairs and multilingual parallel corpora, and Gella et al. (2017) where the images are used as pivot between languages to learn multimodal multilingual common representations. Our work leverages only unpaired data and does not aim to train a machine translation model or obtain multimodal representations explicitly. Related to our goals is also work OUTPUTS a desk with a laptop and"
2020.findings-emnlp.328,P16-1227,0,0.0139201,"top of a desk. ⽊製 の テーブル と 棚 に パソ コン と プリンター が 置い て ある a room with a wooden door and a door. デスク 上 の パソコン の 横 に ⽔ が ⼊っ た ペットボトル が 置か れ て いる a black cat sitting on top of a computer desk. デスク の 上 に パソコン や ラ イト 、 本 が 置か れ て いる Related Work Our work is closely related to the problem of lexicon induction from images which has been used to address the issue when paired texts are not available for machine translation. Works that have leveraged visual features to build such lexicon include Bergsma and Van Durme (2011); Kiela et al. (2015); Hewitt et al. (2018). Other works with similar goals include Hitschler et al. (2016) where visual features are used to assemble a weakly supervised set of text pairs, Gu et al. (2018) where the objective is to leverage both image-caption pairs and multilingual parallel corpora, and Gella et al. (2017) where the images are used as pivot between languages to learn multimodal multilingual common representations. Our work leverages only unpaired data and does not aim to train a machine translation model or obtain multimodal representations explicitly. Related to our goals is also work OUTPUTS a desk with a laptop and a book. Figure 2: Here we showcase interesting examples of the"
2020.findings-emnlp.328,2020.acl-main.731,0,0.0190045,"l representations into natural language e.g. (Andreas et al., 2017; Evtimova et al., 2018). Moreover, general work in multimodal machine translation under supervised/unsupervised learning is also related to our work. Elliott and K´ad´ar (2017) and Helcl et al. (2018) investigate visually grounded representations to improve supervised multimodal machine translation, and ignore input images at test time. Using reinforcement learning, Chen et al. (2018) jointly optimizes a captioner and a neural machine translator to achieve unsupervised multimodal machine translation, while Su et al. (2019) and Huang et al. (2020) explore transformers (Vaswani et al., 3676 2017) to construct a text encoder-decoder for the same goal. Our work is different from referred multimodal machine translation works since our work starts from multilingual image captioning and is applied to machine translation, while some of the other methods start from a multimodal machine translation and are applied to machine translation, however building models that take advantage from these two tasks is a possible avenue for future work. Many of previous methods rely on pre-training on external data for either captioning or machine translation"
2020.findings-emnlp.328,D13-1176,0,0.0591127,"bility of using visually grounded representation learning as a unifying medium across languages, where a single model is used for learning mappings across an exhaustive number of language pairs among target languages. We demonstrate our approach on two datasets of images annotated with German, English, and Japanese, English respectively. 3673 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3673–3678 c November 16 - 20, 2020. 2020 Association for Computational Linguistics 2 Background Our work is different from work in both general neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2015; Luong et al., 2015), and multimodal machine translation (MMT) (Elliott, 2018; Caglayan et al., 2019; Raunak et al., 2019) in that we do not use parallel corpora across languages. This distinction is important and perhaps confusing as we rely on the Multi30k dataset for which several versions and tasks exist (Elliott et al., 2016; Barrault et al., 2018). The first task, task 1, is perhaps the most popular, containing parallel text among languages (German, English, French and Czech) describing 30,000 images from the Flickr30k dataset (Young et al., 2014) with a single ca"
2020.findings-emnlp.328,D15-1015,0,0.101425,"t al. (2011); Domke (2013); Wang et al. (2018). We particularly adopt the single layer version of the most recenlty proposed feedback propagation approach of Wang et al. (2018) as it was more directly applied to convolutional neural networks for visual recognition. Unlike this previous work, we are the first to show that feedback propagation can leverage its latent space to use interactions among target variables even in the absence of any visual input at test time. 3 Method As mentioned earlier, our base model consists of the image captioning model with “soft” attention proposed by Xu et al. (2015) but trained with independent textual decoders for each target language. In this model, the image encoder consists of a convolutional neural network and the textual decoders consist of recurrent neural networks with Long Short Term Memory (LSTM) units. The output soft spatial attention vector computed from the input image is used as input for the decoders to generate captions in each target language. Let the input image be I, and let us consider the bilingual case of language a and language b where the targets are text sequences ta and tb respectively. The model can then be expressed as: F (I)"
2020.findings-emnlp.328,P17-2066,0,0.0177115,"9 53.22 108.53 EN JP 31.92 52.35 84.64 JP EN 24.75 46.81 81.38 Table 1: Results on Multi30k dataset with German (DE) and English (EN) unpaired textual captions. Table 2: Results COCO+STAIR with Japanese (JP) and English (EN) unpaired textual captions. techniques such as beam search from the pseudodistribution fa (ˆ z ). 145, 000, 5, 070, and 5, 000 captions for training, validation and testing for each language. We jointly train the image captioning model to generate captions for both languages. Multi30k provides preprocessed lowercase tokens for all the sentences. We also use STAIR Captions (Yoshikawa et al., 2017), which contains Japanese captions for all images in the MS COCO dataset (Lin et al., 2014). The Japanese captions are also collected independently from the English captions in MS COCO, thus not being paired. No Visual Input In our third type of inference we use the latent feature space to predict ta conditioned exclusively on tb but without access to any image input. We optimize the same expression as in Equation 2 but initialize z as z = g(ξ) instead, where ξ is a trivial input image with pixel values sampled from a gaussian distribution N (µ, σ 2 ) with a mean and standard deviation estimat"
2020.findings-emnlp.328,Q14-1006,0,0.365822,"nde spielen auf dem Strand girl hits a ball and the catcher looks on ein Schiedsrichter beobachtet zwei Baseballspieler Figure 1: Our work shows how visual features capture multi-lingual information in image conditioned models (solid blue arrows) and how to pivot this information across languages during inference by incorporating feedback connections (dotted red arrows) from language back to visual feature space. Introduction There has been great interest in learning visual representations from images paired with natural language annotations. While tasks such as image caption generation e.g. (Young et al., 2014; Lin et al., 2014) have focused mostly on English text, there is a growing body of work extending to a larger set of languages (Calixto et al., 2012; Elliott et al., 2015, 2016). Images annotated in multiple languages offer the possibility of studying grounded models of languages along with their commonalities and intrinsics in direct connection with the visual world. We focus in the multilingual image description generation setting where we train an image encoder with soft-attention (Xu et al., 2015) and multiple text decoders for each target language. Then, we demonstrate that information f"
2020.findings-emnlp.328,D15-1166,0,0.0234652,"ng as a unifying medium across languages, where a single model is used for learning mappings across an exhaustive number of language pairs among target languages. We demonstrate our approach on two datasets of images annotated with German, English, and Japanese, English respectively. 3673 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3673–3678 c November 16 - 20, 2020. 2020 Association for Computational Linguistics 2 Background Our work is different from work in both general neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2015; Luong et al., 2015), and multimodal machine translation (MMT) (Elliott, 2018; Caglayan et al., 2019; Raunak et al., 2019) in that we do not use parallel corpora across languages. This distinction is important and perhaps confusing as we rely on the Multi30k dataset for which several versions and tasks exist (Elliott et al., 2016; Barrault et al., 2018). The first task, task 1, is perhaps the most popular, containing parallel text among languages (German, English, French and Czech) describing 30,000 images from the Flickr30k dataset (Young et al., 2014) with a single caption in each language. This task has often"
2020.findings-emnlp.407,D19-1566,0,0.0281578,"A novel method to regulate the GCN-based representation vectors of the words using the given aspect term for ABSA. • A novel method to encourage the consistency between the syntax-based and model-based importance scores of the words based on the given aspect term. • Extensive experiments on three benchmark datasets for ABSA, resulting in new state-of-theart performance for all the datasets. 2 Related Work Sentiment analysis has been studied under different settings in the literature (e.g., sentence-level, aspect-level, cross-domain) (Wang et al., 2019; Zhang and Zhang, 2019; Sun et al., 2019; Chauhan et al., 2019; Hu et al., 2019). For ABSA, the early works have performed feature engineering to produce useful features for the statistical classification models (e.g., SVM) (Wagner et al., 2014). Recently, deep learning models have superseded the feature based models due to their ability to automatically learn effective features from data (Wagner et al., 2016; Johnson and Zhang, 2015; Tang et al., 2016). The typical network architectures for ABSA in the literature involve convolutional neural networks (CNN) (Johnson and Zhang, 2015), recurrent neural networks (RNN) (Wagner et al., 2016), memory networks"
2020.findings-emnlp.407,N19-1423,0,0.0220111,"rk. 3 Model The task of ABSA can be formalized as follows: Given a sentence X = [x1 , x2 , . . . , xn ] of n words/tokens and the index t (1 ≤ t ≤ n) for the aspect term xt , the goal is to predict the sentiment polarity y ∗ toward the aspect term xt for X. Our model for ABSA in this work consists of three major components: (i) Representation Learning, (ii) Graph Convolution and Regulation, and (iii) Syntax and Model Consistency. (i) Representation Learning: Following the recent work in ABSA (Huang and Carley, 2019; Song et al., 2019), we first utilize the contextualized word embeddings BERT (Devlin et al., 2019) to obtain the representation vectors for the words in X. In particular, we first generate a sequence of words of ˆ = [CLS] + X + [SEP ] + xt + [SEP ] the form X where [CLS] and [SEP ] are the special tokens 4544 in BERT. This word sequence is then fed into the pre-trained BERT model to obtain the hidden vectors in the last layer. Afterwards, we obtain the embedding vector ei for each word xi ∈ X by averaging the hidden vectors of xi ’s sub-word units (i.e., wordpiece). As the result, the input sentence X will be represented by the vector sequence E = e1 , e2 , . . . , en in our model. Finally"
2020.findings-emnlp.407,C18-1096,0,0.0447998,"ering to produce useful features for the statistical classification models (e.g., SVM) (Wagner et al., 2014). Recently, deep learning models have superseded the feature based models due to their ability to automatically learn effective features from data (Wagner et al., 2016; Johnson and Zhang, 2015; Tang et al., 2016). The typical network architectures for ABSA in the literature involve convolutional neural networks (CNN) (Johnson and Zhang, 2015), recurrent neural networks (RNN) (Wagner et al., 2016), memory networks (Tang et al., 2016), attention (Luong et al., 2015) and gating mechanisms (He et al., 2018). The current state-of-the-art deep learning models for ABSA feature the graph-based models where the dependency trees are leveraged to improve the performance. (Huang and Carley, 2019; Zhang et al., 2019; Hou et al., 2019). However, to the best of our knowledge, none of these works has used the information from the aspect term to filter the graph-based hidden vectors and exploited importance scores for words from dependency trees as we do in this work. 3 Model The task of ABSA can be formalized as follows: Given a sentence X = [x1 , x2 , . . . , xn ] of n words/tokens and the index t (1 ≤ t ≤"
2020.findings-emnlp.407,D19-1558,0,0.0195738,"late the GCN-based representation vectors of the words using the given aspect term for ABSA. • A novel method to encourage the consistency between the syntax-based and model-based importance scores of the words based on the given aspect term. • Extensive experiments on three benchmark datasets for ABSA, resulting in new state-of-theart performance for all the datasets. 2 Related Work Sentiment analysis has been studied under different settings in the literature (e.g., sentence-level, aspect-level, cross-domain) (Wang et al., 2019; Zhang and Zhang, 2019; Sun et al., 2019; Chauhan et al., 2019; Hu et al., 2019). For ABSA, the early works have performed feature engineering to produce useful features for the statistical classification models (e.g., SVM) (Wagner et al., 2014). Recently, deep learning models have superseded the feature based models due to their ability to automatically learn effective features from data (Wagner et al., 2016; Johnson and Zhang, 2015; Tang et al., 2016). The typical network architectures for ABSA in the literature involve convolutional neural networks (CNN) (Johnson and Zhang, 2015), recurrent neural networks (RNN) (Wagner et al., 2016), memory networks (Tang et al., 2016"
2020.findings-emnlp.407,D19-1549,0,0.296281,"d be able to return the negative sentiment for input sentence “The staff were very polite, but the quality of the food was terrible.” assuming “food” as the aspect term. Due to its important applications (e.g., for opinion mining), ABSA has been studied extensively ∗ Equal contribution. in the literature. In these studies, deep learning has been employed to produce the state-of-the-art performance for this problem (Wagner et al., 2016; Dehong et al., 2017). Recently, in order to further improve the performance, the syntactic dependency trees have been integrated into the deep learning models (Huang and Carley, 2019; Zhang et al., 2019) for ABSA (called the graph-based deep learning models). Among others, dependency trees help to directly link the aspect term to the syntactically related words in the sentence, thus facilitating the graph convolutional neural networks (GCN) (Kipf and Welling, 2017) to enrich the representation vectors for the aspect terms. However, there are at least two major issues in these graph-based models that should be addressed to boost the performance. First, the representation vectors for the words in different layers of the current graph-based models for ABSA are not customized"
2020.findings-emnlp.407,D19-1654,0,0.0821052,"network with softmax in the end to estimate the probability distribution P (.|X, xt ) over the sentiments for X and xt . The negative log-likelihood Lpred = − log P (y ∗ |X, xt ) is then used as the prediction loss in this work. The overall loss to train the proposed model is then: L = Ldiv +αLconst +βLpred where α and β are trade-off parameters. 4 Experiments Datasets and Parameters: We employ three datasets to evaluate the models in this work. Two datasets, Restaurant and Laptop, are adopted from the SemEval 2014 Task 4 (Pontiki et al., 2014) while the third dataset, MAMS, is introduced in (Jiang et al., 2019). All the three datasets involve three sentiment categories, i.e., positive, neural, and negative. The numbers of examples for different portions of the three datasets are shown in Table 1. As only the MAMS dataset provides the development data, we fine-tune the model’s hyperparameters on the development data of MAMS and use the same hyper-parameters for the other datasets. The following hyper-parameters are suggested for the proposed model by the fine-tuning process: 200 dimensions for the hidden vectors of Dataset Restaurant-Train Restaurant-Test Laptop-Train Laptop-Test MAMS-Train MAMS-Dev"
2020.findings-emnlp.407,D19-1569,0,0.0904824,"utions include: • A novel method to regulate the GCN-based representation vectors of the words using the given aspect term for ABSA. • A novel method to encourage the consistency between the syntax-based and model-based importance scores of the words based on the given aspect term. • Extensive experiments on three benchmark datasets for ABSA, resulting in new state-of-theart performance for all the datasets. 2 Related Work Sentiment analysis has been studied under different settings in the literature (e.g., sentence-level, aspect-level, cross-domain) (Wang et al., 2019; Zhang and Zhang, 2019; Sun et al., 2019; Chauhan et al., 2019; Hu et al., 2019). For ABSA, the early works have performed feature engineering to produce useful features for the statistical classification models (e.g., SVM) (Wagner et al., 2014). Recently, deep learning models have superseded the feature based models due to their ability to automatically learn effective features from data (Wagner et al., 2016; Johnson and Zhang, 2015; Tang et al., 2016). The typical network architectures for ABSA in the literature involve convolutional neural networks (CNN) (Johnson and Zhang, 2015), recurrent neural networks (RNN) (Wagner et al., 2"
2020.findings-emnlp.407,D16-1021,0,0.243901,"elated Work Sentiment analysis has been studied under different settings in the literature (e.g., sentence-level, aspect-level, cross-domain) (Wang et al., 2019; Zhang and Zhang, 2019; Sun et al., 2019; Chauhan et al., 2019; Hu et al., 2019). For ABSA, the early works have performed feature engineering to produce useful features for the statistical classification models (e.g., SVM) (Wagner et al., 2014). Recently, deep learning models have superseded the feature based models due to their ability to automatically learn effective features from data (Wagner et al., 2016; Johnson and Zhang, 2015; Tang et al., 2016). The typical network architectures for ABSA in the literature involve convolutional neural networks (CNN) (Johnson and Zhang, 2015), recurrent neural networks (RNN) (Wagner et al., 2016), memory networks (Tang et al., 2016), attention (Luong et al., 2015) and gating mechanisms (He et al., 2018). The current state-of-the-art deep learning models for ABSA feature the graph-based models where the dependency trees are leveraged to improve the performance. (Huang and Carley, 2019; Zhang et al., 2019; Hou et al., 2019). However, to the best of our knowledge, none of these works has used the informa"
2020.findings-emnlp.407,P19-1432,1,0.816757,"o obtain the hidden vectors in the last layer. Afterwards, we obtain the embedding vector ei for each word xi ∈ X by averaging the hidden vectors of xi ’s sub-word units (i.e., wordpiece). As the result, the input sentence X will be represented by the vector sequence E = e1 , e2 , . . . , en in our model. Finally, we also employ the hidden vector s for the special token ˆ from BERT to encode the overall input [CLS] in X sentence X and its aspect term xt . (ii) Graph Convolution and Regulation: In order to employ the dependency trees for ABSA, we apply the GCN model (Nguyen and Grishman, 2018; Veyseh et al., 2019) to perform L abstraction layers over the word representation vector sequence E. A hidden vector for a word xi in the current layer of GCN is obtained by aggregating the hidden vectors of the dependency-based neighbor words of xi in the previous layer. Formally, let hli (0 ≤ l ≤ L, 1 ≤ i ≤ n) be the hidden vector of the word xi at the l-th layer of GCN. At the beginning, the GCN hidden vector h0i at the zero layer will be set to the word representation vector ei . Afterwards, hli (l > 0) will be computed by: ˆ l = Σj∈N (i) hl−1 /|N (i)| ˆ l ), h hli = ReLU (Wl h i i j where N (i) is the set of"
2020.findings-emnlp.407,S14-2076,0,0.465633,"ed and model-based importance scores of the words based on the given aspect term. • Extensive experiments on three benchmark datasets for ABSA, resulting in new state-of-theart performance for all the datasets. 2 Related Work Sentiment analysis has been studied under different settings in the literature (e.g., sentence-level, aspect-level, cross-domain) (Wang et al., 2019; Zhang and Zhang, 2019; Sun et al., 2019; Chauhan et al., 2019; Hu et al., 2019). For ABSA, the early works have performed feature engineering to produce useful features for the statistical classification models (e.g., SVM) (Wagner et al., 2014). Recently, deep learning models have superseded the feature based models due to their ability to automatically learn effective features from data (Wagner et al., 2016; Johnson and Zhang, 2015; Tang et al., 2016). The typical network architectures for ABSA in the literature involve convolutional neural networks (CNN) (Johnson and Zhang, 2015), recurrent neural networks (RNN) (Wagner et al., 2016), memory networks (Tang et al., 2016), attention (Luong et al., 2015) and gating mechanisms (He et al., 2018). The current state-of-the-art deep learning models for ABSA feature the graph-based models"
2020.findings-emnlp.407,C16-1311,0,0.524864,"n aspect. We focus on the term-based aspects for ABSA where the aspects correspond to some terms (i.e., sequences of words) in the input sentence. For instance, an ABSA system should be able to return the negative sentiment for input sentence “The staff were very polite, but the quality of the food was terrible.” assuming “food” as the aspect term. Due to its important applications (e.g., for opinion mining), ABSA has been studied extensively ∗ Equal contribution. in the literature. In these studies, deep learning has been employed to produce the state-of-the-art performance for this problem (Wagner et al., 2016; Dehong et al., 2017). Recently, in order to further improve the performance, the syntactic dependency trees have been integrated into the deep learning models (Huang and Carley, 2019; Zhang et al., 2019) for ABSA (called the graph-based deep learning models). Among others, dependency trees help to directly link the aspect term to the syntactically related words in the sentence, thus facilitating the graph convolutional neural networks (GCN) (Kipf and Welling, 2017) to enrich the representation vectors for the aspect terms. However, there are at least two major issues in these graph-based mod"
2020.findings-emnlp.407,D19-1560,0,0.0187441,"datasets for ABSA. In summary, our contributions include: • A novel method to regulate the GCN-based representation vectors of the words using the given aspect term for ABSA. • A novel method to encourage the consistency between the syntax-based and model-based importance scores of the words based on the given aspect term. • Extensive experiments on three benchmark datasets for ABSA, resulting in new state-of-theart performance for all the datasets. 2 Related Work Sentiment analysis has been studied under different settings in the literature (e.g., sentence-level, aspect-level, cross-domain) (Wang et al., 2019; Zhang and Zhang, 2019; Sun et al., 2019; Chauhan et al., 2019; Hu et al., 2019). For ABSA, the early works have performed feature engineering to produce useful features for the statistical classification models (e.g., SVM) (Wagner et al., 2014). Recently, deep learning models have superseded the feature based models due to their ability to automatically learn effective features from data (Wagner et al., 2016; Johnson and Zhang, 2015; Tang et al., 2016). The typical network architectures for ABSA in the literature involve convolutional neural networks (CNN) (Johnson and Zhang, 2015), recurren"
2020.findings-emnlp.407,2020.acl-main.295,0,0.0411807,"he proposed method, we compare it with the following baselines: (1) the feature-based model that applies feature engineering and the SVM model (Wagner et al., 2014), (2) the deep learning models based on the sequential order of the words in the sentences, including CNN, LSTM, attention and the gating mechanism (Wagner et al., 2016; Wang et al., 2016; Tang et al., 2016; Huang et al., 2018; Jiang et al., 2019), and (3) the graph-based models that exploit dependency trees to improve the deep learning models for ABSA (Huang and Carley, 2019; Zhang et al., 2019; Hou et al., 2019; Sun et al., 2019; Wang et al., 2020). Table 2 presents the performance of the models on the test sets of the three benchmark datasets. This table shows that the proposed model outperforms all the baselines over different benchmark datasets. The performance gaps are significant with p < 0.01, thereby demonstrating the effectiveness of the proposed model for ABSA. Ablation Study: There are three major components in the proposed model: (1) the gate vectors gl to regulate the hidden vectors of GCN (called Gate), (2) the gate diversity component Ldiv to promote the distinction between the gates (called Div.), and (3) the syntax and m"
2020.findings-emnlp.407,D16-1058,0,0.0300107,"n GCN, the size 32 for the mini-batches, the learning rate of 0.001 for the Adam optimizer, and 1.0 for the trade-off parameters α and β. Finally, we use the cased BERTbase model with 768 hidden dimensions in this work. Results: To demonstrate the effectiveness of the proposed method, we compare it with the following baselines: (1) the feature-based model that applies feature engineering and the SVM model (Wagner et al., 2014), (2) the deep learning models based on the sequential order of the words in the sentences, including CNN, LSTM, attention and the gating mechanism (Wagner et al., 2016; Wang et al., 2016; Tang et al., 2016; Huang et al., 2018; Jiang et al., 2019), and (3) the graph-based models that exploit dependency trees to improve the deep learning models for ABSA (Huang and Carley, 2019; Zhang et al., 2019; Hou et al., 2019; Sun et al., 2019; Wang et al., 2020). Table 2 presents the performance of the models on the test sets of the three benchmark datasets. This table shows that the proposed model outperforms all the baselines over different benchmark datasets. The performance gaps are significant with p < 0.01, thereby demonstrating the effectiveness of the proposed model for ABSA. Abla"
2020.findings-emnlp.407,D19-1464,0,0.454686,"negative sentiment for input sentence “The staff were very polite, but the quality of the food was terrible.” assuming “food” as the aspect term. Due to its important applications (e.g., for opinion mining), ABSA has been studied extensively ∗ Equal contribution. in the literature. In these studies, deep learning has been employed to produce the state-of-the-art performance for this problem (Wagner et al., 2016; Dehong et al., 2017). Recently, in order to further improve the performance, the syntactic dependency trees have been integrated into the deep learning models (Huang and Carley, 2019; Zhang et al., 2019) for ABSA (called the graph-based deep learning models). Among others, dependency trees help to directly link the aspect term to the syntactically related words in the sentence, thus facilitating the graph convolutional neural networks (GCN) (Kipf and Welling, 2017) to enrich the representation vectors for the aspect terms. However, there are at least two major issues in these graph-based models that should be addressed to boost the performance. First, the representation vectors for the words in different layers of the current graph-based models for ABSA are not customized for the aspect terms"
2020.findings-emnlp.407,P19-1342,0,0.0285118,"In summary, our contributions include: • A novel method to regulate the GCN-based representation vectors of the words using the given aspect term for ABSA. • A novel method to encourage the consistency between the syntax-based and model-based importance scores of the words based on the given aspect term. • Extensive experiments on three benchmark datasets for ABSA, resulting in new state-of-theart performance for all the datasets. 2 Related Work Sentiment analysis has been studied under different settings in the literature (e.g., sentence-level, aspect-level, cross-domain) (Wang et al., 2019; Zhang and Zhang, 2019; Sun et al., 2019; Chauhan et al., 2019; Hu et al., 2019). For ABSA, the early works have performed feature engineering to produce useful features for the statistical classification models (e.g., SVM) (Wagner et al., 2014). Recently, deep learning models have superseded the feature based models due to their ability to automatically learn effective features from data (Wagner et al., 2016; Johnson and Zhang, 2015; Tang et al., 2016). The typical network architectures for ABSA in the literature involve convolutional neural networks (CNN) (Johnson and Zhang, 2015), recurrent neural networks (RNN)"
2020.findings-emnlp.407,D15-1166,0,0.039176,"e early works have performed feature engineering to produce useful features for the statistical classification models (e.g., SVM) (Wagner et al., 2014). Recently, deep learning models have superseded the feature based models due to their ability to automatically learn effective features from data (Wagner et al., 2016; Johnson and Zhang, 2015; Tang et al., 2016). The typical network architectures for ABSA in the literature involve convolutional neural networks (CNN) (Johnson and Zhang, 2015), recurrent neural networks (RNN) (Wagner et al., 2016), memory networks (Tang et al., 2016), attention (Luong et al., 2015) and gating mechanisms (He et al., 2018). The current state-of-the-art deep learning models for ABSA feature the graph-based models where the dependency trees are leveraged to improve the performance. (Huang and Carley, 2019; Zhang et al., 2019; Hou et al., 2019). However, to the best of our knowledge, none of these works has used the information from the aspect term to filter the graph-based hidden vectors and exploited importance scores for words from dependency trees as we do in this work. 3 Model The task of ABSA can be formalized as follows: Given a sentence X = [x1 , x2 , . . . , xn ] of"
2020.findings-emnlp.407,P15-2060,1,0.791192,"t dataset of MAMS. As can be seen, the proposed model is significantly Conclusion We introduce a new model for ABSA that addresses two limitations of the prior work. It employs the given aspect terms to customize the hidden vectors. It also benefits from the overall dependency-based importance scores of the words. Our extensive experiments on three benchmark datasets empirically demonstrate the effectiveness of the proposed approach, leading to state-of-the-art results on these datasets. The future work involves applying the proposed model to the related tasks for ABSA, e.g., event detection (Nguyen and Grishman, 2015). Acknowledgement This research is based upon work supported in part by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via IARPA Contract No. 2019-19051600006 under the Better Extraction from Text Towards Enhanced Retrieval (BETTER) Program. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of ODNI, IARPA, the Department of Defense, or the U.S. Government. The U.S. Government is authorized to repro"
2020.findings-emnlp.65,D16-1257,0,0.0569226,"Missing"
2020.findings-emnlp.65,D18-1217,0,0.0516224,"Missing"
2020.findings-emnlp.65,D19-1422,0,0.0851046,"edictions. For example, for their machine translation predictions, Bahdanau et al. (2014) show a heat map of attention weights from source language words to target language words. Similarly, in transformer architectures (Vaswani et al., 2017), a selfattention head produces attention distributions from the input words to the same input words, as shown in the second row on the right side of Figure 1. However, self-attention mechanisms have multiple heads, making the combined outputs difficult to interpret. Recent work in multi-label text classification (Xiao et al., 2019) and sequence labeling (Cui and Zhang, 2019) shows the efficiency and interpretability of label-specific representations. We introduce the Label Attention Layer: a modified version of self-attention, where each classification label corresponds to one or more attention heads. We project the output at the attention head level, rather than after aggregating all outputs, to preserve the source of head-specific information, thus allowing us to match labels to heads. To test our proposed Label Attention Layer, we build upon the parser of Zhou and Zhao (2019) and establish a new state of the art for both constituency and dependency parsing, in"
2020.findings-emnlp.65,P81-1022,0,0.702861,"Missing"
2020.findings-emnlp.65,N16-1024,0,0.0664124,"Missing"
2020.findings-emnlp.65,N19-1076,0,0.0204044,"Missing"
2020.findings-emnlp.65,P18-2075,0,0.0254246,"Missing"
2020.findings-emnlp.65,P17-2025,0,0.0345951,"Missing"
2020.findings-emnlp.65,N18-1091,0,0.0365574,"Missing"
2020.findings-emnlp.65,N19-1357,0,0.0237396,"age modeling (Salton et al., 2017). Self-attention and transformer architectures (Vaswani et al., 2017) are now the state of the art in language understanding (Devlin et al., 2018; Yang et al., 2019), extractive summarization (Liu, 2019), semantic role labeling (Strubell et al., 2018) and machine translation for low-resource languages (Rikters, 2018; Rikters et al., 2018). While attention mechanisms can provide explanations for model predictions, Serrano and Smith (2019) challenge that assumption and find that attention weights only noisily predict overall importance with regard to the model. Jain and Wallace (2019) find that attention distributions rarely correlate with feature importance weights. However, Wiegreffe and Pinter (2019) show through alternative tests that prior work does not discredit the usefulness of attention for interpretability. Xiao et al. (2019) introduce the Label-Specific Attention Network (LSAN) for multi-label document classification. They use label descriptions to compute attention scores for words, and follow the self-attention of Lin et al. (2017). Cui and Zhang (2019) introduce a Label Attention Inference Layer for sequence labeling, which uses the self-attention of Vaswani"
2020.findings-emnlp.65,P19-1237,0,0.0435874,"Missing"
2020.findings-emnlp.65,P18-1249,0,0.121788,"final word representation, as shown in the color-coded vectors in Figure 4. The activation functions of the position-wise feed-forward layer make it difficult to follow the path of the contributions. Therefore we can remove the position-wise feed-forward layer, and compute the contributions from each label. We provide an example in Figure 6, where the contributions are computed using normalization and averaging. In this case, we are computing the contributions of each head to the span vector. The span representation for “the person” is computed following the method of Gaddy et al. (2018) and Kitaev and Klein (2018). However, forward and backward represenEncoder Our parser is an encoder-decoder model. The encoder has self-attention layers (Vaswani et al., 2017), preceding the Label Attention Layer. We follow the attention partition of Kitaev and Klein (2018), who show that separating content embeddings from position ones improves performance. Sentences are pre-processed following Zhou and Zhao (2019). Trees are represented using a simplified Head-driven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1994). In Zhou and Zhao (2019), two kinds of span representations are proposed: the division span and t"
2020.findings-emnlp.65,E17-1117,0,0.039069,"Missing"
2020.findings-emnlp.65,D16-1180,0,0.0639129,"Missing"
2020.findings-emnlp.65,L18-1595,0,0.0609438,"Missing"
2020.findings-emnlp.65,C18-1271,0,0.0199651,"Missing"
2020.findings-emnlp.65,Q17-1029,0,0.0147098,"lar, the last row shows that replacing our LAL with a self-attention layer with an equal number of attention heads decreases performance: the difference between the performance of the first row and the last row is due to the Label Attention Layer’s architecture novelties. 4.4 English and Chinese Results Our best-performing English-language parser does not have residual dropout, but has a position-wise feed-forward layer. We train Chinese-language parsers using the same configuration. The Chinese Treebank has two data splits for the training, development and testing sets: one for Constituency (Liu and Zhang, 2017b) and one for Dependency parsing (Zhang and Clark, 2008). Finally, we compare our results with the state of the art in constituency and dependency parsing in both English and Chinese. We show our Constituency Parsing results in Table 3, and our Dependency Parsing results in Table 4. Our LAL parser establishes new state-of-the-art results in both languages, improving significantly in dependency parsing. 4.5 Interpreting Head Contributions We follow the method in Figure 6 to identify which attention heads contribute to predictions. We collect the span vectors from the Penn Treebank test set, an"
2020.findings-emnlp.65,Q17-1004,0,0.0235094,"lar, the last row shows that replacing our LAL with a self-attention layer with an equal number of attention heads decreases performance: the difference between the performance of the first row and the last row is due to the Label Attention Layer’s architecture novelties. 4.4 English and Chinese Results Our best-performing English-language parser does not have residual dropout, but has a position-wise feed-forward layer. We train Chinese-language parsers using the same configuration. The Chinese Treebank has two data splits for the training, development and testing sets: one for Constituency (Liu and Zhang, 2017b) and one for Dependency parsing (Zhang and Clark, 2008). Finally, we compare our results with the state of the art in constituency and dependency parsing in both English and Chinese. We show our Constituency Parsing results in Table 3, and our Dependency Parsing results in Table 4. Our LAL parser establishes new state-of-the-art results in both languages, improving significantly in dependency parsing. 4.5 Interpreting Head Contributions We follow the method in Figure 6 to identify which attention heads contribute to predictions. We collect the span vectors from the Penn Treebank test set, an"
2020.findings-emnlp.65,I17-1045,0,0.0284859,"ise, for the 193 spans labelled as S but not predicted as such, the top-contributing head of 141 of them is one of the four top-contributing heads for spans predicted as S. This suggests that a stronger prediction link to the label attention heads, through a loss function for instance, may increase the performance. 5 Related Work Since their introduction in Machine Translation, attention mechanisms (Bahdanau et al., 2014; Luong et al., 2015) have been extended to other tasks, such as text classification (Yang et al., 2016), natural language inference (Chen et al., 2016) and language modeling (Salton et al., 2017). Self-attention and transformer architectures (Vaswani et al., 2017) are now the state of the art in language understanding (Devlin et al., 2018; Yang et al., 2019), extractive summarization (Liu, 2019), semantic role labeling (Strubell et al., 2018) and machine translation for low-resource languages (Rikters, 2018; Rikters et al., 2018). While attention mechanisms can provide explanations for model predictions, Serrano and Smith (2019) challenge that assumption and find that attention weights only noisily predict overall importance with regard to the model. Jain and Wallace (2019) find that"
2020.findings-emnlp.65,P19-1282,0,0.0200858,"., 2015) have been extended to other tasks, such as text classification (Yang et al., 2016), natural language inference (Chen et al., 2016) and language modeling (Salton et al., 2017). Self-attention and transformer architectures (Vaswani et al., 2017) are now the state of the art in language understanding (Devlin et al., 2018; Yang et al., 2019), extractive summarization (Liu, 2019), semantic role labeling (Strubell et al., 2018) and machine translation for low-resource languages (Rikters, 2018; Rikters et al., 2018). While attention mechanisms can provide explanations for model predictions, Serrano and Smith (2019) challenge that assumption and find that attention weights only noisily predict overall importance with regard to the model. Jain and Wallace (2019) find that attention distributions rarely correlate with feature importance weights. However, Wiegreffe and Pinter (2019) show through alternative tests that prior work does not discredit the usefulness of attention for interpretability. Xiao et al. (2019) introduce the Label-Specific Attention Network (LSAN) for multi-label document classification. They use label descriptions to compute attention scores for words, and follow the self-attention of"
2020.findings-emnlp.65,P18-1108,0,0.0932308,"Missing"
2020.findings-emnlp.65,P17-1076,0,0.217892,"erson Categ <VP> HEAD driving 3.3 Dependency Parsing NP NP DT the 2 DT the 2 VP NN person 3 NN person 3 VBG driving 4 VBG driving 4 Figure 5: Parsing representations of the example sentence in Figure 2. where LN is Layer Normalization, and W1 , W2 , b1 and b2 are learned parameters. For the l-th syntactic category, the corresponding score s(i, j, l) is then the l-th value in the S(i, j) vector. Consequently, the score of a constituency parse tree T is the sum of all of the scores of its spans and their syntactic categories: s(T ) = X s(i, j, l) (7) (i,j,l)∈T We then use a CKY-style algorithm (Stern et al., 2017; Gaddy et al., 2018) to find the highest scoring tree Tˆ. The model is trained to find the correct parse tree T ∗ , such that for all trees T , the following margin constraint is satisfied: s(T ∗ ) ≥ s(T ) + ∆(T, T ∗ ) (8) where ∆ is the Hamming loss on labeled spans. The corresponding loss function is the hinge loss: We use the biaffine attention mechanism (Dozat and Manning, 2016) to compute a probability distribution for the dependency head of each word. The child-parent score αij for the j-th word to be the head of the i-th word is: (d) T αij = hi (h) (d) (h) Whj +UT hi +VT hj +b (10) (d)"
2020.findings-emnlp.65,D15-1166,0,0.336292,"arn relations between syntactic categories and show pathways to analyze errors. WiVX Repeated Self-Attention Head Ai WiVX Select the person Aggregating with output from other heads Select the person Select the person X WiKX Matrix Projection Matrix Projection From other heads X Select the Select the person Select the person person Figure 1: Comparison of the attention head architectures of our proposed Label Attention Layer and a SelfAttention Layer (Vaswani et al., 2017). The matrix X represents the input sentence “Select the person”. Introduction Attention mechanisms (Bahdanau et al., 2014; Luong et al., 2015) provide arguably explainable attention distributions that can help to interpret predictions. For example, for their machine translation predictions, Bahdanau et al. (2014) show a heat map of attention weights from source language words to target language words. Similarly, in transformer architectures (Vaswani et al., 2017), a selfattention head produces attention distributions from the input words to the same input words, as shown in the second row on the right side of Figure 1. However, self-attention mechanisms have multiple heads, making the combined outputs difficult to interpret. Recent"
2020.findings-emnlp.65,D18-1548,0,0.0554408,"Missing"
2020.findings-emnlp.65,I17-1007,0,0.040274,"Missing"
2020.findings-emnlp.65,P18-2097,0,0.0224856,"Missing"
2020.findings-emnlp.65,P18-1130,0,0.0342223,"Missing"
2020.findings-emnlp.65,D18-1489,0,0.0224491,"Missing"
2020.findings-emnlp.65,J93-2004,0,0.0719252,"g Prediction: Noun Phrase (NP) Span Representation of “the person” Normalization and Average Fraction of contribution from the heads to the span vector Heads #1 #2 #3 #4 Here, heads #1 and #2 have the highest contributions to predicting “the person” as a noun phrase. Figure 6: If we remove the position-wise feed-forward layer, we can compute the contributions from each label attention head to the span representation, and thus interpret head contributions. This illustrative example follows the label color scheme in Figure 4. 3.4 Decoder We evaluate our model on the English Penn Treebank (PTB) (Marcus et al., 1993) and on the Chinese Treebank (CTB) (Xue et al., 2005). We use the Stanford tagger (Toutanova et al., 2003) to predict part-of-speech tags and follow standard data splits. Following standard practice, we use the EVALB algorithm (Sekine and Collins, 1997) for constituency parsing, and report results without punctuation for dependency parsing. the model is not designed to have a one-on-one correspondence between attention heads and syntactic categories. The Chinese Treebank is a smaller dataset, and therefore we use 64 heads in Chineselanguage experiments, even though the number of Chinese syntac"
2020.findings-emnlp.65,C18-1011,0,0.0340458,"Missing"
2020.findings-emnlp.65,N03-1033,0,0.0900673,"f contribution from the heads to the span vector Heads #1 #2 #3 #4 Here, heads #1 and #2 have the highest contributions to predicting “the person” as a noun phrase. Figure 6: If we remove the position-wise feed-forward layer, we can compute the contributions from each label attention head to the span representation, and thus interpret head contributions. This illustrative example follows the label color scheme in Figure 4. 3.4 Decoder We evaluate our model on the English Penn Treebank (PTB) (Marcus et al., 1993) and on the Chinese Treebank (CTB) (Xue et al., 2005). We use the Stanford tagger (Toutanova et al., 2003) to predict part-of-speech tags and follow standard data splits. Following standard practice, we use the EVALB algorithm (Sekine and Collins, 1997) for constituency parsing, and report results without punctuation for dependency parsing. the model is not designed to have a one-on-one correspondence between attention heads and syntactic categories. The Chinese Treebank is a smaller dataset, and therefore we use 64 heads in Chineselanguage experiments, even though the number of Chinese syntactic categories is much higher. For both languages, the query, key and value vectors, as well as the output"
2020.findings-emnlp.65,D18-1311,0,0.0280675,"Missing"
2020.findings-emnlp.65,P19-1230,0,0.0870595,"nt work in multi-label text classification (Xiao et al., 2019) and sequence labeling (Cui and Zhang, 2019) shows the efficiency and interpretability of label-specific representations. We introduce the Label Attention Layer: a modified version of self-attention, where each classification label corresponds to one or more attention heads. We project the output at the attention head level, rather than after aggregating all outputs, to preserve the source of head-specific information, thus allowing us to match labels to heads. To test our proposed Label Attention Layer, we build upon the parser of Zhou and Zhao (2019) and establish a new state of the art for both constituency and dependency parsing, in both English and Chinese. We also release our pre-trained parsers, as well as our code to encourage experiments with the Label Attention Layer 1 . 2 Label Attention Layer The self-attention mechanism of Vaswani et al. (2017) propagates information between the words of a sentence. Each resulting word representation 1 Available at: GitHub.com/KhalilMrini/LAL-Parser 731 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 731–742 c November 16 - 20, 2020. 2020 Association for Computation"
2020.findings-emnlp.65,D19-1002,0,0.0192979,"tate of the art in language understanding (Devlin et al., 2018; Yang et al., 2019), extractive summarization (Liu, 2019), semantic role labeling (Strubell et al., 2018) and machine translation for low-resource languages (Rikters, 2018; Rikters et al., 2018). While attention mechanisms can provide explanations for model predictions, Serrano and Smith (2019) challenge that assumption and find that attention weights only noisily predict overall importance with regard to the model. Jain and Wallace (2019) find that attention distributions rarely correlate with feature importance weights. However, Wiegreffe and Pinter (2019) show through alternative tests that prior work does not discredit the usefulness of attention for interpretability. Xiao et al. (2019) introduce the Label-Specific Attention Network (LSAN) for multi-label document classification. They use label descriptions to compute attention scores for words, and follow the self-attention of Lin et al. (2017). Cui and Zhang (2019) introduce a Label Attention Inference Layer for sequence labeling, which uses the self-attention of Vaswani et al. (2017). In this case, the key and value vectors are learned label embeddings, and the query vectors are hidden vec"
2020.findings-emnlp.65,D19-1044,0,0.126153,"istributions that can help to interpret predictions. For example, for their machine translation predictions, Bahdanau et al. (2014) show a heat map of attention weights from source language words to target language words. Similarly, in transformer architectures (Vaswani et al., 2017), a selfattention head produces attention distributions from the input words to the same input words, as shown in the second row on the right side of Figure 1. However, self-attention mechanisms have multiple heads, making the combined outputs difficult to interpret. Recent work in multi-label text classification (Xiao et al., 2019) and sequence labeling (Cui and Zhang, 2019) shows the efficiency and interpretability of label-specific representations. We introduce the Label Attention Layer: a modified version of self-attention, where each classification label corresponds to one or more attention heads. We project the output at the attention head level, rather than after aggregating all outputs, to preserve the source of head-specific information, thus allowing us to match labels to heads. To test our proposed Label Attention Layer, we build upon the parser of Zhou and Zhao (2019) and establish a new state of the art for"
2020.findings-emnlp.65,N16-1174,0,0.0527521,"them is either head 35 or 47, both top-contributing heads of spans predicted as NP. Likewise, for the 193 spans labelled as S but not predicted as such, the top-contributing head of 141 of them is one of the four top-contributing heads for spans predicted as S. This suggests that a stronger prediction link to the label attention heads, through a loss function for instance, may increase the performance. 5 Related Work Since their introduction in Machine Translation, attention mechanisms (Bahdanau et al., 2014; Luong et al., 2015) have been extended to other tasks, such as text classification (Yang et al., 2016), natural language inference (Chen et al., 2016) and language modeling (Salton et al., 2017). Self-attention and transformer architectures (Vaswani et al., 2017) are now the state of the art in language understanding (Devlin et al., 2018; Yang et al., 2019), extractive summarization (Liu, 2019), semantic role labeling (Strubell et al., 2018) and machine translation for low-resource languages (Rikters, 2018; Rikters et al., 2018). While attention mechanisms can provide explanations for model predictions, Serrano and Smith (2019) challenge that assumption and find that attention weights only noi"
2020.findings-emnlp.87,D18-1217,0,0.0243125,"ed sparse transformer and cross attention information fusion outperform previous systems adapted from the machine translation and graph generation literature. We further contribute our large graph modification datasets to the research community to encourage future research for this new problem. 1 Introduction Parsing text into structured semantics representation is one of the most long-standing and active research problems in NLP. Numerous parsing methods have been developed for many different semantic structure representations (Chen and Manning, 2014; Mrini et al., 2019; Zhou and Zhao, 2019; Clark et al., 2018; Wang et al., 2018). However, most of these previous works focus on parsing a single sentence, while a typical human-computer interaction session or conversation is not singleturn. A prominent example is image search. Users usually start with short phrases describing the main objects or topics they are looking for. Depending on the result, the users may then modify their query to add more constraints or give additional information. In this case, without the modification capability, a static representation is not suitable to track the changing intent of the user. We argue that the back-and-for"
2020.findings-emnlp.87,Q19-1019,0,0.355462,"e (i), the system copies the source graph to the target graph3 . In the “Text2Text” baseline (ii), we flatten the graph and reconstruct the natural sentence similarly to the modification query. In the “Modified GraphRNN” baseline (iii), we use the breadth-first-search (BFS) based node ordering to flatten the graph4 , and use RNNs as the encoders (You et al., 2018) and a decoder similar to our systems. In the final two baselines, “Graph Transformer” (iv) and “Deep Convolutional Graph Networks” (DCGCN) (v), we use the Graph Transformers (Cai and Lam, 2019) and Deep Convolutional Graph Networks (Guo et al., 2019) to encode the source graph (the decoder is identical to ours). boy boy in shirt shirt in &lt;attribute> black black &lt;null> &lt;attribute> Figure 6: Adjacency matrix style decoder. We use an attentional decoder using GRU units for generating edges. It operates similarly to the node-level decoder using Equation 11 and Equation 12. For more accurate typed edge generation, however, we incorporate the hidden states of the source and target nodes (from the node decoder) as inputs when updating the hidden state of the edge decoder: N E hEi,j = GRUE (zi,j−1 , hN i , hj , hi,j−1 ), (14) where hEi,j is the h"
2020.findings-emnlp.87,U19-1013,1,0.829644,"nodes from the source graph are not preserved. We believe that the proposed approach can reduce the noise in graph generation, and retain fine-grained details better than the baselines. 5 Related Work Semantic parsing is a sequence-to-graph transduction task, mapping natural language sentences to their meaning representation, e.g. see (Buys and Blunsom, 2017; Iyer et al., 2017; Dong and Lapata, 2018); this is different from our graph conditional semantic parsing. Recently, context-dependent semantic parsing has gained attraction (Iyyer et al., 2017; Srivastava et al., 2017; Suhr et al., 2018; He et al., 2019). Our work focuses on the update of scene graphs based on users’ queries, while previous works model the modifications of semantic representations in multi-turn dialogue systems. Due to their effectiveness, GCNs and graph transformer have been used as graph encoder for graphto-sequence transduction in semantic-based text generation (Bastings et al., 2017; Beck et al., 2018; Guo et al., 2019; Cai and Lam, 2019; Song et al., 2018; Wu et al., 2020). 6 Conclusion In this paper, we explore a novel problem of conditional graph modification, in which a system needs to modify a source graph according"
2020.findings-emnlp.87,P17-1089,0,0.0249495,"evidenced by the first example A. In addition, example B demonstrates when graph transformer observes a longer description, it lacks the capability of fusing the semantics between the source graph and the modification query; then certain nodes from the source graph are not preserved. We believe that the proposed approach can reduce the noise in graph generation, and retain fine-grained details better than the baselines. 5 Related Work Semantic parsing is a sequence-to-graph transduction task, mapping natural language sentences to their meaning representation, e.g. see (Buys and Blunsom, 2017; Iyer et al., 2017; Dong and Lapata, 2018); this is different from our graph conditional semantic parsing. Recently, context-dependent semantic parsing has gained attraction (Iyyer et al., 2017; Srivastava et al., 2017; Suhr et al., 2018; He et al., 2019). Our work focuses on the update of scene graphs based on users’ queries, while previous works model the modifications of semantic representations in multi-turn dialogue systems. Due to their effectiveness, GCNs and graph transformer have been used as graph encoder for graphto-sequence transduction in semantic-based text generation (Bastings et al., 2017; Beck"
2020.findings-emnlp.87,P17-1167,0,0.0327182,"Missing"
2020.findings-emnlp.87,W15-3014,0,0.0302254,"denote the nodes and edges of the graph zG . Given a training dataset of input-output pairs, denoted by D ≡ {(xGd , yd , zGd )}D d=1 , we train the model by maximizing the conditional loglikelihood `CLL = `Node + `Edge where, `Node = X log p(zN |x, y; θN ) (2) (x,y,z)∈D `Edge = X log p(zE |x, y, zN ; θE ). (3) (x,y,z)∈D During learning and decoding, we sort the nodes according to a topological order which exists for all the directed graphs in our user-generated and synthetic datasets. 3.2 Graph-based Encoder-Decoder Model Inspired by the machine translation literature (Bahdanau et al., 2014; Jean et al., 2015), we build our model based on the encoder-decoder framework. Since our task takes a source graph and a modification query as inputs, we need two encoders to model the graph and text information separately. Thus, there are four main components in our model: the query encoder, the graph encoder, the edge decoder and the node decoder. The information flow between the components is shown in Figure 4. In general, we encode the graph and text modification query into a joint representation, then we generate the target graph in two stages. Firstly, the target 975 nodes are generated via a node-level r"
2020.findings-emnlp.87,P18-1068,0,0.0199741,"rst example A. In addition, example B demonstrates when graph transformer observes a longer description, it lacks the capability of fusing the semantics between the source graph and the modification query; then certain nodes from the source graph are not preserved. We believe that the proposed approach can reduce the noise in graph generation, and retain fine-grained details better than the baselines. 5 Related Work Semantic parsing is a sequence-to-graph transduction task, mapping natural language sentences to their meaning representation, e.g. see (Buys and Blunsom, 2017; Iyer et al., 2017; Dong and Lapata, 2018); this is different from our graph conditional semantic parsing. Recently, context-dependent semantic parsing has gained attraction (Iyyer et al., 2017; Srivastava et al., 2017; Suhr et al., 2018; He et al., 2019). Our work focuses on the update of scene graphs based on users’ queries, while previous works model the modifications of semantic representations in multi-turn dialogue systems. Due to their effectiveness, GCNs and graph transformer have been used as graph encoder for graphto-sequence transduction in semantic-based text generation (Bastings et al., 2017; Beck et al., 2018; Guo et al."
2020.findings-emnlp.87,P81-1022,0,0.561842,"Missing"
2020.findings-emnlp.87,D18-1045,0,0.0220468,", 2017; Ren et al., 2018), we consider the scene graph modification problem as follows. Given an initial scene graph and a new query issued by the user, the goal is to generate a new scene graph taking into account the original graph and the new query. We formulate the problem as conditional graph modification, and create three datasets for this problem. We propose novel encoder-decoder architectures for conditional graph modification. More specifically, our graph encoder is built upon the self-attention architecture popular in state-of-theart machine translation models (Vaswani et al., 2017; Edunov et al., 2018), which is superior to, according to our study, Graph Convolutional Networks (GCN) (Kipf and Welling, 2016). Unique to our problem, however, is the fact that we have an open set of relation types in the graphs. Thus, we propose a novel graph-conditioned sparse transformer, in which the relation information is embed972 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 972–990 c November 16 - 20, 2020. 2020 Association for Computational Linguistics A young boy in a black shirt ded directly into the self-attention grid. For the decoder, we treat the graph modification t"
2020.findings-emnlp.87,D15-1166,0,0.0214868,"according to the connections of the sparsely connected transformer as well as all query tokens. The final representation m is taken from the output of transformer. Figure 5 shows the information flow in the cross-attention mechanism. 3.2.4 We use GRU cells (Cho et al., 2014) for our RNN decoders. The node-level decoder is a vanilla autoregressive model described as, N N hN t = GRU (zt−1 , ht−1 ) (10) N N cN t = ATTN (ht , m) (11) Figure 5: Cross-attention fusion. (12) N softmax(W[hN t , ct ] + b), (13) where z&lt;t denotes the nodes generated before time step t, ATTNN is a Luong-style attention (Luong et al., 2015), and m is the memory vectors from information fusion of the encoders (see §3.2.3). 3.2.5 Recall that the parameters of the graph and query encoders are shared to enable encoding of the two sources in the same semantic space. That is, we use the same transformer encoder for both sources. In cross-attention, we concatenate the x (from Equation 4) and y before rather than after the transformer encoder. As such, the encoder’s input is [x, y]. In the transformer, the representation of each query token gets updated by self-attending to the representations of all the query tokens and graph nodes in"
2020.lrec-1.170,D14-1181,0,0.00306743,"ed baseline models to automatically infer the phenotype based on the patient note, which we approach as a multi-label, multi-class text classification task (Gehrmann et al., 2018). Each of the baseline model is a binary classifier indicating whether a given phenotype is present in the input patient note. As a result, we train a separate model for each phenotype. Bag of Words + Logistic Regression We convert each patient note into a bag of words, and give as input to a logistic regression. Convolutional Neural Network (CNN) We follow the CNN architecture proposed by Collobert et al. (2011) and Kim (2014). We use the convolution widths from 1 to 4, and for each convolution width we set the number of filters to 100. We use dropout with a probability of 0.5 to reduce overfitting (Srivastava et al., 2014). The trainable parameters were initialized using a uniform distribution from −0.05 to 0.05. The model was optimized with adadelta (Zeiler, 2012). We use word2vec (Mikolov et al., 2013) as the word embeddings, which we pretrain on all the notes of MIMIC III v3. Table 5 presents the performance of the two baseline models (F1-score). 1 2 https://mimic.physionet.org https://github.com/EdwardMoseley/"
2020.lrec-1.180,P04-3031,0,0.409686,"Missing"
2020.lrec-1.180,Q17-1010,0,0.0109897,"languages. We calN (King culate the weight for each document category by N l and Zeng, 2001), where N is the number of documents in each language and Nl is the number of documents labeled by the category. Particularly, for training BERT model, we append two additional tokens, “[CLS]” and “[SEP]”, at the start and end of each document respectively. For the neural models, we pad each document or drop rest of words up to 40 tokens. We use “unknown” as a replacement for unknown tokens. We initialize CNN and RNN classifiers by pre-trained word embeddings (Mikolov et al., 2013; Godin et al., 2015; Bojanowski et al., 2017; Deriu et al., 2017) and train the networks up to 10 epochs. LR. We first extract TF-IDF-weighted features of uni, bi-, and tri-grams on the corpora, using the most fre1443 Language English Language Polish Method LR CNN RNN BERT Acc .874 .878 .898 .705 F1-w .874 .877 .896 .635 F1-m .841 .845 .867 .579 AUC .920 .927 .938 .581 Language Method LR CNN RNN BERT Acc .864 .855 .857 .824 F1-w .846 .851 .854 .782 F1-m .653 .688 .696 .478 AUC .804 .813 .822 .474 Language Language Spanish Method LR CNN RNN BERT Italian Portuguese Acc .704 .650 .674 .605 F1-w .707 .654 .674 .573 Method LR CNN RNN BERT Ac"
2020.lrec-1.180,W19-3504,0,0.282638,"in derived synthetic author demographic attributes instead of the original author information. The common data sources either derive from Wikipedia toxic comments (Dixon et al., 2018; Park et al., 2018; Garg et al., 2019) or synthetic document templates (Kiritchenko and Mohammad, 2018; Park et al., 2018). The Wikipedia Talk corpus1 (Wulczyn et al., 2017) provides demographic information of annotators instead of the authors, Equity Evaluation Corpus2 (Kiritchenko and Mohammad, 2018) are created by sentence templates and combinations of racial names and gender coreferences. While existing work (Davidson et al., 2019; Diaz et al., 2018) infers user demographic information (white/black, young/old) from the text, such inference is still likely to cause confounding erThe work was partially done when the first author worked as an intern at Adobe Research. 1 https://figshare.com/articles/Wikipedia_ Detox_Data/4054689 2 http://saifmohammad.com/WebPages/ Biases-SA.html rors that impact and break the independence between demographic factors and the fairness evaluation of text classifiers. Second, existing research in the fairness evaluation mainly focus on only English resources, such as age biases in blog posts"
2020.lrec-1.180,N19-1423,0,0.0873731,"t the values of our approach that to avoid confounding errors, we obtain author demographic information independently from the user generated documents. 4. Demographic variations root in documents, especially in social media data (Volkova et al., 2013; Hovy, 2015; Johannsen et al., 2015). Such variations could further impact the performance and fairness of document classifiers. In this study, we experiment four different classification models including logistic regression (LR), recurrent neural network (RNN) (Chung et al., 2014), convolutional neural network (CNN) (Kim, 2014) and Google BERT (Devlin et al., 2019). We present the baseline results of both performance and fairness evaluations across the multilingual corpus. 4.1. word 16.1 age 16.7 11.7 country gender Demographic Factors 11.7 race Figure 1: Predictability of demographic attributes from the English data. We show the absolute percentage improvements in accuracy over majority-class baselines. The majority-class baselines of accuracy are .500 for the binary predictions. The darker color indicates higher improvements and vice versa. The improved prediction accuracy scores over majority baselines suggest that language variations across demograp"
2020.lrec-1.180,W19-3510,0,0.0900787,"educe and evaluate the demographic bias in English corpora may not apply to other languages. For example, Spanish has gender-dependent nouns, but this does not exist in English (Sun et al., 2019b); and Portuguese varies across Brazil and Portugal in both word usage and grammar (Maier and Gómez-Rodríguez, 2014). The rich variations have not been explored under the fairness evaluation due to lack of multilingual corpora. Additionally, while we have hate speech detection datasets in multiple languages (Waseem and Hovy, 2016; Sanguinetti et al., 2018; Ptaszynski et al., 2019; Basile et al., 2019; Fortuna et al., 2019), there is still no integrated multilingual corpora that contain author demographic attributes which can be used to measure group fairness. The lack of author demographic attributes and multilingual datasets limits research for evaluating classifier fairness and developing unbiased classifiers. In this study, we combine previously published corpora labeled for Twitter hate speech recognition in English (Waseem and Hovy, 2016; Waseem, 2016; Founta et al., 2018), Italian (Sanguinetti et al., 2018), Polish (Ptaszynski et al., 2019), Portuguese (Fortuna et al., 2019), and Spanish (Basile et al., 2"
2020.lrec-1.180,S19-2080,0,0.0228826,"ics and useful signals. We implement a BERT-based classification model by HuggingFace’s Transformers (Wolf et al., 2019). The model encodes each document into a fixed size (768) of representation and feed to a linear prediction layer. The model is optimized by AdamW with a warmup and learning rate as .1 and 2e−5 respectively. We leave parameters as their default, conduct fine-tuning steps with 4 epochs and set batch size as 32 (Sun et al., 2019a). The classification model loads “bert-base-uncased” pre-trained BERT model for English and “bert-base-multilingual-uncased” multilingual BERT model (Gertner et al., 2019) for the other languages. The multilingual BERT model follows the same method of BERT by using Wikipedia text from the top 104 languages. Due to the label imbalance shown in Table 1, we balance training instances by randomly oversampling the minority during the training process. 4.3. Evaluation Metrics Performance Evaluation. To measure overall performance, we evaluate models by four metrics: accuracy (Acc), weighted F1 score (F1-w), macro F1 score (F1-m) and area under the ROC curve (AUC). The F1 score coherently comprecision∗recall bines both precision and recall by 2 ∗ precision+recall . We"
2020.lrec-1.180,W15-4322,0,0.0686378,"Missing"
2020.lrec-1.180,P15-1073,0,0.296437,"er privacy, we will not publicize the personal profile information, including user ids, photos, geocoordinates as well as other user profile information, which were used to infer the demographic attributes. We will, however, provide inferred demographic attributes in their original formats from the Face++ and Google Maps based on per request to allow wider researchers and communities to replicate the methodology and probe more depth of fairness in document classification. 3. Language Variations across Demographic Groups Demographic factors can improve the performances of document classifiers (Hovy, 2015), and demographic variations root in language, especially in social media data (Volkova et al., 2013; Hovy, 2015). For example, language styles are highly correlated with authors’ demographic attributes, such as age, race, gender and location (Coulmas, 2017; PreoţiucPietro and Ungar, 2018). Research (Bolukbasi et al., 2016; Zhao et al., 2017; Garg et al., 2018) find that biases and stereotypes exist in word embeddings, which is widely used in document classification tasks. For example, “receptionist” is closer to females while “programmer” is closer to males, and “professor” is closer to Asian"
2020.lrec-1.180,S19-1015,1,0.834911,"a_ Detox_Data/4054689 2 http://saifmohammad.com/WebPages/ Biases-SA.html rors that impact and break the independence between demographic factors and the fairness evaluation of text classifiers. Second, existing research in the fairness evaluation mainly focus on only English resources, such as age biases in blog posts (Diaz et al., 2018), gender biases in Wikipedia comments (Dixon et al., 2018) and racial biases in hate speech detection (Davidson et al., 2019). Different languages have shown different patterns of linguistic variations across the demographic attributes (Johannsen et al., 2015; Huang and Paul, 2019), methods (Zhao et al., 2017; Park et al., 2018) to reduce and evaluate the demographic bias in English corpora may not apply to other languages. For example, Spanish has gender-dependent nouns, but this does not exist in English (Sun et al., 2019b); and Portuguese varies across Brazil and Portugal in both word usage and grammar (Maier and Gómez-Rodríguez, 2014). The rich variations have not been explored under the fairness evaluation due to lack of multilingual corpora. Additionally, while we have hate speech detection datasets in multiple languages (Waseem and Hovy, 2016; Sanguinetti et al.,"
2020.lrec-1.180,K15-1011,0,0.0372314,"re.com/articles/Wikipedia_ Detox_Data/4054689 2 http://saifmohammad.com/WebPages/ Biases-SA.html rors that impact and break the independence between demographic factors and the fairness evaluation of text classifiers. Second, existing research in the fairness evaluation mainly focus on only English resources, such as age biases in blog posts (Diaz et al., 2018), gender biases in Wikipedia comments (Dixon et al., 2018) and racial biases in hate speech detection (Davidson et al., 2019). Different languages have shown different patterns of linguistic variations across the demographic attributes (Johannsen et al., 2015; Huang and Paul, 2019), methods (Zhao et al., 2017; Park et al., 2018) to reduce and evaluate the demographic bias in English corpora may not apply to other languages. For example, Spanish has gender-dependent nouns, but this does not exist in English (Sun et al., 2019b); and Portuguese varies across Brazil and Portugal in both word usage and grammar (Maier and Gómez-Rodríguez, 2014). The rich variations have not been explored under the fairness evaluation due to lack of multilingual corpora. Additionally, while we have hate speech detection datasets in multiple languages (Waseem and Hovy, 20"
2020.lrec-1.180,D14-1181,0,0.0200037,"the black. This can highlight the values of our approach that to avoid confounding errors, we obtain author demographic information independently from the user generated documents. 4. Demographic variations root in documents, especially in social media data (Volkova et al., 2013; Hovy, 2015; Johannsen et al., 2015). Such variations could further impact the performance and fairness of document classifiers. In this study, we experiment four different classification models including logistic regression (LR), recurrent neural network (RNN) (Chung et al., 2014), convolutional neural network (CNN) (Kim, 2014) and Google BERT (Devlin et al., 2019). We present the baseline results of both performance and fairness evaluations across the multilingual corpus. 4.1. word 16.1 age 16.7 11.7 country gender Demographic Factors 11.7 race Figure 1: Predictability of demographic attributes from the English data. We show the absolute percentage improvements in accuracy over majority-class baselines. The majority-class baselines of accuracy are .500 for the binary predictions. The darker color indicates higher improvements and vice versa. The improved prediction accuracy scores over majority baselines suggest th"
2020.lrec-1.180,S18-2005,0,0.0376384,"irness, multilingual, document classification, hate speech 1. Introduction While document classification models should be objective and independent from human biases in documents, research have shown that the models can learn human biases and therefore be discriminatory towards particular demographic groups (Dixon et al., 2018; Borkan et al., 2019; Sun et al., 2019b). The goal of fairness-aware document classifiers is to train and build non-discriminatory models towards people no matter what their demographic attributes are, such as gender and ethnicity. Existing research (Dixon et al., 2018; Kiritchenko and Mohammad, 2018; Park et al., 2018; Garg et al., 2019; Borkan et al., 2019) in evaluating fairness of document classifiers focus on the group fairness (Chouldechova and Roth, 2018), which refers to every demographic group has equal probability of being assigned to the positive predicted document category. However, the lack of original author demographic attributes and multilingual corpora bring challenges towards the fairness evaluation of document classifiers. First, the datasets commonly used to build and evaluate the fairness of document classifiers obtain derived synthetic author demographic attributes i"
2020.lrec-1.180,D14-1108,0,0.0426124,"Missing"
2020.lrec-1.180,W14-4204,0,0.0302902,"s in Wikipedia comments (Dixon et al., 2018) and racial biases in hate speech detection (Davidson et al., 2019). Different languages have shown different patterns of linguistic variations across the demographic attributes (Johannsen et al., 2015; Huang and Paul, 2019), methods (Zhao et al., 2017; Park et al., 2018) to reduce and evaluate the demographic bias in English corpora may not apply to other languages. For example, Spanish has gender-dependent nouns, but this does not exist in English (Sun et al., 2019b); and Portuguese varies across Brazil and Portugal in both word usage and grammar (Maier and Gómez-Rodríguez, 2014). The rich variations have not been explored under the fairness evaluation due to lack of multilingual corpora. Additionally, while we have hate speech detection datasets in multiple languages (Waseem and Hovy, 2016; Sanguinetti et al., 2018; Ptaszynski et al., 2019; Basile et al., 2019; Fortuna et al., 2019), there is still no integrated multilingual corpora that contain author demographic attributes which can be used to measure group fairness. The lack of author demographic attributes and multilingual datasets limits research for evaluating classifier fairness and developing unbiased classif"
2020.lrec-1.180,D18-1302,0,0.187707,"lassification, hate speech 1. Introduction While document classification models should be objective and independent from human biases in documents, research have shown that the models can learn human biases and therefore be discriminatory towards particular demographic groups (Dixon et al., 2018; Borkan et al., 2019; Sun et al., 2019b). The goal of fairness-aware document classifiers is to train and build non-discriminatory models towards people no matter what their demographic attributes are, such as gender and ethnicity. Existing research (Dixon et al., 2018; Kiritchenko and Mohammad, 2018; Park et al., 2018; Garg et al., 2019; Borkan et al., 2019) in evaluating fairness of document classifiers focus on the group fairness (Chouldechova and Roth, 2018), which refers to every demographic group has equal probability of being assigned to the positive predicted document category. However, the lack of original author demographic attributes and multilingual corpora bring challenges towards the fairness evaluation of document classifiers. First, the datasets commonly used to build and evaluate the fairness of document classifiers obtain derived synthetic author demographic attributes instead of the origi"
2020.lrec-1.180,C18-1130,0,0.164262,"Missing"
2020.lrec-1.180,L18-1443,0,0.110849,"ang and Paul, 2019), methods (Zhao et al., 2017; Park et al., 2018) to reduce and evaluate the demographic bias in English corpora may not apply to other languages. For example, Spanish has gender-dependent nouns, but this does not exist in English (Sun et al., 2019b); and Portuguese varies across Brazil and Portugal in both word usage and grammar (Maier and Gómez-Rodríguez, 2014). The rich variations have not been explored under the fairness evaluation due to lack of multilingual corpora. Additionally, while we have hate speech detection datasets in multiple languages (Waseem and Hovy, 2016; Sanguinetti et al., 2018; Ptaszynski et al., 2019; Basile et al., 2019; Fortuna et al., 2019), there is still no integrated multilingual corpora that contain author demographic attributes which can be used to measure group fairness. The lack of author demographic attributes and multilingual datasets limits research for evaluating classifier fairness and developing unbiased classifiers. In this study, we combine previously published corpora labeled for Twitter hate speech recognition in English (Waseem and Hovy, 2016; Waseem, 2016; Founta et al., 2018), Italian (Sanguinetti et al., 2018), Polish (Ptaszynski et al., 20"
2020.lrec-1.180,P19-1163,0,0.0531452,"re not accurate enough to provide fine-grained information, our attribute categories are still too coarsegrained (binary age groups and gender, and only four race categories). Using coarse-grained attributes would hide the identities of specific demographic groups, including other racial minorities and people with non-binary gender. Broadening our analyses and evaluations to include more attribute values may require better methods of user attribute inference or different sources of data. Third, language variations across demographic groups might introduce annotation biases. Existing research (Sap et al., 2019) shows that annotators are more likely to annotate tweets containing African American English words as hate speech. Additionally, the nationality and educational level might also impact on the quality of annotations (Founta et al., 2018). Similarly, different annotation sources of our dataset (which merged two different corpora) might have variations in annotating schema. To reduce annotation biases due to the different annotating schema, we merge the annotations into the two most compatible document categories: normal and hate speech. Annotation biases might still exist, therefore, we will re"
2020.lrec-1.180,P19-1159,0,0.0527636,"Missing"
2020.lrec-1.180,D13-1187,0,0.15713,"tos, geocoordinates as well as other user profile information, which were used to infer the demographic attributes. We will, however, provide inferred demographic attributes in their original formats from the Face++ and Google Maps based on per request to allow wider researchers and communities to replicate the methodology and probe more depth of fairness in document classification. 3. Language Variations across Demographic Groups Demographic factors can improve the performances of document classifiers (Hovy, 2015), and demographic variations root in language, especially in social media data (Volkova et al., 2013; Hovy, 2015). For example, language styles are highly correlated with authors’ demographic attributes, such as age, race, gender and location (Coulmas, 2017; PreoţiucPietro and Ungar, 2018). Research (Bolukbasi et al., 2016; Zhao et al., 2017; Garg et al., 2018) find that biases and stereotypes exist in word embeddings, which is widely used in document classification tasks. For example, “receptionist” is closer to females while “programmer” is closer to males, and “professor” is closer to Asian Americans while “housekeeper” is closer to Hispanic Americans. This motivates us to explore and tes"
2020.lrec-1.180,N16-2013,0,0.196528,"annsen et al., 2015; Huang and Paul, 2019), methods (Zhao et al., 2017; Park et al., 2018) to reduce and evaluate the demographic bias in English corpora may not apply to other languages. For example, Spanish has gender-dependent nouns, but this does not exist in English (Sun et al., 2019b); and Portuguese varies across Brazil and Portugal in both word usage and grammar (Maier and Gómez-Rodríguez, 2014). The rich variations have not been explored under the fairness evaluation due to lack of multilingual corpora. Additionally, while we have hate speech detection datasets in multiple languages (Waseem and Hovy, 2016; Sanguinetti et al., 2018; Ptaszynski et al., 2019; Basile et al., 2019; Fortuna et al., 2019), there is still no integrated multilingual corpora that contain author demographic attributes which can be used to measure group fairness. The lack of author demographic attributes and multilingual datasets limits research for evaluating classifier fairness and developing unbiased classifiers. In this study, we combine previously published corpora labeled for Twitter hate speech recognition in English (Waseem and Hovy, 2016; Waseem, 2016; Founta et al., 2018), Italian (Sanguinetti et al., 2018), Pol"
2020.lrec-1.180,W16-5618,0,0.416363,"hate speech detection datasets in multiple languages (Waseem and Hovy, 2016; Sanguinetti et al., 2018; Ptaszynski et al., 2019; Basile et al., 2019; Fortuna et al., 2019), there is still no integrated multilingual corpora that contain author demographic attributes which can be used to measure group fairness. The lack of author demographic attributes and multilingual datasets limits research for evaluating classifier fairness and developing unbiased classifiers. In this study, we combine previously published corpora labeled for Twitter hate speech recognition in English (Waseem and Hovy, 2016; Waseem, 2016; Founta et al., 2018), Italian (Sanguinetti et al., 2018), Polish (Ptaszynski et al., 2019), Portuguese (Fortuna et al., 2019), and Spanish (Basile et al., 2019), and publish this multilingual data augmented with author-level demographic information for four attributes: race, gender, age and country. The demographic factors are inferred from user profiles, which are independent from text documents, the tweets. To our best knowledge, this is the first multilingual hate speech corpus annotated with author attributes aiming for fairness evaluation. We start with presenting collection and inferen"
2020.lrec-1.180,D17-1323,0,0.270146,"aifmohammad.com/WebPages/ Biases-SA.html rors that impact and break the independence between demographic factors and the fairness evaluation of text classifiers. Second, existing research in the fairness evaluation mainly focus on only English resources, such as age biases in blog posts (Diaz et al., 2018), gender biases in Wikipedia comments (Dixon et al., 2018) and racial biases in hate speech detection (Davidson et al., 2019). Different languages have shown different patterns of linguistic variations across the demographic attributes (Johannsen et al., 2015; Huang and Paul, 2019), methods (Zhao et al., 2017; Park et al., 2018) to reduce and evaluate the demographic bias in English corpora may not apply to other languages. For example, Spanish has gender-dependent nouns, but this does not exist in English (Sun et al., 2019b); and Portuguese varies across Brazil and Portugal in both word usage and grammar (Maier and Gómez-Rodríguez, 2014). The rich variations have not been explored under the fairness evaluation due to lack of multilingual corpora. Additionally, while we have hate speech detection datasets in multiple languages (Waseem and Hovy, 2016; Sanguinetti et al., 2018; Ptaszynski et al., 20"
2020.lrec-1.180,L18-1404,0,0.0131256,"classifiers. We evaluate overall performance by four metrics including accuracy (Acc), weighted F1 score (F1-w), macro F1 score (F1-m) and area under the ROC curve (AUC). The higher score indicates better performance. We highlight models achieve the best performance in each column. quent 15K features with the minimum feature frequency as 2. We then train a LogisticRegression from scikitlearn (Pedregosa et al., 2011). We use “liblinear” as the solver function and leave the other parameters as default. CNN. We implement the Convolutional Neural Network (CNN) classifier described in (Kim, 2014; Zimmerman et al., 2018) by Keras (Chollet and others, 2015). We first apply 100 filters with three different kernel sizes, 3, 4 and 5. After the convolution operations, we feed the concatenated features to a fully connected layer and output document representations with 100 dimensions. We apply “softplus” function with a l2 regularization with .03 and a dropout rate with .3 in the dense layer. The model feeds the document representation to final prediction. We train the model with batch size 64, set model optimizer as Adam (Kingma and Ba, 2014) and calculate loss values by the cross entropy function. We keep all oth"
2020.lrec-1.180,P06-4018,0,\N,Missing
2020.lrec-1.180,Q18-1041,0,\N,Missing
2020.lrec-1.180,W02-0109,0,\N,Missing
2020.lrec-1.180,S19-2007,0,\N,Missing
2020.lrec-1.664,D18-2029,0,0.135977,"esentations for the question and sentence, respectively (where d0 is the dimensionality of the RNN hidden units). As computing the node representation is an essential process for acquiring information from a text, we investigate various approaches for encoding sentences, such as replacing the ELMo word representations using different methods (the GloVe (Pennington et al., 2014) or the BERT (Devlin et al., 2019)) and replacing the RNN function in equation (6) with the pooling method. Furthermore, we adopt the universal sentence encoding method based on the recently developed transformer model (Cer et al., 2018). Detailed information will be given in the section 5.5. Aggregation: An iterative attentive aggregation function to the neighbor nodes is utilized to compute the amount of information to be propagated to each node in the graph as follows: A(k) v = σ( X (k) a(k) · N(k) vu W u ), u∈N (v) exp(Svu ) , a(k) vu = P k exp(Svk ) (7) (k) | (k) S(k) · N(k) vu = (Nv ) · W u , 0 where Av ∈ Rd is the aggregated information for the vth node computed by attentive weighted summation of its neighbor nodes, avu is the attention weight between node 5402 0 v and its neighbor nodes u (u∈N (v)), Nu ∈ Rd is the uth"
2020.lrec-1.664,P18-1078,0,0.0117688,"Tran et al., 2018; Yoon et al., 2019), the proposed method achieves better performance when classifying supporting sentences. 5400 2. Related Work Previous researchers have also investigated neural networkbased models for MRQA. One line of inquiry employs an attention mechanism between tokens in the question and passage to compute the answer span from the given text (Seo et al., 2016; Wang et al., 2017). As the task scope was extended from specific- to open-domain QA, several models have been proposed to select a relevant paragraph from the text to predict the answer span (Wang et al., 2018; Clark and Gardner, 2018). However, none of these methods have addressed reasoning over multiple sentences. To understand the relational patterns in the dataset, researchers have also proposed graph neural network algorithms. (Kipf and Welling, 2017) proposed a graph convolutional network to classify graph-structured data. This model was further investigated for applications involving large-scale graphs (Hamilton et al., 2017), for the effectiveness of aggregating and combining graph nodes by employing an attention mechanism (Veliˇckovi´c et al., 2018), and for adopting recurrent node updates (Palm et al., 2018). Thes"
2020.lrec-1.664,N19-1240,0,0.0312282,"Missing"
2020.lrec-1.664,N19-1423,0,0.059869,"n the text (see the discussion in section 5.4.). Node representation: Question Q ∈ Rd×Q and sentence Si ∈ Rd×Si (where d is the dimensionality of the word embedding and Q and Si represent the lengths of the sequences in Q and Si , respectively) are processed to acquire the sentence-level information. Recent studies have shown that a pretrained language model helps the model capture Passage 1 ? ? ? ? Passage N q ? ? ? Figure 2: Topology of the proposed model. Each node represents a sentence from the passage and the question. the contextual meaning of words in the sentence (Peters et al., 2018; Devlin et al., 2019). Following this study, we select an ELMo (Peters et al., 2018) language model for the word-embedding layer of our model as follows: LQ = ELMo(Q), (LQ ∈Rd×Q ), LS = ELMo(S), (LS ∈Rd×S ). (5) Using these new representations, we compute the sentence representation as follows: Q Q hQ t = fθ (ht−1 , Lt ), hSt = fθ (hSt−1 , LSt ), Q N = hQ last , S N = (6) hSlast , where fθ is the RNN function with the weight parameters 0 0 θ and NQ ∈ Rd and NS ∈ Rd are node representations for the question and sentence, respectively (where d0 is the dimensionality of the RNN hidden units). As computing the node re"
2020.lrec-1.664,P17-1147,0,0.0467384,"Missing"
2020.lrec-1.664,D14-1181,0,0.00268216,"e question Q ∈ Rd×Q and target sentence S ∈ Rd×S (where d is a dimensionality of word embedding and Q and S are the length of the sequences in the question and sentence, respectively) is computed by applying an attention mechanism over the column vector in Q for each column vector in S. With the computed alignment, we obtain a corresponding vector AQ ∈ Rd×S as follows: | AQ = Q · softmax((WQ) S), (1) where W is a learned model parameter matrix. Comparison: An element-wise multiplication is employed as a comparison function to combine each pair of AQ and S into a vector C ∈ Rd×S . Aggregation: Kim (2014)’s CNN with n-types of filters is applied to aggregate all information in the vector C. Finally, the model employs a fully connected layer to compute the matching score between the question and the target sentence as follows: R = CNN(C), (R ∈ Rnd ), yˆc = softmax((R) |W + b ), (2) where yˆc is the predicted probability for the target class, c, and W ∈ Rnd×c and bias b are learned model parameters. The loss function for the model is cross-entropy between predicted labels and true-labels as follows: L = − log N X C X yi,c log(ˆ yi,c ), (3) i=1 c=1 where yi,c is the true label vector and yˆi,c is"
2020.lrec-1.664,D14-1162,0,0.0827075,"Missing"
2020.lrec-1.664,N18-1202,0,0.282844,"onsidered strong baselines since they can be directly applied to our task with the same objective function. Then we describe our proposed method. 4.1.2. CompAggr-kMax. This model (Bian et al., 2017) is an extension of the CompAggr model. The only differences lie in applying attention (in equation (1)) to both the Q and S side and applying k-max pooling before the softmax function as follows: | AQ = Q · softmax(kMax((WQ) S)), | AS = S · softmax(kMax((WS) Q)). 5401 (4) topology 4.1.3. CompClip-LM-LC. This model (Yoon et al., 2019) is an extension of the CompAggr-kMax model. It employs the ELMo (Peters et al., 2018) model to enhance the word embedding layer for the question and target sentence by adopting the pretrained contextual language model. Additionally, it develops a latent clustering method to compute topic information in texts automatically and to use it as auxiliary information to improve the model performance. 4.1.4. IWAN. This model (Shen et al., 2017a) is a variation model based on the compare aggregate framework. Unlike CompAggr, it employs RNNs to encode a sequence of the words in the text (question and target sentence independently). At the same time, it computes an inter-alignment weight"
2020.lrec-1.664,D16-1264,0,0.0604174,"Missing"
2020.lrec-1.664,D17-1122,0,0.302335,"standing texts and being able to answer a question posed by a human is a long-standing goal in the artificial intelligence field. With the rapid advancement of neural network-based models and the availability of large-scale datasets, such as SQuAD (Rajpurkar et al., 2016) and TriviaQA (Joshi et al., 2017), researchers have begun to concentrate on building automatic question-answering (QA) systems. One example of such a system is the machine-reading question-answering (MRQA) model, which provides answers to questions from given passages (Seo et al., 2016; Xiong et al., 2016; Wang et al., 2017; Shen et al., 2017b). Recently, research has revealed that most questions in existing MRQA datasets do not require reasoning across sentences in the given context (passage); instead, they can be answered by looking at only a single sentence (Weissenborn et al., 2017). Using this characteristic, a simple model can achieve performance competitive with that of a sophisticated model. However, in most real scenarios of QA applications, more than one sentence should be utilized to extract a correct answer. To alleviate this limitation of previous datasets, another type of dataset was developed in which answering the"
2020.lrec-1.664,N18-1115,1,0.865107,"the propagation process, the model learns to understand information that cannot be inferred when considering sentences in isolation. Unlike the previous studies, this work does not use the exact “answer span” information while detecting the supporting sentences. It shows a different way of using the HotPotQA dataset and provides researchers new opportunities to develop a subsystem that is integrated into the full-QA systems (i.e., MRQA). Through experiments, we demonstrate that compared with the widely used answer-selection models (Wang and Jiang, 2016; Bian et al., 2017; Shen et al., 2017a; Tran et al., 2018; Yoon et al., 2019), the proposed method achieves better performance when classifying supporting sentences. 5400 2. Related Work Previous researchers have also investigated neural networkbased models for MRQA. One line of inquiry employs an attention mechanism between tokens in the question and passage to compute the answer span from the given text (Seo et al., 2016; Wang et al., 2017). As the task scope was extended from specific- to open-domain QA, several models have been proposed to select a relevant paragraph from the text to predict the answer span (Wang et al., 2018; Clark and Gardner,"
2020.lrec-1.664,P17-1018,0,0.14571,"ment&apos;s MVP. … Understanding texts and being able to answer a question posed by a human is a long-standing goal in the artificial intelligence field. With the rapid advancement of neural network-based models and the availability of large-scale datasets, such as SQuAD (Rajpurkar et al., 2016) and TriviaQA (Joshi et al., 2017), researchers have begun to concentrate on building automatic question-answering (QA) systems. One example of such a system is the machine-reading question-answering (MRQA) model, which provides answers to questions from given passages (Seo et al., 2016; Xiong et al., 2016; Wang et al., 2017; Shen et al., 2017b). Recently, research has revealed that most questions in existing MRQA datasets do not require reasoning across sentences in the given context (passage); instead, they can be answered by looking at only a single sentence (Weissenborn et al., 2017). Using this characteristic, a simple model can achieve performance competitive with that of a sophisticated model. However, in most real scenarios of QA applications, more than one sentence should be utilized to extract a correct answer. To alleviate this limitation of previous datasets, another type of dataset was developed in w"
2020.lrec-1.664,K17-1028,0,0.0146767,"SQuAD (Rajpurkar et al., 2016) and TriviaQA (Joshi et al., 2017), researchers have begun to concentrate on building automatic question-answering (QA) systems. One example of such a system is the machine-reading question-answering (MRQA) model, which provides answers to questions from given passages (Seo et al., 2016; Xiong et al., 2016; Wang et al., 2017; Shen et al., 2017b). Recently, research has revealed that most questions in existing MRQA datasets do not require reasoning across sentences in the given context (passage); instead, they can be answered by looking at only a single sentence (Weissenborn et al., 2017). Using this characteristic, a simple model can achieve performance competitive with that of a sophisticated model. However, in most real scenarios of QA applications, more than one sentence should be utilized to extract a correct answer. To alleviate this limitation of previous datasets, another type of dataset was developed in which answering the question requires reasoning over multiple sentences in the given passages (Yang et al., 2018; Welbl et al., 2018). Figure 1 shows an example of a recently released dataset, the HotpotQA. This dataset consists of not only question-answer pairs with c"
2020.lrec-1.664,Q18-1021,0,0.020632,"re reasoning across sentences in the given context (passage); instead, they can be answered by looking at only a single sentence (Weissenborn et al., 2017). Using this characteristic, a simple model can achieve performance competitive with that of a sophisticated model. However, in most real scenarios of QA applications, more than one sentence should be utilized to extract a correct answer. To alleviate this limitation of previous datasets, another type of dataset was developed in which answering the question requires reasoning over multiple sentences in the given passages (Yang et al., 2018; Welbl et al., 2018). Figure 1 shows an example of a recently released dataset, the HotpotQA. This dataset consists of not only question-answer pairs with context passages but also supporting sentence information for answering the question annotated by a human. In this study, we build a model that exploits the relational information among sentences in passages to classify the supporting sentences that contain the essential information for answering the question. To this end, we propose a novel graph neural network model named propagateselector (PS), which can be directly employed as a subsystem in the QA pipeline"
2020.lrec-1.664,D18-1259,0,0.0680385,"tasets do not require reasoning across sentences in the given context (passage); instead, they can be answered by looking at only a single sentence (Weissenborn et al., 2017). Using this characteristic, a simple model can achieve performance competitive with that of a sophisticated model. However, in most real scenarios of QA applications, more than one sentence should be utilized to extract a correct answer. To alleviate this limitation of previous datasets, another type of dataset was developed in which answering the question requires reasoning over multiple sentences in the given passages (Yang et al., 2018; Welbl et al., 2018). Figure 1 shows an example of a recently released dataset, the HotpotQA. This dataset consists of not only question-answer pairs with context passages but also supporting sentence information for answering the question annotated by a human. In this study, we build a model that exploits the relational information among sentences in passages to classify the supporting sentences that contain the essential information for answering the question. To this end, we propose a novel graph neural network model named propagateselector (PS), which can be directly employed as a subsyst"
2020.lrec-1.670,P17-1147,0,0.0222362,"ideos with spoken narratives for a photo-editing software. Section 4 presents the baseline models and their experiment details on the sentence-level prediction and video segment retrieval tasks on our dataset. Then, we discuss the experimental results in Section 5 and conclude the paper in Section 6. 2. Related Work Most relevant to our proposed work is the reading comprehension task, which is a question answering task involving a piece of text such as a paragraph or article. Such datasets for the reading comprehension task, such as SQuAD (Rajpurkar et al., 2016) based on Wikipedia, TriviaQA (Joshi et al., 2017) constructed from trivia questions with answer evidence from Wikipedia, or those from Hermann et al. based on CNN and Daily Mail articles (Hermann et al., 2015) are factoid-based, meaning the answers typically involve a single entity. Differing from video transcripts, the structures of these data sources, namely paragraphs from Wikipedia and news sources, are typically straightforward since they are meant to be read. In contrast, video transcripts originate from spoken dialogue, which can be verbose, unstruc5450 Figure 1: An illustration of our task, where the red in the timeline indicates whe"
2020.lrec-1.670,N15-1015,0,0.113683,"tated answers with incorrect examples for each question. In the VideoQA dataset, questions focus on a single entity, contrary to our instructional video dataset. Although not necessarily a visual question-answering task, the work proposed by Gupta et al. involved answering questions over transcript data (Gupta et al., 2018). Contrary to our work, Gupta et al.’s dataset is not publically available and their examples only showcase factoid-style questions involving single entity answers. Malmaud et al. focus on aligning a set of instructions to a video of someone carrying out those instructions (Malmaud et al., 2015). In their task, they use the video transcript to represent the video, which they later augment with a visual cue detector on food entities. Their task focuses on procedure-based cooking videos, and contrary to our task is primarily a text alignment task. In our task we aim to answer questions—using the transcripts—on instructionalstyle videos, in which the answer can involve steps not mentioned in the question. number of videos number of segments number of QA pairs avg. length of answer (sec) avg. length of transcript (sentences) avg. length of question (words) avg. length of answer (sentence"
2020.lrec-1.670,D14-1162,0,0.0823914,"Missing"
2020.lrec-1.670,D16-1264,0,0.119904,"Missing"
2020.lrec-1.670,P16-1044,0,0.0316655,"our problem as a retrieval task. Specifically, in addition to a plain transcript, we also provided the model with the segmentation information which was created during the data collection phrase (See Section. 3). Note that each segments corresponds to a candidate answer. Then, the task is to pick the best segment for given a query. This task is easier than the first baseline’s task in that the segmentation information is provided to the model. Unlike the first baseline, however, it is unable to return an answer span at various granularities. The second baseline is based on the attentive LSTM (Tan et al., 2016), which has been developed for the InsuranceQA task. The right diagram in Fig. 3 illustrates the second baseline’s model. Model. The two inputs, s and q represent the segment text and a question. The model first encodes the two inputs. hs = biLST Mall (s) (Sentence Encoding) hq = biLST Mlast (q) (Question Encoding) hs is then re-weighted using attention weights. a = F F N ([hs , hq ]) (Psg Encoding) (Attention) 0s where n is the number of sentences.6 The output of Passage-level Encoding, p, is a sequence of vector, pi , which represents the latent meaning of each sentence. Then, the model comb"
2020.nli-1.4,W17-5522,0,0.102804,"1:SELECT name FROM Employees WHERE last_name = &apos;Smith&apos; AND dept_name = &apos;IT&apos;; Turn 2: How many of them started working after Jan 1, 2020? Turn 2:SELECT Count(name) FROM Employees WHERE last_name = &apos;Smith&apos; AND dept_name = &apos;IT&apos; AND hire_date > &apos;01-01-2020&apos;; Turn 3: What are their phone numbers? Turn 3:SELECT phone_number FROM Employees WHERE last_name = &apos;Smith&apos; AND dept_name = &apos;IT&apos; AND hire_data > &apos;01-01-2020&apos;; Figure 1: Example illustrating a three-turn dialogue, featuring the natural language (first column) and query language (second column) representations. guages (Ngonga Ngomo et al., 2013; Braun et al., 2017; Dubey et al., 2016; Giordani and Moschitti, 2009; Finegan-Dollak et al., 2018; Giordani, 2008; Xu et al., 2017; Zhong et al., 2017), the state-of-the-art systems typically involve a large amount of training data. Therefore, in order to fully utilize these models that translate a natural language (NL) question into query language (QL), one would need to collect large amounts of both NL-QL pairs. Although there are works which involve the collection of NL-QL pairs in different domains (Hemphill et al., 1990; Zelle and Mooney, 1996; Zhong et al., 2017; Yu et al., 2018, 2019b), data is still not"
2020.nli-1.4,P17-4017,0,0.0275416,"collection process for use in NL-to-QL models. Additionally, our approach focuses on generating conversation data, where the context of a dialogue turn is used to generate a subsequent pair. In this way, we better simulate the data necessary for real world chatbots and voice assistants, as exemplified in Figure 1. Our contributions are as follows: Introduction Chatbots and AI task assistants are widely used today to help users with their everyday needs. One use for these assistants is asking them questions on various areas of knowledge or how to accomplish different tasks (Braun et al., 2017; Cui et al., 2017). Because data is usually stored in a structured database, in order to answer a user’s questions, it is essential that the system should first understand the question, and convert it into a structured language query, such as SQL or SPARQL, to fetch the correct answer. While much research has focused on translating natural languages into query lan27 Proceedings of the First Workshop on Natural Language Interfaces, pages 27–36 c July 10, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 • We develop a novel approach that accelerates the creation of NL-to-QL dat"
2020.nli-1.4,P16-1004,0,0.0133595,", Shah et al. (2018) develop a multi-turn semantic parser. Their approach begins with a task schema and API which is used to create dialogue outlines for the provided domain. In the field of natural language interfaces for structured data there are bodies of work that 1) focus on translating natural language to a specific query language and that 2) relate to collecting semantic parsing data for natural language interfaces. 2.1 Data Collection for Semantic Parsing NL-to-QL NL-to-QL models have worked to transform natural language queries into their respective logical form (LF) representations (Dong and Lapata, 2016), SQL queries (Xu et al., 2017; Zhong et al., 2017; Finegan-Dollak et al., 2018; Cai et al., 2018), or SPARQL queries (Ngonga Ngomo et al., 2013; Dubey et al., 2016). While work in the SPARQL domain first normalize and match the queries, stateof-the-art work in translating NL to SQL involves neural architectures. Dong and Lapata (2016) utilize and encoder-decoder framework to translate NL questions into their LF representation. Xu et al. (2017) propose a sketch-based model where a neural network predicts each slot of the sketch. The ar1 28 https://www.wikidata.org/wiki/Wikidata:Main Page Domai"
2020.nli-1.4,P18-1033,0,0.226845,"= &apos;IT&apos;; Turn 2: How many of them started working after Jan 1, 2020? Turn 2:SELECT Count(name) FROM Employees WHERE last_name = &apos;Smith&apos; AND dept_name = &apos;IT&apos; AND hire_date > &apos;01-01-2020&apos;; Turn 3: What are their phone numbers? Turn 3:SELECT phone_number FROM Employees WHERE last_name = &apos;Smith&apos; AND dept_name = &apos;IT&apos; AND hire_data > &apos;01-01-2020&apos;; Figure 1: Example illustrating a three-turn dialogue, featuring the natural language (first column) and query language (second column) representations. guages (Ngonga Ngomo et al., 2013; Braun et al., 2017; Dubey et al., 2016; Giordani and Moschitti, 2009; Finegan-Dollak et al., 2018; Giordani, 2008; Xu et al., 2017; Zhong et al., 2017), the state-of-the-art systems typically involve a large amount of training data. Therefore, in order to fully utilize these models that translate a natural language (NL) question into query language (QL), one would need to collect large amounts of both NL-QL pairs. Although there are works which involve the collection of NL-QL pairs in different domains (Hemphill et al., 1990; Zelle and Mooney, 1996; Zhong et al., 2017; Yu et al., 2018, 2019b), data is still not available in most domains, and thus this collection process can be both time-c"
2020.nli-1.4,P15-1129,0,0.069728,"Missing"
2020.nli-1.4,N13-1092,0,0.0871696,"Missing"
2020.nli-1.4,D19-1204,0,0.0263013,"Saha et al. (2018) first approach the problem of complex sequential question-answering (CSQA) by first building a large-scale QA dataset made to answer questions found in Wikidata 1 . However, their data collection process was extremely laborious, as their process required in-house annotators, crowdsourced workers, and multiple iterations. Additionally, their approach was end-to-end, meaning the output was an expected answer. Nevertheless, because their approach incorporate the query representation, we plan to further incorporate their approach into our data collection process in future work.Yu et al. (2019a) also develop the first general-purpose DB querying dialogue system. However, their system dialogues focus on clarifying a NL question for user verification, before returning an answer. Our work focuses on generating conversational data about specific database entities and properties. 2 2.2 • We showcase our data collection system on two different QLs, SQL and SPARQL, demonstrating the flexibility of our system. • Finally, we demonstrate the use of current single-turn state-of-the-art approaches on these two domains to prove the adaptability of our system to current models. Related Work NL q"
2020.nli-1.4,D18-1425,0,0.0178291,"nga Ngomo et al., 2013; Braun et al., 2017; Dubey et al., 2016; Giordani and Moschitti, 2009; Finegan-Dollak et al., 2018; Giordani, 2008; Xu et al., 2017; Zhong et al., 2017), the state-of-the-art systems typically involve a large amount of training data. Therefore, in order to fully utilize these models that translate a natural language (NL) question into query language (QL), one would need to collect large amounts of both NL-QL pairs. Although there are works which involve the collection of NL-QL pairs in different domains (Hemphill et al., 1990; Zelle and Mooney, 1996; Zhong et al., 2017; Yu et al., 2018, 2019b), data is still not available in most domains, and thus this collection process can be both time-consuming and expensive. In this work, we address the problem of having insufficient data collection methodologies by proposing a novel approach that accelerates the data collection process for use in NL-to-QL models. Additionally, our approach focuses on generating conversation data, where the context of a dialogue turn is used to generate a subsequent pair. In this way, we better simulate the data necessary for real world chatbots and voice assistants, as exemplified in Figure 1. Our cont"
2020.nli-1.4,H90-1021,0,0.737381,"mn) and query language (second column) representations. guages (Ngonga Ngomo et al., 2013; Braun et al., 2017; Dubey et al., 2016; Giordani and Moschitti, 2009; Finegan-Dollak et al., 2018; Giordani, 2008; Xu et al., 2017; Zhong et al., 2017), the state-of-the-art systems typically involve a large amount of training data. Therefore, in order to fully utilize these models that translate a natural language (NL) question into query language (QL), one would need to collect large amounts of both NL-QL pairs. Although there are works which involve the collection of NL-QL pairs in different domains (Hemphill et al., 1990; Zelle and Mooney, 1996; Zhong et al., 2017; Yu et al., 2018, 2019b), data is still not available in most domains, and thus this collection process can be both time-consuming and expensive. In this work, we address the problem of having insufficient data collection methodologies by proposing a novel approach that accelerates the data collection process for use in NL-to-QL models. Additionally, our approach focuses on generating conversation data, where the context of a dialogue turn is used to generate a subsequent pair. In this way, we better simulate the data necessary for real world chatbo"
2020.nli-1.4,P19-1443,0,0.0155581,"Saha et al. (2018) first approach the problem of complex sequential question-answering (CSQA) by first building a large-scale QA dataset made to answer questions found in Wikidata 1 . However, their data collection process was extremely laborious, as their process required in-house annotators, crowdsourced workers, and multiple iterations. Additionally, their approach was end-to-end, meaning the output was an expected answer. Nevertheless, because their approach incorporate the query representation, we plan to further incorporate their approach into our data collection process in future work.Yu et al. (2019a) also develop the first general-purpose DB querying dialogue system. However, their system dialogues focus on clarifying a NL question for user verification, before returning an answer. Our work focuses on generating conversational data about specific database entities and properties. 2 2.2 • We showcase our data collection system on two different QLs, SQL and SPARQL, demonstrating the flexibility of our system. • Finally, we demonstrate the use of current single-turn state-of-the-art approaches on these two domains to prove the adaptability of our system to current models. Related Work NL q"
2020.nli-1.4,P17-1089,0,0.0141643,"emonstrate the use of current single-turn state-of-the-art approaches on these two domains to prove the adaptability of our system to current models. Related Work NL question semantic parsers have been developed for single-turn QA in order to translate simple NL questions into their respective LFs (Wang et al., 2015). In their approach, Wang et al. (2015) first begin with a domain, building a seed lexicon of that domain. Next, they find the LF and canonical utterance templates corresponding based on the lexicon. Wang et al. (2015) then paraphrase their canonical utterances via crowd-sourcing. Iyer et al. (2017) learn a semantic parser via an encoderdecoder model by using NL/SQL templates. This model is tuned through user feedback, where incorrect queries are annotated by crowd-workers. Paraphrasing is accomplished through the Paraphrasing Database (PPDB) (Ganitkevitch et al., 2013). While the two previously mentioned works are single-turn semantic parsers, Shah et al. (2018) develop a multi-turn semantic parser. Their approach begins with a task schema and API which is used to create dialogue outlines for the provided domain. In the field of natural language interfaces for structured data there are"
2020.nli-1.4,P16-1002,0,0.020093,"eveloped by Finegan-Dollak et al. (2018), where the blue boxes represent LSTM cells and the green box represents a feedforward neural network. ‘Photos’ is classified as a slot value, while the template chosen (Tempalte 42), is depicted above the model. In the template, the entity slot is highlighted in yellow and the properties which make the template unique are in red. In our experiments we utilize single-turn NL-QL models. Specifically, we utilize the baselines defined by Finegan-Dollak et al. (2018). The first baseline is a seq2seq model with attention-based copying, originally proposed by Jia and Liang (2016). This model takes an NL utterance as input and outputs a structured query. Included in the output is a COPY token, which signifies the copying of an input token. In the copying mechanism model, the loss is calculated based on the accumulation of both the probability of distribution of the tokens in the output and the probability of copying from an input token. This copying probability is calculated as the categorical cross entropy of the distributed attention scores across the input’s tokens, where the token with the max attention score is chosen as the output token. The second baseline is a"
2020.nlp4convai-1.11,L18-1683,0,0.0176462,"s, Pi (.|{x1 , x2 , ..., xn }/xi ) = sof tmax(F F (hci )). Finally, we use the following negative log-likelihood as the loss function to be optimized during training: Lwp = 1 |L| Σ − (yks · log(Pk (yks |x1 , x2 , ..., xn ))+ |L |k=1 (1 − yks ) · log(1 − Pk (yks |x1 , x2 , ..., xn ))) (5) L = Lpred + αLdiscr + βLwp + γLsp (6) where α, β and γ are the trade-off parameters to be tuned based on the development set performance. 4 4.1 Experiments Dataset and Parameters We evaluate our model on three SF datasets. Namely, we employ ATIS (Hemphill et al., 1990), SNIPS (Coucke et al., 2018) and EditMe (Manuvinakurike et al., 2018). ATIS and SNIPS are two widely adopted SF dataset and EditMe is a SF dataset for editing images with four slot labels (i.e., Action, Object, Attribute and Value). The 1 n Σ − log(Pi (yi |{x1 , x2 , ..., xn }/xi )) n i=1 (4) 92 Model Joint Seq(2016) Attention-Based(2016) Sloted-Gated(2018) SF-ID(2019) CAPSULE-NLU(2019) SPTID(2019) CVT(2018) GCDT(2019) Ours statistics of the datasets are presented in the Appendix A. Based on the experiments on EditMe development set, the following parameters are selected: GloVe embedding with 300 dimensions to initialize word embedding ; 200 dimensions for the"
2020.nlp4convai-1.11,D18-1217,0,0.0212242,"Table 1: Performance of the model and baselines on the Test sets. We compare our model with other deep learning based models for SF. Namely, we compare the proposed model with Joint Seq (Hakkani-T¨ur et al., 2016), Attention-Based (Liu and Lane, 2016), Sloted-Gated (Goo et al., 2018), SF-ID (E et al., 2019), CAPSULE-NLU (Zhang et al., 2019), and SPTID (Qin et al., 2019). Note that we compare our model with the single-task version of these baselines. We also compare our model with other sequence labeling models which are not specifically proposed for SF. Namely, we compare the model with CVT (Clark et al., 2018) and GCDT (Liu et al., 2019). CVT aims to improve input representation using improving partial views and GCDT exploits contextual information to enhance word representations via concatenation of context and word representation. 4.3 SNIPS 87.3 87.8 89.2 90.9 91.8 90.8 91.4 92.0 93.6 5 Conclusion In this work, we introduced a new deep model for the task of Slot Filling (SF). In a multi-task setting, our model increases the mutual information between the word representation and its context, improves label information in the context and predicts which concepts are expressed in the given sentence."
2020.nlp4convai-1.11,D19-1214,0,0.060801,"spoken language understanding (SLU). Early work employed feature engineering for statistical models, e.g., Conditional Random Field (Raymond and Riccardi, 2007). Due to the lack of generalisation ability of feature based models, deep learning based models superseded them (Yao et al., 2014; Peng et al., 2015; Kurata et al., 2016; Hakkani-T¨ur et al., 2016). Also, joint models to simultaneously predict the intent of the utterance and to extract the semantic slots has also gained a lot of attention (Guo et al., 2014; Liu and Lane, 2016; Zhang and Wang, 2016; Wang et al., 2018; Goo et al., 2018; Qin et al., 2019; E et al., 2019). In addition to the supervised settings, recently other setting such as progressive learning (Shen et al., 2019) or zero-shot learning has also been studied (Shah et al., 2019). To the best of our knowledge, none of the existing work introduces a multi-task learning solely for the SF to incorporate the contextual information in both representation and task levels. 3 3.2 In this sub-task we aim to increase the consistency between the word representation and its context. To obtain the context of each word, we use max pooling over the outputs of the BiLSTM for all words of the s"
2020.nlp4convai-1.11,P19-1544,0,0.0271447,"Missing"
2020.nlp4convai-1.11,N18-2118,0,0.0817736,"f the sub-tasks of spoken language understanding (SLU). Early work employed feature engineering for statistical models, e.g., Conditional Random Field (Raymond and Riccardi, 2007). Due to the lack of generalisation ability of feature based models, deep learning based models superseded them (Yao et al., 2014; Peng et al., 2015; Kurata et al., 2016; Hakkani-T¨ur et al., 2016). Also, joint models to simultaneously predict the intent of the utterance and to extract the semantic slots has also gained a lot of attention (Guo et al., 2014; Liu and Lane, 2016; Zhang and Wang, 2016; Wang et al., 2018; Goo et al., 2018; Qin et al., 2019; E et al., 2019). In addition to the supervised settings, recently other setting such as progressive learning (Shen et al., 2019) or zero-shot learning has also been studied (Shah et al., 2019). To the best of our knowledge, none of the existing work introduces a multi-task learning solely for the SF to incorporate the contextual information in both representation and task levels. 3 3.2 In this sub-task we aim to increase the consistency between the word representation and its context. To obtain the context of each word, we use max pooling over the outputs of the BiLSTM for"
2020.nlp4convai-1.11,D19-1126,0,0.017678,"ield (Raymond and Riccardi, 2007). Due to the lack of generalisation ability of feature based models, deep learning based models superseded them (Yao et al., 2014; Peng et al., 2015; Kurata et al., 2016; Hakkani-T¨ur et al., 2016). Also, joint models to simultaneously predict the intent of the utterance and to extract the semantic slots has also gained a lot of attention (Guo et al., 2014; Liu and Lane, 2016; Zhang and Wang, 2016; Wang et al., 2018; Goo et al., 2018; Qin et al., 2019; E et al., 2019). In addition to the supervised settings, recently other setting such as progressive learning (Shen et al., 2019) or zero-shot learning has also been studied (Shah et al., 2019). To the best of our knowledge, none of the existing work introduces a multi-task learning solely for the SF to incorporate the contextual information in both representation and task levels. 3 3.2 In this sub-task we aim to increase the consistency between the word representation and its context. To obtain the context of each word, we use max pooling over the outputs of the BiLSTM for all words of the sentence excluding the word itself, hci = M axP ooling(h1 , h2 , ..., hn /hi ). We aim to increase the consistency between vectors"
2020.nlp4convai-1.11,N18-2050,0,0.0177448,"ategorized as one of the sub-tasks of spoken language understanding (SLU). Early work employed feature engineering for statistical models, e.g., Conditional Random Field (Raymond and Riccardi, 2007). Due to the lack of generalisation ability of feature based models, deep learning based models superseded them (Yao et al., 2014; Peng et al., 2015; Kurata et al., 2016; Hakkani-T¨ur et al., 2016). Also, joint models to simultaneously predict the intent of the utterance and to extract the semantic slots has also gained a lot of attention (Guo et al., 2014; Liu and Lane, 2016; Zhang and Wang, 2016; Wang et al., 2018; Goo et al., 2018; Qin et al., 2019; E et al., 2019). In addition to the supervised settings, recently other setting such as progressive learning (Shen et al., 2019) or zero-shot learning has also been studied (Shah et al., 2019). To the best of our knowledge, none of the existing work introduces a multi-task learning solely for the SF to incorporate the contextual information in both representation and task levels. 3 3.2 In this sub-task we aim to increase the consistency between the word representation and its context. To obtain the context of each word, we use max pooling over the outputs"
2020.nlp4convai-1.11,P19-1519,0,0.020202,"dataset is more challenging than the other datasets, despite having fewer slot types. This difficulty could be explained by the limited number of training examples and more diversity in sentence structures in this dataset. 4.4 ATIS 94.3 94.2 95.4 95.5 95.2 95.1 94.8 95.1 95.8 Table 1: Performance of the model and baselines on the Test sets. We compare our model with other deep learning based models for SF. Namely, we compare the proposed model with Joint Seq (Hakkani-T¨ur et al., 2016), Attention-Based (Liu and Lane, 2016), Sloted-Gated (Goo et al., 2018), SF-ID (E et al., 2019), CAPSULE-NLU (Zhang et al., 2019), and SPTID (Qin et al., 2019). Note that we compare our model with the single-task version of these baselines. We also compare our model with other sequence labeling models which are not specifically proposed for SF. Namely, we compare the model with CVT (Clark et al., 2018) and GCDT (Liu et al., 2019). CVT aims to improve input representation using improving partial views and GCDT exploits contextual information to enhance word representations via concatenation of context and word representation. 4.3 SNIPS 87.3 87.8 89.2 90.9 91.8 90.8 91.4 92.0 93.6 5 Conclusion In this work, we introduced"
2020.nlp4convai-1.11,H90-1021,0,0.51665,"Missing"
2020.nlp4convai-1.11,D16-1223,0,0.0198318,"of each word. Our extensive experiments on three benchmark datasets, empirically prove the effectiveness of the proposed model leading to new the state-of-the-art results on all three datasets. 2 Related Work In the literature, Slot Filling (SF), is categorized as one of the sub-tasks of spoken language understanding (SLU). Early work employed feature engineering for statistical models, e.g., Conditional Random Field (Raymond and Riccardi, 2007). Due to the lack of generalisation ability of feature based models, deep learning based models superseded them (Yao et al., 2014; Peng et al., 2015; Kurata et al., 2016; Hakkani-T¨ur et al., 2016). Also, joint models to simultaneously predict the intent of the utterance and to extract the semantic slots has also gained a lot of attention (Guo et al., 2014; Liu and Lane, 2016; Zhang and Wang, 2016; Wang et al., 2018; Goo et al., 2018; Qin et al., 2019; E et al., 2019). In addition to the supervised settings, recently other setting such as progressive learning (Shen et al., 2019) or zero-shot learning has also been studied (Shah et al., 2019). To the best of our knowledge, none of the existing work introduces a multi-task learning solely for the SF to incorpor"
2020.nlp4convai-1.11,P19-1233,0,0.023226,"odel and baselines on the Test sets. We compare our model with other deep learning based models for SF. Namely, we compare the proposed model with Joint Seq (Hakkani-T¨ur et al., 2016), Attention-Based (Liu and Lane, 2016), Sloted-Gated (Goo et al., 2018), SF-ID (E et al., 2019), CAPSULE-NLU (Zhang et al., 2019), and SPTID (Qin et al., 2019). Note that we compare our model with the single-task version of these baselines. We also compare our model with other sequence labeling models which are not specifically proposed for SF. Namely, we compare the model with CVT (Clark et al., 2018) and GCDT (Liu et al., 2019). CVT aims to improve input representation using improving partial views and GCDT exploits contextual information to enhance word representations via concatenation of context and word representation. 4.3 SNIPS 87.3 87.8 89.2 90.9 91.8 90.8 91.4 92.0 93.6 5 Conclusion In this work, we introduced a new deep model for the task of Slot Filling (SF). In a multi-task setting, our model increases the mutual information between the word representation and its context, improves label information in the context and predicts which concepts are expressed in the given sentence. Our experiments on three ben"
2020.nuse-1.5,W06-0901,0,0.500858,"detection as a few-shot learning problem to extend ED to new event types and provide a baseline for this new research direction. To our best knowledge, this is a new branch of research that has not been explored. • We propose two novel training signals for FSL. These signals can remarkably improve the performance of existing FSL models. As these signals do not require any additional information (e.g. dependency tree or part-of-speech), they can be applied in any metric-based FSL models. 2 Related work Early studies in event detection mainly address feature engineering for statistical models (Ahn, 2006; Ji and Grishman, 2008; Hong et al., 2011; Li et al., 2014, 2015) including semantic features and syntactic features. Recently, due to the advances with deep learning, many neural network architectures have been presented for ED, e.g. convolutional neural networks (CNN) (Chen et al., 2015; Nguyen and Grishman, 2015, 2016; Nguyen et al., 2016b), recurrent neural networks (RNN) (Liu et al., 2017; Chen et al., 2018; Nguyen et al., 2016a; Nguyen and Nguyen, 2018) and graph convolutional neural networks (GCN) (Nguyen and Grishman, 2018; Pouran Ben Veyseh et al., 2019). These methods formulate ED a"
2020.nuse-1.5,P15-1017,0,0.555985,"emarkably improve the performance of existing FSL models. As these signals do not require any additional information (e.g. dependency tree or part-of-speech), they can be applied in any metric-based FSL models. 2 Related work Early studies in event detection mainly address feature engineering for statistical models (Ahn, 2006; Ji and Grishman, 2008; Hong et al., 2011; Li et al., 2014, 2015) including semantic features and syntactic features. Recently, due to the advances with deep learning, many neural network architectures have been presented for ED, e.g. convolutional neural networks (CNN) (Chen et al., 2015; Nguyen and Grishman, 2015, 2016; Nguyen et al., 2016b), recurrent neural networks (RNN) (Liu et al., 2017; Chen et al., 2018; Nguyen et al., 2016a; Nguyen and Nguyen, 2018) and graph convolutional neural networks (GCN) (Nguyen and Grishman, 2018; Pouran Ben Veyseh et al., 2019). These methods formulate ED as a supervised learning problem which usually fails to predict the labels of new event types. By transitioning the symbolic event types to descriptive event types in the form of bags of keywords (Bronstein et al., 2015; Peng et al., 2016; Lai and Nguyen, 2019), the adaptibility of event de"
2020.nuse-1.5,D18-1158,0,0.221197,"text (e.g. a sentence) and classify it into one of the event types of interest. The following sentence is an example of ED: In 1997, the company hired John D. Idol to take over as chief executive. In this example, an ideal event detection system should detect the word hired as an event, and classify it to class of Personnel:Start-Position, assuming that Personnel:Start-Position is in the set of interested classes. The current works in ED typically employ traditional supervised learning based on feature engineering (Li et al., 2014; Chen et al., 2017) and neural networks (Nguyen et al., 2016a; Chen et al., 2018; Lu and Nguyen, 2018). The main problem with supervised learning models is that they can not perform well on unseen classes (e.g. training a model to classify daily events, then run this 38 Proceedings of the 1st Joint Workshop on Narrative Understanding, Storylines, and Events, pages 38–45 c July 9, 2020. 2020 Association for Computational Linguistics (Grishman et al., 2005). Therefore, in this study, we propose to train an ED model using matching information (1) between query instance and the support set and (2) between the samples in the support themselves. This is implemented by adding tw"
2020.nuse-1.5,P11-1113,0,0.191269,"problem to extend ED to new event types and provide a baseline for this new research direction. To our best knowledge, this is a new branch of research that has not been explored. • We propose two novel training signals for FSL. These signals can remarkably improve the performance of existing FSL models. As these signals do not require any additional information (e.g. dependency tree or part-of-speech), they can be applied in any metric-based FSL models. 2 Related work Early studies in event detection mainly address feature engineering for statistical models (Ahn, 2006; Ji and Grishman, 2008; Hong et al., 2011; Li et al., 2014, 2015) including semantic features and syntactic features. Recently, due to the advances with deep learning, many neural network architectures have been presented for ED, e.g. convolutional neural networks (CNN) (Chen et al., 2015; Nguyen and Grishman, 2015, 2016; Nguyen et al., 2016b), recurrent neural networks (RNN) (Liu et al., 2017; Chen et al., 2018; Nguyen et al., 2016a; Nguyen and Nguyen, 2018) and graph convolutional neural networks (GCN) (Nguyen and Grishman, 2018; Pouran Ben Veyseh et al., 2019). These methods formulate ED as a supervised learning problem which usua"
2020.nuse-1.5,W16-1618,1,0.94803,"triggers from a given text (e.g. a sentence) and classify it into one of the event types of interest. The following sentence is an example of ED: In 1997, the company hired John D. Idol to take over as chief executive. In this example, an ideal event detection system should detect the word hired as an event, and classify it to class of Personnel:Start-Position, assuming that Personnel:Start-Position is in the set of interested classes. The current works in ED typically employ traditional supervised learning based on feature engineering (Li et al., 2014; Chen et al., 2017) and neural networks (Nguyen et al., 2016a; Chen et al., 2018; Lu and Nguyen, 2018). The main problem with supervised learning models is that they can not perform well on unseen classes (e.g. training a model to classify daily events, then run this 38 Proceedings of the 1st Joint Workshop on Narrative Understanding, Storylines, and Events, pages 38–45 c July 9, 2020. 2020 Association for Computational Linguistics (Grishman et al., 2005). Therefore, in this study, we propose to train an ED model using matching information (1) between query instance and the support set and (2) between the samples in the support themselves. This is impl"
2020.nuse-1.5,P18-1201,0,0.205746,"port set and (2) between the samples in the support themselves. This is implemented by adding two auxiliary factors into the loss function to constrain the learning process. We apply the proposed training signals to different FSL models on the benchmark event detection dataset (Grishman et al., 2005). The experiments show that the training signal can improve the performance of the examined FSL models. To summarize, our contributions to this work include: FSL as we do in this work. One can also address this problem in zero-shot learning with data generated from abstract meaning representation (Huang et al., 2018) or two-stage pipeline ( trigger identification and few-shot event classification) based on dynamic memory network (Deng et al., 2020). A recent study has employed few-shot learning for event classification (Lai et al., 2020). Our work is similar in terms of formulation, however, we consider it in a larger extent of event detection where the NULL event is also included. Few-shot learning has been studied early in the literature (Thrun, 1996). Before the era of the deep neural network, FSL approaches focused on building generative models that can transfer priors across classes. However, these m"
2020.nuse-1.5,P08-1030,0,0.606901,"as a few-shot learning problem to extend ED to new event types and provide a baseline for this new research direction. To our best knowledge, this is a new branch of research that has not been explored. • We propose two novel training signals for FSL. These signals can remarkably improve the performance of existing FSL models. As these signals do not require any additional information (e.g. dependency tree or part-of-speech), they can be applied in any metric-based FSL models. 2 Related work Early studies in event detection mainly address feature engineering for statistical models (Ahn, 2006; Ji and Grishman, 2008; Hong et al., 2011; Li et al., 2014, 2015) including semantic features and syntactic features. Recently, due to the advances with deep learning, many neural network architectures have been presented for ED, e.g. convolutional neural networks (CNN) (Chen et al., 2015; Nguyen and Grishman, 2015, 2016; Nguyen et al., 2016b), recurrent neural networks (RNN) (Liu et al., 2017; Chen et al., 2018; Nguyen et al., 2016a; Nguyen and Nguyen, 2018) and graph convolutional neural networks (GCN) (Nguyen and Grishman, 2018; Pouran Ben Veyseh et al., 2019). These methods formulate ED as a supervised learning"
2020.nuse-1.5,P15-2060,1,0.941981,"he performance of existing FSL models. As these signals do not require any additional information (e.g. dependency tree or part-of-speech), they can be applied in any metric-based FSL models. 2 Related work Early studies in event detection mainly address feature engineering for statistical models (Ahn, 2006; Ji and Grishman, 2008; Hong et al., 2011; Li et al., 2014, 2015) including semantic features and syntactic features. Recently, due to the advances with deep learning, many neural network architectures have been presented for ED, e.g. convolutional neural networks (CNN) (Chen et al., 2015; Nguyen and Grishman, 2015, 2016; Nguyen et al., 2016b), recurrent neural networks (RNN) (Liu et al., 2017; Chen et al., 2018; Nguyen et al., 2016a; Nguyen and Nguyen, 2018) and graph convolutional neural networks (GCN) (Nguyen and Grishman, 2018; Pouran Ben Veyseh et al., 2019). These methods formulate ED as a supervised learning problem which usually fails to predict the labels of new event types. By transitioning the symbolic event types to descriptive event types in the form of bags of keywords (Bronstein et al., 2015; Peng et al., 2016; Lai and Nguyen, 2019), the adaptibility of event detection can be formed as a"
2020.nuse-1.5,D14-1181,0,0.00551631,"Once we get the embedding for the whole sentence E(s) = {m1 , m2 , · · · , mL }, we employ a neural network, denoted as f , to encode the information of an instance (s, a) of the anchor wa under the context in the sentence s into a single vector v = f (E(s), a). In this work, consider the three following neural network architectures for this encoding purpose: ... K (s1N , a1N , tN ), . . . , (sK N , aN , tN ), K (s1N +1 , a1N +1 , tnull ), . . . , (sK 1 , aN +1 , tnull )} where: • {t1 , t2 , · · · tN } is the set of positive labels, which indicate an event • Convolution Neural Network (CNN) (Kim, 2014) encodes the sentence by convolution operation on k consecutive vectors representing k-gram. Follow (Nguyen and Grishman, 2015), we use multiple kernel sizes k ∈ {2, 3, 4, 5} to cover the context with 150 filters for each kernel size. To squeeze the information of the sentence, we apply max pooling to the top convolution layer to get a pooled vector p. We also introduce local embedding e[a−w,a+w] with window size w = 2. We concatenate pooled vector and local embeddings, and feed them through multiple dense layer to get the final representation: • tnull a special label for non-event. • (sji , a"
2020.nuse-1.5,D16-1085,1,0.929964,"Missing"
2020.nuse-1.5,D16-1038,0,0.0186814,"ed for ED, e.g. convolutional neural networks (CNN) (Chen et al., 2015; Nguyen and Grishman, 2015, 2016; Nguyen et al., 2016b), recurrent neural networks (RNN) (Liu et al., 2017; Chen et al., 2018; Nguyen et al., 2016a; Nguyen and Nguyen, 2018) and graph convolutional neural networks (GCN) (Nguyen and Grishman, 2018; Pouran Ben Veyseh et al., 2019). These methods formulate ED as a supervised learning problem which usually fails to predict the labels of new event types. By transitioning the symbolic event types to descriptive event types in the form of bags of keywords (Bronstein et al., 2015; Peng et al., 2016; Lai and Nguyen, 2019), the adaptibility of event detection can be formed as a supervised-learning problem. However, these studies have not examined 3 Methodology Our goal in this work is to formulate ED as a FSL problem, which has not been done in prior work. In order to achieve this, this section is divided into three parts. In the section 3.1 we present the overall framework that formulate Event Detection as an Few-Shot Learning problem. Then, we present popular models for FSL in the prior work and common sentence encoders which have been widely used in ED in section 3.2. Finally, we prese"
2020.nuse-1.5,P19-1432,1,0.861759,"ture engineering for statistical models (Ahn, 2006; Ji and Grishman, 2008; Hong et al., 2011; Li et al., 2014, 2015) including semantic features and syntactic features. Recently, due to the advances with deep learning, many neural network architectures have been presented for ED, e.g. convolutional neural networks (CNN) (Chen et al., 2015; Nguyen and Grishman, 2015, 2016; Nguyen et al., 2016b), recurrent neural networks (RNN) (Liu et al., 2017; Chen et al., 2018; Nguyen et al., 2016a; Nguyen and Nguyen, 2018) and graph convolutional neural networks (GCN) (Nguyen and Grishman, 2018; Pouran Ben Veyseh et al., 2019). These methods formulate ED as a supervised learning problem which usually fails to predict the labels of new event types. By transitioning the symbolic event types to descriptive event types in the form of bags of keywords (Bronstein et al., 2015; Peng et al., 2016; Lai and Nguyen, 2019), the adaptibility of event detection can be formed as a supervised-learning problem. However, these studies have not examined 3 Methodology Our goal in this work is to formulate ED as a FSL problem, which has not been done in prior work. In order to achieve this, this section is divided into three parts. In"
2020.nuse-1.5,D19-5532,1,0.662469,"volutional neural networks (CNN) (Chen et al., 2015; Nguyen and Grishman, 2015, 2016; Nguyen et al., 2016b), recurrent neural networks (RNN) (Liu et al., 2017; Chen et al., 2018; Nguyen et al., 2016a; Nguyen and Nguyen, 2018) and graph convolutional neural networks (GCN) (Nguyen and Grishman, 2018; Pouran Ben Veyseh et al., 2019). These methods formulate ED as a supervised learning problem which usually fails to predict the labels of new event types. By transitioning the symbolic event types to descriptive event types in the form of bags of keywords (Bronstein et al., 2015; Peng et al., 2016; Lai and Nguyen, 2019), the adaptibility of event detection can be formed as a supervised-learning problem. However, these studies have not examined 3 Methodology Our goal in this work is to formulate ED as a FSL problem, which has not been done in prior work. In order to achieve this, this section is divided into three parts. In the section 3.1 we present the overall framework that formulate Event Detection as an Few-Shot Learning problem. Then, we present popular models for FSL in the prior work and common sentence encoders which have been widely used in ED in section 3.2. Finally, we present two novel reguarliza"
2020.nuse-1.5,D14-1198,0,0.13744,"ssing (NLP). Event Detection is the task to detect event triggers from a given text (e.g. a sentence) and classify it into one of the event types of interest. The following sentence is an example of ED: In 1997, the company hired John D. Idol to take over as chief executive. In this example, an ideal event detection system should detect the word hired as an event, and classify it to class of Personnel:Start-Position, assuming that Personnel:Start-Position is in the set of interested classes. The current works in ED typically employ traditional supervised learning based on feature engineering (Li et al., 2014; Chen et al., 2017) and neural networks (Nguyen et al., 2016a; Chen et al., 2018; Lu and Nguyen, 2018). The main problem with supervised learning models is that they can not perform well on unseen classes (e.g. training a model to classify daily events, then run this 38 Proceedings of the 1st Joint Workshop on Narrative Understanding, Storylines, and Events, pages 38–45 c July 9, 2020. 2020 Association for Computational Linguistics (Grishman et al., 2005). Therefore, in this study, we propose to train an ED model using matching information (1) between query instance and the support set and (2"
2020.nuse-1.5,W15-4502,1,0.883188,"Missing"
2020.nuse-1.5,P17-1164,0,0.303305,"ormation (e.g. dependency tree or part-of-speech), they can be applied in any metric-based FSL models. 2 Related work Early studies in event detection mainly address feature engineering for statistical models (Ahn, 2006; Ji and Grishman, 2008; Hong et al., 2011; Li et al., 2014, 2015) including semantic features and syntactic features. Recently, due to the advances with deep learning, many neural network architectures have been presented for ED, e.g. convolutional neural networks (CNN) (Chen et al., 2015; Nguyen and Grishman, 2015, 2016; Nguyen et al., 2016b), recurrent neural networks (RNN) (Liu et al., 2017; Chen et al., 2018; Nguyen et al., 2016a; Nguyen and Nguyen, 2018) and graph convolutional neural networks (GCN) (Nguyen and Grishman, 2018; Pouran Ben Veyseh et al., 2019). These methods formulate ED as a supervised learning problem which usually fails to predict the labels of new event types. By transitioning the symbolic event types to descriptive event types in the form of bags of keywords (Bronstein et al., 2015; Peng et al., 2016; Lai and Nguyen, 2019), the adaptibility of event detection can be formed as a supervised-learning problem. However, these studies have not examined 3 Methodol"
2020.nuse-1.5,N19-1080,0,0.0132604,"vent Detection Viet Dac Lai1 , Franck Dernoncourt2 and Thien Huu Nguyen1 1 Department of Computer and Information Science, University of Oregon, Eugene, Oregon, USA 2 Adobe Research, San Jose, CA, USA {vietl, thien}@cs.uoregon.edu franck.dernoncourt@adobe.com Abstract model to classify laboratory operations). As a result, supervised learning ED can not extend to unseen event types. A trivial solution is to annotate more data for unseen event types, then retraining the model with newly annotated data. However, this method is usually impractical because of the extremely high cost of annotation (Liu et al., 2019). A human can learn about a new concept with limited supervision e.g. one can detect and classify events with 3-5 examples (Grishman et al., 2005). This motivates the setting we aim for event detection: few-shot learning (FSL). In FSL, a trained model rapidly learns a new concept from a few examples while keeping great generalization from observed examples (Vinyals et al., 2016). Hence, if we need to extend event detection into a new domain, a few examples are needed to activate the system in the new domain without retraining the model. By formulating ED as FSL, we can significantly reduce the"
2020.nuse-1.5,D18-1517,1,0.831588,"ce) and classify it into one of the event types of interest. The following sentence is an example of ED: In 1997, the company hired John D. Idol to take over as chief executive. In this example, an ideal event detection system should detect the word hired as an event, and classify it to class of Personnel:Start-Position, assuming that Personnel:Start-Position is in the set of interested classes. The current works in ED typically employ traditional supervised learning based on feature engineering (Li et al., 2014; Chen et al., 2017) and neural networks (Nguyen et al., 2016a; Chen et al., 2018; Lu and Nguyen, 2018). The main problem with supervised learning models is that they can not perform well on unseen classes (e.g. training a model to classify daily events, then run this 38 Proceedings of the 1st Joint Workshop on Narrative Understanding, Storylines, and Events, pages 38–45 c July 9, 2020. 2020 Association for Computational Linguistics (Grishman et al., 2005). Therefore, in this study, we propose to train an ED model using matching information (1) between query instance and the support set and (2) between the samples in the support themselves. This is implemented by adding two auxiliary factors in"
2020.nuse-1.5,N16-1034,1,0.960297,"triggers from a given text (e.g. a sentence) and classify it into one of the event types of interest. The following sentence is an example of ED: In 1997, the company hired John D. Idol to take over as chief executive. In this example, an ideal event detection system should detect the word hired as an event, and classify it to class of Personnel:Start-Position, assuming that Personnel:Start-Position is in the set of interested classes. The current works in ED typically employ traditional supervised learning based on feature engineering (Li et al., 2014; Chen et al., 2017) and neural networks (Nguyen et al., 2016a; Chen et al., 2018; Lu and Nguyen, 2018). The main problem with supervised learning models is that they can not perform well on unseen classes (e.g. training a model to classify daily events, then run this 38 Proceedings of the 1st Joint Workshop on Narrative Understanding, Storylines, and Events, pages 38–45 c July 9, 2020. 2020 Association for Computational Linguistics (Grishman et al., 2005). Therefore, in this study, we propose to train an ED model using matching information (1) between query instance and the support set and (2) between the samples in the support themselves. This is impl"
2020.semeval-1.184,2020.semeval-1.219,0,0.0964702,"ll four scores. The next system on our leader board is Hitachi, with a score of 0.814. And finally, IITK, by achieving 0.810 RANK, stands in third place. 7.2 Best Paper Awards Our shared task awarded several best paper distinctions to complement the top performing systems. Here are the categories of best papers and the winners for each: • Best system description paper: IDS (Shin et al., 2020), this paper, with interesting analysis components, advances our understanding regarding the effectiveness of pre-trained language models for this specific task. • Best result interpretation paper: MIDAS (Anand et al., 2020), the authors go the extra mile to analyze the results in this paper. • Best negative results paper: UIC-NLP (Hossu and Parde, 2020), the authors performed extensive experiments with non-contextualized pre-trained models as well as a variety of hand-crafted features. Through the error analysis, the authors identified a number of common challenging patterns for the model, including late-phrase words, sequences of words, and abnormal/poetic sentence structure. 7.3 Top Performing Systems and Novel Architectures In this section, we provide a brief description of the best performing and novel appro"
2020.semeval-1.184,S17-2091,0,0.0211435,"361 model emphasis. We evaluated the model against different baselines on the Spark dataset (introduced in Section 3). Keyword or key-phrase detection may be the closest topic to emphasis selection. Keywords can capture the main topics described in a given document (Turney, 2002). Modeling keywords or key-phrases has been widely addressed in different domains such as news articles (Wan et al., 2007), scientific publications (Nguyen and Kan, 2007) and Twitter data (Zhang et al., 2016; Bellaachia and Al-Dhelaan, 2012). Keyword detection mainly focuses on finding important nouns or noun phrases (Augenstein et al., 2017). In contrast, emphasis could be applied to a subset of words with different roles in a sentence. Generally, word emphasis may use to express emotions, show contrast, capture a reader’s interest or clarify a message. Moreover, emphasis selection in social media posts deals with very short texts and the prediction needs to be made based on a single instance. In the context of expressive prosody generation, emphasis has been addressed based on acoustic and prosodic features that exist in spoken data. For example, (Nakajima et al., 2014) predicted emphasized accent phrases from advertisement text"
2020.semeval-1.184,2020.semeval-1.218,0,0.0611921,"Missing"
2020.semeval-1.184,P19-4007,0,0.0426115,"Missing"
2020.semeval-1.184,2020.semeval-1.222,0,0.0627134,"Missing"
2020.semeval-1.184,2020.semeval-1.215,0,0.0659097,"Missing"
2020.semeval-1.184,2020.semeval-1.223,0,0.154922,", stands in third place. 7.2 Best Paper Awards Our shared task awarded several best paper distinctions to complement the top performing systems. Here are the categories of best papers and the winners for each: • Best system description paper: IDS (Shin et al., 2020), this paper, with interesting analysis components, advances our understanding regarding the effectiveness of pre-trained language models for this specific task. • Best result interpretation paper: MIDAS (Anand et al., 2020), the authors go the extra mile to analyze the results in this paper. • Best negative results paper: UIC-NLP (Hossu and Parde, 2020), the authors performed extensive experiments with non-contextualized pre-trained models as well as a variety of hand-crafted features. Through the error analysis, the authors identified a number of common challenging patterns for the model, including late-phrase words, sequences of words, and abnormal/poetic sentence structure. 7.3 Top Performing Systems and Novel Architectures In this section, we provide a brief description of the best performing and novel approaches. Table 4 shows a high level summary of these systems. ERNIE achieved the highest score by fine-tuning ERNIE 2.0 as the base mo"
2020.semeval-1.184,2020.semeval-1.190,0,0.0808437,"eads of different models like BERT, DistilBERT, GPT-2, RoBERTa, XLNet, and XLM to probe their ability to identify emphasis without any fine-tuning. Their interesting findings indicate that DistilBERT is more successful in predicting emphasis while XLNet and GPT-2 perform poorly when there is no training for this task. The top non-transformer-based model, Procyon (ranked 12th), successfully proposed an ELMo-based 1366 Table 4: Main features in participating systems. An ‘N/A’ in the Ref. column means that we did not receive a system description paper for that entry. Rank 1 Team Name ERNIE Ref. (Huang et al., 2020) 2 Hitachi (Morio et al., 2020) 3 4 IITK Randomseed19 (Singhal et al., 2020) (Shatilov et al., 2020) 5 Sherry N/A 5 Sattiy N/A 7 FPAI (Guo et al., 2020) 10 BugHunter N/A 11 11 Amobee MIDAS N/A (Anand et al., 2020) 12 Procyon N/A 12 Jupyter N/A 13 CrazyRock N/A 14 CLP N/A 15 16 17 18 18 20 TextLearner Bright LAST AP T¨extmarkers EL-BERT (Yang et al., 2020) N/A (Bestgen, 2020) N/A (Glocker and Markianos Wright, 2020) (Kanani et al., 2020) 22 UIC-NLP (Hossu and Parde, 2020) 23 IDS (Shin et al., 2020) 24 YNU-HPCC (Liao et al., 2020) 1367 Best performing Methods ERNIE 2.0 + data augmentation + hand"
2020.semeval-1.184,2020.semeval-1.214,0,0.0562082,"Missing"
2020.semeval-1.184,2020.semeval-1.224,0,0.0398041,"Missing"
2020.semeval-1.184,2020.semeval-1.216,0,0.140749,"ERT, DistilBERT, GPT-2, RoBERTa, XLNet, and XLM to probe their ability to identify emphasis without any fine-tuning. Their interesting findings indicate that DistilBERT is more successful in predicting emphasis while XLNet and GPT-2 perform poorly when there is no training for this task. The top non-transformer-based model, Procyon (ranked 12th), successfully proposed an ELMo-based 1366 Table 4: Main features in participating systems. An ‘N/A’ in the Ref. column means that we did not receive a system description paper for that entry. Rank 1 Team Name ERNIE Ref. (Huang et al., 2020) 2 Hitachi (Morio et al., 2020) 3 4 IITK Randomseed19 (Singhal et al., 2020) (Shatilov et al., 2020) 5 Sherry N/A 5 Sattiy N/A 7 FPAI (Guo et al., 2020) 10 BugHunter N/A 11 11 Amobee MIDAS N/A (Anand et al., 2020) 12 Procyon N/A 12 Jupyter N/A 13 CrazyRock N/A 14 CLP N/A 15 16 17 18 18 20 TextLearner Bright LAST AP T¨extmarkers EL-BERT (Yang et al., 2020) N/A (Bestgen, 2020) N/A (Glocker and Markianos Wright, 2020) (Kanani et al., 2020) 22 UIC-NLP (Hossu and Parde, 2020) 23 IDS (Shin et al., 2020) 24 YNU-HPCC (Liao et al., 2020) 1367 Best performing Methods ERNIE 2.0 + data augmentation + hand-crafted features Distribution"
2020.semeval-1.184,Y14-1022,0,0.288573,"ainly focuses on finding important nouns or noun phrases (Augenstein et al., 2017). In contrast, emphasis could be applied to a subset of words with different roles in a sentence. Generally, word emphasis may use to express emotions, show contrast, capture a reader’s interest or clarify a message. Moreover, emphasis selection in social media posts deals with very short texts and the prediction needs to be made based on a single instance. In the context of expressive prosody generation, emphasis has been addressed based on acoustic and prosodic features that exist in spoken data. For example, (Nakajima et al., 2014) predicted emphasized accent phrases from advertisement text information and (Mass et al., 2018) modeled word emphasis on audience-addressed speeches. 3 Data Collection The data used for this shared task is the integration of two datasets from different sources, which are created from scratch based on texts collected from the Adobe Spark and Wisdom Quotes website. The dataset used for this task can be found in the task’s data repository3 . The following are the descriptions of the two datasets. The Spark dataset is collection of 1,195 instances from Adobe Spark4 . It contains a variety of subj"
2020.semeval-1.184,N18-1202,0,0.0210204,"aged value (RANK). To better handle word duplicates, the computation is based on the position of words in a sentence rather than the actual words. Note that there were many cases where two or more tokens have the exact same probability. In this case, if the model predicts either one of the labels, we considered it as a correct answer. Table 5 shows some examples form the dataset, illustrating how the metric is computed. 6 Baseline Model We provided a baseline model for this task. This model (DL-BiLSTM-ELMo) is a sequence-labeling model that essentially utilizes ELMo contextualized embeddings (Peters et al., 2018) as well as two BiLSTM layers to label emphasis. During the training phase, the Kullback-Leibler Divergence (KL-DIV) (Kullback and Leibler, 1951) is used as the loss function. More analysis and the complete description of this model is provided in (Shirani et al., 2019). 7 Systems and Results This task attracted 197 participants and a total of 31 teams made submissions to this task. The teams that submitted papers for the SemEval-2020 proceedings are listed in Table 3. In total, 25 teams performed higher than the baseline and six teams performed lower. 13 of the 31 teams also submitted their s"
2020.semeval-1.184,2020.semeval-1.220,0,0.0498012,"Missing"
2020.semeval-1.184,2020.semeval-1.185,0,0.460143,"oints higher than the second team and 0.013 points higher than the third team. ERNIE, achieved the highest 1364 Figure 4: Pie chart showing the pre-trained models used in this task. score not only in RANK score but across all four scores. The next system on our leader board is Hitachi, with a score of 0.814. And finally, IITK, by achieving 0.810 RANK, stands in third place. 7.2 Best Paper Awards Our shared task awarded several best paper distinctions to complement the top performing systems. Here are the categories of best papers and the winners for each: • Best system description paper: IDS (Shin et al., 2020), this paper, with interesting analysis components, advances our understanding regarding the effectiveness of pre-trained language models for this specific task. • Best result interpretation paper: MIDAS (Anand et al., 2020), the authors go the extra mile to analyze the results in this paper. • Best negative results paper: UIC-NLP (Hossu and Parde, 2020), the authors performed extensive experiments with non-contextualized pre-trained models as well as a variety of hand-crafted features. Through the error analysis, the authors identified a number of common challenging patterns for the model, in"
2020.semeval-1.184,P19-1112,1,0.428485,"importantly, the insights gained from the task. 1.1 Task Definition Given a sequence of tokens C = {x1 , ..., xn }, a real number yi ∈ [0, 1] needs to be assigned for each token in the sequence, indicating the degree to which the token needs to be emphasized. In other words, we define the emphasis score yi as the probability or weight of the ith token in the sequence. Finally, during the evaluation, the final set of emphases are generated by selecting tokens with the highest values (described in Section 5). 2 Related Work We firstly introduced and formulated the task of emphasis selection in (Shirani et al., 2019) in which an end-to-end label distribution learning (LDL) model in a sequence tagging architecture is proposed to 1361 model emphasis. We evaluated the model against different baselines on the Spark dataset (introduced in Section 3). Keyword or key-phrase detection may be the closest topic to emphasis selection. Keywords can capture the main topics described in a given document (Turney, 2002). Modeling keywords or key-phrases has been widely addressed in different domains such as news articles (Wan et al., 2007), scientific publications (Nguyen and Kan, 2007) and Twitter data (Zhang et al., 20"
2020.semeval-1.184,2020.semeval-1.217,0,0.152241,"XLM to probe their ability to identify emphasis without any fine-tuning. Their interesting findings indicate that DistilBERT is more successful in predicting emphasis while XLNet and GPT-2 perform poorly when there is no training for this task. The top non-transformer-based model, Procyon (ranked 12th), successfully proposed an ELMo-based 1366 Table 4: Main features in participating systems. An ‘N/A’ in the Ref. column means that we did not receive a system description paper for that entry. Rank 1 Team Name ERNIE Ref. (Huang et al., 2020) 2 Hitachi (Morio et al., 2020) 3 4 IITK Randomseed19 (Singhal et al., 2020) (Shatilov et al., 2020) 5 Sherry N/A 5 Sattiy N/A 7 FPAI (Guo et al., 2020) 10 BugHunter N/A 11 11 Amobee MIDAS N/A (Anand et al., 2020) 12 Procyon N/A 12 Jupyter N/A 13 CrazyRock N/A 14 CLP N/A 15 16 17 18 18 20 TextLearner Bright LAST AP T¨extmarkers EL-BERT (Yang et al., 2020) N/A (Bestgen, 2020) N/A (Glocker and Markianos Wright, 2020) (Kanani et al., 2020) 22 UIC-NLP (Hossu and Parde, 2020) 23 IDS (Shin et al., 2020) 24 YNU-HPCC (Liao et al., 2020) 1367 Best performing Methods ERNIE 2.0 + data augmentation + hand-crafted features Distribution fusion of BERT, GPT-2, RoBERTa, XLM-RoBERTa,"
2020.semeval-1.184,N03-1033,0,0.0926157,"label frequencies respectively. Words Tag Your Best Friends 4 A1 B O O O A2 O O B I A3 B O B O A4 O O B O A5 B O B O A6 O O B I A7 O O O B A8 O O B O A9 B O B I Freq. [B,I,O] [4,0,5] [0,0,9] [7,0,2] [1,3,5] Norm. Freq. [B,I,O] [0.44,0,0.55] [0,0,1] [0.77,0,0.22] [0.11,0.33,0.55] Emphasis Probs [B+I] [0.44] [0] [0.77] [0.44] Data Analysis Many systems reported performance gain by using Part of Speech Tags (POS) tags in their models. In this section, we analyze the effectiveness of this feature by closely examining the top 20 POS tags in our dataset. We used the Stanford Part-Of-Speech Tagger (Toutanova et al., 2003) to obtain POS tags for all tokens in our dataset. We divide the emphasis probabilities to four intervals (0-0.25, 0.25-0.50, 0.50-0.75 and 0.75-1.00) and compute how the POS tags are distributed in these four intervals. Figure 3: Frequencies of the top 20 POS tags in 0-0.25, 0.25-0.5, 0.5-0.75, 0.75-1.00 intervals of emphasis probabilities. The vertical values correspond to the percentage of tag counts over the total number of words in training set. Figure 3 shows the occurrence of the top 20 POS tags in four emphasis probability intervals for all token labels in our training set. POS tags li"
2020.semeval-1.184,P07-1070,0,0.01227,"d Work We firstly introduced and formulated the task of emphasis selection in (Shirani et al., 2019) in which an end-to-end label distribution learning (LDL) model in a sequence tagging architecture is proposed to 1361 model emphasis. We evaluated the model against different baselines on the Spark dataset (introduced in Section 3). Keyword or key-phrase detection may be the closest topic to emphasis selection. Keywords can capture the main topics described in a given document (Turney, 2002). Modeling keywords or key-phrases has been widely addressed in different domains such as news articles (Wan et al., 2007), scientific publications (Nguyen and Kan, 2007) and Twitter data (Zhang et al., 2016; Bellaachia and Al-Dhelaan, 2012). Keyword detection mainly focuses on finding important nouns or noun phrases (Augenstein et al., 2017). In contrast, emphasis could be applied to a subset of words with different roles in a sentence. Generally, word emphasis may use to express emotions, show contrast, capture a reader’s interest or clarify a message. Moreover, emphasis selection in social media posts deals with very short texts and the prediction needs to be made based on a single instance. In the context of"
2020.semeval-1.184,2020.semeval-1.221,0,0.0731643,"Missing"
2020.semeval-1.184,D16-1080,0,0.056644,"ni et al., 2019) in which an end-to-end label distribution learning (LDL) model in a sequence tagging architecture is proposed to 1361 model emphasis. We evaluated the model against different baselines on the Spark dataset (introduced in Section 3). Keyword or key-phrase detection may be the closest topic to emphasis selection. Keywords can capture the main topics described in a given document (Turney, 2002). Modeling keywords or key-phrases has been widely addressed in different domains such as news articles (Wan et al., 2007), scientific publications (Nguyen and Kan, 2007) and Twitter data (Zhang et al., 2016; Bellaachia and Al-Dhelaan, 2012). Keyword detection mainly focuses on finding important nouns or noun phrases (Augenstein et al., 2017). In contrast, emphasis could be applied to a subset of words with different roles in a sentence. Generally, word emphasis may use to express emotions, show contrast, capture a reader’s interest or clarify a message. Moreover, emphasis selection in social media posts deals with very short texts and the prediction needs to be made based on a single instance. In the context of expressive prosody generation, emphasis has been addressed based on acoustic and pros"
2020.semeval-1.41,2020.semeval-1.97,0,0.147201,"d task competition period, 279 submissions were made through the Codalab training phase, with many participants additionally developing and testing their systems locally. The evaluation periods brought 217 submissions for Subtask 1, 220 for Subtask 2, and 113 for Subtask 3. Results are based on the evaluation metrics described in Section 3. Participants in DeftEval 2020 used a various array of methods for the three subtasks. For Subtasks 1 and 2 (see Tables 3 and 4), many participants opted simply to use a pre-trained language model, especially BERT, RoBERTa, and XLNet (see Xie et al. (2020), Avram et al. (2020), Singh et al. (2020), Jeawak et al. (2020), Ranasinghe et al. (2020), and Davletov et al. (2020)). Other implementations incorporated various LSTM layers in addition to the use of BERT or other transformer architecture (see Zhang and Ren (2020)). 339 Figure 1: Subtask 1 results; 56 participants, 217 total successful submissions Figure 2: Subtask 2 results; 51 participants, 220 total successful submissions 340 Figure 3: Subtask 3 results; 27 participants, 113 total successful submissions Rank Team Name Submission Name Reference Models Multi-task BERT RoBERTa + Stochastic Weight Averaging Joint"
2020.semeval-1.41,2020.semeval-1.92,0,0.0566443,"Missing"
2020.semeval-1.41,2020.semeval-1.44,0,0.145721,"were made through the Codalab training phase, with many participants additionally developing and testing their systems locally. The evaluation periods brought 217 submissions for Subtask 1, 220 for Subtask 2, and 113 for Subtask 3. Results are based on the evaluation metrics described in Section 3. Participants in DeftEval 2020 used a various array of methods for the three subtasks. For Subtasks 1 and 2 (see Tables 3 and 4), many participants opted simply to use a pre-trained language model, especially BERT, RoBERTa, and XLNet (see Xie et al. (2020), Avram et al. (2020), Singh et al. (2020), Jeawak et al. (2020), Ranasinghe et al. (2020), and Davletov et al. (2020)). Other implementations incorporated various LSTM layers in addition to the use of BERT or other transformer architecture (see Zhang and Ren (2020)). 339 Figure 1: Subtask 1 results; 56 participants, 217 total successful submissions Figure 2: Subtask 2 results; 51 participants, 220 total successful submissions 340 Figure 3: Subtask 3 results; 27 participants, 113 total successful submissions Rank Team Name Submission Name Reference Models Multi-task BERT RoBERTa + Stochastic Weight Averaging Joint classification and sequence labeling pre-t"
2020.semeval-1.41,P16-1200,0,0.0214814,"ant that the actual number of sentences in the corpus was different from the reported number in (Spala et al., 2019). With the existing tokenziation fixes at publication time, the textbook corpus contains 16056 sentences (695627 tokens) and 9439 2-3 sentence context windows. 5.2 Subtask 3 difficulty In organizing Subtask 3, we believed the task of matching term-definition pairs to be particularly difficult, especially considering the number of additional possible tags in the DEFT corpus. Given the precedence of previous performance on other more constrained relation extraction tasks (see also Lin et al. (2016), Shen and Huang (2016), Sorokin and Gurevych (2017), etc.), we predicted this task would see mostly low performing scores. However, participants in the subtask received a median F1 score of 0.9043, with 16 of the 27 participants scoring above 0.9. Upon further inspection, we realize this is due to the narrow search space of the three-sentence context windows. In designing the subtasks, we decided to keep the context windows in the test set so that the format of the data remained the same between released training data and test data and to minimize confusion for an already complicated task. Un"
2020.semeval-1.41,P10-1134,0,0.22963,"3 distinct subtasks: 1) Sentence classification, 2) sequence labeling, and 3) relation extraction. 1 Introduction Definition extraction as a complex, real-world task is currently an emerging field of study. Traditional definition extraction approaches mostly rely on simple, syntactically straight-forward examples with relatively little variance in vocabulary. Corpora, including the WCL (Navigli et al., 2010) and ukWaC (Ferraresi et al., 2008), typically consist of “definition sentences” which follow a standard X is a (type) Y or X, such as Y syntactic structure. Many also contain definitors (Navigli and Velardi, 2010), such as “means”, “is”, or “is defined by”. We provide a complete review of the existing state of definition extraction in Spala et al. (2019). 2 Data The DeftEval task provided data from the DEFT corpus for training, development and testing. Introduced in Spala et al (2019), the DEFT corpus is currently the largest and most comprehensive corpus explicitly for definition extraction. In addition to the typical definition-type sentences discussed in the introduction, the corpus also contains sentence and “sentence windows” of wide variance in syntactic and semantic construction. Sentences for t"
2020.semeval-1.41,navigli-etal-2010-annotated,0,0.78257,"y of definitions in natural language. Definitions and glosses in free text often appear without explicit indicators, across sentences boundaries, or in an otherwise complex linguistic manner. DeftEval involved 3 distinct subtasks: 1) Sentence classification, 2) sequence labeling, and 3) relation extraction. 1 Introduction Definition extraction as a complex, real-world task is currently an emerging field of study. Traditional definition extraction approaches mostly rely on simple, syntactically straight-forward examples with relatively little variance in vocabulary. Corpora, including the WCL (Navigli et al., 2010) and ukWaC (Ferraresi et al., 2008), typically consist of “definition sentences” which follow a standard X is a (type) Y or X, such as Y syntactic structure. Many also contain definitors (Navigli and Velardi, 2010), such as “means”, “is”, or “is defined by”. We provide a complete review of the existing state of definition extraction in Spala et al. (2019). 2 Data The DeftEval task provided data from the DEFT corpus for training, development and testing. Introduced in Spala et al (2019), the DEFT corpus is currently the largest and most comprehensive corpus explicitly for definition extraction."
2020.semeval-1.41,W95-0107,0,0.317519,"d the ability to reliably extract these labels could indicate a model with a broader cabability to identify complex and lengthy definition relationships. Ultimately, we hope for models that could both extract and pair (as seen in Subtask 33.3) related term and definition labels together, but given the potential for these two tasks to be particularly difficult and the novel nature of the DEFT corpus, the tasks were split into two subtasks for DeftEval. The default format of the DEFT corpus data is similar to the CoNLL 2003 format (Tjong Kim Sang and De Meulder, 2003) and formatted as BIO data (Ramshaw and Marcus, 1995). Therefore, participants did not need to convert any of the data for Subtask 2. At test time, participants were provided sentences where each line contained a token, source text file, lower char bound and upper char bound. Participants were required to predict only the appropriate BIO tag. 3 https://github.com/adobe-research/deft corpus https://competitions.codalab.org/competitions/22759 5 https://github.com/adobe-research/deft corpus/blob/master/task1 converter.py 4 338 3.3 Subtask 3: Relation Extraction The final DeftEval subtask focused on extracting individual term-definition pairs, as we"
2020.semeval-1.41,2020.semeval-1.94,0,0.200541,"Codalab training phase, with many participants additionally developing and testing their systems locally. The evaluation periods brought 217 submissions for Subtask 1, 220 for Subtask 2, and 113 for Subtask 3. Results are based on the evaluation metrics described in Section 3. Participants in DeftEval 2020 used a various array of methods for the three subtasks. For Subtasks 1 and 2 (see Tables 3 and 4), many participants opted simply to use a pre-trained language model, especially BERT, RoBERTa, and XLNet (see Xie et al. (2020), Avram et al. (2020), Singh et al. (2020), Jeawak et al. (2020), Ranasinghe et al. (2020), and Davletov et al. (2020)). Other implementations incorporated various LSTM layers in addition to the use of BERT or other transformer architecture (see Zhang and Ren (2020)). 339 Figure 1: Subtask 1 results; 56 participants, 217 total successful submissions Figure 2: Subtask 2 results; 51 participants, 220 total successful submissions 340 Figure 3: Subtask 3 results; 27 participants, 113 total successful submissions Rank Team Name Submission Name Reference Models Multi-task BERT RoBERTa + Stochastic Weight Averaging Joint classification and sequence labeling pre-trained model with MLP and"
2020.semeval-1.41,C16-1238,0,0.0247853,"number of sentences in the corpus was different from the reported number in (Spala et al., 2019). With the existing tokenziation fixes at publication time, the textbook corpus contains 16056 sentences (695627 tokens) and 9439 2-3 sentence context windows. 5.2 Subtask 3 difficulty In organizing Subtask 3, we believed the task of matching term-definition pairs to be particularly difficult, especially considering the number of additional possible tags in the DEFT corpus. Given the precedence of previous performance on other more constrained relation extraction tasks (see also Lin et al. (2016), Shen and Huang (2016), Sorokin and Gurevych (2017), etc.), we predicted this task would see mostly low performing scores. However, participants in the subtask received a median F1 score of 0.9043, with 16 of the 27 participants scoring above 0.9. Upon further inspection, we realize this is due to the narrow search space of the three-sentence context windows. In designing the subtasks, we decided to keep the context windows in the test set so that the format of the data remained the same between released training data and test data and to minimize confusion for an already complicated task. Unfortunately, most conte"
2020.semeval-1.41,2020.semeval-1.93,0,0.0598852,"riod, 279 submissions were made through the Codalab training phase, with many participants additionally developing and testing their systems locally. The evaluation periods brought 217 submissions for Subtask 1, 220 for Subtask 2, and 113 for Subtask 3. Results are based on the evaluation metrics described in Section 3. Participants in DeftEval 2020 used a various array of methods for the three subtasks. For Subtasks 1 and 2 (see Tables 3 and 4), many participants opted simply to use a pre-trained language model, especially BERT, RoBERTa, and XLNet (see Xie et al. (2020), Avram et al. (2020), Singh et al. (2020), Jeawak et al. (2020), Ranasinghe et al. (2020), and Davletov et al. (2020)). Other implementations incorporated various LSTM layers in addition to the use of BERT or other transformer architecture (see Zhang and Ren (2020)). 339 Figure 1: Subtask 1 results; 56 participants, 217 total successful submissions Figure 2: Subtask 2 results; 51 participants, 220 total successful submissions 340 Figure 3: Subtask 3 results; 27 participants, 113 total successful submissions Rank Team Name Submission Name Reference Models Multi-task BERT RoBERTa + Stochastic Weight Averaging Joint classification and s"
2020.semeval-1.41,2020.semeval-1.91,0,0.0261047,"Missing"
2020.semeval-1.41,D17-1188,0,0.0169683,"the corpus was different from the reported number in (Spala et al., 2019). With the existing tokenziation fixes at publication time, the textbook corpus contains 16056 sentences (695627 tokens) and 9439 2-3 sentence context windows. 5.2 Subtask 3 difficulty In organizing Subtask 3, we believed the task of matching term-definition pairs to be particularly difficult, especially considering the number of additional possible tags in the DEFT corpus. Given the precedence of previous performance on other more constrained relation extraction tasks (see also Lin et al. (2016), Shen and Huang (2016), Sorokin and Gurevych (2017), etc.), we predicted this task would see mostly low performing scores. However, participants in the subtask received a median F1 score of 0.9043, with 16 of the 27 participants scoring above 0.9. Upon further inspection, we realize this is due to the narrow search space of the three-sentence context windows. In designing the subtasks, we decided to keep the context windows in the test set so that the format of the data remained the same between released training data and test data and to minimize confusion for an already complicated task. Unfortunately, most context windows only include one s"
2020.semeval-1.41,W19-4015,1,0.475841,"real-world task is currently an emerging field of study. Traditional definition extraction approaches mostly rely on simple, syntactically straight-forward examples with relatively little variance in vocabulary. Corpora, including the WCL (Navigli et al., 2010) and ukWaC (Ferraresi et al., 2008), typically consist of “definition sentences” which follow a standard X is a (type) Y or X, such as Y syntactic structure. Many also contain definitors (Navigli and Velardi, 2010), such as “means”, “is”, or “is defined by”. We provide a complete review of the existing state of definition extraction in Spala et al. (2019). 2 Data The DeftEval task provided data from the DEFT corpus for training, development and testing. Introduced in Spala et al (2019), the DEFT corpus is currently the largest and most comprehensive corpus explicitly for definition extraction. In addition to the typical definition-type sentences discussed in the introduction, the corpus also contains sentence and “sentence windows” of wide variance in syntactic and semantic construction. Sentences for the corpus were retrieved from the open source textbook website cnx.org, as well as from various 2017 SEC contract filings from the publicly ava"
2020.semeval-1.41,storrer-wellinghoff-2006-automated,0,0.169281,"Missing"
2020.semeval-1.41,W03-0419,0,0.67373,"Missing"
2020.semeval-1.41,2020.semeval-1.96,0,0.138479,"course of the shared task competition period, 279 submissions were made through the Codalab training phase, with many participants additionally developing and testing their systems locally. The evaluation periods brought 217 submissions for Subtask 1, 220 for Subtask 2, and 113 for Subtask 3. Results are based on the evaluation metrics described in Section 3. Participants in DeftEval 2020 used a various array of methods for the three subtasks. For Subtasks 1 and 2 (see Tables 3 and 4), many participants opted simply to use a pre-trained language model, especially BERT, RoBERTa, and XLNet (see Xie et al. (2020), Avram et al. (2020), Singh et al. (2020), Jeawak et al. (2020), Ranasinghe et al. (2020), and Davletov et al. (2020)). Other implementations incorporated various LSTM layers in addition to the use of BERT or other transformer architecture (see Zhang and Ren (2020)). 339 Figure 1: Subtask 1 results; 56 participants, 217 total successful submissions Figure 2: Subtask 2 results; 51 participants, 220 total successful submissions 340 Figure 3: Subtask 3 results; 27 participants, 113 total successful submissions Rank Team Name Submission Name Reference Models Multi-task BERT RoBERTa + Stochastic W"
2020.semeval-1.41,2020.semeval-1.90,0,0.0350601,"task 2, and 113 for Subtask 3. Results are based on the evaluation metrics described in Section 3. Participants in DeftEval 2020 used a various array of methods for the three subtasks. For Subtasks 1 and 2 (see Tables 3 and 4), many participants opted simply to use a pre-trained language model, especially BERT, RoBERTa, and XLNet (see Xie et al. (2020), Avram et al. (2020), Singh et al. (2020), Jeawak et al. (2020), Ranasinghe et al. (2020), and Davletov et al. (2020)). Other implementations incorporated various LSTM layers in addition to the use of BERT or other transformer architecture (see Zhang and Ren (2020)). 339 Figure 1: Subtask 1 results; 56 participants, 217 total successful submissions Figure 2: Subtask 2 results; 51 participants, 220 total successful submissions 340 Figure 3: Subtask 3 results; 27 participants, 113 total successful submissions Rank Team Name Submission Name Reference Models Multi-task BERT RoBERTa + Stochastic Weight Averaging Joint classification and sequence labeling pre-trained model with MLP and CRF layer BERT with two-step fine tuning BERT with BiLSTM + attention XLNet 5 6 ∗ ACNLP davletov-aa caspa Davletov et al. (2020) Caspani et al. (2020) 12 UNIXLONG unixlong Xie"
2021.acl-long.119,W19-5059,0,0.0364186,"Missing"
2021.acl-long.119,S17-2057,0,0.0571488,"Missing"
2021.acl-long.119,P19-1215,0,0.349036,"e? Figure 1: We highlight the main four aspects of the CHQ. Our method learns from the task of Recognizing Question Entailment to generate more informative summaries compared to the baseline. Introduction In order to retrieve relevant answers, one of the basic steps in Question Answering (QA) systems is understanding the intent of questions (Chen et al., 2012; Cai et al., 2017). This is particularly important for medical QA systems (Wu et al., 2020), as consumer health questions – questions asked by patients – may use a vocabulary distinct from doctors to describe similar health concepts (Ben Abacha and Demner-Fushman, 2019a). Consumer health questions may also contain peripheral information like patient history (Roberts and Demner-Fushman, 2016), that are not necessary to answer questions. There is a growing number of approaches to medical question understanding, including query relaxation (Ben Abacha and Zweigenbaum, 2015; Lei et al., 2020), question entailment (Ben Abacha and Demner-Fushman, 2016, 2019b; Agrawal et al., 2019), question summarization (Ben Abacha and Demner-Fushman, 2019a), and question similarity (Ben Abacha and Demner-Fushman, 2017; Yan and Li, 2018; McCreery et al., 2019). Medical question s"
2021.acl-long.119,W19-5039,0,0.226837,"benefit from language models that use multi-task learning and transfer learning. There are pretrained language models that are geared towards BioNLP applications, that are based on BERT (Devlin et al., 2019). Those include SciBERT (Beltagy et al., 2019) which has been fine-tuned using biomedical text from PubMed. BioBERT (Lee et al., 2020) has been finetuned on the PMC dataset, whereas models named ClinicalBERT (Huang et al., 2019; Alsentzer et al., 2019) additionally use the MIMIC III dataset (Johnson et al., 2016). Transfer learning was a popular approach at the 2019 MEDIQA shared task (Ben Abacha et al., 2019) on medical NLI, RQE and QA. The question answering task involved re-ranking answers, not generating them (Demner-Fushman et al., 2020). For the RQE task, the best-performing model (Zhu et al., 2019) uses transfer learning on NLI and ensemble methods. 3 Methodology We consider the multi-task learning of medical question summarization and medical RQE. The input to both tasks is a pair of medical questions. The first question is called a Consumer Health Question (CHQ), and the second question is called a Frequently Asked Question (FAQ). The CHQ is written by a patient and is usually longer and m"
2021.acl-long.119,D15-1075,0,0.0339988,"sharing configurations in RQE accuracy, and in the sum of ROUGE F1 scores. These results show our proposed smoother parameter-sharing transition between encoder and decoder layers brings about higher performance. 4.5 4.5.1 Results and Discussion Summarization Results Baselines. We consider three main baselines. The first one is BART (Lewis et al., 2019), where we only train on the summarization task. The second baseline trains BART on the same MTL settings as Pasunuru et al. (2017), using alternative training with entailment generation on the Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015) and having a shared decoder and taskspecific encoders. The third baseline trains BART on the same MTL settings as Guo et al. (2018), where, on top of the entailment generation task, we add the question generation task using the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016), and all parameters are soft-shared, except for the task-specific first encoder layer and last decoder layer. In addition, we also report the baselines assessed by Ben Abacha and Demner-Fushman (2019a) for MeQSum. For data augmentation, they use semantically-selected relevant question pairs from the Q"
2021.acl-long.119,W19-1909,0,0.0577331,"Missing"
2021.acl-long.119,D19-1371,0,0.0148395,"r method outperforms the pointer-generator networks of See et al. (2017) on the CNN-Dailymail news summarization baseline. Here, the authors show performance increase in entailment on some batch sizes and decrease on other batch sizes, and they consider entailment as an auxiliary task. 1506 Transfer Learning for Medical QA. BioNLP is one of many NLP applications to benefit from language models that use multi-task learning and transfer learning. There are pretrained language models that are geared towards BioNLP applications, that are based on BERT (Devlin et al., 2019). Those include SciBERT (Beltagy et al., 2019) which has been fine-tuned using biomedical text from PubMed. BioBERT (Lee et al., 2020) has been finetuned on the PMC dataset, whereas models named ClinicalBERT (Huang et al., 2019; Alsentzer et al., 2019) additionally use the MIMIC III dataset (Johnson et al., 2016). Transfer learning was a popular approach at the 2019 MEDIQA shared task (Ben Abacha et al., 2019) on medical NLI, RQE and QA. The question answering task involved re-ranking answers, not generating them (Demner-Fushman et al., 2020). For the RQE task, the best-performing model (Zhu et al., 2019) uses transfer learning on NLI and"
2021.acl-long.119,N19-1423,0,0.0274121,"ng improves over hard parameter-sharing. Their method outperforms the pointer-generator networks of See et al. (2017) on the CNN-Dailymail news summarization baseline. Here, the authors show performance increase in entailment on some batch sizes and decrease on other batch sizes, and they consider entailment as an auxiliary task. 1506 Transfer Learning for Medical QA. BioNLP is one of many NLP applications to benefit from language models that use multi-task learning and transfer learning. There are pretrained language models that are geared towards BioNLP applications, that are based on BERT (Devlin et al., 2019). Those include SciBERT (Beltagy et al., 2019) which has been fine-tuned using biomedical text from PubMed. BioBERT (Lee et al., 2020) has been finetuned on the PMC dataset, whereas models named ClinicalBERT (Huang et al., 2019; Alsentzer et al., 2019) additionally use the MIMIC III dataset (Johnson et al., 2016). Transfer learning was a popular approach at the 2019 MEDIQA shared task (Ben Abacha et al., 2019) on medical NLI, RQE and QA. The question answering task involved re-ranking answers, not generating them (Demner-Fushman et al., 2020). For the RQE task, the best-performing model (Zhu e"
2021.acl-long.119,P19-1213,0,0.0226625,"Missing"
2021.acl-long.119,W07-1401,0,0.0840394,"received by the U.S. National Library of Medicine (NLM) and a FAQ from NIH institutes. Whereas the train and dev sets have automatically generated CHQs, the test set has manually written CHQs. This results in significantly higher dev set results than for test sets, as has been observed during the 2019 MEDIQA shared task. In addition, we use two pretraining datasets. We use the XSum dataset (Narayan et al., 2018), an abstractive summarization benchmark, for question summarization. For the RQE task, we use the Recognizing Textual Entailment (RTE) dataset (Dagan et al., 2005; Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009) from the GLUE benchmark (Wang et al., 2018). 4 4.2 LGS (θ) = γ ∗ N −1  X e N −n N 2  QS RQE − 1 θdec,n − θdec,n n=1 4.1 Experiments Datasets We consider 3 medical question summarization datasets and 1 medical RQE dataset. We show dataset statistics in Table 1. MeQSum and MEDIQA RQE can be considered low-resource, whereas the other two are far larger. Our datasets are in the English language. Due to space constraints, we briefly introduce the datasets and leave additional details in the appendix. Setup and Training Settings All of our models use the BART large archi"
2021.acl-long.119,P18-1064,0,0.369914,"uistics and the 11th International Joint Conference on Natural Language Processing, pages 1505–1515 August 1–6, 2021. ©2021 Association for Computational Linguistics Task Learning (MTL) problem involving the two tasks of question summarization and Recognizing Question Entailment. We use a simple sum of learning objectives in Mrini et al. (2021b). In this paper, we introduce a novel, gradually soft multi-task and data-augmented approach to medical question understanding.1 Previous work on combining summarization and entailment uses at least 2 datasets – 1 from each task (Pasunuru et al., 2017; Guo et al., 2018). We first establish an equivalence between both tasks. This equivalence is the inspiration behind the data augmentation schemes introduced in our previous work (Mrini et al., 2021b). The goal of the data augmentation is to use a single dataset for MultiTask Learning. We propose to use a weighted loss function to simultaneously optimize for both tasks. Then, we propose a gradually soft parametersharing MTL approach. We conduct ablation studies to show that our two novelties – data augmentation and gradually soft parameter-sharing – improve performance in both tasks. Our proposed gradually soft"
2021.acl-long.119,S14-1010,0,0.0176936,"A either partially or fully. It differs from traditional definitions of entailment, where we consider that the premise entails the hypothesis if and only if the hypothesis is true only if the premise is true. Ben Abacha and Demner-Fushman (2016) define RQE within the context of Medical Question Answering. The goal is to match a Consumer Health Question (CHQ) to a Frequently Asked Question (FAQ), and ultimately match the CHQ to an expertwritten answer. Summarization and Entailment. There is a growing body of work combining summarization and entailment (Lloret et al., 2008; Mehdad et al., 2013; Gupta et al., 2014). Falke et al. (2019) use textual entailment predictions to detect factual errors in abstractive summaries generated by state-of-the-art models. Pasunuru and Bansal (2018) propose an entailment reward for their reinforced abstractive summarizer, where the entailment score is obtained from a pre-trained and frozen natural language inference model. Pasunuru et al. (2017) propose an LSTM encoder-decoder model that incorporates entailment generation and abstractive summarization. The authors optimize alternatively between the two tasks, and use separate Natural Language Inference (NLI) and abstrac"
2021.acl-long.119,2020.acl-main.703,0,0.044172,"Missing"
2021.acl-long.119,C18-1121,0,0.0150455,"ctual errors in abstractive summaries generated by state-of-the-art models. Pasunuru and Bansal (2018) propose an entailment reward for their reinforced abstractive summarizer, where the entailment score is obtained from a pre-trained and frozen natural language inference model. Pasunuru et al. (2017) propose an LSTM encoder-decoder model that incorporates entailment generation and abstractive summarization. The authors optimize alternatively between the two tasks, and use separate Natural Language Inference (NLI) and abstractive summarization datasets. Only the decoder parameters are shared. Li et al. (2018) closely follow the MTL setting of Pasunuru et al. (2017), and propose a model with a shared encoder, an NLI classifier and an NLI-rewarded summarization decoder. Guo et al. (2018) introduce a pointer-generator summarization model with coverage loss (See et al., 2017). They build upon the work of Pasunuru et al. (2017), and add question generation on top of the two tasks of abstractive summarization and entailment generation. They also alternate between the three different objectives. The authors propose to share all parameters except the first layer of the encoder and the last layer of the de"
2021.acl-long.119,P19-1441,0,0.0149728,"y, and the entailment label lentail ∈ {0, 1}, we optimize the following multi-task learning loss function: LMTL (θ) = − λ ∗ logp(y|x; θ) + (1 − λ) ∗ BCE ([x; y] , lentail ; θ) (1) where BCE is binary cross entropy, and λ is a hyperparameter between 0 and 1. 1508 3.4 DATASET MeQSum HealthCareMagic iCliniq MEDIQA RQE Gradually Soft Parameter-Sharing In multi-task learning, there are two widely used approaches: hard parameter-sharing and soft parameter-sharing. Guo et al. (2018) propose soft parameter-sharing for all parameters except the first layer of the encoder and last layer of the decoder. Liu et al. (2019) introduce MT-DNN and show that hard parameter-sharing of all of the transformer encoder layers, and only having task-specific classification heads produces results that set a new state of the art for the GLUE benchmark (Wang et al., 2018). We propose a hybrid approach, where we apply hard parameter-sharing for the encoder, and a novel gradually soft parameter-sharing approach for the decoder layers. We define gradually soft parametersharing as a smooth transition from hard parametersharing to task-specific layers. It is a soft parametersharing approach that is gradually toned down from the fi"
2021.acl-long.119,W13-2117,0,0.023717,"er to A, and answers A either partially or fully. It differs from traditional definitions of entailment, where we consider that the premise entails the hypothesis if and only if the hypothesis is true only if the premise is true. Ben Abacha and Demner-Fushman (2016) define RQE within the context of Medical Question Answering. The goal is to match a Consumer Health Question (CHQ) to a Frequently Asked Question (FAQ), and ultimately match the CHQ to an expertwritten answer. Summarization and Entailment. There is a growing body of work combining summarization and entailment (Lloret et al., 2008; Mehdad et al., 2013; Gupta et al., 2014). Falke et al. (2019) use textual entailment predictions to detect factual errors in abstractive summaries generated by state-of-the-art models. Pasunuru and Bansal (2018) propose an entailment reward for their reinforced abstractive summarizer, where the entailment score is obtained from a pre-trained and frozen natural language inference model. Pasunuru et al. (2017) propose an LSTM encoder-decoder model that incorporates entailment generation and abstractive summarization. The authors optimize alternatively between the two tasks, and use separate Natural Language Infere"
2021.acl-long.119,P17-1099,0,0.513219,"rence model. Pasunuru et al. (2017) propose an LSTM encoder-decoder model that incorporates entailment generation and abstractive summarization. The authors optimize alternatively between the two tasks, and use separate Natural Language Inference (NLI) and abstractive summarization datasets. Only the decoder parameters are shared. Li et al. (2018) closely follow the MTL setting of Pasunuru et al. (2017), and propose a model with a shared encoder, an NLI classifier and an NLI-rewarded summarization decoder. Guo et al. (2018) introduce a pointer-generator summarization model with coverage loss (See et al., 2017). They build upon the work of Pasunuru et al. (2017), and add question generation on top of the two tasks of abstractive summarization and entailment generation. They also alternate between the three different objectives. The authors propose to share all parameters except the first layer of the encoder and the last layer of the decoder, and show that soft parameter-sharing improves over hard parameter-sharing. Their method outperforms the pointer-generator networks of See et al. (2017) on the CNN-Dailymail news summarization baseline. Here, the authors show performance increase in entailment o"
2021.acl-long.119,2021.nlpmc-1.8,1,0.269857,"Missing"
2021.acl-long.119,2021.bionlp-1.28,1,0.864351,"Missing"
2021.acl-long.119,K16-1028,0,0.050147,"Missing"
2021.acl-long.119,D18-1206,0,0.012065,"iCliniq’s patient-written summaries. The medical RQE dataset is the MEDIQA RQE dataset from the 2019 MEDIQA shared task (Ben Abacha et al., 2019). Similarly to MeQSum, the question pairs match a longer CHQ received by the U.S. National Library of Medicine (NLM) and a FAQ from NIH institutes. Whereas the train and dev sets have automatically generated CHQs, the test set has manually written CHQs. This results in significantly higher dev set results than for test sets, as has been observed during the 2019 MEDIQA shared task. In addition, we use two pretraining datasets. We use the XSum dataset (Narayan et al., 2018), an abstractive summarization benchmark, for question summarization. For the RQE task, we use the Recognizing Textual Entailment (RTE) dataset (Dagan et al., 2005; Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009) from the GLUE benchmark (Wang et al., 2018). 4 4.2 LGS (θ) = γ ∗ N −1  X e N −n N 2  QS RQE − 1 θdec,n − θdec,n n=1 4.1 Experiments Datasets We consider 3 medical question summarization datasets and 1 medical RQE dataset. We show dataset statistics in Table 1. MeQSum and MEDIQA RQE can be considered low-resource, whereas the other two are far larger. Our datase"
2021.acl-long.119,N18-2102,0,0.0156156,"othesis is true only if the premise is true. Ben Abacha and Demner-Fushman (2016) define RQE within the context of Medical Question Answering. The goal is to match a Consumer Health Question (CHQ) to a Frequently Asked Question (FAQ), and ultimately match the CHQ to an expertwritten answer. Summarization and Entailment. There is a growing body of work combining summarization and entailment (Lloret et al., 2008; Mehdad et al., 2013; Gupta et al., 2014). Falke et al. (2019) use textual entailment predictions to detect factual errors in abstractive summaries generated by state-of-the-art models. Pasunuru and Bansal (2018) propose an entailment reward for their reinforced abstractive summarizer, where the entailment score is obtained from a pre-trained and frozen natural language inference model. Pasunuru et al. (2017) propose an LSTM encoder-decoder model that incorporates entailment generation and abstractive summarization. The authors optimize alternatively between the two tasks, and use separate Natural Language Inference (NLI) and abstractive summarization datasets. Only the decoder parameters are shared. Li et al. (2018) closely follow the MTL setting of Pasunuru et al. (2017), and propose a model with a"
2021.acl-long.119,W18-5446,0,0.132671,"er between 0 and 1. 1508 3.4 DATASET MeQSum HealthCareMagic iCliniq MEDIQA RQE Gradually Soft Parameter-Sharing In multi-task learning, there are two widely used approaches: hard parameter-sharing and soft parameter-sharing. Guo et al. (2018) propose soft parameter-sharing for all parameters except the first layer of the encoder and last layer of the decoder. Liu et al. (2019) introduce MT-DNN and show that hard parameter-sharing of all of the transformer encoder layers, and only having task-specific classification heads produces results that set a new state of the art for the GLUE benchmark (Wang et al., 2018). We propose a hybrid approach, where we apply hard parameter-sharing for the encoder, and a novel gradually soft parameter-sharing approach for the decoder layers. We define gradually soft parametersharing as a smooth transition from hard parametersharing to task-specific layers. It is a soft parametersharing approach that is gradually toned down from the first layer of the decoder to the last layer, which is entirely task-specific. In gradually soft parameter-sharing, we constrain decoder parameters to be close by penalizing their l2 distances, and the higher the layer the looser the constra"
2021.acl-long.119,W19-5046,0,0.0179303,"gic and MeQSum. However, we note an increase in correctness for the more extractive iCliniq dataset. On average, our gradually soft multi-task and dataaugmented method outputs summarized questions that are more fluent and more informative than the single-task BART baseline. 4.5.2 RQE Results and Discussion Baselines. We compare our method to three baselines. The first one trains a single-task BART on RQE, with a classification head pre-trained on RTE. The second baseline is a feature-based SVM from Ben Abacha and Demner-Fushman (2016) who introduced the MEDIQA RQE dataset. The third baseline (Zhou et al., 2019) is an adversarial MTL method combining medical question answering and RQE. The architecture consists of a shared transformer encoder using BioBERT embeddings (Lee et al., 2020), separate classification heads for RQE and medical QA, and a task discriminator for adversarial training. A separate dataset is used for medical QA (Ben Abacha et al., 2019). 1511 DATASET M ETRIC BASELINES Seq2seq Attentional Model (Nallapati et al., 2016) Pointer-Generator Networks (PG) (See et al., 2017) PG + Data Augmentation (Ben Abacha and DemnerFushman, 2019a) PG + Coverage Loss (See et al., 2017) PG + Coverage L"
2021.acl-long.119,W19-5040,0,0.0168083,"2019). Those include SciBERT (Beltagy et al., 2019) which has been fine-tuned using biomedical text from PubMed. BioBERT (Lee et al., 2020) has been finetuned on the PMC dataset, whereas models named ClinicalBERT (Huang et al., 2019; Alsentzer et al., 2019) additionally use the MIMIC III dataset (Johnson et al., 2016). Transfer learning was a popular approach at the 2019 MEDIQA shared task (Ben Abacha et al., 2019) on medical NLI, RQE and QA. The question answering task involved re-ranking answers, not generating them (Demner-Fushman et al., 2020). For the RQE task, the best-performing model (Zhu et al., 2019) uses transfer learning on NLI and ensemble methods. 3 Methodology We consider the multi-task learning of medical question summarization and medical RQE. The input to both tasks is a pair of medical questions. The first question is called a Consumer Health Question (CHQ), and the second question is called a Frequently Asked Question (FAQ). The CHQ is written by a patient and is usually longer and more informal, whereas the FAQ is usually a singlesentence question written by a medical expert. The purpose of both tasks is to match a CHQ to an FAQ, and ultimately to an expert-written answer that"
2021.acl-long.119,W17-4504,0,0.202468,"for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1505–1515 August 1–6, 2021. ©2021 Association for Computational Linguistics Task Learning (MTL) problem involving the two tasks of question summarization and Recognizing Question Entailment. We use a simple sum of learning objectives in Mrini et al. (2021b). In this paper, we introduce a novel, gradually soft multi-task and data-augmented approach to medical question understanding.1 Previous work on combining summarization and entailment uses at least 2 datasets – 1 from each task (Pasunuru et al., 2017; Guo et al., 2018). We first establish an equivalence between both tasks. This equivalence is the inspiration behind the data augmentation schemes introduced in our previous work (Mrini et al., 2021b). The goal of the data augmentation is to use a single dataset for MultiTask Learning. We propose to use a weighted loss function to simultaneously optimize for both tasks. Then, we propose a gradually soft parametersharing MTL approach. We conduct ablation studies to show that our two novelties – data augmentation and gradually soft parameter-sharing – improve performance in both tasks. Our prop"
2021.acl-long.119,D16-1264,0,0.0512791,"ee main baselines. The first one is BART (Lewis et al., 2019), where we only train on the summarization task. The second baseline trains BART on the same MTL settings as Pasunuru et al. (2017), using alternative training with entailment generation on the Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015) and having a shared decoder and taskspecific encoders. The third baseline trains BART on the same MTL settings as Guo et al. (2018), where, on top of the entailment generation task, we add the question generation task using the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016), and all parameters are soft-shared, except for the task-specific first encoder layer and last decoder layer. In addition, we also report the baselines assessed by Ben Abacha and Demner-Fushman (2019a) for MeQSum. For data augmentation, they use semantically-selected relevant question pairs from the Quora Question Pairs dataset (Iyer et al., 2017). Their results show that coverage loss (See et al., 2017) diminishes the added value of data augmentation in pointer-generator networks. Our summarization-only BART baseline exceeds all of the reported MeQSum baselines in ROUGE-1 F1. Summarization R"
2021.acl-long.126,D19-1290,1,0.78767,", or whether a claim paraphrases another. We build syntopical graphs by transferring pretrained pairwise models, requiring no additional training data to be annotated. We decompose the problem of viewpoint reconstruction into the subtasks of stance detection and aspect detection, and evaluate the benefits of syntopical graphs — which are a collection-level approach — on both tasks. For stance detection, we use the sentential argumentation mining collection (Stab et al., 2018) and the IBM claim stance dataset (Bar-Haim et al., 2017a). For aspect detection we use the argument frames collection (Ajjour et al., 2019). We treat the graph as an input to: (a) a graph neural network architecture for stance detection, and (b) graph algorithms for unsupervised tasks such as aspect clustering. In both settings, our results show that the syntopical graph approach improves significantly over content-only baselines. The contributions of the work are two-fold: 1. A well-motivated data structure for capturing the latent structure of an argumentative corpus, the syntopical graph. 2. An instantiation of syntopical graphs that yields state-of-the-art results on stance detection and aspect detection. 2 Related Work First"
2021.acl-long.126,E17-1024,0,0.105542,"simultaneously represents relationships such as relative stance, relative specificity, or whether a claim paraphrases another. We build syntopical graphs by transferring pretrained pairwise models, requiring no additional training data to be annotated. We decompose the problem of viewpoint reconstruction into the subtasks of stance detection and aspect detection, and evaluate the benefits of syntopical graphs — which are a collection-level approach — on both tasks. For stance detection, we use the sentential argumentation mining collection (Stab et al., 2018) and the IBM claim stance dataset (Bar-Haim et al., 2017a). For aspect detection we use the argument frames collection (Ajjour et al., 2019). We treat the graph as an input to: (a) a graph neural network architecture for stance detection, and (b) graph algorithms for unsupervised tasks such as aspect clustering. In both settings, our results show that the syntopical graph approach improves significantly over content-only baselines. The contributions of the work are two-fold: 1. A well-motivated data structure for capturing the latent structure of an argumentative corpus, the syntopical graph. 2. An instantiation of syntopical graphs that yields sta"
2021.acl-long.126,W17-5104,0,0.114145,"simultaneously represents relationships such as relative stance, relative specificity, or whether a claim paraphrases another. We build syntopical graphs by transferring pretrained pairwise models, requiring no additional training data to be annotated. We decompose the problem of viewpoint reconstruction into the subtasks of stance detection and aspect detection, and evaluate the benefits of syntopical graphs — which are a collection-level approach — on both tasks. For stance detection, we use the sentential argumentation mining collection (Stab et al., 2018) and the IBM claim stance dataset (Bar-Haim et al., 2017a). For aspect detection we use the argument frames collection (Ajjour et al., 2019). We treat the graph as an input to: (a) a graph neural network architecture for stance detection, and (b) graph algorithms for unsupervised tasks such as aspect clustering. In both settings, our results show that the syntopical graph approach improves significantly over content-only baselines. The contributions of the work are two-fold: 1. A well-motivated data structure for capturing the latent structure of an argumentative corpus, the syntopical graph. 2. An instantiation of syntopical graphs that yields sta"
2021.acl-long.126,2020.emnlp-main.3,0,0.031921,"approach with a BertGCN baseline, and we ablate all considered edge types, in order to show the importance of capturing these different textual relationships. Ultimately, we seek to facilitate understanding of the main viewpoints in a text collection. Qiu and Jiang (2013) used clustering-based viewpoint discovery to study the impact of the interaction of topics and users in forum discussions. Egan et al. (2016) used multi-document summarization techniques to mine and organize the main points in a debate, and Vilares and He (2017) mined the main topics and their aspects using a Bayesian model. Bar-Haim et al. (2020) introduced the idea of keypoint analysis, grouping arguments found in a collection by the viewpoint they reflect and summarizing each group to a salient keypoint. While our graph-based analysis is likely to be suitable for finding keypoints, we instead focus on reconstructing latent viewpoints by grouping claims, leaving open the option to identify the key claims in future work as it would require manual evaluation. 3 Syntopical Graphs We now introduce the concept of a syntopical graph. The goal of our syntopical graph is to systematically model the salient interactions of all claims in a col"
2021.acl-long.126,P19-3022,0,0.0191178,"order to help individuals make sense of a collection of documents for a given topic. Viewed through the lens of computational argumentation, these documents state claims or conclusions that can be grouped by the aspects of the topic they discuss as well as by the stance they convey towards the topic (Stede and Schneider, 2018). An individual aiming to form a thorough understanding of the topic needs to get an overview of these viewpoints and their interactions. This may be hard even if adequate tool support for browsing the collection is available (Wachsmuth et al., 2017a; Stab et al., 2018; Chen et al., 2019). We seek to enable systems that are capable of reconstructing viewpoints within a collection, where a viewpoint is expressed as a triple V = (topic, aspect, stance). We consider the argumentative unit of a claim to be the minimal expression of a viewpoint in natural language, such that a single viewpoint can have many claims expressing it. As an example, consider the following two claims: “Nuclear energy emits zero CO2 .” “Nuclear can provide a clean baseload, eliminating the need for fracking and coal mining.” Within a collection these claims express: V = (Nuclear Energy, env. impact, PRO) T"
2021.acl-long.126,C04-1051,0,0.324634,"nodes can have multiple edges of different types between them; a claim can both contradict and refute another claim, for instance. Edge Weights An edge can have a real-valued weight associated with it on the range (−1, 1), representing the strength of the connection. The relative stance edge between a claim which strongly refutes another would receive a weight close to −1. 3.2 Graph Construction For graph edges, we combine four pretrained models and two similarity measures. The pretrained edge types are: relative stance and relative specificity from Durmus et al. (2019), paraphrase edges from Dolan et al. (2004); Morris et al. (2020), and natural language inference edges from Williams et al. (2018); Liu et al. (2019). The edge weights are the confidence scores defined by weight(u, v, r) = ppos(u,v) − pneg(u,v) , where u and v are claims, r is the relation type, and ppos(u,v) is the probability of a positive association between the claims (e.g., “is a paraphrase” or “does entail”), pneg(u,v) for a negative one. For similarity-based edges, we use standard TF-IDF for term-based similarity and LDA for topic-based similarity (Blei et al., 2003), using cosine similarity as the edge weight. The document-cla"
2021.acl-long.126,P19-1456,0,0.0971144,"lated claims and the sentiment towards these aspects. From this information, they derived stance based on the contrastiveness of the aspects. Later, Bar-Haim et al. (2017b) mod1584 eled the context of a claim to account for cases without sentiment. Our work follows up on and generalizes this idea, systematically incorporating implicit and explicit structure induced by the topics, aspects, claims, and participants in a debate. In a similar vein, Li et al. (2018) embedded debate posts and authors jointly based on their interactions, in order to classify a post’s stance towards the debate topic. Durmus et al. (2019) encoded related pairs of claims using BERT to predict the stance and specificity of any claim in a complex structure of online debates. However, neither of these exploited the full graph structure resulting from all the relations and interactions in a debate, which is the gap we fill in this paper. Sridhar et al. (2015) model collective information about debate posts, authors, and their agreement and disagreement using probabilistic soft logic. Whereas they are restricted to the structure available in a forum, our approach can in principle be applied to arbitrary collections of text. We also"
2021.acl-long.126,W16-2816,0,0.0149183,"ce- or documentlevel classification. Our work generalizes this approach, focusing on incorporating many edge types with different meanings, such as relative stance or relative specificity. We compare our approach with a BertGCN baseline, and we ablate all considered edge types, in order to show the importance of capturing these different textual relationships. Ultimately, we seek to facilitate understanding of the main viewpoints in a text collection. Qiu and Jiang (2013) used clustering-based viewpoint discovery to study the impact of the interaction of topics and users in forum discussions. Egan et al. (2016) used multi-document summarization techniques to mine and organize the main points in a debate, and Vilares and He (2017) mined the main topics and their aspects using a Bayesian model. Bar-Haim et al. (2020) introduced the idea of keypoint analysis, grouping arguments found in a collection by the viewpoint they reflect and summarizing each group to a salient keypoint. While our graph-based analysis is likely to be suitable for finding keypoints, we instead focus on reconstructing latent viewpoints by grouping claims, leaving open the option to identify the key claims in future work as it woul"
2021.acl-long.126,P19-1049,0,0.0191126,"ose argument frames dataset we use, instead clustered aspects with Latent Semantic Analysis (LSA) and topic modeling. But, in general, aspects might not be mentioned in a text explicitly. Therefore, we follow these other approaches, treating the task as a clustering problem. Unlike them, however, we do not model only the content and linguistic structure of texts, but we combine them with the debate structure. Different types of argumentation graphs have been proposed, covering expert-stance information (Toledo-Ronen et al., 2016), basic argument and debate structure (Peldszus and Stede, 2015; Gemechu and Reed, 2019), specific effect relations (Al-Khatib et al., 2020; Kobbe et al., 2020), social media graphs (Aldayel and Magdy, 2019), and knowledge graphs (Zhang et al., 2020). Our main focus is not learning to construct ground-truth graphs, but how to use an approximated graph to derive properties such as stance and aspect. Our work resembles approaches that derive the relevance of arguments (Wachsmuth et al., 2017b) or their centrality and divisiveness in a discussion (Lawrence and Reed, 2017) from respective graphs. Sawhney et al. (2020) used a neural graph attention network to classify speech stance ba"
2021.acl-long.126,I13-1191,0,0.0251487,"as aspect clustering. In both settings, our results show that the syntopical graph approach improves significantly over content-only baselines. The contributions of the work are two-fold: 1. A well-motivated data structure for capturing the latent structure of an argumentative corpus, the syntopical graph. 2. An instantiation of syntopical graphs that yields state-of-the-art results on stance detection and aspect detection. 2 Related Work First attempts at stance detection used contentoriented features (Somasundaran and Wiebe, 2009). Later approaches, such as those by Ranade et al. (2013) and Hasan and Ng (2013), exploited common patterns in dialogic structure to improve stance detection. More tailored to argumentation, BarHaim et al. (2017a) first identified the aspects of a discussed topic in two related claims and the sentiment towards these aspects. From this information, they derived stance based on the contrastiveness of the aspects. Later, Bar-Haim et al. (2017b) mod1584 eled the context of a claim to account for cases without sentiment. Our work follows up on and generalizes this idea, systematically incorporating implicit and explicit structure induced by the topics, aspects, claims, and par"
2021.acl-long.126,2020.emnlp-main.4,0,0.0759605,"Missing"
2021.acl-long.126,W17-5114,0,0.0222256,"rt-stance information (Toledo-Ronen et al., 2016), basic argument and debate structure (Peldszus and Stede, 2015; Gemechu and Reed, 2019), specific effect relations (Al-Khatib et al., 2020; Kobbe et al., 2020), social media graphs (Aldayel and Magdy, 2019), and knowledge graphs (Zhang et al., 2020). Our main focus is not learning to construct ground-truth graphs, but how to use an approximated graph to derive properties such as stance and aspect. Our work resembles approaches that derive the relevance of arguments (Wachsmuth et al., 2017b) or their centrality and divisiveness in a discussion (Lawrence and Reed, 2017) from respective graphs. Sawhney et al. (2020) used a neural graph attention network to classify speech stance based on a graph with texts, speakers, and topics as nodes. While we also use a relational graph convolutional network for learning, the graph we propose captures implicit claim relations as well as explicit structure. In addition, text-based graph neural models have been proposed to facilitate classification, such as TextGCN (Yao et al., 2019) as well as the followup work BertGCN (Lin et al., 2021). These approaches build a graph over terms (using normalized mutual information for ed"
2021.acl-long.126,C18-1316,0,0.0233389,"ructure to improve stance detection. More tailored to argumentation, BarHaim et al. (2017a) first identified the aspects of a discussed topic in two related claims and the sentiment towards these aspects. From this information, they derived stance based on the contrastiveness of the aspects. Later, Bar-Haim et al. (2017b) mod1584 eled the context of a claim to account for cases without sentiment. Our work follows up on and generalizes this idea, systematically incorporating implicit and explicit structure induced by the topics, aspects, claims, and participants in a debate. In a similar vein, Li et al. (2018) embedded debate posts and authors jointly based on their interactions, in order to classify a post’s stance towards the debate topic. Durmus et al. (2019) encoded related pairs of claims using BERT to predict the stance and specificity of any claim in a complex structure of online debates. However, neither of these exploited the full graph structure resulting from all the relations and interactions in a debate, which is the gap we fill in this paper. Sridhar et al. (2015) model collective information about debate posts, authors, and their agreement and disagreement using probabilistic soft lo"
2021.acl-long.126,2021.findings-acl.126,0,0.137173,"(Wachsmuth et al., 2017b) or their centrality and divisiveness in a discussion (Lawrence and Reed, 2017) from respective graphs. Sawhney et al. (2020) used a neural graph attention network to classify speech stance based on a graph with texts, speakers, and topics as nodes. While we also use a relational graph convolutional network for learning, the graph we propose captures implicit claim relations as well as explicit structure. In addition, text-based graph neural models have been proposed to facilitate classification, such as TextGCN (Yao et al., 2019) as well as the followup work BertGCN (Lin et al., 2021). These approaches build a graph over terms (using normalized mutual information for edge weights) as well as sentences and documents (using TF-IDF for edge weights) to improve sentence- or documentlevel classification. Our work generalizes this approach, focusing on incorporating many edge types with different meanings, such as relative stance or relative specificity. We compare our approach with a BertGCN baseline, and we ablate all considered edge types, in order to show the importance of capturing these different textual relationships. Ultimately, we seek to facilitate understanding of the"
2021.acl-long.126,2021.ccl-1.108,0,0.0468629,"Missing"
2021.acl-long.126,N15-1046,0,0.0300103,"s. However, neither of these exploited the full graph structure resulting from all the relations and interactions in a debate, which is the gap we fill in this paper. Sridhar et al. (2015) model collective information about debate posts, authors, and their agreement and disagreement using probabilistic soft logic. Whereas they are restricted to the structure available in a forum, our approach can in principle be applied to arbitrary collections of text. We also tackle aspect detection, which may at first seem more content-oriented in nature. Accordingly, previous research such as the works of Misra et al. (2015) and Reimers et al. (2019b) employed word-based features or contextualized word embeddings for topic-specific aspect clustering. Ajjour et al. (2019), whose argument frames dataset we use, instead clustered aspects with Latent Semantic Analysis (LSA) and topic modeling. But, in general, aspects might not be mentioned in a text explicitly. Therefore, we follow these other approaches, treating the task as a clustering problem. Unlike them, however, we do not model only the content and linguistic structure of texts, but we combine them with the debate structure. Different types of argumentation g"
2021.acl-long.126,2020.emnlp-demos.16,0,0.0599366,"Missing"
2021.acl-long.126,D15-1110,0,0.0280648,". Ajjour et al. (2019), whose argument frames dataset we use, instead clustered aspects with Latent Semantic Analysis (LSA) and topic modeling. But, in general, aspects might not be mentioned in a text explicitly. Therefore, we follow these other approaches, treating the task as a clustering problem. Unlike them, however, we do not model only the content and linguistic structure of texts, but we combine them with the debate structure. Different types of argumentation graphs have been proposed, covering expert-stance information (Toledo-Ronen et al., 2016), basic argument and debate structure (Peldszus and Stede, 2015; Gemechu and Reed, 2019), specific effect relations (Al-Khatib et al., 2020; Kobbe et al., 2020), social media graphs (Aldayel and Magdy, 2019), and knowledge graphs (Zhang et al., 2020). Our main focus is not learning to construct ground-truth graphs, but how to use an approximated graph to derive properties such as stance and aspect. Our work resembles approaches that derive the relevance of arguments (Wachsmuth et al., 2017b) or their centrality and divisiveness in a discussion (Lawrence and Reed, 2017) from respective graphs. Sawhney et al. (2020) used a neural graph attention network to"
2021.acl-long.126,N13-1123,0,0.0269051,"erms (using normalized mutual information for edge weights) as well as sentences and documents (using TF-IDF for edge weights) to improve sentence- or documentlevel classification. Our work generalizes this approach, focusing on incorporating many edge types with different meanings, such as relative stance or relative specificity. We compare our approach with a BertGCN baseline, and we ablate all considered edge types, in order to show the importance of capturing these different textual relationships. Ultimately, we seek to facilitate understanding of the main viewpoints in a text collection. Qiu and Jiang (2013) used clustering-based viewpoint discovery to study the impact of the interaction of topics and users in forum discussions. Egan et al. (2016) used multi-document summarization techniques to mine and organize the main points in a debate, and Vilares and He (2017) mined the main topics and their aspects using a Bayesian model. Bar-Haim et al. (2020) introduced the idea of keypoint analysis, grouping arguments found in a collection by the viewpoint they reflect and summarizing each group to a salient keypoint. While our graph-based analysis is likely to be suitable for finding keypoints, we inst"
2021.acl-long.126,W13-4008,0,0.0304941,"unsupervised tasks such as aspect clustering. In both settings, our results show that the syntopical graph approach improves significantly over content-only baselines. The contributions of the work are two-fold: 1. A well-motivated data structure for capturing the latent structure of an argumentative corpus, the syntopical graph. 2. An instantiation of syntopical graphs that yields state-of-the-art results on stance detection and aspect detection. 2 Related Work First attempts at stance detection used contentoriented features (Somasundaran and Wiebe, 2009). Later approaches, such as those by Ranade et al. (2013) and Hasan and Ng (2013), exploited common patterns in dialogic structure to improve stance detection. More tailored to argumentation, BarHaim et al. (2017a) first identified the aspects of a discussed topic in two related claims and the sentiment towards these aspects. From this information, they derived stance based on the contrastiveness of the aspects. Later, Bar-Haim et al. (2017b) mod1584 eled the context of a claim to account for cases without sentiment. Our work follows up on and generalizes this idea, systematically incorporating implicit and explicit structure induced by the topics,"
2021.acl-long.126,D19-1410,0,0.0877671,"hese exploited the full graph structure resulting from all the relations and interactions in a debate, which is the gap we fill in this paper. Sridhar et al. (2015) model collective information about debate posts, authors, and their agreement and disagreement using probabilistic soft logic. Whereas they are restricted to the structure available in a forum, our approach can in principle be applied to arbitrary collections of text. We also tackle aspect detection, which may at first seem more content-oriented in nature. Accordingly, previous research such as the works of Misra et al. (2015) and Reimers et al. (2019b) employed word-based features or contextualized word embeddings for topic-specific aspect clustering. Ajjour et al. (2019), whose argument frames dataset we use, instead clustered aspects with Latent Semantic Analysis (LSA) and topic modeling. But, in general, aspects might not be mentioned in a text explicitly. Therefore, we follow these other approaches, treating the task as a clustering problem. Unlike them, however, we do not model only the content and linguistic structure of texts, but we combine them with the debate structure. Different types of argumentation graphs have been proposed,"
2021.acl-long.126,P19-1054,0,0.105891,"hese exploited the full graph structure resulting from all the relations and interactions in a debate, which is the gap we fill in this paper. Sridhar et al. (2015) model collective information about debate posts, authors, and their agreement and disagreement using probabilistic soft logic. Whereas they are restricted to the structure available in a forum, our approach can in principle be applied to arbitrary collections of text. We also tackle aspect detection, which may at first seem more content-oriented in nature. Accordingly, previous research such as the works of Misra et al. (2015) and Reimers et al. (2019b) employed word-based features or contextualized word embeddings for topic-specific aspect clustering. Ajjour et al. (2019), whose argument frames dataset we use, instead clustered aspects with Latent Semantic Analysis (LSA) and topic modeling. But, in general, aspects might not be mentioned in a text explicitly. Therefore, we follow these other approaches, treating the task as a clustering problem. Unlike them, however, we do not model only the content and linguistic structure of texts, but we combine them with the debate structure. Different types of argumentation graphs have been proposed,"
2021.acl-long.126,2020.coling-main.426,0,0.0250927,", basic argument and debate structure (Peldszus and Stede, 2015; Gemechu and Reed, 2019), specific effect relations (Al-Khatib et al., 2020; Kobbe et al., 2020), social media graphs (Aldayel and Magdy, 2019), and knowledge graphs (Zhang et al., 2020). Our main focus is not learning to construct ground-truth graphs, but how to use an approximated graph to derive properties such as stance and aspect. Our work resembles approaches that derive the relevance of arguments (Wachsmuth et al., 2017b) or their centrality and divisiveness in a discussion (Lawrence and Reed, 2017) from respective graphs. Sawhney et al. (2020) used a neural graph attention network to classify speech stance based on a graph with texts, speakers, and topics as nodes. While we also use a relational graph convolutional network for learning, the graph we propose captures implicit claim relations as well as explicit structure. In addition, text-based graph neural models have been proposed to facilitate classification, such as TextGCN (Yao et al., 2019) as well as the followup work BertGCN (Lin et al., 2021). These approaches build a graph over terms (using normalized mutual information for edge weights) as well as sentences and documents"
2021.acl-long.126,P09-1026,0,0.0669409,"rk architecture for stance detection, and (b) graph algorithms for unsupervised tasks such as aspect clustering. In both settings, our results show that the syntopical graph approach improves significantly over content-only baselines. The contributions of the work are two-fold: 1. A well-motivated data structure for capturing the latent structure of an argumentative corpus, the syntopical graph. 2. An instantiation of syntopical graphs that yields state-of-the-art results on stance detection and aspect detection. 2 Related Work First attempts at stance detection used contentoriented features (Somasundaran and Wiebe, 2009). Later approaches, such as those by Ranade et al. (2013) and Hasan and Ng (2013), exploited common patterns in dialogic structure to improve stance detection. More tailored to argumentation, BarHaim et al. (2017a) first identified the aspects of a discussed topic in two related claims and the sentiment towards these aspects. From this information, they derived stance based on the contrastiveness of the aspects. Later, Bar-Haim et al. (2017b) mod1584 eled the context of a claim to account for cases without sentiment. Our work follows up on and generalizes this idea, systematically incorporatin"
2021.acl-long.126,W16-2814,0,0.0237338,"ntextualized word embeddings for topic-specific aspect clustering. Ajjour et al. (2019), whose argument frames dataset we use, instead clustered aspects with Latent Semantic Analysis (LSA) and topic modeling. But, in general, aspects might not be mentioned in a text explicitly. Therefore, we follow these other approaches, treating the task as a clustering problem. Unlike them, however, we do not model only the content and linguistic structure of texts, but we combine them with the debate structure. Different types of argumentation graphs have been proposed, covering expert-stance information (Toledo-Ronen et al., 2016), basic argument and debate structure (Peldszus and Stede, 2015; Gemechu and Reed, 2019), specific effect relations (Al-Khatib et al., 2020; Kobbe et al., 2020), social media graphs (Aldayel and Magdy, 2019), and knowledge graphs (Zhang et al., 2020). Our main focus is not learning to construct ground-truth graphs, but how to use an approximated graph to derive properties such as stance and aspect. Our work resembles approaches that derive the relevance of arguments (Wachsmuth et al., 2017b) or their centrality and divisiveness in a discussion (Lawrence and Reed, 2017) from respective graphs."
2021.acl-long.126,2020.argmining-1.5,0,0.0354702,"uage inference model that predicts whether the claim entails the topic. We initialize the document representations with a sentence vectorizer over the text of the document. 4 Viewpoint Reconstruction A viewpoint can be understood as a judgment of some aspect of a topic that conveys a stance towards the topic. The goal of viewpoint reconstruction is to identify the set of viewpoints in a collection given a topic, starting with the claims. An example of this process is shown on the right in Figure 1. To denote viewpoints, we borrow notation in line with the idea of aspect-based argument mining (Trautmann, 2020), which in turn was inspired by aspectbased sentiment analysis. In particular, we express a viewpoint as a triple V : V = (topic, aspect, stance) A claim is an expression of a viewpoint in natural language, and a single viewpoint can be expressed in several ways throughout a collection in many claims. Aspects are facets of the broader argument around the topic. While some actual claims may encode multiple viewpoints simultaneously, henceforth we consider each claim to encode one viewpoint for simplicity. To tackle viewpoint reconstruction computationally, we decompose it into two sub-tasks, st"
2021.acl-long.126,D17-1165,0,0.0242068,"h different meanings, such as relative stance or relative specificity. We compare our approach with a BertGCN baseline, and we ablate all considered edge types, in order to show the importance of capturing these different textual relationships. Ultimately, we seek to facilitate understanding of the main viewpoints in a text collection. Qiu and Jiang (2013) used clustering-based viewpoint discovery to study the impact of the interaction of topics and users in forum discussions. Egan et al. (2016) used multi-document summarization techniques to mine and organize the main points in a debate, and Vilares and He (2017) mined the main topics and their aspects using a Bayesian model. Bar-Haim et al. (2020) introduced the idea of keypoint analysis, grouping arguments found in a collection by the viewpoint they reflect and summarizing each group to a salient keypoint. While our graph-based analysis is likely to be suitable for finding keypoints, we instead focus on reconstructing latent viewpoints by grouping claims, leaving open the option to identify the key claims in future work as it would require manual evaluation. 3 Syntopical Graphs We now introduce the concept of a syntopical graph. The goal of our synt"
2021.acl-long.126,W17-5106,1,0.932597,"yntopical reading process computationally in order to help individuals make sense of a collection of documents for a given topic. Viewed through the lens of computational argumentation, these documents state claims or conclusions that can be grouped by the aspects of the topic they discuss as well as by the stance they convey towards the topic (Stede and Schneider, 2018). An individual aiming to form a thorough understanding of the topic needs to get an overview of these viewpoints and their interactions. This may be hard even if adequate tool support for browsing the collection is available (Wachsmuth et al., 2017a; Stab et al., 2018; Chen et al., 2019). We seek to enable systems that are capable of reconstructing viewpoints within a collection, where a viewpoint is expressed as a triple V = (topic, aspect, stance). We consider the argumentative unit of a claim to be the minimal expression of a viewpoint in natural language, such that a single viewpoint can have many claims expressing it. As an example, consider the following two claims: “Nuclear energy emits zero CO2 .” “Nuclear can provide a clean baseload, eliminating the need for fracking and coal mining.” Within a collection these claims express:"
2021.acl-long.126,E17-1105,1,0.929104,"yntopical reading process computationally in order to help individuals make sense of a collection of documents for a given topic. Viewed through the lens of computational argumentation, these documents state claims or conclusions that can be grouped by the aspects of the topic they discuss as well as by the stance they convey towards the topic (Stede and Schneider, 2018). An individual aiming to form a thorough understanding of the topic needs to get an overview of these viewpoints and their interactions. This may be hard even if adequate tool support for browsing the collection is available (Wachsmuth et al., 2017a; Stab et al., 2018; Chen et al., 2019). We seek to enable systems that are capable of reconstructing viewpoints within a collection, where a viewpoint is expressed as a triple V = (topic, aspect, stance). We consider the argumentative unit of a claim to be the minimal expression of a viewpoint in natural language, such that a single viewpoint can have many claims expressing it. As an example, consider the following two claims: “Nuclear energy emits zero CO2 .” “Nuclear can provide a clean baseload, eliminating the need for fracking and coal mining.” Within a collection these claims express:"
2021.acl-long.126,N18-1101,0,0.0323565,"adict and refute another claim, for instance. Edge Weights An edge can have a real-valued weight associated with it on the range (−1, 1), representing the strength of the connection. The relative stance edge between a claim which strongly refutes another would receive a weight close to −1. 3.2 Graph Construction For graph edges, we combine four pretrained models and two similarity measures. The pretrained edge types are: relative stance and relative specificity from Durmus et al. (2019), paraphrase edges from Dolan et al. (2004); Morris et al. (2020), and natural language inference edges from Williams et al. (2018); Liu et al. (2019). The edge weights are the confidence scores defined by weight(u, v, r) = ppos(u,v) − pneg(u,v) , where u and v are claims, r is the relation type, and ppos(u,v) is the probability of a positive association between the claims (e.g., “is a paraphrase” or “does entail”), pneg(u,v) for a negative one. For similarity-based edges, we use standard TF-IDF for term-based similarity and LDA for topic-based similarity (Blei et al., 2003), using cosine similarity as the edge weight. The document-claim edges have a single type, contains, with an edge weight of 1. We compute each of the"
2021.acl-long.126,2020.acl-main.291,0,0.0143002,"in a text explicitly. Therefore, we follow these other approaches, treating the task as a clustering problem. Unlike them, however, we do not model only the content and linguistic structure of texts, but we combine them with the debate structure. Different types of argumentation graphs have been proposed, covering expert-stance information (Toledo-Ronen et al., 2016), basic argument and debate structure (Peldszus and Stede, 2015; Gemechu and Reed, 2019), specific effect relations (Al-Khatib et al., 2020; Kobbe et al., 2020), social media graphs (Aldayel and Magdy, 2019), and knowledge graphs (Zhang et al., 2020). Our main focus is not learning to construct ground-truth graphs, but how to use an approximated graph to derive properties such as stance and aspect. Our work resembles approaches that derive the relevance of arguments (Wachsmuth et al., 2017b) or their centrality and divisiveness in a discussion (Lawrence and Reed, 2017) from respective graphs. Sawhney et al. (2020) used a neural graph attention network to classify speech stance based on a graph with texts, speakers, and topics as nodes. While we also use a relational graph convolutional network for learning, the graph we propose captures i"
2021.acl-long.490,P19-1470,0,0.0657238,"Missing"
2021.acl-long.490,W06-0901,0,0.542316,"hrases that evoke events in text (i.e., event triggers). For instance, in the sentence “The organization donated 2 million dollars to humanitarian helps.”, ED systems should recognize “donated” as an event trigger of type Pay. We differentiate two subtasks in ED, i.e., Event Identification (EI): a binary classification problem to predict if a word in text is an event trigger or not, and Event Classification (EC): a multi-class classification problem to classify event triggers according to predefined event types. Several methods have been introduced for ED, extending from feature-based models (Ahn, 2006; Liao and Grishman, 2010a; Miwa et al., 2014) to advanced deep learning methods (Nguyen and Grishman, 2015; Chen et al., 2015; Nguyen et al., 2016c; Sha et al., 2018; Zhang et al., 2020b; Nguyen et al., 2021). Although deep learning models have achieved substantial improvement, their requirement of large training datasets together with the small sizes of existing ED datasets constitutes a major hurdle to build high-performing ED models. Recently, there have been some efforts to enlarge training data for ED models by exploiting unsupervised (Huang et al., 2016; Yuan et al., 2018) or distantly-"
2021.acl-long.490,C18-1075,0,0.0662701,"d deep learning methods (Nguyen and Grishman, 2015; Chen et al., 2015; Nguyen et al., 2016c; Sha et al., 2018; Zhang et al., 2020b; Nguyen et al., 2021). Although deep learning models have achieved substantial improvement, their requirement of large training datasets together with the small sizes of existing ED datasets constitutes a major hurdle to build high-performing ED models. Recently, there have been some efforts to enlarge training data for ED models by exploiting unsupervised (Huang et al., 2016; Yuan et al., 2018) or distantly-supervised (Keith et al., 2017; Nguyen and Nguyen, 2018; Araki and Mitamura, 2018) techniques. The common strategy in these methods is to exploit unlabeled text data that are rich in event mentions to aid the expansion of training data for ED. In this work, we explore a novel approach for training data expansion in ED by leveraging the existing pre-trained language model GPT-2 (Radford et al., 2019) to automatically generate training data for models. Motivated by the promising performance of GPT models for text generation, we expect our approach to produce effective data for ED in different domains. Specifically, we aim to fine-tune GPT-2 on existing training datasets so it"
2021.acl-long.490,P15-1017,0,0.828899,"lion dollars to humanitarian helps.”, ED systems should recognize “donated” as an event trigger of type Pay. We differentiate two subtasks in ED, i.e., Event Identification (EI): a binary classification problem to predict if a word in text is an event trigger or not, and Event Classification (EC): a multi-class classification problem to classify event triggers according to predefined event types. Several methods have been introduced for ED, extending from feature-based models (Ahn, 2006; Liao and Grishman, 2010a; Miwa et al., 2014) to advanced deep learning methods (Nguyen and Grishman, 2015; Chen et al., 2015; Nguyen et al., 2016c; Sha et al., 2018; Zhang et al., 2020b; Nguyen et al., 2021). Although deep learning models have achieved substantial improvement, their requirement of large training datasets together with the small sizes of existing ED datasets constitutes a major hurdle to build high-performing ED models. Recently, there have been some efforts to enlarge training data for ED models by exploiting unsupervised (Huang et al., 2016; Yuan et al., 2018) or distantly-supervised (Keith et al., 2017; Nguyen and Nguyen, 2018; Araki and Mitamura, 2018) techniques. The common strategy in these me"
2021.acl-long.490,I17-1036,0,0.0277904,"Missing"
2021.acl-long.490,2020.acl-main.718,0,0.0271263,"Missing"
2021.acl-long.490,N18-2058,0,0.0131448,"., 2011; Li et al., 2013; Miwa et al., 2014; Yang and Mitchell, 2016). Later, advanced deep learning methods (Nguyen and Grishman, 2015; Chen et al., 2015; Nguyen et al., 2016a,b; Sha et al., 2018; Zhang et al., 2019; Yang et al., 2019; Nguyen and Nguyen, 2019; Zhang et al., 2020b) have been applied for ED. One challenge for ED research is the limited size of existing datasets that hinder the training of effective models. Prior works have attempted to address this issue via unsupervised (Huang et al., 2016; Yuan et al., 2018), semi-supervised (Liao and Grishman, 2010a; Huang and Riloff, 2012; Ferguson et al., 2018), distantly supervised (Keith et al., 2017; Nguyen and Nguyen, 2018; Zeng et al., 2017; Araki and Mitamura, 2018), and few/zeroshot (Huang et al., 2018; Lai et al., 2020a,b) learn6278 ing. In this work, we propose a novel method to augment training data for ED by exploiting the powerful language model GPT-2 to automatically generate new samples. Leveraging GPT-2 for augmenting training data has also been studied for other NLP tasks recently (e.g., relation extraction, commonsense reasoning) (Papanikolaou and Pierleoni, 2020; Zhang et al., 2020a; Yang et al., 2020; Madaan et al., 2020; Bosselut"
2021.acl-long.490,P11-1113,0,0.12926,"l data O. The results are shown in Table 7. According to this table, the highest performance of the proposed model is achieved when the numbers of the generated and original data are equal. More specifically, decreasing the number of generated samples potentially limits the benefits of data augmentation. On the other hand, increasing the size of generated data might introduces extensive noises and become harmful to the ED models. 4 Related Work Early methods for ED have employed featurebased techniques (Ahn, 2006; Ji and Grishman, 2008; Patwardhan and Riloff, 2009; Liao and Grishman, 2010a,b; Hong et al., 2011; McClosky et al., 2011; Li et al., 2013; Miwa et al., 2014; Yang and Mitchell, 2016). Later, advanced deep learning methods (Nguyen and Grishman, 2015; Chen et al., 2015; Nguyen et al., 2016a,b; Sha et al., 2018; Zhang et al., 2019; Yang et al., 2019; Nguyen and Nguyen, 2019; Zhang et al., 2020b) have been applied for ED. One challenge for ED research is the limited size of existing datasets that hinder the training of effective models. Prior works have attempted to address this issue via unsupervised (Huang et al., 2016; Yuan et al., 2018), semi-supervised (Liao and Grishman, 2010a; Huang an"
2021.acl-long.490,P16-1025,0,0.118895,"r ED, extending from feature-based models (Ahn, 2006; Liao and Grishman, 2010a; Miwa et al., 2014) to advanced deep learning methods (Nguyen and Grishman, 2015; Chen et al., 2015; Nguyen et al., 2016c; Sha et al., 2018; Zhang et al., 2020b; Nguyen et al., 2021). Although deep learning models have achieved substantial improvement, their requirement of large training datasets together with the small sizes of existing ED datasets constitutes a major hurdle to build high-performing ED models. Recently, there have been some efforts to enlarge training data for ED models by exploiting unsupervised (Huang et al., 2016; Yuan et al., 2018) or distantly-supervised (Keith et al., 2017; Nguyen and Nguyen, 2018; Araki and Mitamura, 2018) techniques. The common strategy in these methods is to exploit unlabeled text data that are rich in event mentions to aid the expansion of training data for ED. In this work, we explore a novel approach for training data expansion in ED by leveraging the existing pre-trained language model GPT-2 (Radford et al., 2019) to automatically generate training data for models. Motivated by the promising performance of GPT models for text generation, we expect our approach to produce eff"
2021.acl-long.490,P10-1081,0,0.78252,"evoke events in text (i.e., event triggers). For instance, in the sentence “The organization donated 2 million dollars to humanitarian helps.”, ED systems should recognize “donated” as an event trigger of type Pay. We differentiate two subtasks in ED, i.e., Event Identification (EI): a binary classification problem to predict if a word in text is an event trigger or not, and Event Classification (EC): a multi-class classification problem to classify event triggers according to predefined event types. Several methods have been introduced for ED, extending from feature-based models (Ahn, 2006; Liao and Grishman, 2010a; Miwa et al., 2014) to advanced deep learning methods (Nguyen and Grishman, 2015; Chen et al., 2015; Nguyen et al., 2016c; Sha et al., 2018; Zhang et al., 2020b; Nguyen et al., 2021). Although deep learning models have achieved substantial improvement, their requirement of large training datasets together with the small sizes of existing ED datasets constitutes a major hurdle to build high-performing ED models. Recently, there have been some efforts to enlarge training data for ED models by exploiting unsupervised (Huang et al., 2016; Yuan et al., 2018) or distantly-supervised (Keith et al.,"
2021.acl-long.490,P18-1201,0,0.0737794,"Nguyen et al., 2016a,b; Sha et al., 2018; Zhang et al., 2019; Yang et al., 2019; Nguyen and Nguyen, 2019; Zhang et al., 2020b) have been applied for ED. One challenge for ED research is the limited size of existing datasets that hinder the training of effective models. Prior works have attempted to address this issue via unsupervised (Huang et al., 2016; Yuan et al., 2018), semi-supervised (Liao and Grishman, 2010a; Huang and Riloff, 2012; Ferguson et al., 2018), distantly supervised (Keith et al., 2017; Nguyen and Nguyen, 2018; Zeng et al., 2017; Araki and Mitamura, 2018), and few/zeroshot (Huang et al., 2018; Lai et al., 2020a,b) learn6278 ing. In this work, we propose a novel method to augment training data for ED by exploiting the powerful language model GPT-2 to automatically generate new samples. Leveraging GPT-2 for augmenting training data has also been studied for other NLP tasks recently (e.g., relation extraction, commonsense reasoning) (Papanikolaou and Pierleoni, 2020; Zhang et al., 2020a; Yang et al., 2020; Madaan et al., 2020; Bosselut et al., 2019; Kumar et al., 2020; AnabyTavor et al., 2020; Peng et al., 2020). However, none of those works has explored GPT-2 for ED. In addition, ex"
2021.acl-long.490,E12-1029,0,0.026607,"l., 2011; McClosky et al., 2011; Li et al., 2013; Miwa et al., 2014; Yang and Mitchell, 2016). Later, advanced deep learning methods (Nguyen and Grishman, 2015; Chen et al., 2015; Nguyen et al., 2016a,b; Sha et al., 2018; Zhang et al., 2019; Yang et al., 2019; Nguyen and Nguyen, 2019; Zhang et al., 2020b) have been applied for ED. One challenge for ED research is the limited size of existing datasets that hinder the training of effective models. Prior works have attempted to address this issue via unsupervised (Huang et al., 2016; Yuan et al., 2018), semi-supervised (Liao and Grishman, 2010a; Huang and Riloff, 2012; Ferguson et al., 2018), distantly supervised (Keith et al., 2017; Nguyen and Nguyen, 2018; Zeng et al., 2017; Araki and Mitamura, 2018), and few/zeroshot (Huang et al., 2018; Lai et al., 2020a,b) learn6278 ing. In this work, we propose a novel method to augment training data for ED by exploiting the powerful language model GPT-2 to automatically generate new samples. Leveraging GPT-2 for augmenting training data has also been studied for other NLP tasks recently (e.g., relation extraction, commonsense reasoning) (Papanikolaou and Pierleoni, 2020; Zhang et al., 2020a; Yang et al., 2020; Madaa"
2021.acl-long.490,P08-1030,0,0.141432,"generated samples in G (for the ACE 2005 dataset) are combined with the original data O. The results are shown in Table 7. According to this table, the highest performance of the proposed model is achieved when the numbers of the generated and original data are equal. More specifically, decreasing the number of generated samples potentially limits the benefits of data augmentation. On the other hand, increasing the size of generated data might introduces extensive noises and become harmful to the ED models. 4 Related Work Early methods for ED have employed featurebased techniques (Ahn, 2006; Ji and Grishman, 2008; Patwardhan and Riloff, 2009; Liao and Grishman, 2010a,b; Hong et al., 2011; McClosky et al., 2011; Li et al., 2013; Miwa et al., 2014; Yang and Mitchell, 2016). Later, advanced deep learning methods (Nguyen and Grishman, 2015; Chen et al., 2015; Nguyen et al., 2016a,b; Sha et al., 2018; Zhang et al., 2019; Yang et al., 2019; Nguyen and Nguyen, 2019; Zhang et al., 2020b) have been applied for ED. One challenge for ED research is the limited size of existing datasets that hinder the training of effective models. Prior works have attempted to address this issue via unsupervised (Huang et al., 2"
2021.acl-long.490,D17-1163,0,0.0442459,"Missing"
2021.acl-long.490,P17-1164,0,0.03681,"Missing"
2021.acl-long.490,P19-1429,0,0.326408,"Missing"
2021.acl-long.490,2020.lifelongnlp-1.3,0,0.035258,"Missing"
2021.acl-long.490,2020.nuse-1.5,1,0.953836,"ined loss function is used in our framework: L = Lpred + αLaux + βLKL + γLdist , where α, β, and γ are the trade-off parameters. 3 3.1 Experiments Datasets, Baselines & Hyper-Parameters To evaluate the effectiveness of the proposed model, called the GPT-based data augmentation model for ED with OT (GPTEDOT), we conduct experiments on the following ED datasets: ACE 2005 (Walker et al., 2006): This dataset annotates 599 documents for 33 event types that cover different text domains(e.g., news, weblog or conversation documents). We use the same preprocessing script and data split as prior works (Lai et al., 2020c; Tong et al., 2020b) to achieve fair comparisons. In particular, the data split involves 529/30/40 articles for train/dev/test sets respectively. For this dataset, we compare our model with prior state-of-the-art models reported in the recent works (Lai et al., 2020c; Tong et al., 2020b), including BERT-based models such as DMBERT, AD-DMBERT (Wang et al., 2019), DRMM, EKD (Tong et al., 2020b), and GatedGCN (Lai et al., 2020c). CySecED (Man Duc Trong et al., 2020): This dataset provides 8,014 event triggers for 30 event types from 300 articles of the cybersecurity domain (i.e., cybersecurity"
2021.acl-long.490,2020.emnlp-main.435,1,0.936469,"ined loss function is used in our framework: L = Lpred + αLaux + βLKL + γLdist , where α, β, and γ are the trade-off parameters. 3 3.1 Experiments Datasets, Baselines & Hyper-Parameters To evaluate the effectiveness of the proposed model, called the GPT-based data augmentation model for ED with OT (GPTEDOT), we conduct experiments on the following ED datasets: ACE 2005 (Walker et al., 2006): This dataset annotates 599 documents for 33 event types that cover different text domains(e.g., news, weblog or conversation documents). We use the same preprocessing script and data split as prior works (Lai et al., 2020c; Tong et al., 2020b) to achieve fair comparisons. In particular, the data split involves 529/30/40 articles for train/dev/test sets respectively. For this dataset, we compare our model with prior state-of-the-art models reported in the recent works (Lai et al., 2020c; Tong et al., 2020b), including BERT-based models such as DMBERT, AD-DMBERT (Wang et al., 2019), DRMM, EKD (Tong et al., 2020b), and GatedGCN (Lai et al., 2020c). CySecED (Man Duc Trong et al., 2020): This dataset provides 8,014 event triggers for 30 event types from 300 articles of the cybersecurity domain (i.e., cybersecurity"
2021.acl-long.490,P13-1008,0,0.0737663,". According to this table, the highest performance of the proposed model is achieved when the numbers of the generated and original data are equal. More specifically, decreasing the number of generated samples potentially limits the benefits of data augmentation. On the other hand, increasing the size of generated data might introduces extensive noises and become harmful to the ED models. 4 Related Work Early methods for ED have employed featurebased techniques (Ahn, 2006; Ji and Grishman, 2008; Patwardhan and Riloff, 2009; Liao and Grishman, 2010a,b; Hong et al., 2011; McClosky et al., 2011; Li et al., 2013; Miwa et al., 2014; Yang and Mitchell, 2016). Later, advanced deep learning methods (Nguyen and Grishman, 2015; Chen et al., 2015; Nguyen et al., 2016a,b; Sha et al., 2018; Zhang et al., 2019; Yang et al., 2019; Nguyen and Nguyen, 2019; Zhang et al., 2020b) have been applied for ED. One challenge for ED research is the limited size of existing datasets that hinder the training of effective models. Prior works have attempted to address this issue via unsupervised (Huang et al., 2016; Yuan et al., 2018), semi-supervised (Liao and Grishman, 2010a; Huang and Riloff, 2012; Ferguson et al., 2018),"
2021.acl-long.490,C10-1077,0,0.73535,"evoke events in text (i.e., event triggers). For instance, in the sentence “The organization donated 2 million dollars to humanitarian helps.”, ED systems should recognize “donated” as an event trigger of type Pay. We differentiate two subtasks in ED, i.e., Event Identification (EI): a binary classification problem to predict if a word in text is an event trigger or not, and Event Classification (EC): a multi-class classification problem to classify event triggers according to predefined event types. Several methods have been introduced for ED, extending from feature-based models (Ahn, 2006; Liao and Grishman, 2010a; Miwa et al., 2014) to advanced deep learning methods (Nguyen and Grishman, 2015; Chen et al., 2015; Nguyen et al., 2016c; Sha et al., 2018; Zhang et al., 2020b; Nguyen et al., 2021). Although deep learning models have achieved substantial improvement, their requirement of large training datasets together with the small sizes of existing ED datasets constitutes a major hurdle to build high-performing ED models. Recently, there have been some efforts to enlarge training data for ED models by exploiting unsupervised (Huang et al., 2016; Yuan et al., 2018) or distantly-supervised (Keith et al.,"
2021.acl-long.490,P11-1163,0,0.0457237,"ts are shown in Table 7. According to this table, the highest performance of the proposed model is achieved when the numbers of the generated and original data are equal. More specifically, decreasing the number of generated samples potentially limits the benefits of data augmentation. On the other hand, increasing the size of generated data might introduces extensive noises and become harmful to the ED models. 4 Related Work Early methods for ED have employed featurebased techniques (Ahn, 2006; Ji and Grishman, 2008; Patwardhan and Riloff, 2009; Liao and Grishman, 2010a,b; Hong et al., 2011; McClosky et al., 2011; Li et al., 2013; Miwa et al., 2014; Yang and Mitchell, 2016). Later, advanced deep learning methods (Nguyen and Grishman, 2015; Chen et al., 2015; Nguyen et al., 2016a,b; Sha et al., 2018; Zhang et al., 2019; Yang et al., 2019; Nguyen and Nguyen, 2019; Zhang et al., 2020b) have been applied for ED. One challenge for ED research is the limited size of existing datasets that hinder the training of effective models. Prior works have attempted to address this issue via unsupervised (Huang et al., 2016; Yuan et al., 2018), semi-supervised (Liao and Grishman, 2010a; Huang and Riloff, 2012; Ferguso"
2021.acl-long.490,C14-1214,0,0.824617,"., event triggers). For instance, in the sentence “The organization donated 2 million dollars to humanitarian helps.”, ED systems should recognize “donated” as an event trigger of type Pay. We differentiate two subtasks in ED, i.e., Event Identification (EI): a binary classification problem to predict if a word in text is an event trigger or not, and Event Classification (EC): a multi-class classification problem to classify event triggers according to predefined event types. Several methods have been introduced for ED, extending from feature-based models (Ahn, 2006; Liao and Grishman, 2010a; Miwa et al., 2014) to advanced deep learning methods (Nguyen and Grishman, 2015; Chen et al., 2015; Nguyen et al., 2016c; Sha et al., 2018; Zhang et al., 2020b; Nguyen et al., 2021). Although deep learning models have achieved substantial improvement, their requirement of large training datasets together with the small sizes of existing ED datasets constitutes a major hurdle to build high-performing ED models. Recently, there have been some efforts to enlarge training data for ED models by exploiting unsupervised (Huang et al., 2016; Yuan et al., 2018) or distantly-supervised (Keith et al., 2017; Nguyen and Ngu"
2021.acl-long.490,2021.naacl-main.3,1,0.527941,"Missing"
2021.acl-long.490,C18-1193,1,0.892717,"Missing"
2021.acl-long.490,N16-1034,1,0.958134,"anitarian helps.”, ED systems should recognize “donated” as an event trigger of type Pay. We differentiate two subtasks in ED, i.e., Event Identification (EI): a binary classification problem to predict if a word in text is an event trigger or not, and Event Classification (EC): a multi-class classification problem to classify event triggers according to predefined event types. Several methods have been introduced for ED, extending from feature-based models (Ahn, 2006; Liao and Grishman, 2010a; Miwa et al., 2014) to advanced deep learning methods (Nguyen and Grishman, 2015; Chen et al., 2015; Nguyen et al., 2016c; Sha et al., 2018; Zhang et al., 2020b; Nguyen et al., 2021). Although deep learning models have achieved substantial improvement, their requirement of large training datasets together with the small sizes of existing ED datasets constitutes a major hurdle to build high-performing ED models. Recently, there have been some efforts to enlarge training data for ED models by exploiting unsupervised (Huang et al., 2016; Yuan et al., 2018) or distantly-supervised (Keith et al., 2017; Nguyen and Nguyen, 2018; Araki and Mitamura, 2018) techniques. The common strategy in these methods is to exploit u"
2021.acl-long.490,W16-1618,1,0.957031,"anitarian helps.”, ED systems should recognize “donated” as an event trigger of type Pay. We differentiate two subtasks in ED, i.e., Event Identification (EI): a binary classification problem to predict if a word in text is an event trigger or not, and Event Classification (EC): a multi-class classification problem to classify event triggers according to predefined event types. Several methods have been introduced for ED, extending from feature-based models (Ahn, 2006; Liao and Grishman, 2010a; Miwa et al., 2014) to advanced deep learning methods (Nguyen and Grishman, 2015; Chen et al., 2015; Nguyen et al., 2016c; Sha et al., 2018; Zhang et al., 2020b; Nguyen et al., 2021). Although deep learning models have achieved substantial improvement, their requirement of large training datasets together with the small sizes of existing ED datasets constitutes a major hurdle to build high-performing ED models. Recently, there have been some efforts to enlarge training data for ED models by exploiting unsupervised (Huang et al., 2016; Yuan et al., 2018) or distantly-supervised (Keith et al., 2017; Nguyen and Nguyen, 2018; Araki and Mitamura, 2018) techniques. The common strategy in these methods is to exploit u"
2021.acl-long.490,P15-2060,1,0.915263,"organization donated 2 million dollars to humanitarian helps.”, ED systems should recognize “donated” as an event trigger of type Pay. We differentiate two subtasks in ED, i.e., Event Identification (EI): a binary classification problem to predict if a word in text is an event trigger or not, and Event Classification (EC): a multi-class classification problem to classify event triggers according to predefined event types. Several methods have been introduced for ED, extending from feature-based models (Ahn, 2006; Liao and Grishman, 2010a; Miwa et al., 2014) to advanced deep learning methods (Nguyen and Grishman, 2015; Chen et al., 2015; Nguyen et al., 2016c; Sha et al., 2018; Zhang et al., 2020b; Nguyen et al., 2021). Although deep learning models have achieved substantial improvement, their requirement of large training datasets together with the small sizes of existing ED datasets constitutes a major hurdle to build high-performing ED models. Recently, there have been some efforts to enlarge training data for ED models by exploiting unsupervised (Huang et al., 2016; Yuan et al., 2018) or distantly-supervised (Keith et al., 2017; Nguyen and Nguyen, 2018; Araki and Mitamura, 2018) techniques. The common s"
2021.acl-long.490,D09-1016,0,0.196452,"(for the ACE 2005 dataset) are combined with the original data O. The results are shown in Table 7. According to this table, the highest performance of the proposed model is achieved when the numbers of the generated and original data are equal. More specifically, decreasing the number of generated samples potentially limits the benefits of data augmentation. On the other hand, increasing the size of generated data might introduces extensive noises and become harmful to the ED models. 4 Related Work Early methods for ED have employed featurebased techniques (Ahn, 2006; Ji and Grishman, 2008; Patwardhan and Riloff, 2009; Liao and Grishman, 2010a,b; Hong et al., 2011; McClosky et al., 2011; Li et al., 2013; Miwa et al., 2014; Yang and Mitchell, 2016). Later, advanced deep learning methods (Nguyen and Grishman, 2015; Chen et al., 2015; Nguyen et al., 2016a,b; Sha et al., 2018; Zhang et al., 2019; Yang et al., 2019; Nguyen and Nguyen, 2019; Zhang et al., 2020b) have been applied for ED. One challenge for ED research is the limited size of existing datasets that hinder the training of effective models. Prior works have attempted to address this issue via unsupervised (Huang et al., 2016; Yuan et al., 2018), semi"
2021.acl-long.490,2020.acl-main.522,0,0.817703,"is used in our framework: L = Lpred + αLaux + βLKL + γLdist , where α, β, and γ are the trade-off parameters. 3 3.1 Experiments Datasets, Baselines & Hyper-Parameters To evaluate the effectiveness of the proposed model, called the GPT-based data augmentation model for ED with OT (GPTEDOT), we conduct experiments on the following ED datasets: ACE 2005 (Walker et al., 2006): This dataset annotates 599 documents for 33 event types that cover different text domains(e.g., news, weblog or conversation documents). We use the same preprocessing script and data split as prior works (Lai et al., 2020c; Tong et al., 2020b) to achieve fair comparisons. In particular, the data split involves 529/30/40 articles for train/dev/test sets respectively. For this dataset, we compare our model with prior state-of-the-art models reported in the recent works (Lai et al., 2020c; Tong et al., 2020b), including BERT-based models such as DMBERT, AD-DMBERT (Wang et al., 2019), DRMM, EKD (Tong et al., 2020b), and GatedGCN (Lai et al., 2020c). CySecED (Man Duc Trong et al., 2020): This dataset provides 8,014 event triggers for 30 event types from 300 articles of the cybersecurity domain (i.e., cybersecurity events). We follow t"
2021.acl-long.490,N19-1105,0,0.534464,"ACE 2005 (Walker et al., 2006): This dataset annotates 599 documents for 33 event types that cover different text domains(e.g., news, weblog or conversation documents). We use the same preprocessing script and data split as prior works (Lai et al., 2020c; Tong et al., 2020b) to achieve fair comparisons. In particular, the data split involves 529/30/40 articles for train/dev/test sets respectively. For this dataset, we compare our model with prior state-of-the-art models reported in the recent works (Lai et al., 2020c; Tong et al., 2020b), including BERT-based models such as DMBERT, AD-DMBERT (Wang et al., 2019), DRMM, EKD (Tong et al., 2020b), and GatedGCN (Lai et al., 2020c). CySecED (Man Duc Trong et al., 2020): This dataset provides 8,014 event triggers for 30 event types from 300 articles of the cybersecurity domain (i.e., cybersecurity events). We follow the the same pre-processing and data split as the original work (Man Duc Trong et al., 2020) with 240/30/30 documents for the train/dev/test sets. To be consistent with other experiments and facilitate the data generation based on GPT-2, the experiments on Cy6275 SecED are conducted at the sentence level where inputs for models involve sentence"
2021.acl-long.490,D19-1582,0,0.0306493,"Missing"
2021.acl-long.490,N16-1033,0,0.054236,"st performance of the proposed model is achieved when the numbers of the generated and original data are equal. More specifically, decreasing the number of generated samples potentially limits the benefits of data augmentation. On the other hand, increasing the size of generated data might introduces extensive noises and become harmful to the ED models. 4 Related Work Early methods for ED have employed featurebased techniques (Ahn, 2006; Ji and Grishman, 2008; Patwardhan and Riloff, 2009; Liao and Grishman, 2010a,b; Hong et al., 2011; McClosky et al., 2011; Li et al., 2013; Miwa et al., 2014; Yang and Mitchell, 2016). Later, advanced deep learning methods (Nguyen and Grishman, 2015; Chen et al., 2015; Nguyen et al., 2016a,b; Sha et al., 2018; Zhang et al., 2019; Yang et al., 2019; Nguyen and Nguyen, 2019; Zhang et al., 2020b) have been applied for ED. One challenge for ED research is the limited size of existing datasets that hinder the training of effective models. Prior works have attempted to address this issue via unsupervised (Huang et al., 2016; Yuan et al., 2018), semi-supervised (Liao and Grishman, 2010a; Huang and Riloff, 2012; Ferguson et al., 2018), distantly supervised (Keith et al., 2017; Ngu"
2021.acl-long.490,P19-1522,0,0.4632,"or 30 event types from 300 articles of the cybersecurity domain (i.e., cybersecurity events). We follow the the same pre-processing and data split as the original work (Man Duc Trong et al., 2020) with 240/30/30 documents for the train/dev/test sets. To be consistent with other experiments and facilitate the data generation based on GPT-2, the experiments on Cy6275 SecED are conducted at the sentence level where inputs for models involve sentences. As such, we employ the state-of-the-art sentence-level models reported in (Man Duc Trong et al., 2020), i.e., DMBERT (Wang et al., 2019), BERT-ED (Yang et al., 2019), as the baselines for CySecED. RAMS (Ebner et al., 2020): This dataset annotates 9,124 event triggers for 38 event types. We use the official data split with 3,194, 399, and 400 documents for training, development, and testing respectively for RAMS. We also perform ED at the sentence level in this dataset. For the baselines, we utilize recent state-of-the-art BERT-based models for ED, i.e., DMBERT (Wang et al., 2019) and GatedGCN (Lai et al., 2020c). For a fair comparison, the performance of such baseline models is obtained via their official implementations from the original papers that are"
2021.acl-long.490,2020.findings-emnlp.90,0,0.0444946,"Missing"
2021.acl-short.29,2020.eval4nlp-1.4,1,0.891318,"ataset and pre-trained models to compute the UMIC1 . 1 Human Judgments : 1.875 out of 5 Figure 1: An example where the metric score for a given candidate caption varies significantly depending on the reference type. Introduction Image captioning is a task that aims to generate a description that explains the given image in a natural language. While there have been many advances for caption generation algorithms (Vinyals et al., 2015; Anderson et al., 2018) and target datasets (Fang et al., 2015; Sharma et al., 2018), few studies (Vedantam et al., 2015; Anderson et al., 2016; Cui et al., 2018; Lee et al., 2020) have focused on assessing the quality of the generated captions. Especially, most of the evaluation metrics only use reference captions to evaluate the caption although the main context is an image. However, as shown in Figure 1, since there are many possible reference captions for a single image, a candidate caption can receive completely different scores depending on the type of reference (Yi 1 https://github.com/hwanheelee1993/UMIC et al., 2020). Because of this diverse nature of image captions, reference-based metrics usually use multiple references which are difficult to obtain. To overc"
2021.acl-short.29,2021.naacl-main.253,0,0.0328394,"valuate our proposed metric on four benchmark datasets, including our new dataset. Experimental results show that our proposed unreferenced metric is highly correlated with human judgments than all of the previous metrics that use reference captions. 2 UNITER UNITER Quality Estimation Quality Estimation (QE) is a task that estimates the quality of the generated text without using the human references and this task is same as developing an unreferenced metric. QE is widely established in machine translation (MT) tasks (Specia et al., 2013; Martins et al., 2017; Specia et al., 2018). Recently, (Levinboim et al., 2021) introduces a large scale human ratings on image-caption pairs for training QE models in image captioning tasks. Our work also trains caption QE model, (i.e. unreferenced captioning metric) but we do not use human ratings to train the metric. Instead, we create diverse synthetic negative samples and train the metric with these samples via ranking loss. ????� Ranking Loss ???? ????� A person on bike going through green light with red truck nearby in a sunny day. Figure 2: Overall training procedure of UMIC. Given an image I, a positive caption x and a negative caption x ˆ, we compute the score"
2021.acl-short.29,W05-0909,0,0.436479,"?� Ranking Loss ???? ????� A person on bike going through green light with red truck nearby in a sunny day. Figure 2: Overall training procedure of UMIC. Given an image I, a positive caption x and a negative caption x ˆ, we compute the score of each image-caption pair Sx and Sxˆ using UNITER respectively. Then, we fine-tune UNITER using raking loss that Sx is higher than Sxˆ . Related Work Image Captioning Metrics Following other text generation tasks such as dialogue systems and machine translation, n-gram similarity metrics such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004) and METEOR (Banerjee and Lavie, 2005) are widely used to evaluate an image caption. Especially, CIDEr (Vedantam et al., 2015), which weights each n-gram using TF-IDF, is widely used. SPICE (Anderson et al., 2016) is a captioning metric based on scene graph. BERTScore (Zhang et al., 2019), which computes the similarity of the contextualized embeddings, are also used. BERT-TBR (Yi et al., 2020) focuses on the variance in multiple hypothesis and ViLBERTScore (VBTScore) (Lee et al., 2020) utilizes ViLBERT (Lu et al., 2019) to improve BERTScore. Different from these metrics, VIFIDEL (Madhyastha et al., 2019) computes the word mover di"
2021.acl-short.29,P19-1654,0,0.0163619,"E (Lin, 2004) and METEOR (Banerjee and Lavie, 2005) are widely used to evaluate an image caption. Especially, CIDEr (Vedantam et al., 2015), which weights each n-gram using TF-IDF, is widely used. SPICE (Anderson et al., 2016) is a captioning metric based on scene graph. BERTScore (Zhang et al., 2019), which computes the similarity of the contextualized embeddings, are also used. BERT-TBR (Yi et al., 2020) focuses on the variance in multiple hypothesis and ViLBERTScore (VBTScore) (Lee et al., 2020) utilizes ViLBERT (Lu et al., 2019) to improve BERTScore. Different from these metrics, VIFIDEL (Madhyastha et al., 2019) computes the word mover distance (Kusner et al., 2015) between the object labels in the image and the candidate captions, and it does not require reference captions. Similar to VIFIDEL, our proposed UMIC does not utilize the reference captions. However, UMIC directly uses image features and evaluates a caption in various perspectives compared to VIFIDEL. ???? 3 UMIC We propose UMIC, an unreferenced metric for image captioning using UNITER. We construct negative captions using the reference captions through the pre-defined rules. Then, we fine-tune UNITER to distinguish the reference captions"
2021.acl-short.29,Q17-1015,0,0.0243491,"udgments for the model-generated caption. Finally, we evaluate our proposed metric on four benchmark datasets, including our new dataset. Experimental results show that our proposed unreferenced metric is highly correlated with human judgments than all of the previous metrics that use reference captions. 2 UNITER UNITER Quality Estimation Quality Estimation (QE) is a task that estimates the quality of the generated text without using the human references and this task is same as developing an unreferenced metric. QE is widely established in machine translation (MT) tasks (Specia et al., 2013; Martins et al., 2017; Specia et al., 2018). Recently, (Levinboim et al., 2021) introduces a large scale human ratings on image-caption pairs for training QE models in image captioning tasks. Our work also trains caption QE model, (i.e. unreferenced captioning metric) but we do not use human ratings to train the metric. Instead, we create diverse synthetic negative samples and train the metric with these samples via ranking loss. ????� Ranking Loss ???? ????� A person on bike going through green light with red truck nearby in a sunny day. Figure 2: Overall training procedure of UMIC. Given an image I, a positive c"
2021.acl-short.29,P02-1040,0,0.113307,"in the metric with these samples via ranking loss. ????� Ranking Loss ???? ????� A person on bike going through green light with red truck nearby in a sunny day. Figure 2: Overall training procedure of UMIC. Given an image I, a positive caption x and a negative caption x ˆ, we compute the score of each image-caption pair Sx and Sxˆ using UNITER respectively. Then, we fine-tune UNITER using raking loss that Sx is higher than Sxˆ . Related Work Image Captioning Metrics Following other text generation tasks such as dialogue systems and machine translation, n-gram similarity metrics such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004) and METEOR (Banerjee and Lavie, 2005) are widely used to evaluate an image caption. Especially, CIDEr (Vedantam et al., 2015), which weights each n-gram using TF-IDF, is widely used. SPICE (Anderson et al., 2016) is a captioning metric based on scene graph. BERTScore (Zhang et al., 2019), which computes the similarity of the contextualized embeddings, are also used. BERT-TBR (Yi et al., 2020) focuses on the variance in multiple hypothesis and ViLBERTScore (VBTScore) (Lee et al., 2020) utilizes ViLBERT (Lu et al., 2019) to improve BERTScore. Different from these metrics, VIF"
2021.acl-short.29,P18-1238,0,0.0645235,"Missing"
2021.acl-short.29,W18-6451,0,0.0639535,"Missing"
2021.acl-short.29,P13-4014,0,0.0638536,"Missing"
2021.acl-short.29,2020.acl-main.93,0,0.0325461,"r than Sxˆ . Related Work Image Captioning Metrics Following other text generation tasks such as dialogue systems and machine translation, n-gram similarity metrics such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004) and METEOR (Banerjee and Lavie, 2005) are widely used to evaluate an image caption. Especially, CIDEr (Vedantam et al., 2015), which weights each n-gram using TF-IDF, is widely used. SPICE (Anderson et al., 2016) is a captioning metric based on scene graph. BERTScore (Zhang et al., 2019), which computes the similarity of the contextualized embeddings, are also used. BERT-TBR (Yi et al., 2020) focuses on the variance in multiple hypothesis and ViLBERTScore (VBTScore) (Lee et al., 2020) utilizes ViLBERT (Lu et al., 2019) to improve BERTScore. Different from these metrics, VIFIDEL (Madhyastha et al., 2019) computes the word mover distance (Kusner et al., 2015) between the object labels in the image and the candidate captions, and it does not require reference captions. Similar to VIFIDEL, our proposed UMIC does not utilize the reference captions. However, UMIC directly uses image features and evaluates a caption in various perspectives compared to VIFIDEL. ???? 3 UMIC We propose UMIC"
2021.acl-short.67,2020.coling-main.453,0,0.0309839,"88) to leverage long-range inter-dependencies through a discourse tree. The rhetorical discourse tree of a document contains nodes of phrases, where each phrase (a.k.a, Elementary Discourse Unit or EDU) is contiguous, adjacent and nonoverlapping. The interdependencies among EDUs are represented by conventional rhetorical relations (Mann, 1987), e.g. Elaboration, Span, Condition, Attribution. Prior work showed discourse features in the form of RST connections help leverage longrange document-level interactions between phrase units (Bhatia et al., 2015) and identify backgroundforeground events (Aldawsari et al., 2020). Elementary Discourse Unit (EDU), a subsentence phrase unit, is the minimal selection unit for discourse segmentation of a document. We generate the document vector representations at EDU-level hi ∈ H = {h1 , · · · , hd } via the SelfAttentive Span Extractor (SpanExt) from Lee et al. (2017) over the BERT token embeddings. We use the converted dependency version of the tree to build the Rhetorical-aware graph (GDG ) by treating every discourse dependency from the i-th EDU to the j-th EDU as a directed edge weighted by the type of the rhetorical relation. Train 4000 32609 231 4032 Model Vashish"
2021.acl-short.67,2020.emnlp-main.436,0,0.259493,"me-aware summarization (Noh et al., 2020), temporal question-answering (Ning et al., 2020), and temporal information extraction (Leeuwenberg and Moens, 2019). Prior work focuses on extracting temporal relations between event pairs (a.k.a., TLINKS) present in the same sentence (Intra-sentence TLINKS) or adjacent sentences (Inter-sentence TLINKS), mostly ignoring document-level pairs (Crossdocument TLINKS) (Reimers et al., 2016). Past works have used RNN (Cheng and Miyao, 2017; Meng et al., 2017; Goyal and Durrett, 2019; Ning et al., 2019; Han et al., 2019a,c,b, 2020b) and Transformer networks (Ballesteros et al., 2020; Zhao et al., 2020b) for encoding a few sentences or a short paragraph but do not capture longrange dependencies and multi-hop reasoning at the document-level. This shortcoming is shown in the TDDiscourse dataset (Naik et al., 2019), which was Dinesh Manocha dmanocha@umd.edu designed to highlight global discourse-level challenges, e.g., multi-hop chain reasoning, future or hypothetical events, and reasoning requiring world knowledge. We propose TIMERS - a TIME, Rhetorical, and Syntactic-aware model for document-level temporal relation extraction. TIMERS uses discourse features in the form of"
2021.acl-short.67,W19-5929,0,0.218342,"TLINKS) present in the same sentence (Intra-sentence TLINKS) or adjacent sentences (Inter-sentence TLINKS), mostly ignoring document-level pairs (Crossdocument TLINKS) (Reimers et al., 2016). Past works have used RNN (Cheng and Miyao, 2017; Meng et al., 2017; Goyal and Durrett, 2019; Ning et al., 2019; Han et al., 2019a,c,b, 2020b) and Transformer networks (Ballesteros et al., 2020; Zhao et al., 2020b) for encoding a few sentences or a short paragraph but do not capture longrange dependencies and multi-hop reasoning at the document-level. This shortcoming is shown in the TDDiscourse dataset (Naik et al., 2019), which was Dinesh Manocha dmanocha@umd.edu designed to highlight global discourse-level challenges, e.g., multi-hop chain reasoning, future or hypothetical events, and reasoning requiring world knowledge. We propose TIMERS - a TIME, Rhetorical, and Syntactic-aware model for document-level temporal relation extraction. TIMERS uses discourse features in the form of connections from Rhetorical Structure Theory (RST) parsers (Bhatia et al., 2015) to leverage long-range inter-sentential relationships. It also extends existing contextual embeddings with structural and syntactic dependency parse con"
2021.acl-short.67,W15-1843,0,0.070451,"Missing"
2021.acl-short.67,D17-1108,0,0.0655338,"Missing"
2021.acl-short.67,D19-1642,0,0.0459017,"Missing"
2021.acl-short.67,P16-1207,0,0.0263638,"a text (Pustejovsky et al., 2003). Understanding the temporal ordering of events in a document plays a key role in downstream tasks such as timeline creation (Leeuwenberg and Moens, 2018), time-aware summarization (Noh et al., 2020), temporal question-answering (Ning et al., 2020), and temporal information extraction (Leeuwenberg and Moens, 2019). Prior work focuses on extracting temporal relations between event pairs (a.k.a., TLINKS) present in the same sentence (Intra-sentence TLINKS) or adjacent sentences (Inter-sentence TLINKS), mostly ignoring document-level pairs (Crossdocument TLINKS) (Reimers et al., 2016). Past works have used RNN (Cheng and Miyao, 2017; Meng et al., 2017; Goyal and Durrett, 2019; Ning et al., 2019; Han et al., 2019a,c,b, 2020b) and Transformer networks (Ballesteros et al., 2020; Zhao et al., 2020b) for encoding a few sentences or a short paragraph but do not capture longrange dependencies and multi-hop reasoning at the document-level. This shortcoming is shown in the TDDiscourse dataset (Naik et al., 2019), which was Dinesh Manocha dmanocha@umd.edu designed to highlight global discourse-level challenges, e.g., multi-hop chain reasoning, future or hypothetical events, and reas"
2021.acl-short.67,P17-2035,0,0.0678477,"Missing"
2021.acl-short.67,P19-1280,0,0.0511024,"Missing"
2021.acl-short.67,2020.emnlp-main.51,0,0.0740861,"Missing"
2021.acl-short.67,2020.coling-main.143,0,0.0751586,"Missing"
2021.acl-short.67,2021.adaptnlp-1.20,0,0.0680033,"Missing"
2021.adaptnlp-1.18,K16-1017,0,0.017608,"representations from online content can predict user profile (Volkova et al., 2015; Wang et al., 2018; Farnadi et al., 2018; Lynn et al., 2020) and behaviors (Zhang et al., 2015; Amir et al., 2017; Benton et al., 2017; Ding et al., 2017). User embeddings can personalize classification models, and further improve model performance (Tang et al., 2015; Chen et al., 2016a; Yang and Eisenstein, 2017; Wu et al., 2018; Zeng et al., 2019; Huang et al., 2019). The representations of user language can help models better understand documents as global contexts. However, existing user embedding methods (Amir et al., 2016; Benton et al., 2016; Xing and Paul, 2017; Pan and Ding, 2019) mainly focus on extracting features from language itself while ignoring user interests. Recent research has demonstrated that adapting the user factors can further improve user geolocation prediction (Miura et al., 2017), demographic attribute prediction (Farnadi et al., 2018), and sentiment analysis (Yang and Eisenstein, 2017). Lynn et al. (2017); Huang and Paul (2019) treated the language variations as a domain adaptation problem and referred to this idea as user factor adaptation. In this study, we treat the user interest as do"
2021.adaptnlp-1.18,P16-2003,1,0.855504,"om online content can predict user profile (Volkova et al., 2015; Wang et al., 2018; Farnadi et al., 2018; Lynn et al., 2020) and behaviors (Zhang et al., 2015; Amir et al., 2017; Benton et al., 2017; Ding et al., 2017). User embeddings can personalize classification models, and further improve model performance (Tang et al., 2015; Chen et al., 2016a; Yang and Eisenstein, 2017; Wu et al., 2018; Zeng et al., 2019; Huang et al., 2019). The representations of user language can help models better understand documents as global contexts. However, existing user embedding methods (Amir et al., 2016; Benton et al., 2016; Xing and Paul, 2017; Pan and Ding, 2019) mainly focus on extracting features from language itself while ignoring user interests. Recent research has demonstrated that adapting the user factors can further improve user geolocation prediction (Miura et al., 2017), demographic attribute prediction (Farnadi et al., 2018), and sentiment analysis (Yang and Eisenstein, 2017). Lynn et al. (2017); Huang and Paul (2019) treated the language variations as a domain adaptation problem and referred to this idea as user factor adaptation. In this study, we treat the user interest as domains (e.g., restaura"
2021.adaptnlp-1.18,E17-1015,0,0.123495,"ectronic domain or praise medicine effectiveness of the medical products; users can also use the words “cool” to describe a property of AC products or express sentiments. User embedding, which is to learn a fixed-length representation based on multiple user reviews of each user, can infer the user latent information into a unified vector space (Benton, 2018; Pan and Ding, 2019). The inferred latent representations from online content can predict user profile (Volkova et al., 2015; Wang et al., 2018; Farnadi et al., 2018; Lynn et al., 2020) and behaviors (Zhang et al., 2015; Amir et al., 2017; Benton et al., 2017; Ding et al., 2017). User embeddings can personalize classification models, and further improve model performance (Tang et al., 2015; Chen et al., 2016a; Yang and Eisenstein, 2017; Wu et al., 2018; Zeng et al., 2019; Huang et al., 2019). The representations of user language can help models better understand documents as global contexts. However, existing user embedding methods (Amir et al., 2016; Benton et al., 2016; Xing and Paul, 2017; Pan and Ding, 2019) mainly focus on extracting features from language itself while ignoring user interests. Recent research has demonstrated that adapting th"
2021.adaptnlp-1.18,P04-3031,0,0.270174,"sources (He and McAuley, 2016; Yelp, 2018; IMDb, 2020). For the IMDb dataset, we included English movies produced in the US from 1960 to 2019. Each review associates with its author and the rated item, which refers to a movie in the IMDb data, a business unit in the Yelp data and a product in the Amazon data. To keep consistency in each dataset, we retain top 4 frequent genres of rated items and the review documents with no less than 10 tokens.1 We dropped non-English review documents by the language detector (Lui and Baldwin, 2012), lowercased all tokens and tokenized the corpora using NLTK (Bird and Loper, 2004). The review datasets have different score scales. We 1 The top 4 rated categories of Amazon-Health, IMDb and Yelp are [sports nutrition, sexual wellness, shaving & hair removal, vitamins & dietary supplements], [comedy, thriller, drama, action] and [restaurants, health & medical, home services, beauty & spas] respectively. normalize the scales and encode each review score into three discrete categories: positive (&gt; 3 for the Yelp and Amazon, &gt; 6 for the IMDb), negative (&lt; 3 for the Yelp and Amazon, &lt; 5 for the IMDb) and neutral. Table 1 shows a summary of the datasets. 2.1 Privacy Considerati"
2021.adaptnlp-1.18,D16-1171,0,0.109652,"ress sentiments. User embedding, which is to learn a fixed-length representation based on multiple user reviews of each user, can infer the user latent information into a unified vector space (Benton, 2018; Pan and Ding, 2019). The inferred latent representations from online content can predict user profile (Volkova et al., 2015; Wang et al., 2018; Farnadi et al., 2018; Lynn et al., 2020) and behaviors (Zhang et al., 2015; Amir et al., 2017; Benton et al., 2017; Ding et al., 2017). User embeddings can personalize classification models, and further improve model performance (Tang et al., 2015; Chen et al., 2016a; Yang and Eisenstein, 2017; Wu et al., 2018; Zeng et al., 2019; Huang et al., 2019). The representations of user language can help models better understand documents as global contexts. However, existing user embedding methods (Amir et al., 2016; Benton et al., 2016; Xing and Paul, 2017; Pan and Ding, 2019) mainly focus on extracting features from language itself while ignoring user interests. Recent research has demonstrated that adapting the user factors can further improve user geolocation prediction (Miura et al., 2017), demographic attribute prediction (Farnadi et al., 2018), and sentim"
2021.adaptnlp-1.18,W14-4012,0,0.0651627,"Missing"
2021.adaptnlp-1.18,D17-1241,0,0.0587459,"Missing"
2021.adaptnlp-1.18,2020.acl-main.700,0,0.0209386,"et al., 2020; Lynn et al., 2020). The demographic user factors influence how online users express their opinions (Volkova et al., 2013; Hovy, 2015; WoodDoughty et al., 2017) and show promising improvements in the text classification task (Lynn et al., 2017; Huang and Paul, 2019; Lynn et al., 2019). However, in this work, the goal of modeling user factor is to train robust user embeddings via domain adaptation, rather than the end goal being demographic factor prediction and document classification itself. Personalized classification generally improves the performance of document classifiers (Flek, 2020). The multitask learning framework has been applied for personalizing document classifiers by optimizing the classifiers on multiple document levels (Benton et al., 2017) or general and individual levels (Wu and Huang, 2016). The social relation can bridge connections between users and generalize classification models across users (Wu and Huang, 2016; Yang and Eisenstein, 2017). For example, (Wu and Huang, 2016) optimizes document Amazon-Health Precision Recall F1 .834 .768 .793 .841 .777 .801 .838 .771 .796 .813 .844 .812 .836 .811 .821 .821 .832 .825 .866 .822 .840 .863 .812 .831 .873 .838 ."
2021.adaptnlp-1.18,S19-1015,1,0.873981,"19; Huang et al., 2019). The representations of user language can help models better understand documents as global contexts. However, existing user embedding methods (Amir et al., 2016; Benton et al., 2016; Xing and Paul, 2017; Pan and Ding, 2019) mainly focus on extracting features from language itself while ignoring user interests. Recent research has demonstrated that adapting the user factors can further improve user geolocation prediction (Miura et al., 2017), demographic attribute prediction (Farnadi et al., 2018), and sentiment analysis (Yang and Eisenstein, 2017). Lynn et al. (2017); Huang and Paul (2019) treated the language variations as a domain adaptation problem and referred to this idea as user factor adaptation. In this study, we treat the user interest as domains (e.g., restaurants vs. home services domains) and propose a multitask framework to model language variations and incorporate the user factor into user embeddings. We focus on three online 172 Proceedings of the Second Workshop on Domain Adaptation for NLP, pages 172–182 April 20, 2021. ©2021 Association for Computational Linguistics review datasets from Amazon, IMDb, and Yelp containing diverse behaviors conditioned on user in"
2021.adaptnlp-1.18,C18-1079,0,0.0517273,"Missing"
2021.adaptnlp-1.18,P12-3005,0,0.0502504,"glish reviews of Amazon (health product), IMDb and Yelp from the publicly available sources (He and McAuley, 2016; Yelp, 2018; IMDb, 2020). For the IMDb dataset, we included English movies produced in the US from 1960 to 2019. Each review associates with its author and the rated item, which refers to a movie in the IMDb data, a business unit in the Yelp data and a product in the Amazon data. To keep consistency in each dataset, we retain top 4 frequent genres of rated items and the review documents with no less than 10 tokens.1 We dropped non-English review documents by the language detector (Lui and Baldwin, 2012), lowercased all tokens and tokenized the corpora using NLTK (Bird and Loper, 2004). The review datasets have different score scales. We 1 The top 4 rated categories of Amazon-Health, IMDb and Yelp are [sports nutrition, sexual wellness, shaving & hair removal, vitamins & dietary supplements], [comedy, thriller, drama, action] and [restaurants, health & medical, home services, beauty & spas] respectively. normalize the scales and encode each review score into three discrete categories: positive (&gt; 3 for the Yelp and Amazon, &gt; 6 for the IMDb), negative (&lt; 3 for the Yelp and Amazon, &lt; 5 for the"
2021.adaptnlp-1.18,2020.acl-main.472,0,0.217767,"line users can use the word “fast” to criticize battery quality of the electronic domain or praise medicine effectiveness of the medical products; users can also use the words “cool” to describe a property of AC products or express sentiments. User embedding, which is to learn a fixed-length representation based on multiple user reviews of each user, can infer the user latent information into a unified vector space (Benton, 2018; Pan and Ding, 2019). The inferred latent representations from online content can predict user profile (Volkova et al., 2015; Wang et al., 2018; Farnadi et al., 2018; Lynn et al., 2020) and behaviors (Zhang et al., 2015; Amir et al., 2017; Benton et al., 2017; Ding et al., 2017). User embeddings can personalize classification models, and further improve model performance (Tang et al., 2015; Chen et al., 2016a; Yang and Eisenstein, 2017; Wu et al., 2018; Zeng et al., 2019; Huang et al., 2019). The representations of user language can help models better understand documents as global contexts. However, existing user embedding methods (Amir et al., 2016; Benton et al., 2016; Xing and Paul, 2017; Pan and Ding, 2019) mainly focus on extracting features from language itself while"
2021.adaptnlp-1.18,W19-2103,0,0.0193578,"processing. Online generated user texts show demographic variations in the linguistic styles, and the linguistic style variability could be used for predicting user’s personality and demographic attributes (Rosenthal and McKeown, 2011; Zhang et al., 2016; Hovy and Fornaciari, 2018; WoodDoughty et al., 2020; Gjurkovi´c et al., 2020; Lynn et al., 2020). The demographic user factors influence how online users express their opinions (Volkova et al., 2013; Hovy, 2015; WoodDoughty et al., 2017) and show promising improvements in the text classification task (Lynn et al., 2017; Huang and Paul, 2019; Lynn et al., 2019). However, in this work, the goal of modeling user factor is to train robust user embeddings via domain adaptation, rather than the end goal being demographic factor prediction and document classification itself. Personalized classification generally improves the performance of document classifiers (Flek, 2020). The multitask learning framework has been applied for personalizing document classifiers by optimizing the classifiers on multiple document levels (Benton et al., 2017) or general and individual levels (Wu and Huang, 2016). The social relation can bridge connections between users and g"
2021.adaptnlp-1.18,D17-1119,0,0.0919775,"018; Zeng et al., 2019; Huang et al., 2019). The representations of user language can help models better understand documents as global contexts. However, existing user embedding methods (Amir et al., 2016; Benton et al., 2016; Xing and Paul, 2017; Pan and Ding, 2019) mainly focus on extracting features from language itself while ignoring user interests. Recent research has demonstrated that adapting the user factors can further improve user geolocation prediction (Miura et al., 2017), demographic attribute prediction (Farnadi et al., 2018), and sentiment analysis (Yang and Eisenstein, 2017). Lynn et al. (2017); Huang and Paul (2019) treated the language variations as a domain adaptation problem and referred to this idea as user factor adaptation. In this study, we treat the user interest as domains (e.g., restaurants vs. home services domains) and propose a multitask framework to model language variations and incorporate the user factor into user embeddings. We focus on three online 172 Proceedings of the Second Workshop on Domain Adaptation for NLP, pages 172–182 April 20, 2021. ©2021 Association for Computational Linguistics review datasets from Amazon, IMDb, and Yelp containing diverse behaviors"
2021.adaptnlp-1.18,P17-1116,0,0.02697,"tion models, and further improve model performance (Tang et al., 2015; Chen et al., 2016a; Yang and Eisenstein, 2017; Wu et al., 2018; Zeng et al., 2019; Huang et al., 2019). The representations of user language can help models better understand documents as global contexts. However, existing user embedding methods (Amir et al., 2016; Benton et al., 2016; Xing and Paul, 2017; Pan and Ding, 2019) mainly focus on extracting features from language itself while ignoring user interests. Recent research has demonstrated that adapting the user factors can further improve user geolocation prediction (Miura et al., 2017), demographic attribute prediction (Farnadi et al., 2018), and sentiment analysis (Yang and Eisenstein, 2017). Lynn et al. (2017); Huang and Paul (2019) treated the language variations as a domain adaptation problem and referred to this idea as user factor adaptation. In this study, we treat the user interest as domains (e.g., restaurants vs. home services domains) and propose a multitask framework to model language variations and incorporate the user factor into user embeddings. We focus on three online 172 Proceedings of the Second Workshop on Domain Adaptation for NLP, pages 172–182 April 2"
2021.adaptnlp-1.18,N19-1215,0,0.115669,"cross user factors including user interests, demographic attributes, personalities, and latent factors from user history. Research shows that language usage diversifies according to online user groups (Volkova et al., 2013), which women were more likely to use the word weakness in a positive way while men were the opposite. In social media, the user interests can include topics of user reviews (e.g., home vs. health services in Yelp) and categories of reviewed items (electronic vs kitchen products in Amazon). The ways that users express themselves depend on current contexts of user interests (Oba et al., 2019) that users may use the same words for opposite meanings and different words for the same meaning. For example, online users can use the word “fast” to criticize battery quality of the electronic domain or praise medicine effectiveness of the medical products; users can also use the words “cool” to describe a property of AC products or express sentiments. User embedding, which is to learn a fixed-length representation based on multiple user reviews of each user, can infer the user latent information into a unified vector space (Benton, 2018; Pan and Ding, 2019). The inferred latent representat"
2021.adaptnlp-1.18,P11-1077,0,0.0151672,"es of rated items. To map the 300d user embeddings, we use the TSNE algorithm from scikit-learn (Pedregosa et al., 2011) to compress the dimension into 2-d vectors. We set the n component as 2 and leave the other parameters as their defaults in the TSNE. We can observe that the MTL user embedding model shows more cluster178 6 Related Work User Profiling is a common task in natural language processing. Online generated user texts show demographic variations in the linguistic styles, and the linguistic style variability could be used for predicting user’s personality and demographic attributes (Rosenthal and McKeown, 2011; Zhang et al., 2016; Hovy and Fornaciari, 2018; WoodDoughty et al., 2020; Gjurkovi´c et al., 2020; Lynn et al., 2020). The demographic user factors influence how online users express their opinions (Volkova et al., 2013; Hovy, 2015; WoodDoughty et al., 2017) and show promising improvements in the text classification task (Lynn et al., 2017; Huang and Paul, 2019; Lynn et al., 2019). However, in this work, the goal of modeling user factor is to train robust user embeddings via domain adaptation, rather than the end goal being demographic factor prediction and document classification itself. Per"
2021.adaptnlp-1.18,D15-1036,0,0.247783,"ion 3. We then propose our user embedding model that adapts the user interests using a multitask learning framework in Section 4. Research (Pan and Ding, 2019) generally evaluates the user embedding via downstream tasks, but user annotations sometimes are hard to obtain and those evaluations are extrinsic instead of intrinsic tasks. For example, the MyPersonality (Kosinski et al., 2015) that was used in previous work (Ding et al., 2017; Farnadi et al., 2018; Pan and Ding, 2019) is no longer available, and an extrinsic task is to evaluate if user embeddings can help text classifiers. Research (Schnabel et al., 2015) suggests that the intrinsic evaluation including clustering is better than the extrinsic evaluation for controlling less hyperparameters. We propose an intrinsic evaluation for user embedding, which can provide a new perspective for testing future experiments. We show that our user-factor-adapted user embedding can generally outperform the existing methods on both intrinsic and extrinsic tasks. 2 Data We collected English reviews of Amazon (health product), IMDb and Yelp from the publicly available sources (He and McAuley, 2016; Yelp, 2018; IMDb, 2020). For the IMDb dataset, we included Engli"
2021.adaptnlp-1.18,P15-1098,0,0.0239691,"AC products or express sentiments. User embedding, which is to learn a fixed-length representation based on multiple user reviews of each user, can infer the user latent information into a unified vector space (Benton, 2018; Pan and Ding, 2019). The inferred latent representations from online content can predict user profile (Volkova et al., 2015; Wang et al., 2018; Farnadi et al., 2018; Lynn et al., 2020) and behaviors (Zhang et al., 2015; Amir et al., 2017; Benton et al., 2017; Ding et al., 2017). User embeddings can personalize classification models, and further improve model performance (Tang et al., 2015; Chen et al., 2016a; Yang and Eisenstein, 2017; Wu et al., 2018; Zeng et al., 2019; Huang et al., 2019). The representations of user language can help models better understand documents as global contexts. However, existing user embedding methods (Amir et al., 2016; Benton et al., 2016; Xing and Paul, 2017; Pan and Ding, 2019) mainly focus on extracting features from language itself while ignoring user interests. Recent research has demonstrated that adapting the user factors can further improve user geolocation prediction (Miura et al., 2017), demographic attribute prediction (Farnadi et al."
2021.adaptnlp-1.18,D13-1187,0,0.210638,"on. While existing work mainly evaluated the user embedding by extrinsic tasks, we propose an intrinsic evaluation via clustering and evaluate user embeddings by an extrinsic task, text classification. The experiments on the three Englishlanguage social media datasets show that our proposed approach can generally outperform baselines via adapting the user factor. 1 Introduction Language varies across user factors including user interests, demographic attributes, personalities, and latent factors from user history. Research shows that language usage diversifies according to online user groups (Volkova et al., 2013), which women were more likely to use the word weakness in a positive way while men were the opposite. In social media, the user interests can include topics of user reviews (e.g., home vs. health services in Yelp) and categories of reviewed items (electronic vs kitchen products in Amazon). The ways that users express themselves depend on current contexts of user interests (Oba et al., 2019) that users may use the same words for opposite meanings and different words for the same meaning. For example, online users can use the word “fast” to criticize battery quality of the electronic domain or"
2021.adaptnlp-1.18,C18-1119,0,0.020667,"rds for the same meaning. For example, online users can use the word “fast” to criticize battery quality of the electronic domain or praise medicine effectiveness of the medical products; users can also use the words “cool” to describe a property of AC products or express sentiments. User embedding, which is to learn a fixed-length representation based on multiple user reviews of each user, can infer the user latent information into a unified vector space (Benton, 2018; Pan and Ding, 2019). The inferred latent representations from online content can predict user profile (Volkova et al., 2015; Wang et al., 2018; Farnadi et al., 2018; Lynn et al., 2020) and behaviors (Zhang et al., 2015; Amir et al., 2017; Benton et al., 2017; Ding et al., 2017). User embeddings can personalize classification models, and further improve model performance (Tang et al., 2015; Chen et al., 2016a; Yang and Eisenstein, 2017; Wu et al., 2018; Zeng et al., 2019; Huang et al., 2019). The representations of user language can help models better understand documents as global contexts. However, existing user embedding methods (Amir et al., 2016; Benton et al., 2016; Xing and Paul, 2017; Pan and Ding, 2019) mainly focus on extra"
2021.adaptnlp-1.18,W17-2912,1,0.698095,"Missing"
2021.adaptnlp-1.18,W17-4406,1,0.722932,"predict user profile (Volkova et al., 2015; Wang et al., 2018; Farnadi et al., 2018; Lynn et al., 2020) and behaviors (Zhang et al., 2015; Amir et al., 2017; Benton et al., 2017; Ding et al., 2017). User embeddings can personalize classification models, and further improve model performance (Tang et al., 2015; Chen et al., 2016a; Yang and Eisenstein, 2017; Wu et al., 2018; Zeng et al., 2019; Huang et al., 2019). The representations of user language can help models better understand documents as global contexts. However, existing user embedding methods (Amir et al., 2016; Benton et al., 2016; Xing and Paul, 2017; Pan and Ding, 2019) mainly focus on extracting features from language itself while ignoring user interests. Recent research has demonstrated that adapting the user factors can further improve user geolocation prediction (Miura et al., 2017), demographic attribute prediction (Farnadi et al., 2018), and sentiment analysis (Yang and Eisenstein, 2017). Lynn et al. (2017); Huang and Paul (2019) treated the language variations as a domain adaptation problem and referred to this idea as user factor adaptation. In this study, we treat the user interest as domains (e.g., restaurants vs. home services"
2021.adaptnlp-1.18,Q17-1021,0,0.374267,"r embedding, which is to learn a fixed-length representation based on multiple user reviews of each user, can infer the user latent information into a unified vector space (Benton, 2018; Pan and Ding, 2019). The inferred latent representations from online content can predict user profile (Volkova et al., 2015; Wang et al., 2018; Farnadi et al., 2018; Lynn et al., 2020) and behaviors (Zhang et al., 2015; Amir et al., 2017; Benton et al., 2017; Ding et al., 2017). User embeddings can personalize classification models, and further improve model performance (Tang et al., 2015; Chen et al., 2016a; Yang and Eisenstein, 2017; Wu et al., 2018; Zeng et al., 2019; Huang et al., 2019). The representations of user language can help models better understand documents as global contexts. However, existing user embedding methods (Amir et al., 2016; Benton et al., 2016; Xing and Paul, 2017; Pan and Ding, 2019) mainly focus on extracting features from language itself while ignoring user interests. Recent research has demonstrated that adapting the user factors can further improve user geolocation prediction (Miura et al., 2017), demographic attribute prediction (Farnadi et al., 2018), and sentiment analysis (Yang and Eisen"
2021.adaptnlp-1.18,P19-1270,0,0.0235045,"h representation based on multiple user reviews of each user, can infer the user latent information into a unified vector space (Benton, 2018; Pan and Ding, 2019). The inferred latent representations from online content can predict user profile (Volkova et al., 2015; Wang et al., 2018; Farnadi et al., 2018; Lynn et al., 2020) and behaviors (Zhang et al., 2015; Amir et al., 2017; Benton et al., 2017; Ding et al., 2017). User embeddings can personalize classification models, and further improve model performance (Tang et al., 2015; Chen et al., 2016a; Yang and Eisenstein, 2017; Wu et al., 2018; Zeng et al., 2019; Huang et al., 2019). The representations of user language can help models better understand documents as global contexts. However, existing user embedding methods (Amir et al., 2016; Benton et al., 2016; Xing and Paul, 2017; Pan and Ding, 2019) mainly focus on extracting features from language itself while ignoring user interests. Recent research has demonstrated that adapting the user factors can further improve user geolocation prediction (Miura et al., 2017), demographic attribute prediction (Farnadi et al., 2018), and sentiment analysis (Yang and Eisenstein, 2017). Lynn et al. (2017); Hu"
2021.adaptnlp-1.18,L16-1478,0,0.0153144,"300d user embeddings, we use the TSNE algorithm from scikit-learn (Pedregosa et al., 2011) to compress the dimension into 2-d vectors. We set the n component as 2 and leave the other parameters as their defaults in the TSNE. We can observe that the MTL user embedding model shows more cluster178 6 Related Work User Profiling is a common task in natural language processing. Online generated user texts show demographic variations in the linguistic styles, and the linguistic style variability could be used for predicting user’s personality and demographic attributes (Rosenthal and McKeown, 2011; Zhang et al., 2016; Hovy and Fornaciari, 2018; WoodDoughty et al., 2020; Gjurkovi´c et al., 2020; Lynn et al., 2020). The demographic user factors influence how online users express their opinions (Volkova et al., 2013; Hovy, 2015; WoodDoughty et al., 2017) and show promising improvements in the text classification task (Lynn et al., 2017; Huang and Paul, 2019; Lynn et al., 2019). However, in this work, the goal of modeling user factor is to train robust user embeddings via domain adaptation, rather than the end goal being demographic factor prediction and document classification itself. Personalized classifica"
2021.bionlp-1.28,P19-1215,0,0.0689377,"f 2019 MEDIQA participants (Zhu et al., 2019), and add the validation set to training for the leaderboard submissions only. We notice that the validation results for the BART + XSum base model are significantly lower than other models. The corresponding test results are also the lowest-ranking, even though the difference is not as large as we trained on the validation set. These results show that training on an out-ofdomain abstractive summarization dataset is not efficient for this task. In addition to the XSum base model, we train on two additional datasets. The first dataset is MeQSum (Ben Abacha and Demner-Fushman, 2019). It is an abstractive medical question summarization We consider now the training on the medical dataset, which consists of 1,000 consumer health questions (CHQs) and their corresponding one- question summarization datasets. First, the valsentence-long frequently asked questions (FAQs). idation results show that training on MeQSum achieves comparable F1 scores as training on It was released by the U.S. National Institutes of Health (NIH), and the FAQs are written by med- HealthCareMagic. The main contrasting point is ical experts. Whereas Ben Abacha and Demner- that training on HealthCareMagi"
2021.bionlp-1.28,2021.bionlp-1.8,0,0.10773,"tractive summarization cannot be as long as the multiple documents in this task. We therefore propose to mitigate this weakness by proposing to cut up the input into pairs of sentences, where the first sentence is the input question, and the second one is a candidate answer. We then train our BART model to score the relevance of each candidate answer with regards to its corresponding question. We also describe in this paper the algorithm used to extract an AS2 dataset from an multi-document extractive summarization dataset. The 2021 Medical NLP and Question Answering (MEDIQA) shared task (Ben Abacha et al., 2021) is comprised of three tasks, centered around summarization in the medical domain: Question Summarization, Multi-Answer Summarization, and Radiology Report Summarization. In this paper, we focus on the first two tasks. In Question Summarization, the goal is to generate a one-sentence 2 Question Summarization formal question summary from a consumer health question – a relatively long question asked by a user. In Multi-Answer Summarization, we are given a Our approach to question summarization involves one-sentence question and multiple relevant answer two kinds of transfer learning. First, we t"
2021.bionlp-1.28,2020.emnlp-main.743,0,0.0708107,"on (AS2) problem. We show how we can preprocess the MEDIQA-AnS dataset such that it can be trained in an AS2 setting. Our AS2 model is able to generate extractive summaries achieving high ROUGE scores. 1 Introduction high results for question summarization. Sequenceto-sequence language model BART (Lewis et al., 2020) has achieved state-of-the-art results in various NLP benchmarks, including in the CNNDailymail news article summarization dataset (Hermann et al., 2015). We leverage this success and train BART on summarization datasets from the medical domain (Ben Abacha and DemnerFushman, 2019; Zeng et al., 2020; Mrini et al., 2021). Moreover, we find that training on a different task in the medical domain – Recognizing Question Entailment (RQE) (Ben Abacha and DemnerFushman, 2016) – can yield better improvements, especially in terms of ROUGE precision scores. Second, we tackle the extractive track of the multi-answer summarization task, and we cast multi-answer extractive summarization as an Answer Sentence Selection (AS2) problem. A limitation of BART is that the input to its abstractive summarization cannot be as long as the multiple documents in this task. We therefore propose to mitigate this we"
2021.bionlp-1.28,W19-5040,0,0.176069,"ing from Medical Summarization Summarization Datasets formal style in the FAQs of the U.S. National Library of Medicine (NLM), whereas iCliniq question summaries are noisier and more extractive. Given that MeQSum is 180 times smaller than HealthCareMagic, we train for 100 epochs on MeQSum, and 10 epochs for HealthCareMagic. We use the validation set of the MEDIQA question summarization task to select the best parameters. 2.2.2 Results and Discussion We show the validation results in Table 1 and the test results in Table 2. In all test results, we follow approaches of 2019 MEDIQA participants (Zhu et al., 2019), and add the validation set to training for the leaderboard submissions only. We notice that the validation results for the BART + XSum base model are significantly lower than other models. The corresponding test results are also the lowest-ranking, even though the difference is not as large as we trained on the validation set. These results show that training on an out-ofdomain abstractive summarization dataset is not efficient for this task. In addition to the XSum base model, we train on two additional datasets. The first dataset is MeQSum (Ben Abacha and Demner-Fushman, 2019). It is an ab"
2021.bionlp-1.28,W19-5039,0,0.0698702,"eQSum to the training (RQE + MeQSum) seems to decrease precision, increase recall, achieve similar ROUGE-1 F1, but lower ROUGE-2 and ROUGE-L F1 scores. In Table 2, we notice that the test results that the RQE + MeQSum model is the clear winner, providing the highest scores across the board, with the exception of ROUGE-2 precision. Overall, it seems that pre-training on a similar task in the medical domain is beneficial for this medical question summarization task. 3 Multi-Answer Extractive Summarization For the RQE task, we use the RQE dataset from 3.1 Dataset the 2019 MEDIQA shared task (Ben Abacha et al., 2019). The training set was introduced in The dataset for this task is the MEDIQA-AnS Ben Abacha and Demner-Fushman (2016). Sim- dataset (Savery et al., 2020). It contains 156 userilarly to MeQSum, this dataset is released by the written medical questions, and answer articles to U.S. National Institutes of Health. The MEDIQA- these questions, such that one question usually RQE dataset contains 8,588 training question pairs. has more than one answer article. There are also We train for 10 epochs and choose the best parame- manually-written abstractive and extractive sumters using the validation set"
2021.bionlp-1.28,2020.acl-main.703,0,0.267742,"to train on the task of Recognizing Question Entailment (RQE) in the medical domain. We show that both transfer learning methods combined achieve the highest ROUGE scores. Finally, we cast the question-driven extractive summarization of multiple relevant answer documents as an Answer Sentence Selection (AS2) problem. We show how we can preprocess the MEDIQA-AnS dataset such that it can be trained in an AS2 setting. Our AS2 model is able to generate extractive summaries achieving high ROUGE scores. 1 Introduction high results for question summarization. Sequenceto-sequence language model BART (Lewis et al., 2020) has achieved state-of-the-art results in various NLP benchmarks, including in the CNNDailymail news article summarization dataset (Hermann et al., 2015). We leverage this success and train BART on summarization datasets from the medical domain (Ben Abacha and DemnerFushman, 2019; Zeng et al., 2020; Mrini et al., 2021). Moreover, we find that training on a different task in the medical domain – Recognizing Question Entailment (RQE) (Ben Abacha and DemnerFushman, 2016) – can yield better improvements, especially in terms of ROUGE precision scores. Second, we tackle the extractive track of the m"
2021.bionlp-1.28,W02-0109,0,0.281306,"n: Mean In the AS2 setting, we train BART to predict Average Precision (MAP) and Mean Reciprocal the relevance score of a candidate answer given a Rank (MRR). MAP measures how many of the topquestion. To obtain the pairs of questions and canranked answers are relevant, whereas MRR meadidate answers from the MEDIQA-AnS dataset, we sures how highly a first relevant answer is ranked. proceed as follows. First, we concatenate for each We compute the scores as follows, given a set Q of question the text data of its corresponding answer questions: articles. Then, we use the NLTK sentence tokenizer (Loper and Bird, 2002) to split this text data into P individual sentences. Finally, we form questionq∈Q average_precision(q) MAP(Q) = (1) sentence pairs for AS2 by pairing the user question |Q| with each sentence from the corresponding answer P article text data. 1 q∈Q rank(q) In this training context, AS2 is a binary clasMRR(Q) = (2) |Q| sification task, where each pair of question and We take as base models the BART + XSum candidate answer is labeled as relevant (1) or irrelevant (0). We use cross-entropy as the loss function. model, as well as the best-performing model in We label sentences contained in the ref"
2021.bionlp-1.28,2021.nlpmc-1.8,1,0.794107,"e show how we can preprocess the MEDIQA-AnS dataset such that it can be trained in an AS2 setting. Our AS2 model is able to generate extractive summaries achieving high ROUGE scores. 1 Introduction high results for question summarization. Sequenceto-sequence language model BART (Lewis et al., 2020) has achieved state-of-the-art results in various NLP benchmarks, including in the CNNDailymail news article summarization dataset (Hermann et al., 2015). We leverage this success and train BART on summarization datasets from the medical domain (Ben Abacha and DemnerFushman, 2019; Zeng et al., 2020; Mrini et al., 2021). Moreover, we find that training on a different task in the medical domain – Recognizing Question Entailment (RQE) (Ben Abacha and DemnerFushman, 2016) – can yield better improvements, especially in terms of ROUGE precision scores. Second, we tackle the extractive track of the multi-answer summarization task, and we cast multi-answer extractive summarization as an Answer Sentence Selection (AS2) problem. A limitation of BART is that the input to its abstractive summarization cannot be as long as the multiple documents in this task. We therefore propose to mitigate this weakness by proposing t"
2021.bionlp-1.28,D18-1206,0,0.0672727,"ransfer learning from other tasks in the medical domain increases from pre-trained language models can achieve very ROUGE scores. 257 Proceedings of the BioNLP 2021 workshop, pages 257–262 June 11, 2021. ©2021 Association for Computational Linguistics 2.1 Training Details We adopt the BART Large architecture (Lewis et al., 2020), as it set a state of the art in abstractive summarization benchmarks, and allows us to train a single model on generation and classification tasks. We use a base model, which is trained on BART’s language modeling tasks and the XSum abstractive summarization dataset (Narayan et al., 2018). We use a learning rate of 3 ∗ 10−5 for summarization tasks and 1 ∗ 10−5 for the recognizing question entailment task. We use 512 as the maximum number of token positions. Following the MEDIQA instructions and leaderboard, we use precision, recall and F1 scores for the ROUGE-1, ROUGE-2 and ROUGE-L metrics (Lin, 2004). 2.2 2.2.1 Transfer Learning from Medical Summarization Summarization Datasets formal style in the FAQs of the U.S. National Library of Medicine (NLM), whereas iCliniq question summaries are noisier and more extractive. Given that MeQSum is 180 times smaller than HealthCareMagic,"
2021.eacl-demos.20,C18-1221,0,0.0808659,"on and disambiguation system Evaluation This section provides more insight into the performance of the proposed acronym identification and disambiguation models. To evaluate the performance of the models in comparison with other state-of-the-art AI and AD models, we report the performance of the proposed models on SciAI and SciAD benchmark datasets (Pouran Ben Veyseh et al., 2020d). We also compare the performance of the proposed model with the baselines provided in the recent work (Pouran Ben Veyseh et al., 2020d). More specifically, on SciAI, we compare our model with rule-based models NOA (Charbonnier and Wartena, 2018), ADE (Li et al., 2018) and UAD (Ciosici et al., 2019); and also the feature-based models BIOADI (Kuo et al., 2009) and LNCRF (Liu et al., 2017); and finally the SOTA deep model LSTM-CRF (Pouran Ben Veyseh et al., 2020d). For evaluation metrics, following prior work, we report precision, recall, and F1 score for the acronym and long-form prediction and also their macro-averaged F1 score. The results are shown in Table 2. This table shows that our model outperforms both rulebased and more advanced feature-based or deep learning models. More interestingly, while the proposed model has comparable"
2021.eacl-demos.20,C18-2001,0,0.215514,"orks are not capable to expand all acronyms of a domain or acronyms in other domains other than the one used in the training set. Although in the recent work (Wen et al., 2020), authors proposed a big dataset for acronym disambiguation in the medical domain with more than 14 million samples, it is still limited to a specific domain (i.e., medical domain). Another limitation in prior works is that they do not provide a unified system capable of performing both tasks in various domains and to be publicly available. To our knowledge, the only exiting web-based system for AI and AD is proposed by Ciosici and Assent (2018). For acronym identification, this system employs the rule-based model introduced by (Schwartz and Hearst, 2002). To handle corner cases, they add extra rules in addition to Schwartz’s rules in their system. Unfortunately, they do not provide detailed information about these corner cases and extra rules or any evaluation to assess the performance of the model. For acronym disambiguation, they resort to a statistical model in which a pre-computed vector representation for each candidate long-form is employed to compute the similarity between candidate long-form with the context of the ambiguous"
2021.eacl-demos.20,W19-5010,0,0.0207905,"odels. Specifically, we compare the model with non-deep learning models including most frequent (MF) meaning (Pouran Ben Veyseh et al., 2020d), feature-based model (i.e., ADE (Li et al., 2018)), and deep learning models including NOA (Charbonnier and Wartena, 2018), UAD (Ciosici et al., 2019), BEM (Blevins 164 Model MF ADE NOA UAD BEM DECBAE GAD MadDog P 89.03 86.74 78.14 89.01 86.75 88.67 89.27 92.27 R 42.2 43.25 35.06 70.08 35.94 74.32 76.66 85.01 F1 57.26 57.72 48.40 78.37 50.82 80.86 81.90 88.49 Table 3: Performance of models for acronym disambiguation (AD) and Zettlemoyer, 2020), DECBAE (Jin et al., 2019) and GAD (Pouran Ben Veyseh et al., 2020d). The results are shown in Table 3. This table demonstrates the effectiveness of the proposed model compared with the baselines. Our hypothesis for the higher performance of the proposed model is the massive number of training examples for all acronyms which results in low generalization error. 4 Related Work Acronym identification (AI) and acronym disambiguation (AD) are two well-known tasks with several prior works in the past two decades. For AI, both rule-based models (Park and Byrd, 2001; Wren and Garner, 2002; Schwartz and Hearst, 2002; Adar, 200"
2021.eacl-demos.20,W16-6107,0,0.0211405,"esults are shown in Table 3. This table demonstrates the effectiveness of the proposed model compared with the baselines. Our hypothesis for the higher performance of the proposed model is the massive number of training examples for all acronyms which results in low generalization error. 4 Related Work Acronym identification (AI) and acronym disambiguation (AD) are two well-known tasks with several prior works in the past two decades. For AI, both rule-based models (Park and Byrd, 2001; Wren and Garner, 2002; Schwartz and Hearst, 2002; Adar, 2004; Nadeau and Turney, 2005; Ao and Takagi, 2005; Kirchhoff and Turner, 2016) and supervised feature-based or deep learning models (Kuo et al., 2009; Liu et al., 2017; Pouran Ben Veyseh et al., 2020d, 2021) are utilized. Due to the higher accuracy of rule-based models, they are dominantly used in the majority of the related works, especially to automatically create acronym dictionary (Ciosici et al., 2019; Li et al., 2018; Charbonnier and Wartena, 2018). However, the existing works prepare a small-size dictionary in a specific domain. In contrast, in this work, we first improve the existing rules for acronym identification, then, we use a diverse acronym glossary in ou"
2021.eacl-demos.20,P18-1121,0,0.0995157,"on This section provides more insight into the performance of the proposed acronym identification and disambiguation models. To evaluate the performance of the models in comparison with other state-of-the-art AI and AD models, we report the performance of the proposed models on SciAI and SciAD benchmark datasets (Pouran Ben Veyseh et al., 2020d). We also compare the performance of the proposed model with the baselines provided in the recent work (Pouran Ben Veyseh et al., 2020d). More specifically, on SciAI, we compare our model with rule-based models NOA (Charbonnier and Wartena, 2018), ADE (Li et al., 2018) and UAD (Ciosici et al., 2019); and also the feature-based models BIOADI (Kuo et al., 2009) and LNCRF (Liu et al., 2017); and finally the SOTA deep model LSTM-CRF (Pouran Ben Veyseh et al., 2020d). For evaluation metrics, following prior work, we report precision, recall, and F1 score for the acronym and long-form prediction and also their macro-averaged F1 score. The results are shown in Table 2. This table shows that our model outperforms both rulebased and more advanced feature-based or deep learning models. More interestingly, while the proposed model has comparable precision with the exi"
2021.eacl-demos.20,W01-0516,0,0.375926,"or acronym disambiguation (AD) and Zettlemoyer, 2020), DECBAE (Jin et al., 2019) and GAD (Pouran Ben Veyseh et al., 2020d). The results are shown in Table 3. This table demonstrates the effectiveness of the proposed model compared with the baselines. Our hypothesis for the higher performance of the proposed model is the massive number of training examples for all acronyms which results in low generalization error. 4 Related Work Acronym identification (AI) and acronym disambiguation (AD) are two well-known tasks with several prior works in the past two decades. For AI, both rule-based models (Park and Byrd, 2001; Wren and Garner, 2002; Schwartz and Hearst, 2002; Adar, 2004; Nadeau and Turney, 2005; Ao and Takagi, 2005; Kirchhoff and Turner, 2016) and supervised feature-based or deep learning models (Kuo et al., 2009; Liu et al., 2017; Pouran Ben Veyseh et al., 2020d, 2021) are utilized. Due to the higher accuracy of rule-based models, they are dominantly used in the majority of the related works, especially to automatically create acronym dictionary (Ciosici et al., 2019; Li et al., 2018; Charbonnier and Wartena, 2018). However, the existing works prepare a small-size dictionary in a specific domain."
2021.eacl-demos.20,2020.emnlp-main.713,0,0.0129276,"the most likely expansion. However, there are some limitations to this previous system. Firstly, it is restricted to the general domain (i.e., Wikipedia) and it covers a limited number of acronyms. Second, it does not provide any analysis and evaluations of the performance of the proposed model. Lastly, it is not publicly available anymore. The proposed MadDog system could be useful for many downstream applications including definition extraction (Pouran Ben Veyseh et al., 2020a; Spala et al., 2020, 2019), information extraction (Pouran Ben Veyseh et al., 2019, 2020b,c) or question answering (Perez et al., 2020) 5 System Deployment MadDog is purely written in Python 3 and could be run as a FLASK (Grinberg, 2018) server. For text toknization, it employs SpaCy 2 (Honnibal and Montani, 2017). Also, the trained acronym expansion model requires PyTorch 1.7 and 64 GB of disk space. Note that all acronyms with their long-forms are encoded in the trained model so they can perform both the dictionary look-up operation and the disambiguation task. Moreover, the trained models could be loaded both on GPU and CPU. 6 Conclusion In this work, we propose a new web-based system for acronym identification and disambi"
2021.eacl-demos.20,2020.acl-main.715,1,0.235653,"r science papers), biomedical (e.g., Medline abstracts), or financial (e.g., financial discussions in Reddit). Note that the proposed system is capable to identify acronyms and their long-forms in all Latin-script languages. More specifically, for acronym identification, we propose a rule-based model by extending the set of rules proposed by (Schwartz and Hearst, 2002). We empirically show that the proposed model outperforms both the previous rule-based model and also the existing state-ofthe-art deep learning models for acronym identification on the recent benchmark dataset SciAI (Pouran Ben Veyseh et al., 2020d). Next, we use a large dataset created from corpora in various domains for acronym disambiguation to train a deep model for this task. Specifically, we employ a sequential deep model to encode the context of the ambiguous acronym and solve the AD task using a feedforward multi-class classifier. We also evaluate the performance of the proposed acronym disambiguation model on the recent benchmark dataset SciAD (Pouran Ben Veyseh et al., 2020d). To summarize, our contributions are: • The first web-based multi-domain acronym identification and disambiguation system • Extensive evaluation of the"
2021.eacl-demos.20,2020.coling-main.292,1,0.739404,"r science papers), biomedical (e.g., Medline abstracts), or financial (e.g., financial discussions in Reddit). Note that the proposed system is capable to identify acronyms and their long-forms in all Latin-script languages. More specifically, for acronym identification, we propose a rule-based model by extending the set of rules proposed by (Schwartz and Hearst, 2002). We empirically show that the proposed model outperforms both the previous rule-based model and also the existing state-ofthe-art deep learning models for acronym identification on the recent benchmark dataset SciAI (Pouran Ben Veyseh et al., 2020d). Next, we use a large dataset created from corpora in various domains for acronym disambiguation to train a deep model for this task. Specifically, we employ a sequential deep model to encode the context of the ambiguous acronym and solve the AD task using a feedforward multi-class classifier. We also evaluate the performance of the proposed acronym disambiguation model on the recent benchmark dataset SciAD (Pouran Ben Veyseh et al., 2020d). To summarize, our contributions are: • The first web-based multi-domain acronym identification and disambiguation system • Extensive evaluation of the"
2021.eacl-demos.20,2020.semeval-1.41,1,0.710258,"onary of acronyms. For AD, unlike our work that trains a deep model, they use word embedding similarity to predict the most likely expansion. However, there are some limitations to this previous system. Firstly, it is restricted to the general domain (i.e., Wikipedia) and it covers a limited number of acronyms. Second, it does not provide any analysis and evaluations of the performance of the proposed model. Lastly, it is not publicly available anymore. The proposed MadDog system could be useful for many downstream applications including definition extraction (Pouran Ben Veyseh et al., 2020a; Spala et al., 2020, 2019), information extraction (Pouran Ben Veyseh et al., 2019, 2020b,c) or question answering (Perez et al., 2020) 5 System Deployment MadDog is purely written in Python 3 and could be run as a FLASK (Grinberg, 2018) server. For text toknization, it employs SpaCy 2 (Honnibal and Montani, 2017). Also, the trained acronym expansion model requires PyTorch 1.7 and 64 GB of disk space. Note that all acronyms with their long-forms are encoded in the trained model so they can perform both the dictionary look-up operation and the disambiguation task. Moreover, the trained models could be loaded both"
2021.eacl-demos.20,W19-4015,1,0.872953,"Missing"
2021.eacl-demos.20,2020.clinicalnlp-1.15,0,0.0245505,"if it has multiple meanings. Despite the progress made on the AI and AD task in the last two decades, there are some limitations in the prior works that prevent achieving a functional system to be used in practice. More specifically, considering the research on the AD task, all of the prior works employ a small-size dataset covering a few hundred to a few thousand long-forms in a specific domain. Therefore, the models trained in these works are not capable to expand all acronyms of a domain or acronyms in other domains other than the one used in the training set. Although in the recent work (Wen et al., 2020), authors proposed a big dataset for acronym disambiguation in the medical domain with more than 14 million samples, it is still limited to a specific domain (i.e., medical domain). Another limitation in prior works is that they do not provide a unified system capable of performing both tasks in various domains and to be publicly available. To our knowledge, the only exiting web-based system for AI and AD is proposed by Ciosici and Assent (2018). For acronym identification, this system employs the rule-based model introduced by (Schwartz and Hearst, 2002). To handle corner cases, they add extr"
2021.eacl-demos.20,W15-3822,0,0.0906585,"er, some feature-based models have been also used for acronym identification (Kuo et al., 2009; Liu et al., 2017). In addition, some of the existing software employs regular expressions for acronym identification in the biomedical domain (Gooch, 2011). Acronym disambiguation is also approached with feature-based models (Wang et al., 2016) or more advanced deep learning meth160 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations, pages 160–167 April 19 - 23, 2021. ©2021 Association for Computational Linguistics ods (Wu et al., 2015; Ciosici et al., 2019). The majority of deep models employ word embeddings to compute the similarity between the candidate longform and the acronym context. In addition to the existing research for AD, there is some web-based software that employ dictionary look-up to expand an acronym to its long-form (ABBREX2018). Note that the methods based on dictionary look-up are not able to disambiguate the acronym if it has multiple meanings. Despite the progress made on the AI and AD task in the last two decades, there are some limitations in the prior works that prevent achieving a functional system"
2021.emnlp-main.427,W06-0901,0,0.124594,"produces more accurate predictions than Proto, as 5273 shown on the diagonal. Second, ProAcT involves remarkably more correct predictions for negative examples than Proto. In the mean time, it generates significantly lower number of errors in both false positive and false negative related to the NULL class, i.e. Other class in Figure 1, suggesting that our proposed model effectively mitigates the effect of noise introduced by the NULL class. 4 Related works Prior studies in ED mainly follow the supervised learning scheme. The early work focuses on feature engineering with statistical models (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011). Recently, many deep learning architectures have been explored for automatic feature learning (Nguyen and Grishman, 2015; Chen et al., 2015; Nguyen et al., 2016; Feng et al., 2016; Nguyen and Grishman, 2018; Lai et al., 2020c; Veyseh et al., 2021). Some recent studies have also introduced methods to extending ED to new event types (Liao and Grishman, 2011; Huang and Riloff, 2012; Nguyen et al., 2016b,g; Chen et al., 2017; Huang et al., 2018; Tong et al., 2020; Lai et al., 2020b). FSL has been extensively studied in computer v"
2021.emnlp-main.427,P17-1038,0,0.0137312,"inly follow the supervised learning scheme. The early work focuses on feature engineering with statistical models (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011). Recently, many deep learning architectures have been explored for automatic feature learning (Nguyen and Grishman, 2015; Chen et al., 2015; Nguyen et al., 2016; Feng et al., 2016; Nguyen and Grishman, 2018; Lai et al., 2020c; Veyseh et al., 2021). Some recent studies have also introduced methods to extending ED to new event types (Liao and Grishman, 2011; Huang and Riloff, 2012; Nguyen et al., 2016b,g; Chen et al., 2017; Huang et al., 2018; Tong et al., 2020; Lai et al., 2020b). FSL has been extensively studied in computer vision (Vinyals et al., 2016; Snell et al., 2017; Finn et al., 2017; Lee et al., 2019; Fei et al., 2021). Recent work has also considered FSL for tasks in natural language processing (Han et al., 2018; Bao et al., 2020). For ED, prior FSL work has mostly relied on Prototypical network (Lai et al., 2020b; Deng et al., 2020). However, these models do not explore cross-task modeling as we do. 5 Conclusion In this paper, we propose to exploit the relationship between training tasks for few-sho"
2021.emnlp-main.427,P15-1017,0,0.0198,"tes significantly lower number of errors in both false positive and false negative related to the NULL class, i.e. Other class in Figure 1, suggesting that our proposed model effectively mitigates the effect of noise introduced by the NULL class. 4 Related works Prior studies in ED mainly follow the supervised learning scheme. The early work focuses on feature engineering with statistical models (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011). Recently, many deep learning architectures have been explored for automatic feature learning (Nguyen and Grishman, 2015; Chen et al., 2015; Nguyen et al., 2016; Feng et al., 2016; Nguyen and Grishman, 2018; Lai et al., 2020c; Veyseh et al., 2021). Some recent studies have also introduced methods to extending ED to new event types (Liao and Grishman, 2011; Huang and Riloff, 2012; Nguyen et al., 2016b,g; Chen et al., 2017; Huang et al., 2018; Tong et al., 2020; Lai et al., 2020b). FSL has been extensively studied in computer vision (Vinyals et al., 2016; Snell et al., 2017; Finn et al., 2017; Lee et al., 2019; Fei et al., 2021). Recent work has also considered FSL for tasks in natural language processing (Han et al., 2018; Bao et"
2021.emnlp-main.427,P16-2011,0,0.0167668,"in both false positive and false negative related to the NULL class, i.e. Other class in Figure 1, suggesting that our proposed model effectively mitigates the effect of noise introduced by the NULL class. 4 Related works Prior studies in ED mainly follow the supervised learning scheme. The early work focuses on feature engineering with statistical models (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011). Recently, many deep learning architectures have been explored for automatic feature learning (Nguyen and Grishman, 2015; Chen et al., 2015; Nguyen et al., 2016; Feng et al., 2016; Nguyen and Grishman, 2018; Lai et al., 2020c; Veyseh et al., 2021). Some recent studies have also introduced methods to extending ED to new event types (Liao and Grishman, 2011; Huang and Riloff, 2012; Nguyen et al., 2016b,g; Chen et al., 2017; Huang et al., 2018; Tong et al., 2020; Lai et al., 2020b). FSL has been extensively studied in computer vision (Vinyals et al., 2016; Snell et al., 2017; Finn et al., 2017; Lee et al., 2019; Fei et al., 2021). Recent work has also considered FSL for tasks in natural language processing (Han et al., 2018; Bao et al., 2020). For ED, prior FSL work has m"
2021.emnlp-main.427,D18-1514,0,0.0275321,", 2015; Chen et al., 2015; Nguyen et al., 2016; Feng et al., 2016; Nguyen and Grishman, 2018; Lai et al., 2020c; Veyseh et al., 2021). Some recent studies have also introduced methods to extending ED to new event types (Liao and Grishman, 2011; Huang and Riloff, 2012; Nguyen et al., 2016b,g; Chen et al., 2017; Huang et al., 2018; Tong et al., 2020; Lai et al., 2020b). FSL has been extensively studied in computer vision (Vinyals et al., 2016; Snell et al., 2017; Finn et al., 2017; Lee et al., 2019; Fei et al., 2021). Recent work has also considered FSL for tasks in natural language processing (Han et al., 2018; Bao et al., 2020). For ED, prior FSL work has mostly relied on Prototypical network (Lai et al., 2020b; Deng et al., 2020). However, these models do not explore cross-task modeling as we do. 5 Conclusion In this paper, we propose to exploit the relationship between training tasks for few-shot learning event detection. We compute prototypes based on cross-task modeling and present a regularization to enforce prediction consistency of classifiers across tasks. The experiment results show that exploiting cross-task relation can alleviate the poor sampling and outliers in the support set for FSL"
2021.emnlp-main.427,P11-1113,0,0.0350367,"wn on the diagonal. Second, ProAcT involves remarkably more correct predictions for negative examples than Proto. In the mean time, it generates significantly lower number of errors in both false positive and false negative related to the NULL class, i.e. Other class in Figure 1, suggesting that our proposed model effectively mitigates the effect of noise introduced by the NULL class. 4 Related works Prior studies in ED mainly follow the supervised learning scheme. The early work focuses on feature engineering with statistical models (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011). Recently, many deep learning architectures have been explored for automatic feature learning (Nguyen and Grishman, 2015; Chen et al., 2015; Nguyen et al., 2016; Feng et al., 2016; Nguyen and Grishman, 2018; Lai et al., 2020c; Veyseh et al., 2021). Some recent studies have also introduced methods to extending ED to new event types (Liao and Grishman, 2011; Huang and Riloff, 2012; Nguyen et al., 2016b,g; Chen et al., 2017; Huang et al., 2018; Tong et al., 2020; Lai et al., 2020b). FSL has been extensively studied in computer vision (Vinyals et al., 2016; Snell et al., 2017; Finn et al., 2017;"
2021.emnlp-main.427,P18-1201,0,0.0136827,"ervised learning scheme. The early work focuses on feature engineering with statistical models (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011). Recently, many deep learning architectures have been explored for automatic feature learning (Nguyen and Grishman, 2015; Chen et al., 2015; Nguyen et al., 2016; Feng et al., 2016; Nguyen and Grishman, 2018; Lai et al., 2020c; Veyseh et al., 2021). Some recent studies have also introduced methods to extending ED to new event types (Liao and Grishman, 2011; Huang and Riloff, 2012; Nguyen et al., 2016b,g; Chen et al., 2017; Huang et al., 2018; Tong et al., 2020; Lai et al., 2020b). FSL has been extensively studied in computer vision (Vinyals et al., 2016; Snell et al., 2017; Finn et al., 2017; Lee et al., 2019; Fei et al., 2021). Recent work has also considered FSL for tasks in natural language processing (Han et al., 2018; Bao et al., 2020). For ED, prior FSL work has mostly relied on Prototypical network (Lai et al., 2020b; Deng et al., 2020). However, these models do not explore cross-task modeling as we do. 5 Conclusion In this paper, we propose to exploit the relationship between training tasks for few-shot learning event det"
2021.emnlp-main.427,2020.nuse-1.5,1,0.896678,"rtunately, the sample ducing cross-task prototypes. We further propose to enforce prediction consistency among size is so small (K ∈ [1, 10]) that the FSL models classifiers across tasks to make the model more might suffer from sample bias, thus hindering the robust to outliers. Our extensive experiment generalization to novel event types. shows a consistent improvement on three fewPrototypical network is a popular metric-based shot learning datasets. The findings suggest few-shot learning model (Snell et al., 2017) that has that our model is more robust when labeled been explored for FSL ED (Lai et al., 2020b; Deng data of novel event types is limited. The source code is available at http://github.com/ et al., 2020). It introduces a prototype vector for laiviet/fsl-proact. each event type by averaging the representations of the instances of that type. A non-parametric classi1 Introduction fier then predicts the event type of a query instance In Information Extraction, Event Detection (ED) based on its distances from the prototypes (Snell et al., 2017). Hence, an outlier in the support set is an important task that aims to identify and might significantly change the prototypes and flip classify ev"
2021.emnlp-main.427,2020.emnlp-main.435,1,0.741589,"rtunately, the sample ducing cross-task prototypes. We further propose to enforce prediction consistency among size is so small (K ∈ [1, 10]) that the FSL models classifiers across tasks to make the model more might suffer from sample bias, thus hindering the robust to outliers. Our extensive experiment generalization to novel event types. shows a consistent improvement on three fewPrototypical network is a popular metric-based shot learning datasets. The findings suggest few-shot learning model (Snell et al., 2017) that has that our model is more robust when labeled been explored for FSL ED (Lai et al., 2020b; Deng data of novel event types is limited. The source code is available at http://github.com/ et al., 2020). It introduces a prototype vector for laiviet/fsl-proact. each event type by averaging the representations of the instances of that type. A non-parametric classi1 Introduction fier then predicts the event type of a query instance In Information Extraction, Event Detection (ED) based on its distances from the prototypes (Snell et al., 2017). Hence, an outlier in the support set is an important task that aims to identify and might significantly change the prototypes and flip classify ev"
2021.emnlp-main.427,P10-1081,0,0.0460022,"s than Proto, as 5273 shown on the diagonal. Second, ProAcT involves remarkably more correct predictions for negative examples than Proto. In the mean time, it generates significantly lower number of errors in both false positive and false negative related to the NULL class, i.e. Other class in Figure 1, suggesting that our proposed model effectively mitigates the effect of noise introduced by the NULL class. 4 Related works Prior studies in ED mainly follow the supervised learning scheme. The early work focuses on feature engineering with statistical models (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011). Recently, many deep learning architectures have been explored for automatic feature learning (Nguyen and Grishman, 2015; Chen et al., 2015; Nguyen et al., 2016; Feng et al., 2016; Nguyen and Grishman, 2018; Lai et al., 2020c; Veyseh et al., 2021). Some recent studies have also introduced methods to extending ED to new event types (Liao and Grishman, 2011; Huang and Riloff, 2012; Nguyen et al., 2016b,g; Chen et al., 2017; Huang et al., 2018; Tong et al., 2020; Lai et al., 2020b). FSL has been extensively studied in computer vision (Vinyals et al., 2016; Snell et al., 2017;"
2021.emnlp-main.427,R11-1002,0,0.0279005,"oise introduced by the NULL class. 4 Related works Prior studies in ED mainly follow the supervised learning scheme. The early work focuses on feature engineering with statistical models (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011). Recently, many deep learning architectures have been explored for automatic feature learning (Nguyen and Grishman, 2015; Chen et al., 2015; Nguyen et al., 2016; Feng et al., 2016; Nguyen and Grishman, 2018; Lai et al., 2020c; Veyseh et al., 2021). Some recent studies have also introduced methods to extending ED to new event types (Liao and Grishman, 2011; Huang and Riloff, 2012; Nguyen et al., 2016b,g; Chen et al., 2017; Huang et al., 2018; Tong et al., 2020; Lai et al., 2020b). FSL has been extensively studied in computer vision (Vinyals et al., 2016; Snell et al., 2017; Finn et al., 2017; Lee et al., 2019; Fei et al., 2021). Recent work has also considered FSL for tasks in natural language processing (Han et al., 2018; Bao et al., 2020). For ED, prior FSL work has mostly relied on Prototypical network (Lai et al., 2020b; Deng et al., 2020). However, these models do not explore cross-task modeling as we do. 5 Conclusion In this paper, we pro"
2021.emnlp-main.427,2021.naacl-main.3,1,0.815166,"Missing"
2021.emnlp-main.427,P08-1030,0,0.0744155,"ore accurate predictions than Proto, as 5273 shown on the diagonal. Second, ProAcT involves remarkably more correct predictions for negative examples than Proto. In the mean time, it generates significantly lower number of errors in both false positive and false negative related to the NULL class, i.e. Other class in Figure 1, suggesting that our proposed model effectively mitigates the effect of noise introduced by the NULL class. 4 Related works Prior studies in ED mainly follow the supervised learning scheme. The early work focuses on feature engineering with statistical models (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011). Recently, many deep learning architectures have been explored for automatic feature learning (Nguyen and Grishman, 2015; Chen et al., 2015; Nguyen et al., 2016; Feng et al., 2016; Nguyen and Grishman, 2018; Lai et al., 2020c; Veyseh et al., 2021). Some recent studies have also introduced methods to extending ED to new event types (Liao and Grishman, 2011; Huang and Riloff, 2012; Nguyen et al., 2016b,g; Chen et al., 2017; Huang et al., 2018; Tong et al., 2020; Lai et al., 2020b). FSL has been extensively studied in computer vision (Vinyals et al.,"
2021.emnlp-main.427,N16-1034,1,0.819061,"ower number of errors in both false positive and false negative related to the NULL class, i.e. Other class in Figure 1, suggesting that our proposed model effectively mitigates the effect of noise introduced by the NULL class. 4 Related works Prior studies in ED mainly follow the supervised learning scheme. The early work focuses on feature engineering with statistical models (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011). Recently, many deep learning architectures have been explored for automatic feature learning (Nguyen and Grishman, 2015; Chen et al., 2015; Nguyen et al., 2016; Feng et al., 2016; Nguyen and Grishman, 2018; Lai et al., 2020c; Veyseh et al., 2021). Some recent studies have also introduced methods to extending ED to new event types (Liao and Grishman, 2011; Huang and Riloff, 2012; Nguyen et al., 2016b,g; Chen et al., 2017; Huang et al., 2018; Tong et al., 2020; Lai et al., 2020b). FSL has been extensively studied in computer vision (Vinyals et al., 2016; Snell et al., 2017; Finn et al., 2017; Lee et al., 2019; Fei et al., 2021). Recent work has also considered FSL for tasks in natural language processing (Han et al., 2018; Bao et al., 2020). For ED, p"
2021.emnlp-main.427,W16-1618,1,0.830547,"ower number of errors in both false positive and false negative related to the NULL class, i.e. Other class in Figure 1, suggesting that our proposed model effectively mitigates the effect of noise introduced by the NULL class. 4 Related works Prior studies in ED mainly follow the supervised learning scheme. The early work focuses on feature engineering with statistical models (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011). Recently, many deep learning architectures have been explored for automatic feature learning (Nguyen and Grishman, 2015; Chen et al., 2015; Nguyen et al., 2016; Feng et al., 2016; Nguyen and Grishman, 2018; Lai et al., 2020c; Veyseh et al., 2021). Some recent studies have also introduced methods to extending ED to new event types (Liao and Grishman, 2011; Huang and Riloff, 2012; Nguyen et al., 2016b,g; Chen et al., 2017; Huang et al., 2018; Tong et al., 2020; Lai et al., 2020b). FSL has been extensively studied in computer vision (Vinyals et al., 2016; Snell et al., 2017; Finn et al., 2017; Lee et al., 2019; Fei et al., 2021). Recent work has also considered FSL for tasks in natural language processing (Han et al., 2018; Bao et al., 2020). For ED, p"
2021.emnlp-main.427,2020.acl-main.522,0,0.0251637,"eme. The early work focuses on feature engineering with statistical models (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011). Recently, many deep learning architectures have been explored for automatic feature learning (Nguyen and Grishman, 2015; Chen et al., 2015; Nguyen et al., 2016; Feng et al., 2016; Nguyen and Grishman, 2018; Lai et al., 2020c; Veyseh et al., 2021). Some recent studies have also introduced methods to extending ED to new event types (Liao and Grishman, 2011; Huang and Riloff, 2012; Nguyen et al., 2016b,g; Chen et al., 2017; Huang et al., 2018; Tong et al., 2020; Lai et al., 2020b). FSL has been extensively studied in computer vision (Vinyals et al., 2016; Snell et al., 2017; Finn et al., 2017; Lee et al., 2019; Fei et al., 2021). Recent work has also considered FSL for tasks in natural language processing (Han et al., 2018; Bao et al., 2020). For ED, prior FSL work has mostly relied on Prototypical network (Lai et al., 2020b; Deng et al., 2020). However, these models do not explore cross-task modeling as we do. 5 Conclusion In this paper, we propose to exploit the relationship between training tasks for few-shot learning event detection. We compute"
2021.emnlp-main.427,2021.acl-long.490,1,0.676738,"ass, i.e. Other class in Figure 1, suggesting that our proposed model effectively mitigates the effect of noise introduced by the NULL class. 4 Related works Prior studies in ED mainly follow the supervised learning scheme. The early work focuses on feature engineering with statistical models (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011). Recently, many deep learning architectures have been explored for automatic feature learning (Nguyen and Grishman, 2015; Chen et al., 2015; Nguyen et al., 2016; Feng et al., 2016; Nguyen and Grishman, 2018; Lai et al., 2020c; Veyseh et al., 2021). Some recent studies have also introduced methods to extending ED to new event types (Liao and Grishman, 2011; Huang and Riloff, 2012; Nguyen et al., 2016b,g; Chen et al., 2017; Huang et al., 2018; Tong et al., 2020; Lai et al., 2020b). FSL has been extensively studied in computer vision (Vinyals et al., 2016; Snell et al., 2017; Finn et al., 2017; Lee et al., 2019; Fei et al., 2021). Recent work has also considered FSL for tasks in natural language processing (Han et al., 2018; Bao et al., 2020). For ED, prior FSL work has mostly relied on Prototypical network (Lai et al., 2020b; Deng et al."
2021.emnlp-main.427,P19-1522,0,0.0138166,"ypes memory network running on the data of the same (Ebner et al., 2020). ACE is a benchmark dataset class (Deng et al., 2020). Since the source code of in event extraction with 33 event subtypes (Walker et al., 2006). LR-KBP is a large scale event de- DMB-Proto is not published, we reimplement the few-shot classifier with a dynamic memory module tection dataset for FSL. It merges ACE-2005 and (Xiong et al., 2016). We examine two state-of-theTAC-KBP datasets and extends some event types art BERT-based sentence encoders φ for ED, i.e. by automatically collecting data from Freebase and BERTMLP (Yang et al., 2019) and BERTGCN (Lai Wikipedia (Deng et al., 2020). Since RAMS and ACE datasets are designed for supervised learn- et al., 2020c). Hyperparameters: In this paper, stochastic graing, we need to resplit them for FSL training. We dient decent optimizer is used with learning rate use the exact training/development/testing split for −4 ACE as presented in a prior study (Lai et al., 2020b). 1e . The training/evaluation are set to 6,000 and 500 iterations respectively; the evaluation is done Following the same method, for RAMS, we merge the original training/development and testing splits. after every 5"
2021.emnlp-main.483,N19-1124,0,0.0255408,"and coherent text has been demonstrated in several prior works (Radford et al., 2019; Brown et al., 2020; Zellers et al., 2019) when given only a few words or a sentence as a prompt. More recent research has addressed the inability of these models to infill text, or insert new words/tokens between tokens that already exist (Donahue et al., 2020; Zhu et al., 2019; Huang et al., 2020; Stern et al., 2019; Welleck et al., 2019; Zhang et al., 2020; Liao et al., 2020; Moryossef et al., 2019). In this vein, Rashkin et al. (2020) generate coherent stories given just bullet-point plot outlines, while Cai et al. (2019) perform token insertion using a retrieval engine in combination with a language model for dialogue agents. Unlike IGA, however, none of this prior work can control generation using high-level rhetorical directives specified by an author. 2.3 Controllable Text Generation IGA conditions its generated text on tags, which has previously been done for left-to-right language models. For example, Dathathri et al. Numerous studies within the humanities focus on (2020) combine a large language model with an modeling the process of effective writing (Flower attribute discriminator to generate text that"
2021.emnlp-main.483,P02-1040,0,0.111848,"onstrained outputs. Since the infilled spans of BIO are strictly post-modifiers that follow a very specific structure (i.e., enclosed by two commas), the superior performance of ILM indicates that it memorizes this simple form of construction without requiring a separate tag input. PARA is the only substitution-based tag in our system and is not supported by ILM. Therefore, we compare performance of PARA with the stateof-the-art paraphraser STRAP released by Krishna et al. (2020) with the default nucleus sampling p = 0.6. We compute BLEURT scores to check semantic similarity, as well as BLEU (Papineni et al., 2002), self-BLEU (Sun and Zhou, 2012), and iBLEU (Sun and Zhou, 2012) with α = 0.8 to check the diversity of output. Table 4 indicates that IGA outperforms STRAP in all dimensions. We hypothesize that this is primarily because the diverse paraphraser in STRAP normalizes (and often simplifies) stylized text, while our PARA tag is associated with complex, embellished paraphrases during fine-tuning. 7 All automatic metrics are computed only on the infilled spans, excluding the context. Intrinsic crowdsourced evaluation The above automatic evaluations can only tell us so much about IGA’s capabilities."
2021.emnlp-main.483,2020.emnlp-main.55,1,0.819061,"Missing"
2021.emnlp-main.483,2020.emnlp-main.349,0,0.0598188,"Missing"
2021.emnlp-main.483,2020.acl-main.703,0,0.0275975,"Given context, by specifying different writing intents, the system generates output satisfying the intent. In addition to wellformed sentence fragments, keywords can also be part of user input, serving as arguments for the intents, and are preserved in the output. specifically, we build an authoring assistant capable of following fine-grained user directives (e.g., add descriptive text, use idiomatic language, or paraphrase a clunky bit of wording). Our system, the Intent-Guided Assistant (IGA), combines controllable text generation with text infilling (Zhu et al., 2019; Keskar et al., 2019a; Lewis et al., 2020; Donahue et al., 2020); more specifically, we adapt the tag-based control of Keskar et al. (2019b) to include a set of rhetorical directives that our model learns to infill with relevant and fluent text. Our system can handle the following authorguided tags: cause, effect, concession (contrast), description, biography, idiom, and rephrase. User input to IGA can be as simple as a list of keywords and does not have to include well-formed text (Figure 1). We train IGA in supervised fashion by creating a large multi-domain dataset in which spans corresponding to particular directives are ∗ Most o"
2021.emnlp-main.483,2020.acl-main.24,0,0.0248466,"actual implementation of IGA relies on controllable text infilling via language modeling. The ability of large-scale language models to generate fluent and coherent text has been demonstrated in several prior works (Radford et al., 2019; Brown et al., 2020; Zellers et al., 2019) when given only a few words or a sentence as a prompt. More recent research has addressed the inability of these models to infill text, or insert new words/tokens between tokens that already exist (Donahue et al., 2020; Zhu et al., 2019; Huang et al., 2020; Stern et al., 2019; Welleck et al., 2019; Zhang et al., 2020; Liao et al., 2020; Moryossef et al., 2019). In this vein, Rashkin et al. (2020) generate coherent stories given just bullet-point plot outlines, while Cai et al. (2019) perform token insertion using a retrieval engine in combination with a language model for dialogue agents. Unlike IGA, however, none of this prior work can control generation using high-level rhetorical directives specified by an author. 2.3 Controllable Text Generation IGA conditions its generated text on tags, which has previously been done for left-to-right language models. For example, Dathathri et al. Numerous studies within the humanities"
2021.emnlp-main.483,2020.acl-main.704,0,0.015412,"Tufte, 2006). Unlike sentence simplification, the intent of our &lt;paraphrase&gt; tag is to paraphrase with improved writing quality, similar to embellishment. We construct parallel data for this tag by combining ParaNMT-50M (Wieting and Gimpel, 2018), a large corpus consisting of back-translated sentence pairs, with WikiLarge, a sentence simplification dataset with parallel simple and complex sentences. The original sentence in ParaNMT-50M and complex sentence in WikiLarge are treated as targets, while the back-translated sentence and the simplified sentence are used as the source. We use BLEURT (Sellam et al., 2020) to filter noisy pairs from ParaNMT-50M,6 discarding pairs whose word-level edit distance is less than five. To further encourage complex paraphrases, we require the reference sentence to have more lowfrequency words than the candidate sentence. 5 Evaluation against references As an initial comparison of IGA and ILM, we evaluate the generated outputs of each model against reference completions from our dataset, both automatically and through a crowdsourced evaluation. We acknowledge that this type of evaluation (especially using automatic metrics) is limited for open-ended generation tasks lik"
2021.emnlp-main.483,N19-1351,0,0.0370365,"Missing"
2021.emnlp-main.483,W04-1013,0,0.0174926,"a crowdsourced evaluation. We acknowledge that this type of evaluation (especially using automatic metrics) is limited for open-ended generation tasks like ours (Fan et al., 2018; Akoury et al., 2020; Rashkin et al., 2020), which is why we also conduct an in-depth user study in Section 6. While results of these evaluations cannot reflect how practical IGA can be used as an authoring assistant, they do indicate that IGA is more constrained than ILM and produces output that better fulfills the writing intents. 5.1 Automatic evaluation We compare IGA with ILM on automatic metrics such as ROUGE (Lin, 2004) and self-BLEU (Zhu et al., 2018) following Rashkin et al. (2020), computing both scores against reference completions 6 We set BLEURT threshold to be (0.7, 0.9) to avoid semantically dissimilar sentences and sentences without too much change. 5976 ROUGE-2 BIO CAUSE EFFECT CNTRA DESCP IDIOM BLEU-2 5.2 Length ILM IGA ILM IGA ILM IGA 10.4 4.1 5.2 4.2 2.1 33.7 9.9 9.0 6.6 4.9 2.2 37.8 47.7 35.0 37.2 32.3 23.4 62.3 44.2 37.1 37.8 34.6 23.6 64.5 6.3 10.1 13.2 10.3 8.9 3.0 6.0 10.2 13.4 10.3 8.9 2.7 Table 3: ROUGE-2, self-BLEU2, and total number of infilled tokens of each example on test set. STRAP"
2021.emnlp-main.483,2020.acl-main.173,0,0.0189018,"ask. Ethics statement Our data collection is for research purposes only, and thus consistent with the terms of use of all source corpora we mined. For the evaluation process, we strive to compensate the Mechanical Turk workers as well as participants of our user study with competitive payments. The intended use of IGA is for creative writing. Although generating factually-correct output is not a major focus of creative writing tasks, IGA often hallucinates facts about real-world entities, a phenomenon that raises ethical concerns and has become an increasing focus in text generation research (Maynez et al., 2020; Wang and Sennrich, 2020). The model can on rare occasions produce offensive outputs, due in large part to GPT-2’s pretraining corpora. One potential way to reduce the toxicity of output is to apply profanity filter as a post-processing step before final output is returned. Acknowledgements We thank the reviewers for the thoughtful comments. We thank Andrew Drozdov, Katherine Thai, Nicholas Monath and other UMass computer science graduate students for helping us with the user study. We thank UMass NLP group for the great advice on the initial draft of this paper. MI was partially supported by"
2021.emnlp-main.483,P12-2008,0,0.00983197,"led spans of BIO are strictly post-modifiers that follow a very specific structure (i.e., enclosed by two commas), the superior performance of ILM indicates that it memorizes this simple form of construction without requiring a separate tag input. PARA is the only substitution-based tag in our system and is not supported by ILM. Therefore, we compare performance of PARA with the stateof-the-art paraphraser STRAP released by Krishna et al. (2020) with the default nucleus sampling p = 0.6. We compute BLEURT scores to check semantic similarity, as well as BLEU (Papineni et al., 2002), self-BLEU (Sun and Zhou, 2012), and iBLEU (Sun and Zhou, 2012) with α = 0.8 to check the diversity of output. Table 4 indicates that IGA outperforms STRAP in all dimensions. We hypothesize that this is primarily because the diverse paraphraser in STRAP normalizes (and often simplifies) stylized text, while our PARA tag is associated with complex, embellished paraphrases during fine-tuning. 7 All automatic metrics are computed only on the infilled spans, excluding the context. Intrinsic crowdsourced evaluation The above automatic evaluations can only tell us so much about IGA’s capabilities. Many of our tags (e.g., DESCP, C"
2021.emnlp-main.483,2020.acl-main.326,0,0.012549,"Our data collection is for research purposes only, and thus consistent with the terms of use of all source corpora we mined. For the evaluation process, we strive to compensate the Mechanical Turk workers as well as participants of our user study with competitive payments. The intended use of IGA is for creative writing. Although generating factually-correct output is not a major focus of creative writing tasks, IGA often hallucinates facts about real-world entities, a phenomenon that raises ethical concerns and has become an increasing focus in text generation research (Maynez et al., 2020; Wang and Sennrich, 2020). The model can on rare occasions produce offensive outputs, due in large part to GPT-2’s pretraining corpora. One potential way to reduce the toxicity of output is to apply profanity filter as a post-processing step before final output is returned. Acknowledgements We thank the reviewers for the thoughtful comments. We thank Andrew Drozdov, Katherine Thai, Nicholas Monath and other UMass computer science graduate students for helping us with the user study. We thank UMass NLP group for the great advice on the initial draft of this paper. MI was partially supported by award IIS-1955567 from th"
2021.emnlp-main.483,W19-3620,0,0.0285998,"rocess. 2.2 Infilling language models The actual implementation of IGA relies on controllable text infilling via language modeling. The ability of large-scale language models to generate fluent and coherent text has been demonstrated in several prior works (Radford et al., 2019; Brown et al., 2020; Zellers et al., 2019) when given only a few words or a sentence as a prompt. More recent research has addressed the inability of these models to infill text, or insert new words/tokens between tokens that already exist (Donahue et al., 2020; Zhu et al., 2019; Huang et al., 2020; Stern et al., 2019; Welleck et al., 2019; Zhang et al., 2020; Liao et al., 2020; Moryossef et al., 2019). In this vein, Rashkin et al. (2020) generate coherent stories given just bullet-point plot outlines, while Cai et al. (2019) perform token insertion using a retrieval engine in combination with a language model for dialogue agents. Unlike IGA, however, none of this prior work can control generation using high-level rhetorical directives specified by an author. 2.3 Controllable Text Generation IGA conditions its generated text on tags, which has previously been done for left-to-right language models. For example, Dathathri et al."
2021.emnlp-main.483,P18-1042,0,0.0643661,"put: There are individual and social beliefs that should lead us to be skeptical of the facts and the wrong. Input: This report only shows the &lt;idiom&gt; , as many details can only be uncovered if you sign the document. Output: This report only shows the tip of the iceberg , as many details can only be uncovered if you sign the document. BIO CAUSE EFFECT CNTRA DESCP IDIOM Table 2: Example output of each tag from IGA. tically mining the N EWSROOM corpus (Grusky et al., 2018), the largest available summarization dataset with 1.3 million news articles. We also collect partial data from ParaNMT-50M (Wieting and Gimpel, 2018), WikiLarge (Zhang and Lapata, 2017) for “sentence embellishment” writing intent, and PoMo (Kang et al., 2019) to extract postmodifier that comes after an entity. Our dataset (statistics shown in Table 1) contains 75M tokens with a mean example length of 60.5 words tokenized with NLTK (Bird et al., 2009). volve open-ended generation loosely constrained by keywords and intent. 4.1 Data collection for each writing intent CAUSE: This tag helps an author invent a reason for the occurrence of an event. Clauses with CAUSE intent usually follow words/phrases like ‘because’ or ‘due to’. We manually ex"
2021.emnlp-main.483,2020.emnlp-main.226,0,0.0426399,"crip&gt; roof. (2) Post-processing: Identify tags The wind blew over the farm, the rain came down and &lt;descrip&gt; pings &lt;descrip&gt; roof. &lt;sep&gt; she heard ominous &lt;answer&gt; on the &lt;answer&gt; &lt;eos&gt; The wind blew over the farm, the rain came down and &lt;descrip&gt; pings &lt;descrip&gt; roof. &lt;sep&gt; (1) Fine-tuned GPT-2 generates output prefixed with input and &lt;sep&gt; Inference time Figure 2: On the left, we show how each example is constructed for fine-tuning. On the right, we show how the final output is constructed by post-processing the output of a fine-tuned GPT-2 model at inference time. the Megatron-CNTRL model (Xu et al., 2020) control the output with predicted keyword. In contrast to these works, IGA focuses on finegrained, intra-sentential controlled infilling. Previous work has also explored controlling stylistic parameters (Ficler and Goldberg, 2017) and syntactic structures (Iyyer et al., 2018; Goyal and Durrett, 2020). 3 Intent-Guided Assistant IGA extends text infilling models with finegrained rhetorical control. Specifically, we build on the Infilling Language Model (ILM) of Donahue et al. (2020), which fine-tunes an off-theshelf language model such as GPT-2 on a dataset of text with masked spans. To continu"
2021.emnlp-main.483,D17-1062,0,0.0245485,"eliefs that should lead us to be skeptical of the facts and the wrong. Input: This report only shows the &lt;idiom&gt; , as many details can only be uncovered if you sign the document. Output: This report only shows the tip of the iceberg , as many details can only be uncovered if you sign the document. BIO CAUSE EFFECT CNTRA DESCP IDIOM Table 2: Example output of each tag from IGA. tically mining the N EWSROOM corpus (Grusky et al., 2018), the largest available summarization dataset with 1.3 million news articles. We also collect partial data from ParaNMT-50M (Wieting and Gimpel, 2018), WikiLarge (Zhang and Lapata, 2017) for “sentence embellishment” writing intent, and PoMo (Kang et al., 2019) to extract postmodifier that comes after an entity. Our dataset (statistics shown in Table 1) contains 75M tokens with a mean example length of 60.5 words tokenized with NLTK (Bird et al., 2009). volve open-ended generation loosely constrained by keywords and intent. 4.1 Data collection for each writing intent CAUSE: This tag helps an author invent a reason for the occurrence of an event. Clauses with CAUSE intent usually follow words/phrases like ‘because’ or ‘due to’. We manually extracted 16 markers, many from the di"
2021.emnlp-main.483,2020.emnlp-main.698,0,0.0280605,"Missing"
2021.emnlp-main.483,J11-1005,0,0.024604,"writing intent CAUSE: This tag helps an author invent a reason for the occurrence of an event. Clauses with CAUSE intent usually follow words/phrases like ‘because’ or ‘due to’. We manually extracted 16 markers, many from the discourse marker list in Sileo et al. (2019), and then mine N EWSROOM (Grusky et al., 2018) to find sentences that match any of the markers. For all mined examples, we also preserve the previous sentence as the context of the matched sentence. Simple declarative clauses that start with matched discourse markers are extracted through shift-reduce constituency parser ZPar (Zhang and Clark, 2011). The YAKE algorithm is later applied to those clauses for keyword extraction. Choosing a collection of tags: Before we start collecting data, we conduct an internal survey with potential users of our system to determine what writing assistance functions they would most benefit from. We surveyed nine NLP researchers about their opinions on the ideal functionality of an authoring assistant. After removing simple functions such as generating synonyms, antonyms, adjectives, and adverbs, which are already impleEFFECT: As a conjugate writing intent of CAUSE, EFFECT is used when one needs to demente"
2021.emnlp-main.520,J08-4004,0,0.14758,"ange of what is normally found in annotating speech transcripts for extractive summaries (0.1∼0.3; Marge 4 We use 10-second intervals rather than utterances as measuring units as the duration of utterances vary. If annotators all selected some content, or no content at all, from a 10-second interval, they are in agreement. et al., 2010), as annotating spoken text is a highly challenging task. We find that annotators tend to perceive the same region as salient but they may disagree as to which utterances should be included in the summary due to verbosity of spoken text. We refer the reader to (Artstein and Poesio, 2008) for interpretations and improvements to IAA. 4 Summarization Let X denote a sequence of spoken utterances from a segment of the transcript. Our summarizer aims to extract a subset of utterances Y ⊂ X that convey the essential content of the input. We experiment with an unsupervised summarizer that leverages vector-quantized variational autoencoders (VQVAE; van den Oord et al., 2017) to learn utterance representations and identifies summary utterances. The method was explored for opinion summarization (Angelidis et al., 2021) and machine translation (Prato et al., 2020). We are interested in u"
2021.emnlp-main.520,P18-1063,0,0.0232141,"ng baselines on the dataset and shed light on the task of livestream transcript summarization. Our contributions are as follows. • We create a detailed annotation interface and new benchmark dataset for automatic summarization of livestream transcripts. An informative preview of streamed content is of crucial importance to users when considering whether to hit play. Figure 2: A transcript snippet from “How to Create an Audio CD in Adobe Audition.” Most utterances are offtopic, except for the one marked in blue, suggesting the information density of livestream transcripts is low. et al., 2017; Chen and Bansal, 2018; Narayan et al., 2018a; Gehrmann et al., 2018; Cohan et al., 2018; Liu and Lapata, 2019; Fabbri et al., 2019; Bražinskas et al., 2020; Ladhak et al., 2020; Song et al., 2021). Despite their success, it remains unclear as to if and how the summarizers can be extended to spoken text, whose utterances may have very low information density. It is crucial to identify salient content from transcripts where a substantial number of utterances • We present StreamHover, a unsupervised model are devoted to informal chit-chats in an attempt to based on VQ-VAE to identify salient utterances connect with t"
2021.emnlp-main.520,N18-2097,1,0.84334,"transcript summarization. Our contributions are as follows. • We create a detailed annotation interface and new benchmark dataset for automatic summarization of livestream transcripts. An informative preview of streamed content is of crucial importance to users when considering whether to hit play. Figure 2: A transcript snippet from “How to Create an Audio CD in Adobe Audition.” Most utterances are offtopic, except for the one marked in blue, suggesting the information density of livestream transcripts is low. et al., 2017; Chen and Bansal, 2018; Narayan et al., 2018a; Gehrmann et al., 2018; Cohan et al., 2018; Liu and Lapata, 2019; Fabbri et al., 2019; Bražinskas et al., 2020; Ladhak et al., 2020; Song et al., 2021). Despite their success, it remains unclear as to if and how the summarizers can be extended to spoken text, whose utterances may have very low information density. It is crucial to identify salient content from transcripts where a substantial number of utterances • We present StreamHover, a unsupervised model are devoted to informal chit-chats in an attempt to based on VQ-VAE to identify salient utterances connect with the audience (Figure 2). We investifrom livestream transcripts to f"
2021.emnlp-main.520,D18-1409,0,0.0460202,"Missing"
2021.emnlp-main.520,P19-1102,0,0.0217495,"s are as follows. • We create a detailed annotation interface and new benchmark dataset for automatic summarization of livestream transcripts. An informative preview of streamed content is of crucial importance to users when considering whether to hit play. Figure 2: A transcript snippet from “How to Create an Audio CD in Adobe Audition.” Most utterances are offtopic, except for the one marked in blue, suggesting the information density of livestream transcripts is low. et al., 2017; Chen and Bansal, 2018; Narayan et al., 2018a; Gehrmann et al., 2018; Cohan et al., 2018; Liu and Lapata, 2019; Fabbri et al., 2019; Bražinskas et al., 2020; Ladhak et al., 2020; Song et al., 2021). Despite their success, it remains unclear as to if and how the summarizers can be extended to spoken text, whose utterances may have very low information density. It is crucial to identify salient content from transcripts where a substantial number of utterances • We present StreamHover, a unsupervised model are devoted to informal chit-chats in an attempt to based on VQ-VAE to identify salient utterances connect with the audience (Figure 2). We investifrom livestream transcripts to form preview sum- gate extractive rather tha"
2021.emnlp-main.520,D18-1443,0,0.0204063,"the task of livestream transcript summarization. Our contributions are as follows. • We create a detailed annotation interface and new benchmark dataset for automatic summarization of livestream transcripts. An informative preview of streamed content is of crucial importance to users when considering whether to hit play. Figure 2: A transcript snippet from “How to Create an Audio CD in Adobe Audition.” Most utterances are offtopic, except for the one marked in blue, suggesting the information density of livestream transcripts is low. et al., 2017; Chen and Bansal, 2018; Narayan et al., 2018a; Gehrmann et al., 2018; Cohan et al., 2018; Liu and Lapata, 2019; Fabbri et al., 2019; Bražinskas et al., 2020; Ladhak et al., 2020; Song et al., 2021). Despite their success, it remains unclear as to if and how the summarizers can be extended to spoken text, whose utterances may have very low information density. It is crucial to identify salient content from transcripts where a substantial number of utterances • We present StreamHover, a unsupervised model are devoted to informal chit-chats in an attempt to based on VQ-VAE to identify salient utterances connect with the audience (Figure 2). We investifrom livestr"
2021.emnlp-main.520,D19-1620,0,0.0182944,"eamer talks about Digital Painting with Maddy Bellwoar to create fairytale themed images. The annotators are asked to write a concise summary of this clip using their own words (Task A) and identify summary utterances (Task B). indicates the number of minutes since the beginning of the recording. When a user hovers over the thumbnail or scrolls past a video, we expect a textual summary to give a glimpse of the verbal content. This view of summarization leads us to annotate salient content across the video in an equally detailed manner. It naturally avoids lead bias that is ubiquitous in news (Grenander et al., 2019). We segment a video into 5-minute clips and annotate each clip for summary-worthy content. A clip contains an average of 51 utterances and 460 words. Due to time and budget constraints, we select 370 streamed video for summary annotation.3 Table 1 provides a detailed comparison of our annotated corpus with previous datasets, including Switchboard (Godfrey et al., 1992), ICSI (Janin et al., 2003) and AMI (Carletta et al., 2006) that contain both transcripts and human-annotated ex6459 3 Details of video selection are provided in Supplementary. Dataset Switchboard ICSI AMI StreamHover Type Telep"
2021.emnlp-main.520,2020.acl-main.437,0,0.0372448,"multiple as the latter are prone to generate hallucinated condimensions and discuss its strengths and weak- tent that does not exist in the source text (Cao nesses. Empirical results show that our method et al., 2017; Kryscinski et al., 2019; Lebanoff et al., 1 outperforms strong summarization baselines. 2019; Maynez et al., 2020). The problem could be exacerbated by ungrammatical spoken utterances and transcription errors. Instead, we consider VQ2 Related Work VAE, an unsupervised representation learning techClosed captions are often provided onscreen, turn- nique (van den Oord et al., 2017; Jin et al., 2020; ing streaming videos into text on an unprecedented Angelidis et al., 2021) for content extraction. Unsuscale (Besik, 2020). However, there are very few pervised training of the VQ-VAE model and its insummarization studies that attempt to generate text ference could potentially be performed at the same previews of streaming videos to help users browse time, allowing important utterances to be extracted or refind information that has been watched before. from a transcript segment on-the-fly during streamNeural text summarizers have focused primarily on ing, without interrupting the learning pr"
2021.emnlp-main.520,L18-1016,0,0.0280773,"017; Dong et al., 2018; Xu and Our annotations and source code are available at https: //github.com/ucfnlp/streamhover Durrett, 2019; Wang et al., 2020). 6458 Our work contributes to a refined understanding of transcript summarization, which is understudied relative to its importance and potential. The transcripts may be obtained from channels such as movies and TVs (Papalampidi et al., 2020; Chen et al., 2021), interviews (Zhu et al., 2021), multiparty meetings (Murray and Carenini, 2008; Wang and Cardie, 2013; Li et al., 2019b; Koay et al., 2020, 2021; Zhong et al., 2021), telephone speech (Kafle and Huenerfauth, 2018) and more. The main thrust distinguishing our work with others is the combination of a benchmark summarization dataset, novel summarization methods and a challenging new domain where salient content is scattered throughout the transcript and mixed with substantial chit-chats. We do not make use of video event detection or multi-modal fusion (Zhu et al., 2018; Palaskar et al., 2019; Li et al., 2020) as little information could be gleaned from videos that mirror the artists’ desktop. Instead, we focus on generating short descriptions from transcripts and leave for future work crossmodality resea"
2021.emnlp-main.520,D18-1208,0,0.0382294,"Missing"
2021.emnlp-main.520,D19-5413,1,0.902295,"Missing"
2021.emnlp-main.520,2020.acl-main.703,0,0.0764347,"o training, validation and test splits: • 3,884 clips (320 videos / 323 hours) in training, • 728 clips (25 videos / 61 hours) in validation, • 809 clips (25 videos / 67 hours) in test split. lead bias in news writing. Our setting is challenging as not only are there few utterances deemed to be summary-worthy but such utterances can occur anywhere in a video clip. Baselines. We compare StreamHover with stateof-the-art extractive and abstractive summarizers. The abstractive summarizers generate an abstract from the transcript of a clip without tuning.6 These include BART-large, BART-large-cnn (Lewis et al., 2020) and T5 (Raffel et al., 2020), which are some of the strongest performing neural abstractive summarizers that are pre-trained on language modeling and summarization tasks. The unsupervised extractive summarizers extract salient utterances from a clip. LexRank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004) are graph-based models that extract relevant sentences based on eigenvector centrality. SumBasic (Vanderwende et al., 2007) assigns higher scores to sentences containing frequently occurring content words. We further compare to a novel unsupervised graph-based summarization m"
2021.emnlp-main.520,D19-1370,0,0.154554,"t al., 2017; Tan compared to contemporary extractive methods (Ya1 sunaga et al., 2017; Dong et al., 2018; Xu and Our annotations and source code are available at https: //github.com/ucfnlp/streamhover Durrett, 2019; Wang et al., 2020). 6458 Our work contributes to a refined understanding of transcript summarization, which is understudied relative to its importance and potential. The transcripts may be obtained from channels such as movies and TVs (Papalampidi et al., 2020; Chen et al., 2021), interviews (Zhu et al., 2021), multiparty meetings (Murray and Carenini, 2008; Wang and Cardie, 2013; Li et al., 2019b; Koay et al., 2020, 2021; Zhong et al., 2021), telephone speech (Kafle and Huenerfauth, 2018) and more. The main thrust distinguishing our work with others is the combination of a benchmark summarization dataset, novel summarization methods and a challenging new domain where salient content is scattered throughout the transcript and mixed with substantial chit-chats. We do not make use of video event detection or multi-modal fusion (Zhu et al., 2018; Palaskar et al., 2019; Li et al., 2020) as little information could be gleaned from videos that mirror the artists’ desktop. Instead, we focus"
2021.emnlp-main.520,P19-1210,0,0.100629,"t al., 2017; Tan compared to contemporary extractive methods (Ya1 sunaga et al., 2017; Dong et al., 2018; Xu and Our annotations and source code are available at https: //github.com/ucfnlp/streamhover Durrett, 2019; Wang et al., 2020). 6458 Our work contributes to a refined understanding of transcript summarization, which is understudied relative to its importance and potential. The transcripts may be obtained from channels such as movies and TVs (Papalampidi et al., 2020; Chen et al., 2021), interviews (Zhu et al., 2021), multiparty meetings (Murray and Carenini, 2008; Wang and Cardie, 2013; Li et al., 2019b; Koay et al., 2020, 2021; Zhong et al., 2021), telephone speech (Kafle and Huenerfauth, 2018) and more. The main thrust distinguishing our work with others is the combination of a benchmark summarization dataset, novel summarization methods and a challenging new domain where salient content is scattered throughout the transcript and mixed with substantial chit-chats. We do not make use of video event detection or multi-modal fusion (Zhu et al., 2018; Palaskar et al., 2019; Li et al., 2020) as little information could be gleaned from videos that mirror the artists’ desktop. Instead, we focus"
2021.emnlp-main.520,2020.emnlp-main.752,0,0.0312864,"), interviews (Zhu et al., 2021), multiparty meetings (Murray and Carenini, 2008; Wang and Cardie, 2013; Li et al., 2019b; Koay et al., 2020, 2021; Zhong et al., 2021), telephone speech (Kafle and Huenerfauth, 2018) and more. The main thrust distinguishing our work with others is the combination of a benchmark summarization dataset, novel summarization methods and a challenging new domain where salient content is scattered throughout the transcript and mixed with substantial chit-chats. We do not make use of video event detection or multi-modal fusion (Zhu et al., 2018; Palaskar et al., 2019; Li et al., 2020) as little information could be gleaned from videos that mirror the artists’ desktop. Instead, we focus on generating short descriptions from transcripts and leave for future work crossmodality research. We describe our data annotation process in the following section. 3 Our Dataset We aim to create a large and representative corpus containing transcripts and summaries of streamed videos. We explore a leading social media platform (Behance.net) supported by Adobe Creative Cloud that features livestreams of creative work by artists and designers. The website boasts over 10 million users, who wa"
2021.emnlp-main.520,W04-1013,0,0.109384,"Missing"
2021.emnlp-main.520,P08-2051,0,0.0604866,"4, 2048}. The coefficient β used for commitment loss is set 7 The recent automatic metrics (Zhang et al., 2020; Sellam to 0.25 (Eq. (6)). These hyperparameters are tuned et al., 2020) have not been tested on speech transcripts. Spoken text contains filled pauses (um, uh, well), disfluencies (go-goon the validation set. We keep only utterances that go away), repetitions and verbal interruptions. ROUGE is the contain >5 words in consideration. The final train- only metric that has been validated to attain good correlation ing set contains 168,111 utterances. with human judgments on transcripts (Liu and Liu, 2008). 6463 FluCovRank Utterances • top left bottom / cloud studies today / find links to their original posts / hey jennifer saw the images / love the top left and bottom / info tab and i uploaded / colors are beautiful but im partial through colorful sky scenes / pretty large about 4000 by 4000 pixels / photo studies of today / moment 0 Hello good morning everybody welcome high foster highly art. 1 Hi Lisa, welcome everyone. 2 I hope you guys are having a good day so far. 3 Good to see you were going to be doing cloud studies today. 4 So if anybody is interested in joining in, if you want to work"
2021.emnlp-main.520,D19-1387,0,0.0905612,"tion. Our contributions are as follows. • We create a detailed annotation interface and new benchmark dataset for automatic summarization of livestream transcripts. An informative preview of streamed content is of crucial importance to users when considering whether to hit play. Figure 2: A transcript snippet from “How to Create an Audio CD in Adobe Audition.” Most utterances are offtopic, except for the one marked in blue, suggesting the information density of livestream transcripts is low. et al., 2017; Chen and Bansal, 2018; Narayan et al., 2018a; Gehrmann et al., 2018; Cohan et al., 2018; Liu and Lapata, 2019; Fabbri et al., 2019; Bražinskas et al., 2020; Ladhak et al., 2020; Song et al., 2021). Despite their success, it remains unclear as to if and how the summarizers can be extended to spoken text, whose utterances may have very low information density. It is crucial to identify salient content from transcripts where a substantial number of utterances • We present StreamHover, a unsupervised model are devoted to informal chit-chats in an attempt to based on VQ-VAE to identify salient utterances connect with the audience (Figure 2). We investifrom livestream transcripts to form preview sum- gate"
2021.emnlp-main.520,W10-0716,0,0.0167421,"Missing"
2021.emnlp-main.520,2020.acl-main.174,0,0.0249024,"is written text, including news articles, reviews, scien- also easier to tailor the model to specific domains tific papers and book chapters (See et al., 2017; Tan compared to contemporary extractive methods (Ya1 sunaga et al., 2017; Dong et al., 2018; Xu and Our annotations and source code are available at https: //github.com/ucfnlp/streamhover Durrett, 2019; Wang et al., 2020). 6458 Our work contributes to a refined understanding of transcript summarization, which is understudied relative to its importance and potential. The transcripts may be obtained from channels such as movies and TVs (Papalampidi et al., 2020; Chen et al., 2021), interviews (Zhu et al., 2021), multiparty meetings (Murray and Carenini, 2008; Wang and Cardie, 2013; Li et al., 2019b; Koay et al., 2020, 2021; Zhong et al., 2021), telephone speech (Kafle and Huenerfauth, 2018) and more. The main thrust distinguishing our work with others is the combination of a benchmark summarization dataset, novel summarization methods and a challenging new domain where salient content is scattered throughout the transcript and mixed with substantial chit-chats. We do not make use of video event detection or multi-modal fusion (Zhu et al., 2018; Pala"
2021.emnlp-main.520,P08-1054,0,0.137101,"Missing"
2021.emnlp-main.520,2020.findings-emnlp.1,0,0.0164094,"r the reader to (Artstein and Poesio, 2008) for interpretations and improvements to IAA. 4 Summarization Let X denote a sequence of spoken utterances from a segment of the transcript. Our summarizer aims to extract a subset of utterances Y ⊂ X that convey the essential content of the input. We experiment with an unsupervised summarizer that leverages vector-quantized variational autoencoders (VQVAE; van den Oord et al., 2017) to learn utterance representations and identifies summary utterances. The method was explored for opinion summarization (Angelidis et al., 2021) and machine translation (Prato et al., 2020). We are interested in using the method to account for domain characteristics of livestreams, which showcase new and creative work of artists and designers on their use of Photoshop, Illustrator, and other tools.5 VQ-VAE is a powerful framework for learning latent variable models using deep neural networks. It learns discrete vector representations for an utterance, which is then used to categorize the utterance along various dimensions. E.g., “Good morning Hi Everybody” suggests a greeting and opens up a dialogue; “I had probably 3 or 4 different customers on YouTube and ... on Facebook asked"
2021.emnlp-main.520,P17-1099,0,0.0301439,"w pervised training of the VQ-VAE model and its insummarization studies that attempt to generate text ference could potentially be performed at the same previews of streaming videos to help users browse time, allowing important utterances to be extracted or refind information that has been watched before. from a transcript segment on-the-fly during streamNeural text summarizers have focused primarily on ing, without interrupting the learning process. It is written text, including news articles, reviews, scien- also easier to tailor the model to specific domains tific papers and book chapters (See et al., 2017; Tan compared to contemporary extractive methods (Ya1 sunaga et al., 2017; Dong et al., 2018; Xu and Our annotations and source code are available at https: //github.com/ucfnlp/streamhover Durrett, 2019; Wang et al., 2020). 6458 Our work contributes to a refined understanding of transcript summarization, which is understudied relative to its importance and potential. The transcripts may be obtained from channels such as movies and TVs (Papalampidi et al., 2020; Chen et al., 2021), interviews (Zhu et al., 2021), multiparty meetings (Murray and Carenini, 2008; Wang and Cardie, 2013; Li et al.,"
2021.emnlp-main.520,2020.acl-main.704,0,0.0382223,"Missing"
2021.emnlp-main.520,P18-1062,0,0.125413,"ome of the strongest performing neural abstractive summarizers that are pre-trained on language modeling and summarization tasks. The unsupervised extractive summarizers extract salient utterances from a clip. LexRank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004) are graph-based models that extract relevant sentences based on eigenvector centrality. SumBasic (Vanderwende et al., 2007) assigns higher scores to sentences containing frequently occurring content words. We further compare to a novel unsupervised graph-based summarization method for speech transcripts: FluCovRank (Shang et al., 2018) groups utterances into clusters, generates an abstractive sentence from each cluster, then selects the best elements from abstractive sentences under a budget constraint. Finally, we compare our approach with the Quantized Transformer (Angelidis et al., 2021), which uses a clustering interpretation of the quantized space and two-step sampling algorithm to extract summary sentences from reviews. Settings. We use pretrained BERT-BASE as our embedder Embedθ (·). The model has 12 layers, 12 heads per layer and a hidden size (H) of 768. A 6layer Transformer decoder is used as the generator Generat"
2021.emnlp-main.520,D19-1298,0,0.0170271,"t, livestreaming platforms may not of general-domain models. We refrain from using fully meet the needs of their customers. sequential methods for utterance selection. First, 6457 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6457–6474 c November 7–11, 2021. 2021 Association for Computational Linguistics it is difficult to scale up sequential prediction to process transcripts that exceed the maximum allowed length, even with models that handle long text (Beltagy et al., 2020; Zhao et al., 2020). Second, sequential methods (Narayan et al., 2018b; Xiao and Carenini, 2019) may not give enough flexibility to select salient utterances on-the-fly when content is being streamed live, thus they are unsuitable for our case. There has been a shortage of annotated datasets that are necessary for livestream transcript summarization. We build a browser-based user interface for summary annotation that provides to the annotators a clip of the livestream recording alongside a synchronized display of the transcript. The interface allows annotators to conveniently label summary utterances and write an abstractive summary using their own words (Figure 3). With a total of 500 h"
2021.emnlp-main.520,2021.naacl-main.110,1,0.73255,"ew benchmark dataset for automatic summarization of livestream transcripts. An informative preview of streamed content is of crucial importance to users when considering whether to hit play. Figure 2: A transcript snippet from “How to Create an Audio CD in Adobe Audition.” Most utterances are offtopic, except for the one marked in blue, suggesting the information density of livestream transcripts is low. et al., 2017; Chen and Bansal, 2018; Narayan et al., 2018a; Gehrmann et al., 2018; Cohan et al., 2018; Liu and Lapata, 2019; Fabbri et al., 2019; Bražinskas et al., 2020; Ladhak et al., 2020; Song et al., 2021). Despite their success, it remains unclear as to if and how the summarizers can be extended to spoken text, whose utterances may have very low information density. It is crucial to identify salient content from transcripts where a substantial number of utterances • We present StreamHover, a unsupervised model are devoted to informal chit-chats in an attempt to based on VQ-VAE to identify salient utterances connect with the audience (Figure 2). We investifrom livestream transcripts to form preview sum- gate extractive rather than abstractive approaches maries. We evaluate the method across mul"
2021.emnlp-main.520,2020.acl-main.553,0,0.0121756,"ortant utterances to be extracted or refind information that has been watched before. from a transcript segment on-the-fly during streamNeural text summarizers have focused primarily on ing, without interrupting the learning process. It is written text, including news articles, reviews, scien- also easier to tailor the model to specific domains tific papers and book chapters (See et al., 2017; Tan compared to contemporary extractive methods (Ya1 sunaga et al., 2017; Dong et al., 2018; Xu and Our annotations and source code are available at https: //github.com/ucfnlp/streamhover Durrett, 2019; Wang et al., 2020). 6458 Our work contributes to a refined understanding of transcript summarization, which is understudied relative to its importance and potential. The transcripts may be obtained from channels such as movies and TVs (Papalampidi et al., 2020; Chen et al., 2021), interviews (Zhu et al., 2021), multiparty meetings (Murray and Carenini, 2008; Wang and Cardie, 2013; Li et al., 2019b; Koay et al., 2020, 2021; Zhong et al., 2021), telephone speech (Kafle and Huenerfauth, 2018) and more. The main thrust distinguishing our work with others is the combination of a benchmark summarization dataset, nove"
2021.emnlp-main.520,P13-1137,0,0.030033,"nd book chapters (See et al., 2017; Tan compared to contemporary extractive methods (Ya1 sunaga et al., 2017; Dong et al., 2018; Xu and Our annotations and source code are available at https: //github.com/ucfnlp/streamhover Durrett, 2019; Wang et al., 2020). 6458 Our work contributes to a refined understanding of transcript summarization, which is understudied relative to its importance and potential. The transcripts may be obtained from channels such as movies and TVs (Papalampidi et al., 2020; Chen et al., 2021), interviews (Zhu et al., 2021), multiparty meetings (Murray and Carenini, 2008; Wang and Cardie, 2013; Li et al., 2019b; Koay et al., 2020, 2021; Zhong et al., 2021), telephone speech (Kafle and Huenerfauth, 2018) and more. The main thrust distinguishing our work with others is the combination of a benchmark summarization dataset, novel summarization methods and a challenging new domain where salient content is scattered throughout the transcript and mixed with substantial chit-chats. We do not make use of video event detection or multi-modal fusion (Zhu et al., 2018; Palaskar et al., 2019; Li et al., 2020) as little information could be gleaned from videos that mirror the artists’ desktop. I"
2021.findings-acl.377,W09-1115,0,0.0449813,"understand quickly. Benchmarking The Task Instead of providing baselines for the proposed dataset, we organized a shared task and invited researchers to work on the new corpus. Section 6 describes the top-performing methods. By examining the challenges of the 4314 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4314–4320 August 1–6, 2021. ©2021 Association for Computational Linguistics dataset, we provide different analysis components. 2 Related Work 3 Prior work explored automatically generating presentation slides from documents such as scientific articles (Beamer and Girju, 2009; Wang et al., 2017; Hu and Wan, 2013; Shibata and Kurohashi, 2005; Sravanthi et al., 2009). These projects assume that a slide page is a summarization of some part of the paper, and many summarization methods have been proposed to improve the effectiveness. Other studies provide guidelines or alternatives to traditional designs to communicate a presentation’s content more effectively (Alley and Robertshaw, 2004; Jennings, 2009; Alley et al., 2006; Atkinson, 2005; Doumont, 2005). These create slides with sentence headlines and visual elements to reinforce ideas and increase the audience’s rete"
2021.findings-acl.377,D19-1371,0,0.0937184,", 2020), XLNet(Yang et al., 2019), XLMRoBERTa and BERT (Devlin et al., 2019). Comparing the results of all seven models, XLMRoBERTa performed the best. Besides pre-trained language models, UBRI-604 leveraged lexical features such as capitalized words and punctuation, for further improvement. DeepBlueAI team stood in second place (0.519), a RANK score that was 0.006 lower than the first team’s. DeepBlueAI introduced an ensemble Transformer-based model with two fully-connected layers combined with POS tags embedding and hand-crafted features. The ensemble model takes advantage of BERT, SciBERT (Beltagy et al., 2019) and ERNIE 2.0 pre-trained language models by taking the average of the scores predicted by these models. 7 CAD21 shared task: https://competitions.c odalab.org/competitions/27419 4317 Lastly, Cisco (Ghosh et al.), with a score 0.001 lower than the second team, ranked third. Cisco explored two approaches based on BiLSTM+ELMo (Shirani et al., 2019) architecture and Transformerbased pre-trained models with the base model of RoBERTa and XLNet. They enriched the ELMo contextual embedding in BiLSTM+ELMo model by incorporating a character-level BiLSTM Network. Their results show an increase of 0.026"
2021.findings-acl.377,P19-4007,0,0.0603597,"Missing"
2021.findings-acl.377,N19-1423,0,0.029042,"tures (such as words with capital letters and punctuation) were explored to improve the models’ performance. We describe and compare top-performing approaches next. The top-performing team, UBRI-604 (Hu et al., 2021), by proposing end-to-end Transformer-based approach, ranked in the first place with RANK score of (0.525). Different rich Transformer-based pre-trained language models were explored during the experiment, such as ALBERT (Lample and Conneau, 2019), GPT-2 (Radford and Wu, 2019), RoBERTa (Liu et al., 2019), ERNIE 2.0 (Sun et al., 2020), XLNet(Yang et al., 2019), XLMRoBERTa and BERT (Devlin et al., 2019). Comparing the results of all seven models, XLMRoBERTa performed the best. Besides pre-trained language models, UBRI-604 leveraged lexical features such as capitalized words and punctuation, for further improvement. DeepBlueAI team stood in second place (0.519), a RANK score that was 0.006 lower than the first team’s. DeepBlueAI introduced an ensemble Transformer-based model with two fully-connected layers combined with POS tags embedding and hand-crafted features. The ensemble model takes advantage of BERT, SciBERT (Beltagy et al., 2019) and ERNIE 2.0 pre-trained language models by taking th"
2021.findings-acl.377,2020.semeval-1.190,0,0.0154764,"s Selection for written text in visual media. The proposed model with an end-to-end sequence tagging architecture utilizes label distribution learning (LDL) (Geng, 2016) to handle the task’s subjectivity, and predicts emphasis scores for short written texts. They trained and evaluated the model against a collection of social media short texts from Adobe Spark2 . Later on in SemEval 2020 (Shirani et al., 2020b), 31 teams proposed novel approaches to model emphasis more effectively. The organizers augmented the social media dataset with a large dataset of short quotations. Top-performing teams (Huang et al., 2020; Morio et al., 2020; Singhal et al., 2020) used rich contextualized pre-trained language models such as ERNIE 2.0 (Sun et al., 2020), XLMRoBERTa (Conneau et al., 2019), XLNet (Yang et al., 2019), and T5 (Raffel et al., 2019). This study focuses on a new domain, presentation slides, where emphasis serves a different purpose than in social media. For social media the main purpose is to draw the audience’s attention, while for presentations, the main purpose is to help the audience better understand the content. Identifying emphasis in presentations brings unique 2 https://spark.adobe.com challe"
2021.findings-acl.377,2020.semeval-1.216,0,0.017681,"ten text in visual media. The proposed model with an end-to-end sequence tagging architecture utilizes label distribution learning (LDL) (Geng, 2016) to handle the task’s subjectivity, and predicts emphasis scores for short written texts. They trained and evaluated the model against a collection of social media short texts from Adobe Spark2 . Later on in SemEval 2020 (Shirani et al., 2020b), 31 teams proposed novel approaches to model emphasis more effectively. The organizers augmented the social media dataset with a large dataset of short quotations. Top-performing teams (Huang et al., 2020; Morio et al., 2020; Singhal et al., 2020) used rich contextualized pre-trained language models such as ERNIE 2.0 (Sun et al., 2020), XLMRoBERTa (Conneau et al., 2019), XLNet (Yang et al., 2019), and T5 (Raffel et al., 2019). This study focuses on a new domain, presentation slides, where emphasis serves a different purpose than in social media. For social media the main purpose is to draw the audience’s attention, while for presentations, the main purpose is to help the audience better understand the content. Identifying emphasis in presentations brings unique 2 https://spark.adobe.com challenges due to differen"
2021.findings-acl.377,N18-1202,0,0.022139,"on the evaluation phase used an ensemble of XLNet and RoBERTa, giving them third place. They boosted the model further in the Post Evaluation phase by ensembling XLNet and BiLSTM+ELMo models and incorporating hand-crafted features like POS and Keyphrase. We used the same baseline model (DLBiLSTM+ELMo) introduced in Shirani et al. (2019) to better show the challenges of PSED dataset. This model achieved RANK score of 0.475 (Table 4) which is 0.275 lower than the reported score by Shirani et al. (0.75).8 With a sequence-labeling architecture, this model utilizes ELMo contextualized embeddings (Peters et al., 2018) and two BiLSTM layers to label emphasis. The Kullback-Leibler Divergence (KL-DIV) (Kullback and Leibler, 1951) is used as the loss function during the training phase. 7 Discussion The PSED dataset contains slides with different lengths. To better examine how the length of slides can affect the prediction, we performed an error analysis to examine this relationship. We divided the test set into three groups based on the instances’ lengths, namely &lt;60, 60–90, and >90 tokens. Then we computed the average Matchm scores over all shared task submissions, four in total, for every example in each gro"
2021.findings-acl.377,I05-1066,0,0.107551,"g baselines for the proposed dataset, we organized a shared task and invited researchers to work on the new corpus. Section 6 describes the top-performing methods. By examining the challenges of the 4314 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4314–4320 August 1–6, 2021. ©2021 Association for Computational Linguistics dataset, we provide different analysis components. 2 Related Work 3 Prior work explored automatically generating presentation slides from documents such as scientific articles (Beamer and Girju, 2009; Wang et al., 2017; Hu and Wan, 2013; Shibata and Kurohashi, 2005; Sravanthi et al., 2009). These projects assume that a slide page is a summarization of some part of the paper, and many summarization methods have been proposed to improve the effectiveness. Other studies provide guidelines or alternatives to traditional designs to communicate a presentation’s content more effectively (Alley and Robertshaw, 2004; Jennings, 2009; Alley et al., 2006; Atkinson, 2005; Doumont, 2005). These create slides with sentence headlines and visual elements to reinforce ideas and increase the audience’s retention of the information during presentation. Many applications pr"
2021.findings-acl.377,P19-1112,1,0.922551,"commendations to enhance the slides’ communication power could improve authoring even more. Our goal is predicting emphasis words in presentation slides. Emphasis uses special formatting like boldface or italics to make words stand out. Well-designed emphasis can significantly increase the viewers’ retention by guiding their focus to a few words (Alley and Robertshaw, 2004). Instead of reading the entire slide, they can read only the emphasized parts, keeping their attention on the speaker and their speech, as Figure 1 illustrates.1 The Emphasis Selection (ES) task was initially introduced by Shirani et al. (2019) with a focus on 1 Source: Web Marketing for Fundraisers: Get Found, Get Traffic, Get Ahead (http://www.fundraising123.o rg/files/web-marketing-for-fundraisers-g et-found-get-traffic-get-ahead652.pdf) short written text in social media, and later became a SemEval 2020 task (Shirani et al., 2020b). In this paper, we focus on presentation slides, introducing a new corpus as well as automated emphasis prediction approaches. We are among the first to use the content of the slides to provide automated design assistance. Task Characteristics Emphasis selection poses new challenges specific to presen"
2021.findings-acl.377,2020.acl-main.762,1,0.795688,"rs’ retention by guiding their focus to a few words (Alley and Robertshaw, 2004). Instead of reading the entire slide, they can read only the emphasized parts, keeping their attention on the speaker and their speech, as Figure 1 illustrates.1 The Emphasis Selection (ES) task was initially introduced by Shirani et al. (2019) with a focus on 1 Source: Web Marketing for Fundraisers: Get Found, Get Traffic, Get Ahead (http://www.fundraising123.o rg/files/web-marketing-for-fundraisers-g et-found-get-traffic-get-ahead652.pdf) short written text in social media, and later became a SemEval 2020 task (Shirani et al., 2020b). In this paper, we focus on presentation slides, introducing a new corpus as well as automated emphasis prediction approaches. We are among the first to use the content of the slides to provide automated design assistance. Task Characteristics Emphasis selection poses new challenges specific to presentation slides. They can have different structures, and authors may follow traditional styles, or modern styles with more visual content. Slides cover a wide range of topics, from technical, marketing, and legal presentations to children’s illustrations. The requirement to generalize to differen"
2021.findings-acl.377,2020.semeval-1.184,1,0.745753,"rs’ retention by guiding their focus to a few words (Alley and Robertshaw, 2004). Instead of reading the entire slide, they can read only the emphasized parts, keeping their attention on the speaker and their speech, as Figure 1 illustrates.1 The Emphasis Selection (ES) task was initially introduced by Shirani et al. (2019) with a focus on 1 Source: Web Marketing for Fundraisers: Get Found, Get Traffic, Get Ahead (http://www.fundraising123.o rg/files/web-marketing-for-fundraisers-g et-found-get-traffic-get-ahead652.pdf) short written text in social media, and later became a SemEval 2020 task (Shirani et al., 2020b). In this paper, we focus on presentation slides, introducing a new corpus as well as automated emphasis prediction approaches. We are among the first to use the content of the slides to provide automated design assistance. Task Characteristics Emphasis selection poses new challenges specific to presentation slides. They can have different structures, and authors may follow traditional styles, or modern styles with more visual content. Slides cover a wide range of topics, from technical, marketing, and legal presentations to children’s illustrations. The requirement to generalize to differen"
2021.findings-acl.377,2020.semeval-1.217,0,0.0156287,"edia. The proposed model with an end-to-end sequence tagging architecture utilizes label distribution learning (LDL) (Geng, 2016) to handle the task’s subjectivity, and predicts emphasis scores for short written texts. They trained and evaluated the model against a collection of social media short texts from Adobe Spark2 . Later on in SemEval 2020 (Shirani et al., 2020b), 31 teams proposed novel approaches to model emphasis more effectively. The organizers augmented the social media dataset with a large dataset of short quotations. Top-performing teams (Huang et al., 2020; Morio et al., 2020; Singhal et al., 2020) used rich contextualized pre-trained language models such as ERNIE 2.0 (Sun et al., 2020), XLMRoBERTa (Conneau et al., 2019), XLNet (Yang et al., 2019), and T5 (Raffel et al., 2019). This study focuses on a new domain, presentation slides, where emphasis serves a different purpose than in social media. For social media the main purpose is to draw the audience’s attention, while for presentations, the main purpose is to help the audience better understand the content. Identifying emphasis in presentations brings unique 2 https://spark.adobe.com challenges due to differences in topic, length, a"
2021.findings-emnlp.395,D17-1238,0,0.0772919,"lease the pre-trained models to compute QACE.1 1 Introduction Image captioning is a task that aims to generate a description containing the main content of a given image. The field of caption generation is prolific (Vinyals et al., 2015; Anderson et al., 2018), and it is, therefore, important to provide reliable evaluation metrics to compare the systems. Most of the prior works still report n-gram similarity metrics such as BLEU (Papineni et al., 2002) or CIDEr (Vedantam et al., 2015). However, these ngram similarity metrics often fail to capture the semantic errors in the generated captions (Novikova et al., 2017). To overcome this limitation, we propose QACE, a radically different evaluation framework from n-gram metrics. QACE first generates questions about the candidate caption, and then checks if the 1 https://github.com/hwanheelee1993/QACE answers are consistent w.r.t. either the reference or the source image. We depict QACE in Figure 1. Specifically, we propose two variants of QACE, depending on what content the evaluated caption is compared to: QACERef when it is compared to the reference, and QACEImg when it is compared to the source image. QACEImg has the desired feature to be reference-less,"
2021.findings-emnlp.395,P02-1040,0,0.113774,"metric, QACEImg is multi-modal, reference-less, and explainable. Our experiments show that QACEImg compares favorably w.r.t. other reference-less metrics. We will release the pre-trained models to compute QACE.1 1 Introduction Image captioning is a task that aims to generate a description containing the main content of a given image. The field of caption generation is prolific (Vinyals et al., 2015; Anderson et al., 2018), and it is, therefore, important to provide reliable evaluation metrics to compare the systems. Most of the prior works still report n-gram similarity metrics such as BLEU (Papineni et al., 2002) or CIDEr (Vedantam et al., 2015). However, these ngram similarity metrics often fail to capture the semantic errors in the generated captions (Novikova et al., 2017). To overcome this limitation, we propose QACE, a radically different evaluation framework from n-gram metrics. QACE first generates questions about the candidate caption, and then checks if the 1 https://github.com/hwanheelee1993/QACE answers are consistent w.r.t. either the reference or the source image. We depict QACE in Figure 1. Specifically, we propose two variants of QACE, depending on what content the evaluated caption is"
2021.findings-emnlp.395,P18-2124,0,0.0455675,"ly proposed to evaluate abstractive summarization. However, all those prior works are limited to text-to-text evaluation, while our work develops a multi-modal metric. 3 What type of bus is driving down a street? <img> Textual Embedding … Visual Embedding Encoder‐Decoder red double decker bus Figure 2: The overview of Visual-T5, an abstractive VQA model. We embed questions with additional special separation token and concatenate the visual embeddings to make inputs for T5. didate caption. Our QG model is a text-to-text generation model (i.e., T5 (Raffel et al., 2020)), fine-tuned on SQuAD v2 (Rajpurkar et al., 2018) to generate answer-aware questions. Given a caption, we extract possible answer span; in particular, we focus on extracting noun phrases since they mostly contain salient information and can be easily foiled (Shekhar et al., 2017). We argue that questions generated on this salient information should be answered similarly from the image or the captions if they share the same information. QACE We propose QACE, which is a QG- and QA-based framework for evaluating an image caption. As shown in Figure 1, QACE first extracts answer candidates (i.e., 1) wave, 2) top, 3) surfboard) from a candidate c"
2021.findings-emnlp.395,D16-1264,0,0.0850139,"of M noun phrases of x. Then, we compare the answers for each question in Q on x with the answers on the reference source. We introduce two QACE variants, QACERef for which the reference caption is compared, and QACEImg for which the source image is compared. Using QG and QA, we compute QACERef and QACEImg as follows: QACE = ΣM i=1 f (QA(qi , x), QA(qi , ctx)) (1) , M where ctx corresponds to the image for QACEImg and the gold reference for QACERef , f (A1 , A2 ) is the function that measures the similarity between two answers A1 and A2 . The standard metric in QA is the F1, as introduced by Rajpurkar et al. (2016). However, two abstractive answers can be similar but written in two different ways, limiting the effectiveness of a naive F1. Hence, in addition to the F1, we propose to use the BERTScore. Finally, we also complete the similarity metrics using the answerability of the questions for function f , in order to measure whether the question is answerable. The answerability corresponds to 1−Punanswerable , where Punanswerable is the probability attributed by the model to the token unanswerable.2 To consider all the different aspects, we use the average of three values computed using each function as"
2021.findings-emnlp.395,2020.acl-main.450,0,0.0430631,"on et al., 2016) metric is based on scene graph, while more recently, BERTScore (Zhang et al., 2019) compute the similarity of the contextualized embeddings. Different from prior works, we are the first to use Question Generation (QG) and Question Answering (QA) to evaluate the image captions. Question and Answering for Evaluation Fisch et al. (2020) proposes a new method to generate informal captions that can answer the visual questions. In our work, we focus on caption evaluation using the QA systems, not on generating the captions. Several QA-based evaluation metrics (Scialom et al., 2019; Wang et al., 2020) are recently proposed to evaluate abstractive summarization. However, all those prior works are limited to text-to-text evaluation, while our work develops a multi-modal metric. 3 What type of bus is driving down a street? <img> Textual Embedding … Visual Embedding Encoder‐Decoder red double decker bus Figure 2: The overview of Visual-T5, an abstractive VQA model. We embed questions with additional special separation token and concatenate the visual embeddings to make inputs for T5. didate caption. Our QG model is a text-to-text generation model (i.e., T5 (Raffel et al., 2020)), fine-tuned on"
2021.findings-emnlp.395,2020.emnlp-demos.6,0,0.0566555,"Missing"
2021.findings-emnlp.395,2020.inlg-1.39,1,0.754856,"e Visual Question Answering When no reference captions are available, one of the most important parts of QACE is the VQA The goal of this component is to generate ques- model that can produce correct answers. To move tions that ask the primary information of the can- beyond VQA as a classification task, we are the 4632 3.1 Question Generation first, to the best of our knowledge, to develop an abstractive VQA model that can generate free-form answers. Specifically, we enable multimodal encoding for T5, inspired by the previous works on adapting pre-trained language models for multimodal tasks (Scialom et al., 2020). We illustrate our proposed Visual-T5 in Figure 2. Based on default T5 architecture, Visual-T5 has an additional visual embedding layer that encodes regional features of the image from Faster RCNN (Ren et al., 2015). This linear layer maps detection features to 768 dimensions, same as the dimension of textual embedding. This 768d features are therefore considered as a standard token in Visual-T5, which can encode an image and a question together. We provide more details in Appendix. 3.4 QACE Metric For a given candidate caption x, We use QG to generate questions Q = (q1 , ..., qM ) for all of"
2021.findings-emnlp.395,D19-1320,1,0.78084,"ilarity. SPICE (Anderson et al., 2016) metric is based on scene graph, while more recently, BERTScore (Zhang et al., 2019) compute the similarity of the contextualized embeddings. Different from prior works, we are the first to use Question Generation (QG) and Question Answering (QA) to evaluate the image captions. Question and Answering for Evaluation Fisch et al. (2020) proposes a new method to generate informal captions that can answer the visual questions. In our work, we focus on caption evaluation using the QA systems, not on generating the captions. Several QA-based evaluation metrics (Scialom et al., 2019; Wang et al., 2020) are recently proposed to evaluate abstractive summarization. However, all those prior works are limited to text-to-text evaluation, while our work develops a multi-modal metric. 3 What type of bus is driving down a street? <img> Textual Embedding … Visual Embedding Encoder‐Decoder red double decker bus Figure 2: The overview of Visual-T5, an abstractive VQA model. We embed questions with additional special separation token and concatenate the visual embeddings to make inputs for T5. didate caption. Our QG model is a text-to-text generation model (i.e., T5 (Raffel et al., 2"
2021.findings-emnlp.395,P17-1024,0,0.0211061,"ing … Visual Embedding Encoder‐Decoder red double decker bus Figure 2: The overview of Visual-T5, an abstractive VQA model. We embed questions with additional special separation token and concatenate the visual embeddings to make inputs for T5. didate caption. Our QG model is a text-to-text generation model (i.e., T5 (Raffel et al., 2020)), fine-tuned on SQuAD v2 (Rajpurkar et al., 2018) to generate answer-aware questions. Given a caption, we extract possible answer span; in particular, we focus on extracting noun phrases since they mostly contain salient information and can be easily foiled (Shekhar et al., 2017). We argue that questions generated on this salient information should be answered similarly from the image or the captions if they share the same information. QACE We propose QACE, which is a QG- and QA-based framework for evaluating an image caption. As shown in Figure 1, QACE first extracts answer candidates (i.e., 1) wave, 2) top, 3) surfboard) from a candidate caption and generates corresponding questions. With these questions, visual-QA (VQA) and textual-QA (TQA) models answers given their context (i.e., image and reference x ˆ). By comparing the answers from each source, we can directly"
2021.naacl-main.170,W05-0909,0,0.449127,"hat have been used to evaluate GenQA systems. BLEU is a popular evaluation metric for generated text based on n-gram precision. BLEU scores a candidate by counting the number present in the reference among the n-gram of the candidate. In general, n varies from 1 to 4, and the scores for varying n are aggregated with a geometric mean. ROUGE is a set of evaluation metrics used for automatic text generation such as summarization and machine translation. Typically, most studies use ROUGE-L, which is a F-measure based on the longest common subsequence between a candidate and the reference. METEOR (Banerjee and Lavie, 2005) is an F1 score of a set of unigram alignments. METEOR has a unique property that it considers stemmed words, synonyms, and paraphrases, as well as the standard exact word matches. CIDER (Vedantam et al., 2015) is a consensusbased evaluation metric that is designed for a high correlation with human judgment in the image captioning problem. CIDEr uses Term FrequencyInverse Document Frequency (TF-IDF) weights for human-like evaluation. BERTScore is a recently proposed text evaluation metric that use pre-trained representations from BERT (Devlin et al., 2019). BERTScore first computes the context"
2021.naacl-main.170,D18-1454,0,0.0373618,"Missing"
2021.naacl-main.170,D19-1255,0,0.0186571,"2019). attention from the natural language processing comFor instance, in the example in Figure 1 from munity. Recently, research on QA systems has the MS-MARCO (Bajaj et al., 2016), the generated reached the stage of generating free-form answers, answer receives a high score on BLEU-1 (0.778) called GenQA, beyond extracting the answer to a and ROUGE-L (0.713) due to the many overlaps given question from the context (Yin et al., 2016; of words with those in the reference. However, huSong et al., 2017; Bauer et al., 2018; Nishida et al., mans assign a low score of 0.063 on the scale from 2019; Bi et al., 2019, 2020). However, as a bot- 0 to 1 due to the mismatch of critical information. tleneck in developing GenQA models, there are As in this example, we find that existing metrics ofno proper automatic metrics to evaluate generated ten fail to capture the correctness of the generated answers (Chen et al., 2019). answer that considers the key information for the In evaluating a GenQA model, it is essential to question. consider whether a generated response correctly To overcome this shortcoming of the existing contains vital information to answer the question. metrics, we propose a new metric calle"
2021.naacl-main.170,D19-5817,0,0.3038,"QA, beyond extracting the answer to a and ROUGE-L (0.713) due to the many overlaps given question from the context (Yin et al., 2016; of words with those in the reference. However, huSong et al., 2017; Bauer et al., 2018; Nishida et al., mans assign a low score of 0.063 on the scale from 2019; Bi et al., 2019, 2020). However, as a bot- 0 to 1 due to the mismatch of critical information. tleneck in developing GenQA models, there are As in this example, we find that existing metrics ofno proper automatic metrics to evaluate generated ten fail to capture the correctness of the generated answers (Chen et al., 2019). answer that considers the key information for the In evaluating a GenQA model, it is essential to question. consider whether a generated response correctly To overcome this shortcoming of the existing contains vital information to answer the question. metrics, we propose a new metric called KPQAThere exist several n-gram similarity metrics such metric for evaluating GenQA systems. To derive ∗ the metric, we first develop Keyphrase Predictor This research was done while the author was affiliated with Adobe Research. for Question Answering (KPQA). KPQA computes 2105 Proceedings of the 2021 Con"
2021.naacl-main.170,N19-1423,0,0.0275875,"idate and the reference. METEOR (Banerjee and Lavie, 2005) is an F1 score of a set of unigram alignments. METEOR has a unique property that it considers stemmed words, synonyms, and paraphrases, as well as the standard exact word matches. CIDER (Vedantam et al., 2015) is a consensusbased evaluation metric that is designed for a high correlation with human judgment in the image captioning problem. CIDEr uses Term FrequencyInverse Document Frequency (TF-IDF) weights for human-like evaluation. BERTScore is a recently proposed text evaluation metric that use pre-trained representations from BERT (Devlin et al., 2019). BERTScore first computes the contextual embeddings for given references and candidates independently with BERT, and then computes pairwise cosine similarity scores. When computing similarity, BERTScore adopts Inverse Document Frequency (IDF) to apply importance weighting. 3 Proposed Metric for Evaluating GenQA To build a better metric for GenQA, we first propose KPQA. By considering the question, the KPQA assigns different weights to each token in the answer sentence such that salient tokens receive a high value. We then integrate the KPQA into existing metrics to make them evaluate correctn"
2021.naacl-main.170,P19-1346,0,0.0737762,"n “-KP"" metric for all of the three variants. These results show that training keyphrase predictor to find the short answer candidate in the sentence is effective for capturing the key information in the generated answer, but it is more effective when the question information is integrated. 5.4 Analysis is lower for BERTScore. We speculate that this is because the original BERTScore uses IDF-based importance weighting, unlike other metrics. Multiple Sentence Answers: Most of the answers in MS-MARCO and AVSD consist of single sentences, but the answers for GenQA can be multiple sentences like (Fan et al., 2019). To verify our KPQA-metric on multiple sentence answers, we collect additional 100 human judgments for the generated answer whose answers are multiple sentences in the MS-MARCO like the example in Figure 5, and evaluate the various metrics on this dataset. As shown in Table 6, our KPQA integrated metric shows still higher correlations than other metrics. We observe that the gap between KPQA integrated metrics and existing metrics is relatively lower than that of Table 3. We speculate this is because many of the multiple sentence answers are DESCRIPTION type answers whose keyphrases are someti"
2021.naacl-main.170,W18-2605,0,0.0175729,"useful for comparing the generated answers from different models. 6 Related Work are due to higher weights on units such as “million"" or “years."" There exist a total of ten error cases for One important next step for current QA systems this type, and we believe that there is room for is to generate answers in natural language for a improvement with regard to these errors through given question and context. Following this interest, post-processing. In the case of the DESCRIPTION several generative (abstractive) QA datasets (Bajaj question type, 17 out of 31 cases are due to inap- et al., 2016; He et al., 2018; Koˇciský et al., 2018; propriate importance weights. We speculate this Fan et al., 2019), where the answer is not necesresult is because the keyphrases for the answers sarily in the passage, have recently been released. to questions belonging to the DESCRIPTION type Since the task is to generate natural language for are sometimes vague; thus, the entire answer needs the given question, the QA system is often trained to be considered when it is evaluated. with seq2seq (Sutskever et al., 2014) objective simi2112 larly to other natural generation tasks such as neural machine translation. Hence,"
2021.naacl-main.170,2020.tacl-1.5,0,0.0172133,"utput example of KPQA. KPQA classifies whether each word in the answer sentences is in the answer span for a given question. We use the output probability KPW as an importance weight to be integrated into KPQA-metric. since these sentences are short summaries for the given question. Specifically, for a single-hop QA dataset such as SQuAD, we pick a single sentence that includes answer-span as the answer sentence. For the answers in a multi-hop QA dataset such as HotpotQA (Yang et al., 2018b), there are multiple supporting sentences for the single answer span. For these cases, we use SpanBERT (Joshi et al., 2020) to resolve the coreferences in the paragraphs and extract all of the supporting sentences to compose answer sentences. The {question, [SEP], answer-sentences} is then fed into the KPQA to classify the answer-span, which is a set of salient tokens, in the given answer-sentences considering the question. 3.2 KPQA Metric Since KPQA’s training process allows KPQA to find essential words in the answer sentences to a given question, we use a pre-trained KPQA to get the importance weights that are useful for evaluaterated answer. As shown in Figure 1, there exist keywords or keyphrases that are cons"
2021.naacl-main.170,Q18-1023,0,0.0408319,"Missing"
2021.naacl-main.170,D19-1051,0,0.0494586,"Missing"
2021.naacl-main.170,P19-1564,0,0.012562,"metrics for general form of GenQA. To fill this gap, we collect the human judgments of correctness for model generated answers on two other GenQA datasets, MS-MARCO and AVSD, which have longer answers than NarrativeQA and SemEval as shown in Table 1. For the MS-MARCO, we use the Natural Language Generation (NLG) subset, which has more abstractive and longer answers than the Q&A subset. GenQA Models: For each of the two datasets, we first generate answers for questions on validation sets using two trained GenQA models: UniLM (Dong et al., 2019) and MHPGM (Bauer et al., 2018) for MS-MARCO, MTN (Le et al., 2019) and AMF (Alamri et al., 2018; Hori et al., 2017) for AVSD. Details on these QA models are in Appendix. After training, we select 1k samples for each dataset in the validation set. Specifically, we first randomly pick the 500 questions in the validation set of each dataset and collect the corresponding model generated answers for each model so that we have two generated answers for each sample. Therefore, we collect a total of 1k samples, two different answers for 500 questions for each dataset. Also, we discard samples if one of two GenQA models exactly generates the ground-truth answer since"
2021.naacl-main.170,W04-1013,0,0.238546,"many steps are involved in a hypothesis test? Reference Answer : Four steps are involved in a hypothesis test. Generated Answer : There are seven steps involved in a hypothesis test . Human Judgment : 0.063 BLEU-1 : 0.778 ROUGE-L : 0.713 BLEU-1-KPQA : 0.057 ROUGE-L-KPQA : 0.127 Figure 1: An example from MS-MARCO (Bajaj et al., 2016) where widely used n-gram similarity metrics does not align with human judgments of correctness. On the other hand, our KPQA-metrics focus on the key information and give low scores to incorrect answers similar to humans. as BLEU (Papineni et al., 2002) and ROUGEL (Lin, 2004), that measure the word overlaps between the generated response and the reference answer; however, these metrics are insufficient to 1 Introduction evaluate a GenQA system (Yang et al., 2018a; Chen Question answering (QA) has received consistent et al., 2019). attention from the natural language processing comFor instance, in the example in Figure 1 from munity. Recently, research on QA systems has the MS-MARCO (Bajaj et al., 2016), the generated reached the stage of generating free-form answers, answer receives a high score on BLEU-1 (0.778) called GenQA, beyond extracting the answer to a and"
2021.naacl-main.170,D16-1230,0,0.0631099,"Missing"
2021.naacl-main.170,N06-1048,0,0.103548,"-gram metrics including BLEU were originally developed to evaluate machine translation and previous works (Liu et al., 2016; Nema and Khapra, 2018; Kryscinski et al., 2019) have shown that these metrics have poor correlations with human judgments in other language generation tasks such as dialogue systems. As with other text generation systems, for GenQA, it is difficult to assess the performance through ngram metrics. Especially, n-gram similarity metrics can give a high score to a generated answer that is incorrect but shares many unnecessary words with the reference answer. Previous works (Marton and Radul, 2006; Yang et al., 2018a; Chen et al., 2019) have pointed out the difficulty of similar problems and studied automated metrics for evaluating QA systems. Inspired by these works, we focus on studying and developing evaluation metrics for GenQA datasets that have more abstractive and diverse answers. We analyze the problem of using existing n-gram similarity metrics across multiple GenQA datasets and propose alternative metrics for GenQA. 7 Conclusion In this paper, we create high-quality human judgments on two GenQA datasets, MS-MARCO and AVSD, and show that previous evaluation metrics are poorly"
2021.naacl-main.170,D18-1429,0,0.0173087,"natural language for are sometimes vague; thus, the entire answer needs the given question, the QA system is often trained to be considered when it is evaluated. with seq2seq (Sutskever et al., 2014) objective simi2112 larly to other natural generation tasks such as neural machine translation. Hence, researchers often use n-gram based similarity metrics such as BLEU to evaluate the GenQA systems, following other natural language generation tasks. However, most of these n-gram metrics including BLEU were originally developed to evaluate machine translation and previous works (Liu et al., 2016; Nema and Khapra, 2018; Kryscinski et al., 2019) have shown that these metrics have poor correlations with human judgments in other language generation tasks such as dialogue systems. As with other text generation systems, for GenQA, it is difficult to assess the performance through ngram metrics. Especially, n-gram similarity metrics can give a high score to a generated answer that is incorrect but shares many unnecessary words with the reference answer. Previous works (Marton and Radul, 2006; Yang et al., 2018a; Chen et al., 2019) have pointed out the difficulty of similar problems and studied automated metrics f"
2021.naacl-main.170,P19-1220,0,0.0319396,"Missing"
2021.naacl-main.170,S18-1119,0,0.0617337,"Missing"
2021.naacl-main.170,W16-0106,0,0.0300988,"erence answer; however, these metrics are insufficient to 1 Introduction evaluate a GenQA system (Yang et al., 2018a; Chen Question answering (QA) has received consistent et al., 2019). attention from the natural language processing comFor instance, in the example in Figure 1 from munity. Recently, research on QA systems has the MS-MARCO (Bajaj et al., 2016), the generated reached the stage of generating free-form answers, answer receives a high score on BLEU-1 (0.778) called GenQA, beyond extracting the answer to a and ROUGE-L (0.713) due to the many overlaps given question from the context (Yin et al., 2016; of words with those in the reference. However, huSong et al., 2017; Bauer et al., 2018; Nishida et al., mans assign a low score of 0.063 on the scale from 2019; Bi et al., 2019, 2020). However, as a bot- 0 to 1 due to the mismatch of critical information. tleneck in developing GenQA models, there are As in this example, we find that existing metrics ofno proper automatic metrics to evaluate generated ten fail to capture the correctness of the generated answers (Chen et al., 2019). answer that considers the key information for the In evaluating a GenQA model, it is essential to question. cons"
2021.naacl-main.170,P02-1040,0,0.109942,"f four steps. , ... Question : How many steps are involved in a hypothesis test? Reference Answer : Four steps are involved in a hypothesis test. Generated Answer : There are seven steps involved in a hypothesis test . Human Judgment : 0.063 BLEU-1 : 0.778 ROUGE-L : 0.713 BLEU-1-KPQA : 0.057 ROUGE-L-KPQA : 0.127 Figure 1: An example from MS-MARCO (Bajaj et al., 2016) where widely used n-gram similarity metrics does not align with human judgments of correctness. On the other hand, our KPQA-metrics focus on the key information and give low scores to incorrect answers similar to humans. as BLEU (Papineni et al., 2002) and ROUGEL (Lin, 2004), that measure the word overlaps between the generated response and the reference answer; however, these metrics are insufficient to 1 Introduction evaluate a GenQA system (Yang et al., 2018a; Chen Question answering (QA) has received consistent et al., 2019). attention from the natural language processing comFor instance, in the example in Figure 1 from munity. Recently, research on QA systems has the MS-MARCO (Bajaj et al., 2016), the generated reached the stage of generating free-form answers, answer receives a high score on BLEU-1 (0.778) called GenQA, beyond extract"
2021.naacl-main.170,D16-1264,0,0.344673,".., xn ) and generated answer X x1 , shown in Figure 3, KPQA is a BERT-based (Devlin ..., x ˆm ) using pre-trained KPQA. We provide each et al., 2019) classifier that predicts salient tokens in the answer sentences depending on the ques- pair {question, generated answer} and {question, reference answer} to pre-trained KPQA and get the tion. We regard it as a multi-class classification output of the softmax layer. We define these parts task where each token is a single class. To train as KeyPhrase Weight (KPW) as shown in Figure 3. KPQA, we first prepare extractive QA datasets ˆ such as SQuAD (Rajpurkar et al., 2016), which con- We note that KPW(Q,X) = (w1 , ..., wm ) is an imporˆ for a given sist of {passage, question, answer-span}. We trans- tance weight of generated answer X form these datasets into pairs of {answer-sentences, question Q. These weights reflect the importance question, answer-span}. We extract the answer- of each token for evaluating the correctness. sentences that contain answer-span in the passage We then compute KPQA-metric by incorporat2107 ing the KPW into several existing metrics modifying the precision and recall to compute the weighted similarity. (Q,X) KP QA RBERT = BLEU-1-KPQA"
2021.naacl-main.170,D18-1259,0,0.43377,"Human Judgment : 0.063 BLEU-1 : 0.778 ROUGE-L : 0.713 BLEU-1-KPQA : 0.057 ROUGE-L-KPQA : 0.127 Figure 1: An example from MS-MARCO (Bajaj et al., 2016) where widely used n-gram similarity metrics does not align with human judgments of correctness. On the other hand, our KPQA-metrics focus on the key information and give low scores to incorrect answers similar to humans. as BLEU (Papineni et al., 2002) and ROUGEL (Lin, 2004), that measure the word overlaps between the generated response and the reference answer; however, these metrics are insufficient to 1 Introduction evaluate a GenQA system (Yang et al., 2018a; Chen Question answering (QA) has received consistent et al., 2019). attention from the natural language processing comFor instance, in the example in Figure 1 from munity. Recently, research on QA systems has the MS-MARCO (Bajaj et al., 2016), the generated reached the stage of generating free-form answers, answer receives a high score on BLEU-1 (0.778) called GenQA, beyond extracting the answer to a and ROUGE-L (0.713) due to the many overlaps given question from the context (Yin et al., 2016; of words with those in the reference. However, huSong et al., 2017; Bauer et al., 2018; Nishida e"
2021.naacl-main.274,P15-1017,0,0.0851188,"Missing"
2021.naacl-main.274,W09-3208,1,0.881814,"rly describes the event, vir- not actually happen (i.e., its modality attribute is tually all previous approaches employ features re- OTHER). Therefore, our model should be able to avoid the mistake if it utilizes additional symbolic lated to event triggers in one form or another. To features such as the modality attribute in this case. achieve better performance, many methods also There are several previous methods that use conneed to use a variety of additional symbolic featextual embeddings together with type-based or tures such as event types, attributes, and arguments (Chen et al., 2009; Chen and Ji, 2009; Zhang et al., argument-based information (Lu et al., 2020; Yu 2015; Sammons et al., 2015; Lu and Ng, 2016; et al., 2020). For example, Lu et al. (2020) proposes a new mechanism to better exploit event type inChen and Ng, 2016; Duncan et al., 2017). Previous formation for coreference resolution. Despite their neural methods (Nguyen et al., 2016; Choubey and Huang, 2017; Huang et al., 2019) also use non- impressive performance, these methods are specific to one particular type of additional information. contextual word embeddings such as word2vec 1 In this paper, we propose general and effecti"
2021.naacl-main.274,D19-1610,1,0.829458,"t straightforward way to build the final pair representation fij of mi and mj is to simply concatenate the trigger-based representation and all the feature-based representations together: (1) (2) (K) fij = [tij , hij , hij , . . . , hij ] (4) However, this approach is not always optimal. First, as the symbolic features are predicted, they can be noisy and contain errors. The performance of most symbolic feature predictors is far from perfect (Table 2). Also, depending on the specific context, some features can be more useful than others. Inspired by studies on gated modules (Lin et al., 2019; Lai et al., 2019), we propose ContextDependent Gated Module (CDGM), which uses a gating mechanism to extract information from the input symbolic features selectively (Figure 1). Given two mentions mi and mj , we use their trigger feature vector tij as the main controlling con(u) ti = ei X j=si xj ei − si + 1 text to compute the filtered representation hij : (1) (u) hij 3492 (u)  = CDGM(u) tij , hij (5) Figure 1: Overall architecture of our mention-pair encoder, which uses CDGMs to incorporate symbolic features. where u ∈ {1, 2, . . . , K}. More specifically: (u) (u)  (u)  gij = σ FFNNg tij , hij (u) (u) ("
2021.naacl-main.274,D17-1018,0,0.0441632,"ersa. Finally, after using CDGMs to distill symbolic features, the final pair representation fij of mi and mj can be computed as follows: (1) (2) (K) fij = [tij , hij , hij , . . . , hij ] (8) And the coreference score s(i, j) of mi and mj is: s(i, j) = FFNNa (fij ) where FFNNa is a mapping from R(K+1)×p → R. 2.4 Training and Inference Algorithm 1: Noise Addition for Symbolic Features Input: Document D Hyperparameters: {1 , 2 , · · · , K } for i = 1 . . . k do for u = 1 . . . K do (u) With prob. u , replace ci by (u) cˆi ∼ Uniform(Nu ) end end Training We use the same loss function as in (Lee et al., 2017). Also, notice that the training accuracy of a feature predictor is typically much higher than its accuracy on the dev/test set (Table 2). If we simply train our model without any regularization, our CDGMs will rarely come across noisy symbolic features during training. Therefore, to encourage our CDGMs to actually learn to distill reliable signals, we also propose a simple but effective noisy training method. Before passing a training data batch to the model, we randomly add noise to the predicted features. More specifically, for each document D in the batch, we go through every symbolic feat"
2021.naacl-main.274,2020.acl-main.713,1,0.626665,"d indices of its trigger by si and ei respectively. We assume the mentions are ordered based on si (i.e., If i ≤ j then si ≤ sj ). We also assume each mi has K (predicted) cat(1) (2) (K) egorical features {ci , ci , . . . , ci }, with each (u) ci ∈ {1, 2, . . . , Nu } taking one of Nu different discrete values. Table 2 lists the symbolic features we consider in this work. The definitions of the features and their possible values are in ACE and Rich ERE guidelines (LDC, 2005; Mitamura et al., 2016). The accuracy scores of the symbolic feature predictors are also shown in Table 2. We use OneIE (Lin et al., 2020) to identify event mentions along with their subtypes. For other symbolic features, we train a joint classification model based on SpanBERT. The appendix contains more details. 2.2 Dataset Single-Mention Encoder Given a document D, our model first forms a contextualized representation for each input token using a Transformer encoder (Joshi et al., 2020). Let X = (x1 , ..., xn ) be the output of the encoder, where xi ∈ Rd . Then, for each mention mi , its trigger’s representation ti is defined as the average of its token embeddings: where FFNNt is a feedforward network mapping from R3×d → Rp ,"
2021.naacl-main.274,P19-1016,1,0.846259,"Rp . Now, the most straightforward way to build the final pair representation fij of mi and mj is to simply concatenate the trigger-based representation and all the feature-based representations together: (1) (2) (K) fij = [tij , hij , hij , . . . , hij ] (4) However, this approach is not always optimal. First, as the symbolic features are predicted, they can be noisy and contain errors. The performance of most symbolic feature predictors is far from perfect (Table 2). Also, depending on the specific context, some features can be more useful than others. Inspired by studies on gated modules (Lin et al., 2019; Lai et al., 2019), we propose ContextDependent Gated Module (CDGM), which uses a gating mechanism to extract information from the input symbolic features selectively (Figure 1). Given two mentions mi and mj , we use their trigger feature vector tij as the main controlling con(u) ti = ei X j=si xj ei − si + 1 text to compute the filtered representation hij : (1) (u) hij 3492 (u)  = CDGM(u) tij , hij (5) Figure 1: Overall architecture of our mention-pair encoder, which uses CDGMs to incorporate symbolic features. where u ∈ {1, 2, . . . , K}. More specifically: (u) (u)  (u)  gij = σ FFNNg"
2021.naacl-main.274,L16-1631,0,0.207491,"ches employ features re- OTHER). Therefore, our model should be able to avoid the mistake if it utilizes additional symbolic lated to event triggers in one form or another. To features such as the modality attribute in this case. achieve better performance, many methods also There are several previous methods that use conneed to use a variety of additional symbolic featextual embeddings together with type-based or tures such as event types, attributes, and arguments (Chen et al., 2009; Chen and Ji, 2009; Zhang et al., argument-based information (Lu et al., 2020; Yu 2015; Sammons et al., 2015; Lu and Ng, 2016; et al., 2020). For example, Lu et al. (2020) proposes a new mechanism to better exploit event type inChen and Ng, 2016; Duncan et al., 2017). Previous formation for coreference resolution. Despite their neural methods (Nguyen et al., 2016; Choubey and Huang, 2017; Huang et al., 2019) also use non- impressive performance, these methods are specific to one particular type of additional information. contextual word embeddings such as word2vec 1 In this paper, we propose general and effective The code is publicly available at https://github.com/ laituan245/eventcoref. methods for incorporating a"
2021.naacl-main.274,P17-1009,0,0.159707,"Missing"
2021.naacl-main.274,D16-1038,0,0.128683,"Missing"
2021.naacl-main.274,D14-1162,0,0.0903407,"bservations, we propose a novel context-dependent gated module to adaptively control the information flows from the input symbolic features. Combined with a simple noisy training method, our best models achieve state-of-the-art results on two datasets: ACE 2005 and KBP 2016.1 ... we are seeing these soldiers {head out}ev1 ... ... these soldiers were set to {leave}ev2 in January ... ev1 (Movement:Transport): Modality = ASSERTED ev2 (Movement:Transport): Modality = OTHER Table 1: An example of using the modality attribute to improve event coreference resolution. (Mikolov et al., 2013) or GloVe (Pennington et al., 2014). With the recent remarkable success of language models such as BERT (Devlin et al., 2019) and SpanBERT (Joshi et al., 2020), one natural question is whether we can simply use these models for coreference resolution without relying on any additional features. We argue that it is still highly beneficial to utilize symbolic features, especially when they are clean and have complementary information. Table 1 shows an example in the ACE 2005 dataset, where our baseline SpanBERT model 1 Introduction incorrectly predicts the highlighted event mentions to be coreferential. The event triggers are sema"
2021.naacl-main.283,2020.acl-main.421,0,0.0358265,". (Tjong Kim Sang, 2002; Tjong Kim Sang and ∗ De Meulder, 2003). On the other hand, crossWork was started while the first author was a research intern at Adobe. lingual Natural Language Understanding (NLU) 3617 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3617–3632 June 6–11, 2021. ©2021 Association for Computational Linguistics tasks have gained less attention, with smaller benchmark datasets that cover a handful of languages and don’t truly model linguistic variety (Conneau et al., 2018; Artetxe et al., 2020). Natural Language Understanding tasks are critical for dialog systems, as they make up an integral part of the dialog pipeline. Understanding and improving the mechanism behind cross-lingual transfer for natural language understanding in dialog systems require evaluations on more challenging and typologically diverse benchmarks. Numerous approaches have attempted to build stronger cross-lingual representations on top of those multilingual models; however, most require parallel corpora (Wang et al., 2019; Lample and Conneau, 2019) and are biased towards highresource and balanced setups. This f"
2021.naacl-main.283,2020.repl4nlp-1.1,0,0.0435506,"learning is a technique used et al., 2019). to adapt a model trained on a downstream task in a The generalization of such representations has source language to directly generalize to the task in been extensively evaluated on traditional tasks such new languages. It aims to come up with common as Part-of-Speech (POS) tagging, Named Entity cross-lingual representations and leverages them to Recognition (NER) and Cross-lingual Document bridge the divide between resources to make any Classification (CLDC) (Ahmad et al., 2019; Wu NLP application scale to multiple languages. This and Dredze, 2019; Bari et al., 2020a; Schwenk is particularly useful for data-scarce scenarios, as it and Li, 2018), with ever-growing open commureduces the need for API calls implied by machine nity annotation efforts like Universal Dependentranslation or costly task-specific annotation for cies (Nivre et al., 2020) and CoNLL shared tasks new languages. (Tjong Kim Sang, 2002; Tjong Kim Sang and ∗ De Meulder, 2003). On the other hand, crossWork was started while the first author was a research intern at Adobe. lingual Natural Language Understanding (NLU) 3617 Proceedings of the 2021 Conference of the North American Chapter of t"
2021.naacl-main.283,D18-1398,0,0.115069,"oss-lingual representations on top of those multilingual models; however, most require parallel corpora (Wang et al., 2019; Lample and Conneau, 2019) and are biased towards highresource and balanced setups. This fuels the need for a method that doesn’t require explicit crosslingual alignment for faster adaptation to lowresource setups. Meta-learning, a method for “learning to learn”, has found favor especially among the computer vision and speech recognition communities (Nichol et al., 2018; Triantafillou et al., 2020; Winata et al., 2020). Meta-learning has been used for machine translation (Gu et al., 2018), few-shot relation classification (Gao et al., 2019), and on a variety of GLUE tasks (Dou et al., 2019). Recently, Nooralahzadeh et al. (2020) apply the MAML (Finn et al., 2017) algorithm to crosslingual transfer learning for XNLI (Conneau et al., 2018) and MLQA (Lewis et al., 2020), NLU tasks that are naturally biased towards machine translation-based solutions. Nooralahzadeh et al. are able to show improvement over strong multilingual models, including M-BERT. However, they mainly show the effects of meta-learning as a first step in a framework that relies on supervised finetuning, making i"
2021.naacl-main.283,D19-1252,0,0.0471437,"Missing"
2021.naacl-main.283,2020.clssts-1.5,0,0.161057,"are biased towards highresource and balanced setups. This fuels the need for a method that doesn’t require explicit crosslingual alignment for faster adaptation to lowresource setups. Meta-learning, a method for “learning to learn”, has found favor especially among the computer vision and speech recognition communities (Nichol et al., 2018; Triantafillou et al., 2020; Winata et al., 2020). Meta-learning has been used for machine translation (Gu et al., 2018), few-shot relation classification (Gao et al., 2019), and on a variety of GLUE tasks (Dou et al., 2019). Recently, Nooralahzadeh et al. (2020) apply the MAML (Finn et al., 2017) algorithm to crosslingual transfer learning for XNLI (Conneau et al., 2018) and MLQA (Lewis et al., 2020), NLU tasks that are naturally biased towards machine translation-based solutions. Nooralahzadeh et al. are able to show improvement over strong multilingual models, including M-BERT. However, they mainly show the effects of meta-learning as a first step in a framework that relies on supervised finetuning, making it difficult to properly compare and contrast both approaches. sification task, MTOD is a joint classification and sequence labelling task and i"
2021.naacl-main.283,2020.acl-main.653,0,0.0411571,"l alignment for faster adaptation to lowresource setups. Meta-learning, a method for “learning to learn”, has found favor especially among the computer vision and speech recognition communities (Nichol et al., 2018; Triantafillou et al., 2020; Winata et al., 2020). Meta-learning has been used for machine translation (Gu et al., 2018), few-shot relation classification (Gao et al., 2019), and on a variety of GLUE tasks (Dou et al., 2019). Recently, Nooralahzadeh et al. (2020) apply the MAML (Finn et al., 2017) algorithm to crosslingual transfer learning for XNLI (Conneau et al., 2018) and MLQA (Lewis et al., 2020), NLU tasks that are naturally biased towards machine translation-based solutions. Nooralahzadeh et al. are able to show improvement over strong multilingual models, including M-BERT. However, they mainly show the effects of meta-learning as a first step in a framework that relies on supervised finetuning, making it difficult to properly compare and contrast both approaches. sification task, MTOD is a joint classification and sequence labelling task and is more typologically diverse. TyDiQA is not a classification task, but we show how meta-learning can be applied usefully to it. We also show"
2021.naacl-main.283,2020.emnlp-main.484,0,0.0460222,"Missing"
2021.naacl-main.283,D19-1129,0,0.0634266,"Missing"
2021.naacl-main.283,K19-1061,1,0.840535,"Missing"
2021.naacl-main.283,D19-1575,0,0.0235775,"a handful of languages and don’t truly model linguistic variety (Conneau et al., 2018; Artetxe et al., 2020). Natural Language Understanding tasks are critical for dialog systems, as they make up an integral part of the dialog pipeline. Understanding and improving the mechanism behind cross-lingual transfer for natural language understanding in dialog systems require evaluations on more challenging and typologically diverse benchmarks. Numerous approaches have attempted to build stronger cross-lingual representations on top of those multilingual models; however, most require parallel corpora (Wang et al., 2019; Lample and Conneau, 2019) and are biased towards highresource and balanced setups. This fuels the need for a method that doesn’t require explicit crosslingual alignment for faster adaptation to lowresource setups. Meta-learning, a method for “learning to learn”, has found favor especially among the computer vision and speech recognition communities (Nichol et al., 2018; Triantafillou et al., 2020; Winata et al., 2020). Meta-learning has been used for machine translation (Gu et al., 2018), few-shot relation classification (Gao et al., 2019), and on a variety of GLUE tasks (Dou et al., 2019)."
2021.naacl-main.283,2020.acl-main.348,0,0.292112,"are biased towards highresource and balanced setups. This fuels the need for a method that doesn’t require explicit crosslingual alignment for faster adaptation to lowresource setups. Meta-learning, a method for “learning to learn”, has found favor especially among the computer vision and speech recognition communities (Nichol et al., 2018; Triantafillou et al., 2020; Winata et al., 2020). Meta-learning has been used for machine translation (Gu et al., 2018), few-shot relation classification (Gao et al., 2019), and on a variety of GLUE tasks (Dou et al., 2019). Recently, Nooralahzadeh et al. (2020) apply the MAML (Finn et al., 2017) algorithm to crosslingual transfer learning for XNLI (Conneau et al., 2018) and MLQA (Lewis et al., 2020), NLU tasks that are naturally biased towards machine translation-based solutions. Nooralahzadeh et al. are able to show improvement over strong multilingual models, including M-BERT. However, they mainly show the effects of meta-learning as a first step in a framework that relies on supervised finetuning, making it difficult to properly compare and contrast both approaches. sification task, MTOD is a joint classification and sequence labelling task and i"
2021.naacl-main.283,D19-1077,0,0.046422,"Missing"
2021.naacl-main.71,D19-6115,0,0.0900155,"freeze the parameters for the encoder for the first epoch, because weights from the classification head will be randomly initialised and we do not want the loss to affect the weights from the pertained encoder. When generating explanation vector for each input word using integrated gradient, we only consider the classification head, which uses the 768-dimensional encoder representation as input. Parameter m in Eq. (2) is fixed as 50 for all experiments. Comparing Baselines. We compare with three representative families of methods. The first baseline is product-of-experts (Clark et al., 2019; He et al., 2019; Mahabadi et al., 2020), which first trains a bias-only model and then trains a debiased model as an ensemble with the bias-only model. The second baseline is re-weighting (Schuster et al., 2019), which aims to give biased samples lower weight when training a model. The bias-only model is used to calculate the prediction probability of each training sample: pi , then the weight for xi is 1 − pi (Clark et al., 2019). Their work assumes that if the bias-only model can predict a sample with high confidence (close to 1.0), this example is potentially biased. The third baseline is changing example"
2021.naacl-main.71,D17-1215,0,0.0518963,"different NLU tasks. Related Work We briefly review two lines of research that are most relevant to our work: shortcut learning demonstration and shortcut mitigation. Shortcut Learning Phenomena. Recently, the community has revealed the shortcut learning phenomenon for different kinds of language and vision tasks, such as NLI (Niven and Kao, 2019), question answering (Mudrakarta et al., 2018), reading comprehension (Si et al., 2019), VQA (Agrawal et al., 2018; Manjunatha et al., 2019), and deepfake detection (Du et al., 2020). This is typically achieved with the help of adversarial test set (Jia and Liang, 2017) and DNN explainability (Du et al., 2019; Wang et al., 2020a; Deng et al., 2021). These analysis indicates that DNNs are prone to capture lowlevel superficial patterns (including lexical bias, overlap bias, etc), rather than high-level task relevant features. We focus on lexical bias in this work. Motivated by the high-frequency preference for CNNs, i.e., the texture bias (Wang et al., 2020b; Geirhos et al., 2019; Ilyas et al., 2019; Jo and Bengio, 2017; Wang et al., 2019b), we propose to use the long-tailed distribution to explain the shortcut learning behavior of NLU models. Shortcut Mitigat"
2021.naacl-main.71,2020.acl-main.249,0,0.0974296,"Missing"
2021.naacl-main.71,2020.acl-main.769,0,0.161667,"al “‘’, since ‘“’ appears infrequently in both the original MNLI training and validation set. NLU Models. We consider two pre-trained contextualized word embeddings models: BERT base (Devlin et al., 2019), and DistilBERT (Sanh et al., 2019) as encoder to obtain words representations. We use the pre-trained BERT models from Huggingface Transformers1 . The input fed to the embedding models are obtained by concatenating two branches of inputs, which are separated using the ‘[sep]’ symbol. Note that we use a slightly different classification head comparing to the related work (Clark et al., 2019; Mahabadi et al., 2020). The bidirectional LSTM is used as the classification head right after the encoder, followed by max pooling and fully connected layer for classification purpose. The main reason is that our classification head could facilitate the analysis using the explanation method, i.e., integrated gradient, to analyze model behavior. More model details are put in the Sec. A in Appendix. Implementation Details. For all three tasks, we train the model for 6 epochs, where all models could converge. Hyperparameter α is fixed as 0.8 for all models. We use Adam optimizer, where the 1 https://huggingface.co/tra"
2021.naacl-main.71,P19-1334,0,0.0580898,"Missing"
2021.naacl-main.71,2020.emnlp-main.665,0,0.0512531,"Missing"
2021.naacl-main.71,K18-1007,0,0.0201688,"and a questiononly model for VQA task (Cadene et al., 2019) are regarded as bias-only model. Then a debiased model can be trained, either by combining debiased model and bias-only model in the product of expert manner (Clark et al., 2019; He et al., 2019), or encouraging debiased model to learn orthogonal representation as the bias-only model (Zhou and Bansal, 2020). Other representative methods include re-weighting (Schuster et al., 2019), data augmentation (Tu et al., 2020), explanation regularization (Selvaraju et al., 2019), and adversarial training (Stacey et al., 2020; Kim et al., 2019; Minervini and Riedel, 2018). Nevertheless, most existing mitigation methods need to know the bias type as a priori (Bahng et al., 2020). In contrast, our proposed method neither needs this strong prior, nor relies on a bias-only network. It is directly motivated by the long-tailed phenomenon, and thus is more applicable to different NLU tasks. Related Work We briefly review two lines of research that are most relevant to our work: shortcut learning demonstration and shortcut mitigation. Shortcut Learning Phenomena. Recently, the community has revealed the shortcut learning phenomenon for different kinds of language and"
2021.naacl-main.71,P18-1176,0,0.0222368,"s type as a priori (Bahng et al., 2020). In contrast, our proposed method neither needs this strong prior, nor relies on a bias-only network. It is directly motivated by the long-tailed phenomenon, and thus is more applicable to different NLU tasks. Related Work We briefly review two lines of research that are most relevant to our work: shortcut learning demonstration and shortcut mitigation. Shortcut Learning Phenomena. Recently, the community has revealed the shortcut learning phenomenon for different kinds of language and vision tasks, such as NLI (Niven and Kao, 2019), question answering (Mudrakarta et al., 2018), reading comprehension (Si et al., 2019), VQA (Agrawal et al., 2018; Manjunatha et al., 2019), and deepfake detection (Du et al., 2020). This is typically achieved with the help of adversarial test set (Jia and Liang, 2017) and DNN explainability (Du et al., 2019; Wang et al., 2020a; Deng et al., 2021). These analysis indicates that DNNs are prone to capture lowlevel superficial patterns (including lexical bias, overlap bias, etc), rather than high-level task relevant features. We focus on lexical bias in this work. Motivated by the high-frequency preference for CNNs, i.e., the texture bias ("
2021.naacl-main.71,P19-1459,0,0.11272,"shortcut words and ∗ Most of the work was done while the first author was an intern at Adobe Research. labels. This eventually results in their low generalizability on out-of-distribution (OOD) samples and low adversarial robustness (Zellers et al., 2018). In this work, we show that the shortcut learning behavior of NLU models can be explained by the long-tailed phenomenon. Previous empirical analysis indicates that the performance of BERT-like models for NLI task could be mainly explained by the reliance of spurious statistical cues such as unigrams “not”, “do”, “is” and bigrams “will not” (Niven and Kao, 2019; Gururangan et al., 2018). Here we generalize these hypotheses using the long-tailed phenomenon. Specifically, the features in training set could be modeled using a long-tailed distribution via using local mutual information (Evert, 2005) as a measurement. By utilizing an interpretation method to analyze model behavior, we observe that these NLU models concentrate mainly on information on the head of the distribution, which usually corresponds to nongeneralizable shortcut features. In contrast, the tail of the distribution is poorly learned, although it contains high information for the NLU t"
2021.naacl-main.71,D19-1341,0,0.47802,"el y. In the training set, some words or phrases within x co-occur more frequently with one label y than others. The NLU model would capture those shortcut features for prediction. Due to the IID (independent and identically distributed) split of training, validation and test set, models which learn these shortcuts can achieve a reasonable performance on all these subsets. Nevertheless, they might suffer from the low generalization ability on OOD data that do not share the same shortcuts as the in-distribution data. Dataset Statistics. We model statistics using local mutual information (LMI) (Schuster et al., 2019) between a word w and a label y, denoted as follows: LMI(w, y) = p(w, y) · log( p(y|w) ), p(y) (1) where p(w, y) = count(w,y) , p(y|w) = count(w,y) |D| count(w) . |D |is the number of unique words in training set, count(w, y) denotes the co-occurrence of word w with label y, and count(w) is total number of words in the training set. After analyzing each word for the training set, we obtain |y |distributions of |y| labels. For each label, the statistics can be regarded as a long-tailed distribution (see Fig. 1(a)). It can be observed that the head of each distribution typically contains functio"
2021.naacl-main.71,2020.tacl-1.40,0,0.0214654,"instance, a hypothesis-only model (Clark et al., 2019; He et al., 2019) or bag of words model (Zhou and Bansal, 2020) for the NLI task, and a questiononly model for VQA task (Cadene et al., 2019) are regarded as bias-only model. Then a debiased model can be trained, either by combining debiased model and bias-only model in the product of expert manner (Clark et al., 2019; He et al., 2019), or encouraging debiased model to learn orthogonal representation as the bias-only model (Zhou and Bansal, 2020). Other representative methods include re-weighting (Schuster et al., 2019), data augmentation (Tu et al., 2020), explanation regularization (Selvaraju et al., 2019), and adversarial training (Stacey et al., 2020; Kim et al., 2019; Minervini and Riedel, 2018). Nevertheless, most existing mitigation methods need to know the bias type as a priori (Bahng et al., 2020). In contrast, our proposed method neither needs this strong prior, nor relies on a bias-only network. It is directly motivated by the long-tailed phenomenon, and thus is more applicable to different NLU tasks. Related Work We briefly review two lines of research that are most relevant to our work: shortcut learning demonstration and shortcut"
2021.naacl-main.71,2020.acl-main.770,0,0.0221891,"ormalizing the summation to the range of 0 and 1. Ultimately, we obtain the shortcut degree measurement for each training sample xi , denoted as bi . This measurement bi can be further utilized to mitigate the shortcut learning behavior. 917 3 Proposed Mitigation Framework Algorithm 1: LTGR mitigation framework. Equipped with the observation of long-tailed phenomenon and the shortcut degree measurement bi obtained from the last section, we propose a shortcut mitigation solution, called LTGR (LongTailed distribution Guided Regularizer). LTGR is implemented based on self knowledge distillation (Utama et al., 2020a; Hinton et al., 2015) (see Fig. 1(b)). The proposed distillation loss is based on the observation that NLU models would give over-confident predictions when there exist strong shortcut features in the input. This is because NLU models over-associate the shortcut features with certain class labels. The proposed distillation loss aims to suppress the NLU models from giving overconfident predictions for samples with strong shortcut features. It forces the model to down-weight its reliance on shortcut features and implicitly encourages the model to shift its attention to more task relevant featu"
2021.naacl-main.71,2020.emnlp-main.613,0,0.60814,"ormalizing the summation to the range of 0 and 1. Ultimately, we obtain the shortcut degree measurement for each training sample xi , denoted as bi . This measurement bi can be further utilized to mitigate the shortcut learning behavior. 917 3 Proposed Mitigation Framework Algorithm 1: LTGR mitigation framework. Equipped with the observation of long-tailed phenomenon and the shortcut degree measurement bi obtained from the last section, we propose a shortcut mitigation solution, called LTGR (LongTailed distribution Guided Regularizer). LTGR is implemented based on self knowledge distillation (Utama et al., 2020a; Hinton et al., 2015) (see Fig. 1(b)). The proposed distillation loss is based on the observation that NLU models would give over-confident predictions when there exist strong shortcut features in the input. This is because NLU models over-associate the shortcut features with certain class labels. The proposed distillation loss aims to suppress the NLU models from giving overconfident predictions for samples with strong shortcut features. It forces the model to down-weight its reliance on shortcut features and implicitly encourages the model to shift its attention to more task relevant featu"
2021.naacl-main.71,N18-1101,0,0.0473466,"t al., 2018). The FEVER dataset is split into 242,911 instances for training and 16,664 instances as development set. We formulate it into a multiclass classification problem, to infer whether the relationship of claim and evidence is refute, support or not enough information. The two adversarial sets are Symmetric v1 and v2 (Sym1 and Sym 2), where a shortcut word appears in both support and refute label (Schuster et al., 2019). Both Symmetric v1 and v2 contain 712 samples (Schuster et al., 2019). • MNLI: The second task is NLI (natural language inference), where the original dataset is MNLI (Williams et al., 2018). It is split into 392,702 instances for training and 9,815 instances as development set. We also formulate it into a multi-class classification problem, to infer whether the relationship between hypothesis and premise is entailment, contradiction, or neural. Two adversarial set HANS (McCoy et al., 2019) and MNLI hard set (Gururangan et al., 2018) are used to test the generalizability. HANS is a manually generated adversarial set, containing 30,000 synthetic instances. Although originally HANS is mainly used to test whether NLU model employs overlap-bias for prediction, we find that models rel"
2021.naacl-main.71,D18-1009,0,0.0240157,"n many NLU (natural language understanding) benchmarks. However, recent studies show that these models tend to exploit dataset biases as shortcuts to make predictions, rather than learn the semantic understanding and reasoning (Geirhos et al., 2020; Gururangan et al., 2018). Here we focus on the lexical bias, where NLU models rely on spurious correlations between shortcut words and ∗ Most of the work was done while the first author was an intern at Adobe Research. labels. This eventually results in their low generalizability on out-of-distribution (OOD) samples and low adversarial robustness (Zellers et al., 2018). In this work, we show that the shortcut learning behavior of NLU models can be explained by the long-tailed phenomenon. Previous empirical analysis indicates that the performance of BERT-like models for NLI task could be mainly explained by the reliance of spurious statistical cues such as unigrams “not”, “do”, “is” and bigrams “will not” (Niven and Kao, 2019; Gururangan et al., 2018). Here we generalize these hypotheses using the long-tailed phenomenon. Specifically, the features in training set could be modeled using a long-tailed distribution via using local mutual information (Evert, 200"
2021.naacl-main.71,2020.acl-main.773,0,0.0135418,"ccuracy could be achieved for MNLI hard validation set. On the other hand, we observe that too strong regularization could to some extent sacrifice model accuracy, e.g., when α = 0.9. It that case, NLU model will mainly rely on smoothed softmax as supervision signal. This could be around [ 13 , 31 , 13 ] for shortcut samples with close to 1 shortcut degree, providing too strong penalization for those samples. 5 2020b), guided by the domain knowledge what in general the shortcut should look like. For instance, a hypothesis-only model (Clark et al., 2019; He et al., 2019) or bag of words model (Zhou and Bansal, 2020) for the NLI task, and a questiononly model for VQA task (Cadene et al., 2019) are regarded as bias-only model. Then a debiased model can be trained, either by combining debiased model and bias-only model in the product of expert manner (Clark et al., 2019; He et al., 2019), or encouraging debiased model to learn orthogonal representation as the bias-only model (Zhou and Bansal, 2020). Other representative methods include re-weighting (Schuster et al., 2019), data augmentation (Tu et al., 2020), explanation regularization (Selvaraju et al., 2019), and adversarial training (Stacey et al., 2020;"
2021.naacl-srw.9,D18-2029,0,0.0202166,"on retriever but use Google USE for the passage retriever and the individual question retriever. We record the answer ai associated to the top-ranked question set qi as Answer 3. The pipeline in Figure 2 that goes from Input Corpus to Candidate Answers, QA Space, {Q}A Space and finally Answer 3 is a valid reader-retriever workflow. We denote this workflow as Reader-Retriever-{Q}A-Space. 3.2.2 Passage Retriever and QA Reader Given a query, the passage retriever uses the dot product of the query embedding and passage embedding vectors generated by Google Universal Sentence Encoder (Google USE) (Cer et al., 2018) to retrieve from the corpus a passage that is semantically most similar to the query. We then use BERT (Devlin et al., 2019), fine-tuned on SQuAD, to read the retrieved passage, predict the answer, and record the predicted answer as Answer 1. The pipeline in Figure 2 that goes from Input Corpus 3.2.5 Answer Aggregator Now that we have Answer 1, {Answer 2}, and Answer 3, the last step is to aggregate them into one single answer to return to the user. Our answer aggregation works as follows: if Answer 1 appears in the set {Answer 2}, then accept Answer 1 and return it; otherwise reject Answer 1"
2021.naacl-srw.9,P17-1171,0,0.130857,"One family, namely retriever-readers (Fig. 1, left branch), first retrieves from the corpus some documents or paragraphs that are likely to be relevant to the question, and then uses neural networks to read the retrieved passages and locate the answer. Another line of work, namely question answering using knowledge bases (abbreviated as QA using KB in this paper; Fig. 1, middle branch), first constructs a knowledge 2 2.1 Related Work Retriever-Readers Retriever-readers solve OpenQA by converting it to easier single-passage QA tasks. Examples of popular algorithms in this family include DrQA (Chen et al., 2017), which has a TF-IDF retriever followed by a recurrent neural network reader, and BERTserini (Yang et al., 2019), which consists of a BM25 retriever and a BERT reader. All retriever-readers face a trade-off between efficiency and accuracy. When the retriever module is 61 Proceedings of NAACL-HLT 2021: Student Research Workshop, pages 61–67 June 6–11, 2021. ©2021 Association for Computational Linguistics Figure 1: Retriever-readers (left), QA using KB (middle), and reader-retrievers (right). computationally efficient, the retrieved results are not very reliable, and the performance of the subse"
2021.naacl-srw.9,D16-1264,0,0.0347826,"agree at all. We denote the complete workflow depicted in Figure 2 as R6 . 4 greatest extent, so that we can make sure we correctly reproduce others’ work and do not put their models into disadvantages when comparing them with ours. More experimental details are available in Section 4.2. Although not critical to this study, using different datasets for training and testing has one additional benefit that it shows the ability of the systems to adapt to new corpora. Experiments We evaluate the OpenQA performance of our proposed method R6 and baseline methods using two public QA datasets, SQuAD (Rajpurkar et al., 2016) and TriviaQA (Joshi et al., 2017). We adopt a rather challenging setting that all trainable components of the models are trained on SQuAD, while the final models are tested on TriviaQA. Furthermore, we use TriviaQA in an open-domain setting by removing all annotated associations between questions and documents and enforcing the systems to answer every question with the entire corpus. We write TriviaQA-Open to distinguish such an opendomain setting from those officially adopted by TriviaQA. One may wonder why we choose to use different datasets for training and testing. Because our goal of the"
2021.naacl-srw.9,N19-1423,0,0.0156048,"ssociated to the top-ranked question set qi as Answer 3. The pipeline in Figure 2 that goes from Input Corpus to Candidate Answers, QA Space, {Q}A Space and finally Answer 3 is a valid reader-retriever workflow. We denote this workflow as Reader-Retriever-{Q}A-Space. 3.2.2 Passage Retriever and QA Reader Given a query, the passage retriever uses the dot product of the query embedding and passage embedding vectors generated by Google Universal Sentence Encoder (Google USE) (Cer et al., 2018) to retrieve from the corpus a passage that is semantically most similar to the query. We then use BERT (Devlin et al., 2019), fine-tuned on SQuAD, to read the retrieved passage, predict the answer, and record the predicted answer as Answer 1. The pipeline in Figure 2 that goes from Input Corpus 3.2.5 Answer Aggregator Now that we have Answer 1, {Answer 2}, and Answer 3, the last step is to aggregate them into one single answer to return to the user. Our answer aggregation works as follows: if Answer 1 appears in the set {Answer 2}, then accept Answer 1 and return it; otherwise reject Answer 1 and return Answer 3. In other words, the answer aggregator checks the consistency between the retrieverreader results and th"
2021.naacl-srw.9,N19-1237,0,0.0141348,"gator Given a corpus, a named entity recognition (NER) tool called TAGME (Ferragina and Scaiella, 2010, 2012) is applied to detect named entities from the corpus and link the entities to Wikipedia titles. Those entities form the set of candidate answers A in Definition 1. Then a question-generating (QG) reader is applied to the set of candidate answers to generate a question for each answer based on the local context. This reader features an encoderdecoder model structure with a question-answering reward and a question fluency reward tuned with policy gradient optimization (Yuan et al., 2017; Hosking and Riedel, 2019). Then we use a question aggregator to build the {Q}A Space by putting together all the questions with the same answer entity. Given a query, the aggregated question retriever uses the BM25 score (Robertson and Zaragoza, 2009) to retrieve from the {Q}A space the answer whose associated set of questions is most similar to the given query. We query the {Q}A Space by treating each qi as a single document which contains qi,j for all j as sentences. In practice, we observe that BM25 works better for long documents and Google USE works better for short passages. That is why we use BM25 as the aggreg"
2021.naacl-srw.9,N19-4013,0,0.0314758,"Missing"
2021.naacl-srw.9,N18-4017,0,0.012718,"lowed by a recurrent neural network reader, and BERTserini (Yang et al., 2019), which consists of a BM25 retriever and a BERT reader. All retriever-readers face a trade-off between efficiency and accuracy. When the retriever module is 61 Proceedings of NAACL-HLT 2021: Student Research Workshop, pages 61–67 June 6–11, 2021. ©2021 Association for Computational Linguistics Figure 1: Retriever-readers (left), QA using KB (middle), and reader-retrievers (right). computationally efficient, the retrieved results are not very reliable, and the performance of the subsequent reader is also constrained (Htut et al., 2018). On the other hand, there exist systems such as R3 (Wang et al., 2018) and DS-QA (Lin et al., 2018) that have sophisticated retrievers jointly trained with the readers, but they are computationally expensive and thus not scalable to large corpora (Das et al., 2019). the KB construction step to the graph query step, and how to handle questions whose answers do not fall within the KB schema. Due to those complexities, the community is observing a recent trend that retriever-readers are dominating the leaderboards of public QA datasets but KB-based methods are not. Therefore, we choose to focus"
2021.naacl-srw.9,W17-2603,0,0.0211891,"and Question Aggregator Given a corpus, a named entity recognition (NER) tool called TAGME (Ferragina and Scaiella, 2010, 2012) is applied to detect named entities from the corpus and link the entities to Wikipedia titles. Those entities form the set of candidate answers A in Definition 1. Then a question-generating (QG) reader is applied to the set of candidate answers to generate a question for each answer based on the local context. This reader features an encoderdecoder model structure with a question-answering reward and a question fluency reward tuned with policy gradient optimization (Yuan et al., 2017; Hosking and Riedel, 2019). Then we use a question aggregator to build the {Q}A Space by putting together all the questions with the same answer entity. Given a query, the aggregated question retriever uses the BM25 score (Robertson and Zaragoza, 2009) to retrieve from the {Q}A space the answer whose associated set of questions is most similar to the given query. We query the {Q}A Space by treating each qi as a single document which contains qi,j for all j as sentences. In practice, we observe that BM25 works better for long documents and Google USE works better for short passages. That is wh"
2021.naacl-srw.9,P17-1147,0,0.0260436,"orkflow depicted in Figure 2 as R6 . 4 greatest extent, so that we can make sure we correctly reproduce others’ work and do not put their models into disadvantages when comparing them with ours. More experimental details are available in Section 4.2. Although not critical to this study, using different datasets for training and testing has one additional benefit that it shows the ability of the systems to adapt to new corpora. Experiments We evaluate the OpenQA performance of our proposed method R6 and baseline methods using two public QA datasets, SQuAD (Rajpurkar et al., 2016) and TriviaQA (Joshi et al., 2017). We adopt a rather challenging setting that all trainable components of the models are trained on SQuAD, while the final models are tested on TriviaQA. Furthermore, we use TriviaQA in an open-domain setting by removing all annotated associations between questions and documents and enforcing the systems to answer every question with the entire corpus. We write TriviaQA-Open to distinguish such an opendomain setting from those officially adopted by TriviaQA. One may wonder why we choose to use different datasets for training and testing. Because our goal of the experiments is to compare the eff"
2021.newsum-1.15,2020.findings-emnlp.428,0,0.0357653,"Missing"
2021.newsum-1.15,2020.emnlp-main.509,0,0.0813628,"Missing"
2021.newsum-1.15,N18-2097,1,0.836213,"ore, we establish various state-of-the-art extractive and abstractive summarization models on our proposed datasets. Finally, we carry out an analysis over the results on both datasets to shed lights on future direction. We believe that our datasets can be utilized to pave the path for future research. Our miner code and data are made publicly available at https:// github.com/sajastu/reddit_collector, along with the licensing details included. 2 Related work Over the past few years, summarization community has witnessed variety of summarization datasets in different domains (See et al., 2017; Cohan et al., 2018; Kornilova and Eidelman, 2019; Grusky et al., 2018; Sotudeh et al., 2021b). While these collections have provided a fair basis to perform different neural text summarization models, the necessity of introducing large-scale collections, in magnitude of over 4 millions, has not been much explored. Among the first attempts on this track, Rush et al. (2015) gathered the English Gigaword corpus (Graff et al., 2003) which contains around 4 millions article-headline pairs for the task of news headline generation. Researchers have noted that lead bias is the common phenomenon in most news datasets, w"
2021.newsum-1.15,N19-1260,0,0.0167429,"Google Patents Public Datasets, with human143 2021 2020 2019 2018 2017 2016 2015 2014 2013 2012 2011 2010 2009 2020 2019 2018 2017 2016 2015 2014 2013 2012 2011 2010 2009 2008 2007 0.0 0.2 0.4 0.6 Percentage 0.8 1.0 0.00 (a) submission-T LDR 0.05 0.10 0.15 Percentage 0.20 0.25 (m) comment-T LDR Figure 2: The proportion of T LDRs over entire posts (submissions and comments) submitted per year (Figures (c) and (d)). At the time of writing this paper, submissions dumps are partly uploaded for 2021 (until 2021-06), while there is no comments dumps uploaded for 2021. written abstractive summaries. Kim et al. (2019) proposed Reddit TIFU in which the abstractive gold summaries are sampled from diverse regions of the source document, rather than lead regions. Our proposed datasets are more suited for the task of extreme summarization (Narayan et al., 2018; Cachola et al., 2020), where the task is to create a short one-sentence summary. To this end, Narayan et al. (2018) proposed XSUM dataset which is a real-word dataset compiling online articles from the British Broadcasting Corportation (BBC). TLDR generation task is also a new form of extreme summarization. Kim et al. (2019) collected Reddit-TIFU dataset"
2021.newsum-1.15,D19-5406,0,0.0627887,"Missing"
2021.newsum-1.15,2020.wnut-1.3,0,0.0368881,"stractive models with B ERTS UM E XT, we observe relatively large performance gap. This might be due to the fact that T LDRs in both T LDR 9+ and T LDR HQ datasets are rather abstractive than extractive as also shown in Section 3.4. Yet with the existence of such a huge gap, the O RACLE -E XT (i.e., upper bound of an extractive summarizer) scores prove that more developed extractive summarizers can perform outof-the-box and mitigate this gap. The performance gap on T LDR 9+ brings various challenges to develop summarization models that better fit on the larger dataset that include noisy data (Kumar et al., 2020). This noise might be handled via methods such as noise-aware training models (Namysl et al., 2020), while enabling the models to benefit from the large-scale T LDR 9+ dataset. We leave this part for future work. It has to be mentioned that automatic evaluation of summarization continues to be an issue and while this dataset does not solve that, instead can be used with any evaluation metric as they evolve. 6 Analysis To gain insights into the qualities of summarization model, we analyze the outputs generated by the models. The diagrams demonstrating n-gram abstractiveness and percentage of no"
2021.newsum-1.15,2020.emnlp-main.338,1,0.77347,"went to an arcade earlier as well. She doesn't seem like the cuddling type of friend, and I'm very worried she has a crush on me. I don't want to ruin a friendship, I don't like her back. Should I just ignore it until she asks me? What if she thinks that was a date? TL;DR I took my friend to see a show, she leant on my shoulder the whole time. I 'm not into her but I think she has a crush on me? Figure 1: An example Reddit post with T LDR summary. As seen, the T LDR summary is extremely short, and highly abstractive. et al., 2019; Zhang et al., 2019; Sotudeh et al., 2020a; Lewis et al., 2020; Lebanoff et al., 2020) and is considered more challenging as the model needs to deal with novel words generation beyond sentence extraction. Over the past few years, different neural models including RNN (Hochreiter and Schmidhuber, 1997) and Transformer-based (Vaswani et al., 1 Introduction 2017) networks have been proposed to facilitate the Text summarization is defined as generating a con- summarization task. While promising, the perforcise sequence of text as summary, given relatively mance of such models is bound to the abundance a longer document as source. A high-quality sum- of training data due to the mass"
2021.newsum-1.15,2020.acl-main.703,0,0.0729235,"uddling all day, we went to an arcade earlier as well. She doesn't seem like the cuddling type of friend, and I'm very worried she has a crush on me. I don't want to ruin a friendship, I don't like her back. Should I just ignore it until she asks me? What if she thinks that was a date? TL;DR I took my friend to see a show, she leant on my shoulder the whole time. I 'm not into her but I think she has a crush on me? Figure 1: An example Reddit post with T LDR summary. As seen, the T LDR summary is extremely short, and highly abstractive. et al., 2019; Zhang et al., 2019; Sotudeh et al., 2020a; Lewis et al., 2020; Lebanoff et al., 2020) and is considered more challenging as the model needs to deal with novel words generation beyond sentence extraction. Over the past few years, different neural models including RNN (Hochreiter and Schmidhuber, 1997) and Transformer-based (Vaswani et al., 1 Introduction 2017) networks have been proposed to facilitate the Text summarization is defined as generating a con- summarization task. While promising, the perforcise sequence of text as summary, given relatively mance of such models is bound to the abundance a longer document as source. A high-quality sum- of train"
2021.newsum-1.15,W15-1527,0,0.0290885,"T LDR-style keywords is found. To find T LDR-style keywords within a given text, we declare a regular expression that matches words starting with “TL” and ending with “DR”, with permission of having up to three characters in-between as also done by Völske et al. (2017). This stage yields the T LDR 9+ dataset as the full corpus. At the next filtering stage, we utilize a heuristic method along with human supervision to narrow down to a more fine-grained dataset that contain high-quality instances. T LDR HQ. A few studies have noted that usergenerated content in social media platforms is noisy (Liu and Inkpen, 2015) in terms of having spams, bad grammar, and spelling errors. To filter out such noisy instances from the T LDR 9+ dataset, we use a heuristic method to drop low-quality instances while retaining high-quality ones. To be more specific, given a post-T LDR pair, we firstly identify the highest score source sentence in terms of ROUGE -2 and ROUGE -L mean scores (i.e., oracle sentence). The choice of oracle sentence lies in the fact that we postulate to extract a sentence from the longer post that has the highest similarity with the T LDR summary as the gold standard. We then decide to either drop"
2021.newsum-1.15,D19-1387,0,0.0747639,"(c) over the all instances in T LDR HQ dataset. As indicated, there are quite a large proportion of novel n-gram words appeared in the T LDR summary as the heat extent is mostly concentrated in the upper half of the y-axis. These plots show the promising capability and challenges of this dataset to be used for abstractive summarization models. 4 4.1 Experimental Setup Baselines We benchmark several extractive and abstractive summarization baselines over our two proposed with its importance is shown in Figure 5 (a). We datasets. define the oracle importance score as follows: B ERT S UM E XT. (Liu and Lapata, 2019) BertSumExt model is the extractive variant of B ERTmax RG 2+L (si ) S UM which is the B ERT Model fine-tuned on text P oracle importance = RG 2+L summarization task. In this regard, B ERT [CLS] si ∈D tokens are appended to the start of each input senwhere D is the set of all sentences within the post, tence, and their associated representations are used and si denotes the ith sentence. RG 2+L (.) is a to predict if the sentence should be included in the function that takes in a post’s sentence, and outputs final summary or not. the mean of its ROUGE -2 and ROUGE -L score with B ERT S UM A BS"
2021.newsum-1.15,D18-1409,0,0.0242781,"ively mance of such models is bound to the abundance a longer document as source. A high-quality sum- of training data due to the massive model complexmary conveys the most important points of its asso- ity (Ying, 2019). Lack of sufficient training data ciated source. The task is generally performed in worsens the model’s ability to generalize patterns in two ways: 1) extractive in which salient sentences training data to unseen data (Althnian et al., 2021). are identified and concatenated to form the final In addition, overfitting will be likely inevitable as summary (Nallapati et al., 2017; Dong et al., 2018; the model is forced to learn from a limited set of Sotudeh et al., 2021a; Narayan et al., 2020; Cho data; hence, hindering the generalization. This et al., 2020); and 2) abstractive that produces a para- justifies the necessity of large-scale corpora for phrasing of the main contents of the given text. (See training large and complex models. et al., 2017; Gehrmann et al., 2018; MacAvaney Prevalence of social media platforms has pro*Work done during the internship at Adobe Research. vided communities with an opportunity to ex142 Proceedings of the Third Workshop on New Frontiers in Summarizat"
2021.newsum-1.15,D18-1443,0,0.0542697,"Missing"
2021.newsum-1.15,W19-8665,0,0.0277928,"Missing"
2021.newsum-1.15,2020.acl-main.138,0,0.0230782,"ue to the fact that T LDRs in both T LDR 9+ and T LDR HQ datasets are rather abstractive than extractive as also shown in Section 3.4. Yet with the existence of such a huge gap, the O RACLE -E XT (i.e., upper bound of an extractive summarizer) scores prove that more developed extractive summarizers can perform outof-the-box and mitigate this gap. The performance gap on T LDR 9+ brings various challenges to develop summarization models that better fit on the larger dataset that include noisy data (Kumar et al., 2020). This noise might be handled via methods such as noise-aware training models (Namysl et al., 2020), while enabling the models to benefit from the large-scale T LDR 9+ dataset. We leave this part for future work. It has to be mentioned that automatic evaluation of summarization continues to be an issue and while this dataset does not solve that, instead can be used with any evaluation metric as they evolve. 6 Analysis To gain insights into the qualities of summarization model, we analyze the outputs generated by the models. The diagrams demonstrating n-gram abstractiveness and percentage of novel n-grams, 5 Experimental Results generated by BART and B ERT S UM A BS, are plotTable 5 presents"
2021.newsum-1.15,D19-1620,0,0.0184296,"basis to perform different neural text summarization models, the necessity of introducing large-scale collections, in magnitude of over 4 millions, has not been much explored. Among the first attempts on this track, Rush et al. (2015) gathered the English Gigaword corpus (Graff et al., 2003) which contains around 4 millions article-headline pairs for the task of news headline generation. Researchers have noted that lead bias is the common phenomenon in most news datasets, where early parts of the article generally include the most important information (Kedzie et al., 2018; Zhu et al., 2019; Grenander et al., 2019). To alleviate the lead bias for training summarization models, there have been recent efforts to propose summarization datasets, where the lead bias phenomenon is mitigated and summaries are sampled from diverse source regions. Amongst those, Sharma et al. (2019) proposed B IG PATENT, consisting 1.3 million patent documents, collected from Google Patents Public Datasets, with human143 2021 2020 2019 2018 2017 2016 2015 2014 2013 2012 2011 2010 2009 2020 2019 2018 2017 2016 2015 2014 2013 2012 2011 2010 2009 2008 2007 0.0 0.2 0.4 0.6 Percentage 0.8 1.0 0.00 (a) submission-T LDR 0.05 0.10 0.15"
2021.newsum-1.15,N18-1065,0,0.015531,"ive and abstractive summarization models on our proposed datasets. Finally, we carry out an analysis over the results on both datasets to shed lights on future direction. We believe that our datasets can be utilized to pave the path for future research. Our miner code and data are made publicly available at https:// github.com/sajastu/reddit_collector, along with the licensing details included. 2 Related work Over the past few years, summarization community has witnessed variety of summarization datasets in different domains (See et al., 2017; Cohan et al., 2018; Kornilova and Eidelman, 2019; Grusky et al., 2018; Sotudeh et al., 2021b). While these collections have provided a fair basis to perform different neural text summarization models, the necessity of introducing large-scale collections, in magnitude of over 4 millions, has not been much explored. Among the first attempts on this track, Rush et al. (2015) gathered the English Gigaword corpus (Graff et al., 2003) which contains around 4 millions article-headline pairs for the task of news headline generation. Researchers have noted that lead bias is the common phenomenon in most news datasets, where early parts of the article generally include t"
2021.newsum-1.15,D18-1206,0,0.019955,"DR 0.05 0.10 0.15 Percentage 0.20 0.25 (m) comment-T LDR Figure 2: The proportion of T LDRs over entire posts (submissions and comments) submitted per year (Figures (c) and (d)). At the time of writing this paper, submissions dumps are partly uploaded for 2021 (until 2021-06), while there is no comments dumps uploaded for 2021. written abstractive summaries. Kim et al. (2019) proposed Reddit TIFU in which the abstractive gold summaries are sampled from diverse regions of the source document, rather than lead regions. Our proposed datasets are more suited for the task of extreme summarization (Narayan et al., 2018; Cachola et al., 2020), where the task is to create a short one-sentence summary. To this end, Narayan et al. (2018) proposed XSUM dataset which is a real-word dataset compiling online articles from the British Broadcasting Corportation (BBC). TLDR generation task is also a new form of extreme summarization. Kim et al. (2019) collected Reddit-TIFU dataset, consisting of 120K posts from the online discussions from Reddit. Recent efforts have mined around 4 millions Reddit posts along with their T LDR summaries (Völske et al., 2017) which resulted in Webis-TLDR-17 dataset. While our work is sim"
2021.newsum-1.15,2020.emnlp-main.339,0,0.038235,"ity sum- of training data due to the massive model complexmary conveys the most important points of its asso- ity (Ying, 2019). Lack of sufficient training data ciated source. The task is generally performed in worsens the model’s ability to generalize patterns in two ways: 1) extractive in which salient sentences training data to unseen data (Althnian et al., 2021). are identified and concatenated to form the final In addition, overfitting will be likely inevitable as summary (Nallapati et al., 2017; Dong et al., 2018; the model is forced to learn from a limited set of Sotudeh et al., 2021a; Narayan et al., 2020; Cho data; hence, hindering the generalization. This et al., 2020); and 2) abstractive that produces a para- justifies the necessity of large-scale corpora for phrasing of the main contents of the given text. (See training large and complex models. et al., 2017; Gehrmann et al., 2018; MacAvaney Prevalence of social media platforms has pro*Work done during the internship at Adobe Research. vided communities with an opportunity to ex142 Proceedings of the Third Workshop on New Frontiers in Summarization, pages 142–151 November 10, 2021. ©2021 Association for Computational Linguistics Dataset Do"
2021.newsum-1.15,D18-1208,0,0.0209058,"these collections have provided a fair basis to perform different neural text summarization models, the necessity of introducing large-scale collections, in magnitude of over 4 millions, has not been much explored. Among the first attempts on this track, Rush et al. (2015) gathered the English Gigaword corpus (Graff et al., 2003) which contains around 4 millions article-headline pairs for the task of news headline generation. Researchers have noted that lead bias is the common phenomenon in most news datasets, where early parts of the article generally include the most important information (Kedzie et al., 2018; Zhu et al., 2019; Grenander et al., 2019). To alleviate the lead bias for training summarization models, there have been recent efforts to propose summarization datasets, where the lead bias phenomenon is mitigated and summaries are sampled from diverse source regions. Amongst those, Sharma et al. (2019) proposed B IG PATENT, consisting 1.3 million patent documents, collected from Google Patents Public Datasets, with human143 2021 2020 2019 2018 2017 2016 2015 2014 2013 2012 2011 2010 2009 2020 2019 2018 2017 2016 2015 2014 2013 2012 2011 2010 2009 2008 2007 0.0 0.2 0.4 0.6 Percentage 0.8 1."
2021.newsum-1.15,D15-1044,0,0.0532922,"at https:// github.com/sajastu/reddit_collector, along with the licensing details included. 2 Related work Over the past few years, summarization community has witnessed variety of summarization datasets in different domains (See et al., 2017; Cohan et al., 2018; Kornilova and Eidelman, 2019; Grusky et al., 2018; Sotudeh et al., 2021b). While these collections have provided a fair basis to perform different neural text summarization models, the necessity of introducing large-scale collections, in magnitude of over 4 millions, has not been much explored. Among the first attempts on this track, Rush et al. (2015) gathered the English Gigaword corpus (Graff et al., 2003) which contains around 4 millions article-headline pairs for the task of news headline generation. Researchers have noted that lead bias is the common phenomenon in most news datasets, where early parts of the article generally include the most important information (Kedzie et al., 2018; Zhu et al., 2019; Grenander et al., 2019). To alleviate the lead bias for training summarization models, there have been recent efforts to propose summarization datasets, where the lead bias phenomenon is mitigated and summaries are sampled from diverse"
2021.newsum-1.15,P17-1099,0,0.0496576,"LDR HQ). Furthermore, we establish various state-of-the-art extractive and abstractive summarization models on our proposed datasets. Finally, we carry out an analysis over the results on both datasets to shed lights on future direction. We believe that our datasets can be utilized to pave the path for future research. Our miner code and data are made publicly available at https:// github.com/sajastu/reddit_collector, along with the licensing details included. 2 Related work Over the past few years, summarization community has witnessed variety of summarization datasets in different domains (See et al., 2017; Cohan et al., 2018; Kornilova and Eidelman, 2019; Grusky et al., 2018; Sotudeh et al., 2021b). While these collections have provided a fair basis to perform different neural text summarization models, the necessity of introducing large-scale collections, in magnitude of over 4 millions, has not been much explored. Among the first attempts on this track, Rush et al. (2015) gathered the English Gigaword corpus (Graff et al., 2003) which contains around 4 millions article-headline pairs for the task of news headline generation. Researchers have noted that lead bias is the common phenomenon in m"
2021.newsum-1.15,P19-1212,0,0.0180183,"orpus (Graff et al., 2003) which contains around 4 millions article-headline pairs for the task of news headline generation. Researchers have noted that lead bias is the common phenomenon in most news datasets, where early parts of the article generally include the most important information (Kedzie et al., 2018; Zhu et al., 2019; Grenander et al., 2019). To alleviate the lead bias for training summarization models, there have been recent efforts to propose summarization datasets, where the lead bias phenomenon is mitigated and summaries are sampled from diverse source regions. Amongst those, Sharma et al. (2019) proposed B IG PATENT, consisting 1.3 million patent documents, collected from Google Patents Public Datasets, with human143 2021 2020 2019 2018 2017 2016 2015 2014 2013 2012 2011 2010 2009 2020 2019 2018 2017 2016 2015 2014 2013 2012 2011 2010 2009 2008 2007 0.0 0.2 0.4 0.6 Percentage 0.8 1.0 0.00 (a) submission-T LDR 0.05 0.10 0.15 Percentage 0.20 0.25 (m) comment-T LDR Figure 2: The proportion of T LDRs over entire posts (submissions and comments) submitted per year (Figures (c) and (d)). At the time of writing this paper, submissions dumps are partly uploaded for 2021 (until 2021-06), whil"
2021.newsum-1.15,2020.acl-main.172,1,0.905812,"lder. She was kind of cuddling all day, we went to an arcade earlier as well. She doesn't seem like the cuddling type of friend, and I'm very worried she has a crush on me. I don't want to ruin a friendship, I don't like her back. Should I just ignore it until she asks me? What if she thinks that was a date? TL;DR I took my friend to see a show, she leant on my shoulder the whole time. I 'm not into her but I think she has a crush on me? Figure 1: An example Reddit post with T LDR summary. As seen, the T LDR summary is extremely short, and highly abstractive. et al., 2019; Zhang et al., 2019; Sotudeh et al., 2020a; Lewis et al., 2020; Lebanoff et al., 2020) and is considered more challenging as the model needs to deal with novel words generation beyond sentence extraction. Over the past few years, different neural models including RNN (Hochreiter and Schmidhuber, 1997) and Transformer-based (Vaswani et al., 1 Introduction 2017) networks have been proposed to facilitate the Text summarization is defined as generating a con- summarization task. While promising, the perforcise sequence of text as summary, given relatively mance of such models is bound to the abundance a longer document as source. A high-"
2021.newsum-1.15,2020.semeval-1.203,1,0.765531,"Missing"
2021.newsum-1.15,W17-4508,0,0.219479,"asets is yet challenging. This might be due to the specific writing style of social media content such as informal language and massive noise within such content (Sotudeh et al., 2020b). Table 1 shows some of the existing summarization datasets in social and non-social media domains. These datasets are specifically proposed for extreme summarization task, where the aim is to produce one to two summary sentences in extreme compression and high abstraction. In this paper, we introduce our dataset, T LDR 9+ with over 9 millions instances which is more than twice larger than the previous dataset (Völske et al., 2017). We further sample high-quality instances in virtue of human annotations from T LDR 9+ to construct T L DR HQ yielding 1.7 million instances in the hope of providing firm grounds for future work. Owing to extremely short length of T LDR summaries (less 1 https://www.reddit.com/ that 40 words), our datasets are rather suitable for extreme summarization task, than for longer ones. In this research, we aim at harvesting instances that include T LDRs written by the Reddit users spanning the period of 2005-2021. Our early attempt at gathering such instances yields over 9 millions instances with T"
2021.newsum-1.15,2020.emnlp-demos.6,0,0.0809229,"Missing"
2021.nlpmc-1.8,N19-1423,0,0.0362992,"n (2016) define RQE as follows: given a pair of questions A and B, question A entails question B if every answer to B is a correct answer to A, and answers A either partially or fully. 2.2 Transfer Learning for Medical QA Language models that use multi-task learning and transfer learning have become ubiquitous in various 1 https://github.com/KhalilMrini/ Medical-Question-Understanding 58 Proceedings of the Second Workshop on Natural Language Processing for Medical Conversations, pages 58–65 July 6, 2021. ©2021 Association for Computational Linguistics NLP applications, including BioNLP. BERT (Devlin et al., 2019) has been fine-tuned using biomedical text from PubMed (Beltagy et al., 2019), PMC (Lee et al., 2020), and/or the MIMIC III dataset (Johnson et al., 2016; Huang et al., 2019; Alsentzer et al., 2019). In this paper, we use pre-trained BART models (Lewis et al., 2019). Transfer learning was a popular approach at the 2019 MEDIQA shared task (Ben Abacha et al., 2019) on medical NLI, RQE and QA. The question answering task involved re-ranking answers, not generating them (Demner-Fushman et al., 2020). For the RQE task, the best-performing model (Zhu et al., 2019) uses transfer learning on NLI and e"
2021.nlpmc-1.8,P19-1215,0,0.207102,"ction In order to answer questions, Conversational AI systems have to first understand the intent of questions (Chen et al., 2012; Cai et al., 2017). This is particularly important for medical conversational agents (Wu et al., 2020), as Consumer Health Questions (CHQ) are often long and contain peripheral information not needed to answer the question. Approaches to medical question understanding include query relaxation (Ben Abacha and Zweigenbaum, 2015; Lei et al., 2020), question entailment recognition (Ben Abacha and Demner-Fushman, 2016, 2019b; Agrawal et al., 2019) and summarization (Ben Abacha and Demner-Fushman, 2019a). We approach the problem of medical question understanding using joint learning of medical question pairs in the two tasks of question summarization and Recognizing Question Entailment (RQE). Previous work on combining summarization and entailment uses at least two datasets – one for each 2 2.1 Background and Related Work Recognizing Question Entailment (RQE) The task of RQE was introduced by Ben Abacha and Demner-Fushman (2016) in the context of medical question answering. It is closely related to the task of Recognizing Textual Entailment (RTE) (Dagan et al., 2005, 2013), and early defini"
2021.nlpmc-1.8,P19-1213,0,0.147027,"Missing"
2021.nlpmc-1.8,2021.bionlp-1.8,0,0.123137,"ang et al., 2019; Alsentzer et al., 2019). In this paper, we use pre-trained BART models (Lewis et al., 2019). Transfer learning was a popular approach at the 2019 MEDIQA shared task (Ben Abacha et al., 2019) on medical NLI, RQE and QA. The question answering task involved re-ranking answers, not generating them (Demner-Fushman et al., 2020). For the RQE task, the best-performing model (Zhu et al., 2019) uses transfer learning on NLI and ensemble methods. In contemporaneous work of ours (Mrini et al., 2021), we participate in the question summarization task of the 2021 MEDIQA shared task (Ben Abacha et al., 2021). We show that transfer learning using medical RQE can improve performance on medical question summarization. 2.3 ment (RQE). In both tasks, a question pair includes a first medical question, written in an informal style by a patient – thus called a Consumer Health Question (CHQ). The second medical question is shorter, and often written in a formal style by medical experts: it is a Frequently Asked Question (FAQ). The inspiration for our joint learning scheme stems from the observation that a CHQ entails an FAQ, if and only if the FAQ is a summary of the CHQ. Our data-augmented joint learning"
2021.nlpmc-1.8,W07-1401,0,0.399376,"E-L (Lin, 2004). 4.2 Setup CHQ and FAQ as input to the model. For question summarization, we only feed the CHQ as input to the model. This way, we ensure that the model never sees the reference FAQs when being evaluated for question summarization. All of our models use the BART large architecture, with different pre-trained models for transfer learning. For the question summarization experiments, we use the BART Large model pre-trained on the XSum dataset (Narayan et al., 2018). For the RQE experiments, we pre-train a BART Large model on the RTE dataset (Dagan et al., 2005; Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009) from the GLUE benchmark (Wang et al., 2018), and re-use the same classification head for RQE. 4.3 5 5.1 Summarization Results In their introduction of MeQSum, Ben Abacha and Demner-Fushman (2019a) show results with seq2seq models and pointer-generated networks. They additionally propose to augment MeQSum using semantically selected relevant pairs from the Quora Question Pairs dataset (Iyer et al., 2017). We report these baselines as well as our BART baseline results. We show our summarization results in Table 2. On MeQSum and iCliniq, our joint learning objective ach"
2021.nlpmc-1.8,W19-5039,0,0.187372,"Question-Understanding 58 Proceedings of the Second Workshop on Natural Language Processing for Medical Conversations, pages 58–65 July 6, 2021. ©2021 Association for Computational Linguistics NLP applications, including BioNLP. BERT (Devlin et al., 2019) has been fine-tuned using biomedical text from PubMed (Beltagy et al., 2019), PMC (Lee et al., 2020), and/or the MIMIC III dataset (Johnson et al., 2016; Huang et al., 2019; Alsentzer et al., 2019). In this paper, we use pre-trained BART models (Lewis et al., 2019). Transfer learning was a popular approach at the 2019 MEDIQA shared task (Ben Abacha et al., 2019) on medical NLI, RQE and QA. The question answering task involved re-ranking answers, not generating them (Demner-Fushman et al., 2020). For the RQE task, the best-performing model (Zhu et al., 2019) uses transfer learning on NLI and ensemble methods. In contemporaneous work of ours (Mrini et al., 2021), we participate in the question summarization task of the 2021 MEDIQA shared task (Ben Abacha et al., 2021). We show that transfer learning using medical RQE can improve performance on medical question summarization. 2.3 ment (RQE). In both tasks, a question pair includes a first medical questi"
2021.nlpmc-1.8,P18-1064,0,0.0473483,"Falke et al. (2019) use textual entailment predictions to detect factual errors in abstractive summaries generated by state-of-the-art models. Pasunuru and Bansal (2018) propose an entailment reward for their abstractive summarizer, where the entailment score is obtained from a pre-trained and frozen natural language inference model. Pasunuru et al. (2017) propose an LSTM encoder-decoder model that incorporates entailment generation and abstractive summarization. They use separate natural language inference and summarization datasets, and train by optimizing the two objectives alternatively. Guo et al. (2018) build upon the work of Pasunuru et al. (2017), and add question generation as an auxiliary task. Li et al. (2018) propose an encoder-decoder summarization model, with an entailment-aware encoder with a separate classification module, and an entailment-rewarded decoder. They follow closely the multi-task setting of Pasunuru et al. (2017). 3 Data Augmentation 3.2 Joint Model We adopt the architecture of BART Large (Lewis et al., 2019), a model that set a new state of the art in XSum (Narayan et al., 2018) and CNNDailymail (Hermann et al., 2015), two popular abstractive summarization benchmark d"
2021.nlpmc-1.8,S14-1010,0,0.461399,"a different, randomly selected from the FAQs of the same dataset split. Inversely, for the RQE dataset, we create equivalent summarization pairs. For each existing RQE pair, we consider two cases. If the RQE pair is labeled as entailment, we create an identical summarization pair. If the RQE pair is labeled as not entailment, we create a summarization pair that is identical to a randomly selected entailment-labeled RQE pair from the same dataset split. Summarization and Entailment There is a growing body of work combining summarization and entailment (Lloret et al., 2008; Mehdad et al., 2013; Gupta et al., 2014). Falke et al. (2019) use textual entailment predictions to detect factual errors in abstractive summaries generated by state-of-the-art models. Pasunuru and Bansal (2018) propose an entailment reward for their abstractive summarizer, where the entailment score is obtained from a pre-trained and frozen natural language inference model. Pasunuru et al. (2017) propose an LSTM encoder-decoder model that incorporates entailment generation and abstractive summarization. They use separate natural language inference and summarization datasets, and train by optimizing the two objectives alternatively."
2021.nlpmc-1.8,K16-1028,0,0.107934,"Missing"
2021.nlpmc-1.8,D18-1206,0,0.360366,"nference and summarization datasets, and train by optimizing the two objectives alternatively. Guo et al. (2018) build upon the work of Pasunuru et al. (2017), and add question generation as an auxiliary task. Li et al. (2018) propose an encoder-decoder summarization model, with an entailment-aware encoder with a separate classification module, and an entailment-rewarded decoder. They follow closely the multi-task setting of Pasunuru et al. (2017). 3 Data Augmentation 3.2 Joint Model We adopt the architecture of BART Large (Lewis et al., 2019), a model that set a new state of the art in XSum (Narayan et al., 2018) and CNNDailymail (Hermann et al., 2015), two popular abstractive summarization benchmark datasets. BART is an encoder-decoder seq2seq model, that can train generation as well as classification tasks, such as RQE. BART trains for abstractive summarization by feeding the source text (CHQ) to the encoder, and the negative log-likelihood loss is computed between the decoder output and the Joint Learning for Consumer Health Question Understanding We consider the joint learning of medical question summarization and Recognizing Question Entail59 Dataset MeQSum HealthCareMagic iCliniq MEDIQA RQE CHQ:"
2021.nlpmc-1.8,N18-2102,0,0.0546019,"RQE pair, we consider two cases. If the RQE pair is labeled as entailment, we create an identical summarization pair. If the RQE pair is labeled as not entailment, we create a summarization pair that is identical to a randomly selected entailment-labeled RQE pair from the same dataset split. Summarization and Entailment There is a growing body of work combining summarization and entailment (Lloret et al., 2008; Mehdad et al., 2013; Gupta et al., 2014). Falke et al. (2019) use textual entailment predictions to detect factual errors in abstractive summaries generated by state-of-the-art models. Pasunuru and Bansal (2018) propose an entailment reward for their abstractive summarizer, where the entailment score is obtained from a pre-trained and frozen natural language inference model. Pasunuru et al. (2017) propose an LSTM encoder-decoder model that incorporates entailment generation and abstractive summarization. They use separate natural language inference and summarization datasets, and train by optimizing the two objectives alternatively. Guo et al. (2018) build upon the work of Pasunuru et al. (2017), and add question generation as an auxiliary task. Li et al. (2018) propose an encoder-decoder summarizati"
2021.nlpmc-1.8,W17-4504,0,0.17389,"r that is identical to a randomly selected entailment-labeled RQE pair from the same dataset split. Summarization and Entailment There is a growing body of work combining summarization and entailment (Lloret et al., 2008; Mehdad et al., 2013; Gupta et al., 2014). Falke et al. (2019) use textual entailment predictions to detect factual errors in abstractive summaries generated by state-of-the-art models. Pasunuru and Bansal (2018) propose an entailment reward for their abstractive summarizer, where the entailment score is obtained from a pre-trained and frozen natural language inference model. Pasunuru et al. (2017) propose an LSTM encoder-decoder model that incorporates entailment generation and abstractive summarization. They use separate natural language inference and summarization datasets, and train by optimizing the two objectives alternatively. Guo et al. (2018) build upon the work of Pasunuru et al. (2017), and add question generation as an auxiliary task. Li et al. (2018) propose an encoder-decoder summarization model, with an entailment-aware encoder with a separate classification module, and an entailment-rewarded decoder. They follow closely the multi-task setting of Pasunuru et al. (2017). 3"
2021.nlpmc-1.8,P17-1099,0,0.137058,"Missing"
2021.nlpmc-1.8,C18-1121,0,0.0931172,"by state-of-the-art models. Pasunuru and Bansal (2018) propose an entailment reward for their abstractive summarizer, where the entailment score is obtained from a pre-trained and frozen natural language inference model. Pasunuru et al. (2017) propose an LSTM encoder-decoder model that incorporates entailment generation and abstractive summarization. They use separate natural language inference and summarization datasets, and train by optimizing the two objectives alternatively. Guo et al. (2018) build upon the work of Pasunuru et al. (2017), and add question generation as an auxiliary task. Li et al. (2018) propose an encoder-decoder summarization model, with an entailment-aware encoder with a separate classification module, and an entailment-rewarded decoder. They follow closely the multi-task setting of Pasunuru et al. (2017). 3 Data Augmentation 3.2 Joint Model We adopt the architecture of BART Large (Lewis et al., 2019), a model that set a new state of the art in XSum (Narayan et al., 2018) and CNNDailymail (Hermann et al., 2015), two popular abstractive summarization benchmark datasets. BART is an encoder-decoder seq2seq model, that can train generation as well as classification tasks, such"
2021.nlpmc-1.8,W18-5446,0,0.101833,"Missing"
2021.nlpmc-1.8,W13-2117,0,0.233593,"Q of the RQE pair is a different, randomly selected from the FAQs of the same dataset split. Inversely, for the RQE dataset, we create equivalent summarization pairs. For each existing RQE pair, we consider two cases. If the RQE pair is labeled as entailment, we create an identical summarization pair. If the RQE pair is labeled as not entailment, we create a summarization pair that is identical to a randomly selected entailment-labeled RQE pair from the same dataset split. Summarization and Entailment There is a growing body of work combining summarization and entailment (Lloret et al., 2008; Mehdad et al., 2013; Gupta et al., 2014). Falke et al. (2019) use textual entailment predictions to detect factual errors in abstractive summaries generated by state-of-the-art models. Pasunuru and Bansal (2018) propose an entailment reward for their abstractive summarizer, where the entailment score is obtained from a pre-trained and frozen natural language inference model. Pasunuru et al. (2017) propose an LSTM encoder-decoder model that incorporates entailment generation and abstractive summarization. They use separate natural language inference and summarization datasets, and train by optimizing the two obje"
2021.nlpmc-1.8,2021.bionlp-1.28,1,0.655863,"bMed (Beltagy et al., 2019), PMC (Lee et al., 2020), and/or the MIMIC III dataset (Johnson et al., 2016; Huang et al., 2019; Alsentzer et al., 2019). In this paper, we use pre-trained BART models (Lewis et al., 2019). Transfer learning was a popular approach at the 2019 MEDIQA shared task (Ben Abacha et al., 2019) on medical NLI, RQE and QA. The question answering task involved re-ranking answers, not generating them (Demner-Fushman et al., 2020). For the RQE task, the best-performing model (Zhu et al., 2019) uses transfer learning on NLI and ensemble methods. In contemporaneous work of ours (Mrini et al., 2021), we participate in the question summarization task of the 2021 MEDIQA shared task (Ben Abacha et al., 2021). We show that transfer learning using medical RQE can improve performance on medical question summarization. 2.3 ment (RQE). In both tasks, a question pair includes a first medical question, written in an informal style by a patient – thus called a Consumer Health Question (CHQ). The second medical question is shorter, and often written in a formal style by medical experts: it is a Frequently Asked Question (FAQ). The inspiration for our joint learning scheme stems from the observation"
2021.nlpmc-1.8,W19-5040,0,0.127853,"cations, including BioNLP. BERT (Devlin et al., 2019) has been fine-tuned using biomedical text from PubMed (Beltagy et al., 2019), PMC (Lee et al., 2020), and/or the MIMIC III dataset (Johnson et al., 2016; Huang et al., 2019; Alsentzer et al., 2019). In this paper, we use pre-trained BART models (Lewis et al., 2019). Transfer learning was a popular approach at the 2019 MEDIQA shared task (Ben Abacha et al., 2019) on medical NLI, RQE and QA. The question answering task involved re-ranking answers, not generating them (Demner-Fushman et al., 2020). For the RQE task, the best-performing model (Zhu et al., 2019) uses transfer learning on NLI and ensemble methods. In contemporaneous work of ours (Mrini et al., 2021), we participate in the question summarization task of the 2021 MEDIQA shared task (Ben Abacha et al., 2021). We show that transfer learning using medical RQE can improve performance on medical question summarization. 2.3 ment (RQE). In both tasks, a question pair includes a first medical question, written in an informal style by a patient – thus called a Consumer Health Question (CHQ). The second medical question is shorter, and often written in a formal style by medical experts: it is a F"
2021.semeval-1.47,2021.semeval-1.38,0,0.0385065,"quantity (i.e., [QT]), measured property (i.e., [PR]) and qualifier (i.e., [QL]) (best viewed in color). Introduction One of the key indicators of scientific writing is the quantities description of various experiments and results. While the mentions of all measurements could provide a rigorous understanding of the topic, it might make the reading and automatic processing of the text more difficult. As such, designing effective methods to recognize the mentions of measurements and also the conditions in which they are valid is necessary. According to the definition of the SemEval 2021 Task 8 (Harper et al., 2021), a measurement might consist of the following components: (i) Measure Entity: A span referring to an entity that one of its properties has been measured and its value is provided in the document; (ii) Measured Property: A span referring to the characteristics of an entity that has been measured; (iii) Quantity: A span in the document that refers to a value and possibly it comes with a unit; and (iv) Qualifier: A span referring to a condition in which more information about the Quantity, Measured Property or Measured Entity is provided. Figure 1 shows a sample document annotated with the afore"
2021.semeval-1.47,S18-1127,1,0.901263,"Missing"
2021.semeval-1.47,S17-2171,1,0.843726,"en the path representation, i.e., hp , and the input document representation, i.e., hd , by replacing the Ldisc by Ldot = hp · hd . The results are presented in Table 2. This table shows that all components of the proposed model Related Work Measurement Relation Extraction (MRE) is one specific formulation of the general Relation Extraction (RE) task. In the literature, RE has been tackled by feature-based methods (Zelenko et al., 2003; Zhou et al., 2005; Sun et al., 2011; Nguyen and Grishman, 2014; Nguyen et al., 2015c) and advanced deep learning models (Zeng et al., 2014; Wang et al., 2016; Lee et al., 2017; Zhang et al., 2017; Nguyen et al., 2019; Jin et al., 2018; Veyseh et al., 2020b). Recently, structure-aware deep models have shown significant improvement for RE (Peng et al., 2017; Song et al., 2018; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016; Nguyen and Grishman, 2018a; Zhang et al., 2018). For a thorough review of the prior works, refer to the recent work (Gupta et al., 2019; Nan et al., 2020; Veyseh et al., 2020a) 5 Conclusion We proposed a new model for the MRE task. The introduced model employs a dynamic path reasoning component which induces important context words to pr"
2021.semeval-1.47,2020.acl-main.141,0,0.0121964,"ods (Zelenko et al., 2003; Zhou et al., 2005; Sun et al., 2011; Nguyen and Grishman, 2014; Nguyen et al., 2015c) and advanced deep learning models (Zeng et al., 2014; Wang et al., 2016; Lee et al., 2017; Zhang et al., 2017; Nguyen et al., 2019; Jin et al., 2018; Veyseh et al., 2020b). Recently, structure-aware deep models have shown significant improvement for RE (Peng et al., 2017; Song et al., 2018; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016; Nguyen and Grishman, 2018a; Zhang et al., 2018). For a thorough review of the prior works, refer to the recent work (Gupta et al., 2019; Nan et al., 2020; Veyseh et al., 2020a) 5 Conclusion We proposed a new model for the MRE task. The introduced model employs a dynamic path reasoning component which induces important context words to predict the relation between two measurement components. Furthermore, we proposed a novel regularization method based on Information Bottleneck to exclude noisy information from the input. Our experiments on the SemEval 2021 Task 8 reveal the effectiveness of the proposed model. 401 Acknowledgments This research has been supported by the Army Research Office (ARO) grant W911NF-21-1-0112. This research is also bas"
2021.semeval-1.47,P14-2012,1,0.802966,"the regularization component. However, instead of using Information Bottleneck, it directly decreases the similarity between the path representation, i.e., hp , and the input document representation, i.e., hd , by replacing the Ldisc by Ldot = hp · hd . The results are presented in Table 2. This table shows that all components of the proposed model Related Work Measurement Relation Extraction (MRE) is one specific formulation of the general Relation Extraction (RE) task. In the literature, RE has been tackled by feature-based methods (Zelenko et al., 2003; Zhou et al., 2005; Sun et al., 2011; Nguyen and Grishman, 2014; Nguyen et al., 2015c) and advanced deep learning models (Zeng et al., 2014; Wang et al., 2016; Lee et al., 2017; Zhang et al., 2017; Nguyen et al., 2019; Jin et al., 2018; Veyseh et al., 2020b). Recently, structure-aware deep models have shown significant improvement for RE (Peng et al., 2017; Song et al., 2018; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016; Nguyen and Grishman, 2018a; Zhang et al., 2018). For a thorough review of the prior works, refer to the recent work (Gupta et al., 2019; Nan et al., 2020; Veyseh et al., 2020a) 5 Conclusion We proposed a new model for the MRE"
2021.semeval-1.47,P15-1062,1,0.8271,"t. However, instead of using Information Bottleneck, it directly decreases the similarity between the path representation, i.e., hp , and the input document representation, i.e., hd , by replacing the Ldisc by Ldot = hp · hd . The results are presented in Table 2. This table shows that all components of the proposed model Related Work Measurement Relation Extraction (MRE) is one specific formulation of the general Relation Extraction (RE) task. In the literature, RE has been tackled by feature-based methods (Zelenko et al., 2003; Zhou et al., 2005; Sun et al., 2011; Nguyen and Grishman, 2014; Nguyen et al., 2015c) and advanced deep learning models (Zeng et al., 2014; Wang et al., 2016; Lee et al., 2017; Zhang et al., 2017; Nguyen et al., 2019; Jin et al., 2018; Veyseh et al., 2020b). Recently, structure-aware deep models have shown significant improvement for RE (Peng et al., 2017; Song et al., 2018; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016; Nguyen and Grishman, 2018a; Zhang et al., 2018). For a thorough review of the prior works, refer to the recent work (Gupta et al., 2019; Nan et al., 2020; Veyseh et al., 2020a) 5 Conclusion We proposed a new model for the MRE task. The introduced"
2021.semeval-1.47,D19-6203,1,0.812798,"and the input document representation, i.e., hd , by replacing the Ldisc by Ldot = hp · hd . The results are presented in Table 2. This table shows that all components of the proposed model Related Work Measurement Relation Extraction (MRE) is one specific formulation of the general Relation Extraction (RE) task. In the literature, RE has been tackled by feature-based methods (Zelenko et al., 2003; Zhou et al., 2005; Sun et al., 2011; Nguyen and Grishman, 2014; Nguyen et al., 2015c) and advanced deep learning models (Zeng et al., 2014; Wang et al., 2016; Lee et al., 2017; Zhang et al., 2017; Nguyen et al., 2019; Jin et al., 2018; Veyseh et al., 2020b). Recently, structure-aware deep models have shown significant improvement for RE (Peng et al., 2017; Song et al., 2018; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016; Nguyen and Grishman, 2018a; Zhang et al., 2018). For a thorough review of the prior works, refer to the recent work (Gupta et al., 2019; Nan et al., 2020; Veyseh et al., 2020a) 5 Conclusion We proposed a new model for the MRE task. The introduced model employs a dynamic path reasoning component which induces important context words to predict the relation between two measuremen"
2021.semeval-1.47,Q17-1008,0,0.0122545,"shows that all components of the proposed model Related Work Measurement Relation Extraction (MRE) is one specific formulation of the general Relation Extraction (RE) task. In the literature, RE has been tackled by feature-based methods (Zelenko et al., 2003; Zhou et al., 2005; Sun et al., 2011; Nguyen and Grishman, 2014; Nguyen et al., 2015c) and advanced deep learning models (Zeng et al., 2014; Wang et al., 2016; Lee et al., 2017; Zhang et al., 2017; Nguyen et al., 2019; Jin et al., 2018; Veyseh et al., 2020b). Recently, structure-aware deep models have shown significant improvement for RE (Peng et al., 2017; Song et al., 2018; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016; Nguyen and Grishman, 2018a; Zhang et al., 2018). For a thorough review of the prior works, refer to the recent work (Gupta et al., 2019; Nan et al., 2020; Veyseh et al., 2020a) 5 Conclusion We proposed a new model for the MRE task. The introduced model employs a dynamic path reasoning component which induces important context words to predict the relation between two measurement components. Furthermore, we proposed a novel regularization method based on Information Bottleneck to exclude noisy information from the in"
2021.semeval-1.47,D18-1246,0,0.0329273,"onents of the proposed model Related Work Measurement Relation Extraction (MRE) is one specific formulation of the general Relation Extraction (RE) task. In the literature, RE has been tackled by feature-based methods (Zelenko et al., 2003; Zhou et al., 2005; Sun et al., 2011; Nguyen and Grishman, 2014; Nguyen et al., 2015c) and advanced deep learning models (Zeng et al., 2014; Wang et al., 2016; Lee et al., 2017; Zhang et al., 2017; Nguyen et al., 2019; Jin et al., 2018; Veyseh et al., 2020b). Recently, structure-aware deep models have shown significant improvement for RE (Peng et al., 2017; Song et al., 2018; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016; Nguyen and Grishman, 2018a; Zhang et al., 2018). For a thorough review of the prior works, refer to the recent work (Gupta et al., 2019; Nan et al., 2020; Veyseh et al., 2020a) 5 Conclusion We proposed a new model for the MRE task. The introduced model employs a dynamic path reasoning component which induces important context words to predict the relation between two measurement components. Furthermore, we proposed a novel regularization method based on Information Bottleneck to exclude noisy information from the input. Our experiment"
2021.semeval-1.47,P11-1053,0,0.0383277,"d model preserves the regularization component. However, instead of using Information Bottleneck, it directly decreases the similarity between the path representation, i.e., hp , and the input document representation, i.e., hd , by replacing the Ldisc by Ldot = hp · hd . The results are presented in Table 2. This table shows that all components of the proposed model Related Work Measurement Relation Extraction (MRE) is one specific formulation of the general Relation Extraction (RE) task. In the literature, RE has been tackled by feature-based methods (Zelenko et al., 2003; Zhou et al., 2005; Sun et al., 2011; Nguyen and Grishman, 2014; Nguyen et al., 2015c) and advanced deep learning models (Zeng et al., 2014; Wang et al., 2016; Lee et al., 2017; Zhang et al., 2017; Nguyen et al., 2019; Jin et al., 2018; Veyseh et al., 2020b). Recently, structure-aware deep models have shown significant improvement for RE (Peng et al., 2017; Song et al., 2018; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016; Nguyen and Grishman, 2018a; Zhang et al., 2018). For a thorough review of the prior works, refer to the recent work (Gupta et al., 2019; Nan et al., 2020; Veyseh et al., 2020a) 5 Conclusion We propos"
2021.semeval-1.47,2020.acl-main.715,1,0.693326,"i.e., hd , by replacing the Ldisc by Ldot = hp · hd . The results are presented in Table 2. This table shows that all components of the proposed model Related Work Measurement Relation Extraction (MRE) is one specific formulation of the general Relation Extraction (RE) task. In the literature, RE has been tackled by feature-based methods (Zelenko et al., 2003; Zhou et al., 2005; Sun et al., 2011; Nguyen and Grishman, 2014; Nguyen et al., 2015c) and advanced deep learning models (Zeng et al., 2014; Wang et al., 2016; Lee et al., 2017; Zhang et al., 2017; Nguyen et al., 2019; Jin et al., 2018; Veyseh et al., 2020b). Recently, structure-aware deep models have shown significant improvement for RE (Peng et al., 2017; Song et al., 2018; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016; Nguyen and Grishman, 2018a; Zhang et al., 2018). For a thorough review of the prior works, refer to the recent work (Gupta et al., 2019; Nan et al., 2020; Veyseh et al., 2020a) 5 Conclusion We proposed a new model for the MRE task. The introduced model employs a dynamic path reasoning component which induces important context words to predict the relation between two measurement components. Furthermore, we proposed"
2021.semeval-1.47,P16-1123,0,0.0375449,"Missing"
2021.semeval-1.47,D15-1206,0,0.0294848,"sed model Related Work Measurement Relation Extraction (MRE) is one specific formulation of the general Relation Extraction (RE) task. In the literature, RE has been tackled by feature-based methods (Zelenko et al., 2003; Zhou et al., 2005; Sun et al., 2011; Nguyen and Grishman, 2014; Nguyen et al., 2015c) and advanced deep learning models (Zeng et al., 2014; Wang et al., 2016; Lee et al., 2017; Zhang et al., 2017; Nguyen et al., 2019; Jin et al., 2018; Veyseh et al., 2020b). Recently, structure-aware deep models have shown significant improvement for RE (Peng et al., 2017; Song et al., 2018; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016; Nguyen and Grishman, 2018a; Zhang et al., 2018). For a thorough review of the prior works, refer to the recent work (Gupta et al., 2019; Nan et al., 2020; Veyseh et al., 2020a) 5 Conclusion We proposed a new model for the MRE task. The introduced model employs a dynamic path reasoning component which induces important context words to predict the relation between two measurement components. Furthermore, we proposed a novel regularization method based on Information Bottleneck to exclude noisy information from the input. Our experiments on the SemEval"
2021.semeval-1.47,P15-2047,0,0.0218897,"Work Measurement Relation Extraction (MRE) is one specific formulation of the general Relation Extraction (RE) task. In the literature, RE has been tackled by feature-based methods (Zelenko et al., 2003; Zhou et al., 2005; Sun et al., 2011; Nguyen and Grishman, 2014; Nguyen et al., 2015c) and advanced deep learning models (Zeng et al., 2014; Wang et al., 2016; Lee et al., 2017; Zhang et al., 2017; Nguyen et al., 2019; Jin et al., 2018; Veyseh et al., 2020b). Recently, structure-aware deep models have shown significant improvement for RE (Peng et al., 2017; Song et al., 2018; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016; Nguyen and Grishman, 2018a; Zhang et al., 2018). For a thorough review of the prior works, refer to the recent work (Gupta et al., 2019; Nan et al., 2020; Veyseh et al., 2020a) 5 Conclusion We proposed a new model for the MRE task. The introduced model employs a dynamic path reasoning component which induces important context words to predict the relation between two measurement components. Furthermore, we proposed a novel regularization method based on Information Bottleneck to exclude noisy information from the input. Our experiments on the SemEval 2021 Task 8 reveal"
2021.semeval-1.47,P16-1105,0,0.0263793,"Relation Extraction (MRE) is one specific formulation of the general Relation Extraction (RE) task. In the literature, RE has been tackled by feature-based methods (Zelenko et al., 2003; Zhou et al., 2005; Sun et al., 2011; Nguyen and Grishman, 2014; Nguyen et al., 2015c) and advanced deep learning models (Zeng et al., 2014; Wang et al., 2016; Lee et al., 2017; Zhang et al., 2017; Nguyen et al., 2019; Jin et al., 2018; Veyseh et al., 2020b). Recently, structure-aware deep models have shown significant improvement for RE (Peng et al., 2017; Song et al., 2018; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016; Nguyen and Grishman, 2018a; Zhang et al., 2018). For a thorough review of the prior works, refer to the recent work (Gupta et al., 2019; Nan et al., 2020; Veyseh et al., 2020a) 5 Conclusion We proposed a new model for the MRE task. The introduced model employs a dynamic path reasoning component which induces important context words to predict the relation between two measurement components. Furthermore, we proposed a novel regularization method based on Information Bottleneck to exclude noisy information from the input. Our experiments on the SemEval 2021 Task 8 reveal the effectiveness of t"
2021.semeval-1.47,C14-1220,0,0.0382041,"irectly decreases the similarity between the path representation, i.e., hp , and the input document representation, i.e., hd , by replacing the Ldisc by Ldot = hp · hd . The results are presented in Table 2. This table shows that all components of the proposed model Related Work Measurement Relation Extraction (MRE) is one specific formulation of the general Relation Extraction (RE) task. In the literature, RE has been tackled by feature-based methods (Zelenko et al., 2003; Zhou et al., 2005; Sun et al., 2011; Nguyen and Grishman, 2014; Nguyen et al., 2015c) and advanced deep learning models (Zeng et al., 2014; Wang et al., 2016; Lee et al., 2017; Zhang et al., 2017; Nguyen et al., 2019; Jin et al., 2018; Veyseh et al., 2020b). Recently, structure-aware deep models have shown significant improvement for RE (Peng et al., 2017; Song et al., 2018; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016; Nguyen and Grishman, 2018a; Zhang et al., 2018). For a thorough review of the prior works, refer to the recent work (Gupta et al., 2019; Nan et al., 2020; Veyseh et al., 2020a) 5 Conclusion We proposed a new model for the MRE task. The introduced model employs a dynamic path reasoning component which"
2021.semeval-1.47,D18-1244,0,0.0123771,"on of the general Relation Extraction (RE) task. In the literature, RE has been tackled by feature-based methods (Zelenko et al., 2003; Zhou et al., 2005; Sun et al., 2011; Nguyen and Grishman, 2014; Nguyen et al., 2015c) and advanced deep learning models (Zeng et al., 2014; Wang et al., 2016; Lee et al., 2017; Zhang et al., 2017; Nguyen et al., 2019; Jin et al., 2018; Veyseh et al., 2020b). Recently, structure-aware deep models have shown significant improvement for RE (Peng et al., 2017; Song et al., 2018; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016; Nguyen and Grishman, 2018a; Zhang et al., 2018). For a thorough review of the prior works, refer to the recent work (Gupta et al., 2019; Nan et al., 2020; Veyseh et al., 2020a) 5 Conclusion We proposed a new model for the MRE task. The introduced model employs a dynamic path reasoning component which induces important context words to predict the relation between two measurement components. Furthermore, we proposed a novel regularization method based on Information Bottleneck to exclude noisy information from the input. Our experiments on the SemEval 2021 Task 8 reveal the effectiveness of the proposed model. 401 Acknowledgments This resea"
2021.semeval-1.47,D17-1004,0,0.0131131,"entation, i.e., hp , and the input document representation, i.e., hd , by replacing the Ldisc by Ldot = hp · hd . The results are presented in Table 2. This table shows that all components of the proposed model Related Work Measurement Relation Extraction (MRE) is one specific formulation of the general Relation Extraction (RE) task. In the literature, RE has been tackled by feature-based methods (Zelenko et al., 2003; Zhou et al., 2005; Sun et al., 2011; Nguyen and Grishman, 2014; Nguyen et al., 2015c) and advanced deep learning models (Zeng et al., 2014; Wang et al., 2016; Lee et al., 2017; Zhang et al., 2017; Nguyen et al., 2019; Jin et al., 2018; Veyseh et al., 2020b). Recently, structure-aware deep models have shown significant improvement for RE (Peng et al., 2017; Song et al., 2018; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016; Nguyen and Grishman, 2018a; Zhang et al., 2018). For a thorough review of the prior works, refer to the recent work (Gupta et al., 2019; Nan et al., 2020; Veyseh et al., 2020a) 5 Conclusion We proposed a new model for the MRE task. The introduced model employs a dynamic path reasoning component which induces important context words to predict the relation b"
2021.semeval-1.47,P05-1053,0,0.0670348,"lldot , this ablated model preserves the regularization component. However, instead of using Information Bottleneck, it directly decreases the similarity between the path representation, i.e., hp , and the input document representation, i.e., hd , by replacing the Ldisc by Ldot = hp · hd . The results are presented in Table 2. This table shows that all components of the proposed model Related Work Measurement Relation Extraction (MRE) is one specific formulation of the general Relation Extraction (RE) task. In the literature, RE has been tackled by feature-based methods (Zelenko et al., 2003; Zhou et al., 2005; Sun et al., 2011; Nguyen and Grishman, 2014; Nguyen et al., 2015c) and advanced deep learning models (Zeng et al., 2014; Wang et al., 2016; Lee et al., 2017; Zhang et al., 2017; Nguyen et al., 2019; Jin et al., 2018; Veyseh et al., 2020b). Recently, structure-aware deep models have shown significant improvement for RE (Peng et al., 2017; Song et al., 2018; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016; Nguyen and Grishman, 2018a; Zhang et al., 2018). For a thorough review of the prior works, refer to the recent work (Gupta et al., 2019; Nan et al., 2020; Veyseh et al., 2020a) 5 Co"
C18-2017,W04-1016,0,0.139672,"Missing"
C18-2017,W16-3201,1,0.88497,"Missing"
C18-2017,W11-0416,0,0.0268974,"Missing"
D17-2017,C96-2187,0,0.484448,"enmark, September 7–11, 2017. 2017 Association for Computational Linguistics 2 Related Work The NER engine’s ANN contains three layers: • Character-enhanced token-embedding layer, Existing publicly available NER systems geared toward non-experts do not use ANNs. For example, Stanford NER (Finkel et al., 2005), ABNER (Settles, 2005), the MITRE Identification Scrubber Toolkit (MIST) (Aberdeen et al., 2010), (Boag et al., 2015), BANNER (Leaman et al., 2008) and NERsuite (Cho et al., 2010) rely on CRFs. GAPSCORE uses SVMs (Chang et al., 2004). Apache cTAKES (Savova et al., 2010) and Gate’s ANNIE (Cunningham et al., 1996; Maynard and Cunningham, 2003) use mostly rules. NeuroNER, the first ANN-based NER system for non-experts, is more generalizable to new corpus due to the ANNs’ capability to learn effective features jointly with model parameters. Furthermore, in many cases, the NER systems assume that the user already has an annotated corpus formatted in a specific data format. As a result, users often have to connect their annotation tool with the NER systems by reformatting annotated data, which can be time-consuming and errorprone. Moreover, if users want to manually improve the annotations predicted by th"
D17-2017,S13-2009,0,0.0147339,"-to-use program for named-entity recognition based on neural networks Franck Dernoncourt∗ MIT francky@mit.edu Ji Young Lee∗ MIT jjylee@mit.edu Abstract Fully supervised approaches to NER include support vector machines (SVM) (Asahara and Matsumoto, 2003), maximum entropy models (Borthwick et al., 1998), decision trees (Sekine et al., 1998) as well as sequential tagging methods such as hidden Markov models (Bikel et al., 1997), Markov maximum entropy models (Kumar and Bhattacharyya, 2006), and conditional random fields (CRFs) (McCallum and Li, 2003; Tsai et al., 2006; Benajiba and Rosso, 2008; Filannino et al., 2013). Similar to rule-based systems, these approaches rely on handcrafted features, which are challenging and time-consuming to develop and may not generalize well to new datasets. More recently, artificial neural networks (ANNs) have been shown to outperform other supervised algorithms for NER (Collobert et al., 2011; Lample et al., 2016; Lee et al., 2016; Labeau et al., 2015; Dernoncourt et al., 2016). The effectiveness of ANNs can be attributed to their ability to learn effective features jointly with model parameters directly from the training dataset, instead of relying on handcrafted feature"
D17-2017,N03-1002,0,0.134501,"Missing"
D17-2017,W10-0713,0,0.0151477,"Missing"
D17-2017,P05-1045,0,0.0126444,"Missing"
D17-2017,D15-1025,0,0.00826651,"Missing"
D17-2017,D14-1162,0,0.112782,"the user may gain further insights on the ANN performances. 3.3 Pre-trained models 3.5 Some users may prefer not to train any ANN model, either due to time constraints or unavailable gold labels. For example, if the user wants to tag protected health information, they might not be able to have access to a labeled identifiable dataset. To address this need, NeuroNER provides a set of pre-trained models. Users are encouraged to contribute by uploading their own trained models. NeuroNER also comes with several pre-trained token embeddings, either with word2vec (Mikolov et al., 2013a,b) or GloVe (Pennington et al., 2014), which the NeuroNER engine can load easily once specified in the configuration file. 3.4 System requirements NeuroNER runs on Linux, Mac OS X, and Microsoft Windows. It requires Python 3.5, TensorFlow 1.0 (Abadi et al., 2016), scikit-learn (Pedregosa et al., 2011), and BRAT. A setup script is provided to make the installation straightforward. It can use the GPU if available, and the number of CPU threads and GPUs to use can be specified in the configuration file. 3.6 Performances To assess the quality of NeuroNER’s predictions, we use two publicly and freely available datasets for named-entit"
D17-2017,N16-1030,0,0.00894496,") as well as sequential tagging methods such as hidden Markov models (Bikel et al., 1997), Markov maximum entropy models (Kumar and Bhattacharyya, 2006), and conditional random fields (CRFs) (McCallum and Li, 2003; Tsai et al., 2006; Benajiba and Rosso, 2008; Filannino et al., 2013). Similar to rule-based systems, these approaches rely on handcrafted features, which are challenging and time-consuming to develop and may not generalize well to new datasets. More recently, artificial neural networks (ANNs) have been shown to outperform other supervised algorithms for NER (Collobert et al., 2011; Lample et al., 2016; Lee et al., 2016; Labeau et al., 2015; Dernoncourt et al., 2016). The effectiveness of ANNs can be attributed to their ability to learn effective features jointly with model parameters directly from the training dataset, instead of relying on handcrafted features developed from a specific dataset. However, ANNs remain challenging to use for non-expert users. Named-entity recognition (NER) aims at identifying entities of interest in a text. Artificial neural networks (ANNs) have recently been shown to outperform existing NER systems. However, ANNs remain challenging to use for non-expert user"
D17-2017,M98-1019,0,0.320904,"Missing"
D17-2017,W16-4204,1,0.816607,"al tagging methods such as hidden Markov models (Bikel et al., 1997), Markov maximum entropy models (Kumar and Bhattacharyya, 2006), and conditional random fields (CRFs) (McCallum and Li, 2003; Tsai et al., 2006; Benajiba and Rosso, 2008; Filannino et al., 2013). Similar to rule-based systems, these approaches rely on handcrafted features, which are challenging and time-consuming to develop and may not generalize well to new datasets. More recently, artificial neural networks (ANNs) have been shown to outperform other supervised algorithms for NER (Collobert et al., 2011; Lample et al., 2016; Lee et al., 2016; Labeau et al., 2015; Dernoncourt et al., 2016). The effectiveness of ANNs can be attributed to their ability to learn effective features jointly with model parameters directly from the training dataset, instead of relying on handcrafted features developed from a specific dataset. However, ANNs remain challenging to use for non-expert users. Named-entity recognition (NER) aims at identifying entities of interest in a text. Artificial neural networks (ANNs) have recently been shown to outperform existing NER systems. However, ANNs remain challenging to use for non-expert users. In this paper,"
D17-2017,E03-2009,0,0.0498865,"017. 2017 Association for Computational Linguistics 2 Related Work The NER engine’s ANN contains three layers: • Character-enhanced token-embedding layer, Existing publicly available NER systems geared toward non-experts do not use ANNs. For example, Stanford NER (Finkel et al., 2005), ABNER (Settles, 2005), the MITRE Identification Scrubber Toolkit (MIST) (Aberdeen et al., 2010), (Boag et al., 2015), BANNER (Leaman et al., 2008) and NERsuite (Cho et al., 2010) rely on CRFs. GAPSCORE uses SVMs (Chang et al., 2004). Apache cTAKES (Savova et al., 2010) and Gate’s ANNIE (Cunningham et al., 1996; Maynard and Cunningham, 2003) use mostly rules. NeuroNER, the first ANN-based NER system for non-experts, is more generalizable to new corpus due to the ANNs’ capability to learn effective features jointly with model parameters. Furthermore, in many cases, the NER systems assume that the user already has an annotated corpus formatted in a specific data format. As a result, users often have to connect their annotation tool with the NER systems by reformatting annotated data, which can be time-consuming and errorprone. Moreover, if users want to manually improve the annotations predicted by the NER system (e.g., if they use"
D17-2017,E12-2021,0,0.0240045,"Missing"
D17-2017,W03-0430,0,0.0355502,"Missing"
D17-2017,W03-0419,0,0.0192843,"Missing"
D17-2017,M98-1018,0,\N,Missing
D17-2017,N13-1090,0,\N,Missing
D17-2017,A97-2017,0,\N,Missing
D19-5413,J05-3002,0,0.451038,"nces are merged together. This work presents the first in-depth human evaluation of multiple diverse summarization models. We differentiate between two methods of shortening text: sentence compression and sentence fusion. Sentence compression reduces the length of a single sentence by removing words or rephrasing parts of the sentence (Cohn and Lapata, 2008; ∗ Amazon Alexa AI feiliu@cs.ucf.edu seokhwk@amazon.com Wang et al., 2013; Li et al., 2013, 2014; Filippova et al., 2015). Sentence fusion reduces two or more sentences to one by taking content from each sentence and merging them together (Barzilay and McKeown, 2005; McKeown et al., 2010; Thadani and McKeown, 2013). Compression is considered an easier task because unimportant clauses within the sentence can be removed while retaining the grammaticality and truth of the sentence (McDonald, 2006). In contrast, fusion requires selection of important content and stitching of that content in a grammatical and meaningful way. We focus on sentence fusion in this work. We examine the outputs of five abstractive summarization systems on CNN/DailyMail (Hermann et al., 2015) using human judgments. Particularly, we focus on summary sentences that involve sentence fu"
D19-5413,P18-1015,0,0.0197885,"kim,wachang}@adobe.com Abstract Introduction Modern abstractive summarizers excel at finding and extracting salient content (See et al., 2017; Chen and Bansal, 2018; Celikyilmaz et al., 2018; Liu and Lapata, 2019). However, one of the key tenets of summarization is consolidation of information, and these systems can struggle to combine content from multiple source texts, yielding output summaries that contain poor grammar and even incorrect facts. Truthfulness of summaries is a vitally important feature in order for summarization to be widely accepted in real-world applications (Reiter, 2018; Cao et al., 2018b). In this work, we perform an extensive analysis of summary outputs generated by state-of-the-art systems, examining features such as truthfulness to the original document, grammaticality, and method of how sentences are merged together. This work presents the first in-depth human evaluation of multiple diverse summarization models. We differentiate between two methods of shortening text: sentence compression and sentence fusion. Sentence compression reduces the length of a single sentence by removing words or rephrasing parts of the sentence (Cohn and Lapata, 2008; ∗ Amazon Alexa AI feiliu@"
D19-5413,N18-1150,0,0.235026,"vily on automatic metrics. However, ROUGE (Lin, 2004) and other n-gram based metrics are limited in evaluation power and do not tell the whole story (Novikova et al., 2017). They often focus on informativeness, which misses out on important facets These authors contributed equally to this work. 104 Proceedings of the 2nd Workshop on New Frontiers in Summarization, pages 104–110 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics System PG (See et al., 2017) Novel (Kryciski et al., 2018) Fast-Abs-RL (Chen and Bansal, 2018) Bottom-Up (Gehrmann et al., 2018) DCA (Celikyilmaz et al., 2018) Reference Summaries R-1 39.53 40.19 40.88 41.22 41.69 - ROUGE R-2 17.28 17.38 17.80 18.68 19.47 - R-L 36.38 37.52 38.54 38.34 37.92 - Compress 63.14 71.25 96.65 71.15 64.11 60.65 Created By Fuse Copy 6.44 30.24 19.77 5.39 0.83 2.21 16.35 11.76 23.96 7.07 31.93 1.36 Fail 0.18 3.59 0.31 0.74 4.86 6.06 Avg Summ Sent Len 15.7 11.8 15.6 10.7 14.5 19.3 Table 1: Comparison of state-of-the-art summarization systems. Middle column describes how summary sentences are generated. Compress: single sentence is shortened. Fuse: multiple sentences are merged. Copy: sentence is copied word-for-word. Fail: did"
D19-5413,P18-1063,0,0.369441,"valuation Setup Evaluation of summarization systems relies heavily on automatic metrics. However, ROUGE (Lin, 2004) and other n-gram based metrics are limited in evaluation power and do not tell the whole story (Novikova et al., 2017). They often focus on informativeness, which misses out on important facets These authors contributed equally to this work. 104 Proceedings of the 2nd Workshop on New Frontiers in Summarization, pages 104–110 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics System PG (See et al., 2017) Novel (Kryciski et al., 2018) Fast-Abs-RL (Chen and Bansal, 2018) Bottom-Up (Gehrmann et al., 2018) DCA (Celikyilmaz et al., 2018) Reference Summaries R-1 39.53 40.19 40.88 41.22 41.69 - ROUGE R-2 17.28 17.38 17.80 18.68 19.47 - R-L 36.38 37.52 38.54 38.34 37.92 - Compress 63.14 71.25 96.65 71.15 64.11 60.65 Created By Fuse Copy 6.44 30.24 19.77 5.39 0.83 2.21 16.35 11.76 23.96 7.07 31.93 1.36 Fail 0.18 3.59 0.31 0.74 4.86 6.06 Avg Summ Sent Len 15.7 11.8 15.6 10.7 14.5 19.3 Table 1: Comparison of state-of-the-art summarization systems. Middle column describes how summary sentences are generated. Compress: single sentence is shortened. Fuse: multiple senten"
D19-5413,D14-1085,0,0.0388355,"Missing"
D19-5413,D18-1207,0,0.075492,"Missing"
D19-5413,W19-4828,0,0.0791459,"Missing"
D19-5413,C08-1018,0,0.0273932,"rld applications (Reiter, 2018; Cao et al., 2018b). In this work, we perform an extensive analysis of summary outputs generated by state-of-the-art systems, examining features such as truthfulness to the original document, grammaticality, and method of how sentences are merged together. This work presents the first in-depth human evaluation of multiple diverse summarization models. We differentiate between two methods of shortening text: sentence compression and sentence fusion. Sentence compression reduces the length of a single sentence by removing words or rephrasing parts of the sentence (Cohn and Lapata, 2008; ∗ Amazon Alexa AI feiliu@cs.ucf.edu seokhwk@amazon.com Wang et al., 2013; Li et al., 2013, 2014; Filippova et al., 2015). Sentence fusion reduces two or more sentences to one by taking content from each sentence and merging them together (Barzilay and McKeown, 2005; McKeown et al., 2010; Thadani and McKeown, 2013). Compression is considered an easier task because unimportant clauses within the sentence can be removed while retaining the grammaticality and truth of the sentence (McDonald, 2006). In contrast, fusion requires selection of important content and stitching of that content in a gra"
D19-5413,P19-1209,1,0.913218,"on interface. A sentence from a random summarization system is shown along with four questions. • Replacement: a pronoun or description of an entity in one sentence is replaced by a different description of that entity in the other sentence. 16, and Reference: 100. The number of sentences we evaluate for each system is proportional to the number of observed fusion cases. In order to answer the Method of Merging and Coverage questions, the annotator must be provided with which two article sentences were fused together to create the summary sentence in question. We use the heuristic proposed by Lebanoff et al. (2019) to estimate which pair of sentences should be chosen. They use averaged ROUGE-1, 2, -L scores (Lin, 2004) to represent sentence similarity. The heuristic calculates the ROUGE similarity between the summary sentence and each article sentence. The article sentence with the highest similarity is chosen as the first sentence, then overlapping words are removed from the summary sentence. It continues to find the article sentence most similar to the remaining summary sentence, which is chosen as the second sentence. Our interface automatically highlights this pair of sentences (Figure 1). The same"
D19-5413,P19-1213,0,0.242649,"Missing"
D19-5413,D13-1047,1,0.892723,"Missing"
D19-5413,D14-1076,1,0.905021,"Missing"
D19-5413,D15-1042,0,0.0230084,"nerated by state-of-the-art systems, examining features such as truthfulness to the original document, grammaticality, and method of how sentences are merged together. This work presents the first in-depth human evaluation of multiple diverse summarization models. We differentiate between two methods of shortening text: sentence compression and sentence fusion. Sentence compression reduces the length of a single sentence by removing words or rephrasing parts of the sentence (Cohn and Lapata, 2008; ∗ Amazon Alexa AI feiliu@cs.ucf.edu seokhwk@amazon.com Wang et al., 2013; Li et al., 2013, 2014; Filippova et al., 2015). Sentence fusion reduces two or more sentences to one by taking content from each sentence and merging them together (Barzilay and McKeown, 2005; McKeown et al., 2010; Thadani and McKeown, 2013). Compression is considered an easier task because unimportant clauses within the sentence can be removed while retaining the grammaticality and truth of the sentence (McDonald, 2006). In contrast, fusion requires selection of important content and stitching of that content in a grammatical and meaningful way. We focus on sentence fusion in this work. We examine the outputs of five abstractive summariz"
D19-5413,C18-1121,0,0.0605997,"Missing"
D19-5413,D08-1019,0,0.505392,"Missing"
D19-5413,D18-1443,0,0.0879495,"arization systems relies heavily on automatic metrics. However, ROUGE (Lin, 2004) and other n-gram based metrics are limited in evaluation power and do not tell the whole story (Novikova et al., 2017). They often focus on informativeness, which misses out on important facets These authors contributed equally to this work. 104 Proceedings of the 2nd Workshop on New Frontiers in Summarization, pages 104–110 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics System PG (See et al., 2017) Novel (Kryciski et al., 2018) Fast-Abs-RL (Chen and Bansal, 2018) Bottom-Up (Gehrmann et al., 2018) DCA (Celikyilmaz et al., 2018) Reference Summaries R-1 39.53 40.19 40.88 41.22 41.69 - ROUGE R-2 17.28 17.38 17.80 18.68 19.47 - R-L 36.38 37.52 38.54 38.34 37.92 - Compress 63.14 71.25 96.65 71.15 64.11 60.65 Created By Fuse Copy 6.44 30.24 19.77 5.39 0.83 2.21 16.35 11.76 23.96 7.07 31.93 1.36 Fail 0.18 3.59 0.31 0.74 4.86 6.06 Avg Summ Sent Len 15.7 11.8 15.6 10.7 14.5 19.3 Table 1: Comparison of state-of-the-art summarization systems. Middle column describes how summary sentences are generated. Compress: single sentence is shortened. Fuse: multiple sentences are merged. Copy: sentence is"
D19-5413,W04-1013,0,0.191302,"un or description of an entity in one sentence is replaced by a different description of that entity in the other sentence. 16, and Reference: 100. The number of sentences we evaluate for each system is proportional to the number of observed fusion cases. In order to answer the Method of Merging and Coverage questions, the annotator must be provided with which two article sentences were fused together to create the summary sentence in question. We use the heuristic proposed by Lebanoff et al. (2019) to estimate which pair of sentences should be chosen. They use averaged ROUGE-1, 2, -L scores (Lin, 2004) to represent sentence similarity. The heuristic calculates the ROUGE similarity between the summary sentence and each article sentence. The article sentence with the highest similarity is chosen as the first sentence, then overlapping words are removed from the summary sentence. It continues to find the article sentence most similar to the remaining summary sentence, which is chosen as the second sentence. Our interface automatically highlights this pair of sentences (Figure 1). The same heuristic is also employed in deciding whether a summary sentence was generated by sentence compression or"
D19-5413,N15-1114,1,0.922296,"Missing"
D19-5413,D14-1168,0,0.0619594,"Missing"
D19-5413,P19-1500,0,0.118887,"Missing"
D19-5413,N19-1348,0,0.056554,"Missing"
D19-5413,W05-1612,0,0.738677,"Missing"
D19-5413,E06-1038,0,0.0547777,"educes the length of a single sentence by removing words or rephrasing parts of the sentence (Cohn and Lapata, 2008; ∗ Amazon Alexa AI feiliu@cs.ucf.edu seokhwk@amazon.com Wang et al., 2013; Li et al., 2013, 2014; Filippova et al., 2015). Sentence fusion reduces two or more sentences to one by taking content from each sentence and merging them together (Barzilay and McKeown, 2005; McKeown et al., 2010; Thadani and McKeown, 2013). Compression is considered an easier task because unimportant clauses within the sentence can be removed while retaining the grammaticality and truth of the sentence (McDonald, 2006). In contrast, fusion requires selection of important content and stitching of that content in a grammatical and meaningful way. We focus on sentence fusion in this work. We examine the outputs of five abstractive summarization systems on CNN/DailyMail (Hermann et al., 2015) using human judgments. Particularly, we focus on summary sentences that involve sentence fusion, since fusion is the task that requires the most improvement. We analyze several dimensions of the outputs, including faithfulness to the original article, grammaticality, and method of fusion. We present three main findings: Wh"
D19-5413,P13-1000,0,0.235438,"Missing"
D19-5413,N10-1044,0,0.0568235,"is work presents the first in-depth human evaluation of multiple diverse summarization models. We differentiate between two methods of shortening text: sentence compression and sentence fusion. Sentence compression reduces the length of a single sentence by removing words or rephrasing parts of the sentence (Cohn and Lapata, 2008; ∗ Amazon Alexa AI feiliu@cs.ucf.edu seokhwk@amazon.com Wang et al., 2013; Li et al., 2013, 2014; Filippova et al., 2015). Sentence fusion reduces two or more sentences to one by taking content from each sentence and merging them together (Barzilay and McKeown, 2005; McKeown et al., 2010; Thadani and McKeown, 2013). Compression is considered an easier task because unimportant clauses within the sentence can be removed while retaining the grammaticality and truth of the sentence (McDonald, 2006). In contrast, fusion requires selection of important content and stitching of that content in a grammatical and meaningful way. We focus on sentence fusion in this work. We examine the outputs of five abstractive summarization systems on CNN/DailyMail (Hermann et al., 2015) using human judgments. Particularly, we focus on summary sentences that involve sentence fusion, since fusion is"
D19-5413,W13-2117,0,0.2105,"Missing"
D19-5413,C18-1102,0,0.0563215,"Missing"
D19-5413,D17-1238,0,0.0620009,"Missing"
D19-5413,J18-3002,0,0.0218832,"du {dernonco,dkim,wachang}@adobe.com Abstract Introduction Modern abstractive summarizers excel at finding and extracting salient content (See et al., 2017; Chen and Bansal, 2018; Celikyilmaz et al., 2018; Liu and Lapata, 2019). However, one of the key tenets of summarization is consolidation of information, and these systems can struggle to combine content from multiple source texts, yielding output summaries that contain poor grammar and even incorrect facts. Truthfulness of summaries is a vitally important feature in order for summarization to be widely accepted in real-world applications (Reiter, 2018; Cao et al., 2018b). In this work, we perform an extensive analysis of summary outputs generated by state-of-the-art systems, examining features such as truthfulness to the original document, grammaticality, and method of how sentences are merged together. This work presents the first in-depth human evaluation of multiple diverse summarization models. We differentiate between two methods of shortening text: sentence compression and sentence fusion. Sentence compression reduces the length of a single sentence by removing words or rephrasing parts of the sentence (Cohn and Lapata, 2008; ∗ Amazo"
D19-5413,P17-1099,0,0.359587,"Missing"
D19-5413,C18-1146,1,0.890007,"Missing"
D19-5413,I13-1198,0,0.47836,"irst in-depth human evaluation of multiple diverse summarization models. We differentiate between two methods of shortening text: sentence compression and sentence fusion. Sentence compression reduces the length of a single sentence by removing words or rephrasing parts of the sentence (Cohn and Lapata, 2008; ∗ Amazon Alexa AI feiliu@cs.ucf.edu seokhwk@amazon.com Wang et al., 2013; Li et al., 2013, 2014; Filippova et al., 2015). Sentence fusion reduces two or more sentences to one by taking content from each sentence and merging them together (Barzilay and McKeown, 2005; McKeown et al., 2010; Thadani and McKeown, 2013). Compression is considered an easier task because unimportant clauses within the sentence can be removed while retaining the grammaticality and truth of the sentence (McDonald, 2006). In contrast, fusion requires selection of important content and stitching of that content in a grammatical and meaningful way. We focus on sentence fusion in this work. We examine the outputs of five abstractive summarization systems on CNN/DailyMail (Hermann et al., 2015) using human judgments. Particularly, we focus on summary sentences that involve sentence fusion, since fusion is the task that requires the m"
D19-5413,P13-1136,0,0.0201008,"n extensive analysis of summary outputs generated by state-of-the-art systems, examining features such as truthfulness to the original document, grammaticality, and method of how sentences are merged together. This work presents the first in-depth human evaluation of multiple diverse summarization models. We differentiate between two methods of shortening text: sentence compression and sentence fusion. Sentence compression reduces the length of a single sentence by removing words or rephrasing parts of the sentence (Cohn and Lapata, 2008; ∗ Amazon Alexa AI feiliu@cs.ucf.edu seokhwk@amazon.com Wang et al., 2013; Li et al., 2013, 2014; Filippova et al., 2015). Sentence fusion reduces two or more sentences to one by taking content from each sentence and merging them together (Barzilay and McKeown, 2005; McKeown et al., 2010; Thadani and McKeown, 2013). Compression is considered an easier task because unimportant clauses within the sentence can be removed while retaining the grammaticality and truth of the sentence (McDonald, 2006). In contrast, fusion requires selection of important content and stitching of that content in a grammatical and meaningful way. We focus on sentence fusion in this work. We"
D19-6203,W16-3009,0,0.0451508,"Missing"
D19-6203,W18-2311,0,0.0355959,"Missing"
D19-6203,H05-1091,0,0.829015,"tand how the entities are related to each other in the documents. In the literature, this problem is formalized as relation extraction (RE), an important task in information extraction. RE aims to identify the semantic relationships between two entity mentions within the same sentences in text. Due to its important applications on many areas of natural language processing (e.g., question answering, knowledge base construction), RE has been actively studied in the last decade, featuring a variety of feature-based or kernel-based models for this problem (Zelenko et al., 2002; Zhou et al., 2005; Bunescu and Mooney, 2005; Sun et al., 2011; Chan and Roth, 2010; Nguyen et al., 2009). Recently, the introduction of deep learning has produced a new generation of models for RE with 18 Proceedings of the 10th International Workshop on Health Text Mining and Information Analysis (LOUHI 2019), pages 18–27 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/D19-62 the input. The goal is to predict the semantic relation between these two entity mentions according to some predefined set of relations. Formally, let W = [w1 , w2 , . . . , wn ] be the input sentence wher"
D19-6203,C10-1018,0,0.829447,"her in the documents. In the literature, this problem is formalized as relation extraction (RE), an important task in information extraction. RE aims to identify the semantic relationships between two entity mentions within the same sentences in text. Due to its important applications on many areas of natural language processing (e.g., question answering, knowledge base construction), RE has been actively studied in the last decade, featuring a variety of feature-based or kernel-based models for this problem (Zelenko et al., 2002; Zhou et al., 2005; Bunescu and Mooney, 2005; Sun et al., 2011; Chan and Roth, 2010; Nguyen et al., 2009). Recently, the introduction of deep learning has produced a new generation of models for RE with 18 Proceedings of the 10th International Workshop on Health Text Mining and Information Analysis (LOUHI 2019), pages 18–27 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/D19-62 the input. The goal is to predict the semantic relation between these two entity mentions according to some predefined set of relations. Formally, let W = [w1 , w2 , . . . , wn ] be the input sentence where n is the number of tokens and wi is t"
D19-6203,P16-1105,0,0.548297,"the ability to automatically learn effective features for the sentences from data via various network architectures. The same trend has also been observed for RE in the biomedical domain where deep learning is gaining more and more attention from the research community (Mehryary et al., 2016; Bj¨orne and Salakoski, 2018; Nguyen and Verspoor, 2018; Verga et al., 2018). The typical deep learning models for RE have involved Convolutional Neural Networks (CNN) (Zeng et al., 2014; Nguyen and Grishman, 2015b; Zeng et al., 2015; Lin et al., 2016; Zeng et al., 2017), Recurrent Neural Networks (RNN), (Miwa and Bansal, 2016; Zhang et al., 2017), Transformer (self-attention) networks (Verga et al., 2018), and Graph Convolutional Neural Networks (GCNN) (Zhang et al., 2018b). There are two major common components in such deep learning models for RE, i.e., the representation component and the pooling component. First, in the representation component, some deep learning architectures are employed to compute a sequence of vectors to represent an input sentence for RE for which each vector tends to capture the specific context information for a word in that sentence. Such word-specific representation sequence is then f"
D19-6203,S13-2057,0,0.0173866,"ed into two separate sets (i.e., the training set and the validation set). BB3 also include a test set; however, the relation types for the examples in this test set are not provided. In order to obtain the performance of the models on the test set, the performers need to submit their system outputs to an official API that would evaluate the output and return the model performance. We train the models in this work on the training data and employ the official API to obtain their test set performance to be reported in the experiments for this dataset. Following the prior work on these datasets (Chowdhury and Lavelli, 2013; Lever and Jones, 2016; Zhou et al., 2018; Le et al., 2018), we use the micro-averaged F1 scores as the performance measure in the experiments to ensure a compatible comparison. FDEP 1 = max-poolai ∈SDP 1(M1 ,M2 ) (ai ) FEN T −DEP 1 = [FDEP 1 , FEN T −ON LY ] Once the overall representation vector F for the input sentence W and the two entity mentions of interest has been produced, we feed it into a feed-forward neural network with a softmax layer in the end to obtain the probability distribution P (y|W, M1 , M2 ) = feed-forward(F ) over the possible relation types for our RE problem. This pr"
D19-6203,W18-2314,0,0.0622919,"-of-the-art performance on many different benchmark datasets (Zeng et al., 2014; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Zhou et al., 2016; Wang et al., 2016; Zhang et al., 2017, 2018b). The advantage of deep learning over the previous approaches for RE is the ability to automatically learn effective features for the sentences from data via various network architectures. The same trend has also been observed for RE in the biomedical domain where deep learning is gaining more and more attention from the research community (Mehryary et al., 2016; Bj¨orne and Salakoski, 2018; Nguyen and Verspoor, 2018; Verga et al., 2018). The typical deep learning models for RE have involved Convolutional Neural Networks (CNN) (Zeng et al., 2014; Nguyen and Grishman, 2015b; Zeng et al., 2015; Lin et al., 2016; Zeng et al., 2017), Recurrent Neural Networks (RNN), (Miwa and Bansal, 2016; Zhang et al., 2017), Transformer (self-attention) networks (Verga et al., 2018), and Graph Convolutional Neural Networks (GCNN) (Zhang et al., 2018b). There are two major common components in such deep learning models for RE, i.e., the representation component and the pooling component. First, in the representation componen"
D19-6203,W16-3002,0,0.063042,"Missing"
D19-6203,P15-1061,0,0.567064,"e options for such components in the following sections. 2.2 The Representation Component for RE Given the input sequence of vectors V = [v1 , v2 , . . . , vn ], the next step in the deep learning models for RE is to transform this vector sequence into a more abstract vector sequence A = [a1 , a2 , . . . , an ] so ai would capture the underlying representation for the context information specific to the i-th word in the sentence. In this work, we examine the following typical architectures to obtain such an abstract sequence A for V : 1. CNN (Zeng et al., 2014; Nguyen and Grishman, 2015b; dos Santos et al., 2015): CNN is one of the early deep learning models for RE. It involves an 1D convolution layer over the input vector sequence V with multiple window sizes for the filters. CNN produces a sequence of vectors in which each vector capture some n-grams specific to a word in the sentence. This sequence of vectors is used as A for our purpose. 2. BiLSTM (Nguyen and Grishman, 2015a): In BiLSTM, two Long-short Term Memory Networks (LSTM) are run over the input vector sequence V in the forward and backward direction. The hidden vectors generated at the position i by the two networks are then concatenated t"
D19-6203,W15-1506,1,0.935855,"; Wang et al., 2016; Zhang et al., 2017, 2018b). The advantage of deep learning over the previous approaches for RE is the ability to automatically learn effective features for the sentences from data via various network architectures. The same trend has also been observed for RE in the biomedical domain where deep learning is gaining more and more attention from the research community (Mehryary et al., 2016; Bj¨orne and Salakoski, 2018; Nguyen and Verspoor, 2018; Verga et al., 2018). The typical deep learning models for RE have involved Convolutional Neural Networks (CNN) (Zeng et al., 2014; Nguyen and Grishman, 2015b; Zeng et al., 2015; Lin et al., 2016; Zeng et al., 2017), Recurrent Neural Networks (RNN), (Miwa and Bansal, 2016; Zhang et al., 2017), Transformer (self-attention) networks (Verga et al., 2018), and Graph Convolutional Neural Networks (GCNN) (Zhang et al., 2018b). There are two major common components in such deep learning models for RE, i.e., the representation component and the pooling component. First, in the representation component, some deep learning architectures are employed to compute a sequence of vectors to represent an input sentence for RE for which each vector tends to capture"
D19-6203,D09-1143,0,0.0389433,"In the literature, this problem is formalized as relation extraction (RE), an important task in information extraction. RE aims to identify the semantic relationships between two entity mentions within the same sentences in text. Due to its important applications on many areas of natural language processing (e.g., question answering, knowledge base construction), RE has been actively studied in the last decade, featuring a variety of feature-based or kernel-based models for this problem (Zelenko et al., 2002; Zhou et al., 2005; Bunescu and Mooney, 2005; Sun et al., 2011; Chan and Roth, 2010; Nguyen et al., 2009). Recently, the introduction of deep learning has produced a new generation of models for RE with 18 Proceedings of the 10th International Workshop on Health Text Mining and Information Analysis (LOUHI 2019), pages 18–27 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/D19-62 the input. The goal is to predict the semantic relation between these two entity mentions according to some predefined set of relations. Formally, let W = [w1 , w2 , . . . , wn ] be the input sentence where n is the number of tokens and wi is the ith word/token in W"
D19-6203,D18-1250,0,0.229712,"sequence of vectors is used as A for our purpose. 2. BiLSTM (Nguyen and Grishman, 2015a): In BiLSTM, two Long-short Term Memory Networks (LSTM) are run over the input vector sequence V in the forward and backward direction. The hidden vectors generated at the position i by the two networks are then concatenated to constitute the abstract vector ai for this position. Due to the recurrent nature, ai involves the context information over the whole input sentence W although a greater focus is put on the context of the current word. 3. BiLSTM-CNN: This models resembles the MASS model presented in (Le et al., 2018). It first applies a bidirectional LSTM layer over the input sequence V whose results are further processed by a Convolutional Neural Network (CNN) layer as in CNN. We also use the output of the CNN layer as the abstract vector sequence A for this model. 4. BiLSTM-GCNN (Zhang et al., 2018b): Similar to BiLSTM-CNN, BiLSTM-GCNN also first employs a bidirectional LSTM network to abstract the input vector sequence V . However, in the second step, different from BiLSTM-CNN, BiLSTMGCNN introduces a Graph Convolutional Neural 2.3 The Pooling Component for RE The goal of the pooling component is to ag"
D19-6203,P11-1053,0,0.842044,"related to each other in the documents. In the literature, this problem is formalized as relation extraction (RE), an important task in information extraction. RE aims to identify the semantic relationships between two entity mentions within the same sentences in text. Due to its important applications on many areas of natural language processing (e.g., question answering, knowledge base construction), RE has been actively studied in the last decade, featuring a variety of feature-based or kernel-based models for this problem (Zelenko et al., 2002; Zhou et al., 2005; Bunescu and Mooney, 2005; Sun et al., 2011; Chan and Roth, 2010; Nguyen et al., 2009). Recently, the introduction of deep learning has produced a new generation of models for RE with 18 Proceedings of the 10th International Workshop on Health Text Mining and Information Analysis (LOUHI 2019), pages 18–27 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/D19-62 the input. The goal is to predict the semantic relation between these two entity mentions according to some predefined set of relations. Formally, let W = [w1 , w2 , . . . , wn ] be the input sentence where n is the number"
D19-6203,N18-1080,0,0.387214,"many different benchmark datasets (Zeng et al., 2014; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Zhou et al., 2016; Wang et al., 2016; Zhang et al., 2017, 2018b). The advantage of deep learning over the previous approaches for RE is the ability to automatically learn effective features for the sentences from data via various network architectures. The same trend has also been observed for RE in the biomedical domain where deep learning is gaining more and more attention from the research community (Mehryary et al., 2016; Bj¨orne and Salakoski, 2018; Nguyen and Verspoor, 2018; Verga et al., 2018). The typical deep learning models for RE have involved Convolutional Neural Networks (CNN) (Zeng et al., 2014; Nguyen and Grishman, 2015b; Zeng et al., 2015; Lin et al., 2016; Zeng et al., 2017), Recurrent Neural Networks (RNN), (Miwa and Bansal, 2016; Zhang et al., 2017), Transformer (self-attention) networks (Verga et al., 2018), and Graph Convolutional Neural Networks (GCNN) (Zhang et al., 2018b). There are two major common components in such deep learning models for RE, i.e., the representation component and the pooling component. First, in the representation component, some deep learning"
D19-6203,P16-1200,0,0.0250144,". The advantage of deep learning over the previous approaches for RE is the ability to automatically learn effective features for the sentences from data via various network architectures. The same trend has also been observed for RE in the biomedical domain where deep learning is gaining more and more attention from the research community (Mehryary et al., 2016; Bj¨orne and Salakoski, 2018; Nguyen and Verspoor, 2018; Verga et al., 2018). The typical deep learning models for RE have involved Convolutional Neural Networks (CNN) (Zeng et al., 2014; Nguyen and Grishman, 2015b; Zeng et al., 2015; Lin et al., 2016; Zeng et al., 2017), Recurrent Neural Networks (RNN), (Miwa and Bansal, 2016; Zhang et al., 2017), Transformer (self-attention) networks (Verga et al., 2018), and Graph Convolutional Neural Networks (GCNN) (Zhang et al., 2018b). There are two major common components in such deep learning models for RE, i.e., the representation component and the pooling component. First, in the representation component, some deep learning architectures are employed to compute a sequence of vectors to represent an input sentence for RE for which each vector tends to capture the specific context information for"
D19-6203,P15-2047,0,0.326704,"Missing"
D19-6203,P16-1123,0,0.472654,"Missing"
D19-6203,C16-1138,0,0.229098,"for deep relation extraction on the same setting. In the experiments, we find that syntactic information (i.e., dependency parsing) can be exploited to provide the best pooling strategies for biomedical RE. In fact, our experiments also suggest that it is more beneficial to apply the syntactic information in the pooling component of the deep learning models for biomedical RE than that in the representation component. This is different from most of the prior work on relation extraction that has only employed the syntactic information in the representation component of the deep learning models (Xu et al., 2016; Miwa and Bansal, 2016). Based on the syntax-based pooling mechanism, we achieve the state-of-the-art performance on two benchmark datasets for biomedical RE. 2 2.1 Input Vector Representation In order to encode the positions and the entity types of the two entity mentions in the input sentence, following (Zhang et al., 2018b), we first replace the tokens in the entity mentions M1 and M2 with the special tokens of format M1 -Type1 and M2 -Type2 respectively (Type1 and Type2 represent the entity types of M1 and M2 respectively). The purpose of this replacement is to help the models to abstract"
D19-6203,D15-1206,0,0.272494,"Missing"
D19-6203,W02-1010,0,0.425759,"entities in text, it is crucial to understand how the entities are related to each other in the documents. In the literature, this problem is formalized as relation extraction (RE), an important task in information extraction. RE aims to identify the semantic relationships between two entity mentions within the same sentences in text. Due to its important applications on many areas of natural language processing (e.g., question answering, knowledge base construction), RE has been actively studied in the last decade, featuring a variety of feature-based or kernel-based models for this problem (Zelenko et al., 2002; Zhou et al., 2005; Bunescu and Mooney, 2005; Sun et al., 2011; Chan and Roth, 2010; Nguyen et al., 2009). Recently, the introduction of deep learning has produced a new generation of models for RE with 18 Proceedings of the 10th International Workshop on Health Text Mining and Information Analysis (LOUHI 2019), pages 18–27 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/D19-62 the input. The goal is to predict the semantic relation between these two entity mentions according to some predefined set of relations. Formally, let W = [w1 ,"
D19-6203,D15-1203,0,0.48593,"t al., 2017, 2018b). The advantage of deep learning over the previous approaches for RE is the ability to automatically learn effective features for the sentences from data via various network architectures. The same trend has also been observed for RE in the biomedical domain where deep learning is gaining more and more attention from the research community (Mehryary et al., 2016; Bj¨orne and Salakoski, 2018; Nguyen and Verspoor, 2018; Verga et al., 2018). The typical deep learning models for RE have involved Convolutional Neural Networks (CNN) (Zeng et al., 2014; Nguyen and Grishman, 2015b; Zeng et al., 2015; Lin et al., 2016; Zeng et al., 2017), Recurrent Neural Networks (RNN), (Miwa and Bansal, 2016; Zhang et al., 2017), Transformer (self-attention) networks (Verga et al., 2018), and Graph Convolutional Neural Networks (GCNN) (Zhang et al., 2018b). There are two major common components in such deep learning models for RE, i.e., the representation component and the pooling component. First, in the representation component, some deep learning architectures are employed to compute a sequence of vectors to represent an input sentence for RE for which each vector tends to capture the specific contex"
D19-6203,C14-1220,0,0.804017,"; Zhou et al., 2016; Wang et al., 2016; Zhang et al., 2017, 2018b). The advantage of deep learning over the previous approaches for RE is the ability to automatically learn effective features for the sentences from data via various network architectures. The same trend has also been observed for RE in the biomedical domain where deep learning is gaining more and more attention from the research community (Mehryary et al., 2016; Bj¨orne and Salakoski, 2018; Nguyen and Verspoor, 2018; Verga et al., 2018). The typical deep learning models for RE have involved Convolutional Neural Networks (CNN) (Zeng et al., 2014; Nguyen and Grishman, 2015b; Zeng et al., 2015; Lin et al., 2016; Zeng et al., 2017), Recurrent Neural Networks (RNN), (Miwa and Bansal, 2016; Zhang et al., 2017), Transformer (self-attention) networks (Verga et al., 2018), and Graph Convolutional Neural Networks (GCNN) (Zhang et al., 2018b). There are two major common components in such deep learning models for RE, i.e., the representation component and the pooling component. First, in the representation component, some deep learning architectures are employed to compute a sequence of vectors to represent an input sentence for RE for which e"
D19-6203,D17-1186,0,0.0136087,"deep learning over the previous approaches for RE is the ability to automatically learn effective features for the sentences from data via various network architectures. The same trend has also been observed for RE in the biomedical domain where deep learning is gaining more and more attention from the research community (Mehryary et al., 2016; Bj¨orne and Salakoski, 2018; Nguyen and Verspoor, 2018; Verga et al., 2018). The typical deep learning models for RE have involved Convolutional Neural Networks (CNN) (Zeng et al., 2014; Nguyen and Grishman, 2015b; Zeng et al., 2015; Lin et al., 2016; Zeng et al., 2017), Recurrent Neural Networks (RNN), (Miwa and Bansal, 2016; Zhang et al., 2017), Transformer (self-attention) networks (Verga et al., 2018), and Graph Convolutional Neural Networks (GCNN) (Zhang et al., 2018b). There are two major common components in such deep learning models for RE, i.e., the representation component and the pooling component. First, in the representation component, some deep learning architectures are employed to compute a sequence of vectors to represent an input sentence for RE for which each vector tends to capture the specific context information for a word in that sente"
D19-6203,D18-1244,0,0.234362,"ed for RE in the biomedical domain where deep learning is gaining more and more attention from the research community (Mehryary et al., 2016; Bj¨orne and Salakoski, 2018; Nguyen and Verspoor, 2018; Verga et al., 2018). The typical deep learning models for RE have involved Convolutional Neural Networks (CNN) (Zeng et al., 2014; Nguyen and Grishman, 2015b; Zeng et al., 2015; Lin et al., 2016; Zeng et al., 2017), Recurrent Neural Networks (RNN), (Miwa and Bansal, 2016; Zhang et al., 2017), Transformer (self-attention) networks (Verga et al., 2018), and Graph Convolutional Neural Networks (GCNN) (Zhang et al., 2018b). There are two major common components in such deep learning models for RE, i.e., the representation component and the pooling component. First, in the representation component, some deep learning architectures are employed to compute a sequence of vectors to represent an input sentence for RE for which each vector tends to capture the specific context information for a word in that sentence. Such word-specific representation sequence is then fed into the second pooling component (e.g., max pooling) that aggregates the representation vectors to obtain an overall vector to represent the whol"
D19-6203,D17-1004,0,0.583532,"cally learn effective features for the sentences from data via various network architectures. The same trend has also been observed for RE in the biomedical domain where deep learning is gaining more and more attention from the research community (Mehryary et al., 2016; Bj¨orne and Salakoski, 2018; Nguyen and Verspoor, 2018; Verga et al., 2018). The typical deep learning models for RE have involved Convolutional Neural Networks (CNN) (Zeng et al., 2014; Nguyen and Grishman, 2015b; Zeng et al., 2015; Lin et al., 2016; Zeng et al., 2017), Recurrent Neural Networks (RNN), (Miwa and Bansal, 2016; Zhang et al., 2017), Transformer (self-attention) networks (Verga et al., 2018), and Graph Convolutional Neural Networks (GCNN) (Zhang et al., 2018b). There are two major common components in such deep learning models for RE, i.e., the representation component and the pooling component. First, in the representation component, some deep learning architectures are employed to compute a sequence of vectors to represent an input sentence for RE for which each vector tends to capture the specific context information for a word in that sentence. Such word-specific representation sequence is then fed into the second po"
D19-6203,P05-1053,0,0.935285,"s crucial to understand how the entities are related to each other in the documents. In the literature, this problem is formalized as relation extraction (RE), an important task in information extraction. RE aims to identify the semantic relationships between two entity mentions within the same sentences in text. Due to its important applications on many areas of natural language processing (e.g., question answering, knowledge base construction), RE has been actively studied in the last decade, featuring a variety of feature-based or kernel-based models for this problem (Zelenko et al., 2002; Zhou et al., 2005; Bunescu and Mooney, 2005; Sun et al., 2011; Chan and Roth, 2010; Nguyen et al., 2009). Recently, the introduction of deep learning has produced a new generation of models for RE with 18 Proceedings of the 10th International Workshop on Health Text Mining and Information Analysis (LOUHI 2019), pages 18–27 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/D19-62 the input. The goal is to predict the semantic relation between these two entity mentions according to some predefined set of relations. Formally, let W = [w1 , w2 , . . . , wn ]"
D19-6203,P16-2034,0,0.0557972,"omponent of the deep learning models for biomedical RE. 4 Traditional work on RE has mostly used feature engineering with syntactical information for statistical or kernel based classifiers (Zelenko et al., 2002; Zhou et al., 2005; Bunescu and Mooney, 2005; Sun et al., 2011; Chan and Roth, 2010). Recently, deep learning has been shown to advance many benchmark datasets for this RE problem due to its representation learning capacity. The typical architectures for such deep learning models involve CNN, LSTM, the attention mechanism and their variants (Zeng et al., 2014; dos Santos et al., 2015; Zhou et al., 2016; Wang et al., 2016; Nguyen and Grishman, 2015a; Miwa and Bansal, 2016; Zhang et al., 2017, 2018b). Deep learning has also been applied to biomedical RE in the last couple of years and started to demonstrate much potentials for this area (Mehryary et al., 2016; Bj¨orne and Salakoski, 2018; Nguyen and Verspoor, 2018; Verga et al., 2018). Pooling is a common and crucial component in most of the deep learning models for RE. (Nguyen and Grishman, 2015b; dos Santos et al., 2015) apply the pooling operation over the whole sentence for RE while Zeng et al. (2015) proposes the dynamic pooling mechanis"
E17-2110,U12-1017,0,0.496152,"Missing"
E17-2110,P14-1062,0,0.00904547,"cs y1 tionaries, cue words), semantic (synonyms, hyponyms), structural (part-of-speech tags, headings), and sequential (sentenced position, surrounding features) information. On the other hand, recent approaches to natural language processing (NLP) based on artificial neural networks (ANNs) do not require manual features, as they are trained to automatically learn features based on word as well as character embeddings. Moreover, ANN-based models have achieved state-of-the-art results on various NLP tasks, including the most relevant task of text classification (Socher et al., 2013; Kim, 2014; Kalchbrenner et al., 2014; Zhang et al., 2015; Conneau et al., 2016; Xiao and Cho, 2016; dos Santos and Gatti, 2014). For text classification, many ANN models use word embeddings (Socher et al., 2013; Kim, 2014; Kalchbrenner et al., 2014; Gehrmann et al., 2017), and most recent works are based on character embeddings (Zhang et al., 2015; Conneau et al., 2016; Xiao and Cho, 2016). Approaches combining word and character embeddings have also been explored (dos Santos and Gatti, 2014; Dernoncourt et al., 2016). However, most existing works using ANNs for short-text classification do not use any context. This is in contra"
E17-2110,D14-1181,0,0.00452501,"l Linguistics y1 tionaries, cue words), semantic (synonyms, hyponyms), structural (part-of-speech tags, headings), and sequential (sentenced position, surrounding features) information. On the other hand, recent approaches to natural language processing (NLP) based on artificial neural networks (ANNs) do not require manual features, as they are trained to automatically learn features based on word as well as character embeddings. Moreover, ANN-based models have achieved state-of-the-art results on various NLP tasks, including the most relevant task of text classification (Socher et al., 2013; Kim, 2014; Kalchbrenner et al., 2014; Zhang et al., 2015; Conneau et al., 2016; Xiao and Cho, 2016; dos Santos and Gatti, 2014). For text classification, many ANN models use word embeddings (Socher et al., 2013; Kim, 2014; Kalchbrenner et al., 2014; Gehrmann et al., 2017), and most recent works are based on character embeddings (Zhang et al., 2015; Conneau et al., 2016; Xiao and Cho, 2016). Approaches combining word and character embeddings have also been explored (dos Santos and Gatti, 2014; Dernoncourt et al., 2016). However, most existing works using ANNs for short-text classification do not use any"
E17-2110,I17-2052,1,0.0974156,"n A(n−1,y n−1 ) |yn−1 ∈ Y2 , computing A |y ∈ Y takes Θ(|C |) time given (n,yn ) n  A(n−1,y  n−1 ) |yn−1 ∈ Y . Consequently, computing A(n,yn ) |yn ∈ Y takes O(n|C|2 ) time. 4 4.1 Experiments Datasets We evaluate our model on the sentence classification task using the following two medical abstract datasets, where each sentence of the abstract is annotated with one label. Table 1 presents statistics on each dataset. NICTA-PIBOSO This dataset was introduced in (Kim et al., 2011) and was the basis of the ALTA 2012 Shared Task (Amini et al., 2012). PubMed 20k RCT This corpus was introduced in (Dernoncourt et al., 2017)1 . It is based on the PubMed database of biomedical literature and uses 5 sentence labels: objectives, background, methods, results and conclusions T [yi−1 , yi ]. i=2 These scores can be turned into probabilities of the label sequences by taking a softmax function over all possible label sequences: p(ˆ y1:n ) = n−1  3.1.3 Label sequence optimization layer The label sequence optimization layer takes the sequence of probability vectors a1:n from the label prediction layer as input, and outputs a sequence of labels y1:n , where yi is the label assigned to the token xi . In order to model depen"
E17-2110,N16-1062,1,0.905377,"most recent works are based on character embeddings (Zhang et al., 2015; Conneau et al., 2016; Xiao and Cho, 2016). Approaches combining word and character embeddings have also been explored (dos Santos and Gatti, 2014; Dernoncourt et al., 2016). However, most existing works using ANNs for short-text classification do not use any context. This is in contrast with sequential sentence classification, where each sentence in a text is classified taking into account its context, i.e. the surrounding sentences and possibly the whole text. One exception is a recent work on dialog act classification (Lee and Dernoncourt, 2016), where each utterance in a dialog is classified into its dialog act, but only the preceding utterances were used, as the system was designed with real-time applications in mind. 3 a1 yn-1 yn an-1 an … … aj a2 Feed forward s bi-LSTM concatanate e1 … … … … bi-LSTM em ei e2 concatenate c t concatanate Token embeddings … … c1 c2 cl-1 cl Character embeddings z1 z2 zl-1 zl x Figure 1: ANN model for sequential sentence classification. x: token, t: token embeddings (300), zi : ith character of x, ci : character embeddings (25), c: character-based token embeddings (50), ei : hybrid token embeddings (3"
E17-2110,C14-1008,0,0.00752812,"headings), and sequential (sentenced position, surrounding features) information. On the other hand, recent approaches to natural language processing (NLP) based on artificial neural networks (ANNs) do not require manual features, as they are trained to automatically learn features based on word as well as character embeddings. Moreover, ANN-based models have achieved state-of-the-art results on various NLP tasks, including the most relevant task of text classification (Socher et al., 2013; Kim, 2014; Kalchbrenner et al., 2014; Zhang et al., 2015; Conneau et al., 2016; Xiao and Cho, 2016; dos Santos and Gatti, 2014). For text classification, many ANN models use word embeddings (Socher et al., 2013; Kim, 2014; Kalchbrenner et al., 2014; Gehrmann et al., 2017), and most recent works are based on character embeddings (Zhang et al., 2015; Conneau et al., 2016; Xiao and Cho, 2016). Approaches combining word and character embeddings have also been explored (dos Santos and Gatti, 2014; Dernoncourt et al., 2016). However, most existing works using ANNs for short-text classification do not use any context. This is in contrast with sequential sentence classification, where each sentence in a text is classified tak"
E17-2110,W06-3309,0,0.0460733,"ocate the information of interest is highly desired, as it may reduce the time required to locate relevant information. When researchers search for previous literature, for example, they often skim through abstracts in order to quickly check whether the papers match ∗ Peter Szolovits MIT psz@mit.edu 2 Related Work Existing systems for sequential sentence classification are mostly based on naive Bayes (Ruch et al., 2007; Huang et al., 2013), support vector machine (McKnight and Srinivasan, 2003; Yamamoto and Takagi, 2005; Hirohata et al., 2008; Yamamoto and Takagi, 2005), Hidden Markov models (Lin et al., 2006), and conditional random fields (CRFs) (Kim et al., 2011; Hassanzadeh et al., 2014; Hirohata et al., 2008). They often require numerous hand-engineered features based on lexical (bag-of-words, n-grams, dicThese authors contributed equally to this work. 694 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 694–700, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics y1 tionaries, cue words), semantic (synonyms, hyponyms), structural (part-of-speech tags, headings), and sequent"
E17-2110,U12-1019,0,0.0913118,"Dernoncourt and Lee, 2016). 5 PubMed 20k 89.9 89.7 88.7 88.9 85.0 Table 3: Ablation analysis. F1-scores are reported. “- character emb” is our model using only token embeddings, without character-based token embeddings. “- pre-train” is our model where token embeddings are initialized with random values instead of pre-trained embeddings. “- token emb” is our model using only character-based token embeddings, without token embeddings. “- seq opt” is our model without the label sequence optimization layer. Co Table 2: F1-scores on the test set with several baselines, the best published method (Lui, 2012) from the literature, and our model. Since PubMed 20k RCT was introduced in this work, there is no previously published method for this dataset. The presented results for the ANN-based models are the F1-scores on the test set of the run with the highest F1score on the validation set. tho PubMed 20k 83.1 86.1 89.5 – 90.0 Me Model LR Forward ANN CRF Best published Our model Figure 2: Transition matrix learned on PubMed 20k RCT. The rows represent the label of the previous sentence, the columns represent the label of the current sentence. Table 2 compares our model against several baselines as we"
E17-2110,N13-1090,0,0.00220868,"eddings are initialized with GloVe (Pennington et al., 2014) embeddings pretrained on Wikipedia and Gigaword 5 (Parker et al., 2011). Replacing LSTMs with convolutional neural networks did not improve the results: we therefore use LSTMs. 3.1.1 Hybrid token embedding layer The hybrid token embedding layer takes a token as an input and outputs its vector representation utilizing both the token embeddings and as well as the character embeddings. Token embeddings are a direct mapping VT (·) from token to vector, which can be pre-trained on large unlabeled datasets using programs such as word2vec (Mikolov et al., 2013b; Mikolov et al., 2013a; Mikolov et al., 2013c) or GloVe (Pennington et al., 2014). Character embeddings are also defined in an analogous manner, as a direct mapping VC (·) from character to vector. Let z1:` be the sequence of characters that comprise a token x. Each character zi is first mapped to its embedding ci = VC (zi ), and the resulting sequence c1:` is input to a bidirectional LSTM, which outputs the character-based token embedding c. Model In the following, we denote scalars in italic lowercase (e.g., k, bf ), vectors in bold lowercase (e.g., s, xi ), and matrices in italic uppercas"
E17-2110,D14-1162,0,0.11738,"Missing"
E17-2110,D13-1170,0,0.00149314,"tion for Computational Linguistics y1 tionaries, cue words), semantic (synonyms, hyponyms), structural (part-of-speech tags, headings), and sequential (sentenced position, surrounding features) information. On the other hand, recent approaches to natural language processing (NLP) based on artificial neural networks (ANNs) do not require manual features, as they are trained to automatically learn features based on word as well as character embeddings. Moreover, ANN-based models have achieved state-of-the-art results on various NLP tasks, including the most relevant task of text classification (Socher et al., 2013; Kim, 2014; Kalchbrenner et al., 2014; Zhang et al., 2015; Conneau et al., 2016; Xiao and Cho, 2016; dos Santos and Gatti, 2014). For text classification, many ANN models use word embeddings (Socher et al., 2013; Kim, 2014; Kalchbrenner et al., 2014; Gehrmann et al., 2017), and most recent works are based on character embeddings (Zhang et al., 2015; Conneau et al., 2016; Xiao and Cho, 2016). Approaches combining word and character embeddings have also been explored (dos Santos and Gatti, 2014; Dernoncourt et al., 2016). However, most existing works using ANNs for short-text classification do"
F12-3025,W96-0304,0,0.257148,"Missing"
F12-3025,2001.mtsummit-papers.68,0,0.0480964,"Missing"
F12-3025,perez-etal-2004-application,0,0.0609353,"Missing"
I17-2052,I08-1050,0,0.767523,"Missing"
I17-2052,J00-3003,0,0.166222,"Missing"
I17-2052,N16-1062,1,0.81691,"ence. Figure 6 shows the distribution of the number of sentences per abstract. Figures 4, 5 and 6 are based on PubMed 200k RCT. Objective 191,403 Background 201,927 We report the performance of several systems to characterize our dataset. The first baseline is a classifier based on logistic regression (LR) using n-gram features extracted from the current sentence: it does not use any information from the surrounding sentences. This baseline was implemented with scikit-learn (Pedregosa et al., 2011). The second baseline (Forward ANN) uses the artificial neural network (ANN) model presented in (Lee and Dernoncourt, 2016): it computes sentence embeddings for each sentence, then classifies the current sentence given a few preceding sentence embeddings as well as the current sentence embedding. The third baseline is a conditional random field (CRF) that uses n-grams as features: each output variable of the CRF corresponds to a label for a sentence, and the sequence the CRF considers is the entire abstract. The CRF baseline therefore uses both preceding and succeeding sentences when classifying the current sentence. CRFs have been shown to give strong performances for sequential sentence classification (Amini et"
I17-2052,P14-5010,0,0.0019728,"Missing"
I17-2052,E17-2110,1,\N,Missing
I17-2052,U12-1017,0,\N,Missing
L18-1509,D17-1320,0,0.0475018,"Missing"
L18-1509,D15-1229,0,0.0463421,"Missing"
L18-1509,P16-1195,0,0.0159069,"oice recording (e.g., meetings or presentations) (Zhang et al., 2007). Different type of inputs may also be combined to perform the summary, which is a task referred to as multi-modal summarization (Li et al., 2017). 2.4. Meta-information When choosing a suitable corpus to train or evaluate a summarization algorithm, many parameters must be taken into account, including: • Source code summarization: it aims at either summarizing in a human language what a code snippet performs, or automatically folding blocks of code that are deemed less informative (also referred as the autofolding problem). Iyer et al. (2016) compiled a corpus from StackOverflow to summarize source code into English. Fowkes et al. (2017) presented a system to perform autofolding and created a corpus based on the source code of the top six most popular Java projects on GitHub. • Overview synthesis: the task is very similar to multi-document summarization, except that the output is much longer than a typical summary. Zhang and Wan (2017) constructed a corpus based on Wikinews, where each Wikinews is regarded as the gold overview, while the linked news articles are the input of the overview synthesis system. • Sentence fusion: this t"
L18-1509,J02-4006,0,0.200494,"Missing"
L18-1509,W14-1504,0,0.0373444,"Missing"
L18-1509,D17-1114,0,0.0538064,"Missing"
L18-1509,W02-0406,0,0.149519,"text are referred to as compressive.1 Summarization systems may span single, or multipledocuments, and can produce outputs of varying lengths and structures. When the original text spans over multiple documents, the task is called multi-document summarization. The length of the summaries differs across corpora: for example, sentence-level summarization aims at summarizing a text into a single sentence, typically abstractively, and headline generation aims at summarizing the text into a headline, which tends to be shorter than a sentence. Summarization is a subjective task (Rath et al., 1961; Lin and Hovy, 2002), requiring human input to assess performance. Generic summarization aims at creating a summary that is as reader-independent as possible, i.e. satisfying as many readers as possible. There has been some work on non-generic summarization, such as query-based and topicbased summarization, which bias the summary toward a query or a topic expressed by the intended reader (Hand, 1997). Automated evaluation methods have been developed. The most widely used automated evaluation metric for summarization is ROUGE and its variants (Lin and Hovy, 2003; Lin, 2004), followed by METEOR (Banerjee and Lavie,"
L18-1509,N03-1020,0,0.542673,"marization is a subjective task (Rath et al., 1961; Lin and Hovy, 2002), requiring human input to assess performance. Generic summarization aims at creating a summary that is as reader-independent as possible, i.e. satisfying as many readers as possible. There has been some work on non-generic summarization, such as query-based and topicbased summarization, which bias the summary toward a query or a topic expressed by the intended reader (Hand, 1997). Automated evaluation methods have been developed. The most widely used automated evaluation metric for summarization is ROUGE and its variants (Lin and Hovy, 2003; Lin, 2004), followed by METEOR (Banerjee and Lavie, 2005). Other metrics include Basic Elements (Hovy et al., 2005), LSA-based evaluation measures (Steinberger and Jeˇzek, 2012) and SIRA (Cohan and Goharian, 2016). Ideally, one can perform human-based evaluation strategies, such as the pyramid method (Nenkova and Passonneau, 2004). Given the diversity of summarization approaches, and assessment protocols, it may be challenging for researchers to identify the subset of corpora that are best-suited for a given summarization research task. In this paper, we attempt to solve this problem by pres"
L18-1509,W04-1013,0,0.0889731,"ective task (Rath et al., 1961; Lin and Hovy, 2002), requiring human input to assess performance. Generic summarization aims at creating a summary that is as reader-independent as possible, i.e. satisfying as many readers as possible. There has been some work on non-generic summarization, such as query-based and topicbased summarization, which bias the summary toward a query or a topic expressed by the intended reader (Hand, 1997). Automated evaluation methods have been developed. The most widely used automated evaluation metric for summarization is ROUGE and its variants (Lin and Hovy, 2003; Lin, 2004), followed by METEOR (Banerjee and Lavie, 2005). Other metrics include Basic Elements (Hovy et al., 2005), LSA-based evaluation measures (Steinberger and Jeˇzek, 2012) and SIRA (Cohan and Goharian, 2016). Ideally, one can perform human-based evaluation strategies, such as the pyramid method (Nenkova and Passonneau, 2004). Given the diversity of summarization approaches, and assessment protocols, it may be challenging for researchers to identify the subset of corpora that are best-suited for a given summarization research task. In this paper, we attempt to solve this problem by presenting an ov"
L18-1509,N04-1019,0,0.0900182,"-based and topicbased summarization, which bias the summary toward a query or a topic expressed by the intended reader (Hand, 1997). Automated evaluation methods have been developed. The most widely used automated evaluation metric for summarization is ROUGE and its variants (Lin and Hovy, 2003; Lin, 2004), followed by METEOR (Banerjee and Lavie, 2005). Other metrics include Basic Elements (Hovy et al., 2005), LSA-based evaluation measures (Steinberger and Jeˇzek, 2012) and SIRA (Cohan and Goharian, 2016). Ideally, one can perform human-based evaluation strategies, such as the pyramid method (Nenkova and Passonneau, 2004). Given the diversity of summarization approaches, and assessment protocols, it may be challenging for researchers to identify the subset of corpora that are best-suited for a given summarization research task. In this paper, we attempt to solve this problem by presenting an overview of existing corpora, and evaluating their utility for common summarization tasks. 2. 2.1. Corpora Overview Table 1 presents an overview of the main summarization corpora. The most widely used corpora are the Document Understanding Conference (DUC) and the Text Analysis Conference (TAC) corpora. The DUC corpora wer"
L18-1509,N04-1000,0,0.292398,"Missing"
L18-1509,D15-1044,0,0.245757,"Missing"
L18-1509,P17-1099,0,0.27906,"Missing"
L18-1509,D07-1047,0,0.0748869,"Missing"
L18-1509,D16-1033,0,0.254193,"20 100 guided TAC 2010 (Owczarzak and Dang, 2010) a en news y 46x20 100 guided TAC 2011 (Owczarzak and Dang, 2011) a en news y 44x20 100 guided ICSI (Janin et al., 2003) a,e en meetings n 57 390 y AMI (McCowan et al., 2005) a,e en meetings n 137 300 y Opinosis (Ganesan et al., 2010) a en reviews y 51x100 25 y Gigaword (David and Cieri, 2003) a en news n 4,111,240 headline y Gigaword 5 (Parker and others, 2011) a en news n 9,876,086 headline y LCSTS (Hu et al., 2015) a zh blogs n 2,400,591 a few sentences y CNN/Daily Mail (Hermann et al., 2015) a en news n 312,084 50 average y MSR Abstractive (Toutanova et al., 2016) a en misc n 6,000 a few sentences y arXiv (Cohan et al., 2018) a en science n 194,000 220 y PubMed (Cohan et al., 2018) a en science n 278,000 216 y Table 1: Overview of existing datasets for summarization. Abbreviations; a: abstractive; ar: arabic; e: extractive; en: English; multi-doc: multi-document summarization; n: no; y: yes; zh: Chinese. The size is expressed in terms of number of summarized texts. For multi-document summarization corpora, 60x10 means that the corpus contains 60 clusters of documents, each of them is comprised of 10 documents. The output length corresponds to the lengt"
L18-1509,P10-1058,0,0.0647089,"Missing"
L18-1509,D17-1101,0,0.0188021,"training. DUC corpora are typically used for testing only, as they tend to be too small to train neural networks on. TAC 2008 and 2009 had an update summarization track (Dang and Owczarzak, 2008). The Text Retrieval Conference (TREC) also organized an update summarization shared task yearly from 2013 to 2017, which they sometimes referred to as temporal summarization (Aslam et al., 2013; Aslam et al., 2015a; Aslam et al., 2015b) or real-time summarization (Lin et al., 2016). Summarization may also be performed for non-textual input, such as single images (Fan et al., 2008), albums of images (Yu et al., 2017), videos (Evangelopoulos et al., 2008), or voice recording (e.g., meetings or presentations) (Zhang et al., 2007). Different type of inputs may also be combined to perform the summary, which is a task referred to as multi-modal summarization (Li et al., 2017). 2.4. Meta-information When choosing a suitable corpus to train or evaluate a summarization algorithm, many parameters must be taken into account, including: • Source code summarization: it aims at either summarizing in a human language what a code snippet performs, or automatically folding blocks of code that are deemed less informative"
L18-1509,D17-1224,0,0.0405606,"Missing"
L18-1708,D16-1046,0,0.0166477,"2015) and finance (Stamate et al., 2015). The successes of ANNs for many applications over the last few years have escalated the interest in studying transfer learning for ANNs. In particular, much work has been done for computer vision (Yosinski et al., 2014; Oquab et al., 2014; Zeiler and Fergus, 2014). In these studies, some of the parameters learned on the source dataset are used to initialize the corresponding parameters of the ANNs for the target dataset. Fewer studies have been performed on transfer learning for ANN-based models in the field of natural language processing. For example, Mou et al. (2016) focused on transfer learning with convolutional neural networks for sentence classification. To the best of our knowledge, no study has analyzed transfer learning for ANN-based models in the context of NER. 4470 3. Model 4.2. The model we use for transfer learning experiments is based on a type of recurrent neural networks called long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997), and utilizes both token embeddings and character embeddings. It comprises six major components: 1. Token embedding layer maps each token to a token embedding. 2. Character embedding layer maps each cha"
N16-1062,P14-1062,0,0.131912,"Missing"
N16-1062,W14-4012,0,0.0685695,"Missing"
N16-1062,D10-1084,0,0.117175,"Missing"
N16-1062,D14-1181,0,0.0750786,"Missing"
N16-1062,2007.sigdial-1.30,0,0.175647,"Missing"
N16-1062,W04-2319,0,0.823086,"Missing"
N16-1062,D12-1110,0,0.157525,"Missing"
N16-1062,J00-3003,0,0.947743,"Missing"
N16-1062,N06-1036,0,\N,Missing
N16-1062,N10-1120,0,\N,Missing
N16-1062,D14-1162,0,\N,Missing
N18-2097,D17-1221,0,0.0155952,"ated according to: (t) βj = (s) (d) softmax(score(hj , ht−1 )) j   (e) (t) (d) (t) (t) α(j,i) = softmax βj score(h(j,i) , cov(j,i) , ht−1 ) (6) (i,j) (d) 4 At each timestep t, the decoder state ht and the context vector ct are used to estimate the probability distribution of next word yt :  (d) > Neural abstractive summarization models have been studied in the past (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016) and later extended by source copying (Miao and Blunsom, 2016; See et al., 2017), reinformcement learning (Paulus et al., 2017), and sentence salience information (Li et al., 2017). One model variant of Nallapati et al. (2016) is related to our model in using sentence-level information in attention. However, our model is different as it contains a hierarchical encoder, uses discourse sections in the decoding step, and has a coverage mechanism. Similarly, Ling and Rush (2017) proposed a coarseto-fine attention model that uses hard attention to find the text chunks of importance and then only attend to words in that chunk. In contrast, we consider all the discourse sections using soft attention. The closest model to ours is that of See et al. (2017) and Paulus et al. (201"
N18-2097,N16-1012,0,0.715294,"tion j in the document consisting of a sequence of tokens.  (s) hj = RNNsec x(j,1) , ...x(j,M ) } at a time. Given an input document along with the corresponding ground-truth summary y, the ˆ that is model is trained to output a summary y close to y. The output at timestep t is predicted using the decoder input x0t , decoder hidden state (d) ht , and some information about the input sequence. This framework is the general seq2seq framework employed in many generation tasks including machine translation (Sutskever et al., 2014; Bahdanau et al., 2014) and summarization (Nallapati et al., 2016; Chopra et al., 2016). Attentive decoding The attention mechanism maps the decoder state and the encoder states to an output vector, which is a weighted sum of the encoder states and is called context vector (Bahdanau et al., 2014). Incorporating this context vector at each decoding timestep (attentive decoding) is proven effective in seq2seq models. Formally, the context vector ct is defined as: ct = PN (t) (e) (t) i=1 αi hi where αi are the attention weights calculated as follows: i (3) We now describe our discourse-aware summarization model (shown in Figure 1). Encoder Our encoder extends the RNN encoder to a h"
N18-2097,W04-1013,0,0.304618,"ization. Our datasets are obtained from scientific papers. Scientific document summarization has been recently received extended attention (Qazvinian et al., 2013; Cohan and Goharian, 2015, 2017b,a). In contrast to ours, existing approaches are extractive and rely on external information such as citations, which may not be available for all papers. 5 6 Data Experiments Setup Similar to the majority of published research in the summarization literature (Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017), evaluation was done using the ROUGE automatic summarization evaluation metric (Lin, 2004) with full-length F-1 ROUGE scores. We lowercase all tokens and perform sentence and word tokenization using spaCy (Honnibal and Johnson, 2015). Implementation details We use Tensorflow 1.4 for implementing our models. We use the hyperparameters suggested by See et al. (2017). In particular, we use two bidirectional LSTMs with cell size of 256 and embedding dimensions of 128. Embeddings are trained from scratch and we did not find any gain using pre-trained embeddings. The vocabulary size is constrained to 50,000; using larger vocabulary size did not result in any improvement. We use mini-batc"
N18-2097,D15-1045,1,0.889316,"Missing"
N18-2097,W17-4505,0,0.0313005,"(d) > Neural abstractive summarization models have been studied in the past (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016) and later extended by source copying (Miao and Blunsom, 2016; See et al., 2017), reinformcement learning (Paulus et al., 2017), and sentence salience information (Li et al., 2017). One model variant of Nallapati et al. (2016) is related to our model in using sentence-level information in attention. However, our model is different as it contains a hierarchical encoder, uses discourse sections in the decoding step, and has a coverage mechanism. Similarly, Ling and Rush (2017) proposed a coarseto-fine attention model that uses hard attention to find the text chunks of importance and then only attend to words in that chunk. In contrast, we consider all the discourse sections using soft attention. The closest model to ours is that of See et al. (2017) and Paulus et al. (2017) who used a joint pointer-generator network for summarization. However, our model extends theirs by (i) a hierarchical encoder for modeling long documents and (ii) a discourse-aware decoder that captures the information flow from all discourse sections of the document. Finally, in a recent work,"
N18-2097,D15-1166,0,0.082704,"in and the nature of the document, they write about important points from different discourse sections of the document. For example, scientific paper abstracts typically include the description of the problem, discussion of the methods, and finally results and conclusions (Suppe, 1998). Motivated by this observation, we propose a discourse-aware attention method. Intuitively, at each decoding timestep, in addition to the words (1) where softmax means that the denominator’s sum i in the softmax function is over i. The score function can be defined in bilinear, additive, or multiplicative ways (Luong et al., 2015). We use the additive scoring function:  (e) (d) (e) (d) > Model RNN(.) denotes a function which is a recurrent neural network whose output is the final state of the network encoding the entire sequence. N is (s) the number of sections in the document and hj is representation of section j in the document consisting of a sequence of tokens.  (s) hj = RNNsec x(j,1) , ...x(j,M ) } at a time. Given an input document along with the corresponding ground-truth summary y, the ˆ that is model is trained to output a summary y close to y. The output at timestep t is predicted using the decoder input x0"
N18-2097,D16-1031,0,0.0162803,"e(h(j,i) , ht−1 ) (5) (i,j) The score function is the additive attention func(t) tion (Equation 2) and the weights βj are updated according to: (t) βj = (s) (d) softmax(score(hj , ht−1 )) j   (e) (t) (d) (t) (t) α(j,i) = softmax βj score(h(j,i) , cov(j,i) , ht−1 ) (6) (i,j) (d) 4 At each timestep t, the decoder state ht and the context vector ct are used to estimate the probability distribution of next word yt :  (d) > Neural abstractive summarization models have been studied in the past (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016) and later extended by source copying (Miao and Blunsom, 2016; See et al., 2017), reinformcement learning (Paulus et al., 2017), and sentence salience information (Li et al., 2017). One model variant of Nallapati et al. (2016) is related to our model in using sentence-level information in attention. However, our model is different as it contains a hierarchical encoder, uses discourse sections in the decoding step, and has a coverage mechanism. Similarly, Ling and Rush (2017) proposed a coarseto-fine attention model that uses hard attention to find the text chunks of importance and then only attend to words in that chunk. In contrast, we consider all the"
N18-2097,P16-1154,0,0.036547,"om all discourse sections of the document. Finally, in a recent work, Liu et al. (2018) proposed a model based on the transformer network (Vaswani et al., 2017) for abstractive generation of Wikipedia articles. However, their focus (7) p(yt |y1:t−1 ) = softmax V linear ht , ct where V is a vocabulary weight matrix and softmax is over the entire vocabulary. Copying from source There has been a surge of recent works in sequence learning tasks to address the problem of unkown token prediction by allowing the model to occasionally copy words directly from source instead of generating a new token (Gu et al., 2016; See et al., 2017; Paulus et al., 2017; Wiseman et al., 2017). Following these works, we add an additional binary variable zt to the decoder, indicating generating a word from vocabulary (zt =0) or copying a word from the source (zt =1). The probability is learnt during training according to the following equation: (d) p(zt =1|y1:t−1 ) = σ(linear(ht , ct , x0t )) (8) Then the next word yt is generated according to: X p(yt |y1:t−1 ) = p(yt , zt =z|y1:t−1 ); z = {0, 1} z The joint probability is decomposed as: ( pc (yt |y1:t−1 ) p(zt =z|y1:t−1 ), p(yt , zt =z) = pg (yt |y1:t−1 ) p(zt =z|y1:t−1"
N18-2097,K16-1028,0,0.534593,"is representation of section j in the document consisting of a sequence of tokens.  (s) hj = RNNsec x(j,1) , ...x(j,M ) } at a time. Given an input document along with the corresponding ground-truth summary y, the ˆ that is model is trained to output a summary y close to y. The output at timestep t is predicted using the decoder input x0t , decoder hidden state (d) ht , and some information about the input sequence. This framework is the general seq2seq framework employed in many generation tasks including machine translation (Sutskever et al., 2014; Bahdanau et al., 2014) and summarization (Nallapati et al., 2016; Chopra et al., 2016). Attentive decoding The attention mechanism maps the decoder state and the encoder states to an output vector, which is a weighted sum of the encoder states and is called context vector (Bahdanau et al., 2014). Incorporating this context vector at each decoding timestep (attentive decoding) is proven effective in seq2seq models. Formally, the context vector ct is defined as: ct = PN (t) (e) (t) i=1 αi hi where αi are the attention weights calculated as follows: i (3) We now describe our discourse-aware summarization model (shown in Figure 1). Encoder Our encoder extends"
N18-2097,D15-1162,0,0.0135992,"ed attention (Qazvinian et al., 2013; Cohan and Goharian, 2015, 2017b,a). In contrast to ours, existing approaches are extractive and rely on external information such as citations, which may not be available for all papers. 5 6 Data Experiments Setup Similar to the majority of published research in the summarization literature (Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017), evaluation was done using the ROUGE automatic summarization evaluation metric (Lin, 2004) with full-length F-1 ROUGE scores. We lowercase all tokens and perform sentence and word tokenization using spaCy (Honnibal and Johnson, 2015). Implementation details We use Tensorflow 1.4 for implementing our models. We use the hyperparameters suggested by See et al. (2017). In particular, we use two bidirectional LSTMs with cell size of 256 and embedding dimensions of 128. Embeddings are trained from scratch and we did not find any gain using pre-trained embeddings. The vocabulary size is constrained to 50,000; using larger vocabulary size did not result in any improvement. We use mini-batches of size 16 and we limit the document length to 2000 and section length to 500 tokens, and number of sections to 4. We use batch-padding and"
N18-2097,D15-1044,0,0.115968,"(t) The scalar weights α(j,i) are obtained according to:   (t) (t) (e) (d) α(j,i) = softmax βj score(h(j,i) , ht−1 ) (5) (i,j) The score function is the additive attention func(t) tion (Equation 2) and the weights βj are updated according to: (t) βj = (s) (d) softmax(score(hj , ht−1 )) j   (e) (t) (d) (t) (t) α(j,i) = softmax βj score(h(j,i) , cov(j,i) , ht−1 ) (6) (i,j) (d) 4 At each timestep t, the decoder state ht and the context vector ct are used to estimate the probability distribution of next word yt :  (d) > Neural abstractive summarization models have been studied in the past (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016) and later extended by source copying (Miao and Blunsom, 2016; See et al., 2017), reinformcement learning (Paulus et al., 2017), and sentence salience information (Li et al., 2017). One model variant of Nallapati et al. (2016) is related to our model in using sentence-level information in attention. However, our model is different as it contains a hierarchical encoder, uses discourse sections in the decoding step, and has a coverage mechanism. Similarly, Ling and Rush (2017) proposed a coarseto-fine attention model that uses hard attention to find"
N18-2097,P17-1099,0,0.681695,"x` is defined as: X (t) α(j,i) (9) pc (yt = x` |y1:t−1 ) = in the document, we also attend to the relevant discourse section (the “section attention” block in Figure 1). Then we use the discourse-related information to modify the word-level attention function. Specifically, the context vector representing the source document is: XN XM (t) (e) α(j,i) h(j,i) (4) ct = j=1 (j,i):x(j,i) =x` Decoder coverage In long sequences, the neural generation models tend to repeat phrases where the softmax layer predicts the same phrase multiple times over multiple timesteps. To address this issue, following See et al. (2017), we track attention coverage to avoid repeatedly attending to the same steps. This is done with a coverage vector cov(t) , the sum of attention weight vectors at prePt−1 (k) vious timesteps: cov(t) k=0 α(j,i) (j,i) = The coverage implicitly includes information about the attended document discourse sections. We incorporate the decoder coverage as an additional input to the attention function: i=1 (e) where h(j,i) shows the encoder state of word i (t) in discourse section j and α(j,i) shows the corresponding attention weight to that encoder state. (t) The scalar weights α(j,i) are obtained acc"
N18-2097,D17-1235,0,0.0315041,"017). These approaches employ a general framework of sequence-to-sequence (seq2seq) models (Sutskever et al., 2014) where the document is fed to an encoder network and another (recurrent) network learns to decode the summary. While promising, these methods focus on summarizing news articles which are relatively short. Many other document types, however, are longer and structured. Seq2seq models tend to struggle with longer sequences because at each decoding step, the decoder needs to learn to construct a context vector capturing relevant information from all the tokens in the source sequence (Shao et al., 2017). Our main contribution is an abstractive model for summarizing scientific papers which are an example of long-form structured document types. Our model includes a hierarchical encoder, capturing the discourse structure of the document and a discourse-aware decoder that generates the summary. Our decoder attends to different discourse sections and allows the model to more accurately represent important information from the source resulting in a better context vector. We also introduce two large-scale datasets of long and structured scientific papers obtained from arXiv and PubMed to support bo"
N18-2097,D17-1239,0,0.0207893,"recent work, Liu et al. (2018) proposed a model based on the transformer network (Vaswani et al., 2017) for abstractive generation of Wikipedia articles. However, their focus (7) p(yt |y1:t−1 ) = softmax V linear ht , ct where V is a vocabulary weight matrix and softmax is over the entire vocabulary. Copying from source There has been a surge of recent works in sequence learning tasks to address the problem of unkown token prediction by allowing the model to occasionally copy words directly from source instead of generating a new token (Gu et al., 2016; See et al., 2017; Paulus et al., 2017; Wiseman et al., 2017). Following these works, we add an additional binary variable zt to the decoder, indicating generating a word from vocabulary (zt =0) or copying a word from the source (zt =1). The probability is learnt during training according to the following equation: (d) p(zt =1|y1:t−1 ) = σ(linear(ht , ct , x0t )) (8) Then the next word yt is generated according to: X p(yt |y1:t−1 ) = p(yt , zt =z|y1:t−1 ); z = {0, 1} z The joint probability is decomposed as: ( pc (yt |y1:t−1 ) p(zt =z|y1:t−1 ), p(yt , zt =z) = pg (yt |y1:t−1 ) p(zt =z|y1:t−1 ), Related work z=1 z=0 pg is the probability of generating a"
N18-2097,J02-4006,0,\N,Missing
N19-1168,W17-4210,0,0.0155703,"s by extractively compressing the most salient sentence of each paragraph, as shown in Figure 1. While there has been much recent work on abstractive headline generation from a single sentence (Nallapati et al., 2016), abstractive models require larger datasets, which are not available in many domains and languages. Moreover, abstractive text-generation models tend to generate incorrect information for complex inputs (See et al., 2017; Wiseman et al., 2017). Misleading headlines can have unintended effects, affecting readers’ memory and reasoning skills and even bias them (Ecker et al., 2014; Chesney et al., 2017). Especially in times of sensationalism and click-baiting, the unguided generation of titles can be considered unethical and we thus focus on the investigation of deletion-only approaches to title-generation. While this restricts this approach to languages that, similar to English, do not 1677 Proceedings of NAACL-HLT 2019, pages 1677–1688 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics lose grammatical soundness when clauses are removed, this approach is highly data-efficient and preserves the original meaning in most cases. We approach the prob"
N19-1168,L18-1509,1,0.814055,"mbers and our own implementation. 3 s using a weighting parameter λ. Since exact inference for this target is intractable, we approximate this inference by constraining the re-ranking to the K best segmentations according to a K-best Viterbi algorithm. The R ANKER uses the same word embeddings as the C OMPRESSOR, and a bidirectional LSTM in order to maximize the probability of a sequence Sequence-to-Sequence Baselines Data and Experiments The S ELECTOR is trained on the CNN-DM corpus (Hermann et al., 2015; Nallapati et al., 2016), which is the most commonly used corpus for news summarization (Dernoncourt et al., 2018). Each summary comprises a number of bullet points for an article, with an average length of 66 tokens and 4.9 bullet points. The C OMPRESSOR is trained on the Google sentence compression dataset (Filip1680 pova and Altun, 2013), which comprises 200,000 sentence-headline pairs from news articles. The deletion-only version of the headlines was created by pruning the syntactic tree of the sentence and aligning the words with the headline. The largest comparable corpus Gigaword (Rush et al., 2015) does not include deletion-only headlines. We limit the vocabulary size to 50,000 words for both corp"
N19-1168,W03-0501,0,0.14833,"recently introduced a hybrid SCRF that uses both word- and phrase-level information. Alternative approaches for similar tasks are CRFs that estimate pairwise potentials rather than using a fixed transition matrix (Jagannatha and Yu, 2016) or high-order CRFs which outperform SCRFs in some sequence labeling tasks (Cuong et al., 2014). While this work is the first to apply SCRFs to sentence compression, Grootjen et al. (2018) also use extractive summarization techniques to improve reading comprehension by highlighting relevant sentences. Most similar to our compression approach is Hedge Trimmer (Dorr et al., 2003), which compresses sentences through deletion, but uses an iterative shortening algorithm based on linguistic features. Extending this work, Filippova and Altun (2013) apply a similar approach on linguistic features, but learn weights for the shortening algorithm. Both approaches also do not consider the selection of the sentence to be compressed, unlike our proposed model. 6 Conclusion In this work, we have presented a novel approach to section title generation that uses an efficient sentence compression model. We demonstrated that our approach performs almost as well as sequence-to-sequence"
N19-1168,D15-1042,0,0.0727468,"Missing"
N19-1168,D13-1155,0,0.444871,"ng in most cases. We approach the problem with a two-part pipeline where we aim to generate a title for each paragraph of a text, as illustrated in Figure 2. First, a S ELECTOR selects the most salient sentence within a paragraph and then the C OMPRES SOR compresses the sentence. The selector is an extractive summarization algorithm that assigns a score to each sentence corresponding to its likelihood to be used in a summary (Gehrmann et al., 2018). Algorithms for word deletion typically rely on linguistic features within a tree-pruning algorithm that identifies which phrases can be excluded (Filippova and Altun, 2013). Following recent work that shows the efficiency of contextual span-representations (Lee et al., 2016; Peters et al., 2018), we develop an alternative approach based on a Semi-Markov Conditional Random Field (SCRF) (Sarawagi and Cohen, 2005). The SCRF is further extended by a language model that ranks multiple compression candidates to generate grammatically correct compressions. We evaluate this approach by comparing it to strong sequence-to-sequence baselines on an English sentence-compression dataset and show that our approach performs almost as well on large datasets while outperforming t"
N19-1168,D18-1443,1,0.885135,"019 Association for Computational Linguistics lose grammatical soundness when clauses are removed, this approach is highly data-efficient and preserves the original meaning in most cases. We approach the problem with a two-part pipeline where we aim to generate a title for each paragraph of a text, as illustrated in Figure 2. First, a S ELECTOR selects the most salient sentence within a paragraph and then the C OMPRES SOR compresses the sentence. The selector is an extractive summarization algorithm that assigns a score to each sentence corresponding to its likelihood to be used in a summary (Gehrmann et al., 2018). Algorithms for word deletion typically rely on linguistic features within a tree-pruning algorithm that identifies which phrases can be excluded (Filippova and Altun, 2013). Following recent work that shows the efficiency of contextual span-representations (Lee et al., 2016; Peters et al., 2018), we develop an alternative approach based on a Semi-Markov Conditional Random Field (SCRF) (Sarawagi and Cohen, 2005). The SCRF is further extended by a language model that ranks multiple compression candidates to generate grammatically correct compressions. We evaluate this approach by comparing it"
N19-1168,D16-1082,0,0.0231925,", 2015). Similar to this work, it has also been shown that coupling an LM with an SCRF can improve segmentation through multi-task training (Lu et al., 2017; Liu et al., 2017). SCRFs have also been applied to sequence tagging tasks, for example, the extraction of phrases that indicate opinions (Yang and Cardie, 2012). In this work, we built upon an approach by Ye and Ling (2018) who recently introduced a hybrid SCRF that uses both word- and phrase-level information. Alternative approaches for similar tasks are CRFs that estimate pairwise potentials rather than using a fixed transition matrix (Jagannatha and Yu, 2016) or high-order CRFs which outperform SCRFs in some sequence labeling tasks (Cuong et al., 2014). While this work is the first to apply SCRFs to sentence compression, Grootjen et al. (2018) also use extractive summarization techniques to improve reading comprehension by highlighting relevant sentences. Most similar to our compression approach is Hedge Trimmer (Dorr et al., 2003), which compresses sentences through deletion, but uses an iterative shortening algorithm based on linguistic features. Extending this work, Filippova and Altun (2013) apply a similar approach on linguistic features, but"
N19-1168,D15-1166,0,0.0101844,"quences (Sutskever et al., 2014; Bahdanau et al., 2014). S2S models are autoregressive and generate one word at a time by maximizing the probability p(y|x) = P i p(yi |x, y1:i−1 ). Since this condition is stronger than that of CRF-based approaches, we hypothesize that S2S models perform better with unlimited training data. However, since S2S models need to jointly learn the alignment and generate words, they typically perform worse with limited data. To test this hypothesis, we define two S2S baselines we compare to our models. First, we use a standard S2S model with attention as described by Luong et al. (2015). In contrast to the other approaches, this model is abstractive and has the ability to paraphrase and re-order words. We constrain these abilities in a second S2S approach as described by Filippova et al. (2015). This model is a sequential pointer-network (Vinyals et al., 2015) and can only generate words from the source sentence. Instead of using an attention mechanism to compute which word to copy, the model enforces a monotonically increasing index of copied words to prevent the re-ordering. We compare both against their reported numbers and our own implementation. 3 s using a weighting pa"
N19-1168,K16-1028,0,0.130413,"ts or only provide a very abstract description of their topics, e.g. “Geography” or “Introduction”. This makes them more inaccessible especially to readers with less developed reading skills, who have trouble identifying relevant information in text (Englert et al., 2009) and therefore more strongly rely on text-markups (Bell and Limber, 2009). This paper introduces an approach to generate section titles by extractively compressing the most salient sentence of each paragraph, as shown in Figure 1. While there has been much recent work on abstractive headline generation from a single sentence (Nallapati et al., 2016), abstractive models require larger datasets, which are not available in many domains and languages. Moreover, abstractive text-generation models tend to generate incorrect information for complex inputs (See et al., 2017; Wiseman et al., 2017). Misleading headlines can have unintended effects, affecting readers’ memory and reasoning skills and even bias them (Ecker et al., 2014; Chesney et al., 2017). Especially in times of sensationalism and click-baiting, the unguided generation of titles can be considered unethical and we thus focus on the investigation of deletion-only approaches to title"
N19-1168,D14-1162,0,0.0826885,"ing aP S ELECTOR model that maximizes log p(t|x) = ni=1 log p(ti |x). Using this model, we calculate the relevance of a sentence sent := xstart , . . . , xend , with 1 ≤ start &lt; end ≤ n, with a saliency function defined as |sent| 1 X saliency(sent) = p(ti |sent). |sent| i=1 The sentence selection problem thus reduces to sentence with the most relevant words within a paragraph para, argmax saliency(sent). sent ∈ para We first represent each word using two different embedding channels. The first is a contextual word representation using ELMo (Peters et al., 2018), and the second one uses GloVe (Pennington et al., 2014). Preliminary experiments corroborated the findings by Peters et al. that the combination of the embeddings help the model converge faster, and perform better with limited training data. Both embeddings for a word xi are concatenated into one vector ei , and used as input to a bidirectional LSTM (Hochreiter and Schmidhuber, 1997). Finally, the output of the LSTM hi is used to compute the probability that a word is selected σ(WsT hi +bs ) with the trainable parameters Ws and bs . 1678 2.2 C OMPRESSOR We next define the problem of deletion-only compression of a single sentence. For simplicity of"
N19-1168,N18-1202,0,0.188571,"as illustrated in Figure 2. First, a S ELECTOR selects the most salient sentence within a paragraph and then the C OMPRES SOR compresses the sentence. The selector is an extractive summarization algorithm that assigns a score to each sentence corresponding to its likelihood to be used in a summary (Gehrmann et al., 2018). Algorithms for word deletion typically rely on linguistic features within a tree-pruning algorithm that identifies which phrases can be excluded (Filippova and Altun, 2013). Following recent work that shows the efficiency of contextual span-representations (Lee et al., 2016; Peters et al., 2018), we develop an alternative approach based on a Semi-Markov Conditional Random Field (SCRF) (Sarawagi and Cohen, 2005). The SCRF is further extended by a language model that ranks multiple compression candidates to generate grammatically correct compressions. We evaluate this approach by comparing it to strong sequence-to-sequence baselines on an English sentence-compression dataset and show that our approach performs almost as well on large datasets while outperforming the complex models with limited training data. We further show the results of a human study to compare the effects of showing"
N19-1168,D16-1264,0,0.0386574,"stions and then the text, prompting participants to find the answers. Previous findings indicate that titles help with retention only when presented towards the beginning of a text (Dooling and Mullet, 1973). Thus, we place texts in the left margin at the top of a paragraph as shown in the example in Figure 3. This further avoids interrupting the reading flow of the long text while being integrated into the natural left-to-right reading process. Although reading comprehension is well studied in natural language processing, most datasets focus on machine comprehension (Richardson et al., 2013; Rajpurkar et al., 2016). Therefore, we adapted texts from the interactive reading practice by National Geographic, written by Helen Stephenson1 . The 33 texts are based on articles and comprise three versions for each story; elementary, intermediate, and advanced, from which we selected intermediate and advanced versions. Topics of the 1 http://www.ngllife.com/student-zone/ interactive-reading-practice 1681 Model Features P R F1 Length Filippova et al. (2015) Yes S2S w/o copy Sequential Pointer Naive Tagger SCRF SCRF+Ranking No 83.2 87.1 81.4 85.2 86.1 ±4.5 ±4.1 ±3.7 ±3.9 ±3.9 73.0 76.0 74.6 71.6 72.9 ±5.9 ±5.1 ±6.0"
N19-1168,W09-1119,0,0.026943,"vidual tags such that exp Score(s, x) , 0 s0 exp Score(s , x) p(s|x) = P marginalizing over all possible segmentations. The CRF represents a special case of the SCRF with L = 1. To account for the segment-level information, we extend the emission potential function to φiE (x, h˜ y , start, endi) = end X WeT h0i , (2) i=start where h0i is the concatenation of hi , hstart − hend , and a span length embedding elen to account for both individual words and global segment information. We also extend φT to include transitions to and from longer tagged sequences by representing targets as BIEUO tags (Ratinov and Roth, 2009). This formulation allows for similar training by minimizing the negative loglikelihood. 2.3 R ANKER The inference of the CRF and the SCRF require an estimation of the best possible segmentation s∗ = p(s|x), which can be computed using the Viterbi algorithm. However, CRFs and SCRFs are typically employed in sequence-level tagging with no inter-segment dependencies. The sentence compression task differs since a resulting compressed sentence should be grammatical. Therefore, we employ a language model (LM) to rank compression candidates based on the likelihood of the compressed sentence compress"
N19-1168,D13-1020,0,0.0285273,"e first presented two questions and then the text, prompting participants to find the answers. Previous findings indicate that titles help with retention only when presented towards the beginning of a text (Dooling and Mullet, 1973). Thus, we place texts in the left margin at the top of a paragraph as shown in the example in Figure 3. This further avoids interrupting the reading flow of the long text while being integrated into the natural left-to-right reading process. Although reading comprehension is well studied in natural language processing, most datasets focus on machine comprehension (Richardson et al., 2013; Rajpurkar et al., 2016). Therefore, we adapted texts from the interactive reading practice by National Geographic, written by Helen Stephenson1 . The 33 texts are based on articles and comprise three versions for each story; elementary, intermediate, and advanced, from which we selected intermediate and advanced versions. Topics of the 1 http://www.ngllife.com/student-zone/ interactive-reading-practice 1681 Model Features P R F1 Length Filippova et al. (2015) Yes S2S w/o copy Sequential Pointer Naive Tagger SCRF SCRF+Ranking No 83.2 87.1 81.4 85.2 86.1 ±4.5 ±4.1 ±3.7 ±3.9 ±3.9 73.0 76.0 74.6"
N19-1168,D15-1044,0,0.04241,"2015; Nallapati et al., 2016), which is the most commonly used corpus for news summarization (Dernoncourt et al., 2018). Each summary comprises a number of bullet points for an article, with an average length of 66 tokens and 4.9 bullet points. The C OMPRESSOR is trained on the Google sentence compression dataset (Filip1680 pova and Altun, 2013), which comprises 200,000 sentence-headline pairs from news articles. The deletion-only version of the headlines was created by pruning the syntactic tree of the sentence and aligning the words with the headline. The largest comparable corpus Gigaword (Rush et al., 2015) does not include deletion-only headlines. We limit the vocabulary size to 50,000 words for both corpora. Both S ELECTOR and C OM PRESSOR use a two-layer bidirectional LSTM with 64 hidden dimensions for each direction, and a word-embedding size of 200. Each linguistic feature is embedded into 30-dimensional space. During training, the dropout probability is set to 0.5 (Srivastava et al., 2014). The model is trained for up to 50 epochs or until the validation loss does not decrease for three consecutive epochs. We additionally halve the learning rate every time the validation loss does not decr"
N19-1168,P17-1099,0,0.0201896,"t information in text (Englert et al., 2009) and therefore more strongly rely on text-markups (Bell and Limber, 2009). This paper introduces an approach to generate section titles by extractively compressing the most salient sentence of each paragraph, as shown in Figure 1. While there has been much recent work on abstractive headline generation from a single sentence (Nallapati et al., 2016), abstractive models require larger datasets, which are not available in many domains and languages. Moreover, abstractive text-generation models tend to generate incorrect information for complex inputs (See et al., 2017; Wiseman et al., 2017). Misleading headlines can have unintended effects, affecting readers’ memory and reasoning skills and even bias them (Ecker et al., 2014; Chesney et al., 2017). Especially in times of sensationalism and click-baiting, the unguided generation of titles can be considered unethical and we thus focus on the investigation of deletion-only approaches to title-generation. While this restricts this approach to languages that, similar to English, do not 1677 Proceedings of NAACL-HLT 2019, pages 1677–1688 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Compu"
N19-1168,1983.tc-1.13,0,0.408663,"Missing"
N19-1168,D12-1122,0,0.0183413,"e label to an entire segment. While they were originally developed for information extraction (Sarawagi and Cohen, 2005), it is most commonly applied to speech recognition within the acoustic model to improve segmentation between different words (He and Fosler-Lussier, 2015; Lu et al., 2016; Kong et al., 2015). Similar to this work, it has also been shown that coupling an LM with an SCRF can improve segmentation through multi-task training (Lu et al., 2017; Liu et al., 2017). SCRFs have also been applied to sequence tagging tasks, for example, the extraction of phrases that indicate opinions (Yang and Cardie, 2012). In this work, we built upon an approach by Ye and Ling (2018) who recently introduced a hybrid SCRF that uses both word- and phrase-level information. Alternative approaches for similar tasks are CRFs that estimate pairwise potentials rather than using a fixed transition matrix (Jagannatha and Yu, 2016) or high-order CRFs which outperform SCRFs in some sequence labeling tasks (Cuong et al., 2014). While this work is the first to apply SCRFs to sentence compression, Grootjen et al. (2018) also use extractive summarization techniques to improve reading comprehension by highlighting relevant se"
N19-1168,P18-2038,0,0.105024,"i−1 , yi )) . i During training, we can minimize the negative log-likelihood in which the partition function is computed with the forward-backward algorithm. During inference, this formulation allows for exact inference with the Viterbi algorithm. Semi-Markov Conditional Random Field Although compressed sentences should often include entire phrases, the CRF does not take into account local dependencies beyond neighboring words. Therefore, we relax the Markov assumption and score longer spans of words. This can be achieved with a SCRF (Sarawagi and Cohen, 2005). Following a similar approach as Ye and Ling (2018), let s = {s1 , s2 , . . . , sp } denote the segmentation of x. Each segment si is represented as a tuple hstart, end, y˜i i, where start and the end denote the indices of the boundaries of the phrase, and y˜ the corresponding label for the entire phrase. To ensure the validity of the representation, we impose the restrictions that start1 = 1, endp = |x|, starti+1 = endi + 1, and starti ≤ endi . Additionally, we set a fixed maximum length L. We 1679 extend Eq. 1 to account for a segmentation s instead of individual tags such that exp Score(s, x) , 0 s0 exp Score(s , x) p(s|x) = P marginalizing"
N19-1168,P18-1061,0,0.0181498,"mans and our approach summarize the content of a section play a major role in how reading comprehension is affected. 2 2.1 Methods S ELECTOR To select the most important sentence, we adapt an approach to the problem of content-selection in summarization, which has been shown to be efSELECTOR COMPRESSOR RANKER Figure 2: Overview of the three steps. The S ELECTOR detects the most salient sentence. Then, the C OMPRES SOR generates compressions and the R ANKER scores the them. fective and data-efficient (Gehrmann et al., 2018). An advantage of this approach over other extractive summarizers (e.g. Zhou et al. (2018)) is that those often model dependencies between selected sentences, which is not applicable to this problem since we only aim to extract a single sentence. Let x = x1 , . . . , xn denote a sequence of words within a paragraph, and y = y1 , . . . , ym a multi-sentence summary with n  m. Further, let t = t1 , . . . , tn be a binary alignment variable where ti =1 iff xi ∈ y. Using this alignment, the word-saliency problem is defined as learning aP S ELECTOR model that maximizes log p(t|x) = ni=1 log p(ti |x). Using this model, we calculate the relevance of a sentence sent := xstart , . . . , xe"
P19-1112,S17-2091,0,0.0190702,"one label sequence is correct and should be considered as ground truth. This is contrary to the ambiguous nature of our task, where different interpretations are possible. Our solution is to utilize label distribution learning (Subsection 3.2). LDL methods have been used before to solve various visual recognition problems such as facial age prediction (Rondeau and Alvarez, 2018; Gao et al., 2017). We are the first to introduce LDL for sequence labeling. 2 3 Related Work A large amount of work in NLP addresses finding keywords or key-phrases in long texts from scientific articles, news, etc. (Augenstein et al., 2017; Zhang et al., 2016). Keyword detection mainly focuses on finding important nouns or noun phrases. In contrast, social media text is much shorter, and users tend to emphasize a subset of words with different roles to convey specific intent. Emphasis words are not necessarily the words with the highest or lowest frequency in the text. Often a high sentiment adjective can be emphasized, such as Hot in Hot Summer. Generally, word emphasis may express emotions, show contrast, capture a reader’s interest or clarify a message. In a different context, modeling word emphasis has been addressed in exp"
P19-1112,N18-1202,0,0.0286867,"ps to build a deeper feature extractor; having more than two does not help the performance as the model becomes too complicated. We investigate the impact of attention mechanisms to the model (Vinyals et al., 2015; Zhang et al., 2017), where attention weights ai represent the relative contribution of a specific word to the text representation. We compute ai at each output time i as follows: P (x) log Q(x) P (x) Experimental Settings and Results Training Details We use two different word representations: pretrained 100-dim GloVe embedding (Pennington et al., 2014), and 2048-dim ELMo embedding (Peters et al., 2018). We use BiLSTM layers with hidden size of 512 and 2048 when using GloVe and ELMo embeddings respectively. We use the Adam optimizer (Kingma and Ba, 2014) with the learning rate set to 0.001. In order to better train and to force the network finding different activation paths, we use two dropout layers with a rate of 0.5 in the sequence and inference layers. Finetuning is performed for 160 epochs, and the reported test result corresponds to the best accuracy obtained on the validation set. 3 The implementation is available online: https:// github.com/RiTUAL-UH/emphasis-2019 1169 Model/Evals m="
P19-1112,D11-1143,0,0.110937,"Missing"
P19-1112,D16-1080,0,0.0603252,"rrect and should be considered as ground truth. This is contrary to the ambiguous nature of our task, where different interpretations are possible. Our solution is to utilize label distribution learning (Subsection 3.2). LDL methods have been used before to solve various visual recognition problems such as facial age prediction (Rondeau and Alvarez, 2018; Gao et al., 2017). We are the first to introduce LDL for sequence labeling. 2 3 Related Work A large amount of work in NLP addresses finding keywords or key-phrases in long texts from scientific articles, news, etc. (Augenstein et al., 2017; Zhang et al., 2016). Keyword detection mainly focuses on finding important nouns or noun phrases. In contrast, social media text is much shorter, and users tend to emphasize a subset of words with different roles to convey specific intent. Emphasis words are not necessarily the words with the highest or lowest frequency in the text. Often a high sentiment adjective can be emphasized, such as Hot in Hot Summer. Generally, word emphasis may express emotions, show contrast, capture a reader’s interest or clarify a message. In a different context, modeling word emphasis has been addressed in expressive prosody gener"
P19-1112,D17-1004,0,0.0136072,"ed results. 3 KL-Divergence Loss During the training phase, the Kullback-Leibler Divergence (KLDIV) (Kullback and Leibler, 1951) is used as the loss function. KL-DIV is a measure of how one probability distribution P is different from a second reference probability distribution Q: KL-DIV(P ||Q) = X x∈X 5 5.1 w1 Input both forward and backward directions. Having two BiLSTM layers helps to build a deeper feature extractor; having more than two does not help the performance as the model becomes too complicated. We investigate the impact of attention mechanisms to the model (Vinyals et al., 2015; Zhang et al., 2017), where attention weights ai represent the relative contribution of a specific word to the text representation. We compute ai at each output time i as follows: P (x) log Q(x) P (x) Experimental Settings and Results Training Details We use two different word representations: pretrained 100-dim GloVe embedding (Pennington et al., 2014), and 2048-dim ELMo embedding (Peters et al., 2018). We use BiLSTM layers with hidden size of 512 and 2048 when using GloVe and ELMo embeddings respectively. We use the Adam optimizer (Kingma and Ba, 2014) with the learning rate set to 0.001. In order to better tra"
P19-1112,Y14-1022,0,0.250052,"or lowest frequency in the text. Often a high sentiment adjective can be emphasized, such as Hot in Hot Summer. Generally, word emphasis may express emotions, show contrast, capture a reader’s interest or clarify a message. In a different context, modeling word emphasis has been addressed in expressive prosody generation. Most studies detect emphasis words based on acoustic and prosodic features that exist in spoken data (Mishra et al., 2012; Chen and Pan, 2017). More recently, few works model emphasis from text to improve expressive prosody generation in modern Text-To-Speech (TTS) systems (Nakajima et al., 2014; Mass et al., 2018). For example, (Mass et al., 2018) trained a deep neural network model on audience-addressed speeches to predict word emphasis. The dataset consists of relatively long paragraphs which are labeled by four annotators based on words that clearly stand out in a recorded speech. Many approaches have been proposed to deal with annotations coming from multiple annota3.1 Emphasis Selection Task Definition Given a sequence of words or tokens C = {x1 , ..., xn }, we want to determine the subset S of words in C that are good candidates to emphasize, where 1 ≤ |S |≤ n. 3.2 Label Distr"
P19-1112,D14-1162,0,0.082989,"d and backward directions. Having two BiLSTM layers helps to build a deeper feature extractor; having more than two does not help the performance as the model becomes too complicated. We investigate the impact of attention mechanisms to the model (Vinyals et al., 2015; Zhang et al., 2017), where attention weights ai represent the relative contribution of a specific word to the text representation. We compute ai at each output time i as follows: P (x) log Q(x) P (x) Experimental Settings and Results Training Details We use two different word representations: pretrained 100-dim GloVe embedding (Pennington et al., 2014), and 2048-dim ELMo embedding (Peters et al., 2018). We use BiLSTM layers with hidden size of 512 and 2048 when using GloVe and ELMo embeddings respectively. We use the Adam optimizer (Kingma and Ba, 2014) with the learning rate set to 0.001. In order to better train and to force the network finding different activation paths, we use two dropout layers with a rate of 0.5 in the sequence and inference layers. Finetuning is performed for 160 epochs, and the reported test result corresponds to the best accuracy obtained on the validation set. 3 The implementation is available online: https:// git"
P19-1182,D16-1125,0,0.0208753,"., 2016; Johnson et al., 2017). In terms of model progress, recent years witnessed strong research progress in generating natural language sentences to describe visual contents, such as Vinyals et al. (2015); Xu et al. (2015); Ranzato et al. (2016); Anderson et al. (2018) in single image captioning, Venugopalan et al. (2015); Pan et al. (2016); Pasunuru and Bansal (2017) in video captioning, Mao et al. (2016); Liu et al. (2017a); Yu et al. (2017); Luo and Shakhnarovich (2017) in referring expressions, Jain et al. (2017); Li et al. (2018); Misra et al. (2018) in visual question generation, and Andreas and Klein (2016); CohnGordon et al. (2018); Luo et al. (2018); Vedantam et al. (2017) in other setups. Single image captioning is the most relevant problem to the two-images captioning. Vinyals et al. (2015) created a powerful encoder-decoder (i.e., CNN to LSTM) framework in solving the captioning problem. Xu et al. (2015) further equipped it with an attention module to handle the memorylessness of fixed-size vectors. Ranzato et al. (2016) used reinforcement learning to eliminate exposure bias. Recently, Anderson et al. (2018) brought the information from object detection system to further boost the performan"
P19-1182,W05-0909,0,0.384545,", we compare the performance of our models on all three datasets with various automated metrics. Results on the test sets are reported. Following the setup in Jhamtani and Berg-Kirkpatrick (2018), we takes CIDEr (Vedantam et al., 2015) as the main metric in evaluating the Spot-the-Diff and NLVR2 datasets. However, CIDEr is known as its problem in up-weighting unimportant details (Kilickaya et al., 2017; Liu et al., 2017b). In our dataset, we find that instructions generated from a small set of short phrases could get a high CIDEr score. We thus change the main metric of our dataset to METEOR (Banerjee and Lavie, 2005), which is manually verified to be aligned with human judgment on the validation set in our dataset. To avoid over-fitting, the model is 1879 Ours(IEdit) Spot-the-Diff NLVR2 Basic 11 22 24 Full 24 37 37 Both Good 5 6 17 Both Not 60 35 22 Table 3: Human evaluation on 100 examples. Image pair and two captions generated by our basic model and full model are shown to the user. The user chooses one from ‘Basic’ model wins, ‘Full’ model wins, ‘Both Good’, or ‘Both Not’. Better model marked in bold font. we do not explicitly model the pixel-level differences; however, we still find that the model cou"
P19-1182,N18-1150,0,0.0223056,"e also report the BLEU-4 (Papineni et al., 2002) and ROUGE-L (Lin, 2004) scores. The results on various datasets shows the gradual improvement made by our novel neural components, which are designed to better describe the relationship between 2 images. Our full model has a significant improvement in result over baseline. The improvement on the NLVR2 dataset is limited because the comparison of two images was not forced to be considered when generating instructions. 4.3 Human Evaluation and Qualitative Analysis We conduct a pairwise human evaluation on our generated sentences, which is used in Celikyilmaz et al. (2018) and Pasunuru and Bansal (2017). Agarwala (2018) also shows that the pairwise comparison is better than scoring sentences individually. We randomly select 100 examples from the test set in each dataset and generate captions via our full speaker model. We ask users to choose a better instruction between the captions generated by our full model and the basic model, or alternatively indicate that the two captions are equal in quality. The Image Editing Request dataset is specifically annotated by the image editing expert. The winning rate of our full model (dynamic relation attention) versus the"
P19-1182,N18-2070,0,0.0416472,"Missing"
P19-1182,D18-1436,0,0.345133,"Missing"
P19-1182,D14-1086,0,0.420249,"n social 1 Our data and code are publicly available at: https://github.com/airsplay/ VisualRelationships media, etc. This task has drawn significant attention in the research community with numerous studies (Vinyals et al., 2015; Xu et al., 2015; Anderson et al., 2018), and recent state of the art methods have achieved promising results on large captioning datasets, such as MS COCO (Lin et al., 2014). Besides single image captioning, the community has also explored other visual captioning problems such as video captioning (Venugopalan et al., 2015; Xu et al., 2016), and referring expressions (Kazemzadeh et al., 2014; Yu et al., 2017). However, the problem of two-image captioning, especially the task of describing the relationships and differences between two images, is still underexplored. In this paper, we focus on advancing research in this challenging problem by introducing a new dataset and proposing novel neural relational-speaker models.2 To the best of our knowledge, Jhamtani and Berg-Kirkpatrick (2018) is the only public dataset aimed at generating natural language descriptions for two real images. This dataset is about ‘spotting the difference’, and hence focuses more on describing exhaustive di"
P19-1182,N18-2120,0,0.029005,"e DDLA (Difference Description with Latent Alignment) method proposed in Jhamtani and Berg-Kirkpatrick (2018) learns the alignment between descriptions and visual differences. It relies on the nature of the particular dataset and thus could not be easily transferred to other dataset where the visual relationship is not obvious. The two-images captioning could also be considered as a two key-frames video captioning problem, and our sequential multi-heads attention is a modified version of the seq-to-seq model (Venugopalan et al., 2015). Some existing work (Chen et al., 2018; Wang et al., 2018; Manjunatha et al., 2018) also learns how to modify images. These datasets and methods focus on the image colorization and adjustment tasks, while our dataset aims to study the general image editing request task. 6 Conclusion In this paper, we explored the task of describing the visual relationship between two images. We collected the Image Editing Request dataset, which contains image pairs and human annotated editing instructions. We designed novel relational speaker models and evaluate them on our collected and other public existing dataset. Based on automatic and human evaluations, our relational speaker model imp"
P19-1182,P02-1040,0,0.103959,"The user chooses one from ‘Basic’ model wins, ‘Full’ model wins, ‘Both Good’, or ‘Both Not’. Better model marked in bold font. we do not explicitly model the pixel-level differences; however, we still find that the model could learn these differences in the Spot-the-Diff dataset. Since the descriptions in Spot-the-Diff is relatively simple, the errors mostly come from wrong entities or undetected differences as shown in Fig. 5. Our model is also sensitive to the image contents as shown in the NLVR2 dataset. 5 early-stopped based on the main metric on validation set. We also report the BLEU-4 (Papineni et al., 2002) and ROUGE-L (Lin, 2004) scores. The results on various datasets shows the gradual improvement made by our novel neural components, which are designed to better describe the relationship between 2 images. Our full model has a significant improvement in result over baseline. The improvement on the NLVR2 dataset is limited because the comparison of two images was not forced to be considered when generating instructions. 4.3 Human Evaluation and Qualitative Analysis We conduct a pairwise human evaluation on our generated sentences, which is used in Celikyilmaz et al. (2018) and Pasunuru and Bansa"
P19-1182,D17-1103,1,0.908355,"neni et al., 2002) and ROUGE-L (Lin, 2004) scores. The results on various datasets shows the gradual improvement made by our novel neural components, which are designed to better describe the relationship between 2 images. Our full model has a significant improvement in result over baseline. The improvement on the NLVR2 dataset is limited because the comparison of two images was not forced to be considered when generating instructions. 4.3 Human Evaluation and Qualitative Analysis We conduct a pairwise human evaluation on our generated sentences, which is used in Celikyilmaz et al. (2018) and Pasunuru and Bansal (2017). Agarwala (2018) also shows that the pairwise comparison is better than scoring sentences individually. We randomly select 100 examples from the test set in each dataset and generate captions via our full speaker model. We ask users to choose a better instruction between the captions generated by our full model and the basic model, or alternatively indicate that the two captions are equal in quality. The Image Editing Request dataset is specifically annotated by the image editing expert. The winning rate of our full model (dynamic relation attention) versus the basic model is shown in Table 3"
P19-1182,P19-1644,0,0.199322,"further extend it by designing a dynamic relational attention module to combine the advantages of these two components, which finds the relationship between two images while decoding. The computation of dynamic relational attention is mathematically equivalent to attention over all visual “relationships”. Thus, our method provides a direct way to model visual relationships in language. To show the effectiveness of our models, we evaluate them on three datasets: our new dataset, the ”Spot-the-Diff” dataset (Jhamtani and BergKirkpatrick, 2018), and the two-image visual reasoning NLVR2 dataset (Suhr et al., 2019) (adapted for our task). We train models separately on each dataset with the same hyper-parameters and evaluate them on the same test set across all methods. Experimental results demonstrate that our model outperforms all the baselines and existing methods. The main contributions of our paper are: (1) We create a novel human language guided image editing dataset to boost the study in describing visual relationships; (2) We design novel relationalspeaker models, including a dynamic relational attention module, to handle the problem of twoimage captioning by focusing on all their visual relation"
P19-1209,P18-1063,0,0.276081,"., 2016b; Tan et al., 2017; See et al., 2017; Paulus et al., 2017; Celikyilmaz et al., 2018; Narayan et al., 2018). Training these models end-to-end means learning to perform both tasks simultaneously and can require a massive amount of data that is unavailable and unaffordable for many summarization tasks. Recent approaches emphasize the importance of separating content selection from summary generation for abstractive summarization. Studies exploit extractive methods to identify content words and sentences that should be part of the summary and use them to guide the generation of abstracts (Chen and Bansal, 2018; Gehrmann et al., 2018; Lebanoff et al., 2018). On the other hand, surface lexical features have been shown to be effective in identifying pertinent content (Carenini et al., 2006; Wong et al., 2008; Galanis et al., 2012). Examples include sentence length, position, centrality, word frequency, whether a sentence contains topic words, and others. The surface cues can also be customized for new domains relatively easily. This paper represents a step forward in this direction, where we focus on developing lightweight models to select summary-worthy sentence singletons and pairs and use them as t"
P19-1209,C16-1101,0,0.0219898,"thar Abbas said the report “unfounded and malicious” and an “effort to malign the ISI,” – Pakistan’s directorate of inter-services intelligence. Compressed Sentence: Maj. Gen. Athar Abbas said the report was an “effort to malign the ISI.” Table 1: Example sentence singleton and pair, before and after compression/merging. and identifying the sentence relationships for performing fusion. Previous studies assume a set of similar source sentences can be gathered by clustering sentences or by comparing to a reference summary sentence (Barzilay and McKeown, 2005; Filippova, 2010; Shen and Li, 2010; Chenal and Cheung, 2016; Liao et al., 2018); but these methods can be suboptimal. Joint models for sentence selection and fusion implicitly perform content planning (Martins and Smith, 2009; BergKirkpatrick et al., 2011; Bing et al., 2015; Durrett et al., 2016) and there is limited control over which sentences are merged and how. In contrast, this work attempts to teach the system to determine if a sentence singleton or a pair should be selected to produce a summary sentence. A sentence pair (A, B) is preferred over its consisting sentences if they carry complementary content. Table 1 shows an example. Sentence B co"
P19-1209,P16-1046,0,0.250922,"Missing"
P19-1209,P19-1098,1,0.870684,"Missing"
P19-1209,P02-1057,0,0.244805,"Missing"
P19-1209,W04-1016,0,0.0791209,"Missing"
P19-1209,P16-1188,0,0.0590186,"able 1: Example sentence singleton and pair, before and after compression/merging. and identifying the sentence relationships for performing fusion. Previous studies assume a set of similar source sentences can be gathered by clustering sentences or by comparing to a reference summary sentence (Barzilay and McKeown, 2005; Filippova, 2010; Shen and Li, 2010; Chenal and Cheung, 2016; Liao et al., 2018); but these methods can be suboptimal. Joint models for sentence selection and fusion implicitly perform content planning (Martins and Smith, 2009; BergKirkpatrick et al., 2011; Bing et al., 2015; Durrett et al., 2016) and there is limited control over which sentences are merged and how. In contrast, this work attempts to teach the system to determine if a sentence singleton or a pair should be selected to produce a summary sentence. A sentence pair (A, B) is preferred over its consisting sentences if they carry complementary content. Table 1 shows an example. Sentence B contains a reference (“the attack”) and A contains a more complete description for it (“bombing that killed 58”). Sentences A and B each contain certain valuable information, and an appropriate way to merge them exists. As a result, a sente"
P19-1209,C10-1037,0,0.204438,"s been studied in the literature, but there lacks a mechanism to weigh sentence singletons and pairs in a unified space. Extractive methods focus on selecting sentence singletons using greedy (Carbonell and Goldstein, 1998), optimization-based (Gillick and Favre, 2009; Kulesza and Taskar, 2011; Cho et al., 2019), and (non-)autoregressive methods (Cheng and Lapata, 2016; Kedzie et al., 2018). In contrast, existing sentence fusion studies tend to assume ground sets of source sentences are already provided, and the system fuses each set of sentences into a single one (Daum´e III and Marcu, 2004; Filippova, 2010; Thadani and McKeown, 2013). There is thus a crucial gap between sentence selection and fusion to support summarizing by both compressing single sentences and fusing pairs. This paper attempts to bridge the gap by ranking singletons and pairs together by their likelihoods of producing summary sentences. The selection of sentence singletons and pairs can bring benefit to neural abstractive summarization, as a number of studies seek to separate content selection from summary generation (Chen 2175 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2175"
P19-1209,D15-1042,0,0.0711279,"le-like summary (Nallapati et al., 2016; Zhou et al., 2017; Li et al., 2017; Song et al., 2018; Guo et al., 2018; Cao et al., 2018a). Compressive summaries can be generated in a similar vein by selecting important source sentences and then dropping inessential sentence elements such as prepositional phrases. Before the era of deep neural networks it has been an active area of research, where sentence selection and compression can be accomplished using a pipeline or a joint model (Daum´e III and Marcu, 2002; Zajic et al., 2007; Gillick and Favre, 2009; Wang et al., 2013; Li et al., 2013, 2014; Filippova et al., 2015). A majority of these studies focus on selecting and compressing sentence singletons only. A sentence can also be generated through fusing multiple source sentences. However, many aspects of this approach are largely underinvestigated, such as determining the set of source sentences to be fused, handling its large cardinality, 2176 Sentence Pair: (A) The bombing killed 58 people. (B) Wajid Shamsul Hasan, Pakistan’s high commissioner to Britain, and Hamid Gul, former head of the ISI, firmly denied the agency’s involvement in the attack. Merged Sentence: Pakistan denies its spy agency helped pla"
P19-1209,C12-1056,0,0.0226888,"sive amount of data that is unavailable and unaffordable for many summarization tasks. Recent approaches emphasize the importance of separating content selection from summary generation for abstractive summarization. Studies exploit extractive methods to identify content words and sentences that should be part of the summary and use them to guide the generation of abstracts (Chen and Bansal, 2018; Gehrmann et al., 2018; Lebanoff et al., 2018). On the other hand, surface lexical features have been shown to be effective in identifying pertinent content (Carenini et al., 2006; Wong et al., 2008; Galanis et al., 2012). Examples include sentence length, position, centrality, word frequency, whether a sentence contains topic words, and others. The surface cues can also be customized for new domains relatively easily. This paper represents a step forward in this direction, where we focus on developing lightweight models to select summary-worthy sentence singletons and pairs and use them as the basis for summary generation. A succinct sentence can be generated by shortening or rewriting a lengthy source text. Recent studies have leveraged neural encoder-decoder models to rewrite the first sentence of an articl"
P19-1209,C10-1039,0,0.33218,"Missing"
P19-1209,D18-1443,0,0.220446,"Missing"
P19-1209,W09-1802,0,0.15857,"decoder models to rewrite the first sentence of an article to a title-like summary (Nallapati et al., 2016; Zhou et al., 2017; Li et al., 2017; Song et al., 2018; Guo et al., 2018; Cao et al., 2018a). Compressive summaries can be generated in a similar vein by selecting important source sentences and then dropping inessential sentence elements such as prepositional phrases. Before the era of deep neural networks it has been an active area of research, where sentence selection and compression can be accomplished using a pipeline or a joint model (Daum´e III and Marcu, 2002; Zajic et al., 2007; Gillick and Favre, 2009; Wang et al., 2013; Li et al., 2013, 2014; Filippova et al., 2015). A majority of these studies focus on selecting and compressing sentence singletons only. A sentence can also be generated through fusing multiple source sentences. However, many aspects of this approach are largely underinvestigated, such as determining the set of source sentences to be fused, handling its large cardinality, 2176 Sentence Pair: (A) The bombing killed 58 people. (B) Wajid Shamsul Hasan, Pakistan’s high commissioner to Britain, and Hamid Gul, former head of the ISI, firmly denied the agency’s involvement in the"
P19-1209,P18-1064,0,0.0317942,"c words, and others. The surface cues can also be customized for new domains relatively easily. This paper represents a step forward in this direction, where we focus on developing lightweight models to select summary-worthy sentence singletons and pairs and use them as the basis for summary generation. A succinct sentence can be generated by shortening or rewriting a lengthy source text. Recent studies have leveraged neural encoder-decoder models to rewrite the first sentence of an article to a title-like summary (Nallapati et al., 2016; Zhou et al., 2017; Li et al., 2017; Song et al., 2018; Guo et al., 2018; Cao et al., 2018a). Compressive summaries can be generated in a similar vein by selecting important source sentences and then dropping inessential sentence elements such as prepositional phrases. Before the era of deep neural networks it has been an active area of research, where sentence selection and compression can be accomplished using a pipeline or a joint model (Daum´e III and Marcu, 2002; Zajic et al., 2007; Gillick and Favre, 2009; Wang et al., 2013; Li et al., 2013, 2014; Filippova et al., 2015). A majority of these studies focus on selecting and compressing sentence singletons only"
P19-1209,N09-1041,0,0.119576,"ffectively train on single-document inputs and transfer to the multi-document setting. 5 Results Evaluation Setup In this section we evaluate our proposed methods on identifying summaryworthy instances including singletons and pairs. We compare this scheme with traditional methods extracting only singletons, then introduce novel evaluation strategies to compare results. We exploit several strong extractive baselines: (i) SumBasic (Vanderwende et al., 2007) extracts sentences by assuming words occurring frequently in a document have higher chances of being included in the summary; (ii) KL-Sum (Haghighi and Vanderwende, 2009) greedily adds sentences to the summary to minimize KL divergence; (iii) LexRank (Erkan and Radev, 2004) estimates sentence importance based on eigenvector centrality in a document graph representation. Further, we include the L EAD method that selects the first N sentences from each document. We then require all systems to extract N instances, i.e., either singletons or pairs, from the input document(s).5 We compare system-identified instances with ground-truth instances, and in particular, we compare against the primary, secondary, and full set of ground-truth sentences. A primary sentence i"
P19-1209,E14-1075,0,0.0126762,"and pairs. BERT learns instance representations by attending to important content words, where the importance is signaled by word and position embeddings as well as pairwise word relationships. Nonetheless, it remains an open question whether BERT can successfully weave the meaning of topically important words into representations. A word “border” is topically important if the input document discusses border security. A topic word is likely to be repeatedly mentioned in the input document but less frequently elsewhere. Because sentences containing topical words are often deemed summaryworthy (Hong and Nenkova, 2014), it is desirable to represent sentence singletons and pairs based on the amount of topical content they convey. VSM represents each sentence as a sparse vector. Each dimension of the vector corresponds to an n-gram weighted by its TF-IDF score. A high TF-IDF score suggests the n-gram is important to the topic of discussion. We further strengthen the sentence vector with position and centrality information, i.e., the sentence position in the document and the cosine similarity between the sentence and document vector. We obtain a document vector by averaging over its sentence vectors, and we si"
P19-1209,P18-1013,0,0.0358373,"ng by both compressing single sentences and fusing pairs. This paper attempts to bridge the gap by ranking singletons and pairs together by their likelihoods of producing summary sentences. The selection of sentence singletons and pairs can bring benefit to neural abstractive summarization, as a number of studies seek to separate content selection from summary generation (Chen 2175 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2175–2189 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics and Bansal, 2018; Hsu et al., 2018; Gehrmann et al., 2018; Lebanoff et al., 2018). Content selection draws on domain knowledge to identify relevant content, while summary generation weaves together selected source and vocabulary words to form a coherent summary. Despite having local coherence, system summaries can sometimes contain erroneous details (See et al., 2017) and forged content (Cao et al., 2018b; Song et al., 2018). Separating the two tasks of content selection and summary generation allows us to closely examine the compressing and fusing mechanisms of an abstractive summarizer. In this paper we propose a method to l"
P19-1209,D18-1208,0,0.116907,"Missing"
P19-1209,D18-1446,1,0.933437,"nd fusing pairs. This paper attempts to bridge the gap by ranking singletons and pairs together by their likelihoods of producing summary sentences. The selection of sentence singletons and pairs can bring benefit to neural abstractive summarization, as a number of studies seek to separate content selection from summary generation (Chen 2175 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2175–2189 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics and Bansal, 2018; Hsu et al., 2018; Gehrmann et al., 2018; Lebanoff et al., 2018). Content selection draws on domain knowledge to identify relevant content, while summary generation weaves together selected source and vocabulary words to form a coherent summary. Despite having local coherence, system summaries can sometimes contain erroneous details (See et al., 2017) and forged content (Cao et al., 2018b; Song et al., 2018). Separating the two tasks of content selection and summary generation allows us to closely examine the compressing and fusing mechanisms of an abstractive summarizer. In this paper we propose a method to learn to select sentence singletons and pairs, w"
P19-1209,D13-1047,1,0.937752,"Missing"
P19-1209,D17-1222,0,0.0239747,"cy, whether a sentence contains topic words, and others. The surface cues can also be customized for new domains relatively easily. This paper represents a step forward in this direction, where we focus on developing lightweight models to select summary-worthy sentence singletons and pairs and use them as the basis for summary generation. A succinct sentence can be generated by shortening or rewriting a lengthy source text. Recent studies have leveraged neural encoder-decoder models to rewrite the first sentence of an article to a title-like summary (Nallapati et al., 2016; Zhou et al., 2017; Li et al., 2017; Song et al., 2018; Guo et al., 2018; Cao et al., 2018a). Compressive summaries can be generated in a similar vein by selecting important source sentences and then dropping inessential sentence elements such as prepositional phrases. Before the era of deep neural networks it has been an active area of research, where sentence selection and compression can be accomplished using a pipeline or a joint model (Daum´e III and Marcu, 2002; Zajic et al., 2007; Gillick and Favre, 2009; Wang et al., 2013; Li et al., 2013, 2014; Filippova et al., 2015). A majority of these studies focus on selecting and"
P19-1209,C18-1101,1,0.867493,"t “unfounded and malicious” and an “effort to malign the ISI,” – Pakistan’s directorate of inter-services intelligence. Compressed Sentence: Maj. Gen. Athar Abbas said the report was an “effort to malign the ISI.” Table 1: Example sentence singleton and pair, before and after compression/merging. and identifying the sentence relationships for performing fusion. Previous studies assume a set of similar source sentences can be gathered by clustering sentences or by comparing to a reference summary sentence (Barzilay and McKeown, 2005; Filippova, 2010; Shen and Li, 2010; Chenal and Cheung, 2016; Liao et al., 2018); but these methods can be suboptimal. Joint models for sentence selection and fusion implicitly perform content planning (Martins and Smith, 2009; BergKirkpatrick et al., 2011; Bing et al., 2015; Durrett et al., 2016) and there is limited control over which sentences are merged and how. In contrast, this work attempts to teach the system to determine if a sentence singleton or a pair should be selected to produce a summary sentence. A sentence pair (A, B) is preferred over its consisting sentences if they carry complementary content. Table 1 shows an example. Sentence B contains a reference ("
P19-1209,W04-1013,0,0.299189,"Missing"
P19-1209,W09-1801,0,0.104518,"Maj. Gen. Athar Abbas said the report was an “effort to malign the ISI.” Table 1: Example sentence singleton and pair, before and after compression/merging. and identifying the sentence relationships for performing fusion. Previous studies assume a set of similar source sentences can be gathered by clustering sentences or by comparing to a reference summary sentence (Barzilay and McKeown, 2005; Filippova, 2010; Shen and Li, 2010; Chenal and Cheung, 2016; Liao et al., 2018); but these methods can be suboptimal. Joint models for sentence selection and fusion implicitly perform content planning (Martins and Smith, 2009; BergKirkpatrick et al., 2011; Bing et al., 2015; Durrett et al., 2016) and there is limited control over which sentences are merged and how. In contrast, this work attempts to teach the system to determine if a sentence singleton or a pair should be selected to produce a summary sentence. A sentence pair (A, B) is preferred over its consisting sentences if they carry complementary content. Table 1 shows an example. Sentence B contains a reference (“the attack”) and A contains a more complete description for it (“bombing that killed 58”). Sentences A and B each contain certain valuable inform"
P19-1209,D18-1206,0,0.20014,"ntence singletons and pairs. We perform extensive experiments and report findings on sentence selection and abstraction.1 2 Related Work Content selection is integral to any summarization system. Neural approaches to abstractive summarization often perform content selection jointly with surface realization using an encoder-decoder architecture (Rush et al., 2015; Nallapati et al., 1 We make our code and models publicly available at https: //github.com/ucfnlp/summarization-sing-pair-mix 2016; Chen et al., 2016b; Tan et al., 2017; See et al., 2017; Paulus et al., 2017; Celikyilmaz et al., 2018; Narayan et al., 2018). Training these models end-to-end means learning to perform both tasks simultaneously and can require a massive amount of data that is unavailable and unaffordable for many summarization tasks. Recent approaches emphasize the importance of separating content selection from summary generation for abstractive summarization. Studies exploit extractive methods to identify content words and sentences that should be part of the summary and use them to guide the generation of abstracts (Chen and Bansal, 2018; Gehrmann et al., 2018; Lebanoff et al., 2018). On the other hand, surface lexical features"
P19-1209,D15-1044,0,0.156666,"ng a pair. Compared to abstractive summarizers that perform content selection implicitly, our method is flexible and can be extended to multi-document summarization where training data is limited; • we investigate the factors involved in representing sentence singletons and pairs. We perform extensive experiments and report findings on sentence selection and abstraction.1 2 Related Work Content selection is integral to any summarization system. Neural approaches to abstractive summarization often perform content selection jointly with surface realization using an encoder-decoder architecture (Rush et al., 2015; Nallapati et al., 1 We make our code and models publicly available at https: //github.com/ucfnlp/summarization-sing-pair-mix 2016; Chen et al., 2016b; Tan et al., 2017; See et al., 2017; Paulus et al., 2017; Celikyilmaz et al., 2018; Narayan et al., 2018). Training these models end-to-end means learning to perform both tasks simultaneously and can require a massive amount of data that is unavailable and unaffordable for many summarization tasks. Recent approaches emphasize the importance of separating content selection from summary generation for abstractive summarization. Studies exploit ex"
P19-1209,P17-1099,0,0.81231,"tent selection from summary generation (Chen 2175 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2175–2189 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics and Bansal, 2018; Hsu et al., 2018; Gehrmann et al., 2018; Lebanoff et al., 2018). Content selection draws on domain knowledge to identify relevant content, while summary generation weaves together selected source and vocabulary words to form a coherent summary. Despite having local coherence, system summaries can sometimes contain erroneous details (See et al., 2017) and forged content (Cao et al., 2018b; Song et al., 2018). Separating the two tasks of content selection and summary generation allows us to closely examine the compressing and fusing mechanisms of an abstractive summarizer. In this paper we propose a method to learn to select sentence singletons and pairs, which then serve as the basis for an abstractive summarizer to compose a summary sentence-by-sentence, where singletons are shortened (i.e., compressed) and pairs are merged (i.e., fused). We exploit stateof-the-art neural representations and traditional vector space models to characterize"
P19-1209,C10-1111,0,0.168223,"kistani Maj. Gen. Athar Abbas said the report “unfounded and malicious” and an “effort to malign the ISI,” – Pakistan’s directorate of inter-services intelligence. Compressed Sentence: Maj. Gen. Athar Abbas said the report was an “effort to malign the ISI.” Table 1: Example sentence singleton and pair, before and after compression/merging. and identifying the sentence relationships for performing fusion. Previous studies assume a set of similar source sentences can be gathered by clustering sentences or by comparing to a reference summary sentence (Barzilay and McKeown, 2005; Filippova, 2010; Shen and Li, 2010; Chenal and Cheung, 2016; Liao et al., 2018); but these methods can be suboptimal. Joint models for sentence selection and fusion implicitly perform content planning (Martins and Smith, 2009; BergKirkpatrick et al., 2011; Bing et al., 2015; Durrett et al., 2016) and there is limited control over which sentences are merged and how. In contrast, this work attempts to teach the system to determine if a sentence singleton or a pair should be selected to produce a summary sentence. A sentence pair (A, B) is preferred over its consisting sentences if they carry complementary content. Table 1 shows"
P19-1209,C18-1146,1,0.947592,"ings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2175–2189 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics and Bansal, 2018; Hsu et al., 2018; Gehrmann et al., 2018; Lebanoff et al., 2018). Content selection draws on domain knowledge to identify relevant content, while summary generation weaves together selected source and vocabulary words to form a coherent summary. Despite having local coherence, system summaries can sometimes contain erroneous details (See et al., 2017) and forged content (Cao et al., 2018b; Song et al., 2018). Separating the two tasks of content selection and summary generation allows us to closely examine the compressing and fusing mechanisms of an abstractive summarizer. In this paper we propose a method to learn to select sentence singletons and pairs, which then serve as the basis for an abstractive summarizer to compose a summary sentence-by-sentence, where singletons are shortened (i.e., compressed) and pairs are merged (i.e., fused). We exploit stateof-the-art neural representations and traditional vector space models to characterize singletons and pairs; we then provide suggestions on the"
P19-1209,P17-1108,0,0.0475676,"training data is limited; • we investigate the factors involved in representing sentence singletons and pairs. We perform extensive experiments and report findings on sentence selection and abstraction.1 2 Related Work Content selection is integral to any summarization system. Neural approaches to abstractive summarization often perform content selection jointly with surface realization using an encoder-decoder architecture (Rush et al., 2015; Nallapati et al., 1 We make our code and models publicly available at https: //github.com/ucfnlp/summarization-sing-pair-mix 2016; Chen et al., 2016b; Tan et al., 2017; See et al., 2017; Paulus et al., 2017; Celikyilmaz et al., 2018; Narayan et al., 2018). Training these models end-to-end means learning to perform both tasks simultaneously and can require a massive amount of data that is unavailable and unaffordable for many summarization tasks. Recent approaches emphasize the importance of separating content selection from summary generation for abstractive summarization. Studies exploit extractive methods to identify content words and sentences that should be part of the summary and use them to guide the generation of abstracts (Chen and Bansal, 2018; Geh"
P19-1209,I13-1198,0,0.208774,"Missing"
P19-1209,P13-1136,0,0.0812164,"the first sentence of an article to a title-like summary (Nallapati et al., 2016; Zhou et al., 2017; Li et al., 2017; Song et al., 2018; Guo et al., 2018; Cao et al., 2018a). Compressive summaries can be generated in a similar vein by selecting important source sentences and then dropping inessential sentence elements such as prepositional phrases. Before the era of deep neural networks it has been an active area of research, where sentence selection and compression can be accomplished using a pipeline or a joint model (Daum´e III and Marcu, 2002; Zajic et al., 2007; Gillick and Favre, 2009; Wang et al., 2013; Li et al., 2013, 2014; Filippova et al., 2015). A majority of these studies focus on selecting and compressing sentence singletons only. A sentence can also be generated through fusing multiple source sentences. However, many aspects of this approach are largely underinvestigated, such as determining the set of source sentences to be fused, handling its large cardinality, 2176 Sentence Pair: (A) The bombing killed 58 people. (B) Wajid Shamsul Hasan, Pakistan’s high commissioner to Britain, and Hamid Gul, former head of the ISI, firmly denied the agency’s involvement in the attack. Merged Sen"
P19-1209,C08-1124,0,0.0538945,"d can require a massive amount of data that is unavailable and unaffordable for many summarization tasks. Recent approaches emphasize the importance of separating content selection from summary generation for abstractive summarization. Studies exploit extractive methods to identify content words and sentences that should be part of the summary and use them to guide the generation of abstracts (Chen and Bansal, 2018; Gehrmann et al., 2018; Lebanoff et al., 2018). On the other hand, surface lexical features have been shown to be effective in identifying pertinent content (Carenini et al., 2006; Wong et al., 2008; Galanis et al., 2012). Examples include sentence length, position, centrality, word frequency, whether a sentence contains topic words, and others. The surface cues can also be customized for new domains relatively easily. This paper represents a step forward in this direction, where we focus on developing lightweight models to select summary-worthy sentence singletons and pairs and use them as the basis for summary generation. A succinct sentence can be generated by shortening or rewriting a lengthy source text. Recent studies have leveraged neural encoder-decoder models to rewrite the firs"
P19-1209,P17-1101,0,0.0565003,"ality, word frequency, whether a sentence contains topic words, and others. The surface cues can also be customized for new domains relatively easily. This paper represents a step forward in this direction, where we focus on developing lightweight models to select summary-worthy sentence singletons and pairs and use them as the basis for summary generation. A succinct sentence can be generated by shortening or rewriting a lengthy source text. Recent studies have leveraged neural encoder-decoder models to rewrite the first sentence of an article to a title-like summary (Nallapati et al., 2016; Zhou et al., 2017; Li et al., 2017; Song et al., 2018; Guo et al., 2018; Cao et al., 2018a). Compressive summaries can be generated in a similar vein by selecting important source sentences and then dropping inessential sentence elements such as prepositional phrases. Before the era of deep neural networks it has been an active area of research, where sentence selection and compression can be accomplished using a pipeline or a joint model (Daum´e III and Marcu, 2002; Zajic et al., 2007; Gillick and Favre, 2009; Wang et al., 2013; Li et al., 2013, 2014; Filippova et al., 2015). A majority of these studies focus"
P19-1209,J05-3002,0,\N,Missing
P19-1209,D14-1076,1,\N,Missing
P19-1209,W01-0100,0,\N,Missing
P19-1209,P18-1015,0,\N,Missing
P19-1209,E06-1039,0,\N,Missing
P19-1209,P11-1049,0,\N,Missing
S17-2171,U12-1017,0,0.0540775,"Missing"
S17-2171,P05-1053,0,0.34906,"and ranked first for relation extraction (subtask C). Existing systems for relation extraction can be classified into five categories (Zettlemoyer, 2013): systems based on hand-built patterns (Yangarber and Grishman, 1998), bootstrapping methods (Brin, 1998), unsupervised methods (Gonzalez and Turmo, 2009), distant supervision (Snow et al., 2004), and supervised methods. We focus on supervised methods, as the ScienceIE shared task provides a labeled training set. Supervised methods for relation extraction commonly employ support vector machines (Uzuner et al., 2010, 2011; Minard et al., 2011; GuoDong et al., 2005), na¨ıve Bayes (Zayaraz and Kumara, 2015), maximum entropy (Sun and Grishman, 2012), or conditional random fields (Sutton and McCallum, 2006). These methods require the practitioner to handcraft features, such as surface, lexical, syntactic features (Grouin et al., 2010) or features derived from existing ontologies (Rink et al., 2011). The use of kernels based on dependency trees has also been explored (Bunescu and Mooney, 2005; Culotta and Sorensen, 2004; Zhou et al., 2007). More recently, a few studies have investigated the use of artificial neural networks for relation extraction (Socher et"
S17-2171,D13-1137,0,0.0192466,"5), maximum entropy (Sun and Grishman, 2012), or conditional random fields (Sutton and McCallum, 2006). These methods require the practitioner to handcraft features, such as surface, lexical, syntactic features (Grouin et al., 2010) or features derived from existing ontologies (Rink et al., 2011). The use of kernels based on dependency trees has also been explored (Bunescu and Mooney, 2005; Culotta and Sorensen, 2004; Zhou et al., 2007). More recently, a few studies have investigated the use of artificial neural networks for relation extraction (Socher et al., 2012; Nguyen and Grishman, 2015; Hashimoto et al., 2013). Our approach follows this line of work. Over 50 million scholarly articles have been published: they constitute a unique repository of knowledge. In particular, one may infer from them relations between scientific concepts. Artificial neural networks have recently been explored for relation extraction. In this work, we continue this line of work and present a system based on a convolutional neural network to extract relations. Our model ranked first in the SemEval-2017 task 10 (ScienceIE) for relation extraction in scientific articles (subtask C). 1 Introduction and related work The number o"
S17-2171,H05-1091,0,0.24816,"d task provides a labeled training set. Supervised methods for relation extraction commonly employ support vector machines (Uzuner et al., 2010, 2011; Minard et al., 2011; GuoDong et al., 2005), na¨ıve Bayes (Zayaraz and Kumara, 2015), maximum entropy (Sun and Grishman, 2012), or conditional random fields (Sutton and McCallum, 2006). These methods require the practitioner to handcraft features, such as surface, lexical, syntactic features (Grouin et al., 2010) or features derived from existing ontologies (Rink et al., 2011). The use of kernels based on dependency trees has also been explored (Bunescu and Mooney, 2005; Culotta and Sorensen, 2004; Zhou et al., 2007). More recently, a few studies have investigated the use of artificial neural networks for relation extraction (Socher et al., 2012; Nguyen and Grishman, 2015; Hashimoto et al., 2013). Our approach follows this line of work. Over 50 million scholarly articles have been published: they constitute a unique repository of knowledge. In particular, one may infer from them relations between scientific concepts. Artificial neural networks have recently been explored for relation extraction. In this work, we continue this line of work and present a syste"
S17-2171,P04-1054,0,0.19287,"training set. Supervised methods for relation extraction commonly employ support vector machines (Uzuner et al., 2010, 2011; Minard et al., 2011; GuoDong et al., 2005), na¨ıve Bayes (Zayaraz and Kumara, 2015), maximum entropy (Sun and Grishman, 2012), or conditional random fields (Sutton and McCallum, 2006). These methods require the practitioner to handcraft features, such as surface, lexical, syntactic features (Grouin et al., 2010) or features derived from existing ontologies (Rink et al., 2011). The use of kernels based on dependency trees has also been explored (Bunescu and Mooney, 2005; Culotta and Sorensen, 2004; Zhou et al., 2007). More recently, a few studies have investigated the use of artificial neural networks for relation extraction (Socher et al., 2012; Nguyen and Grishman, 2015; Hashimoto et al., 2013). Our approach follows this line of work. Over 50 million scholarly articles have been published: they constitute a unique repository of knowledge. In particular, one may infer from them relations between scientific concepts. Artificial neural networks have recently been explored for relation extraction. In this work, we continue this line of work and present a system based on a convolutional n"
S17-2171,D14-1181,0,0.0107608,"d B are synonyms. If any of B, C, ... , Z is a hyponym of A, then all of them are hyponyms of A. A and B have no relation. A and B have no relation. Table 1: Rules used for postprocessing. We considered B to be an abbreviation of A if the first letters of each token in A form B. The examples are from the training and development sets 2 Model 2.2 The CNN takes each preprocessed sentence as input, and predicts the relation between the two entities. The CNN architecture, illustrated in Figure 1, consists of four main layers, similar to the one used in text classification (Collobert et al., 2011; Kim, 2014; Lee and Dernoncourt, 2016; Gehrmann et al., 2017). 1. the embedding layer converts each feature (word, relative positions 1 and 2, type of entity, and POS tag) into an embedding vector via a lookup table and concatenates them. 2. the convolutional layer with ReLU activation transforms the embeddings into feature maps by sliding filters over the tokens. 3. the max pooling layer selects the highest feature value in each feature map by applying the max operator. 4. the fully connected layer with softmax activation outputs the probability of each relation. Our model for relation extraction compr"
S17-2171,N16-1062,1,0.84587,"onyms. If any of B, C, ... , Z is a hyponym of A, then all of them are hyponyms of A. A and B have no relation. A and B have no relation. Table 1: Rules used for postprocessing. We considered B to be an abbreviation of A if the first letters of each token in A form B. The examples are from the training and development sets 2 Model 2.2 The CNN takes each preprocessed sentence as input, and predicts the relation between the two entities. The CNN architecture, illustrated in Figure 1, consists of four main layers, similar to the one used in text classification (Collobert et al., 2011; Kim, 2014; Lee and Dernoncourt, 2016; Gehrmann et al., 2017). 1. the embedding layer converts each feature (word, relative positions 1 and 2, type of entity, and POS tag) into an embedding vector via a lookup table and concatenates them. 2. the convolutional layer with ReLU activation transforms the embeddings into feature maps by sliding filters over the tokens. 3. the max pooling layer selects the highest feature value in each feature map by applying the max operator. 4. the fully connected layer with softmax activation outputs the probability of each relation. Our model for relation extraction comprises three parts: preproces"
S17-2171,P14-1038,0,0.0212532,"ome increasingly difficult to take advantage of all available information due to its sheer amount. One challenge is that the knowledge present in scholarly articles is mostly unstructured. One approach to organize this knowledge is to classify each sentence (Kim et al., 2011; Amini et al., 2012; Hassanzadeh et al., 2014; Dernoncourt et al., 2016). Another approach is to extract entities and relations between them, which is the focus of the ScienceIE shared task at SemEval2017 (Augenstein et al., 2017). Relation extraction can be seen as a process comprising two steps that can be done jointly (Li and Ji, 2014) or separately: first, entities of interest need to be identified, and second, the relation among the entities has to be determined. In ∗ Peter Szolovits MIT psz@mit.edu These authors contributed equally to this work. 978 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 978–984, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics word relative position1 relative position2 type of entity multivariate 0 -5 B-Process JJ EMD 0 -4 I-Process NN is 1 -3 O VBZ an 2 -2 O DT effective 3 -1 O JJ signal 4 0 B-Process NN decomp"
S17-2171,P14-5010,0,0.00408804,"rocessing. 2.1 CNN architecture Preprocessing The preprocessing step takes as input each raw text (i.e., a paragraph of a scientific article in ScienceIE) as well as the location of all entities present in the text, and output several examples. Each example is represented as a list of tokens, each with four features: the relative positions of the two entity mentions, and their entity types and part-ofspeech (POS) tags. Figure 1 shows an example from the ScienceIE corpus in the table on the left. Sentence and token boundaries as well as POS tags are detected using the Stanford CoreNLP toolkit (Manning et al., 2014), and every pair of entity mentions of the same type within each sentence boundary are considered to be of a potential relation. We also remove any references (e.g. [1, 2]), which are irrelevant to the task, and ensure that the sentences are not too long by eliminating the tokens before the beginning of the first entity mention and after the end of the second entity mention. 2.3 Rule-based postprocessing The postprocessing step uses the rules in Table 1 to correct the relations detected by the CNN, or to detect additional relations. These rules were developed from the examples in the training"
S17-2171,R11-1086,0,0.161159,"onal neural networks and ranked first for relation extraction (subtask C). Existing systems for relation extraction can be classified into five categories (Zettlemoyer, 2013): systems based on hand-built patterns (Yangarber and Grishman, 1998), bootstrapping methods (Brin, 1998), unsupervised methods (Gonzalez and Turmo, 2009), distant supervision (Snow et al., 2004), and supervised methods. We focus on supervised methods, as the ScienceIE shared task provides a labeled training set. Supervised methods for relation extraction commonly employ support vector machines (Uzuner et al., 2010, 2011; Minard et al., 2011; GuoDong et al., 2005), na¨ıve Bayes (Zayaraz and Kumara, 2015), maximum entropy (Sun and Grishman, 2012), or conditional random fields (Sutton and McCallum, 2006). These methods require the practitioner to handcraft features, such as surface, lexical, syntactic features (Grouin et al., 2010) or features derived from existing ontologies (Rink et al., 2011). The use of kernels based on dependency trees has also been explored (Bunescu and Mooney, 2005; Culotta and Sorensen, 2004; Zhou et al., 2007). More recently, a few studies have investigated the use of artificial neural networks for relatio"
S17-2171,W15-1506,0,0.212423,"es (Zayaraz and Kumara, 2015), maximum entropy (Sun and Grishman, 2012), or conditional random fields (Sutton and McCallum, 2006). These methods require the practitioner to handcraft features, such as surface, lexical, syntactic features (Grouin et al., 2010) or features derived from existing ontologies (Rink et al., 2011). The use of kernels based on dependency trees has also been explored (Bunescu and Mooney, 2005; Culotta and Sorensen, 2004; Zhou et al., 2007). More recently, a few studies have investigated the use of artificial neural networks for relation extraction (Socher et al., 2012; Nguyen and Grishman, 2015; Hashimoto et al., 2013). Our approach follows this line of work. Over 50 million scholarly articles have been published: they constitute a unique repository of knowledge. In particular, one may infer from them relations between scientific concepts. Artificial neural networks have recently been explored for relation extraction. In this work, we continue this line of work and present a system based on a convolutional neural network to extract relations. Our model ranked first in the SemEval-2017 task 10 (ScienceIE) for relation extraction in scientific articles (subtask C). 1 Introduction and"
S17-2171,D15-1062,0,0.183277,"Missing"
S17-2171,M98-1011,0,0.257888,"y relations, i.e. relations between two entities. Extracted relations can be used for a variety of tasks such as question-answering systems (Ravichandran and Hovy, 2002), ontology extension (Schutz and Buitelaar, 2005), and clinical trials (Frunza and Inkpen, 2011). In this paper, we describe the system that we submitted for the ScienceIE shared task. Our system is based on convolutional neural networks and ranked first for relation extraction (subtask C). Existing systems for relation extraction can be classified into five categories (Zettlemoyer, 2013): systems based on hand-built patterns (Yangarber and Grishman, 1998), bootstrapping methods (Brin, 1998), unsupervised methods (Gonzalez and Turmo, 2009), distant supervision (Snow et al., 2004), and supervised methods. We focus on supervised methods, as the ScienceIE shared task provides a labeled training set. Supervised methods for relation extraction commonly employ support vector machines (Uzuner et al., 2010, 2011; Minard et al., 2011; GuoDong et al., 2005), na¨ıve Bayes (Zayaraz and Kumara, 2015), maximum entropy (Sun and Grishman, 2012), or conditional random fields (Sutton and McCallum, 2006). These methods require the practitioner to handcraft featur"
S17-2171,D14-1162,0,0.0836004,"ass. During training, the objective is to maximize the log probability of the correct relation type. The model is trained using stochastic gradient descent with minibatch of size 16, updating all parameters, i.e., token embeddings, feature embeddings, CNN filter weights, and fully connected layer weights, at each gradient descent step. For regularization, dropout is applied before the fully connected layer, and early stop with a patience of 10 epochs is used based on the development set. The token embeddings are initialized using publicly available2 pre-trained token embeddings, namely GloVe (Pennington et al., 2014) trained on Wikipedia and Gigaword 5 (Parker et al., 2011). The feature embeddings and the other parameters of the neural network are initialized randomly. To deal with class imbalance, we upsampled the synonym and hyponym classes by duplicating the examples in the positive classes so that the upsampling ratio, i.e., the ratio of the number of positive examples in each class to that of the negative examples, is at least 0.5. Without the upsampling, the trained model would have poor performances. 3 Relation Train Dev Test Hyponym-of 420 123 95 Synonym-of 253 45 112 None 5355 1240 1503 Total 602"
S17-2171,P02-1006,0,0.100103,"Missing"
S17-2171,D07-1076,0,0.0397302,"hods for relation extraction commonly employ support vector machines (Uzuner et al., 2010, 2011; Minard et al., 2011; GuoDong et al., 2005), na¨ıve Bayes (Zayaraz and Kumara, 2015), maximum entropy (Sun and Grishman, 2012), or conditional random fields (Sutton and McCallum, 2006). These methods require the practitioner to handcraft features, such as surface, lexical, syntactic features (Grouin et al., 2010) or features derived from existing ontologies (Rink et al., 2011). The use of kernels based on dependency trees has also been explored (Bunescu and Mooney, 2005; Culotta and Sorensen, 2004; Zhou et al., 2007). More recently, a few studies have investigated the use of artificial neural networks for relation extraction (Socher et al., 2012; Nguyen and Grishman, 2015; Hashimoto et al., 2013). Our approach follows this line of work. Over 50 million scholarly articles have been published: they constitute a unique repository of knowledge. In particular, one may infer from them relations between scientific concepts. Artificial neural networks have recently been explored for relation extraction. In this work, we continue this line of work and present a system based on a convolutional neural network to ext"
S17-2171,D12-1110,0,0.0478416,"l., 2005), na¨ıve Bayes (Zayaraz and Kumara, 2015), maximum entropy (Sun and Grishman, 2012), or conditional random fields (Sutton and McCallum, 2006). These methods require the practitioner to handcraft features, such as surface, lexical, syntactic features (Grouin et al., 2010) or features derived from existing ontologies (Rink et al., 2011). The use of kernels based on dependency trees has also been explored (Bunescu and Mooney, 2005; Culotta and Sorensen, 2004; Zhou et al., 2007). More recently, a few studies have investigated the use of artificial neural networks for relation extraction (Socher et al., 2012; Nguyen and Grishman, 2015; Hashimoto et al., 2013). Our approach follows this line of work. Over 50 million scholarly articles have been published: they constitute a unique repository of knowledge. In particular, one may infer from them relations between scientific concepts. Artificial neural networks have recently been explored for relation extraction. In this work, we continue this line of work and present a system based on a convolutional neural network to extract relations. Our model ranked first in the SemEval-2017 task 10 (ScienceIE) for relation extraction in scientific articles (subt"
S17-2171,E17-2110,1,\N,Missing
S18-1127,S18-1111,0,0.0532723,"ai et al. (2016), which demonstrated a novel approach merging ideas from recurrent networks and convolutional networks based on shortest dependency path (SDP). Xu et al. (2015a) and Santos et al. (2015) both used convolutional architectures along with negative sampling to pursue this task. More recently, Wang et al. (2016) used two levels of attention, one for input selection and the other for output pooling, to boost the performance of their model to state of the art. The 2017 SemEval Task 10 (Augenstein et al., 2017) also featured relation extraction within Introduction SemEval 2018 Task 7 (Gbor et al., 2018) focuses on relation classification and extraction on a corpus of 350 scientific paper abstracts consisting of 1228 and 1248 annotated sentences for subtasks 1.1 and 1.2, respectively. There are six possible relations: USAGE, RESULT, MODEL-FEATURE, PART WHOLE, TOPIC, and COMPARE. Given this data, our task is to take an example sentence, as well as the left and right entities within that sentence, and an indicator as to whether the relation is reversed, and predict the relation type for that sentence. In subtasks 1.1 and 1.2, all presented sentences have a relation. 798 Proceedings of the 12th"
S18-1127,S17-2091,0,0.102261,"Missing"
S18-1127,S10-1057,0,0.247332,"Work Previous SemEval challenges have explored relation identification and extraction. The 2010 SemEval Task 8 (Hendrickx et al., 2010) explored classification of natural language relations, such as CONTENT-CONTAINER or ENTITY-ORIGIN. This challenge differs from ours in its generalizability; our relations are specific to ACL papers (e.g. MODEL-FEATURE) whereas the 2010 relations are more general, and may necessitate more common-sense knowledge than the 2018 relations. The 2010 data has been extensively studied and has offered significant opportunity for other researchers to test their model. Rink and Harabagiu (2010) produced a strong SVM/LR model to attack this challenge. Several deep architectures have also been proposed for this task, including the work of Cai et al. (2016), which demonstrated a novel approach merging ideas from recurrent networks and convolutional networks based on shortest dependency path (SDP). Xu et al. (2015a) and Santos et al. (2015) both used convolutional architectures along with negative sampling to pursue this task. More recently, Wang et al. (2016) used two levels of attention, one for input selection and the other for output pooling, to boost the performance of their model"
S18-1127,P16-1072,0,0.0139699,"language relations, such as CONTENT-CONTAINER or ENTITY-ORIGIN. This challenge differs from ours in its generalizability; our relations are specific to ACL papers (e.g. MODEL-FEATURE) whereas the 2010 relations are more general, and may necessitate more common-sense knowledge than the 2018 relations. The 2010 data has been extensively studied and has offered significant opportunity for other researchers to test their model. Rink and Harabagiu (2010) produced a strong SVM/LR model to attack this challenge. Several deep architectures have also been proposed for this task, including the work of Cai et al. (2016), which demonstrated a novel approach merging ideas from recurrent networks and convolutional networks based on shortest dependency path (SDP). Xu et al. (2015a) and Santos et al. (2015) both used convolutional architectures along with negative sampling to pursue this task. More recently, Wang et al. (2016) used two levels of attention, one for input selection and the other for output pooling, to boost the performance of their model to state of the art. The 2017 SemEval Task 10 (Augenstein et al., 2017) also featured relation extraction within Introduction SemEval 2018 Task 7 (Gbor et al., 201"
S18-1127,P15-1061,0,0.0513017,") whereas the 2010 relations are more general, and may necessitate more common-sense knowledge than the 2018 relations. The 2010 data has been extensively studied and has offered significant opportunity for other researchers to test their model. Rink and Harabagiu (2010) produced a strong SVM/LR model to attack this challenge. Several deep architectures have also been proposed for this task, including the work of Cai et al. (2016), which demonstrated a novel approach merging ideas from recurrent networks and convolutional networks based on shortest dependency path (SDP). Xu et al. (2015a) and Santos et al. (2015) both used convolutional architectures along with negative sampling to pursue this task. More recently, Wang et al. (2016) used two levels of attention, one for input selection and the other for output pooling, to boost the performance of their model to state of the art. The 2017 SemEval Task 10 (Augenstein et al., 2017) also featured relation extraction within Introduction SemEval 2018 Task 7 (Gbor et al., 2018) focuses on relation classification and extraction on a corpus of 350 scientific paper abstracts consisting of 1228 and 1248 annotated sentences for subtasks 1.1 and 1.2, respectively."
S18-1127,P16-1123,0,0.0973069,"Missing"
S18-1127,D15-1062,0,0.0524266,"Missing"
S18-1127,D15-1206,0,0.0437846,"rs (e.g. MODEL-FEATURE) whereas the 2010 relations are more general, and may necessitate more common-sense knowledge than the 2018 relations. The 2010 data has been extensively studied and has offered significant opportunity for other researchers to test their model. Rink and Harabagiu (2010) produced a strong SVM/LR model to attack this challenge. Several deep architectures have also been proposed for this task, including the work of Cai et al. (2016), which demonstrated a novel approach merging ideas from recurrent networks and convolutional networks based on shortest dependency path (SDP). Xu et al. (2015a) and Santos et al. (2015) both used convolutional architectures along with negative sampling to pursue this task. More recently, Wang et al. (2016) used two levels of attention, one for input selection and the other for output pooling, to boost the performance of their model to state of the art. The 2017 SemEval Task 10 (Augenstein et al., 2017) also featured relation extraction within Introduction SemEval 2018 Task 7 (Gbor et al., 2018) focuses on relation classification and extraction on a corpus of 350 scientific paper abstracts consisting of 1228 and 1248 annotated sentences for subtasks"
S18-1127,P16-2034,0,0.0234061,"Section 3.4. Pre-processing tokenizer1 . Data was tokenized using the SpaCy Part of speech (POS) tags were extracted using SpaCy, while lemmas and hypernyms were extracted via WordNet (Miller et al., 1990), inspired by Rink and Harabagiu (2010). 3.2 Initial Experiments We tested several machine learning methods on these data, including a logistic regression classifier over tf-idf features extracted from words, lemmas, hypernyms, and POS. Additionally, we tested deep random forests with multi-grain sequence scanning over word embeddings sequences (Zhou and Feng, 2017) and LSTM with attention (Zhou et al., 2016) over both word/lemma/hypernym embeddings, character sequence embeddings, and position indicators. Lastly, we tested a CNN model over these data, using word/lemma embeddings, position embeddings, and a variant of negative sampling. After optimizing all model configurations and doing preliminary hyperparameter optimization via automatic grid search, early comparisons between the differing model classes yielded the results in Table 1. These results were measured in accuracy over 15-fold cross validation on the 1.1 train set. Given these initial results, we focused principally on the CNN model. 1"
W16-4204,D14-1162,0,0.0822705,"Missing"
W19-4015,U10-1005,0,0.0241306,"ding of the topic to parse. If a definition crosses a sentence boundary, the sequence (in some cases, a full sentence) following the boundary identified as definition-like is labelled as a secondary definition. 3.3 3.2 Contract Data Textbook Data In the textbook data, three-sentence context windows were sampled from sentences that contained a bold n-gram (a strong signal in educational texts indicating a formally defined term) with a context sentence on either side of the sentence with those bold token(s). Consistent with previous research (Cui et al., 2007; Degorski and Przepiorkowski, 2008; Curtotti and McCreath, 2010; Navigli et al., 2010) definitions do in fact appear in the X is a Y form, with a clear “definitor”. However, many textbook examples also lack this explicit trigger, and instead implicitly define the relationship between the term and definition, either by a referential term or referential definition, or through the As mentioned above, the corpus consists of 2443 sentences from SEC contract filings. These sentences are often long, with several term-definition pairs appearing within one sentence. While it is well known that many contracts contain “definition sections”, glossaries, or definition"
W19-4015,P10-1134,0,0.67152,"se of a neural approach, which reached state-of-theart performance on the word class lattices (WCL) datasets (Navigli et al., 2010). Even so, these methods require both term and definition to appear in the same sentence and for terms to appear before definitions. Hypernym detection, a related field, has also garnered interest for quite some time (see e.g., Hearst (1992); Snow et al. (2005); Ritter et al. (2009); Shwartz et al. (2017)). Because many hypernym glosses follow the pattern X, such as Y or X is a (type of) Y, this work contains a subset of cases considered for definition extraction. Navigli and Velardi (2010) demonstrated the use of word class lattices for both hypernym detection and definition extraction, and Yin and Roth (2018) proved the effectiveness of including definitions in the training of hypernym detection models. Most work on definition extraction has been applied solely to English datasets, including the WCL dataset mentioned above (Navigli et al., 2010), the ukWaC dataset (Ferraresi et al., 2008), a large crawled dataset of the .uk domain name, and the W00 dataset, a small, expertly annotated corpus introduced by Jin et al. (2013). There does exist a smaller effort for multilingual ex"
W19-4015,degorski-etal-2008-definition,0,0.100222,"Missing"
W19-4015,navigli-etal-2010-annotated,0,0.895298,"definition phrases. In this paper we present a new corpus of natural language term-definition pairs, as well as a novel schema that can be generally applied for a wide range of domains. 2 Adobe Research 345 Park Ave. San Jose, CA dernonco @adobe.com lack these explicit markers. In an effort to expand on the type of phrases used to extract definitions, Cui et al. (2007) used soft pattern matching in a modified HMM (PHMM). More recent work from Espinosa Anke and Schockaert (2018) makes use of a neural approach, which reached state-of-theart performance on the word class lattices (WCL) datasets (Navigli et al., 2010). Even so, these methods require both term and definition to appear in the same sentence and for terms to appear before definitions. Hypernym detection, a related field, has also garnered interest for quite some time (see e.g., Hearst (1992); Snow et al. (2005); Ritter et al. (2009); Shwartz et al. (2017)). Because many hypernym glosses follow the pattern X, such as Y or X is a (type of) Y, this work contains a subset of cases considered for definition extraction. Navigli and Velardi (2010) demonstrated the use of word class lattices for both hypernym detection and definition extraction, and Y"
W19-4015,passonneau-2006-measuring,0,0.111744,", do not require this level of specificity; though they may state similar facts, such as the date or location of an event, this information is arguably not crucial to the understanding of the core definition. While we may argue for either including or excluding these textbook counterparts, the DEFT corpus does not label them. Our annotation process favors maintaining the most basic definition of the term without compromising Inter-annotator Agreement Inter-annotator agreement (IAA) is measured using a modified version of Krippendorff’s alpha (Krippendorff, 2011) with the MASI distance metric (Passonneau, 2006) in order to account for and score partial sequential overlaps of text: ( M ASI(c, k), if c = k δ(c, k) = 1, otherwise Where c = k and the text spans match exactly, the MASI distance is 0. IAA was calculated after every training period, with a final annotator agreement score of αterm = 0.80 and αdef inition = 0.50 for the textbook corpus and αterm = 0.85 and αdef inition = 0.54 for the contract corpus. We believe these IAA scores match the reality of human performance on such a complicated task. After training time, each sentence in the corpus was labeled by one annotator. For the textbook ann"
W19-4015,W09-4406,0,0.222964,"m detection models. Most work on definition extraction has been applied solely to English datasets, including the WCL dataset mentioned above (Navigli et al., 2010), the ukWaC dataset (Ferraresi et al., 2008), a large crawled dataset of the .uk domain name, and the W00 dataset, a small, expertly annotated corpus introduced by Jin et al. (2013). There does exist a smaller effort for multilingual explorations, including German (Storrer and Wellinghoff, 2006), Portuguese (Del Gaudio and Branco, 2007), and Slavic (Przepi´orkowski et al., 2007), as well as some language-independent approaches (Del Gaudio and Branco, 2009). The vast majority of these approaches are for unstructured text, typically scraped from online sources, as in the ukWaC dataset, though some interest has been given specifically for semi-structured text in legal contracts (see e.g. Curtotti and McCreath Definition extraction has been a popular topic in NLP research for well more than a decade, but has been historically limited to welldefined, structured, and narrow conditions. In reality, natural language is messy, and messy data requires both complex solutions and data that reflects that reality. In this paper, we present a robust English c"
W19-4015,N18-2061,0,0.540801,"Missing"
W19-4015,W06-2609,0,0.662356,"both complex solutions and data that reflects that reality. In this paper, we present a robust English corpus and annotation schema that allows us to explore the less straightforward examples of term-definition structures in free and semi-structured text. 1 3 Related Work Most related work on definition extraction has relied on the idea that definitions can be captured by common “definitor” verb phrases like “means”, “refers to”, and “is”. Early work in the field incorporated rule-based methods that extracted sentences that met this narrow standard (JL Clavens, 2001; Cui and Chua, 2004, 2005; Fahmi and Bouma, 2006; Zhang and Jiang, 2009). While predictable and easily applied, these models subsequently failed to extract sentences that ∗ Work was completed while individual was employed at Adobe Research. 124 Proceedings of the 13th Linguistic Annotation Workshop, pages 124–131 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics Dataset # of positive annotations Size (in sentences) WCL W00 DEFT 1,871 731 11,004 4,718 2,185 23,746 ture (e.g., containing secondary information, containing ambiguous references to previously stated terms or definitions) whereby the relationship be"
W19-4015,E17-1007,0,0.0208515,"he type of phrases used to extract definitions, Cui et al. (2007) used soft pattern matching in a modified HMM (PHMM). More recent work from Espinosa Anke and Schockaert (2018) makes use of a neural approach, which reached state-of-theart performance on the word class lattices (WCL) datasets (Navigli et al., 2010). Even so, these methods require both term and definition to appear in the same sentence and for terms to appear before definitions. Hypernym detection, a related field, has also garnered interest for quite some time (see e.g., Hearst (1992); Snow et al. (2005); Ritter et al. (2009); Shwartz et al. (2017)). Because many hypernym glosses follow the pattern X, such as Y or X is a (type of) Y, this work contains a subset of cases considered for definition extraction. Navigli and Velardi (2010) demonstrated the use of word class lattices for both hypernym detection and definition extraction, and Yin and Roth (2018) proved the effectiveness of including definitions in the training of hypernym detection models. Most work on definition extraction has been applied solely to English datasets, including the WCL dataset mentioned above (Navigli et al., 2010), the ukWaC dataset (Ferraresi et al., 2008), a"
W19-4015,C92-2082,0,0.189056,"lack these explicit markers. In an effort to expand on the type of phrases used to extract definitions, Cui et al. (2007) used soft pattern matching in a modified HMM (PHMM). More recent work from Espinosa Anke and Schockaert (2018) makes use of a neural approach, which reached state-of-theart performance on the word class lattices (WCL) datasets (Navigli et al., 2010). Even so, these methods require both term and definition to appear in the same sentence and for terms to appear before definitions. Hypernym detection, a related field, has also garnered interest for quite some time (see e.g., Hearst (1992); Snow et al. (2005); Ritter et al. (2009); Shwartz et al. (2017)). Because many hypernym glosses follow the pattern X, such as Y or X is a (type of) Y, this work contains a subset of cases considered for definition extraction. Navigli and Velardi (2010) demonstrated the use of word class lattices for both hypernym detection and definition extraction, and Yin and Roth (2018) proved the effectiveness of including definitions in the training of hypernym detection models. Most work on definition extraction has been applied solely to English datasets, including the WCL dataset mentioned above (Nav"
W19-4015,D13-1073,0,0.471548,"f cases considered for definition extraction. Navigli and Velardi (2010) demonstrated the use of word class lattices for both hypernym detection and definition extraction, and Yin and Roth (2018) proved the effectiveness of including definitions in the training of hypernym detection models. Most work on definition extraction has been applied solely to English datasets, including the WCL dataset mentioned above (Navigli et al., 2010), the ukWaC dataset (Ferraresi et al., 2008), a large crawled dataset of the .uk domain name, and the W00 dataset, a small, expertly annotated corpus introduced by Jin et al. (2013). There does exist a smaller effort for multilingual explorations, including German (Storrer and Wellinghoff, 2006), Portuguese (Del Gaudio and Branco, 2007), and Slavic (Przepi´orkowski et al., 2007), as well as some language-independent approaches (Del Gaudio and Branco, 2009). The vast majority of these approaches are for unstructured text, typically scraped from online sources, as in the ukWaC dataset, though some interest has been given specifically for semi-structured text in legal contracts (see e.g. Curtotti and McCreath Definition extraction has been a popular topic in NLP research fo"
W19-4015,storrer-wellinghoff-2006-automated,0,0.313153,"class lattices for both hypernym detection and definition extraction, and Yin and Roth (2018) proved the effectiveness of including definitions in the training of hypernym detection models. Most work on definition extraction has been applied solely to English datasets, including the WCL dataset mentioned above (Navigli et al., 2010), the ukWaC dataset (Ferraresi et al., 2008), a large crawled dataset of the .uk domain name, and the W00 dataset, a small, expertly annotated corpus introduced by Jin et al. (2013). There does exist a smaller effort for multilingual explorations, including German (Storrer and Wellinghoff, 2006), Portuguese (Del Gaudio and Branco, 2007), and Slavic (Przepi´orkowski et al., 2007), as well as some language-independent approaches (Del Gaudio and Branco, 2009). The vast majority of these approaches are for unstructured text, typically scraped from online sources, as in the ukWaC dataset, though some interest has been given specifically for semi-structured text in legal contracts (see e.g. Curtotti and McCreath Definition extraction has been a popular topic in NLP research for well more than a decade, but has been historically limited to welldefined, structured, and narrow conditions. In"
W19-4015,S18-2025,0,0.0208554,"). Even so, these methods require both term and definition to appear in the same sentence and for terms to appear before definitions. Hypernym detection, a related field, has also garnered interest for quite some time (see e.g., Hearst (1992); Snow et al. (2005); Ritter et al. (2009); Shwartz et al. (2017)). Because many hypernym glosses follow the pattern X, such as Y or X is a (type of) Y, this work contains a subset of cases considered for definition extraction. Navigli and Velardi (2010) demonstrated the use of word class lattices for both hypernym detection and definition extraction, and Yin and Roth (2018) proved the effectiveness of including definitions in the training of hypernym detection models. Most work on definition extraction has been applied solely to English datasets, including the WCL dataset mentioned above (Navigli et al., 2010), the ukWaC dataset (Ferraresi et al., 2008), a large crawled dataset of the .uk domain name, and the W00 dataset, a small, expertly annotated corpus introduced by Jin et al. (2013). There does exist a smaller effort for multilingual explorations, including German (Storrer and Wellinghoff, 2006), Portuguese (Del Gaudio and Branco, 2007), and Slavic (Przepi´"
W19-8632,N19-1168,1,0.825976,"tion Long documents can be difficult and timeconsuming to read and comprehend, especially for people with reading or visual impairments. Before deciding whether to read a document or to find some information faster, it may be helpful to have an overview first. However, this is not possible with screen readers or without advanced skimming/reading skills. Moreover, it is easy to lose track of the overall story and message. Previous work showed that presenting a brief summary for each section of a text can help readers understand a text better (Kintsch and Van Dijk, 1978; Wiley and Rayner, 2000; Gehrmann et al., 2019), increase the reading speed (Bransford and Johnson, 1972) and improve the recall of content (Dooling and Lachman, 1971; Smith and Swinney, 1992). To that end, we introduce in this work a web-based assistive text viewer that presents a document in an accessible way. The tool is accessible online at https://github.com/ Franck-Dernoncourt/margincall. 2.2 User Interface Margin Call displays the generated summary next to each corresponding paragraph and highlights the sentence in the paragraph the summary was extracted from, as shown in Figure 1. This lets the user know which sentence in the parag"
Y18-1071,Y18-1000,0,0.206941,"Missing"
Y18-1071,W04-1016,0,0.149412,"Missing"
Y18-1071,W16-3201,1,0.793163,"nt ratings, as raters tend to be more consistent in pairwise comparisons than when scoring directly (Agarwala, 2018). Vote Methodology Using two batches, each consisting of 140 Mechanical Turk users with the same English fluency requirement as the human agreement task, annotators were instructed to “upvote” and “downvote” individual highlights that they believed help identify the main point(s) of the document. Annotators were shown two different highlighted versions, generated from a set of 5 models: the summarization models Recollect (Modani and others, 2015; Modani et al., 2016) and Sedona (Elhoseiny et al., 2016), the SMMRY summarizer (smmry.com), and two models derived from the data collected in the previous human agreement task representing the most common and least common human-selected highlights. Models were completely randomized and anonymized, both in location (e.g., left or right side of the content frame) and pairing. Each batch of annotators worked on 5 documents of the same subset of 10 documents from the 2001 DUC data. Annotators had as long as needed to complete the task, as well as a brief demographics survey before the task. Similar to the human agreement task, a shortened version of th"
Y18-1071,W04-1013,0,0.0189956,"arge body of research on extractive summarization, utilizing many different approaches (see (Nenkova and MecKeown, 2012; Yogan et al., 2016) for details on extractive summarization techniques), but these works focus on summarization, and thus, evaluation is done out of the direct context of the document. That is, summaries are scored based on their completeness, coherence, and importance as a standalone paragraph rather than in line with the text. Because of this shortage of computational research, we must define a proper metric for evaluating machine-generated highlights. While ROUGE scores (Lin, 2004) continue to be the standard metric for evaluating the coherence and correctness of extractive and abstractive summaries, it is difficult to apply them to a highlighting task. Instead, we are interested in human reactions to and interest in these highlights. 3 Human Agreement Task Before developing a capable highlight generation model, we must answer whether it is possible to find a ground truth to highlights in a given document. We designed an Amazon Mechanical Turk experiment to test whether, given an appropriate stimulus question, humans could agree on which sentences in a document should b"
